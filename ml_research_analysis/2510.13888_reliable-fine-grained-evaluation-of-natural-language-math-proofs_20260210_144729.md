---
ver: rpa2
title: Reliable Fine-Grained Evaluation of Natural Language Math Proofs
arxiv_id: '2510.13888'
source_url: https://arxiv.org/abs/2510.13888
tags:
- proof
- score
- solution
- marking
- scheme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliably evaluating natural
  language mathematical proofs, which is crucial for advancing proof generation in
  large language models. The authors identify the absence of a fine-grained evaluator
  as a key bottleneck and propose a systematic methodology for developing evaluators
  that assign scores on a 0-7 scale.
---

# Reliable Fine-Grained Evaluation of Natural Language Math Proofs

## Quick Facts
- arXiv ID: 2510.13888
- Source URL: https://arxiv.org/abs/2510.13888
- Reference count: 40
- Key outcome: ProofGrader evaluator achieves MAE 0.926 against expert scores, closing 78% of the performance gap between binary evaluators and human oracle in best-of-n selection

## Executive Summary
This paper addresses the critical bottleneck of reliable evaluation for natural language mathematical proofs, which is essential for advancing proof generation in large language models. The authors identify the absence of fine-grained evaluators as the key challenge and systematically develop ProofGrader, an evaluator that combines a strong reasoning backbone with rich context from reference solutions and marking schemes. Their methodology achieves expert-level performance on a novel dataset spanning 145 problems from major math competitions, demonstrating that providing problem-specific rubrics and reference solutions to evaluators substantially reduces scoring error.

## Method Summary
The authors introduce ProofBench, a dataset of 145 competition math problems with 435 expert-annotated LLM-generated proofs, reference solutions, and marking schemes. They systematically explore the evaluator design space across backbone models, context types (reference solutions, marking schemes, both, or none), instruction styles (flexible vs. strict), and evaluation workflows. The best configuration uses o3 as backbone with reference solutions and marking schemes as context, flexible NORM instructions, and 5-run ensembling with median aggregation. Marking schemes are generated by Gemini-2.5-Pro using a specific prompt that decomposes ideal solutions into checkpoints with point values.

## Key Results
- ProofGrader achieves MAE 0.926 against expert scores, significantly outperforming naive baselines
- Best-of-n selection: ProofGrader achieves 4.14/7 at n=16, closing 78% of the gap between binary evaluator (2.48) and human oracle (4.62)
- Strongest reasoning backbone (o3) dominates other factors with effect size order: backbone >> context >> instruction
- Simple 5-run ensembling reduces variance, achieving MAE 0.926 vs 0.944 for best single run

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing reference solutions and marking schemes to the evaluator substantially reduces scoring error by grounding evaluation in problem-specific criteria.
- Mechanism: The marking scheme decomposes the ideal solution into checkpoints with point values, enabling the evaluator to recognize partial progress rather than treating proofs as correct/incorrect. Reference solutions provide a concrete anchor for sufficiency, helping distinguish fluent but incorrect reasoning from valid arguments.
- Core assumption: The evaluator model can reliably map alternative solution approaches to equivalent checkpoints when guided to do so.
- Evidence anchors:
  - [abstract]: "Our analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes... it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores"
  - [section 3.3, Table 1]: O3 with REF+MS achieves MAE 0.964 vs 1.680 with NONE context; marking scheme alone (MS) provides majority of gain over reference alone (REF)
  - [corpus]: Weak direct corpus support; related work on fine-grained evaluation (FActBench, FACE) suggests aspect-based rubrics improve evaluation, but not specifically for mathematical proofs.
- Break condition: If the marking scheme is poorly constructed or doesn't account for valid alternative approaches, strict adherence may unfairly penalize correct but unconventional proofs. Table 10 shows regenerated marking schemes degrade performance relative to human-authored ones.

### Mechanism 2
- Claim: Simple ensembling across multiple independent evaluation runs improves robustness by reducing variance in scores.
- Mechanism: Aggregating scores (mean, median, or majority voting) cancels out random fluctuations in individual evaluations while preserving the signal. Median aggregation is particularly effective for MAE, while mean improves rank correlation.
- Core assumption: Individual evaluation runs have independent error distributions around the true score.
- Evidence anchors:
  - [section 3.4, Table 4]: Five-run ensemble with median aggregation achieves MAE 0.926 vs 0.944 for best single run; RMSE drops from 1.225 to 1.169 with mean aggregation; Kendall-τ improves from 0.540 to 0.578
  - [section 3.4]: "Although the absolute performance gains may appear modest, the key advantage of ensembling is variance reduction"
  - [corpus]: No direct corpus support for ensembling in proof evaluation specifically.
- Break condition: Systematic biases (e.g., consistent over-scoring of certain proof styles) won't be corrected by ensembling; Table 1 shows bias varies by model/context configuration but is not eliminated.

### Mechanism 3
- Claim: Stronger reasoning backbone models produce better evaluators, with effect size dominating other design factors.
- Mechanism: Models capable of deeper mathematical reasoning can better identify subtle logical errors and assess partial progress. The paper finds effect sizes ordered as backbone ≫ context ≫ instruction.
- Core assumption: Reasoning capability transfers from problem-solving to evaluation; models that can solve problems better can also judge solutions better.
- Evidence anchors:
  - [section 3.3, Table 1]: O3 achieves MAE 0.964 (REF+MS) vs GPT-4O at 2.197; weaker models show higher bias and lower WTA ≤1
  - [section 3.3]: "the strength of the backbone model strongly correlates with performance"
  - [corpus]: Weak support; corpus neighbors focus on formal verification (LeanTutor, FANS) rather than LLM-as-judge backbone comparisons.
- Break condition: Section 3.5 notes evaluators struggle most on problems they cannot solve themselves: "when we examine problems that o3 (the evaluator model) cannot solve well (scoring ≤2 when acting as a prover), we find that the no-context evaluator over-scores by 1.40 points on average"

## Foundational Learning

- Concept: **LLM-as-a-Judge Paradigm**
  - Why needed here: The entire ProofGrader system is built on using LLMs to evaluate other LLMs' outputs. Understanding prompt design, output parsing (XML structure), and failure modes (length bias, self-preference) is essential.
  - Quick check question: Can you explain why binary correct/incorrect judgments underperform fine-grained 0-7 scales for best-of-n selection?

- Concept: **Marking Schemes / Rubrics in Proof Evaluation**
  - Why needed here: The paper's key innovation is providing problem-specific rubrics with checkpoints, zero-credit items, and deductions. Understanding how these are constructed (Appendix A.6) and how they guide scoring is critical.
  - Quick check question: How should a marking scheme handle two different but equally valid solution approaches?

- Concept: **Best-of-N Selection as Reward Model Proxy**
  - Why needed here: The paper validates ProofGrader primarily through best-of-n selection performance, a standard proxy for reward model quality. Understanding why this matters—selection correlates with downstream training utility—is essential for interpreting results.
  - Quick check question: At n=16, what does it mean that ProofGrader closes 78% of the gap between a binary evaluator and human oracle?

## Architecture Onboarding

- Component map:
  - Generator -> Marking Scheme Generator (M_MS) -> Evaluator Backbone -> Context Assembler -> Ensembling Layer

- Critical path:
  1. Collect problem + reference solution(s) from competition sources
  2. Generate marking scheme using M_MS with prompt from Appendix A.6
  3. Generate candidate proofs from generator models
  4. Run evaluator (ProofGrader configuration: O3 + REF+MS + NORM instruction) multiple times
  5. Aggregate scores via median for final 0-7 rating

- Design tradeoffs:
  - **Context richness vs. cost**: REF+MS performs best but requires human-authored solutions; MS alone captures ~80% of gains (Table 1)
  - **Instruction flexibility vs. guidance**: Strong models (O3) benefit from flexible NORM instructions; weaker models need STRICT guidance (Table 2)
  - **Single-pass vs. staged**: Staged pipelines help O4-MINI but hurt O3 (Tables 5-6); complexity not universally beneficial
  - **Ensembling vs. latency**: 5-run ensembling improves MAE by ~0.02 but 5x inference cost

- Failure signatures:
  - **Over-scoring fluent but incorrect proofs**: Without context, evaluators over-score by 1.7 points for low-quality proofs (Section 3.5)
  - **Evaluator-prover capability gap**: When evaluator can't solve the problem itself, context-free evaluation degrades significantly
  - **Marking scheme misalignment**: Using different marking schemes than human graders reduces accuracy (Table 10)
  - **Self-family bias**: Evaluators show highest MAE on outputs from their own model family (Table 3)

- First 3 experiments:
  1. **Context ablation on held-out problems**: Take 20 problems not in ProofBench, generate marking schemes, compare evaluator performance with REF+MS vs. MS vs. REF vs. NONE. Expected: MS alone should capture most gains over NONE.
  2. **Ensembling scale test**: Run the same evaluator with n=1,3,5,7,9 runs and plot MAE/Kendall-τ vs. n. Expected: Diminishing returns after n=5; identify knee point.
  3. **Alternative backbone comparison**: Swap O3 for a strong open-source model (e.g., DeepSeek-R1) with identical context/instructions. Expected: Performance gap proportional to backbone reasoning capability as shown in Table 1.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the ProofGrader methodology effectively generalize to research-grade mathematics or specialized domains like algebraic geometry, which differ structurally from Olympiad-style proofs?
- **Basis in paper:** [Explicit] The authors state their scope is limited to olympiad-style proofs and "does not yet cover research-grade arguments, specialized domains (e.g., algebraic geometry), or applied settings; extending to these regimes is an important next step."
- **Why unresolved:** The current ProofBench dataset is constructed solely from high school and undergraduate competitions (USAMO, IMO, Putnam), which prioritize concise, tricky logical steps. Research mathematics often involves longer, denser arguments and specialized notation not present in the training data.
- **What evidence would resolve it:** Testing the evaluator on a new dataset of advanced mathematical proofs (e.g., from arXiv) and measuring the correlation between the evaluator's score and expert assessment of correctness in those specialized domains.

### Open Question 2
- **Question:** Can fine-grained evaluators like ProofGrader be successfully utilized as reward signals for reinforcement learning to improve a model's proof generation capabilities?
- **Basis in paper:** [Explicit] The authors note, "Beyond evaluation, our graders could be used to synthesize supervision for reward-model training... we have not yet train with it; incorporating it into proof generator training is a promising future direction."
- **Why unresolved:** The paper validates the evaluator via best-of-n selection (test-time compute), but it does not demonstrate that the signal is robust enough to prevent reward hacking or model collapse during the training (gradient descent) process.
- **What evidence would resolve it:** Running an RL training loop (e.g., RLHF or PPO) using ProofGrader as the reward model and measuring the resulting generator's performance on a held-out set of competition problems.

### Open Question 3
- **Question:** Can fully open-source models match the performance of proprietary models (like O3 or Gemini) in fine-grained proof evaluation using the proposed design methodology?
- **Basis in paper:** [Explicit] The authors acknowledge, "Our strongest evaluators currently rely on closed models... further improving fully open-source evaluators to match those based on closed models is an important direction."
- **Why unresolved:** The study found a strong correlation between backbone reasoning strength and evaluation accuracy. Open models (like DeepSeek-R1) performed worse in the ablation studies, and it is unclear if prompt engineering or ensembling alone can close this specific capability gap.
- **What evidence would resolve it:** Fine-tuning a strong open-source model on the ProofBench dataset or applying the ensembling techniques specifically to open-source backbones to see if they can achieve a comparable MAE (approx. 0.92) to O3.

## Limitations
- The evaluation relies on proprietary models (o3, Gemini-2.5-Pro) that may not be widely accessible
- Marking scheme generation requires significant manual effort or access to the same model family
- The study focuses on competition mathematics, which may not generalize to broader mathematical domains
- Best-of-n selection evaluation is a proxy metric that may not fully capture downstream training utility

## Confidence
- **High Confidence**: The empirical finding that stronger reasoning backbones significantly improve evaluator performance (MAE 0.964 for o3 vs 2.197 for GPT-4O with REF+MS context)
- **Medium Confidence**: The claim that reference solutions and marking schemes are the primary driver of evaluator performance
- **Medium Confidence**: The effectiveness of simple ensembling for variance reduction

## Next Checks
1. **Context Ablation on Held-out Problems**: Evaluate the same evaluator with REF+MS, MS-only, REF-only, and NONE contexts on 20 new competition problems. This will validate that marking schemes alone capture most of the performance gains.

2. **Ensembling Scale Test**: Systematically vary the number of evaluation runs (n=1,3,5,7,9) and measure the marginal benefit to MAE and Kendall-τ. This will identify the optimal trade-off between performance and computational cost.

3. **Alternative Backbone Comparison**: Replace o3 with a strong open-source reasoning model (e.g., DeepSeek-R1) using identical context and instructions. This will test whether the performance gains are due to specific model capabilities or general reasoning strength.