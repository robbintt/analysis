---
ver: rpa2
title: 'Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning
  for LLMs'
arxiv_id: '2506.04044'
source_url: https://arxiv.org/abs/2506.04044
tags:
- unlearning
- gradients
- task
- retain
- sophia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LIBU, a two-phase unlearning algorithm that
  combines influence functions with Sophia optimization to remove specific knowledge
  from large language models. The method uses Fisher Information approximation for
  parameter-wise updates and second-order optimization for stability.
---

# Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs

## Quick Facts
- arXiv ID: 2506.04044
- Source URL: https://arxiv.org/abs/2506.04044
- Authors: Aleksey Kudelya; Alexander Shirnin
- Reference count: 6
- Primary result: LIBU achieves 0.283 regurgitation rate while maintaining 0.469 MMLU accuracy on OLMo-7B

## Executive Summary
This paper introduces LIBU, a two-phase unlearning algorithm that combines influence functions with Sophia optimization to remove specific knowledge from large language models. The method uses Fisher Information approximation for parameter-wise updates and second-order optimization for stability. Tested on the OLMo-7B model for SemEval-2025 Task 4, LIBU achieved a regurgitation rate of 0.283 and maintained MMLU accuracy of 0.469, exceeding the competition threshold. The approach addresses the challenge of machine unlearning by efficiently removing sensitive data while preserving model utility, without requiring full retraining. Results demonstrate that balanced hyperparameter tuning is critical, with overly aggressive configurations causing catastrophic forgetting and overly conservative ones leaving residual influences.

## Method Summary
LIBU implements a two-phase algorithm for machine unlearning. Phase 1 uses influence functions to compute how much each training sample affects the model's parameters, identifying which parameters to update for forgetting specific knowledge. Phase 2 applies Sophia optimization, a second-order optimization technique that provides stability during parameter updates. The algorithm approximates the Fisher Information matrix to guide parameter-wise updates, allowing selective forgetting while preserving utility. The method is designed to be lightweight and computationally efficient, avoiding the need for full model retraining. The approach leverages LoRA (Low-Rank Adaptation) enhancements to further reduce computational overhead during the unlearning process.

## Key Results
- LIBU achieved 0.283 regurgitation rate on SemEval-2025 Task 4, exceeding competition threshold
- Maintained MMLU accuracy of 0.469 on OLMo-7B, demonstrating preserved model utility
- Demonstrated computational efficiency through lightweight design without full retraining
- Showed sensitivity to hyperparameter tuning, with aggressive settings causing catastrophic forgetting

## Why This Works (Mechanism)
LIBU works by combining influence functions to identify which parameters store the knowledge to be forgotten with Sophia optimization to perform stable parameter updates. The influence function approach traces back from the target output to identify training samples that most strongly influence the model's behavior. The Fisher Information approximation then guides parameter-wise updates, allowing the algorithm to selectively modify only the parameters most responsible for storing the target knowledge. Sophia optimization provides second-order optimization capabilities that stabilize the update process, preventing the catastrophic forgetting that can occur with naive gradient-based approaches. This combination allows precise unlearning while maintaining the model's overall knowledge structure.

## Foundational Learning
- Influence Functions: Trace how individual training samples affect model parameters - needed to identify which parameters store target knowledge for removal; quick check: verify influence scores correlate with expected knowledge patterns
- Fisher Information Matrix: Measures parameter sensitivity to data - needed to guide selective parameter updates during unlearning; quick check: confirm diagonal approximation maintains sufficient accuracy for update direction
- Second-Order Optimization: Uses curvature information for stable updates - needed to prevent catastrophic forgetting during parameter modifications; quick check: monitor loss stability during unlearning iterations
- LoRA (Low-Rank Adaptation): Parameter-efficient fine-tuning technique - needed to reduce computational overhead of unlearning updates; quick check: verify rank decomposition maintains update effectiveness
- Parameter-Wise Updates: Selective modification of individual parameters - needed for precise unlearning without global retraining; quick check: measure parameter change distribution across unlearning phases

## Architecture Onboarding
Component Map: Influence Functions -> Fisher Approximation -> Sophia Optimization -> Parameter Updates
Critical Path: Knowledge identification through influence functions → parameter sensitivity estimation via Fisher approximation → stable updates using Sophia optimization → final parameter modification
Design Tradeoffs: Computational efficiency vs. approximation accuracy in Fisher Information; unlearning aggressiveness vs. retention of useful knowledge; parameter update stability vs. unlearning speed
Failure Signatures: Catastrophic forgetting (overly aggressive updates), residual knowledge retention (insufficient updates), optimization divergence (poor Fisher approximation), slow convergence (inadequate Sophia tuning)
First Experiments:
1. Baseline influence function calculation on small model to verify knowledge tracing accuracy
2. Fisher Information diagonal approximation validation on parameter sensitivity estimation
3. Sophia optimization stability test on parameter update process with controlled forgetting targets

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the specific contributions of the influence-based update versus the Sophia optimization phase to the final unlearning performance?
- Basis in paper: The authors state, "Due to computational constraints... we leave fine-grained ablations of individual components (Sophia, Influence Functions) as future work."
- Why unresolved: The study prioritizes evaluating the full system over isolating components, making the distinct utility of the second-order optimization versus the influence function unclear.
- What evidence would resolve it: An ablation study measuring MMLU accuracy and regurgitation rates for the full LIBU pipeline against variants using only Phase 1 or Phase 2.

### Open Question 2
- Question: Can the LIBU algorithm maintain stability and efficiency when applied to larger state-of-the-art models (e.g., 70B+ parameters)?
- Basis in paper: The limitations section notes the evaluation "has been limited to relatively small models (1B and 7B parameters), leaving the behavior of current large-scale SOTA models unknown."
- Why unresolved: The scalability of the diagonal Fisher Information approximation and gradient accumulation strategy to architectures with significantly higher parameter counts is untested.
- What evidence would resolve it: Benchmarks of LIBU on models exceeding 70B parameters, monitoring for optimization divergence and memory constraints during the unlearning process.

### Open Question 3
- Question: How does the algorithm perform when the target forget set contains information highly similar to the retain set?
- Basis in paper: The authors list the challenge where "forget and retain sets may contain highly similar information" and designate handling this as future work.
- Why unresolved: The Fisher approximation dampens updates for parameters deemed important to the retain set; high semantic overlap might prevent the model from unlearning critical features.
- What evidence would resolve it: Experiments using datasets with controlled semantic overlap between forget and retain samples to quantify the resulting utility loss or retention failure.

## Limitations
- Effectiveness highly sensitive to hyperparameter tuning, with aggressive configurations causing catastrophic forgetting
- Evaluation limited to single task (SemEval-2025 Task 4) and model (OLMo-7B), limiting generalizability
- Computational efficiency claims lack comparative analysis against alternative unlearning methods
- Fisher Information approximation may introduce errors affecting unlearning precision, not thoroughly explored

## Confidence
- LIBU's effectiveness on SemEval-2025 Task 4: **High** - Results are specific and measurable against established competition metrics
- Computational efficiency and lightweight design: **Medium** - Supported by design rationale but lacks comparative benchmarks
- General applicability across model architectures: **Low** - Limited to single model and task, with explicit sensitivity to hyperparameters

## Next Checks
1. Evaluate LIBU across multiple model architectures (varying sizes and pretraining objectives) to assess generalizability beyond OLMo-7B
2. Conduct ablation studies isolating the impact of Fisher Information approximation versus alternative influence estimation methods
3. Perform cross-task validation using datasets with different knowledge types and forgetting requirements to test robustness against catastrophic forgetting