---
ver: rpa2
title: Sample-Optimal Agnostic Boosting with Unlabeled Data
arxiv_id: '2503.04706'
source_url: https://arxiv.org/abs/2503.04706
tags:
- boosting
- algorithm
- unlabeled
- learning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies agnostic boosting when unlabeled samples are\
  \ available. In the standard agnostic setting, the best known boosting algorithms\
  \ have sample complexity (log |H|)/\u03B5^4, which is worse than the optimal (log\
  \ |H|)/\u03B5^2 achievable by Empirical Risk Minimization (ERM)."
---

# Sample-Optimal Agnostic Boosting with Unlabeled Data

## Quick Facts
- arXiv ID: 2503.04706
- Source URL: https://arxiv.org/abs/2503.04706
- Reference count: 40
- Key result: Agnostic boosting with unlabeled data achieving optimal labeled sample complexity matching ERM

## Executive Summary
This paper introduces a novel approach to agnostic boosting that leverages unlabeled data to achieve optimal labeled sample complexity matching Empirical Risk Minimization (ERM). Traditional agnostic boosting suffers from a sample complexity of (log |H|)/ε^4, while ERM achieves the optimal (log |H|)/ε^2. The authors design a computationally efficient algorithm that achieves the ERM-optimal labeled sample complexity while requiring only (log |H|)/ε^4 unlabeled samples, with total complexity never exceeding previous methods. Further improvements reduce unlabeled sample requirements to (log |H|)/ε^3 using data reuse techniques.

## Method Summary
The key innovation is a potential function φ(z,y) = ψ(z) - yz (Huber loss) that can be decomposed into parts estimable by labeled and unlabeled data separately. Algorithm 1 mixes labeled data 50/50 with pseudo-labeled unlabeled data where p_t(x) = (1 - ψ'(H_t(x)))/2. The weak learner receives samples from this mixed distribution and returns a hypothesis W_t. If corr(W_t) > τ, the ensemble updates H_{t+1} = H_t + ηW_t/γ; otherwise, it updates H_{t+1} = H_t - η·sign(H_t). The method uses decision stumps as weak learners and employs fractional relabeling from Kanade & Kalai [2009].

## Key Results
- Achieves labeled sample complexity (log |H|)/ε^2, matching optimal ERM complexity
- Requires only (log |H|)/ε^4 unlabeled samples with total complexity not exceeding previous methods
- Further improves unlabeled sample efficiency to (log |H|)/ε^3 using data reuse techniques
- Demonstrates improved performance on UCI datasets with label noise compared to PAB baseline

## Why This Works (Mechanism)
The algorithm exploits the structure of the Huber loss potential function, which can be decomposed into terms that depend on labeled and unlabeled data separately. This decomposition allows the algorithm to use unlabeled data to estimate parts of the gradient while using labeled data for other components, achieving better sample efficiency than using either source alone.

## Foundational Learning
- **Agnostic learning**: Learning when no perfect hypothesis exists in the class
  - *Why needed*: The paper addresses agnostic boosting where the best hypothesis may have non-zero error
  - *Quick check*: Verify understanding that corr_D(h) = E[yh(x)] measures correlation rather than zero-one loss

- **Boosting**: Sequential ensemble method that converts weak learners to strong learners
  - *Why needed*: The algorithm builds on boosting framework to combine weak hypotheses
  - *Quick check*: Understand that weak learners need only achieve correlation slightly better than random guessing

- **Huber loss**: Loss function ψ(z) = |z| - 1/2 (if |z|>1) else z²/2 with derivative ψ'(z) = sign(z)·min{1,|z|}
  - *Why needed*: The Huber loss potential enables decomposition between labeled and unlabeled data
  - *Quick check*: Verify ψ'(z) implementation correctly computes sign(z)·min{1,|z|}

- **Fractional relabeling**: Technique allowing both (x,y) and (x,-y) with probability weights
  - *Why needed*: Improves convergence and allows better use of limited labeled data
  - *Quick check*: Implement relabeling scheme correctly with probability weights as in Kanade & Kalai [2009]

## Architecture Onboarding

Component map: Labeled data + Unlabeled data -> Mixed distribution -> Weak learner -> Ensemble update -> Potential function -> Algorithm 1

Critical path: Algorithm 1 orchestrates the entire process, combining labeled and unlabeled data through the potential function decomposition to achieve optimal sample complexity.

Design tradeoffs: The decomposition approach enables optimal labeled sample complexity but requires unlabeled data. The Huber loss is chosen specifically for its decomposable structure, trading off other possible loss functions.

Failure signatures: If potential φ(z,y) doesn't decrease per round, check ψ'(z) implementation and label flipping probability p_t(x) = (1 - ψ'(H_t(x)))/2. If no improvement over PAB, verify unlabeled data utilization and check that Algorithm 1 reuses labeled samples across rounds.

First experiments: 1) Verify Huber potential implementation with φ(z,y) = ψ(z) - yz decreases monotonically across rounds. 2) Test Algorithm 1 on synthetic data with known weak learner advantage. 3) Compare Algorithm 1 with PAB baseline on a simple UCI dataset with moderate label noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the unlabeled sample complexity be improved beyond O(log|H|/ε³), ideally to O(log|H|/ε²)?
- **Basis in paper:** [explicit] Section 4 states: "Applying techniques from Ghai and Singh [2024] selectively to unlabeled data, we further reduce the number of unlabeled samples needed to log(|H|)/ε³." The labeled complexity achieves the optimal ε⁻² rate, but unlabeled remains at ε⁻³.
- **Why unresolved:** The data reuse technique improves from ε⁻⁴ to ε⁻³ but appears to hit a ceiling; the decomposition of the potential function may inherently require more unlabeled samples due to higher variance in estimating the feature-dependent term.
- **What evidence would resolve it:** A lower bound proof showing ε⁻³ is necessary for any boosting algorithm using this potential decomposition approach, or a new potential/algorithm achieving ε⁻² unlabeled complexity.

### Open Question 2
- **Question:** Can this approach extend to multiclass or online agnostic boosting settings?
- **Basis in paper:** [inferred] The paper explicitly restricts to binary classification (labels ∈ {±1}) and notes that prior work extended agnostic boosting to online [Brukhim et al., 2020] and multiclass [Raman and Tewari, 2022] settings. The potential function φ(z,y) = ψ(z) - yz is designed specifically for binary labels.
- **Why unresolved:** The decomposition relies on y ∈ {±1} for the linear term −yz; multiclass would require a different potential structure, and online settings require fresh samples per round, conflicting with the data reuse strategy.
- **What evidence would resolve it:** A multiclass potential with similar separable gradient structure, or adaptation of the online boosting framework to incorporate unlabeled samples.

## Limitations
- Missing experimental hyperparameters (η, T, τ, m, S, U values)
- Weak learner advantage parameter γ not specified
- Train/validation split strategy for hypothesis selection unspecified
- Decision stump implementation details for pseudo-labeled samples unclear

## Confidence
- High confidence in theoretical analysis and algorithm design
- Medium confidence in experimental implementation feasibility
- Low confidence in achieving exact match to reported results without hyperparameter tuning

## Next Checks
1. Verify potential function implementation by checking φ(z,y) = ψ(z) - yz decreases monotonically across rounds
2. Implement fractional relabeling scheme from Kanade & Kalai [2009] correctly with probability weights
3. Run ablation study comparing Algorithm 1 with and without unlabeled data to confirm claimed improvements over PAB baseline