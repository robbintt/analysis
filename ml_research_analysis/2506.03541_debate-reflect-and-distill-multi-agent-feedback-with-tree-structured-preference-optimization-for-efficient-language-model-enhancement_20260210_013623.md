---
ver: rpa2
title: 'Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference
  Optimization for Efficient Language Model Enhancement'
arxiv_id: '2506.03541'
source_url: https://arxiv.org/abs/2506.03541
tags:
- debate
- student
- reasoning
- teacher
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Debate and Reflect (D&R) framework
  that improves small language models by engaging them in multi-turn debates with
  stronger teacher models and leveraging structured feedback for iterative refinement.
  The approach uses a tree-structured direct preference optimization (T-DPO) to distill
  debate logs into hierarchical preference trees, enabling the student model to learn
  from both correct reasoning paths and error corrections.
---

# Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement

## Quick Facts
- arXiv ID: 2506.03541
- Source URL: https://arxiv.org/abs/2506.03541
- Authors: Xiaofeng Zhou; Heyan Huang; Lizi Liao
- Reference count: 34
- Key outcome: D&R framework improves small language models through multi-turn debates and structured feedback, achieving up to 18.41% improvement on biology tasks and 9.30% on MATH benchmarks

## Executive Summary
This paper introduces the Debate and Reflect (D&R) framework, a novel approach to enhance small language models by engaging them in multi-turn debates with stronger teacher models and leveraging structured feedback for iterative refinement. The method uses tree-structured direct preference optimization (T-DPO) to distill debate logs into hierarchical preference trees, enabling the student model to learn from both correct reasoning paths and error corrections. Empirical results across MMLU-Pro and MATH benchmarks show that D&R significantly outperforms traditional distillation methods, achieving substantial improvements in reasoning tasks while enhancing inference efficiency and equipping models with self-correction abilities.

## Method Summary
The D&R framework consists of a two-stage distillation pipeline: first, Supervised Fine-Tuning (SFT) on correct debate responses; second, Tree-structured Direct Preference Optimization (T-DPO) on preference trees extracted from debate logs. The process begins with multi-turn debates between a student model (Mistral-7B-Instruct) and multiple teacher models (GPT-4o, Claude 3.5, Gemini 1.5 Pro), where each round includes self-reflection from incorrect agents and teacher feedback on errors. These debates are structured into Multi-Agent Interaction Graphs (MAGs), from which preference trees are extracted by selecting correct and incorrect responses at each turn. T-DPO then optimizes the student to prefer correct reasoning paths over incorrect ones, effectively distilling both the knowledge and the reasoning process from the debates.

## Key Results
- D&R achieves up to 18.41% improvement on MMLU-Pro biology tasks compared to baselines
- The method shows 9.30% improvement on the MATH benchmark
- Outperforms traditional distillation methods and single-teacher approaches across multiple student model sizes