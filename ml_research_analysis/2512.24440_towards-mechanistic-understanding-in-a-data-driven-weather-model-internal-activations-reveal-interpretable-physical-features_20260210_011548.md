---
ver: rpa2
title: 'Towards mechanistic understanding in a data-driven weather model: internal
  activations reveal interpretable physical features'
arxiv_id: '2512.24440'
source_url: https://arxiv.org/abs/2512.24440
tags:
- features
- feature
- graphcast
- interpretable
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors apply sparse autoencoders to uncover interpretable
  physical features in GraphCast, a state-of-the-art deep learning weather model.
  By analyzing intermediate layer activations, they discover distinct features corresponding
  to tropical cyclones, atmospheric rivers, diurnal/seasonal patterns, large-scale
  precipitation, and sea-ice extent.
---

# Towards mechanistic understanding in a data-driven weather model: internal activations reveal interpretable physical features

## Quick Facts
- arXiv ID: 2512.24440
- Source URL: https://arxiv.org/abs/2512.24440
- Reference count: 40
- Authors find interpretable physical features in GraphCast's internal activations using sparse autoencoders

## Executive Summary
This study applies sparse autoencoders to uncover interpretable physical features in GraphCast, a state-of-the-art deep learning weather model. By analyzing intermediate layer activations, the researchers discover distinct features corresponding to tropical cyclones, atmospheric rivers, diurnal/seasonal patterns, large-scale precipitation, and sea-ice extent. Using logistic probes, they identify features encoding tropical cyclones and atmospheric rivers with high F1-scores. To validate physical consistency, they demonstrate that modifying the tropical cyclone feature leads to predictable and physically plausible changes in predicted hurricane strength, maintaining conservation laws and force balances.

## Method Summary
The researchers applied sparse autoencoders to analyze intermediate layer activations in GraphCast, decomposing these activations into a sparse basis of interpretable features. They trained logistic probes to identify specific weather phenomena encoded by these features and conducted controlled experiments by modifying feature activations to test physical consistency. The methodology involved systematic perturbation of discovered features and measurement of resulting changes in model predictions, particularly focusing on tropical cyclone strength predictions to verify conservation laws and force balances were maintained.

## Key Results
- Discovered distinct features corresponding to tropical cyclones, atmospheric rivers, diurnal/seasonal patterns, large-scale precipitation, and sea-ice extent
- Achieved high F1-scores for tropical cyclone and atmospheric river identification using logistic probes
- Validated physical consistency by demonstrating predictable and physically plausible changes in hurricane strength when modifying tropical cyclone features

## Why This Works (Mechanism)
The sparse autoencoder architecture effectively decomposes complex neural network activations into interpretable components by enforcing sparsity constraints during training. This decomposition reveals that GraphCast has learned distinct representations for fundamental atmospheric phenomena, which manifest as coherent activation patterns in intermediate layers. The logistic probes provide a systematic way to identify which features correspond to specific weather patterns, while controlled perturbation experiments demonstrate that these learned representations maintain physical consistency when modified.

## Foundational Learning
- Sparse autoencoders: dimensionality reduction technique that learns sparse representations, needed for decomposing complex activations into interpretable features; quick check: verify reconstruction error vs. sparsity tradeoff
- Logistic probes: simple classifiers trained on intermediate representations, needed for automated feature-phenomenon mapping; quick check: evaluate F1-scores across different weather phenomena
- Feature perturbation: controlled modification of discovered features, needed for testing physical consistency; quick check: verify conservation laws hold after perturbation

## Architecture Onboarding
**Component map:** Sparse autoencoder -> Logistic probes -> Feature perturbation -> Physical validation
**Critical path:** Input data → GraphCast forward pass → Activation extraction → Sparse autoencoder decomposition → Feature interpretation → Logistic probe training → Controlled perturbation → Physical consistency verification
**Design tradeoffs:** The study chose GraphCast specifically for its state-of-the-art performance, but this limits generalizability; sparse autoencoders balance interpretability with reconstruction accuracy; logistic probes offer simplicity but may miss complex feature-phenomenon relationships
**Failure signatures:** If features aren't interpretable, the sparse autoencoder may need different hyperparameters; low F1-scores suggest features don't cleanly separate weather phenomena; physical inconsistencies during perturbation indicate learned representations don't capture underlying physics
**3 first experiments:** 1) Apply same methodology to alternative deep learning weather models to assess generalizability; 2) Conduct systematic perturbation experiments across multiple atmospheric phenomena beyond tropical cyclones; 3) Implement cross-validation of feature interpretations by independent atmospheric scientists

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on a single deep learning weather model (GraphCast), limiting generalizability to other architectures or training regimes
- Feature discovery process relies on human interpretation of activation patterns, introducing potential subjectivity in labeling and understanding discovered features
- Validation of physical consistency remains preliminary, limited to a single weather phenomenon and specific metrics

## Confidence
- High confidence in the technical methodology for feature discovery using sparse autoencoders
- Medium confidence in the interpretability of discovered features, pending additional validation
- Medium confidence in the physical consistency results, requiring broader testing across weather phenomena
- Low confidence in generalizability beyond the specific GraphCast model configuration

## Next Checks
1. Apply the same methodology to alternative deep learning weather models to assess generalizability of discovered features and interpretation methods
2. Conduct systematic perturbation experiments across multiple atmospheric phenomena beyond tropical cyclones to verify consistent physical behavior
3. Implement cross-validation of feature interpretations by independent atmospheric scientists to assess subjectivity in feature labeling and understanding