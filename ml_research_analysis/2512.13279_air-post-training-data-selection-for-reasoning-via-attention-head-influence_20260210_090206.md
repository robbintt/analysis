---
ver: rpa2
title: 'AIR: Post-training Data Selection for Reasoning via Attention Head Influence'
arxiv_id: '2512.13279'
source_url: https://arxiv.org/abs/2512.13279
tags:
- reasoning
- data
- which
- theta
- separation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AIR (Attention Influence for Reasoning),
  a mechanism-driven data selection framework for post-training reasoning in LLMs.
  The core idea is to identify reasoning-critical attention heads, construct a weakened
  reference model by disabling their influence, and quantify loss divergence to score
  data importance at both step and sample levels.
---

# AIR: Post-training Data Selection for Reasoning via Attention Head Influence

## Quick Facts
- arXiv ID: 2512.13279
- Source URL: https://arxiv.org/abs/2512.13279
- Authors: Jinrui Liu; Jeff Wu; Xuanguang Pan; Gavin Cheung; Shuai Ma; Chongyang Tao
- Reference count: 31
- Key outcome: Introduces AIR, a post-training data selection framework that uses attention head influence to identify reasoning-critical data, achieving state-of-the-art performance on reasoning benchmarks with significantly less data than manual curation.

## Executive Summary
AIR (Attention Influence for Reasoning) introduces a mechanism-driven approach to post-training data selection for reasoning tasks in large language models. The method identifies reasoning-critical attention heads through copy-paste detection, creates a weakened reference model by masking these heads, and uses loss divergence between base and weakened models to score data importance. This enables both sample-level selection and step-level weighted fine-tuning, consistently improving reasoning accuracy across multiple benchmarks while requiring significantly less training data than manual curation approaches.

## Method Summary
AIR operates through a three-stage pipeline: (1) Head identification using Retrieval Score to find top δ% retrieval heads via copy-paste detection, (2) Weakened reference model creation by masking these heads with uniform attention, and (3) Loss divergence computation between base and weakened models to score token, step, and sample importance. The method supports two selection modes: sample-level selection using relative loss divergence scores and step-level weighted fine-tuning that boosts critical reasoning steps. Experiments demonstrate AIR's effectiveness on AIME, MATH500, and GPQA benchmarks using Qwen2.5 models.

## Key Results
- AIR-selected 1K samples match or exceed manually curated s1K/s1K-1.1 on AIME/MATH500/GPQA benchmarks
- Step-level AIR with 20% critical step weighting (α=2) improves reasoning accuracy while reducing training time
- AIR consistently outperforms heuristic baselines (random, length-based, complexity-based selection)
- Achieves comparable performance to models trained on much larger datasets while using significantly less data

## Why This Works (Mechanism)

### Mechanism 1: Retrieval Head Identification via Copy-Paste Detection
Certain attention heads are functionally specialized for context fidelity. A head h is classified as a retrieval head if the generated token exists in context (C1) and the context position receives maximal attention weight in head h (C2). The Retrieval Score Rh quantifies this as |gh ∩ k| / |k|, identifying heads causally involved in reasoning through accurate context retrieval.

### Mechanism 2: Weakened Reference Model via Head Masking
Selectively disabling retrieval heads creates controlled degradation in reasoning capability. The top δ percentile of heads (Hcritical) are masked by setting attention weights to uniform (aij = 1/L), creating θref with reasoning-specific deficits while keeping parameters identical to θbase. This enables isolation of reasoning-specific dependencies via loss comparison.

### Mechanism 3: Loss Divergence as Attention Influence Score
Cross-entropy loss difference between base and weakened models quantifies token, step, or sample dependence on reasoning-critical mechanisms. Token-level divergence ∆ℓ(xt) = ℓ(θref, xt) − ℓ(θbase, xt) indicates reliance on retrieval mechanisms. Scores aggregate to step-level (S(k)step) and sample-level (Ssamp) for selection and weighting.

## Foundational Learning

- Concept: **Retrieval / Induction Heads**
  - Why needed here: AIR's premise that certain heads are reasoning-critical derives from mechanistic interpretability work on retrieval/induction heads
  - Quick check question: On a simple copy task, which attention heads would you expect to show high retrieval scores, and why?

- Concept: **Loss-based Influence Estimation**
  - Why needed here: AIR uses loss divergence between strong and weakened models as an influence signal
  - Quick check question: If a sample has high loss under both θbase and θref but low divergence, would AIR select it? Should it?

- Concept: **Weighted Supervised Fine-Tuning (SFT)**
  - Why needed here: Step-level AIR applies different loss weights to different reasoning steps
  - Quick check question: If you apply 2× weight to 20% of steps without normalization, what happens to the effective learning rate? How does normalization prevent this?

## Architecture Onboarding

- Component map: Base model (θbase) -> Reference model (θref via head masking) -> Loss divergence module -> Selection/weighting engine

- Critical path: 1) Compute Rh for all heads on calibration data, 2) Select Hcritical, construct θref via masking, 3) Compute token-level ∆ℓ between models, 4) Aggregate to step/sample scores, 5) Select top V samples OR apply weighted SFT with critical step boost α

- Design tradeoffs:
  - δ (head masking threshold): 5% optimal; too low misses critical heads, too high includes non-reasoning heads
  - P% (critical step ratio): 20% optimal; higher includes noise, lower misses important steps
  - α (weight amplification): 2× optimal; 5-10× causes degradation due to gradient distortion
  - Granularity: Token-level is noisy; step/sample aggregation trades precision for stability

- Failure signatures:
  - Selected samples merely difficult but not reasoning-critical → marginal/no improvement
  - Retrieval heads misidentified (δ wrong) → loss divergence reflects non-reasoning factors
  - Over-aggressive weighting (high α) → model overfits to local patterns, loses reasoning coherence

- First 3 experiments:
  1. Mask identified retrieval heads and measure performance drop on reasoning vs. non-reasoning benchmarks
  2. Sweep δ ∈ {1%, 3%, 5%, 10%, 20%} and measure downstream SFT performance
  3. Compare token-level, step-level, and sample-level AIR scores against human-annotated reasoning-critical samples

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several important ones unresolved through its empirical findings.

## Limitations

- The 5% head masking threshold (δ) is empirically validated but not theoretically grounded, raising questions about generalizability across model architectures
- The method assumes uniform attention masking creates clean causal separation between retrieval and general language modeling capabilities
- Loss divergence may conflate reasoning importance with sample difficulty or surface complexity if retrieval heads are misidentified

## Confidence

- **High Confidence**: AIR's empirical performance improvements over heuristic baselines on established reasoning benchmarks; mathematical soundness of step-level weighting mechanism
- **Medium Confidence**: Causal link between identified retrieval heads and reasoning capability; need for ablation studies isolating mechanism contributions
- **Low Confidence**: Generalizability of 5% head masking threshold; assumption that uniform attention masking creates clean causal separation

## Next Checks

1. **Causal Intervention Validation**: Perform head ablation studies where identified retrieval heads are either masked or artificially enhanced, then measure reasoning performance changes to establish genuine causality.

2. **Domain Transfer Analysis**: Apply AIR to reasoning tasks in domains very different from calibration data (e.g., logical puzzles vs. mathematical problems) to test threshold and head selection generalizability.

3. **Sample Quality Annotation**: Manually annotate a subset of AIR-selected samples to verify they contain superior reasoning traces compared to baseline-selected samples, distinguishing between difficulty-based and quality-based selection.