---
ver: rpa2
title: 'HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval'
arxiv_id: '2506.22356'
source_url: https://arxiv.org/abs/2506.22356
tags:
- retrieval
- passages
- context
- system
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HLTCOE team developed a RAG system for the LiveRAG challenge
  using a GPT-researcher framework with ColBERT retrieval. The system generated queries
  based on context, retrieved passages from the FineWeb10-BT collection using a PLAID-X
  compressed index, filtered snippets by cosine similarity with the original question,
  and generated final answers using Falcon3-10B-Instruct.
---

# HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval

## Quick Facts
- **arXiv ID:** 2506.22356
- **Source URL:** https://arxiv.org/abs/2506.22356
- **Reference count:** 14
- **Primary result:** 5th place in automatic correctness (score 1.07) and 14th in faithfulness (score 0.34) at LiveRAG challenge

## Executive Summary
The HLTCOE team developed a RAG system for the LiveRAG challenge using a GPT-researcher framework with ColBERT retrieval. The system employed a pipeline approach where queries were generated based on context, passages were retrieved from the FineWeb10-BT collection using a PLAID-X compressed index, snippets were filtered by cosine similarity with the original question, and final answers were generated using Falcon3-10B-Instruct. The system achieved moderate success in automatic correctness evaluation but showed significant limitations in faithfulness, highlighting the ongoing challenge of balancing accurate retrieval with reliable generation in RAG systems.

## Method Summary
The HLTCOE LiveRAG system used a multi-stage pipeline approach. First, the system generated queries based on the given context using a GPT-researcher framework. These queries were then used to retrieve relevant passages from the FineWeb10-BT collection through ColBERT retrieval implemented on a PLAID-X compressed index for computational efficiency. Retrieved passages were filtered by computing cosine similarity between snippets and the original question to ensure relevance. Finally, the system generated answers using Falcon3-10B-Instruct, a large language model fine-tuned for instruction following. This architecture aimed to combine efficient retrieval with high-quality generation while maintaining reasonable computational costs through the use of a compressed index.

## Key Results
- Achieved 5th place in automatic correctness evaluation with a score of 1.07
- Placed 14th in faithfulness evaluation with a score of 0.34
- Demonstrated significant gap between correctness and faithfulness performance
- Showed that compressed index retrieval can achieve competitive results in RAG challenges

## Why This Works (Mechanism)
The system's effectiveness stems from its multi-stage pipeline that combines query generation, efficient retrieval, and answer generation. The GPT-researcher framework generates contextually relevant queries that guide the retrieval process, while ColBERT's late interaction mechanism allows for more flexible matching between queries and passages. The PLAID-X compressed index enables efficient storage and retrieval without significant quality loss. The cosine similarity filtering step helps ensure that only highly relevant passages are passed to the generation stage, reducing noise in the input to Falcon3-10B-Instruct.

## Foundational Learning
- **ColBERT retrieval**: Why needed - Provides efficient and effective dense retrieval with late interaction scoring; Quick check - Verify that embeddings are properly indexed and retrieval scores are reasonable
- **PLAID-X compression**: Why needed - Reduces storage requirements while maintaining retrieval quality; Quick check - Compare compressed vs uncompressed retrieval performance on sample queries
- **Cosine similarity filtering**: Why needed - Ensures retrieved snippets are relevant to the original question; Quick check - Examine filtered vs unfiltered results for a sample query
- **Falcon3-10B-Instruct**: Why needed - Generates high-quality answers conditioned on retrieved passages; Quick check - Validate that generated answers are coherent and address the question
- **RAG pipeline architecture**: Why needed - Combines retrieval and generation for knowledge-intensive tasks; Quick check - Trace data flow through all pipeline stages
- **FineWeb10-BT collection**: Why needed - Provides a large corpus of web data for retrieval; Quick check - Verify that the collection is properly indexed and accessible

## Architecture Onboarding

**Component Map:** Query Generation -> ColBERT Retrieval -> Cosine Similarity Filtering -> Answer Generation

**Critical Path:** The most critical path is Query Generation → ColBERT Retrieval → Answer Generation, as failures in any of these stages directly impact the final output quality.

**Design Tradeoffs:** The system trades retrieval comprehensiveness for computational efficiency by using a compressed index and similarity filtering. This approach may miss relevant passages that don't meet the strict similarity threshold but enables faster processing and lower resource requirements.

**Failure Signatures:** Common failure modes include: 1) Query generation producing irrelevant or overly broad queries, 2) Retrieval missing relevant passages due to compressed index limitations, 3) Cosine similarity filtering being too strict and removing useful context, 4) Generation producing hallucinated content despite having relevant retrieved passages.

**3 First Experiments:**
1. Run the system on a small set of hand-crafted queries to verify end-to-end functionality
2. Test retrieval quality with and without the compressed index on representative queries
3. Evaluate the impact of cosine similarity threshold on both retrieval coverage and final answer quality

## Open Questions the Paper Calls Out
None

## Limitations
- Significant gap between correctness (5th place) and faithfulness (14th place) performance
- Reliance on single dense retrieval approach may miss diverse relevant passages
- Cosine similarity filtering may bias toward shorter, more literal matches
- Compressed index may impact retrieval quality compared to uncompressed alternatives

## Confidence

**High:** The technical specifications and evaluation results are clearly documented and internally consistent.

**Medium:** The relative performance compared to other systems is clear, but the specific reasons for the faithfulness gap require deeper analysis of the generated responses.

**Low:** The impact of the compressed index choice on retrieval quality compared to uncompressed alternatives cannot be definitively assessed without additional experiments.

## Next Checks

1. Conduct ablation studies comparing PLAID-X compressed index performance against uncompressed ColBERT retrieval on the same LiveRAG queries to quantify any quality degradation

2. Analyze the distribution of faithfulness errors (e.g., hallucinations, confabulations) in the system's outputs to identify whether they stem from retrieval failures or generation issues

3. Test the system's performance on a manually curated subset of LiveRAG queries to determine if the correctness faithfulness gap persists under controlled conditions