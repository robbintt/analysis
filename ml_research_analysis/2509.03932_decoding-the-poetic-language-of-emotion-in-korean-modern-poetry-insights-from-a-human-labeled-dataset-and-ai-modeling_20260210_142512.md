---
ver: rpa2
title: 'Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights
  from a Human-Labeled Dataset and AI Modeling'
arxiv_id: '2509.03932'
source_url: https://arxiv.org/abs/2509.03932
tags:
- emotion
- korean
- dataset
- kpoem
- poetry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The KPoEM dataset was developed to enable emotion classification
  in Korean modern poetry, a domain challenging for current models due to figurative
  language and cultural nuance. Using 7,662 expert-annotated entries from five poets,
  the dataset captures 44 fine-grained emotion categories at line and work levels.
---

# Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling

## Quick Facts
- arXiv ID: 2509.03932
- Source URL: https://arxiv.org/abs/2509.03932
- Reference count: 0
- The KcELECTRA model fine-tuned sequentially on KOTE→KPoEM achieves 0.60 F1-micro, outperforming general emotion models (0.34 F1-micro) on Korean poetic emotion classification

## Executive Summary
The KPoEM dataset was developed to enable emotion classification in Korean modern poetry, a domain challenging for current models due to figurative language and cultural nuance. Using 7,662 expert-annotated entries from five poets, the dataset captures 44 fine-grained emotion categories at line and work levels. A KcELECTRA model fine-tuned sequentially on general and KPoEM data achieved an F1-micro of 0.60, outperforming baselines. Emotion-aware retrieval-augmented generation experiments demonstrated that the dataset enables the production of culturally grounded poetic text. This work advances computational literary analysis and emotion-driven generative poetics in Korean.

## Method Summary
The KPoEM dataset was constructed from 483 poems by five Korean poets, with 7,662 expert-annotated entries (7,007 line-level, 615 work-level). Five annotators labeled each instance with up to 10 emotion labels from a 44-category taxonomy. Labels were aggregated into normalized score vectors (0-1) using frequency voting and min-max normalization. A KcELECTRA-base-v2022 model was fine-tuned sequentially: first on KOTE (general emotion corpus), then on KPoEM (poetry-specific emotion). The resulting classifier achieved 0.60 F1-micro. For emotion-aware generation, retrieved context was filtered by both semantic similarity and emotion alignment before being fed to an LLM.

## Key Results
- Sequential fine-tuning (KOTE→KPoEM) achieved 0.60 F1-micro, outperforming KOTE-only (0.43) and KPoEM-only (0.59)
- The KPoEM dataset captures 44 fine-grained emotion categories with expert annotations from five Korean poets
- Emotion-aware RAG retrieval produces more emotionally coherent poetry generation by filtering context through emotion alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential fine-tuning from general emotion corpora to domain-specific poetry improves classification performance
- Mechanism: KcELECTRA pre-trained on Korean comments → fine-tuned on KOTE (44-category general emotion) → fine-tuned on KPoEM (poetry-specific emotion). This curriculum-style transfer enables the model to first learn broad emotion patterns, then specialize for metaphorical/figurative expression.
- Core assumption: General emotion representations are a useful initialization for literary emotion; poetic emotion is a specialized subset rather than a wholly different task.
- Evidence anchors: [abstract]: "A state-of-the-art Korean language model fine-tuned on this dataset significantly outperformed previous models, achieving 0.60 F1-micro compared to 0.34 from models trained on general corpora." [Table 11]: KOTE-only F1=0.43; KPoEM-only F1=0.59; KOTE→KPoEM sequential F1=0.60

### Mechanism 2
- Claim: Multi-annotator label aggregation into normalized score vectors handles emotion ambiguity and subjectivity
- Mechanism: Five expert annotators assign up to 10 emotion labels per instance → labels aggregated into a frequency vector (0–5 votes per emotion) → min-max normalized to [0,1] → classification threshold of 0.3 applied. This converts subjective disagreement into a learnable soft-label signal.
- Core assumption: Annotator disagreement reflects genuine emotional ambiguity rather than error; consensus frequency correlates with emotional salience.
- Evidence anchors: [section III.B.2]: Detailed aggregation, vectorization, and normalization pipeline [Table 8]: 99.88% of texts have ≥2 annotators agreeing on at least one label; only 22.63% have full 5-annotator agreement

### Mechanism 3
- Claim: Emotion-constrained RAG retrieval produces more emotionally coherent poetry generation
- Mechanism: Input text → KPoEM classifier produces emotion scores → retrieve top-100 lines by semantic similarity → filter to top-10 by emotion score alignment → inject as context for generation LLM. Two-stage filtering mitigates retrieval of thematically similar but emotionally incongruent context.
- Core assumption: Emotional coherence between retrieved context and target emotion improves generation quality; context size of ~10 is optimal for RAG.
- Evidence anchors: [section V.B.2.2]: "selects the top 10 poetic lines that satisfy both semantic similarity and emotional alignment" [Table 15]: Input poem and generated poem show aligned top emotion categories (care, realization, admiration, expectancy, joy)

## Foundational Learning

### Multi-label Emotion Classification
- Why needed: Poetry frequently expresses multiple concurrent emotions (e.g., "sorrow" + "expectancy" + "resolute"). Binary or single-label frameworks fail to capture this complexity.
- Quick check question: Given the line "The reason I cannot count all the stars being engraved one by one in my heart now is because morning comes too soon" (from Yun Dong-ju's "The Night Counting Stars"), what multiple emotions might coexist?

### ELECTRA Architecture and Korean Language Models
- Why needed: Understanding KcELECTRA's discriminator-based pre-training clarifies why it's suitable for classification tasks. KoBERT and KcELECTRA are standard Korean-language transformer backbones.
- Quick check question: How does ELECTRA's replaced-token detection pre-training differ from BERT's masked language modeling, and why might this be advantageous for classification?

### RAG (Retrieval-Augmented Generation) Fundamentals
- Why needed: The emotion-aware generation pipeline combines semantic retrieval with emotion filtering; understanding standard RAG is prerequisite.
- Quick check question: In a standard RAG pipeline, what are the tradeoffs between retrieving more documents (higher recall) vs. fewer documents (lower noise)?

## Architecture Onboarding

### Component Map
[Raw Poetry Text] → [Preprocessing Pipeline] → [Expert Annotation (5 annotators)] → [Label Aggregation + Normalization] → [KcELECTRA-base-v2022] ← [KOTE Dataset] ← [Korean Comments Corpus] → [KOTE Fine-tuned Model] → [KPoEM Fine-tuning] → [KPoEM Emotion Classifier] → [Input Text] → [Emotion Classification] → [Emotion Scores (44-dim)] → [Vector Database (FAISS)] ← [KPoEM Lines + Emotion Metadata] → [Semantic Retrieval (top 100)] → [Emotion Filtering (top 10)] → [Context Construction] → [LLM (Midm-2.0-Base-Instruct)] ← [Prompt Template + Retrieved Context + Emotion List] → [Generated Poem]

### Critical Path
1. **Dataset Construction**: Wikisource extraction → preprocessing → line/work segmentation → 5-annotator labeling → aggregation
2. **Classifier Training**: KcELECTRA → KOTE fine-tuning → KPoEM fine-tuning (sequential strategy)
3. **Vector Database**: Embed KPoEM lines using KcELECTRA → attach normalized emotion metadata → index with FAISS
4. **Generation Pipeline**: Classify input → retrieve + filter → construct prompt → generate

### Design Tradeoffs
- **Shuffled vs. ordered line annotation**: Shuffling removes narrative context, testing whether individual lines carry emotion independently. This enables more robust line-level classification but may miss inter-line emotional transitions.
- **Threshold = 0.3**: Lower threshold increases recall for rare emotions but may increase false positives. Paper adopts this from KOTE (Jeon et al., 2024) without ablation.
- **Context size = 10**: Balances retrieval diversity with LLM context constraints. Based on Leto et al. (2024) findings; not explicitly validated in this paper.
- **Work-level vs. line-level annotation**: Work-level captures holistic emotional arc; line-level enables fine-grained classification. Dataset provides both.

### Failure Signatures
- **Culturally specific emotion misclassification**: Direct import of English emotion taxonomies (e.g., GoEmotions-Korean) may miss culturally embedded emotions like *seoreo-um* (서러움) and *bijang-ham* (비장함). See page 5 discussion.
- **Diffuse emotion distributions**: KOTE-only model produces noisy, scattered predictions (Table 12: Han Kang poem shows "despair: 0.32" when inappropriate).
- **Retrieval-context mismatch**: Semantic-only retrieval returns lines with similar imagery but conflicting emotion, degrading generation coherence.

### First 3 Experiments
1. **Sequential fine-tuning ablation**: Train three models (KOTE-only, KPoEM-only, KOTE→KPoEM) on identical test split; compare F1-micro, F1-macro, MCC. Verify paper's 0.43 → 0.59 → 0.60 progression.
2. **Emotion filtering ablation in RAG**: Generate poems with (a) semantic-only retrieval (no emotion filtering), (b) emotion-constrained retrieval. Use KPoEM classifier to measure emotion alignment between input and output. Assumption: emotion-filtered output should show higher alignment.
3. **Threshold sensitivity analysis**: Evaluate classifier at thresholds 0.2, 0.3, 0.4, 0.5. Plot precision/recall tradeoffs. Determine if 0.3 is optimal or if domain-specific tuning is warranted.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating a sensory element dataset with the KPoEM framework enable a more holistic analysis of literary affect than emotion data alone?
- Basis in paper: [explicit] The Conclusion states, "Future work will extend this framework by constructing a dataset of sensory elements... enabling more holistic literary analysis that encompasses the two fundamental dimensions of human experience—sense and emotion."
- Why unresolved: The current study is limited to the emotional dimension; the sensory dataset (Ji, 2025) is presented as a separate, future contribution required to bridge the gap between sense and sentiment.
- What evidence would resolve it: A multimodal model combining KPoEM with the proposed sensory dataset that demonstrates superior performance in capturing poetic nuance compared to the emotion-only baseline.

### Open Question 2
- Question: How does the inclusion of broader temporal, authorial, and gender diversity affect the performance and cultural grounding of the classification model?
- Basis in paper: [explicit] The authors acknowledge the dataset is "restricted to works by five representative poets," leading to "limited temporal and authorial diversity, as well as the underrepresentation of female poets."
- Why unresolved: The current dataset focuses exclusively on five canonical poets from the 1920s–1940s whose copyrights have expired, potentially limiting the model's applicability to contemporary or diverse literary voices.
- What evidence would resolve it: Benchmarking the fine-tuned model on a test set of poems by contemporary or female poets to evaluate generalization capabilities outside the current narrow scope.

### Open Question 3
- Question: Can computational methods be refined to model the inherent ambiguity and subjectivity of emotional interpretation in poetry?
- Basis in paper: [inferred] The Conclusion notes that "the inherent ambiguity of poetic language and the subjectivity of emotional interpretation remain fundamental challenges that cannot be fully resolved through computational approaches alone."
- Why unresolved: While the multi-annotator approach captures some variance, the paper's methodology aggregates labels into normalized scores, potentially flattening the subjective ambiguity inherent in figurative language.
- What evidence would resolve it: A modeling approach that successfully predicts a probability distribution of valid emotional interpretations per line, reflecting annotator disagreement as a feature rather than noise.

## Limitations
- The dataset focuses exclusively on five canonical poets from the 1920s–1940s, limiting temporal and authorial diversity
- Emotion-aware RAG generation experiments rely primarily on qualitative case studies rather than systematic quantitative evaluation
- The 0.3 classification threshold appears inherited from KOTE without ablation studies to justify its optimality for poetic text

## Confidence

**High Confidence**: The dataset construction methodology (7,662 expert-annotated entries, 5 annotators per instance, multi-label emotion taxonomy) is methodologically sound and well-documented. The sequential fine-tuning architecture and implementation details are sufficiently specified for reproduction.

**Medium Confidence**: The claim that KcELECTRA fine-tuned sequentially on KOTE→KPoEM outperforms general models (F1-micro 0.60 vs 0.34) is supported by reported results, but the experimental design could be strengthened with additional ablation studies and cross-validation across different poet subsets.

**Low Confidence**: The emotion-aware RAG generation results rely primarily on qualitative case studies rather than systematic evaluation. The assumption that retrieving emotionally aligned context improves generation quality is plausible but not empirically validated with rigorous metrics.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically evaluate the KPoEM classifier at multiple thresholds (0.2, 0.3, 0.4, 0.5) and plot precision-recall curves to determine whether 0.3 is truly optimal for poetic emotion classification, or if domain-specific tuning is warranted.

2. **Cross-Validation Across Poet Subsets**: Split the dataset not just by random assignment but also by poet to test whether the model's performance holds across different stylistic approaches. This would reveal whether the sequential fine-tuning approach generalizes beyond the specific poet selection.

3. **Emotion Filtering Ablation in RAG**: Conduct a controlled experiment comparing emotion-constrained retrieval (semantic + emotion filtering) against semantic-only retrieval (top-100 lines by semantic similarity alone). Use the KPoEM classifier to measure emotion alignment between input and generated output, quantifying whether the two-stage filtering actually improves emotional coherence.