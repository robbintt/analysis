---
ver: rpa2
title: Improving Topic Relevance Model by Mix-structured Summarization and LLM-based
  Data Augmentation
arxiv_id: '2404.02616'
source_url: https://arxiv.org/abs/2404.02616
tags:
- relevance
- document
- query
- data
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of topic relevance modeling in
  social search, specifically addressing challenges posed by long, redundant documents
  and the scarcity of reliable training data. To address these issues, the authors
  propose two key approaches: 1) Mix-structured Summarization, which combines query-focused
  summaries and general document summaries as input to the model, enabling it to differentiate
  between strong and weak relevance.'
---

# Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation

## Quick Facts
- arXiv ID: 2404.02616
- Source URL: https://arxiv.org/abs/2404.02616
- Reference count: 20
- Primary result: AUC improves from 73.64% to 77.41% with mix-structured summarization and LLM-based data augmentation

## Executive Summary
This paper addresses topic relevance modeling challenges in social search, where long, redundant documents and scarce training data hinder performance. The authors propose two complementary approaches: Mix-structured Summarization, which combines query-focused and general document summaries as input to better capture relevance degrees, and LLM-based Data Augmentation, which uses a large language model to generate synthetic query-document pairs through rewriting and keyword extraction. Experiments on a social search dataset show significant improvements in classification accuracy and positive online A/B test results.

## Method Summary
The approach combines mix-structured summarization with LLM-based data augmentation for 3-class topic relevance classification (strong/weak/irrelevant). Documents are summarized using query-focused extractive summaries (max 128 tokens) and general document summaries (max 64 tokens) concatenated with [SEP] tokens. An LLM (GPT-3.5-turbo) generates synthetic training data through synonym/antonym query rewriting and keyword-based query generation. A BERT-based cross-encoder model is trained on the augmented dataset and evaluated using multi-class AUC metrics.

## Key Results
- AUC improves from 73.64% to 77.41% on the test set
- Online A/B test shows positive results with ΔGSB = 0.1
- Training data expands from 46,306 to 123,896 samples through LLM augmentation
- Weak relevance samples increase from 6,530 to 39,564 through augmentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining query-focused summaries with general document summaries improves the model's ability to distinguish between strong and weak relevance, compared to query-focused summaries alone.
- **Mechanism:** Query-focused summaries isolate relevant content but lose proportion information. By concatenating with a general document summary (via SEP token), the model receives both "what matches the query" and "what the document is mostly about," enabling it to assess both presence and prevalence of relevant information.
- **Core assumption:** The relative proportion of query-relevant content in a document determines whether relevance is strong or weak; this proportion can be inferred from comparing query-focused vs. general summaries.
- **Evidence anchors:**
  - [abstract] "query concatenated with the query-based summary and the document summary without query as the input... can help model learn the relevance degree"
  - [section 2.1 / Table 1] Shows Doc 1 (strong) and Doc 2 (weak) produce similar query-focused summaries but distinguishable mix-structured summaries; MSD-CE achieves 75.71% AUC vs. Baseline 73.64%
  - [corpus] Related work on query-relevant summarization (arXiv:2508.08404) supports query-focused extraction, but does not address multi-class relevance distinction
- **Break condition:** When documents are uniformly relevant or uniformly irrelevant (no mixed content), the proportion signal degrades; also when summaries are too short to capture document scope.

### Mechanism 2
- **Claim:** LLM-based query rewriting generates synthetic training samples that improve model robustness, particularly for underrepresented relevance categories.
- **Mechanism:** Synonym rewriting creates semantic variations (same label), increasing lexical diversity. Antonym rewriting creates hard negatives (opposite semantics, similar surface form) labeled as irrelevant. This addresses class imbalance and exposes the model to challenging edge cases.
- **Core assumption:** LLM-generated synonyms preserve relevance labels accurately, and LLM-generated antonyms produce semantically opposite queries that should be labeled irrelevant for the original document.
- **Evidence anchors:**
  - [abstract] "utilize the language understanding and generation abilities of large language model (LLM) to rewrite and generate query"
  - [section 2.2.1 / Table 2] Shows example: "Beef hot pot" → Syn: "hot pot" (Strong), Ant: "beef snacks" (Irrelevant)
  - [corpus] arXiv:2506.06015 (LLM-Based Corpus Enrichment) and arXiv:2507.12126 (IASR for augmentation) support LLM augmentation viability, but lack direct replication for multi-class relevance
- **Break condition:** When LLM produces synonyms that shift intent (e.g., "beef hot pot" → "spicy hot pot" might match different documents) or antonyms that remain partially relevant, label noise increases.

### Mechanism 3
- **Claim:** LLM-based keyword extraction from documents, ranked by importance, can generate new query-document pairs with assignable relevance labels.
- **Mechanism:** The LLM extracts keywords and ranks them by document importance. Top-ranked keywords are treated as strong-relevance queries; lower-ranked keywords as weak-relevance queries. This creates additional labeled pairs without manual annotation.
- **Core assumption:** Keyword importance as judged by the LLM correlates with the degree of relevance between that keyword (as query) and the source document.
- **Evidence anchors:**
  - [section 2.2.2 / Figure 3] Prompt asks for "3 keywords... sort them according to their importance"; Q₁ (most important) → strong, Q₃ (least important) → weak
  - [section 3.4.2 / Table 4] Training data grows from 46,306 to 123,896 samples; weak relevance samples increase from 6,530 to 39,564
  - [corpus] No direct corpus evidence for keyword-importance-to-relevance mapping; this assumption is weakly validated
- **Break condition:** When documents cover multiple distinct topics with no clear primary focus, or when LLM keyword rankings do not align with human relevance judgments.

## Foundational Learning

- **Concept: Cross-encoder relevance modeling**
  - **Why needed here:** The base model is a BERT-based cross-encoder that takes concatenated query-document input and outputs a relevance classification. Understanding joint encoding vs. bi-encoder approaches is prerequisite.
  - **Quick check question:** Can you explain why a cross-encoder might capture query-document interaction better than separate embeddings, and what the latency tradeoff is?

- **Concept: Extractive summarization methods**
  - **Why needed here:** Both query-focused and document summaries are extractive (selecting sentences, not generating new text). The Lead-3 baseline and query-focused extraction logic are core to input preparation.
  - **Quick check question:** Given a 50-sentence document and a query, how would you implement a simple query-focused extractive summarizer?

- **Concept: Multi-class AUC evaluation**
  - **Why needed here:** The task is 3-class (strong/weak/irrelevant), not binary. Standard AUC doesn't apply directly; the paper uses a generalized formulation.
  - **Quick check question:** How does the multi-class AUC formula in equations (1)-(2) differ from binary AUC, and why assign scores 1.0, 0.7, 0.0 to the three classes?

## Architecture Onboarding

- **Component map:**
  1. Document preprocessing → (a) Query-focused summary extractor, (b) Document summary extractor
  2. Input formation: Query + [SEP] + query-focused summary + [SEP] + document summary
  3. LLM augmentation pipeline: Existing pairs → synonym/antonym rewriting + keyword generation
  4. Relevance model: BERT-based cross-encoder → 3-class softmax output
  5. Serving: Model scores query-document pairs for retrieval

- **Critical path:**
  1. Implement and validate query-focused summarizer (Section 2.1.1)
  2. Implement document summarizer (Lead-3 per paragraph)
  3. Integrate LLM augmentation (prompt design for synonym/antonym/keyword generation)
  4. Train cross-encoder on augmented data with mix-structured inputs
  5. Evaluate on held-out test set (AUC) and online A/B test (ΔGSB)

- **Design tradeoffs:**
  - **Input length vs. information coverage:** 192 total tokens forces compression; increasing length improves coverage but increases latency
  - **LLM augmentation quality vs. cost:** GPT-3.5-turbo used; cheaper/smaller models may produce lower-quality synonyms/keywords
  - **Extractive vs. abstractive summaries:** Paper uses extractive for simplicity and reproducibility; abstractive may improve coherence but adds complexity

- **Failure signatures:**
  - **Weak/irrelevant class confusion:** Model predicts "weak" for truly irrelevant documents if keyword extraction generates marginal queries
  - **Summary truncation loss:** Long documents with key info beyond first 3 sentences per paragraph may lose signal
  - **LLM prompt drift:** Antonym generation produces semantically similar (not opposite) queries, introducing label noise

- **First 3 experiments:**
  1. **Ablation on summary components:** Train with (a) query-focused only, (b) document summary only, (c) mix-structured; compare AUC to isolate contribution of each summary type
  2. **Augmentation impact by class:** Train with full augmented data vs. original data; measure per-class precision/recall to verify weak/irrelevant class improvement
  3. **LLM quality sensitivity:** Replace GPT-3.5-turbo with a smaller model (e.g., 7B parameter LLM); measure degradation in augmentation quality and downstream AUC

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would replacing the extractive "Lead-3" document summary with an abstractive LLM-based summary improve the model's ability to capture global context?
- **Basis in paper:** [inferred] The paper utilizes a simple extractive method (first 3 sentences) for document summarization to represent core information, noting it is "the simplest method" but effective, yet does not compare it against semantic compression techniques.
- **Why unresolved:** The study establishes that including document context helps, but does not explore if the noise in extractive sentences hampers the "mix-structured" signal compared to a denser, generated summary.
- **What evidence would resolve it:** Experimental results comparing the current extractive approach against abstractive summaries (e.g., generated by GPT-4) within the mix-structured input framework.

### Open Question 2
- **Question:** How sensitive is the data augmentation performance to the specific capability or size of the Large Language Model employed?
- **Basis in paper:** [inferred] The authors utilize GPT-3.5-turbo for all augmentation tasks (query rewriting and generation) but do not investigate if smaller, lower-cost models or open-source alternatives could achieve similar diversity.
- **Why unresolved:** While the paper proves LLM-based augmentation is effective, it leaves the cost-benefit analysis of using high-capacity proprietary models versus smaller instruction-tuned models unexplored.
- **What evidence would resolve it:** A comparison of model performance when training data is augmented using different LLMs (e.g., Llama-2, GPT-4 vs. GPT-3.5).

### Open Question 3
- **Question:** Does the inclusion of LLM-generated "hard negatives" (antonyms) introduce semantic drift that limits the model's ability to distinguish fine-grained relevance?
- **Basis in paper:** [inferred] The paper generates "antonyms" to create irrelevant training samples, but relies on the LLM's interpretation of "opposite semantics" which may not align with user behavior in search scenarios.
- **Why unresolved:** The improvement in AUC is aggregated; it is unclear if the model gains come from better identification of true irrelevance or simply from learning the specific artifacts of LLM-generated contrastive pairs.
- **What evidence would resolve it:** An error analysis of false negatives in the test set specifically for samples derived from antonym-based augmentation.

## Limitations
- Model architecture and training hyperparameters are not specified, making exact reproduction difficult
- LLM-generated samples may introduce label noise, particularly for antonym-based augmentation
- 192-token input limit may lose critical relevance signals from long documents

## Confidence
- **High Confidence:** Core methodology of combining summaries is well-grounded; reported AUC improvement and online results are strong
- **Medium Confidence:** LLM augmentation approach shows promise but quality control mechanisms are limited
- **Low Confidence:** Individual contribution of each component to overall improvement cannot be precisely determined

## Next Checks
1. **Ablation Study on Summary Components:** Train and evaluate models using (a) query-focused summaries only, (b) document summaries only, and (c) mix-structured summaries to quantify the individual contribution of each summary type to the 3.77% AUC improvement.

2. **LLM Augmentation Quality Analysis:** Manually audit 100 LLM-generated query-document pairs (50 synonyms, 50 antonyms) to measure label accuracy rates and identify systematic failure patterns in the augmentation process.

3. **Token Budget Sensitivity Analysis:** Evaluate model performance across different input length constraints (128, 192, 256 tokens total) to determine the optimal balance between information retention and computational efficiency, and to quantify the impact of compression on relevance modeling accuracy.