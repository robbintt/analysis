---
ver: rpa2
title: Explainable Depression Detection using Masked Hard Instance Mining
arxiv_id: '2505.24609'
source_url: https://arxiv.org/abs/2505.24609
tags:
- attention
- encoder
- dual
- depression
- mhim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve explainability in text-based
  depression detection using Masked Hard Instance Mining (MHIM). The core idea is
  to strategically mask high-attention features in the model, forcing it to distribute
  attention across a wider range of salient features.
---

# Explainable Depression Detection using Masked Hard Instance Mining

## Quick Facts
- arXiv ID: 2505.24609
- Source URL: https://arxiv.org/abs/2505.24609
- Reference count: 0
- Primary result: MHIM improves depression detection accuracy and attention-based explainability through strategic masking of high-attention features

## Executive Summary
This paper addresses the challenge of improving both prediction accuracy and explainability in text-based depression detection. The proposed method, Masked Hard Instance Mining (MHIM), strategically masks high-attention features during training, forcing the model to distribute attention across a wider range of salient features. Evaluated on Thai-Maywe and DAIC-WOZ datasets, MHIM significantly improves depression score prediction accuracy while enhancing explainability metrics. The method demonstrates that forcing attention diversification through masking creates more interpretable models without sacrificing performance.

## Method Summary
The method employs a two-phase training approach with Multi-Instance Learning (MIL) for depression detection. Phase 1 trains a Dual Encoder model that uses separate encoders for questions and responses, aggregates with Bi-LSTM, and applies attention for prediction. Phase 2 uses the Phase 1 model's attention weights to create masks that blind the Phase 2 model to previously attended features, forcing attention diversification. The approach uses MSE loss, AdamW optimizer, and is implemented with RoBERTa-based encoders for English (DAIC-WOZ) and WangchanBERTa-based encoders for Thai (Thai-Maywe).

## Key Results
- Thai-Maywe HAM-D prediction: RMSE reduced from 0.54 to 0.48, MAE from 0.37 to 0.33
- Attention entropy increased from 2.158 to 2.496 on Thai-Maywe, indicating better feature distribution
- Recall@k improvements across all k values: 0.13→0.17 (k=10%), 0.52→0.62 (k=50%), 0.90→0.95 (k=90%)
- Similar improvements observed on DAIC-WOZ dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masking high-attention features during training forces the model to distribute attention across more diverse input features, improving both prediction accuracy and explainability.
- **Mechanism:** During Phase-2 training, MHIM masks sentence embeddings that received high attention scores from the Phase-1 donor model. This prevents the receiver model from over-relying on a small subset of features (e.g., only suicide mentions) and encourages discovery of additional salient signals (e.g., sleep disturbances).
- **Core assumption:** Attention overfitting occurs in low-resource depression datasets, causing models to concentrate on few features while ignoring other clinically relevant indicators.
- **Evidence anchors:**
  - [abstract] "MHIM strategically masks attention weights within the model, compelling it to distribute attention across a wider range of salient features."
  - [section 3] "We believe that by spreading out the attention weights, more diverse information aggregation is encouraged, providing better prediction performance and explainability at the same time."
  - [corpus] Related paper "LMILAtt" similarly applies attention-based MIL for depression detection, suggesting attention distribution is a recognized strategy, though MHIM's masking approach is distinct.
- **Break condition:** If the dataset has abundant labeled examples or if important features are truly sparse (only 1-2 relevant sentences per interview), masking may force attention onto noise.

### Mechanism 2
- **Claim:** Two-phase donor-receiver training creates a form of self-supervised hardness augmentation without requiring additional labels.
- **Mechanism:** Phase-1 produces attention weights that identify which instances the model currently considers "easy" (high attention). Phase-2 masks these instances, effectively creating hard positive samples that force the model to learn alternative pathways to the same prediction.
- **Core assumption:** The donor model's attention patterns, while potentially overfit, still capture meaningful signal that can guide augmentation.
- **Evidence anchors:**
  - [section 3] "The masks comes from two sets: top scoring masks and random masks. t top scoring masks are randomly chosen from the top r attention weights (t ≤ r)."
  - [section 2.3] "MHIM [14] is a data augmentation technique designed for MIL models that use attention mechanisms... It encourage the model to focus on other important instances."
  - [corpus] No direct corpus evidence for donor-receiver transfer; related papers use single-stage attention training.
- **Break condition:** If donor attention is random or highly miscalibrated, masking may remove the wrong features and degrade performance.

### Mechanism 3
- **Claim:** Higher attention entropy correlates with improved identification of clinically relevant sentences as measured by human expert labels.
- **Mechanism:** By increasing entropy (spreading attention), more sentences receive non-trivial attention weights. When ranked by attention, this broader distribution captures more expert-identified important sentences (higher Recall@k).
- **Core assumption:** Depression severity is signaled by multiple linguistic markers across an interview, not concentrated in a single utterance.
- **Evidence anchors:**
  - [table 1] Entropy increases from 2.406 to 2.496 (Dual Encoder) and from 2.158 to 2.819 (HAM-D Overall) when MHIM is applied.
  - [table 2] Recall@k improves across all k values (e.g., k=10%: 0.13 → 0.17; k=50%: 0.52 → 0.62).
  - [corpus] "DepressionX" and "AttentionDep" papers similarly treat attention weights as explainability signals, supporting the attention-interpretability link.
- **Break condition:** If expert labels (ISL) are noisy or if depression manifests differently across languages/cultures, entropy-rewarding may not align with clinical relevance.

## Foundational Learning

- **Concept: Multi-Instance Learning (MIL)**
  - Why needed here: Depression detection uses weak labels (single interview-level score) without sentence-level annotations. MIL provides the theoretical framework for learning from bags of instances where only bag-level labels exist.
  - Quick check question: Can you explain why standard supervised learning fails when you have one label for an entire interview but want to identify which sentences matter?

- **Concept: Attention Mechanisms for Aggregation**
  - Why needed here: MIL requires combining variable-length instance representations into a fixed-dimensional bag representation. Attention provides both aggregation and interpretability (weights indicate importance).
  - Quick check question: Given attention weights [0.6, 0.3, 0.1] for three sentences, how would you explain which sentence "drove" the prediction to a clinician?

- **Concept: Hard Instance Mining**
  - Why needed here: Standard training may overfit to easy instances. Hard mining strategically focuses learning on challenging samples, but in MIL, "hard" must be defined at the instance level within bags.
  - Quick check question: Why might masking the "easiest" (highest-attention) instances create a harder learning problem?

## Architecture Onboarding

- **Component map:**
  Input (dialogue turns) -> Prefix Encoder + Sentence Encoder (separate for questions/responses) -> Bi-LSTM (sequential aggregation) -> Attention Layer -> Prediction Head

- **Critical path:**
  1. Train Phase-1 Dual Encoder → extract attention patterns
  2. Configure masking hyperparameters (t top masks, b bottom masks, r = top-r pool)
  3. Train Phase-2 receiver with masked attention
  4. **Inference uses receiver WITHOUT masking** (masking only during training)

- **Design tradeoffs:**
  - **t (top masks) vs. b (bottom masks):** Higher t increases pressure to diversify but risks removing critical signal. Bottom masks encourage generalization but add noise. Paper uses r = 2t; tune t on validation set.
  - **Donor quality:** Phase-1 must train sufficiently to produce meaningful attention, but not so long it overfits severely.
  - **Language-specific encoders:** Thai uses WangchanBERTa + ConGen; English uses RoBERTa + all-mpnet. Encoder choice matters more than MHIM for baseline performance.

- **Failure signatures:**
  - Attention weights remain highly concentrated (low entropy) after MHIM → Phase-1 may be too weak or t too small
  - RMSE increases significantly → masking too aggressive (t too large) or donor attention poorly calibrated
  - High Recall@k but poor prediction → model attending broadly but not learning discriminative patterns

- **First 3 experiments:**
  1. **Baseline sanity check:** Train Dual Encoder without MHIM; verify attention entropy is low (over-concentration) and establish RMSE baseline.
  2. **Hyperparameter sweep:** Vary t ∈ {1, 2, 3, 5} and b ∈ {0, 1, 2} on validation set; plot entropy vs. RMSE to find the tradeoff frontier.
  3. **Ablation on masking strategy:** Compare (a) masking only top, (b) masking only bottom, (c) masking both (full MHIM) to isolate the contribution of each component.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would replacing the binary masking strategy with a "softer" attenuation method improve the model's ability to capture nuanced depression indicators?
- **Basis in paper:** [explicit] The authors state in the conclusion that investigating "method less stringent than complete masking" is left for future work.
- **Why unresolved:** The current MHIM implementation completely blinds the model to top-attended features, which may force the model to rely on sub-optimal features rather than simply down-weighting dominant ones.
- **What evidence would resolve it:** Experiments comparing binary masks against continuous penalty functions (e.g., inverse attention weighting) on the same DAIC-WOZ and Thai-Maywe datasets.

### Open Question 2
- **Question:** How can the Multi-Instance Learning (MIL) aggregation be modified to prevent false positives arising from single high-relevance sentences?
- **Basis in paper:** [explicit] The discussion notes the model is "prone to cases where a single high-ISL sentence may lead to a false positive" due to the MIL framing, a issue they plan to remedy.
- **Why unresolved:** The standard MIL assumption allows a single positive instance to determine the bag label, which is problematic in depression detection where context is cumulative.
- **What evidence would resolve it:** A comparative analysis of MIL aggregation functions (e.g., noisy-AND vs. mean-pooling) showing a reduction in false-positive rates while maintaining Recall@k.

### Open Question 3
- **Question:** Can the training efficiency be improved by utilizing the "doner" model's attention weights more effectively than simple selection?
- **Basis in paper:** [explicit] The conclusion lists "better utilization of the doner's attention weights" as a specific avenue for future work.
- **Why unresolved:** The current method uses the Phase-1 doner weights strictly to generate a binary mask list, potentially discarding distributional information that could guide the Phase-2 model more smoothly.
- **What evidence would resolve it:** An ablation study where Phase-2 training incorporates the doner's attention distribution as a regularization term (KL-divergence) rather than just a masking guide.

## Limitations

- **Limited generalizability:** Method effectiveness may vary across different depression assessment instruments and cultural contexts beyond Thai and English clinical interviews.
- **Donor dependence:** Method's success heavily depends on the quality of Phase-1 donor attention weights, which may be poorly calibrated or random in some cases.
- **Masking sensitivity:** Performance is highly sensitive to hyperparameter choices (t and b values), requiring careful tuning for optimal results.

## Confidence

**High Confidence:** The core mechanism of using MHIM to improve attention distribution (entropy increase from 2.158 to 2.496 on Thai-Maywe) and its positive impact on prediction metrics (RMSE reduction from 0.54 to 0.48) are well-supported by the presented results.

**Medium Confidence:** The claim that higher attention entropy correlates with improved clinical explainability (measured by Recall@k improvements) assumes that ISL labels are accurate and comprehensive, which may not always be true across different cultural contexts or depression assessment instruments.

**Low Confidence:** The general applicability of MHIM across different depression detection tasks and languages, particularly for English DAIC-WOZ where explainability metrics were not directly measured against human labels, remains uncertain.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary t and b values across a wider range (t ∈ {1, 2, 3, 5}, b ∈ {0, 1, 2, 3}) to identify optimal masking configurations and understand the tradeoff between attention entropy and prediction accuracy.

2. **Cross-Cultural Validation:** Apply MHIM to depression detection datasets from different cultural contexts and using different assessment instruments to verify the method's robustness and generalizability beyond Thai and English clinical interviews.

3. **Clinical Expert Evaluation:** Conduct a study where clinical experts evaluate the top-attended sentences identified by MHIM against ground truth clinical importance, to validate that increased attention entropy actually improves clinical explainability rather than just increasing attention spread.