---
ver: rpa2
title: Adversarial Attack Classification and Robustness Testing for Large Language
  Models for Code
arxiv_id: '2506.07942'
source_url: https://arxiv.org/abs/2506.07942
tags:
- code
- robustness
- perturbations
- language
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how adversarial perturbations in natural
  language inputs affect the robustness of large language models for code (LLM4Code).
  A novel two-dimensional classification system categorizes attacks by content type
  (code, task description, comments) and granularity (character, word, sentence level).
---

# Adversarial Attack Classification and Robustness Testing for Large Language Models for Code

## Quick Facts
- **arXiv ID:** 2506.07942
- **Source URL:** https://arxiv.org/abs/2506.07942
- **Reference count:** 40
- **Key outcome:** This study investigates how adversarial perturbations in natural language inputs affect the robustness of large language models for code (LLM4Code). A novel two-dimensional classification system categorizes attacks by content type (code, task description, comments) and granularity (character, word, sentence level). Experimental results reveal that LLM4Code models exhibit varying levels of vulnerability: sentence-level perturbations show the highest resilience, while word-level perturbations pose the most significant challenges, exposing semantic vulnerabilities. The findings highlight the critical role of natural language in adversarial evaluation and emphasize the need for targeted defense mechanisms to improve model resilience, particularly against semantic-level disruptions, ensuring secure and reliable code-generation systems.

## Executive Summary
This paper introduces a comprehensive framework for evaluating the robustness of large language models for code (LLM4Code) against adversarial attacks. The authors develop a two-dimensional classification system that categorizes attacks by both content type (code, task description, comments) and granularity (character, word, sentence level). Through extensive experimentation using HumanEval and MBPP benchmarks, the study reveals that word-level perturbations targeting semantic meaning pose the greatest threat to model robustness, while sentence-level perturbations are surprisingly resilient. The research demonstrates that robustness varies significantly across models and perturbation types, with specialized mono-language models showing enhanced resilience compared to multi-language alternatives. These findings establish critical insights for developing more secure and reliable code generation systems.

## Method Summary
The study evaluates LLM4Code robustness using a mixed-methods approach. The authors employ HumanEval (164 samples) and MBPP (974 samples) datasets, applying adversarial perturbations generated via ReCode (28 methods) and OpenAttack (6 methods). The evaluation framework measures functional correctness through unit test pass rates, using Robust Pass@k (RPs@k), Robust Drop@k (RDs@k), and Robust Relative@k (RRs@k) metrics with k=1. Models tested include CodeGen-2B/6B (mono/multi), InCoder-1B, GPT-J-6B, CodeLlama-7b-hf, and Llama-3-8B-Instruct. The experimental setup runs on Python 3.8/3.10 with Nvidia V100/A6000 GPUs, implementing a sandbox environment for secure code execution and systematic perturbation application across the two-dimensional classification framework.

## Key Results
- Word-level perturbations targeting semantic meaning represent the most significant threat to LLM4Code robustness, causing substantial functional failures
- Sentence-level perturbations demonstrate the highest resilience, suggesting models can maintain logical flow despite structural changes
- Specialized mono-language models show superior robustness compared to multi-language alternatives, indicating specialization enhances resistance to adversarial attacks
- Comment perturbations disproportionately degrade accuracy by introducing conflicting signals into the attention mechanism
- The two-dimensional classification framework effectively captures the complex landscape of adversarial vulnerabilities in code generation models

## Why This Works (Mechanism)

### Mechanism 1: Semantic Granularity Asymmetry
- **Claim:** LLM4Code models exhibit an inverse relationship between perturbation granularity and vulnerability, where word-level semantic shifts are significantly more disruptive than sentence-level structural changes.
- **Mechanism:** Sentence-level perturbations (e.g., back-translation, dead-code insertion) often preserve the broader functional intent or logical flow, allowing the model to rely on high-level context. In contrast, word-level perturbations (e.g., synonym substitution in variable names or task descriptions) disrupt specific token-level mappings required for precise logic execution, leading to a breakdown in functional correctness.
- **Core assumption:** The model relies more heavily on local semantic token dependencies for logic generation than on global syntactic structures.
- **Evidence anchors:**
  - [abstract] "sentence-level perturbations show the highest resilience, while word-level perturbations pose the most significant challenges."
  - [section 5.3] "Models exhibit the highest robustness to statement-level perturbations... They are most susceptible to word-level perturbations because these can significantly alter the meaning and relationships."
  - [corpus] "Adversarial Text Generation with Dynamic Contextual Perturbation" notes that attacks focusing on local text segments are effective, aligning with the finding that granular word-level changes are high-risk.
- **Break condition:** If a model employs a semantic abstraction layer that normalizes word-level synonyms into canonical representations before logic synthesis.

### Mechanism 2: Context Dilution via Non-Executable Noise
- **Claim:** Perturbations in non-executable components (comments/docstrings) disproportionately degrade generation accuracy by introducing conflicting signals into the attention mechanism.
- **Mechanism:** LLM4Code models treat comments and docstrings as high-value context. When random or perturbed comments are injected (e.g., `randominsertcomments`), they dilute the attention allocated to the actual functional signature or logic. This "context overload" forces the model to process irrelevant noise, leading to logical hallucinations or syntax errors that do not occur when the context is clean.
- **Core assumption:** The model's attention mechanism assigns significant weight to natural language comments as logical priors, regardless of their relevance.
- **Evidence anchors:**
  - [section 5.3 (RQ3)] "Randomly inserted comments might obscure critical code sections or introduce noise that confuses the model's ability to maintain logical code flow."
  - [section 6] "Models exhibit notably lower robustness to comment perturbations... LLM4Code rely heavily on contextual information provided by comments."
  - [corpus] Corpus evidence is weak regarding the specific internal mechanism of comment-induced attention dilution; "Evaluating the robustness of adversarial defenses..." discusses general evasion but not specifically comment-to-code attention breakdown.
- **Break condition:** If the architecture implements a strict code-only attention mask that ignores non-executable lines during inference.

### Mechanism 3: Specialization-Induced Robustness
- **Claim:** Mono-language models (trained on a single language like Python) demonstrate superior robustness compared to multi-language models due to reduced token vocabulary collision and deeper syntactic specialization.
- **Mechanism:** Multi-language models must distribute representational capacity across diverse syntaxes, potentially leading to interference or "cognitive load" when resolving ambiguous tokens. Mono-language models optimize specifically for the target syntax, making them less sensitive to perturbations that might otherwise trigger cross-linguistic ambiguity or confusion.
- **Core assumption:** Robustness is a function of representation density; models with focused training distributions have "sharper" decision boundaries for specific syntax rules.
- **Evidence anchors:**
  - [section 5.3] "Mono language models demonstrate a distinct decrease in RD [Robust Drop]... indicating enhanced robustness across all perturbation levels."
  - [section 6] "Monolingual models... demonstrated superior resistance... due to their specialized training."
  - [corpus] Corpus neighbors focus on general robustness or other domains (genomics/malware) and do not provide comparative evidence for the mono- vs. multi-language mechanism.
- **Break condition:** If the multi-language model uses a Mixture-of-Experts (MoE) architecture with strictly isolated language experts, effectively simulating a mono-language environment.

## Foundational Learning

- **Concept: Robustness Metrics (RPs@k, RDs@k)**
  - **Why needed here:** Standard accuracy (Pass@k) measures success on clean inputs. You need *Robust Pass* (RPs) to measure worst-case success and *Robust Drop* (RDs) to quantify the reliability gap.
  - **Quick check question:** If a model maintains 90% accuracy on clean code but drops to 10% when a variable is renamed, which metric captures this failure?

- **Concept: Perturbation Granularity Taxonomy**
  - **Why needed here:** To diagnose *where* the model fails. You must distinguish between "typos" (character-level), "semantic drift" (word-level), and "logic restructuring" (sentence-level).
  - **Quick check question:** Which perturbation level targets the semantic meaning of identifiers rather than syntax or structure?

- **Concept: Cross-Modal Sensitivity (Code vs. NL)**
  - **Why needed here:** LLM4Code are not just code compilers; they are bimodal. An attack on the Natural Language (NL) description can break the Code generation, independent of the code's syntax.
  - **Quick check question:** Can a perturbation in a docstring (NL) cause a functional error in the generated Python code even if the function signature is unchanged?

## Architecture Onboarding

- **Component map:** HumanEval/MBPP (Tasks + Solutions) -> ReCode/OpenAttack (Perturbation Engine) -> LLMs (CodeGen, CodeLlama, Llama-3) -> Unit Test Execution (Sandbox) -> Metric Calculator (Pass@k, RP, RD)

- **Critical path:** The flow from *Perturbation Engine* to *Evaluation Harness* is the bottleneck. You must ensure that "Code Perturbation" (e.g., changing a loop) doesn't break the syntax so badly it fails to compile *before* the LLM even generates code. The paper filters for "functional correctness," implying inputs must remain valid.

- **Design tradeoffs:**
  - **ReCode vs. OpenAttack:** ReCode offers deterministic, code-aware perturbations (logic focus). OpenAttack offers stochastic, NL-aware attacks (semantic focus). Use ReCode for structural stress-testing; OpenAttack for prompt-injection simulation.
  - **Mono vs. Multi Models:** Use Mono (e.g., CodeGen-mono) for robustness benchmarks; use Multi for generalization tests.

- **Failure signatures:**
  - **Word-level attack failure:** A sudden spike in RD (Robust Drop) > 20% when synonyms are swapped in the task description.
  - **Comment noise failure:** Generated code follows the logic of the *comment* (even if random) rather than the function signature, indicating high attention bias toward NL.
  - **Llama-3 Anomaly:** General-purpose models (Llama-3) occasionally outperforming specialized models (CodeLlama), suggesting overfitting in specialized models.

- **First 3 experiments:**
  1. **Baseline Character Attack:** Apply `FuncRenameChangeChar` (typo injection) on HumanEval using CodeGen-mono. Measure RD. (Expect low impact; models are resilient to chars).
  2. **Semantic Word Attack:** Apply `SynonymSubstitution` to task descriptions. Measure RD. (Expect high impact; semantic vulnerability).
  3. **Context Overload Test:** Inject `randominsertcomments` into the function body. Compare CodeLlama vs. Llama-3. (Identify which model is more easily "distracted").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can robustness-aware training pipelines, such as adversarial data augmentation or contrastive learning, effectively mitigate the semantic vulnerabilities identified at the word-level?
- **Basis in paper:** [explicit] Section 8.2 states future work should "develop robustness-aware training pipelines" using these methods to mitigate brittleness against semantic perturbations.
- **Why unresolved:** The current study serves as an evaluation framework for existing models rather than an intervention study exploring training modifications.
- **What evidence would resolve it:** A comparative study measuring model robustness (RP@k/RDs@k) before and after applying adversarial fine-tuning or contrastive learning on the identified vulnerable perturbation types.

### Open Question 2
- **Question:** Do the robustness rankings observed in Python-based models generalize to other programming languages with distinct syntax and semantic rules?
- **Basis in paper:** [explicit] Section 8.1 notes that experiments were conducted on Python-based datasets and results "may not fully translate to other programming languages."
- **Why unresolved:** The study's design was limited to the HumanEval and MBPP Python benchmarks.
- **What evidence would resolve it:** Replicating the two-dimensional classification and evaluation using multi-language benchmarks (e.g., MultiPL-E) to assess if word-level perturbations remain the most detrimental across languages.

### Open Question 3
- **Question:** How does functional correctness under adversarial perturbation correlate with deeper software quality attributes, specifically security vulnerabilities and code maintainability?
- **Basis in paper:** [inferred] Section 8.1 acknowledges that relying on functional correctness (passing unit tests) "may overlook subtler defects related to security vulnerabilities, code quality, and developer intent."
- **Why unresolved:** Current evaluation metrics are binary (pass/fail) based on test cases and do not capture non-functional correctness issues.
- **What evidence would resolve it:** Integrating static analysis tools into the evaluation pipeline to quantify security flaws and code smells in code generated from perturbed inputs.

## Limitations
- The study's findings are based on synthetic perturbations rather than real-world attack scenarios, which may limit ecological validity
- The evaluation framework focuses on functional correctness through unit tests, potentially missing more subtle semantic failures that could manifest in production environments
- The sample size of 164 HumanEval and 974 MBPP tasks may not provide sufficient statistical power for all subgroups and fine-grained comparisons

## Confidence
- **High Confidence:** The characterization of perturbation granularity effects (word-level > sentence-level vulnerability) is strongly supported by consistent experimental results across multiple models and datasets
- **Medium Confidence:** The mechanism explaining comment-induced context dilution relies on reasonable assumptions about attention mechanisms but lacks direct empirical validation
- **Low Confidence:** The specialization-induced robustness hypothesis for mono-language models is supported by correlational evidence but lacks causal validation

## Next Checks
1. **Attention Heatmap Validation:** Conduct targeted experiments using attention visualization tools to empirically verify whether perturbed comments receive disproportionate attention weight compared to code signatures, directly testing the context dilution mechanism.
2. **Cross-Perturbation Chaining:** Design experiments that chain multiple perturbation types (e.g., word-level followed by sentence-level) to assess whether the vulnerability patterns observed for individual perturbation types compound or interact in unexpected ways.
3. **Real-World Attack Simulation:** Implement a small-scale study using actual bug reports or code review comments as adversarial inputs, comparing model performance on these naturalistic perturbations versus synthetic ones to assess ecological validity.