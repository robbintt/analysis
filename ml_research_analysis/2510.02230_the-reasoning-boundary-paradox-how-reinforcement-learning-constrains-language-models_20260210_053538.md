---
ver: rpa2
title: 'The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language
  Models'
arxiv_id: '2510.02230'
source_url: https://arxiv.org/abs/2510.02230
tags:
- training
- learning
- rlvr
- problems
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the "reasoning boundary paradox" in Reinforcement
  Learning with Verifiable Rewards (RLVR), where RLVR training can actually shrink
  the set of problems a language model can solve rather than expand it. The authors
  identify two key phenomena: negative interference, where learning to solve certain
  problems reduces the likelihood of correct solutions for others, and a winner-take-all
  effect, where RLVR disproportionately reinforces problems with high initial success
  rates while suppressing low-likelihood ones.'
---

# The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models

## Quick Facts
- arXiv ID: 2510.02230
- Source URL: https://arxiv.org/abs/2510.02230
- Authors: Phuc Minh Nguyen; Chinh D. La; Duy M. H. Nguyen; Nitesh V. Chawla; Binh T. Nguyen; Khoa D. Doan
- Reference count: 40
- Primary result: RLVR training can shrink reasoning boundaries by suppressing low-likelihood problems while reinforcing already high-likelihood solutions.

## Executive Summary
This paper identifies a fundamental limitation in Reinforcement Learning with Verifiable Rewards (RLVR) where training can paradoxically reduce the set of problems a language model can solve. Through theoretical and empirical analysis, the authors demonstrate that standard RLVR objectives cause negative interference between problems and a winner-take-all effect that concentrates probability mass on already-successful solutions. They propose SELF (Selective Examples with Low-likelihood and Forward-KL), a data curation algorithm that focuses learning on low-likelihood problems, which significantly improves coverage (Pass@k at large k) while maintaining accuracy (Pass@1).

## Method Summary
The authors investigate how RLVR training causes reasoning boundary shrinkage through two key phenomena: negative interference where learning one problem reduces performance on others, and winner-take-all effects where high-likelihood solutions dominate training. They propose SELF, which filters training examples to exclude problems already solved by the greedy response and replaces reverse KL regularization with forward KL (SFT-style loss). The method is evaluated on mathematical reasoning benchmarks using Qwen2.5-Math models trained on the DeepScaleR dataset, with performance measured via Pass@1 and Pass@k metrics.

## Key Results
- RLVR improves Pass@1 but degrades Pass@k at large k, with Pass@256 falling below the base model after ~300 training steps
- Negative interference strength (∆+) correlates strongly (Pearson ρ 0.50-0.59) with Pass@k decline across benchmarks
- SELF significantly improves Pass@k performance compared to standard GRPO, particularly for larger sampling budgets
- Larger models (7B) exhibit more severe negative interference than smaller models (1.5B)

## Why This Works (Mechanism)

### Mechanism 1: Negative Interference Through Gradient Coupling
Learning to solve one problem can reduce the likelihood of correct solutions for others during RLVR training. This occurs because per-step parameter updates influence other problem-solution pairs via gradient dot products K(x,x',y,y') = ∇θ log π(y|x) · ∇θ log π(y'|x'). When gradients from different problems are correlated, updating on problem x can decrease log-likelihood for unrelated (x', y'). On-policy sampling causes highly solvable problems to dominate training, increasing cross-problem interference.

### Mechanism 2: Winner-Take-All via Probability-Scaled Gradient Updates
Standard RLVR objectives disproportionately reinforce already high-likelihood correct solutions while suppressing low-likelihood ones. The on-policy REINFORCE objective scales gradients by π_θ(y): ∇θ L = A(y)·π_θ(y)·∇θ log π_θ(y). High-likelihood correct responses receive larger updates (both frequent sampling and larger gradient magnitude), while low-likelihood correct responses get minimal signal. This creates a rich-get-richer effect that concentrates probability mass on already-successful solutions.

### Mechanism 3: Coverage Shrinkage Through Diversity Collapse
The combined effects of negative interference and winner-take-all reduce the diversity of learned reasoning strategies, shrinking Pass@k at large k. Winner-take-all concentrates probability mass on high-likelihood solutions; negative interference degrades confidence on alternative correct paths. As training progresses, ||∆(π_θt, μ)|| increases, indicating stronger gradient correlation across examples. This amplifies bias propagation and suppresses alternative reasoning modes.

## Foundational Learning

- **Policy Gradient (REINFORCE) with Advantage Function**: The on-policy gradient ∇θ L = A(y)·π_θ(y)·∇θ log π_θ(y) scales updates by current probability, driving winner-take-all. Quick check: Why does π_θ(y) appear in the REINFORCE gradient, and what does this imply for low-probability actions?

- **KL Divergence: Forward KL vs Reverse KL**: Forward KL (π_b || π_θ) penalizes zeroing out probability mass on regions where π_b is nonzero, while reverse KL does not. Quick check: Why does forward KL maintain coverage of base model behaviors while reverse KL can cause mode collapse?

- **Pass@k Metric and Sampling Budget**: Pass@k measures the probability of solving a problem within k samples, reflecting solution diversity. Quick check: If Pass@1 improves but Pass@1024 declines, what does this indicate about the model's solution diversity?

## Architecture Onboarding

- **Component map**: Base model π_b (frozen reference) -> Training policy π_θ (updated via RLVR) -> Reward function r(x,y) -> GRPO baseline or SELF modification -> Probing dataset D_prob

- **Critical path**:
  1. Generate greedy response y* per problem x
  2. Skip gradient update if r(x, y*) = 1 (filtering step)
  3. Compute group-wise advantage A(x, y) from sampled rollouts
  4. Apply gradient update with Forward KL regularization: J(π_θ) = E[1{r(x,y*)∈C(x)}·r(x,y)] - β KL(π_b || π_θ)
  5. Periodically evaluate Pass@k on held-out benchmarks

- **Design tradeoffs**:
  - Filtering highly solvable problems reduces training reward but preserves diversity and improves Pass@k
  - Forward KL maintains coverage of base model behaviors but may slow convergence on high-likelihood correct solutions
  - Probing dataset adds computational overhead but enables real-time interference monitoring
  - Larger models exhibit more severe negative interference, suggesting scaling may amplify the problem

- **Failure signatures**:
  - Pass@k declining below base model while Pass@1 improves (classic winner-take-all signature)
  - Rapid entropy collapse during training (diversity loss)
  - Trust region violations increasing on probing data while stable on training data
  - Reasoning mode collapse (e.g., code→language switch when language has higher initial accuracy)

- **First 3 experiments**:
  1. Reproduce Fig 2: Train GRPO on Qwen2.5-Math-1.5B, plot Pass@1 vs Pass@256 over 500 steps. Confirm divergence emerges around step 300.
  2. Implement per-step interference tracking: Using D_prob, compute ∆+ and ||∆|| after each update. Correlate with Pass@128 changes (target Pearson ρ > 0.5).
  3. Ablate SELF components: (a) filtering only, no Forward KL; (b) Forward KL only, no filtering; (c) full SELF. Compare Pass@k curves against GRPO baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Does the correlation between model scale and negative interference severity persist or amplify in frontier-scale models (>70B parameters), or do emergent capabilities at scale mitigate the effect? The authors note larger models suffer more from negative interference (7B vs 1.5B) but don't test on larger frontier models where training dynamics might differ.

### Open Question 2
To what extent do negative interference and winner-take-all phenomena generalize to reasoning domains beyond mathematics, such as code generation or open-ended logic? The paper restricts experiments to mathematical reasoning benchmarks, leaving universality of learning dynamics in other reasoning tasks unverified.

### Open Question 3
Is "greedy response failure" the optimal proxy for identifying problems requiring active learning, or does it risk neglecting problems with high-likelihood errors? SELF filters based on greedy success, but RLVR often reinforces high-likelihood incorrect responses, potentially missing problems where greedy is correct but alternatives are not.

### Open Question 4
Can explicit exploration strategies or off-policy corrections resolve on-policy sampling limitations without requiring data filtering used in SELF? The authors identify on-policy sampling as root cause but propose data curation rather than modifying the underlying RL sampling mechanism.

## Limitations
- Findings primarily validated on mathematical reasoning benchmarks; generalization to other domains (code, QA) remains untested
- Limited to models up to 7B parameters; scalability and behavior in frontier-scale models (>70B) unknown
- SELF requires additional computational overhead for greedy filtering and probing dataset generation

## Confidence
- **High**: Empirical observation that RLVR improves Pass@1 while degrading Pass@k at large k; demonstration that SELF mitigates this effect
- **Medium**: Theoretical explanations for negative interference and winner-take-all mechanisms; claim that coverage shrinkage arises from diversity collapse
- **Low**: Generalization of findings to non-mathematical domains; scalability of SELF to larger models; long-term stability of SELF-trained models

## Next Checks
1. **Ablation study of SELF components**: Isolate effects of greedy filtering versus Forward KL regularization by testing each component independently on Math500, measuring Pass@k curves and entropy evolution.

2. **Cross-domain validation**: Apply SELF to code generation (HumanEval) and factual QA (MMLU) benchmarks to test whether reasoning boundary paradox generalizes beyond mathematics.

3. **Scaling analysis**: Train SELF on Qwen2.5-Math-7B and compare interference strength (∆+, ||∆||) and coverage shrinkage relative to 1.5B model, testing hypothesis that larger models exhibit more severe negative interference.