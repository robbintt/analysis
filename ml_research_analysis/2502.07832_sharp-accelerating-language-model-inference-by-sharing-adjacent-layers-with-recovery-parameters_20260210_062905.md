---
ver: rpa2
title: 'SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with
  Recovery Parameters'
arxiv_id: '2502.07832'
source_url: https://arxiv.org/abs/2502.07832
tags:
- layers
- sharp
- arxiv
- layer
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARP addresses the challenge of deploying large language models
  (LLMs) on resource-constrained devices by introducing a novel layer-sharing approach
  that reduces memory overhead while maintaining performance. The core idea involves
  sharing parameters across adjacent layers and using low-rank recovery parameters
  to predict subsequent layers, enabling significant inference acceleration.
---

# SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters

## Quick Facts
- **arXiv ID:** 2502.07832
- **Source URL:** https://arxiv.org/abs/2502.07832
- **Reference count:** 40
- **Primary result:** SHARP reduces stored MLP parameters by 38%-65% while recovering perplexity within 0.5 points on in-distribution tasks

## Executive Summary
SHARP introduces a novel approach to accelerate LLM inference by sharing parameters across adjacent transformer layers and using low-rank recovery parameters to predict subsequent layers. The method employs a two-stage recovery process—Single Layer Warmup (SLW) to align outputs using L2 loss, followed by Supervised Fine-Tuning (SFT) to restore model performance. Extensive experiments show SHARP can recover model perplexity on various tasks using no more than 50k fine-tuning examples while reducing stored MLP parameters by 38%-65%. On mobile devices, SHARP achieves 42.8% storage savings and 42.2% total runtime reduction compared to the original Llama2-7b model.

## Method Summary
SHARP accelerates LLM inference by replacing target layer parameters with shared reference layers plus low-rank recovery parameters (LoRA). The method uses a two-stage training process: SLW stage minimizes L2 loss between predicted and original layer outputs using ~10% of training data, providing stable initialization for recovery parameters. SFT stage then jointly optimizes all recovery parameters with task loss using 50k+ examples. The approach targets MLP layers specifically, as they constitute the majority of intermediate layer parameters. Three replacement strategies are evaluated: Tnext (alternating layers), Tback (preserving early layers), and Tmore (aggressive compression). Layer-specific functional analysis shows early-middle layers (5-15) are critical for foundational representations, while later layers show task-specific importance.

## Key Results
- SHARP recovers perplexity within 0.5 points on Arxiv-math using Tback strategy at 38% stored ratio
- Achieves 42.8% storage savings and 42.2% total runtime reduction on mobile devices compared to original Llama2-7b
- Maintains near-baseline performance on memorization tasks (BoolQ) while showing larger gaps on complex reasoning tasks (GSM8k, ARC)
- Layer sensitivity analysis reveals layers 5-15 are critical for both memorization and reasoning, while later layers show sparse importance for memorization but dense importance for reasoning

## Why This Works (Mechanism)

### Mechanism 1: Adjacent Layer Representational Redundancy
The paper leverages the observation that consecutive transformer layers produce similar output representations despite having different parameters. Rather than approximating parameters directly (which differ substantially—Figure 3 shows 100-160% relative error), SHARP approximates the *output function* using low-rank adapters added to shared base weights. The core assumption is that the input-output mapping of adjacent layers lies in a low-dimensional subspace that can be captured by recovery parameters with rank ≪ original weight rank.

### Mechanism 2: Two-Stage Recovery Prevents Optimization Collapse
Separating output alignment (SLW) from end-to-end fine-tuning (SFT) enables aggressive layer replacement that would fail with direct SFT alone. Stage 1 (SLW) minimizes L2 loss between predicted and original layer outputs in isolation, providing stable initialization for recovery parameters. Stage 2 (SFT) then jointly optimizes all recovery parameters with task loss. Without SLW, aggressive replacement leaves the model too far from optimal solution for SFT to converge efficiently.

### Mechanism 3: Layer-Specific Functional Specialization
Later layers are more amenable to sharing than early layers (5-15), which encode critical foundational representations. Knowledge is stored in a layer-distributed manner with task-specific sparsity patterns. Early-middle layers process and integrate information critical for both memorization and reasoning. Later layers show task-specific sparse importance for memorization (only specific indices matter per task) but dense importance for reasoning.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: SHARP uses LoRA-style recovery parameters (A∈ℝ^(d₁×r), B∈ℝ^(r×d₂)) to bridge the gap between shared base weights and target layer functions. Understanding that W' = αW + AB allows efficient fine-tuning without modifying frozen base weights.
  - Quick check question: Given a 4096×11008 weight matrix, how many parameters does rank-400 LoRA add? (Answer: 4096×400 + 400×11008 = 6.04M, ~13% of original 45M)

- **Transformer MLP Layer Function**
  - Why needed here: SHARP specifically targets MLP layers (gate, up, down projections) for sharing because they constitute the majority of intermediate layer parameters. Understanding that MLP(x) = down(silu(x·gate) ⊙ (x·up)) clarifies what's being approximated.
  - Quick check question: Why might MLP layers be better candidates for sharing than attention layers? (Answer: Paper focuses on MLPs as main parameter contributors; attention layers not examined)

- **Perplexity as Distribution Matching Metric**
  - Why needed here: The paper uses perplexity to measure how well recovered models match original behavior on in-distribution tasks. Understanding perplexity = exp(cross-entropy loss) and that lower is better is necessary to interpret results.
  - Quick check question: If baseline perplexity is 3.0 and SHARP achieves 3.2, what does this mean practically? (Answer: ~7% degradation in predicted token probability quality)

## Architecture Onboarding

- **Component map:**
  ```
  Original: Θ₁ → Θ₂ → Θ₃ → Θ₄ → ... → Θ₃₂ (each layer stored)
  
  SHARP (Tnext): Θ₁ → Θ₂ → Θ₃ → Θ₃+ΔΘ₄ → Θ₅ → Θ₅+ΔΘ₆ → ...
                         ↑         ↑
                    reference  target (recovered via g(Θ_ref, ΔΘ))
  ```

- **Critical path:**
  1. Define replacement type (Table 1/8): Tnext (56% stored), Tback (38%), Tmore (25%)
  2. Initialize recovery parameters (rank 400 typical for Llama2-7B)
  3. **SLW stage:** For each target layer, minimize L2 loss between f(X; g(Θ_ref, ΔΘ)) and f(X; Θ_target) using ~10% of data activations
  4. **SFT stage:** Jointly fine-tune all ΔΘ parameters with task loss (50k+ examples typical)
  5. Deploy: Load only reference layers + all ΔΘ; compute targets dynamically

- **Design tradeoffs:**
  - **Higher rank (r=400):** Better recovery, more stored parameters (~13% of MLP per target), SLW becomes critical
  - **Lower rank (r=5-20):** Less storage, SLW less beneficial, may insufficient for aggressive replacement
  - **Tback vs Tfront:** Tback (preserve early layers) consistently outperforms Tfront at same stored ratio
  - **Candidate transformations (Eq. 3-6):** LoRA addition, left/right/dot multiplication perform similarly when parameter-counts matched

- **Failure signatures:**
  - Direct sharing without recovery: Perplexity >1000 (Table 2)
  - SFT-only with aggressive replacement: Slow convergence, poor final perplexity (Table 3)
  - Replacing layers 0-1 or final layer: Disproportionate damage (Figure 2)
  - Too little data for SFT (<10%): Incomplete recovery, especially for reasoning tasks

- **First 3 experiments:**
  1. **Validate layer robustness:** Directly replace layer i+1 MLP with layer i in Llama2-7B; measure perplexity on Arxiv-math subset. Expect: near-baseline for middle layers, spike for layer 0 and final layer.
  2. **Minimal SHARP (Tnext, r=400):** Apply SLW+SFT on 50k Arxiv-math samples. Target: perplexity <3.5 (vs baseline 3.0). If >5.0, check SLW convergence.
  3. **Ablate SLW necessity:** Compare SFT-only vs SLW+SFT on Tback with 10% data. Target: SLW+SFT should outperform by >3 perplexity points. If gap small, rank may be too low.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance degradation in complex reasoning tasks (e.g., GSM8k, ARC) be mitigated while maintaining high compression rates, given that SHARP currently preserves memorization capabilities significantly better than reasoning capabilities?
- Basis in paper: [explicit] Section 3.4 notes that "for tasks that further require more complex reasoning capabilities... the performance gap between SHARP and the original model is still large," whereas memorization tasks show near-baseline performance.
- Why unresolved: The paper identifies the discrepancy (Figure 4) but does not propose a mechanism to recover the lost reasoning depth other than increasing data or rank, which trades off compression.
- What evidence would resolve it: A modification to the recovery parameterization or training objective that specifically targets multi-step logical consistency, resulting in recovered GSM8k scores comparable to the baseline.

### Open Question 2
- Question: Can the SHARP layer-sharing strategy be effectively extended to self-attention modules without catastrophic performance loss?
- Basis in paper: [inferred] The method restricts itself to MLP layers (Section 2.2.1) based on the observation that they occupy the main parameter count, but leaves open whether the high cosine similarity between adjacent layers holds for attention weights.
- Why unresolved: The paper relies on the "similarity of outputs" insight for MLPs; applying this to attention mechanisms involves different functional roles (e.g., positional mixing vs. feature transformation) which may not tolerate sharing as well.
- What evidence would resolve it: Experiments applying the Single Layer Warmup (SLW) and SFT process to the $W_q, W_k, W_v, W_o$ matrices, comparing perplexity and downstream task performance against MLP-only sharing.

### Open Question 3
- Question: How does the optimal placement of reference vs. target layers change based on the specific domain or modality of the task (e.g., code generation vs. natural language)?
- Basis in paper: [explicit] The Conclusion states the ablation study on layer sensitivity "will also be useful for future works of interpreting how models capture knowledge and capabilities at each layer."
- Why unresolved: While the paper finds replacing later layers is generally better (Section 3.3.1), the sensitivity analysis (Figure 4) suggests different layers host different capabilities, implying a static replacement strategy (like $T_{back}$) may not be universally optimal.
- What evidence would resolve it: A study comparing the performance of a "code-specialized" SHARP model (retaining layers identified as critical for code logic) against the standard replacement strategy.

## Limitations

- The method shows significant performance gaps on complex reasoning tasks (GSM8k, ARC) while maintaining good performance on memorization tasks
- The optimal replacement strategy may be model/dataset dependent, as the paper relies on general layer sensitivity patterns rather than task-specific optimization
- Layer-wise functional specialization analysis is based on sparse ablation data without examining underlying mechanisms or cross-architecture generalizability

## Confidence

- **High Confidence:** Layer sharing with LoRA recovery works in practice (empirical results on Llama2-7b are reproducible); Tback strategy consistently outperforms alternatives; SLW+SFT two-stage training improves convergence
- **Medium Confidence:** The claim that consecutive layers have highly similar outputs (basis for sharing) relies on Deja Vu citation rather than direct measurement; the optimal replacement strategy (Tback vs others) may be model/dataset dependent
- **Low Confidence:** Layer-wise functional specialization analysis (layers 5-15 being foundational) is based on sparse ablation data without examining why these layers are critical or whether this pattern holds across architectures

## Next Checks

1. **Ablate SLW hyperparameters:** Run Tback compression at 38% stored ratio with SLW epochs=1,3,10 and LR=1e-4,1e-3,1e-2 to establish sensitivity and confirm the two-stage design is robust rather than coincidentally optimal

2. **Test layer robustness boundaries:** Systematically replace non-adjacent layers (skip-1, skip-2 patterns) and measure perplexity degradation to validate the adjacent-layer similarity assumption beyond the reported Deja Vu citation

3. **Extreme compression stress test:** Apply SHARP to Llama2-7b with Tmore strategy targeting 10% stored ratio, then gradually increase rank from 5→400 to map the recovery parameter capacity curve and identify the minimum viable rank for acceptable perplexity degradation