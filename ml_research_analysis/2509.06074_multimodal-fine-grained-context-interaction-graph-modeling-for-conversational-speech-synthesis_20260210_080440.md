---
ver: rpa2
title: Multimodal Fine-grained Context Interaction Graph Modeling for Conversational
  Speech Synthesis
arxiv_id: '2509.06074'
source_url: https://arxiv.org/abs/2509.06074
tags:
- prosody
- interaction
- speech
- word-level
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating natural prosody
  in conversational speech synthesis by modeling fine-grained semantic and prosodic
  interactions in multimodal dialogue history (MDH). While existing methods focus
  on utterance-level interactions, they overlook the critical role of word-level semantics
  and prosody in influencing subsequent utterances.
---

# Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis

## Quick Facts
- **arXiv ID**: 2509.06074
- **Source URL**: https://arxiv.org/abs/2509.06074
- **Reference count**: 16
- **Primary result**: MFCIG-CSS achieves significant improvements in naturalness (N-DMOS +0.122) and prosody (P-DMOS +0.104) metrics for conversational speech synthesis by modeling fine-grained semantic and prosodic interactions in dialogue history

## Executive Summary
This paper addresses the challenge of generating natural prosody in conversational speech synthesis by modeling fine-grained semantic and prosodic interactions in multimodal dialogue history. Unlike existing methods that focus on utterance-level interactions, MFCIG-CSS explicitly models word-level semantic and prosodic features and their influence on subsequent utterances. The framework constructs two specialized interaction graphs - a Semantic Interaction Graph and a Prosody Interaction Graph - which capture these fine-grained interactions and encode them into features that enhance synthesized speech with natural conversational prosody. Experiments on the DailyTalk dataset demonstrate significant improvements over baseline models, validating the effectiveness of this approach for more expressive and natural conversational speech generation.

## Method Summary
MFCIG-CSS employs a two-stage approach combining graph-based context modeling with neural speech synthesis. First, it extracts multimodal features from dialogue history using TOD-BERT (word-level text), Sentence-BERT (utterance-level text), Wav2Vec2.0 (word-level speech prosody), and Wav2Vec2.0-IEMOCAP (utterance-level speech). These features are organized into two separate interaction graphs: a Semantic Interaction Graph (SIG) modeling semantic relationships and a Prosody Interaction Graph (PIG) modeling prosodic relationships. Both graphs use GraphSAGE encoders to propagate information forward through the dialogue sequence, with final features aggregated via average pooling. The resulting interaction features are then injected into a FastSpeech 2-based synthesizer to condition speech generation, producing more natural and contextually appropriate prosody that reflects the conversational context.

## Key Results
- N-DMOS: 3.980 (+0.122) improvement over baseline
- P-DMOS: 3.899 (+0.104) improvement over baseline
- MAE-P: 0.439 (+0.011) improvement over baseline
- MCD: 9.53 (+0.37) improvement over baseline
- Performance gains validated across multiple evaluation metrics on DailyTalk dataset

## Why This Works (Mechanism)
The model works by capturing fine-grained conversational influences that occur at the word level rather than just the utterance level. In natural conversation, specific words and their prosodic realization (pitch, energy, rhythm) create expectations and influence how subsequent utterances are spoken. By constructing separate graphs for semantic and prosodic interactions, MFCIG-CSS can model these influences more precisely. The forward-only propagation through the dialogue history allows the model to capture how earlier words and utterances shape the prosodic characteristics of later speech. These interaction features are then injected into the speech synthesizer, providing rich contextual information that biases the generated speech toward more natural conversational prosody that reflects the dynamic flow of dialogue.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) for Sequential Data**
  - Why needed here: The core uses GraphSAGE to process dialogue history as a graph where words and utterances are nodes with conversational influences as edges, enabling more expressive modeling than standard sequential models
  - Quick check question: Given a simple chain graph A -> B -> C, how would a 2-layer GNN update the representation of node C? What information would it incorporate?

- **Concept: Word-Level Prosodic Features**
  - Why needed here: Modeling prosody at word level (not utterance level) captures acoustic information that influences meaning and subsequent speech, requiring tools like Wav2Vec2.0 and MFA alignment
  - Quick check question: For the sentence "I didn't say he stole the money," how would the prosodic features of the word "HE" change if emphasizing who the thief was versus emphasizing the act of stealing?

- **Concept: Conditioning in Neural TTS**
  - Why needed here: The model generates speech conditioned on complex context; interaction features are additional control signals that bias output, requiring understanding of how to inject them into TTS models
  - Quick check question: In standard TTS, what is the primary input? If you want speech to sound "happy," where and how would you inject an "emotion" embedding?

## Architecture Onboarding

- **Component map**: Input Data (Text + Audio) -> **Forced Alignment (MFA)** to get word timings -> **Feature Extraction** (word/utterance text/speech embeddings) -> **Graph Construction** (building SIG & PIG with nodes and edges) -> **Graph Encoding** (GraphSAGE propagation and pooling to get I'_s and I'_p) -> **Feature Injection** (adding interaction features into synthesizer's pipeline) -> **Speech Synthesis** (generating mel-spectrogram and audio)

- **Critical path**: Input Data (Text + Audio) -> MFA Alignment -> Feature Extraction (TOD-BERT, Sentence-BERT, Wav2Vec2.0) -> Graph Construction (SIG & PIG) -> Graph Encoding (GraphSAGE) -> Feature Injection -> FastSpeech 2 Synthesis

- **Design tradeoffs**: 
  - Granularity vs. Complexity: Word-level modeling is more expressive but increases computational cost and depends on accurate alignment
  - Dual-Graph vs. Single-Graph: Separate SIG and PIG allow specialized modeling but require training two encoders versus parameter sharing in a single graph
  - Forward-Only Propagation: Efficient but may not capture cases where later utterances recontextualize earlier ones

- **Failure signatures**:
  - Misaligned Nodes: MFA errors create incorrect word-level speech features, corrupting graph inputs and leading to unnatural prosody
  - Vanishing Gradients: Long dialogues may struggle with backpropagation through sequential graph layers, limiting long-range influence learning
  - Over-Smoothing: GraphSAGE aggregation and pooling might smooth features too much, losing fine-grained distinctiveness of key words

- **First 3 experiments**:
  1. Ablation on Interaction Branches: Remove one branch from SIG and measure impact on N-DMOS/P-DMOS to validate each information flow's contribution
  2. Analysis by Dialogue Length: Test performance on dialogues of varying lengths (2 turns vs. 10+ turns) to identify if forward-only propagation or graph depth introduces long-range context issues
  3. Qualitative Case Study: Generate speech for sentence pairs differing by a single key word and compare synthesized prosody to see if model adjusts pitch and energy appropriately based on graph-encoded context

## Open Questions the Paper Calls Out

- Can MFCIG-CSS be effectively adapted to one-stage architectures like VITS or discrete token-based speech encoders without losing prosodic benefits observed with FastSpeech 2?
- How does explicitly modeling finer-grained acoustic features such as emotion, emphasis, and pauses within the interaction graphs impact the model's ability to render complex conversational prosody?
- Does MFCIG-CSS generalize to languages with different prosodic structures or to spontaneous, unscripted dialogue datasets?
- What is the computational cost and latency overhead introduced by constructing and encoding the two distinct fine-grained interaction graphs during inference?

## Limitations
- Current implementation only supports FastSpeech 2 architecture, limiting applicability to other speech synthesis frameworks
- No explicit modeling of high-level acoustic features like emotion, emphasis, and pauses, which could enhance expressiveness
- Experiments limited to a single dataset (DailyTalk) with specific characteristics, raising generalization concerns
- Heavy reliance on forced alignment quality, with potential degradation if MFA alignment is inaccurate

## Confidence

- **High Confidence**: The MFCIG-CSS architecture is novel and technically sound; the model outperforms baseline systems on the DailyTalk dataset; the dual-graph approach is logically justified for capturing distinct interaction types
- **Medium Confidence**: The improvement in N-DMOS (+0.122) and P-DMOS (+0.104) is statistically significant within the tested dataset; forward-only graph propagation captures relevant conversational influences for studied dialogue lengths; FastSpeech 2 backbone integration is feasible
- **Low Confidence**: Claims about superiority over all baseline models without broader comparative analysis; generalization to other conversational datasets beyond DailyTalk; scalability to extremely long dialogues or different speaker configurations

## Next Checks

1. **Ablation on Graph Encoder Components**: Remove one branch from the SIG (e.g., the word-level prosody branch) and measure the impact on N-DMOS/P-DMOS. This validates whether each interaction path contributes meaningfully to the improvements or if some components are redundant.

2. **Cross-Dataset Validation**: Test the pretrained MFCIG-CSS model on a different conversational dataset (e.g., DailyDialog or Friends) without fine-tuning. Measure whether the prosody improvements transfer across datasets with different dialogue styles, lengths, and speaker configurations.

3. **Robustness to Alignment Errors**: Intentionally corrupt a subset of MFA alignments (e.g., shift word boundaries by 50-200ms) and evaluate model performance degradation. This quantifies the model's sensitivity to alignment quality that the authors acknowledge as a limitation.