---
ver: rpa2
title: 'HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform
  and In-Memory Computing'
arxiv_id: '2509.23103'
source_url: https://arxiv.org/abs/2509.23103
tags:
- htma
- hadamard
- transform
- in-memory
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HTMA-Net, a novel framework that integrates
  the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory
  computing to reduce arithmetic complexity in deep neural networks while maintaining
  accuracy. The core idea is to replace intermediate convolutions with Hybrid Hadamard-based
  transform layers, whose internal convolutions are implemented via multiplication-avoiding
  in-memory operations.
---

# HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing

## Quick Facts
- arXiv ID: 2509.23103
- Source URL: https://arxiv.org/abs/2509.23103
- Authors: Emadeldeen Hamdan; Ahmet Enis Cetin
- Reference count: 0
- Primary result: Eliminates up to 52% of multiplications in ResNet-18 while maintaining accuracy

## Executive Summary
HTMA-Net is a novel framework that combines the Hadamard Transform with multiplication-avoiding in-memory computing to reduce arithmetic complexity in deep neural networks. The approach replaces traditional convolutional layers with Hybrid Hadamard-based transform layers implemented via SRAM-based in-memory computing. When evaluated on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, the framework achieves significant multiplication reduction while maintaining comparable accuracy to baseline models.

## Method Summary
The HTMA-Net framework integrates Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to create efficient deep neural network architectures. The core innovation involves replacing intermediate convolutions with Hybrid Hadamard-based transform layers, where internal convolutions are implemented using multiplication-avoiding in-memory operations. This hybrid approach leverages the orthogonality properties of the Hadamard Transform to decompose and restructure neural network computations, enabling the use of addition-based operations instead of multiplications in critical computational paths.

## Key Results
- Eliminates up to 52% of multiplications compared to baseline ResNet-18 models
- Maintains comparable accuracy across CIFAR-10, CIFAR-100, and Tiny ImageNet datasets
- Significantly reduces computational complexity and parameters in evaluated models
- Demonstrates viability of combining structured transform layers with in-memory computing

## Why This Works (Mechanism)
The framework exploits the orthogonal properties of the Hadamard Transform to restructure neural network computations in a way that replaces expensive multiplication operations with addition-based operations. The in-memory computing aspect leverages SRAM-based architectures to perform these addition operations directly within memory arrays, eliminating data movement overhead and further reducing energy consumption. This combination allows the network to maintain representational capacity while dramatically reducing the computational burden of matrix multiplications.

## Foundational Learning
1. **Hadamard Transform** - An orthogonal transform that decomposes signals into orthogonal basis functions using only additions and subtractions; needed because it enables multiplication-free computation of convolutional operations; quick check: verify orthogonality property through transform matrix multiplication yielding identity

2. **In-Memory Computing** - Computing paradigm that performs operations directly in memory arrays rather than moving data to processors; needed to eliminate data movement overhead and reduce energy consumption; quick check: confirm that operations are performed within SRAM bit-cells

3. **SRAM-based MAC Operations** - Using SRAM arrays to perform multiply-accumulate operations through analog or digital computation within memory; needed to implement the addition-heavy operations required by Hadamard-based transforms efficiently; quick check: verify that SRAM bit-cells can perform addition/subtraction operations

## Architecture Onboarding

**Component Map:** Input -> Hadamard Transform Layer -> In-Memory Computing MAC Units -> Activation Functions -> Output

**Critical Path:** The critical computational path involves the Hadamard transform decomposition followed by in-memory addition operations, which must maintain numerical precision while eliminating multiplications.

**Design Tradeoffs:** The framework trades computational complexity (multiplications) for structural complexity (additional transform layers and specialized hardware), requiring careful balance to maintain accuracy while achieving efficiency gains.

**Failure Signatures:** Potential failure modes include numerical precision loss during transform operations, insufficient representational capacity due to aggressive multiplication reduction, and hardware-specific limitations in SRAM-based computing implementations.

**First Experiments:**
1. Implement a single Hadamard transform layer with in-memory computing on a small dataset to verify basic operation
2. Compare accuracy degradation when incrementally increasing multiplication reduction percentage
3. Profile energy consumption and latency of in-memory addition operations versus traditional multiplications

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not provide specific accuracy numbers for comparison across all datasets
- Hardware implementation details for SRAM-based in-memory computing are not fully specified
- Energy consumption and inference time improvements are not quantified
- The comparison baseline for "baseline models" is not explicitly defined

## Confidence

**Major Claims Assessment:**
- High confidence: The claim that HTMA-Net eliminates up to 52% of multiplications is likely accurate based on experimental setup
- Medium confidence: The assertion that accuracy remains comparable to baseline models requires verification of specific accuracy metrics
- Medium confidence: The claim about reducing computational complexity and parameters is plausible given multiplication reduction

## Next Checks

1. Verify the exact accuracy percentages achieved on all three datasets (CIFAR-10, CIFAR-100, Tiny ImageNet) and compare them to standard ResNet-18 baselines

2. Calculate and compare the total number of operations (including additions) and parameters between HTMA-Net and the baseline to assess full computational complexity reduction

3. Implement a small-scale hardware prototype or simulation to validate the claimed multiplication reduction in actual in-memory computing conditions