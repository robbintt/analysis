---
ver: rpa2
title: 'CodeSimpleQA: Scaling Factuality in Code Large Language Models'
arxiv_id: '2512.19424'
source_url: https://arxiv.org/abs/2512.19424
tags:
- code
- language
- qwen2
- chinese
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeSimpleQA introduces a bilingual benchmark with 1,498 factual
  QA pairs and a 66.9M sample instruction corpus to evaluate code LLM factuality.
  The post-training framework combining SFT and GRPO significantly improves factual
  accuracy over base models, with CodeSimpleQA-RL achieving 45.2% F-score versus 40.0%
  for the baseline.
---

# CodeSimpleQA: Scaling Factuality in Code Large Language Models

## Quick Facts
- arXiv ID: 2512.19424
- Source URL: https://arxiv.org/abs/2512.19424
- Reference count: 13
- Introduces 1,498 bilingual QA pairs and 66.9M instruction corpus for code LLM factuality evaluation

## Executive Summary
CodeSimpleQA addresses the critical challenge of factual accuracy in code large language models through a comprehensive post-training framework. The system combines supervised fine-tuning (SFT) and group relative policy optimization (GRPO) with a carefully curated bilingual benchmark and instruction corpus. The approach demonstrates significant improvements in factual accuracy, particularly for specialized programming domains where traditional execution metrics fall short. The benchmark reveals that even state-of-the-art models struggle with domain-specific knowledge, highlighting the need for factuality-aware alignment beyond conventional evaluation methods.

## Method Summary
CodeSimpleQA introduces a post-training framework that leverages both SFT and GRPO to enhance factual accuracy in code LLMs. The system is built around a bilingual benchmark containing 1,498 carefully curated factual QA pairs and a massive 66.9M sample instruction corpus. The framework employs a dual approach where SFT handles stable programming concepts while GRPO optimizes for factual consistency. The system also explores retrieval-augmented generation (RAG) for handling up-to-date documentation, contrasting it with pure post-training methods. The evaluation reveals that thinking mode consistently outperforms chat mode across different model sizes, and that domain-specific knowledge areas like bioinformatics present significant challenges even for frontier models.

## Key Results
- CodeSimpleQA-RL achieves 45.2% F-score versus 40.0% for baseline, representing a 5.2% absolute improvement
- Thinking mode outperforms chat mode logarithmically across all tested model sizes
- RAG excels at handling up-to-date documentation while SFT better handles stable programming concepts
- Even state-of-the-art models struggle significantly with specialized domains like bioinformatics

## Why This Works (Mechanism)
The effectiveness of CodeSimpleQA stems from its targeted approach to factual accuracy through specialized alignment rather than general capability enhancement. By focusing specifically on factual QA pairs across multiple programming languages, the framework addresses the gap between code execution ability and factual knowledge retention. The combination of SFT for stable concepts and GRPO for optimization creates a complementary training regime that improves both breadth and depth of factual knowledge. The thinking mode architecture provides additional reasoning capacity that proves particularly valuable for complex factual queries. The benchmark design itself, with its emphasis on bilingual evaluation and domain-specific challenges, creates a more rigorous test of factual accuracy than traditional execution-based metrics.

## Foundational Learning
- **Factuality in Code LLMs**: The ability to accurately recall and apply programming knowledge without fabrication is critical for practical deployment. Why needed: Code execution ability doesn't guarantee factual accuracy in explanations or documentation. Quick check: Measure improvement in F-score across multiple factual QA datasets.
- **Bilingual Evaluation**: Assessing models across multiple programming languages reveals cross-lingual generalization patterns and language-specific knowledge gaps. Why needed: Programming knowledge often transfers across languages but with important exceptions. Quick check: Compare performance consistency between languages in the same domain.
- **Domain-Specific Knowledge**: Specialized programming domains like bioinformatics require targeted factual knowledge beyond general programming concepts. Why needed: Standard benchmarks often miss critical domain-specific accuracy requirements. Quick check: Identify performance drop in specialized versus general programming tasks.
- **Thinking Mode Architecture**: Extended reasoning capabilities improve factual accuracy by allowing more thorough knowledge retrieval and verification. Why needed: Simple chat mode often produces superficial or hallucinated responses. Quick check: Compare F-scores between thinking and chat modes across diverse queries.
- **Post-Training vs RAG Trade-offs**: Understanding when to use fine-tuning versus retrieval-augmented generation for different types of factual knowledge. Why needed: Different knowledge types require different handling approaches for optimal accuracy. Quick check: Benchmark RAG and post-training methods across up-to-date versus stable knowledge domains.
- **Evaluation Methodology**: The importance of moving beyond execution metrics to measure factual accuracy directly. Why needed: Code can execute correctly while containing factual errors in explanations or documentation. Quick check: Compare evaluation results using execution metrics versus direct factual accuracy measures.

## Architecture Onboarding

Component Map: SFT/GRPO training -> Bilingual QA evaluation -> Domain-specific testing -> Cross-lingual comparison

Critical Path: Instruction corpus (66.9M samples) → SFT training → GRPO optimization → Benchmark evaluation → Factuality assessment

Design Tradeoffs: The system balances between RAG's ability to handle dynamic documentation and post-training's superior handling of stable concepts. This creates a complementary approach but requires careful selection based on knowledge type. The thinking mode provides better accuracy but at computational cost compared to chat mode.

Failure Signatures: Domain-specific weaknesses in areas like bioinformatics, performance degradation in cross-lingual scenarios, and the gap between execution ability and factual accuracy. The system also shows limitations when evaluation relies solely on exact string matching rather than semantic equivalence.

First Experiments:
1. Evaluate baseline model F-score on the bilingual benchmark to establish performance floor
2. Test thinking mode versus chat mode performance across different model sizes
3. Compare RAG and post-training approaches on up-to-date versus stable documentation tasks

## Open Questions the Paper Calls Out
The paper identifies several key open questions: How to scale the bilingual benchmark to cover more programming languages and paradigms? What is the optimal balance between instruction corpus size and quality? How can evaluation methodology move beyond exact string matching to capture semantic equivalence? What are the statistical significance patterns across multiple training runs? How do different instruction corpus distributions affect domain-specific performance? The paper also questions the scalability of the approach beyond tested model sizes and the generalizability to non-programming factual domains.

## Limitations
- The 1,498 bilingual QA pairs may not fully represent programming knowledge breadth across all languages and paradigms
- Evaluation relies on exact string matching which may miss semantically equivalent code explanations
- The instruction corpus size and distribution could introduce training-evaluation data shifts
- Limited ablation studies on individual component contributions to overall performance
- Statistical significance testing is not comprehensively reported across multiple runs

## Confidence

High confidence:
- SFT and GRPO improvements over base models
- Thinking mode superiority over chat mode
- Domain-specific weaknesses in specialized areas like bioinformatics

Medium confidence:
- Relative performance rankings between different alignment approaches
- RAG vs post-training trade-offs
- Cross-lingual generalization patterns

Low confidence:
- Absolute F-score values due to potential evaluation artifacts
- The 45.2% vs 40.0% comparison without statistical significance testing
- Scalability claims beyond tested model sizes

## Next Checks
1. Conduct statistical significance testing across multiple runs to verify the 5.2% F-score improvement is robust
2. Perform ablation studies isolating SFT vs GRPO contributions and testing different instruction corpus sizes
3. Expand evaluation to include semantic similarity metrics beyond exact matching to capture code-equivalent responses