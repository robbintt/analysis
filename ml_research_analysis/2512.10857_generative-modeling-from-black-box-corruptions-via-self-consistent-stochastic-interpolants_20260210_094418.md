---
ver: rpa2
title: Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic
  Interpolants
arxiv_id: '2512.10857'
source_url: https://arxiv.org/abs/2512.10857
tags:
- noise
- diffusion
- samples
- gaussian
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles generative modeling from corrupted data without
  clean samples, a common problem in scientific and engineering domains where measurements
  are observed through noisy, ill-conditioned channels. The authors introduce the
  Self-Consistent Stochastic Interpolant (SCSI) framework, which iteratively updates
  a transport map between corrupted and clean data samples using only black-box access
  to the corruption channel.
---

# Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants

## Quick Facts
- **arXiv ID:** 2512.10857
- **Source URL:** https://arxiv.org/abs/2512.10857
- **Reference count:** 40
- **Primary result:** SCSI achieves FID scores of 5.38 for random masking and 6.74 for JPEG compression, outperforming variational alternatives while requiring only black-box access to the corruption channel.

## Executive Summary
This paper tackles generative modeling from corrupted data without clean samples, a common problem in scientific and engineering domains where measurements are observed through noisy, ill-conditioned channels. The authors introduce the Self-Consistent Stochastic Interpolant (SCSI) framework, which iteratively updates a transport map between corrupted and clean data samples using only black-box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, enabling a generative model for the clean data.

The method demonstrates superior performance on inverse problems in natural image processing and scientific reconstruction, including random masking, Gaussian blurring with noise, motion blur, JPEG compression, and quasar spectra recovery. The approach is computationally efficient compared to variational alternatives, highly flexible handling arbitrary nonlinear forward models with only black-box access, and enjoys theoretical guarantees including convergence in Wasserstein metric and KL divergence. Experimental results show that SCSI achieves competitive performance even against methods that rely on additional access to the forward model or even clean data.

## Method Summary
SCSI learns a generative model for clean data using only corrupted observations and black-box access to the forward corruption model. The method iteratively updates a transport map through a self-consistency scheme: starting from corrupted samples, it applies the current backward transport to get pseudo-clean samples, passes these through the forward model to get synthetic corrupted samples, and trains a new interpolant between the pseudo-clean and synthetic corrupted distributions. The framework builds on stochastic interpolants to construct continuous-time transport between distributions without requiring explicit score matching on clean data. The training involves least-squares regression objectives for velocity fields and denoisers, implicitly regularizing the transport map to prevent overfitting.

## Key Results
- Achieves FID scores of 5.38 for random masking and 6.74 for JPEG compression on CIFAR-10
- Outperforms variational alternatives while requiring only black-box access to corruption channel
- Demonstrates superior performance on scientific reconstruction tasks including quasar spectra recovery
- Shows robustness to hyperparameters with T_tr = 1 inner step being compute-efficient
- Converges towards self-consistent transport map that effectively inverts the corruption channel

## Why This Works (Mechanism)

### Mechanism 1: Self-Consistency Fixed-Point Iteration
- **Claim:** The iterative scheme converges to a transport map that inverts the corruption channel at the distribution level.
- **Mechanism:** Starting from corrupted samples y ~ μ, the method iteratively: (1) applies current backward transport Φ_Θ to get "pseudo-clean" samples x = Φ_Θ(y), (2) passes these through the forward model F(x) to get synthetic observations, (3) trains a new interpolant between the pseudo-clean and synthetic corrupted distributions. At convergence, Kπ_Θ = μ, satisfying self-consistency.
- **Core assumption:** The forward channel K is injective at the distribution level (Kπ = Kπ̃ implies π = π̃).
- **Break condition:** If the forward channel is not injective, multiple distributions may satisfy Kπ = μ, and the fixed point is not unique.

### Mechanism 2: Distribution-Level Regularization via Stochastic Interpolant Training
- **Claim:** The least-squares regression objectives (3)-(4) implicitly regularize the transport map, preventing overfitting to finite corrupted samples.
- **Mechanism:** Training the velocity field b via E_b(̂b) = ∫₀¹ E[|̂b(t, I_t) - İ_t|²]dt and denoiser g via similar objective constrains the solution space to smooth transport maps. This regularized hypothesis class provides a finite condition number χ for the inverse problem.
- **Core assumption:** The true distribution π cannot be exactly represented by the model (misspecified setting), ensuring χ < ∞.
- **Break condition:** If the model capacity is too large relative to data, the condition number may diverge, breaking the KL contraction guarantee.

### Mechanism 3: Contraction Through Lipschitz Stability
- **Claim:** Under appropriate Lipschitz conditions on the map π → Φ_π, the iterative scheme contracts in Wasserstein and KL metrics.
- **Mechanism:** Theorem 9 shows that if L_ε χ < 4, where L_ε measures sensitivity of the SI drift to initial conditions and χ is the condition number, then KL(π||π^(k)) decreases geometrically. The proof exploits the relationship between forward and reverse Fokker-Planck equations along the diffusion bridge.
- **Core assumption:** Assumption 7 (Lipschitz stability in KL): ∀π, π̃, ||f_π - f_π̃||²_{π[0,1]} ≤ L_ε KL(π||π̃).
- **Break condition:** High corruption (very low SNR) or highly ill-conditioned channels may violate L_ε χ < 4, leading to divergence or slow convergence.

## Foundational Learning

- **Concept: Stochastic Interpolants**
  - **Why needed here:** The entire framework builds on SI to construct continuous-time transport between distributions without requiring explicit score matching on clean data.
  - **Quick check question:** Can you explain how equation (1) defines a valid interpolant between π and μ, and what role the schedules α_t, β_t, γ_t play?

- **Concept: Fokker-Planck Equations and Time Reversal**
  - **Why needed here:** The theoretical analysis relies on understanding how probability densities evolve under SDEs and how to reverse them—essential for grasping why the backward transport works.
  - **Quick check question:** What is the relationship between the forward SDE dX_t = b dt + √(2ε) dW_t and its reverse-time formulation?

- **Concept: Injectivity at Distribution Level**
  - **Why needed here:** This is the fundamental condition distinguishing "recoverable" from "non-recoverable" corruption channels—more subtle than sample-level invertibility.
  - **Quick check question:** Why is the AWGN channel y = x + σξ injective at the distribution level even though it's non-invertible at the sample level?

## Architecture Onboarding

- **Component map:** Corrupted samples y ~ μ -> Backward transport x = Φ_Θ(y) -> Forward model ỹ = F(x) -> Interpolant I_t = α_t·x + β_t·ỹ + γ_t·z -> Update Θ via SGD

- **Critical path:**
  1. Sample y ~ μ from corrupted dataset
  2. Backward transport: x = Φ_Θ(y) via ODE/SDE integration (64 steps)
  3. Forward pass: ỹ = F(x) using black-box simulator
  4. Construct interpolant: I_t = α_t·x + β_t·ỹ + γ_t·z
  5. Update Θ via SGD on losses (3)-(4)
  6. Repeat for K outer iterations (default T_tr = 1 inner step per outer)

- **Design tradeoffs:**
  - **ODE vs SDE:** ODE (ε=0) shows faster convergence in Gaussian AWGN case and is more robust to hyperparameters; SDE may help in extreme corruption regimes (Fig. 4)
  - **Network size:** Larger networks (96 channels) improve restoration quality but increase compute; 64 channels sufficient for competitive results (Table 5)
  - **T_tr (inner steps):** Performance robust to T_tr ∈ {1, 10, 100}; T_tr = 1 is compute-efficient (Table 4)

- **Failure signatures:**
  - **Collapse to thin distributions:** In high-noise synthetic data, ODE restoration may produce artificially concentrated samples (Fig. 4, σ_n = 1.0)
  - **Slow convergence:** If R ≥ 1 in Assumption 5, Wasserstein contraction fails—check by monitoring W₂(π^(k), π^(k+1))
  - **Extrapolation instability:** JPEG model trained only on q ∈ [0.1, 0.5] shows degraded but stable extrapolation to q > 0.5 (Fig. 17)

- **First 3 experiments:**
  1. **Sanity check on Gaussian AWGN:** Implement with 2D Gaussian prior, verify the explicit update rules (21) match theoretical prediction, plot convergence of Σ_k → Σ
  2. **Ablation on corruption severity:** Train on CIFAR-10 with random masking at ρ ∈ {0.25, 0.5, 0.75}, report LPIPS/FID restoration metrics (Table 6 provides baselines)
  3. **Black-box forward model test:** Implement motion blur or JPEG compression using standard library calls (no autodiff), verify the method works without gradients—compare to DPS which requires them

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do marginal transport methods (as in SCSI) compare with posterior transport approaches (EM-based) in terms of sample quality, convergence speed, and computational efficiency?
- Basis in paper: [explicit] Conclusion states: "an important aspect to be further investigated is the comparison between marginal and posterior transport, as in the EM algorithm and its diffusion-based implementations"
- Why unresolved: The paper theoretically establishes marginal transport advantages in simple settings (Gaussian AWGN) but does not systematically compare against EM posterior sampling in complex, high-dimensional regimes.
- What evidence would resolve it: Controlled experiments comparing SCSI with EM-based diffusion methods across multiple forward models, measuring FID, posterior calibration, and wall-clock time.

### Open Question 2
- Question: What is the quantitative tradeoff between condition number reduction and approximation error when restricting the velocity/score function class?
- Basis in paper: [explicit] Section 4.2 states: "a quantitative analysis of this tradeoff in specific function classes is beyond the scope of this work, but an interesting question deserving further attention."
- Why unresolved: Theoretical analysis establishes finite condition number under compact hypothesis classes but provides no quantitative bounds on χ or guidance on selecting regularization strength λ.
- What evidence would resolve it: Explicit bounds on condition number for concrete function classes (e.g., neural networks with bounded Lipschitz constants) coupled with empirical validation.

### Open Question 3
- Question: Can convergence guarantees be extended to the probability flow ODE regime (ϵ → 0)?
- Basis in paper: [explicit] Remark 11 states: "the current KL contraction results do not cover the limiting regime of ϵ → 0 where the transport is performed with the probability flow ODE."
- Why unresolved: The KL contraction proof requires diffusion coefficient ϵ > 0; the ODE case imposes that b_π must be constant (independent of π), which only holds for deterministic diffeomorphisms.
- What evidence would resolve it: Alternative proof techniques avoiding the Girsanov-based arguments that require diffusion, or modified assumptions specifically for the ODE setting.

### Open Question 4
- Question: How does time discretization affect convergence rates and sample quality?
- Basis in paper: [explicit] Section 4 states: "We focus on the idealised continuous-time limit, and leave time discretization aspects for future work."
- Why unresolved: Practical implementation uses finite transport steps (T_tr = 64); theoretical guarantees assume continuous-time integration.
- What evidence would resolve it: Convergence bounds incorporating Euler/Milstein discretization error, validated by experiments varying the number of transport steps.

### Open Question 5
- Question: Does the ODE scheme achieve accelerated O(1/k²) non-asymptotic convergence rates analogous to accelerated convex optimization?
- Basis in paper: [explicit] Section 5 states the non-asymptotic analysis "is out of the present scope" despite empirical observations in Figure 2 suggesting faster convergence than EM's O(1/k).
- Why unresolved: Theoretical analysis only provides asymptotic linear convergence rates; the mirror-descent structure of EM suggests an analogy to accelerated methods but remains unproven.
- What evidence would resolve it: Formal non-asymptotic rate analysis for the ODE scheme, potentially leveraging acceleration theory from optimization.

## Limitations
- Framework relies critically on injectivity of the forward corruption channel at the distribution level, which may not hold for all realistic forward models
- Theoretical analysis assumes strong regularity conditions that are difficult to verify in practice
- Performance degradation observed in high-noise regimes, particularly with ODE formulation
- Empirical validation focuses primarily on moderate corruption levels

## Confidence
- **High Confidence:** The self-consistency iteration mechanism and basic architecture design are well-supported by both theory and experiments
- **Medium Confidence:** Convergence guarantees and KL contraction claims require Assumption 7 which is difficult to verify empirically
- **Low Confidence:** Robustness claims to black-box forward models are supported by experiments with specific simulators but may not generalize to all possible corruption channels

## Next Checks
1. **Distribution-level injectivity test:** Systematically evaluate which forward models satisfy the injectivity assumption by testing whether Kπ = Kπ̃ implies π = π̃ across different corruption types and noise levels.

2. **Extreme corruption regime study:** Conduct controlled experiments with progressively higher corruption levels to identify the threshold where the ODE formulation breaks down and whether SDE stabilization consistently prevents collapse.

3. **Cross-dataset generalization:** Train models on one dataset (e.g., CIFAR-10) and evaluate restoration quality on out-of-distribution datasets (e.g., CelebA) to assess the method's ability to learn corruption-agnostic priors.