---
ver: rpa2
title: Adaptive Diffusion Policy Optimization for Robotic Manipulation
arxiv_id: '2505.08376'
source_url: https://arxiv.org/abs/2505.08376
tags:
- policy
- diffusion
- learning
- tasks
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an Adam-based Diffusion Policy Optimization
  (ADPO) framework to improve the stability and efficiency of diffusion-based reinforcement
  learning methods for robotic manipulation. ADPO incorporates adaptive gradient methods,
  particularly Adam and ADAPG, to enhance policy updates and reduce error accumulation
  caused by noise gradients.
---

# Adaptive Diffusion Policy Optimization for Robotic Manipulation

## Quick Facts
- **arXiv ID:** 2505.08376
- **Source URL:** https://arxiv.org/abs/2505.08376
- **Reference count:** 38
- **Primary result:** ADPO significantly improves training stability and final policy performance across GYM and ROBOMIMIC robotic tasks, with optimal hyperparameters ε=10⁻¹¹ and ω∈(0,1.5].

## Executive Summary
This paper introduces Adaptive Diffusion Policy Optimization (ADPO), a framework that incorporates adaptive gradient methods (Adam and ADAPG) to enhance the stability and efficiency of diffusion-based reinforcement learning for robotic manipulation. The framework addresses error accumulation from noise gradients by implementing per-parameter learning rates and momentum interpolation. Tested on three GYM environments and three ROBOMIMIC tasks, ADPO consistently improved training stability and final policy performance across six diffusion-based RL methods, with specific hyperparameter settings identified as optimal for different tasks.

## Method Summary
ADPO replaces standard optimizers in diffusion-based RL methods with adaptive gradient methods, primarily Adam and ADAPG. The framework calculates first-order and second-order moment estimates to create per-parameter learning rates that reduce step sizes for high-variance parameters while increasing them for low-variance parameters. ADAPG adds momentum interpolation via parameter ω to smooth weight trajectories and reduce overshoot error. The method was validated by replacing optimizers in six diffusion-based RL methods (DPPO, DIPO, IDQL, DAWR, QSM, DQL) across GYM and ROBOMIMIC environments, with pre-training followed by fine-tuning using ADPO.

## Key Results
- ADPO significantly improved training stability and final policy performance across all tested environments and tasks
- Hyperparameter ε=10⁻¹¹ consistently provided strong performance across multiple tasks
- Momentum parameter ω showed sensitivity to task dynamics, with optimal values ranging from 0.6 to 1.5
- ADPO demonstrated superior performance especially in complex or high-variance environments, accelerating convergence while maintaining robust exploration

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Gradient Normalization
The framework replaces standard optimizers with a tailored adaptive gradient method that calculates first-order (m_i) and second-order (v_i) moment estimates. By dividing the learning rate by √(v_i) + ε, the update step size is automatically reduced for parameters with high gradient variance and increased for parameters with small gradients. This per-parameter learning rate prevents noise inherent in RL policy gradients from destabilizing the diffusion model's denoising trajectory.

### Mechanism 2: Momentum Interpolation via ω
ADAPG modifies the standard momentum update with an interpolation factor ω to reduce overshoot error common in stochastic RL environments. The update rule calculates a tentative step h_i and sets the final parameter θ_i as a convex combination between h_i and the previous step h_{i-1} using ω, acting as a variance reduction technique that smooths the trajectory of weights in parameter space.

### Mechanism 3: Numerical Stability via ε Tuning
The framework achieves significant performance gains by aggressively tuning the numerical stabilizer ε to extremely small values (10⁻¹¹), preventing division by zero artifacts without dampening gradient magnitude. This ensures the adaptive scaling factor remains faithful to gradient magnitude, which is critical for fine-grained adjustments required in diffusion denoising steps.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** The policy is a generative process that iteratively refines noise into an action, requiring understanding of forward/backward Markov chains to interpret loss functions
  - **Quick check question:** Can you explain why the loss function for the diffusion policy involves predicting noise ε rather than directly predicting the action a?

- **Concept: Actor-Critic Reinforcement Learning**
  - **Why needed here:** ADPO wraps around existing methods that rely on a Critic (Q_φ or V_ψ) to estimate value, with the optimizer modifying how the Actor updates based on this value
  - **Quick check question:** In the advantage function A_{π_θ}, what does it represent and why is it used instead of the raw reward?

- **Concept: Adaptive Gradient Methods (Adam)**
  - **Why needed here:** The core contribution is a specific optimizer configuration, requiring understanding of how Adam computes running averages of gradients and squared gradients
  - **Quick check question:** What is the role of β_1 and β_2 in the Adam update, and how do they affect the optimizer's memory of past gradients?

## Architecture Onboarding

- **Component map:** Policy Network (U-Net/Transformer DDPM) -> Critic Network (MLPs Q_{φ1}, Q_{φ2}) -> ADPO Optimizer (custom class replacing torch.optim.Adam)

- **Critical path:** Environment Step → Replay Buffer → Sample Batch → Compute Critic Loss → Compute Actor Loss → Backprop → Extract Gradients → ADPO Step → Apply momentum interpolation to update weights

- **Design tradeoffs:** AdamW is stable but may converge slowly on complex tasks, while ADAPG introduces ω to accelerate convergence and handle noise but requires additional hyperparameter tuning

- **Failure signatures:** Runaway gradients if ε is too small or rewards are unnormalized, stagnation if ω is set too low losing momentum benefits

- **First 3 experiments:**
  1. Optimizer Replacement Validation: Swap standard optimizer for ADPO (ADAPG mode, ω=1.2) on pre-trained DPPO Hopper-v2 checkpoint and compare return curve
  2. Ablation on ε: Run three short runs with ε ∈ {10⁻⁸, 10⁻¹¹, 10⁻¹²} on HalfCheetah-v2 to verify 10⁻¹¹ provides lowest variance
  3. Complex Task Transfer: Apply ADPO to Square task (Robomimic) and verify acceleration of success rate improvement compared to standard DPPO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sensitivity of the momentum hyperparameter ω be mitigated through adaptive tuning rather than manual selection?
- Basis in paper: The authors note that ω is "sensitive to the environment, with its optimal value varying across different environments," requiring static manual tuning
- Why unresolved: The current framework relies on grid search to determine ω for each specific task, hindering generalization "out-of-the-box"

### Open Question 2
- Question: Does the ADPO framework transfer effectively to physical robotic hardware, or is it constrained by the simulation-to-reality gap?
- Basis in paper: While targeting "Robotic Manipulation," experimental validation is conducted exclusively on simulated GYM and ROBOMIMIC benchmarks
- Why unresolved: No empirical evidence regarding policy behavior under real-world noise, latency, and physical dynamics

### Open Question 3
- Question: Why does ADPO fail to improve upon the baseline IDQL method in specific environments like HalfCheetah and Can?
- Basis in paper: Table I indicates ADPO-enhanced IDQL (AIDQL) achieves performance only on par with baseline in specific tasks
- Why unresolved: The paper discusses aggregate success but does not analyze specific interaction between ADAPG and implicit Q-learning updates leading to stagnation

## Limitations
- Performance improvements are method-specific rather than general optimizer effects, with ADPO failing to improve IDQL in certain environments
- Extremely low ε value (10⁻¹¹) requires careful numerical validation to ensure stability across different hardware and gradient scales
- No empirical validation of transfer from simulation to physical robotic hardware, leaving simulation-to-reality gap unaddressed

## Confidence

- **High Confidence:** Adaptive gradient normalization (AdamW variant) improving stability by scaling per-parameter learning rates based on gradient variance is well-established and directly supported by text
- **Medium Confidence:** Momentum interpolation via ω (ADAPG) providing robust variance reduction technique is supported by ablation results but theoretical justification is incomplete
- **Medium Confidence:** ε=10⁻¹¹ being optimal numerical stabilizer is directly supported by experimental findings but explanation for universality is lacking

## Next Checks

1. **Numerical Stability Test:** Reproduce ε ablation for HalfCheetah-v2 while monitoring gradient norm explosion and NaN generation to confirm safety of 10⁻¹¹ setting
2. **Architecture Transfer:** Apply ADPO to different independently-implemented diffusion policy architecture (standard DDPM U-Net) on Hopper-v2 to test optimizer improvements are architecture-agnostic
3. **Momentum Analysis:** For Square task, run finer-grained sweep of ω values (0.9, 1.0, 1.1, 1.2, 1.3) to precisely map optimal range and confirm claimed sensitivity