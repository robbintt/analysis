---
ver: rpa2
title: Distribution-Aware Feature Selection for SAEs
arxiv_id: '2508.21324'
source_url: https://arxiv.org/abs/2508.21324
tags:
- features
- batchtopk
- feature
- selection
- sampledsae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the inefficiency of sparse autoencoders (SAEs)
  in feature selection, particularly the "activation lottery" problem where rare,
  high-magnitude features dominate selection at the expense of more consistent, informative
  features. The authors propose Sampled-SAE, which introduces a two-stage feature
  selection process: first scoring features using batch-level statistics (L2 norm,
  entropy, or squared L2), then restricting the candidate pool before applying Top-K
  selection.'
---

# Distribution-Aware Feature Selection for SAEs

## Quick Facts
- arXiv ID: 2508.21324
- Source URL: https://arxiv.org/abs/2508.21324
- Reference count: 40
- Key outcome: Sampled-SAE introduces two-stage feature selection that improves probing accuracy and reduces feature absorption while maintaining reconstruction quality, achieving better interpretability than BatchTopK on Pythia-160M

## Executive Summary
This paper addresses the inefficiency of sparse autoencoders (SAEs) in feature selection, particularly the "activation lottery" problem where rare, high-magnitude features dominate selection at the expense of more consistent, informative features. The authors propose Sampled-SAE, which introduces a two-stage feature selection process: first scoring features using batch-level statistics (L2 norm, entropy, or squared L2), then restricting the candidate pool before applying Top-K selection. This allows tuning between global consistency (small ℓ) and fine-grained reconstruction (large ℓ). On Pythia-160M, Sampled-SAE variants with ℓ=3-5 achieved better probing accuracy and reduced feature absorption compared to BatchTopK, while maintaining comparable reconstruction fidelity.

## Method Summary
Sampled-SAE modifies the standard BatchTopK SAE architecture by introducing a two-stage feature selection mechanism. First, features are scored using batch-level statistics (L2 norm, entropy, or squared L2) to create a candidate pool of size Kℓ. Then, Top-K selection is applied only within this restricted pool. This approach filters out rare, high-magnitude features that dominate BatchTopK selection, instead promoting consistent mid-frequency features. The expansion factor ℓ controls the trade-off between global consistency and fine-grained reconstruction, with smaller ℓ values producing more interpretable but less reconstruction-accurate features.

## Key Results
- Sampled-SAE with ℓ=3-5 achieved 2-3× higher densities of frequently activating features (>10% activation) compared to BatchTopK
- Probing accuracy improved by 5-10% while maintaining comparable FVU to BatchTopK
- Feature absorption fraction reduced from 0.3-0.4 (BatchTopK) to 0.2-0.3 (Sampled-SAE variants)
- L2-norm and Squared-L2 scoring functions outperformed entropy on probing accuracy, though all functions recovered unique feature sets

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Candidate Filtering
Pre-filtering features by batch-level statistics before Top-K selection reduces dominance by rare high-magnitude activations. For batch activations Z ∈ R^(m×B), compute feature scores q = s_φ(Z), select top Kℓ features as candidates, then apply BatchTopK only within this restricted pool. Small ℓ restricts competition to globally consistent features.

### Mechanism 2: Distribution-Aware Scoring Functions
Different scoring functions (L2-norm, squared-L2, entropy) bias feature selection toward distinct activation patterns. L2-norm rewards consistency, Squared-L2 is sensitive to both frequency and intensity, and entropy rewards specialization—features that concentrate activation on specific inputs.

### Mechanism 3: Tunable Reconstruction-Interpretability Trade-off
The expansion factor ℓ traces a spectrum between global consistency (ℓ≈1) and fine-grained reconstruction (large ℓ). At ℓ=1, all tokens draw from only K globally influential features. As ℓ increases toward n/K, the method recovers BatchTopK with unrestricted competition.

## Foundational Learning

- Concept: **BatchTopK SAE architecture**
  - Why needed here: Sampled-SAE is a modification of BatchTopK; understanding the baseline selection mechanism is prerequisite.
  - Quick check question: Can you explain why BatchTopK selects features at batch level rather than per-token, and what problem this solves?

- Concept: **Column subset selection (CSS) in matrix approximation**
  - Why needed here: The paper frames feature selection as a streaming CSS problem; L2-norm scoring approximates leverage scores from randomized linear algebra.
  - Quick check question: Why would column norms provide a reasonable proxy for feature importance in matrix reconstruction?

- Concept: **Feature absorption in SAEs**
  - Why needed here: A key evaluation metric; absorption occurs when hierarchical concepts (A implies B) cause gerrymandered latents ("B except A").
  - Quick check question: How would reducing rare high-magnitude features help maintain cleaner concept boundaries?

## Architecture Onboarding

- Component map:
  - Encoder: W_enc X + b_enc → pre-activations Z (m features × B batch)
  - Scoring module: s_φ(Z) → q ∈ R^m (one score per feature)
  - Candidate selector: TopK(q, Kℓ) → binary mask c
  - BatchTopK layer: Apply mask, then batch-level Top-K → sparse codes F
  - Decoder: F W_dec + b_dec → reconstruction

- Critical path: Scoring function choice → candidate pool size ℓ → which features compete → probing accuracy vs reconstruction trade-off. The scoring function and ℓ are the only new hyperparameters vs standard BatchTopK.

- Design tradeoffs:
  - L2-norm vs Squared-L2: L2-norm more aggressively filters rare spikes; Squared-L2 retains some high-magnitude sensitivity
  - Small ℓ vs large ℓ: Small ℓ improves probing/absorption but increases FVU; paper found ℓ=3-5 optimal on Pythia-160M
  - Entropy vs norm-based: Entropy captures specialization but underperformed BatchTopK on probing in experiments

- Failure signatures:
  - Training instability on synthetic data: Paper notes "Sampled-SAE training instabilities" with FVE ≈0.99 for BatchTopK but substantially worse for Sampled-SAE
  - Low cross-seed agreement: MMCS for Sampled-SAE variants (0.157-0.179) much lower than BatchTopK (0.277)—different seeds learn different features
  - Reconstruction degradation: As ℓ decreases, FVU increases; monitor reconstruction loss to detect over-aggressive filtering

- First 3 experiments:
  1. Reproduce BatchTopK baseline on your target model: Train standard BatchTopK, measure FVU, probing accuracy, and absorption.
  2. Sweep ℓ with L2-norm scoring: Train Sampled-SAE with ℓ ∈ {3, 5, 10, 20} and fixed K. Plot probing accuracy vs FVU and absorption vs FVU.
  3. Compare scoring functions at fixed ℓ: At your chosen ℓ, compare L2-norm, Squared-L2, and entropy on unique features discovered (using decoder similarity + semantic similarity matching against baseline).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretically grounded leverage scores or ridge leverage scores outperform the heuristic L2-norm and entropy scoring functions in feature selection?
- Basis: [explicit] Related Work states: "Future work could explore true leverage scores or ridge leverage scores for SAE feature selection."
- Why unresolved: The current study relies on computationally efficient heuristics (L2, entropy) without testing the stronger theoretical guarantees offered by leverage scores in matrix approximation.
- What evidence would resolve it: Implementing leverage score sampling and comparing the reconstruction fidelity and probing accuracy against the L2-norm and Squared-L2 variants.

### Open Question 2
- Question: Do the trade-offs observed in Sampled-SAE (specifically the benefits of low $\ell$ values) generalize to larger models and different layers?
- Basis: [explicit] Limitations section notes the scope was restricted: "We train SAEs only on a single layer of a single model due to computational constraints."
- Why unresolved: It is unclear if the "activation lottery" problem scales linearly with model size or if the optimal $\ell$ spectrum shifts in deeper layers.
- What evidence would resolve it: Training Sampled-SAE on larger architectures (e.g., 7B parameter models) to verify if the balance between global consistency and reconstruction holds.

### Open Question 3
- Question: Does promoting mid-frequency features actually improve human interpretability, or does it merely optimize for automated metrics like probing and absorption?
- Basis: [explicit] Limitations states: "we do not contrast interpretability of these features vis-a-vis high activating and less dense features to fully validate this hypothesis."
- Why unresolved: The paper relies on automated proxies (autointerp, probing) and admits finding "no evidence that this improves interpretability" despite density improvements.
- What evidence would resolve it: A human or gold-standard evaluation comparing the semantic clarity of features learned by Sampled-SAE against those learned by BatchTopK.

## Limitations

- Limited model scope: The study only evaluated Sampled-SAE on Pythia-160M layer 6, leaving generalization to larger models and different layers unknown
- Training instability: The method showed training instabilities on synthetic data, suggesting potential optimization challenges in certain settings
- Automated metrics only: The paper relies on automated interpretability proxies rather than human evaluation to validate that mid-frequency features are actually more interpretable

## Confidence

- **High Confidence**: The mechanism of two-stage candidate filtering is well-specified and the empirical results on Pythia-160M are robust
- **Medium Confidence**: The claim that distribution-aware selection improves interpretability without sacrificing reconstruction quality is supported but context-dependent
- **Low Confidence**: The generalizability of the feature distribution assumptions across different models, tasks, and scoring functions

## Next Checks

1. **Cross-Model Generalization Test**: Train Sampled-SAE with ℓ=3-5 on a different model family (e.g., Llama or Mistral) and compare feature activation distributions and interpretability metrics.

2. **Scoring Function Complementarity Analysis**: Train multiple Sampled-SAE variants with different scoring functions (L2-norm, Squared-L2, entropy) at the same ℓ, then use decoder similarity + semantic matching to measure unique features discovered by each.

3. **Reconstruction Stability Sweep**: Systematically vary ℓ from 1 to 20 while monitoring training stability and FVU on synthetic datasets with known ground truth.