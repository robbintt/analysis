---
ver: rpa2
title: 'SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in
  Visual Question Answering'
arxiv_id: '2509.20871'
source_url: https://arxiv.org/abs/2509.20871
tags:
- question
- visual
- llms
- knowledge
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge-based visual question
  answering (KB-VQA) by proposing a novel method called SCRA-VQA that leverages frozen
  large language models (LLMs) without requiring expensive end-to-end training. The
  core idea is to convert images into captions using pre-trained visual-language models,
  then generate contextual question-answer pairs, and finally summarize and rerank
  these captions to remove irrelevant information before passing them to the LLM.
---

# SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering

## Quick Facts
- arXiv ID: 2509.20871
- Source URL: https://arxiv.org/abs/2509.20871
- Authors: Yan Zhang; Jiaqing Lin; Miao Zhang; Kui Xiao; Xiaoju Hou; Yue Zhao; Zhifei Li
- Reference count: 38
- Primary result: Achieves 38.8% accuracy on OK-VQA and 34.6% on A-OKVQA using frozen LLMs without end-to-end training

## Executive Summary
SCRA-VQA addresses knowledge-based visual question answering by converting images into question-relevant captions using GradCAM attention maps, then summarizing and reranking these captions before passing them to a frozen LLM. The method filters irrelevant visual information through a caption-rerank process, helping LLMs better understand image content and questions. By leveraging pre-trained models without requiring expensive training, SCRA-VQA achieves strong performance on two challenging KB-VQA datasets while maintaining a lightweight structure.

## Method Summary
The SCRA-VQA pipeline consists of three stages: (1) BLIP-446M generates captions from question-relevant image regions identified via GradCAM cross-attention scores, sampling top-20 patches per image; (2) VQ2A generates synthetic QA pairs from captions using predefined templates; (3) BGE-Rerank-large scores caption-question pairs, selects top-5 captions, and Gemma2b summarizes them. The final prompt format combines multi-caption context, summary, and synthetic QA pairs before feeding to OPT-6.7B. The method uses 2×RTX4090 for inference.

## Key Results
- Achieves 38.8% accuracy on OK-VQA test set (5,046 questions)
- Achieves 34.6% accuracy on A-OKVQA validation set
- Outperforms previous zero-shot approaches while using frozen models
- Optimal performance with 5 captions for A-OKVQA and 20 for OK-VQA
- Summary length of 100 tokens optimal for A-OKVQA, 140 for OK-VQA

## Why This Works (Mechanism)

### Mechanism 1: Visual Attention-Based Caption Relevance
Filtering image captions by question relevance improves LLM reasoning by reducing noise-induced distraction. BLIP's cross-attention identifies question-relevant image regions through GradCAM, then captions are generated for top-K patches. This produces focused descriptions rather than generic scene summaries.

### Mechanism 2: Caption Reranking via Semantic Similarity
Reranking captions by direct question-caption similarity produces higher-quality LLM inputs than naïve aggregation. BGE-Rerank-large independently scores each caption-question pair using embedding similarity, selecting top-5 for the prompt. This direct matching strategy enables more precise text relevance calculation.

### Mechanism 3: Caption Summarization for Information Condensation
Summarizing multiple captions into condensed representation improves LLM answer accuracy by reducing token overhead while preserving key information. Gemma2b distills essential information from reranked captions into a format better suited for LLMs, with ablation showing summary inclusion boosts OK-VQA accuracy from 38.29% to 38.82%.

## Foundational Learning

- **Cross-attention in Vision-Language Models**
  - Why needed: SCRA-VQA relies on BLIP's cross-attention to identify which image regions align with question tokens
  - Quick check: Given a kitchen image and question "What color is the refrigerator?", which regions would high cross-attention scores likely highlight?

- **Retrieval-Augmented Generation (RAG) Reranking**
  - Why needed: The caption rerank module borrows from RAG retrieval scoring
  - Quick check: If a reranker assigns scores [0.8, 0.3, 0.75] to three captions, which would be retained with K=2?

- **Prompt Engineering for Frozen LLMs**
  - Why needed: SCRA-VQA's performance hinges on prompt structure (instruction + contexts + synthetic QA + target question)
  - Quick check: What prompt formatting risk arises from interleaving captions and QA pairs versus grouping all captions before all QA pairs?

## Architecture Onboarding

- **Component map:**
  Image → BLIP (446M) → Patch Attention (GradCAM) → Top-K Region Sampling → Caption Generation → Question → Synthetic QA Generator (VQ2A templates) → Caption + QA Pairs → BGE-Rerank → Top-5 Captions → Gemma2b Summarization → Prompt Constructor (Instruction + Reranked Captions + Summary + Synthetic QA + Question) → Frozen LLM (OPT 6.7B) → Answer

- **Critical path:**
  1. Patch relevance scoring (GradCAM) determines which regions are captioned—errors here propagate downstream
  2. Reranking filters captions; top-5 selection controls LLM input quality
  3. Summarization condenses information; overly long summaries (160+ tokens) reduced accuracy
  4. Prompt assembly order (MC+S+MQA) outperformed alternatives—deviating from this order risks performance loss

- **Design tradeoffs:**
  - Caption quantity: 5 captions optimal for A-OKVQA, 20 for OK-VQA; more captions increase noise, fewer reduce coverage
  - Summary length: 100 tokens for A-OKVQA, 140 for OK-VQA; longer summaries introduced irrelevant information
  - Reranker choice: BGE-Rerank-large outperformed base and CohereAI in experiments

- **Failure signatures:**
  - BLIP misidentification: Case 6(d) shows "string" instead of "kite" led to incorrect material prediction
  - Excessive captions: Performance drops when caption count exceeds optimal threshold
  - Poor prompt order: S+C+QA repetition pattern underperformed MC+S+MQA by ~2%

- **First 3 experiments:**
  1. Validate patch attention: Run SCRA-VQA on 50 OK-VQA samples with GradCAM visualization; verify attended regions align with question-relevant objects
  2. Ablate reranker: Replace BGE-Rerank-large with random caption selection on A-OKVQA validation set; expect ~3-5% accuracy drop
  3. Test summary length sensitivity: Evaluate OK-VQA with summary lengths [60, 100, 140, 180]; plot accuracy curve to confirm 140-token optimum

## Open Questions the Paper Calls Out
- Can the SCRA-VQA framework be effectively adapted for non-visual modalities, such as audio or video, without significant structural modifications?
- How can the image-to-text conversion stage be refined to prevent the propagation of factual errors into the reasoning LLM?
- Does the optimal number of retrieved captions vary dynamically based on question complexity rather than dataset-wide tuning?

## Limitations
- Relies heavily on visual attention mechanisms that may miss question-critical information outside attended regions
- Performance gains from caption summarization appear modest (38.29% → 38.82% on OK-VQA)
- Caption count optimization is dataset-specific rather than universal

## Confidence

**High confidence:** The core pipeline architecture is clearly specified and validated on benchmark datasets with substantial accuracy improvements.

**Medium confidence:** The effectiveness of GradCAM-based patch selection mechanism—the assumption that attended regions contain answer-relevant information is not extensively validated across diverse question types.

**Low confidence:** The generalization of caption summarization benefits—modest improvement and dataset-specific optimal lengths suggest the summarization step may be more brittle than presented.

## Next Checks

1. **Attention alignment validation:** Run SCRA-VQA on 100 OK-VQA samples with GradCAM visualization overlay; calculate percentage of attended regions containing objects referenced in questions. Target: >60% alignment.

2. **Reranker ablation with semantic probes:** Replace BGE-Rerank-large with keyword-matching reranker on A-OKVQA validation set; measure accuracy drop and compare semantic relevance of retained vs. discarded captions using human evaluation.

3. **Summary length robustness sweep:** Systematically evaluate OK-VQA performance with summary lengths [80, 100, 120, 140, 160, 180] tokens; plot accuracy vs. length to identify degradation threshold and test across question types.