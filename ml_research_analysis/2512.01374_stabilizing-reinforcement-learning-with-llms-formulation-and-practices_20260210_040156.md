---
ver: rpa2
title: 'Stabilizing Reinforcement Learning with LLMs: Formulation and Practices'
arxiv_id: '2512.01374'
source_url: https://arxiv.org/abs/2512.01374
tags:
- training
- policy
- gradient
- minirl
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel formulation for reinforcement learning
  (RL) with large language models (LLMs), explaining why and under what conditions
  the true sequence-level reward can be optimized via a surrogate token-level objective
  in policy gradient methods such as REINFORCE. Specifically, through a first-order
  approximation, we show that this surrogate becomes increasingly valid only when
  both the training-inference discrepancy and policy staleness are minimized.
---

# Stabilizing Reinforcement Learning with LLMs: Formulation and Practices

## Quick Facts
- arXiv ID: 2512.01374
- Source URL: https://arxiv.org/abs/2512.01374
- Reference count: 33
- Primary result: Policy gradient with importance sampling correction achieves highest stability for on-policy RL training in 30B MoE models

## Executive Summary
This paper establishes a theoretical framework for understanding when token-level surrogate objectives can reliably optimize sequence-level rewards in LLM reinforcement learning. Through first-order approximation analysis, the authors demonstrate that surrogate validity requires minimizing both training-inference discrepancy and policy staleness. The work provides principled justification for widely-used stabilization techniques including importance sampling correction, gradient clipping, and Routing Replay for MoE architectures. Extensive experiments with a 30B MoE model validate these insights, showing that on-policy training with basic policy gradient and importance sampling achieves optimal stability, while off-policy methods require additional stabilization through clipping and Routing Replay.

## Method Summary
The authors propose a novel formulation explaining the conditions under which token-level surrogate objectives in policy gradient methods can effectively optimize sequence-level rewards. They employ first-order Taylor approximation to derive conditions for surrogate validity, showing that both training-inference discrepancy and policy staleness must be minimized. The theoretical analysis leads to practical recommendations: for on-policy training, basic policy gradient with importance sampling correction suffices, while off-policy updates require clipping and Routing Replay to mitigate policy staleness-induced instability. The framework is validated through extensive experiments totaling hundreds of thousands of GPU hours on a 30B MoE model.

## Key Results
- On-policy policy gradient with importance sampling correction achieves highest training stability
- Off-policy updates require clipping and Routing Replay to prevent instability from policy staleness
- Prolonged optimization yields comparable final performance regardless of cold-start initialization
- Theoretical conditions for surrogate validity are empirically validated

## Why This Works (Mechanism)
The surrogate token-level objective becomes increasingly valid as a proxy for sequence-level reward optimization when the first-order Taylor approximation accurately captures the reward landscape. This accuracy depends on minimizing the difference between training and inference behavior (discrepancy) and ensuring policy updates remain small enough to prevent stale gradient estimates (staleness). When these conditions hold, the gradient estimates remain unbiased and have reasonable variance, enabling stable learning. The importance sampling correction directly addresses the discrepancy issue by reweighting trajectories to match the current policy, while clipping and Routing Replay mitigate staleness effects by controlling update magnitude and maintaining relevant routing decisions respectively.

## Foundational Learning
- **Policy gradient methods**: Why needed - Core optimization framework for RL with LLMs; Quick check - Verify REINFORCE implementation matches standard formulation
- **Importance sampling**: Why needed - Corrects for distribution shift between behavior and target policies; Quick check - Confirm importance weights remain bounded
- **Policy staleness**: Why needed - Quantifies how outdated policy parameters degrade gradient estimates; Quick check - Measure parameter drift between update batches
- **First-order Taylor approximation**: Why needed - Provides mathematical foundation for surrogate validity conditions; Quick check - Verify approximation error remains small
- **Mixture-of-Experts routing**: Why needed - Critical for understanding MoE-specific stabilization needs; Quick check - Track routing stability during training
- **Reward shaping**: Why needed - Influences the landscape where surrogate objectives operate; Quick check - Analyze reward function smoothness

## Architecture Onboarding

**Component Map**: Environment -> Reward Function -> Policy Network (MoE) -> RL Algorithm -> Training Loop -> Routing Manager

**Critical Path**: Environment interaction → Reward computation → Policy gradient estimation → Parameter update → Routing decision → Next environment interaction

**Design Tradeoffs**: On-policy training offers maximum stability but slower convergence versus off-policy methods that accelerate learning at the cost of increased instability requiring additional stabilization techniques

**Failure Signatures**: Training instability manifests as exploding gradients, divergent reward curves, or routing collapse in MoE architectures; can be diagnosed through monitoring gradient norms and routing entropy

**First Experiments**:
1. Verify on-policy training stability with basic REINFORCE and importance sampling on a small MoE model
2. Test clipping effectiveness by comparing gradient norm distributions with and without clipping
3. Evaluate Routing Replay impact by measuring routing consistency before and after implementation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section suggests several areas for future investigation, including scalability to larger architectures and dense models, and more precise characterization of when surrogate objectives become invalid.

## Limitations
- First-order Taylor approximations may break down with large policy updates or highly non-linear reward landscapes
- Experimental validation primarily focuses on 30B MoE models, limiting generalizability claims
- Simplified discrepancy metric may not capture all distribution shift aspects between training and inference
- Does not specify quantitative thresholds for when surrogate objectives become invalid

## Confidence
- Routing Replay effectiveness for MoE models: High
- Importance sampling and clipping general applicability: Medium
- First-order approximation validity across all scenarios: Medium
- Policy staleness impact quantification: High

## Next Checks
1. Test framework on dense transformer models (7B, 70B) to verify importance sampling and clipping effectiveness generalizability
2. Conduct controlled experiments measuring policy staleness duration versus training instability magnitude across different RL algorithms
3. Evaluate correlation between first-order approximation error and reward function characteristics (sparsity, non-monotonicity) across diverse task domains