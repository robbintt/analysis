---
ver: rpa2
title: Can We Reliably Rank Model Performance across Domains without Labeled Data?
arxiv_id: '2510.09519'
source_url: https://arxiv.org/abs/2510.09519
tags:
- performance
- across
- error
- datasets
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether label-free methods can reliably
  rank model performance across diverse domains. Using a two-step framework, base
  classifiers are trained on source domains, and auxiliary error models predict where
  these classifiers will fail on unseen data.
---

# Can We Reliably Rank Model Performance across Domains without Labeled Data?

## Quick Facts
- arXiv ID: 2510.09519
- Source URL: https://arxiv.org/abs/2510.09519
- Reference count: 19
- Large language model-based error predictors achieve Spearman correlations above 0.62 with true accuracy rankings across domains.

## Executive Summary
This study investigates whether label-free methods can reliably rank model performance across diverse domains. Using a two-step framework, base classifiers are trained on source domains, and auxiliary error models predict where these classifiers will fail on unseen data. Experiments on GeoOLID and Amazon Reviews datasets show that large language model-based error predictors (e.g., LLaMa-3.1-8B, Gemma-3-12B-it) achieve Spearman correlations above 0.62 with true accuracy rankings, outperforming drift-based and zero-shot baselines. Ranking reliability depends on the magnitude of true performance differences and alignment between predicted and actual errors. When accuracy ranges are narrow, simpler error models like logistic regression perform competitively by capturing lexical and confidence-based patterns.

## Method Summary
The proposed method uses a two-step framework: first, train base classifiers on labeled source domains; second, train error predictors to estimate where these base models will fail on unlabeled target domains. Error predictors take as input the text instance and the base model's prediction, outputting a binary correctness judgment. These instance-level predictions are aggregated to estimate domain-level accuracy, which is then ranked. The study compares LLM-based error predictors against traditional approaches across two datasets: GeoOLID (offensive language detection across 15 US cities) and Amazon Reviews (sentiment analysis across 15 product categories).

## Key Results
- LLM-based error predictors (LLaMa-3.1-8B, Gemma-3-12B-it) achieve Spearman correlations above 0.62 with true accuracy rankings
- Ranking reliability depends on the magnitude of true performance differences between domains
- When accuracy ranges are narrow (e.g., Amazon Reviews SD=0.015), simpler error models like logistic regression perform competitively
- Error model accuracy strongly predicts ranking reliability (ρ = 0.77 between error model accuracy and Spearman correlation)

## Why This Works (Mechanism)
The method works by decoupling the task of solving the downstream problem from the task of evaluating performance quality. The base model performs the classification, while the error predictor acts as an independent judge that learns to identify systematic failure patterns. This separation allows the error predictor to capture domain-specific failure modes without requiring labels in the target domain. The aggregation of instance-level error predictions provides a domain-level performance estimate that correlates with true accuracy when the error predictor has learned generalizable failure patterns.

## Foundational Learning

**Concept: Spearman Rank Correlation (ρ)**
- Why needed here: This is the primary metric for evaluating ranking reliability. It measures the monotonic relationship between the predicted performance ranking and the true performance ranking across domains.
- Quick check question: If model A ranks domains as X>Y>Z and model B ranks them as Y>X>Z, what does a Spearman correlation reveal about their agreement? (Answer: It would show positive but not perfect correlation, penalizing the swapped rank of X and Y).

**Concept: Two-Step (Base + Error Model) Architecture**
- Why needed here: This is the core framework of the proposed method. Understanding the decoupling is critical: the base model performs the task, while the separate error model predicts the quality of that performance.
- Quick check question: In this framework, which model is trained to solve the downstream task (e.g., sentiment analysis), and which model is trained to predict the output of the first model?

**Concept: Distributional Spread and Ranking Difficulty**
- Why needed here: The paper highlights that ranking is fundamentally harder when domains have similar performance.
- Quick check question: Why would a Spearman correlation be inherently lower when trying to rank five domains with accuracies of 70%, 71%, 70.5%, 69.5%, and 70.2% compared to ranking domains with accuracies of 50%, 80%, 65%, 90%, and 40%?

## Architecture Onboarding

**Component Map:**
Base Classifier (f_θ) -> Instance-Level Error Signal (eᵢ) -> Error Predictor (g_φ) -> Performance Estimator -> Spearman Correlation

**Critical Path:**
The path to a reliable performance ranking flows through the Error Predictor. The most critical step is training/prompting g_φ effectively. The method requires a labeled source domain to create the training data for the error predictor, which is then applied without labels to target domains.

**Design Tradeoffs:**
- LLM vs. Simple Error Model: LLM-based error predictors are more robust, but a simple Linear Model is competitive when accuracy ranges are narrow. Tradeoff: Computational cost vs. ranking robustness.
- Base Model Choice: The base model can be a fine-tuned model (RoBERTa) or a few-shot LLM. Tradeoff: A fine-tuned base model may have different, potentially more systematic failure modes that are easier for the error predictor to learn than a few-shot LLM's failures.

**Failure Signatures:**
- Collapsed Rankings: All domains receive similar estimated performance scores. Diagnosis: Check the true performance spread. If it is narrow, the signal is weak and ranking is inherently unreliable.
- Negative Correlation: The estimated rankings are the inverse of true performance. Diagnosis: The error model is systematically predicting "correct" for wrong answers or vice-versa, indicating poor generalization or a mis-specified prompt.
- High Variance Across Source Domains: Ranking reliability fluctuates wildly depending on which source domain was used for training. Diagnosis: The error model is overfitting to source-domain specific artifacts.

**First 3 Experiments:**
1. Establish a Baseline with a Linear Model: Train a simple classifier (e.g., Logistic Regression) as the base model, then train a linear error predictor using a held-out validation set. Compute Spearman correlation.
2. Compare with an LLM Error Predictor: Replace the linear error predictor with an LLM (e.g., LLaMA-3.1-8B) prompted for binary correctness judgment. Compare its Spearman correlation to the linear baseline.
3. Probe with Synthetic Variation: Artificially inject errors into your best and worst domains to widen the performance margin. Re-run the ranking evaluation to confirm that a larger performance spread improves correlation.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can label-free performance estimation methods reliably rank model performance across generation and multimodal tasks?
- Basis in paper: The conclusion states: "Future work will extend this analysis to generation and multimodal tasks, where the sources of variation and error alignment may differ."
- Why unresolved: The study only evaluated classification tasks. Generation and multimodal tasks involve different error types that may not be captured by binary error prediction.
- What evidence would resolve it: Experiments applying the framework to text generation benchmarks and multimodal tasks, measuring Spearman correlations between predicted and true performance rankings.

**Open Question 2**
- Question: How can task-specific priors be integrated to improve ranking stability across heterogeneous datasets?
- Basis in paper: The conclusion explicitly calls for investigating "how task-specific priors can improve ranking stability across heterogeneous datasets."
- Why unresolved: The paper demonstrates that ranking reliability varies with domain heterogeneity but does not propose methods to incorporate prior knowledge about task structure.
- What evidence would resolve it: A comparative study showing whether incorporating task metadata, linguistic features, or domain embeddings as priors improves ranking correlations.

**Open Question 3**
- Question: What mechanisms can better align predicted error patterns with actual model failure modes when accuracy ranges are narrow?
- Basis in paper: The paper finds that "ranking is more reliable when performance differences across domains are larger" and shows poor ranking when accuracy is tightly clustered.
- Why unresolved: The paper identifies the problem but does not explain why simpler lexical and confidence-based patterns outperform LLM error predictors in narrow accuracy ranges.
- What evidence would resolve it: Ablation studies isolating which error model features contribute most to ranking accuracy under varying accuracy distributions.

## Limitations
- Ranking reliability is highly dependent on the magnitude of true performance differences between domains
- The study uses only two specific datasets that may not represent all domain adaptation scenarios
- Limited exploration of the full space of base model and error predictor combinations
- No investigation of how ranking reliability scales with model size beyond the tested 8B-32B parameter range

## Confidence
- **High Confidence:** The core finding that LLM-based error predictors outperform traditional drift-based and zero-shot baselines when accuracy ranges are sufficient.
- **Medium Confidence:** The generalizability of the two-step framework to domains with different characteristics (e.g., semantic drift vs. stylistic differences).
- **Low Confidence:** The specific performance thresholds (e.g., ρ > 0.62) as universal benchmarks for ranking reliability.

## Next Checks
1. **Synthetic Performance Spread Test:** Artificially widen or narrow the performance gap between domains by injecting errors or improving model performance. Re-evaluate ranking reliability to confirm that correlation scales with performance variance as predicted.
2. **Domain Type Sensitivity Analysis:** Apply the framework to datasets with different types of domain shifts (e.g., semantic drift vs. stylistic variation) to determine whether LLM error predictors maintain their advantage across different domain shift characteristics.
3. **Cost-Benefit Analysis:** Compare the computational cost of LLM-based error predictors against their performance gains, particularly for narrow accuracy ranges where simpler models like logistic regression perform competitively.