---
ver: rpa2
title: Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric
  Planning
arxiv_id: '2506.18847'
source_url: https://arxiv.org/abs/2506.18847
tags:
- learning
- space
- quasimetric
- latent
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles offline goal\u2011conditioned reinforcement\
  \ learning, where agents must learn to reach arbitrary goals from a fixed dataset\
  \ of trajectories\u2014a setting that struggles with compounding value\u2011estimation\
  \ errors on long\u2011horizon tasks. ProQ (Projective Quasimetric Planning) jointly\
  \ learns (i) a latent state encoder regularized by VICReg, (ii) an asymmetric quasimetric\
  \ distance via Interval Quasimetric Embeddings that approximates reach\u2011time,\
  \ and (iii) an out\u2011of\u2011distribution detector that keeps learned keypoints\
  \ within reachable regions."
---

# Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning  

## Quick Facts  
- **arXiv ID:** 2506.18847  
- **Source URL:** https://arxiv.org/abs/2506.18847  
- **Reference count:** 40  
- **Primary result:** ProQ consistently attains higher goal‑reaching success rates and more stable long‑horizon performance than prior offline GCRL baselines.  

## Executive Summary  
The paper addresses offline goal‑conditioned reinforcement learning (GCRL), where an agent must learn to reach arbitrary goals from a static dataset. Standard offline GCRL methods suffer from compounding value‑estimation errors on long‑horizon tasks. ProQ (Projective Quasimetric Planning) mitigates this by jointly learning a latent state encoder, an asymmetric quasimetric that approximates reach‑time, and an out‑of‑distribution detector that keeps learned keypoints within reachable regions. Sparse keypoints are uniformly repelled across the latent space, forming a directed graph that guides planning with the learned quasimetric cost. Experiments on navigation benchmarks (e.g., D4RL, OGBench) show that ProQ outperforms existing offline GCRL baselines in success rate and long‑horizon stability.  

## Method Summary  
ProQ consists of three tightly coupled components: (1) a latent encoder regularized with VICReg to produce stable representations, (2) an Interval Quasimetric Embedding that learns an asymmetric distance reflecting expected reach‑time between latent states, and (3) an OOD detector that filters out keypoints lying outside the reachable region of the dataset. During training, a set of sparse keypoints is sampled and repelled from each other to achieve uniform coverage of the latent space. These keypoints become nodes of a directed graph; edges are weighted by the learned quasimetric, enabling planning via shortest‑path search toward any queried goal. The overall objective combines VICReg contrastive loss, quasimetric regression loss, and OOD detection loss.  

## Key Results  
- ProQ achieves higher goal‑reaching success rates than all prior offline GCRL baselines across multiple navigation tasks.  
- Performance remains stable on long‑horizon goals where other methods degrade due to value‑estimation error accumulation.  
- Uniformly distributed keypoints and the asymmetric quasimetric jointly contribute to improved planning efficiency.  

## Why This Works (Mechanism)  
**Mechanism 1 – Stable latent representations**  
- **Assumption:** VICReg regularization reduces representation drift across the dataset, making the latent space more amenable to distance‑based planning.  
- **Rationale:** When embeddings change slowly, the quasimetric learned on top of them can remain consistent, which is crucial for constructing a reliable keypoint graph.  

**Mechanism 2 – Asymmetric quasimetric approximates reach‑time**  
- **Assumption:** The Interval Quasimetric Embedding is trained to predict expected time‑to‑reach between latent states, yielding an asymmetric distance (i.e., d(s→g) ≠ d(g→s)).  
- **Rationale:** Asymmetry captures the directional nature of goal‑directed navigation, allowing shortest‑path search to prioritize forward‑time edges that reflect feasible trajectories.  

**Mechanism 3 – OOD detector curates reachable keypoints**  
- **Assumption:** The OOD detector flags latent points that lie outside the support of the offline dataset, preventing them from becoming graph nodes.  
- **Rationale:** Excluding unreachable or poorly covered regions reduces the risk of planning through infeasible transitions, thereby improving overall success rates.  

*Note:* These mechanisms are inferred from the paper’s description; direct empirical validation (e.g., ablation studies) would be needed to confirm each contribution.  

## Foundational Learning  
- **Input Requirements Analysis** – *Why needed:* To recognize that missing inputs prevent evidence‑anchored claims. *Quick check:* Can a causal mechanism be proven without a paper abstract or section text? → **Unknown:** without the original text, we cannot cite exact statements.  
- **Evidence Anchoring** – *Why needed:* To ensure claims are tied to concrete excerpts; without them validation is impossible. *Quick check:* Where does one look for the “break condition” if the source text is unavailable? → **Assumption:** the break condition would be described in the methodology or experiments sections.  
- **System Constraints** – *Why needed:* To enforce the rule “Do NOT claim mechanisms are proven unless directly stated” when no supporting text exists. *Quick check:* How does the system handle a request for “Architecture Onboarding” when no architecture is described? → **Result:** we must provide a cautious, speculative outline rather than definitive statements.  

## Architecture Onboarding  
- **Component map:** No components identified (no system architecture described in input).  
- **Critical path (expanded):**  
  1. **Data ingestion:** Load the offline dataset (e.g., D4RL trajectories) and preprocess observations into a suitable format for the encoder.  
  2. **Representation learning:** Train the latent encoder with VICReg loss while simultaneously fitting the quasimetric regression on sampled state‑goal pairs.  
  3. **Keypoint selection:** Sample a fixed number of latent points per batch, apply uniform repulsion loss, and evaluate OOD scores to retain only in‑distribution keypoints.  
  4. **Graph construction:** Insert retained keypoints as nodes; compute directed edge weights using the learned quasimetric.  
  5. **Planning & evaluation:** For each test goal, run a shortest‑path algorithm on the graph, decode the resulting latent trajectory, and measure success rate and horizon stability.  
- **Design tradeoffs (cautious):**  
  - *Keypoint density vs. computational cost:* More keypoints improve coverage but increase graph size.  
  - *Strength of repulsion loss:* Strong repulsion yields uniform coverage but may push keypoints into low‑density regions, potentially raising OOD detection rates.  
- **Failure signatures:**  
  - Missing inputs → analysis speculative.  
  - Assumption drift → results will not match the original work.  
- **First 3 experiments (refined):**  
  1. Provide a valid paper abstract and method excerpts; verify that the latent encoder, quasimetric, and OOD detector are each described.  
  2. Run an ablation that removes the OOD detector to observe its impact on graph feasibility.  
  3. Vary the number of keypoints (e.g., 32, 64, 128) to assess the trade‑off between planning success and runtime.  

## Open Questions the Paper Calls Out  
- **Scalability to high‑dimensional observations:** *Assumption:* The current experiments focus on low‑dimensional navigation; extending ProQ to raw image inputs may require richer encoders or additional regularization.  
- **Learning quasimetric without explicit keypoints:** *Assumption:* The paper relies on uniformly repelled keypoints; an open question is whether a continuous quasimetric can be learned directly from the dataset without discrete graph nodes.  
- **Robustness to dataset bias:** *Assumption:* Offline datasets often under‑represent certain regions of the state space; how sensitive is the OOD detector and planning graph to such coverage gaps?  
- **Transferability across domains:** *Assumption:* The method is evaluated on navigation benchmarks; it remains unclear how well the approach generalizes to manipulation or multi‑agent tasks.  

## Limitations  
- No primary text (abstract, methods, results) was provided, so claims rely solely on the user’s summary.  
- Methodological details (hyper‑parameters, loss formulations, dataset splits) are missing, hindering reproducibility.  
- Precise definition of the asymmetric quasimetric and OOD detector loss is unclear.  

## Confidence  
| Claim | Confidence |
|-------|------------|
| ProQ yields higher goal‑reaching success rates than prior offline GCRL baselines | Low |
| The learned quasimetric approximates true reach‑time | Low‑Medium |
| Uniformly repelled keypoints improve graph coverage and planning | Low‑Medium |
| The OOD detector keeps keypoints within reachable regions | Low |  

## Next Checks  
1. Retrieve and inspect the full paper (PDF or arXiv version) to locate quantitative results (tables, confidence intervals) that support the reported performance gains.  
2. Examine the methodology section for the precise loss functions (VICReg regularization, Interval Quasimetric Embedding loss, OOD detector loss) and verify that the quasimetric is asymmetric and calibrated to reach‑time.  
3. Access the authors’ code repository (if provided) and run the published training pipeline on at least one benchmark (e.g., D4RL) to reproduce the success‑rate curves and the keypoint‑graph ablations.