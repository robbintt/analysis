---
ver: rpa2
title: 'Performative Policy Gradient: Optimality in Performative Reinforcement Learning'
arxiv_id: '2512.20576'
source_url: https://arxiv.org/abs/2512.20576
tags:
- policy
- learning
- performative
- gradient
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the gap in reinforcement learning where deployed\
  \ policies alter the environment they act in, causing distribution shifts that standard\
  \ RL ignores. It introduces Performative Policy Gradient (PePG), a policy\u2011\
  gradient algorithm that explicitly incorporates the performative feedback loop by\
  \ differentiating through the environment\u2019s response to the current policy\
  \ (the \u201Cperformative value\u201D function)."
---

# Performative Policy Gradient: Optimality in Performative Reinforcement Learning  

## Quick Facts  
- **arXiv ID:** 2512.20576  
- **Source URL:** https://arxiv.org/abs/2512.20576  
- **Reference count:** 40  
- **Primary result:** The Performative Policy Gradient (PePG) algorithm provably converges to *performatively optimal* policies and empirically outperforms existing performativeâ€‘RL baselines.  

## Executive Summary  
Standard reinforcement learning assumes a static environment, yet many deployed policies actively reshape the dynamics they encounter (e.g., pricing, traffic control). This paper formalises that feedback loop as a *performative* shift and introduces **Performative Policy Gradient (PePG)**, a policyâ€‘gradient method that differentiates through the environmentâ€™s response to the current policy. Under a softmax policy parametrisationâ€”optionally with entropy regularisationâ€”PePG is shown to converge to policies that remain optimal after the induced shift, extending prior work that only guaranteed performative *stability*. Experiments on benchmark tasks demonstrate consistent gains over prior performativeâ€‘RL baselines, with strong entropy regularisation (Î»â€¯=â€¯2) and larger learning rates (Î·â€¯=â€¯0.5) delivering the best final performance and value estimates, respectively.  

## Method Summary  
PePG augments the classic REINFORCE update by inserting a *performative value* functionâ€¯\( \hat V_{\pi}^{\pi} \) that captures the expected return after the environment has adapted to the current policyâ€¯\( \pi_\theta \). The algorithm computes  

\[
\nabla_\theta \hat V_{\pi}^{\pi}(\theta) = \mathbb{E}_{s\sim d_{\pi_\theta}, a\sim\pi_\theta}\!\big[ \nabla_\theta \log \pi_\theta(a|s)\, Q_{\pi_\theta}^{\text{perf}}(s,a) \big],
\]

where \( Q_{\pi_\theta}^{\text{perf}} \) is evaluated under the *performatively shifted* transition dynamics. The environment response is modelled as a differentiable map \( \mathcal{F}(\pi_\theta) \); gradients are backâ€‘propagated through \( \mathcal{F} \) using automatic differentiation. Entropy regularisation adds \( \lambda \mathcal{H}(\pi_\theta) \) to the objective, stabilising updates. Convergence proofs rely on stochastic approximation under standard smoothness and boundedness assumptions.  

## Key Results  
- **Convergence to performatively optimal policies** under softmax parametrisation (with/without entropy regularisation).  
- **Empirical superiority**: PePG beats prior performativeâ€‘RL baselines on all reported benchmarks.  
- **Ablation insights**: Strong entropy regularisation (Î»â€¯=â€¯2) yields the highest final performance (â‰ˆâ€¯0.05 vsâ€¯â‰ˆâ€¯â€‘0.01 for Î»â€¯â‰¤â€¯1); a larger learning rate (Î·â€¯=â€¯0.5) achieves the best value estimate (Ä¤VÏ€Ï€â€¯â‰ˆâ€¯4) at the cost of higher variance.  

## Why This Works (Mechanism)  

### Mechanismâ€¯1 â€“ Gradient through the performative response  
- **Claim:** Differentiating through the environmentâ€™s response aligns policy updates with the *true* postâ€‘shift objective.  
- **Mechanism:** The map \( \mathcal{F}(\pi_\theta) \) captures how the transition kernel changes with the policy; backâ€‘propagation yields \(\partial \mathcal{F}/\partial \theta\) which informs the gradient of the performative value.  
- **Core assumption:** \( \mathcal{F} \) is smooth and differentiable w.r.t.â€¯Î¸.  
- **Evidence anchors:** Theoremâ€¯2 (convergence) builds on this gradient; Figureâ€¯3 shows the gradient magnitude decreasing as the policy approaches the performatively optimal point.  

### Mechanismâ€¯2 â€“ Softmax policy parametrisation  
- **Claim:** A softmax parametrisation ensures the policy gradient remains wellâ€‘behaved under performative shifts.  
- **Mechanism:** Softmax yields a smooth probability simplex, allowing the Jacobian of \(\log \pi_\theta\) to be analytically tractable and bounded.  
- **Core assumption:** Action space is discrete and the temperature parameter is fixed.  
- **Evidence anchors:** Lemmaâ€¯1 derives Lipschitz constants for the softmax policy; empirical Sectionâ€¯5.2 reports stable training curves only with softmax (vs. linear policies).  

### Mechanismâ€¯3 â€“ Entropy regularisation for stability  
- **Claim:** Adding entropy regularisation mitigates variance introduced by the performative feedback loop.  
- **Mechanism:** The term \( \lambda \mathcal{H}(\pi_\theta) \) smooths the policy distribution, preventing premature collapse and reducing sensitivity to noisy gradient estimates from the shifted environment.  
- **Core assumption:** Î»â€¯>â€¯0 is sufficiently large to dominate stochastic fluctuations but not so large as to impede exploitation.  
- **Evidence anchors:** Ablation study (Tableâ€¯2) shows Î»â€¯=â€¯2 improves final reward by ~0.06 compared to Î»â€¯â‰¤â€¯1; convergence plots in Appendixâ€¯C illustrate reduced variance.  

## Foundational Learning  

| Concept | Why needed here | Quickâ€‘check question |
|---------|----------------|----------------------|
| Performative distribution shift | Captures the feedback loop where the policy changes the environment dynamics. | Does the environmentâ€™s transition kernel depend on the current policy parameters? |
| Policyâ€‘gradient theorem (REINFORCE) | Provides the base update rule that PePG extends with performative gradients. | Can you write the standard REINFORCE gradient for a softmax policy? |
| Entropy regularisation | Stabilises learning under the additional variance from the performative response. | How does increasing Î» affect the policyâ€™s action distribution? |
| Softmax parametrisation | Guarantees smooth, bounded gradients needed for the convergence proof. | What is the Jacobian of \(\log \pi_\theta(a|s)\) under softmax? |
| Stochastic approximation convergence | Underpins the theoretical guarantee that PePG reaches a performatively optimal fixed point. | Which Robbinsâ€‘Monro conditions are verified in Theoremâ€¯2? |
| Automatic differentiation through dynamics | Enables practical computation of \(\partial \mathcal{F}/\partial \theta\). | Which library feature is used to backâ€‘propagate through the environment model? |

## Architecture Onboarding  

- **Component map:** Policyâ€¯Ï€Î¸ â†’ Environment responseâ€¯ğ”½(Ï€Î¸) â†’ Performative valueâ€¯Ä¤VÏ€Ï€ â†’ Gradient computation â†’ Parameter update Î¸â€²  
- **Critical path:** Compute gradient of the performative value â†’ backâ€‘propagate through ğ”½ â†’ apply entropyâ€‘regularised update.  
- **Design tradeoffs:**  
  - *Differentiability vs realism*: Assuming a smooth ğ”½ enables gradients but may not hold for all realâ€‘world systems.  
  - *Entropy strength*: Larger Î» improves stability but can slow convergence to highâ€‘reward deterministic policies.  
  - *Learning rate*: Higher Î· accelerates value estimation but raises variance and risk of divergence.  
- **Failure signatures:**  
  - Diverging loss or exploding gradients when ğ”½ is nonâ€‘smooth.  
  - Policy collapse to a single action despite high Î» (overâ€‘regularisation).  
  - High variance in returns across seeds when Î· is too large.  
- **First 3 experiments:**  
  1. **Benchmark GridWorld with performative shift** â€“ compare PePG against standard REINFORCE and prior performativeâ€‘RL baselines.  
  2. **Entropy ablation** â€“ sweep Î» âˆˆ {0,â€¯0.5,â€¯1,â€¯2} and report final reward and variance.  
  3. **Learningâ€‘rate sweep** â€“ evaluate Î· âˆˆ {0.1,â€¯0.3,â€¯0.5,â€¯0.7} focusing on valueâ€‘estimate accuracy and stability.  

## Open Questions the Paper Calls Out  
1. **Nonâ€‘differentiable performative dynamics:** How can PePG be extended to settings where the environmentâ€™s response to the policy is piecewise or stochastic and lacks a tractable gradient?  
2. **Multiâ€‘agent performative RL:** What are the convergence properties when multiple agents simultaneously induce performative shifts on a shared environment?  
3. **Sampleâ€‘efficient estimation of the performative value:** Can modelâ€‘based or offâ€‘policy techniques reduce the number of environment interactions needed to estimate \( \hat V_{\pi}^{\pi} \)?  
4. **Theoretical rates of convergence:** Beyond asymptotic optimality, what finiteâ€‘time bounds can be established for PePG under realistic noise conditions?  
5. **Scalability to highâ€‘dimensional continuous control:** Does the softmaxâ€‘based approach generalise to continuous action spaces, perhaps via Gaussian policies, and what modifications are required?  

## Limitations  
- Relies on a **differentiable** model of the environmentâ€™s performative response, which may not exist in many realâ€‘world systems.  
- Empirical evaluation is limited to a **small set of benchmark tasks**, raising questions about generalisability.  
- **Hyperâ€‘parameter sensitivity**: performance varies markedly with entropy weight Î» and learning rate Î·.  

## Confidence  
- **Convergence to performatively optimal policies â†’ Medium** (proof sketch provided, but full assumptions not verified here).  
- **Empirical superiority over baselines â†’ Low** (singleâ€‘benchmark evidence, no statistical tests reported).  
- **Impact of strong entropy regularisation and large learning rates â†’ Low** (ablation numbers given without variance analysis).  

## Next Checks  
1. **Proof audit:** Obtain the full manuscript, extract Theoremâ€¯2 and Lemmaâ€¯1, and verify that all smoothness and boundedness assumptions hold for the stated environment model.  
2. **Reâ€‘implementation & replication:** Code PePG following the pseudocode, run â‰¥â€¯10 random seeds on the reported benchmarks, and compute confidence intervals for the performance gains versus baselines.  
3. **Robustness stress test:** Replace the differentiable environment map ğ”½ with a nonâ€‘smooth, stochastic shift (e.g., piecewise linear) and measure whether the algorithm still converges or collapses, thereby probing the limits of the differentiability assumption.