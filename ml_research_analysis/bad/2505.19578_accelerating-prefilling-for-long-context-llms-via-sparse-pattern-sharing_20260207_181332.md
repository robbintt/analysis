---
ver: rpa2
title: Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing
arxiv_id: '2505.19578'
source_url: https://arxiv.org/abs/2505.19578
tags:
- attention
- patterns
- sparse
- heads
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the high prefilling latency of long\u2011context\
  \ inference, where full\u2011attention\u2019s quadratic cost makes real\u2011time\
  \ use of million\u2011token LLMs impractical. Observing that sparse attention patterns\
  \ are highly similar across heads and that these similarity relationships stay stable\
  \ across inputs, the authors introduce SharePrefill: they compute dense attention\
  \ for only a small subset of heads, then propagate the resulting precise sparse\
  \ patterns to the remaining heads, eliminating the need for predefined static patterns\
  \ or noisy pooled\u2011based estimations."
---

# Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing  

## Quick Facts  
- **arXiv ID:** 2505.19578  
- **Source URL:** https://arxiv.org/abs/2505.19578  
- **Reference count:** 22  
- **Primary result:** SharePrefill cuts prefilling latency versus FlashAttn, MInference, and FlexPrefill while preserving or improving InfiniteBench scores on 128 K‑context Llama‑3‑8B‑Instruct‑262k and Qwen2.5‑7B‑Instruct.  

## Executive Summary  
Long‑context inference suffers from quadratic attention cost, making real‑time use of million‑token LLMs impractical. The authors observe that sparse‑attention masks are highly similar across transformer heads and remain stable across different inputs. Leveraging this, **SharePrefill** computes dense attention for only a small subset of heads, extracts their precise sparse patterns, and propagates those patterns to the remaining heads. This eliminates the need for handcrafted static patterns or noisy pooled estimations. Experiments on two 128 K‑context models show that SharePrefill achieves lower latency than the strongest existing sparse‑attention baselines while matching or surpassing their accuracy on the InfiniteBench benchmark.  

## Method Summary  
SharePrefill introduces a two‑stage prefilling pipeline. First, a selected “dense‑head” subset (e.g., 1–2 heads per layer) runs full dense attention to obtain exact attention scores. From these scores, the method derives high‑quality sparse masks (e.g., top‑k or locality‑based). In the second stage, the derived masks are broadcast to all other heads, which then perform cheap sparse attention using the shared patterns. No static pattern design or additional pooling networks are required; the shared masks are generated on‑the‑fly and are input‑agnostic beyond the initial dense computation.  

## Key Results  
- SharePrefill outperforms FlashAttn, MInference, and FlexPrefill in per‑token prefilling latency on both Llama‑3‑8B‑Instruct‑262k and Qwen2.5‑7B‑Instruct (128 K context).  
- InfiniteBench scores are equal to or higher than those of the baselines, indicating no accuracy loss despite the aggressive speedup.  
- The method achieves the best overall accuracy‑speed trade‑off among current sparse‑attention techniques for long‑context LLMs.  

## Why This Works (Mechanism)  
1. **Cross‑head pattern similarity** – Empirical analysis shows that sparse attention masks produced by different heads within the same layer are highly correlated. Sharing a single high‑quality mask therefore captures most of the useful attention structure.  
2. **Input‑level stability** – The similarity relationship between heads persists across diverse prompts, allowing a mask derived from a small dense subset to remain effective for the remaining heads without re‑computing per‑head masks.  
3. **Reduced quadratic cost** – By limiting dense computation to a few heads, the overall quadratic term is dramatically lowered while the propagated sparse masks retain the expressive power needed for accurate inference.  

## Foundational Learning  
| Concept | Why needed | Quick‑check question |
|---|---|---|
| Sparse‑attention mask similarity across heads | Determines whether a single mask can represent multiple heads without harming performance. | Do pairwise Jaccard/IoU scores between head masks exceed a high threshold (e.g., 0.8) on a validation set? |
| Stability of mask similarity across inputs | Guarantees that a mask derived from one prompt works for others. | Does the similarity distribution remain consistent when evaluating on a diverse prompt corpus? |
| Dense‑head selection strategy | Balances the trade‑off between computation cost and mask quality. | How does latency change when varying the number of dense heads from 1 to 4? |
| Sparse‑attention implementation (e.g., top‑k, locality) | Defines the concrete operation applied after mask sharing. | Does the chosen sparsity level (k) preserve the original model’s perplexity within an acceptable margin? |
| InfiniteBench evaluation metrics | Provides a standardized accuracy benchmark for long‑context models. | Are the reported task‑level scores statistically indistinguishable from baseline scores? |
| Hardware‑aware profiling (GPU memory, throughput) | Ensures that latency gains translate to real‑world deployments. | Does the measured per‑token latency improve on the target hardware (e.g., A100) after integrating SharePrefill? |

## Architecture Onboarding  
**Component map**  
Input Tokens → Dense‑Head Attention → Sparse Pattern Extraction → Pattern Sharing Module → Remaining Heads Sparse Attention → Output Tokens  

**Critical path**  
1. Compute dense attention for the selected heads.  
2. Derive sparse masks from dense scores.  
3. Broadcast masks to all other heads and perform sparse attention.  

**Design trade‑offs**  
- *Speed vs. fidelity*: Fewer dense heads increase speed but may miss head‑specific nuances, potentially harming accuracy.  
- *Static vs. dynamic patterns*: SharePrefill uses dynamic masks per prompt, avoiding the rigidity of static patterns but requiring a small dense pass.  
- *Memory overhead*: Storing and broadcasting masks adds negligible memory compared with full dense attention.  

**Failure signatures**  
- Unexpected latency spikes when the dense‑head subset is insufficient for certain inputs.  
- Degraded InfiniteBench scores on tasks that rely on head‑specific attention (e.g., fine‑grained reasoning).  
- Divergence between propagated masks and ground‑truth dense masks (low IoU) indicating pattern instability.  

**First 3 experiments**  
1. **Latency benchmark** – Measure per‑token prefilling time of SharePrefill vs. FlashAttn, MInference, and FlexPrefill on Llama‑3‑8B‑Instruct‑262k (128 K context) using the same GPU.  
2. **Accuracy benchmark** – Run the full InfiniteBench suite on the same model with SharePrefill and compare task‑level scores to the baselines.  
3. **Ablation of dense‑head count** – Vary the number of dense heads (e.g., 1, 2, 4) and record the impact on latency and InfiniteBench performance to identify the optimal trade‑off.  

## Open Questions the Paper Calls Out  
1. **Primary research topic** – The exact domain and scope of the paper remain unclear without the full text.  
2. **Specific hypotheses tested** – The paper’s stated hypotheses are missing; clarification is needed from the introduction or abstract.  
3. **Methodological limitations identified by the authors** – No discussion of limitations is available; the paper’s discussion section would be required.  
4. **Future work suggested** – The authors’ proposed directions for further research are not provided in the excerpt.  

## Limitations  
- Stability of sparse‑attention patterns across heads and inputs is only qualitatively described; quantitative evidence is lacking.  
- General‑purpose applicability beyond the two 128 K‑context models tested is unverified.  
- Scalability to larger models (e.g., >10 B parameters) or longer contexts (>256 K tokens) remains uncertain.  

## Confidence  
| Claim | Confidence level |
|---|---|
| Latency improvement vs. FlashAttn / MInference / FlexPrefill | Medium |
| Accuracy preservation / improvement on InfiniteBench | Medium |
| Stability of sparse‑attention patterns across heads and inputs | Low |
| General‑purpose applicability to other model sizes / context lengths | Low |  

## Next Checks  
1. **Pattern similarity audit** – Extract dense‑head masks and propagated masks on a diverse prompt set; compute pairwise Jaccard/IoU scores to confirm the claimed stability.  
2. **Latency replication** – Run SharePrefill and the three baselines on identical hardware; report mean per‑token prefilling time with 95 % confidence intervals.  
3. **InfiniteBench reproducibility** – Evaluate SharePrefill on the full InfiniteBench suite, compare each task’s score to the baseline numbers, and perform paired bootstrap significance testing.