---
ver: rpa2
title: Greedy Sampling Is Provably Efficient for RLHF
arxiv_id: '2510.24700'
source_url: https://arxiv.org/abs/2510.24700
tags:
- preference
- policy
- rlhf
- general
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles KL\u2011regularized contextual bandits for RLHF,\
  \ where learning must rely solely on pairwise human preference feedback rather than\
  \ explicit rewards, and seeks guarantees for the general preference model (and its\
  \ BT special case). The authors propose a simple greedy\u2011sampling algorithm:\
  \ at each round the learner draws one action from its current policy and a second\
  \ from the fixed reference policy, observes the binary preference, updates a maximum\u2011\
  likelihood estimate of the preference (or reward) function within a finite function\
  \ class, and directly sets the next policy to the exact KL\u2011regularized optimum\
  \ for this empirical model\u2014no optimism or pessimism bonuses are added."
---

# Greedy Sampling Is Provably Efficient for RLHF

## Quick Facts
- **arXiv ID:** 2510.24700  
- **Source URL:** https://arxiv.org/abs/2510.24700  
- **Reference count:** 40  
- **Primary result:** Greedyâ€‘sampling with KLâ€‘regularized updates attainsâ€¯O(logâ€¯T) regret (online) andâ€¯O(Îµâ»Â¹) sample complexity (offline) for RLHF without optimism/pessimism bonuses.

## Executive Summary
The paper studies KLâ€‘regularized contextual bandits for RLHF, where only pairwise human preference feedback is available. By repeatedly sampling one action from the current policy and another from a fixed reference policy, updating a maximumâ€‘likelihood estimate of the preference model, and then setting the next policy to the exact KLâ€‘regularized optimum for this empirical model, the authors obtain logarithmic regret in the online setting and linearâ€‘inâ€‘Îµâ»Â¹ offline sample complexity. Crucially, no exploration bonuses are required; the bounded densityâ€‘ratio induced by the KL penalty and the dualâ€‘sampling scheme provide sufficient exploration under realizability.

## Method Summary
The algorithm proceeds in rounds. At round *t* it draws an action *aâ‚â‚œ* from the learned policyâ€¯Ï€Ì‚â‚œ and a second action *aâ‚‚â‚œ* from a fixed reference policyâ€¯Ï€â‚€, observes a binary preference label *yâ‚œ* (whether *aâ‚â‚œ* is preferred over *aâ‚‚â‚œ*), and updates a maximumâ€‘likelihood estimate of the underlying preference (or reward) function within a finite, realizable function classâ€¯ğ’« (or Bradleyâ€‘Terry classâ€¯â„›). The next policy is computed greedily as the exact KLâ€‘regularized optimum for the empirical model:  

Ï€Ì‚â‚œâ‚Šâ‚(a|x)â€¯âˆâ€¯Ï€â‚€(a|x)Â·exp(Î·Â·fÌ‚(x,a))  

where *fÌ‚* is the MLE estimate. For the general preference model the update requires the Nashâ€‘equilibrium policy associated with the estimated preference; for the Bradleyâ€‘Terry case a closedâ€‘form solution exists. No optimism or pessimism bonuses are added.

## Key Results
- **Online regret:**â€¯Regretâ€¯=â€¯O(exp(Î·)Â·d_GPÂ·log(N_Pâ€¯T/Î´)). When the Eluder dimension *d_GP* is constant, this reduces to O(logâ€¯T), improving prior O(âˆšT) bounds.  
- **Offline sample complexity:**â€¯An Îµâ€‘optimal policy is obtained with O(Îµâ»Â¹) preference samples, a factorâ€‘âˆšÎµ improvement over classic O(Îµâ»Â²) results.  
- **Bonusâ€‘free guarantee:**â€¯The bounded densityâ€‘ratio between any KLâ€‘regularized optimal policy and the reference policy eliminates the need for explicit exploration bonuses.

## Why This Works (Mechanism)

**Mechanismâ€¯1 â€“ Bounded density ratio**  
- *Claim:* Greedy MLE updates remain stable because any policy in the KLâ€‘regularized optimal class satisfies Ï€_f(a|x)/Ï€â‚€(a|x)â€¯âˆˆâ€¯[exp(âˆ’Î·),â€¯exp(Î·)].  
- *Evidence:* Lemmaâ€¯1 proves the ratio bound; the KL penalty forces all optimal policies to share the support of Ï€â‚€ and to stay stochastic.  
- *Break condition:* If Ï€â‚€ lacks coverage of highâ€‘reward actions or Î· is set very large, the ratio bound may be violated and greedy updates can diverge.

**Mechanismâ€¯2 â€“ Dualâ€‘policy sampling provides exploration**  
- *Claim:* Sampling from both the learned policy and the reference policy yields sufficient exploration under realizability, removing the need for explicit uncertainty bonuses.  
- *Evidence:* Assumptionâ€¯1 (finite, realizable class) together with Algorithmâ€¯1â€™s sampling scheme; MLE on paired comparisons implicitly regularizes via the finite class.  
- *Break condition:* Misspecification (P*â€¯âˆ‰â€¯ğ’«) or a reference policy with gaps in support can cause the learner to miss optimal actions.

**Mechanismâ€¯3 â€“ Eluder dimension controls regret**  
- *Claim:* The Eluder dimension *d_GP* of the preference function class quantifies how quickly new samples reduce uncertainty, directly scaling the regret bound.  
- *Evidence:* Theoremâ€¯1 links regret to d_GPÂ·log(N_Pâ€¯T/Î´); Definitionâ€¯3 formalizes the uncertainty measure used in the Eluder dimension.  
- *Break condition:* Highly expressive models (e.g., deep nets) may have large d_GP, leading to regret that grows faster than logarithmic despite the algorithmâ€™s simplicity.

## Foundational Learning
| Concept | Why needed | Quick check |
|--------|------------|-------------|
| KLâ€‘regularized contextual bandits | The KL penalty reshapes the optimal policy set, enabling bounded density ratios and stochastic optimal policies. | Explain why adding a KL penalty changes optimal policies from deterministic to stochastic. |
| Maximumâ€‘likelihood estimation for preference models | The algorithmâ€™s update relies on fitting a preference (or reward) function to paired comparison data. | Write the MLE objective for the Bradleyâ€‘Terry model given binary preferences. |
| Eluder dimension as a complexity measure | Regret scales with d_GP; understanding its magnitude for a given model class is essential for interpreting the bound. | For a linear preference model P(x,aâ‚,aâ‚‚)=Î¸áµ€Ï†(x,aâ‚,aâ‚‚), would you expect low or high Eluder dimension? |
| Nash equilibrium policy for general preferences | Greedy policy update requires the exact KLâ€‘optimal distribution, which is the Nash equilibrium of a zeroâ€‘sum game defined by the estimated preference. | Describe how a Nash equilibrium relates to the KLâ€‘regularized optimal policy in this setting. |
| Realizability & finite function class | Guarantees assume the true preference lies within the candidate class; this underpins the concentration arguments. | What does â€œrealizableâ€ mean for the preference class ğ’«? |

## Architecture Onboarding
**Component map**  
Context *x* â†’ Policy Sampler (aâ‚â€¯âˆ¼â€¯Ï€Ì‚â‚œ, aâ‚‚â€¯âˆ¼â€¯Ï€â‚€) â†’ Preference label *y* â†’ MLE Module (estimate PÌ‚ or RÌ‚) â†’ Greedy Policy Update (Ï€Ì‚â‚œâ‚Šâ‚ âˆ Ï€â‚€Â·exp(Î·Â·fÌ‚))

**Critical path**
1. Implement pairedâ€‘action sampling from both learned and reference policies.  
2. Build an MLE solver for the chosen preference model (convex for BT, possibly nonâ€‘convex for general ğ’«).  
3. Compute the greedy KLâ€‘regularized policy: closedâ€‘form for BT, Nashâ€‘equilibrium solver for general preferences.  

**Design tradeoffs**
- *Finite vs. infinite function class*: Finite ğ’« gives clean theory but limits expressivity; infinite classes need coveringâ€‘number extensions not proved.  
- *BT vs. general preference*: BT offers analytic policy updates; general ğ’« requires solving a Nash equilibrium, which can be computationally heavy.  
- *Reference policy choice*: Ï€â‚€ must have broad support; a poorly covering Ï€â‚€ harms exploration despite the dualâ€‘sampling scheme.  

**Failure signatures**
- Regret plateaus â†’ possible misspecification (P*â€¯âˆ‰â€¯ğ’«) or insufficient Ï€â‚€ coverage.  
- Policy ratios exceed exp(Â±Î·) â†’ KL weight Î· too large or implementation bug in densityâ€‘ratio computation.  
- MLE loss diverges â†’ optimization issues, inconsistent preference labels, or overly expressive model causing overâ€‘fit.  

**First 3 experiments**
1. **Synthetic realizable test** â€“ Construct a finite preference class with known P*, run the online algorithm, and verify that cumulative regret follows O(logâ€¯T) as predicted for varying Î· and d_GP.  
2. **Referenceâ€‘policy coverage ablation** â€“ Systematically prune Ï€â‚€â€™s support over highâ€‘preference actions and measure the point at which regret stops decreasing, testing Mechanismâ€¯2â€™s break condition.  
3. **Bonusâ€‘free vs. optimistic baselines** â€“ Compare the greedyâ€‘sampling algorithm against prior optimismâ€‘based RLHF methods on the same synthetic task, evaluating regret curves and computational overhead.

## Open Questions the Paper Calls Out
1. **Extension to infinite function classes** â€“ Can the greedyâ€‘sampling guarantees be extended to infinite classes (e.g., neural networks) with rigorous coveringâ€‘number bounds? The paper hints at a possible relaxation but provides no proof. Evidence would require a theorem linking regret to logâ€‘covering numbers and empirical validation on neural preference models.  
2. **Eluder dimension of practical preference models** â€“ What is the Eluder dimension of transformerâ€‘based preference heads used in modern LLM fineâ€‘tuning? The O(logâ€¯T) claim assumes a constant d_GP, yet this quantity is unknown for realistic architectures. Resolving this needs either theoretical bounds for attentionâ€‘based functions or empirical estimation via disagreement probing.  
3. **Efficient Nashâ€‘equilibrium computation** â€“ How can the NE policy be computed efficiently for the general preference model in practice? The analysis assumes exact computation, but finding Nash equilibria is generally PPADâ€‘hard. A practical approximate solver with convergence guarantees would close this gap.

## Limitations
- No concrete algorithm for computing the Nash equilibrium required by the general preference update.  
- Theory relies on a finite, realizable function class; extending to expressive neural models remains unproven.  
- Guarantees assume the reference policy has full support; poor Ï€â‚€ coverage can invalidate the boundedâ€‘ratio property.

## Confidence
| Claim | Confidence |
|-------|------------|
| Bounded densityâ€‘ratio â‡’ bonusâ€‘free learning | Medium |
| Logarithmic regret (Theoremâ€¯1) | Low |
| Offline O(Îµâ»Â¹) sample complexity | Medium |

## Next Checks
1. **Implement a Nashâ€‘equilibrium solver** for a small finite preference class and verify that the greedy policy update matches the theoretical KLâ€‘optimal distribution.  
2. **Empirically estimate the Eluder dimension** for a linearâ€‘feature Bradleyâ€‘Terry model and a shallow neuralâ€‘net preference model by tracking disagreement growth; compare observed regret scaling to the O(logâ€¯T) prediction.  
3. **Ablate referenceâ€‘policy support** by progressively removing actions from Ï€â‚€, monitor the densityâ€‘ratio bound and regret trajectory to identify the threshold where the algorithmâ€™s guarantees break down.