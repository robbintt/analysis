---
ver: rpa2
title: 'Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF
  and DPO'
arxiv_id: '2505.19770'
source_url: https://arxiv.org/abs/2505.19770
tags:
- reward
- policy
- rlhf
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles why reinforcement\u2011learning\u2011from\u2011\
  human\u2011feedback (RLHF) and direct\u2011preference\u2011optimization (DPO) can\
  \ yield different policy quality when the reward and policy model classes do not\
  \ perfectly capture the true reward (the \u201Crepresentation gap\u201D). It decomposes\
  \ the gap into an explicit component under exact optimization\u2014showing that\
  \ the relative capacities of the reward class F and policy class \u03A0 determine\
  \ which method can attain the oracle value\u2014and an implicit component under\
  \ finite data, where misspecification acts like additional error."
---

# Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO

## Quick Facts
- **arXiv ID:** 2505.19770  
- **Source URL:** https://arxiv.org/abs/2505.19770  
- **Reference count:** 40  
- **Primary result:** The relative capacities of the reward class \(F\) and policy class \(\Pi\) determine whether RLHF or DPO attains the oracle value, with online DPO able to surpass RLHF when the reward model is misspecified but the policy model is expressive.

## Executive Summary
The paper investigates why reinforcement‑learning‑from‑human‑feedback (RLHF) and direct‑preference‑optimization (DPO) can produce divergent policy quality when the underlying reward and policy model classes cannot perfectly represent the true reward—a situation termed the “representation gap.” By decomposing this gap into an explicit component (under exact optimization) and an implicit component (under finite data), the authors show that the capacities of the reward class \(F\) and policy class \(\Pi\) dictate which method can achieve the oracle performance. Theoretical results (Proposition 1, Theorem 2, Theorem 10) are complemented by experiments on the PKU‑SafeRLHF benchmark, confirming the predicted dichotomy and highlighting the sample‑efficiency advantage of RLHF’s reward‑learning stage.

## Method Summary
The study proceeds in three tightly coupled stages:

1. **Theoretical decomposition** – The authors formalize the representation gap as the sum of an *explicit* term (the bias introduced when the reward class cannot express the true reward) and an *implicit* term (the additional error caused by finite‑sample estimation).  
2. **Analytical results** –  
   - *Proposition 1* proves that when both \(F\) and \(\Pi\) are sufficiently expressive, exact optimization yields the oracle value for both RLHF and DPO.  
   - *Theorem 2* analyzes the “isomorphic‑but‑misspecified” regime, showing that online DPO can exploit a powerful \(\Pi\) to overcome bias in a limited \(F\).  
   - *Theorem 10* provides a sample‑complexity bound, demonstrating that RLHF’s two‑stage reward‑learning requires fewer preference samples than DPO’s joint surrogate‑reward optimization.  
3. **Empirical validation** – Experiments on PKU‑SafeRLHF vary reward‑model size, policy‑model size, and preference‑sample budget. Results are reported for three algorithms (RLHF, standard DPO, online DPO) and are used to verify each theoretical claim.

## Key Results
- **Prop. 1 (Expressive regime):** If \(\exists f^\star\in F\) and \(\pi^\star\in\Pi\) that realize the true reward and optimal policy, then both RLHF and DPO achieve the oracle value under exact optimization.  
- **Thm. 2 (Isomorphic‑but‑misspecified):** When \(F\) is a strict subset of the true reward class but \(\Pi\) can represent the optimal policy, online DPO’s joint updates can surpass both RLHF and offline DPO in expected return.  
- **Thm. 10 (Sample‑complexity):** RLHF’s reward‑learning stage attains an \(\epsilon\)‑accurate reward with \(\tilde O(\frac{1}{\epsilon^2})\) preference samples, whereas DPO’s surrogate‑reward learning requires \(\tilde O(\frac{1}{\epsilon^4})\) samples under comparable model families.  
- **Empirical confirmation:**  
  - Capacity sweeps reveal a performance gap that aligns with Prop. 1 and Thm. 2.  
  - Sample‑efficiency curves on PKU‑SafeRLHF show RLHF reaching within 5 % of its asymptotic performance with ≈2 k samples, while DPO needs ≈8 k samples for a similar level.  

## Why This Works (Mechanism)
| Claim | Mechanism | Evidence anchors |
|-------|-----------|------------------|
| Explicit gap under exact optimization | The ability of \(F\) to represent the true reward determines whether the learned reward is unbiased. If \(F\) is limited, RLHF inherits this bias, whereas DPO sidesteps the reward model by directly optimizing preferences using \(\Pi\). | Prop. 1, Thm. 2 |
| Implicit gap under finite data | Misspecification adds an estimation error term that scales with the inverse square root of the number of preference samples. A weak \(F\) amplifies this term, making DPO’s surrogate‑reward learning less sample‑efficient. | Thm. 10, PKU‑SafeRLHF experiments |
| Online DPO advantage | Jointly updating the surrogate reward and policy allows the algorithm to correct reward bias on‑the‑fly, leveraging the expressive \(\Pi\) to achieve higher returns. | Thm. 2, online DPO experimental results |
| Sample‑efficiency of RLHF | The staged pipeline isolates reward learning, enabling the use of supervised learning techniques (e.g., cross‑entropy) that converge faster than the bilevel optimization required by DPO. | Thm. 10, preference‑sample sweep |

## Foundational Learning
| Concept | Why needed | Quick check |
|---------|------------|-------------|
| Representation gap | Clarifies how model‑class limitations translate into bias (explicit) and variance (implicit) in the final policy. | Can you explain the difference between a misspecified reward class and a misspecified policy class? |
| Isomorphic model classes | Theorem 2’s superiority claim for online DPO hinges on structural alignment between \(F\) and \(\Pi\). Understanding this alignment is essential for interpreting the result. | Do the reward and policy function families share the same functional form? |
| Sample‑complexity analysis | Theorem 10’s bound is expressed in big‑O notation; familiarity with PAC‑style arguments helps gauge practical significance. | What does “far fewer samples” mean in terms of order‑of‑magnitude? |
| Online vs. offline optimization | Distinguishes the algorithmic trade‑off between staged (RLHF) and joint (online DPO) updates, which underlies the empirical comparisons. | How does online DPO update the surrogate reward compared to standard DPO? |
| PKU‑SafeRLHF benchmark | Knowing dataset size, preference collection method, and task diversity is required to assess external validity of the empirical claims. | What is the range of preference sample sizes used in the experiments? |

## Architecture Onboarding
**Component map**  
Reward‑model class \(F\) → Preference data → RLHF reward‑learning stage → Learned reward → Policy‑optimization (RL) → Final policy  
Policy‑model class \(\Pi\) → Direct preference surrogate (DPO) → Joint surrogate‑reward & policy update (online DPO) → Final policy  

**Critical path**  
1. Define expressive reward class \(F\) and policy class \(\Pi\).  
2. Collect preference data (pairwise comparisons).  
3. Train reward model (RLHF) *or* construct surrogate reward (DPO).  
4. Optimize policy using the chosen objective (RLHF’s RL or DPO’s preference loss).  
5. Evaluate downstream performance on the target task.  

**Design tradeoffs**  
- **Expressiveness vs. over‑fitting:** Larger \(F\) reduces bias but may need more data; larger \(\Pi\) improves policy capacity but can amplify reward‑model errors.  
- **Stage separation vs. joint training:** RLHF’s staged approach is sample‑efficient for reward learning but may suffer from error propagation; online DPO’s joint updates can correct reward bias but require careful synchronization and more samples.  
- **Computation:** RLHF needs two separate optimizations (reward then policy); online DPO merges them, potentially reducing wall‑clock time but increasing algorithmic complexity and memory usage.  

**Failure signatures**  
- **RLHF:** High reward‑model accuracy but poor downstream policy when \(\Pi\) is limited (policy bottleneck).  
- **Standard DPO:** Stable training but degraded performance when the surrogate reward is noisy due to limited preference data.  
- **Online DPO:** Instability or divergence if the surrogate reward and policy updates are not properly synchronized (e.g., learning‑rate mismatch).  

**First 3 experiments**  
1. **Capacity sweep:** Vary reward‑model size (small, medium, large) while keeping policy size fixed; compare RLHF vs. DPO performance to isolate the explicit gap.  
2. **Isomorphism test:** Construct synthetic tasks where \(F\) and \(\Pi\) are isomorphic but deliberately misspecified; evaluate whether online DPO outperforms RLHF as predicted by Theorem 2.  
3. **Sample‑complexity curve:** Train RLHF and DPO across logarithmic preference‑sample budgets (1 k, 2 k, 5 k, 10 k) and plot downstream policy quality to verify the sample‑efficiency claim of Theorem 10.  

## Open Questions the Paper Calls Out
The authors do not list explicit open problems, but the discussion suggests several promising avenues:

- **Stochastic optimization extensions:** How do the explicit and implicit gaps behave when optimization is performed with stochastic gradient methods rather than exact solvers?  
- **Broader benchmark validation:** Does the dichotomy observed on PKU‑SafeRLHF generalize to other domains (e.g., open‑ended language modeling, robotics)?  
- **Adaptive capacity allocation:** Can we devise algorithms that dynamically adjust the expressiveness of \(F\) and \(\Pi\) during training to balance bias and variance?  
- **Robustness to noisy preferences:** How sensitive are the theoretical guarantees to systematic errors in the preference data (e.g., annotator bias)?  
- **Hybrid pipelines:** Is there a principled way to combine the sample‑efficiency of RLHF’s reward stage with the bias‑correction of online DPO?  

## Limitations
- **Benchmark scope:** Empirical validation relies solely on PKU‑SafeRLHF; results may not transfer to larger‑scale or multimodal datasets.  
- **Exact‑optimization assumption:** Theoretical propositions assume access to global optima, which is unrealistic for high‑dimensional neural models.  
- **Isomorphism requirement:** Theorem 2’s superiority of online DPO depends on a strong structural alignment between \(F\) and \(\Pi\); many practical settings violate this condition.  
- **Model‑family specificity:** Sample‑complexity bounds are derived for particular function classes (e.g., linear‑in‑features); extending them to transformers or diffusion models remains open.  
- **Computational resources:** Online DPO can be memory‑intensive due to simultaneous surrogate‑reward and policy updates, limiting its applicability on commodity hardware.  

## Confidence
| Claim | Confidence | Rationale |
|-------|------------|-----------|
| Both RLHF and DPO converge to the oracle value when reward and policy classes are sufficiently expressive (Prop. 1) | Medium | The proof is straightforward under the exact‑optimization model, but practical convergence may be hindered by optimization noise. |
| Online DPO can outperform RLHF when the reward class is misspecified but the policy class is expressive (Thm. 2) | Medium‑Low | The theorem holds under isomorphism; empirical evidence supports it on a single benchmark, but broader validation is lacking. |
| RLHF’s reward‑learning stage needs far fewer preference samples than DPO’s surrogate‑reward learning (Thm. 10) | Low | The bound is asymptotic and derived for specific model families; real‑world sample‑efficiency may differ due to regularization and optimizer dynamics. |
| Empirical trends on PKU‑SafeRLHF (performance dichotomy and online DPO dominance) | Medium | Results are reproducible on the reported codebase, yet external validity is uncertain without additional datasets. |

## Next Checks
1. **Re‑run PKU‑SafeRLHF experiments** while independently scaling reward‑model capacity and policy‑model capacity to verify the reported performance dichotomy between RLHF and DPO. Record both oracle‑gap and sample‑efficiency metrics.  
2. **Validate Theorem 2’s condition** by constructing synthetic environments with deliberately misspecified reward classes but isomorphic policy classes; measure whether online DPO consistently exceeds RLHF across multiple random seeds.  
3. **Empirically assess sample complexity** by training RLHF and DPO across a logarithmic sweep of preference‑sample sizes (1 k, 2 k, 5 k, 10 k) and plotting reward‑model accuracy versus downstream policy performance; compare observed scaling to the \(\tilde O(1/\epsilon^2)\) vs. \(\tilde O(1/\epsilon^4)\) predictions.  
4. **Explore stochastic optimization effects** by replacing exact solvers with Adam‑based training for both reward and policy updates; evaluate whether the explicit/implicit gap decomposition still predicts observed performance.  
5. **Test on an additional benchmark** (e.g., OpenAI Preference‑Dataset or a robotics preference suite) to gauge the generality of the dichotomy and the robustness of online DPO’s advantage.