---
ver: rpa2
title: 'PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization'
arxiv_id: '2511.00010'
source_url: https://arxiv.org/abs/2511.00010
tags:
- chart
- visualization
- plotcraft
- plot
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PlotCraft addresses the gap in evaluating and developing LLMs for
  complex data visualization by introducing a new benchmark of 1k challenging tasks,
  spanning seven high-level visualization intents and 48 chart types. The benchmark
  uniquely evaluates both single-turn generation and multi-turn refinement across
  tasks of varying complexity, from simple single-panel charts to intricate multi-panel
  grids.
---

# PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization

## Quick Facts
- arXiv ID: 2511.00010
- Source URL: https://arxiv.org/abs/2511.00010
- Reference count: 40
- Primary result: Specialized SFT on synthetic visualization data improves performance by 25% on challenging visualization code generation tasks

## Executive Summary
PlotCraft addresses the critical gap in evaluating and developing LLMs for complex data visualization by introducing a comprehensive benchmark of 1k challenging tasks. The benchmark uniquely evaluates both single-turn generation and multi-turn refinement across tasks of varying complexity, from simple single-panel charts to intricate multi-panel grids. A comprehensive evaluation of 23 leading LLMs reveals significant performance deficiencies on complex tasks, with most models failing on medium and hard tasks requiring multi-panel layouts and composite charts. To address this, the authors develop SynthVis-30K, a 30k-instance dataset synthesized via a multi-agent framework, and train PlotCraftor, a specialized code generation model that achieves strong performance improvements over its base model.

## Method Summary
The method centers on a multi-agent collaborative framework for synthesizing high-quality visualization training data. The pipeline begins with a Task Cycle where a Task Generator and Task Judge iterate until feasible tasks are produced. Code Generation then proceeds through parallel Debug Cycles (catching execution errors) and Refine Cycles (using visual quality feedback) until quality thresholds are met. The resulting SynthVis-30K dataset is used to fine-tune Qwen3-Coder-30B-A3B with 3 epochs, batch size 64, and learning rate 7e-6, producing the specialized PlotCraftor model. Evaluation uses both automated Gemini-2.5-Pro judging across 9 sub-metrics and human validation.

## Key Results
- Most leading LLMs fail on medium and hard tasks requiring multi-panel layouts and composite charts
- PlotCraftor improves performance on PlotCraft benchmark by 25% over its base model
- PlotCraftor achieves performance comparable to leading proprietary LLMs, particularly on hard tasks
- Single-turn and multi-turn evaluation reveal distinct capability gaps that simple benchmarks miss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A multi-agent collaborative framework with iterative feedback loops can synthesize high-quality visualization training data at scale.
- **Mechanism:** Specialized agents (Data Analyzer, Task Generator, Task Judge, Code Generator, Executor, Visual Judge) operate in structured cycles. The Task Cycle iterates between generation and judging until tasks are feasible. The Code Generation phase runs parallel Debug Cycles (catching execution errors) and Refine Cycles (using visual quality feedback) until quality thresholds are met.
- **Core assumption:** Decomposing the complex visualization task into specialized roles with explicit feedback loops yields higher-quality training data than single-model generation.
- **Evidence anchors:** [abstract] "SynthVis-30K, a 30k-instance dataset synthesized via a multi-agent framework"; [section 4.1] Describes complete pipeline with iterative cycles; [corpus] CoDA paper validates agentic approach for "collaborative data visualization"
- **Break condition:** If Visual Judge cannot reliably detect quality issues or if code never achieves Task Compliance 4/4 and Quality ≥6/10 within 10 iterations.

### Mechanism 2
- **Claim:** Supervised fine-tuning on domain-specific visualization code with explicit reasoning trajectories improves complex visualization capabilities beyond general code training.
- **Mechanism:** PlotCraftor is fine-tuned from Qwen3-Coder-30B-A3B on SynthVis-30K. Single-turn data includes Chain-of-Thought rationales explaining the path from instruction to code. Multi-turn data extracts successful refinement interactions where Visual Judge feedback led to quality improvements.
- **Core assumption:** Visualization code generation benefits from explicit reasoning about spatial layout, chart type selection, and data transformations—skills undertrained in general code models.
- **Evidence anchors:** [abstract] "PlotCraftor achieves strong performance, improving by 25% over its base model"; [section 4.2] Training config details; [section 5.2] "PlotCraftor improves performance on PlotCraft by 25%"
- **Break condition:** If base model lacks sufficient code generation capability to leverage domain training.

### Mechanism 3
- **Claim:** Evaluating both single-turn generation and multi-turn refinement reveals distinct capability gaps that simple benchmarks miss.
- **Mechanism:** PlotCraft benchmarks two task types from the same visualization goal. Single-turn measures zero-shot generation; multi-turn measures debugging and iterative improvement. This dual evaluation surfaced that models often produce high-quality but non-compliant outputs, and that refinement capability doesn't correlate with generation capability.
- **Core assumption:** Real-world visualization workflows involve iterative correction, so benchmarks must test both initial generation and refinement from error states.
- **Evidence anchors:** [abstract] "first to systematically evaluate both single-turn generation and multi-turn refinement"; [section 3.2] 491 single-turn + 491 multi-turn tasks; [section 5.2] Different models excel at different task types
- **Break condition:** If multi-turn evaluation data doesn't represent realistic error patterns.

## Foundational Learning

- **Concept: Matplotlib layout management and subplot composition**
  - **Why needed here:** Paper identifies "conflicting layout managers" as a critical failure mode; constrained_layout conflicts with fig.legend() cause blank outputs. Most hard tasks require multi-panel grids (2x2, 3x3).
  - **Quick check question:** Given a 3x3 subplot grid requirement where each subplot is itself composite (histogram + KDE + box plot), which Matplotlib layout approach would avoid conflicts with figure-level legends?

- **Concept: Visualization intent taxonomy**
  - **Why needed here:** PlotCraft organizes 48 chart types under 7 high-level intents (Correlation, Deviation, Ranking, Distribution, Composition, Change, Groups). Models fail when they select chart types that don't match the intent.
  - **Quick check question:** A prompt asks to "analyze deviation patterns in prediction errors"—which intent category applies, and what chart types would be appropriate?

- **Concept: LLM-as-Judge evaluation with multi-dimensional metrics**
  - **Why needed here:** Evaluation uses Gemini-2.5-Pro as automated judge across 9 sub-metrics (4 compliance binary, 5 quality 0-2 scale). Understanding why models fail specific metrics (e.g., Claude-4-Sonnet scores 0.39 on Clarity overlap detection) is essential for interpretation.
  - **Quick check question:** Why might a judge model struggle specifically with the "Clarity" (element overlap) metric more than "Chart Type Compliance"?

## Architecture Onboarding

- **Component map:**
  Kaggle Dataset → Data Analyzer (extract metadata) → Task Cycle: Task Generator ↔ Task Judge (iterate until feasible) → Code Generation: Planner → Code Generator → [Debug Cycle: Executor → error feedback] → [Refine Cycle: Visual Judge → quality feedback] → SFT Trajectory Synthesis: Add CoT rationales (single-turn) or extract refinement logs (multi-turn) → Training: Qwen3-Coder-30B-A3B → PlotCraftor (SFT on SynthVis-30K)

- **Critical path:** Data filtering → Task generation with intent mapping → Code generation with dual-loop refinement → Quality-gated data acceptance → SFT training. If any stage fails quality thresholds, the entire trajectory is discarded (Appendix E: "if criteria not met within 10 iterations, entire task-code pair discarded").

- **Design tradeoffs:**
  - 30B model size vs. hard task capability (paper: SFT helps easy tasks but "SFT provides minimal benefit for Hard tasks" without scale)
  - Multi-agent complexity vs. data quality (costly synthesis vs. 25% improvement)
  - Gemini-2.5-Pro as judge vs. human evaluation correlation (0.90 on Layout, but only 0.69 on Format)
  - Zero-reference generation vs. task ambiguity (no sample images forces reasoning but increases failure rate)

- **Failure signatures:**
  - `constrained_layout=True` + `fig.legend()` → blank canvas (Figure 31)
  - Non-white background with high-contrast gridlines → Format score 0
  - Correct chart type but diverging color scheme not using positive/negative distinction → Task Compliance failure
  - Uniform bubble sizes in bubble chart → Chart Type Compliance failure
  - X-axis label overlap in dense multi-panel layouts → Clarity score reduction

- **First 3 experiments:**
  1. **Baseline capability audit:** Run target model on PlotCraft benchmark split by difficulty (Easy/Medium/Hard) to identify specific failure modes (chart type, layout, clarity, or color quality).
  2. **Error pattern analysis:** Use the 3-category error taxonomy (Code-Level, Task Compliance, Chart Quality) to classify failures; if Code-Level dominates, debug code generation before fine-tuning.
  3. **Synthetic data quality validation:** Before full SFT, sample 500 generated instances and measure Visual Judge agreement with human evaluators using Cohen's Kappa; if agreement <0.7 on critical metrics, iterate on Visual Judge prompts before scaling synthesis.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can automated visual evaluation frameworks be improved to reliably detect subtle visual errors, such as minor element overlaps or suboptimal color styles? The authors state that current "MLLM-based judges may fail to detect subtle visual errors, like minor overlaps or color style."
- **Open Question 2:** Can high-quality visualization instruction-tuning data be synthesized efficiently without relying on expensive, iterative multi-agent frameworks? The conclusion identifies that a key limitation is that "The synthesis of high-quality data relies on costly multi-agent systems."
- **Open Question 3:** To what extent can task-specific fine-tuning compensate for the lack of "emergent reasoning" in smaller models when handling complex, multi-panel visualization tasks? The results indicate that while scaling helps Hard tasks, "SFT provides minimal benefit for Hard tasks" for smaller models.

## Limitations

- Automated visual evaluation frameworks struggle to detect subtle visual errors like minor overlaps or color style issues, limiting reliable quality assessment
- High-quality visualization instruction-tuning data synthesis requires expensive, iterative multi-agent systems that are computationally costly
- Performance on hard tasks plateaus for models below the 100B threshold, suggesting fundamental reasoning limitations that SFT cannot fully address

## Confidence

- **High confidence:** The benchmark construction methodology (1k tasks across 7 intents, 48 chart types, with difficulty scaling) is well-documented and reproducible. The error taxonomy (Code-Level, Task Compliance, Chart Quality) is empirically grounded.
- **Medium confidence:** The synthetic data generation pipeline description is complete, but specific agent prompts and quality thresholds are not fully specified. The SFT training configuration is detailed, but hardware requirements and parallel scaling strategies are missing.
- **Low confidence:** The selection criteria for high-quality multi-turn refinement trajectories are vague ("significant improvement" not quantified), and the exact format of training conversation data is unspecified.

## Next Checks

1. **Judge reliability validation:** Sample 100 PlotCraft benchmark instances and measure inter-judge agreement between Gemini-2.5-Pro and human evaluators on all 9 sub-metrics. If Format and Clarity metrics show <0.7 Cohen's Kappa, revise the automated judge protocol before relying on these scores.

2. **Error pattern replication:** Implement the 3-category error taxonomy on a subset of 50 failed instances from current best models. Verify that Code-Level errors (40.6%) truly dominate due to layout conflicts and that Task Compliance failures (30.2%) consistently stem from chart type mismatches.

3. **Synthetic data quality audit:** Generate a 500-instance validation set using the full SynthVis-30K pipeline and have human experts rate the quality of resulting visualizations against the stated thresholds (Task Compliance 4/4, Quality ≥6/10). If human ratings disagree with agent scores on >30% of instances, iterate on the Visual Judge prompts and thresholds before full-scale synthesis.