---
ver: rpa2
title: 'State over Tokens: Characterizing the Role of Reasoning Tokens'
arxiv_id: '2512.12777'
source_url: https://arxiv.org/abs/2512.12777
tags:
- tokens
- reasoning
- state
- arxiv
- computation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reframes the role of reasoning tokens in large language
  models (LLMs) not as human-readable explanations, but as externalized computational
  state that enables multi-cycle reasoning. The State over Tokens (SoT) framework
  challenges the common misconception that these tokens provide faithful explanations
  of model reasoning.
---

# State over Tokens: Characterizing the Role of Reasoning Tokens

## Quick Facts
- arXiv ID: 2512.12777
- Source URL: https://arxiv.org/abs/2512.12777
- Reference count: 14
- Key outcome: Reasoning tokens serve as externalized computational state enabling multi-cycle reasoning rather than faithful explanations

## Executive Summary
This paper challenges the conventional view that reasoning tokens in large language models provide faithful explanations of the model's reasoning process. Instead, it proposes the State over Tokens (SoT) framework, arguing that these tokens primarily function as an externalized computational state that enables multi-cycle reasoning. The framework identifies two key misconceptions: completeness (reasoning tokens show partial results rather than full computation) and shared meaning (human interpretations of tokens may differ fundamentally from how the model uses them). This ontological divergence suggests that the same tokens serve dual roles as both human-readable text and computational state, raising new questions about how LLMs encode and utilize reasoning state.

## Method Summary
The paper presents a theoretical framework for understanding reasoning tokens in LLMs, focusing on reframing their role rather than introducing new computational methods. The authors conduct empirical analyses to demonstrate that reasoning tokens often omit crucial details, can be semantically meaningless to humans, yet remain functionally critical for correct answers. Through ablation studies and analysis of token behavior, they show that reasoning tokens serve as externalized state rather than faithful explanations. The work involves systematic examination of how tokens function during reasoning tasks and their relationship to model performance.

## Key Results
- Reasoning tokens function as externalized computational state rather than faithful explanations
- Tokens often omit crucial reasoning details and can be semantically meaningless to humans
- Two key misconceptions identified: completeness (partial results) and shared meaning (divergent interpretations)

## Why This Works (Mechanism)
The State over Tokens framework works because it reframes reasoning tokens as computational state externalized in natural language form, rather than as explanations meant for human consumption. This perspective explains why tokens can be semantically meaningless yet functionally criticalâ€”they encode information in ways optimized for the model's internal computation rather than human understanding. The mechanism allows models to maintain and build upon intermediate results across multiple reasoning cycles, similar to how humans might use scratch paper. The ontological divergence between human-readable text and computational state is fundamental to how these models operate, with the same tokens serving both communicative and computational purposes.

## Foundational Learning

**Computational state representation**: Understanding how models maintain intermediate results across processing cycles. *Why needed*: Essential for grasping why reasoning tokens matter functionally even when semantically meaningless. *Quick check*: Can the model complete reasoning tasks when reasoning tokens are removed?

**Ontological divergence**: The concept that human and model interpretations of the same tokens can fundamentally differ. *Why needed*: Explains why faithful explanations may be unachievable under the SoT framework. *Quick check*: Do humans and models derive different meanings from the same reasoning tokens?

**Multi-cycle reasoning**: How models perform complex reasoning through multiple processing cycles. *Why needed*: Provides context for why externalized state is necessary for complex reasoning. *Quick check*: Does removing reasoning tokens break multi-step reasoning capabilities?

**Token ablation**: The practice of removing or modifying specific tokens to study their function. *Why needed*: Key experimental method for demonstrating functional importance of seemingly meaningless tokens. *Quick check*: What happens to reasoning performance when specific token types are removed?

## Architecture Onboarding

**Component map**: Input -> Reasoning tokens -> Computational state representation -> Answer generation -> Output

**Critical path**: Token generation -> State maintenance -> Answer synthesis -> Output

**Design tradeoffs**: Natural language tokens vs. alternative state representations (efficiency vs. interpretability)

**Failure signatures**: Performance degradation when reasoning tokens are removed or semantically altered

**3 first experiments**:
1. Ablate reasoning tokens in simple arithmetic reasoning tasks and measure performance drop
2. Replace reasoning tokens with semantically equivalent but structurally different text and test reasoning capability
3. Compare reasoning performance using natural language tokens versus numerical encodings of intermediate states

## Open Questions the Paper Calls Out

The paper raises several open questions about the SoT framework: How exactly do LLMs encode and use state through reasoning tokens? Is natural language truly special for reasoning state, or could other representations work equally well? Are faithful explanations achievable within this framework, or is the ontological divergence fundamental? What are the implications for interpretability and trust in AI systems if reasoning tokens serve primarily as computational state rather than explanations?

## Limitations

- The ontological divergence between human-readable text and computational state remains primarily theoretical
- The claim that natural language is not special for reasoning tokens lacks direct experimental validation with alternative representations
- Causal mechanisms linking token content to reasoning success remain underspecified despite functional importance being demonstrated

## Confidence

**Reasoning tokens primarily serve as externalized computational state**: Medium
**Human and model interpretations of tokens diverge fundamentally**: Medium
**Natural language tokens are not inherently special for reasoning state**: Low
**Faithful explanations are likely unachievable under SoT framework**: Medium

## Next Checks

1. Conduct controlled experiments comparing reasoning performance using natural language tokens versus alternative state representations (numerical encodings, structured data formats) while holding reasoning architecture constant.

2. Perform causal intervention studies where specific token sequences are replaced with semantically equivalent but structurally different text to test whether meaning or structure drives reasoning performance.

3. Implement interpretability methods that map token activation patterns directly to computational states in intermediate layers, distinguishing between tokens used for human consumption versus internal state management.