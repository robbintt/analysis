---
ver: rpa2
title: What Matters to an LLM? Behavioral and Computational Evidences from Summarization
arxiv_id: '2602.00459'
source_url: https://arxiv.org/abs/2602.00459
tags:
- importance
- samsum
- across
- summarization
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how Large Language Models (LLMs) prioritize
  information during summarization by combining behavioral and computational analyses.
  Behaviorally, the study generates length-controlled summaries and derives empirical
  importance distributions based on how often each token is selected.
---

# What Matters to an LLM? Behavioral and Computational Evidences from Summarization

## Quick Facts
- arXiv ID: 2602.00459
- Source URL: https://arxiv.org/abs/2602.00459
- Reference count: 40
- This work investigates how Large Language Models (LLMs) prioritize information during summarization by combining behavioral and computational analyses.

## Executive Summary
This study investigates how Large Language Models prioritize information during summarization through a dual approach: behavioral analysis of generated summaries and computational probing of internal representations. The researchers generate multiple length-controlled summaries per document and derive empirical importance distributions based on token selection frequency. These distributions reveal consistent patterns across model scales that differ from pre-LLM baselines and cluster more by architecture family than model size. Computational probing identifies specific attention heads and middle-to-late transformer layers that encode these importance patterns, suggesting robust internal representations of information value that are highly task-dependent.

## Method Summary
The researchers employ a two-pronged methodology to study information importance in LLM summarization. Behaviorally, they generate 10 summaries per document with target lengths ranging from 10-100 words, then compute empirical importance scores by counting token frequencies across these summaries. Computationally, they train one-hidden-layer MLPs to predict these importance distributions from hidden states, using KL divergence as the loss function. The study uses CNN/DailyMail (3,000 samples) and SAMSum (819 samples) datasets with Llama-3.2-1B, Llama-3.1-8B, and Qwen2.5 models of varying sizes. They also analyze attention head importance and layer-wise predictive power across the transformer architecture.

## Key Results
- LLMs develop consistent importance patterns that differ systematically from pre-LLM baselines and cluster more by architecture family than model size
- Middle-to-late transformer layers are most predictive of importance distributions, with specific attention heads showing strong alignment
- Importance encoding is robust across model scales but highly task-dependent, being far more predictable in conversational dialogues (SAMSum) than in long-form news (CNN/DailyMail)

## Why This Works (Mechanism)
The study demonstrates that LLMs maintain consistent internal hierarchies of information importance during summarization, with certain architectural components more directly encoding these patterns. By combining behavioral observation (what tokens get selected) with computational probing (which internal states predict these selections), the researchers establish a correlation between observable outputs and latent representations. This dual approach reveals that importance encoding emerges from specific attention mechanisms and layer positions rather than being uniformly distributed throughout the model, suggesting targeted points of control for future manipulation experiments.

## Foundational Learning

**Importance Distribution**: A vector representing how frequently each token appears across multiple generated summaries, serving as a proxy for semantic importance. *Why needed:* Provides the ground truth target for probing experiments. *Quick check:* Verify that high-frequency tokens correspond to summary-worthy content in sample documents.

**KL Divergence**: A measure of how one probability distribution differs from another, used here to quantify prediction error in probing experiments. *Why needed:* Enables training probes to match predicted importance distributions to empirical ones. *Quick check:* Confirm KL divergence decreases during probe training on validation set.

**Attention Head Analysis**: Identifying which specific attention heads correlate most strongly with importance distributions. *Why needed:* Pinpoints architectural components responsible for information selection. *Quick check:* Verify identified heads show consistent patterns across multiple documents.

## Architecture Onboarding

**Component Map**: CNN/DailyMail/SAMSum datasets -> Length-controlled summarization generation -> Token frequency counting -> Empirical importance distribution -> Hidden state extraction -> Probing MLP training -> KL divergence minimization -> Attention head/layer analysis

**Critical Path**: Behavioral generation → Importance distribution calculation → Probing experiment → Model interpretation

**Design Tradeoffs**: The study uses token-level importance for tractability but acknowledges this may miss semantic units; the length-control mechanism assumes exact word counts which LLMs struggle with; the probing approach assumes linear separability between hidden states and importance distributions.

**Failure Signatures**: High variance in generated summary lengths indicates the length-control mechanism isn't working; poor probe performance on validation set suggests either weak encoding or insufficient model capacity; inconsistent attention head importance across documents may indicate dataset artifacts.

**First Experiments**: 1) Generate 10 summaries for a single document and verify token frequency patterns match human intuitions; 2) Train a probe on a single hidden layer and measure KL divergence improvement; 3) Compare attention head importance scores between two different document types.

## Open Questions the Paper Calls Out
The paper identifies three key unresolved questions: First, whether causal manipulation of identified attention heads or layer representations can directly control information selection in summarization outputs remains untested. Second, the reason why importance encoding is far more predictable in conversational dialogues than in long-form news needs systematic investigation, as multiple dataset properties differ between these domains. Third, whether semantically meaningful information units beyond individual tokens (such as phrases or propositions) yield stronger alignment with model-internal importance representations is unexplored.

## Limitations
- The behavioral proxy for importance (token frequency across generated summaries) may conflate model-specific generation artifacts with true semantic importance
- The probing approach assumes linear separability between hidden states and importance distributions, which may not capture complex encoding patterns
- The claim about middle-to-late layers being most predictive could be influenced by architectural differences in layer count across models

## Confidence
- **High Confidence**: The finding that importance distributions differ systematically between pre-LLM and LLM baselines is well-supported by empirical data
- **Medium Confidence**: The claim that middle-to-late transformer layers are most predictive is supported but could be confounded by architectural differences
- **Low Confidence**: The assertion that importance encoding is "robust across model scales" requires more systematic scaling analysis than presented

## Next Checks
1. Conduct ablation studies on the behavioral proxy by comparing importance distributions derived from different numbers of generated summaries (k=5 vs k=20) to assess stability
2. Validate the probing results using alternative metrics beyond KL divergence, such as correlation with human-annotated importance scores where available
3. Test whether the observed layer-wise patterns persist when controlling for absolute layer position versus relative position within each architecture's depth