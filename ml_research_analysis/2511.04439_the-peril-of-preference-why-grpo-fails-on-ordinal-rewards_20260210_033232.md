---
ver: rpa2
title: 'The Peril of Preference: Why GRPO fails on Ordinal Rewards'
arxiv_id: '2511.04439'
source_url: https://arxiv.org/abs/2511.04439
tags:
- baseline
- grpo
- policy
- training
- advantage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental flaw in GRPO when applied
  to ordinal rewards: its group-average baseline can assign positive advantage to
  failed trajectories, reinforcing incorrect behavior. The authors propose CoRPO,
  which uses an adaptive baseline that enforces a minimum quality threshold, preventing
  failed solutions from receiving positive reinforcement while transitioning to relative
  preference mode as policy performance improves.'
---

# The Peril of Preference: Why GRPO fails on Ordinal Rewards

## Quick Facts
- arXiv ID: 2511.04439
- Source URL: https://arxiv.org/abs/2511.04439
- Authors: Anisha Garg; Ganesh Venkatesh
- Reference count: 20
- Key outcome: CoRPO achieves 95.8% pass@16 on out-of-domain "both correct" evaluation versus 89.6% for GRPO

## Executive Summary
This paper identifies a fundamental flaw in GRPO when applied to ordinal rewards: its group-average baseline can assign positive advantage to failed trajectories, reinforcing incorrect behavior. The authors propose CoRPO, which uses an adaptive baseline that enforces a minimum quality threshold, preventing failed solutions from receiving positive reinforcement while transitioning to relative preference mode as policy performance improves. On a code verification task, CoRPO demonstrates more stable convergence and achieves 95.8% pass@16 on out-of-domain "both correct" evaluation versus 89.6% for GRPO.

## Method Summary
CoRPO addresses GRPO's ordinal reward failure by implementing an adaptive baseline mechanism. The algorithm operates in two phases: Phase 1 (Correctness-Seeking) clamps the baseline to a minimum correctness threshold to prevent reinforcing failed trajectories, while Phase 2 (Preference-Seeking) transitions to standard GRPO once group performance exceeds the threshold. This approach ensures that during early training when the policy is weak, failed solutions are never positively reinforced, but as the model improves, it naturally shifts to optimizing for the best possible solution.

## Key Results
- CoRPO achieves 95.8% pass@16 on out-of-domain "both correct" evaluation versus 89.6% for GRPO
- Demonstrates superior exploration by uniformly reinforcing correct solutions regardless of initial likelihood
- Maintains performance without aggressive data filtering or dynamic rollouts

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Baseline Clamping
CoRPO prevents reinforcement of failed trajectories during early training by replacing GRPO's group-average baseline with a clamped floor defined by $b_{corpo} = \max(R_{min\_correct}, b_{mean})$. When policy performance is poor, the baseline locks to the minimum correctness threshold rather than the negative group average, ensuring failed solutions receive negative reinforcement.

### Mechanism 2: Preference-Seeking Transition
The method transitions from binary correctness filtering to relative optimization as policy performance improves. Once the group mean exceeds the minimum correctness threshold, the baseline automatically switches to the standard GRPO relative baseline, creating an "Aspirational Drive" where the model optimizes for the best possible solution rather than just an acceptable one.

### Mechanism 3: Intrinsic Regularization via Negative Feedback
CoRPO acts as an intrinsic regularizer by enforcing strict correctness guarantees, assigning high proportions of negative advantages. This reduces overfitting risk typically managed by weight decay, allowing for larger, more impactful weight updates when weight decay is set to 0.

## Foundational Learning

- **Advantage Calculation in Policy Gradients ($A = R - b$)**: Why needed: The core flaw is that in standard GRPO, if all samples fail, $b$ is negative, meaning a "less bad" failure gets positive advantage. Quick check: In a batch where the highest reward is -2 and average is -5, does standard GRPO reinforce the -2 reward?

- **Ordinal vs. Binary Rewards**: Why needed: Binary rewards mask failure severity, while ordinal rewards expose the "better than average" flaw by differentiating between "catastrophic failure" and "near miss." Quick check: Why does partial credit make the "central tendency" baseline unstable compared to binary success/failure?

- **Distribution Sharpening (Rank Bias)**: Why needed: GRPO disproportionately reinforces already-likely solutions ("double down"). Understanding this explains why CoRPO's uniform reinforcement of correct solutions improves exploration. Quick check: How does reinforcing "unlikely" correct trajectories prevent mode collapse?

## Architecture Onboarding

- **Component map**: Policy Model -> Reward Oracle -> Baseline Calculator -> Optimizer
- **Critical path**: The adaptive baseline calculation must happen after rollouts are generated but before loss calculation. The transition point (where $b_{mean} > R_{min\_correct}$) is automatic but critical to monitor.

- **Design tradeoffs**:
  - Static vs. Adaptive Baseline: Static guarantees correctness but lacks "Aspirational Drive"; Adaptive offers both but requires tuning $R_{min\_correct}$
  - Reward Granularity: 10-point vs. 4-point - coarser rewards increase advantage magnitude and stability; finer rewards risk vanishing gradients
  - Weight Decay: CoRPO suggests WD=0 for maximum exploration, relying on negative feedback for regularization

- **Failure signatures**:
  - "GRPO Trap": Accuracy plateaus or degrades because the model optimizes for "less wrong" answers
  - Vanishing Updates: High-granularity ordinal rewards cause advantage magnitudes to approach zero
  - Jitter: Validation accuracy fluctuates due to baseline toggling between Phase 1 and Phase 2 modes

- **First 3 experiments**:
  1. Replicate the flaw: Run standard GRPO on a task with low initial success rate. Plot advantage distributions to confirm failed trajectories receive positive advantage
  2. Ablate the baseline: Compare Static Baseline vs. Adaptive (CoRPO) vs. GRPO. Measure "In-Domain" vs. "Out-of-Domain" accuracy to verify correctness guarantee improves generalization
  3. Optimize signal magnitude: Train CoRPO with 10-point vs. 4-point reward bucketing and WD=0 vs. WD=0.1. Monitor training speed and final pass@N

## Open Questions the Paper Calls Out

### Open Question 1
How can CoRPO be effectively extended to denser, per-step supervision signals? The current formulation is validated on trajectory-level ordinal rewards; per-step credit assignment requires a different structural approach to feedback.

### Open Question 2
What specific training dynamics cause CoRPO to upweight low-probability trajectories when weight decay is removed? The observation of expanded solution horizons without weight decay warrants deeper analysis to understand the underlying training dynamics.

### Open Question 3
How can signal density be enhanced to prevent vanishing updates when policy predictions cluster within narrow bands? While coarse bucketing helped, the fundamental issue of diluting the learning signal when predictions are similar remains.

## Limitations

- The severity of the GRPO flaw may be task-dependent, varying based on whether near-misses are catastrophic versus merely suboptimal
- The adaptive baseline threshold ($R_{min\_correct}$) requires task-specific calibration with limited guidance on systematic selection across domains
- Claims about generalization to other domains (mathematical reasoning, instruction following) lack additional validation beyond code verification

## Confidence

- **High Confidence**: The mathematical derivation of the advantage calculation flaw and CoRPO mechanics
- **Medium Confidence**: Empirical superiority claims on code verification tasks, given single benchmark focus
- **Low Confidence**: Generalization claims to other domains without additional validation

## Next Checks

1. **Cross-Domain Validation**: Apply CoRPO to a mathematical reasoning benchmark (e.g., GSM8K) with ordinal rewards based on step-by-step correctness to verify generalization claims beyond code tasks

2. **Threshold Sensitivity Analysis**: Systematically vary $R_{min\_correct}$ across multiple orders of magnitude on the same code task to map the hyperparameter landscape and identify optimal selection strategies

3. **Advantage Distribution Audit**: Implement instrumentation to log and visualize the distribution of advantages throughout training for GRPO vs. CoRPO, confirming the "less bad" reinforcement pattern is eliminated