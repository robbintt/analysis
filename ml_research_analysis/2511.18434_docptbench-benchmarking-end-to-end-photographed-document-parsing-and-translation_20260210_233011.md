---
ver: rpa2
title: 'DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation'
arxiv_id: '2511.18434'
source_url: https://arxiv.org/abs/2511.18434
tags:
- translation
- document
- photographed
- original
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocPTBench, a benchmark for evaluating end-to-end
  document parsing and translation on photographed documents. Unlike existing benchmarks
  that use clean digital-born documents, DocPTBench contains over 1,300 high-resolution
  photographed documents from multiple domains, includes eight translation scenarios,
  and provides human-verified annotations.
---

# DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation

## Quick Facts
- **arXiv ID:** 2511.18434
- **Source URL:** https://arxiv.org/abs/2511.18434
- **Reference count:** 40
- **Primary result:** End-to-end parsing accuracy drops 18% and translation accuracy drops 12% when transitioning from digital-born to photographed documents

## Executive Summary
This paper introduces DocPTBench, a benchmark specifically designed to evaluate end-to-end document parsing and translation on photographed documents. Unlike existing benchmarks that use clean digital-born documents, DocPTBench contains over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides human-verified annotations. The benchmark is designed to reflect real-world conditions, including geometric distortions and photometric variations. Experiments show that transitioning from digital-born to photographed documents causes substantial performance drops: popular MLLMs exhibit an 18% average accuracy decline in end-to-end parsing and 12% in translation, while specialized document parsing models show a 25% average decrease. These results reveal the limited robustness of current models to real-world document capture conditions.

## Method Summary
The authors constructed DocPTBench by collecting high-resolution photographs of documents across three domains (bills, forms, ads) and eight translation scenarios. The dataset includes human-verified annotations for both parsing and translation tasks. The benchmark was designed to capture real-world conditions including geometric distortions from camera angles and photometric variations from lighting conditions. The evaluation framework measures performance degradation when models process photographed documents compared to digital-born documents, revealing substantial accuracy drops across multiple model families including commercial MLLMs and specialized document parsing models.

## Key Results
- Popular MLLMs show 18% average accuracy decline in end-to-end parsing when moving from digital-born to photographed documents
- Translation accuracy drops 12% on photographed documents versus digital documents
- Specialized document parsing models experience 25% average performance decrease on photographed versus digital documents

## Why This Works (Mechanism)
Assumption: The benchmark effectively captures real-world document capture conditions that challenge existing models, particularly through geometric distortions from camera angles and photometric variations from lighting conditions. These factors likely disrupt OCR accuracy and spatial understanding capabilities that are crucial for both parsing and translation tasks.

## Foundational Learning
- **Document parsing fundamentals:** Understanding how models extract structured information from unstructured document images
- **Translation model architectures:** Knowledge of how multimodal models handle cross-lingual document translation
- **Geometric distortion effects:** How perspective, rotation, and skew impact OCR and document understanding
- **Photometric variation impacts:** How lighting conditions affect document image quality and model performance
- **End-to-end evaluation metrics:** Methods for assessing combined parsing and translation performance

## Architecture Onboarding
**Component map:** Document image -> Preprocessing -> OCR engine -> Layout analysis -> Text extraction -> Translation module -> Structured output

**Critical path:** Image capture (photographed) -> OCR preprocessing (distortion correction) -> Text recognition -> Layout understanding -> Translation generation -> Evaluation against ground truth

**Design tradeoffs:** The benchmark prioritizes real-world photographic conditions over controlled digital document environments, accepting higher variance in input quality to better reflect deployment scenarios. This creates a more challenging evaluation but provides more actionable insights for production systems.

**Failure signatures:** Models exhibit systematic degradation in regions affected by perspective distortion, shadows, and glare. Performance drops are most pronounced in dense text regions and complex layouts requiring precise spatial understanding.

**3 first experiments:**
1. Test model performance on photographed vs. digitally scanned versions of identical documents
2. Evaluate individual distortion factors (perspective, lighting, blur) in isolation
3. Compare end-to-end performance across different document domains (bills vs. forms vs. ads)

## Open Questions the Paper Calls Out
Unknown: The source material did not provide specific open questions called out by the paper.

## Limitations
- Limited model diversity with only five commercial MLLMs and three specialized parsers evaluated
- Focus on English-language documents restricts claims about multilingual robustness
- Manual data collection process may limit reproducibility despite human-verified annotation quality

## Confidence
**Major Claim Clusters - Confidence Labels:**
- Performance drop statistics (18% parsing, 12% translation): **High**
- Cause attribution to geometric/photographic distortions: **Medium**
- Claims about real-world representativeness: **Medium**
- Generalizability across document domains: **Low**

## Next Checks
1. Test DocPTBench on additional multilingual document datasets to validate claims about language-specific robustness limitations
2. Evaluate newer open-weight MLLMs and specialized parsers not included in the current benchmark to assess whether observed performance gaps persist
3. Conduct ablation studies varying specific distortion factors (perspective, lighting, blur) to isolate their individual contributions to performance degradation