---
ver: rpa2
title: 'MAPS: Preserving Vision-Language Representations via Module-Wise Proximity
  Scheduling for Better Vision-Language-Action Generalization'
arxiv_id: '2511.19878'
source_url: https://arxiv.org/abs/2511.19878
tags:
- maps
- arxiv
- pretrained
- language
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAPS improves Vision-Language-Action (VLA) models by preserving
  pretrained Vision-Language Model (VLM) representations during action fine-tuning.
  It addresses generalization loss in VLAs caused by catastrophic forgetting when
  adapting pretrained VLMs to robotics tasks.
---

# MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization

## Quick Facts
- arXiv ID: 2511.19878
- Source URL: https://arxiv.org/abs/2511.19878
- Authors: Chengyue Huang; Mellon M. Zhang; Robert Azarcon; Glen Chou; Zsolt Kira
- Reference count: 40
- One-line result: Module-wise regularization scheduling preserves pretrained VLM priors, improving VLA generalization by up to 30% on out-of-distribution tasks

## Executive Summary
MAPS addresses catastrophic forgetting in Vision-Language-Action (VLA) models during robotics fine-tuning by preserving pretrained Vision-Language Model (VLM) representations. It uses Module-Wise Proximity Scheduling to apply stronger regularization to visual components while allowing flexible adaptation of language layers. The method consistently improves both in-distribution and out-of-distribution performance across multiple benchmarks including SimplerEnv, CALVIN, LIBERO, and real-world Franka Emika Panda robot evaluations.

## Method Summary
MAPS extends L2-SP regularization with module-wise scheduling, applying linear decay of proximity constraints from strongest (DINOv2) to weakest (late language). After Adam updates, parameters are projected toward pretrained weights θ_0 only when gradient-displacement correlation c_t < 0, using layer-specific strength λ_k × r_t. The approach preserves geometric/spatial priors critical for manipulation while enabling task-specific language adaptation, achieving generalization gains without additional parameters or architectural changes.

## Key Results
- Up to +30% improvement in out-of-distribution performance across SimplerEnv, CALVIN, LIBERO, and real robot benchmarks
- Consistent gains in both in-distribution and out-of-distribution metrics without additional parameters or data
- Linear scheduling outperforms constant and cosine alternatives on OOD metrics, with LIBERO showing 4.75 avg vs 2-3 for alternatives

## Why This Works (Mechanism)

### Mechanism 1
Module-wise regularization scheduling preserves pretrained priors better than uniform constraints. Linear schedule λ_k = λ_max(1 - (k-1)/(|L|-1)) decays from maximum constraint on early vision layers to zero on language layers, encoding the empirical hierarchy DINOv2 > SigLIP > Early Language > Late Language. Different VLM components encode distinct priors with varying sensitivity to fine-tuning disruption.

### Mechanism 2
Strong constraints on DINOv2 preserve geometric/spatial priors critical for manipulation. When c_t = -g^T(θ_{t-1} - θ_0) < 0, projection pulls parameters toward pretrained initialization with layer-specific strength λ_k × r_t. DINOv2 captures geometric priors (depth, spatial reasoning) that sparse robotics data cannot recover once corrupted.

### Mechanism 3
Selective projection based on gradient-displacement correlation prevents harmful weight drift. Compute c_t = -g^T(θ_{t-1} - θ_0); project only when c_t < 0 (update direction misaligned with preservation), using deviation ratio r_t to scale correction. Gradient-displacement misalignment indicates updates that harm generalization.

## Foundational Learning

- Concept: **L2-SP regularization**
  - Why needed here: MAPS builds on L2-SP which penalizes distance from pretrained weights (θ_0) rather than from zero
  - Quick check question: How does L2-SP differ from standard weight decay, and why does this matter for transfer learning?

- Concept: **Projected gradient descent**
  - Why needed here: Core optimization technique—projecting updated weights back into L2 ball around θ_0 with radius γ
  - Quick check question: Given parameters θ̃_t outside the constraint ball, how would you project them to satisfy ||θ - θ_0||₂ ≤ γ?

- Concept: **Catastrophic forgetting**
  - Why needed here: MAPS directly addresses loss of pretrained VLM capabilities during robotics fine-tuning
  - Quick check question: What happens to a model's original task performance when fine-tuned extensively on a smaller, specialized dataset?

## Architecture Onboarding

- Component map:
  DINOv2 (visual encoder) → SigLIP (vision-language alignment) → Bridge layers → Early language → Late language → Action head / proprioception projector

- Critical path:
  1. Initialize from pretrained VLM weights θ_0
  2. Standard forward pass → compute task loss L
  3. Backprop → gradient g_t
  4. Adam update → tentative parameters θ̃_t
  5. Per-layer: compute c_t = -g^T(θ_{t-1} - θ_0)
  6. If c_t < 0: project with θ_t ← θ̃_t - λ_k × r_t × (θ̃_t - θ_0)
  7. Else: accept θ̃_t unchanged

- Design tradeoffs:
  - λ_max too high → over-constrained adaptation, poor ID task performance
  - λ_max too low → insufficient preservation, OOD generalization collapse
  - Linear vs cosine vs constant schedule: Table 6 shows linear (v=0.5) yields best OOD on LIBERO (4.75 avg vs 2-3 for alternatives)

- Failure signatures:
  - ID good, OOD poor → visual encoder drifted; increase λ_max
  - Both ID and OOD near zero → language over-constrained; decrease λ_max
  - Inconsistent across tasks → freezing-style bias; verify MAPS is active (not hard freeze)

- First 3 experiments:
  1. Baseline comparison on SimplerEnv: Train MiniVLA-OFT with/without MAPS, measure ID (4 tasks) vs OOD (Visual/Novel Object/Novel Category) gaps
  2. λ_max sensitivity sweep: Grid search λ_max ∈ {0.5, 1.0, 1.5, 2.0, 2.5, 3.0} on validation; expect task-dependent optimal values (Table 8 shows 0.5-3.2 across benchmarks)
  3. Layer-wise distance monitoring: Track ||θ_t - θ_0||₂ per component during training; confirm MAPS produces hierarchical divergence (Fig. 3 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
Does the empirical hierarchy of module importance (DINOv2 > SigLIP > Early Language) generalize to VLA architectures utilizing different visual backbones or pretraining objectives? The study is restricted to models using the DINOv2 and SigLIP encoders; it is unclear if the same prioritization applies to pure ViT or 3D-aware encoders.

### Open Question 2
Can a dynamic, adaptive schedule outperform the fixed linear decay used in MAPS? The paper empirically selects a linear schedule over constant or cosine alternatives, stating it offers "structured, layer-wise proximity control," but does not explore non-linear or gradient-based adaptive scheduling.

### Open Question 3
How does the benefit of MAPS scale with the size and diversity of the training dataset? The method addresses generalization gaps caused by the "limited scope" of robotics datasets, but evaluations are performed on relatively small datasets (e.g., 600 real-world demos, LIBERO-90).

## Limitations
- The specific module hierarchy and linear scheduling mechanism are empirically derived but not theoretically justified
- Claims about geometric priors in DINOv2 being specifically critical for manipulation lack direct evidence beyond performance correlations
- The method doesn't address potential negative effects of over-constraining language layers in complex, compositional tasks

## Confidence

- **High**: Core experimental results showing MAPS improves both ID and OOD performance across all tested benchmarks
- **Medium**: The specific module hierarchy and linear scheduling mechanism—empirical findings but not theoretically derived
- **Low**: Claims about geometric priors in DINOv2 being specifically critical for manipulation; limited direct evidence beyond observed performance correlations

## Next Checks

1. **Scheduler ablation**: Compare linear scheduling against alternative decay functions (exponential, logarithmic, task-specific) on LIBERO to determine if linear is optimal or simply adequate

2. **Geometric prior isolation**: Train identical VLAs with DINOv2 frozen vs. fine-tuned on a depth-focused manipulation task to isolate whether preserved geometric priors drive the OOD gains

3. **Cross-architecture generalization**: Apply MAPS to non-MiniVLA architectures (e.g., GR0, VIMA) to test whether the DINOv2 > SigLIP > Language hierarchy holds across different VLM backbones and scales