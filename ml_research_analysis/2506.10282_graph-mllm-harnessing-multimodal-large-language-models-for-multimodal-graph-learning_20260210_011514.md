---
ver: rpa2
title: 'Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph
  Learning'
arxiv_id: '2506.10282'
source_url: https://arxiv.org/abs/2506.10282
tags:
- image
- information
- text
- multimodal
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Graph-MLLM, a comprehensive benchmark designed\
  \ to evaluate multimodal graph learning (MMGL) using multimodal large language models\
  \ (MLLMs). The benchmark categorizes existing MMGL approaches into three paradigms\u2014\
  MLLM-as-Encoder, MLLM-as-Aligner, and MLLM-as-Predictor\u2014and systematically\
  \ evaluates them across six datasets spanning e-commerce and social network domains."
---

# Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning

## Quick Facts
- arXiv ID: 2506.10282
- Source URL: https://arxiv.org/abs/2506.10282
- Reference count: 40
- This paper introduces Graph-MLLM, a comprehensive benchmark designed to evaluate multimodal graph learning (MMGL) using multimodal large language models (MLLMs).

## Executive Summary
This paper introduces Graph-MLLM, a comprehensive benchmark designed to evaluate multimodal graph learning (MMGL) using multimodal large language models (MLLMs). The benchmark categorizes existing MMGL approaches into three paradigms—MLLM-as-Encoder, MLLM-as-Aligner, and MLLM-as-Predictor—and systematically evaluates them across six datasets spanning e-commerce and social network domains. Extensive experiments reveal that fine-tuning MLLMs as standalone predictors delivers the most significant performance gains, even without explicit graph structure, highlighting their potential as powerful backbones for MMGL. The study also shows that combining visual and textual attributes consistently benefits graph learning, and that structure-aware augmentation methods improve performance, especially in denser graphs. However, effectiveness depends heavily on dataset properties such as graph density and multimodal quality. The benchmark library is open-sourced to support equitable evaluation and inspire future research in this emerging field.

## Method Summary
The Graph-MLLM benchmark evaluates three paradigms for MMGL: (1) MLLM-as-Encoder, where pre-trained multimodal models like CLIP provide features for GNNs; (2) MLLM-as-Aligner, where MLLMs generate textual summaries of images to feed GraphLLMs; and (3) MLLM-as-Predictor, where MLLMs are fine-tuned directly on the graph data. The evaluation uses six datasets with text and image node attributes, testing performance across various model combinations. Structure-aware augmentation and fine-tuning strategies are explored, particularly for the MLLM-as-Predictor paradigm, to integrate graph structure information.

## Key Results
- Fine-tuning MLLMs as standalone predictors achieves the highest performance, even without explicit graph structure.
- Combining visual and textual modalities consistently improves performance over unimodal inputs.
- Structure-aware augmentation benefits denser graphs but can degrade performance on sparse graphs.
- MLLM-as-Encoder with CLIP features and GNNs provides a strong baseline, but lags behind fine-tuned MLLMs.

## Why This Works (Mechanism)

### Mechanism 1
Combining visual and textual modalities improves multimodal graph learning performance compared to unimodal inputs, even with simple pre-trained encoders like CLIP. Multimodal inputs provide complementary information that enriches node representations. Text captures semantic descriptions while images capture visual attributes, reducing ambiguity and improving feature discriminability. Core assumption: The pre-trained encoder (e.g., CLIP) has sufficiently aligned text and image embedding spaces, and the downstream GNN can effectively fuse these modalities. Evidence: Modality ablation studies show consistent gains across 5/6 datasets. Break condition: If visual and textual modalities are highly uncorrelated or noisy, fusion may not help or could degrade performance.

### Mechanism 2
Converting visual attributes into textual descriptions (image-to-text alignment) and using them in LLM-based graph models improves performance compared to using visual embeddings directly. LLM-based methods primarily operate on text. Transforming images into descriptive text allows these models to leverage their strong natural language understanding capabilities on visual information, creating a unified textual representation for reasoning. Core assumption: The MLLM used as an aligner can generate accurate, concise, and relevant textual summaries of images that capture essential features for the downstream task. Evidence: GraphPrompter and GraphGPT benefit from modality transformation. Break condition: If the MLLM-generated image summaries are poor, contain hallucinations, or introduce noise, downstream LLM performance may degrade.

### Mechanism 3
Fine-tuning MLLMs on specific multimodal graphs achieves state-of-the-art results, often outperforming methods that use explicit graph structure. Fine-tuning adapts the MLLM's pre-trained multimodal understanding and reasoning capabilities to the specific distribution of the target graph dataset. The model may infer implicit structural patterns from multimodal attributes without needing the adjacency matrix. Core assumption: The MLLM has sufficient capacity and pre-trained knowledge to adapt to the target task, which can be framed as a multimodal-to-label prediction problem. Evidence: Fine-tuned Qwen-VL-7B achieves gains of +41.62% compared to zero-shot baseline. Break condition: If node attributes are sparse or uninformative, the lack of explicit structure will likely cause performance to lag behind structure-aware GNN-based methods.

## Foundational Learning

- **Concept: Multimodal Large Language Models (MLLMs)**
  - **Why needed here**: This is the core backbone of the "MLLM-as-Predictor" paradigm and is also used as an aligner. You must understand that MLLMs (e.g., Qwen-VL, LLaVA) are models trained to align and reason across visual and textual modalities.
  - **Quick check question**: Can you explain the difference between using CLIP (an MLLM-as-Encoder) and Qwen-VL (an MLLM-as-Predictor) in the context of this paper?

- **Concept: Graph Neural Networks (GNNs)**
  - **Why needed here**: The "MLLM-as-Encoder" paradigm is a hybrid approach where MLLM features are fed into a GNN. Understanding the GNN's role in aggregating neighbor information is critical to appreciating why structure-aware augmentation is explored for MLLMs.
  - **Quick check question**: Based on Equation 1 in Section 2.2, what is the fundamental operation a GNN performs to generate a node's representation?

- **Concept: Modality Alignment & Fusion**
  - **Why needed here**: A central challenge in MMGL is combining information from text, images, and graph structure. This paper explores multiple fusion points: early fusion in the encoder (CLIP), mid-fusion via text summarization (Aligner), and late fusion via the MLLM's internal reasoning (Predictor).
  - **Quick check question**: How does the "MLLM-as-Aligner" paradigm perform modality fusion compared to the "MLLM-as-Encoder" paradigm?

## Architecture Onboarding

- **Component map**: Raw Data -> Encoder Backbone (CLIP variants) -> Predictor Backbone (GNN/LLM/MLLM) -> Fine-tuning/Augmentation Modules -> Node Classification Output

- **Critical path**: The path with the highest demonstrated performance is the MLLM-as-Predictor with fine-tuning. Prepare Data: Format each node's data into an instruction prompt with its image and text attributes. Fine-tune MLLM: Apply instruction tuning (e.g., using LoRA) on the dataset's training split. The prompt may or may not include neighbor information. Predict: Use the fine-tuned MLLM to predict labels for nodes in the test set.

- **Design tradeoffs**:
  - **MLLM-as-Encoder vs. Predictor**: Encoder approach is computationally cheaper at inference but requires a separate GNN and may not leverage the full reasoning power of the LLM. Predictor approach is more expensive but yields higher accuracy.
  - **Structure-Aware Augmentation**: Adding neighbors can help (denser graphs) or hurt (sparser graphs) performance and increases prompt length/compute cost.
  - **Fine-tuning vs. Zero-shot**: Fine-tuning provides large gains but requires a labeled dataset.

- **Failure signatures**:
  - **Low-quality modalities**: If images are poor or text is too short, MLLM-based methods may struggle, reducing the effectiveness of the entire approach.
  - **Sparse graphs**: Structure-aware methods may fail to provide benefits or even degrade performance on sparse graphs.
  - **Prompt noise**: Poorly generated image summaries or irrelevant neighbor information in prompts can mislead the LLM, causing performance drops.

- **First 3 experiments**:
  1. **Reproduce Modality Ablation**: Using a GNN (e.g., GraphSAGE), compare performance with text-only, image-only, and text+image inputs using a frozen CLIP encoder on one dataset (e.g., Grocery). This validates the multimodal benefit.
  2. **Benchmark the Three Paradigms**: Implement and evaluate one representative from each paradigm (e.g., CLIP+GCN for Encoder, LLaGA for Aligner, fine-tuned Qwen-VL for Predictor) on a single dataset to establish baseline performance and confirm the Predictor paradigm's superiority.
  3. **Ablate Structure in MLLM Fine-tuning**: Fine-tune an MLLM (e.g., Qwen-VL) with and without structure-aware prompts (neighbor info) on a dense (CDs) and sparse (Toys) dataset to observe the conditional benefit of structure.

## Open Questions the Paper Calls Out

### Open Question 1
How can graph structural information be seamlessly integrated into Multimodal Large Language Models (MLLMs) beyond prompt-level injection to achieve consistent performance gains? Basis: Finding 7 states that "injecting them into MLLM prompts is not sufficient" and emphasizes the "need for more advanced methods to seamlessly integrate graph structure information into MLLMs." Why unresolved: Current structure-aware fine-tuning strategies yield modest gains (<1%) or degrade accuracy, particularly when incorporating visual information from neighbors. What evidence would resolve it: Architectural modifications that allow MLLMs to ingest structural topology directly, demonstrating significant accuracy improvements over non-structure baselines across diverse datasets.

### Open Question 2
What advanced alignment techniques are required to effectively integrate multiple node attribute modalities with graph structural information in GNN-based methods? Basis: Finding 2 notes that "modality and structure alignment enhancement methods do not consistently enable GNN-based models" and highlights the "need for advanced alignment techniques." Why unresolved: The experiments show that structure-aware encoders (CLIP-F-S) often underperform compared to non-structure-aware encoders (CLIP), failing to consistently leverage the relationships among data points during feature extraction. What evidence would resolve it: A new pre-training or fine-tuning objective for multimodal encoders that consistently improves node classification accuracy over standard baselines by successfully fusing text, images, and graph topology.

### Open Question 3
How can Multimodal Graph Learning (MMGL) methods be adapted to prevent performance degradation in sparse graphs or those with imbalanced multimodal quality? Basis: Finding 9 observes that structure-aware augmentation "may even degrade performance" in sparse graphs (e.g., Toys) or those with poor image features (e.g., Movies). Why unresolved: Current structure-aware augmentation methods are sensitive to data properties, acting as noise rather than signal when the graph is too sparse or modalities are low-quality. What evidence would resolve it: A method that dynamically weighs structural information based on local graph density or modality signal-to-noise ratios, resulting in positive or neutral performance impacts on sparse/low-quality datasets.

## Limitations

- **Dataset Generalization**: Findings may not generalize beyond e-commerce and social network domains to scientific or biological networks.
- **Hyperparameter Sensitivity**: Critical hyperparameters like neighbor sampling size and learning rates are underspecified, creating uncertainty about optimal configurations.
- **Resource Requirements**: Fine-tuning 7B MLLMs requires substantial computational resources (≥24GB VRAM), limiting accessibility and reproducibility.

## Confidence

**High Confidence**: Combining visual and textual modalities consistently improves GNN-based model performance across six datasets with substantial effect sizes.

**Medium Confidence**: Fine-tuning MLLMs achieves state-of-the-art results without explicit graph structure, though the underlying mechanism remains somewhat unclear.

**Low Confidence**: Structure-aware augmentation improves performance for denser graphs, but evidence is limited to comparisons between only two datasets (CDs vs. Toys).

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the three paradigms to a non-ecommerce dataset such as a scientific citation network with multimodal attributes to assess domain transferability.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary neighbor sampling size (m) for CLIP-F-S and learning rates for LoRA fine-tuning across ranges to identify optimal values and stability boundaries.

3. **Resource-Constrained Reproduction**: Attempt to reproduce key findings using consumer-grade hardware (12-24GB VRAM) with 4-bit quantization and gradient accumulation to document performance degradation and identify minimum viable requirements.