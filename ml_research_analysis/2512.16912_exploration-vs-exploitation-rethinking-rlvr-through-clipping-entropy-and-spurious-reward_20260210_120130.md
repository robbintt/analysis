---
ver: rpa2
title: 'Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and
  Spurious Reward'
arxiv_id: '2512.16912'
source_url: https://arxiv.org/abs/2512.16912
tags:
- entropy
- policy
- have
- training
- clipping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the exploration-exploitation trade-off in
  reinforcement learning with verifiable rewards (RLVR) for improving the reasoning
  of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong
  mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious
  rewards, which suppress exploitation by rewarding outcomes unrelated to the ground
  truth, and entropy minimization, which suppresses exploration by pushing the model
  toward more confident and deterministic outputs.'
---

# Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward

## Quick Facts
- **arXiv ID**: 2512.16912
- **Source URL**: https://arxiv.org/abs/2512.16912
- **Reference count**: 40
- **Primary result**: Both suppressing exploitation (via spurious rewards) and suppressing exploration (via entropy minimization) can improve LLM reasoning, but the mechanisms remain poorly understood

## Executive Summary
This paper investigates the paradoxical observation that both spurious rewards (which suppress exploitation) and entropy minimization (which suppresses exploration) can improve LLM reasoning in RLVR. Through theoretical analysis and empirical validation, the authors show that clipping bias under spurious rewards implicitly reduces policy entropy, leading to more confident outputs, while entropy minimization alone is insufficient for improvement. The paper proposes a reward-misalignment model explaining why spurious rewards benefit stronger models more, as damage from mislabeling shrinks and shifts compositionally with model competence.

## Method Summary
The study uses GRPO (Group Relative Policy Optimization) with random Bernoulli(1/2) rewards independent of ground truth to train LLMs on mathematical reasoning tasks. The training pipeline includes: drawing 16 rollouts per prompt from the current policy, computing standardized advantages from binary rewards, applying clipped surrogate objectives, and tracking policy entropy. Experiments use DeepScaleR dataset (from AMC, AIME, Omni-Math, Still) with MATH500 validation, training Qwen2.5-Math models of various sizes with specified hyperparameters (batch size 128, learning rate 5×10⁻⁷, clipping ratio ε=0.2).

## Key Results
- Clipping under spurious rewards systematically reduces policy entropy through negative entropy change bounds
- Stronger models benefit more from random rewards as damage from mislabeling decreases with competence
- Policy entropy and validation accuracy lack deterministic relationship; entropy reduction can lead to either gains or losses depending on regime
- Unclipped training on strong models leads to gradient explosion after ~150 steps

## Why This Works (Mechanism)

### Mechanism 1: Clipping as Implicit Entropy Minimization
- Under spurious rewards, clipping provides negligible learning signal but creates deterministic entropy reduction through negative terms in entropy change bounds
- Theorem 4.3 shows E[H(πnew)−H(πold)] ≤ −c_G Φ(πold)η² + E[R(η)] + c(p)_G(...) where the first term drives entropy down
- Asymmetric effects confirmed: Clip-High decreases entropy while Clip-Low increases it

### Mechanism 2: Reward Misalignment Benefits Strong Models
- For group size G with nc correct and ni incorrect rollouts, expected advantage loss is E[Δ] = nc(G-nc)/G
- As nc increases, total damage decreases and false-positive damage becomes smaller fraction of total damage
- Weaker models (nc < G/2) experience more instability and less benefit from random rewards

### Mechanism 3: No Direct Entropy-Performance Causality
- Both entropy increase (unclipped, skewed policies) and entropy decrease (clipped training) can accompany performance gains or losses
- Regime-dependent relationship: entropy minimization can drive policies toward incorrect low-entropy solutions on harder data
- Convergence to low-entropy policy does NOT necessarily improve accuracy

## Foundational Learning

- **Importance Sampling Ratio** r_t(θ) = π_θ/π_old
  - Why needed: GRPO estimates gradients using samples from old policy; ratio correction enables unbiased updates but requires clipping for stability
  - Quick check: What happens to variance when π_θ drifts far from π_old?

- **Group-Relative Advantage** A_i = (r_i - mean)/std
  - Why needed: Central to GRPO; zero-mean standardized advantages create symmetry properties analyzed in reward-misalignment model
  - Quick check: Why must Σ_i A_i = 0, and how does this affect random reward analysis?

- **Policy Skewness Φ(π)**
  - Why needed: Theorem 4.1 shows entropy change sign depends on Φ(π); skewed policies (|Φ| < 0) can experience entropy growth under unclipped training
  - Quick check: For a two-armed policy with β = 0.9, will entropy increase or decrease under unclipped spurious rewards?

## Architecture Onboarding

- **Component map**: Prompt → G rollouts → Verifier → Standardized advantages → Clipped loss → Policy update
- **Critical path**: Single forward pass generating G samples, binary verification, advantage standardization, clipped gradient update
- **Design tradeoffs**: Clipping on prevents gradient explosion but reduces entropy; group size G=16 balances reward distribution vs variance
- **Failure signatures**: Gradient explosion after 100+ steps (unclipped training on strong models), random walk trajectories on hard datasets, clipping activation < 0.2% with no improvement
- **First 3 experiments**:
  1. Replicate Figure 1: Train Qwen2.5-Math-7B with/without clipping on DeepScaleR with random rewards; log entropy and accuracy
  2. Test unclipped training on AIME dataset to observe entropy increase regime and gradient stability
  3. Validate reward-misalignment: Compare Qwen2.5-Math-1.5B vs R1-Distill-Llama-8B under identical random-reward training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can spurious rewards be systematically combined with explicit entropy regularization or true rewards to optimize the exploration-exploitation trade-off in RLVR?
- Basis: Section 4.1 mentions this motivates new ways to modulate entropy using spurious-reward setups alongside explicit entropy regularization
- Why unresolved: Paper demonstrates implicit entropy reduction but doesn't explore hybrid training schemes
- What evidence would resolve: Empirical results from mixed random/ground-truth reward training showing superior performance

### Open Question 2
- Question: How can the "damage" predicted by reward-misalignment model be mitigated for weaker models or difficult datasets?
- Basis: Section 5 defines damage as advantage loss diverted by mislabeling, noting weaker models suffer from instability
- Why unresolved: Explains why strong models benefit but doesn't propose mechanisms to reduce variance for weak models
- What evidence would resolve: Modified training algorithm that allows weak models to achieve consistent gains under spurious rewards

### Open Question 3
- Question: What specific metrics can predict whether entropy reduction will reinforce correct reasoning versus causing convergence to incorrect solutions?
- Basis: Section 4.3 warns entropy minimization can drive policies toward incorrect low-entropy solutions on harder data
- Why unresolved: Establishes regime-dependent relationship but doesn't identify reliable signals to distinguish beneficial vs harmful entropy minimization
- What evidence would resolve: Quantitative proxy that correlates with successful entropy minimization across diverse datasets

### Open Question 4
- Question: How should clipping thresholds (ε) be adapted to balance training stability against suppression of necessary exploration?
- Basis: Section 4.2 shows clipping prevents gradient explosion but acts as regularization that may limit exploration
- Why unresolved: Analyzes fixed clipping ratios but leaves open question of dynamic clipping schedules
- What evidence would resolve: Experiments showing adaptive clipping schedule outperforms fixed ε=0.2 baseline

## Limitations
- Reward-misalignment model assumes i.i.d. Bernoulli rewards independent of correctness, which may not generalize to real-world verifiers
- Entropy analysis focuses on single-group dynamics without accounting for multi-group interactions or curriculum effects
- Hardware-specific factors like gradient accumulation strategies are not fully specified, potentially affecting reproducibility

## Confidence

- **High Confidence**: Entropy-clipping relationship under spurious rewards is well-supported by theoretical bounds and empirical validation
- **Medium Confidence**: Reward-misalignment model's predictions about stronger models benefiting more rely on assumptions about reward independence that may not hold universally
- **Low Confidence**: Claim that entropy minimization alone is insufficient lacks comprehensive validation across diverse datasets and architectures

## Next Checks
1. **Cross-Dataset Validation**: Test reward-misalignment hypothesis on non-mathematical reasoning tasks (code generation, commonsense QA) to verify nc(G-nc)/G damage formula generalizes
2. **Reward Distribution Sensitivity**: Replace i.i.d. Bernoulli rewards with structured noise patterns (correlated errors, systematic verifier biases) to test stronger-model benefit persistence
3. **Multi-Group Dynamics**: Extend single-group analysis to examine entropy and accuracy trajectories when training on heterogeneous task groups with varying difficulty levels and reward distributions