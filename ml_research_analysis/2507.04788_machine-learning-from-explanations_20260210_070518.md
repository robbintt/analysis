---
ver: rpa2
title: Machine Learning from Explanations
arxiv_id: '2507.04788'
source_url: https://arxiv.org/abs/2507.04788
tags:
- training
- explanations
- learning
- data
- reasons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training reliable classification
  models when labeled data is limited, imbalanced, or contains spurious correlations.
  The key challenge is that standard training algorithms often learn arbitrary classification
  rules that don't generalize well, especially when explanations for label assignments
  are missing.
---

# Machine Learning from Explanations

## Quick Facts
- arXiv ID: 2507.04788
- Source URL: https://arxiv.org/abs/2507.04788
- Reference count: 40
- Primary result: Two-stage training with explanation masks improves accuracy from 0.754 to 0.874 on 500-sample triangle data

## Executive Summary
This paper tackles the challenge of training reliable classifiers when labeled data is limited, imbalanced, or contains spurious correlations. Standard training algorithms often learn arbitrary classification rules that don't generalize well, especially when explanations for label assignments are missing. The authors propose a two-stage training pipeline that uses simple explanation signals (like important input features) alongside labels to guide the model toward learning more generalizable decision rules.

The method significantly outperforms baselines across multiple datasets and settings. On a triangle orientation dataset, it achieves 0.874 accuracy with 500 samples versus 0.754 for label-only training. With 9:1 class imbalance, it reaches 0.806 accuracy versus 0.725 for baselines. When spurious features are added, the method maintains 0.708 accuracy on clean test sets while baselines drop to 0.561. The models also show superior consistency, with pairwise agreement of 0.949 versus 0.711 for standard training on the same data.

## Method Summary
The approach decomposes a CNN into three components: feature extractor (f), mapping layer (m), and classifier (c). In Stage 1, the model trains normally using cross-entropy loss with all weights updated. In Stage 2, the input is masked using the explanation (e(x)), and a feature misalignment loss is computed between the model's latent features and those extracted from the masked input using KL divergence. This loss is used to update only the mapping layer, helping the model focus on the right features while maintaining the classifier's learned decision boundaries.

## Key Results
- Triangle dataset (500 samples, balanced): 0.874 accuracy vs 0.754 baseline
- Imbalanced setting (9:1 ratio): 0.806 accuracy vs 0.725 baseline
- With spurious features: 0.708 accuracy on clean test vs 0.561 baseline
- Pairwise agreement across 30 models: 0.949 vs 0.711 for standard training

## Why This Works (Mechanism)
The method works by explicitly aligning the model's feature extraction with human-provided explanations. By masking inputs and comparing feature distributions, the model learns to prioritize features that humans deem important for classification decisions. The two-stage approach ensures that the classifier maintains its learned decision boundaries while the feature extractor is guided toward more generalizable representations.

## Foundational Learning
- **KL divergence for feature alignment**: Needed to measure similarity between feature distributions; quick check: verify KL loss decreases during training
- **Two-stage optimization**: Separates classifier learning from feature alignment; quick check: confirm mapping layer updates don't affect classifier weights
- **Feature masking**: Creates counterfactual inputs highlighting important regions; quick check: visualize masked inputs and corresponding feature maps
- **Cross-entropy with limited data**: Standard approach for small datasets; quick check: monitor training loss to avoid overfitting

## Architecture Onboarding

Component map: Input -> Conv layers (f) -> Mapping layer (m) -> Classifier (c) -> Output

Critical path: Masked input features -> KL alignment -> Mapping layer update

Design tradeoffs: Sequential vs simultaneous optimization of losses; identity mapping vs learned transformation

Failure signatures: 
- Loss plateaus or diverges (gradients counteract)
- NaN losses (improper normalization)
- No improvement over baseline (mapping layer ineffective)

First experiments:
1. Verify KL loss computation with softmax-normalized features
2. Test mapping layer ablation - remove and compare performance
3. Check sequential vs simultaneous optimization impact

## Open Questions the Paper Calls Out
- **Multi-class extension**: The method could naturally extend to multi-class classification, but empirical validation is needed on datasets with more than two classes and varying imbalance ratios.
- **LLM adaptation**: Future work could extend the method to large language models where explanations are important tokens, using LoRA-style mapper layers appended to attention blocks.
- **Constrained optimization**: The current loss-based approach cannot guarantee exclusive use of specified features, suggesting potential for constrained optimization formulations.

## Limitations
- Cannot ensure models exclusively use specified features (loss-based vs constrained optimization limitation)
- Performance depends on quality and reliability of explanation masks
- Only validated on binary classification tasks; multi-class extension unproven

## Confidence
- Triangle dataset results: High
- Fox vs Cat and Bird datasets: Medium (limited detail about explanation masks)
- Consistency claims: Medium (needs more baseline context)

## Next Checks
1. Verify KL loss is computed between normalized feature distributions to prevent instability
2. Test mapping layer ablation to confirm it's essential rather than an artifact of training schedule
3. Compare sequential vs simultaneous optimization to isolate the effect of two-stage training