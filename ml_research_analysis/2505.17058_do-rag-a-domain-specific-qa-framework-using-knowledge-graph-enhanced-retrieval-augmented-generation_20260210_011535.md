---
ver: rpa2
title: 'DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented
  Generation'
arxiv_id: '2505.17058'
source_url: https://arxiv.org/abs/2505.17058
tags:
- knowledge
- graph
- answer
- do-rag
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DO-RAG, a hybrid framework that combines knowledge
  graph construction with retrieval-augmented generation to improve accuracy and reasoning
  in domain-specific QA. The system automates extraction of entities and relationships
  from multimodal documents, integrates graph-based and semantic search for context
  retrieval, and applies a refinement step to reduce hallucinations.
---

# DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.17058
- Source URL: https://arxiv.org/abs/2505.17058
- Reference count: 32
- One-line primary result: DO-RAG achieves near-perfect contextual recall (1.0) and over 94% answer relevancy in domain-specific QA by combining knowledge graph construction with retrieval-augmented generation.

## Executive Summary
DO-RAG introduces a hybrid framework that automates knowledge graph construction from multimodal documents and integrates it with retrieval-augmented generation to improve accuracy and reasoning in domain-specific question answering. The system employs a four-agent extraction pipeline to capture multi-granular relationships, uses hybrid retrieval fusion with query refinement, and applies a three-stage generation process to mitigate hallucinations. Evaluated on database and electrical engineering datasets with 245 questions each, DO-RAG demonstrates superior performance compared to baseline RAG frameworks, achieving near-perfect contextual recall and over 94% answer relevancy.

## Method Summary
DO-RAG operates through a four-stage pipeline: multimodal ingestion and chunking, multi-level knowledge graph extraction via four specialized agents (High-Level for structure, Mid-Level for entities, Low-Level for relations, Covariate for attributes), hybrid retrieval combining graph traversal and vector search with score fusion, and grounded generation with refinement stages. The framework stores chunks in pgvector and constructs a hierarchical knowledge graph with deduplication and synopsis nodes. Retrieval uses query decomposition, multi-hop graph traversal, and α-weighted fusion of graph and vector relevance scores. Generation proceeds through naive prompting, refinement, and condensation stages with citation requirements and follow-up question generation.

## Key Results
- Near-perfect contextual recall (1.0) and over 94% answer relevancy on 245-question database and electrical engineering datasets
- Outperforms baseline RAG frameworks by up to 33.38% in composite score
- Creative models like DeepSeek-R1 achieve higher composite scores (0.847) but lower faithfulness (0.678) despite KG grounding

## Why This Works (Mechanism)

### Mechanism 1: Multi-Level Knowledge Graph Construction via Agentic Extraction
- Claim: Hierarchical agent-based extraction captures multi-granular relationships that chunk-based approaches miss, improving retrieval precision.
- Mechanism: Four specialized agents operate at different abstraction levels—High-Level Agent identifies structural elements, Mid-Level Agent extracts domain entities, Low-Level Agent captures fine-grained operational relationships, and Covariate Agent attaches attributes. Deduplication via cosine similarity prevents redundancy; synopsis nodes compress similar entities.
- Core assumption: Domain documents contain implicit relational structure that can be reliably extracted and formalized into a knowledge graph without excessive manual curation.
- Evidence anchors:
  - [abstract]: "employs a novel agentic chain-of-thought architecture to extract structured relationships from unstructured, multimodal documents, constructing dynamic knowledge graphs that enhance retrieval precision"
  - [section III.B]: Figure 2 and accompanying text describe the four-agent hierarchy with deduplication and synopsis node synthesis
  - [corpus]: GFM-RAG (FMR 0.608, h-index 66) demonstrates graph-based retrieval captures complex relationships; M³KG-RAG validates multi-hop multimodal KG effectiveness
- Break condition: If entity extraction quality degrades due to poor document quality (OCR errors, ambiguous terminology), noisy graphs will propagate errors through retrieval.

### Mechanism 2: Hybrid Retrieval Fusion with Query Refinement
- Claim: Sequential fusion of graph traversal and vector search, mediated by query refinement, yields higher contextual recall than either method alone.
- Mechanism: User query → intent decomposition into sub-queries → KG entity matching + multi-hop traversal → graph-derived context rewrites query → refined query drives vector search → results aggregated into unified prompt. The fusion score S = α · max sim(Q, Ci) + (1 − α) · R(GQ) balances semantic and structural relevance.
- Core assumption: Graph context provides structural disambiguation that improves subsequent vector search relevance; the two retrieval modalities are complementary rather than redundant.
- Evidence anchors:
  - [abstract]: "fuses graph and vector retrieval results to generate context-aware responses"
  - [section III.C]: Equation (3) and Figure 3 detail the retrieval scoring and query refinement pipeline
  - [corpus]: KG2QA validates KG-enhanced RAG for standards-domain QA; CDF-RAG (FMR 0.613) shows causal feedback improves over correlation-only retrieval
- Break condition: If α calibration is wrong or graph/vector results are highly contradictory without reconciliation logic, fusion may degrade rather than improve relevance.

### Mechanism 3: Grounded Refinement for Hallucination Mitigation
- Claim: Multi-stage generation with explicit evidence constraints reduces factual errors compared to single-pass generation.
- Mechanism: Naive prompt constrains generation to retrieved evidence → refinement prompt restructures and validates → condensation aligns tone → system returns "I do not know" if evidence insufficient. Citation footnotes enforce traceability.
- Core assumption: LLMs can self-correct when prompted with explicit grounding constraints; the refinement stage does not introduce new hallucinations.
- Evidence anchors:
  - [abstract]: "hallucination mitigation via grounded refinement"
  - [section III.D]: Figure 4 and text describe staged prompting with "I do not know" fallback and citation requirements
  - [corpus]: DSRAG (FMR 0.580) addresses hallucination in domain-specific RAG; corpus evidence for refinement-stage hallucination reduction specifically is weak
- Break condition: Highly creative models (e.g., DeepSeek-R1) may still deviate despite grounding—Table II shows R1 faithfulness dropped 5.6% with KG integration, suggesting model creativity can override refinement constraints.

## Foundational Learning

- Concept: **Knowledge Graph Construction (Entities, Relations, Multi-hop Traversal)**
  - Why needed here: DO-RAG's core innovation is automated KG building from multimodal documents; understanding graph structure is prerequisite to debugging retrieval failures.
  - Quick check question: Given a technical manual describing "Service A calls API B which updates Table C," can you sketch the entity-relation triple and explain how multi-hop traversal would retrieve all three nodes from a query about "Service A"?

- Concept: **Hybrid Retrieval Scoring and Fusion**
  - Why needed here: The α-weighted fusion of vector similarity and graph relevance is the critical path; mis calibrated α values will skew results.
  - Quick check question: If α = 0.8 in Equation (3), which retrieval modality dominates, and what types of queries might this disadvantage?

- Concept: **Hallucination Detection and Faithfulness Metrics**
  - Why needed here: The framework explicitly trades off model creativity against faithfulness; understanding how faithfulness is measured (claims verified against context) is essential for evaluating refinement effectiveness.
  - Quick check question: Table II shows DeepSeek-R1 faithfulness at 0.678 with KG. What does this metric measure, and why might a "creative" model score lower despite having structured context?

## Architecture Onboarding

- Component map:
  - Ingestion: Multimodal document parser → chunker → pgvector storage
  - KG Construction: Four-agent extraction pipeline → deduplication → synopsis synthesis → PostgreSQL graph storage
  - Retrieval: Intent decomposition → KG entity matching + multi-hop traversal → query refinement → vector search → fusion
  - Generation: Naive prompt → refinement prompt → condensation → follow-up question generator
  - Infrastructure: LangFuse (tracing), Redis (caching), MinIO (document storage), ClickHouse (analytics)

- Critical path: The hybrid retrieval fusion (Equation 3) where graph-derived context rewrites the query before vector search—this sequential dependency is where most retrieval quality variance originates.

- Design tradeoffs:
  - KG construction overhead vs. retrieval precision: Multi-agent extraction is computationally expensive but enables near-perfect recall (Table II: 1.0 with KG vs. 0.964-0.977 without)
  - Model creativity vs. faithfulness: Creative models (DeepSeek-R1) achieve higher composite scores (0.847) but lower faithfulness (0.678)
  - Real-time latency vs. multi-stage refinement: Three-stage generation improves quality but adds latency

- Failure signatures:
  - Faithfulness drops despite KG grounding: Indicates model creativity overriding constraints (observed with DeepSeek-R1 in Table II)
  - Contextual recall < 0.98: Suggests KG construction gaps or entity extraction failures
  - "I do not know" over-triggering: Refinement stage may be overly conservative or retrieval failing to surface relevant context

- First 3 experiments:
  1. **Ablate KG component**: Run DO-RAG with vector-only retrieval (α = 1.0) on the 245-question benchmark; expect contextual recall drop from 1.0 to ~0.97 per Table II.
  2. **Calibrate α parameter**: Sweep α ∈ {0.2, 0.4, 0.6, 0.8} on a held-out subset; plot composite score vs. α to identify domain-optimal fusion balance.
  3. **Model faithfulness comparison**: Run identical retrieval with DeepSeek-R1, DeepSeek-V3, and GPT-4o-mini; correlate faithfulness scores with model creativity metrics to validate the creativity-faithfulness tradeoff hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can stricter prompt engineering mitigate hallucinations in highly creative reasoning models (e.g., DeepSeek-R1) within the DO-RAG framework without degrading answer fluency?
- Basis in paper: [explicit] The authors note in the Discussion that "creative models like DeepSeek-R1 occasionally introduced hallucinations despite knowledge graph grounding" and explicitly propose "enhancing hallucination mitigation through stricter prompt engineering" in Future Work.
- Why unresolved: While the framework includes a "grounded refinement" step, the intrinsic creativity of certain models still overrides this grounding, leading to factual errors. It is unclear if prompt engineering alone can balance creativity with strict factual adherence.
- What evidence would resolve it: A comparative ablation study measuring Faithfulness and Answer Relevancy scores for DeepSeek-R1 under varying prompt strictness levels versus the current baseline configuration.

### Open Question 2
- Question: How can distributed processing and adaptive caching be optimized to reduce the computational overhead of DO-RAG’s multi-agent extraction pipeline for real-time, large-scale deployments?
- Basis in paper: [explicit] The Discussion identifies "computational overhead of multi-agent extraction and hybrid retrieval" as a limitation for "real-time updates in large-scale deployments," and Future Work commits to exploring "distributed processing and adaptive caching mechanisms."
- Why unresolved: The current architecture relies on a hierarchical, multi-agent pipeline (High-Level, Mid-Level, Low-Level, Covariate agents) which is computationally intensive, making real-time scaling difficult.
- What evidence would resolve it: Performance benchmarks showing latency and resource utilization of the DO-RAG pipeline when implemented with specific distributed computing or caching strategies compared to the current single-node implementation.

### Open Question 3
- Question: How does DO-RAG performance hold when evaluated against diverse, edge-case queries and domains significantly different from the tested database and electrical engineering datasets?
- Basis in paper: [explicit] The Limitations section states that the "dataset, limited to 245 questions per domain, may not capture rare or edge-case queries, potentially limiting generalizability," with Future Work proposing the expansion of datasets to "diverse, edge-case queries."
- Why unresolved: High performance on 245 expert-curated questions per domain demonstrates efficacy in standard scenarios but leaves the system's robustness against ambiguous or rare inputs unproven.
- What evidence would resolve it: Evaluation results using a new benchmark dataset specifically designed to include "edge-case" queries (e.g., ambiguous phrasing, out-of-distribution topics) across additional specialized domains.

## Limitations
- Computational overhead from multi-agent extraction and three-stage generation poses scalability challenges for real-time applications
- Creative models like DeepSeek-R1 achieve higher composite scores but lower faithfulness, indicating grounding mechanisms may not fully mitigate hallucination risks
- Framework effectiveness depends on document quality—poor OCR or ambiguous terminology could propagate errors through extraction pipeline

## Confidence
- High Confidence: Contextual recall (1.0) and answer relevancy (>94%) metrics, as these are directly measured and reported
- Medium Confidence: Faithfulness improvements via refinement, given the weak corpus evidence for hallucination reduction specifically in refinement-stage generation
- Low Confidence: Long-term scalability of multi-agent extraction and the framework's generalizability to domains outside database and electrical engineering

## Next Checks
1. **Ablate KG component**: Run DO-RAG with vector-only retrieval (α = 1.0) on the 245-question benchmark; expect contextual recall drop from 1.0 to ~0.97 per Table II
2. **Calibrate α parameter**: Sweep α ∈ {0.2, 0.4, 0.6, 0.8} on a held-out subset; plot composite score vs. α to identify domain-optimal fusion balance
3. **Model faithfulness comparison**: Run identical retrieval with DeepSeek-R1, DeepSeek-V3, and GPT-4o-mini; correlate faithfulness scores with model creativity metrics to validate the creativity-faithfulness tradeoff hypothesis