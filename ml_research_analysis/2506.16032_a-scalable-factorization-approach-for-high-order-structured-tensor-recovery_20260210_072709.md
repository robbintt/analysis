---
ver: rpa2
title: A Scalable Factorization Approach for High-Order Structured Tensor Recovery
arxiv_id: '2506.16032'
source_url: https://arxiv.org/abs/2506.16032
tags:
- tensor
- tucker
- condition
- decomposition
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for the factorization approach
  to high-order structured tensor recovery problems. The key idea is to optimize orthonormal
  factors directly on the Stiefel manifold using Riemannian gradient descent (RGD),
  which addresses the computational and memory challenges of traditional tensor optimization
  methods.
---

# A Scalable Factorization Approach for High-Order Structured Tensor Recovery

## Quick Facts
- arXiv ID: 2506.16032
- Source URL: https://arxiv.org/abs/2506.16032
- Reference count: 40
- Authors: Zhen Qin; Michael B. Wakin; Zhihui Zhu
- Primary result: Riemannian gradient descent on Stiefel manifold achieves linear convergence for high-order tensor recovery with polynomial scaling in tensor order

## Executive Summary
This paper presents a unified factorization framework for high-order structured tensor recovery that overcomes computational and memory challenges of traditional methods. The key innovation is optimizing orthonormal factors directly on the Stiefel manifold using Riemannian gradient descent (RGD), which addresses scalability issues for high-order tensors. The authors establish theoretical guarantees showing linear convergence to the ground-truth tensor when properly initialized, with both initialization requirements and convergence rates scaling polynomially with tensor order N - a significant improvement over existing results showing exponential scaling.

The framework applies to orthogonal CP, Tucker, and tensor-train decompositions, and when applied to tensor sensing problems, requires only 4r-RIP compared to (N+3)r-RIP in previous work. Numerical experiments on tensor sensing and completion problems confirm the theoretical findings, demonstrating stable linear convergence rates across varying tensor orders.

## Method Summary
The method optimizes orthonormal factors directly on the Stiefel manifold using Riemannian gradient descent, addressing computational challenges of traditional tensor optimization. For a tensor of order N, the approach factorizes it as X = X(Z₁,...,Z_Ñ) with Z_i ∈ R^{I_i×r_i} satisfying Z_i^T Z_i = I. The optimization problem minimizes H(Z₁,...,Z_Ñ) = h([Z₁,...,Z_Ñ]) subject to these orthonormal constraints. The Riemannian gradient descent algorithm uses polar decomposition retraction for updates, with step size μ = 0.5 and γ = σ²(X*). Spectral initialization is performed via HOSVD or TT-SVD on the measurement operator applied to observations. The framework handles Tucker and Tensor Train decompositions with provable linear convergence under a Riemannian regularity condition, requiring fewer RIP measurements than previous approaches.

## Key Results
- RGD on Stiefel manifold achieves linear convergence for high-order tensor recovery
- Both initialization requirement and convergence rate scale polynomially with tensor order N
- Requires only 4r-RIP for tensor sensing (compared to (N+3)r-RIP in previous work)
- Convergence rates remain stable as tensor order increases, confirmed by numerical experiments

## Why This Works (Mechanism)
The Riemannian gradient descent approach directly optimizes on the Stiefel manifold, avoiding the computational complexity of optimizing in the full tensor space. By establishing a Riemannian regularity condition for the factorized objective function, the authors prove that RGD converges linearly to the ground-truth tensor when initialized within a specific radius. The key insight is that this initialization radius and convergence rate scale polynomially with tensor order rather than exponentially, making the method scalable to high-order tensors. The use of polar decomposition retraction ensures factors remain on the Stiefel manifold throughout optimization, maintaining the orthonormal structure essential for the factorization.

## Foundational Learning
- **Stiefel Manifold**: Set of orthonormal matrices Z where Z^T Z = I; needed to maintain factor orthogonality during optimization; check by verifying Z_i^T Z_i = I after each update
- **Riemannian Gradient Descent**: Gradient descent adapted to manifolds using tangent spaces and retractions; needed for optimization on non-Euclidean spaces; check by monitoring monotonic decrease in objective function
- **RIP (Restricted Isometry Property)**: Measurement operator property ensuring stable recovery; needed to guarantee sufficient measurement information; check by computing δ_3r and verifying it's below threshold
- **Spectral Initialization**: HOSVD/TT-SVD based initialization from measurements; needed to start within convergence radius; check by computing initialization error relative to ground truth
- **Riemannian Regularity Condition**: Technical condition ensuring descent direction; needed for linear convergence proof; check by verifying gradient bounds hold during optimization
- **Polar Decomposition Retraction**: Method to map updates back to manifold; needed to maintain orthonormality; check by measuring deviation from Stiefel manifold

## Architecture Onboarding

**Component Map**: Measurement operator A → Spectral initialization → RGD on Stiefel manifold → Recovered tensor X

**Critical Path**: 1) Generate ground truth tensor X* → 2) Create measurements y = A(X*) → 3) Compute spectral initialization → 4) Run RGD iterations → 5) Return recovered tensor

**Design Tradeoffs**: 
- Manifold optimization vs. Euclidean optimization: avoids full tensor space but requires retraction operations
- Spectral initialization vs. random initialization: provides convergence guarantees but needs RIP conditions
- Fixed step size vs. adaptive: simpler implementation but may be suboptimal for all problem instances

**Failure Signatures**:
- Initialization outside RRC radius a₁: initialization error exceeds δ_3r(1+√N)
- Step size too large: dist² increases or factors leave Stiefel manifold
- Numerical instability: polar decomposition fails or gradient projections become ill-conditioned

**3 First Experiments**:
1. Verify spectral initialization error is within theoretical bounds for varying RIP constants
2. Test RGD convergence on small tensors with known ground truth and monitor Stiefel manifold distance
3. Compare recovery accuracy with varying measurement counts to verify 4r-RIP requirement

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Requires specific RIP conditions that may not hold for all measurement designs
- Numerical tolerances for manifold operations and convergence criteria not fully specified
- Limited comparison with specialized algorithms for specific tensor structures
- Practical computational efficiency relative to other methods needs further investigation

## Confidence
- Theoretical framework and convergence proofs: High
- Experimental reproducibility: Medium (due to unspecified implementation details)

## Next Checks
1. Verify numerical stability of polar decomposition retraction by testing with increasingly ill-conditioned gradient projections and measuring deviation from Stiefel manifold
2. Benchmark recovery accuracy and convergence speed against state-of-the-art tensor recovery algorithms on standardized datasets with varying tensor orders
3. Test sensitivity to initialization quality by varying the RIP constant δ₃ᵣ and measuring the probability of spectral initialization falling within the RRC radius a₁