---
ver: rpa2
title: Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation
  Disentanglement
arxiv_id: '2501.08566'
source_url: https://arxiv.org/abs/2501.08566
tags:
- speech
- speaker
- zero-shot
- content
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high computational cost and instability
  issues in existing zero-shot TTS systems by proposing a lightweight and stable architecture.
  The core method employs a two-stage self-distillation framework that disentangles
  linguistic content from speaker attributes at the data level, while using a multi-level
  speaker representation approach to model global timbre and temporal style features
  separately.
---

# Towards Lightweight and Stable Zero-shot TTS with Self-distilled Representation Disentanglement

## Quick Facts
- arXiv ID: 2501.08566
- Source URL: https://arxiv.org/abs/2501.08566
- Reference count: 0
- Primary result: 22.5M parameters, 0.13 RTF on CPU, CER of 1.8, SIM of 0.73

## Executive Summary
This paper proposes a lightweight and stable zero-shot TTS system that addresses the high computational cost and instability of existing autoregressive models. The core innovation is a two-stage self-distillation framework that disentangles linguistic content from speaker attributes at the data level, combined with a multi-level speaker representation approach. The system achieves superior efficiency with 22.5M parameters while maintaining competitive audio quality, demonstrating RTFs of 0.13 and 0.012 on CPU and GPU respectively.

## Method Summary
The method employs a two-stage self-distillation framework where a teacher model generates synthetic speech pairs that preserve text content but swap speaker identity. A student model is then trained on these paired samples to learn speaker-invariant content representations. The architecture uses a multi-level speaker representation approach with a frozen pre-trained timbre encoder for global speaker embeddings and a trainable style encoder for temporal prosodic features. Content extraction combines linguistic and mel-spectrogram encoding via VP-Flow, while speaker adaptation uses cross-attention to align style features with content before generation.

## Key Results
- Achieves 22.5M parameters compared to 200M+ in baseline models
- Real-time factors of 0.13 (CPU) and 0.012 (GPU) demonstrate significant computational efficiency
- Maintains excellent content integrity with CER of 1.8 and MOScon of 4.43
- Achieves competitive speaker similarity with SIM of 0.73 and MOSsim of 3.31

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation at data level reduces content-speaker coupling in content encoder
- Mechanism: Teacher generates synthetic speech pairs preserving text content but swapping speaker identity. Student training on these pairs forces content encoder to extract speaker-invariant representations since same linguistic content appears with multiple speaker characteristics
- Core assumption: Teacher has sufficient zero-shot capability to produce speaker-swapped samples preserving phoneme-level temporal alignment when duration is explicitly controlled
- Evidence: Two-stage self-distillation framework constructs parallel data pairs for disentangling linguistic content and speakers; explicit specification of duration predictor output during inference ensures temporal alignment
- Break condition: Teacher artifacts or mispronunciations propagate to student; σ=0.8 blend ratio may need adjustment for lower-quality teacher outputs

### Mechanism 2
- Claim: Multi-level speaker representation with global timbre and temporal style separation improves similarity while reducing complexity
- Mechanism: Frozen pre-trained timbre encoder provides global speaker embeddings; trainable style encoder with positional encoding captures time-varying attributes. Variation adapter uses cross-attention to align style features with content
- Core assumption: Speaker identity can be decomposed into global timbre and temporal style components modeled independently without interference
- Evidence: Multi-level speaker representation approach models global timbre and temporal style features separately; frozen timbre encoder requires no optimization while style encoder includes positional encoding
- Break condition: Prompt speech with unusual prosody may capture speaker-inappropriate temporal features conflicting with global timbre

### Mechanism 3
- Claim: Combining linguistic encoder output with mel-encoded speech via VP-Flow creates richer content representation than text-only conditioning
- Mechanism: Text and mel-spectrogram are encoded to phoneme-level representations. VP-Flow models conditional distribution P(emel|eling), and fused content representation is fed to speaker adaptation. Captures information not present in text
- Core assumption: Mel encoder can extract speaker-invariant content features from source speech, and residual speaker information leakage can be mitigated by self-distillation
- Evidence: Model extracts linguistic content and various speaker attributes from source and prompt speech respectively; self-distillation framework addresses content-speaker coupling issue
- Break condition: Source speech with unusual acoustic conditions may extract spurious features unrelated to linguistic content

## Foundational Learning

- Concept: Variational Autoencoders (VAE) with Flow-based posterior estimation
  - Why needed: VP-Flow component models P(emel|eling) to create latent content space; understanding KL divergence loss and volume-preserving transformations is essential for debugging reconstruction quality
  - Quick check: Can you explain why standard VAE might produce over-smoothed outputs and how normalizing flows can mitigate this?

- Concept: Contrastive learning objectives for speaker representation
  - Why needed: Cyclic contrastive loss enforces reconstructed speech matches prompt speaker while diverging from other speakers in batch; directly impacts speaker similarity metrics
  - Quick check: Given batch of B samples, how does contrastive loss behave if two utterances in batch are from same speaker?

- Concept: Adaptive Instance Normalization (AdaIN) for speaker-conditioned generation
  - Why needed: Mel decoder uses AdaIN to inject timbre representations into generation process; understanding how AdaIN modulates feature statistics is critical for debugging speaker transfer quality
  - Quick check: What happens to output if speaker embedding space has poor coverage of test speakers?

## Architecture Onboarding

- Component map: Source Mel → Style Encoder (with positional encoding) → Variation Adapter (cross-attention + predictors) + Prompt Mel → Timbre Encoder (frozen ERes2NetV2) → Linear projection; Linguistic Encoder → Mel Encoder (2D ResBlocks + pooling) → VP-Flow → Fused content representation; Fused content + Style features → Mel Decoder (ResNet with AdaIN conditioned on timbre) → Vocoder (NFS-HiFiGAN)

- Critical path: Self-distillation coefficient σ controls proportion of synthetic vs. real samples (σ=0.8 default). Cyclic contrastive loss relies on batch diversity for effective negative sampling. Duration predictor outputs must be explicitly specified during teacher inference to ensure temporal alignment of parallel pairs.

- Design tradeoffs: Frozen timbre encoder vs. joint training (reduces parameters but may limit adaptation to domain-specific speakers). Separate gradient computation for variance predictors (improves stability but may reduce end-to-end optimization). Content-speaker disentanglement via data augmentation vs. architectural constraints (self-distillation is model-agnostic but requires two-stage training).

- Failure signatures: High CER with good speaker similarity indicates content encoder may be capturing speaker information; increase σ or check teacher quality. Low SIM scores suggest checking batch diversity for contrastive loss, verifying prompt speech quality, or considering fine-tuning timbre encoder projection layer. Temporal misalignment in generated speech indicates verifying duration predictor usage during inference and checking ground-truth duration extraction consistency.

- First 3 experiments: 1) Ablate self-distillation (σ=0 vs. σ=0.8) on held-out speaker set and measure SIM/CER gap to quantify disentanglement benefit. 2) Replace frozen ERes2NetV2 with jointly-trained speaker encoder and compare speaker similarity on out-of-domain speakers to assess generalization tradeoffs. 3) Evaluate RTF and audio quality degradation when reducing model parameters to find efficiency-performance frontier for target deployment hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does self-distillation framework performance scale when trained on datasets significantly larger than 531 hours used in study?
- Basis: Table 2 highlights vast discrepancy in training data scale between proposed system (531 hours) and baselines like CosyVoice (173K hours), suggesting current results are limited to low-resource scenarios
- Why unresolved: Authors focus on data efficiency but do not test if lightweight architecture saturates or continues to improve when exposed to massive datasets used by large-scale baselines
- What evidence would resolve it: Objective and subjective metrics (SIM, CER) reported for student model after training on dataset exceeding 50,000 hours

### Open Question 2
- Question: Can multi-level speaker representation approach be refined to close performance gap in speaker similarity compared to large-scale models?
- Basis: While model excels in efficiency and content integrity, Table 1 shows it trails state-of-the-art CosyVoice in speaker similarity (0.73 vs 0.84)
- Why unresolved: Paper claims system "matches the Baseline" generally, but specific similarity gap suggests lightweight timbre encoder or style encoder may lack capacity to capture full speaker identity nuances that larger models capture
- What evidence would resolve it: Ablation studies showing improved SIM scores resulting from increasing capacity of timbre/style encoders specifically, rather than whole model

### Open Question 3
- Question: To what extent do artifacts or errors from teacher model propagate to student model during self-distillation process?
- Basis: Section 3.2 notes student model is trained using synthetic speech generated by teacher, which acts as ground truth for content
- Why unresolved: If teacher generates subtle prosodic errors or artifacts during synthesis of parallel data pairs, student may learn to mimic these imperfections, potentially limiting upper bound of synthesis quality
- What evidence would resolve it: Comparative analysis of student models trained on teacher-generated data versus student models fine-tuned with small percentage of human ground-truth data to measure performance delta

## Limitations
- Self-distillation framework effectiveness depends heavily on teacher model's zero-shot performance, but teacher performance metrics before self-distillation are not provided
- Frozen ERes2NetV2 timbre encoder assumes sufficient speaker coverage in pre-training corpus, which may not hold for diverse speaker populations or underrepresented accents
- Optimal self-distillation coefficient σ may vary with dataset size and quality; 80/20 synthetic-to-real ratio appears effective but requires validation across different conditions

## Confidence
- **High confidence**: Lightweight architecture claims (22.5M parameters vs. 200M+ baselines) are directly verifiable from model specifications. Computational efficiency metrics (RTF of 0.13 CPU, 0.012 GPU) are well-defined and reproducible.
- **Medium confidence**: Speaker similarity results (SIM of 0.73, MOSsim of 3.31) are plausible given multi-level speaker representation approach, but depend heavily on batch diversity for contrastive loss and quality of frozen timbre encoder's speaker space.
- **Low confidence**: Content integrity claims (CER of 1.8, MOScon of 4.43) are most sensitive to implementation details of self-distillation process, particularly whether teacher model maintains phoneme-level alignment when generating synthetic speech with swapped speaker identities.

## Next Checks
1. **Ablation of self-distillation coefficient**: Train student models with σ values ranging from 0 to 1.0 and measure degradation in SIM and CER metrics to quantify precise contribution of data-level disentanglement versus architectural design.

2. **Cross-dataset generalization**: Evaluate pre-trained ERes2NetV2 timbre encoder on held-out speaker set from different corpus to assess whether speaker similarity degrades significantly when test distribution differs from training data.

3. **Teacher model quality assessment**: Measure teacher model's zero-shot performance (SIM, CER) on validation set before self-distillation to establish quality threshold required for effective student training and identify potential failure modes in synthetic data generation process.