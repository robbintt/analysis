---
ver: rpa2
title: 'S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward
  Models'
arxiv_id: '2509.22099'
source_url: https://arxiv.org/abs/2509.22099
tags:
- arxiv
- reward
- preprint
- judging
- solving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a "solve-to-judge gap" in generative reward
  models (GRMs), where models fail to correctly evaluate problems they can solve (14%-37%
  error rate). The authors propose S2J, which addresses this by requiring models to
  first solve the query internally before judging, then rewarding both solving and
  judging correctness.
---

# S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models

## Quick Facts
- arXiv ID: 2509.22099
- Source URL: https://arxiv.org/abs/2509.22099
- Authors: Shaoning Sun; Jiachen Yu; Zongqi Wang; Xuewei Yang; Tianle Gu; Yujiu Yang
- Reference count: 36
- One-line primary result: S2J reduces solve-to-judge gap by 16.2% and improves judgment accuracy by 5.8% using 20K samples

## Executive Summary
This paper addresses a fundamental limitation in generative reward models (GRMs) where models fail to correctly evaluate problems they can solve, termed the "solve-to-judge gap." The authors propose S2J, a training framework that requires models to first solve queries internally before judging, then rewards both solving and judging correctness. S2J achieves state-of-the-art results among GRMs on the same base model while using significantly less training data through a self-evolving process without distillation from more powerful models.

## Method Summary
S2J trains GRMs using reinforcement learning with a composite reward combining solving and judging rewards. The model generates trajectories containing both a self-generated solution and a judgment prediction. For objective tasks, solving correctness is verified via rule-based verifiers; for subjective tasks, an auxiliary scalar RM provides partial ordering. The training uses DAPO optimization with 20K total samples from Math-DPO, WebInstruct-verified, and HelpSteer3 datasets, achieving improvements while using significantly less data than prior approaches.

## Key Results
- Reduces solve-to-judge gap (P(j=0|s=1)) by 16.2% across four benchmarks
- Improves judgment accuracy by 5.8% compared to baseline GRMs
- Achieves SOTA performance using only 20K training samples vs 72K-112K in prior work
- Maintains effectiveness on both objective and subjective tasks

## Why This Works (Mechanism)

### Mechanism 1: Solve-to-Judge Gap Reduction via Unified Reward
Combining solving and judgment rewards explicitly links problem-solving knowledge to evaluation capability, reducing the gap where models fail to judge problems they can solve. The composite reward R(τ) = R_solve(τ) + R_judge(τ) creates gradient signals that reinforce the connection between correct internal solving and correct external judging, rather than optimizing them independently.

### Mechanism 2: Judging-Time Solving as Grounding Signal
Requiring models to generate their own solution before judging provides an internal reference that reduces reliance on superficial heuristics. By forcing trajectory τ = (ŷ, c, l̂) where ŷ is the self-generated solution, the model must engage in genuine problem-solving before evaluation, making judgment errors more detectable when they contradict the internal solution.

### Mechanism 3: Objective/Subjective Task Differentiation
Using rule-based verifiers for objective tasks and auxiliary scalar RMs for subjective tasks enables S2J to handle both domains without sacrificing either. For objective tasks (math, QA), Verifier(ŷ, y) provides ground-truth signal. For subjective tasks, RM_aux scores solutions relative to preference pairs, with rewards only when RM_aux correctly orders the original pair.

## Foundational Learning

- **Generative Reward Models (GRMs) vs Scalar RMs**
  - Why needed here: S2J operates on GRMs that generate reasoning chains, not scalar scores. Understanding this distinction is essential for grasping why internal solving can be incorporated into the trajectory.
  - Quick check question: Can you explain why a scalar RM cannot implement S2J's judging-time solving mechanism?

- **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: S2J builds on RLVR frameworks (GRPO, DAPO, RLOO). The verifiable reward comes from judgment correctness plus solving correctness.
  - Quick check question: How does adding R_solve change the reward landscape compared to standard RLVR that only uses judgment outcome?

- **Conditional Probability P(j=0|s=1)**
  - Why needed here: This metric quantifies the solve-to-judge gap. Understanding it is critical for interpreting Table 2 and measuring whether S2J actually works.
  - Quick check question: If P(j=0|s=1) = 21.1% after S2J training vs 37.3% before, what does this tell you about the model's ability to leverage its solving knowledge?

## Architecture Onboarding

- **Component map**: Input layer (preference triplets) -> Generation layer (trajectory τ = (ŷ, c, l̂)) -> Reward computation layer (R_solve via Verifier/RM_aux; R_judge via label matching) -> RL optimizer (DAPO/GRPO)

- **Critical path**:
  1. Prompt engineering (Figure 3 template) to enforce solve-first format
  2. Rollout generation with temperature 1.0, 16 responses per prompt
  3. Dual reward computation (rule-based for objective, RM_aux for subjective)
  4. DAPO optimization with learning rate 1e-6, 300 steps

- **Design tradeoffs**:
  - Using RM_aux introduces dependency on external model quality; fallback to R_judge-only when RM_aux ordering is incorrect mitigates this
  - 20K training samples vs 72K-112K in prior work: efficiency gain assumes higher-quality signal from dual rewards
  - Self-evolution (no distillation) trades potential ceiling performance for reproducibility and lower resource requirements

- **Failure signatures**:
  - High P(j=0|s=1) persists → check if prompt template correctly enforces solve-first behavior
  - Subjective task performance drops → verify RM_aux quality on held-out preference data
  - Training instability → examine reward variance; R_solve=0 cases may create sparse gradients

- **First 3 experiments**:
  1. **Ablation on reward components**: Train with R_solve only vs R_judge only vs both to validate dual-reward necessity
  2. **Cross-domain generalization**: Train on Math-DPO only, evaluate on PPE Preference to confirm subjective task degradation without HelpSteer3 data
  3. **Gap metric tracking**: Monitor P(j=0|s=1) throughout training steps to identify when convergence occurs and whether early stopping is viable

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is S2J's performance on subjective tasks to the specific choice and capability of the auxiliary scalar reward model ($RM_{aux}$)? The authors state they employ a separate scalar reward model for subjective tasks to determine solving quality (Equation 7) and specifically use Skywork-Reward-V2-Llama-3.1-8B. The paper evaluates the final S2J performance but does not ablate the impact of the auxiliary RM's quality. An ablation study swapping $RM_{aux}$ for different scalar reward models of varying sizes and accuracies would resolve this.

### Open Question 2
Does the effectiveness of the "solve-to-judge" reward diminish for significantly larger base models (e.g., 70B+) compared to the 7B model tested? The authors analyze the gap in 32B models and train a 7B model. However, Figure 1b suggests the solve-to-judge gap is naturally smaller in larger models. Training S2J on a 70B+ parameter base model and comparing the relative reduction in the solve-to-judge gap against the 7B baseline would resolve this.

### Open Question 3
How does S2J affect judgment accuracy on queries where the model is unable to solve the problem correctly ($P(s=0)$)? While S2J improves performance on solvable problems, the paper does not isolate performance on "unsolvable" queries to ensure the added objective does not confuse the model or degrade performance on tasks beyond its reasoning capacity. A stratified evaluation of judgment accuracy on datasets partitioned by the base model's solving success rate would resolve this.

## Limitations

- The mechanism claim that internal solving grounds judgment in deeper understanding lacks direct validation beyond stated performance improvements
- Use of auxiliary scalar RM for subjective tasks introduces potential noise sources not thoroughly analyzed for reliability
- Paper doesn't explore whether solve-to-judge gap reduction translates to practical improvements in real-world preference alignment scenarios

## Confidence

- **High confidence**: The empirical results showing 5.8% judgment accuracy improvement and 16.2% gap reduction are well-supported by the experimental data across four benchmarks.
- **Medium confidence**: The mechanism claim that internal solving grounds judgment in deeper understanding is plausible but lacks direct validation beyond the stated performance improvements.
- **Medium confidence**: The efficiency claim (20K samples vs 72K-112K in prior work) is supported by data but assumes the dual-reward signal quality rather than establishing it definitively.

## Next Checks

1. Conduct ablation studies isolating R_solve vs R_judge effects to quantify the marginal contribution of each reward component beyond their combined effect.
2. Implement cross-task generalization experiments where models trained on objective tasks (Math-DPO) are evaluated on subjective tasks (PPE Preference) to determine if the solve-to-judge mechanism transfers across domains.
3. Analyze the quality of the auxiliary scalar RM's partial ordering on held-out preference data to establish the reliability of subjective task rewards and identify potential systematic biases.