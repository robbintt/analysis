---
ver: rpa2
title: 'Four Quadrants of Difficulty: A Simple Categorisation and its Limits'
arxiv_id: '2601.01488'
source_url: https://arxiv.org/abs/2601.01488
tags:
- difficulty
- task-agnostic
- human
- task-dependent
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a four-quadrant framework for analysing difficulty
  signals in curriculum learning (CL) for NLP, distinguishing between human vs. model
  and task-agnostic vs.
---

# Four Quadrants of Difficulty: A Simple Categorisation and its Limits

## Quick Facts
- arXiv ID: 2601.01488
- Source URL: https://arxiv.org/abs/2601.01488
- Reference count: 15
- Primary result: Task-agnostic linguistic features fail to predict model learning difficulty; only task-dependent signals (human disagreement, training dynamics) align meaningfully

## Executive Summary
This work introduces a four-quadrant framework for analyzing difficulty signals in curriculum learning for NLP, distinguishing between human vs. model and task-agnostic vs. task-dependent difficulty. The study systematically tests correlations across these quadrants on the SNLI dataset. Results show that task-agnostic difficulty measures (e.g., linguistic complexity) behave largely independently and fail to predict task-dependent difficulty (e.g., model learning difficulty or annotation entropy). Only task-dependent signals—human disagreement and model learning dynamics—show meaningful alignment. This challenges the common assumption that task-agnostic heuristics reflect model learning difficulty and highlights the need for lightweight, task-dependent difficulty estimators for more effective CL.

## Method Summary
The study analyzes difficulty signals across four quadrants using the SNLI dataset: TA-H (human-annotated linguistic features like length, word rarity, readability), TA-M (perplexity from pretrained models), TD-H (annotation entropy from 4 human labels), and TD-M (training dynamics including confidence, variability, correctness, and loss across epochs). The authors finetune BERT-base, RoBERTa-base, and GPT-2-base on SNLI with specified hyperparameters, logging metrics at 12 checkpoints across 5 epochs with 10 random seeds per architecture. They compute Pearson correlations within and across quadrants, perform regression analysis, and compare distributional overlap between easy and ambiguous samples identified through dataset cartography.

## Key Results
- Task-agnostic features show virtually no correlation with task-dependent difficulty signals (r < 0.2)
- Human annotation disagreement aligns with model learning difficulty, reflecting shared dependence on label uncertainty
- Linguistic difficulty is multi-dimensional, with different features capturing largely uncorrelated aspects
- Distributional overlap between easy and ambiguous samples is substantial on task-agnostic features
- Only task-dependent signals (TD-H and TD-M) show meaningful alignment

## Why This Works (Mechanism)

### Mechanism 1: Task-Agnostic Signals Fail to Predict Model Learning Difficulty
Surface-level linguistic features (sentence length, syntactic tree depth, readability scores) operate orthogonally to the learning dynamics neural models experience. The model's task-specific learning behavior depends on label uncertainty and feature-label interactions, not input complexity alone. This orthogonality challenges the assumption that model learning difficulty can be derived from general linguistic properties without task context.

### Mechanism 2: Human Annotation Disagreement Aligns with Model Learning Difficulty
Annotation entropy captures genuine task ambiguity—instances with unclear ground truth cause both human uncertainty and model instability during training. This shared dependence on label uncertainty creates alignment between TD-H and TD-M signals, making human disagreement a meaningful predictor of model difficulty.

### Mechanism 3: Linguistic Difficulty is Multi-Dimensional, Not Unitary
Different task-agnostic linguistic measures capture distinct, largely uncorrelated aspects of difficulty rather than a single underlying construct. Linguistic difficulty spans lexical, syntactic, and conceptual dimensions, each driven by different cognitive and linguistic processes, preventing reduction to a single scalar measure.

## Foundational Learning

- **Concept: Curriculum Learning (CL)**
  - Why needed: The entire framework addresses how to estimate difficulty for CL scheduling
  - Quick check: Can you explain why CL requires both a difficulty function AND a scheduler, rather than just sorting by difficulty once?

- **Concept: Training Dynamics / Dataset Cartography**
  - Why needed: TD-M metrics are derived from tracking model behavior across training epochs
  - Quick check: If a sample has high confidence but high variability across epochs, what might that indicate about its learning behavior?

- **Concept: Annotation Entropy / Human Label Variation**
  - Why needed: TD-H is operationalized through inter-annotator disagreement
  - Quick check: Given labels [A, A, B, B] for one instance and [A, A, A, A] for another, which has higher annotation entropy and why might this matter for model training?

## Architecture Onboarding

- **Component map:**
  Input Data -> TA-H Calculator (pre-processing) -> TA-M Calculator -> TD-H Calculator -> TD-M Tracker -> Difficulty Scores -> CL Scheduler -> Training Loop

- **Critical path:**
  1. Determine which quadrant(s) of difficulty signals are available for your dataset
  2. If using TD-M: instrument training loop to log per-sample metrics at multiple checkpoints
  3. If using TD-H: verify multi-annotator labels exist; compute entropy
  4. Validate correlations before assuming TA signals will work for CL

- **Design tradeoffs:**
  - TA-H: Computed once, no training needed, but empirically uncorrelated with model difficulty (R² < 0.1)
  - TA-M (Perplexity): Requires pre-trained model, still shows near-zero correlation with TD signals
  - TD-H: Most informative but expensive; requires 3+ annotations per instance
  - TD-M: Accurate but requires initial training pass (chicken-and-egg for CL)

- **Failure signatures:**
  - CL with TA-H heuristics shows no improvement over random/shuffled training
  - High-perplexity samples turn out to be easy-to-learn (low loss, high confidence)
  - Easy-to-learn and ambiguous samples have overlapping distributions on all TA features

- **First 3 experiments:**
  1. Compute Pearson correlations between your available TA features and TD-M metrics. If r < 0.2 across the board, TA-based CL is unlikely to help via difficulty estimation.
  2. Identify easy vs. ambiguous samples via dataset cartography, then plot histograms of TA features. Substantial overlap indicates TA features cannot separate difficulty classes.
  3. Fit a linear or tree-based model predicting TD-M from all TA features. If R² < 0.1, TA features lack predictive power for model learning difficulty regardless of scheduler design.

## Open Questions the Paper Calls Out

### Open Question 1
Can inexpensive, pre-computed features be engineered to reliably approximate task-dependent difficulty (annotation entropy or learning dynamics) without requiring prior model training or multi-annotator data? The conclusion explicitly calls for developing new, inexpensive ways to approximate task-dependent difficulty at pre-processing time to overcome the cost limitations of annotation entropy.

### Open Question 2
If task-agnostic linguistic heuristics do not reflect model learning difficulty, what specific mechanisms (e.g., distributional reshaping or noise reduction) drive the success of heuristic-based Curriculum Learning? The discussion notes that the success of task-agnostic CL heuristics must stem from mechanisms other than accurate difficulty estimation, but does not isolate what these mechanisms are.

### Open Question 3
Do the orthogonal relationships between task-agnostic and task-dependent difficulty signals persist across different NLP tasks (e.g., generation, QA) and linguistic structures? The empirical study is restricted to the SNLI dataset; the authors acknowledge that SNLI label disagreement is likely driven by inherent ambiguity specific to the task, suggesting results may vary for tasks with different difficulty profiles.

## Limitations
- Analysis constrained to single NLP task (SNLI), limiting generalizability to other domains
- Requires multi-annotator labels for TD-H computation, which is not standard for many NLP datasets
- Specific implementation details for psycholinguistic features and SLE metrics are underspecified
- Assumes human disagreement reflects genuine ambiguity rather than guideline issues

## Confidence
- **High confidence:** Task-agnostic features show near-zero correlation with task-dependent difficulty signals
- **Medium confidence:** Human annotation disagreement meaningfully predicts model learning difficulty
- **Medium confidence:** Linguistic difficulty is multi-dimensional with low inter-feature correlation

## Next Checks
1. Apply the four-quadrant framework to a morphologically rich language task or syntactic task to test whether TA-H features show stronger alignment with TD-M in tasks with tighter linguistic-surface-to-label coupling.
2. Independently reproduce the easy vs. ambiguous sample separation using dataset cartography methods on SNLI, then verify that TA feature distributions for these groups show the reported overlap patterns.
3. Implement and test lightweight task-dependent difficulty estimators (e.g., ensemble predictions from small proxy models, or sentence embedding clustering) to evaluate whether they can approximate TD-M signals without full training dynamics instrumentation.