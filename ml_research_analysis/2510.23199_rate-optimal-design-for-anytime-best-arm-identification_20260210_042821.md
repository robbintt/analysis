---
ver: rpa2
title: Rate-optimal Design for Anytime Best Arm Identification
arxiv_id: '2510.23199'
source_url: https://arxiv.org/abs/2510.23199
tags:
- algorithm
- algorithms
- page
- tracking
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of best arm identification (BAI)
  in multi-armed bandits under a fixed sampling budget. Existing methods require advance
  knowledge of the total budget or rely on sample-discard strategies, limiting practicality.
---

# Rate-optimal Design for Anytime Best Arm Identification

## Quick Facts
- **arXiv ID:** 2510.23199
- **Source URL:** https://arxiv.org/abs/2510.23199
- **Reference count:** 40
- **Primary result:** Achieves rate-optimal fixed-budget best arm identification without requiring advance knowledge of the total budget.

## Executive Summary
This paper addresses the problem of best arm identification (BAI) in multi-armed bandits under a fixed sampling budget. Existing methods typically require advance knowledge of the total budget or rely on sample-discard strategies, limiting their practicality. The authors propose a new framework that achieves rate-optimal performance without these constraints by analyzing the problem as a one-shot game and introducing an adaptive allocation scheme. The key innovation is Almost Tracking, a closed-form algorithm that achieves minimax optimality (up to constant factors) for the standard H1 risk measure without requiring the budget horizon.

The method works by continuously adjusting sampling proportions to track an optimal allocation derived from a "one-shot game" characterization. The algorithm splits sampling effort across insufficiently explored arms in a batched manner, avoiding elimination and retaining all samples. Theoretical analysis shows the method is a constant-factor approximation of the optimal allocation. Experiments on synthetic and real-world datasets (OBD, MovieLens) demonstrate that Almost Tracking consistently outperforms both anytime and fixed-budget baselines, including SR, SH, and CR, even without knowing the budget T. A specific instance shows Almost Tracking achieves rates O((logK)/(log logK)) times better than SR, proving strict improvement in certain cases.

## Method Summary
The paper tackles fixed-budget best arm identification by reframing it as a one-shot game where an allocation strategy minimizes a loss function involving KL divergence. The proposed Almost Tracking algorithm implements this by continuously tracking the optimal allocation derived from empirical means. It uses a batched sampling strategy to mitigate tracking errors caused by rapid changes in empirical estimates. The key innovation is a closed-form weight function proportional to the inverse of a modified complexity term D_i(Q), which serves as a constant-factor approximation for the H1 risk measure. This allows the algorithm to achieve near-optimal rates without requiring advance knowledge of the total budget T, while avoiding the sample-discard approach used by previous methods.

## Key Results
- Proposes Almost Tracking, a closed-form algorithm achieving rate-optimal performance for H1 risk without budget horizon knowledge
- Theoretical guarantee: constant-factor approximation of optimal allocation (up to factors of log K and log log K)
- Experimental results show consistent improvement over baselines (SR, SH, CR) on synthetic and real-world datasets
- Specific instance demonstrates O((logK)/(log logK)) improvement factor over Successive Rejects
- Avoids sample-discard strategy by retaining all samples and using batched allocation

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Tracking of One-Shot Game Allocation
If an algorithm continuously adjusts its sampling proportions to track the optimal allocation derived from a "one-shot game" characterization (Theorem 1), it can achieve near-optimal minimax rates without knowing the total budget T. The paper defines a "one-shot game" where an agent chooses an allocation w(Q) based on empirical means Q to minimize a loss function involving KL divergence. The proposed algorithms (Simple Tracking and Almost Tracking) greedily or batch-wise assign samples to arms that are currently "under-sampled" relative to this optimal target weight w*(Q). The optimal allocation w(Q) can be approximated efficiently, and the empirical means Q are sufficiently stable to guide sampling without causing runaway tracking errors.

### Mechanism 2: Batched Sampling for Variance Compensation
Using a batched sampling strategy (pulling arms in groups rather than strictly sequentially) mitigates tracking errors caused by rapid changes in empirical means. The "Almost Tracking" algorithm allocates samples in batches, and the theoretical analysis decomposes the error probability into a term for the overall mean and a term for the variance of the means across batches. By batching, the algorithm implicitly penalizes allocations that rely on unstable empirical estimates, ensuring that the "cost" of tracking a moving target is bounded. The variance of empirical means contributes positively to the large deviation bound, allowing the lower bound on the error rate to hold even with imperfect tracking.

### Mechanism 3: Closed-Form Allocation for H1 Risk
A specific closed-form weight function (Eq. 14), proportional to the inverse of a modified complexity term D_i(Q), serves as a constant-factor approximation for the standard H1 risk measure. Instead of solving the non-convex optimization of the one-shot game at every step, the paper proposes w*_i(Q) ∝ 1/D_i(Q). Here D_i(Q) combines the gap Δ^Q_i and a hypothetical H1 complexity where arm i becomes optimal. This heuristic approximates the optimal solution efficiently. The specific structure of H1 complexity allows for this inverse-proportional relationship to hold within a constant factor C_stbl.

## Foundational Learning

- **Concept: Multi-Armed Bandits (Fixed-Budget vs. Fixed-Confidence)**
  - **Why needed here:** The paper specifically targets the "fixed-budget" setting (accuracy maximization after T samples) and critiques "fixed-confidence" algorithms (sample minimization for error δ) like Top-Two Thompson Sampling as suboptimal here.
  - **Quick check question:** Can you explain why an algorithm designed to minimize samples for a given error rate might perform poorly when asked to maximize accuracy for a fixed sample count?

- **Concept: Large Deviation Theory**
  - **Why needed here:** The entire theoretical framework relies on bounding the probability that empirical means deviate from true means (KL divergence/Gaussian squared error) to establish "rate-optimality."
  - **Quick check question:** How does the probability of misidentification scale with the number of samples T and the "gap" between the best and second-best arm?

- **Concept: Minimax Optimality**
  - **Why needed here:** The paper evaluates algorithms based on "minimax rates" (performance on the worst-case problem instance) rather than instance-dependent performance.
  - **Quick check question:** Why might an algorithm that is "optimal" for one set of arm means be considered "suboptimal" in the minimax sense?

## Architecture Onboarding

- **Component map:** Initializer → Allocation Evaluator → Insufficiency Filter → Batch Sampler → Recommender
- **Critical path:** Calculate D_i(Q) for all arms → Normalize weights to get w_{b,i} → Map fractional weights to integer pulls N_i → Execute pulls and update Q
- **Design tradeoffs:** Simple Tracking (Algo 1) vs. Almost Tracking (Algo 2): Simple Tracking is easier to implement (greedy) but theoretically less stable; Almost Tracking adds complexity (batches, C_{suf} parameter) for robust rate guarantees. Batch Size (N): Must be large enough to stabilize variance but small enough to allow frequent reallocation. Paper uses N=2K.
- **Failure signatures:** Stagnation: If K_{insuf} becomes empty prematurely, sampling stops or becomes uniform (check C_{suf} tuning). Sensitivity: On instances with very close means (small gaps), the empirical best arm might flip-flop, causing unstable w^*. Sub-optimality on H2: While robust on H1, the paper acknowledges SR/SH might be comparable or better on H2 complexity instances.
- **First 3 experiments:** 1) Replicate Synthetic Benchmark: Implement Almost Tracking and Successive Rejects (SR). Run on "Instance 1" (Linear) and "Instance 3" (Square root) from the paper to verify the H1 vs. H2 rate differences. 2) Ablation on Batching: Compare Simple Tracking vs. Almost Tracking on a high-variance instance to observe the "trackability" failure mode of the simple greedy approach. 3) Real Data Validation: Run on Open Bandit Dataset (as per Section 5.2) to verify performance against the "Continuous Rejects" (CR) baseline, specifically looking for the "delayed elimination" failure of CR mentioned in Appendix L.6.

## Open Questions the Paper Calls Out

- **Can a computationally efficient algorithm be designed to achieve rate-optimality for a larger class of complexity measures beyond H1?** An interesting future work is to provide an efficient optimization algorithm to achieve the rate-optimality for a larger class of complexity measures. The proposed closed-form allocation w* is specific to the H1 risk measure. For general measures, the optimization problem in the one-shot game is non-convex and currently intractable.

- **Can the Almost Tracking framework be adapted to structured pure exploration problems, such as linear or combinatorial bandits?** The method's ability to avoid elimination while retaining samples "...motivates a redesign of algorithms for many structured pure exploration problems." The current theoretical guarantees rely on properties of independent arms; extending the trackability analysis to dependent, structured parameter spaces remains unexplored.

- **Is the derived error rate bound for Almost Tracking tight in instances where Successive Rejects (SR) appears theoretically superior?** Lemma 9 identifies instances where the theoretical rate of SR is O(log K) times larger than the derived bound for Almost Tracking, though the paper suggests the Almost Tracking bound may not be tight. It is unclear if the performance gap is due to a fundamental limitation of the adaptive allocation strategy or simply a looseness in the theoretical analysis.

## Limitations

- The core theoretical guarantees hinge on the approximation quality of the closed-form allocation and the stability of the tracking process, with exact quantification of constants and their dependence on problem instance parameters missing.
- The batched sampling mechanism is posited to mitigate tracking instability, but the required batch size for a given level of stability is not characterized.
- The specific numerical claims for real-world datasets and the relative performance against specific baselines are difficult to fully verify without access to the exact preprocessing and code.

## Confidence

- **High Confidence:** The problem formulation and minimax rate analysis are sound. The identification of existing algorithms' limitations (requiring budget knowledge or relying on elimination) is accurate.
- **Medium Confidence:** The theoretical framework (one-shot game, Almost Tracking) is logically constructed and the stated guarantees are plausible given the derivations. However, the tightness of the constant-factor approximation and the precise conditions under which the batched sampling fully mitigates tracking errors require further validation.
- **Low Confidence:** The specific numerical claims for real-world datasets and the relative performance against specific baselines are difficult to fully verify without access to the exact preprocessing and code. The "O((logK)/(log logK))" improvement factor over SR is cited but the derivation of this specific ratio is not shown.

## Next Checks

1. **Benchmark Replication:** Implement and run Almost Tracking and SR on the specified "Instance 1" and "Instance 3" from Appendix L.2. Verify that Almost Tracking achieves the claimed H1 vs. H2 rate differences and that the PoE scales as expected with T.

2. **Trackability Stress Test:** Create a high-variance synthetic instance where empirical means fluctuate significantly. Run both Simple Tracking and Almost Tracking to empirically demonstrate the "trackability failure" of the greedy algorithm and the stabilization effect of batching.

3. **CR Baseline Specificity:** On the Open Bandit Dataset, specifically analyze the "delayed elimination" failure mode of the CR algorithm. Reproduce the conditions under which CR fails and confirm that Almost Tracking avoids this pitfall.