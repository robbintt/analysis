---
ver: rpa2
title: Deceptive Exploration in Multi-armed Bandits
arxiv_id: '2510.08794'
source_url: https://arxiv.org/abs/2510.08794
tags:
- agent
- best
- public
- private
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses deceptive exploration in multi-armed bandits,
  where an agent aims to identify the best private arm while appearing to optimize
  public rewards under a Kullback-Leibler (KL) divergence constraint. The authors
  formalize this as a constrained best arm identification problem with history-dependent
  costs, where pulling suboptimal arms becomes increasingly difficult.
---

# Deceptive Exploration in Multi-armed Bandits

## Quick Facts
- arXiv ID: 2510.08794
- Source URL: https://arxiv.org/abs/2510.08794
- Reference count: 40
- Primary result: Proves Θ(√T) rate for deceptive exploration under KL constraint vs O(log T) for Thompson Sampling

## Executive Summary
This paper addresses the problem of deceptive exploration in multi-armed bandits, where an agent must identify the best private arm while appearing to follow Thompson Sampling on public rewards under a Kullback-Leibler divergence constraint. The authors formalize this as a constrained best arm identification problem with history-dependent costs, proving that deceptive agents can explore public suboptimal arms at most at a Θ(√T) rate. They propose a top-two sampling algorithm that adapts exploration based on public suboptimality gaps and validates their theoretical results through numerical experiments.

## Method Summary
The method involves maintaining separate public and private posterior distributions for each arm, using Thompson Sampling as a reference algorithm for public rewards. At each time step, the agent identifies a leader and challenger from the private posterior and solves a KL-constrained optimization problem to boost the probability of pulling one of these arms while staying within the allowed divergence from Thompson Sampling. The algorithm uses Gauss-Hermite quadrature for numerical integration and a bisection method or analytical approximation to solve the boosting problem. The optimal exploration allocation is characterized through a maximin formulation that balances information gain against pulling difficulty.

## Key Results
- Proves deceptive agents can pull public suboptimal arms at most at Θ(√T) rate (improving upon O(log T) Thompson Sampling)
- Characterizes optimal error exponent through a maximin problem balancing information gain and pulling difficulty
- Proposes top-two sampling algorithm that achieves near-optimal rates while adapting to public suboptimality gaps
- Numerical experiments validate theoretical rates and demonstrate effectiveness across asymmetric arms

## Why This Works (Mechanism)

### Mechanism 1: KL-Budget-Constrained Probability Amplification
- **Claim**: Deceptive agent amplifies suboptimal arm pulling probability from O(log T) to Θ(√T) using fixed per-step KL budget
- **Mechanism**: Thompson Sampling's exponential decay in suboptimal arm probabilities is converted to harmonic decay through KL-constrained redistribution, allowing polynomial exploration rates
- **Core assumption**: Reference algorithm maintains randomized action selection with strictly positive probabilities for all arms
- **Evidence anchors**: Abstract proves Θ(√T) rate; Theorem 1 shows proof based on KL-constrained maximization solution
- **Break condition**: KL budget set to zero or reference algorithm is deterministic

### Mechanism 2: Asymmetric Information-Theoretic Exploration
- **Claim**: Optimal exploration requires balancing information gain against arm pulling "hardness" defined by public suboptimality gap
- **Mechanism**: Optimal boosting effort allocation across arms follows concave maximin solution, ensuring information balance weighted by sampling difficulty
- **Core assumption**: Best public arm differs from best private arm; per-step KL budget is sole deviation constraint
- **Evidence anchors**: Abstract describes maximin formulation; section 5 explains information balance property
- **Break condition**: Public and private reward structures are perfectly aligned

### Mechanism 3: Top-Two Sampling with Information-Directed Selection
- **Claim**: Computationally efficient algorithm achieves near-optimal exploration by adapting top-two sampling framework
- **Mechanism**: Leader/challenger selection from private posterior, with boosting choice balancing information gain against time cost (inverse boosted probability)
- **Core assumption**: Gaussian rewards or exponential family distributions with tractable posterior sampling
- **Evidence anchors**: Abstract mentions top-two algorithm inspiration; Algorithm 1 presents formal top-two approach
- **Break condition**: Posteriors become computationally intractable to sample from

## Foundational Learning

- **Concept: Thompson Sampling (Posterior Sampling)**
  - **Why needed here**: Reference algorithm agent is expected to follow; understanding randomized nature and posterior evolution is essential for KL-constrained deviation formulation
  - **Quick check question**: If you observe 10 pulls of arm with mean 0.5 and 2 pulls of arm with mean 0.2, what is approximate probability that Thompson Sampling pulls second arm next?

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here**: Mathematical formalization of "deceptiveness" or "plausibility"; core constraint defined by KL divergence between agent's and observer's action distributions
  - **Quick check question**: What is KL divergence between two Bernoulli distributions with parameters 0.5 and 0.7? How does KL budget of 0.1 constrain feasible distributions?

- **Concept: Best Arm Identification (Fixed-Confidence)**
  - **Why needed here**: Agent's goal is pure exploration - identifying best private arm with specified confidence level, changing objective from maximizing cumulative reward to minimizing sample complexity
  - **Quick check question**: How does sample complexity of best arm identification typically scale with suboptimality gaps between best arm and others?

## Architecture Onboarding

- **Component map**: Observe Rewards -> Update Posteriors -> Identify Top-Two Candidates -> Compute Reference Probabilities -> Solve KL-Constrained Boosting Problem -> Pull Arm
- **Critical path**: Execution loop at each time step follows: Observe Rewards → Update Posteriors → Identify Top-Two Candidates → Compute Reference Probabilities → Solve KL-Constrained Boosting Problem → Pull Arm
- **Design tradeoffs**: KL Budget (ε) larger budget allows faster exploration but increases detection risk; Tracking vs Top-Two tracking-based approach could more precisely target optimal allocation but would be more complex
- **Failure signatures**: Zero Exploration if ε too small or algorithm incorrectly prioritizes exploitation; Over-Exploration/Detection if ε set too high or Boosting Optimizer implemented incorrectly; Numerical Instability in Reference Probability Calculator with extreme probabilities
- **First 3 experiments**:
  1. Rate Validation: Implement naive agent repeatedly boosting single suboptimal arm, verify Θ(√T) growth, compare against theoretical coefficient
  2. Algorithm Benchmarking: Run Algorithm 1 on bandit instances with varying public/private misalignments and KL budgets, compare error decay rate 1-p*t against theoretical optimum Γ(w*)
  3. Asymmetry Test: Construct two instances with identical private means but different public means, verify Algorithm 1 produces different sample proportions, allocating more effort to arms easier to pull publicly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deceptive exploration framework and Θ(√T) rate result extend to regret minimization settings where agent must balance cumulative private reward optimization with maintaining plausibility?
- Basis in paper: [explicit] Conclusion states "Future work involves extending our results to regret minimization or other pure exploration problems"
- Why unresolved: Current paper focuses solely on pure exploration where objective is identifying optimal private arm rather than maximizing cumulative reward; regret minimization introduces different exploration-exploitation trade-offs
- What evidence would resolve it: Theoretical characterization of achievable regret rates under KL constraints, or proof framework cannot meaningfully extend to regret settings

### Open Question 2
- Question: How does analysis change when public and private rewards are correlated rather than independent, or when agent does not directly observe private rewards?
- Basis in paper: [explicit] Conclusion states "Another possible direction is studying deceptive exploration in scenarios with more complex information structures, for instance, a case in which the agent does not directly observe private rewards, but public and private rewards are correlated"
- Why unresolved: Current model assumes independence between public and private rewards and full observability of both by agent, simplifying posterior updates and KL constraint analysis
- What evidence would resolve it: Derivation of modified posterior update rules and achievable exploration rates under correlated reward structures

### Open Question 3
- Question: What are finite-time (non-asymptotic) guarantees for proposed algorithm beyond asymptotic Θ(√T) rate characterization?
- Basis in paper: [inferred] Theoretical results focus on asymptotic limits as T → ∞; numerical experiments show early-time deviations due to "initial exploration phase before posterior concentration"
- Why unresolved: Practical deployment requires understanding behavior at finite horizons, especially during early phases before posterior concentration occurs
- What evidence would resolve it: Finite-time bounds on pull rates and error probabilities with explicit constants and horizon-dependent terms

## Limitations

- The theoretical analysis assumes reference algorithm maintains strictly positive probabilities for all arms, critical for KL-boosting mechanism but not addressed for deterministic algorithms like UCB
- Asymptotic Θ(√T) rate is proven but paper does not characterize finite-time deviations or provide explicit concentration bounds
- Optimal KL budget ε is treated as given parameter rather than being optimized based on detection risk, creating gap between theoretical optimality and practical deployment

## Confidence

**High Confidence**: Core Θ(√T) pull rate bound and maximin problem formulation for optimal exponent characterization are mathematically rigorous and well-supported by proofs

**Medium Confidence**: Practical performance of top-two sampling algorithm depends on implementation details (numerical integration accuracy, initial warm-up) not fully specified; assumption of known Gaussian variances is standard but may not hold in real-world applications

**Low Confidence**: Framework extension to non-Gaussian reward distributions is unexplored; optimal ε budget selection in practice given detection constraints is not addressed; robustness to prior misspecification is unexplored

## Next Checks

1. **Finite-Time Rate Validation**: Implement algorithm and empirically measure actual pull rate of public suboptimal arms over finite horizons (T = 100, 1000, 10000) to verify convergence to Θ(√T) asymptotic rate

2. **Detection Risk Assessment**: Quantify Kullback-Leibler divergence between agent's actual pull distribution and reference Thompson Sampling distribution over time to empirically validate that per-step KL budget ε prevents detection

3. **Algorithm Robustness Test**: Evaluate algorithm's performance when assumed known variance σ² is perturbed (e.g., ±20%) to assess sensitivity to this critical hyperparameter