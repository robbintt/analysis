---
ver: rpa2
title: 'SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation'
arxiv_id: '2502.05539'
source_url: https://arxiv.org/abs/2502.05539
tags:
- arxiv
- learning
- lora
- tasks
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SSH introduces a parameter-efficient fine-tuning method based on
  Discrete Hartley Transform (DHT) that selects and updates the most informative spectral
  components while maintaining strong model performance. By leveraging DHT's real-valued
  and symmetric properties, SSH eliminates the computational overhead of complex arithmetic
  present in Fourier-based approaches.
---

# SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation

## Quick Facts
- **arXiv ID**: 2502.05539
- **Source URL**: https://arxiv.org/abs/2502.05539
- **Reference count**: 21
- **Primary result**: Achieves state-of-the-art parameter-efficient fine-tuning across NLP, vision, and multi-modal tasks using Discrete Hartley Transform with 55% fewer GFLOPs than Fourier-based methods

## Executive Summary
SSH introduces a parameter-efficient fine-tuning method that transforms model weights into the Hartley spectral domain, selectively updates the most informative spectral components, and reconstructs parameter updates via inverse transform. By leveraging DHT's real-valued symmetric properties, SSH eliminates complex arithmetic overhead present in Fourier-based approaches while achieving state-of-the-art performance across diverse domains. The method attains 7.93 GPT-4 score on LLaMA3.1-8B with less than 0.1% of parameters and matches full fine-tuning accuracy on vision tasks using only 750 trainable frequencies.

## Method Summary
SSH applies 2D Discrete Hartley Transform to pretrained weight matrices, computes spectral energy, and selects a sparse set of trainable frequencies combining top-energy components with random sampling. The method maintains frozen base weights while updating only spectral coefficients in a trainable matrix ΔH, which are merged via inverse DHT. Forward and inverse transforms are identical, simplifying implementation. SSH uses energy ratio hyperparameter δ (optimal=0.7) to balance exploitation of dominant frequencies with exploration of diverse spectral regions, achieving parameter efficiency through selective adaptation in the spectral domain.

## Key Results
- Achieves 7.93 GPT-4 score on LLaMA3.1-8B with <0.1% parameters (matching full fine-tuning performance)
- Attains 77.4% accuracy on vision tasks matching full fine-tuning while using only 750 frequencies
- Reduces computational overhead by 55% compared to FourierFT with real-valued arithmetic

## Why This Works (Mechanism)

### Mechanism 1
Transforming weight matrices to Hartley spectral domain concentrates information into sparse high-energy coefficients, enabling selective updates. SSH computes coefficient energy via |H(u,v)|², selects top-energy frequencies plus random ones for diversity, and updates only these trainable coefficients. This works because downstream task adaptations often reside in high-energy spectral components. Break condition: if weight updates require low-energy spectral regions, energy-based selection may discard critical components.

### Mechanism 2
DHT's real-valued symmetric property eliminates complex arithmetic, reducing computational overhead versus Fourier-based methods. Forward and inverse transforms are identical, avoiding separate real/imaginary processing required in FourierFT. This yields numerical stability and computational savings without sacrificing representation quality. Break condition: if tasks inherently benefit from complex-valued spectral representations, DHT's real-only constraint may underperform.

### Mechanism 3
Combining energy-based selection with random sampling prevents overfitting to dominant frequencies and preserves representational diversity. SSH selects δ·n coefficients by top energy and (1−δ)·n randomly, with δ=0.7 optimal for GLUE tasks. This balances exploitation of informative components with exploration of underrepresented spectral regions. Break condition: if tasks require only dominant frequency adjustments, random components may introduce noise.

## Foundational Learning

**Concept: Discrete Hartley Transform (DHT)**
- Why needed here: Core transformation mapping spatial-domain weights to real-valued spectral coefficients for sparse adaptation
- Quick check question: Can you explain why DHT is real-valued and symmetric, and how this differs from DFT?

**Concept: Spectral Energy Compaction**
- Why needed here: Justifies why few coefficients can capture most information, enabling parameter-efficient updates
- Quick check question: What does high energy in a spectral coefficient indicate about its contribution to the weight matrix?

**Concept: Parameter-Efficient Fine-Tuning (PEFT) Objectives**
- Why needed here: Frames tradeoff between trainable parameter count, memory, and task performance that SSH optimizes
- Quick check question: Why might sparse spectral updates be more efficient than low-rank spatial decomposition for certain models?

## Architecture Onboarding

**Component map:**
Weight initialization → DHT → selection (energy+random) → ΔH training via masked gradients → iDHT → W₀ + ΔW

**Critical path:** Pre-trained weights undergo one-time DHT transformation, frequency selection based on energy ranking plus random sampling creates trainable mask, sparse spectral parameters ΔH are updated via masked gradients, inverse DHT reconstructs weight updates merged with frozen base weights.

**Design tradeoffs:**
- n (number of selected frequencies): Higher n increases capacity but reduces parameter efficiency
- δ (energy ratio): Higher δ emphasizes dominant frequencies but risks missing diverse features; δ=0.7 recommended
- Scaling factor α: Balances contribution of spectral updates; too high may destabilize, too low may underfit

**Failure signatures:**
- Performance collapse with high n: May indicate overfitting; reduce n or increase regularization
- No improvement over baseline: Check if selected frequencies are informative; increase δ
- Numerical instability: Ensure DHT implementation is stable; verify gradient masking is applied correctly

**First 3 experiments:**
1. Baseline comparison on GLUE with RoBERTa-base: Vary n (200, 750, 1500) and δ (0.5, 0.7, 0.9); report accuracy and parameter count
2. Efficiency scaling test on LLaMA3.1-8B: Compare SSH vs LoRA vs FourierFT on Alpaca instruction tuning; track GFLOPs, memory, and GPT-4 evaluation scores
3. Ablation on energy vs random selection: Fix n, vary δ from 0 to 1; analyze impact on task-specific performance (e.g., CoLA vs STS-B)

## Open Questions the Paper Calls Out

**Open Question 1:** What theoretical principles explain why the energy ratio δ=0.7 provides optimal balance between energy-based and random frequency selection across diverse tasks? The paper empirically observes this prevents overfitting and underfitting but lacks theoretical justification for this specific value.

**Open Question 2:** How does SSH's performance scale when applied to ultra-large models (70B+ parameters) and to what extent does the one-time DHT computational overhead become a bottleneck? The paper notes LLaMA3.1 70B requires 48GB memory even with QLoRA and identifies DHT transformation as a limitation, but only evaluates up to 13B parameters.

**Open Question 3:** Can SSH be effectively combined with quantization techniques beyond QLoRA to achieve multiplicative efficiency gains? The paper uses QLoRA for instruction tuning but does not systematically investigate how SSH interacts with aggressive quantization schemes.

## Limitations
- No systematic analysis of which downstream task characteristics determine SSH's relative advantage over low-rank methods
- Claims about universal superiority across all domains lack domain-specific limitation analysis
- Implementation details remain unclear due to unreleased code repository

## Confidence
- **High**: Claims about computational efficiency gains from real-valued arithmetic
- **Medium**: Claims about parameter efficiency matching full fine-tuning performance
- **Low**: Claims about universal superiority across all domains without domain-specific limitations

## Next Checks
1. **Spectral Transferability Test**: Apply SSH across domains (NLP→vision) to verify if high-energy coefficients transfer meaningfully between domains or require domain-specific retuning
2. **Phase Information Recovery**: Design experiment comparing SSH against complex-valued spectral method on phase-sensitive tasks to quantify information loss from DHT's real-only constraint
3. **Overparameterization Boundary**: Systematically vary n from 0.1% to 10% of total frequencies to identify inflection point where additional parameters cease providing performance gains