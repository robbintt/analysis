---
ver: rpa2
title: On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling
arxiv_id: '2506.21874'
source_url: https://arxiv.org/abs/2506.21874
tags:
- image
- images
- vlms
- poison
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the feasibility of poisoning text-to-image
  generative models via adversarial mislabeling of training images. The core method
  involves creating adversarial perturbations on images to induce Vision-Language
  Models (VLMs) to generate incorrect captions, thereby creating "dirty-label" poison
  samples in the training pipeline.
---

# On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling

## Quick Facts
- arXiv ID: 2506.21874
- Source URL: https://arxiv.org/abs/2506.21874
- Reference count: 40
- Primary result: Adversarial mislabeling of training images can poison text-to-image models with >94% success rate

## Executive Summary
This paper introduces a novel attack vector against text-to-image generative models by exploiting vulnerabilities in Vision-Language Models (VLMs). The attack involves creating adversarial perturbations on images to induce VLMs to generate incorrect captions, thereby creating "dirty-label" poison samples in the training pipeline. The approach achieves high success rates in mislabeling attacks across multiple VLMs and demonstrates effective downstream poisoning of diffusion models, even at low poison doses. The attack transfers well to commercial VLMs and is robust against basic defenses, though more sophisticated countermeasures can disrupt it at high computational cost.

## Method Summary
The attack optimizes pixel-level perturbations on reference images to minimize the distance between their VLM feature representation and a target image's features, causing VLMs to generate captions matching the target concept instead of the actual image content. These adversarially mislabeled image-caption pairs are then injected into the training pipeline of diffusion models, where they corrupt specific prompts. The method uses a white-box or transfer-based black-box approach, with the latter optimizing perturbations against multiple ViT feature extractors simultaneously to achieve generalization to unseen VLMs.

## Key Results
- High mislabeling success rates (71-93% MSR) across three VLMs with adversarial perturbations
- Downstream poisoning achieves 94-99% success rate in diffusion models with minimal poison samples (25-125 samples)
- Black-box attacks against commercial VLMs achieve 42-45% MSR with downstream PSR of 73-82%
- Attack is robust against basic defenses but can be disrupted by more sophisticated countermeasures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs are vulnerable to targeted adversarial perturbations that induce specific mislabeling while remaining visually imperceptible.
- **Mechanism:** The attack optimizes pixel-level perturbations on a reference image (e.g., statue) to minimize the distance between its VLM feature representation and a target image's (e.g., cat) features: `min Dist(φ(x_r + δ), φ(x_t))` subject to `|δ| < ε`. This causes the VLM to generate captions matching the target concept instead of the actual image content.
- **Core assumption:** VLMs have consistent, exploitable vulnerabilities in their visual feature extractors (typically ViT-based) that transfer across architectures.
- **Evidence anchors:**
  - [abstract]: "VLMs are highly vulnerable to adversarial perturbations, allowing attackers to produce benign-looking images that are consistently miscaptioned by the VLM models."
  - [section 5.3, Table 2]: MSR of 71-93% across three VLMs with AAR >0.90 and BAR <0.06.
  - [corpus]: Related work confirms VLM adversarial vulnerabilities (Silent Branding Attack, VAGUEGAN) though specific mislabeling mechanisms differ.
- **Break condition:** If VLMs achieve robustness to L∞ perturbations through adversarial training or architectural changes to the visual encoder.

### Mechanism 2
- **Claim:** Mislabeled image-caption pairs act as highly effective poison samples that corrupt specific prompts in downstream diffusion models with minimal samples.
- **Mechanism:** When diffusion models train on adversarially mislabeled pairs (statue image + "cat" caption), they learn incorrect associations between text prompts and visual features. The adversarial perturbations cause higher loss during training, forcing larger gradient updates and making poison more potent than standard dirty-label attacks.
- **Core assumption:** Diffusion models trained on web-scale data cannot verify caption accuracy and rely on VLM-generated labels as ground truth.
- **Evidence anchors:**
  - [abstract]: "successfully altering their behavior with a small number of poisoned samples"
  - [section 5.4, Table 6]: 25 samples achieve PSR >0.60; 125 samples achieve PSR 0.94-0.99 across SD2.1, SDXL, FLUX.
  - [corpus]: Silent Branding Attack and Nightshade (cited) demonstrate related poisoning mechanisms but with different attack vectors.
- **Break condition:** If training pipelines implement robust caption verification or significantly increase data diversity for each concept.

### Mechanism 3
- **Claim:** Transfer-based black-box attacks using ensemble optimization can exploit commercial VLMs without model access.
- **Mechanism:** Optimizing perturbations against multiple ViT feature extractors simultaneously (`E_k[Dist(φ_k(x_r + δ), φ_k(x_t))]`) with momentum-based stabilization (SSA-CWA) produces perturbations that generalize to unseen VLMs including proprietary services.
- **Core assumption:** Most VLMs share similar ViT-based visual encoders with transferable adversarial subspaces.
- **Evidence anchors:**
  - [abstract]: "achieving high attack success (over 73%) even in black-box scenarios against commercial VLMs"
  - [section 8.2, Table 11]: MSR 42-45% against Microsoft Azure and Google Vertex with downstream PSR 73-82%.
  - [corpus]: Limited corpus evidence for commercial VLM transfer attacks; this represents a novel contribution.
- **Break condition:** If commercial VLMs use sufficiently diverse visual encoders or implement query-based attack detection.

## Foundational Learning

- **Adversarial perturbations (L∞ budget):**
  - Why needed here: Understanding how imperceptible pixel changes (ε = 16/255) manipulate VLM outputs is core to the attack mechanism.
  - Quick check question: Can you explain why small L∞-bounded perturbations affect ViT feature representations while remaining visually undetectable?

- **Vision-Language Model architectures:**
  - Why needed here: The attack exploits the visual encoder component (ViT) in models like LLaVA, CogVLM, and BLIP-3.
  - Quick check question: How does the coupling between visual feature extractors and language models create the vulnerability this attack exploits?

- **Diffusion model fine-tuning dynamics:**
  - Why needed here: Understanding how poison samples affect model behavior during training explains the attack's downstream impact.
  - Quick check question: Why might adversarially perturbed images cause higher loss and larger gradient updates than unperturbed dirty-label samples?

## Architecture Onboarding

- **Component map:** [Attacker's Images] → [VLM Captioner] → [Image-Caption Pairs] → [Diffusion Model Training] → [Corrupted prompt response]

- **Critical path:**
  1. Optimize perturbations using white-box access to target VLM or ensemble of ViTs (Eq. 1 or 3)
  2. Generate captions using VLM with standard prompting
  3. Inject poisoned pairs into training pipeline at low dose (0.1-1% of concept samples)
  4. Evaluate poisoning success via CLIP-based PSR on target prompts

- **Design tradeoffs:**
  - Perturbation budget (ε): Higher values increase MSR/PSR but risk visual detectability; paper uses ε = 16/255 as default
  - Transfer vs. white-box: Transfer attacks sacrifice ~15-30% MSR for broader applicability
  - Adaptive attacks: Adding transformation robustness (Eq. 2) requires more optimization iterations but bypasses basic defenses

- **Failure signatures:**
  - Low MSR despite high perturbation budget suggests VLM architecture mismatch
  - Clean PSR with high MSR indicates sufficient benign samples diluting poison effect
  - Detection by image-caption alignment filtering (BAR >0.15) reveals insufficient stealth optimization

- **First 3 experiments:**
  1. Replicate basic white-box mislabeling (Table 2): Test your VLM target with ε = 16/255 on 25 concept pairs to establish baseline MSR and verify implementation correctness.
  2. Dose-response analysis (Table 6): Fine-tune SD2.1 with varying poison counts (0, 25, 50, 100, 125) to understand your target model's vulnerability threshold.
  3. Defense robustness check (Table 7): Apply JPEG compression and image-caption alignment filtering to test if your attack requires adaptive augmentation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can effective reconnaissance detection methods be developed to identify adversarial mislabeling attacks when attackers use very low query budgets (e.g., 5 queries per image)?
- Basis in paper: [explicit] Section 8.3 states: "Developing effective countermeasures under such low-query conditions remains an open research question, which we leave to future work."
- Why unresolved: Existing detection methods assume high query volumes (thousands per image); the black-box attack requires only ~5 queries, bypassing current approaches.
- What evidence would resolve it: Development of detection methods that maintain high true positive rates with low false positives specifically for low-query attack scenarios.

### Open Question 2
- Question: Can defense mechanisms be designed that achieve strong protection against AMP attacks without incurring the high computational costs (26+ days for 50k images) or caption quality degradation (25% data loss) observed with DiffPure?
- Basis in paper: [inferred] Section 7.3 documents DiffPure's effectiveness but notes its prohibitive computational cost and 25% training data discard rate due to reduced caption quality.
- Why unresolved: The paper shows a fundamental trade-off between defense effectiveness and practical deployability at scale.
- What evidence would resolve it: A defense method demonstrating comparable MSR reduction (<2%) with significantly lower computational overhead and minimal caption quality impact.

### Open Question 3
- Question: What explains the significantly lower transferability of enhanced black-box attacks to CogVLM (19% MSR) compared to LLaVA (73%) and BLIP-3 (81%), and can transferability be further improved?
- Basis in paper: [explicit] Section 8.2 notes: "The transfer to CogVLM is less effective, with a limited MSR of 19%. This is likely because CogVLM utilizes a newer, differently trained ViT."
- Why unresolved: The hypothesis about architectural differences is not empirically validated; the specific features enabling or blocking transfer remain unidentified.
- What evidence would resolve it: Systematic ablation across ViT architectures identifying which architectural or training differences most strongly predict transfer success rates.

## Limitations

- **Transfer attack robustness**: The paper demonstrates successful transfer to commercial VLMs but the mechanism remains partially unexplained, with limited testing of alternative transfer strategies.
- **Defense circumvention**: While the attack appears robust against basic defenses, it can be defeated by more sophisticated countermeasures (adversarial training, multi-encoder verification) that weren't fully explored.
- **Practical feasibility**: Claims about real-world attack viability lack substantiation regarding production system detection mechanisms and cost-benefit analysis for actual attackers.

## Confidence

- **High confidence**: Core finding that VLMs are vulnerable to targeted adversarial mislabeling is well-supported (MSR 71-93% across three models); downstream poisoning mechanism (94-99% PSR) is convincingly demonstrated.
- **Medium confidence**: Transfer attack effectiveness against commercial VLMs is supported but based on single ensemble optimization approach; suggests general vulnerability but doesn't fully explain mechanism.
- **Low confidence**: Claims about practical feasibility in real-world scenarios are not fully substantiated; doesn't address potential detection mechanisms or cost-benefit analysis for actual attackers.

## Next Checks

**Validation Check 1**: Replicate the defense experiments with additional countermeasures including (a) adversarial training on the target VLM, (b) multi-encoder verification systems, and (c) caption quality scoring models. This will clarify whether the attack can be systematically blocked or requires ongoing adaptation.

**Validation Check 2**: Test the attack across a broader range of VLM architectures including those with different visual encoders (not just ViT-based models). This will determine whether the vulnerability is specific to current architectures or represents a more fundamental weakness in VLM design.

**Validation Check 3**: Conduct a comprehensive analysis of detection mechanisms that could identify poisoned image-caption pairs before they enter training pipelines. This includes testing CLIP-based alignment thresholds, human verification protocols, and automated caption quality assessment tools to understand the practical barriers to real-world deployment.