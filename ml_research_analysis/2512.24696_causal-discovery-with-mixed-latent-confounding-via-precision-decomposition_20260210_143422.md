---
ver: rpa2
title: Causal Discovery with Mixed Latent Confounding via Precision Decomposition
arxiv_id: '2512.24696'
source_url: https://arxiv.org/abs/2512.24696
tags:
- cond
- confounding
- causal
- pervasive
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses causal discovery under mixed latent confounding\u2014\
  where both pervasive (globally acting) and localized (sparsely acting) unobserved\
  \ factors affect observed variables. The authors propose DCL-DECOR, a three-stage\
  \ pipeline that first isolates pervasive confounding via precision matrix decomposition\
  \ (structured - low-rank), then learns the DAG structure on the deconfounded representation\
  \ using a correlated-noise continuous DAG learner (DECOR-GL)."
---

# Causal Discovery with Mixed Latent Confounding via Precision Decomposition

## Quick Facts
- arXiv ID: 2512.24696
- Source URL: https://arxiv.org/abs/2512.24696
- Reference count: 23
- Primary result: DCL-DECOR improves causal discovery under mixed latent confounding, with F1 gains from 0.266 to 0.417 and SHD reductions from 74.9 to 55.3

## Executive Summary
This paper tackles causal discovery when unobserved confounders act in two distinct ways: pervasive factors affecting many variables globally, and localized factors affecting only a few. The authors propose DCL-DECOR, a three-stage pipeline that first isolates pervasive confounding via precision matrix decomposition, then learns the DAG structure using a correlated-noise continuous DAG learner with bow reconciliation. The method achieves theoretical identifiability guarantees under standard assumptions and demonstrates consistent improvements over baseline methods in synthetic experiments, particularly as pervasive confounding strengthens.

## Method Summary
DCL-DECOR operates in three stages: (I) Precision decomposition using LVGLASSO to split the observed precision matrix into structured and low-rank components, isolating pervasive confounders; (II) Inversion of the structured component to obtain the conditional covariance that removes pervasive confounding while preserving DAG structure and localized confounding; (III) Application of DECOR-GL to the deconfounded covariance to learn directed edges under correlated noise, followed by bow reconciliation to ensure bow-freeness. The approach builds on the correlated-error SEM framework but introduces a novel two-level precision decomposition to handle mixed confounding patterns.

## Key Results
- Mean F1 score improvement from 0.266 to 0.417 over baseline DECOR-GL
- SHD reduction from 74.9 to 55.3 across varying confounding strengths
- Performance gains increase with pervasive confounding strength (q_P ≥ 3, U_d ≥ 1.0)
- Consistent outperformance across 10 replicates per synthetic setting

## Why This Works (Mechanism)

### Mechanism 1: Precision Matrix Decomposition Separates Pervasive from Local Confounding
The hierarchical noise model (diagonal + sparse low-rank + dense low-rank) decomposes at the precision level via Sherman-Morrison-Woodbury identity. This yields an additive decomposition Θ = S_x - L_x where L_x captures pervasive latent factors (rank ≤ r_L) and S_x preserves DAG structure with localized confounding. Transversality ensures unique identifiability of this split.

### Mechanism 2: Conditioning on Pervasive Factors Removes Their Confounding Effect
Inverting the structured precision component S_x analytically conditions out pervasive factors without explicit estimation. The resulting conditional covariance equals the distribution after removing pervasive confounding while preserving the DAG and localized confounding structure.

### Mechanism 3: Correlated-Noise DAG Learning Recovers Directed Edges Under Local Confounding
DECOR-GL jointly optimizes DAG structure and error correlations via alternating updates, with bow reconciliation resolving any remaining bidirected edges by comparing signal strengths. This yields the minimal bow-free equivalence class under linear Gaussian assumptions.

## Foundational Learning

- **Precision Matrix and Sherman-Morrison-Woodbury Identity**: Essential for deriving the additive decomposition of the observed precision matrix; if Ω = D + VV^⊤ + UU^⊤ with diagonal D, what is the structure of Ω^{-1}?

- **Transversality and Identifiability in Matrix Decompositions**: Guarantees the sparse+low-rank split is unique; why can't robust PCA separate arbitrary sparse+low-rank matrices without this geometric condition?

- **Bow-Free ADMGs and Linear Gaussian Identifiability**: The conditional model is only identifiable up to the minimal bow-free equivalence class; in a two-node system with both X→Y and X↔Y (bow), can you distinguish the direct effect from the latent confounding?

## Architecture Onboarding

- **Component map**: Stage I (LVGLASSO) → Stage II (Inversion) → Stage III (DECOR-GL)
- **Critical path**: Stage I estimates precision split, Stage II conditions out pervasive factors via inversion, Stage III learns DAG on deconfounded representation. Errors propagate forward.
- **Design tradeoffs**: Locality regularizer choice (ℓ_1 vs. banded vs. block), bow reconciliation constant c, penalty ratios (λ_s, λ_*, λ_B, λ_S) control sparsity and rank constraints
- **Failure signatures**: High rank(L̂_x) indicates under-removal of pervasive confounding; negative eigenvalues in Σ̂_cond suggest Stage I failure; many remaining bows indicate localized confounding violates bow-free assumption
- **First 3 experiments**: (1) Ablation on pervasive strength (fix q_P, U_d at low/medium/high levels; verify ΔF1 increases with strength); (2) Penalty sensitivity sweep (vary λ_s, λ_* around defaults; monitor rank(L̂_x) and condition number); (3) Bow-free violation stress test (inject localized confounders creating bows; quantify reconciliation accuracy)

## Open Questions the Paper Calls Out

### Open Question 1
Can the precision decomposition approach be extended to non-Gaussian or nonlinear structural equation models to leverage stronger identifiability guarantees? The authors explicitly limit their scope to the strictly Gaussian, linear case, and the method relies on precision matrix identities specific to Gaussian distributions.

### Open Question 2
How sensitive is the method to the selection of the pervasive rank r_L and the balance of regularization parameters? The algorithm requires tuning multiple penalties (λ_s, λ_*, λ_B, λ_S) and the latent rank, but experiments use fixed or provided values without addressing misspecification.

### Open Question 3
Does the modular separation of precision decomposition and DAG learning introduce error propagation that a joint optimization approach could mitigate? The paper's "modular, precision-led pipeline" creates dependencies where Stage I accuracy directly impacts Stage III performance, but finite-sample error accumulation is not analyzed.

## Limitations

- The method critically depends on transversality assumption to ensure unique decomposition; overlapping support patterns between pervasive and localized confounder loadings break identifiability
- Bow-freeness at the conditional level is fundamentally unverifiable from observational data and violations lead to ambiguous edge attribution
- Performance on real-world data with unknown confounding structure remains untested despite synthetic success
- The method is limited to linear Gaussian systems, excluding potential benefits from non-Gaussian or nonlinear identifiability

## Confidence

- **High**: The precision decomposition mechanism and its connection to isolating pervasive confounding via SMW identity
- **Medium**: The theoretical identifiability claims under stated assumptions (bow-freeness, mutual independence)
- **Medium**: The empirical improvement over baseline DECOR-GL on synthetic data across varying confounding strengths
- **Low**: Generalization to settings where transversality is violated or confounding structure deviates from assumed hierarchical model

## Next Checks

1. **Ablation study on confounding strength**: Systematically vary q_P and U_d parameters to quantify when DCL-DECOR provides maximum benefit over baseline DECOR-GL, replicating the improvement pattern shown in Figure 2a

2. **Penalty sensitivity analysis**: Sweep λ_s and λ_* around defaults (0.001, 0.005) to identify stable operating regions and understand their impact on L̂_x rank and overall performance

3. **Bow-free violation stress test**: Deliberately inject localized confounders that create bows (affecting parent-child pairs) to evaluate how DCL-DECOR handles fundamental identifiability issues and assess the reliability of the reconciliation step