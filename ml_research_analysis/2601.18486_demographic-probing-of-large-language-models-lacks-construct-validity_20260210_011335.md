---
ver: rpa2
title: Demographic Probing of Large Language Models Lacks Construct Validity
arxiv_id: '2601.18486'
source_url: https://arxiv.org/abs/2601.18486
tags:
- cues
- demographic
- race
- black
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether demographic cues in prompts are interchangeable
  indicators of the same underlying construct in large language models. Using first-person,
  advice-seeking interactions across healthcare, salary, and legal tasks, the authors
  test construct validity by measuring convergent and discriminant validity across
  race and gender in the U.S.
---

# Demographic Probing of Large Language Models Lacks Construct Validity

## Quick Facts
- **arXiv ID**: 2601.18486
- **Source URL**: https://arxiv.org/abs/2601.18486
- **Reference count**: 40
- **Primary result**: Demographic cues in LLM prompts produce inconsistent behavioral changes, lacking construct validity

## Executive Summary
This paper evaluates whether demographic cues in prompts are interchangeable indicators of the same underlying construct in large language models. Using first-person, advice-seeking interactions across healthcare, salary, and legal tasks, the authors test construct validity by measuring convergent and discriminant validity across race and gender in the U.S. context. They find that different cues induce only partially overlapping changes in model behavior for the same group, with correlations averaging 0.66–0.84 within race and 0.98 within cue type. Group differentiation is weak and uneven, and intergroup outcome disparities vary in magnitude and direction depending on the cue used. These inconsistencies are partly explained by variation in cue–group association strength and by linguistic confounders such as readability, which independently influence responses.

## Method Summary
The authors conducted a comprehensive evaluation of demographic probing in LLMs using three task domains: healthcare, salary, and legal advice scenarios. They systematically varied demographic cues including race (Black, White) and gender (male, female) in first-person prompts while measuring model responses. The study employed multiple validity tests including convergent validity (measuring correlation between different cues for the same demographic group), discriminant validity (measuring differentiation between groups), and controlled for linguistic confounds such as readability scores. They also analyzed cue-to-group association strength using real-world data to understand how well demographic cues map to actual user populations.

## Key Results
- Different demographic cues produce only partially overlapping behavioral changes (correlations: 0.66–0.84 within race, 0.98 within cue type)
- Group differentiation is weak and inconsistent across different demographic cues
- Linguistic confounders like readability independently influence model responses, explaining some behavioral variations

## Why This Works (Mechanism)
Demographic cues affect LLM behavior through learned associations in training data, but these associations are context-dependent and inconsistent across different prompting strategies. The mechanism involves the model's attempt to match prompt characteristics with learned patterns, but without a stable underlying construct that maps consistently across different demographic indicators.

## Foundational Learning
- **Construct validity**: Why needed - to ensure measurement tools actually measure what they claim to measure; Quick check - compare correlations between different measurement approaches
- **Convergent validity**: Why needed - to verify that different measures of the same construct produce similar results; Quick check - calculate correlation coefficients between different demographic cues for same group
- **Discriminant validity**: Why needed - to ensure measures of different constructs produce distinct results; Quick check - measure group separation using statistical distance metrics
- **Linguistic confounding**: Why needed - to isolate true demographic effects from language-based influences; Quick check - control for readability scores and linguistic features
- **Cue-to-group association strength**: Why needed - to understand how well demographic indicators match actual populations; Quick check - measure recall rates for different demographic cues
- **Ecological validity**: Why needed - to ensure findings generalize to real-world usage; Quick check - compare synthetic probing results with actual user interaction data

## Architecture Onboarding

**Component Map**: Demographic cues -> LLM behavior -> Response patterns -> Validity metrics

**Critical Path**: Demographic cue selection → Prompt engineering → Model response generation → Behavioral measurement → Validity assessment

**Design Tradeoffs**: Synthetic vs. real-world prompts (controlled but potentially artificial vs. ecologically valid but noisy), number of demographic dimensions (comprehensive but complex vs. focused but limited), linguistic control (reduces confounds but may alter natural behavior)

**Failure Signatures**: Inconsistent correlations across cues, weak group differentiation, linguistic confound effects, cue-to-group association variability

**First Experiments**: 
1. Test additional demographic cues beyond race and gender to assess generalizability
2. Vary linguistic complexity independently while holding demographic content constant
3. Conduct user studies to compare synthetic probing results with actual demographic influences

## Open Questions the Paper Calls Out
None

## Limitations
- Limited scope of tested tasks and populations may affect generalizability
- Synthetic prompts may not capture all real-world demographic influences
- Focus on U.S. context may not generalize to other cultural settings

## Confidence
- Core finding of construct validity concerns: High
- Specific numerical estimates of cue effectiveness: Medium
- Linguistic confound quantification: Medium

## Next Checks
1. Replicate analysis across broader range of demographic cues and cultural contexts
2. Conduct controlled experiments varying linguistic features independently
3. Implement ecological momentary assessments with real user interactions