---
ver: rpa2
title: 'Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in
  Language Models'
arxiv_id: '2501.17420'
source_url: https://arxiv.org/abs/2501.17420
tags:
- biases
- implicit
- language
- agents
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit significant sociodemographic
  disparities in agent-based simulations, even as explicit biases decline with model
  advancements. This study proposes a technique to systematically uncover implicit
  biases by contrasting "actions" of sociodemographically-informed language agents
  with "words" of LLMs when directly prompted.
---

# Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models

## Quick Facts
- arXiv ID: 2501.17420
- Source URL: https://arxiv.org/abs/2501.17420
- Authors: Yuxuan Li; Hirokazu Shirado; Sauvik Das
- Reference count: 40
- Primary result: Large language models exhibit significant sociodemographic disparities in agent-based simulations, even as explicit biases decline with model advancements.

## Executive Summary
Large language models (LLMs) show significant sociodemographic disparities in agent-based simulations, even as explicit biases decline with model advancements. This study proposes a technique to systematically uncover implicit biases by contrasting "actions" of sociodemographically-informed language agents with "words" of LLMs when directly prompted. Testing six LLMs across three sociodemographic groups and four decision-making scenarios, the results show that state-of-the-art models exhibit significant implicit biases in nearly all simulations, with more advanced models displaying greater implicit biases despite reducing explicit biases. Furthermore, the implicit biases uncovered align directionally with real-world disparities but are markedly amplified. This finding underscores the utility of the proposed technique in identifying implicit biases in LLMs and highlights the need for novel strategies to address these biases in LLM-powered applications.

## Method Summary
The study uses a two-step pipeline to evaluate implicit biases in LLMs. First, a persona generator creates detailed agent profiles based on sociodemographic attributes and scenario contexts (temperature=0.7). Second, an action generator simulates the agent making binary decisions in the given scenario (temperature=0.2). Explicit biases are measured by directly prompting LLMs to guess attributes from decisions. The Demographic Parity Difference (DPD) between groups is calculated and tested for significance using bootstrapping. Six LLMs are tested across three sociodemographic groups (gender, race/ethnicity, politics) and four scenarios (emergency response, authority compliance, negative info sharing, career path), with 100 unique personas generated per attribute/scenario combination.

## Key Results
- GPT-4o exhibited significant implicit biases in 11 out of 12 scenario-attribute combinations tested.
- State-of-the-art models (GPT-3 → GPT-4o) showed a clear inverse relationship: explicit bias decreased from 12/12 to 1/12 significant cases, while implicit bias increased from 2/12 to 11/12.
- All six validated predictions from empirical literature aligned directionally with LLM implicit biases.
- The amplification effect was pronounced: LLM-simulated disparities aligned with real-world patterns but were "markedly amplified" in magnitude.

## Why This Works (Mechanism)

### Mechanism 1: Contextualized Persona Generation as Bias Elicitation
- Claim: Scenario-specific context statements in persona generation reveal implicit biases that remain latent with demographic labels alone.
- Mechanism: The two-step process (persona generation → action generation) creates rich, stereotype-infused agent narratives that subsequently constrain decision-making along sociodemographic lines.
- Core assumption: LLMs encode group-level behavioral associations in training data; contextual prompts activate these associations more than bare labels.
- Evidence anchors: [section 5.4] Ablation tests show contextualized personas produce significant disparities in 11/12 cases with GPT-4o, while no-persona conditions show near-zero bias for gender and race/ethnicity scenarios.

### Mechanism 2: Alignment Tradeoff — Explicit Mitigation, Implicit Amplification
- Claim: Fairness and safety alignment techniques reduce explicit biases in direct questioning while leaving (or amplifying) implicit biases in agent simulations.
- Mechanism: Alignment training modifies surface-level refusal patterns for directly biased statements but does not retrain latent associations activated indirectly through persona-conditioned generation.
- Core assumption: Alignment methods target overt bias expressions; implicit biases emerge through context-dependent generation paths not covered by training.
- Evidence anchors: [section 5.3] GPT-3 → GPT-4o: explicit bias dropped from 12/12 to 1/12 significant cases; implicit bias rose from 2/12 to 11/12 significant cases.

### Mechanism 3: Stereotype-Amplified Decision Rationales
- Claim: Agent rationales reveal deindividuated, stereotype-consistent justifications that align directionally with real-world disparities but are exaggerated in magnitude.
- Mechanism: LLMs generate persona-consistent rationales by retrieving and synthesizing group-stereotypical content from training data, amplifying differences.
- Core assumption: Training data contains group-behavior correlations; generation amplifies these through persona-consistency constraints.
- Evidence anchors: [section 5.2] 80/100 Asian-coded agents used "safety/authority" language vs. 1/100 Black-coded; 84/100 Black-coded used "standing up/community" language vs. 20/100 Asian-coded.

## Foundational Learning

- **Language Agents**: The technique treats LLMs as goal-driven agents with personas, not just text generators; understanding this abstraction is prerequisite to interpreting the methodology.
  - Why needed here: The technique treats LLMs as goal-driven agents with personas, not just text generators; understanding this abstraction is prerequisite to interpreting the methodology.
  - Quick check question: Can you explain the difference between prompting an LLM for text vs. simulating an agent with a persistent persona making decisions?

- **Demographic Parity Difference (DPD)**: DPD is the quantitative metric for detecting bias; understanding its calculation is necessary to interpret results.
  - Why needed here: DPD is the quantitative metric for detecting bias; understanding its calculation is necessary to interpret results.
  - Quick check question: Given decision rates of 70% for Group A and 30% for Group B, what is the DPD?

- **Implicit vs. Explicit Bias in NLP**: The core claim hinges on distinguishing biases from direct prompts (explicit) vs. indirect persona-conditioned decisions (implicit).
  - Why needed here: The core claim hinges on distinguishing biases from direct prompts (explicit) vs. indirect persona-conditioned decisions (implicit).
  - Quick check question: If an LLM refuses to stereotype when asked directly but shows group differences in agent simulations, which bias type is exhibited?

## Architecture Onboarding

- **Component map**: Sociodemographic Attribute → Persona Generation (with context) → Action Generation → DPD Calculation → Significance Test
- **Critical path**: Attribute → Persona Generation (with context) → Action Generation → DPD Calculation → Significance Test
- **Design tradeoffs**: Higher persona generation temperature (0.7) increases diversity but may introduce variance; lower action temperature (0.2) stabilizes decisions. Binary choices simplify analysis but may miss nuance; paper notes technique generalizes to non-binary.
- **Failure signatures**: Near-uniform decisions across all demographics (under-elcitation of bias; see GPT-3 in Figure 10). Agents refusing to choose (e.g., GPT-3.5-turbo career path scenario; 227/1400 excluded). Logical contradictions across sociodemographic groups (e.g., political ideology agents showing near-zero evacuation while gender agents show ~50%, which is demographically impossible).
- **First 3 experiments**: 
  1. Reproduce one scenario (e.g., Emergency Response) with GPT-4o across all three sociodemographic groups; verify DPD calculations and significance thresholds match reported values.
  2. Ablation replication: Run the same scenario with no-persona and non-contextualized persona conditions; confirm bias attenuation patterns.
  3. Rationale analysis: Extract and cluster rationale language by demographic attribute; verify stereotype-consistent language patterns (e.g., "safety" vs. "community" framing).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How are agent actions influenced by personas incorporating multiple, intersectional sociodemographic identities (e.g., race combined with gender and political ideology)?
- Basis in paper: [explicit] Section 6.3 notes that the current study focuses exclusively on single sociodemographic groups and suggests future research examine personas with intersectional identities.
- Why unresolved: The paper identifies that LLMs currently over-align decisions with a single provided social identity, but it is unknown how competing or intersecting identities modify this behavior.
- What evidence would resolve it: Simulation results from agents assigned multi-attribute personas compared against control groups with single attributes.

### Open Question 2
- Question: To what extent do the implicit biases in LLMs align with the magnitude of real-world disparities, rather than just the direction?
- Basis in paper: [explicit] Section 6.3 states that future work should explore magnitude alignment, noting that the current study was limited to directional alignment.
- Why unresolved: The authors established that biases are directionally aligned but "markedly amplified," yet they lacked the data to quantify how the size of the effect in LLMs compares to human effect sizes.
- What evidence would resolve it: Running directly comparable human-subject studies using the same scenarios to calculate and compare effect sizes.

### Open Question 3
- Question: What novel strategies can effectively mitigate implicit biases in LLMs without compromising the models' ability to simulate realistic human behavioral variability?
- Basis in paper: [inferred] The conclusion highlights the need for "novel strategies," and Section 6.2 questions whether biases should be fully mitigated if they reflect reality.
- Why unresolved: While the paper demonstrates that current alignment techniques reduce explicit bias but increase implicit bias, effective methods for decoupling harmful stereotypes from realistic behavioral simulations remain undefined.
- What evidence would resolve it: The development and testing of new alignment techniques that reduce the Demographic Parity Difference (DPD) amplification while maintaining scenario-specific behavioral variance.

## Limitations

- Bootstrap iteration count not specified, creating ambiguity in significance thresholds.
- Model versioning/API dates not documented, potentially affecting reproducibility of refusal rates and persona generation.
- System prompts not reported, which may influence agent behavior.

## Confidence

- **High confidence**: The core finding that agent simulations reveal implicit biases missed by direct questioning is robustly supported by systematic comparisons across six models and multiple scenarios.
- **Medium confidence**: The alignment tradeoff mechanism (explicit bias reduction accompanied by implicit bias amplification) is well-documented within this study but lacks external validation from the corpus.
- **Low confidence**: The exact magnitude of bias amplification relative to real-world disparities, given the artificial nature of persona generation and scenario contexts.

## Next Checks

1. **Bootstrap methodology replication**: Repeat the DPD significance testing using different bootstrap iteration counts (n=1,000, 10,000, 100,000) to verify stability of reported significance levels.
2. **Cross-model persona-action isolation**: Generate personas with one LLM version and actions with another (e.g., personas with GPT-3, actions with GPT-4o) to isolate bias sources between persona generation and decision-making.
3. **Real-world correlation validation**: Collect real-world demographic decision rate data for one scenario (e.g., emergency evacuation rates by race/ethnicity) and quantitatively compare the magnitude of disparities to LLM-simulated implicit biases.