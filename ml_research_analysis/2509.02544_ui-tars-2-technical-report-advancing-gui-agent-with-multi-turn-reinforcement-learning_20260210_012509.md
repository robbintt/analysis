---
ver: rpa2
title: 'UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement
  Learning'
arxiv_id: '2509.02544'
source_url: https://arxiv.org/abs/2509.02544
tags:
- training
- arxiv
- agents
- agent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UI-TARS-2 addresses challenges in developing autonomous GUI agents
  by introducing a systematic training methodology that integrates scalable data generation,
  multi-turn reinforcement learning, hybrid GUI environments, and a unified sandbox
  platform. The approach uses a data flywheel to co-evolve model capabilities and
  data quality, stabilizes long-horizon RL through asynchronous rollouts and reward
  shaping, extends GUI operation with file systems and terminals, and enables large-scale
  reproducible rollouts.
---

# UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.02544
- Source URL: https://arxiv.org/abs/2509.02544
- Reference count: 40
- Key outcome: UI-TARS-2 achieves 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents.

## Executive Summary
UI-TARS-2 presents a systematic training methodology for autonomous GUI agents that integrates scalable data generation, multi-turn reinforcement learning, hybrid GUI environments, and a unified sandbox platform. The approach employs a data flywheel to co-evolve model capabilities and data quality, stabilizes long-horizon reinforcement learning through asynchronous rollouts and reward shaping, extends GUI operation to include file systems and terminals, and enables large-scale reproducible rollouts. The model demonstrates state-of-the-art performance across multiple benchmarks including 88.2 on Online-Mind2Web, competitive game performance at 59.8 mean normalized score, and strong generalization to software engineering tasks with 68.7 on SWE-Bench.

## Method Summary
UI-TARS-2 advances autonomous GUI agents through a systematic training methodology that integrates scalable data generation, multi-turn reinforcement learning, hybrid GUI environments, and a unified sandbox platform. The approach uses a data flywheel to co-evolve model capabilities and data quality, stabilizes long-horizon RL through asynchronous rollouts and reward shaping, extends GUI operation with file systems and terminals, and enables large-scale reproducible rollouts. The methodology addresses key challenges in GUI agent development by providing stable training dynamics, comprehensive environment support, and strong generalization capabilities across diverse task domains.

## Key Results
- Achieves 88.2 on Online-Mind2Web benchmark
- Attains 59.8 mean normalized score across 15 game titles, approximately 60% of human performance
- Scores 68.7 on SWE-Bench with GUI-SDK extensions for software engineering tasks

## Why This Works (Mechanism)
UI-TARS-2 succeeds by addressing the fundamental challenges of GUI agent development through a multi-pronged approach. The data flywheel mechanism ensures continuous improvement of both model capabilities and data quality through iterative co-evolution. Multi-turn reinforcement learning with asynchronous rollouts and reward shaping provides stability for long-horizon tasks that typically suffer from credit assignment and exploration challenges. The hybrid GUI environment support, including file systems and terminals, extends the agent's operational scope beyond traditional GUI interactions. The unified sandbox platform enables reproducible, large-scale training rollouts that scale effectively across different task domains and operating systems.

## Foundational Learning
- **Data Flywheel**: Why needed - To continuously improve model capabilities and data quality through iterative co-evolution. Quick check - Verify quality improvement metrics show consistent upward trend across training iterations.
- **Multi-Turn Reinforcement Learning**: Why needed - To handle long-horizon tasks with proper credit assignment and exploration-exploitation balance. Quick check - Measure task completion rates and reward convergence across different horizon lengths.
- **Hybrid GUI Environments**: Why needed - To extend agent functionality beyond GUI interactions to include file systems and terminal operations. Quick check - Test agent performance across GUI, file system, and terminal tasks independently.
- **Asynchronous Rollouts**: Why needed - To stabilize training and enable parallel execution of multiple trajectories. Quick check - Compare training stability metrics with and without asynchronous execution.

## Architecture Onboarding

**Component Map**: Data Generation -> Multi-Turn RL -> Hybrid Environments -> Unified Sandbox -> Performance Evaluation

**Critical Path**: The critical path begins with data generation through the flywheel mechanism, flows into the multi-turn RL training pipeline, passes through hybrid environment integration, executes in the unified sandbox platform, and culminates in performance evaluation across benchmarks.

**Design Tradeoffs**: The architecture prioritizes scalability and generalization over task-specific optimization, trading some fine-grained control for broader applicability. The unified sandbox approach sacrifices some environment-specific optimizations to enable cross-platform consistency and reproducibility.

**Failure Signatures**: Training instability manifests as reward plateaus or degradation, often indicating issues with asynchronous rollout synchronization or reward shaping configuration. Poor generalization typically signals insufficient diversity in the synthetic training data or inadequate exploration during RL training.

**First 3 Experiments**:
1. **Data Quality Validation**: Run controlled training with varying data quality levels to quantify the impact of the flywheel mechanism on model performance.
2. **Environment Coverage Test**: Execute the agent across GUI, file system, and terminal tasks to validate hybrid environment integration.
3. **Benchmark Reproduction**: Replicate performance on Online-Mind2Web, OSWorld, and AndroidWorld to verify reported benchmark results.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on benchmark performance without extensive real-world deployment testing, raising generalizability concerns
- Training methodology relies heavily on synthetic data generation without thorough analysis of potential biases compared to human-generated interactions
- Implementation details for scaling hybrid GUI environments across different operating systems and application types remain somewhat abstract

## Confidence
- **High Confidence**: Architectural framework and multi-turn RL methodology are well-described and theoretically sound; benchmark results on established datasets are specific and reproducible
- **Medium Confidence**: Game performance claims and LMGame-Bench competitiveness with OpenAI o3 are supported by metrics but lack detailed normalization methodology
- **Medium Confidence**: GUI-SDK extension results on Terminal Bench and SWE-Bench are promising but integration approach and generalization beyond these benchmarks require further validation

## Next Checks
1. **Real-World Deployment Testing**: Conduct extended trials of UI-TARS-2 in uncontrolled, diverse environments across different operating systems and application types to assess generalization beyond benchmark settings

2. **Synthetic Data Quality Analysis**: Perform systematic evaluation comparing synthetic training data quality and diversity against human-generated interaction data, including bias analysis and failure mode identification

3. **Direct Baseline Comparison**: Replicate benchmark evaluations using identical conditions and versions of competing models (Claude, OpenAI agents, o3) to enable fair, apples-to-apples performance comparison