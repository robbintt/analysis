---
ver: rpa2
title: 'MTMD: A Multi-Task Multi-Domain Framework for Unified Ad Lightweight Ranking
  at Pinterest'
arxiv_id: '2510.09857'
source_url: https://arxiv.org/abs/2510.09857
tags:
- expert
- domain
- ranking
- mtmd
- lightweight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MTMD addresses the challenge of unifying multiple ad ranking tasks\
  \ and domains in a lightweight ranking layer by proposing a Multi-Task Multi-Domain\
  \ architecture based on the two-tower paradigm. The core method introduces Domain\
  \ Experts\u2014mixture-of-experts modules that learn both specialized and shared\
  \ knowledge across surfaces and ad products, with domain adaptation to handle missing\
  \ features and constrained modeling to capture task dependencies."
---

# MTMD: A Multi-Task Multi-Domain Framework for Unified Ad Lightweight Ranking at Pinterest

## Quick Facts
- arXiv ID: 2510.09857
- Source URL: https://arxiv.org/abs/2510.09857
- Reference count: 25
- Primary result: Unified multi-task multi-domain ad ranking with 2% CPC reduction and 2-3% CTR increase in production

## Executive Summary
MTMD addresses the challenge of unifying multiple ad ranking tasks and domains in a lightweight ranking layer by proposing a Multi-Task Multi-Domain architecture based on the two-tower paradigm. The core method introduces Domain Experts—mixture-of-experts modules that learn both specialized and shared knowledge across surfaces and ad products, with domain adaptation to handle missing features and constrained modeling to capture task dependencies. Each Domain Expert combines deep/shallow task-specific experts, shared experts, and expert routing with DCN for feature crossing. Offline, MTMD improves LogMAE by 12–36% across all tasks and domains. Online A/B tests show a 2% reduction in CPC and 2–3% increase in CTR, with consistent gains across surfaces, ad products, and optimization goals. The model replaced 9 production models, demonstrating superior performance and maintenance efficiency.

## Method Summary
MTMD implements a two-tower architecture where each tower contains Domain Experts for different surfaces (home feed, search, related pins) or ad products (standard, shopping). Each Domain Expert uses mixture-of-experts with task-specific deep/shallow experts, shared experts, and learned routing weights, followed by DCN V2 for feature crossing. Domain adaptation via per-domain BatchNorm and SE-Block handles missing features across heterogeneous domains. Constrained modeling explicitly encodes task dependencies through conditional probabilities and KL-divergence loss to heavyweight ranker predictions. The model serves as a pre-ranker before fine ranking, handling CTR, GCTR, OCTR, and CVR tasks simultaneously.

## Key Results
- Offline LogMAE improvement: 12–36% across all tasks and domains
- Online A/B tests: 2% CPC reduction, 2–3% CTR increase
- Replaced 9 separate production models with unified architecture
- Consistent improvements across all surfaces, ad products, and optimization goals

## Why This Works (Mechanism)

### Mechanism 1: Domain Expert with Mixture-of-Experts Decomposition
Decomposing domain knowledge into specialized and shared experts improves multi-domain generalization compared to monolithic data-mixing. Each Domain Expert contains task-specific deep experts (4-layer FFN for specialized knowledge), task-specific shallow experts (2-layer FFN for high-level categorical features), plus domain-shared and task-shared experts. An expert routing layer (3-layer FFN + softmax) learns weighted importance, followed by DCN V2 for explicit feature crossing. This allows the model to isolate domain-specific patterns while preserving transferable representations. Core assumption: Domain-specific and domain-invariant features can be meaningfully separated through learned routing; routing weights capture real expertise relevance. Evidence: Domain Adaptation provides +3.86% LogMAE improvement—largest single factor. Break condition: If routing weights collapse to uniform distribution (experts not specializing) or if gradient conflict between experts dominates, mechanism fails.

### Mechanism 2: Domain Adaptation via Feature Normalization and Squeeze-Excitation
Per-domain normalization and learned feature importance weighting mitigates missing-feature degradation when unifying heterogeneous domains. Continuous features pass through domain-specific BatchNorm layers before SE-Block, which learns adaptive channel weights per domain. This allows each domain to emphasize different feature subsets and handle cases where features exist only for specific domains. Core assumption: SE-Block can learn meaningful feature importance differences across domains; per-domain BatchNorm prevents distribution mismatch from contaminating shared experts. Evidence: DCN V2 contributes +1.24% LogMAE; Domain Adaptation +3.86%. Break condition: If missing features cause SE-Block to learn spurious correlations (default values treated as signal), mechanism degrades.

### Mechanism 3: Constrained Modeling for Task Dependencies
Explicitly encoding conditional dependencies between hierarchical tasks improves both primary and derived task predictions. Instead of independent task heads, GCTR and OCTR are formulated as P(GCTR|CTR) × P(CTR), trained with KL-Divergence loss between lightweight and heavyweight ranker predictions. CTR embedding dimension (128) is 4× larger than GCTR/OCTR (32), with higher loss weight reflecting its foundational role. Core assumption: Task hierarchies reflect true causal structure; KL-Divergence to heavyweight scores is a meaningful consistency signal. Evidence: Constrained modeling adds +0.61% CTR and -0.81% CPC on Home feed beyond MTMD baseline. Break condition: If heavyweight ranker predictions are themselves noisy or biased, KL-Divergence loss propagates errors.

## Foundational Learning

- **Two-Tower Architecture (Dual Encoder)**: MTMD builds on two-tower paradigm for fast inference via dot product; understanding why towers are separated and how they interact at inference is prerequisite. Quick check: Can you explain why dot product inference is O(1) relative to candidate set size after pre-computation?

- **Mixture-of-Experts (MoE) Routing**: Domain Experts use MoE with learned routing; need to understand soft vs hard routing, load balancing, and expert collapse. Quick check: What happens to gradient flow if one expert receives near-zero routing weight for all samples?

- **Multi-Task Learning with Negative Transfer**: MTMD unifies 9+ models; task interference and negative transfer are key risks the architecture addresses. Quick check: Why might joint training of CTR and CVR tasks hurt CTR performance compared to single-task training?

## Architecture Onboarding

- **Component map**: Input Features → Domain Adaptation (BatchNorm + SE-Block) → Task-Specific Experts (Deep: 512→256→128→128 per task) + Task-Specific Experts (Shallow: 128→64 per task) + Domain-Shared Expert (512→256→128→128) + Task-Shared Expert (512→256→128→128) → Expert Routing (128→64→|Experts| → Softmax) → DCN V2 (Low-rank feature crossing) → Task Embeddings (CTR: 128-dim, GCTR/OCTR: 32-dim) → Query Tower: N Domain Experts by surface, Item Tower: M Domain Experts by ad product → Inference: Dot product of task-specific embeddings

- **Critical path**: Feature preprocessing → Domain Adaptation quality determines downstream expert effectiveness → Expert routing weights → If collapsed, shared experts dominate and domain specialization lost → KL-Divergence loss weighting → Too high forces lightweight to copy heavyweight without learning

- **Design tradeoffs**: Single unified model vs. 9 separate models (maintenance simplicity vs. risk of negative transfer → unified wins with proper MoE); Embedding dimension allocation (larger for CTR vs. equal allocation → unequal 128/32/32 wins); DCN crossing depth (more crossing power vs. latency budget → low-rank DCN V2 for efficiency)

- **Failure signatures**: Expert routing entropy near maximum (uniform weights: experts not specializing); LogMAE improves but online CTR flat/degrades (distillation gap from heavyweight ranker); One surface improves, another degrades significantly (negative transfer between domains); Training loss spikes on specific domain batches (BatchNorm statistics unstable for that domain)

- **First 3 experiments**: 1. Domain adaptation ablation: Train MTMD with and without SE-Block + per-domain BatchNorm; measure LogMAE per domain. Expected: +3-4% improvement with adaptation. 2. Expert utilization analysis: Log routing weight distribution across experts per domain. Verify specialists activate for their domain; shared experts have broader activation. If uniform, reduce expert count or add load-balancing regularization. 3. Constrained vs. independent task heads: A/B test with KL-Divergence loss and hierarchical embedding dimensions vs. equal-dimension independent heads. Expected: constrained modeling adds +0.5-0.8% CTR improvement.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can interaction modules between the query and item towers be incorporated while still satisfying the strict latency requirements of lightweight ranking at scale?
**Basis in paper**: Future work section states: "The other direction is to allow better interaction between the query tower and the pin tower to make the model more effective in capturing the personalized needs of the users."
**Why unresolved**: The paper explicitly excludes interaction modules because dot product is the only affordable inference function at Pinterest's scale; the trade-off between interaction complexity and latency remains unexplored.
**What evidence would resolve it**: A study measuring latency vs. ranking quality when introducing controlled interaction modules (e.g., cross-attention layers) with varying computational budgets.

### Open Question 2
**Question**: What methods can improve consistency between lightweight and heavyweight ranking outputs to reduce funnel misalignment?
**Basis in paper**: Future work section states: "The first is the alignment of the delivery funnel, where the goal is to make the output of lightweight ranking and heavyweight ranking more consistent for better ad delivery efficiency."
**Why unresolved**: While MTMD uses heavyweight scores as labels for natural alignment, explicit techniques for enforcing consistency are not investigated.
**What evidence would resolve it**: Experiments with distillation-based or joint training approaches that directly optimize a consistency loss between the two ranking stages.

### Open Question 3
**Question**: What is the optimal formulation for constrained modeling of task dependencies beyond the conditional probability approach used for CTR/GCTR/OCTR?
**Basis in paper**: Table 5 shows constrained modeling yields mixed results: Home feed CTR improves (+0.61%) but Search CTR degrades (-0.18%). The paper notes offline gains were "marginal after the 5th bucket."
**Why unresolved**: The current KL-divergence-based constraint formulation may not capture all task relationships optimally, and the inconsistent online results suggest the approach needs refinement.
**What evidence would resolve it**: Ablation studies comparing alternative constraint formulations (e.g., multi-label dependencies, hierarchical losses) with consistent improvements across all surfaces and tasks.

### Open Question 4
**Question**: Can the MTMD architecture generalize effectively to platforms with fundamentally different domain structures (e.g., e-commerce with purchase funnels, video platforms with watch-time metrics)?
**Basis in paper**: Section 2.1.1 defines domain as "platform dependent" with Pinterest-specific surfaces (home feed, search, related Pin) and ad products (standard, shopping). No evaluation exists beyond this context.
**Why unresolved**: The domain expert design encodes assumptions about Pinterest's specific domain taxonomy; applicability to other domain structures is untested.
**What evidence would resolve it**: Implementation and evaluation of MTMD on a different platform's ad ranking system with distinct surface/product definitions, reporting comparable offline and online metrics.

## Limitations
- Limited details on exact feature preprocessing pipelines, embedding cardinalities, and training hyperparameters (learning rate, batch size, optimizer, DCN V2 cross-layer count and rank)
- Constrained modeling approach relies heavily on quality of heavyweight ranker predictions as distillation targets, but paper doesn't analyze what happens if these predictions are biased or noisy
- While domain adaptation shows strong offline gains, there's limited analysis of how well the SE-Block and per-domain BatchNorm handle extreme missing-feature scenarios

## Confidence

- **High Confidence**: Claims about offline LogMAE improvements (12-36%) and online A/B test results (2% CPC reduction, 2-3% CTR increase) - these are directly measured outcomes from production deployment
- **Medium Confidence**: Claims about mechanism effectiveness (DCN V2 contribution, domain adaptation benefits) - supported by ablation studies but lack external validation in corpus papers
- **Low Confidence**: Claims about negative transfer prevention and long-term maintenance benefits - supported by single deployment case without comparative analysis against alternative architectures

## Next Checks

1. **Feature Missingness Analysis**: Systematically remove features that exist only for specific domains (e.g., shopping-specific features for standard ads) and measure SE-Block's ability to maintain prediction quality without learning spurious correlations from default values

2. **Expert Routing Dynamics**: Monitor routing weight distributions during training to verify experts are specializing rather than collapsing to uniform utilization. Implement and test load-balancing regularization if collapse occurs

3. **Heavyweight Ranker Sensitivity**: Inject controlled noise into heavyweight prediction targets and measure how quickly and severely MTMD's performance degrades, establishing the true reliability of the distillation signal