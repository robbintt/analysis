---
ver: rpa2
title: 'On Modality Incomplete Infrared-Visible Object Detection: An Architecture
  Compatibility Perspective'
arxiv_id: '2511.06406'
source_url: https://arxiv.org/abs/2511.06406
tags:
- modality
- dropout
- detection
- scenarios
- scarf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of infrared-visible object detection
  (IVOD) when modality data is incomplete, a common real-world issue due to sensor
  faults or poor imaging conditions. The authors propose a plug-and-play Scarf Neck
  module for DETR variants, introducing a modality-agnostic deformable attention mechanism
  that enables flexible adaptation to single or double modalities during training
  and inference.
---

# On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective

## Quick Facts
- **arXiv ID:** 2511.06406
- **Source URL:** https://arxiv.org/abs/2511.06406
- **Reference count:** 16
- **Primary result:** Scarf-DETR achieves up to 40.5% mAP gains in VIS-only scenarios and superior or competitive results in standard IVOD benchmarks.

## Executive Summary
This paper addresses the challenge of infrared-visible object detection (IVOD) when modality data is incomplete, a common real-world issue due to sensor faults or poor imaging conditions. The authors propose a plug-and-play Scarf Neck module for DETR variants, introducing a modality-agnostic deformable attention mechanism that enables flexible adaptation to single or double modalities during training and inference. A pseudo modality dropout strategy is also introduced to enhance model robustness and performance in both complete and incomplete modality scenarios. Experiments across three datasets show significant improvements, with the proposed Scarf-DETR achieving up to 40.5% mAP gains in VIS-only scenarios and superior or competitive results in standard IVOD benchmarks.

## Method Summary
The method introduces a Scarf Neck module that replaces standard DETR necks, featuring Modality-Agnostic Deformable Attention (MADA) that processes both modalities with shared parameters regardless of input completeness. The architecture uses a shared Swin-S backbone to extract multi-scale features, which are processed through two Scarf Blocks per scale using MADA and feed-forward networks, then fused via 1×1 convolution. During training, a pseudo modality dropout strategy separates paired images into individual samples, effectively doubling the batch while maintaining modality independence. The model is trained for 12 epochs using AdamW with 1e-4 learning rate, with a 60% dropout ratio that balances complete and incomplete modality performance.

## Key Results
- Scarf-DETR achieves up to 40.5% mAP gains in VIS-only scenarios compared to baselines.
- Superior or competitive performance in standard IVOD benchmarks across three datasets (FLIR-aligned, M3FD, LLVIP).
- Pseudo modality dropout with 60% ratio effectively balances complete and single-modality performance.
- The plug-and-play architecture demonstrates compatibility with multiple DETR variants (Align-DETR, Co-DETR, DINO, DDQ).

## Why This Works (Mechanism)

### Mechanism 1: Modality-Agnostic Deformable Attention (MADA)
A unified attention mechanism enables the same model parameters to process both complete and incomplete modality inputs without architectural modification. In complete-modality scenarios, MADA concatenates tokens from both modalities at each spatial position to generate combined sampling offsets and attention weights, which are then split to update each modality's features separately. When one modality is missing, the available modality's tokens are stacked to match the expected dimensionality, and sampling points are doubled (from K to 2K) to maintain the same computational budget and feature enhancement capacity.

### Mechanism 2: Pseudo Modality Dropout for Training Robustness
Decoupling paired images during training—rather than removing them—improves robustness to missing modalities while preserving sample diversity. Instead of vanilla dropout which excludes images entirely (losing up to 30% of training data at 60% dropout rate), pseudo modality dropout separates paired images into individual samples that share the same ground truth labels. This creates `[I_v; I_t] ∈ R^(2bs×3×H×W)` from the original concatenated `I_C ∈ R^(bs×6×H×W)`, effectively doubling the batch while treating each modality independently.

### Mechanism 3: Architecture Decoupling via Plug-and-Play Neck Replacement
Inserting a compatibility-focused neck between shared backbone and task-specific heads enables any DETR variant to handle variable-modality inputs. The Scarf Neck replaces the standard neck in DETR architectures, processing multi-scale features from a shared backbone. Each scale uses two Scarf Blocks followed by 1×1 convolution. In complete scenarios, features from both modalities are fused through cross-modal attention within MADA. In incomplete scenarios, the available modality undergoes intra-modal enhancement only.

## Foundational Learning

- **Concept: Deformable Attention**
  - **Why needed here:** MADA builds directly on multi-scale deformable attention; understanding how sampling offsets and attention weights are learned from query features is essential for grasping how modality-agnostic sampling works.
  - **Quick check question:** Can you explain why deformable attention uses K sampling points per query rather than attending to all spatial positions?

- **Concept: DETR Architecture (Encoder-Decoder with Object Queries)**
  - **Why needed here:** Scarf-DETR is explicitly designed as a plug-and-play module for DETR variants; the neck outputs must be compatible with the transformer encoder-decoder pipeline.
  - **Quick check question:** What role do object queries play in DETR's detection head, and why must the neck output maintain consistent feature dimensions?

- **Concept: Multi-Scale Feature Pyramids**
  - **Why needed here:** Scarf Neck operates at multiple scales (4 levels with different feature map resolutions), and understanding how features are processed hierarchically is critical for implementation.
  - **Quick check question:** Why might small objects benefit from higher-resolution feature maps, and how does the Scarf Neck maintain separate processing per scale?

## Architecture Onboarding

- **Component map:** Input: IR image + VIS image (both optional) → Shared Backbone (Swin-S/L): Extracts multi-scale features for each modality → Scarf Neck (4 groups, one per scale): 2× Scarf Blocks (each with MADA + FFN) → 1× 1×1 Conv (fuses modality features) → Encoder: Standard deformable transformer encoder → Decoder: Standard transformer decoder with object queries → Detection Heads: Class + Box predictions

- **Critical path:**
  1. Implement MADA first—this is the core innovation. Start with the modality-complete case (Equations 1-2), then add the incomplete case (Equation 3).
  2. Integrate Scarf Blocks into existing DETR neck position; verify dimensions match at boundaries.
  3. Add pseudo modality dropout to data loader; this requires modifying how paired images are batched during training.

- **Design tradeoffs:**
  - **Number of Scarf Blocks:** Paper uses 2 per scale (Table 5 shows 2 blocks outperform 1, but 4 gives marginal gains with more compute). Start with 2.
  - **Sampling points K:** Default is 4; increasing K improves fine-grained sampling but increases computation linearly.
  - **Dropout ratio:** 60% is recommended (Table 6 shows this balances complete/incomplete performance); higher ratios improve missing-modality robustness at cost of complete-scenario performance.
  - **Backbone sharing:** Shared backbone reduces parameters but assumes both modalities benefit from same pre-training; separate backbones may help if modalities have very different statistics.

- **Failure signatures:**
  - **Complete-modality performance drops significantly:** Likely dropout ratio too high (>60%) or backbone not properly initialized.
  - **Single-modality performance near zero:** Pseudo modality dropout not implemented correctly, or MADA not handling stacked tokens properly.
  - **Memory overflow during training:** Pseudo dropout doubles effective batch size; reduce batch size or implement gradient accumulation.
  - **Inconsistent results across runs:** Modality dropout is stochastic; fix random seed as paper does (Appendix B).

- **First 3 experiments:**
  1. **Sanity check:** Train Scarf-DETR on FLIR-aligned with 0% dropout. Test on complete, VIS-only, and IR-only. Expect: high complete performance, near-zero single-modality performance (reproducing the problem).
  2. **Ablation on dropout ratio:** Train with 20%, 40%, 60% pseudo dropout on FLIR-aligned. Plot complete vs. single-modality performance. Expect: tradeoff curve with 60% near optimal (Table 6).
  3. **Cross-dataset transfer:** Train on FLIR (daytime driving), test on LLVIP (nighttime surveillance) with IR-only input. Expect: significant domain gap, but Scarf-DETR should maintain reasonable performance compared to baselines that may completely fail.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Scarf Neck architecture generalize effectively to non-DETR detection frameworks (e.g., YOLO, Faster R-CNN, RetinaNet) while maintaining modality compatibility?
- **Basis in paper:** The authors explicitly state the module is designed for "DETR variants" and demonstrate integration with Align-DETR, Co-DETR, DINO, and DDQ, but do not explore whether the modality-agnostic deformable attention mechanism transfers to anchor-based or single-stage detectors with fundamentally different neck structures.

### Open Question 2
- **Question:** Would an adaptive pseudo dropout strategy (rather than a fixed 60% ratio) yield better robustness across datasets with varying modality dominance patterns?
- **Basis in paper:** The ablation study (Table 6) tests dropout ratios from 20% to 100% and selects 60% as a balanced choice, but this selection is dataset-agnostic. The paper also observes that IR dominates on LLVIP while VIS dominates on M3FD, suggesting optimal dropout may vary by dataset characteristics.

## Limitations

- **Pseudo modality dropout implementation ambiguity:** The exact mechanism for separating paired images into individual samples within MMDetection's fixed-batch-size pipeline is not clearly specified, potentially affecting reproducibility.
- **MI benchmark construction underspecification:** The paper mentions creating MI benchmarks by randomly discarding specific percentages of images but doesn't specify random seed selection or exact indices used for train/val splits.
- **Limited cross-dataset generalization validation:** Only LLVIP is tested for cross-dataset performance, providing insufficient evidence for the model's ability to handle domain shifts between different IVOD scenarios.

## Confidence

- **High confidence:** The MADA mechanism and its implementation details are well-specified and directly supported by equations and architectural descriptions. The plug-and-play nature of the Scarf Neck and its compatibility with DETR variants is clearly demonstrated.
- **Medium confidence:** The pseudo modality dropout strategy's effectiveness is supported by Table 6, but the implementation details are ambiguous enough to potentially affect reproducibility. The 60% dropout ratio appears optimal but the sensitivity to this hyperparameter isn't thoroughly explored.
- **Low confidence:** Cross-dataset generalization claims are based on limited experiments (only LLVIP tested), and the paper doesn't adequately address domain shift challenges or provide ablation studies on different dataset combinations.

## Next Checks

1. **Implement pseudo modality dropout correctly:** Verify the data loader properly separates paired images into individual samples with shared ground truth labels, ensuring batch dimension handling matches the described `2B × 3 × H × W` output format.
2. **Test dropout ratio sensitivity:** Train models with 20%, 40%, 60%, and 80% dropout ratios on FLIR-aligned, measuring the complete vs. single-modality performance tradeoff curve to validate the claimed 60% optimality.
3. **Cross-dataset domain transfer:** Train on FLIR-aligned (daytime) and test on LLVIP (nighttime) with IR-only input, measuring performance degradation compared to same-domain testing to quantify domain adaptation challenges.