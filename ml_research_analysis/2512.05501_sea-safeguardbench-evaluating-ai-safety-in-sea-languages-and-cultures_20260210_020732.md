---
ver: rpa2
title: 'SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures'
arxiv_id: '2512.05501'
source_url: https://arxiv.org/abs/2512.05501
tags:
- content
- safety
- prompt
- cultural
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SEA-SafeguardBench is the first culturally grounded multilingual\
  \ safety benchmark for Southeast Asian languages. It contains 13,830 prompts and\
  \ 7,810 responses across three subsets\u2014general, in-the-wild, and content generation\u2014\
  covering eight languages and 1,338 cultural topics."
---

# SEA-SafeguardBench: Evaluating AI Safety in Southeast Asian Languages and Cultures

## Quick Facts
- **arXiv ID**: 2512.05501
- **Source URL**: https://arxiv.org/abs/2512.05501
- **Reference count**: 40
- **Primary result**: First culturally grounded multilingual safety benchmark for SEA languages

## Executive Summary
SEA-SafeguardBench introduces a novel benchmark for evaluating AI safety in Southeast Asian languages, addressing the critical gap in culturally relevant safety testing. The benchmark contains 13,830 prompts and 7,810 responses across eight SEA languages, covering 1,338 cultural topics through three distinct subsets: general, in-the-wild, and content generation. Evaluations on 20 safeguard models reveal consistent underperformance on SEA languages compared to English, with average AUPRC drops of 5.7-6.1 points. The findings highlight that current safety models struggle with culturally nuanced content and demonstrate the importance of incorporating cultural awareness into safety alignment for underrepresented regions.

## Method Summary
The benchmark was constructed through a mixed-method approach combining expert knowledge, crowdsourced contributions, and GPT-generated prompts. The dataset includes 13,830 prompts across eight SEA languages (Indonesian, Thai, Vietnamese, Malay, Tagalog, Burmese, Khmer, and Lao) and English. Prompts were organized into three subsets: general prompts (culturally safe topics), in-the-wild prompts (derived from real user queries), and content generation prompts (sensitive topics). Each prompt was evaluated for safety across four categories: offensive, sexual, violence, and profanity. Responses were generated using both GPT models and crowdsourced workers. The benchmark evaluates 20 safeguard models including both commercial API-based systems and open-source models, using metrics such as AUPRC, F1, and calibration error.

## Key Results
- Safety models show consistent underperformance on SEA languages with average AUPRC drops of 5.7-6.1 points compared to English
- Content generation subset reveals the largest performance gap, with AUPRC drops of 36.4 points in English and 36.2 in SEA languages
- Models pretrained on culturally relevant data show improved performance when cultural awareness is incorporated
- Error analysis reveals models misclassify sensitive content as either clearly safe or harmful, with sensitivity to threshold settings

## Why This Works (Mechanism)
The benchmark addresses the fundamental limitation of existing safety evaluations that are primarily designed for Western contexts and English language. By incorporating culturally specific knowledge and linguistic diversity from Southeast Asia, the benchmark reveals performance gaps that would otherwise remain hidden. The three-subset structure captures different real-world scenarios from general conversations to sensitive content generation. The inclusion of both expert and crowdsourced prompt generation ensures cultural authenticity while maintaining scalability.

## Foundational Learning
- **Multilingual safety evaluation**: Why needed - existing benchmarks focus on English; Quick check - compare model performance across languages
- **Cultural context in safety**: Why needed - safety perceptions vary across cultures; Quick check - expert review of culturally sensitive prompts
- **AUPRC metric**: Why needed - handles class imbalance in safety classification; Quick check - compare AUPRC vs accuracy on imbalanced datasets
- **Prompt-based evaluation**: Why needed - simulates real-world user interactions; Quick check - analyze response generation quality
- **Error analysis in safety models**: Why needed - identify systematic failures; Quick check - confusion matrix analysis across categories
- **Threshold calibration**: Why needed - safety models require different operating points; Quick check - ROC curve analysis

## Architecture Onboarding
**Component Map**: Prompts -> Generation -> Evaluation -> Analysis
**Critical Path**: Prompt generation → Response generation → Safety model inference → Metric calculation → Error analysis
**Design Tradeoffs**: Mixed prompt sources (quality vs scalability), limited responses per prompt (cost vs coverage), fixed thresholds vs calibrated scores
**Failure Signatures**: Systematic misclassification of cultural content, threshold sensitivity, performance degradation on low-resource languages
**Three First Experiments**: 
1. Evaluate additional safeguard models to establish performance baselines
2. Test prompt variations to measure sensitivity to linguistic differences
3. Conduct human evaluation of model decisions on edge cases

## Open Questions the Paper Calls Out
The paper identifies several key open questions regarding the generalizability of results beyond tested models and languages, potential cultural bias in prompt generation despite expert review, and the impact of dataset construction methods on evaluation outcomes. It also raises questions about the scalability of culturally-aware pretraining approaches and the need for more diverse representation in safety benchmarks.

## Limitations
- Results may not generalize beyond specific models and languages tested
- Potential cultural bias in prompt generation despite expert review
- Small number of responses per prompt limits statistical power
- Dataset construction methods may introduce inconsistencies in quality

## Confidence
- High confidence in the existence of performance gaps between English and SEA languages
- Medium confidence in the specific magnitude of AUPRC drops due to dataset construction methods
- Medium confidence in the effectiveness of culturally-aware pretraining based on limited model comparisons
- Low confidence in generalizability to all safety scenarios given the specific prompt categories tested

## Next Checks
1. Evaluate the same models on independently collected SEA language safety prompts to verify robustness of the performance gap findings
2. Test additional safety models including both larger and smaller parameter versions to establish whether performance patterns hold across model scales
3. Conduct cross-cultural validation by having SEA cultural experts review model outputs to assess whether misclassification patterns align with actual cultural sensitivities