---
ver: rpa2
title: 'ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual
  Question Answering'
arxiv_id: '2508.07321'
source_url: https://arxiv.org/abs/2508.07321
tags:
- answer
- question
- base
- indirection
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ObfusQAte, a framework to evaluate LLM robustness
  against obfuscated factual questions. ObfusQAte systematically generates complex,
  multi-layered question variants (Named-Entity Indirection, Distractor Indirection,
  and Contextual Overload) from base factual questions, increasing cognitive demand
  while preserving semantic intent.
---

# ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering

## Quick Facts
- **arXiv ID**: 2508.07321
- **Source URL**: https://arxiv.org/abs/2508.07321
- **Reference count**: 40
- **Primary result**: LLM performance drops significantly on obfuscated factual questions, especially for Distractor Indirection and Contextual Overload variants.

## Executive Summary
ObfusQAte is a novel framework designed to systematically evaluate the robustness of large language models (LLMs) on obfuscated factual question answering. The framework generates complex, multi-layered question variants—Named-Entity Indirection (NEI), Distractor Indirection (DI), and Contextual Overload (CO)—from base factual questions, thereby increasing cognitive demand while preserving semantic intent. Evaluated on 1024 questions using seven state-of-the-art LLMs, ObfusQAte reveals substantial performance declines, especially under zero-shot prompting, highlighting a critical vulnerability in LLMs' factual reasoning capabilities.

## Method Summary
The ObfusQAte framework generates obfuscated factual questions from a base set of 256 questions (sampled from TriviaQA and GKToday), creating three obfuscated variants per base question (NEI, DI, CO) for a total of 1024 questions. Obfuscations are produced using Gemini 2.0 Flash at temperature 0.75, with human-in-the-loop verification ensuring semantic preservation (inter-annotator agreement κ = 0.862). The framework evaluates seven LLMs using Exact Match (EM) accuracy, with normalization (lowercase, strip punctuation/whitespace), and intrinsic analyses including P(IK) confidence scores, membership inference, and layer-wise norm drop. Three prompting strategies are tested: zero-shot, few-shot (k=2–5), and Chain-of-Thought (CoT).

## Key Results
- GPT-4o accuracy drops from 67.97% on base questions to 25.78% on Distractor Indirection under zero-shot prompting.
- Significant performance declines observed across all obfuscation types, with DI and CO being most challenging.
- Intrinsic analyses reveal reduced model confidence, lack of memorization of obfuscated queries, and earlier hidden-state compression under obfuscation.

## Why This Works (Mechanism)
ObfusQAte works by increasing the cognitive load required to map obfuscated questions to their base semantic intent, thereby revealing LLM vulnerabilities in handling nuanced or contextually complex queries. The framework's systematic generation and evaluation methodology isolates the effects of different obfuscation types, allowing for precise measurement of LLM robustness.

## Foundational Learning
- **Exact Match (EM) Accuracy**: Needed to measure factual correctness; quick check: ensure normalization aligns with ground truth.
- **Inter-annotator Agreement (Cohen's κ)**: Validates human review quality; quick check: target κ > 0.8.
- **Layer-wise Norm Drop**: Indicates hidden-state compression; quick check: monitor norm across layers for each obfuscation type.

## Architecture Onboarding
- **Component Map**: Base Questions → Obfuscation Generation (Gemini 2.0 Flash) → Human Verification → LLM Evaluation (7 models) → EM + Intrinsic Analysis
- **Critical Path**: Obfuscation generation → semantic preservation → LLM evaluation → accuracy measurement
- **Design Tradeoffs**: Strict EM vs. semantic equivalence; human verification cost vs. automation; few-shot vs. zero-shot evaluation
- **Failure Signatures**: Obfuscation drifts semantic intent; EM too strict for valid paraphrases; subset selection bias
- **First Experiments**:
  1. Generate a small set of NEI, DI, and CO variants using a different LLM (e.g., Claude 3.5 Sonnet) and compare semantic preservation.
  2. Run EM accuracy on a subset of questions with fuzzy matching (e.g., Levenshtein distance threshold) alongside strict EM.
  3. Conduct ablation studies on obfuscation components to isolate which type most impacts performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on human-verified Gemini 2.0 Flash outputs introduces potential subjectivity in obfuscation generation.
- Strict EM accuracy metric may underrepresent model performance on semantically equivalent but surface-level different answers.
- Study focuses on factual QA without exploring obfuscation effects on other LLM capabilities like reasoning or multi-hop inference.

## Confidence
- Claim cluster (LLM performance drop on obfuscated questions): High
- Claim cluster (framework validity and methodology): Medium
- Claim cluster (intrinsic analysis interpretations): Low

## Next Checks
1. Replicate the obfuscation generation pipeline using a different LLM (e.g., Claude 3.5 Sonnet) to assess consistency across generation models.
2. Implement fuzzy matching (e.g., Levenshtein distance threshold) alongside EM accuracy to capture semantically correct but surface-level different responses.
3. Conduct ablation studies on obfuscation components (NEI, DI, CO) to isolate which obfuscation type most impacts performance.