---
ver: rpa2
title: 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents'
arxiv_id: '2503.20201'
source_url: https://arxiv.org/abs/2503.20201
tags:
- action
- search
- thought
- reasoning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Open Deep Search (ODS) closes the gap between proprietary search
  AI and open-source solutions by integrating powerful open-source LLMs with novel
  search and reasoning capabilities. ODS introduces Open Search Tool, which enhances
  web search retrieval by rephrasing queries, extracting relevant context, and implementing
  custom handling for major websites, outperforming proprietary counterparts.
---

# Open Deep Search: Democratizing Search with Open-source Reasoning Agents

## Quick Facts
- arXiv ID: 2503.20201
- Source URL: https://arxiv.org/abs/2503.20201
- Authors: Salaheddin Alzubi; Creston Brooks; Purva Chiniya; Edoardo Contente; Chiara von Gerlach; Lucas Irwin; Yihan Jiang; Arda Kaz; Windsor Nguyen; Sewoong Oh; Himanshu Tyagi; Pramod Viswanath
- Reference count: 40
- Primary result: ODS achieves 88.3% accuracy on SimpleQA and 75.3% on FRAMES, surpassing proprietary solutions like Perplexity Sonar Reasoning Pro and GPT-4o Search Preview.

## Executive Summary
Open Deep Search (ODS) closes the gap between proprietary search AI and open-source solutions by integrating powerful open-source LLMs with novel search and reasoning capabilities. ODS introduces Open Search Tool, which enhances web search retrieval by rephrasing queries, extracting relevant context, and implementing custom handling for major websites, outperforming proprietary counterparts. It also includes Open Reasoning Agent, offering two versions: a ReAct-based agent (ODS-v1) and a CodeAct-based agent (ODS-v2). These agents interpret queries, orchestrate tools, and leverage web search to answer questions effectively. When combined with DeepSeek-R1, ODS achieves state-of-the-art performance, surpassing closed-source solutions like Perplexity Sonar Reasoning Pro and GPT-4o Search Preview on benchmarks like SimpleQA (88.3% accuracy) and FRAMES (75.3% accuracy). ODS is a flexible, plug-and-play framework designed to democratize access to advanced search AI.

## Method Summary
Open Deep Search (ODS) is an open-source framework that combines reasoning agents with web search capabilities to answer factual queries. The method has two main components: (1) Open Search Tool: processes queries through rephrasing, SERP retrieval, content scraping, passage chunking, and reranking to produce relevant context; (2) Open Reasoning Agent: offers two implementations - ODS-v1 uses ReAct framework with Chain-of-Thought and self-consistency sampling, while ODS-v2 uses CodeAct with SmolAgents framework. The agents use tools including web search, Wolfram Alpha calculator, and continue-thinking functionality. ODS is evaluated on FRAMES (828 multi-hop questions) and SimpleQA (4,326 short-form factuality questions) using DeepSeek-R1 and Llama3.1-70B as base LLMs.

## Key Results
- ODS achieves 88.3% accuracy on SimpleQA, surpassing Perplexity Sonar Reasoning Pro (78.5%) and GPT-4o Search Preview (80.5%)
- ODS achieves 75.3% accuracy on FRAMES, outperforming Perplexity Sonar Reasoning Pro (64.7%) and GPT-4o Search Preview (69.1%)
- ODS-v2+DeepSeek-R1 uses adaptive search allocation (1.45 searches on SimpleQA vs 3.39 on FRAMES), demonstrating efficient multi-hop reasoning

## Why This Works (Mechanism)

### Mechanism 1: Query Rephrasing for Retrieval Diversity
Generating multiple semantically related queries improves coverage when original queries are ambiguous or overly broad. The Open Search Tool produces k rephrased queries that preserve intent while expanding lexical diversity. This bridges the gap between natural-language phrasing and search-index vocabulary. The base LLM can accurately generate semantically equivalent paraphrases without drift.

### Mechanism 2: Reasoning-Action Loop with Self-Correction
Interleaving explicit reasoning traces with tool calls enables iterative refinement when initial retrieval is insufficient. The ReAct agent follows a Thought → Action → Observation → Thought cycle. The CodeAct agent generates executable Python code for tool calls. Both allow the agent to detect information gaps and trigger additional searches dynamically.

### Mechanism 3: Adaptive Search Allocation
Agents that conditionally decide when to search outperform fixed-retrieval approaches on variable-complexity queries. ODS agents assess initial search quality and model confidence to determine whether additional searches are warranted. Agents that can accurately diagnose when retrieval is incomplete achieve better performance with fewer searches.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** ODS extends RAG by adding reasoning agents that dynamically call retrieval tools rather than single-shot retrieval.
  - **Quick check question:** Can you explain why standard RAG struggles with multi-hop questions requiring information from multiple sources?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** ODS-v1 uses CoT to decompose queries before tool calls; self-consistency sampling further improves robustness.
  - **Quick check question:** How does eliciting step-by-step reasoning change an LLM's behavior on arithmetic or multi-step inference?

- **Concept: ReAct Agent Framework**
  - **Why needed here:** The Thought/Action/Observation loop is the core execution pattern for ODS-v1.
  - **Quick check question:** What is the difference between a ReAct agent's "Thought" step and its "Action" step?

## Architecture Onboarding

- **Component map:** Query → Open Reasoning Agent → (decides to search) → Open Search Tool → context returned → Agent reasons → (optional: additional searches) → Final Answer

- **Critical path:** User query flows through reasoning agent, which decides whether to invoke Open Search Tool. Retrieved context returns to agent for reasoning and answer synthesis, with optional additional search iterations.

- **Design tradeoffs:**
  - **ODS-v1 (ReAct) vs ODS-v2 (CodeAct):** v2 achieves higher FRAMES accuracy (75.3% vs 56.7%) but uses more searches (3.39 vs 1.0 average). CodeAct appears better for multi-hop reasoning; ReAct is more parameter-efficient.
  - **DeepSeek-R1 vs Llama3.1-70B:** Stronger reasoning model reduces search iterations but increases compute cost per inference.

- **Failure signatures:**
  - Agent terminates with "insufficient information" when retrieval succeeds but reasoning fails to synthesize
  - Conflicting retrieved values cause incorrect selection without verification
  - Unit/format mismatches in final answers when conversion tools aren't invoked

- **First 3 experiments:**
  1. **Ablate query rephrasing:** Run ODS-v1 with k=1 (no rephrasing) vs k=3 on a held-out subset of FRAMES; measure accuracy drop to quantify rephrasing contribution.
  2. **Compare ReAct vs CodeAct on same base model:** Run both agent types with Llama3.1-70B on SimpleQA; isolate agent-architecture effects from model-effects.
  3. **Stress-test adaptive stopping:** Inject deliberately incomplete retrieval for a known subset of queries; measure how often the agent correctly requests additional searches vs terminates incorrectly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific properties of base LLMs predict success when integrated with ODS, and how does performance scale across the full range of open and closed models?
- **Basis in paper:** [inferred] The paper tests only Llama3.1-70B and DeepSeek-R1 as base models, noting DeepSeek-R1's superior reasoning leads to better ODS performance, but does not systematically characterize which model properties drive these gains.
- **Why unresolved:** Without systematic comparison across diverse base models, it remains unclear whether ODS benefits primarily from reasoning capabilities, parameter scale, or other factors.
- **What evidence would resolve it:** A controlled study evaluating ODS with multiple base LLMs varying in size, architecture, and reasoning training, correlating model characteristics with ODS performance metrics.

### Open Question 2
- **Question:** What factors determine when ODS reasoning agents decide additional web searches are needed, and can this decision boundary be formalized and optimized?
- **Basis in paper:** [inferred] The paper observes that ODS-v1+DeepSeek-R1 uses exactly 1.00 search per query on both benchmarks, while ODS-v1+Llama3.1-70B uses 1.09 and 1.05 searches, and ODS-v2+DeepSeek-R1 uses 1.45 and 3.39 searches respectively. The paper states agents "judiciously use" searches but does not characterize the decision criteria.
- **Why unresolved:** The mechanism for adaptive multi-search decisions remains opaque, making it difficult to predict or improve when agents should seek additional information.
- **What evidence would resolve it:** Analysis of agent reasoning traces correlating intermediate outputs with search decisions, followed by experiments testing whether explicit decision criteria can improve efficiency without sacrificing accuracy.

### Open Question 3
- **Question:** How does ODS performance generalize to evaluation benchmarks beyond SimpleQA and FRAMES, particularly for multi-step reasoning and real-world search scenarios?
- **Basis in paper:** [inferred] The paper evaluates only two factuality benchmarks and acknowledges they were originally designed for different purposes. No evaluation on other search or reasoning benchmarks is presented.
- **Why unresolved:** Without broader evaluation, it is unclear whether ODS's strong results reflect genuine search capabilities or benchmark-specific optimization.
- **What evidence would resolve it:** Evaluation of ODS on diverse benchmarks including multi-hop QA (HotpotQA), temporal reasoning (TimeQA), and open-ended information seeking tasks.

## Limitations
- Performance claims rely heavily on DeepSeek-R1 as the base model; ODS architecture may not transfer equally well to weaker LLMs
- The paper lacks ablation studies isolating the contribution of individual components (query rephrasing, reranking, self-consistency sampling)
- Embedding model and reranker specifications are unspecified, making faithful reproduction difficult
- Self-consistency sampling implementation details are not fully described, affecting reproducibility

## Confidence
- **High confidence:** Core architecture claims (ReAct and CodeAct agents successfully execute tool calls and produce answers on benchmark tasks) are well-supported by examples and performance metrics
- **Medium confidence:** Performance superiority claims over proprietary solutions are valid for the specific model combinations tested but may not generalize across all model families or benchmarks
- **Medium confidence:** Query rephrasing mechanism's contribution to retrieval diversity is logically sound and mechanistically plausible but lacks direct ablation evidence quantifying its impact

## Next Checks
1. **Ablate query rephrasing:** Run ODS-v1 with k=1 (no rephrasing) vs k=3 on a held-out subset of FRAMES; measure accuracy drop to quantify rephrasing contribution.
2. **Compare ReAct vs CodeAct on same base model:** Run both agent types with Llama3.1-70B on SimpleQA; isolate agent-architecture effects from model-effects.
3. **Stress-test adaptive stopping:** Inject deliberately incomplete retrieval for a known subset of queries; measure how often the agent correctly requests additional searches vs terminates incorrectly.