---
ver: rpa2
title: Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration
arxiv_id: '2505.23187'
source_url: https://arxiv.org/abs/2505.23187
tags:
- arxiv
- experience
- https
- agents
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAEL introduces cross-task experiential learning for LLM-based
  multi-agent systems, addressing the inefficiency of solving tasks in isolation.
  The framework models agent collaboration as a graph network where agents store and
  retrieve task-solving experiences across different tasks.
---

# Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration

## Quick Facts
- arXiv ID: 2505.23187
- Source URL: https://arxiv.org/abs/2505.23187
- Reference count: 18
- Key outcome: MAEL achieves up to 76.4% quality on SRDD software development tasks, outperforming AutoGen's 56.0% while reducing token consumption by up to 49% and accelerating convergence by 26%.

## Executive Summary
MAEL introduces cross-task experiential learning for LLM-based multi-agent systems, addressing the inefficiency of solving tasks in isolation. The framework models agent collaboration as a graph network where agents store and retrieve task-solving experiences across different tasks. During an experiential learning phase, agents collect and store step-level rewards with corresponding inputs/outputs. At inference, agents retrieve high-reward, task-relevant experiences to guide each reasoning step. Experiments across five diverse datasets show MAEL consistently outperforms state-of-the-art baselines, demonstrating significant efficiency gains through cross-task knowledge transfer.

## Method Summary
MAEL operates through a 4-agent undirected graph topology where agents collaboratively solve tasks using a divide-and-conquer workflow with iterative solver-critique refinement. During experiential learning, the system runs 30 training tasks per dataset, collecting step-level rewards at five decision types (solve, solvability judge, decompose, critique, aggregate) and storing (state, action, reward) tuples in per-agent experience pools. At inference, agents retrieve experiences using a reward-weighted similarity score (α=0.5) via text-embedding-ada-002, appending top-ranked examples as few-shot guidance. The workflow starts from the agent with highest closeness centrality, recursively decomposing tasks, solving subtasks, critiquing solutions, and aggregating results until convergence.

## Key Results
- MAEL_Step achieves 76.4% quality on SRDD software development, outperforming AutoGen's 56.0%
- Token consumption reduced by up to 49% compared to baselines across all datasets
- Convergence accelerated by up to 26% on complex tasks
- HumanEval performance improves from 76.7% to 90.0% as experience pool scales from 0 to 30 tasks

## Why This Works (Mechanism)

### Mechanism 1
Storing step-level experiences with reward annotations enables more targeted retrieval than storing complete task traces. During experiential learning, each critical decision step receives a task-specific reward computed by metric g(s_t, a_t), with the tuple (s_t, a_t, r_t) stored in the agent's experience pool P. This enables credit assignment across the workflow through task-specific metrics that meaningfully quantify intermediate reasoning step quality.

### Mechanism 2
Combining semantic similarity with reward scores in retrieval yields better guidance than either factor alone. For each agent input s_t, embeddings are computed and the retrieval score = α·similarity(s_t, s_j) + (1−α)·reward(s_j) with default α=0.5. This approach enables high-reward experiences from semantically similar past tasks to transfer effectively to new tasks without fine-tuning.

### Mechanism 3
A divide-and-conquer workflow with iterative solver-critique refinement improves solution quality over linear agent pipelines. Starting from the agent with highest closeness centrality, the system recursively decomposes tasks, solves subtasks if executable, critiques sub-solutions, and aggregates results. This iterative pattern catches errors earlier and reduces wasted exploration compared to single-pass generation.

## Foundational Learning

- **In-Context Learning / Few-Shot Prompting**: MAEL retrieves stored experiences as few-shot examples appended to agent inputs at each reasoning step. Quick check: Can you explain why increasing the number of few-shot examples does not always improve performance?

- **Graph Centrality (Closeness Centrality)**: The task-solving workflow initiates from the agent with highest closeness centrality in the multi-agent graph. Quick check: In an undirected graph of 4 agents where one agent connects to all others and the rest form a chain, which agent has highest closeness centrality?

- **Credit Assignment / Reward Modeling**: MAEL assigns step-level rewards to quantify action quality across decomposition, solving, critique, and aggregation steps. Quick check: Why is assigning credit harder in multi-step collaborative workflows than in single-agent tasks?

## Architecture Onboarding

- **Component map**: Graph Topology (4-agent undirected graph) → Experiential Learning Phase (collect/store (s_t, a_t, r_t)) → Inference Phase (reward-weighted retrieval) → Workflow Engine (divide-and-conquer + solver-critique)

- **Critical path**: 1. Define 4-agent graph topology; 2. Run experiential learning on 30 training tasks to populate experience pools; 3. At inference, retrieve top-ranked experience per decision step; 4. Execute decomposition → solve → critique → aggregate loop until convergence

- **Design tradeoffs**: Step-wise vs Task-wise retrieval (Step-wise best for SRDD quality but higher overhead); Experience pool size scaling (0→30 tasks improves HumanEval from 76.7%→90.0%, but non-monotonic at 20 tasks); Reward vs similarity weight (default α=0.5, dataset-dependent tuning)

- **Failure signatures**: Performance degradation on low-overlap tasks (MMLU: 80%→76.67% with Step-wise retrieval); Non-monotonic scaling with pool size (HumanEval dip at 20 tasks); Increased token consumption from overly long exemplars

- **First 3 experiments**: 1. Ablate retrieval strategy: Compare MAEL_ØExp, MAEL_Task, MAEL_Step on SRDD to isolate retrieval contribution; 2. Vary experience pool size: Run MAEL_Task on HumanEval with 0, 10, 20, 30 training tasks; 3. Tune retrieval scoring weights: Sweep α ∈ {0.0, 0.25, 0.5, 0.75, 1.0} on CommonGen-Hard

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework adaptively mitigate performance degradation on tasks with low solution overlap? The current retrieval mechanism lacks a rejection threshold or dynamic gating to prevent retrieval when prior experiences are structurally dissimilar. Evidence: Performance drops on MMLU and GSM-8K attributed to "distraction" from irrelevant retrieved experiences.

### Open Question 2
Can MAEL replace task-specific metrics with learned, generalizable reward functions? The experiential learning phase relies on pre-defined, task-specific metrics to quantify step-level rewards, limiting autonomy and applicability to open-ended tasks. Evidence: Dependence on ground-truth metrics like pass@k and accuracy for credit assignment.

### Open Question 3
How does retrieval noise and latency impact performance when the experience pool scales significantly? Current analysis is limited to 30 tasks with non-monotonic performance fluctuations. Evidence: Unclear if efficiency gains hold when retrieval must search through thousands of entries, potentially increasing latency and false positives.

## Limitations
- Experimental design doesn't clearly separate experience retrieval contributions from underlying agent graph architecture
- Reward calculation methods for intermediate steps are not fully specified, hindering exact reproduction
- Lacks ablation studies on experience pool size beyond 30 tasks and different graph topologies

## Confidence

**High Confidence**: Claims about reducing token consumption (up to 49%) and accelerating convergence (up to 26%) are well-supported by direct comparisons with AutoGen baselines.

**Medium Confidence**: Step-level experience storage with reward annotations is more effective than complete task traces, but mechanism's generality across domains remains untested.

**Low Confidence**: Combining semantic similarity with reward scores in retrieval is superior to either factor alone lacks direct comparative evidence within the paper.

## Next Checks

1. Implement ablation study comparing MAEL with complete task traces vs step-level experiences, keeping all other components constant.

2. Conduct experiments varying retrieval scoring weight α across wider range (0.0 to 1.0) on all five datasets to validate optimal balance.

3. Test MAEL's performance on additional datasets with varying task overlap to assess robustness of cross-task knowledge transfer when task similarity is low.