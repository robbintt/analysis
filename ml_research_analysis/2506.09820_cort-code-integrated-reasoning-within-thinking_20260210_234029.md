---
ver: rpa2
title: 'CoRT: Code-integrated Reasoning within Thinking'
arxiv_id: '2506.09820'
source_url: https://arxiv.org/abs/2506.09820
tags:
- code
- reasoning
- arxiv
- python
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving mathematical reasoning
  in Large Reasoning Models (LRMs) by integrating computational tools like code interpreters.
  The core method, CoRT, introduces Hint-Engineering, a data synthesis framework that
  strategically inserts hints to optimize LRM-Code Interpreter interaction, overcoming
  issues of delayed computation and code result distrust.
---

# CoRT: Code-integrated Reasoning within Thinking

## Quick Facts
- **arXiv ID:** 2506.09820
- **Source URL:** https://arxiv.org/abs/2506.09820
- **Reference count:** 40
- **Primary result:** CoRT achieves 4-8% accuracy gains and 30-50% token savings on mathematical reasoning tasks through hint-engineered tool integration.

## Executive Summary
This paper addresses the challenge of improving mathematical reasoning in Large Reasoning Models (LRMs) by integrating computational tools like code interpreters. The core method, CoRT, introduces Hint-Engineering, a data synthesis framework that strategically inserts hints to optimize LRM-Code Interpreter interaction, overcoming issues of delayed computation and code result distrust. Models ranging from 1.5B to 32B parameters were post-trained using supervised fine-tuning, rejection fine-tuning, and reinforcement learning on high-quality, manually curated data. Experimental results demonstrate that Hint-Engineering models achieve significant performance gains: 4% and 8% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and 1.5B respectively across five challenging mathematical reasoning datasets. Furthermore, these models achieve 30% and 50% token savings compared to natural language models, showcasing superior efficiency.

## Method Summary
CoRT addresses the inefficiency of LRMs in mathematical reasoning by strategically integrating code interpreters through hint engineering. The approach involves three training stages: supervised fine-tuning on 30 manually curated AIME problems with strategically inserted hints, rejection fine-tuning that expands the dataset while filtering for efficiency and correctness, and reinforcement learning with group relative policy optimization (GRPO). The hint engineering process targets two specific failure modes: delayed code computation (where models spend excessive tokens planning calculations that could be done immediately) and code result distrust (where models verify code outputs with manual recalculations). The framework uses output masking during RL to prevent gradient corruption through the non-differentiable code interpreter and employs a dual reward system that balances answer accuracy with successful code execution.

## Key Results
- CoRT models achieve 4% absolute accuracy improvement on DeepSeek-R1-Distill-Qwen-32B and 8% on 1.5B models across five mathematical reasoning datasets
- Hint-Engineering models demonstrate 30% and 50% token savings compared to natural language models
- The balanced interaction pattern (51.1% calculation vs 82.4% verification in baseline) contributes to superior efficiency
- Models successfully solve complex operations that would be inefficient to handle through pure natural language reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strategic hint insertion forces a behavioral shift from verbose natural language reasoning to efficient tool usage.
- Mechanism: Large Reasoning Models (LRMs) default to long chain-of-thought (CoT), often resulting in "delayed code computation" or "code result distrust." Hint-engineering injects specific directives (e.g., "we don't need to doubt python calculations") at decision points where the model typically hallucinates calculations. This suppresses the generation of redundant verification tokens and steers the policy toward immediate code execution.
- Core assumption: The base model possesses sufficient latent coding capability but lacks the default policy to trigger it efficiently.
- Evidence anchors:
  - [abstract]: Mentions "strategic insertion of hints" to optimize interaction and overcome inefficiencies.
  - [section]: Section 2.2.2 identifies "delayed code computation" and "code result distrust" as specific failure modes targeted by hints.
  - [corpus]: "OThink-R1" discusses "Over-Reasoning Mitigation," supporting the hypothesis that LRMs require intervention to stop excessive thinking.

### Mechanism 2
- Claim: Rejection Fine-Tuning (RFT) on a small, high-quality dataset is sufficient to "cold start" code-integrated reasoning capabilities.
- Mechanism: The framework uses a "less is more" approach, starting with only 30 manually curated samples. RFT then expands this by sampling new trajectories and filtering them not just for correctness, but for adherence to the new efficiency policy (filtering out responses that verify code results). This creates a dataset that teaches the model *how* to think with tools, not just *how* to answer.
- Core assumption: The efficiency of the reasoning process (the "how") is a learnable behavioral trait distinct from raw problem-solving ability.
- Evidence anchors:
  - [abstract]: States models were post-trained on "manually curated data" (specifically 30 samples).
  - [section]: Section 2.2.2 describes the RFT process filtering trajectories to eliminate "code result distrust behaviors."
  - [corpus]: "Towards Effective Code-Integrated Reasoning" highlights the challenge of acquiring tool use capabilities, validating the need for specialized data synthesis.

### Mechanism 3
- Claim: Output masking and execution rewards stabilize Reinforcement Learning (RL) in a non-differentiable tool environment.
- Mechanism: Standard LLM training cannot propagate gradients through a Python interpreter. CoRT masks the interpreter's output during loss calculation to prevent gradient corruption ("model collapse"). It complements this with a dual reward system: an accuracy reward for the final answer and a penalty for failed code execution, forcing the model to write robust code.
- Core assumption: The Code Interpreter provides deterministic ground truth that can serve as a reliable reward signal.
- Evidence anchors:
  - [section]: Section 2.4 details "Output Masking" to ensure stability and the dual reward equation $R = R_a + \omega R_c$.
  - [section]: Section 2.1 formulates the sequential process involving the executor environment $E$.
  - [corpus]: "Teaching Language Models to Reason with Tools" reinforces the complexity of integrating non-differentiable external tools.

## Foundational Learning

- **Tool-Integrated Reasoning (TIR)**
  - Why needed here: This is the core capability—deciding *when* to generate code versus natural language.
  - Quick check question: Can you distinguish between a problem requiring semantic analysis (natural language) and numerical optimization (code)?

- **Rejection Fine-Tuning (RFT)**
  - Why needed here: The paper relies on RFT to synthesize high-quality data from a small seed set.
  - Quick check question: How does filtering for "process efficiency" differ from filtering for "answer correctness"?

- **GRPO (Group Relative Policy Optimization)**
  - Why needed here: Used as the RL algorithm to optimize the tool-use policy based on group statistics.
  - Quick check question: Why might group-relative rewards be more stable than absolute rewards for reasoning tasks?

## Architecture Onboarding

- **Component map:**
  Input: Problem $P$ + System Prompt -> LRM (Actor) -> Executor (Environment) -> Filter/Reward

- **Critical path:**
  1. **Cold Start:** Fine-tune on 30 manually hinted samples (SFT)
  2. **Expansion:** Generate samples via RFT; filter for efficiency and correctness
  3. **Optimization:** Run RL (GRPO) with output masking and dual rewards

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Executing code is slower than pure text generation but yields higher accuracy
  - **Manual vs. Auto:** Manual hint insertion ensures high quality but is not scalable; the authors tried LLM automation but found it insufficiently precise

- **Failure signatures:**
  - **Code Result Distrust:** Model generates code, sees the output, and then proceeds to manually recalculate the result in text (wasting tokens)
  - **Delayed Computation:** Model spends 1000 tokens "planning" a calculation that could be done in 3 lines of code immediately
  - **Infinite Loops:** Model repeats code errors; mitigated by the $\omega R_c$ penalty

- **First 3 experiments:**
  1. **Prompt-Hint Baseline:** Implement the "Okay, let's try to solve..." trigger to verify the base model's latent tool capability (expect ~90% trigger rate)
  2. **Hint-Engineering SFT:** Train on the 30-sample dataset and measure token reduction on a held-out AIME problem set
  3. **RL Ablation:** Run RL with and without the code execution penalty ($\omega$) to observe the change in code error rates

## Open Questions the Paper Calls Out

- **Can the "Hint-Engineering" process be automated effectively, or does optimal data synthesis for tool-integrated reasoning inevitably require human curation?**
  - Basis in paper: [explicit] Section 2.2.2 states that attempts to automate hint insertion using LLMs (Qwen2.5-72B, DeepSeek-v3) yielded "insufficiently precise" results, forcing the authors to manually create the 30 high-quality samples.
  - Why unresolved: The reliance on manual annotation limits the scalability of the proposed framework; the paper does not propose a solution to automate the strategic hint insertion at appropriate decision points.
  - What evidence would resolve it: An automated pipeline capable of identifying "delayed code computation" or "code result distrust" behaviors within raw model outputs and inserting corrective hints with accuracy comparable to human annotators.

- **Do the performance gains observed from reinforcement learning (RL) on lightweight models (1.5B) transfer to large-scale reasoning models (32B+)?**
  - Basis in paper: [explicit] Section 2.3 notes that for the 32B parameter models, "RL remains computationally infeasible within our infrastructure." Consequently, the complete SFT-RFT-RL pipeline was only validated on the 1.5B models.
  - Why unresolved: It is unknown if the 8% absolute improvement seen in the 1.5B RL model would hold for the 32B model, or if the larger model's capacity negates the specific efficiency benefits provided by the RL stage.
  - What evidence would resolve it: Applying the GRPO-based RL pipeline with code rewards to the 32B model and comparing the performance delta against the SFT/RFT baselines.

- **Does the Hint-Engineering approach generalize to non-mathematical reasoning domains?**
  - Basis in paper: [inferred] The methodology and experiments are strictly confined to mathematical reasoning datasets (AIME, MATH, OlympiadBench). The specific hints designed (e.g., "tedious calculation," "accuracy of python") are math-centric.
  - Why unresolved: It is unclear if the "delayed computation" and "code result distrust" inefficiencies exist in other domains like logical deduction or general coding tasks, or if the current hints would be effective there.
  - What evidence would resolve it: Evaluation of the Hint-Engineering models on logic or coding benchmarks (e.g., Big-Bench Hard, MBPP) to see if token efficiency and accuracy gains persist without domain-specific prompt tuning.

- **What is the optimal interaction pattern (e.g., ratio of calculation vs. verification) between a Large Reasoning Model and a Code Interpreter?**
  - Basis in paper: [inferred] Section 3.3 observes that Hint-Engineering models adopt a balanced usage (51.1% calculation) while Prompt-Hint models favor verification (82.4%). However, the paper establishes correlation with efficiency rather than defining a causal mechanism for the *optimal* ratio.
  - Why unresolved: The paper shows that balanced usage is more efficient, but does not define a theoretical "gold standard" for when a model should trust its internal reasoning versus verifying with code.
  - What evidence would resolve it: An ablation study enforcing specific ratios of calculation-to-verification during training to identify the interaction distribution that minimizes token usage while maximizing accuracy.

## Limitations
- The manual hint engineering process relies heavily on human judgment, creating a scalability bottleneck for broader application
- The code execution penalty mechanism's optimal value may be task-dependent, but this hyperparameter sensitivity wasn't explored
- Output masking technique lacks detailed implementation specifications, making faithful reproduction challenging

## Confidence

- **High Confidence:** The claim that Hint-Engineering models achieve 30-50% token savings compared to natural language models is well-supported by experimental results across multiple datasets (AIME24/25, AMC23, MATH500, OlympiadBench).
- **Medium Confidence:** The assertion that 4% and 8% absolute accuracy improvements on DeepSeek-R1-Distill-Qwen-32B and 1.5B models respectively are primarily attributable to the hint engineering approach.
- **Low Confidence:** The claim that the "less is more" approach (starting with only 30 manually curated samples) is optimal for cold-starting code-integrated reasoning.

## Next Checks
1. **Scalability Test:** Implement an automated hint generation system based on the provided rules and measure its effectiveness compared to manual hint engineering. Generate 100+ problems with automated hints and compare the resulting model's performance and efficiency gains against the manually curated baseline.

2. **Robustness Analysis:** Create a diverse test suite of 200+ mathematical problems spanning different domains (algebra, geometry, combinatorics, number theory) and evaluate whether the hint-engineered models maintain their efficiency and accuracy advantages across all subdomains.

3. **Hyperparameter Sensitivity:** Systematically vary the code execution penalty ω from 0.01 to 0.5 in increments of 0.05 and measure the impact on both accuracy and token efficiency to identify optimal ranges for different model sizes and problem difficulty levels.