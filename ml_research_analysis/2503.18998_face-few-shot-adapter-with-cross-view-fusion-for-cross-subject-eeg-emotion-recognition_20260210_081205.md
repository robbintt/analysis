---
ver: rpa2
title: 'FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG Emotion
  Recognition'
arxiv_id: '2503.18998'
source_url: https://arxiv.org/abs/2503.18998
tags:
- emotion
- learning
- recognition
- ieee
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FACE, a few-shot adapter with cross-view
  fusion for cross-subject EEG emotion recognition. The method addresses the challenge
  of significant inter-subject and intra-subject variability in EEG signals by dynamically
  integrating global brain connectivity with localized patterns via subject-specific
  fusion weights.
---

# FACE: Few-shot Adapter with Cross-view Fusion for Cross-subject EEG Emotion Recognition

## Quick Facts
- **arXiv ID:** 2503.18998
- **Source URL:** https://arxiv.org/abs/2503.18998
- **Reference count:** 40
- **Key outcome:** Achieves up to 98.95% accuracy under 10-shot setting for cross-subject EEG emotion recognition

## Executive Summary
This paper introduces FACE, a few-shot adapter with cross-view fusion for cross-subject EEG emotion recognition. The method addresses the challenge of significant inter-subject and intra-subject variability in EEG signals by dynamically integrating global brain connectivity with localized patterns via subject-specific fusion weights. It also incorporates a few-shot adapter module to enable rapid adaptation for unseen subjects while reducing overfitting. The experimental results on three public EEG emotion recognition benchmarks (SEED, SEED-IV, and SEED-V) demonstrate FACE's superior generalization performance over state-of-the-art methods.

## Method Summary
FACE combines a dual-branch backbone (DGCN for global connectivity and Conv-3 for local patterns) with cross-view fusion using multi-head attention and a few-shot adapter module. The method employs a two-stage training procedure: first pre-training on source subjects using standard supervised learning, then meta-training using MAML-based bi-level optimization. The adapter enables rapid adaptation for unseen subjects while the fusion mechanism dynamically combines global and local features based on subject-specific weights.

## Key Results
- Achieves up to 98.95% accuracy under 10-shot setting
- Demonstrates superior generalization performance on three public EEG emotion recognition benchmarks (SEED, SEED-IV, and SEED-V)
- Effectively addresses inter-subject and intra-subject variability in EEG signals

## Why This Works (Mechanism)
FACE works by combining multiple complementary perspectives of EEG data: the DGCN captures global brain connectivity patterns while the Conv-3 backbone extracts localized spatial features. The cross-view fusion module uses multi-head attention to dynamically weigh these perspectives based on subject-specific characteristics. The few-shot adapter enables rapid adaptation to new subjects with minimal labeled data, reducing overfitting through batch normalization and bottleneck architecture.

## Foundational Learning
- **Differential Entropy (DE) features**: Why needed: Captures information content of EEG signals across frequency bands; Quick check: Verify DE features are properly normalized across subjects
- **Graph Convolutional Networks (GCN)**: Why needed: Models spatial relationships between EEG channels as a brain connectivity graph; Quick check: Confirm adjacency matrix captures meaningful channel correlations
- **Meta-learning (MAML)**: Why needed: Enables rapid adaptation to new subjects with few samples; Quick check: Monitor adaptation performance on support vs query sets
- **Multi-head Attention**: Why needed: Dynamically weighs global vs local feature importance; Quick check: Visualize attention weights across different subjects
- **Batch Normalization in adapters**: Why needed: Stabilizes training with few samples; Quick check: Monitor training stability with different batch sizes

## Architecture Onboarding

**Component Map:** DE Features -> DGCN + Conv-3 -> Cross-View Fusion (MHSA) -> Few-Shot Adapter (FSA) -> Classification

**Critical Path:** EEG DE features → DGCN (global connectivity) + Conv-3 (local patterns) → CVF (dynamic fusion) → FSA (adaptation) → Classification

**Design Tradeoffs:**
- Spatial projection from 62 channels to 32×32 grid: Enables CNN processing but loses some channel-specific information
- MAML vs fine-tuning: MAML enables better few-shot adaptation but requires more complex training
- Multi-head vs single attention: Multi-head captures diverse relationships but increases computational cost

**Failure Signatures:**
- Poor cross-subject performance: Likely indicates insufficient fusion adaptation or inadequate spatial projection
- Meta-training instability: May suggest learning rate issues or insufficient support set size
- Overfitting on few-shot tasks: Adapter capacity may be too high or regularization insufficient

**First Experiments:**
1. Train FACE on single-subject data to verify baseline performance
2. Implement and test spatial projection from 62 channels to 32×32 grid with visualization
3. Compare meta-learning vs simple fine-tuning for few-shot adaptation

## Open Questions the Paper Calls Out
None

## Limitations
- Spatial projection from 62 channels to 32×32 grid lacks precise definition of electrode-to-grid mapping
- Adapter dimensions and reduction ratios for dynamic adjacency matrices are not explicitly stated
- Number of inner-loop optimization steps during meta-training is not clearly specified

## Confidence
- Claims regarding superior performance: **Medium confidence** due to underspecified implementation details
- Effectiveness of cross-view fusion mechanism: **Medium confidence** without ablation studies
- Generalizability across different EEG datasets: **Medium confidence** pending additional validation

## Next Checks
1. Implement multiple electrode-to-grid mapping strategies and evaluate their impact on final performance to establish sensitivity to this critical design choice
2. Conduct ablation studies systematically removing the adapter module and cross-view fusion components to quantify their individual contributions
3. Perform 10 independent training runs with different random seeds to establish confidence intervals and statistical significance of reported accuracy improvements