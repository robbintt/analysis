---
ver: rpa2
title: On the Role of Label Noise in the Feature Learning Process
arxiv_id: '2505.18909'
source_url: https://arxiv.org/abs/2505.18909
tags:
- noise
- learning
- label
- lemma
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a theoretical framework to understand how label
  noise affects the feature learning process in deep neural networks. The authors
  consider a binary classification setup where each sample contains label-dependent
  signal and label-independent noise, and introduce label noise by flipping labels
  with a certain probability.
---

# On the Role of Label Noise in the Feature Learning Process

## Quick Facts
- arXiv ID: 2505.18909
- Source URL: https://arxiv.org/abs/2505.18909
- Reference count: 40
- Primary result: Theoretical framework showing two-stage feature learning process under label noise

## Executive Summary
This paper develops a theoretical framework to understand how label noise affects the feature learning process in deep neural networks. The authors analyze a two-layer CNN under a setup where samples contain both label-dependent signal and label-independent noise, with labels being flipped with certain probability. They identify two distinct training stages: Stage I where the model fits clean samples while ignoring noisy ones, learning generalizable signal features; and Stage II where noise memorization overtakes signal learning as training converges. This provides theoretical support for early stopping and sample selection techniques commonly used to combat label noise.

## Method Summary
The authors establish a theoretical framework for analyzing feature learning under label noise by considering a binary classification setup with label-dependent signal and label-independent noise. They rigorously analyze the training dynamics of a two-layer CNN, showing that during early training (Stage I), the model learns to fit clean samples while ignoring noisy ones, thereby extracting generalizable signal features. As training progresses and loss converges (Stage II), the model begins memorizing noise, which degrades generalization. The theoretical analysis is validated through experiments on synthetic and real-world datasets, demonstrating that early stopping before Stage II and selecting samples with small training loss can improve robustness to label noise.

## Key Results
- Theoretical proof of two distinct training stages under label noise: Stage I (clean sample fitting) and Stage II (noise memorization)
- Stage I enables learning generalizable signal features while ignoring noisy samples
- Early stopping before Stage II and sample selection based on training loss improve robustness to label noise
- Empirical validation on synthetic and real-world datasets supports theoretical predictions

## Why This Works (Mechanism)
The theoretical framework works by modeling label noise as a combination of label-dependent signal and label-independent noise, then analyzing how a two-layer CNN's training dynamics evolve under this setup. The key mechanism is that during early training, the model's gradient descent dynamics naturally separate clean samples (which contribute to generalizable feature learning) from noisy samples (which are initially ignored). This separation occurs because clean samples provide consistent gradient signals that the model can fit, while noisy samples create conflicting gradients that the model cannot fit effectively in early stages. As training progresses and approaches convergence, the model's capacity to ignore noisy samples diminishes, leading to noise memorization that degrades generalization.

## Foundational Learning

**Binary Classification Theory**
- Why needed: Provides tractable mathematical framework for analyzing label noise effects
- Quick check: Verify that the binary setup captures essential phenomena before extending to multi-class

**Gradient Descent Dynamics**
- Why needed: Understanding how optimization process separates clean vs noisy samples
- Quick check: Confirm that gradient-based learning naturally leads to the two-stage behavior

**Feature Learning Theory**
- Why needed: Framework for understanding when and how models extract generalizable features
- Quick check: Validate that learned features in Stage I are indeed more generalizable

## Architecture Onboarding

**Component Map**
Input data -> Two-layer CNN (convolution + pooling + fully connected) -> Binary classifier -> Training loss

**Critical Path**
Data generation (signal + noise) → CNN feature extraction → Binary classification → Training loss minimization → Stage I/II transition

**Design Tradeoffs**
- Simplified two-layer CNN vs. complex modern architectures
- Binary classification vs. multi-class problems
- Theoretical tractability vs. practical applicability

**Failure Signatures**
- Model performs well on clean data but poorly on noisy data
- Early stopping doesn't improve generalization
- Sample selection based on training loss doesn't help

**First Experiments**
1. Test two-stage theory on CIFAR-10 with synthetic label noise
2. Compare early stopping performance across different noise rates
3. Evaluate sample selection effectiveness with varying training set sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based on simplified two-layer CNN may not generalize to deeper networks
- Binary classification framework limits direct applicability to multi-class problems
- Timing and transition between stages may depend on factors not fully explored (learning rate, architecture, optimization algorithm)
- Empirical validation limited to synthetic and specific benchmark datasets

## Confidence

**High**: The existence of distinct training stages and early-stage clean sample fitting are well-supported

**Medium**: The theoretical framework explaining two-stage process and connection to early stopping effectiveness

**Medium**: Empirical validation showing improved robustness through early stopping and sample selection

## Next Checks
1. Test the two-stage theory on deeper networks (beyond two-layer CNN) and different architectures to assess robustness of theoretical predictions
2. Conduct experiments on multi-class classification problems to validate whether Stage I/II framework extends beyond binary classification
3. Investigate impact of different optimization algorithms and learning rate schedules on timing and characteristics of stage transition