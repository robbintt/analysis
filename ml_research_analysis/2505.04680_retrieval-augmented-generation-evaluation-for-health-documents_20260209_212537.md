---
ver: rpa2
title: Retrieval Augmented Generation Evaluation for Health Documents
arxiv_id: '2505.04680'
source_url: https://arxiv.org/abs/2505.04680
tags:
- arxiv
- answer
- information
- system
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of using Large Language Models
  (LLMs) for processing health documents and scientific papers, focusing on safe and
  trustworthy knowledge synthesis. The authors propose Retrieval Augmented Generation
  (RAG) as a promising method to enhance LLM accuracy.
---

# Retrieval Augmented Generation Evaluation for Health Documents

## Quick Facts
- arXiv ID: 2505.04680
- Source URL: https://arxiv.org/abs/2505.04680
- Reference count: 0
- Authors: Mario Ceresa; Lorenzo Bertolini; Valentin Comte; Nicholas Spadaro; Barbara Raffael; Brigitte Toussaint; Sergio Consoli; Amalia Muñoz Piñeiro; Alex Patak; Maddalena Querci; Tobias Wiesenthal
- Primary result: Hybrid search with reranking and SHy pipeline achieve 0.85 accuracy on binary questions and BERTScore F1 of 0.83 for health document QA

## Executive Summary
This study evaluates Retrieval Augmented Generation (RAG) for processing health documents and scientific papers, proposing RAGEv as a proof-of-concept pipeline with a benchmark dataset RAGEv-Bench. The system employs state-of-the-art practices including hybrid search with reranking and a novel SHy (Single Hybrid) pipeline to improve retrieval and generation accuracy. Experiments on PubMedQA and manual usability checks demonstrate high performance, with the SHy pipeline achieving 0.85 accuracy on binary questions and BERTScore F1 of 0.83. The study concludes that careful RAG implementation can minimize common LLM issues like hallucinations, though further efforts are needed for consistent and trustworthy healthcare applications.

## Method Summary
The authors developed RAGEv, a RAG pipeline incorporating hybrid search with Reciprocal Rank Fusion (RRF) combining full-text and vector queries, plus a SHy pipeline that treats each document as an independent collection during retrieval. They created RAGEv-Bench, a benchmark dataset for health document question-answering, and evaluated performance using PubMedQA and three health-related datasets through manual usability checks. The system was implemented with a NextJS frontend, Azure Kubernetes backend, Milvus vector database, and various LLMs including GPT-3.5, GPT-4, Llama 3 70B, and Mistral variants.

## Key Results
- SHy pipeline achieved 0.85 accuracy on binary yes/no questions
- BERTScore F1 of 0.83 demonstrates strong semantic answer quality
- Hybrid search with reranking outperformed single-method retrieval approaches
- RAG implementation successfully reduced hallucination frequency compared to baseline LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid search with reranking outperforms single-method retrieval for health document question-answering.
- Mechanism: Combines full-text search (keyword matching) with vector search (semantic similarity), then uses Reciprocal Rank Fusion (RRF) to merge results from both ranking functions into a single relevance-ordered set.
- Core assumption: Keyword-based and semantic-based retrieval capture complementary signals; neither alone is sufficient for medical/scientific queries with specialized vocabulary.
- Evidence anchors:
  - [abstract] The system employs state-of-the-art practices, including hybrid search with reranking and SHy pipeline, to improve retrieval and generation.
  - [section 4.3.1] Hybrid search is a combination of full-text and vector queries... A Reciprocal Rank Fusion (RRF) algorithm merges the results.
  - [corpus] Related work (Enhancing Health Information Retrieval with RAG) emphasizes prioritizing both topical relevance and factual accuracy, supporting multi-signal retrieval strategies.
- Break condition: When documents are extremely short or lack keyword overlap, full-text component degrades; when vocabulary is highly specialized beyond embedding training data, vector component underperforms.

### Mechanism 2
- Claim: The SHy (Single Hybrid) pipeline improves answer completeness for broad, summarization-oriented questions across document collections.
- Mechanism: Treats each document as an independent collection during retrieval, querying all documents rather than selecting top-k chunks globally. This returns relevant context from each paper, capturing horizontal knowledge across the full corpus.
- Core assumption: Broad questions require sampling from multiple documents rather than concentrating on highest-scoring chunks; completeness matters more than precision for synthesis tasks.
- Evidence anchors:
  - [abstract] SHy pipeline achieving 0.85 accuracy on binary questions and BERTScore F1 of 0.83.
  - [section 5.1] The SHY pipeline, which consistently score the highest results with an average precision of 0.85 on the binary yes/no questions.
  - [section 5.2.2] The SHy pipeline was developed, which explicitly interrogates all documents and returns much more additional context, to deal with use cases where it is important to extract all relevant information.
  - [corpus] No direct corpus comparison available; SHy appears novel to this implementation.
- Break condition: When users need highly specific answers from a single source, SHy may return excessive/irrelevant references, increasing cognitive load for verification.

### Mechanism 3
- Claim: RAG reduces hallucination frequency by constraining generation to retrieved document context rather than parametric model knowledge.
- Mechanism: Retrieved chunks are injected into the prompt as context; the LLM is instructed (via prompt engineering) to base responses on this context. External grounding replaces reliance on potentially outdated or fabricated internal knowledge.
- Core assumption: Retrieved documents contain accurate, relevant information; the generation model will follow instructions to stay grounded rather than inventing content.
- Evidence anchors:
  - [abstract] Careful RAG implementation can minimize common LLM issues.
  - [section 5.2.2] The version of the reference pipeline (RAGEv) that was tested... successfully impedes hallucinations and provides answers always based on the content of the source documents.
  - [corpus] SQuAI paper addresses RAG trustworthiness in scientific QA; "When Evidence Contradicts" paper examines safety when retrieved evidence conflicts, relevant to healthcare RAG reliability.
- Break condition: When retrieval fails to surface relevant chunks, the model may hallucinate despite RAG architecture; when documents contain contradictions or noise, grounded generation may still produce unreliable outputs.

## Foundational Learning

- Concept: **Vector embeddings and semantic similarity**
  - Why needed here: RAG relies on encoding documents and queries into dense vectors; understanding that similar meanings cluster in vector space is essential for debugging retrieval failures.
  - Quick check question: If a query uses "cardiovascular disease" but documents use "heart condition," will vector search likely connect them? Why or why not?

- Concept: **Chunking strategies and context windows**
  - Why needed here: Documents must be split for LLM context limits; chunk size affects noise vs. completeness tradeoffs.
  - Quick check question: What happens to table data when chunking splits mid-row? How does Small2Big address this?

- Concept: **Evaluation metrics: BERTScore vs. ROUGE vs. Accuracy**
  - Why needed here: The paper reports multiple metrics; understanding what each measures (semantic similarity vs. lexical overlap vs. classification correctness) is critical for interpreting results.
  - Quick check question: Why might BERTScore correlate better with human evaluation than ROUGE for health document answers?

## Architecture Onboarding

- Component map: Frontend (NextJS) -> Backend (Azure Kubernetes) -> Vector Database (Milvus) -> LLM Layer (GPT@JRC) -> Training/Finetuning (BDAP)
- Critical path: Document upload → text extraction/cleaning → chunking → embedding → vectors stored in Milvus → user query → embedding → similarity search → retrieved chunks + query → prompt assembly → LLM generation → response returned with source chunk references
- Design tradeoffs:
  - SHy vs. standard hybrid: SHy improves completeness but returns more references (some irrelevant), slower due to querying all documents
  - Chunk size: Larger = more context but more noise; smaller = cleaner but may fragment meaning
  - Embedding model choice: Domain-specific (SciBERT/BioBERT) may outperform large general models for highly specialized text; decoder-only LLM embeddings (e.g., SFR-Embedding-Mistral) score highest on MTEB but may miss domain nuances
- Failure signatures:
  - Missing numerical answers: Tables/charts not properly parsed during chunking
  - Irrelevant references: SHy pipeline returns chunks from documents not actually used in answer
  - Incomplete answers: General/summarization questions miss key information across documents
  - Cut-off responses: Long answers truncated due to token limits
  - Hallucinated follow-up: Model generates unprompted continuation (observed in AMR dataset example)
- First 3 experiments:
  1. Baseline comparison: Run NORAG (direct LLM query) vs. each pipeline (Vanilla, Vector, Full-text, Hybrid, SHy, ColBERTv2) on PubMedQA subset; measure accuracy on binary questions and BERTScore F1 on long answers. Confirm RAG outperforms vanilla LLM.
  2. Chunk size sensitivity: Test multiple chunk sizes (100, 256, 512 tokens) with the Hybrid pipeline on the same benchmark; identify optimal balance between context completeness and noise.
  3. Embedding model ablation: Compare domain-specific embeddings (SciBERT, BioBERT) vs. general-purpose leaders (SFR-Embedding-Mistral, NV-Embed) on retrieval recall for health-specific vocabulary queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can retrieval pipelines be adapted to reliably extract and synthesize information from non-textual elements like tables and figures in scientific health documents?
- Basis in paper: [explicit] The paper notes "issues when extracting information from tables, figures and/or subheadings" and suggests writing "specialized models for processing information specifically from tables and figures" in future works.
- Why unresolved: Current chunking strategies break table structures, and general LLM approaches for image/non-text data are inconsistent.
- What evidence would resolve it: A version of RAGEv that successfully processes tabular data in the AMR or VHT datasets without manual removal of those questions.

### Open Question 2
- Question: How does the RAGEv pipeline perform when processing collections containing "Noise only" (irrelevant documents) or "Contrafactual" (contradictory) information?
- Basis in paper: [explicit] Table 2 defines these collection types, but Section 4.5.3 states RAGEv is currently conceived for relevant documents only, with "Future extensions... demand[ing] testing other types of collections."
- Why unresolved: The study explicitly excluded these categories to focus on user-defined collections, so robustness against noise/contradiction is unknown.
- What evidence would resolve it: Benchmark results from RAGEv-Bench datasets including noise-only and contrafactual document sets.

### Open Question 3
- Question: Does incorporating medical ontologies (UMLS, SNOMED CT) into query processing improve the alignment between user queries and the technical language of scientific papers?
- Basis in paper: [explicit] Section 6.2 suggests implementing a query processing module with medical ontologies to "bridge the gap between user queries and the technical language."
- Why unresolved: Standard lexical and vector search may miss semantically related concepts expressed differently by lay-users versus researchers.
- What evidence would resolve it: Comparative retrieval accuracy scores (e.g., Precision/Recall) on the VHT or AMR datasets with and without ontology-based query expansion.

## Limitations
- Benchmark dataset RAGEv-Bench was constructed specifically for this study without external validation of its difficulty or coverage
- Performance evaluation relied on manual usability checks rather than systematic measurement of hallucination frequency or safety failures
- Study excluded noise-only and contrafactual document collections, leaving robustness against these failure modes untested
- Processing of non-textual elements like tables and figures remains problematic with current chunking strategies

## Confidence

- **High**: Hybrid search with RRF improves retrieval over single-method approaches for health documents (supported by multiple retrieval comparisons showing Hybrid outperforming Vector and Full-text baselines)
- **Medium**: SHy pipeline achieves 0.85 accuracy on binary questions (well-documented on PubMedQA but limited to binary classification; generalization to open-ended questions uncertain)
- **Medium**: RAG implementation minimizes common LLM issues (supported by usability checks but lacking systematic measurement of hallucination frequency, bias, or safety failures)
- **Low**: The system can be "consistently and trustworthily used" in healthcare (study acknowledges further efforts are needed; current evidence shows promise but not production readiness)

## Next Checks

1. Conduct systematic hallucination measurement comparing RAGEv to baseline LLM across diverse failure modes (missing chunks, contradictory sources, ambiguous queries) using both automated detection and human evaluation.
2. Benchmark SHy pipeline precision-recall tradeoff with controlled experiments varying document collection size and query specificity; establish clear guidelines for when SHy's completeness benefits outweigh its noise and speed costs.
3. Validate RAGEv-Bench against established health QA datasets (MedQA, PubMedQA variants) and real-world health document collections to confirm benchmark difficulty and relevance to actual healthcare use cases.