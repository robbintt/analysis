---
ver: rpa2
title: 'MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models
  with Real-World MCP Servers'
arxiv_id: '2512.15163'
source_url: https://arxiv.org/abs/2512.15163
tags:
- attack
- tool
- attacks
- task
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCP-SafetyBench, a comprehensive benchmark
  for evaluating the safety of large language models (LLMs) operating in real-world
  Model Context Protocol (MCP) environments. Unlike prior work that focuses on isolated
  attacks or lacks integration with actual MCP servers, MCP-SafetyBench provides realistic
  multi-step evaluation across five domains (browser automation, financial analysis,
  location navigation, repository management, and web search) using real MCP servers.
---

# MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers

## Quick Facts
- arXiv ID: 2512.15163
- Source URL: https://arxiv.org/abs/2512.15163
- Reference count: 40
- This paper introduces MCP-SafetyBench, a comprehensive benchmark for evaluating the safety of large language models operating in real-world MCP environments.

## Executive Summary
MCP-SafetyBench addresses a critical gap in evaluating large language model safety within real-world Model Context Protocol (MCP) environments. Unlike prior work focusing on isolated attacks, this benchmark provides realistic multi-step evaluation across five domains using actual MCP servers. The study systematically evaluates five prominent LLMs against 20 distinct attack types, revealing substantial safety gaps where no model achieves both strong task performance and robust defense. The results demonstrate a pronounced safety-utility trade-off and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.

## Method Summary
The benchmark employs execution-based evaluation using 13 real MCP servers across five domains: browser automation, financial analysis, location navigation, repository management, and web search. Each domain contains 10 attack scenarios with two ground truth reference answers. The evaluation measures both task success (completing intended operations) and attack success (whether malicious actions succeed), with task success required for attack success. Models are scored based on successful attack completions and normalized against reference answers. The evaluation also incorporates hallucination penalties for fabricated information and cross-checks outputs for factual accuracy.

## Key Results
- No evaluated model achieves both strong task performance and robust defense against attacks
- Attack success rates range from 29.8% to 48.2% across models, revealing substantial safety gaps
- Financial Analysis domain shows highest vulnerability (46.6% attack success) while Web Search shows lowest (30.3%)
- Pronounced safety-utility trade-off: higher task success correlates with lower defense success

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its execution-based evaluation methodology that measures actual outcomes rather than theoretical responses. By using real MCP servers and multi-step attack scenarios, it captures realistic failure modes that isolated testing would miss. The dual measurement of task and attack success provides nuanced insight into how safety mechanisms impact utility, while the cross-checking system ensures objective evaluation of both factual accuracy and safety compliance.

## Foundational Learning

**MCP Protocol Fundamentals**: Understanding how models interact with external tools via standardized protocols is essential for grasping attack vectors and safety boundaries.

*Why needed*: Attack scenarios exploit protocol-level interactions between models and MCP servers.

*Quick check*: Can identify the three attack categories (server, host, user side) and their mechanisms.

**Execution-Based Evaluation**: Moving beyond theoretical response checking to measure actual outcomes of model actions.

*Why needed*: Many attacks only manifest through successful execution, not just inappropriate responses.

*Quick check*: Understands why task success is a prerequisite for attack success measurement.

**Safety-Utility Trade-offs**: Recognizing that stronger safety mechanisms can reduce functional capabilities in legitimate use cases.

*Why needed*: Explains why no model achieves perfect scores on both dimensions.

*Quick check*: Can explain the inverse correlation between task success and defense success rates.

## Architecture Onboarding

**Component Map**: MCP Server -> LLM Agent -> Attack Scenario Generator -> Evaluation Engine -> Results Dashboard

**Critical Path**: Attack Scenario → LLM Execution → Task Success Check → Attack Success Check → Safety Evaluation

**Design Tradeoffs**: Real servers vs. simulated environments (realism vs. control), multi-step vs. single-step attacks (complexity vs. isolation), execution vs. response checking (accuracy vs. resource intensity)

**Failure Signatures**: High task success with high attack success indicates weak safety mechanisms; low task success with low attack success suggests overly restrictive safety; fabricated outputs indicate hallucination issues

**First 3 Experiments**:
1. Run baseline evaluation with a known safe model to establish reference performance
2. Test a single attack type across all domains to identify domain-specific vulnerabilities
3. Compare execution-based vs. response-only evaluation for a subset of scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on specific MCP server implementations that may not represent all real-world deployments
- Benchmark covers only 13 MCP servers across five domains, potentially missing broader ecosystem diversity
- Results based on five prominent LLMs may not generalize to the full landscape of MCP-capable models

## Confidence

**High Confidence**: The fundamental finding that LLMs exhibit safety gaps when operating with MCP servers in realistic multi-step tasks is well-supported by execution-based evaluation.

**Medium Confidence**: The safety-utility trade-off relationship and domain-specific vulnerability patterns are observed consistently but require further investigation with broader model and server diversity.

**Low Confidence**: Claims about being the "first comprehensive" evaluation tool should be interpreted cautiously given the rapidly evolving MCP landscape.

## Next Checks

1. Validate findings across a broader range of LLMs, including emerging models with enhanced MCP safety features, to assess generalizability of the safety gaps.

2. Conduct longitudinal studies with MCP servers deployed in production environments to evaluate how safety gaps manifest under real-world usage patterns and scale.

3. Systematically test the effectiveness of various proposed mitigation strategies against the identified attack vectors in the MCP-SafetyBench framework to establish actionable defense protocols.