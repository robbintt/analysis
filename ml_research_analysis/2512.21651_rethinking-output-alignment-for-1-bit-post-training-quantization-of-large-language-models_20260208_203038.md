---
ver: rpa2
title: Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language
  Models
arxiv_id: '2512.21651'
source_url: https://arxiv.org/abs/2512.21651
tags:
- quantization
- output
- alignment
- diag
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates why output alignment, despite being more
  aligned with the quantization objective, is less effective than weight alignment
  in 1-bit post-training quantization of large language models. The authors identify
  three key challenges: layer-wise output matching does not guarantee block-level
  loss reduction, accumulated quantization errors degrade alignment effectiveness,
  and attention mechanisms are disrupted by indiscriminate output matching.'
---

# Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models

## Quick Facts
- arXiv ID: 2512.21651
- Source URL: https://arxiv.org/abs/2512.21651
- Reference count: 40
- Primary result: Selective layer-wise output alignment with attention preservation consistently outperforms existing 1-bit PTQ methods across multiple LLMs

## Executive Summary
This paper addresses why output alignment, despite being theoretically aligned with quantization objectives, underperforms compared to weight alignment in 1-bit post-training quantization of LLMs. The authors identify three key challenges: layer-wise output matching doesn't guarantee block-level loss reduction, accumulated quantization errors degrade alignment effectiveness, and attention mechanisms are disrupted by indiscriminate output matching. To solve these issues, they propose a selective layer-wise output alignment strategy that applies output alignment only to final FC layers per block, combined with an attention-aware masking mechanism (AMP) that preserves attention behavior. Experiments demonstrate their method consistently outperforms existing 1-bit PTQ approaches across various benchmarks.

## Method Summary
The method parameterizes quantized weights as $\tilde{W} = \text{diag}(\alpha_r)B\text{diag}(\alpha_c)$ where $B \in \{-1, +1\}$. It applies selective output alignment only to final FC layers per block while using weight alignment (ARB-RC) for other layers. The optimization uses closed-form solutions for $\alpha_c$, $\alpha_r$, and $B$ with alternating updates every $k=5$ iterations. Attention Matrix Preservation (AMP) generates masks from attention-preserving gradients to selectively override parameter updates. The key innovation is using the true full-precision target output $XW$ instead of the activation-conditioned pseudo target $bXW$ in the optimization objective, which accounts for accumulated quantization errors.

## Key Results
- Selective output alignment to final FC layers achieves 4.85 perplexity reduction for smaller models compared to ARB-X
- Attention Matrix Preservation (AMP) improves LLaMA-2-7B perplexity by >10 points
- True target error optimization yields 0.7+ perplexity improvement over activation-conditioned error
- Zero-shot QA accuracy improves by 0.78% compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Applying output alignment selectively to the final FC layer of each block yields better block-level loss reduction than naive layer-wise output alignment.
- Mechanism: Layer-wise output matching minimizes local loss but ignores inter-layer dependencies; errors propagate non-linearly within blocks. Restricting output alignment to the layer with the most direct impact on block output preserves optimization direction while avoiding interference.
- Core assumption: The final fully-connected layer dominates block-level output error more than attention projections.
- Evidence anchors:
  - [abstract] "layer-wise output matching does not guarantee block-level loss reduction"
  - [section 3.1] Figure 1 shows ARB-X produces higher block-level loss than ARB for multiple layers despite lower layer-level loss
  - [section 4.2] "restricting the output alignment to only the last fully connected layer of each block, since it has the most direct impact on the block loss"
  - [corpus] Related work on block-wise quantization (BRECQ) confirms block-level optimization is more effective; no direct corpus contradiction
- Break condition: If applying output alignment to attention layers (Q, K, V) produces lower perplexity than final FC layer, selectivity assumption is wrong.

### Mechanism 2
- Claim: Using the true full-precision target output (XW) instead of the activation-conditioned pseudo target (bXW) in the optimization objective accounts for accumulated quantization error.
- Mechanism: Prior layers' quantization errors contaminate bX, causing the pseudo target bXW to diverge from true target XW over depth. Matching XW directly constrains the quantized model toward the original model's behavior rather than a degraded approximation.
- Core assumption: Full-precision input X remains informative for alignment even when quantized activations bX have accumulated errors.
- Evidence anchors:
  - [abstract] "accumulated quantization errors degrade alignment effectiveness"
  - [section 3.2] Figure 2 upper panels show cosine similarity with true output decreases while activation-conditioned similarity is maintained
  - [section 4] Equation 3 changes objective from ||bXW - bXcW|| to ||XW - bXcW||
  - [corpus] Corpus papers on low-bit PTQ (PTQ1.61, Mixed-Precision Graph Neural Quantization) do not explicitly address this accumulation mechanism
- Break condition: If activation-conditioned error minimization produces lower perplexity than output error minimization (Table 4 contradicts), accumulated error hypothesis fails.

### Mechanism 3
- Claim: Attention Matrix Preservation (AMP) masks prevent degradation of token similarity structure during output alignment, especially for RMSNorm-based architectures like LLaMA.
- Mechanism: Output alignment optimizes high-magnitude channels at the expense of directional alignment, disrupting token-to-token similarity matrices that correlate with attention behavior. AMP generates masks from attention-preserving gradients and selectively applies closed-form updates only where attention structure would benefit.
- Core assumption: Token similarity matrices computed from layer outputs are proxies for attention mask quality.
- Evidence anchors:
  - [abstract] "attention mechanisms are disrupted by indiscriminate output matching"
  - [section 3.3] Figure 2 lower panel shows token-similarity matrices diverge from full-precision baseline with depth
  - [section 4.1] Equations 9-11 define the AMP masking mechanism
  - [section 5.3] Table 3 shows LLaMA-2-7B perplexity increases by >10 points without AMP
  - [corpus] No corpus papers directly address attention preservation in quantization; mechanism is novel to this work
- Break condition: If removing AMP improves or matches performance for LLaMA models, the attention degradation hypothesis is invalid for those architectures.

## Foundational Learning

- Concept: Post-Training Quantization (PTQ)
  - Why needed here: The entire paper operates within PTQ constraints—no retraining, only calibration data. Understanding PTQ's efficiency/accuracy tradeoff is prerequisite.
  - Quick check question: Can you explain why PTQ uses a small calibration set instead of full retraining, and what error sources this introduces?

- Concept: Weight vs. Output Alignment Objectives
  - Why needed here: The paper's central contribution is diagnosing why output alignment underperforms and fixing it. Without understanding the distinction (||W - cW|| vs. ||XW - XcW||), the analysis is inaccessible.
  - Quick check question: Given layer input X and weights W, write the weight alignment loss and the output alignment loss. Which directly optimizes the inference goal?

- Concept: Binary Quantization with Scaling Factors
  - Why needed here: The method parameterizes quantized weights as cW = diag(α_r) B diag(α_c) where B ∈ {-1, 1}. Understanding this factorization is required to follow the closed-form derivations.
  - Quick check question: Why do binary quantization methods typically include per-row and per-column scaling factors instead of using pure ±1 weights?

## Architecture Onboarding

- Component map:
  - Full-precision weights W -> Calibration activations X -> Gram matrices S = X^T bX, bS = bX^T bX -> Closed-form updates for α_r, α_c, B -> AMP masks M_r, M_c, M_B -> Quantized weights cW

- Critical path:
  1. Compute true target output XW for each calibration batch
  2. Initialize α_r, α_c, B from weight alignment
  3. For T iterations: refine α_r → apply AMP → (every k iterations) refine α_c → refine B → apply AMP to both
  4. Forward pass through quantized block; measure block-level loss for validation

- Design tradeoffs:
  - Output alignment vs. weight alignment: Output alignment is more theoretically sound but requires AMP to avoid attention degradation; weight alignment is safer but suboptimal
  - Selective vs. full layer coverage: Restricting to final FC layer avoids inter-layer interference but may miss optimization opportunities in attention layers
  - Closed-form vs. gradient descent: Closed-form solutions are faster and more stable but require specific parameterizations; may not generalize to non-binary quantization

- Failure signatures:
  - Perplexity spikes in LLaMA but not OPT: AMP is likely disabled or misconfigured; LLaMA's RMSNorm makes it more sensitive to attention disruption
  - Block-level loss increases despite layer-level decrease: Naive layer-wise output alignment is being applied; switch to selective strategy
  - Numerical instability in α_r updates: Direct pseudoinverse computation is unstable; use torch.linalg.lstsq as specified
  - No improvement over ARB-RC: Verify output alignment is using true target XW, not pseudo target bXW

- First 3 experiments:
  1. Reproduce Figure 1 on a small model (OPT-1.3B): measure block-level loss for ARB vs. ARB-X per layer to confirm layer-wise output alignment doesn't guarantee block improvement.
  2. Ablate the true target objective (Table 4): run with activation-conditioned error vs. output error on LLaMA-2-7B to validate 0.7+ perplexity improvement.
  3. Ablate AMP on LLaMA vs. OPT (Table 3): disable AMP and confirm severe degradation on LLaMA (>10 perplexity increase) but milder impact on OPT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "True Target Error" optimization objective ($\|XW - \tilde{X}\tilde{W}\|$) remain superior to Activation-conditioned Error for quantization levels higher than 1-bit (e.g., 2-bit or 4-bit)?
- Basis in paper: [Inferred] The paper focuses exclusively on 1-bit quantization where accumulated errors are severe; it does not test if the complexity of accumulated error correction is necessary when quantization noise is lower.
- Why unresolved: The method is tailored for extreme binarization; the trade-off between computational overhead and accuracy gain for less aggressive quantization is unexplored.
- What evidence would resolve it: Comparative perplexity and zero-shot accuracy results of the proposed method against ARB-X/ARB-RC on LLaMA models at 2-bit and 4-bit precision.

### Open Question 2
- Question: Can the "selective" layer-wise alignment strategy be improved by a dynamic or learnable selection mechanism rather than the fixed heuristic of targeting only the final fully connected layer?
- Basis in paper: [Inferred] Section 4.2 states the authors "restrict output alignment to only the last fully connected layer," while Table 5 suggests performance varies significantly by layer choice.
- Why unresolved: The paper empirically chooses the final layer, but does not prove it is the global optimum or explore if combining multiple layers (e.g., attention output + FC) yields better block-level loss reduction.
- What evidence would resolve it: An ablation study optimizing the subset of layers $S \subset \{Q, K, V, Out, FC\}$ for output alignment to minimize block-wise reconstruction error.

### Open Question 3
- Question: To what extent does the use of RMSNorm (in LLaMA) versus LayerNorm (in OPT) specifically contribute to the vulnerability of attention mechanisms during output alignment?
- Basis in paper: [Inferred] Section 5.3 hypothesizes that LLaMA's severe degradation is due to RMSNorm making the model "more dependent on the direction of representations," but this is not isolated.
- Why unresolved: The difference in architecture could be conflated with other model-specific factors (e.g., attention head dimensions, activation functions).
- What evidence would resolve it: A controlled experiment replacing LayerNorm with RMSNorm in the OPT architecture (and vice-versa) to isolate the normalization layer's effect on token similarity matrix degradation.

## Limitations
- Selective layer-wise approach may not generalize to architectures with different residual connection patterns
- AMP mechanism relies on token similarity matrices as proxies for attention quality without direct validation
- Evaluation focuses on perplexity and zero-shot QA accuracy, omitting downstream task performance
- Calibration dataset size is not specified, creating uncertainty about data efficiency claims
- Does not address quantization of LayerNorm parameters

## Confidence

**High Confidence** in the core observation that layer-wise output alignment fails to guarantee block-level improvement. The empirical evidence from Figure 1 is clear and the theoretical justification (non-linear error propagation) is sound.

**Medium Confidence** in the selective layer-wise approach and true target error objective, as these are well-supported by ablation studies but rely on specific architectural assumptions.

**Medium Confidence** in the AMP mechanism - while ablation results are compelling, the theoretical link between token similarity preservation and attention quality preservation needs further validation.

**Low Confidence** in the claim of "no additional inference overhead" without empirical runtime measurements across different hardware platforms.

## Next Checks

1. **Architectural Generalization Test**: Apply the method to transformer architectures with different attention mechanisms (e.g., linear attention, gated attention) and verify that selective output alignment still outperforms full layer coverage. Measure whether AMP remains beneficial when token similarity matrices do not correlate with attention quality.

2. **Downstream Task Performance**: Evaluate quantized models on fine-tuning benchmarks (GLUE, SuperGLUE, or task-specific datasets) to verify that perplexity improvements translate to task performance. This would test whether the optimization objective aligns with practical utility.

3. **Runtime Overhead Validation**: Measure actual inference latency and memory usage on GPU and CPU platforms for the proposed method versus ARB-RC. Include measurements for different sequence lengths and batch sizes to verify the "no overhead" claim under realistic deployment conditions.