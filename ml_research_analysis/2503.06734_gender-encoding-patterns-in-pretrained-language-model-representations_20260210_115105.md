---
ver: rpa2
title: Gender Encoding Patterns in Pretrained Language Model Representations
arxiv_id: '2503.06734'
source_url: https://arxiv.org/abs/2503.06734
tags:
- bias
- gender
- language
- representations
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how gender biases are encoded in pretrained
  language models (PLMs) by analyzing internal representations across various encoder-based
  architectures. Using Minimum Description Length (MDL) probing, the authors systematically
  examine how gender information is stored and propagated through model layers, and
  evaluate the effectiveness of different bias mitigation techniques.
---

# Gender Encoding Patterns in Pretrained Language Model Representations

## Quick Facts
- **arXiv ID:** 2503.06734
- **Source URL:** https://arxiv.org/abs/2503.06734
- **Authors:** Mahdi Zakizadeh; Mohammad Taher Pilehvar
- **Reference count:** 17
- **Key outcome:** Post-hoc debiasing methods often fail to reduce encoded gender bias, while task-specific fine-tuning consistently reduces representation bias by shifting it to classification components.

## Executive Summary
This study investigates how gender biases are encoded in pretrained language models (PLMs) by analyzing internal representations across various encoder-based architectures. Using Minimum Description Length (MDL) probing, the authors systematically examine how gender information is stored and propagated through model layers, and evaluate the effectiveness of different bias mitigation techniques. The research reveals that gender encoding follows a consistent pattern across all tested models: early layers initially suppress gender signals, but later layers amplify and strongly encode gender information, particularly in final layers. Surprisingly, post-hoc debiasing methods like Orthogonal Projection and Adapter-based approaches often fail to reduce encoded bias, sometimes even increasing it while only reducing output bias. Task-specific fine-tuning, however, consistently reduces gender information compression in representations, suggesting bias mitigation may be more effective when targeting classification components rather than encoder representations.

## Method Summary
The authors employ Minimum Description Length (MDL) probing to measure gender information compression in model representations across layers. Using the Bias in Bios dataset of 396,347 biographies, they extract representations from multiple encoder architectures (BERT, RoBERTa, ALBERT, JINA) and train incremental probe classifiers using block-based online coding. Compression ratios (C = L_online/L_unif) quantify gender bias, with values exceeding random baselines indicating bias presence. The study evaluates training-time debiasing (CDA, Dropout) and post-hoc methods (Orthogonal Projection, ADELE), then examines fine-tuning effects on a 28-class occupation classification task.

## Key Results
- Gender encoding follows a consistent suppression-amplification pattern: early layers compress gender information while final layers strongly encode it
- Post-hoc debiasing methods often fail to reduce internal representation bias, sometimes increasing compression while only reducing output bias
- Task-specific fine-tuning consistently reduces gender information compression in encoder representations, potentially shifting bias to classification components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gender information follows a consistent encoding pattern across encoder architectures—early layers suppress gender signals while later layers amplify them.
- Mechanism: Early layers abstract from specific demographic attributes to learn general linguistic features. As representations propagate toward final layers, models reconstruct and strongly encode gender information, potentially for downstream task utility. MDL compression ratios reveal this by showing lower gender compressibility in middle layers versus high compressibility in final layers.
- Core assumption: The compression measured via MDL probing reflects intentional model behavior rather than artifact.
- Evidence anchors:
  - [abstract] "early layers initially suppress gender signals, but later layers amplify and strongly encode gender information, particularly in final layers"
  - [section 5] "This pattern suggests a two-phase process in how encoder models handle gender information: (i) In the early layers, models may abstract away from specific attributes like gender... (ii) In the later layers, models reintroduce and amplify gender-related information"
  - [corpus] Related work on bias localization (Chintam et al., Lutz et al.) identifies specific attention heads and weights responsible for bias, supporting layer-specific intervention—but does not directly validate the suppression-amplification pattern.
- Break condition: If decoder-based or multimodal architectures exhibit different layer-wise encoding dynamics, the pattern may be architecture-specific rather than universal.

### Mechanism 2
- Claim: Post-hoc debiasing methods can reduce output bias while increasing internal representation bias.
- Mechanism: Techniques like Orthogonal Projection and ADELE modify representations without retraining the underlying encoder. They may decorrelate gender from outputs superficially while leaving—or inadvertently amplifying—gender information in latent representations. MDL probing exposes this by showing unchanged or increased compression in debiased models despite improved bias benchmark scores.
- Core assumption: Output-level bias metrics capture only surface-level fairness, not representational bias.
- Evidence anchors:
  - [abstract] "post-hoc debiasing methods like Orthogonal Projection and Adapter-based approaches often fail to reduce encoded bias, sometimes even increasing it while only reducing output bias"
  - [section 6.2] "Post-hoc CDA and dropout, applied across all models, were generally less effective... Dropout exhibited inconsistent behavior; in some cases, it preserved or even amplified gender information"
  - [corpus] Dual Debiasing work notes that preserving factual gender while removing stereotypes is nuanced—suggesting output-level interventions may not fully address representational encoding (limited direct corroboration).
- Break condition: If new post-hoc methods directly target the bias propagation pathways identified through MDL analysis, efficacy may improve.

### Mechanism 3
- Claim: Task-specific fine-tuning reduces gender information compression in encoder representations, potentially shifting bias to classification components.
- Mechanism: Fine-tuning optimizes representations for task-relevant features (e.g., occupation prediction), actively suppressing demographic attributes like gender. This can push compression below random baseline levels. However, bias may migrate to the classification head rather than being eliminated, explaining why intrinsic debiasing has limited impact on extrinsic bias metrics.
- Core assumption: Fine-tuning redirecting representational focus implies bias concentration in classifier layers, not full mitigation.
- Evidence anchors:
  - [abstract] "Task-specific fine-tuning, however, consistently reduces gender information compression in representations, suggesting bias mitigation may be more effective when targeting classification components"
  - [section 7.2] "In many cases, the compression values of fine-tuned models fell below those of their random baselines... fine-tuning redirects the model's internal representations toward task-specific features"
  - [corpus] Limited corpus evidence on fine-tuning's effect on representation bias specifically; related work focuses on output-level debiasing.
- Break condition: If classification heads are regularized with fairness constraints during fine-tuning, this bias shift may be prevented.

## Foundational Learning

- Concept: Minimum Description Length (MDL) Probing
  - Why needed here: This is the core methodology for measuring how much gender information is compressible from model representations at each layer. Understanding MDL is essential to interpret the paper's central findings about bias encoding.
  - Quick check question: Given a model with high MDL compression for gender at layer 12, what does this tell you about how gender is encoded at that layer?

- Concept: Encoder Architecture Layer Functions
  - Why needed here: The paper's findings depend on understanding how representations transform across transformer layers. Early vs. late layer behavior is central to the suppression-amplification pattern.
  - Quick check question: Why might early transformer layers encode less task-specific demographic information than final layers?

- Concept: Intrinsic vs. Extrinsic Bias Metrics
  - Why needed here: The paper reveals a disconnect between internal representation bias (intrinsic, measured via MDL) and output distribution bias (extrinsic, measured via benchmarks like CrowS-Pairs). This distinction explains why post-hoc debiasing can appear effective on benchmarks while failing internally.
  - Quick check question: If a model shows improved fairness on StereoSet but unchanged MDL compression, what might be happening?

## Architecture Onboarding

- Component map: Encoder-based PLMs (BERT, RoBERTa, ALBERT, JINA) → Layer-wise representations → MDL probing classifier → Compression scores per layer. Bias mitigation techniques (CDA, Dropout, Orthogonal Projection, ADELE) applied at different stages (pretraining vs. post-hoc). Fine-tuning adds task-specific classification head on top of encoder.

- Critical path: Identify final layers (highest gender compression) → Apply training-time debiasing (CDA most effective) → Fine-tune with task-specific objectives → Audit classification head separately for residual bias.

- Design tradeoffs: Training-time debiasing requires full retraining (high cost) but achieves better internal bias reduction. Post-hoc methods are lightweight but may only address output-level bias. Fine-tuning reduces representation bias but may shift bias to classifier—requiring separate mitigation.

- Failure signatures:
  - Post-hoc debiasing shows improved benchmark scores but unchanged/increased MDL compression
  - Fine-tuned model has low representation bias but high downstream task bias
  - Larger models (BERT-large) show higher compression than smaller models (BERT-base), indicating capacity amplifies bias encoding

- First 3 experiments:
  1. Replicate MDL probing on your target encoder architecture using Bias in Bios or equivalent dataset, mapping compression across all layers to identify peak gender encoding layers.
  2. Compare training-time CDA vs. post-hoc Orthogonal Projection on your model, measuring both output bias (CrowS-Pairs, StereoSet) and internal compression to quantify the disconnect.
  3. Fine-tune your encoder on a downstream classification task, then probe both encoder representations and the classification head separately to determine where residual bias concentrates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do decoder-based and multimodal architectures exhibit the same layer-wise gender encoding patterns (initial suppression followed by amplification) observed in encoder-only models?
- Basis in paper: [explicit] The authors state in the Limitations section that the "study centers on encoder-based models" and explicitly call for future work to "validate findings in decoder-based architectures and multimodal systems."
- Why unresolved: The current study restricted its methodology to encoders like BERT, RoBERTa, and JINA, leaving the internal bias propagation mechanisms of generative and multimodal models unexplored.
- What evidence would resolve it: Applying the MDL probing methodology to decoder models (e.g., GPT, Llama) to measure compression rates across their hidden layers.

### Open Question 2
- Question: Can hybrid strategies combining training-time and post-hoc techniques successfully enhance bias suppression in internal representations without requiring full model retraining?
- Basis in paper: [explicit] In the Results and Analysis section, the authors note that training-time interventions are most effective but costly, concluding: "Future work could explore hybrid strategies that combine training-time and post-hoc techniques to enhance bias suppression without requiring full retraining."
- Why unresolved: The study evaluated training-time and post-hoc methods in isolation, finding post-hoc methods largely ineffective for internal representations.
- What evidence would resolve it: Experiments applying lightweight post-hoc methods (like Adapters) to models that have undergone partial or selective training-time debiasing.

### Open Question 3
- Question: How does the interplay between task-specific fine-tuning and bias propagation function across diverse downstream applications beyond occupation classification?
- Basis in paper: [explicit] The Limitations section notes that "the interplay between task-specific fine-tuning and bias propagation requires deeper exploration across diverse applications" to understand how fine-tuning shifts bias to classification heads.
- Why unresolved: The current findings rely on a single dataset (Bias in Bios), making it unclear if the observed shift of bias from representations to the classifier generalizes to other tasks.
- What evidence would resolve it: Layer-wise MDL probing experiments on models fine-tuned for a wider variety of downstream tasks (e.g., sentiment analysis, NLI) to see if compression decreases consistently.

## Limitations

- The study focuses exclusively on encoder-based architectures, leaving uncertainty about whether decoder-based or multimodal models follow the same gender encoding dynamics
- The MDL probing methodology, while innovative, has limitations in distinguishing intentional model behavior from artifacts, and key implementation details are underspecified
- The research doesn't explore whether more sophisticated architectural interventions could succeed where current post-hoc debiasing methods fail

## Confidence

- **High Confidence:** The finding that task-specific fine-tuning reduces gender information compression in encoder representations is well-supported by empirical evidence across multiple models and tasks.
- **Medium Confidence:** The suppression-amplification pattern across layers is consistently observed across tested architectures, though the underlying mechanism (intentional abstraction vs. training dynamics) remains somewhat speculative.
- **Medium Confidence:** The conclusion that post-hoc debiasing methods fail to reduce internal representation bias is well-supported, but the extent to which this generalizes to newer debiasing approaches is uncertain.

## Next Checks

1. Replicate the MDL probing methodology on a decoder-based architecture (e.g., GPT-2) to test whether the suppression-amplification pattern is universal or encoder-specific.
2. Implement and test a debiasing method that directly targets the identified bias propagation pathways (e.g., layer-specific attention regularization during pretraining) to evaluate whether architecture-aware interventions can succeed where post-hoc methods fail.
3. Conduct ablation studies on the classification head of fine-tuned models to quantify how much residual bias concentrates there versus being truly eliminated from the system.