---
ver: rpa2
title: Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition
arxiv_id: '2510.06774'
source_url: https://arxiv.org/abs/2510.06774
tags:
- reasoning
- solver
- problems
- problem
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel adaptive neuro-symbolic reasoning
  framework that automatically identifies formal reasoning strategies from natural
  language problems and dynamically selects specialized logical solvers. The framework
  decomposes problems, classifies reasoning types (LP, FOL, CSP, SMT), and composes
  appropriate solvers via autoformalization interfaces.
---

# Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition

## Quick Facts
- arXiv ID: 2510.06774
- Source URL: https://arxiv.org/abs/2510.06774
- Reference count: 40
- Primary result: Dynamic solver composition framework outperforms pure LLM approaches by 27% on sequential multi-paradigm reasoning tasks

## Executive Summary
This paper introduces a novel adaptive neuro-symbolic reasoning framework that automatically identifies formal reasoning strategies from natural language problems and dynamically selects specialized logical solvers. The framework decomposes problems, classifies reasoning types (LP, FOL, CSP, SMT), and composes appropriate solvers via autoformalization interfaces. Experimental results demonstrate that frontier models achieve over 98% accuracy in strategy prediction, enabling flexible solver integration that outperforms competing baselines by 27% and 6% compared to GPT-4o and DeepSeek-V3.1 respectively.

## Method Summary
The framework implements a dynamic logical solver composition system that bridges natural language problem statements with formal reasoning engines. It employs a multi-stage pipeline: problem decomposition, reasoning type classification (linear programming, first-order logic, constraint satisfaction, satisfiability modulo theories), autoformalization of decomposed subproblems, and dynamic solver selection based on predicted reasoning strategies. The system leverages frontier models for strategy prediction while smaller models can be fine-tuned for improved performance on specific reasoning types.

## Key Results
- Frontier models achieve over 98% accuracy in strategy prediction
- Adaptive approach yields 10%, 5%, and 6% gains over zero-shot, CoT, and symbolic CoT settings with GPT-4o
- Shows particular robustness on sequential multi-paradigm reasoning tasks (54.4% accuracy vs 27.3% for pure LLM methods)
- Smaller models show substantial improvement through fine-tuning (up to 59.6% accuracy)

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to automatically identify the most appropriate reasoning paradigm for each problem component and leverage specialized solvers for that domain. By decomposing complex problems into subproblems that can be addressed by different formal reasoning systems, it overcomes the limitations of monolithic approaches that attempt to solve all reasoning tasks with a single model or technique.

## Foundational Learning

**Constraint Satisfaction Problems (CSP)**: Why needed - forms the basis for many reasoning tasks involving variable assignments under constraints. Quick check - verify solver correctly handles binary constraints and can scale to moderate problem sizes.

**First-Order Logic (FOL)**: Why needed - enables formal reasoning about quantified statements and relationships. Quick check - test with basic syllogisms and logical inference chains.

**Satisfiability Modulo Theories (SMT)**: Why needed - extends propositional logic with domain-specific theories for arithmetic, arrays, etc. Quick check - validate with simple arithmetic constraints and array operations.

**Linear Programming (LP)**: Why needed - handles optimization problems with linear constraints and objectives. Quick check - test with basic resource allocation and scheduling problems.

**Autoformalization**: Why needed - translates natural language specifications into formal representations. Quick check - verify translation accuracy on benchmark datasets.

## Architecture Onboarding

**Component Map**: Natural Language Problem -> Problem Decomposition -> Strategy Classification -> Autoformalization -> Solver Selection -> Solution Integration

**Critical Path**: The most critical path involves accurate strategy classification followed by correct autoformalization, as errors in either stage propagate to solver execution and final solution quality.

**Design Tradeoffs**: The framework balances between the flexibility of dynamic solver selection and the overhead of strategy prediction and problem decomposition. It trades computational efficiency for accuracy gains compared to pure LLM approaches.

**Failure Signatures**: Common failure modes include misclassifying reasoning types, incorrect autoformalization leading to unsolvable formal representations, and integration errors when combining solutions from multiple solvers.

**First Experiments**:
1. Test basic strategy classification accuracy on a curated dataset of simple problems across all reasoning types
2. Validate autoformalization accuracy for each reasoning type separately
3. Verify solver integration works correctly on problems requiring exactly two different reasoning paradigms

## Open Questions the Paper Calls Out
None

## Limitations
- High strategy prediction accuracy (98%) may not generalize to more diverse or noisy real-world problem distributions
- Comparison lacks broader evaluation across different model families and problem domains
- Computational overhead introduced by dynamic solver composition and autoformalization interfaces needs careful consideration

## Confidence
- High confidence: Framework design principles and solver integration methodology
- Medium confidence: Reported performance improvements on tested benchmarks
- Low confidence: Generalization to real-world heterogeneous reasoning challenges

## Next Checks
1. Evaluate framework robustness across diverse problem distributions with varying levels of ambiguity and noise
2. Conduct ablation studies isolating the contribution of strategy prediction accuracy versus solver composition quality
3. Benchmark computational overhead and latency compared to pure LLM approaches across different problem scales