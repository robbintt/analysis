---
ver: rpa2
title: Exploring the features used for summary evaluation by Human and GPT
arxiv_id: '2512.19620'
source_url: https://arxiv.org/abs/2512.19620
tags:
- human
- evaluation
- arxiv
- features
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the underlying features used by GPT models
  when evaluating text summaries for relevance and coherence. The authors analyze
  statistical and machine learning metrics, including readability indices, information-theoretic
  measures, and embedding-based similarity scores, to identify features that correlate
  with both human and GPT judgments.
---

# Exploring the features used for summary evaluation by Human and GPT

## Quick Facts
- arXiv ID: 2512.19620
- Source URL: https://arxiv.org/abs/2512.19620
- Authors: Zahra Sadeghi; Evangelos Milios; Frank Rudzicz
- Reference count: 27
- Primary result: Identifies uncertainty-based features (entropy, perplexity, Gini index) that strongly correlate with both human and GPT summary evaluation

## Executive Summary
This paper investigates the underlying features that influence how both humans and GPT models evaluate text summaries for relevance and coherence. The authors systematically analyze statistical and machine learning metrics including readability indices, information-theoretic measures, and embedding-based similarity scores. They introduce a conditional perplexity metric that incorporates source text context and find it highly aligned with human scoring. The study reveals that GPT evaluations are strongly influenced by uncertainty-based features such as entropy, perplexity, and diversity measures alongside text complexity metrics.

## Method Summary
The authors conducted a comprehensive analysis of features used in summary evaluation by collecting human judgments on a CNN/DailyMail dataset and comparing them with GPT model evaluations. They employed a wide range of statistical and machine learning metrics including readability indices (Flesch-Kincaid, Gunning Fog), information-theoretic measures (entropy, perplexity), embedding-based similarity scores (cosine similarity, BERTScore), and diversity measures (Gini index). The study used simulated summaries generated by existing LLMs (GPT-2, GPT-3, and BART) rather than human-written summaries. They performed correlation analysis between human and GPT evaluations across these features and introduced a conditional perplexity metric that incorporates source text context to improve evaluation alignment.

## Key Results
- Uncertainty-based features (entropy, perplexity, Gini index) show strong correlation with both human and GPT evaluation judgments
- Conditional perplexity metric incorporating source text context demonstrates high alignment with human scoring
- Prompting GPT models with human-relevant metrics significantly improves their alignment with human evaluation patterns
- Readability and complexity measures are important features for both human and GPT evaluators

## Why This Works (Mechanism)
The study works by systematically mapping the feature space that influences summary evaluation, revealing that both humans and GPTs rely on similar underlying principles when judging summary quality. The mechanism operates through information-theoretic uncertainty measures that capture the predictability and diversity of text, which align with human cognitive processes for assessing coherence and relevance. By introducing context-aware metrics like conditional perplexity, the authors bridge the gap between traditional evaluation methods and LLM-based assessment, showing that uncertainty quantification is fundamental to quality judgment.

## Foundational Learning
- Information Theory Basics (why needed: to understand entropy and perplexity measures; quick check: can calculate entropy of a probability distribution)
- Text Readability Metrics (why needed: to comprehend Flesch-Kincaid and Gunning Fog indices; quick check: can compute average sentence length and word complexity)
- Embedding Similarity Measures (why needed: to understand cosine similarity and BERTScore; quick check: can calculate cosine similarity between two vectors)
- Conditional Probability (why needed: to grasp conditional perplexity concept; quick check: can compute P(A|B) from joint and marginal probabilities)
- Statistical Correlation (why needed: to interpret Spearman and Pearson correlations; quick check: can calculate correlation coefficient between two variables)
- Text Diversity Measures (why needed: to understand Gini index application to text; quick check: can compute Gini coefficient from frequency distribution)

## Architecture Onboarding

**Component Map:** Source text + Summary -> Feature Extraction (Readability, Information Theory, Embeddings, Diversity) -> Human Evaluation + GPT Evaluation -> Correlation Analysis -> Feature Importance Ranking

**Critical Path:** Feature extraction → Correlation analysis → Model prompting → Evaluation alignment

**Design Tradeoffs:** Simulated summaries vs. human-written summaries (efficiency vs. ecological validity), single dataset vs. multi-domain approach (depth vs. breadth), GPT-3 specific analysis vs. cross-LLM comparison (specificity vs. generalizability)

**Failure Signatures:** Low correlation between human and GPT evaluations suggests missing key features or misalignment in evaluation criteria; inconsistent feature importance across different metrics indicates potential overfitting or dataset-specific patterns

**3 First Experiments:**
1. Compute basic readability scores (Flesch-Kincaid, Gunning Fog) on a sample summary to verify implementation
2. Calculate entropy and perplexity for a simple probability distribution to validate information-theoretic measures
3. Measure cosine similarity between embeddings of source text and summary to test embedding-based metrics

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on simulated summaries generated by LLMs rather than human-written summaries may limit generalizability to real-world evaluation tasks
- Analysis conducted on a single dataset (CNN/DailyMail) may not represent diversity across different domains and languages
- Moderate effect sizes in correlation analysis (Spearman correlations around 0.5-0.6) indicate GPT models don't fully replicate human evaluation patterns
- Conditional perplexity metric tested only with GPT-3, raising questions about applicability to other models

## Confidence

**High confidence:** Identification of uncertainty-based features (entropy, perplexity, Gini index) as important for both human and GPT evaluation - well-supported by correlation analysis and aligns with existing literature

**Medium confidence:** Effectiveness of conditional perplexity metric - promising results but only tested with one LLM, general applicability remains unproven

**Medium confidence:** Impact of prompting GPTs with human-relevant metrics - observed improvement in alignment but effect size and generalizability need further investigation

## Next Checks
1. Cross-dataset validation using summaries from diverse domains (scientific articles, product reviews, legal documents) to assess feature robustness
2. Human-written summaries analysis to determine if feature importance patterns hold in human-to-human evaluations
3. Multi-LLM comparison testing conditional perplexity metric and other findings across different architectures (BERT, T5, Claude) to evaluate generalizability beyond GPT-3