---
ver: rpa2
title: 'KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and
  Optimization'
arxiv_id: '2601.21526'
source_url: https://arxiv.org/abs/2601.21526
tags:
- knowledge
- kapso
- evaluator
- experiment
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'KAPSO introduces a framework for autonomous program synthesis
  and optimization that treats synthesis as an operator within a long-horizon optimization
  loop rather than as an endpoint. It integrates three tightly coupled components:
  a git-native experimentation engine for reproducible, provenance-preserving iteration;
  a knowledge system that ingests heterogeneous sources into a structured, MediaWiki-hosted
  knowledge base with typed retrieval; and a cognitive memory layer that learns from
  experiment traces to reduce repeated failures.'
---

# KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization

## Quick Facts
- **arXiv ID**: 2601.21526
- **Source URL**: https://arxiv.org/abs/2601.21526
- **Reference count**: 5
- **Primary result**: Achieves 50.67% medal rate on MLE-Bench and 1909.4 ELO rating on ALE-Bench

## Executive Summary
KAPSO introduces a framework for autonomous program synthesis and optimization that treats synthesis as an operator within a long-horizon optimization loop rather than as an endpoint. It integrates three tightly coupled components: a git-native experimentation engine for reproducible, provenance-preserving iteration; a knowledge system that ingests heterogeneous sources into a structured, MediaWiki-hosted knowledge base with typed retrieval; and a cognitive memory layer that learns from experiment traces to reduce repeated failures. Evaluated on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), KAPSO achieves a 50.67% medal rate on MLE-Bench (vs. 35.11% for the best open-source baseline) and attains a 1909.4 ELO rating on ALE-Bench (vs. 1879.3 for the prior best), demonstrating superior performance and efficiency in long-horizon, evaluator-grounded program optimization.

## Method Summary
KAPSO operates as an iterative loop of ideation, synthesis, execution, evaluation, and learning. Users call `Kapso.evolve(goal, evaluator, budget)` which triggers an `OrchestratorAgent` to coordinate a `SearchStrategy` (linear or tree), `ContextManager`, `KnowledgeSearch`, and `CodingAgent`. Each experiment runs in an isolated git branch via `ExperimentSession`, which applies code edits, executes the evaluator, and commits artifacts (diffs, logs, outputs). Knowledge is retrieved through a cascaded system: `RepoRetrieve` selects a seed repository, `KnowledgeRetrieve` fetches typed pages (Principle, Implementation, Environment, Heuristic) from Neo4j/Weaviate, and `ERA` augments context with failure-conditioned heuristics on contract violations. After each experiment, `UpdateEpisodic` distills lessons into an episodic memory store, which is retrieved on subsequent iterations via `RetrieveEpisodic`. The loop continues until a `Stop` condition is met, returning the best artifact under a selection rule.

## Key Results
- Achieves 50.67% medal rate on MLE-Bench (vs. 35.11% for best baseline)
- Attains 1909.4 ELO rating on ALE-Bench (vs. 1879.3 for prior best)
- Demonstrates superior efficiency through git-native provenance and episodic memory

## Why This Works (Mechanism)

### Mechanism 1: Git-Native Experiment Isolation
Branch-per-experiment isolation preserves state, enables reproducibility, and supports tree-based search by making any prior attempt available as a parent. Each `ExperimentSession` creates a git branch from a parent, applies edits via `CodingAgent`, executes the evaluator, and commits artifacts (diffs, logs, evaluator outputs). Branches are pushed to the local `ExperimentWorkspace`, making them immediately selectable as parents for child experiments. Core assumption: The cost of branch creation and artifact persistence is negligible relative to evaluation time, and the task benefits from backtracking to prior artifact states. Evidence anchors: [abstract] "git-native experimentation engine that isolates each attempt as a reproducible branch with explicit provenance"; [Section 4.1.1] "Each experiment persists a lightweight, reproducible bundle sufficient for debugging and audit... committed artifacts include: code changes relative to parent branch, evaluator configuration, run logs, evaluator-produced artifacts". Break condition: If evaluation is cheap and fast, or if experiments rarely benefit from backtracking, branch overhead may dominate.

### Mechanism 2: Cascaded Retrieval with Error-Recovery Augmentation (ERA)
Structured, typed retrieval—augmented with failure-conditioned heuristics on error—provides higher-utility context than monolithic RAG, accelerating convergence on hard tasks. Two-stage retrieval: (1) `RepoRetrieve` selects an optional seed repository; (2) `KnowledgeRetrieve` fetches typed pages (Principle, Implementation, Environment, Heuristic) from Neo4j/Weaviate. On failure or contract violation, `ERA(g, s, r, K_base)` augments the packet with recovery heuristics and alternative implementations, preserving provenance (`query_used`, `source_pages`). Core assumption: Typed knowledge with explicit relationships (e.g., `IMPLEMENTED_BY`, `USES_HEURISTIC`) yields better retrieval than unstructured text, and failure-conditioned augmentation corrects more effectively than generic context. Evidence anchors: [abstract] "cascaded retrieval (WSR with PFR fallback, plus error-recovery augmentation)"; [Section 3.4] "K := ERA(g, s, r, K_base(g, r)) if s ≠ ∅, else K_base(g, r)" defines the augmentation logic; [Section 5.1] On MLE-Bench hard tasks, KAPSO achieves 40.00% vs. 22.22% for best baseline. Break condition: If the knowledge base is sparse, poorly typed, or domain-mismatched, ERA adds noise.

### Mechanism 3: Episodic Memory for Cross-Task Learning
Distilling lessons from experiment traces and retrieving them on similar failures reduces repeated error modes across tasks. After each experiment, `ExtractIssue` or `ExtractInsight` generalizes the outcome into a reusable lesson (trigger, lesson, action, provenance). Lessons are stored in a vector DB. On subsequent iterations, `RetrieveEpisodic(E, g, e)` fetches relevant lessons, which are rendered into context for the coding agent and controller. Core assumption: Error patterns and successful strategies generalize across tasks, and compact lessons are more useful than raw logs. Evidence anchors: [abstract] "cognitive memory layer that maintains an episodic store of reusable lessons distilled from experiment traces"; [Section 4.3] "Episodic memory entries are derived from experiment traces and are designed to be reusable across tasks. Each entry includes a compact trigger description, a generalized lesson, recommended actions, and provenance". Break condition: If tasks are highly idiosyncratic or lessons over-generalize, retrieved insights may be misleading.

## Foundational Learning

- **Black-box optimization with expensive evaluations**: The KAPSO loop is an instance of this—each experiment is costly, so search strategy (linear vs. tree), budget allocation, and early stopping are critical. Why needed here: Understanding `OrchestratorAgent.solve()` as a search over artifacts helps reason about tradeoffs in `SearchStrategy` selection. Quick check question: Can you explain why KAPSO uses expected utility `E[U(R(c))]` rather than `U(E[R(c)])` for stochastic evaluators?

- **Typed knowledge graphs & hybrid retrieval**: The knowledge system uses typed nodes and edges plus vector similarity. Why needed here: Debugging retrieval quality requires understanding how typed edges (e.g., `REQUIRES_ENV`) and vector indices interact. Quick check question: What is the difference between `K_base` retrieval and `ERA` augmentation, and when does each fire?

- **LLM-based code generation & debugging loops**: KAPSO's `CodingAgent` operates within a bounded debug loop. Why needed here: The `implement-and-debug` loop (Algorithm 2) has a fixed budget; understanding when to increase `D` vs. pivot is practical. Quick check question: What is the intended purpose of the debug loop, and what class of issues is it NOT designed to fix?

## Architecture Onboarding

- **Component map**: `Kapso