---
ver: rpa2
title: On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large
  Vision-Language Models
arxiv_id: '2510.09008'
source_url: https://arxiv.org/abs/2510.09008
tags:
- image
- visual
- object
- hallucination
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses object hallucination in large vision-language
  models (LVLMs), where models generate descriptions of objects not present in the
  input image. The core method identifies uncertain visual tokens in the vision encoder
  using adversarial perturbations, which are shown to correlate with hallucination
  occurrence.
---

# On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2510.09008
- Source URL: https://arxiv.org/abs/2510.09008
- Reference count: 40
- Primary result: Training-free hallucination mitigation via adversarial uncertainty masking reduces CHAIR_s by 38.4% on LLaVA-1.5-7B

## Executive Summary
This paper tackles object hallucination in large vision-language models (LVLMs) by identifying uncertain visual tokens using adversarial perturbations and masking them during inference. The method is training-free, model-agnostic (for transformer-based LVLMs), and significantly reduces hallucination rates across multiple benchmarks while maintaining caption quality. It addresses a critical limitation of LVLMs where models generate descriptions of objects not present in the input image.

## Method Summary
The approach estimates epistemic uncertainty of visual tokens via adversarial PGD attacks on the vision encoder, then masks high-uncertainty tokens in intermediate self-attention layers during inference. Uncertainty is computed as the deviation between clean and perturbed features across early layers, aggregated and binarized into a mask. This mask is applied to attention outputs in layers 13-17 (LLaVA) to suppress propagation of unstable visual features to the LLM.

## Key Results
- LLaVA-1.5-7B: CHAIR_s ↓38.4%, CHAIR_i ↓23.8%, F1 ↓0.8% (preserving caption quality)
- POPE accuracy ↑3.5%, AMBER score ↑5.8%
- Effective across architectures: LLaVA-1.5, Shikra, MM1-3B, MiniGPT-4
- Minimal degradation on non-existential reasoning tasks (TextVQA, GQA)

## Why This Works (Mechanism)

### Mechanism 1
Adversarial perturbations serve as an efficient proxy for estimating epistemic uncertainty of visual tokens. The PGD attack reveals which tokens are unstable under small perturbations, approximating the upper bound of differential entropy. This is computationally efficient compared to Monte Carlo dropout sampling.

### Mechanism 2
Visual tokens identified as high-uncertainty are statistically causal drivers of object hallucination. The model hallucinates objects when it attends to visual features that lack grounding or are ambiguous. Uncertain tokens act as noise sources for the LLM, correlating with higher hallucination rates (ρ > 0.7).

### Mechanism 3
Suppressing uncertain tokens via masking in intermediate self-attention layers isolates the LLM from noisy visual signals. Binary masking stops propagation of unstable features into the global image representation without altering model weights, preserving the global semantic context.

## Foundational Learning

- **Projected Gradient Descent (PGD)**: The engine for uncertainty estimation - you cannot implement the attack process without understanding how to iteratively maximize loss while keeping perturbations imperceptible. Quick check: How does step size α and iteration count I affect stability of uncertainty map U?

- **Epistemic vs. Aleatoric Uncertainty**: The paper targets epistemic (model) uncertainty inherent to the Vision Encoder, not data noise. Quick check: Why does MC Dropout estimate epistemic uncertainty, and how does adversarial deviation approximate the same thing?

- **Vision Transformer (ViT) Layer Architecture**: The method relies on hooking into intermediate layers. You must distinguish between residual stream, MLP blocks, and Self-Attention heads to apply the mask correctly. Quick check: If you apply mask M to the query matrix instead of attention output, would the mechanism still function?

## Architecture Onboarding

- **Component map**: Input Image → PGD Attacker → Uncertainty Map Generator → Mask Generator → Inference Module (Vision Encoder + LLM with masking)

- **Critical path**: 1) Run adversarial attack on image, 2) Generate mask M using early-layer features, 3) Run standard inference with M injected into vision encoder self-attention at specific blocks

- **Design tradeoffs**: Efficiency vs. Accuracy (PGD adds ~2.5s overhead), Masking Location (intermediate layers 13-17 optimal), Threshold tuning (high masks fewer tokens, low masks aggressively)

- **Failure signatures**: Poor performance on Q-Former architectures like MiniGPT-4, generic captions from overly aggressive masking, potential context loss from binary token removal

- **First 3 experiments**: 1) Sanity check: Generate masks with 5 random seeds, calculate mIoU for consistency, 2) Layer ablation: Generate masks using only late vs. early layers, 3) Visual verification: Visualize uncertainty map on hallucination-prone images

## Open Questions the Paper Calls Out

### Open Question 1
Can the uncertainty estimation method be adapted for architectures utilizing Q-Formers or similar query-based abstraction modules, where intervening solely on the vision encoder yields limited downstream impact? The current method is less effective for models like MiniGPT-4 due to the Q-Former bottlenecking visual tokens.

### Open Question 2
Can the reliance on computationally expensive PGD-based adversarial attacks for uncertainty estimation be replaced by a more efficient proxy without sacrificing accuracy? The current method requires iterative backpropagation, hindering real-time application.

### Open Question 3
To what extent does masking uncertain visual tokens degrade performance on fine-grained vision-language tasks like counting or OCR where ambiguous tokens might encode essential high-frequency details? The method may result in loss of visual information for perception-oriented tasks.

### Open Question 4
Is there a rigorous theoretical justification for the specific intermediate-layer self-attention masking strategy, given the current reliance on a trace-based approximation of entropy? The masking strategy lacks formal theoretical foundation and relies on effective heuristic supported by ablation.

## Limitations

- Adversarial attack dependence: Method's effectiveness hinges on quality of uncertainty estimation via PGD attacks; perturbation magnitude may introduce artifacts
- Correlation vs. causation: High-uncertainty tokens correlate with hallucinations but may not be causal drivers; could be equally driven by LLM language priors
- Architecture-specific effectiveness: Strong results on transformer-based models but minimal gains on Q-Former architectures like MiniGPT-4

## Confidence

- **High Confidence**: Experimental results showing hallucination reduction on CHAIR, POPE, and AMBER benchmarks are reproducible and statistically significant
- **Medium Confidence**: Theoretical framework linking adversarial deviation to epistemic uncertainty is sound but relies on smooth function assumptions
- **Low Confidence**: Claim that uncertain tokens are causal drivers of hallucination rather than correlated symptoms is not definitively proven

## Next Checks

1. **Ablation on Adversarial Attack Parameters**: Systematically vary PGD step count (10, 50, 100, 200) and perturbation magnitude (k=1, 2, 3) to quantify impact on hallucination reduction vs. computational overhead

2. **Causal Intervention Analysis**: Modify masking strategy to target tokens based on predicted hallucination likelihood rather than uncertainty scores; compare performance to test whether uncertainty is truly causal

3. **Cross-Architecture Generalization Test**: Apply method to diverse LVLM architectures (Qwen-VL, InternVL) and analyze whether optimal masking layers and thresholds transfer or require per-architecture tuning