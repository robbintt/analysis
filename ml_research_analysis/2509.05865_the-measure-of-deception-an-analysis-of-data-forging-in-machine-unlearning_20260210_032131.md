---
ver: rpa2
title: 'The Measure of Deception: An Analysis of Data Forging in Machine Unlearning'
arxiv_id: '2509.05865'
source_url: https://arxiv.org/abs/2509.05865
tags:
- forging
- data
- then
- measure
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the feasibility of adversarial data forging
  in machine unlearning, where an adversary attempts to construct data points that
  mimic the gradients of target data, thereby creating the appearance of unlearning
  without actual data removal. The authors formalize this concept using an $\epsilon$-forging
  set, which consists of data points whose gradients approximate a target gradient
  within tolerance $\epsilon$.
---

# The Measure of Deception: An Analysis of Data Forging in Machine Unlearning

## Quick Facts
- **arXiv ID:** 2509.05865
- **Source URL:** https://arxiv.org/abs/2509.05865
- **Reference count:** 31
- **Primary result:** Adversarial forging sets have vanishingly small Lebesgue measure, limiting feasibility of undetectable unlearning attacks.

## Executive Summary
This paper provides a rigorous mathematical analysis of adversarial data forging in machine unlearning, where an attacker attempts to create data points that mimic the gradients of target data to create false unlearning claims. The authors formalize this concept through an $\epsilon$-forging set, which contains data points whose gradients approximate a target gradient within tolerance $\epsilon$. Under various model settings including linear regression, one-layer neural networks, and batch SGD with smooth losses, they prove that the Lebesgue measure of this set scales on the order of $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and $r$ is the nullity of a variation matrix. This fundamental result demonstrates that adversarial forging is theoretically limited and that false unlearning claims can, in principle, be detected through gradient-based verification.

## Method Summary
The authors formalize the adversarial forging problem by defining an $\epsilon$-forging set for target gradients and data distributions. They analyze the measure of this set under different model architectures, starting with linear regression where they prove the measure scales as $O(\epsilon)$ (or $O(\epsilon^d)$ for small $\epsilon$). For one-layer neural networks, they extend the analysis by introducing a variation matrix whose nullity determines the scaling behavior. The theoretical framework is then generalized to batch SGD and almost-everywhere smooth loss functions, showing that the same asymptotic scaling holds. Probability bounds are derived to quantify the likelihood of randomly sampling a forging point under non-degenerate data distributions, providing theoretical evidence for the practical difficulty of successful adversarial forging attacks.

## Key Results
- The Lebesgue measure of the $\epsilon$-forging set scales as $O(\epsilon)$ for linear regression and $O(\epsilon^{(d-r)/2})$ for general models
- The nullity $r$ of the variation matrix fundamentally determines the scaling of the forging set measure
- Under non-degenerate data distributions, the probability of randomly sampling a forging point is vanishingly small

## Why This Works (Mechanism)
The theoretical guarantees arise from measure-theoretic analysis of the gradient space. The key insight is that the set of data points whose gradients match a target gradient within tolerance $\epsilon$ forms a low-dimensional manifold in the high-dimensional data space. The nullity of the variation matrix captures the effective degrees of freedom in this manifold, with higher nullity leading to larger forging sets. However, the exponential dependence on $(d-r)/2$ ensures that even modest nullity values result in vanishingly small measures as $\epsilon \to 0$. The extension to batch SGD preserves this scaling because the gradient variance introduced by sampling does not fundamentally alter the underlying geometry of the gradient space.

## Foundational Learning
- **Lebesgue measure theory**: Provides the mathematical framework for quantifying the size of forging sets in continuous data spaces; quick check: verify that the measure is well-defined and additive over disjoint sets
- **Variation matrix and nullity**: Captures the local linear independence of model gradients; quick check: compute the rank of the variation matrix for simple models
- **Almost-everywhere smooth losses**: Ensures gradient regularity conditions needed for measure bounds; quick check: verify that the loss has bounded derivatives almost everywhere
- **Gradient-based unlearning verification**: Forms the practical motivation for theoretical analysis; quick check: confirm that gradient similarity implies similar parameter updates
- **Non-degenerate data distributions**: Guarantees that probability bounds are meaningful; quick check: verify that the data covariance matrix is full rank

## Architecture Onboarding
**Component map:** Data points -> Model gradients -> Variation matrix -> Forging set measure -> Probability bounds
**Critical path:** Target gradient identification → Variation matrix computation → Measure bound derivation → Probability bound calculation
**Design tradeoffs:** Theoretical rigor vs. practical applicability; continuous analysis vs. discrete implementation; idealized assumptions vs. real-world noise
**Failure signatures:** Degenerate data distributions (rank-deficient variation matrix), highly overparameterized models (large nullity), non-smooth loss functions
**First experiments:**
1. Construct explicit $\epsilon$-forging sets for trained linear regression models and measure their actual size
2. Compute the nullity of variation matrices for one-layer neural networks with different architectures
3. Verify probability bounds empirically by sampling from non-degenerate distributions and checking forging success rates

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes deterministic, infinite-precision gradients without accounting for SGD noise
- Results rely on smoothness and non-degeneracy conditions that may not hold for overparameterized models
- Extension to batch SGD assumes uniform sampling, which may not reflect real training dynamics
- Absence of empirical validation against practical unlearning mechanisms
- Probability bounds depend on distribution assumptions that may not capture sophisticated adversarial strategies

## Confidence
- **High confidence:** Theoretical bounds on forging set measures and their scaling with $\epsilon$
- **Medium confidence:** Extensions to batch SGD and almost-everywhere smooth losses
- **Low confidence:** Practical implications for real-world unlearning verification and adversarial attack feasibility

## Next Checks
1. Empirically test the theoretical bounds by constructing $\epsilon$-forging sets for trained models and measuring their actual size and gradient similarity
2. Evaluate the robustness of the results under stochastic gradient noise and adaptive optimization algorithms
3. Investigate the effect of overparameterization on the nullity $r$ and its impact on the forging set measure scaling