---
ver: rpa2
title: Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning
arxiv_id: '2502.05435'
source_url: https://arxiv.org/abs/2502.05435
tags:
- audio
- captioning
- wasserstein
- sliced
- enclap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the ACUS framework to address exposure bias
  in audio captioning by developing an unbiased sliced Wasserstein RBF (USW-RBF) kernel.
  The USW-RBF kernel measures similarity between audio and text sequences while incorporating
  temporal information through rotary positional embeddings.
---

# Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning

## Quick Facts
- **arXiv ID**: 2502.05435
- **Source URL**: https://arxiv.org/abs/2502.05435
- **Reference count**: 40
- **Primary result**: Introduces ACUS framework with unbiased sliced Wasserstein RBF kernel achieving significant improvements on AudioCaps and Clotho datasets

## Executive Summary
This paper addresses exposure bias in audio captioning through the ACUS framework, which introduces an unbiased sliced Wasserstein RBF (USW-RBF) kernel. The approach measures similarity between audio and text sequences while incorporating temporal information via rotary positional embeddings. The kernel provides unbiased estimation through Monte Carlo sampling, making it compatible with stochastic gradient optimization with O(L⁻¹/²) approximation error. The framework also employs stochastic decoding methods at inference to mitigate caption degeneration. Experimental results demonstrate substantial improvements across multiple evaluation metrics on standard audio captioning benchmarks.

## Method Summary
The ACUS framework tackles exposure bias by developing an unbiased sliced Wasserstein RBF kernel that measures sequence similarity while incorporating temporal information. The kernel uses rotary positional embeddings to capture temporal relationships and employs Monte Carlo sampling to achieve unbiased estimation with O(L⁻¹/²) approximation error. This design enables compatibility with stochastic gradient optimization. The framework integrates this kernel into a transformer-based architecture and employs stochastic decoding methods during inference to prevent caption degeneration. The approach is evaluated on AudioCaps and Clotho datasets, showing significant improvements across multiple captioning metrics.

## Key Results
- Up to 0.262 METEOR, 0.509 ROUGE-L, 0.807 CIDEr, 0.192 SPICE, and 0.5 SPIDEr improvements on AudioCaps dataset
- Enhanced caption length and lexical diversity demonstrated qualitatively
- Improved text-to-audio retrieval performance compared to baseline methods
- Theoretical O(L⁻¹/²) approximation error bound for the unbiased kernel estimation

## Why This Works (Mechanism)
The approach addresses exposure bias by providing an unbiased similarity measure between audio and text sequences. Traditional methods accumulate error during inference as they condition on their own predictions, leading to caption degeneration. The USW-RBF kernel offers an unbiased estimation through Monte Carlo sampling, which prevents this accumulation of error. The kernel incorporates temporal information via rotary positional embeddings, capturing the sequential nature of both audio and text. During inference, stochastic decoding methods further mitigate degeneration by introducing controlled randomness. The combination of unbiased training objective and stochastic inference creates a more robust captioning system that maintains quality throughout the generation process.

## Foundational Learning

**Sliced Wasserstein Distance**
- *Why needed*: Provides a computationally efficient approximation of optimal transport distance between distributions
- *Quick check*: Verify that the sliced Wasserstein distance converges to the true Wasserstein distance as the number of projections increases

**Monte Carlo Sampling**
- *Why needed*: Enables unbiased estimation of the sliced Wasserstein distance while maintaining computational efficiency
- *Quick check*: Confirm that the variance of the Monte Carlo estimator decreases with O(L⁻¹/²) as sequence length increases

**Rotary Positional Embeddings**
- *Why needed*: Captures temporal relationships in sequences without increasing model complexity
- *Quick check*: Validate that rotary embeddings effectively encode relative positions in both audio and text sequences

**Exposure Bias**
- *Why needed*: Understanding this phenomenon is crucial for addressing caption degeneration in sequence generation
- *Quick check*: Demonstrate that conditioning on ground truth during training versus model predictions during inference leads to performance degradation

## Architecture Onboarding

**Component Map**: Audio Encoder -> USW-RBF Kernel -> Text Decoder -> Stochastic Decoding

**Critical Path**: The core innovation flows through the USW-RBF kernel, which computes unbiased similarity scores between audio and text sequences. These scores guide the text decoder during training, while stochastic decoding methods handle inference generation.

**Design Tradeoffs**: The approach trades computational efficiency for unbiased estimation - Monte Carlo sampling requires multiple projections but provides theoretical guarantees. Rotary positional embeddings add minimal overhead while significantly improving temporal modeling. Stochastic decoding introduces randomness during inference but prevents degeneration.

**Failure Signatures**: If the number of Monte Carlo samples is too low, the kernel estimate becomes noisy, leading to unstable training. Insufficient rotary embeddings may fail to capture temporal relationships, resulting in poor sequence alignment. Overly aggressive stochastic decoding can produce incoherent captions.

**First Experiments**: 
1. Compare caption quality with varying numbers of Monte Carlo samples to find the optimal tradeoff between accuracy and efficiency
2. Test different positional encoding schemes (absolute, relative, rotary) to validate the choice of rotary embeddings
3. Evaluate the impact of different stochastic decoding temperatures on caption coherence versus diversity

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Theoretical framework relies on specific assumptions about sliced Wasserstein distance distributions that may not hold in all practical scenarios
- Monte Carlo sampling introduces variance that could affect training stability, particularly in early training stages
- Evaluation focuses on standard metrics without comprehensive human evaluation to validate perceived quality improvements

## Confidence
- **High** confidence in the technical contribution of the unbiased kernel formulation and its compatibility with stochastic optimization
- **Medium** confidence in the empirical improvements given strong results across multiple metrics, though ablation studies could be more comprehensive
- **Low** confidence in qualitative claims about caption diversity and length due to limited quantitative analysis

## Next Checks
1. Conduct human evaluation studies to validate whether metric improvements correspond to perceived quality gains in captions
2. Perform ablation studies isolating the contributions of the USW-RBF kernel versus the stochastic decoding approach
3. Analyze the variance introduced by Monte Carlo sampling during training and its impact on convergence speed and stability across different sequence lengths