---
ver: rpa2
title: 'AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial
  Vulnerabilities in Specialized LLM Applications'
arxiv_id: '2512.20164'
source_url: https://arxiv.org/abs/2512.20164
tags:
- attack
- resume
- adversarial
- https
- match
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of Large Language Models
  (LLMs) to adversarial resume injection attacks, where malicious candidates embed
  hidden instructions or manipulated content in resumes to manipulate automated screening
  decisions. The study introduces a systematic benchmark framework evaluating four
  attack types (instruction injection, invisible keywords, fabricated experience,
  and job manipulation) across four injection positions within candidate profiles.
---

# AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications

## Quick Facts
- arXiv ID: 2512.20164
- Source URL: https://arxiv.org/abs/2512.20164
- Reference count: 18
- LLM-based resume screening systems vulnerable to adversarial injection attacks with success rates exceeding 80%

## Executive Summary
This paper investigates adversarial vulnerabilities in LLM-based resume screening systems, where malicious candidates can manipulate automated screening decisions through carefully crafted resume content. The study systematically evaluates four attack types across different injection positions and demonstrates that current defense mechanisms are insufficient to prevent exploitation. Through comprehensive experiments across nine state-of-the-art models, the research reveals that adversarial attacks can significantly compromise the integrity of automated hiring processes, with job manipulation attacks proving particularly effective at bypassing screening criteria.

## Method Summary
The study introduces a systematic benchmark framework for evaluating LLM-based resume screening systems against adversarial attacks. Four attack types were designed: instruction injection (hidden directives within resumes), invisible keywords (stealthy manipulation using homoglyphs), fabricated experience (falsified credentials), and job manipulation (altered job descriptions). These attacks were tested across four injection positions within candidate profiles. Two defense mechanisms were evaluated: prompt-based defenses using carefully crafted instructions, and FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation for fine-tuning. The framework was applied to nine state-of-the-art LLM models to measure attack success rates and defense effectiveness.

## Key Results
- Attack success rates exceeded 80% for certain attack types, with job manipulation achieving the highest effectiveness
- FIDS defense achieved 15.4% attack reduction with 10.4% false rejection increase, outperforming prompt-based defenses
- Combined approach of FIDS and prompt-based defenses provided 26.3% attack reduction
- Training-time defenses (FIDS) outperformed inference-time mitigations (prompt-based) in both security and utility preservation

## Why This Works (Mechanism)
The effectiveness of adversarial attacks stems from LLMs' inherent vulnerability to instruction injection and content manipulation. When malicious candidates embed hidden instructions or manipulated content within resumes, the models can be tricked into interpreting these signals as legitimate criteria for screening decisions. The job manipulation attack is particularly effective because it exploits the model's tendency to align responses with explicitly stated job requirements, allowing candidates to artificially match criteria that would normally exclude them.

## Foundational Learning
1. **Adversarial Injection Attacks** - Why needed: Understanding how malicious content can manipulate LLM behavior through hidden instructions or content manipulation. Quick check: Can you identify potential injection points in a standard resume format?
2. **Resume Screening Automation** - Why needed: Grasping how LLMs are deployed in HR contexts to automate candidate evaluation and filtering. Quick check: What are the key decision criteria typically used in automated resume screening?
3. **Defense Mechanisms Classification** - Why needed: Differentiating between training-time defenses (model fine-tuning) and inference-time mitigations (prompt engineering). Quick check: What are the trade-offs between these two defense approaches in terms of effectiveness and false rejection rates?
4. **LoRA Fine-tuning** - Why needed: Understanding the technique used in FIDS to adapt models for foreign instruction detection without full retraining. Quick check: How does LoRA enable efficient adaptation compared to full fine-tuning?
5. **Benchmark Framework Design** - Why needed: Appreciating the systematic approach to evaluating security vulnerabilities across multiple attack types and positions. Quick check: What are the advantages of testing across multiple injection positions?
6. **False Rejection Analysis** - Why needed: Recognizing the balance between security (attack prevention) and utility (legitimate candidate acceptance) in defense deployment. Quick check: How do you quantify the acceptable trade-off between security and false rejections?

## Architecture Onboarding

**Component Map:** Resume Data -> LLM Model -> Screening Decision <- Attack Vector (4 types) / Defense Mechanism (2 types)

**Critical Path:** Resume preprocessing → LLM inference → Decision output → Defense filtering → Final decision

**Design Tradeoffs:** Training-time defenses (FIDS) provide better security but require model adaptation and retraining; inference-time defenses (prompt-based) are easier to deploy but less effective and may increase false rejections.

**Failure Signatures:** High attack success rates (>80%) indicate model vulnerability; increased false rejection rates signal overly aggressive defense mechanisms that may harm legitimate candidates.

**First 3 Experiments:**
1. Baseline evaluation: Measure LLM screening accuracy without any adversarial attacks or defenses
2. Attack-only testing: Apply each of the four attack types across all injection positions to establish baseline vulnerability
3. Defense effectiveness: Evaluate FIDS and prompt-based defenses individually and in combination against the most successful attack types

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on synthetic resume datasets with artificially crafted adversarial examples, not real-world data
- Evaluation focused on nine specific LLM models, limiting generalizability to other architectures
- False rejection rates measured relative to baseline without explicit business impact analysis
- Combined defense approach (26.3% attack reduction) may be difficult to implement in production environments

## Confidence
- High: Attack success rates exceeding 80% demonstrate fundamental security vulnerabilities
- High: Comparative effectiveness of FIDS versus prompt-based defenses supported by systematic evaluation
- Medium: Generalizability of results to production environments and diverse real-world scenarios
- Medium: Combined defense effectiveness may be challenging to achieve in practical deployment

## Next Checks
1. Test FIDS and prompt-based defenses against real-world adversarial resumes collected from actual job application data to validate practical effectiveness
2. Evaluate defenses across a broader range of LLM architectures including open-source models with varying parameter counts and training methodologies
3. Conduct cost-benefit analysis measuring trade-off between attack reduction and false rejection rates in terms of actual hiring outcomes and business metrics