---
ver: rpa2
title: 'Let''s Put Ourselves in Sally''s Shoes: Shoes-of-Others Prefixing Improves
  Theory of Mind in Large Language Models'
arxiv_id: '2506.05970'
source_url: https://arxiv.org/abs/2506.05970
tags:
- prefilling
- prompting
- tomato
- llms
- tombench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Shoes-of-Others (SoO) prefilling, a novel
  inference-time method to improve Theory of Mind (ToM) in large language models (LLMs).
  SoO prefilling enhances ToM by specifying the beginning of LLM outputs with "Let's
  put ourselves in A's shoes," where A is the target character's name, encouraging
  the model to adopt the character's perspective before answering questions.
---

# Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models

## Quick Facts
- arXiv ID: 2506.05970
- Source URL: https://arxiv.org/abs/2506.05970
- Authors: Kazutoshi Shinoda; Nobukatsu Hojo; Kyosuke Nishida; Yoshihiro Yamazaki; Keita Suzuki; Hiroaki Sugiyama; Kuniko Saito
- Reference count: 34
- Primary result: SoO prefilling consistently improves Theory of Mind performance across five mental states in both first- and second-order ToM tasks.

## Executive Summary
This paper introduces Shoes-of-Others (SoO) prefilling, a novel inference-time method to improve Theory of Mind (ToM) in large language models (LLMs). SoO prefilling enhances ToM by specifying the beginning of LLM outputs with "Let's put ourselves in A's shoes," where A is the target character's name, encouraging the model to adopt the character's perspective before answering questions. Evaluated on two benchmarks—ToMATO (conversational) and ToMBench (narrative)—SoO prefilling consistently improves ToM performance across five mental states (beliefs, intentions, desires, emotions, knowledge) in both first- and second-order ToM tasks. The method outperforms existing inference-time approaches like Chain-of-Thought prompting and prefilling, and analysis shows that SoO prefilling elicits more faithful reasoning without relying solely on lengthening thought processes.

## Method Summary
The SoO method works by prefixing the LLM's output with a perspective-taking instruction specific to the target character in a ToM task. A rule-based module extracts the character's name from the question, and the LLM is forced to begin generation with "Let's put ourselves in [name]'s shoes." This prefix is intended to orient the model's reasoning toward the character's mental state, improving the relevance and accuracy of its answers. The approach is evaluated against standard zero-shot and Chain-of-Thought baselines on two Theory of Mind benchmarks, with ablation studies confirming the importance of character-specific naming and faithfulness analysis linking SoO's gains to more relevant reasoning.

## Key Results
- SoO prefilling improves accuracy on both ToMATO and ToMBench benchmarks across five mental state categories.
- Performance gains are consistent across first- and second-order ToM tasks.
- SoO outperforms Chain-of-Thought prefilling and shows diminishing returns on larger models (GPT-4, GPT-4o).
- Ablation shows specific character naming is crucial; generic prefixes lead to sharp accuracy drops.

## Why This Works (Mechanism)
The core insight is that forcing the LLM to begin its output with a character-specific perspective-taking instruction improves the alignment between its reasoning and the correct answer. By explicitly orienting the model to adopt the character's viewpoint, SoO prefilling encourages more faithful and relevant reasoning chains, rather than simply producing longer or more generic thought processes.

## Foundational Learning
- **Theory of Mind (ToM):** The ability to attribute mental states (beliefs, intentions, desires, emotions, knowledge) to oneself and others. Needed to understand why perspective-taking instructions help models reason about others' mental states. Quick check: Can the model predict what a character believes, even if it's false?
- **Inference-Time Methods:** Techniques applied during model output generation (e.g., prompting, prefilling) to improve performance, as opposed to training-time modifications. Needed to contextualize SoO as an inference-time intervention. Quick check: Does the method change how the model generates answers, not how it's trained?
- **Chain-of-Thought (CoT):** A prompting strategy where the model is encouraged to generate intermediate reasoning steps before answering. Needed to compare SoO's effectiveness against a standard inference-time approach. Quick check: Does CoT lead to longer, but not necessarily more relevant, reasoning?
- **Prefilling:** Forcing the model to begin its output with a specified string, as opposed to prompting which suggests but does not require. Needed to explain how SoO controls model output. Quick check: Is the prefix a hard constraint on generation?
- **Faithfulness in Reasoning:** The degree to which a model's reasoning chain is logically connected to and supports its final answer. Needed to interpret why SoO improves performance. Quick check: Is the reasoning relevant to the question and answer, or just verbose?

## Architecture Onboarding

- **Component Map:**
  Context + Question -> SoO Prefixer (extracts name) -> Prefix: "Let's put ourselves in [name]'s shoes." -> LLM Generator (outputs answer)

- **Critical Path:**
  The success of the system hinges on the **LLM Generator's** ability to correctly condition its entire output probability distribution on the perspective established in the forced prefix. If the model ignores or fails to propagate the perspective signal from the prefix into its reasoning chain, the mechanism degrades into a generic CoT. The extraction of the correct character name by the **SoO Prefixer** is the second critical dependency; ablation shows that using generic terms ("others' shoes") causes performance to crash.

- **Design Tradeoffs:**
  - **Specificity vs. Generality:** SoO is designed for ToM tasks involving mental state attribution. Applying it to factual or logical tasks might be counterproductive.
  - **Prefilling vs. Prompting:** Prefilling is a hard constraint on generation, offering more control than prompting but requiring API support. Prompting (as tested with GPT models) showed inconsistent results and often degraded performance.
  - **Faithfulness vs. Length:** SoO optimizes for reasoning *relevance* (faithfulness) rather than *quantity* (length). This is more efficient but assumes the task benefits from targeted, not exhaustive, reasoning.

- **Failure Signatures:**
  - **Name Extraction Failure:** If the question doesn't contain a clear character name or the rule-based extractor fails, the prefix becomes generic ("others' shoes") and performance drops sharply (ToMATO accuracy falls from 62.8% to 29.4%).
  - **Model Non-Compliance:** Some models/APIs may not support output prefilling. Without it, you are forced to use less reliable prompting.
  - **Unfaithful Reasoning:** Despite the prefix, the model might still generate a reasoning chain that is logically disconnected from its final answer (the issue SoO is designed to mitigate, but it's not fully solved).

- **First 3 Experiments:**
  1. **Reproduction on Benchmarks:** Implement the SoO prefilling on Llama-3-8B-Instruct and evaluate on the ToMATO (conversational) and ToMBench (narrative) benchmarks. Compare accuracy against a Vanilla (zero-shot) and standard CoT prefilling baseline. **Success metric:** SoO accuracy > CoT accuracy across mental state categories.
  2. **Ablation on Prefix Specificity:** Compare three prefilling conditions: (a) `Let's put ourselves in {name}'s shoes.`, (b) `Let's put ourselves in others' shoes.`, (c) `Let's think step-by-step.` **Success metric:** Condition (a) must significantly outperform (b) and (c), confirming the need for specific perspective-taking.
  3. **Correlation with Faithfulness:** Use a separate LLM (e.g., GPT-4o mini) to judge the faithfulness of reasoning chains generated by SoO vs. CoT. Plot the "win rate" in faithfulness against the accuracy improvement on the task. **Success metric:** A positive correlation, indicating that SoO's performance gains are linked to more faithful reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific cognitive or computational factors, other than faithfulness, contribute to the superior ToM performance of SoO prefilling?
- Basis in paper: [explicit] The conclusion (Section 5) states, "Identifying other crucial factors for ToM is future work," and Section C.4 notes that in some cases, win rates regarding faithfulness did not align with accuracy improvements, implying other latent factors are at play.
- Why unresolved: While the analysis establishes a correlation between faithfulness and accuracy, it does not fully account for performance variations in cases where faithfulness scores were tied or lower, suggesting the mechanism is not yet fully understood.
- What evidence would resolve it: An ablation study or mediation analysis that isolates specific linguistic features or reasoning heuristics (distinct from simple faithfulness) in the generated outputs that correlate with correct ToM predictions.

### Open Question 2
- Question: How does SoO prefilling perform when implemented with LLM-based name extraction compared to the current rule-based method?
- Basis in paper: [explicit] Appendix B.2 states, "While we employed the rule-based approach for name extraction, LLM-based approaches... can also be useful. Investigating the performance of LLM-based name extraction is future work."
- Why unresolved: The method currently relies on a deterministic, rule-based script to extract names for the prefix, which may fail on complex sentence structures or ambiguous references, potentially capping the method's performance.
- What evidence would resolve it: A comparison of ToM benchmark scores using the current rule-based extraction versus an LLM-based extraction method (e.g., using in-context learning) for identifying target characters.

### Open Question 3
- Question: Does SoO prefilling provide consistent performance improvements on proprietary models (e.g., GPT-4) that are not specifically fine-tuned for instruction following?
- Basis in paper: [explicit] Section 6 lists the inability to evaluate proprietary models as a limitation: "our proposed method is not evaluated with proprietary LLMs" because they lack support for specifying output prefixes.
- Why unresolved: The study restricts its findings to open-weight models (Llama, Mistral) and accessible proprietary models (GPT-3.5/4o-mini via prompting only), leaving the efficacy of prefilling on the most advanced proprietary architectures unknown.
- What evidence would resolve it: Evaluation results from a proprietary model API that supports output prefilling (or a simulated environment) comparing SoO prefilling against standard prompting methods.

### Open Question 4
- Question: Can the observed inconsistencies in performance gains across different datasets and models be mitigated through optimized prompt formatting?
- Basis in paper: [explicit] Section 6 states, "The performance gain drawn by SoO prefilling depends on models and data. This may be due to suboptimal designs of prompt formats."
- Why unresolved: The paper tests a specific phrasing ("Let's put ourselves in..."), and the variable results suggest this format may not align optimally with the training data of all model architectures.
- What evidence would resolve it: A systematic optimization of the SoO prefix phrasing for each specific model architecture to determine if the performance variance can be reduced.

## Limitations
- Performance gains diminish on larger models (GPT-4, GPT-4o), suggesting SoO is most beneficial for smaller LLMs.
- The rule-based name extraction mechanism may fail on complex or ambiguous questions, as evidenced by sharp performance drops when generic prefixes are used.
- The study focuses exclusively on English-language benchmarks and does not address cultural or linguistic variations in Theory of Mind reasoning.

## Confidence

**High Confidence:** The core finding that SoO prefilling improves Theory of Mind performance on established benchmarks (ToMATO and ToMBench) is well-supported by experimental results showing consistent accuracy gains across multiple mental state categories and both first- and second-order ToM tasks. The ablation studies demonstrating the importance of specific character naming provide strong evidence for the mechanism's validity.

**Medium Confidence:** The claim that SoO prefilling elicits more faithful reasoning is supported by faithfulness analysis, but the moderate correlation (R² = 0.38) suggests this is not the sole driver of performance improvements. The observation that performance gains diminish on larger models is based on limited comparisons and requires broader validation across model families.

**Low Confidence:** Claims about the general applicability of SoO prefilling to non-ToM tasks or different cultural contexts are speculative, as the current study does not test these scenarios. The assertion that SoO is superior to prompting-based approaches is qualified by the observation that prompting showed inconsistent results and often degraded performance, but this comparison is limited to the specific models and benchmarks tested.

## Next Checks
1. **Cross-Lingual Validation:** Test SoO prefilling on Theory of Mind benchmarks in languages other than English to assess whether the perspective-taking mechanism generalizes across linguistic and cultural contexts. This would involve translating existing benchmarks or developing new ones in target languages.
2. **Failure Mode Analysis:** Systematically identify question types where SoO prefilling fails or provides no benefit by conducting fine-grained error analysis on benchmark results. This would help characterize the boundaries of the method's effectiveness and inform targeted improvements to the name extraction component.
3. **Real-World Application Test:** Evaluate SoO prefilling on a practical Theory of Mind task outside of benchmark settings, such as detecting false beliefs in dialogue systems or assessing perspective-taking in educational contexts. This would demonstrate whether the benchmark improvements translate to meaningful real-world capabilities.