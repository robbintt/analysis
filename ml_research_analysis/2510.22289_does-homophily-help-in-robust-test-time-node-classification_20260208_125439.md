---
ver: rpa2
title: Does Homophily Help in Robust Test-time Node Classification?
arxiv_id: '2510.22289'
source_url: https://arxiv.org/abs/2510.22289
tags:
- graph
- test
- homophily
- graphs
- graphost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GrapHoST tackles robust test-time node classification by transforming\
  \ graph structure based on homophily properties, without requiring model retraining.\
  \ It learns a homophily predictor from training data to estimate edge homophily\
  \ confidence scores, then constructs a homophily-weighted test graph and applies\
  \ confidence-aware edge filtering\u2014removing low-confidence edges to increase\
  \ homophily in homophilic graphs or decrease it in heterophilic ones."
---

# Does Homophily Help in Robust Test-time Node Classification?

## Quick Facts
- **arXiv ID:** 2510.22289
- **Source URL:** https://arxiv.org/abs/2510.22289
- **Reference count:** 40
- **Primary result:** GrapHoST improves fixed GNN test-time performance across nine datasets with data quality issues, achieving up to 10.92% accuracy gains without model retraining.

## Executive Summary
GrapHoST is a plug-and-play framework for robust test-time node classification that transforms graph structure based on homophily properties. It learns a homophily predictor from training data to estimate edge confidence scores, then constructs a homophily-weighted test graph with confidence-aware edge filtering. The method removes low-confidence edges to increase homophily in homophilic graphs or decrease it in heterophilic ones. Theoretical analysis shows this structural transformation reduces misclassification probability, while empirical results demonstrate effectiveness across nine datasets with data quality issues.

## Method Summary
GrapHoST operates by first training a separate GNN (HOM_θ) as a binary classifier to distinguish homophilic vs heterophilic edges on the training graph using weighted binary cross-entropy loss. At test time, this predictor assigns confidence scores to edges in the test graph. The method then constructs a homophily-weighted test graph using these scores and applies confidence-aware edge filtering, removing the top-δ fraction of edges confidently predicted to be harmful (heterophilic in homophilic graphs, homophilic in heterophilic graphs). The transformed graph is then fed to a frozen pre-trained GNN for final classification. This approach requires no test-time model retraining and works across both homophilic and heterophilic graphs.

## Key Results
- Achieves up to 10.92% accuracy gains over baselines on datasets with data quality issues
- Consistently outperforms test-time methods like GTrans and model-centric approaches
- Demonstrates effectiveness on both homophilic and heterophilic graphs while maintaining efficiency
- Improves performance across nine benchmark datasets with various data quality challenges

## Why This Works (Mechanism)

### Mechanism 1: Homophily Alignment via Structural Transformation
The method transforms test graphs to align their homophily degree with the graph type, reducing misclassification probability for fixed pre-trained GNNs. Theoretical analysis using Contextual Stochastic Block Models shows that adjusting intra-class (p) and inter-class (q) edge probabilities moves expected GNN node embeddings further from the decision boundary. Increasing p and decreasing q in homophilic graphs increases distance between class centroids and the classifier boundary, reducing error. The core assumption is that the fixed classifier's decision boundary is optimal for training distribution, and node embeddings follow Gaussian distributions derived from CSBM assumptions.

### Mechanism 2: Inductive Edge Filtering via Confidence Scores
A proxy "homophily predictor" trained on labeled training edges effectively identifies and prunes harmful edges in unlabeled test graphs. The method trains a separate GNN as a binary classifier to distinguish homophilic vs heterophilic edges, then applies these scores to prune only the top-δ fraction of edges confidently predicted to be "harmful" (heterophilic in homophilic graphs, homophilic in heterophilic graphs). This improves the signal-to-noise ratio of message passing without requiring ground-truth test labels. The core assumption is that homophily patterns learned from training graphs transfer inductively to test graphs despite distribution shifts.

### Mechanism 3: Reweighted Message Passing
The raw probability scores from the homophily predictor serve as edge weights, enhancing the GNN's aggregation process before hard filtering. The predictor's output (probability of an edge being homophilic) serves as a weight, scaling messages from neighbors during propagation. This amplifies the contribution of edges predicted to support the graph's homophily type and dampens harmful signals. The core assumption is that the GNN architecture supports weighted adjacency matrices, and soft reweighting provides a gradient of utility that binary filtering cannot.

## Foundational Learning

- **Homophily vs. Heterophily**: The method defines "good" vs "bad" edges relative to the graph's dominant property. In homophilic graphs, "good" edges connect same-class nodes; in heterophilic graphs, they connect different-class nodes. *Quick check*: If a graph is heterophilic (p < q), should the method remove homophilic or heterophilic edges? (Answer: Homophilic edges, to lower the homophily degree).

- **Contextual Stochastic Block Model (CSBM)**: This theoretical framework explains why increasing separation between class centroids aids a fixed classifier. *Quick check*: In CSBM, what do parameters p and q represent? (Answer: Intra-class and inter-class edge connection probabilities).

- **Transductive vs. Inductive Test-Time Adaptation**: The predictor is trained on training data (inductive learning) but applied to a test graph that might have structure/attribute shifts. *Quick check*: Does GrapHoST update the base GNN parameters during the test phase? (Answer: No, it transforms the data structure only).

## Architecture Onboarding

- **Component map:** Base GNN (Frozen) -> Homophily Predictor (HOM_θ) -> Transformation Engine -> Transformed Graph
- **Critical path:** 1) Train HOM_θ using WBCE loss on training edges, 2) Pass test graph through HOM_θ to get edge scores, 3) Determine filtering threshold δ and filter edges, 4) Run frozen Base Model on transformed graph
- **Design tradeoffs:** Higher δ removes more noise but risks disconnecting the graph or losing structural information; using heterophilic GNN predictor for heterophilic datasets is theoretically better but GCN is used as robust default; requires training labels for predictor but no test graph node labels
- **Failure signatures:** Performance collapse on heterophilic graphs if filtering logic is inverted; OOM on large graphs if edge matrices exceed memory; over-smoothing if excessive filtering leaves nodes isolated
- **First 3 experiments:** 1) Measure ROC-AUC of HOM_θ predictor on test edges to ensure accuracy, 2) Run model with only reweighting vs only filtering to isolate contributions, 3) Sweep δ from 0.0 to 0.9 on validation splits to find optimal threshold

## Open Questions the Paper Calls Out

1. **Automatic δ determination:** Can the edge filtering ratio δ be determined automatically based on predictive confidence rather than manual tuning? The paper notes this as a future exploration opportunity, as the current method relies on searching for δ via validation data.

2. **Automatic transformation direction detection:** How can the framework automatically detect the appropriate transformation direction (increasing vs decreasing homophily) for an unseen test graph? The paper requires a binary switch for "Hom. case" or "Het. case" but provides no method to detect this property for test graphs undergoing distribution shift.

3. **Theoretical guarantees under degree imbalance:** Do the theoretical guarantees for reduced misclassification probability hold under severe node degree imbalance without the strict constraints identified in the appendix? The paper notes that real-world graphs often exhibit heavy-tailed degree distributions that may violate constraints required for current theoretical proof.

## Limitations

- Assumes the homophily predictor trained on training data generalizes to test graphs with distribution shifts without explicit validation of predictor accuracy on test edges
- Theoretical analysis relies on CSBM assumptions (degree invariance, Gaussian embeddings) that may not hold in real-world graphs
- Uses fixed optimal filtering ratio (δ=0.3) across datasets without sensitivity analysis in main results

## Confidence

- **High confidence:** Empirical results showing consistent accuracy improvements across nine datasets with data quality issues
- **Medium confidence:** Theoretical analysis linking homophily transformation to reduced misclassification probability under CSBM assumptions
- **Low confidence:** Inductive transfer assumption of homophily predictor to shifted test distributions without explicit validation

## Next Checks

1. Measure ROC-AUC of the homophily predictor on test edges (if ground truth available) to validate inductive transfer
2. Compare performance with only reweighting vs only filtering to isolate their individual contributions
3. Sweep δ from 0.0 to 0.9 on validation splits to identify optimal thresholds and detect information loss trade-offs