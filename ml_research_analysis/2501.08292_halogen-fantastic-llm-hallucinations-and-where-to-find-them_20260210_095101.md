---
ver: rpa2
title: 'HALoGEN: Fantastic LLM Hallucinations and Where to Find Them'
arxiv_id: '2501.08292'
source_url: https://arxiv.org/abs/2501.08292
tags:
- atomic
- response
- text
- units
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HALoGEN is a benchmark for measuring hallucination in large language
  models (LLMs) across nine diverse domains including code, summarization, scientific
  attribution, and historical events. It provides over 10,000 prompts and automatic
  verifiers to decompose model generations into atomic facts and verify them against
  high-quality knowledge sources.
---

# HALoGEN: Fantastic LLM Hallucinations and Where to Find Them

## Quick Facts
- arXiv ID: 2501.08292
- Source URL: https://arxiv.org/abs/2501.08292
- Reference count: 37
- Primary result: No single domain is predictive of hallucination behavior across others

## Executive Summary
HALoGEN introduces a comprehensive benchmark for measuring hallucination in large language models across nine diverse domains including code, summarization, scientific attribution, and historical events. The benchmark provides over 10,000 prompts and automatic verifiers that decompose model generations into atomic facts and verify them against high-quality knowledge sources. Evaluation of 14 LLMs reveals hallucination rates ranging from 4% to 86% depending on the domain, with even the best models frequently producing factual errors. The study introduces a novel error classification system (Type A, B, C) to trace hallucinations back to pretraining data and finds that performance in one domain does not predict performance in others, highlighting the need for diverse benchmarks.

## Method Summary
HALoGEN evaluates LLM hallucinations using a decomposition-verification pipeline across nine domains. The method generates responses from 14 target LLMs using 10,923 prompts, then decomposes each generation into atomic units using task-specific engines (regex, LLM prompting, FActScore). Each unit is verified against authoritative knowledge sources (PyPI, Semantic Scholar, entailment models, Wikipedia). Three metrics are computed: Hallucination Score (fraction of unsupported atomic facts), Response Ratio (fraction of prompts where model responds), and Utility Score (combines both). The framework also traces hallucinated facts to pretraining corpora to classify errors as Type A (correct fact present - retrieval failure), Type B (incorrect fact present - contamination), or Type C (neither present - fabrication).

## Key Results
- Hallucination rates range from 4% to 86% across domains, with no single domain predictive of performance across others
- GPT-4 and GPT-3.5 show the best performance overall, but even these models produce significant factual errors
- Error type analysis reveals that hallucinated code packages often appear in pretraining data (Type B), while historical event entities rarely co-occur (Type C)
- Domain-specificity is strong: summarization and simplification show high correlation, but biographies weakly predict other domains

## Why This Works (Mechanism)

### Mechanism 1: Atomic Decomposition-Verification Pipeline
- **Claim:** Hallucinations can be systematically measured by decomposing model outputs into atomic units and verifying each against authoritative knowledge sources.
- **Mechanism:** The pipeline extracts verifiable units (e.g., package names, citations, factual claims), then checks each against domain-specific sources (PyPI, Semantic Scholar, entailment classifiers). Hallucination score is computed as the fraction of unsupported atomic units.
- **Core assumption:** Hallucinations manifest at the atomic fact level and can be detected via discrete verification steps.
- **Evidence anchors:** Automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source; defines decomposition engine D and verifier V as core components; describes nine tasks with tailored verifiers.

### Mechanism 2: Error Type Attribution via Training Data Tracing
- **Claim:** Hallucinations can be traced to three distinct causal pathways related to pretraining data presence and correctness.
- **Mechanism:** By searching pretraining corpora (C4, Dolma, OpenWebText) for hallucinated facts, errors are classified as: Type A (correct fact present - retrieval failure), Type B (incorrect fact present - contamination), or Type C (neither present - fabrication/over-generalization).
- **Core assumption:** Pretraining corpora can be meaningfully searched for exact or near-exact fact matches, and presence/absence in corpus causally relates to model output behavior.
- **Evidence anchors:** Novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A), or incorrect knowledge in training data (Type B), or are fabrication (Type C errors); code packages: 38-72% of hallucinated packages found in pretraining corpora (Type B); historical events: entities rarely co-occur (Type C).

### Mechanism 3: Domain-Specific Hallucination Divergence
- **Claim:** Hallucination behavior does not transfer predictably across domains; performance in one domain is not strongly predictive of performance in others.
- **Mechanism:** Model rankings on hallucination metrics show low cross-domain correlation (Spearman correlation matrix). This suggests hallucination arises from domain-specific failure modes rather than a uniform model-level property.
- **Core assumption:** Hallucination propensity is task- and domain-conditional, not a stable model trait. Different domains stress different capabilities (retrieval, reasoning, abstention calibration).
- **Evidence anchors:** No single domain is predictive of hallucination behavior across others, highlighting the need for diverse benchmarks; correlation analysis shows summarization and simplification highly correlated (both content-grounded), but biographies weakly predictive of other domains; code generation shows distinct patterns.

## Foundational Learning

- **Concept: Hallucination as misalignment (not just fabrication)**
  - **Why needed here:** The paper defines hallucination broadly - including both factual errors and statements contradicting provided context (even if externally true). This matters for grounded tasks like summarization.
  - **Quick check question:** If a model summarizes a news article and adds a true fact not in the article, is this a hallucination under HALoGEN's definition?

- **Concept: Atomic fact decomposition**
  - **Why needed here:** The entire verification pipeline depends on breaking outputs into independently verifiable units. Different domains require different decomposition strategies (regex for code imports, LLM prompting for narrative facts).
  - **Quick check question:** What decomposition approach would you use for verifying claims in a model-generated medical diagnosis?

- **Concept: Response-based vs. refusal-based evaluation**
  - **Why needed here:** HALoGEN evaluates both tasks where models should respond (biographies, code) and tasks where they should refuse (impossible historical meetings, false presuppositions). Utility score combines factual precision with appropriate abstention.
  - **Quick check question:** A model correctly refuses 95% of impossible historical meeting prompts but hallucinates on 80% of biography prompts - what is its overall utility?

## Architecture Onboarding

- **Component map:**
  Prompts (9 domains, 10,923 total) -> Model Generation (14 LLMs, ~150K outputs) -> Decomposition Engine D (domain-specific: regex, LLM, FActScore) -> Atomic Units (packages, citations, facts, entities) -> Verifier V (PyPI, Semantic Scholar, entailment models, rule-based) -> Scoring (Hallucination Score, Response Ratio, Utility Score) -> Error Attribution (search pretraining corpora -> Type A/B/C)

- **Critical path:** Prompt design -> Decomposition accuracy -> Verifier precision -> Score reliability. Verifier accuracy varies: code/package verification is high-precision (exact PyPI match); open-ended text verification depends on LLM judge reliability (83-92% human agreement reported).

- **Design tradeoffs:**
  - Automated vs. human verification: Automated enables scale (150K generations) but introduces verifier noise; human verification is precise but cost-prohibitive at this scale.
  - Single vs. multiple knowledge sources: Each domain uses one "source of truth" (PyPI, Semantic Scholar); this simplifies evaluation but may misclassify edge cases where truth is contested.
  - Open vs. proprietary model attribution: Training data tracing works for OLMo (open corpus) but is speculative for GPT-4 (assumes OpenWebText as proxy).

- **Failure signatures:**
  - High hallucination on code packages despite high corpus coverage (Type B): model retrieves deprecated/local/context-specific imports
  - High hallucination on senator affiliations with correct data in corpus (Type A): retrieval failure despite knowledge presence
  - Low refusal rate on impossible historical meetings (Type C): models fabricate narratives when entities never co-occur in training

- **First 3 experiments:**
  1. **Verifier calibration study:** Sample 200 atomic units per task; compare automated verifier judgments against human annotations to quantify precision/recall gaps per domain.
  2. **Cross-domain transfer test:** Fine-tune a model on high-factuality domain (e.g., summarization with low hallucination); evaluate if hallucination improves on other domains (tests domain-specificity hypothesis).
  3. **Error type intervention:** For Type A-heavy domains (senator affiliations), implement retrieval-augmented generation with Wikipedia; measure hallucination reduction vs. baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Verifier reliability varies significantly across domains, with human agreement ranging from 83% to 92% for open-ended text tasks
- Training data attribution assumptions rely on surface-level corpus search that may miss implicit knowledge and is speculative for proprietary models
- Domain coverage uses single "source of truth" per domain, potentially misclassifying edge cases where truth is contested

## Confidence
- **High Confidence:** No single domain is predictive of hallucination behavior across others - supported by cross-domain correlation analysis
- **Medium Confidence:** GPT-4 and GPT-3.5 show the best performance overall - based on utility score rankings
- **Medium Confidence:** Hallucination rates range from 4% to 86% depending on the domain - metric calculations are explicit
- **Low Confidence:** Error type attribution to pretraining data - speculative for proprietary models and dependent on corpus search assumptions

## Next Checks
1. **Verifier Calibration Study:** Manually annotate 200 atomic units per task (100 high-confidence and 100 ambiguous cases) and compare automated verifier judgments against human annotations. Calculate precision/recall gaps per domain and identify systematic verifier biases.

2. **Cross-Domain Transfer Experiment:** Fine-tune a model on high-factuality domain (e.g., summarization with low hallucination) while controlling for model size and training data. Evaluate hallucination on other domains to test whether improvements transfer.

3. **Error Type Intervention Test:** For Type A-heavy domains (senator affiliations), implement retrieval-augmented generation with Wikipedia as external knowledge source. Measure hallucination reduction compared to baseline.