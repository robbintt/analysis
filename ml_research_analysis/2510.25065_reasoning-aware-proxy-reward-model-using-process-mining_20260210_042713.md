---
ver: rpa2
title: Reasoning-Aware Proxy Reward Model using Process Mining
arxiv_id: '2510.25065'
source_url: https://arxiv.org/abs/2510.25065
tags:
- uni00000013
- reasoning
- process
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TACReward, a reasoning-aware proxy reward
  model that uses process mining techniques to evaluate the structural alignment between
  policy and teacher reasoning traces in sparse-reward reinforcement learning for
  language models. By formalizing reasoning steps into structured event logs and measuring
  conformance via fitness and precision, TACReward provides a scalar reward in [0,1]
  that acts as a proxy for step-level reasoning quality without requiring human annotations
  or architectural changes.
---

# Reasoning-Aware Proxy Reward Model using Process Mining

## Quick Facts
- arXiv ID: 2510.25065
- Source URL: https://arxiv.org/abs/2510.25065
- Reference count: 40
- Primary result: GSPO+TACReward achieves 89.2% average relative accuracy improvement on mathematical reasoning benchmarks

## Executive Summary
This paper introduces TACReward, a reasoning-aware proxy reward model that uses process mining techniques to evaluate structural alignment between policy and teacher reasoning traces in sparse-reward reinforcement learning for language models. By formalizing reasoning steps into structured event logs and measuring conformance via fitness and precision, TACReward provides a scalar reward in [0,1] that acts as a proxy for step-level reasoning quality without requiring human annotations or architectural changes. Experiments on multiple mathematical reasoning benchmarks show that integrating TACReward into sparse-reward policy gradient methods—especially with GSPO—yields consistent performance gains, with an average relative accuracy improvement of 89.2%. The method effectively encourages improved logical maturity in reasoning processes and outperforms several state-of-the-art RL baselines.

## Method Summary
TACReward formalizes reasoning traces as event logs using a 20-activity taxonomy, applies process discovery algorithms to extract process models, and computes optimal alignments between policy and teacher traces. The conformance score combines fitness (completeness) and precision (selectivity) into an F1 metric that serves as a proxy reward for policy gradient methods. The method is integrated with sparse-reward algorithms like GRPO and GSPO, where TACReward aggregates step-level structural deviations into a scalar reward that provides implicit credit assignment for reasoning quality without dense annotations.

## Key Results
- GSPO+TACReward achieves average relative accuracy improvement of 89.2% across multiple benchmarks
- Using both fitness and precision components yields balanced reasoning behavior, while fitness-only increases verbosity and precision-only decreases output length
- TACReward integration shows consistent performance gains across all tested RL algorithms, with GSPO showing strongest synergy due to sequence-level granularity match

## Why This Works (Mechanism)

### Mechanism 1: Structural Conformance as Step-Level Proxy
TACReward computes optimal alignments between policy process models and teacher traces, where each move (synchronous, log-only, model-only) incurs local cost. Aggregating these stepwise deviations into a scalar reward provides implicit credit assignment for reasoning quality without dense annotations. The teacher model is assumed to produce "more logically mature" reasoning traces than the policy model.

### Mechanism 2: Sequence-Level Granularity Match with GSPO
GSPO treats each sampled response as a single optimization unit with importance ratio s_i(θ) = (π_θ(y_i|x) / π_θ^old(y_i|x))^(1/|y_i|). TACReward aggregates step-level deviations into one conformance score per response, creating a coherent unit of credit assignment that allows GSPO to exploit TACReward signals more effectively than token-level or step-mismatched objectives.

### Mechanism 3: Fitness-Precision Balance Controls Reasoning Behavior
Fitness penalizes missing essential steps (log-only moves), while precision penalizes excessive or unjustified behavior (model-only moves). Their F1 harmonic mean produces balanced reasoning: fitness-only encourages verbose outputs with longer chains, precision-only produces conservative outputs with shorter chains, and F1 balances both completeness and selectivity.

## Foundational Learning

- **Concept: Event Logs and Process Mining**
  - Why needed: TACReward formalizes reasoning as structured event logs (case ID, activity, timestamp) and applies process discovery algorithms. Without this, alignment computation is opaque.
  - Quick check: Given a reasoning trace ⟨Formulate Strategy, Apply Formula, Identify Contradiction, Verify Answer⟩, what is the difference between a synchronous move, log-only move, and model-only move in alignment?

- **Concept: Sparse vs Dense Rewards in Policy Gradient**
  - Why needed: The paper targets sparse-reward methods (GRPO, GSPO, RLOO) where outcome rewards provide limited intermediate feedback. This clarifies why TACReward is designed as a scalar sequence-level reward rather than step-level dense feedback.
  - Quick check: Why does the paper avoid integrating TACReward as a step-level dense reward despite computing step-level alignments?

- **Concept: Group-Relative Advantages**
  - Why needed: GRPO/GSPO compute advantages by normalizing rewards within a response group. TACReward signals interact with this baseline computation.
  - Quick check: In equation (21), how does adding r^TAC to the total reward affect the group-relative advantage when all G responses have similar conformance scores?

## Architecture Onboarding

- **Component map:** Query x → [Policy Model π_θ] → G responses {y_i} → [Trace Formalizer π_ψ (DeepSeek-V3.2)] → Policy traces {σ_i} → [Inductive Miner] → Process models {M_i} → [Alignment Computation] → Optimal alignments {γ*_i} → [Conformance Scoring] → Fitness + Precision → F1 = r^TAC_i → [Policy Gradient Update (GSPO)] → Updated π_θ

- **Critical path:** Trace formalization quality determines everything downstream. If the taxonomy mapper mislabels activities, alignment costs become meaningless.

- **Design tradeoffs:**
  - Overhead vs signal quality: 4-6× per-step training time increase is acceptable for high-value reasoning tasks but may be prohibitive for large-scale pretraining
  - External API dependency: DeepSeek-V3.2 API for trace formalization introduces latency and cost that scale with training steps and G
  - Teacher model choice: DeepSeek-R1 is strong but not infallible; method only requires teacher be "more mature" than policy

- **Failure signatures:**
  1. Reward stagnation: TACReward stops increasing → possible taxonomy mismatch or teacher policy collapse
  2. Length explosion with fitness-only: Mean completion length grows unbounded → fitness component encouraging verbosity
  3. Over-conservative outputs with precision-only: Model produces minimal reasoning → precision component penalizing exploration
  4. PPO+TAC underperformance: Negative relative gains → token-level objective mismatch with sequence-level reward

- **First 3 experiments:**
  1. Taxonomy ablation: Train with MRT vs without on multi-step benchmarks (AIME, KSAT)
  2. Component ablation: Train with fitness-only, precision-only, and F1 on mean length, entropy, and accuracy
  3. Algorithm compatibility: Compare TACReward integration with PPO, RLOO, GRPO, GSPO on fixed compute budget (240 steps)

## Open Questions the Paper Calls Out

- **Computational Overhead:** Can the computational overhead of trace formalization and process discovery be minimized to allow TACReward to scale efficiently without relying on heavy external API calls? The authors identify this as a key limitation and call for future work to improve efficiency.

- **Domain Generalization:** Can TACReward generalize to other reasoning domains, such as coding or logical deduction, without manually defining new activity taxonomies? Section D.4 notes that applying the method to other domains requires defining appropriate sets of reasoning activities.

- **Trace Extraction Stability:** To what extent does noise or error in the intermediate trace extraction phase impact the stability of the final policy gradient? The paper assumes accurate trace extraction but does not ablate the impact of extraction errors on the reward signal.

## Limitations

- Computational overhead from trace formalization and process discovery steps (4-6× per-step training time increase)
- Reliance on external DeepSeek APIs for trace formalization and teacher reference generation
- Method requires defining appropriate reasoning activity taxonomies for different domains

## Confidence

- **High:** Structural conformance as proxy reward and fitness-precision balance controlling reasoning behavior are well-supported by experimental evidence and ablation studies
- **Medium:** Sequence-level granularity match with GSPO shows strong empirical results (+89.2%) but lacks corpus evidence explaining specific synergy
- **Medium:** Overall performance gains are convincingly demonstrated, though computational overhead is a significant practical limitation

## Next Checks

1. Apply TACReward to a reasoning domain outside mathematics (e.g., code generation) and verify the 20-activity taxonomy still captures meaningful structural patterns
2. Compare performance when using different teacher models (e.g., GPT-4o, Claude-3) or when injecting synthetic errors into teacher traces
3. Implement the alignment computation from scratch using standard PM4Py libraries and verify fitness/precision scores match reported values on a validation set