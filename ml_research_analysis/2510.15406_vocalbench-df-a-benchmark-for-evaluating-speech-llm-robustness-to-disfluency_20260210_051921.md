---
ver: rpa2
title: 'VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency'
arxiv_id: '2510.15406'
source_url: https://arxiv.org/abs/2510.15406
tags:
- speech
- disfluency
- original
- repetition
- largest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces VOCALBENCH-DF, the first benchmark designed\
  \ to evaluate Speech Large Language Models (Speech-LLMs) on their ability to handle\
  \ speech disfluency. It categorizes disfluency along two dimensions\u2014linguistic\
  \ realization and interactional interference\u2014and decomposes them into six subcategories\
  \ and nine subtasks."
---

# VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency

## Quick Facts
- arXiv ID: 2510.15406
- Source URL: https://arxiv.org/abs/2510.15406
- Authors: Hongcheng Liu; Yixuan Hou; Heyang Liu; Yuhao Wang; Yanfeng Wang; Yu Wang
- Reference count: 9
- Primary result: Introduces VOCALBENCH-DF, the first benchmark to systematically evaluate Speech-LLMs on their ability to handle speech disfluency across two dimensions and nine subtasks.

## Executive Summary
This paper introduces VOCALBENCH-DF, the first benchmark designed to evaluate Speech Large Language Models (Speech-LLMs) on their ability to handle speech disfluency. It categorizes disfluency along two dimensions—linguistic realization and interactional interference—and decomposes them into six subcategories and nine subtasks. The benchmark evaluates 22 mainstream Speech-LLMs across diverse disfluency types, revealing that most models experience substantial performance degradation, particularly under phoneme-level disfluency and long-context modeling. Analysis identifies stronger speech recognition and reasoning capabilities as critical for improving robustness. The findings highlight the urgent need for methods that enhance disfluency handling to build more inclusive Speech-LLMs.

## Method Summary
The benchmark uses OpenAudioBench subset (2300 samples) with synthesized speech via CosyVoice. Disfluency transformations are applied at text and speech levels using parameters from Table 6 (e.g., repetition probability 0.6-0.8, pause duration 0.1-0.9s, noise levels -35 to -15dB). 22 Speech-LLMs are evaluated across 9 subtasks with three severity levels. Performance is measured by normal accuracy and relative accuracy rate (disfluent/normal). Human evaluation validates quality (>0.94 score). Results are compared against ASR+LLM pipelines (Whisper+GPT-4o baseline).

## Key Results
- Most Speech-LLMs show substantial performance degradation under disfluency, especially phoneme-level and long-context scenarios
- Stronger speech encoders (e.g., Whisper-large-v3) and better reasoning LLMs correlate with improved robustness
- Cascaded ASR+LLM pipelines outperform end-to-end Speech-LLMs on utterance modifications like corrections and rebacks
- Phoneme-level disfluencies are identified as the primary bottleneck, causing up to 28% performance drop compared to word-level repetition

## Why This Works (Mechanism)

### Mechanism 1: Stronger Speech Encoders Improve Phoneme-Level Disfluency Handling
- Claim: Models using higher-capacity speech encoders demonstrate greater robustness to phoneme-level disfluencies
- Mechanism: More powerful encoders extract cleaner acoustic features, better separating noise from semantic signal before LLM processing
- Core assumption: Improved robustness stems from superior feature extraction and noise resilience, not other concurrent changes
- Evidence anchors:
  - [abstract] "...Analysis identifies stronger speech recognition... as critical for improving robustness"
  - [section 6, Observation 8] "...Kimi-Audio uses Kimi-Audio-Tokenizer (Whisper-Large) and attains 0.98, but MiniCPM-o-2.6 uses Whisper-Medium and reaches 0.91"
  - [corpus] Weak or missing direct evidence on encoder capacity mechanism

### Mechanism 2: Cascaded ASR+LLM Pipelines Can Be More Robust Than End-to-End Speech-LLMs
- Claim: ASR followed by text LLM can be more robust to certain disfluencies than end-to-end Speech-LLM
- Mechanism: Dedicated ASR optimized to produce clean transcripts, potentially normalizing disfluent speech before LLM reasoning
- Core assumption: Primary failure of end-to-end models is "bleeding" of acoustic disfluency into LLM reasoning
- Evidence anchors:
  - [abstract] "...Analysis identifies stronger speech recognition and reasoning capabilities as critical for improving robustness"
  - [section 5.3, Observation 1] "...Whisper+GPT-4o maintains accuracy across all three [utterance] modification types... whereas Speech-LLMs degrade"
  - [corpus] Weak or missing direct evidence on cascaded vs end-to-end mechanism

### Mechanism 3: LLM Reasoning Capacity and Training Objectives Shape Disfluency Recovery
- Claim: LLM reasoning capability and training objectives determine ability to recover intent from disfluent input
- Mechanism: More capable LLM performs better semantic reconstruction, using context to fill gaps or discard irrelevant parts
- Core assumption: Performance differences attributable to LLM reasoning and fine-tuning, not just model scale
- Evidence anchors:
  - [abstract] "...Analysis identifies... reasoning capabilities as critical for improving robustness"
  - [section 6, Observation 15] "...stronger LLMs achieve higher overall accuracy, with improvements exceeding 4%. Better reasoning helps normalize disfluent inputs"
  - [section 6, Observation 10] "...the thinking model shows... more consistent performance [than the captioner variant]"
  - [corpus] Related paper [59673] introduces LLM-based disfluency removal benchmark

## Foundational Learning

- Concept: Speech Disfluency Taxonomy
  - Why needed here: The benchmark's core is a systematic two-axis framework: Linguistic Realization (speaker's own speech disfluencies) and Interactional Interference (environmental disfluencies)
  - Quick check question: Is a "filled pause" (e.g., "um") a Linguistic Realization or an Interactional Interference? What about "background music"?

- Concept: Phonetic vs. Semantic Disfluency
  - Why needed here: Paper identifies phoneme-level disfluency as primary bottleneck; understanding difference between acoustic distortion (e.g., "c-c-capital") vs. meaning alteration (e.g., word repetition) is critical
  - Quick check question: Which type of disfluency did the paper find to be "most consistently harmful" across models?

- Concept: End-to-End vs. Cascaded Speech-LLMs
  - Why needed here: Paper compares these architectures; need to know difference to understand why cascaded model might be more robust but slower
  - Quick check question: Which architecture does paper suggest is better at handling utterance modifications like corrections or rebacks?

## Architecture Onboarding

- Component Map:
  Input Layer -> Speech Encoder (e.g., Whisper) -> Projector/Adapter -> LLM Backbone (e.g., Qwen, LLaMA) -> Output

- Critical Path: Disfluent utterance processed; Speech Encoder must robustly extract features despite acoustic noise; LLM must reason over features to perform semantic reconstruction and identify core user intent

- Design Tradeoffs:
  - Encoder Strength vs. Cost/Latency: Larger encoders improve robustness but are slower
  - LLM Reasoning vs. Scale: More capable LLM improves intent recovery but increases size and cost; scale alone insufficient
  - End-to-End vs. Cascaded: End-to-end is lower latency, but cascaded ASR+LLM pipeline is empirically more robust

- Failure Signatures:
  - "Anchoring to False Starts": Model answers first incorrect part instead of final corrected query (failure in handling Correction)
  - "Semantic Distortion": Model misled by word repetitions, taking them as part of core query (failure in handling Repetition)
  - "Context Loss": Model fails to recall information from beginning of long, disfluent input (failure in Long-Context modeling)

- First 3 Experiments:
  1. Encoder Ablation: Same LLM backbone, compare performance across disfluency types using different speech encoders (Whisper-small vs. Whisper-large-v3)
  2. Cascaded vs. End-to-End Benchmark: Evaluate top end-to-end Speech-LLM against cascaded ASR+LLM pipeline on VOCALBENCH-DF tasks
  3. Disfluency-Specific Training: Fine-tune Speech-LLM with training objective targeting failure mode (e.g., dataset of "restart" queries to teach anchoring to final query)

## Open Questions the Paper Calls Out

- Does synthetic nature of VocalBench-DF mask performance discrepancies found in natural, spontaneous disfluent speech?
  - Basis: Authors state synthetic generation "may fail to capture full variability of spontaneous speech" and list shifting to spontaneous speech as future work
  - Why unresolved: CosyVoice simulation may lack acoustic nuance of actual pathological speech or natural conversation
  - What evidence would resolve it: Correlation analysis comparing model performance on VocalBench-DF against natural spontaneous speech dataset

- What specific architectural modifications are required to normalize fine-grained phoneme repetitions into coherent semantic representations?
  - Basis: Observation 11 notes models "ignore semantic modeling in phonemes" and lack mechanisms to normalize phonemic disruptions, resulting in 28% performance drop
  - Why unresolved: Paper identifies phoneme-level processing as bottleneck but only concludes "stronger speech recognition" helps, without proposing specific interventions
  - What evidence would resolve it: Ablation studies showing dedicated phoneme error correction layers significantly improve "Any Phoneme Repetition" task scores

- How does model robustness vary when evaluating articulatory disorders and phoneme confusions not covered by current taxonomy?
  - Basis: Authors explicitly state "Articulatory disorders and phoneme confusions, such as contrast between alveolar and velar nasals, are not modeled"
  - Why unresolved: Benchmark focuses on repetition and filler words but excludes specific motor-speech errors common in conditions like Parkinson's disease
  - What evidence would resolve it: Evaluation results from extended benchmark including tasks specifically designed to test phoneme confusion and articulatory distortion

## Limitations

- Synthetic speech generation through CosyVoice may not capture full variability of naturally occurring disfluent speech
- Evaluation primarily covers encoder-decoder architectures, limiting generalizability to emerging architectural families
- Cannot definitively separate whether performance degradation stems from disfluency itself or additional cognitive load of filtering environmental interference

## Confidence

- **High Confidence**: Phoneme-level disfluencies cause most severe performance degradation; cascaded ASR+LLM pipelines outperform end-to-end models for utterance modifications
- **Medium Confidence**: Stronger speech encoders directly improve phoneme-level handling (correlational evidence); LLM reasoning capability relates to disfluency recovery (consistent patterns but lacks mechanistic isolation)
- **Low Confidence**: Targeted training objectives can improve robustness (speculative, no experimental validation); exact failure points in disfluency handling pipeline not identified

## Next Checks

1. Evaluate VOCALBENCH-DF on diverse Speech-LLM architectures including Mamba-based models and hybrid systems to determine if observed patterns hold across architectural families

2. Collect naturally disfluent speech corpus and evaluate same models on both synthetic and natural disfluencies to quantify ecological validity gap

3. Design controlled experiments isolating contribution of each architectural component through encoder-only, LLM-only, and projector configuration variations to identify precise failure points in disfluency handling pipeline