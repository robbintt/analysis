---
ver: rpa2
title: 'ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos'
arxiv_id: '2509.20467'
source_url: https://arxiv.org/abs/2509.20467
tags:
- video
- detection
- fact-checking
- videos
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHORTCHECK addresses the challenge of detecting checkworthy short-form
  videos on platforms like TikTok, where multimodal, noisy content complicates misinformation
  detection. The system integrates speech transcription, OCR, deepfake detection,
  video summarization, and claim verification into a modular, inference-only pipeline
  designed for fact-checkers.
---

# ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos

## Quick Facts
- arXiv ID: 2509.20467
- Source URL: https://arxiv.org/abs/2509.20467
- Authors: Henrik Vatndal; Vinay Setty
- Reference count: 12
- Primary result: F1-weighted scores over 70% on multilingual short-form video checkworthiness detection

## Executive Summary
SHORTCHECK is an inference-only pipeline for detecting checkworthy short-form videos on platforms like TikTok. The system integrates speech transcription, OCR, deepfake detection, video summarization, and claim verification into a modular architecture designed for fact-checkers. Evaluated on Norwegian and English TikTok datasets, SHORTCHECK achieves strong performance with F1-weighted scores over 70%, demonstrating particularly high recall for checkworthy content in Norwegian (85%) and high precision in English (82%). The system prioritizes text-derived signals from speech and ideological language detection, while visual features like deepfake detection show limited standalone impact.

## Method Summary
SHORTCHECK processes TikTok videos through a modular pipeline that extracts and analyzes multimodal content. The system uses EasyOCR for overlay text extraction, Whisper for speech transcription, LLaVA for video frame summarization, and LLaMA 3 for semantic classification. A rule-based aggregator combines outputs from individual modules, with transcript and ideological language detection contributing most to performance. The pipeline operates without task-specific fine-tuning, using multilingual foundation models to support over 30 languages. Evaluation was conducted on manually annotated datasets of 491 Norwegian and English TikTok videos, with the system achieving F1-weighted scores above 70%.

## Key Results
- F1-weighted scores exceed 70% on both Norwegian (0.72) and English (0.74) TikTok datasets
- Norwegian recall for checkworthy content reaches 85%, while English precision achieves 82%
- Ablation studies show transcript and ideological language modules contribute most to performance, with visual features like deepfake detection having limited standalone impact

## Why This Works (Mechanism)

### Mechanism 1: Text-Dominant Signal Extraction
- Claim: Transcribed speech and ideological language detection contribute more to checkworthiness classification than visual features in short-form videos.
- Mechanism: The pipeline prioritizes textual modalities—OCR for overlay text and Whisper for speech transcription—which are then analyzed for ideological buzzwords and claim structures. These signals feed into a rule-based scorer that weights transcript verdicts heavily. When cumulative text-derived signals exceed threshold, the video is flagged as checkworthy.
- Core assumption: Claims of public interest (political, health, societal) are primarily communicated through spoken or overlaid text rather than purely visual storytelling.
- Evidence anchors:
  - [abstract]: "Ablation studies show that transcript and ideological language modules contribute most to performance"
  - [Table 5, Page 6]: Removing Transcript causes -0.076 recall drop; removing Buzzword causes -0.033 F1-W drop. Weapon detection removal slightly improves performance.
  - [corpus]: Related work on short-form video analysis (Lotus, GET-Tok) similarly relies on transcript enrichment, suggesting text extraction is a common bottleneck.

### Mechanism 2: Modular Fallback for Noisy Modalities
- Claim: A rule-based aggregation of independent module outputs provides robustness when individual modalities are missing or degraded.
- Mechanism: Each module (OCR, transcription, deepfake detection, video summarization) operates independently. The aggregation engine combines scores additively—if OCR fails due to stylized fonts, transcript and buzzword signals can still trigger classification. Advertisement detection overrides all other signals early in the pipeline.
- Core assumption: No single modality is consistently reliable across short-form video content; redundancy compensates for noise.
- Evidence anchors:
  - [Page 4]: "Each component in the pipeline can be independently replaced, which makes the system robust to failures in specific modalities"
  - [Page 5]: "removing individual modules does not show a huge drop in performance, the overall performance is attributed to contribution of modules"

### Mechanism 3: Cross-Lingual Generalization via Multilingual Foundation Models
- Claim: Using multilingual-capable foundation models (Whisper, LLaMA 3) enables the pipeline to process 30+ languages without language-specific fine-tuning.
- Mechanism: Whisper provides transcription in 100+ languages; LLaMA 3 handles semantic classification. The rule-based buzzword module requires language-specific curation but operates on extracted text rather than raw audio.
- Core assumption: Multilingual foundation models transfer sufficiently to short-form video domains without task-specific adaptation.
- Evidence anchors:
  - [Page 4]: "predict checkworthiness in over 30 major languages" via intersection of LLaMA 3 and Whisper support
  - [Table 2, Page 6]: Norwegian F1-W = 0.72, English F1-W = 0.74, showing comparable cross-lingual performance

## Foundational Learning

- **Multimodal Fusion Strategies**
  - Why needed here: SHORTCHECK uses late fusion (rule-based aggregation of module outputs) rather than early fusion (concatenated embeddings). Understanding when each approach is appropriate informs architectural decisions.
  - Quick check question: Can you explain why late fusion might outperform early fusion when modalities have different reliability patterns?

- **Whisper ASR Limitations**
  - Why needed here: The system relies on Whisper for transcription, but performance degrades with overlapping music/sound effects common in TikTok content.
  - Quick check question: What preprocessing steps might improve Whisper accuracy on music-heavy short-form videos?

- **Checkworthiness vs. Fact-Checking**
  - Why needed here: SHORTCHECK predicts whether a video warrants verification, not whether it is true/false. This is a triage task, not a verdict task.
  - Quick check question: How would you explain to a stakeholder why high recall on checkworthiness doesn't imply the system identifies misinformation?

## Architecture Onboarding

- **Component map:**
  - Video ingestion -> EasyOCR (overlay text) -> Whisper (transcript) -> Frame sampling -> LLaVA (frame descriptions) -> Face extraction -> Deepfake models (MesoNet/EfficientNet/Wvolf-ViT) -> LLaMA 3 (transcript verdict + summary verdict) -> Buzzword detection (rule-based) -> Fact-check API lookup -> Rule-based scorer -> Final label (Checkworthy / Not Checkworthy)

- **Critical path:**
  1. Video ingestion → OCR + Whisper + Frame extraction (parallel)
  2. Text streams → Buzzword + Verdict classification
  3. All module scores → Aggregation engine → Threshold comparison
  4. Advertisement override check → Final output

- **Design tradeoffs:**
  - Modularity vs. end-to-end: Chosen modularity enables component replacement and interpretability but may sacrifice joint optimization benefits
  - Rule-based vs. learned aggregation: Rule-based provides transparency but requires manual tuning; paper notes future work may explore learned fusion
  - Inference-only vs. fine-tuned: No task-specific training reduces deployment complexity but limits domain adaptation

- **Failure signatures:**
  - High false negatives on English content (recall 0.58 vs. Norwegian 0.85)—may indicate conservative threshold calibration
  - Deepfake detection near-random: Wvolf/ViT achieves 0.0 recall, MesoNet 0.019 recall; only EfficientNet usable (0.573 recall)
  - OCR failures on stylized fonts/rapid transitions—no quantitative metric reported

- **First 3 experiments:**
  1. Threshold sensitivity analysis: Vary the aggregation threshold to plot precision-recall tradeoffs; identify optimal operating point per language
  2. Deepfake module ablation with replacement: Swap EfficientNet for a newer detector (e.g., CLIP-based) and measure impact on checkworthy recall
  3. Cross-domain validation: Test on YouTube Shorts to assess generalization beyond TikTok; compare modality reliability patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced multimodal fusion techniques significantly outperform the current rule-based aggregation method in detecting checkworthy content?
- Basis in paper: [explicit] The authors state they aim to "explore advanced multimodal fusion techniques beyond rule-based aggregation to boost accuracy and generalization."
- Why unresolved: The current system relies on a transparent, configurable rule-based logic engine; it is unknown if learned fusion would better capture complex interactions between modalities.
- What evidence would resolve it: Comparative benchmarking showing statistically significant F1-score improvements using learned fusion on the existing TikTok datasets.

### Open Question 2
- Question: Does the pipeline maintain performance robustness when applied to short-form videos on platforms other than TikTok?
- Basis in paper: [explicit] The authors list "testing on more datasets, including multilingual video content from X and YouTube" as a primary goal for future work.
- Why unresolved: The system is currently validated only on TikTok data, which may possess unique platform-specific characteristics (e.g., aspect ratios, editing styles) not present elsewhere.
- What evidence would resolve it: Evaluation results on newly curated datasets from YouTube Shorts and X (Twitter) showing comparable F1-weighted scores (>70%).

### Open Question 3
- Question: Can visual feature modules (e.g., deepfake detection) be improved to provide significant standalone utility in the checkworthiness pipeline?
- Basis in paper: [inferred] Ablation studies showed that removing deepfake or weapon detection modules caused negligible performance drops, suggesting limited utility, yet they remain part of the architecture.
- Why unresolved: It is unclear if the low contribution is intrinsic to the checkworthiness task or a result of the specific zero-shot models (MesoNet, EfficientNet) failing to generalize to TikTok's noisy content.
- What evidence would resolve it: Improving visual module recall without sacrificing precision, resulting in a measurable positive contribution in subsequent ablation studies.

## Limitations
- The deepfake detection module shows particularly weak performance, with most models achieving near-random recall (0.0-0.019)
- Cross-lingual generalization is only validated on Norwegian and English datasets, with no evidence for the claimed 30+ language support
- The rule-based aggregation lacks systematic optimization and may be sensitive to threshold tuning

## Confidence
- **High confidence**: The modular pipeline architecture and its implementation details (OCR, Whisper, LLaVA, LLaMA 3 integration) are well-specified and reproducible.
- **Medium confidence**: The reported F1-weighted scores (>70%) and ablation study results are credible given the evaluation methodology, though the small dataset size warrants caution.
- **Medium confidence**: The claim that transcript and ideological language modules contribute most to performance is supported by ablation results, though the specific contribution magnitudes may vary with dataset composition.
- **Low confidence**: Claims about cross-lingual generalization to 30+ languages and deepfake detection utility lack empirical validation beyond the primary English/Norwegian datasets.

## Next Checks
1. **Threshold optimization study**: Systematically vary the aggregation threshold across the full precision-recall spectrum to identify optimal operating points for different stakeholder needs (fact-checkers vs. automated filtering).
2. **Cross-platform generalization test**: Evaluate SHORTCHECK on YouTube Shorts and Instagram Reels to assess modality reliability patterns and performance degradation outside TikTok's specific ecosystem.
3. **Deepfake module replacement experiment**: Replace the current deepfake detectors with a modern CLIP-based approach and measure impact on overall checkworthy recall and false positive rates.