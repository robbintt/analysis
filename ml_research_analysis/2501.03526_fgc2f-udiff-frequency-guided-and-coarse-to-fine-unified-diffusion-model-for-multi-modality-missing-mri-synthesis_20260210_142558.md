---
ver: rpa2
title: 'FgC2F-UDiff: Frequency-guided and Coarse-to-fine Unified Diffusion Model for
  Multi-modality Missing MRI Synthesis'
arxiv_id: '2501.03526'
source_url: https://arxiv.org/abs/2501.03526
tags:
- synthesis
- image
- diffusion
- images
- fgc2f-udiff
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of synthesizing missing MRI
  modalities in brain tumor imaging, where incomplete scans are common due to various
  limitations. The proposed method, Frequency-guided and Coarse-to-fine Unified Diffusion
  Model (FgC2F-UDiff), leverages diffusion models with a frequency-guided collaborative
  strategy and coarse-to-fine unified network to synthesize high-fidelity images from
  multiple inputs and outputs.
---

# FgC2F-UDiff: Frequency-guided and Coarse-to-fine Unified Diffusion Model for Multi-modality Missing MRI Synthesis

## Quick Facts
- arXiv ID: 2501.03526
- Source URL: https://arxiv.org/abs/2501.03526
- Reference count: 40
- Multi-modality MRI synthesis with PSNR 26.34 dB (BraTS T1,T2→T1ce) and 34.17 dB (IXI T1→PD)

## Executive Summary
This paper addresses the challenge of synthesizing missing MRI modalities in brain tumor imaging by proposing a frequency-guided and coarse-to-fine unified diffusion model (FgC2F-UDiff). The method leverages diffusion models with a novel frequency-guided collaborative strategy and coarse-to-fine unified network to synthesize high-fidelity images from multiple inputs and outputs. By dynamically incorporating low-frequency (global anatomical structure) and high-frequency (fine-grained details) information during different denoising stages, the model achieves superior performance compared to state-of-the-art methods on both BraTS 2021 and IXI datasets.

## Method Summary
FgC2F-UDiff is a UNet-based denoiser with 5-channel input (4 MRI modalities + frequency guidance) that performs coarse-to-fine denoising with T=200 timesteps split at T/2. The model uses low-frequency guidance via Gaussian low-pass filter (kernel size=21) in the coarse stage (T→T/2-1) and high-frequency guidance via Gaussian high-pass filter in the fine stage (T/2→0). Dynamic selection chooses optimal modalities for frequency guidance using left-to-right scan for low-frequency and right-to-left for high-frequency. The model is trained with curriculum learning from easy (1 missing) to hard (3 missing) tasks using Adam optimizer (lr=0.0001).

## Key Results
- BraTS 2021: PSNR of 26.34 dB and SSIM of 0.894 for T1,T2→T1ce synthesis
- IXI: PSNR of 34.17 dB and SSIM of 0.973 for T1→PD synthesis
- Inference time of 1.5-2 seconds per image on BraTS dataset
- Superior performance compared to state-of-the-art diffusion and GAN-based methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Segregating the denoising process into coarse and fine stages guided by corresponding frequency domains improves anatomical consistency and detail fidelity.
- **Mechanism:** The model partitions the reverse diffusion process (total steps T=200) into two phases. In the "Coarse" phase (T → T/2), the network is conditioned on Low-Frequency (LF) images (global structure) extracted via Gaussian Low Pass Filters (GLPF). In the "Fine" phase (T/2 → 0), it switches to High-Frequency (HF) conditioning (texture/edges) extracted via Gaussian High Pass Filters (GHPF).
- **Core assumption:** The iterative denoising nature of diffusion models follows a "global-to-detail" reconstruction pattern, which can be externally reinforced by frequency-specific priors.
- **Evidence anchors:**
  - [abstract]: "dynamically incorporates low-frequency (global anatomical structure) and high-frequency (fine-grained details) information during different denoising stages."
  - [Section III-B]: "divides the denoising process into two stages, coarse and fine... The coarse and fine denoising stages work collaboratively."
  - [Section V-D]: Ablation study "W/O CUN" shows a significant drop in PSNR (from 26.34 to 20.58), validating the necessity of the coarse-to-fine split.
  - [corpus]: Related work [Pattern-Aware Diffusion Synthesis] supports the general efficacy of incorporating tissue-specific refinement in diffusion models, though the specific coarse-to-fine split is unique to this paper.

### Mechanism 2
- **Claim:** Dynamic selection of source modalities for frequency guidance maximizes the information content available for synthesis.
- **Mechanism:** The Frequency-guided Collaborative Strategy (FCS) does not use a fixed source for guidance. Instead, it sorts available modalities based on heuristic frequency content (T1 > T2 > FLAIR > T1ce for low-freq; reverse for high-freq). It uses a "Left-to-Right" search to find the best available modality for LF and "Right-to-Left" for HF.
- **Core assumption:** Different MRI modalities inherently carry distinct distributions of frequency information (e.g., T1 is structure-heavy, T1ce is edge-heavy), and the "best" source is modalities-specific rather than task-specific.
- **Evidence anchors:**
  - [Section III-C]: "sorting the four modalities according to the amount of low-frequency... T1 > T2 > FLAIR > T1ce."
  - [Section V-D]: "Ours w/o dsf" (dynamic selection) results in lower performance (PSNR drop of ~0.47 dB) compared to the full method.
  - [corpus]: General consensus in [Benchmarking GANs, Diffusion Models...] suggests input contrast heavily influences output quality, supporting the need for intelligent source selection.

### Mechanism 3
- **Claim:** Curriculum Learning (CL) combined with a reduced time step schedule accelerates convergence and enables a feasible "many-to-many" unified training regime.
- **Mechanism:** The Specific-acceleration Hybrid Mechanism (SHM) trains the model using a curriculum: starting with "easy" tasks (1 modality missing) and progressing to "hard" tasks (3 modalities missing). It restricts diffusion steps to T=200 (vs. standard 1000) by leveraging the strong priors from the frequency guidance.
- **Core assumption:** Unified training benefits from a "warm-start" where the model learns simple mappings before complex ones, and frequency guidance reduces the denoising burden allowing for fewer steps.
- **Evidence anchors:**
  - [Section III-D]: "CL begins by emphasizing easier examples... designating the task of synthesizing one missing sequence as 'easy'."
  - [Section V-E]: Shows that with SHM, T=200 achieves better results than baseline LDM at T=800 or T=1000, proving acceleration without quality loss.
  - [corpus]: Weak support; few unified diffusion papers explicitly detail curriculum strategies for missing modalities.

## Foundational Learning

- **Concept:** **Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** This is the generative backbone. You must understand the forward process (adding Gaussian noise) and the reverse process (learning to denoise) to grasp how the Coarse-to-Fine Unified Network modifies the standard reverse loop.
  - **Quick check question:** If the model splits the denoising loop at step T/2, does it change the forward noise schedule? (Answer: No, only the reverse conditioning changes).

- **Concept:** **Frequency Domain Filtering (Low-pass vs. High-pass)**
  - **Why needed here:** The core innovation (FCS) relies on extracting structure vs. detail using Gaussian filters. Understanding how a GLPF blurs an image to reveal structure and a GHPF highlights edges is critical.
  - **Quick check question:** Why would a Gaussian filter be preferred over a simple box filter for extracting "global anatomical structure" for medical MRI?

- **Concept:** **Many-to-Many vs. Many-to-One Synthesis**
  - **Why needed here:** The paper critiques existing "task-specific" models. You need to understand that "Many-to-Many" means a *single* set of weights handles any permutation of inputs and outputs, rather than training separate models for "T1->T2" and "T1,T2->FLAIR".
  - **Quick check question:** In the input tensor of size R^{H × W × 5}, how does the model distinguish between an "available" modality and a "missing" one?

## Architecture Onboarding

- **Component map:** Input (5-channel Tensor) -> UNet Backbone -> Frequency-guided Collaborative Strategy (FCS) + Time Embedding -> Output (Synthesized MRI)

- **Critical path:**
  1. Input Prep: Identify missing/available modalities → Create binary mask logic → Apply FCS "Left-to-Right" / "Right-to-Left" search → Filter source images to generate the 5th channel
  2. Coarse Phase (T → T/2): Run reverse diffusion steps. Condition the UNet with the Low-Frequency channel
  3. Fine Phase (T/2 → 0): Switch the 5th channel to High-Frequency input. Continue reverse diffusion

- **Design tradeoffs:**
  - Unified Capability vs. Task-Specific Precision: A single unified model is more efficient and flexible than 12 separate models, but may sacrifice small amounts of accuracy for specific pairs compared to a dedicated model
  - Speed (T=200) vs. Quality: The paper claims T=200 is sufficient due to strong frequency priors. Increasing T to 1000 results in "over-diffusion" and slower inference without quality gain (see Section V-E)

- **Failure signatures:**
  - Modality Collapse: If the frequency guide is too weak, the model defaults to generating blurry, averaged outputs
  - Anatomical Hallucination: If the "Coarse" stage fails to align structure, the "Fine" stage may add high-frequency texture to the wrong anatomical location
  - Training Instability: Without Curriculum Learning (CL), the unified model may struggle to converge when 3 modalities are missing simultaneously

- **First 3 experiments:**
  1. Ablation on Frequency: Train without the 5th frequency channel (standard DDPM) to establish a baseline and confirm the "global-to-detail" hypothesis
  2. Time Step Analysis: Run inference with T=50, 100, 200, 500 to find the optimal speed/quality tradeoff and verify the "over-diffusion" claim
  3. Modality Robustness: Test the unified model by randomly dropping input modalities (e.g., feed only T1, then feed T1+T2) to ensure a single checkpoint handles all permutations gracefully

## Open Questions the Paper Calls Out
- Does the synthesized MRI quality from FgC2F-UDiff directly improve performance in downstream clinical tasks (e.g., tumor segmentation, diagnosis) compared to real or other synthetic modalities?
- Can the frequency-guided collaborative strategy (FCS), which uses fixed heuristics ("left-to-right", "right-to-left") to select frequency bands, be improved with a learnable or adaptive mechanism?
- Is the achieved inference time (1.5-2 seconds per image on BraTS) sufficient for clinical integration, and can it be further reduced without sacrificing synthesis fidelity?
- How does FgC2F-UDiff perform on multi-modal synthesis tasks beyond brain MRI, such as for other organs or with different imaging combinations?

## Limitations
- Limited evaluation on brain MRI datasets may not generalize to other anatomical regions with different texture-frequency characteristics
- Dynamic selection strategy relies on heuristics about modality frequency content that may not hold across all imaging conditions
- Claim of unified training efficiency lacks direct comparison against a properly tuned ensemble of task-specific models

## Confidence
- **High confidence:** The coarse-to-fine frequency guidance mechanism is well-supported by ablation studies and aligns with established diffusion model principles
- **Medium confidence:** The dynamic selection strategy shows measurable improvement but relies on heuristics about modality frequency content that may not hold across all imaging conditions
- **Medium confidence:** The curriculum learning acceleration demonstrates empirical speed gains but lacks comparison to other acceleration methods like DDIM sampling

## Next Checks
1. Test the model's ability to synthesize pathological regions (tumors, lesions) when the missing modality would normally show enhanced contrast not present in available inputs
2. Evaluate performance when input modalities contain varying levels of noise and artifacts to assess the robustness of the dynamic selection strategy
3. Compare the unified model's performance against an ensemble of individually trained, task-specific diffusion models to quantify the efficiency-quality tradeoff more precisely