---
ver: rpa2
title: Reducing Reward Dependence in RL Through Adaptive Confidence Discounting
arxiv_id: '2502.21181'
source_url: https://arxiv.org/abs/2502.21181
tags:
- reward
- agent
- rewards
- feedback
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of reducing dependence on expensive
  or human-provided rewards in reinforcement learning. The core method introduces
  an adaptive confidence discounting algorithm that uses entropy measures from both
  action and reward models to determine when to request explicit rewards from the
  environment.
---

# Reducing Reward Dependence in RL Through Adaptive Confidence Discounting

## Quick Facts
- **arXiv ID:** 2502.21181
- **Source URL:** https://arxiv.org/abs/2502.21181
- **Reference count:** 31
- **Primary result:** Achieves comparable performance to baseline RL methods while requiring as few as 20% of environment rewards through adaptive confidence-based reward requests

## Executive Summary
This paper addresses the challenge of reducing dependence on expensive or human-provided rewards in reinforcement learning by introducing an adaptive confidence discounting algorithm. The method uses entropy measures from both action and reward models to determine when to request explicit rewards from the environment, requesting rewards only when confidence in value predictions is low. Experiments across three domains demonstrate the approach achieves comparable performance to baseline methods while significantly reducing the number of environment rewards needed, with AE+RE (action and reward entropy) combined with hyperbolic regularization showing the most consistent improvements.

## Method Summary
The approach extends standard deep RL algorithms (DQN for discrete, A2C for continuous actions) with an adaptive reward request mechanism. It maintains separate learning and target reward models, along with dual replay buffers (all transitions vs. feedback-only transitions). Confidence is computed using entropy from both action and reward model outputs, with rewards requested only when confidence falls below a threshold (0.25). When confidence is high, the target reward model provides proxy rewards. The method includes regularization terms that decay confidence over time without explicit rewards, preventing excessive skipping. Four-layer fully-connected networks are used for all models with hyperparameters including ε=1→0.01 decay, α=0.005, δ=0.99, τ=0.99, batch=16, buffer=40K, and Adamax optimizer.

## Key Results
- AE+RE with hyperbolic regularization achieved best performance, reducing environment rewards to 116K-123K (vs. 226K baseline) while maintaining comparable scores
- Key-Lock domain: DQN baseline scored 996, AE-only scored 659 (stuck at local optima), AE+RE with regularization reached 850+
- Small reward models outperformed large ones due to overfitting concerns
- The approach showed consistent reward reduction across all three tested domains (grid-world, robotics, parking)

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Confidence Measurement
Low entropy in model output distributions correlates with high confidence in predictions, enabling selective reward requests. For discrete actions, entropy is computed as H(π(·|s)) = -Σ P(s,a) * log(P(s,a)) where P is softmax of Q-values. For continuous outputs, differential entropy of Gaussian distributions is used. Confidence = 1 - H, with action and reward entropies combined via harmonic mean, biasing toward lower confidence values. Core assumption: entropy serves as reliable heuristic for epistemic uncertainty. Break condition: when entropy poorly tracks actual uncertainty due to model being confidently wrong or high-entropy regions being well-understood through diverse experience.

### Mechanism 2: Reward Model as Proxy for Expensive Feedback
A learned reward function substitutes for environment rewards in high-confidence states without degrading policy quality. Dual reward models (learning and target) are maintained, with the learning model trained only on transitions with explicit rewards stored in a feedback buffer. When confidence exceeds threshold, environment reward is skipped and target reward model prediction is used instead. All transitions go to replay buffer; missing rewards are filled by target model during agent updates. Core assumption: reward function is learnable from sparse samples and generalizes to unqueried state-action pairs. Break condition: when reward function has complex non-local dependencies or requires state information not captured in training distribution.

### Mechanism 3: Confidence Regularization Prevents Reward Starvation
Decay-based regularization on confidence prevents excessive reward skipping and maintains learning stability. Confidence is multiplied by regularization term R(n) where n = steps since last environment reward. Exponential: e^(-νn) with ν=0.5 (steep decay). Hyperbolic: 1/(1+νn) with ν=1 (gentler decay, resembling human psychological discounting). After receiving a reward, n resets to 0. Core assumption: extended periods without ground-truth rewards lead to accumulated errors; interleaving proxy and real rewards acts as implicit curriculum. Break condition: when task requires dense feedback for credit assignment or when regularization is too weak (excessive skipping) or too strong (minimal reward reduction).

## Foundational Learning

**Concept: Shannon entropy and differential entropy**
- Why needed: The entire confidence mechanism rests on understanding entropy as uncertainty quantification
- Quick check: Given a distribution [0.9, 0.05, 0.05], is entropy higher or lower than [0.33, 0.33, 0.34]? Why does this map to confidence?

**Concept: Target networks and experience replay in deep RL**
- Why needed: The algorithm extends DQN/A2C architectures with dual reward models and dual buffers
- Quick check: Why does DQN use a separate target network, and how does this pattern transfer to the reward model?

**Concept: Gaussian distributions (mean, variance, NLL loss)**
- Why needed: Continuous action and reward models output Gaussian parameters; training uses negative log-likelihood
- Quick check: If a reward model outputs μ=5, σ=2 for state-action (s,a), how would you sample a reward and compute the entropy?

## Architecture Onboarding

**Component map:**
Agent model (DQN/A2C) -> Action selection -> Environment -> Next state + optional reward request -> Confidence computation -> (If Conf ≤ 0.25) Request environment reward -> Store in FB and RB -> (Always) Store in RB -> Train reward model on FB -> Fill RB missing rewards from target -> Train agent on RB -> End-of-episode: copy learning→target reward model

**Critical path:**
(1) Agent selects action → (2) Observe next state → (3) Compute confidence from both models → (4) If confidence ≤ threshold (0.25), request environment reward → (5) Store transition in FB if reward requested → (6) Always store in RB → (7) Train reward model on FB batch → (8) Fill missing RB rewards with target model → (9) Train agent on RB batch → (10) End-of-episode: copy learning→target reward model

**Design tradeoffs:**
- Smaller reward model → better generalization (large models overfit to early rewards, become overconfident)
- Harmonic mean vs. arithmetic → harmonic biases toward lower confidence (more conservative reward skipping)
- Hyperbolic vs. exponential regularization → hyperbolic gentler, more stable across domains

**Failure signatures:**
- AE (action entropy only) without regularization: stuck at local optima (Key-Lock score 659 vs. DQN 996)
- Large reward models: "fail to learn the task"—overfitting causes premature confidence
- High variance in non-regularized methods: large performance spread across runs

**First 3 experiments:**
1. Replicate DQN baseline on Key-Lock domain with full rewards; verify convergence and baseline score (~996)
2. Implement AE-only version (no reward model, no regularization); confirm it underperforms (gets stuck ~659) to validate entropy-as-confidence mechanism is incomplete alone
3. Add reward model + hyperbolic regularization; target ~116K-123K rewards (vs. 226K baseline) with comparable performance. Ablate by trying exponential regularization to observe steeper decay effects

## Open Questions the Paper Calls Out
- How do specific environment characteristics, such as the density of critical states, influence the spatial distribution of reward requests?
- Is the entropy-based confidence measure robust in highly stochastic environments where high entropy results from inherent noise rather than agent uncertainty?
- Can the confidence threshold and regularization temperature be adapted dynamically to eliminate the need for manual hyperparameter tuning?

## Limitations
- Results demonstrated on only three domains with specific reward structures; performance on tasks with highly discontinuous rewards or long-term credit assignment remains untested
- Method requires maintaining and updating separate reward models and buffers, increasing memory and computation without quantified overhead analysis
- Assumes entropy-based confidence is reliable proxy for epistemic uncertainty with limited empirical validation across different task complexities

## Confidence
**High Confidence** (supported by clear evidence):
- Adaptive confidence mechanism with entropy-based selection works as described in experimental domains
- Regularization is necessary to prevent reward starvation and maintain performance
- Dual-buffer architecture with target reward models functions correctly

**Medium Confidence** (reasonable but requires more validation):
- Specific hyperparameters (threshold=0.25, regularization parameters) generalize across different domains
- Harmonic mean combination of action and reward entropy is optimal
- Approach scales to more complex environments beyond the three tested

**Low Confidence** (limited evidence or significant unknowns):
- Method's effectiveness with alternative confidence metrics beyond entropy
- Performance guarantees when reward functions are highly non-stationary
- Transferability to domains with fundamentally different state/action spaces

## Next Checks
1. **Ablation on Confidence Metric**: Replace entropy-based confidence with an alternative uncertainty metric (e.g., dropout uncertainty, ensemble disagreement) while keeping all other components constant to test whether entropy is uniquely suited for this application

2. **Reward Function Complexity Stress Test**: Create a modified Key-Lock environment where rewards depend on multi-step state history or have complex non-local dependencies to measure whether the reward model can still provide accurate proxy rewards

3. **Computational Overhead Analysis**: Implement timing measurements to quantify additional computation and memory required by the dual-buffer architecture, comparing wall-clock training time and memory usage against baseline DQN across all three domains