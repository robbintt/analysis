---
ver: rpa2
title: 'Optimal Transport Learning: Balancing Value Optimization and Fairness in Individualized
  Treatment Rules'
arxiv_id: '2507.23349'
source_url: https://arxiv.org/abs/2507.23349
tags:
- function
- fairness
- decision
- optimal
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in individualized treatment rules
  (ITRs) by proposing a method to transform any optimal ITR into a fair one that ensures
  demographic parity. The core idea is to use optimal transport theory to map decision
  functions across sensitive attribute groups to their Wasserstein barycenter, achieving
  demographic parity.
---

# Optimal Transport Learning: Balancing Value Optimization and Fairness in Individualized Treatment Rules

## Quick Facts
- **arXiv ID:** 2507.23349
- **Source URL:** https://arxiv.org/abs/2507.23349
- **Reference count:** 7
- **Primary result:** Method transforms optimal ITRs into fair ones using Wasserstein barycenters while maintaining value through adjustable trade-off parameter

## Executive Summary
This paper addresses the challenge of learning fair individualized treatment rules (ITRs) that maximize expected outcomes while satisfying demographic parity constraints. The authors propose a post-processing framework that uses optimal transport theory to transform any optimal ITR into a fair one by mapping decision functions across sensitive attribute groups to their Wasserstein barycenter. The method introduces an "improved trade-off ITR" that balances value optimization and fairness through a self-adjusted weight function correlated with conditional average treatment effect (CATE). The approach is validated through simulations and applied to real-world entrepreneurial program admissions data, demonstrating the ability to achieve near-perfect fairness while maintaining reasonable value.

## Method Summary
The paper presents a two-stage approach to learning fair ITRs. First, base learners estimate an optimal decision function using outcome-weighted learning (OWL) and conditional average treatment effect (CATE) using R-learner. Second, the method applies optimal transport to map group-specific decision function distributions to their Wasserstein barycenter, achieving demographic parity. The "improved trade-off ITR" then combines the original optimal function with the fair function using a weight determined by the magnitude of CATE. The key innovation is the use of smoothed surrogate constraints to optimize the trade-off parameter while ensuring the hard demographic parity constraint is satisfied. The framework includes theoretical guarantees on value loss and is implemented through a combination of empirical CDF calculations and quantile transformations.

## Key Results
- The fair ITR achieves DI=1 (perfect demographic parity) while the improved trade-off ITR can achieve DI≥0.8 with minimal value loss
- Simulation studies demonstrate the method's effectiveness across different data-generating processes
- Application to Next 36 entrepreneurial program data shows the method can achieve fair admissions while maintaining reasonable value
- Theoretical upper bound on value loss is established and validated empirically
- The self-adjusted weight function successfully balances fairness and value optimization

## Why This Works (Mechanism)
The method leverages optimal transport theory to enforce demographic parity by mapping the distributions of decision functions across sensitive attribute groups to their Wasserstein barycenter. This ensures that the probability of receiving treatment becomes independent of the sensitive attribute. The improved trade-off ITR uses a weight function that increases with CATE magnitude, allowing the method to preserve more of the original optimal decision function when treatment effects are large (where fairness violations are less concerning) and shift toward the fair function when treatment effects are small (where fairness is more critical).

## Foundational Learning

**Optimal Transport**: Mathematical framework for comparing probability distributions through optimal coupling
- Why needed: Enables formal treatment of distributional similarity between groups for fairness enforcement
- Quick check: Can compute Wasserstein distance between two empirical distributions

**Wasserstein Barycenter**: Geometric mean of probability distributions under optimal transport geometry
- Why needed: Provides principled way to find "fair" distribution that minimizes transport cost to all groups
- Quick check: Can verify barycenter property by checking transport optimality

**Conditional Average Treatment Effect (CATE)**: Expected difference in outcomes between treatment arms conditional on covariates
- Why needed: Guides trade-off between fairness and value by identifying where treatment effects are large/small
- Quick check: Can estimate CATE consistently using appropriate causal inference methods

## Architecture Onboarding

**Component map**: Base learners (OWL + R-learner) -> Optimal Transport mapping -> Trade-off optimization -> Fair ITR

**Critical path**: The transport mapping is the core innovation - it transforms any optimal ITR into a fair one while providing theoretical guarantees on value preservation. The trade-off optimization fine-tunes the balance between fairness and value.

**Design tradeoffs**: Post-processing approach (vs in-processing) allows any optimal ITR to be made fair but may sacrifice some value. The smoothed constraint approach enables optimization but introduces a gap between surrogate and true constraints.

**Failure signatures**: 
- Transport mapping produces degenerate or highly concentrated distributions (indicates jittering parameter too small or groups too imbalanced)
- Trade-off parameter fails to satisfy hard DI constraint despite satisfying smoothed constraint (indicates surrogate gap)
- Value loss exceeds theoretical bound (indicates estimation error or violation of assumptions)

**First experiments**:
1. Verify barycenter mapping preserves marginal distributions across groups
2. Test trade-off parameter selection on simple synthetic data with known CATE structure
3. Compare hard vs smoothed DI constraint satisfaction on validation set

## Open Questions the Paper Calls Out
1. Can the proposed post-processing optimal transport framework be extended to multiple treatment settings (e.g., multi-arm trials)?
2. How can this fairness framework be effectively integrated into dynamic treatment regimes (DTRs) involving multiple decision stages?
3. Can the optimal transport method be modified to satisfy alternative fairness criteria, such as equalized odds, rather than strictly demographic parity?

## Limitations
- The method assumes sensitive attributes are categorical and may not extend directly to continuous attributes
- Implementation details for key hyperparameters (jittering parameter, optimization settings) are not fully specified
- The smoothed constraint approach introduces a gap between surrogate and true constraints that may affect practical performance
- Real-world application lacks comprehensive sensitivity analyses for different demographic compositions

## Confidence

**High confidence**: The theoretical framework for using Wasserstein barycenters to achieve demographic parity is mathematically sound and the proof of the upper bound on value loss follows logically from established optimal transport theory.

**Medium confidence**: Simulation results demonstrate the method's effectiveness under controlled conditions, though specific data-generating processes and parameter settings are not fully detailed, limiting reproducibility assessment.

**Low confidence**: The real-world application to the Next 36 program lacks detailed validation metrics beyond DI and value, and the absence of sensitivity analyses for different demographic compositions makes it difficult to assess robustness.

## Next Checks

1. Implement the complete pipeline: Reproduce the fair ITR and improved trade-off ITR on a benchmark dataset (e.g., ACIC or IHDP) using the described base learners and transport method, ensuring all hyperparameters are explicitly specified.

2. Validate surrogate relaxation gap: Compare the DI values obtained using the smoothed constraint H_{β,γ} versus the hard indicator function on a validation set to quantify the gap between surrogate and true constraints.

3. Sensitivity to group imbalance: Test the method's performance across datasets with varying levels of demographic imbalance to assess robustness and identify conditions under which the transport mapping becomes unstable or produces poor fairness guarantees.