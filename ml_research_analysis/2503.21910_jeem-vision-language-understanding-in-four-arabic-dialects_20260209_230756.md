---
ver: rpa2
title: 'JEEM: Vision-Language Understanding in Four Arabic Dialects'
arxiv_id: '2503.21910'
source_url: https://arxiv.org/abs/2503.21910
tags:
- image
- arabic
- caption
- evaluation
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We introduce JEEM, a benchmark designed to evaluate Vision-Language
  Models (VLMs) on visual understanding across four Arabic-speaking countries: Jordan,
  The Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning and
  visual question answering, and features culturally rich and regionally diverse content.'
---

# JEEM: Vision-Language Understanding in Four Arabic Dialects

## Quick Facts
- **arXiv ID**: 2503.21910
- **Source URL**: https://arxiv.org/abs/2503.21910
- **Reference count**: 33
- **Primary result**: JEEM is a benchmark evaluating VLMs on visual understanding across four Arabic dialects (Jordanian, Emirati, Egyptian, Moroccan) with image captioning and VQA tasks.

## Executive Summary
This paper introduces JEEM, a benchmark designed to evaluate Vision-Language Models on visual understanding across four Arabic-speaking countries. The dataset includes 2,178 images with captions in Modern Standard Arabic and regional dialects, plus 5 question-answer pairs per image. Through evaluation of five Arabic VLMs and GPT-4V, the authors find that Arabic VLMs consistently underperform, struggling with both visual understanding and dialect-specific generation. While GPT-4V ranks best, its performance varies across dialects and visual understanding remains a challenge. The work highlights the need for more culturally inclusive models and evaluation paradigms.

## Method Summary
The JEEM benchmark consists of 2,178 images (Jordan: 606, Emirates: 132, Egypt: 863, Morocco: 577) with captions in MSA and four dialects, plus 5 Q&A pairs per image. Five Arabic VLMs (Maya, PALO, Peacock, AIN, AyaV) and GPT-4o were evaluated on image captioning and VQA tasks. Traditional metrics (CIDEr, ROUGE, BLEU, BERTScore with CamelBERT) were used alongside GPT-4 evaluation on four criteria (Consistency, Relevance, Fluency, Dialect Authenticity) using both image+reference and reference-only settings. Human evaluation was conducted on a 200-image subset for correlation analysis.

## Key Results
- Arabic VLMs consistently underperform GPT-4V, struggling with both visual understanding and dialect-specific generation
- GPT-4V evaluation shows higher correlation with human judgments in reference-only setting (τc = 39.1) versus image+reference (τc = 34.7)
- Models struggle more with low-resource dialects like Emirati compared to high-resource variants like MSA and Egyptian
- Traditional metrics show particularly low correlation with human judgment (τc = 13.2-24.5) due to Arabic morphology

## Why This Works (Mechanism)

### Mechanism 1: Cultural-Dialectal Coupling in Visual Recognition
Visual object recognition and dialectal linguistic expression are coupled; accurate cultural identification depends on region-specific visual-linguistic grounding. When annotators from different regions view culturally-specific items (e.g., Omani Halwa), their cultural background determines both recognition and naming. Only Emirati annotators correctly identified Omani Halwa; Jordanian annotators called it "Karawya," while Egyptian and Moroccan annotators misidentified it as "chocolate dessert."

### Mechanism 2: Reference-Based LLM Evaluation Reduces Self-Evaluation Bias
GPT-4-based evaluation correlates better with human judgment when using reference-only evaluation versus image-and-reference combined, likely reducing self-evaluation bias. When GPT-4 evaluates GPT-4o outputs with access to the image, it may be biased toward its own visual interpretation. Reference-only evaluation forces comparison against ground truth without the model's visual "opinion."

### Mechanism 3: Dialect Resource Asymmetry Drives Performance Gaps
Performance on dialectal tasks is proportional to training data availability; low-resource dialects (Emirati) show larger gaps than higher-resource dialects (Egyptian, MSA). Arabic VLMs trained primarily on MSA or translated data lack sufficient dialectal exposure. GPT-4o scores 3.56 on dialect authenticity for Jordanian but only 2.20 for Emirati.

## Foundational Learning

- **Arabic Dialect Continuum (MSA vs. Dialects)**: JEEM evaluates four distinct dialects (Levantine, Egyptian, Gulf, Maghrebi); understanding that these differ in lexicon, phonetics, and syntax is essential for interpreting authenticity scores.
  - Quick check: Can you explain why "kandura" (Gulf) and "djellaba" (Egypt/Morocco) refer to similar garment categories but invoke different cultural contexts?

- **Vision-Language Model Cross-Modal Alignment**: The benchmark evaluates how well visual features map to dialectal linguistic outputs; failures indicate misalignment between vision encoder representations and language decoder outputs.
  - Quick check: If a VLM correctly identifies "a dessert" but uses MSA instead of the target dialect, is this a visual grounding failure or a linguistic generation failure?

- **Evaluation Metric Validity for Morphologically Rich Languages**: Standard metrics (BLEU, CIDEr) assume token-level overlap; Arabic's rich morphology and dialectal spelling variations invalidate this assumption (τc = 13.2-24.5 in Table 3).
  - Quick check: Why would BERTScore using CamelBERT (trained on MSA + dialects) still show low correlation with human judgment?

## Architecture Onboarding

- **Component map**: Vision encoder (e.g., CLIP ViT) -> Cross-modal projection -> Language decoder (Arabic-capable, dialect-conditioned) -> Caption/Answer
- **Critical path**: Image → Vision encoder → Cross-modal projection → Language decoder (dialect-conditioned) → Caption/Answer
- **Design tradeoffs**:
  - Shared dialect-agnostic encoder vs. dialect-specific adapters: Shared is parameter-efficient but may conflate dialect features; adapters preserve dialect specificity but increase complexity.
  - MSA pretraining + dialect fine-tuning vs. joint dialect training: Sequential risks catastrophic forgetting; joint requires balanced dialect data (currently unavailable).
  - Traditional vs. LLM-based evaluation: Traditional is fast but invalid (τc < 25); LLM-based is expensive but correlates better with humans (τc ≈ 39).
- **Failure signatures**:
  - Generic descriptions: Model outputs "a dessert" instead of "Omani halwa" (cultural grounding failure)
  - Dialect drift: Output starts in target dialect but reverts to MSA mid-sentence
  - Hallucinated cultural references: Model describes non-visible elements based on stereotypical regional associations
  - Fluent but irrelevant: High fluency scores (4.0+) but low relevance (2.0-3.0) indicate visual grounding failure
- **First 3 experiments**:
  1. Dialect probing: Run JEEM with dialect specified in prompt vs. implicit dialect detection; measure if explicit conditioning improves dialect authenticity scores.
  2. Error stratification: Classify failures into (a) visual recognition errors, (b) dialect generation errors, (c) both; this identifies whether to prioritize vision or language improvements.
  3. Reference-only vs. combined evaluation: Replicate the GPT-4 evaluation protocol on a held-out sample to validate whether reference-only evaluation consistently reduces self-bias across model families.

## Open Questions the Paper Calls Out
- **Question**: Does the reference-only evaluation setting consistently outperform the combined image-and-reference setting for GPT-4 based evaluation in dialect-rich contexts?
- **Question**: How does the performance of VLMs vary across a broader spectrum of Arabic dialects not included in the current benchmark?
- **Question**: What specific training interventions are required to improve visual grounding for culturally specific objects without sacrificing linguistic fluency?

## Limitations
- Benchmark covers only four dialects (Jordanian, Emirati, Egyptian, Moroccan), leaving out many other Arabic-speaking regions
- Unknown training data composition for evaluated VLMs makes it difficult to attribute performance gaps to specific causes
- GPT-4 evaluation protocol may introduce bias when evaluating GPT-4o outputs, though reference-only setting mitigates this partially

## Confidence

- **High Confidence**: Arabic VLMs underperform on both visual understanding and dialect generation tasks
- **Medium Confidence**: Cultural-dialectal coupling mechanism (supported by annotator disagreement on culturally-specific items)
- **Medium Confidence**: Reference-only evaluation reducing GPT-4 self-bias (based on one comparison showing higher correlation)
- **Low Confidence**: Exact contribution of training data imbalance versus architectural limitations to dialect performance gaps

## Next Checks

1. **Replication of Reference-Only Evaluation Effect**: Replicate the GPT-4 evaluation comparison (image+reference vs. reference-only) on a held-out subset of JEEM images using the exact prompts from Appendix C. Compute Kendall's Tau-C correlation for both settings across multiple model families.

2. **Dialect Conditioning Ablation Study**: Run JEEM with explicit dialect conditioning prompts versus implicit dialect detection. Compare dialect authenticity scores to determine if explicit conditioning improves performance, particularly for low-resource dialects like Emirati.

3. **Error Type Stratification Analysis**: Systematically classify model failures on JEEM into three categories: (a) visual recognition errors, (b) dialect generation errors, and (c) combined failures. This analysis would clarify whether improving visual grounding or dialect generation should be prioritized.