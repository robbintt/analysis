---
ver: rpa2
title: 'Uncertainty Quantification for Data-Driven Machine Learning Models in Nuclear
  Engineering Applications: Where We Are and What Do We Need?'
arxiv_id: '2503.17385'
source_url: https://arxiv.org/abs/2503.17385
tags:
- uni00000013
- uncertainty
- uni00000048
- data
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical need for uncertainty quantification
  (UQ) in machine learning models applied to nuclear engineering. The authors systematically
  categorize and compare uncertainty sources in both physics-based and data-driven
  models, then demonstrate five key UQ methods (Monte Carlo Dropout, Deep Ensemble,
  Bayesian Neural Networks, Gaussian Processes, and Conformal Prediction) through
  analytical and realistic SAFARI-1 reactor examples.
---

# Uncertainty Quantification for Data-Driven Machine Learning Models in Nuclear Engineering Applications: Where We Are and What Do We Need?

## Quick Facts
- arXiv ID: 2503.17385
- Source URL: https://arxiv.org/abs/2503.17385
- Reference count: 40
- Authors systematically compare five UQ methods for ML models in nuclear applications

## Executive Summary
This paper addresses the critical need for uncertainty quantification (UQ) in machine learning models applied to nuclear engineering. The authors systematically categorize and compare uncertainty sources in both physics-based and data-driven models, then demonstrate five key UQ methods (Monte Carlo Dropout, Deep Ensemble, Bayesian Neural Networks, Gaussian Processes, and Conformal Prediction) through analytical and realistic SAFARI-1 reactor examples. Deep Ensemble and Studentized Residual Conformal Prediction with XGBoost showed the best alignment with analytical solutions, while all methods performed well in predicting SAFARI-1 axial flux profiles. The study highlights that UQ is essential for building credibility in ML models for high-consequence nuclear systems and proposes a tentative VVUQ framework incorporating explainability, interpretability, reproducibility, and applicability metrics.

## Method Summary
The authors compared five uncertainty quantification methods - Monte Carlo Dropout, Deep Ensemble, Bayesian Neural Networks, Gaussian Processes, and Conformal Prediction - on both analytical benchmark problems and realistic SAFARI-1 reactor physics calculations. They first established an analytical problem with known solution to evaluate how well each method captures true uncertainty bounds. They then applied the methods to predict axial power distribution in a PWR reactor using a small dataset. For the reactor case, they used XGBoost with conformal prediction methods, comparing performance against diffusion theory solutions. The comparison included metrics like prediction interval coverage probability (PICP) and average prediction interval width (MPIW).

## Key Results
- Deep Ensemble and Studentized Residual Conformal Prediction with XGBoost provided the most accurate uncertainty quantification aligned with analytical solutions
- All five UQ methods successfully predicted axial flux profiles in SAFARI-1 reactor with comparable accuracy
- The study demonstrates ML models can provide reliable uncertainty estimates for nuclear engineering applications when properly quantified
- Five distinct sources of ML uncertainty were identified: data noise, data coverage, extrapolation, imperfect model, and training uncertainty

## Why This Works (Mechanism)
The success of uncertainty quantification methods stems from their ability to capture and represent the inherent stochasticity in both the training data and the learning process. Deep Ensemble methods work by training multiple models with different initializations, capturing model uncertainty through prediction variance. Conformal prediction methods provide distribution-free uncertainty bounds by calibrating prediction intervals on calibration data. Bayesian approaches incorporate prior knowledge and update beliefs through observed data. The combination of these methods with modern ML architectures like neural networks and gradient boosting allows for robust uncertainty estimation in complex nuclear engineering problems where analytical solutions may not exist.

## Foundational Learning
- **Uncertainty Quantification (UQ)**: Essential for assessing confidence in ML predictions, particularly in high-consequence domains like nuclear engineering. Needed to move ML from exploratory to decision-support tools.
- **Domain Generalization**: Critical for ensuring ML models perform reliably on unseen scenarios. Quick check: evaluate model performance on data from different operating conditions or reactor types.
- **VVUQ Framework**: Verification, Validation, and Uncertainty Quantification standards must be adapted for ML models, incorporating metrics for explainability and interpretability beyond traditional physics-based methods.
- **Conformal Prediction**: Distribution-free method for constructing prediction intervals that maintain coverage probability. Useful when true data distribution is unknown.
- **Deep Ensemble Methods**: Capture model uncertainty by training multiple models and aggregating predictions. More computationally expensive but often more reliable than single-model approaches.
- **Bayesian Neural Networks**: Incorporate uncertainty through probabilistic weights, providing both aleatoric and epistemic uncertainty estimates.

## Architecture Onboarding

**Component Map:**
Data → Preprocessing → ML Model → UQ Method → Prediction + Uncertainty Interval

**Critical Path:**
Model Training → Uncertainty Quantification → Validation against analytical/benchmark solutions

**Design Tradeoffs:**
- Computational cost vs. uncertainty accuracy (Deep Ensemble requires multiple model trainings)
- Model complexity vs. interpretability (complex neural networks vs. simpler models)
- Data requirements vs. reliability (more data enables better uncertainty estimation)

**Failure Signatures:**
- Under-coverage: Prediction intervals too narrow, failing to capture true values
- Over-coverage: Prediction intervals too wide, reducing practical utility
- Mode collapse: Ensemble methods converging to similar solutions, reducing diversity

**3 First Experiments:**
1. Implement Monte Carlo Dropout on analytical benchmark to verify uncertainty bounds match analytical solution
2. Train Deep Ensemble with varying ensemble sizes to assess trade-off between computational cost and uncertainty quality
3. Apply conformal prediction with different calibration set sizes to evaluate sensitivity to calibration data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a complete Verification, Validation, and Uncertainty Quantification (VVUQ) framework with quantifiable metrics be established for machine learning models in nuclear engineering?
- Basis in paper: [explicit] The authors state that the presented ML VVUQ framework is "tentative" and that a "more complete version with quantifiable metrics is under development."
- Why unresolved: Existing VVUQ standards for physics-based models do not fully address the data-driven nature, reproducibility, and interpretability requirements unique to ML algorithms.
- What evidence would resolve it: The publication of a standardized VVUQ framework defining specific metrics for ML explainability, interpretability, reproducibility, and applicability.

### Open Question 2
- Question: How can quantitative metrics be developed to evaluate ML applicability and domain generalization for unseen scenarios in advanced reactors?
- Basis in paper: [explicit] The text states, "Quantitative metrics need to be developed, for example, can be based on the domain generalization methods," specifically regarding ML applicability.
- Why unresolved: There is currently a lack of standards for evaluating the cost, improvement, and usability of ML models in new, unseen domains compared to traditional human processes.
- What evidence would resolve it: The formulation of specific metrics derived from domain generalization methods (e.g., data manipulation, representation learning) that quantify applicability.

### Open Question 3
- Question: How can the distinct sources of ML uncertainty (data noise, coverage, extrapolation, imperfect model, training) be disentangled to allow for independent quantification?
- Basis in paper: [inferred] The paper notes that unlike physics-based modeling, these sources "are usually not well-separated and independent from each other," making it "usually only possible to quantify them together."
- Why unresolved: Current UQ methods provide an aggregate uncertainty band but lack the capability to attribute specific portions of that uncertainty to individual sources like data noise versus architectural flaws.
- What evidence would resolve it: A methodology capable of decomposing the total prediction variance into components corresponding to the five defined sources.

## Limitations
- SAFARI-1 reactor case study represents only one reactor design and operating condition, limiting broader applicability claims
- Comparison focuses on five specific UQ methods without exploring hybrid approaches or newer techniques
- VVUQ framework remains preliminary and requires further development for practical implementation

## Confidence
- High: Methodological comparisons and performance metrics within tested scenarios
- Medium: Cross-method performance rankings across analytical and reactor problems
- Low: Claims about broader nuclear engineering applicability given limited case studies

## Next Checks
1. Test all five UQ methods across multiple reactor types and operating conditions to assess robustness
2. Implement and validate the proposed VVUQ framework with domain experts on additional nuclear applications
3. Benchmark against emerging UQ techniques and hybrid physics-informed ML approaches to identify potential improvements