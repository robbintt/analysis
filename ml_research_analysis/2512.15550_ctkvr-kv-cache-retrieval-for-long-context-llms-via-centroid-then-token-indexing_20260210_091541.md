---
ver: rpa2
title: 'CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing'
arxiv_id: '2512.15550'
source_url: https://arxiv.org/abs/2512.15550
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000048
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CTkvr, a novel centroid-then-token KV retrieval
  scheme for long-context LLM inference. The method addresses the trade-off between
  accuracy and efficiency in dynamic KV selection by leveraging the observation that
  adjacent query vectors exhibit high similarity after RoPE and share top-k KV entries.
---

# CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing

## Quick Facts
- **arXiv ID:** 2512.15550
- **Source URL:** https://arxiv.org/abs/2512.15550
- **Reference count:** 40
- **Primary result:** Sub-1% accuracy degradation while achieving 3-4× throughput speedup on long-context LLMs

## Executive Summary
CTkvr addresses the trade-off between accuracy and efficiency in long-context LLM inference by proposing a centroid-then-token KV retrieval scheme. The method exploits the observation that adjacent query vectors exhibit high similarity after RoPE and share top-k KV entries, enabling coarse-grained centroid indexing followed by fine-grained token refinement. CTkvr achieves less than 1% accuracy degradation while delivering 3× and 4× throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.

## Method Summary
CTkvr employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for coarse-grained indexing, followed by token-level refinement for precise KV retrieval. The system design offloads partial KV cache computation to the CPU and uses CUDA optimizations for maximum throughput. During prefilling, the last C query vectors are selected as centroids, and attention scores against the full Key cache are computed using max pooling over GQA groups. The top-ρ Key indices per centroid are stored in qcIVF. During decoding, cosine similarity between query and centroids selects top-C' centroids, which gather associated Keys for recall. These are then reranked to select top-ρ' Keys for sparse attention computation on CPU, while static attention is computed on GPU for local and initial keys.

## Key Results
- Sub-1% accuracy degradation versus full KV on RULER, LongBench, and Needle-in-a-Haystack benchmarks
- 3× and 4× throughput speedups on Llama-3-8B and Yi-9B at 96K context length
- Effective across diverse GPU hardware including A6000 and V100 configurations

## Why This Works (Mechanism)
CTkvr leverages the observation that query vectors adjacent in position exhibit high similarity after RoPE and share top-k KV entries. By precomputing centroids during prefilling, the method enables coarse-grained indexing that reduces the search space for KV retrieval. The two-stage approach (recall then rerank) balances efficiency and accuracy by first identifying relevant centroids and then performing fine-grained token-level refinement. CPU offloading of sparse attention computation further enhances throughput by alleviating GPU memory bottlenecks.

## Foundational Learning
- **Rotary Position Embedding (RoPE):** Why needed: CTkvr's core insight relies on query similarity after RoPE. Quick check: Verify query vectors at adjacent positions show cosine similarity >0.9 after RoPE transformation.
- **Group Query Attention (GQA):** Why needed: Enables sharing KV entries across multiple query heads, critical for the max pooling operation in centroid scoring. Quick check: Confirm max pooling over GQA groups produces different results than mean pooling.
- **KV Cache Management:** Why needed: Understanding static vs dynamic KV partitioning is essential for implementing the CPU offloading strategy. Quick check: Verify initial (64 tokens) and local (1024 tokens) KV remain on GPU while the rest is offloaded.
- **CUDA Memory Management:** Why needed: Custom kernel implementation for index deduplication is crucial for achieving reported throughput. Quick check: Profile PCIe bandwidth usage during sparse key transfer to ensure it doesn't saturate the bus.
- **Two-Stage Retrieval:** Why needed: The recall-then-rerank strategy balances efficiency and accuracy. Quick check: Experiment with different C' and ρ' values to find the optimal tradeoff point.
- **Asynchronous GPU-CPU Execution:** Why needed: Overlapping CPU attention computation with GPU operations is key to throughput gains. Quick check: Implement multi-streaming to overlap sparse attention on CPU with static attention on GPU.

## Architecture Onboarding

**Component Map:** Prefilling -> Centroid Selection -> qcIVF Construction -> Decoding -> Recall -> Rerank -> Sparse Attention (CPU) + Static Attention (GPU) -> Output Merge

**Critical Path:** The most performance-critical sequence is: query → centroid similarity computation → KV gathering → sparse attention (CPU) → static attention (GPU) → output merge. Any bottleneck in this path directly impacts throughput.

**Design Tradeoffs:** CTkvr trades a small amount of accuracy (<1%) for significant throughput gains (3-4×). The CPU offloading strategy shifts memory pressure from GPU to CPU, requiring careful management of PCIe bandwidth. The two-stage retrieval adds computational overhead but dramatically reduces the KV search space.

**Failure Signatures:** Accuracy degradation >1% typically indicates incorrect GQA handling (using mean instead of max pooling) or improper centroid selection (not using the last C queries). Throughput degradation suggests PCIe saturation during KV gathering or inefficient CPU-GPU overlap.

**Three First Experiments:**
1. Implement the centroid selection and qcIVF construction in pure PyTorch, verifying <1% accuracy degradation on RULER tasks
2. Profile PCIe bandwidth utilization during CPU offloading to identify potential bottlenecks
3. Benchmark standard PyTorch indexing versus custom CUDA kernel for index deduplication to quantify performance impact

## Open Questions the Paper Calls Out
- Does the centroid-based retrieval approach generalize to models utilizing positional encodings other than Rotary Position Embedding (RoPE)?
- How sensitive is the throughput performance to the bandwidth of the CPU-GPU interconnect when scaling batch sizes?
- Is the First-In-First-Out (FIFO) strategy sufficient for maintaining context in extremely long generative tasks where query drift is significant?

## Limitations
- Custom CUDA kernel for index deduplication is described but no source code is provided
- CPU offloading strategy lacks detailed implementation guidance for memory management
- Exact handling of GQA groups during centroid scoring is insufficiently specified

## Confidence
- **High Confidence:** Theoretical framework and two-stage retrieval strategy are well-founded
- **Medium Confidence:** Throughput improvements depend heavily on custom CUDA kernel implementation
- **Low Confidence:** CPU-GPU hybrid execution model implementation details are insufficiently described

## Next Checks
1. Implement centroid selection and qcIVF construction in PyTorch, verifying <1% accuracy degradation on RULER tasks
2. Measure PCIe bandwidth utilization during CPU offloading to verify sparse key transfer doesn't saturate the bus
3. Benchmark standard PyTorch indexing versus custom CUDA kernel for index deduplication and gathering step