---
ver: rpa2
title: A bag of tricks for real-time Mitotic Figure detection
arxiv_id: '2508.19804'
source_url: https://arxiv.org/abs/2508.19804
tags:
- detection
- training
- domain
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting mitotic figures
  in histopathology images across diverse domains, such as different scanners, staining
  protocols, tissue types, and artifacts. The core method employs a compact, real-time
  RTMDet-S single-stage object detector trained with a multi-domain dataset, balanced
  sampling, data augmentation, and targeted hard negative mining from necrotic and
  debris tissue.
---

# A bag of tricks for real-time Mitotic Figure detection

## Quick Facts
- arXiv ID: 2508.19804
- Source URL: https://arxiv.org/abs/2508.19804
- Reference count: 0
- Primary result: RTMDet-S achieves 0.81 F1 on MIDOG 2025 preliminary test set

## Executive Summary
This paper tackles the challenge of detecting mitotic figures across diverse histopathology domains, including different scanners, staining protocols, and tissue types. The authors propose a compact RTMDet-S single-stage detector trained with extensive multi-domain data, hierarchical balanced sampling, and targeted hard negative mining from necrotic and debris tissue. An ensemble of four EMA-weighted models is used for inference. The approach achieves strong F1 scores (0.78–0.84 in 5-fold cross-validation, 0.81 on MIDOG 2025 test set) while maintaining real-time performance, outperforming larger models.

## Method Summary
The method employs RTMDet-S, a compact single-stage anchor-based object detector, trained on six histopathology datasets combined into 112,923 patches at 1920×1280 resolution. Training uses a three-level hierarchical sampling strategy to balance representation across datasets, slides, and patches, along with a 1:1 mitotic figure to non-mitotic figure ratio. Hard negative mining targets necrotic and debris tissue to reduce false positives. The model is trained with AdamW, warm-up, cosine decay, Focal Loss, and IoU Loss, and uses EMA ensembling with three decay rates. Inference combines four EMA models with test-time augmentation.

## Key Results
- F1 scores of 0.78–0.84 in 5-fold cross-validation
- 0.81 F1 on MIDOG 2025 preliminary test set
- Outperforms larger models (Swin-L, ConvNeXt) on the same task
- Strong accuracy-speed trade-off suitable for clinical deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical balanced sampling improves cross-domain generalization by preventing gradient dominance from any single dataset, scanner, or tissue type.
- **Mechanism:** The three-level weighting scheme (dataset → slide → patch) forces equivalent learning signal from all domains during each mini-batch update. This prevents the model from overfitting to characteristics of larger or more annotation-dense datasets (e.g., MIDOG++ with 11,937 MFs vs. smaller canine datasets). The 1:1 MF:NMF patch ratio further ensures the classifier boundary isn't skewed by background majority class.
- **Core assumption:** Domain-invariant features exist and can be learned when no single domain dominates training.
- **Evidence anchors:**
  - [abstract] "addresses scanner variability and tumor heterogeneity via extensive multi-domain training data, balanced sampling"
  - [Methods] "Our ablations determined this strategy was important for achieving consistent performance across all domains"
  - [corpus] Neighbor papers (arXiv:2509.02637, 2509.02593) similarly emphasize multi-domain training for robustness but don't isolate sampling as explicitly
- **Break condition:** If domains share no predictive features (complete domain shift), balanced sampling won't help; if one domain is fundamentally out-of-distribution from test, equal weighting may dilute relevant signal.

### Mechanism 2
- **Claim:** Targeted hard negative mining from necrotic and debris tissue reduces false positives by explicitly training on confounders that visually mimic mitotic figures.
- **Mechanism:** Necrotic tissue contains fragmented nuclei and condensed chromatin that trigger false positives. By running inference on SPIDER/NCT-CRC-HE-100K with low threshold (0.25), capturing all candidate detections in necrotic/debris regions, and labeling these as NMF, the model learns to suppress predictions in these contexts. This is more efficient than random negative sampling because it focuses computational budget on decision boundary ambiguities.
- **Core assumption:** False positive patterns in necrotic/debris tissue are learnable and transfer across domains.
- **Evidence anchors:**
  - [Methods] "Incorporating these mined negatives proved particularly effective in reducing false positives on challenging test slides containing necrosis"
  - [Methods] "fragmented nuclei, condensed chromatin remnants, or irregular debris structures" identified as mitosis-like imposters
  - [corpus] No direct corpus evidence; neighbor papers don't explicitly address hard negative mining strategies
- **Break condition:** If necrotic tissue appearance varies drastically across scanners/stains in ways not captured in mining sources, learned suppression won't transfer.

### Mechanism 3
- **Claim:** EMA ensembling with multiple decay rates improves generalization by smoothing training dynamics and reducing run-to-run variance.
- **Mechanism:** EMA maintains exponentially-weighted averages of model parameters, acting as temporal regularization. Using three decay rates (1×10⁻³, 5×10⁻⁴, 25×10⁻⁵) creates models with different smoothing horizons—faster decay adapts more to recent training, slower decay provides longer-term averaging. Combining these at inference aggregates predictions from models that have "seen" different effective training trajectories.
- **Core assumption:** Parameter trajectories contain complementary information; averaging reduces noise without losing signal.
- **Evidence anchors:**
  - [Methods] "The use of EMA, especially with multiple decay settings, provided model smoothing which improved generalization and reduced variance between training runs"
  - [Results] EMA ensemble used for all reported F1 scores (0.78-0.84 cross-val, 0.81 test)
  - [corpus] No corpus evidence found for multi-decay EMA specifically in pathology detection
- **Break condition:** If training is unstable or diverges, EMA will average poor parameters; if decay rates are too similar, ensemble diversity diminishes.

## Foundational Learning

- **Concept: Domain shift in digital pathology**
  - Why needed here: The entire paper addresses performance degradation when models trained on one scanner/stain/tissue encounter different ones. Understanding that color histograms, resolution, and morphological appearance vary systematically across labs is prerequisite.
  - Quick check question: Can you explain why a model trained on Hamamatsu scanner images might fail on Leica-scanned slides of the same tissue?

- **Concept: Single-stage vs. two-stage object detection**
  - Why needed here: RTMDet is a one-stage anchor-based detector. Understanding the difference from R-CNN style two-stage approaches (region proposal → classification) explains why the authors could abandon their two-stage experiment.
  - Quick check question: What is the computational tradeoff between proposal-based and dense prediction detection architectures?

- **Concept: Class imbalance and focal loss**
  - Why needed here: Mitotic figures are rare (NMF >> MF). The paper uses Focal Loss (γ=2) specifically to down-weight easy negatives and focus learning on hard examples.
  - Quick check question: How does focal loss differ from standard cross-entropy in handling imbalanced datasets?

## Architecture Onboarding

- **Component map:** Input pipeline (1920×1280 patches at 40×) -> 3-level hierarchical sampler -> RTMDet-S backbone (COCO-pretrained) -> Focal Loss + IoU Loss -> AdamW optimizer with warm-up + cosine decay -> EMA tracking (3 decay rates) -> Hard negative mining (SPIDER/NCT-CRC-HE-100K, threshold 0.25) -> 4-model EMA ensemble + TTA

- **Critical path:**
  1. Dataset aggregation and unified annotation format (AI-Studio/EXACT)
  2. Hierarchical sampling weights computation (Eq. 1-4)
  3. Initial training → hard negative mining → re-training with augmented NMF pool
  4. EMA checkpoint selection at lowest validation loss
  5. Ensemble inference with TTA

- **Design tradeoffs:**
  - **Small model (RTMDet-S) vs. larger variants:** Larger models (M, L, X, Swin-L, ConvNeXt) showed no improvement—overfit despite regularization. Small model better bias-variance tradeoff for binary MF/NMF task.
  - **Mosaic augmentation:** Explicitly abandoned—"confused the model with unnatural juxtapositions of tissue"
  - **Stain normalization (Macenko):** Abandoned—negligible impact; HSV jitter sufficient
  - **Two-stage pipeline:** Abandoned—second classifier reduced F1, likely due to loss of tissue context

- **Failure signatures:**
  - High recall, low precision → insufficient hard negative mining; necrotic regions triggering false positives
  - High variance across folds → dataset imbalance not properly addressed; check sampling weights
  - Larger models matching or underperforming small model → overfitting; reduce capacity or increase regularization
  - Mosaic augmentation causing instability → tissue context disrupted; remove

- **First 3 experiments:**
  1. **Baseline replication:** Train RTMDet-S on single dataset (e.g., MIDOG++) without hierarchical sampling. Measure F1 gap vs. paper's 0.78-0.84 to quantify sampling contribution.
  2. **Hard negative ablation:** Train with/without necrotic/debris mining. Compare precision on held-out slides with known necrotic regions to isolate false positive reduction.
  3. **Ensemble decomposition:** Inference with single model vs. EMA ensemble vs. EMA+TTA. Measure variance across seeds and F1 delta to justify inference cost.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does a secondary classification stage fail to improve performance in this specific pipeline?
- **Basis in paper:** [explicit] The authors explicitly state, "The failure of the second-stage classifier in particular warrants further analysis."
- **Why unresolved:** The authors hypothesize that the classifier may lack necessary tissue context (e.g., surrounding necrosis), but they did not validate this experimentally.
- **What evidence would resolve it:** Ablation studies varying the crop size of the second-stage classifier to test if increased context restores performance gains.

### Open Question 2
- **Question:** Why do larger model architectures (e.g., Swin-L, ConvNeXt) underperform compared to the compact RTMDet-S?
- **Basis in paper:** [inferred] The paper notes that larger models "yielded F1 scores... slightly worse" and "appeared to overfit," suggesting a specific limitation in data diversity or volume for high-capacity models.
- **Why unresolved:** This contradicts the general deep learning trend where increased capacity improves feature extraction. The paper suggests the binary nature of the task (MF vs. NMF) might favor smaller architectures, but this remains unproven.
- **What evidence would resolve it:** A scaling law analysis comparing model size against validation loss on increasing amounts of training data to identify the data threshold required for larger models.

### Open Question 3
- **Question:** Can the proposed training pipeline transfer effectively to other histopathology detection tasks?
- **Basis in paper:** [explicit] The authors list "examining the applicability to other histopathology tasks" as an ongoing enhancement.
- **Why unresolved:** The "bag of tricks" (specifically the 3-level sampling and hard negative mining) is tailored to the extreme sparsity and domain shift of mitotic figures.
- **What evidence would resolve it:** Applying the identical RTMDet-S pipeline and sampling strategy to different tasks, such as lymphocyte detection or tissue segmentation, without architectural changes.

## Limitations

- Hard negative mining timing and source dataset composition are not precisely specified, creating potential variability in false positive suppression.
- Cross-validation fold assignments are not detailed, making it difficult to assess whether domain balance was maintained across splits.
- The choice of detection threshold for evaluation is unspecified, leaving performance metrics potentially non-reproducible.

## Confidence

- **High confidence** in domain generalization claims supported by multi-dataset training and balanced sampling strategy.
- **Medium confidence** in hard negative mining effectiveness due to lack of ablation showing isolated impact on false positive reduction.
- **Medium confidence** in EMA ensemble benefits given the absence of comparison to single-model baselines in the results.

## Next Checks

1. **Ablation study:** Train RTMDet-S with and without hierarchical sampling to quantify its contribution to cross-domain F1 scores.
2. **Hard negative mining analysis:** Compare precision on slides containing necrotic tissue with and without mined negatives to measure false positive suppression.
3. **Inference threshold sensitivity:** Evaluate model performance across multiple detection thresholds to identify optimal operating point for clinical deployment.