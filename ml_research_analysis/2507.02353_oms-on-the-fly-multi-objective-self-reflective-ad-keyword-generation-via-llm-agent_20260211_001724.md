---
ver: rpa2
title: 'OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via
  LLM Agent'
arxiv_id: '2507.02353'
source_url: https://arxiv.org/abs/2507.02353
tags:
- keyword
- keywords
- product
- cluster
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating effective advertising
  keywords for sponsored search by proposing an LLM-based framework that is on-the-fly,
  multi-objective, and self-reflective. It introduces agentic clustering-ranking to
  monitor and prioritize keywords using multiple performance metrics, and a multi-turn
  generation-reflection process to iteratively refine keywords based on quality feedback.
---

# OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent

## Quick Facts
- **arXiv ID:** 2507.02353
- **Source URL:** https://arxiv.org/abs/2507.02353
- **Authors:** Bowen Chen; Zhao Wang; Shingo Takamatsu
- **Reference count:** 35
- **Primary Result:** OMS outperforms existing methods in real-world ad campaigns, achieving higher clicks, conversions, and human preference ratings with lower cost per acquisition.

## Executive Summary
This paper introduces OMS, a novel LLM-based framework for generating effective advertising keywords in sponsored search campaigns. OMS is on-the-fly, multi-objective, and self-reflective, leveraging agentic clustering-ranking and a multi-turn generation-reflection process to iteratively refine keywords based on quality feedback. Experiments on benchmarks and real-world ad campaigns demonstrate that OMS significantly outperforms existing approaches, offering advertisers a powerful tool for optimizing keyword generation and campaign performance.

## Method Summary
OMS uses an agentic clustering-ranking approach to monitor and prioritize keywords using multiple performance metrics. It clusters active keywords via Affinity Propagation, generates intent summaries, and ranks keywords using TOPSIS with optional entropy-based weighting. The multi-turn generation-reflection process iteratively refines keywords using tool-return-value-guided generation (e.g., search volume checks, lexical analysis) and self-reflective quality control via LLM evaluation. Newly generated keywords are re-clustered and assigned to existing or new clusters, ensuring semantic coherence across rounds.

## Key Results
- Outperforms existing methods on benchmarks and real-world ad campaigns
- Achieves higher clicks, conversions, and human preference ratings
- Demonstrates lower cost per acquisition (CPA)

## Why This Works (Mechanism)

### Mechanism 1: Agentic Clustering-Ranking
Grouping keywords by semantic intent and scoring them via multi-objective ranking allows the system to prioritize high-impact keywords and identify expansion opportunities. The system clusters active keywords using Affinity Propagation, generates an intent summary per cluster via `LLM_Intent`, normalizes performance metrics (e.g., clicks, conversions), computes TOPSIS scores (with optional entropy-based weighting), and performs intra- and inter-cluster ranking to surface top- and under-performing clusters/keywords for generation focus.

### Mechanism 2: Tool-Return-Value-Guided Multi-Turn Generation
Constraining LLM generation with explicit tool outputs (search volume checks, lexical analysis, filters) improves validity and reduces platform rejections. After dynamic prompt construction (`LLM_Rank`), the agent invokes tools whose return values explicitly gate next steps: reject/replace low-volume or low-performance keywords (Search Volume Validator, Rejected/Repeated Keyword Filters), avoid problematic lexical patterns (Lexical Analysis), and prune high-rejection categories (Category Analysis), iterating until validated.

### Mechanism 3: Self-Reflective Quality Control and Re-Clustering
Explicit quality reflection and LLM-assisted re-clustering maintain keyword relevance and cluster coherence across rounds, reducing semantic drift. `LLM_Reflect` scores each candidate keyword (1–5) for product alignment and suggests keep/replace; the loop continues until the agent deems the set complete. Newly generated keywords are re-clustered by Affinity Propagation; each keyword's top-3 embedding-based clusters are compared by `LLM_Assign`, which selects the best cluster or creates a new one.

## Foundational Learning

**Concept: TOPSIS multi-attribute decision making**
- Why needed here: Provides a principled way to combine and rank keywords across multiple, possibly conflicting metrics (clicks, conversions, cost) into a single score.
- Quick check question: Given two keywords with normalized [click, conversion, cost] vectors [0.9, 0.5, 0.2] and [0.4, 0.8, 0.3], and weights [0.4, 0.4, 0.2], which is closer to the ideal [1,1,0] (minimizing cost)?

**Concept: ReAct-style LLM agents with tools**
- Why needed here: OMS uses a tool-return-value-guided flow rather than fully autonomous tool selection; understanding ReAct helps ground the interleaved reasoning–acting loop.
- Quick check question: In a ReAct loop, what should the model output after an "Action: Search" step?

**Concept: Affinity Propagation clustering**
- Why needed here: OMS uses AP to cluster keywords; it exchanges messages between points to find exemplars without pre-specifying cluster count.
- Quick check question: What is a key advantage of AP over k-means when the number of clusters is unknown?

## Architecture Onboarding

**Component map:**
Inputs (product info, historical keywords, performance vectors) → Agentic Clustering-Ranking (intent analysis → TOPSIS scoring → intra-/inter-cluster ranking) → Multi-Turn Generation-Reflection (dynamic prompting → tool-guided generation → reflection → re-clustering) → Outputs (validated keywords, updated clusters)

**Critical path:**
Accurate intent summaries → reliable TOPSIS ranking → tool validation → reflection acceptance → coherent re-clustering

**Design tradeoffs:**
- Reflection cost vs. quality: Using a stronger model (o3) for `LLM_Reflect` improves reasoning but increases latency/cost per generation round.
- Clustering granularity: Fine-grained clusters yield precise intents but may overfit; coarse clusters risk mixed intents.
- Static vs. adaptive weights: Predefined `W` is simple; entropy-based `W` adapts to metric variance but can be unstable with sparse data.

**Failure signatures:**
- High platform rejection rate → likely tool configuration issues (e.g., search volume threshold τ too strict).
- Semantic drift across rounds → reflection not filtering well or LAR not enforcing cluster coherence.
- Cluster collapse/explosion → AP hyperparameters or embedding model issues; may need to enforce minimum/maximum cluster sizes.

**First 3 experiments:**
1. Unit-test the TOPSIS and entropy weighting modules on synthetic keyword performance data with known ground-truth rankings.
2. Run a minimal version of the loop (intent analysis → TOPSIS → 1-shot generation → reflection) on 2–3 sample products and measure acceptance/rejection rates and Rouge/BERTScore.
3. Run a 5-round campaign simulation on a held-out subset of the benchmark, varying (a) reflection model strength, (b) with vs. without tool-guided validation, and log per-round metric trajectories and final CPA.

## Open Questions the Paper Calls Out

**Open Question 1**
Can OMS maintain its performance advantage against keyword generation methods fine-tuned on large-scale, proprietary query-keyword pairs? The authors state in the Limitations section that "most of those datasets are not released" and they were "not able to train such a model," leaving the comparison against fine-tuned baselines using private SSA data unresolved.

**Open Question 2**
How does OMS perform relative to diverse baselines in a multi-candidate (A/B/n) online deployment? The Limitations section notes, "we were not able to extend it into an A/B/n testing to compare multiple methods in a real-world campaign due to budget limitations."

**Open Question 3**
Is the multi-turn generation-reflection mechanism cost-efficient for low-budget advertising campaigns? While the paper minimizes the Cost Per Acquisition (CPA) of the ads, it notes in the Appendix that each generation costs ~$0.20–$0.50. Eq. 2 constrains the ad budget $B$ but does not explicitly model the operational cost of the LLM agent itself.

## Limitations
- Performance comparison against fine-tuned baselines using large-scale proprietary data is not possible due to lack of released datasets.
- Multi-candidate online deployment (A/B/n testing) was not feasible due to budget constraints.
- Cost-efficiency for low-budget campaigns is unclear as LLM operational costs are not explicitly modeled in the budget.

## Confidence
- **High Confidence**: The core mechanism of using multi-objective TOPSIS ranking to prioritize keywords is well-defined and mathematically grounded. The iterative tool-return-value-guided generation loop is explicitly described and reproducible.
- **Medium Confidence**: The effectiveness of the agentic clustering-ranking in improving keyword relevance is supported by ablation studies, but the exact clustering dynamics and intent summarization quality are not fully characterized. The reflection mechanism's consistency across products and rounds is plausible but not empirically validated in depth.
- **Low Confidence**: The robustness of the framework to API failures, extreme data sparsity, or adversarial keyword inputs is not addressed. The long-term stability of cluster coherence over many rounds is assumed but not tested.

## Next Checks
1. **TOPSIS and Entropy Weighting Validation**: Run the TOPSIS scoring module on synthetic keyword performance datasets with known ground-truth rankings to verify correctness and sensitivity to metric weights.
2. **Tool Reliability Audit**: Simulate tool API failures (e.g., rate limits, incorrect search volume estimates) and measure the agent's fallback behavior and keyword acceptance rates.
3. **Multi-Round Stability Test**: Execute a 10-round simulation on a held-out benchmark product, logging per-round cluster coherence (via embedding drift), reflection score distributions, and CPA trajectories to detect semantic drift or reflection bias.