---
ver: rpa2
title: 'Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs'
arxiv_id: '2602.02104'
source_url: https://arxiv.org/abs/2602.02104
tags:
- hebrew
- data
- training
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Dicta-LM 3.0, a suite of open-weight Hebrew
  language models trained on 100B Hebrew tokens mixed with 30B English tokens. The
  models (24B, 12B, 1.7B parameters) are adapted from Mistral-Small-3.1, NVIDIA Nemotron
  Nano V2, and Qwen3-1.7B respectively, with extended 65k context length.
---

# Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs

## Quick Facts
- arXiv ID: 2602.02104
- Source URL: https://arxiv.org/abs/2602.02104
- Reference count: 4
- Primary result: 24B-parameter model achieves 72.5% average on Hebrew benchmarks, outperforming 4x larger Llama-3.3-70B-Instruct (66.0%)

## Executive Summary
Dicta-LM 3.0 presents a comprehensive framework for adapting English foundation models to Hebrew through continuous pre-training and specialized post-training. The suite includes 24B, 12B, and 1.7B parameter models that achieve state-of-the-art performance on Hebrew benchmarks while retaining >98% of original English capabilities. Through a two-phase training approach with bilingual data mixing and differentiated post-training methods (DPO for instruct models, GRPO for thinking models), the models excel at both general Hebrew understanding and specific tasks like diacritization. The framework is designed to be generalizable to other low-resource languages beyond Hebrew.

## Method Summary
Dicta-LM 3.0 adapts Mistral-Small-3.1, NVIDIA Nemotron Nano V2, and Qwen3-1.7B models to Hebrew through a two-phase continuous pre-training process. Phase 1 trains on 130B tokens with 4k context using a 75/25 Hebrew/English mix to establish language representations while preventing catastrophic forgetting. Phase 2 extends context to 65k tokens on 18B tokens from long documents. Post-training includes SFT for both instruct and reasoning data, DPO for instruct variants using preference pairs, and GRPO for thinking variants using verifiable rewards on math, diacritization, and parsing tasks. The models use a 65k context window and are trained on 80× H200 GPUs.

## Key Results
- 24B model achieves 72.5% average on Hebrew benchmarks vs 66.0% for Llama-3.33-70B-Instruct
- English capability retention >98% across all model sizes
- Hebrew diacritization accuracy of 86.86% on unseen data
- Strong performance across multiple Hebrew-specific tasks including translation, sentiment analysis, and factual QA

## Why This Works (Mechanism)

### Mechanism 1: Bilingual Data Mixing for Knowledge Retention
Mixing 75% Hebrew with 25% English data during continuous pre-training enables Hebrew acquisition while preserving >98% of original English capabilities. English data acts as a regularization signal maintaining learned representations while Hebrew data builds language-specific pathways. The English corpus includes reasoning-dense data (Nemotron-CC, FineWeb-Edu) to transfer logical patterns to Hebrew inputs. Core assumption: Pre-trained English representations can transfer reasoning capabilities to Hebrew without explicit cross-lingual alignment. Evidence shows 24B model drops only 1.3% average on English benchmarks, 12B model gains 0.5%.

### Mechanism 2: Staged Context Extension
Two-phase training (4k → 65k context) enables stable long-context acquisition without destabilizing core representations. Phase 1 trains on all 130B tokens at 4k context to establish Hebrew representations. Phase 2 retrains on 20% of data at 65k context, sampling 75% from documents >6144 tokens to teach positional extrapolation. Core assumption: Short-context representations must stabilize before extending position embeddings; long documents provide signal for learning attention patterns at extended positions.

### Mechanism 3: Differentiated Post-Training by Output Style
GRPO for thinking models and DPO for instruct models optimizes for distinct behavioral objectives (reasoning quality vs. conversational alignment). GRPO uses verifiable rewards (math correctness, diacritization accuracy, UD parsing) with grouped comparisons to improve reasoning chains. DPO uses preference pairs for instruction-following and identity alignment. Core assumption: Thinking models benefit from objective reward signals on verifiable tasks; instruct models benefit more from subjective preference learning. Evidence shows thinking variant improves +25.6% on MATH, +24.2% on ZebraLogic vs instruct variant.

## Foundational Learning

- **Continuous Pre-training**: Why needed: The entire approach builds on initializing from existing models rather than training from scratch—understanding how pre-trained representations transfer is essential. Quick check: Can you explain why initializing from a pre-trained model requires lower learning rates (5e-6 to 1e-5) than from-scratch training?

- **Catastrophic Forgetting**: Why needed: The 25% English data mix explicitly targets this problem; without understanding it, you might under-appreciate the data ratio tradeoff. Quick check: What would happen to English benchmark scores if you trained on 100% Hebrew data for 130B tokens?

- **Group Relative Policy Optimization (GRPO)**: Why needed: This RL method differs from standard PPO/DPO by using grouped comparisons without a separate value function—critical for understanding the thinking model training. Quick check: How does GRPO's reward signal differ from DPO's preference signal? Why does GRPO require verifiable task outputs?

## Architecture Onboarding

- **Component map**:
  Base Model (Mistral-24B / Nemotron-12B / Qwen-1.7B)
       ↓
  Phase 1 CPT (4k ctx, 130B tokens, 75/25 Hebrew/English)
       ↓
  Phase 2 CPT (65k ctx, 18B tokens, long-doc sampling)
       ↓
  ┌────────────────────────────────────────────────────┐
  │                                                    │
  ↓                                                    ↓
  SFT (Instruct data)                        SFT (Thinking data)
       ↓                                           ↓
  DPO (preference pairs)                    GRPO (verifiable rewards)
       ↓                                           ↓
  Instruct Model                            Thinking Model

- **Critical path**: Phase 1 CPT → Phase 2 CPT → SFT is the minimum viable pipeline. DPO and GRPO are refinements that can be skipped for initial experiments.

- **Design tradeoffs**:
  - English ratio: Higher = better retention, slower Hebrew acquisition. Paper uses 25%; validate this for your target language.
  - Context extension strategy: Two-phase vs. single-phase. Paper shows two-phase works, but adds complexity.
  - Post-training split: DPO-only (simpler) vs. DPO+GRPO by variant (better results, more infrastructure).

- **Failure signatures**:
  - Hebrew benchmark scores <50% of comparable models → Check Hebrew data quality and tokenizer coverage
  - English scores drop >10% → Increase English ratio or reduce CPT learning rate
  - GRPO reward plateaus early → Verify reward functions are properly normalized and tasks are difficulty-appropriate
  - Thinking model generates excessive tokens without accuracy gain → Check KL penalty (paper uses 0.01)

- **First 3 experiments**:
  1. Validate tokenizer efficiency: Compare Hebrew token-to-character ratio against the base model. If Hebrew words split into many sub-tokens, consider vocabulary extension or a different base model.
  2. Ablate English ratio: Train small-scale (10B tokens) variants with 15%, 25%, 40% English. Measure Hebrew acquisition vs. English retention tradeoff.
  3. Single-task GRPO validation: Before full GRPO training, verify reward functions on one task (e.g., math verification) with a small model to confirm pipeline correctness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed continuous pre-training framework generalize effectively to other low-resource languages beyond Hebrew?
- Basis in paper: [explicit] The authors state their work "proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP."
- Why unresolved: The paper only validates the methodology on Hebrew; no experiments on other low-resource languages are presented.
- What evidence would resolve it: Application of the same training pipeline (100B target-language tokens with 30B English tokens, two-phase CPT, GRPO) to languages with similar resource constraints (e.g., Arabic dialects, Finnish, Vietnamese) with comparable benchmark improvements.

### Open Question 2
- Question: What causes the inconsistent English capability retention across model sizes, particularly the 1.7B model's +11.8% gain on Arc-Challenge alongside a -4.2% drop on CommonsenseQA?
- Basis in paper: [inferred] Table 4 shows non-uniform English retention: the 1.7B model improves dramatically on Arc-Challenge but declines on other benchmarks, unlike the more stable 24B and 12B models.
- Why unresolved: The paper reports >98% retention as an average but does not investigate why certain capabilities improve while others degrade in smaller models.
- What evidence would resolve it: Ablation studies isolating the contribution of English data composition (SlimPajama vs. Nemotron-CC) and analysis of which English capabilities transfer versus interfere with Hebrew acquisition.

### Open Question 3
- Question: How does the 75/25 Hebrew-to-English pre-training ratio compare to alternative mixtures for balancing Hebrew acquisition against English retention?
- Basis in paper: [inferred] The paper selects a 75% Hebrew / 25% English split empirically but provides no comparison to other ratios (e.g., 50/50 or 90/10) or analysis of whether this is optimal.
- Why unresolved: The chosen ratio appears to be a design decision without systematic exploration of the tradeoff curve between target-language performance and source-language preservation.
- What evidence would resolve it: Controlled experiments varying the Hebrew-English ratio while holding total tokens constant, measuring both Hebrew benchmark performance and English capability retention across multiple model sizes.

## Limitations

- Data availability constraints: Exceptional performance relies on proprietary Hebrew data sources that are not publicly available, creating uncertainty about faithful reproduction outside Dicta lab environment.
- Limited cross-lingual validation: Claims about universal reasoning pattern transfer across languages are based on English-Hebrew experiments only, without validation on other language pairs.
- Two-phase context extension assumptions: The methodology assumes short-context representations must stabilize before extending position embeddings, which may not generalize to all model families.

## Confidence

- **High Confidence**: English retention claims (>98% benchmark scores maintained), basic continuous pre-training methodology, tokenizer efficiency measurements, and core benchmark results (Hebrew task scores, comparative performance against Llama-3.3-70B-Instruct).
- **Medium Confidence**: The bilingual data mixing mechanism (75/25 ratio optimization), context extension effectiveness, and post-training method selection rationale.
- **Low Confidence**: Claims about universal reasoning pattern transfer across languages, optimal data ratio determination, and the necessity of two-phase context extension.

## Next Checks

1. **Tokenizer Efficiency Validation**: Measure tokens-per-character ratio for Hebrew text across different base models (Mistral, Nemotron, Qwen) to identify which provides optimal tokenization for Hebrew. Compare against reported 1.05 ratio to validate whether tokenizer choice significantly impacts downstream performance.

2. **English Ratio Ablation Study**: Conduct controlled experiments varying English data ratio (15%, 25%, 40%) on a smaller model scale (e.g., 7B parameters) with 10B training tokens each. Measure both Hebrew acquisition rate (benchmark improvement per token) and English retention (benchmark drop from baseline).

3. **Cross-Lingual Transfer Mechanism Testing**: Design experiments to test whether English reasoning patterns truly transfer to Hebrew or if the model is developing separate reasoning capabilities. This could involve comparing performance on bilingual reasoning tasks, analyzing attention patterns across languages, or testing zero-shot transfer to other languages using the same 75/25 mixing strategy.