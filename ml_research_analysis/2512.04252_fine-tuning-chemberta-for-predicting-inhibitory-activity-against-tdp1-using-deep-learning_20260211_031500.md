---
ver: rpa2
title: Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using
  Deep Learning
arxiv_id: '2512.04252'
source_url: https://arxiv.org/abs/2512.04252
tags:
- data
- tdp1
- learning
- dataset
- compounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a deep learning framework for predicting TDP1
  inhibitory potency from SMILES strings using fine-tuned ChemBERTa models. Leveraging
  a large-scale dataset of 177,092 compounds, the study systematically compares Masked
  Language Modeling (MLM) and Masked Token Regression (MTR) pretraining strategies,
  along with data handling approaches like stratified oversampling and sample weighting.
---

# Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning

## Quick Facts
- arXiv ID: 2512.04252
- Source URL: https://arxiv.org/abs/2512.04252
- Reference count: 0
- Primary result: ChemBERTa-MTR model achieves EF@1%=17.4 and Precision@1%=37.4 for TDP1 inhibitor prediction

## Executive Summary
This work presents a deep learning framework for predicting TDP1 inhibitory potency from SMILES strings using fine-tuned ChemBERTa models. Leveraging a large-scale dataset of 177,092 compounds, the study systematically compares Masked Language Modeling (MLM) and Masked Token Regression (MTR) pretraining strategies, along with data handling approaches like stratified oversampling and sample weighting. The results show that sample weighting outperforms oversampling, and ChemBERTa-MTR with optimized hyperparameters achieves an Enrichment Factor at 1% (EF@1%) of 17.4 and Precision@1% of 37.4. While classical Random Forest models perform slightly better, the ChemBERTa-MTR model provides a robust, scalable, and 3D-structure-free tool for virtual screening and drug discovery. The study highlights the value of chemical transformers in target-specific inhibitor prediction and offers a practical pipeline for experimental validation.

## Method Summary
The method fine-tunes a 77M parameter ChemBERTa-MTR model (RoBERTa-based) for regression of pIC50 values against TDP1 using a curated dataset of 177,092 SMILES-pIC50 pairs. The model replaces the pretraining head with a regression head and trains using weighted mean squared error loss with sample weights inversely proportional to class frequency. Key hyperparameters (learning rate 4.61e-5, batch size 32, dropout 0.100) were optimized using Optuna. The data split (70/15/15) maintains the 2.1% active ratio through stratified splitting, and sample weighting (weights ~23.3 for active, ~0.51 for inactive) outperforms oversampling.

## Key Results
- ChemBERTa-MTR achieves EF@1%=17.4 and Precision@1%=37.4 for TDP1 inhibitor prediction
- Sample weighting strategy consistently outperforms oversampling for virtual screening in imbalanced datasets
- Random Forest models with hand-crafted features slightly outperform ChemBERTa-MTR (EF@1%=21.5 vs 17.4)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Masked Token Regression (MTR) pretraining confers superior regression performance compared to Masked Language Modeling (MLM) for continuous bioactivity prediction.
- **Mechanism:** Unlike MLM, which learns syntactic reconstruction of SMILES strings, MTR explicitly pre-trains the model to predict continuous molecular properties (e.g., logP, molecular weight) alongside token reconstruction. This creates a latent space where features are already aligned with quantitative physical properties, reducing the gap between pretraining and the downstream pIC50 regression task.
- **Core assumption:** The chemical features required to predict simple physical properties (logP, MW) share a causal or correlational subspace with complex biological interactions (TDP1 inhibition).
- **Evidence anchors:** [section 2.3] "MTR... explicitly aligns with downstream regression tasks, making MTR a theoretically superior starting point... compared to the reconstruction-focused MLM."

### Mechanism 2
- **Claim:** Sample weighting outperforms oversampling for virtual screening in imbalanced datasets.
- **Mechanism:** Oversampling (duplicating active compounds) artificially flattens the activity landscape, potentially causing the model to overfit to specific active scaffolds while distorting the learned decision boundary. Sample weighting preserves the original, "natural" distribution of the data (97.9% inactive) during forward passes but scales the gradient updates for rare active samples (weight â‰ˆ23.3). This prevents distribution shift while ensuring the loss function penalizes errors on active compounds heavily.
- **Core assumption:** Preserving the "true" density of the chemical space in the training data helps the model generalize better to the sparse high-potency regions than presenting a falsified balanced view.
- **Evidence anchors:** [section 5.2] "sample weighting consistently outperforms oversampling... attributed to the preservation of the true pIC50 distribution."

### Mechanism 3
- **Claim:** Deep representation learning from SMILES eliminates the bottleneck of manual feature engineering required for classical models.
- **Mechanism:** Classical Random Forest models rely on hand-crafted fingerprints (e.g., Morgan fingerprints) which explicitly encode predefined substructures. ChemBERTa uses self-attention to learn contextualized embeddings directly from the SMILES sequence. This allows the model to potentially identify novel motifs or long-range dependencies in the string representation that are not explicitly coded in standard circular fingerprints.
- **Core assumption:** The SMILES string contains sufficient information entropy to recover the 3D structural and electronic features necessary for binding affinity, and the attention mechanism can effectively capture this.
- **Evidence anchors:** [section 1] "eliminating the need for manual feature engineering or 3D structural data."

## Foundational Learning

- **Concept: Chemical Language Models (CLMs) & Tokenization**
  - **Why needed here:** The architecture replaces graphs with sequences. You must understand that SMILES strings are tokenized (broken into characters/substrings) and fed into a Transformer (RoBERTa) just like natural language.
  - **Quick check question:** How does the model handle a SMILES string like "CCO"? (Answer: It tokenizes it, likely into ['C', 'C', 'O'], adds positional embeddings, and processes the sequence of vectors).

- **Concept: Evaluation Metrics for Imbalance (EF@1% vs. RMSE)**
  - **Why needed here:** The dataset is 97.9% inactive. A model could achieve low RMSE by predicting "inactive" for everyone and still fail at drug discovery.
  - **Quick check question:** Why is EF@1% (Enrichment Factor) prioritized over Mean Absolute Error (MAE) in this study? (Answer: EF@1% measures how many active compounds are found in the top 1% of predictions, which directly correlates to cost-saving in experimental screening).

- **Concept: Fine-Tuning vs. Feature Extraction**
  - **Why needed here:** The paper compares "Fine-tuned ChemBERTa" (updating all weights) against Random Forest on fixed features.
  - **Quick check question:** In the ChemBERTa-MTR setup, are the pre-trained weights frozen during training? (Answer: No, they are fine-tuned using a weighted MSE loss to adapt the general representations to the specific TDP1 task).

## Architecture Onboarding

- **Component map:** Raw SMILES strings -> Tokenizer (converts to integer IDs) -> ChemBERTa (RoBERTa architecture, 77M params) -> Regression Head (Linear layer outputting a single scalar value) -> Weighted MSE Loss

- **Critical path:**
  1. Data Curation: Merge PubChem/Zenodo datasets -> Deduplication -> pIC50 normalization
  2. Splitting: Stratified split (70/15/15) to maintain the 2.1% active ratio in all sets
  3. Training: Load ChemBERTa-MTR -> Configure Sample Weighting in Loss -> Fine-tune
  4. Optimization: Run Optuna to tune Learning Rate, Batch Size, Dropout

- **Design tradeoffs:**
  - ChemBERTa vs. Random Forest: RF is currently superior (EF@1% 21.5 vs 17.4) and faster. ChemBERTa is chosen for scalability and "end-to-end" integration, avoiding manual feature engineering. *Do not use ChemBERTa if latency or maximum absolute accuracy is the only constraint.*
  - MTR vs. MLM: MTR is computationally distinct but functionally better for regression. *Use MLM only if pre-training data for MTR is unavailable or if the task is classification.*

- **Failure signatures:**
  - Mode Collapse: The model predicts the mean pIC50 for all inputs. (Check: Validation MAE is low, but EF@1% is near 1.0)
  - Overfitting to Actives (Oversampling artifact): High recall on training actives, but poor generalization to test set actives (synthetic minority over-sampling creates unrealistic decision boundaries)
  - Catastrophic Forgetting: If learning rate is too high during fine-tuning, the model loses the chemical semantics learned during pre-training (validation loss diverges rapidly)

- **First 3 experiments:**
  1. Establish Baseline: Train a Random Forest on Morgan Fingerprints (RDKit) to set a performance ceiling (Target: EF@1% ~21.5)
  2. Data Strategy Ablation: Fine-tune ChemBERTa-MTR with default hyperparameters using (a) Oversampling and (b) Sample Weighting. Verify that Sample Weighting yields higher EF@1%
  3. Hyperparameter Sweep: Using the best strategy from step 2, run Optuna (20 trials) minimizing "Active MAE" to find the optimal learning rate/dropout combination

## Open Questions the Paper Calls Out

- **Incorporating uncertainty quantification to assess prediction reliability**
- **Integrating the model with generative chemistry approaches**
- **Evaluating the model's performance on other protein targets beyond TDP1**

## Limitations

- The study demonstrates strong early enrichment but does not validate whether predicted top-ranked compounds would translate to experimental success
- The superiority of MTR pretraining over MLM is theoretically justified but not empirically validated within the paper itself
- Sample weighting may not generalize to datasets with different class imbalance ratios or chemical space distributions

## Confidence

- **High Confidence:** Sample weighting strategy outperforming oversampling, EF@1% and Precision@1% metrics as appropriate evaluation measures
- **Medium Confidence:** MTR pretraining superiority for regression tasks, ChemBERTa providing a practical pipeline for experimental validation
- **Low Confidence:** Generalizability to other protein targets beyond TDP1, claim that model eliminates need for 3D structural data

## Next Checks

1. **Direct MLM vs. MTR Comparison:** Implement and train both ChemBERTa-MLM and ChemBERTa-MTR models on the TDP1 dataset with identical hyperparameters to empirically validate which pretraining strategy yields superior regression performance

2. **Experimental Validation:** Select the top 20-50 compounds predicted by the ChemBERTa-MTR model, synthesize or obtain them, and perform wet-lab TDP1 inhibition assays to verify if the high EF@1% translates to actual hit rates

3. **Cross-Target Generalization:** Apply the trained TDP1 ChemBERTa-MTR model to predict inhibitory activity for a structurally similar target (e.g., TDP2 or another topoisomerase) to assess whether the learned representations transfer or are target-specific