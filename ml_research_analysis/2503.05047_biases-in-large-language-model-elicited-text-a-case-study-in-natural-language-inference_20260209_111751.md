---
ver: rpa2
title: 'Biases in Large Language Model-Elicited Text: A Case Study in Natural Language
  Inference'
arxiv_id: '2503.05047'
source_url: https://arxiv.org/abs/2503.05047
tags:
- snli
- gpt-4
- mistral
- llama
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) generate
  NLP datasets with biases similar to those found in human-elicited data. Researchers
  recreated portions of the Stanford Natural Language Inference (SNLI) corpus using
  GPT-4, Llama-2 70b, and Mistral 7b, following the same instructions given to human
  annotators.
---

# Biases in Large Language Model-Elicited Text: A Case Study in Natural Language Inference

## Quick Facts
- arXiv ID: 2503.05047
- Source URL: https://arxiv.org/abs/2503.05047
- Reference count: 23
- Primary result: LLM-generated NLI datasets contain similar annotation artifacts and social biases as human-elicited data

## Executive Summary
This study investigates whether large language models reproduce the biases found in human-elicited NLP datasets by generating NLI hypotheses using GPT-4, Llama-2 70b, and Mistral 7b with instructions matching the original SNLI corpus. Researchers trained hypothesis-only classifiers and analyzed word associations with social groups, finding that LLM datasets achieved 86-96% accuracy on hypothesis-only classifiers compared to 72% on SNLI. The generated datasets exhibited similar annotation artifacts (give-away words) and gender stereotypes (male→work/violence, female→domesticity/appearance) as human-elicited data. These findings suggest LLM-generated datasets require thorough quality control to address inherent biases.

## Method Summary
The researchers recreated portions of the SNLI corpus by prompting three LLMs (GPT-4, Llama-2 70b Chat, Mistral 7b Instruct) with instructions matching those given to human annotators. They used 1/3 of SNLI training premises and all evaluation premises, generating hypothesis triples at temperature 0.75 with top-p 0.9. Label quality was validated through manual review of 300 samples per model, excluding PaLM 2 due to <80% agreement. They trained hypothesis-only classifiers (Naive Bayes and BERT) and computed PMI for demographic term associations, comparing results against the original SNLI dataset.

## Key Results
- Hypothesis-only classifiers achieved 86-96% accuracy on LLM-generated datasets versus 72% on SNLI, indicating strong annotation artifacts
- LLM-generated datasets contained similar gender stereotypes: male-associated terms linked to work and violence, female-associated terms to domesticity and appearance
- Cross-model classifier transfer showed shared artifact structures (SNLI-trained models performed better on GPT-4 eval than SNLI eval)
- Generated hypotheses mentioned racial/ethnicity words much less often than SNLI
- Capitalized "There" appeared as a give-away word for entailment, tracing to prompt formatting

## Why This Works (Mechanism)

### Mechanism 1: Lexical Shortcut Detection
Hypothesis-only classifiers detect annotation artifacts by exploiting statistically predictable "give-away words" in generated text that strongly correlate with labels. When models generate NLI hypotheses, they produce predictable patterns (e.g., "sleeping" → contradiction, "person/outdoors" → entailment) that classifiers learn without needing premises. This indicates dataset artifacts rather than genuine inference patterns.

### Mechanism 2: Statistical Pattern Completion
LLMs reproduce stereotypical associations from training data through PMI-based word co-occurrence patterns. Models trained on web-scale text learn gender-role stereotypes and reproduce them when generating hypotheses, even without explicit demographic prompts. PMI captures how much more often two words co-occur than chance would predict, revealing underlying biases.

### Mechanism 3: In-Context Format Transfer
In-context learning induces format-specific artifacts that transfer across LLM outputs. Models copy structural patterns from prompt examples, with artifacts persisting across generation instances when prompts share structure. Cross-model transfer performance indicates shared artifact vocabulary stemming from common prompt formatting.

## Foundational Learning

- **Natural Language Inference (NLI)**: Core task requiring determining if hypothesis follows from premise. Essential for understanding how artifacts subvert genuine inference tasks. *Quick check: Given premise "A dog runs" and hypothesis "An animal moves," what label applies and why?*

- **Annotation Artifacts**: Dataset biases where spurious shortcuts predict labels without reasoning. Critical for interpreting hypothesis-only classifier results. *Quick check: If a sentiment dataset contains "great" in 90% of positive reviews, what problem does this create for models?*

- **Pointwise Mutual Information (PMI)**: Measures how much more often two words co-occur than chance would predict. Method for detecting stereotypical associations in generated text. *Quick check: If "doctor" co-occurs with "he" 100x and "she" 50x in a corpus, what would PMI reveal about bias?*

## Architecture Onboarding

- **Component map**: Premise (SNLI) → LLM (GPT-4/Llama/Mistral) → Generated Hypotheses → Train Hypothesis-Only Classifier → Evaluate on Multiple Test Sets → Compare: SNLI vs LLM datasets → PMI Analysis for Social Bias

- **Critical path**: 1) Prompt design matching original SNLI instructions, 2) Label validation through manual agreement check, 3) Hypothesis-only classifier training, 4) Cross-dataset evaluation to detect shared artifacts, 5) PMI computation for demographic term associations

- **Design tradeoffs**: Temperature 0.75 balances diversity vs reproducibility; using ~1/3 of training premises trades coverage for compute cost; Jaccard similarity detects verbatim copying but misses paraphrase contamination; single-prompt approach limits generalizability

- **Failure signatures**: Low label agreement (<80%) indicates model misunderstood task; high hypothesis-only accuracy indicates artifacts present; cross-model transfer suggests shared artifact vocabulary; repeated phrases appearing >10,000 times indicate pattern repetition

- **First 3 experiments**: 1) Generate 1000 hypothesis triples with default settings, train hypothesis-only NB classifier with 50 unigram features (if accuracy >60%, artifacts present), 2) Remove example hypotheses from prompt, regenerate, measure if prompt-derived give-aways disappear, 3) Extract top 20 PMI-associated words for "man/woman" (if male→work/violence and female→domesticity patterns appear, stereotypical bias confirmed)

## Open Questions the Paper Calls Out

- **Extent of downstream harm**: To what extent do annotation artifacts in LLM-generated datasets degrade the robustness of downstream NLI models compared to human-elicited datasets? The study demonstrates artifact existence but doesn't measure causal impact on model failure modes.

- **Prompt influence**: How does prompt phrasing influence the severity and type of annotation artifacts produced by LLMs? The study relied on a single prompt template and explicitly leaves multi-prompt analysis for future work.

- **Task generalization**: Do stereotypical biases and artifacts observed in NLI generalize to other text generation tasks like summarization or sentiment analysis? The study's NLI-specific methodology may not apply to tasks with different structural requirements.

## Limitations

- Only one fixed prompt template used, limiting generalizability to other prompt formulations
- Temperature and top-p values chosen without systematic exploration
- Exact subset of SNLI premises used for training is unspecified
- PMI analysis focuses on English gender terms, may not generalize to other languages or cultural contexts
- Doesn't address potential prompt injection attacks or how artifacts change with different temperature settings

## Confidence

**High confidence**: Hypothesis-only classifier results (86-96% vs 72% accuracy) and PMI analysis showing stereotypical gender associations are well-supported.

**Medium confidence**: Claims about cross-model classifier transfer indicating shared artifact structures and prompt formatting influence on "There" give-away word are plausible but not definitively proven.

**Low confidence**: Assertion that these biases will necessarily appear in any LLM-generated NLI dataset, as study uses only three models and one prompt template.

## Next Checks

1. **Prompt Ablation Test**: Remove example hypotheses from prompt template and regenerate datasets. If give-away words disappear while hypothesis-only accuracy remains high, confirms artifacts stem from both task instructions and example formatting.

2. **Temperature Sweep**: Generate datasets across temperature values (0.1, 0.5, 0.75, 1.0) and measure how hypothesis-only accuracy and artifact vocabulary change to reveal if lower temperatures reduce artifacts.

3. **Cross-Domain Transfer**: Train hypothesis-only classifiers on NLI data generated for a different task (e.g., sentiment analysis) and test on LLM NLI datasets to determine if artifacts are task-agnostic rather than domain-specific.