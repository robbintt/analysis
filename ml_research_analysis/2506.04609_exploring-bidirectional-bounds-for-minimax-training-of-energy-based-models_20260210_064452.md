---
ver: rpa2
title: Exploring bidirectional bounds for minimax-training of Energy-based models
arxiv_id: '2506.04609'
source_url: https://arxiv.org/abs/2506.04609
tags:
- bound
- training
- lower
- learning
- upper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the instability of training Energy-Based Models
  (EBMs) through minimax games by proposing bidirectional bounds instead of minimizing
  a single lower bound. The core method involves maximizing a lower bound and minimizing
  an upper bound on the negative log-likelihood, using four different bounds: two
  lower bounds based on singular values of the generator Jacobian and mutual information,
  and two upper bounds based on gradient penalties and diffusion processes.'
---

# Exploring bidirectional bounds for minimax-training of Energy-based models

## Quick Facts
- **arXiv ID:** 2506.04609
- **Source URL:** https://arxiv.org/abs/2506.04609
- **Reference count:** 40
- **Key outcome:** Bidirectional bounds stabilize EBM training, preventing energy divergence and improving sample quality on CIFAR-10, ImageNet 32x32, and StackedMNIST mode counting.

## Executive Summary
Energy-based models (EBMs) are trained via minimax games that can suffer from instability when minimizing a single lower bound on negative log-likelihood. This paper proposes a bidirectional bound approach that sandwiches the true likelihood between an upper and lower bound, alternating between maximizing the lower bound and minimizing the upper bound. The method uses four bounds: singular value-based entropy lower bounds, mutual information lower bounds, and gradient penalty/diffusion process upper bounds. Experiments demonstrate improved stability, better mode coverage, and competitive generation quality compared to state-of-the-art methods.

## Method Summary
The method trains EBMs by sandwiching the negative log-likelihood between bidirectional bounds rather than minimizing a single lower bound. It combines a lower bound on the generator entropy (using singular values of the generator Jacobian or mutual information) with an upper bound on the KL divergence (using gradient penalties or diffusion processes). The training alternates between updating the energy function to minimize the upper bound and updating the generator to maximize the lower bound. This prevents the energy from diverging to negative infinity while maintaining valid likelihood bounds throughout training.

## Key Results
- Bidirectional bounds stabilize EBM training, preventing energy divergence observed with standard minimax training
- Achieves FID scores around 28-30 and Inception Scores around 7.4-7.5 on CIFAR-10
- Captures all 1000 modes on StackedMNIST while baselines miss modes
- Shows good out-of-distribution detection performance with AUROC/AUPRC improvements

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Bound Sandwiching
Alternating between maximizing a lower bound and minimizing an upper bound on negative log-likelihood stabilizes EBM training, avoiding divergence to unhelpful minima. Standard minimax training minimizes a lower bound L(θ) ≤ L(θ), which can reach -∞ when the bound becomes loose. The paper proposes instead: ⌊L(θ)⌋ ≤ L(θ) ≤ ⌈L(θ)⌉. Minimize the upper bound for energy parameters; maximize the lower bound for generator parameters. This constrains the objective from both sides. Core assumption: The upper and lower bounds remain reasonably tight during training. Evidence anchors: Divergent energy with standard minimax vs. stable training curve with bidirectional bounds. Break condition: If upper bound becomes loose or lower bound approximation fails.

### Mechanism 2: Singular Value-Based Entropy Lower Bound
The smallest singular value of the generator Jacobian provides a tractable lower bound on generator entropy H[pg]. For generator G: Rd → RD, entropy is bounded by: H[pg] ≥ H[p0] + Ez[½ log det(J^T J)] ≥ H[p0] + Ez[d·log s1], where s1 is the smallest singular value. This avoids expensive log-determinant computation while maintaining a valid bound. Core assumption: Generator Jacobian has full rank almost everywhere. Evidence anchors: Derives the bound ½ log det(J^T J) ≥ d·log s1. Break condition: If generator Jacobian becomes rank-deficient (s1 → 0), the bound becomes vacuous.

### Mechanism 3: Gradient Penalty Upper Bound
A gradient penalty term provides a maximum-likelihood-justified upper bound on KL(pg||pθ), preventing energy divergence. Theorem 1 proves KL(pg||pθ) ≤ M·E[‖∇x log f(x)‖_p] + Mm. For EBM with f(x) = exp(-Eθ(x))/pg(x), this yields upper bound: ⌈L(θ)⌉ = L(θ) + M·E[‖∇xEθ(x) + ∇x log pg(x)‖_p]. When pθ = pg, the gradient term vanishes and bound becomes tight. Core assumption: The energy function is L-Lipschitz. Evidence anchors: Formal proof of the gradient penalty bound. Break condition: If gradient penalty coefficient M is poorly tuned.

## Foundational Learning

- **Concept: Energy-Based Models (EBMs)**
  - **Why needed here:** The entire framework builds on EBMs defining pθ(x) = exp(-Eθ(x))/Zθ. Without this foundation, the bounds derivation is opaque.
  - **Quick check question:** Can you explain why computing Zθ (the partition function) is the fundamental challenge in EBM training, and how this motivates variational approaches?

- **Concept: Variational Lower Bounds and Jensen's Inequality**
  - **Why needed here:** The standard lower bound L(θ) = E_pdata[Eθ] - E_pg[Eθ] + H[pg] is derived via Jensen's inequality applied to log Zθ. Understanding this explains why minimizing a lower bound can be problematic.
  - **Quick check question:** If L(θ) ≤ L(θ) and we minimize L(θ), why might we converge to parameters where L(θ) is minimized but L(θ) is far from optimal?

- **Concept: Singular Value Decomposition and Jacobian Rank**
  - **Why needed here:** The SV lower bound requires computing the smallest singular value of the generator Jacobian. Understanding the relationship between singular values, Jacobian rank, and entropy is essential for debugging bound failures.
  - **Quick check question:** What happens to the entropy lower bound when the smallest singular value s1 approaches zero, and what architectural constraints prevent this?

## Architecture Onboarding

- **Component map:** Latent z ~ N(0,I) -> Generator G -> x = G(z) + ε -> Energy Eθ(x) -> Scalar energy; Discriminator T (optional for MI bound); Singular value estimator (LOBPCG)

- **Critical path:**
  1. Sample real data x ~ pdata and latent codes z ~ N(0,I)
  2. Generate samples: x_gen = G(z) + ε
  3. Compute energy terms: Eθ(x_real), Eθ(x_gen)
  4. For SV bound: Estimate s1 via LOBPCG on Jacobian J = ∂G/∂z
  5. For GP bound: Compute gradient penalty ‖∇xEθ(x_gen) + ∇x log pg(x_gen)‖
  6. For diff bound: Sample t ~ Uniform(0,T), compute score matching loss
  7. Alternate: Update Eθ (minimize upper bound), update G (maximize lower bound)

- **Design tradeoffs:**
  - **EBMSV+GP:** Space-efficient (no extra network); ~2.5× training time due to singular value estimation; good for smaller networks/datasets
  - **EBMMI+diff:** Time-efficient (~1.3× WGAN baseline); requires extra discriminator T; better for larger/deeper networks
  - **Entropy regularizer λ:** Must be tuned with gradient penalty coefficient; paper uses λ < 1 for large-scale experiments (violates bound theoretically but works empirically)
  - **Positive margin ζ:** Hinge on gradient penalty (Eq. 33) improves stability; start at ζ=0, increase if training unstable

- **Failure signatures:**
  - Energy divergence to -∞: Upper bound too loose; increase gradient penalty or check margin ζ
  - Mode collapse on simple data: Lower bound too loose; verify singular value estimation quality or switch MI→SV
  - Training instability with SV+GP: Jacobian becoming ill-conditioned; check network doesn't have layers smaller than latent dimension
  - Poor generation with diff bound: Fokker-Planck constraint violated; this is acknowledged limitation in paper

- **First 3 experiments:**
  1. **Sanity check on 25-Gaussians toy dataset:** Train EBMSV+GP and verify density estimation covers all modes. Compare against EBM baseline (no upper bound) to confirm divergent training is prevented. Use MLP architecture.
  2. **Mode counting on StackedMNIST:** Verify all 1000 modes captured. Compare EBMSV+GP vs EBMMI+diff across FC and CNN architectures. If SV+GP fails with CNN but not FC, this confirms paper's architecture sensitivity finding.
  3. **CIFAR-10 generation with FID/IS metrics:** Train both EBMSV+GP and EBMMI+diff with DCGAN architecture. Target: FID ~28-30, IS ~7.4-7.5. Use λ=0.0001 for SV+GP, ζ=1 for GP margin. Monitor upper/lower bound gap during training to verify sandwiching behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the energy function $E_\theta(x,t)$ be designed or constrained to strictly satisfy the Fokker-Planck equation when using the diffusion-based upper bound?
- Basis in paper: The authors state in the Limitations section that "it is not clear how to design $E_\theta(x,t)$ to make it satisfy the Fokker-Planck equation," noting that the current approach remains heuristic.
- Why unresolved: The current parameterization does not enforce the physical constraints required for the theoretical bound to hold strictly.
- What evidence would resolve it: A parameterization method or regularization term that mathematically guarantees the Fokker-Planck equation is satisfied during training.

### Open Question 2
- Question: Why does the singular value-based lower bound ($EBM_{SV+GP}$) suffer from mode collapse when used with convolutional neural networks (CNNs) but performs well with fully connected networks?
- Basis in paper: Section 4.3 notes that the method "is sensitive to the choice of network structure," failing with CNNs despite CNNs being the standard for image generation.
- Why unresolved: The paper observes the phenomenon but does not analyze the interaction between the singular value estimation and the inductive biases of CNNs.
- What evidence would resolve it: An analysis of Jacobian properties in CNNs versus FC networks within this framework, or a modified bound that works robustly with convolutional architectures.

### Open Question 3
- Question: Can an efficient entropy estimator be developed that maintains the theoretical lower bound without requiring premature stopping of the iterative solver?
- Basis in paper: Section 5 notes that the practical implementation "stops prematurely to ensure fast computations," which technically violates the bound to save computational cost.
- Why unresolved: There is a trade-off between computational tractability and theoretical correctness in the current singular value estimation.
- What evidence would resolve it: A closed-form or non-iterative approximation for the singular values that is computationally cheap yet strictly bounds the entropy.

## Limitations
- The diffusion-based upper bound requires the energy function to satisfy the Fokker-Planck equation, which is not enforced in current parameterization
- The singular value-based lower bound is sensitive to network architecture, failing with CNNs despite their standard use in image generation
- Using entropy regularizer λ > 1 improves empirical performance but violates the theoretical bound

## Confidence
- **High confidence:** The bidirectional training framework prevents energy divergence compared to standard minimax training. The gradient penalty bound derivation is mathematically rigorous.
- **Medium confidence:** The singular value-based entropy lower bound provides practical improvement, though the empirical evidence is limited to specific architectures.
- **Low confidence:** The diffusion-based upper bound's superiority over gradient penalty is not fully demonstrated, and the paper acknowledges the Fokker-Planck constraint limitation.

## Next Checks
1. **Bound Tightness Analysis:** Systematically measure the gap between upper and lower bounds during training across different architectures and datasets. Verify that smaller gaps correlate with better sample quality and mode coverage.

2. **Architecture Sensitivity Test:** Compare EBMSV+GP performance with fully-connected vs. convolutional networks on MNIST to reproduce the paper's finding that SV bound works better with FC architectures. Document the precise failure conditions for each bound type.

3. **Out-of-Distribution Transfer:** Test the OOD detection capability on CIFAR-10 when trained on SVHN or Tiny ImageNet. Verify the reported AUROC/AUPRC improvements are consistent across different OOD datasets and not architecture-specific.