---
ver: rpa2
title: The Curious Case of In-Training Compression of State Space Models
arxiv_id: '2510.02823'
source_url: https://arxiv.org/abs/2510.02823
tags:
- state
- training
- reduction
- performance
- dimension
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPRESSM, a principled in-training compression
  method for State Space Models (SSMs) based on balanced truncation from control theory.
  The core idea is to leverage the eigenvalue stability of Hankel matrices to identify
  and truncate low-energy state dimensions during training, rather than compressing
  post-training.
---

# The Curious Case of In-Training Compression of State Space Models

## Quick Facts
- arXiv ID: 2510.02823
- Source URL: https://arxiv.org/abs/2510.02823
- Reference count: 40
- Primary result: Introduces COMPRESSM, an in-training compression method for SSMs that achieves significant training speedups while maintaining or improving accuracy compared to uncompressed models

## Executive Summary
This paper presents COMPRESSM, a principled in-training compression method for State Space Models (SSMs) based on balanced truncation from control theory. The approach leverages the eigenvalue stability of Hankel matrices to identify and truncate low-energy state dimensions during training, exploiting the rank-preserving property of dominant Hankel singular values. Experiments demonstrate that COMPRESSM significantly accelerates training while maintaining or improving performance compared to both uncompressed models and compressed models trained directly at smaller dimensions across multiple benchmarks including vision and language tasks.

## Method Summary
COMPRESSM introduces an in-training compression framework for State Space Models that operates during the optimization process rather than as a post-processing step. The method is based on balanced truncation, a well-established technique from control theory, which identifies and removes states with low Hankel singular values - effectively low-energy dimensions that contribute minimally to the system's behavior. During training, the algorithm continuously monitors the Hankel matrix of the SSM, computes its singular values, and progressively truncates dimensions corresponding to small singular values. This approach exploits the mathematical property that dominant Hankel singular values are rank-preserving during optimization, allowing the model to maintain essential information while reducing dimensionality. The method is integrated directly into the training loop, making it distinct from traditional post-training compression techniques.

## Key Results
- On CIFAR10, a COMPRESSM-reduced model (dimension 92) achieves 85.7% accuracy with 1.5Ã— speedup compared to the full model (86.5% accuracy)
- Directly trained compressed models at the same dimension reach only 81.8% accuracy, demonstrating COMPRESSM's superiority
- COMPRESSM outperforms knowledge distillation and Hankel Nuclear Norm regularization in terms of accuracy and training efficiency
- Consistent performance improvements across multiple benchmarks including sMNIST, ListOps, IMDB, and Pathfinder

## Why This Works (Mechanism)
COMPRESSM works by exploiting the mathematical properties of Hankel singular values during optimization. The method leverages the rank-preserving characteristic of dominant Hankel singular values, which means that the most important modes of the system remain stable throughout training. By continuously monitoring these singular values during training and truncating low-energy dimensions, the method can effectively compress the model without losing critical information. This is fundamentally different from post-training compression because it operates during the learning process when the model's dynamics are still evolving, allowing for more informed decisions about which dimensions can be safely removed.

## Foundational Learning

**Hankel Matrices**: Square matrices constructed from system impulse responses that capture the system's input-output behavior
- Why needed: Essential for analyzing system properties and determining which dimensions carry important information
- Quick check: Verify the matrix is constructed correctly by checking that elements satisfy the Hankel property (constant along anti-diagonals)

**Hankel Singular Values**: The singular values of Hankel matrices that represent the energy distribution across different system modes
- Why needed: Provide a principled way to rank state dimensions by their importance to the system's behavior
- Quick check: Confirm that larger singular values correspond to more energetic and important system modes

**Balanced Truncation**: A model reduction technique that balances controllability and observability gramians
- Why needed: Provides the theoretical foundation for identifying which states can be safely removed
- Quick check: Ensure the reduced model maintains stability and approximates the original system's behavior

## Architecture Onboarding

**Component Map**: SSM Layer -> Hankel Matrix Computation -> Singular Value Decomposition -> Dimension Truncation -> Training Update

**Critical Path**: The integration of Hankel matrix computation and singular value decomposition into the training loop represents the critical path. This must be computed efficiently to avoid becoming a bottleneck, as it occurs at every training step.

**Design Tradeoffs**: The primary tradeoff is between compression effectiveness and computational overhead. More frequent Hankel matrix updates provide better compression but increase computational cost. The truncation threshold must balance between aggressive compression (faster training) and maintaining model capacity (better accuracy).

**Failure Signatures**: If the truncation threshold is too aggressive, the model may lose essential information leading to degraded performance. If computed too infrequently, the compression may miss important dynamics. Computational overhead may become prohibitive if the Hankel matrix computation is not optimized for the specific SSM architecture.

**First Experiments**:
1. Verify that Hankel singular values remain stable during training on a simple SSM task
2. Test different truncation thresholds on a small-scale problem to find the optimal balance
3. Compare training dynamics with and without COMPRESSM on a single benchmark to measure overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to other tasks and domains beyond the specific datasets tested (sMNIST, CIFAR10, ListOps, IMDB, Pathfinder) remains uncertain
- Scalability to very deep or wide SSMs with extremely large hidden dimensions has not been thoroughly explored
- Limited comparison with alternative compression techniques used in deep learning, such as pruning, low-rank factorization, and quantization

## Confidence

- **High Confidence**: The theoretical foundation of COMPRESSM (balanced truncation, Hankel singular values) is well-established in control theory and correctly applied. The empirical results showing speedup and accuracy improvements over uncompressed models are reproducible and clearly demonstrated.

- **Medium Confidence**: The superiority of COMPRESSM over directly trained compressed models and knowledge distillation is demonstrated, but the experimental scope is limited to specific datasets and model sizes. Generalization to other tasks and scales requires further validation.

- **Low Confidence**: Claims about the method's performance relative to all other compression techniques in the literature are not fully supported due to limited comparative analysis.

## Next Checks

1. **Cross-Domain Generalization**: Apply COMPRESSM to SSMs on tasks outside the current scope, such as speech recognition (e.g., TIMIT), time-series forecasting (e.g., traffic prediction), or large-scale language modeling (e.g., next-token prediction on a subset of LLaMA). Evaluate whether the accuracy-speedup trade-off holds and whether the method introduces any domain-specific instabilities.

2. **Scalability Analysis**: Test COMPRESSM on SSMs with significantly larger hidden dimensions (e.g., 1024, 2048) and deeper architectures (e.g., >10 layers). Measure both the computational overhead of the method and the quality of the compression at these scales. Compare the results with standard pruning or low-rank compression techniques to assess relative efficiency.

3. **Robustness to Hyperparameters and Training Dynamics**: Investigate the sensitivity of COMPRESSM to the rank truncation threshold and the stage at which compression is applied during training. Conduct ablation studies to determine the optimal compression schedule and analyze how the method interacts with different learning rates, optimizers, and regularization strategies.