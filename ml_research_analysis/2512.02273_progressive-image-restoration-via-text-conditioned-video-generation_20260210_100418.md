---
ver: rpa2
title: Progressive Image Restoration via Text-Conditioned Video Generation
arxiv_id: '2512.02273'
source_url: https://arxiv.org/abs/2512.02273
tags:
- restoration
- cogvideo
- enhancement
- image
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework that repurposes the text-to-video
  generation model CogVideo for image restoration tasks by fine-tuning it to generate
  progressive restoration trajectories. The key insight is that image restoration
  (super-resolution, deblurring, and low-light enhancement) can be modeled as a temporal
  video generation problem, where a sequence of frames gradually evolves from degraded
  to clean states.
---

# Progressive Image Restoration via Text-Conditioned Video Generation

## Quick Facts
- arXiv ID: 2512.02273
- Source URL: https://arxiv.org/abs/2512.02273
- Authors: Peng Kang; Xijun Wang; Yu Yuan
- Reference count: 20
- Primary result: Repurposes text-to-video model CogVideo for unified image restoration via progressive frame generation

## Executive Summary
This paper proposes a novel framework that repurposes the text-to-video generation model CogVideo for image restoration tasks by fine-tuning it to generate progressive restoration trajectories. The key insight is that image restoration (super-resolution, deblurring, and low-light enhancement) can be modeled as a temporal video generation problem, where a sequence of frames gradually evolves from degraded to clean states. Three synthetic datasets are constructed to train the model on these progressive transformations. Two prompting strategies are evaluated: uniform text prompts and scene-adaptive prompts generated via LLaVA and refined with ChatGPT. The fine-tuned CogVideo successfully generates temporally coherent restoration sequences, with quantitative improvements in PSNR, SSIM, and LPIPS metrics across frames. Notably, the model demonstrates strong zero-shot generalization to real-world motion blur restoration on the ReLoBlur dataset without additional training, highlighting its potential as an interpretable and extensible paradigm for unified visual restoration tasks.

## Method Summary
The method treats image restoration as a video generation problem by generating a sequence of frames that progressively transform from degraded to restored states. The approach fine-tunes the CogVideo text-to-video generation model on three synthetic datasets representing super-resolution, deblurring, and low-light enhancement tasks. Each dataset contains 9-frame sequences where frames gradually improve from the degraded state to the fully restored state. Two prompting strategies are employed: uniform prompts that apply the same text to all frames, and scene-adaptive prompts that use LLaVA to generate initial descriptions of each degraded image, followed by ChatGPT refinement. The model is trained with cross-entropy loss between generated frames and ground truth frames. After fine-tuning, the model can generate restoration trajectories for unseen images through text prompting, with the final frame representing the restored output.

## Key Results
- Quantitative improvements in PSNR, SSIM, and LPIPS metrics across restoration frames
- Successful zero-shot generalization to real-world motion blur restoration on ReLoBlur dataset
- Temporally coherent restoration sequences demonstrating interpretable progressive improvement
- Scene-adaptive prompting strategy outperforms uniform prompting in visual quality metrics

## Why This Works (Mechanism)
The approach leverages the temporal modeling capabilities of video generation models to capture the progressive nature of image restoration. By framing restoration as a sequence of intermediate states rather than a single transformation, the model can learn gradual feature enhancement patterns that mirror how human observers perceive image quality improvement. The text conditioning allows the model to incorporate semantic context about the scene, enabling more semantically aware restoration that preserves content while improving visual quality. The progressive generation also provides interpretability, as users can observe the intermediate steps of restoration, which is valuable for understanding model behavior and potential failure modes.

## Foundational Learning
- Text-to-video generation fundamentals: Why needed - to understand the base model being repurposed; Quick check - ability to generate coherent frame sequences from text
- Image degradation models: Why needed - to create synthetic training data representing different restoration tasks; Quick check - understanding how to simulate blur, downsampling, and low-light effects
- Cross-entropy loss in generative modeling: Why needed - to optimize the frame generation process; Quick check - familiarity with pixel-wise reconstruction objectives
- Scene description generation with LLaVA: Why needed - to create semantically relevant prompts for adaptive conditioning; Quick check - ability to generate accurate image captions
- Zero-shot learning concepts: Why needed - to evaluate generalization without task-specific fine-tuning; Quick check - understanding how models can apply learned patterns to new tasks

## Architecture Onboarding

Component map: Text input -> LLaVA/ChatGPT (for scene-adaptive prompts) -> CogVideo backbone (with LoRA adapters) -> 9-frame progressive output

Critical path: Input image → Text description (via LLaVA) → ChatGPT refinement → Frame generation pipeline → Progressive restoration sequence

Design tradeoffs: The choice between uniform and scene-adaptive prompting represents a fundamental tradeoff between simplicity and performance. Scene-adaptive prompting provides better semantic alignment and higher quality results but requires additional computation for prompt generation and refinement. The progressive frame generation approach trades computational efficiency for interpretability and potentially better handling of complex restoration tasks.

Failure signatures: Poor text descriptions from LLaVA can lead to misaligned restoration, incorrect semantic features, or artifacts in the output. Inconsistent frame progression may indicate instability in the generation process or inadequate training data. Complete generation failures could stem from prompt quality issues, model capacity limitations, or insufficient fine-tuning.

First experiments:
1. Generate restoration sequences for simple synthetic degradations to verify basic functionality
2. Compare uniform vs. scene-adaptive prompting on a held-out validation set
3. Evaluate zero-shot generalization on real-world blur images from ReLoBlur dataset

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Synthetic dataset construction methodology lacks validation of real-world degradation representation
- Computational cost of generating 9-frame sequences for single image restoration not characterized
- Limited evaluation of multi-degradation scenarios combining super-resolution, deblurring, and low-light enhancement
- Scene-adaptive prompting reliability depends on external language models (LLaVA, ChatGPT)

## Confidence
- High confidence: The core methodology of repurposing CogVideo for progressive image restoration is sound and technically feasible
- Medium confidence: The quantitative improvements in restoration metrics are valid but may not fully capture perceptual quality
- Medium confidence: The zero-shot generalization capability shows promise but requires validation across more diverse real-world datasets
- Low confidence: The robustness of scene-adaptive prompting across different image domains and degradation types

## Next Checks
1. Evaluate the model on additional real-world degradation datasets beyond ReLoBlur to assess generalization robustness
2. Conduct ablation studies comparing uniform vs. scene-adaptive prompting across different image restoration tasks
3. Measure inference speed and computational requirements for practical deployment scenarios