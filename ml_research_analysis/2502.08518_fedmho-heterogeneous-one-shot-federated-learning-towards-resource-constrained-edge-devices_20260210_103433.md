---
ver: rpa2
title: 'FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained
  Edge Devices'
arxiv_id: '2502.08518'
source_url: https://arxiv.org/abs/2502.08518
tags:
- local
- global
- data
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FedMHO, a one-shot federated learning framework
  designed for heterogeneous clients with varying computing capabilities. The core
  idea involves deploying deep classification models on resource-sufficient clients
  and lightweight generative models on resource-constrained devices.
---

# FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices

## Quick Facts
- **arXiv ID:** 2502.08518
- **Source URL:** https://arxiv.org/abs/2502.08518
- **Authors:** Dezhong Yao; Yuexin Shi; Tongtong Liu; Zhiqiang Xu
- **Reference count:** 40
- **Key outcome:** Proposes FedMHO, a one-shot FL framework deploying deep models on resource-sufficient clients and lightweight generative models (CVAEs) on constrained devices, achieving 5.17-8.35% accuracy improvements over baselines.

## Executive Summary
This paper addresses heterogeneous one-shot federated learning where clients have varying computing capabilities. The core innovation is deploying deep classification models on resource-sufficient clients while using lightweight conditional variational autoencoders (CVAEs) on resource-constrained devices. During aggregation, classification models initialize the global model through parameter averaging, while generative models produce synthetic samples for further training. To prevent knowledge forgetting, two distillation strategies (multi-teacher and self-distillation) are introduced. Experiments demonstrate significant accuracy improvements across multiple datasets compared to state-of-the-art baselines.

## Method Summary
FedMHO is a one-shot federated learning framework that handles heterogeneous client resources by deploying deep classification models on resource-sufficient clients and lightweight CVAEs on constrained devices. During the single communication round, classification models send their weights to the server while CVAEs send their decoder weights along with local label distributions. The server then generates synthetic samples, filters them using K-means clustering (retaining top 80%), and performs knowledge fusion through parameter averaging initialization followed by training on synthetic data. To mitigate knowledge forgetting during global model training, FedMHO-MD uses multiple teacher distillation while FedMHO-SD employs self-distillation.

## Key Results
- FedMHO achieves 5.17% average accuracy improvement over baselines across various datasets and partitions
- FedMHO-MD shows 8.35% improvement by using multiple teacher models for distillation
- FedMHO-SD achieves 8.25% improvement through self-distillation from the initialized global model
- The framework effectively handles heterogeneous client capabilities in one-shot FL settings

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Model Deployment Strategy
- **Claim:** Deploying deep classification models on resource-sufficient clients and lightweight generative models (CVAEs) on resource-constrained clients enables participation from all device types while mitigating negative knowledge transfer from underperforming models.
- **Mechanism:** Lightweight generative models, while producing lower-fidelity samples, do not generate erroneous logits (misleading soft labels) that poorly trained lightweight classifiers would. The synthetic data captures category information with diversity, allowing constrained devices to contribute without degrading global model quality.
- **Core assumption:** Resource-constrained devices can successfully train lightweight CVAEs that capture the local label distribution. Assumption: The unsupervised data optimization (K-means filtering) effectively removes the noisiest synthetic samples.
- **Evidence anchors:**
  - [abstract] "FedMHO addresses the challenge of heterogeneous one-shot federated learning where clients have varying computing resources. The method deploys deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices."
  - [section 3, Motivation] "FEDCVAE... improvement arises because, while lightweight generative models may produce low-quality samples... they do not introduce erroneous logits like poorly performing classification models do."
  - [corpus] "HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts" supports tailoring model architecture to client resources. "FTTE" confirms general challenges on constrained nodes.
- **Break condition:** This fails if constrained clients cannot train even lightweight CVAEs, or if local data is so sparse/non-IID that generative models fail to capture meaningful category distributions.

### Mechanism 2: Knowledge Fusion via Parameter Initialization and Synthetic Data Training
- **Claim:** A two-stage server-side process—initializing the global model by averaging local classification model parameters, then fine-tuning it on synthetic samples—creates a more effective global model than using either method alone.
- **Mechanism:** Parameter averaging from well-trained classification models provides a strong initialization point (knowledge from real data). Training on synthetic samples incorporates information from the generative models (knowledge from constrained clients' data).
- **Core assumption:** The knowledge encoded in classification model parameters is complementary to the distributional knowledge in synthetic samples. Assumption: Averaging parameters of heterogeneous classification models provides a better global starting point than training from scratch.
- **Evidence anchors:**
  - [abstract] "During aggregation, classification models initialize the global model through parameter averaging, while generative models produce synthetic samples for further training."
  - [section 4.2.3, Knowledge Fusion] "Compared with random parameter initialization, this informed initialization allows the global model to achieve superior performance in fewer training rounds."
  - [corpus] "Federated Learning of Low-Rank One-Shot Image Detection Models" corroborates the need for specialized aggregation in one-shot settings.
- **Break condition:** This breaks if parameter spaces are too divergent for averaging, or if the domain gap between synthetic and real data causes overfitting to generative artifacts.

### Mechanism 3: Mitigating Knowledge-Forgetting via Distillation
- **Claim:** A distillation loss, using local classification models as teachers (FedMHO-MD) or the initialized global model as its own teacher (FedMHO-SD), prevents the global model from forgetting original knowledge while learning from synthetic samples.
- **Mechanism:** The loss function combines cross-entropy (to learn from synthetic labels) and a distillation loss (to maintain consistency with the "teacher's" logits). This regularization preserves initial knowledge while integrating new information.
- **Core assumption:** The teacher model(s) provide stable and accurate logits on synthetic samples. Assumption: The balance parameter λ=0.5 appropriately weights both objectives.
- **Evidence anchors:**
  - [abstract] "Two strategies (FedMHO-MD and FedMHO-SD) mitigate knowledge-forgetting during global model training through multi-teacher distillation and self-distillation respectively."
  - [section 4.3, The Knowledge-Forgetting Problem] "During the fusion of the clients’ knowledge from the generative models, the global model may forget the knowledge learned from the classification models."
  - [corpus] Weak evidence for this specific self-distillation variant. "Federated Distillation on Edge Devices" supports general utility of distillation techniques.
- **Break condition:** This fails if teacher models provide poor logits on synthetic samples (e.g., due to high domain shift), reinforcing incorrect knowledge.

## Foundational Learning

- **Concept: Conditional Variational Autoencoder (CVAE)**
  - **Why needed here:** This is the lightweight generative model deployed on resource-constrained clients to generate synthetic samples conditioned on class labels.
  - **Quick check question:** Can you explain how a CVAE differs from a standard VAE, and how conditioning enables generation of samples from specific classes?

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here:** KD is the core technique in FedMHO-MD and FedMHO-SD to combat knowledge-forgetting using KL divergence between student and teacher logits.
  - **Quick check question:** What is the purpose of using "soft targets" (logits) from a teacher model instead of just hard labels?

- **Concept: One-Shot Federated Learning**
  - **Why needed here:** This is the problem setting with a single communication round, which constrains all design choices.
  - **Quick check question:** What are the primary communication and privacy benefits of one-shot FL compared to standard iterative FL?

## Architecture Onboarding

- **Component map:**
  - **Clients (Resource-Sufficient):** Train deep classification model (e.g., VGG-9). Send weights ($w_k$) to server.
  - **Clients (Resource-Constrained):** Train lightweight CVAE. Send decoder weights ($w_{\theta_k}$) and local label distribution.
  - **Server (Data Generation):** Generate synthetic samples, filter via K-means (retain top 80% closest to cluster center), form global synthetic dataset $D_s$.
  - **Server (Knowledge Fusion):** Initialize global model $w^0$ by averaging classification weights. Train $w$ on $D_s$ with combined cross-entropy and distillation loss.

- **Critical path:**
  1. Correct client model assignment based on resource capability.
  2. Successful CVAE training generating recognizable samples.
  3. Proper parameter averaging for valid global model initialization.
  4. Correct distillation loss implementation (KL divergence).

- **Design tradeoffs:**
  - **FedMHO-MD vs. FedMHO-SD:** MD uses multiple local teachers (more robust but memory-intensive); SD uses only initial global model (memory-efficient but depends on initialization quality).
  - **Rth (Retention Ratio):** Lower ratio filters more noise but reduces diversity; higher ratio retains more data but risks noise. Paper recommends 80%.
  - **λ (Loss Weight):** Fixed 0.5 weighting; making it learnable adds complexity without significant gains.

- **Failure signatures:**
  - **Poor accuracy, no improvement:** CVAE training failure, nonsensical synthetic data.
  - **Accuracy degrades during training:** Severe knowledge-forgetting; distillation loss ineffective.
  - **Training unstable/loss diverges:** Incorrect loss implementation or poor hyperparameters.
  - **Wrong-class synthetic samples:** Data optimization failure or poor CVAE training.

- **First 3 experiments:**
  1. **Baseline Validation:** Run FedMHO on EMNIST with paper configuration (5 VGG-9, 5 CVAE-small). Reproduce accuracy for FedMHO, FedMHO-MD, FedMHO-SD.
  2. **Ablation on Data Optimization:** Same experiment with K-means filtering disabled. Compare accuracy to verify component contribution.
  3. **Sensitivity to Data Heterogeneity:** Run FedMHO-MD on EMNIST varying Dirichlet α (0.5, 0.3, 0.1). Plot accuracy to confirm robustness under non-IID conditions.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical evaluation limited to three datasets (EMNIST, Fashion-MNIST, CIFAR-10) with specific data partitions
- No testing on extreme resource constraints or highly non-IID distributions
- Fixed hyperparameters (Rth=80%, λ=0.5) lack sensitivity analysis
- Assumes synthetic samples form discernible clusters for K-means filtering, which may not hold for complex data distributions

## Confidence
- **High Confidence:** The heterogeneous deployment strategy (deep models on resource-sufficient clients, CVAEs on constrained clients) is well-justified by the fundamental tradeoff between model capacity and resource constraints.
- **Medium Confidence:** The knowledge fusion approach (parameter averaging followed by synthetic data training) shows promise but depends on assumptions about parameter averaging and complementary knowledge.
- **Medium Confidence:** The distillation strategies for knowledge preservation are conceptually valid, but effectiveness depends on teacher model quality on synthetic samples.

## Next Checks
1. **Extreme Heterogeneity Test:** Evaluate FedMHO on a dataset with highly skewed resource distribution (e.g., 1 high-resource client vs. 9 severely constrained clients) to verify the framework's robustness under realistic edge deployment scenarios.

2. **Teacher Quality Assessment:** Implement a diagnostic to measure the KL divergence between teacher and student logits on synthetic samples during training to quantify whether teachers provide meaningful supervision.

3. **Parameter Averaging Sensitivity:** Run ablation experiments comparing FedMHO's parameter averaging initialization against alternative methods to validate the claimed benefit of the averaging approach.