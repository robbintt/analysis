---
ver: rpa2
title: Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio
  Selection
arxiv_id: '2512.02386'
source_url: https://arxiv.org/abs/2512.02386
tags:
- optimal
- function
- policy
- value
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CT-RS-q, a risk-sensitive q-learning algorithm
  for continuous-time reinforcement learning where the objective is a nonlinear functional
  of cumulative rewards. The key insight is that when the risk measure is an optimized
  certainty equivalent (OCE), the optimal policy is Markovian with respect to an augmented
  state space tracking cumulative rewards and discounting effects.
---

# Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection

## Quick Facts
- arXiv ID: 2512.02386
- Source URL: https://arxiv.org/abs/2512.02386
- Reference count: 40
- Key result: CT-RS-q achieves cumulative returns of 0.8163 vs 0.2217 baseline in portfolio selection with mean-variance objectives

## Executive Summary
This paper introduces CT-RS-q, a risk-sensitive Q-learning algorithm for continuous-time reinforcement learning with nonlinear objectives. The algorithm addresses the challenge of learning optimal policies when the objective is a risk measure that depends on the entire distribution of cumulative rewards. By leveraging martingale characterization and the structure of optimized certainty equivalents (OCEs), CT-RS-q enables on-policy temporal-difference learning in continuous time while tracking both state dynamics and cumulative rewards.

## Method Summary
CT-RS-q extends Q-learning to continuous-time settings with risk-sensitive objectives. The key insight is that when the risk measure is an OCE, the optimal policy is Markovian with respect to an augmented state space that includes cumulative rewards and discounting effects. The algorithm uses martingale characterization of value functions to enable temporal-difference learning without requiring full knowledge of reward distributions. The method operates in continuous time and maintains an augmented state representation that tracks both the physical state and cumulative reward information needed for risk evaluation.

## Key Results
- Achieved cumulative returns of 0.8163 vs 0.2217 baseline in dynamic portfolio selection
- Mean-variance objective of 1.4365 vs 1.2171 baseline
- Performance closely matches theoretical optimal policy (0.7128 return, 1.4532 objective)

## Why This Works (Mechanism)
The algorithm works by exploiting the mathematical structure of optimized certainty equivalents (OCEs). When risk measures are OCEs, they have special properties that allow the value function to be characterized as a martingale. This enables temporal-difference learning without requiring explicit knowledge of reward distributions. The augmented state space tracks both the physical state and cumulative rewards, allowing the algorithm to make risk-sensitive decisions while maintaining the Markov property necessary for Q-learning.

## Foundational Learning

**Martingale characterization of value functions**: Why needed - Enables temporal-difference learning without explicit reward distribution knowledge. Quick check - Verify the stochastic integral of the value process has zero drift.

**Optimized certainty equivalents (OCEs)**: Why needed - Provides tractable risk measures with Markovian optimal policies. Quick check - Confirm the risk measure satisfies the OCE conditions (convexity, translation invariance).

**Continuous-time Q-learning**: Why needed - Handles the continuous-time dynamics of many real-world control problems. Quick check - Ensure learning updates maintain consistency with the Bellman equation in continuous time.

**Augmented state space**: Why needed - Tracks cumulative rewards necessary for risk evaluation while preserving Markov property. Quick check - Verify the augmented state space is sufficient for determining optimal actions.

## Architecture Onboarding

Component map: Environment -> Augmented State Space -> Q-function -> Policy -> Actions

Critical path: State observation → Cumulative reward tracking → Q-value update → Policy selection

Design tradeoffs: The algorithm trades computational complexity for the ability to handle continuous-time dynamics and risk-sensitive objectives. The augmented state space increases memory requirements but enables proper risk evaluation.

Failure signatures: Poor performance may indicate inadequate tracking of cumulative rewards, incorrect discretization of continuous-time updates, or violation of OCE assumptions.

First experiments:
1. Test learning in a simple continuous-time environment with known optimal policy
2. Validate cumulative reward tracking accuracy under different discretization schemes
3. Evaluate sensitivity to initial Q-value estimates and learning rate parameters

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical assumptions assume continuous-time dynamics and perfect observability of cumulative rewards
- Performance relies heavily on risk measure being an optimized certainty equivalent
- Empirical validation limited to single portfolio selection environment

## Confidence
- **High confidence**: Martingale characterization of value functions for OCE risk measures is mathematically sound
- **Medium confidence**: Continuous-time Q-learning updates are theoretically valid, though discretization effects need further study
- **Medium confidence**: Portfolio selection results are promising but limited to single simulation setup

## Next Checks
1. Test algorithm robustness under different discretization schemes and observation frequencies
2. Evaluate performance across multiple continuous-time environments beyond portfolio selection
3. Analyze sensitivity to hyperparameters and cumulative reward tracking accuracy in noisy settings