---
ver: rpa2
title: 'MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety'
arxiv_id: '2602.01539'
source_url: https://arxiv.org/abs/2602.01539
tags:
- attacker
- defender
- safety
- adversarial
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAGIC is a multi-turn multi-agent reinforcement learning framework
  that formulates LLM safety alignment as an adversarial asymmetric game between an
  attacker and a defender. The attacker learns to iteratively rewrite harmful queries
  into deceptive prompts, while the defender simultaneously optimizes to recognize
  and refuse such inputs, triggering co-evolution.
---

# MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety

## Quick Facts
- arXiv ID: 2602.01539
- Source URL: https://arxiv.org/abs/2602.01539
- Authors: Xiaoyu Wen; Zhida He; Han Qi; Ziyu Wan; Zhongtian Ma; Ying Wen; Tianhang Zheng; Xingcheng Xu; Chaochao Lu; Qiaosheng Zhang
- Reference count: 40
- Multi-turn multi-agent RL framework achieving strong defense success rates while preserving helpfulness

## Executive Summary
MAGIC is a novel framework that formulates LLM safety alignment as an adversarial asymmetric game between an attacker and a defender. The attacker learns to rewrite harmful queries into deceptive prompts, while the defender simultaneously learns to recognize and refuse such inputs, triggering co-evolution. The framework constructs an Attack Pool Benchmark enriched with Chain-of-Thought completions across 20 rewriting strategies to address data scarcity. Extensive experiments demonstrate that MAGIC significantly improves defense success rates across both single-turn and multi-turn benchmarks while maintaining model helpfulness.

## Method Summary
MAGIC employs a multi-turn multi-agent reinforcement learning framework where an attacker and defender are trained simultaneously. The attacker's goal is to rewrite harmful queries into deceptive prompts that bypass safety filters, while the defender aims to recognize and refuse these inputs. The framework uses a game-theoretic approach where both agents evolve together, with the attacker's strategy space including combinatorial attacks derived from 20 different rewriting strategies. To address the scarcity of adversarial data, MAGIC constructs an Attack Pool Benchmark enriched with Chain-of-Thought completions. The training process is guided by a reward model (Qwen3Guard) that evaluates safety compliance, and the framework demonstrates theoretical guarantees for stronger pointwise safety compared to previous methods.

## Key Results
- MAGIC significantly improves defense success rates across single-turn and multi-turn benchmarks while preserving model helpfulness
- The attacker evolves novel combinatorial attack strategies during training that weren't explicitly programmed
- MAGIC generalizes well to unseen attack patterns without requiring multi-turn supervised fine-tuning
- Theoretical analysis shows the game equilibrium provides stronger pointwise safety guarantees than previous methods

## Why This Works (Mechanism)
MAGIC works by framing LLM safety as a competitive game between two agents that co-evolve. The attacker learns to craft increasingly sophisticated deceptive prompts by combining existing rewriting strategies, while the defender simultaneously learns to recognize and block these evolving threats. This adversarial setup creates a natural selection pressure where both agents continuously improve in response to each other. The co-evolutionary dynamic ensures that the defender doesn't overfit to specific attack patterns but instead develops more robust, generalizable safety mechanisms. The use of Chain-of-Thought completions in the Attack Pool Benchmark provides richer context for the attacker to learn from, enabling more nuanced attack strategies.

## Foundational Learning
- **Adversarial Training**: Understanding how competing agents can be used to improve model robustness through iterative refinement
- **Reinforcement Learning Multi-Agent Systems**: How multiple agents with competing objectives can be trained simultaneously in a shared environment
- **Game Theory Equilibrium Analysis**: The theoretical foundation for why co-evolutionary training leads to stronger safety guarantees
- **Chain-of-Thought Reasoning**: How structured reasoning processes can be leveraged to generate more sophisticated adversarial examples
- **Reward Modeling for Safety**: How safety-specific reward models can guide both attack and defense strategies
- **Asymmetric Game Dynamics**: Understanding how unbalanced agent objectives can still lead to stable training outcomes

## Architecture Onboarding

**Component Map:** Attacker Agent -> Reward Model (Qwen3Guard) -> Defender Agent -> Environment (Attack Pool Benchmark)

**Critical Path:** Attacker generates deceptive prompt → Reward model evaluates safety compliance → Defender decides to accept/refuse → Feedback loop updates both agents

**Design Tradeoffs:** The framework trades computational efficiency for stronger safety guarantees through co-evolutionary training. The choice of Qwen3Guard as reward model provides domain-specific safety evaluation but may introduce bias compared to more general models like GPT-4o.

**Failure Signatures:** Over-specialization to training attack patterns, collapse of attacker exploration leading to limited strategy diversity, reward model bias affecting both training dynamics and evaluation outcomes.

**First Experiments:**
1. Ablation study comparing MAGIC with base model attacker vs. SFT-initialized attacker
2. Cross-validation using multiple independent reward/judge models
3. Stress test with adversarial actors outside the research team

## Open Questions the Paper Calls Out
- Can the MAGIC framework be effectively extended to multimodal or tool-use settings while maintaining co-evolutionary stability?
- How does the choice of reward model (e.g., Qwen3Guard vs. GPT-4o) systematically bias both training dynamics and final evaluation outcomes?
- What is the relationship between SFT initialization quality and the attacker's ability to discover novel combinatorial attack strategies during RL co-evolution?

## Limitations
- Scalability concerns beyond controlled environments and curated benchmarks
- Potential biases introduced by Chain-of-Thought completions in synthetic data generation
- Theoretical equilibrium assumptions may not hold under all practical conditions
- Computational overhead may limit deployment in resource-constrained settings

## Confidence

**High Confidence:** Experimental methodology is rigorous with proper ablation studies and comparative analysis against baselines; multi-turn evaluation framework appears sound; preservation of model helpfulness while improving safety metrics is well-documented.

**Medium Confidence:** Theoretical analysis provides useful insights but makes simplifying assumptions that may not fully capture real-world dynamics; mathematical guarantees depend on conditions that may not always hold in practice.

**Low Confidence:** Generalizability of attack strategies beyond curated dataset remains uncertain; true ability to generate novel, effective attacks in the wild needs validation.

## Next Checks
1. Conduct stress tests with adversarial actors outside the research team to evaluate real-world robustness and identify potential blind spots in the defense mechanism.
2. Perform resource efficiency analysis comparing MAGIC's computational overhead against baseline methods under realistic deployment constraints.
3. Extend evaluation to cross-domain scenarios where attack patterns significantly differ from the training distribution to assess true generalization capabilities.