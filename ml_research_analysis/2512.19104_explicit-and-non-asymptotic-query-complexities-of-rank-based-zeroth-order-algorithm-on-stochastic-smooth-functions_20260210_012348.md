---
ver: rpa2
title: Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm
  on Stochastic Smooth Functions
arxiv_id: '2512.19104'
source_url: https://arxiv.org/abs/2512.19104
tags:
- lemma
- stochastic
- obtain
- holds
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zeroth-order optimization using only ordinal
  feedback in stochastic settings. The key idea is a rank-based algorithm that selects
  descent directions from ordered function evaluations without constructing costly
  comparison graphs.
---

# Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions

## Quick Facts
- arXiv ID: 2512.19104
- Source URL: https://arxiv.org/abs/2512.19104
- Reference count: 7
- Key outcome: First explicit non-asymptotic query complexity bounds O(dLG_u²/μ²ε) for convex and O(dLG_u²/ε²) for nonconvex stochastic rank-based ZO optimization.

## Executive Summary
This paper introduces a rank-based zeroth-order optimization algorithm for stochastic smooth functions that achieves query complexities matching value-based approaches. The method constructs gradient estimates using only ordinal feedback (function rankings) without building comparison graphs. Under standard smoothness and strong convexity assumptions plus bounded variance, the algorithm achieves O(dLG_u²/μ²ε) query complexity for convex objectives and O(dLG_u²/ε²) for finding ε-stationary points in nonconvex cases. These are the first explicit non-asymptotic query complexity bounds for rank-based ZO methods on stochastic functions, proving ordinal information alone suffices for optimal efficiency.

## Method Summary
The algorithm samples N random Gaussian directions per iteration, queries function values at perturbed points via a rank oracle, and constructs descent directions by weighting the top and bottom quartiles of ranked directions. The smoothing parameter α is set based on problem constants, and step sizes adapt to stochastic gradient norms. For convex objectives, η_t = ‖∇f(x_t;ξ_t)‖/(2μt) achieves O(1/T) convergence; for nonconvex, η_t = √(N/T)·‖∇f(x_t;ξ_t)‖·√(Δf₁)/G_u achieves O(1/√T) convergence. The method requires only ordinal feedback while maintaining the same asymptotic query complexity as value-based zeroth-order methods.

## Key Results
- Achieves O(dLG_u²/μ²ε) query complexity for strongly convex objectives (optimality gap ≤ ε)
- Achieves O(dLG_u²/ε²) query complexity for nonconvex objectives (ε-stationary point)
- First explicit non-asymptotic query complexity bounds for rank-based ZO on stochastic functions
- Total per-iteration cost O(dN) vs O(dN²) for prior rank-based methods

## Why This Works (Mechanism)

### Mechanism 1
Ordinal feedback can construct gradient estimators matching value-based ZO efficiency. The algorithm samples N Gaussian directions, queries points via rank oracle, and weights top 25% and bottom 25% of ranked directions to form descent direction d_t. This exploits that lower-ranked directions tend to align with negative gradients. Core assumptions: L-smoothness, bounded stochastic gradient second moment, and lower bound on stochastic gradient norm for α setting. Evidence: Abstract claims ordinal information suffices for optimal efficiency; Algorithm 1 shows explicit weighting scheme; prior work established deterministic case complexity.

### Mechanism 2
Convergence guaranteed with high probability through event-based analysis. The proof defines five critical events bounding approximation errors and gradient alignment, using Gaussian concentration and union bounds. Core assumptions: Gaussian direction sampling, smoothness bounds, and bounded variance. Evidence: Lemmas 1-5 define events with probability bounds; Theorem 1 provides explicit probability statement; analysis simplifies prior rank-based work significantly.

### Mechanism 3
Adaptive step-sizes with stochastic gradient norm enable optimal convergence rates. For strongly convex objectives, η_t = ‖∇f(x_t;ξ_t)‖/(2μt) achieves O(1/T) convergence; for nonconvex, η_t = √(N/T)·‖∇f(x_t;ξ_t)‖·√(Δf₁)/G_u achieves O(1/√T). Core assumptions: Strong convexity constant μ, bounded gradient second moment G_u², and initial suboptimality knowledge. Evidence: Theorem 1 shows step-size choice and convergence bound; Corollaries 3-4 derive explicit query complexities; analysis departs from prior drift-based approaches.

## Foundational Learning

- **Concept**: Zeroth-Order Gradient Estimation via Finite Differences
  - Why needed here: Algorithm builds gradient estimator from function evaluations; understanding smoothing parameter α's bias-variance tradeoff is essential.
  - Quick check question: For an L-smooth function, how does the error in forward finite-difference gradient estimate scale with perturbation size α?

- **Concept**: Order Statistics of Gaussian Samples
  - Why needed here: Algorithm reliability depends on top/bottom quartiles aligning with gradient directions; understanding quantile concentration and KL divergence bounds is critical.
  - Quick check question: For N i.i.d. standard Gaussians, approximately what sample size ensures the 25th percentile is below -2 with probability ≥ 0.99?

- **Concept**: Stochastic Optimization Convergence Rates
  - Why needed here: Paper establishes O(1/T) for strongly convex and O(1/√T) for nonconvex; understanding why these rates are optimal and how step-size schedules achieve them is prerequisite.
  - Quick check question: Why does diminishing step-size η_t ∝ 1/t achieve O(1/T) convergence for strongly convex SGD, while constant step-size only achieves convergence to a neighborhood?

## Architecture Onboarding

- **Component map**: Parameter Server → Direction Sampler → Rank Oracle → Rank-Based Estimator → Step-Size Controller → Update Rule
- **Critical path**: 
  1. Initialize: Set α ≤ G_ℓ/(2C_{d,δ}L), choose N ≥ K(1/4‖p)^(-1)·log(40T)
  2. Per-iteration: Generate N Gaussian directions, query rank oracle, extract quartile indices, construct d_t, compute η_t, update x_{t+1} = x_t + η_t·d_t
  3. Terminate: After T = O(dLG_u²/(μ²ε)) iterations (convex) or T = O(dLG_u²/ε²) (nonconvex)
- **Design tradeoffs**: Larger N reduces iterations but increases per-iteration queries; total Q = TN asymptotically unchanged. Smaller α reduces bias but increases sensitivity to rank noise. O(dN) per iteration vs O(dN²) for prior methods.
- **Failure signatures**: Divergence (check η_t scaling or α upper bound violation); stagnation (check G_ℓ estimation or rank oracle quality); high variance (increase N for dimension d).
- **First 3 experiments**: 
  1. Quadratic validation: Implement on d-dimensional strongly convex quadratic with noise, sweep N ∈ {50, 100, 200}, measure convergence rate vs O(1/T).
  2. Nonconvex robustness: Apply to Rosenbrock with ranking errors, compare against ZO-Gaussian baseline.
  3. Hyperparameter sensitivity: On logistic regression with simulated rankings, sweep α × theoretical upper bound, plot suboptimality vs α.

## Open Questions the Paper Calls Out

### Open Question 1
Can the lower bound assumption on stochastic gradient norm (Assumption 4: ‖∇f(x;ξ)‖ ≥ G_ℓ) be removed while maintaining the same query complexity guarantees? The paper notes this assumption is uncommon in SGD analysis and only used to set smoothing parameter α. An adaptive smoothing scheme or alternative analysis not requiring this assumption would resolve this.

### Open Question 2
What are the precise trade-offs between feedback richness (pairwise comparisons vs partial rankings vs full rankings), query complexity, and convergence rates in zeroth-order optimization? While ordinal information matches value-based efficiency, the paper doesn't quantify how much ranking information is actually necessary. Lower bounds characterizing minimal ranking granularity needed would resolve this.

### Open Question 3
Is the uniform weighting scheme (w^+(k) = 4/N, w^-(k) = -4/N) on top and bottom quartiles optimal for all problem classes, or can adaptive or non-uniform weightings improve convergence? The paper notes this strategy is popular but provides no optimality analysis. Ablation studies comparing weighting strategies would resolve this.

## Limitations
- Relies on uncommon Assumption 4 (bounded stochastic gradient norm from below) for setting smoothing parameter
- Requires careful hyperparameter tuning dependent on problem-specific constants that may be difficult to estimate
- Purely theoretical without empirical validation on concrete problems
- Assumes access to reliable rank oracle providing coherent ordinal feedback

## Confidence
- **High confidence**: Explicit query complexity bounds O(dLG_u²/μ²ε) and O(dLG_u²/ε²) are mathematically derived and follow standard stochastic optimization techniques. Improvement over prior rank-based methods is clearly established.
- **Medium confidence**: Rank-based gradient estimator's effectiveness relies on order statistics of Gaussian projections, which is theoretically sound but may be sensitive to real-world noise. Step-size adaptation using stochastic gradient norms is novel but depends on strong assumptions.
- **Low confidence**: Practical applicability of Assumption 4 and robustness to its violations remain unclear. Without experimental validation, performance with real ordinal feedback is uncertain.

## Next Checks
1. **Empirical rate verification**: Implement on 100-dimensional logistic regression with simulated preference rankings. Measure convergence rate and query complexity vs theoretical predictions O(1/T) for convex case.
2. **Robustness to ranking noise**: Add random pairwise comparison errors to rank oracle and measure degradation in convergence. Compare against standard ZO-Gaussian smoothing baseline under same noise levels.
3. **Hyperparameter sensitivity analysis**: On synthetic strongly convex quadratic, systematically vary α around theoretical upper bound G_ℓ/(2C_{d,δ}L). Plot final suboptimality vs α to validate safety margin and identify practical ranges.