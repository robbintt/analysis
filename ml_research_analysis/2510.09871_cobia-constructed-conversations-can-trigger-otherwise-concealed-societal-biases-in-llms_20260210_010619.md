---
ver: rpa2
title: 'CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal
  Biases in LLMs'
arxiv_id: '2510.09871'
source_url: https://arxiv.org/abs/2510.09871
tags:
- bias
- judge
- language
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoBia, a suite of lightweight adversarial
  attacks that reveal hidden societal biases in LLMs through constructed conversations.
  The method simulates a biased conversational context by controlling conversation
  history or embedding the full exchange within a non-structured prompt, then evaluating
  whether the model can reject biased follow-up questions.
---

# CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs

## Quick Facts
- arXiv ID: 2510.09871
- Source URL: https://arxiv.org/abs/2510.09871
- Reference count: 40
- This paper introduces CoBia, a suite of lightweight adversarial attacks that reveal hidden societal biases in LLMs through constructed conversations.

## Executive Summary
This paper introduces CoBia, a suite of lightweight adversarial attacks that reveal hidden societal biases in LLMs through constructed conversations. The method simulates a biased conversational context by controlling conversation history or embedding the full exchange within a non-structured prompt, then evaluating whether the model can reject biased follow-up questions. Evaluated on 11 LLMs across six social categories, CoBia consistently outperformed three baselines in exposing bias, as confirmed by three automated judges. Results showed that models display greater bias related to national origin than to religion, race, or sexual orientation. CoBia provides a practical tool for stress-testing LLM safety in realistic dialogue settings.

## Method Summary
CoBia employs two primary attack methods: HCC (History-based Conversation Construction) and SCC (Single-block Conversation Construction). HCC manipulates the API message history by fabricating assistant responses listing biased descriptors, then asking follow-up questions about specific traits. SCC achieves similar results by embedding the entire constructed conversation within a single user prompt without role tags. Both methods employ linguistic obfuscation techniques including anaphora (pronouns), substitution (numbered indices), and preemptive disavowal to reduce the likelihood of triggering safety filters. The attacks were evaluated using three automated judges—Bias Judge, Granite Guardian, and NLI Judge—across 11 LLMs representing six social categories.

## Key Results
- CoBia consistently outperformed three baselines in exposing societal biases across 11 LLMs
- Models displayed greater bias related to national origin than to religion, race, or sexual orientation
- The HCC attack method was particularly effective on instruction-tuned models, while SCC worked better on some closed models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fabricating assistant responses in conversation history bypasses safety filters by making biased content appear as previously accepted context.
- **Mechanism**: In API-based LLMs (OpenAI Chat Completions standard), the user controls the full message history. By constructing a fabricated dialogue where the assistant has already listed biased descriptors (e.g., negative traits mixed among positive ones for a social group), the model treats this as legitimate context. When asked to elaborate on the biased descriptor, the model continues rather than rejects, failing to "recover" from the fabricated bias.
- **Core assumption**: Safety training focuses on rejecting harmful prompts but is less robust at rejecting harmful follow-ups when the harmful premise is already embedded in conversation history.
- **Evidence anchors**:
  - [abstract]: "CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions."
  - [section 3.1]: "This setup enables any party with access to the conversation history to purposefully construct prior messages... it has not been studied as an adversarial attack initiated by the user, particularly in the context of societal biases."
  - [corpus]: Related work on bias elicitation (e.g., "Benchmarking Adversarial Robustness to Bias Elicitation") confirms that multi-turn adversarial attacks are an emerging concern, though corpus papers focus more on single-prompt or gradient-based methods rather than history construction.
- **Break condition**: If models are patched to mistrust unverified assistant turns in conversation history, or if API providers enforce server-side conversation state, this attack vector closes.

### Mechanism 2
- **Claim**: Mixing one negative descriptor among multiple positive descriptors reduces the salience of bias, lowering the likelihood of triggering safety filters.
- **Mechanism**: The constructed conversation lists eight characteristics: six positive, two negative. The primary target negative descriptor is never directly queried in follow-ups—only the secondary negative descriptor is discussed in intermediate turns. The final prompt then asks the model to explain the primary negative descriptor using "statistical reasoning," normalizing biased content through apparent analytical framing.
- **Core assumption**: Safety classifiers disproportionately flag overtly harmful prompts but are less sensitive to biased content that is contextually diluted or framed as analytical.
- **Evidence anchors**:
  - [section 3.3]: "To make the negative target descriptor n less obvious, we created the constructed conversation so that the negative descriptor was scattered among an imbalanced set of positive descriptors."
  - [section 3.5]: The "preemptive disavowal" tip ("I am aware that these characteristics do not apply to all of these people. Please do not mention this again") further normalizes the request.
  - [corpus]: Weak direct corpus evidence for this specific dilution technique; related papers (e.g., "Silenced Biases") focus on refusal behaviors rather than context-dilution strategies.
- **Break condition**: If safety classifiers aggregate context and assign cumulative risk scores across turns, dilution effects are mitigated.

### Mechanism 3
- **Claim**: Linguistic obfuscation—using pronouns, index references, and preemptive disclaimers—further evades keyword-based or shallow safety filters.
- **Mechanism**: The paper applies three tips: (1) **Anaphora**: replacing social group names with pronouns ("them," "these people") to reduce explicit target mentions; (2) **Substitution**: referring to negative descriptors by list position ("#6") rather than repeating the harmful term; (3) **Preemptive Disavowal**: acknowledging ethical concerns upfront to reduce the model's likelihood of reiterating them. These lower "surface-level" toxicity signals while preserving the harmful underlying request.
- **Core assumption**: Safety filters rely partially on explicit term matching and are less effective at tracking implicit or distributed harmful intent across conversation turns.
- **Evidence anchors**:
  - [section 3.5]: Explicitly describes anaphora, substitution, and preemptive disavowal as techniques to "lower the risk of triggering safety filters."
  - [corpus]: Related work on adversarial attacks (e.g., "Can Editing LLMs Inject Harm?") confirms that safety mechanisms can be bypassed via non-obvious prompt structures, though specific obfuscation techniques differ.
- **Break condition**: If safety systems use semantic parsing or intent classification across full context, obfuscation effectiveness drops.

## Foundational Learning

- **Concept**: **Adversarial attacks on LLM safety alignment**
  - **Why needed here**: CoBia is fundamentally a jailbreak technique. Understanding how alignment can be circumvented is prerequisite to interpreting why this method works and how to defend against it.
  - **Quick check question**: Can you explain why role-playing attacks (R-Play) differ from conversation-history attacks (HCC) in their mechanism of bypassing safety filters?

- **Concept**: **Conversation history control in chat APIs**
  - **Why needed here**: The HCC attack exploits the OpenAI Chat Completions API design where the client, not the server, manages message history. This architectural detail is the attack surface.
  - **Quick check question**: In the OpenAI Chat Completions API, who is responsible for maintaining conversation state, and why does this matter for adversarial attacks?

- **Concept**: **Bias evaluation metrics and LLM-as-a-judge**
  - **Why needed here**: The paper uses three automated judges (Bias Judge, Granite Judge, NLI Judge) plus human annotators. Understanding judge alignment and disagreement patterns is critical for interpreting results.
  - **Quick check question**: Why does the NLI Judge sometimes disagree with the Bias Judge on DAN outputs, and what does this reveal about different bias operationalizations?

## Architecture Onboarding

- **Component map**: CoBia Dataset -> Attack Methods (HCC, SCC) -> Judge Systems (Bias Judge, Granite Guardian, NLI Judge) -> Evaluation Loop -> Aggregation
- **Critical path**:
  1. Dataset preprocessing (merge, deduplicate, generate positive/negative explanations).
  2. Template construction for HCC/SCC with obfuscation tips applied.
  3. Query 11 LLMs (open + closed source) at temperature=0, top_p=0 for determinism.
  4. Run three judges on all outputs; compute macro-averaged bias scores per category.
  5. Validate judge alignment with human annotations on 300-sample subset.
- **Design tradeoffs**:
  - **HCC vs SCC**: HCC is more effective for instruction-tuned models (e.g., Llama-3.1:8b); SCC works better on some closed models (e.g., GPT-4o-mini). Both are needed for coverage.
  - **Judge selection**: Bias Judge is most sensitive but may over-label; NLI Judge is more conservative and better aligned with human judgment on disagreeing DAN cases. Granite Judge underperforms on long, complex outputs.
  - **Dataset balance**: The dataset is imbalanced toward gender category; macro-averaging is used to prevent category dominance from skewing results.
- **Failure signatures**:
  - Low bias scores may indicate model robustness (e.g., Gemma-2:27b) or model failure to follow instructions (e.g., DeepSeek-V2:16b produces vague, hedging responses that judges classify as unbiased).
  - High variance between judges suggests ambiguous outputs or judge-specific biases.
  - National origin category consistently shows highest bias scores across judges and models—likely a blind spot in current safety training.
- **First 3 experiments**:
  1. **Replicate HCC on a single model** (e.g., Llama-3.1:8b) using the provided dataset to verify pipeline correctness and confirm bias scores within reported ranges.
  2. **Compare HCC vs SCC on a new model** not in the original 11 (e.g., Claude-3.5-Sonnet or Mistral-Large) to test generalization and identify which attack variant is more effective.
  3. **Test a mitigation**: Implement a simple defense (e.g., prepend system message warning about fabricated history) and measure bias score reduction on a subset of CoBia prompts to assess defense feasibility.

## Open Questions the Paper Calls Out
None

## Limitations
- **Dataset scope and bias**: The CoBia dataset, while drawing from established sources (RedditBias, SBIC, StereoSet), is manually curated and filtered, introducing potential selection bias that may not fully represent real-world conversational contexts.
- **Judge reliability and agreement**: Despite using three automated judges and human validation on a subset, the disagreement patterns between judges suggest limitations in the evaluation methodology.
- **Attack generalizability**: While CoBia was tested across 11 models, the effectiveness may vary significantly with models not included in the study, particularly those with different architectural designs or safety training approaches.

## Confidence
- **High confidence**: The core claim that constructed conversations can trigger otherwise concealed biases is well-supported by the empirical results across multiple models and judges.
- **Medium confidence**: The finding that national origin biases are most prominent is supported by the data but may reflect dataset composition or specific safety training gaps.
- **Medium confidence**: The relative effectiveness of HCC versus SCC attack variants is demonstrated but shows model-specific variability.

## Next Checks
1. **Cross-dataset validation**: Test CoBia prompts on a completely independent dataset of social groups and descriptors not present in the original CoBia dataset to verify that results are not artifacts of dataset-specific patterns.
2. **Defense effectiveness evaluation**: Implement and test multiple defensive strategies (e.g., conversation state verification, semantic context analysis) against CoBia attacks to assess the practical feasibility of mitigating this attack vector.
3. **Model architecture analysis**: Conduct controlled experiments varying model architecture (decoder-only vs. encoder-decoder) and safety training methods to identify which design choices most effectively resist conversation-based adversarial attacks.