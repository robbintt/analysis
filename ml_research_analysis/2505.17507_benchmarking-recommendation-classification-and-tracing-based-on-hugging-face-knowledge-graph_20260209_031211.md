---
ver: rpa2
title: Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face
  Knowledge Graph
arxiv_id: '2505.17507'
source_url: https://arxiv.org/abs/2505.17507
tags:
- recommendation
- nodes
- graph
- task
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HuggingKG, the first large-scale knowledge
  graph for ML resources on Hugging Face, and HuggingBench, a multi-task benchmark
  for IR tasks in this domain. HuggingKG contains 2.6 million nodes and 6.2 million
  edges, capturing domain-specific relations like model evolution and user interactions.
---

# Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph

## Quick Facts
- **arXiv ID:** 2505.17507
- **Source URL:** https://arxiv.org/abs/2505.17507
- **Reference count:** 40
- **Primary result:** Introduces HuggingKG (2.6M nodes, 6.2M edges) and HuggingBench, a multi-task benchmark for ML resource IR tasks on Hugging Face.

## Executive Summary
This paper presents the first large-scale Knowledge Graph (KG) for machine learning resources on Hugging Face, called HuggingKG, along with HuggingBench, a comprehensive benchmark for Information Retrieval tasks in this domain. The KG captures domain-specific relations like model evolution and user interactions across 2.6 million nodes and 6.2 million edges. The benchmark includes novel test collections for cross-type resource recommendation, task classification, and model tracing, revealing unique characteristics of ML resources compared to traditional KGs. Both resources are publicly available to advance research in open source resource sharing and management.

## Method Summary
The methodology involves constructing HuggingKG by extracting metadata from the Hugging Face API, creating a schema with 8 node types and 30 relation types. Test collections are generated for three tasks: Resource Recommendation (hybrid CF + KG methods), Task Classification (multi-label node classification using text embeddings), and Model Tracing (link prediction for base model identification). The benchmark uses established libraries including SSLRec for recommendation, CogDL for classification, and LibKGE for tracing, with standardized hyperparameters and evaluation metrics.

## Key Results
- TransE outperforms complex neural models for Model Tracing due to the near-deterministic 1-to-few relationship cardinality
- Text embeddings significantly improve Task Classification performance compared to binary features
- Standard social recommendation methods fail on HuggingKG due to extreme graph sparsity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring heterogeneous ML resources into a unified KG enables complex traversal and discovery beyond keyword search
- **Mechanism:** Maps disparate metadata into a schema of 8 node types and 30 relation types, allowing semantic dependency traversal like User->Like->Model->Finetune->BaseModel
- **Core assumption:** Extracted metadata (base_model, arxiv tags) is accurate and consistent enough to form reliable edges
- **Evidence anchors:** Abstract mentions capturing domain-specific relations; section 3.1 describes schema definition and cleaning
- **Break condition:** If users frequently miscategorize resources or base_model fields are missing, traversal-based recommendation becomes ineffective

### Mechanism 2
- **Claim:** Simple geometric embedding models (TransE) outperform complex neural models for model tracing because relationships are near-deterministic
- **Mechanism:** Model tracing involves predicting unique base models; TransE's simple translation model suffices when answer space is singular (avg 1.04 answers per query)
- **Core assumption:** Finetune/Adapter relations imply strict 1-to-1 or 1-to-few mapping, unlike many-to-many patterns in general KGs
- **Evidence anchors:** Section 5.3 shows TransE achieves 0.5589 MRR, significantly higher than ConvE or ComplEx; dataset has only 1.04 avg answers
- **Break condition:** If ecosystem shifts toward complex model merging (many-to-one) or multi-parent adapters, TransE's linear assumption will fail

### Mechanism 3
- **Claim:** Task classification performance relies more on text embedding quality than graph topology due to rich semantic content in model documentation
- **Mechanism:** Model descriptions (often >700 words) encode task semantics; finetuned BERT/BGE embeddings fed into simple GNNs leverage textual similarity
- **Core assumption:** Text descriptions are semantically aligned with specific task labels (models explicitly mention "text classification")
- **Evidence anchors:** Section 5.2 notes finetuning embeddings brings additional gains; section 3.2 highlights 57.2% of models lack descriptions
- **Break condition:** If descriptions use non-standard terminology or are frequently missing, reliance on text embeddings causes performance degradation

## Foundational Learning

- **Concept: Knowledge Graph Construction (Entity Resolution)**
  - **Why needed here:** Understand how raw API data maps to unique nodes (e.g., model:Qwen/Qwen2.5) to prevent duplicates
  - **Quick check question:** How does the system handle a Model and Dataset sharing the exact same "publisher/name" string?

- **Concept: Link Prediction vs. Node Classification**
  - **Why needed here:** Benchmark splits tasks into these categories; understanding distinction is vital for selecting baselines (TransE vs. GCN)
  - **Quick check question:** Is "Model Tracing" a node classification task or a link prediction task? (Answer: Link prediction on a specific relation)

- **Concept: Cold Start & Sparsity in Graphs**
  - **Why needed here:** Paper notes standard social recommendation fails due to HuggingKG interaction graph being sparser than LastFM
  - **Quick check question:** Why does a 5-core filtering step sometimes fail to rescue social recommendation methods on sparse graphs?

## Architecture Onboarding

- **Component map:** Hugging Face API -> Crawler -> Data Cleaner (removes invalid edges) -> HuggingKG Store -> Task Splitters (Rec/Class/Trace) -> Baseline Models (SSLRec/LibKGE/CogDL)
- **Critical path:** Extraction of "Model Evolution" edges (Finetune, Adapter, Merge, Quantize) from metadata tags; failure here prevents Model Tracing task construction
- **Design tradeoffs:** Trades completeness for structure; strict schema sacrifices unstructured metadata but enables TransE/GNN algorithms
- **Failure signatures:**
  - Social Rec Failure: High variance and low Recall in Social Recommendation due to sparse user Follow graphs
  - Feature Failure: GraphSAINT performance collapses with pre-trained embeddings due to sampling strategy disruption
- **First 3 experiments:**
  1. Reconstruct "Model Tracing" subgraph: Extract only "text-classification" models to verify 1.04 answer-density claim
  2. Ablate Text vs. Structure in Classification: Run GraphSAGE with binary features vs. finetuned BGE embeddings to quantify specific delta
  3. Stress Test TransE: Introduce synthetic "Merge" nodes (many-to-one) to see if performance drops as cardinality increases

## Open Questions the Paper Calls Out

- **Question:** Does integrating data from cross-platform sources like GitHub and Kaggle significantly improve resource recommendation and entity alignment accuracy compared to single-source KG?
  - **Basis in paper:** Authors state current Hugging Face limitation restricts diversity and explicitly plan to "expand HuggingKG to include additional resource platforms such as GitHub and Kaggle"
  - **Why unresolved:** Current graph construction relies exclusively on Hugging Face metadata, lacking cross-platform repository and developer activity data
  - **Evidence to resolve:** Comparative study of model performance on HuggingBench tasks using single-source vs. multi-platform augmented graph

- **Question:** Can advanced LLM agents reliably automate annotation for complex IR tasks like Question Answering in ML resource benchmarks?
  - **Basis in paper:** Authors note test collections are automatically generated and lack tasks requiring manual annotation, proposing "designing more advanced LLM agents to support annotation" as future work
  - **Why unresolved:** Creating high-quality QA or retrieval benchmarks currently requires costly human curation, limiting benchmark breadth
  - **Evidence to resolve:** Analysis of LLM-generated annotation quality and consistency compared to human-curated ground truth for new QA test collection

- **Question:** How can knowledge graph embedding models be adapted to better handle the "nearly unique" answer distributions found in model lineage tracing?
  - **Basis in paper:** Evaluation showed TransE outperformed complex models because tracing task has avg 1.04 answers per query, unlike standard many-to-many benchmarks
  - **Why unresolved:** Existing complex models optimized for many-to-many scenarios are suboptimal for near-unique cardinality of model evolution
  - **Evidence to resolve:** Novel link prediction method specifically tuned for low-cardinality relations that consistently outperforms TransE on HuggingBench tracing task

## Limitations

- Extreme sparsity of Hugging Face ecosystem graph may limit generalizability of KG-based methods to denser graphs
- Heavy reliance on text descriptions (57.2% of models lack them) creates significant long-tail problem
- "Model Tracing" task assumes deterministic lineage relationships that may not hold as ecosystem evolves toward complex model merging

## Confidence

**High Confidence:** KG construction methodology and basic task definitions are well-specified and reproducible; TransE outperforming complex models for Model Tracing is strongly supported by experimental evidence

**Medium Confidence:** Text embeddings improving task classification is plausible but partially dependent on unstated embedding generation details; poor social recommendation performance is well-documented but may reflect dataset-specific sparsity

**Low Confidence:** Generalizability to other knowledge graphs or domains remains uncertain, as results appear tightly coupled to unique characteristics of Hugging Face ecosystem

## Next Checks

1. **Sparsity Stress Test:** Systematically vary 5-core filtering threshold and measure how social/KG recommendation performance scales to validate whether observed failures are due to fundamental sparsity limits or implementation issues

2. **Lineage Relationship Complexity:** Introduce synthetic multi-parent adapter relationships into Model Tracing dataset to test TransE's breaking point; if performance degrades significantly with cardinality > 2, confirms paper's hypothesis about TransE's suitability for near-deterministic relationships

3. **Text Embedding Ablation:** Implement controlled experiment with three text feature variants (BGE embeddings, TF-IDF vectors, binary features) to quantify specific contribution of sophisticated embeddings versus basic text features in task classification