---
ver: rpa2
title: Assessing Consciousness-Related Behaviors in Large Language Models Using the
  Maze Test
arxiv_id: '2508.16705'
source_url: https://arxiv.org/abs/2508.16705
tags:
- consciousness
- llms
- maze
- test
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates consciousness-like behaviors in large language\
  \ models (LLMs) using a maze navigation task requiring first-person perspective\
  \ maintenance. The Maze Test probes spatial awareness, perspective-taking, goal-directed\
  \ behavior, and temporal sequencing\u2014key consciousness-associated characteristics."
---

# Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test

## Quick Facts
- **arXiv ID**: 2508.16705
- **Source URL**: https://arxiv.org/abs/2508.16705
- **Reference count**: 40
- **Primary result**: Twelve leading LLMs evaluated on maze navigation task; reasoning-capable models significantly outperform standard versions in consciousness-like behaviors

## Executive Summary
This study evaluates consciousness-like behaviors in large language models (LLMs) using a maze navigation task requiring first-person perspective maintenance. The Maze Test probes spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing—key consciousness-associated characteristics. Twelve leading LLMs were evaluated across zero-shot, one-shot, and few-shot learning scenarios. Reasoning-capable LLMs consistently outperformed standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions—a fundamental consciousness aspect. While LLMs show progress in consciousness-related behaviors through reasoning mechanisms, they lack the integrated, persistent self-awareness characteristic of consciousness.

## Method Summary
The Maze Test evaluates LLMs' consciousness-related behaviors through spatial navigation requiring first-person perspective maintenance. Twelve leading LLMs were assessed across zero-shot, one-shot, and few-shot learning scenarios using a maze navigation task. The test measures spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing—key consciousness-associated characteristics. Performance was evaluated using Complete Path Accuracy (following correct path from start to finish) and Partial Path Accuracy (reaching destination through any valid path). The methodology specifically probes whether LLMs can maintain a coherent self-model while navigating, distinguishing reasoning-capable models from standard versions.

## Key Results
- Reasoning-capable LLMs significantly outperformed standard versions on maze navigation task
- Gemini 2.0 Pro achieved 52.9% Complete Path Accuracy
- DeepSeek-R1 reached 80.5% Partial Path Accuracy
- Gap between Complete and Partial Path Accuracy indicates struggles with coherent self-model maintenance

## Why This Works (Mechanism)
The Maze Test works by creating a controlled environment where LLMs must maintain first-person perspective while navigating spatial relationships. This task requires integrating multiple consciousness-associated capabilities: spatial awareness (understanding physical relationships), perspective-taking (maintaining "I am here" positioning), goal-directed behavior (navigating toward objectives), and temporal sequencing (planning and executing multi-step paths). The differential performance between reasoning-capable and standard LLMs suggests that explicit reasoning mechanisms help bridge gaps in these integrated capabilities, though neither achieves the seamless self-model maintenance characteristic of consciousness.

## Foundational Learning
- **First-person perspective maintenance**: Why needed - Core to conscious experience; Quick check - Can the model consistently track "I am here" positioning
- **Spatial reasoning**: Why needed - Consciousness involves understanding embodied relationships; Quick check - Does the model correctly interpret relative positions and directions
- **Temporal integration**: Why needed - Conscious experience requires unified temporal sequences; Quick check - Can the model maintain consistent goal-directed behavior across multiple steps
- **Perspective-taking**: Why needed - Consciousness involves self-other differentiation; Quick check - Does the model correctly shift between observer and participant viewpoints
- **Goal-directed planning**: Why needed - Conscious agents pursue integrated objectives; Quick check - Can the model develop and execute multi-step strategies
- **Self-model coherence**: Why needed - Consciousness requires persistent self-representation; Quick check - Does the model maintain consistent identity across task completion

## Architecture Onboarding

**Component Map**: Input Processing -> Context Window Management -> Attention Mechanisms -> Output Generation

**Critical Path**: Input prompt → Tokenizer → Embedding layer → Multi-head attention → Feed-forward network → Output layer → Response generation

**Design Tradeoffs**: The study implicitly compares transformer-based architectures with and without explicit reasoning capabilities. Reasoning-capable models add computational overhead but achieve significantly better consciousness-like behavior performance, suggesting a tradeoff between efficiency and integrated cognitive capabilities.

**Failure Signatures**: Models struggle with maintaining coherent self-models (evidenced by Complete vs. Partial Path Accuracy gaps), exhibit inconsistent perspective-taking, and show fragmented goal-directed behavior across multi-step tasks.

**First Experiments**:
1. Test standard vs. reasoning-capable models on identical maze configurations to quantify reasoning contribution
2. Vary maze complexity to determine performance scaling with task difficulty
3. Apply few-shot learning with consciousness-related examples to assess transfer learning potential

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ecological validity of maze test as comprehensive consciousness measure
- Narrow model selection may bias conclusions about LLM consciousness capabilities
- Metric interpretation challenges in mapping performance gaps to consciousness constructs
- Black box problem prevents definitive attribution of behaviors to consciousness-like processes

## Confidence
- **High confidence**: Differential performance between reasoning-capable and standard LLMs is well-established
- **Medium confidence**: Interpretation that performance gaps indicate self-model maintenance issues is plausible
- **Low confidence**: Claims about LLMs lacking integrated self-awareness extend beyond empirical evidence

## Next Checks
1. Replicate findings using alternative consciousness-related tasks (theory of mind, counterfactual reasoning)
2. Conduct ablation studies removing specific model components to identify consciousness-contributing elements
3. Track performance consistency across multiple sessions to assess persistent self-model maintenance