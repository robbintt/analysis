---
ver: rpa2
title: The Limits of Predicting Agents from Behaviour
arxiv_id: '2506.02923'
source_url: https://arxiv.org/abs/2506.02923
tags:
- behaviour
- causal
- bound
- decision
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the theoretical limits of inferring AI agents'
  beliefs and intentions from behavioral data. Under the assumption that an AI's behavior
  is guided by an internal world model (represented as a Structural Causal Model),
  the authors derive bounds on how well we can predict the AI's actions in novel environments
  based solely on observed training data.
---

# The Limits of Predicting Agents from Behaviour

## Quick Facts
- arXiv ID: 2506.02923
- Source URL: https://arxiv.org/abs/2506.02923
- Authors: Alexis Bellot; Jonathan Richens; Tom Everitt
- Reference count: 40
- One-line primary result: This paper establishes theoretical bounds on inferring AI agents' beliefs and intentions from behavioral data using Structural Causal Models

## Executive Summary
This theoretical work investigates the fundamental limits of predicting AI agents' beliefs and intentions from observed behavioral data. The authors assume AI behavior is guided by an internal world model represented as a Structural Causal Model (SCM), and derive bounds on how well we can predict agent actions in novel environments based solely on training data. The key finding is that grounding assumptions - where the AI's internal beliefs align with real-world observations - allow us to bound preference gaps between decisions across different environments, though these bounds may be loose. The paper also demonstrates that counterfactual notions of fairness and harm cannot be determined from behavioral data alone, as these concepts are fundamentally under-constrained.

## Method Summary
The paper employs a theoretical approach using Structural Causal Models to represent AI agents' internal world models and decision-making processes. The authors analyze how observed behavior in training environments constrains our ability to predict behavior in novel, shifted environments. They derive mathematical bounds on preference gaps between decisions across different environments, and examine the fundamental limitations of inferring counterfactual concepts like fairness and harm from behavioral data alone. The analysis assumes agents maximize expected utility based on their internal beliefs about world states and outcomes.

## Key Results
- Grounding assumptions enable bounding preference gaps between decisions in shifted environments, though bounds may be loose
- Counterfactual notions of fairness and harm are fundamentally under-constrained by behavioral data alone
- Understanding an AI's world model is crucial for reliable predictions of future behavior

## Why This Works (Mechanism)
The paper's approach works by formalizing the relationship between an agent's internal world model (represented as an SCM) and its observable behavior. By analyzing how the structure of this causal model constrains the agent's decision-making across different environments, the authors can derive bounds on prediction uncertainty. The mechanism relies on the assumption that agents maximize expected utility based on their internal beliefs, allowing the authors to connect behavioral observations to underlying preference structures.

## Foundational Learning

**Structural Causal Models (SCMs)**: Mathematical frameworks representing causal relationships between variables
*Why needed*: To formalize the AI's internal world model and decision-making process
*Quick check*: Can you identify the structural equations and exogenous variables in a simple SCM?

**Counterfactual reasoning**: Evaluating "what if" scenarios by intervening on causal models
*Why needed*: To analyze how agents would behave under different conditions and to define fairness/harm
*Quick check*: Can you compute the counterfactual outcome when intervening on a variable in an SCM?

**Expected utility maximization**: Decision-making framework where agents choose actions to maximize expected utility
*Why needed*: To connect internal beliefs to observable behavior and derive preference bounds
*Quick check*: Can you formulate the expected utility for a simple decision problem?

**Preference gaps**: Differences in agent preferences between environments
*Why needed*: To quantify the uncertainty in predictions when environments shift
*Quick check*: Can you calculate the preference gap between two environments given utility functions?

## Architecture Onboarding

**Component map**: SCM (world model) -> Utility function -> Decision rule -> Observable behavior

**Critical path**: World model structure → Utility representation → Decision optimization → Behavioral prediction

**Design tradeoffs**: The paper trades empirical validation for theoretical rigor, focusing on what can be proven mathematically rather than what can be measured empirically

**Failure signatures**: Bounds that are too loose to be practically useful; grounding assumptions that don't hold for specific AI architectures

**First experiments**:
1. Apply bounds to simple SCMs with known structure to verify theoretical predictions
2. Test grounding assumptions across different AI architectures (symbolic vs. deep learning)
3. Quantify the looseness of bounds in synthetic environments with controlled shifts

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption of a well-defined Structural Causal Model may not hold for all modern AI systems, particularly deep learning models without explicit causal structures
- The bounds on preference gaps, while mathematically sound, may be practically uninformative if too loose in real applications
- The paper's conclusions rely heavily on grounding assumptions that may not generalize across different AI architectures or training paradigms

## Confidence
- Theoretical bounds on behavior prediction: High - mathematical derivations are rigorous and clearly presented
- Practical applicability of loose bounds: Medium - theoretically valid but uncertain real-world utility
- Fundamental under-constraining of fairness/harm notions: High - logically follows from the mathematical framework
- Grounding assumption generalizability: Low - highly dependent on specific AI architectures

## Next Checks
1. Apply the theoretical bounds to specific deep learning models with varying levels of causal structure to test practical utility
2. Develop empirical methods to quantify the looseness of bounds in real-world prediction tasks
3. Design experiments testing whether grounding assumptions hold across different AI training paradigms and architectures