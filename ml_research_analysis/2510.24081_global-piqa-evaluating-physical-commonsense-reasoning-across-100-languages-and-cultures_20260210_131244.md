---
ver: rpa2
title: 'Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages
  and Cultures'
arxiv_id: '2510.24081'
source_url: https://arxiv.org/abs/2510.24081
tags:
- examples
- latn
- native
- language
- university
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Global PIQA is a participatory physical commonsense reasoning benchmark
  covering 116 language varieties across five continents. It was constructed by 335
  researchers from 65 countries, with authors writing culturally-specific examples
  in their native languages.
---

# Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures

## Quick Facts
- arXiv ID: 2510.24081
- Source URL: https://arxiv.org/abs/2510.24081
- Reference count: 40
- Primary result: Physical commonsense reasoning benchmark covering 116 language varieties with 100 examples each, showing up to 37% accuracy gap between high- and low-resource languages

## Executive Summary
Global PIQA is a participatory physical commonsense reasoning benchmark covering 116 language varieties across five continents. It was constructed by 335 researchers from 65 countries, with authors writing culturally-specific examples in their native languages. The non-parallel split contains 100 examples per language, with 59.9% annotated as culturally-specific. Proprietary models perform well overall (best: 91.7% accuracy), but open models generally underperform. Significant performance gaps exist for lower-resource languages (up to 37% difference from high-resource languages). The dataset highlights everyday reasoning as an area for improvement across many languages, complementing benchmarks focused on complex reasoning. Global PIQA provides both a challenging evaluation tool and a cultural snapshot of over 100 languages.

## Method Summary
The benchmark consists of 11,600 examples (100 per language) in a binary choice format where models select between two candidate solutions for physical commonsense reasoning prompts. Two evaluation formats are used: completion (log-prob comparison for base models) and prompted (generative sampling for instruction-tuned models). The dataset includes 59.9% culturally-specific examples annotated by native authors. Performance is measured by accuracy against human-annotated labels, with random chance at 50%. The evaluation harness from LM-Evaluation-Harness is used with specific temperature and top-p parameters for sampling.

## Key Results
- Proprietary models achieve high accuracy (91.7% for Gemini 2.5 Pro) while open-weight models lag behind (82.4% for Gemma 3 27B)
- Lower-resource languages show significant performance gaps, with up to 37% accuracy difference compared to high-resource languages
- 59.9% of examples are culturally-specific, with accuracy generally lower on these examples across all model types
- Performance generally plateaus for open-weight models around 50B parameters, while proprietary models continue improving
- Human baseline is approximately 95.1% across 12 languages

## Why This Works (Mechanism)

### Mechanism 1: Native-Language Cultural Encoding
Benchmarks authored natively rather than translated capture "physical commonsense" more accurately because physical interactions and social norms are culturally situated. Native speakers encode local affordances (e.g., specific cooking techniques for akki rotti or properties of DÃ³ paper) that are often lost or distorted in machine translation. By avoiding translation artifacts, the dataset isolates the model's ability to reason within a specific cultural context.

### Mechanism 2: The Resource-Level Performance Gap
Performance degradation in lower-resource languages stems from a dual deficit: insufficient linguistic pre-training data and a lack of exposure to culture-specific heuristics. High-resource languages have saturated training corpora covering both syntax and general knowledge, while low-resource languages lack specific "everyday knowledge" priors, leading to a 37% accuracy gap despite the reasoning task being "commonsense."

### Mechanism 3: Evaluation Mode Interference
For smaller models, the "prompted" (instruction-following) evaluation format underestimates capabilities compared to the "completion" format. Instruction-tuning adds auxiliary task demands (formatting, role-playing) that consume limited model capacity. In smaller models, these demands crowd out the reasoning capability, whereas larger models can handle both the format and the reasoning simultaneously.

## Foundational Learning

- **Concept: Physical Commonsense (Affordances)**
  - Why needed here: This benchmark tests knowledge of object properties and physical interactions (e.g., "palm oil... should you heat it or add water?"), not just grammar.
  - Quick check: Can you distinguish between a question testing gravity (universal) vs. a question testing how to prepare a specific local food (cultural)?

- **Concept: Non-Parallel vs. Parallel Evaluation**
  - Why needed here: Understanding why Global PIQA has a "non-parallel" split is crucial. It means the examples are not translations of English, but unique cultural artifacts.
  - Quick check: Why would translating "How do you make a peanut butter sandwich?" into Yoruba potentially fail to test the local knowledge of a Yoruba speaker?

- **Concept: Completion vs. Prompted Evaluation**
  - Why needed here: You must choose the right evaluation harness depending on whether you are testing a base model (predicts next token) or an instruction model (chats).
  - Quick check: If you feed a prompt into a base model without normalization by solution length, what bias might you introduce?

## Architecture Onboarding

- **Component map:** Data Layer (116 language_variety buckets, each containing 100 examples) -> Evaluation Layer (LM-Evaluation-Harness with Completion and Prompted branches) -> Aggregation (Regional/Family groupings)
- **Critical path:** Filtering (ensure solution length difference < 25 bytes) -> Cultural Annotation (tagging culturally-specific examples) -> Evaluation (running on 146 models across both splits)
- **Design tradeoffs:** Sample Size (100) chosen for contributor feasibility over statistical power; Non-Parallelism maximizes cultural validity but complicates cross-lingual comparability
- **Failure signatures:** Refusal/Overthinking (proprietary models may refuse low-resource prompts or exceed token limits); Translation Artifacts (models scoring high on generic but failing culturally-specific examples likely rely on English transfer)
- **First 3 experiments:** 1) Baseline Sanity Check (random chance vs. naive heuristic) 2) Resource Ablation (compare culturally-specific vs. generic examples across resource levels) 3) Tokenizer Analysis (check if byte premiums correlate with accuracy)

## Open Questions the Paper Calls Out

### Open Question 1
Can the upcoming parallel split of Global PIQA successfully disentangle performance gaps caused by limited language capability from those caused by a lack of cultural knowledge? The current paper describes v0.1, which contains only the non-parallel, culturally-specific split; the parallel split is still under development.

### Open Question 2
Can open-weight models close the significant performance gap (approx. 9.3%) with proprietary models in multilingual physical commonsense reasoning? The results show open models plateau around 50B parameters, whereas proprietary models like Gemini 2.5 Pro achieve significantly higher accuracy (91.7%).

### Open Question 3
What causes the dramatic increase in "overthinking" and refusal rates in proprietary models when processing low-resource languages? Appendix E.2 notes that in languages where the best accuracy was under 80%, the overthinking rate for GPT-5 jumped to 43.7% (from an average of 4.9%) and refusal rates also increased.

## Limitations
- Non-parallel split prevents direct cross-lingual comparisons and makes it difficult to isolate whether performance gaps stem from linguistic differences or genuinely different reasoning requirements
- Dataset provides broad coverage (116 languages) but limited statistical power for fine-grained analysis within any single language
- Cultural authenticity, while core strength, introduces interpretability challenges with subjective "culture-specific" annotations lacking inter-annotator agreement statistics

## Confidence

- **High Confidence:** Performance gaps between high-resource and low-resource languages (91.7% vs 80.2% accuracy) are well-supported by data and consistent across runs
- **Medium Confidence:** Native-authored examples capture cultural physical commonsense more effectively than translated benchmarks, though relies on assumption that "culturally-specific" annotation accurately reflects genuine knowledge differences
- **Low Confidence:** Precise mechanism behind completion vs. prompted evaluation gap for smaller models is plausible but not definitively proven; alternative explanations not fully ruled out

## Next Checks

1. **Cultural Annotation Validation:** Re-annotate a random sample of 100 examples (across 10 languages) with multiple native speakers to establish inter-annotator agreement rates for the "culturally-specific" label.

2. **Tokenization Impact Analysis:** For the 10 lowest-resource languages, compare model performance when using different tokenization strategies (byte-level vs. language-specific tokenizers) while controlling for model size.

3. **Cross-Lingual Transfer Study:** Select 20 examples from the parallel split (where available) and translate them into 5 unrelated high-resource languages to determine whether the reasoning task itself is language-agnostic.