---
ver: rpa2
title: Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers
arxiv_id: '2512.03870'
source_url: https://arxiv.org/abs/2512.03870
tags:
- fusedkv
- cache
- layers
- fusedkv-lite
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies an asymmetric reconstruction principle for
  transformer KV caches: values in upper layers are best reconstructed from the bottom
  layer, while keys are best reconstructed from both bottom and middle layers. Based
  on this, the authors propose FusedKV, which reconstructs top-layer caches via a
  learnable fusion of caches from the bottom and middle layers, preserving relative
  positional information without recomputing RoPE.'
---

# Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers

## Quick Facts
- arXiv ID: 2512.03870
- Source URL: https://arxiv.org/abs/2512.03870
- Reference count: 40
- Primary result: Achieves 50% memory savings on transformer KV caches with lower perplexity than full-cache baseline

## Executive Summary
This paper introduces FusedKV, a novel approach for compressing transformer KV caches by reconstructing upper-layer caches through cross-layer fusion. The key insight is an asymmetric reconstruction principle: values are best reconstructed from bottom-layer caches while keys benefit from fusion of both bottom and middle layers. This allows the model to maintain relative positional information without recomputing RoPE, achieving 50% memory savings while improving perplexity compared to full-cached baselines across models from 332M to 4B parameters.

## Method Summary
FusedKV implements cross-layer fusion to reconstruct upper-layer KV caches from lower layers. The asymmetric reconstruction principle dictates that values should be reconstructed primarily from bottom-layer caches, while keys should fuse information from both bottom and middle layers. This is achieved through learnable fusion mechanisms that preserve relative positional information without requiring RoPE recomputation. A more efficient variant, FusedKV-Lite, directly reuses bottom-layer values and middle-layer keys to further reduce I/O overhead while maintaining performance.

## Key Results
- 50% memory savings on transformer KV caches
- Lower perplexity than full-cached baseline across 332M-4B parameter models
- Strong downstream task performance maintained despite cache compression

## Why This Works (Mechanism)
The asymmetric reconstruction principle leverages the observation that value representations in upper layers depend more heavily on bottom-layer features, while key representations benefit from intermediate fusion. By reconstructing values from the bottom layer and fusing keys across layers, FusedKV preserves the critical information needed for attention while eliminating redundant computation. The preservation of relative positional information through fusion rather than recomputation of RoPE enables efficient cache reconstruction without sacrificing accuracy.

## Foundational Learning

**Transformer Attention Mechanism**: Understanding how queries, keys, and values interact through dot-product attention is crucial for grasping why cache reconstruction works. Quick check: Can you explain how attention scores are computed and how they determine the weighted sum of values?

**KV Cache Compression**: Knowledge of how KV caches store intermediate attention computations and their memory footprint is essential. Quick check: What percentage of inference memory is typically consumed by KV caches in large transformers?

**Positional Encoding (RoPE)**: Understanding rotary positional encoding and its role in maintaining sequence order information is necessary to appreciate why reconstruction preserves positional information. Quick check: How does RoPE encode relative positions in transformer attention?

## Architecture Onboarding

**Component Map**: Input -> Bottom Layer -> Middle Layer -> Upper Layers (FusedKV reconstruction) -> Output

**Critical Path**: The reconstruction mechanism operates on upper layers where the asymmetric principle applies. Bottom and middle layer caches are preserved, while upper layer caches are reconstructed through fusion operations that combine information according to the asymmetric principle.

**Design Tradeoffs**: Memory vs. computation tradeoff - FusedKV trades additional computation for significant memory savings. The asymmetric reconstruction requires careful balance to ensure values receive sufficient bottom-layer information while keys get appropriate cross-layer fusion.

**Failure Signatures**: Performance degradation when reconstruction fails to capture essential attention patterns, particularly if the fusion mechanism doesn't adequately preserve positional information or if the asymmetric principle is violated.

**First Experiments**: 1) Ablation study varying reconstruction sources (bottom vs. middle vs. fusion) for values and keys separately. 2) Sensitivity analysis on the choice of "middle layer" for key fusion. 3) Comparative analysis of FusedKV vs. direct cache compression methods on sequence length scaling.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Asymmetric reconstruction principle primarily validated on narrow task set and model sizes
- Limited ablation studies on alternative fusion strategies and middle layer choices
- Evaluation scope constrained to few downstream benchmarks without considering inference speed differences

## Confidence
- Memory savings claim: High
- Perplexity improvement: High
- Asymmetric reconstruction principle generalizability: Medium
- Downstream task performance robustness: Medium

## Next Checks
1. Conduct ablation studies varying the choice of middle layer and fusion strategy to confirm the robustness of the asymmetric reconstruction principle.
2. Evaluate FusedKV on a broader set of downstream tasks, including those with longer sequences or different domain characteristics, to assess generalizability.
3. Benchmark inference speed and numerical stability of FusedKV against full-cached and other compressed-cache baselines under realistic deployment scenarios.