---
ver: rpa2
title: 'LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search'
arxiv_id: '2510.14331'
source_url: https://arxiv.org/abs/2510.14331
tags:
- parity
- accuracy
- learning
- train
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LLM-ERM, a sample-efficient program learning
  framework that leverages large language models to guide the search for short programs.
  Classical enumeration is computationally intractable, while gradient-based methods
  require exponentially many samples for certain short-program families.
---

# LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search

## Quick Facts
- **arXiv ID**: 2510.14331
- **Source URL**: https://arxiv.org/abs/2510.14331
- **Reference count**: 40
- **Primary result**: LLM-ERM learns short programs with 200 samples vs 100,000+ for SGD

## Executive Summary
This paper introduces LLM-ERM, a sample-efficient program learning framework that combines large language models with empirical risk minimization. The key insight is that classical enumeration is computationally intractable for short programs, while gradient-based methods require exponentially many samples for certain program families. LLM-ERM addresses this by using LLMs to propose candidate programs, which are then selected via empirical risk minimization. This approach preserves the generalization properties of finite-class ERM while dramatically reducing search costs. The framework demonstrates strong performance on parity functions, pattern matching, and primality testing, learning effectively with as few as 200 samples compared to 100,000+ samples required by SGD-trained transformers.

## Method Summary
LLM-ERM works by leveraging a pre-trained LLM to generate candidate programs, which are then evaluated using empirical risk minimization on a given dataset. The method maintains the statistical efficiency of classical ERM by treating the LLM-generated programs as a finite hypothesis class. The approach combines the computational tractability of LLM-guided search with the strong generalization guarantees of finite-class learning. The framework uses empirical risk minimization to select the best-performing program from the LLM's proposals, effectively sidestepping the exponential sample complexity that plagues gradient-based methods for high SQ-dimension families like parity functions.

## Key Results
- Learns parity variants, pattern matching, and primality testing with as few as 200 samples
- Generalizes far better than SGD-trained transformers, which overfit even with 100,000 samples
- Produces interpretable, executable code with verifiable reasoning traces
- Enables cross-dimension generalization and mechanistic interpretability

## Why This Works (Mechanism)
LLM-ERM succeeds by exploiting the compositional structure of short programs through LLM-guided search, avoiding the curse of dimensionality that affects gradient-based approaches. The LLM acts as an efficient heuristic for navigating the combinatorial space of possible programs, while ERM provides the statistical framework for generalization. This hybrid approach combines the pattern recognition capabilities of LLMs with the rigorous statistical guarantees of empirical risk minimization, creating a method that is both computationally tractable and theoretically sound.

## Foundational Learning
- **Empirical Risk Minimization (ERM)**: Why needed - provides statistical guarantees for generalization; Quick check - verify that selected program minimizes training error
- **Statistical Query Dimension (SQ-dimension)**: Why needed - characterizes sample complexity requirements; Quick check - confirm high SQ-dimension for parity families
- **Program Synthesis**: Why needed - maps inputs to executable programs; Quick check - validate generated programs produce correct outputs
- **Combinatorial Search**: Why needed - navigates exponential program space efficiently; Quick check - measure search space reduction compared to enumeration
- **LLM-Guided Search**: Why needed - leverages pattern recognition for program discovery; Quick check - evaluate quality of LLM-proposed programs
- **Finite Hypothesis Class Learning**: Why needed - enables strong generalization bounds; Quick check - confirm effective hypothesis class size

## Architecture Onboarding

**Component Map**: LLM Generator -> Program Candidates -> ERM Selector -> Best Program

**Critical Path**: Input Data → LLM Prompting → Candidate Generation → Evaluation → Selection → Output Program

**Design Tradeoffs**: LLM accuracy vs computational cost, candidate diversity vs search efficiency, program complexity vs generalization guarantees

**Failure Signatures**: LLM generates irrelevant programs, ERM overfits to noise, candidates fail to cover target function space

**First Experiments**: (1) Test LLM-ERM on parity functions of increasing dimension; (2) Compare sample efficiency against SGD on pattern matching tasks; (3) Evaluate generalization to unseen input patterns

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Unclear generalizability beyond tested program families (parity, pattern matching, primality)
- Theoretical analysis relies on idealized LLM behavior assumptions
- Limited validation of cross-dimension generalization and mechanistic interpretability claims

## Confidence

**High**: Empirical demonstrations on tested program families show clear improvements over baseline methods
**Medium**: Theoretical characterization of sample complexity advantages is sound but relies on idealized assumptions
**Low**: Claims about cross-dimension generalization and mechanistic interpretability lack thorough validation

## Next Checks
1. Test LLM-ERM on additional program families beyond the three presented, particularly those with higher algorithmic complexity or different structural properties
2. Conduct ablation studies to determine which components of the LLM-guided search are essential for performance gains
3. Perform rigorous evaluation of the mechanistic interpretability claims by attempting to trace and verify reasoning paths, ideally with comparison to baseline approaches