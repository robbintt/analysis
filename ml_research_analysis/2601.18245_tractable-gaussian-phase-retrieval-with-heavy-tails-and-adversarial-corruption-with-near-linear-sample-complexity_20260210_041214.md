---
ver: rpa2
title: Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption
  with Near-Linear Sample Complexity
arxiv_id: '2601.18245'
source_url: https://arxiv.org/abs/2601.18245
tags:
- robust
- algorithm
- theorem
- phase
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the problem of Gaussian phase retrieval with
  both heavy-tailed noise and adversarial corruptions. The main result is the first
  polynomial-time algorithm that achieves near-linear sample complexity (O(n log n))
  for this problem, improving upon the previous exponential-time algorithm with O(n
  log n) sample complexity.
---

# Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity

## Quick Facts
- arXiv ID: 2601.18245
- Source URL: https://arxiv.org/abs/2601.18245
- Reference count: 40
- Key outcome: First polynomial-time algorithm achieving near-linear sample complexity (O(n log n)) for Gaussian phase retrieval under heavy-tailed noise and adversarial corruptions

## Executive Summary
This paper presents the first polynomial-time algorithm with near-linear sample complexity for Gaussian phase retrieval in the presence of both heavy-tailed noise and adversarial corruptions. The key innovation combines robust spectral initialization with robust gradient descent, using carefully chosen truncation parameters and recent advances in robust PCA. The approach is versatile enough to handle both zero-mean and non-zero-mean noise models, achieving O(n log n) sample complexity.

## Method Summary
The paper introduces a two-stage approach: first, a robust spectral initialization that estimates the top eigenvector of the covariance matrix by applying truncation to measurements and leveraging robust PCA techniques; second, a robust gradient descent procedure that refines the initialization. The truncation parameter is carefully chosen to handle both heavy-tailed noise and adversarial corruptions, while recent advances in robust PCA provide theoretical guarantees for the spectral initialization step.

## Key Results
- First polynomial-time algorithm achieving O(n log n) sample complexity for Gaussian phase retrieval under heavy-tailed noise and adversarial corruptions
- Combines robust spectral initialization with robust gradient descent
- Handles both zero-mean and non-zero-mean noise models
- Achieves near-linear sample complexity improvement over previous exponential-time algorithms

## Why This Works (Mechanism)
The algorithm works by leveraging the stability of the top eigenvector of the covariance matrix under truncated measurements. By carefully choosing the truncation parameter, the method can effectively filter out both heavy-tailed noise and adversarial corruptions. The robust PCA techniques ensure that the spectral initialization remains stable even in the presence of corruptions, providing a good starting point for the gradient descent procedure.

## Foundational Learning

1. **Robust PCA**: Essential for handling corruptions in the data; quick check: verify the corruption level is within the breakdown point of the robust PCA method used.

2. **Truncated Measurements**: Critical for filtering out extreme values from heavy-tailed noise; quick check: confirm the truncation threshold is appropriately chosen based on the noise distribution.

3. **Spectral Initialization**: Provides a stable starting point for iterative refinement; quick check: ensure the top eigenvector estimation error is bounded.

4. **Gradient Descent**: Refines the initialization to achieve better accuracy; quick check: verify convergence rate under the given noise model.

5. **Covariance Matrix Estimation**: Forms the basis for spectral methods; quick check: confirm sample complexity is sufficient for accurate estimation.

6. **Heavy-tailed Distributions**: Characterize the noise model; quick check: verify the tail behavior is properly accounted for in the algorithm design.

## Architecture Onboarding

**Component Map**: Data -> Truncation -> Robust PCA -> Spectral Initialization -> Gradient Descent -> Final Estimate

**Critical Path**: The spectral initialization is critical as it provides the starting point for gradient descent. The truncation step is equally important as it ensures the robustness of the entire pipeline.

**Design Tradeoffs**: The choice of truncation parameter balances between preserving signal information and filtering out corruptions. A higher threshold retains more signal but may include more corruptions, while a lower threshold provides better robustness but may lose signal information.

**Failure Signatures**: The algorithm may fail if the truncation parameter is poorly chosen (too high or too low), if the corruption level exceeds the robustness guarantees of the robust PCA method, or if the heavy-tailed noise has extremely large outliers that cannot be effectively truncated.

**First Experiments**:
1. Test the spectral initialization accuracy under varying levels of corruption
2. Evaluate the gradient descent convergence under different noise distributions
3. Benchmark the overall performance against baseline methods on synthetic data

## Open Questions the Paper Calls Out
The paper poses an open question about whether the empirical distribution over truncated samples satisfies a stability condition, which if answered affirmatively would yield a nearly-linear time algorithm.

## Limitations
- Runtime complexity is not explicitly analyzed, making it unclear whether truly near-linear time complexity is achieved
- Theoretical guarantees are provided but practical performance has not been empirically validated
- The stability condition for empirical distributions over truncated samples remains unproven

## Confidence
- High confidence in the sample complexity result (O(n log n))
- Medium confidence in the polynomial-time claim (runtime not explicitly analyzed)
- Medium confidence in the robustness of the spectral initialization and gradient descent procedures (theoretical guarantees provided but practical performance not demonstrated)

## Next Checks
1. Explicitly analyze and state the runtime complexity of the proposed algorithm to verify whether it achieves truly near-linear time complexity or only near-linear sample complexity.

2. Empirically validate the performance of the algorithm on synthetic and real-world datasets to confirm its robustness to heavy-tailed noise and adversarial corruptions.

3. Investigate whether the empirical distribution over truncated samples satisfies the stability condition mentioned in the open question, which would allow for a nearly-linear time algorithm.