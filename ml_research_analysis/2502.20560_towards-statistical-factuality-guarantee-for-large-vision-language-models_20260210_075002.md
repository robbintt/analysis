---
ver: rpa2
title: Towards Statistical Factuality Guarantee for Large Vision-Language Models
arxiv_id: '2502.20560'
source_url: https://arxiv.org/abs/2502.20560
tags:
- coverage
- error
- claims
- desired
- lvlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in Large Vision-Language
  Models (LVLMs), where generated text is inconsistent with visual content. The authors
  propose CONF LVLM, a framework that applies conformal prediction to achieve statistical
  guarantees on factuality by treating LVLM-generated text as individual testable
  claims.
---

# Towards Statistical Factuality Guarantee for Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2502.20560
- **Source URL**: https://arxiv.org/abs/2502.20560
- **Reference count**: 40
- **Primary result**: ConfLVLM reduces LVLM hallucination error rates from 87.8% to 10.0% while maintaining 95.3% true positive rate using conformal prediction

## Executive Summary
This paper addresses the critical problem of hallucinations in Large Vision-Language Models (LVLMs), where generated text is inconsistent with visual content. The authors propose ConfLVLM, a framework that applies conformal prediction to achieve statistical guarantees on factuality by treating LVLM-generated text as individual testable claims. The method uses uncertainty measures to filter unreliable claims before returning responses, achieving significant error reduction while maintaining high true positive rates across three domains: general scene understanding, medical radiology, and document understanding.

## Method Summary
ConfLVLM decomposes LVLM-generated responses into atomic claims, scores each claim's alignment with visual content using uncertainty measures (internal log-probabilities or external models like CLIP/BiomedCLIP), and applies conformal prediction to filter unreliable claims. The framework uses split conformal prediction where a calibration dataset determines a threshold score that ensures the probability of exceeding a user-defined error tolerance is bounded by a specified rate α. The method provides finite-sample statistical guarantees while maintaining high utility through selective filtering.

## Key Results
- Reduces LLaVA-1.5 error rate from 87.8% to 10.0% on MSCOCO scene descriptions
- Achieves 95.3% true positive rate in claim filtering
- Demonstrates significant hallucination reduction in medical radiology and document understanding domains
- Shows external scoring models (CLIP, BiomedCLIP) outperform internal uncertainty measures

## Why This Works (Mechanism)

### Mechanism 1: Split Conformal Prediction
The framework provides finite-sample statistical guarantees by treating LVLM as a hypothesis generator and using split conformal prediction. It constructs a prediction set using a calibration dataset, computing a threshold from the $\frac{\lceil(n+1)(1-\alpha)\rceil}{n}$-th quantile of calibration scores. This ensures the probability of the final response's loss exceeding tolerance λ is bounded by α, assuming exchangeability between calibration and test data.

### Mechanism 2: Granular Claim Decomposition
Free-form text is decomposed into atomic claims via a decomposition operator (typically LLM prompting). Each claim is independently scored for visual alignment, allowing local uncertainty quantification rather than holistic judgment. This decomposition enables precise filtering while preserving valid information.

### Mechanism 3: Selective Filtering
The framework applies a filtering operator that accepts claims only if their conformity score exceeds the calibrated threshold. This acts as a hypothesis test, retaining factual claims and discarding hallucinated ones. The threshold balances error rate α against information retention (abstention rate).

## Foundational Learning

**Split Conformal Prediction (SCP)**: The mathematical engine converting heuristic uncertainty scores into rigorous statistical guarantees. Without this, filtering is ad-hoc thresholding.

*Quick check*: For 100 calibration samples and 90% coverage (α=0.1), select the 10th quantile of calibration scores.

**LVLM Hallucination**: Understanding that hallucinations stem from language prior bias overriding visual grounding. This informs effective scoring function selection.

*Quick check*: Why does "Log Probability Ratio" (Image-Conditioned/Text-Only) better signal visual grounding than raw token probability?

**Exchangeability**: The core assumption ensuring statistical guarantee validity. Calibration and test data must be exchangeable for coverage to hold.

*Quick check*: If calibrated on MSCOCO but deployed on medical X-rays, does the coverage guarantee still hold?

## Architecture Onboarding

**Component map**: LVLM Generator -> Decomposer (prompting) -> Scorer (internal/external) -> Calibrator (threshold computation) -> Filter (claim acceptance) -> Merger (response reconstruction)

**Critical path**: The Calibration Phase is critical. Representative calibration data with annotated errors is essential for computing the conformal threshold τ̂. Biased or small calibration sets weaken the guarantee or increase variance.

**Design tradeoffs**:
- Internal vs. External Scoring: Internal scores are faster but potentially overconfident; external models provide better separation but add latency
- Coverage (1-α) vs. Utility: Stricter guarantees increase factuality but also increase abstention rate

**Failure signatures**:
- High Abstention Rate: Threshold too high or scorer uncalibrated; system returns empty responses frequently
- Guarantee Violation: Empirical error exceeds α; usually indicates distribution shift breaking exchangeability
- Poor TPR with internal scores: Try external scoring (CLIP/BiomedCLIP) for better image-text alignment

**First 3 experiments**:
1. Verify Coverage Validity: Run on held-out test set, plot empirical vs desired coverage to ensure guarantee holds
2. Scorer Ablation: Compare LogP-Ratio vs CLIP-ViT-Large, measure filtered claims and abstention rate for fixed α
3. Domain Sensitivity: Evaluate performance on medical task (BiomedCLIP) vs scene task, assess need for specialized scorers

## Open Questions the Paper Calls Out

**Open Question 1**: Can the framework extend to conditional validity guarantees for specific subgroups rather than marginal guarantees? The paper notes future work could integrate advanced conformal methods for conditional validity, addressing equity concerns for subgroups like specific patient demographics.

**Open Question 2**: How can the trade-off between response utility and hallucination mitigation be optimized? The paper identifies investigating omission aspects as future research, particularly as strict thresholds may remove valuable correct details while ensuring safety.

**Open Question 3**: Can small domain-specific discriminative models serve as more efficient critics than generative LVLMs? The paper suggests discriminative models (e.g., BiomedCLIP) often outperform LVLMs in capturing claim-image relevance, indicating their potential as lightweight critics.

## Limitations

- Distribution shift sensitivity: Statistical guarantee critically depends on exchangeability between calibration and test data, with potential silent failures under significant domain shifts
- Decomposition quality dependency: Effectiveness relies on reliable claim decomposition via prompting, which lacks rigorous empirical validation
- Scoring function calibration: Practical utility heavily depends on selecting appropriate scorers, with unclear generalizability across arbitrary LVLMs

## Confidence

**High Confidence**: Conformal framework provides correct statistical guarantees when exchangeability holds; method significantly reduces hallucination rates; calibration procedure correctly computes thresholds.

**Medium Confidence**: Decomposition via LLM prompting reliably isolates atomic claims; external scorers consistently outperform internal probabilities; 95.3% TPR is achievable with reasonable abstention rates.

**Low Confidence**: Method generalizes to arbitrary LVLMs without architecture-specific tuning; statistical guarantees remain valid under realistic deployment with gradual distribution shifts; computational overhead is negligible for practical deployment.

## Next Checks

1. **Domain Shift Robustness Test**: Evaluate performance when calibration data comes from one domain (MSCOCO) and test data from another (medical X-rays or document images). Measure empirical coverage violation rates and compare abstention behavior.

2. **Decomposition Quality Analysis**: Systematically evaluate decomposition by measuring claim independence (correlation analysis), completeness (coverage of original response meaning), and consistency across multiple runs with the same prompt.

3. **Real-Time Performance Assessment**: Measure end-to-end latency of the complete pipeline (generation + decomposition + scoring + filtering + merging) on standard hardware. Compare performance with and without external scorers to quantify the speed-accuracy trade-off and assess practical deployment feasibility.