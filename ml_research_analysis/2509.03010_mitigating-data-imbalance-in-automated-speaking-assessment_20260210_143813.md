---
ver: rpa2
title: Mitigating Data Imbalance in Automated Speaking Assessment
arxiv_id: '2509.03010'
source_url: https://arxiv.org/abs/2509.03010
tags:
- data
- loss
- speech
- imbalance
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses class imbalance in Automated Speaking Assessment
  (ASA) by introducing the Balancing Logit Variation (BLV) loss, which perturbs model
  predictions to improve feature representation for minority classes without modifying
  the dataset. The method was integrated into an ASA pipeline using Whisper for speech
  transcription and BERT for proficiency classification, then evaluated on the ICNALE
  benchmark dataset.
---

# Mitigating Data Imbalance in Automated Speaking Assessment

## Quick Facts
- arXiv ID: 2509.03010
- Source URL: https://arxiv.org/abs/2509.03010
- Authors: Fong-Chun Tsai; Kuan-Tang Huang; Bi-Cheng Yan; Tien-Hong Lo; Berlin Chen
- Reference count: 26
- Key outcome: BLV loss with σ=6 significantly improved classification accuracy and fairness for minority classes in ASA tasks compared to baseline approaches

## Executive Summary
This paper addresses class imbalance in Automated Speaking Assessment (ASA) by introducing the Balancing Logit Variation (BLV) loss, which perturbs model predictions to improve feature representation for minority classes without modifying the dataset. The method was integrated into an ASA pipeline using Whisper for speech transcription and BERT for proficiency classification, then evaluated on the ICNALE benchmark dataset. Results showed that BLV loss significantly improved classification accuracy and fairness compared to baseline cross-entropy and focal loss approaches, achieving higher Pearson Correlation Coefficient (PCC), lower Root Mean Square Error (RMSE), improved standard and macro accuracy, and better F1-score.

## Method Summary
The authors propose a two-stage pipeline for ASA: Whisper-large-v2 transcribes spoken responses into text, which is then processed by BERT-base-uncased with mean pooling for classification. The key innovation is the BLV loss, which adds Gaussian noise scaled by class-frequency weights to logits during training. The perturbation magnitude αₖ is computed as log-frequency weights (log(N/qₖ) normalized by max), with larger perturbations for minority classes. This forces the model to learn more robust feature representations for underrepresented proficiency levels. The method is evaluated on the ICNALE Spoken Monologues corpus with 4,332 samples across five CEFR levels.

## Key Results
- BLV loss with σ=6 achieved higher Pearson Correlation Coefficient and lower Root Mean Square Error compared to cross-entropy and focal loss baselines
- Standard and macro accuracy improved significantly, with better F1-score indicating improved minority class performance
- The method demonstrated effectiveness in mitigating class imbalance without requiring dataset modification
- Confusion matrix analysis showed improved classification across all proficiency levels, particularly for minority classes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** BLV loss improves minority class representation by perturbing logits during training, causing samples to project into expanded feature regions rather than fixed points.
- **Mechanism:** At each training step, Gaussian noise δ ~ N(0, σ²) is sampled and scaled by class-frequency weight αₖ, then added to logits before softmax. Minority classes receive larger perturbations (higher αₖ), expanding their learned feature regions.
- **Core assumption:** Minority classes need proportionally larger feature space allocation to compensate for fewer training samples.
- **Evidence anchors:** [abstract] "perturbs model predictions to improve feature representation for minority classes without modifying the dataset"; [section III.B] "sample δ ~ N(0, σ²) and form the perturbed logit ẑᵢᵏ = zᵢᵏ + αₖ|δ|" where αₖ is normalized log-frequency.

### Mechanism 2
- **Claim:** Normalized inverse-frequency weighting (αₖ = log(N/qₖ) / max(log(N/qⱼ))) ensures perturbation magnitude scales proportionally with class rarity.
- **Mechanism:** Log-frequency weights compress extreme ratios while maintaining relative ordering, preventing majority classes from dominating the perturbation budget.
- **Core assumption:** The log normalization preserves meaningful differentiation even when class counts span orders of magnitude.
- **Evidence anchors:** [section III.B] Explicit formula for αₖ computation; [section V] "larger variance injection plays a critical role in enhancing the model's predictive capability."

### Mechanism 3
- **Claim:** The two-stage Whisper→BERT pipeline decouples transcription from classification, allowing BLV loss to operate on text representations.
- **Mechanism:** Whisper-large-v2 transcribes speech (potentially introducing errors); BERT-base-uncased encodes tokenized text; mean pooling aggregates embeddings before classification.
- **Core assumption:** Proficiency signals in transcribed text (vocabulary, grammar, coherence) are sufficient for assessment despite ASR error propagation.
- **Evidence anchors:** [abstract] "integrated into an ASA pipeline using Whisper for speech transcription and BERT for proficiency classification"; [section IV.B] "Spoken responses are first transcribed using Whisper-large-v2... fed into a BERT-base-uncased model."

## Foundational Learning

- **Concept: Cross-entropy loss under class imbalance**
  - Why needed here: Standard CE optimizes overall accuracy by favoring majority classes; understanding this motivates why BLV's perturbation strategy works.
  - Quick check question: On a dataset with 90% class A and 10% class B, what accuracy would a naive classifier achieve by always predicting A?

- **Concept: Logit perturbation and regularization**
  - Why needed here: BLV is a form of noise injection; understanding how noise regularizes and expands learned representations clarifies its role.
  - Quick check question: How does adding noise to logits during training differ from adding noise to input features?

- **Concept: Macro vs. standard evaluation metrics**
  - Why needed here: The paper reports both to demonstrate fairness; macro metrics weight all classes equally regardless of frequency.
  - Quick check question: If a model achieves 90% standard accuracy but 50% macro accuracy on a 3-class problem, what does this indicate about class-level performance?

## Architecture Onboarding

- **Component map:** Audio Input → [Whisper-large-v2 ASR] → Transcribed Text → [BERT Tokenizer] → Token IDs → [BERT-base-uncased Encoder] → Hidden States → [Mean Pooling] → Fixed-length Vector → [Dropout 0.1] → [Linear Head] → Logits (C classes) → [BLV Loss: αₖ|δ| added to logits] → Softmax → Cross-Entropy

- **Critical path:** The BLV loss operates on logits post-classifier-head. Ensure αₖ is computed once per dataset (not per batch) for consistency. The only hyperparameter to tune is σ (paper tested σ=2 and σ=6).

- **Design tradeoffs:**
  - Text-only vs. multimodal: Sacrifices acoustic features (pronunciation, prosody) for simplicity and compatibility with BLV.
  - Whisper transcription: Robust to accents but introduces error propagation; paper does not quantify ASR error impact.
  - σ selection: Larger σ improves imbalance handling but risks gradient instability; no ablation beyond σ∈{2,6}.

- **Failure signatures:**
  - Macro accuracy stagnant while standard accuracy improves → model still biased toward majority classes.
  - Training loss becomes erratic → σ may be too large; reduce and retry.
  - Systematic misclassification of specific proficiency levels → check αₖ computation and class frequency distribution.

- **First 3 experiments:**
  1. Reproduce baseline (BERT + cross-entropy) to establish PCC, RMSE, accuracy benchmarks on ICNALE.
  2. Implement BLV loss with σ=6 as reported; verify macro accuracy improvement and compare confusion matrix to paper's Figure 3.
  3. Ablate σ values (e.g., σ∈{1,2,4,6,8}) to validate optimal range on held-out validation split before test evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Balancing Logit Variation (BLV) loss provide similar performance gains when applied to end-to-end acoustic models (e.g., Wav2Vec 2.0) compared to the text-based BERT pipeline used in this study?
- Basis in paper: [inferred] The authors utilize a text-based BERT model "for simplicity," while explicitly citing literature indicating that self-supervised learning (SSL) features like Wav2Vec 2.0 often outperform text-only approaches in proficiency assessment.
- Why unresolved: The paper validates BLV exclusively on text transcriptions; it is unknown if perturbing logits is as effective for acoustic feature spaces that contain prosodic and phonetic information absent in text.
- What evidence would resolve it: Experiments applying BLV loss to a Wav2Vec 2.0-based classification head on the same ICNALE dataset.

### Open Question 2
- Question: Is there a systematic relationship or heuristic for selecting the variance hyperparameter $\sigma$ based on the severity of the dataset's class imbalance?
- Basis in paper: [inferred] The paper identifies $\sigma$ as the sole hyperparameter and tests values of 2 and 6, finding 6 provided the best performance, but does not propose a method for determining this value a priori.
- Why unresolved: Practitioners lack guidance on how to tune $\sigma$ for new datasets without performing an exhaustive search, which is computationally expensive.
- What evidence would resolve it: An analysis plotting performance against $\sigma$ values across datasets with varying imbalance ratios (e.g., different skewness in score distributions).

### Open Question 3
- Question: How does the BLV method's robustness hold up when the upstream ASR system produces high Word Error Rates (WER) for the minority (lowest proficiency) classes?
- Basis in paper: [inferred] The authors note the pipeline uses Whisper for transcription and that L2 learners often struggle with fluency, yet the study does not isolate how transcription errors specifically impact the BLV loss's ability to correct minority class bias.
- Why unresolved: If ASR errors correlate with low proficiency, the "perturbations" introduced by BLV might inadvertently amplify noise from transcription mistakes rather than useful feature variance.
- What evidence would resolve it: A controlled evaluation analyzing the correlation between ASR WER and classification accuracy for minority classes under the BLV regime.

## Limitations
- Data Split Transparency: The paper does not specify train/validation/test split ratios or stratification strategy, critical for reproducibility.
- ASR Error Propagation: The Whisper→BERT pipeline introduces transcription errors that propagate to classification, but the paper does not quantify ASR Word Error Rate impact.
- Hyperparameter Search Depth: Limited testing of σ values (only 2 and 6) without systematic exploration of the parameter space or justification for training parameters.

## Confidence

**High Confidence:** The core mathematical formulation of BLV loss is sound and the perturbation mechanism is clearly defined. The improvement in macro accuracy and F1-score over baseline approaches is well-documented and statistically meaningful.

**Medium Confidence:** The effectiveness of BLV loss for class imbalance is demonstrated on the ICNALE dataset, but generalizability to other ASA datasets or different imbalance distributions remains uncertain. The Whisper→BERT pipeline architecture is reasonable but not rigorously compared to alternatives.

**Low Confidence:** Claims about BLV loss being superior to focal loss and other imbalance techniques lack comprehensive ablation studies. The optimal σ=6 value is asserted without systematic exploration of the parameter space.

## Next Checks

1. **Split Strategy Validation:** Replicate the experiment using multiple random seeds and stratified splits to verify that the reported performance gains are consistent across different data partitions and not artifacts of a particular split.

2. **ASR Quality Assessment:** Measure Whisper transcription WER on the ICNALE dataset and conduct ablation studies comparing BLV performance on gold-standard text vs. transcribed text to quantify the impact of ASR errors on final assessment accuracy.

3. **Extended Hyperparameter Sweep:** Systematically test σ values across a wider range (e.g., σ∈{1,2,4,6,8,10}) and evaluate training stability to identify the optimal balance between perturbation strength and gradient signal preservation.