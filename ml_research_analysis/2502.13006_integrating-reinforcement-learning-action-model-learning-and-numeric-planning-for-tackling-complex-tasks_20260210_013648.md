---
ver: rpa2
title: Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning
  for Tackling Complex Tasks
arxiv_id: '2502.13006'
source_url: https://arxiv.org/abs/2502.13006
tags:
- learning
- action
- planning
- numeric
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the integration of Reinforcement Learning, Action
  Model Learning, and Numeric Planning for tackling complex tasks. The paper compares
  model-based and model-free approaches for solving numeric planning problems in Minecraft.
---

# Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks

## Quick Facts
- arXiv ID: 2502.13006
- Source URL: https://arxiv.org/abs/2502.13006
- Reference count: 13
- Key outcome: RAMP achieves up to 2 orders of magnitude shorter plans and solves more problems than standard RL baselines in Minecraft numeric planning tasks

## Executive Summary
This work introduces RAMP, a novel hybrid approach that integrates Reinforcement Learning, Action Model Learning, and Numeric Planning to solve complex Minecraft tasks with numeric state variables. RAMP simultaneously trains an RL policy (PPO) and learns a symbolic numeric planning domain model (NSAM) from expert trajectories. The learned model enables finding significantly shorter plans through shortcut search, and allows generalization to larger problem instances where model-free approaches fail. Experimental results show RAMP outperforms standard RL algorithms by solving more problems and finding plans that are often two orders of magnitude shorter.

## Method Summary
RAMP combines PPO with NSAM and Metric-FF planner in a feedback loop. PPO explores and finds goal-reaching trajectories, which NSAM uses to learn PDDL action models with numeric preconditions and effects. The learned model enables shortcut search that compresses trajectories by replacing action sequences with more efficient alternatives. These compressed trajectories serve as high-quality training examples for PPO. The approach handles numeric planning by representing actions with linear inequalities for preconditions and linear equations for effects, allowing Metric-FF to find optimal plans that generalize across problem instances with different object counts.

## Key Results
- RAMP achieves up to 2 orders of magnitude shorter plans than standard RL baselines
- Model-based NSAM(+p) approach generalizes to larger environments (15×15) and solves harder tasks that model-free methods struggle with
- Zero-shot transfer results show NSAM(+pt) trained on 6×6 maps performs nearly equivalently to NSAM(+p) trained directly on 15×15 maps
- RAMP successfully solves tasks requiring long-term planning and discovers macro-actions (e.g., diagonal moves) that RL policies struggle to learn directly

## Why This Works (Mechanism)

### Mechanism 1: Generalization through Symbolic Models
Learned symbolic domain models enable generalization to larger state spaces where model-free methods degrade. NSAM learns action preconditions and effects as symbolic rules (linear inequalities for preconditions, linear equations for numeric effects). These rules generalize across problem instances with different object counts because they operate on lifted (parameterized) representations rather than grounded state-action pairs. When the planner (Metric-FF) receives a new problem instance, it applies the same learned rules regardless of map size. Core assumption: The domain has regular, compositional structure that can be captured by conjunctive preconditions and linear effects.

### Mechanism 2: Feedback Loop for Sample Efficiency
The RAMP feedback loop improves sample efficiency by using learned models to generate high-quality training demonstrations for RL. When PPO reaches a goal, RAMP runs NSAM to update the action model, then attempts to find shortcuts in the trajectory. These shortened trajectories are fed back to PPO as additional training episodes. This creates a virtuous cycle: (1) RL explores and discovers goal-reaching trajectories, (2) NSAM refines the model from these trajectories, (3) the model enables finding more efficient paths, (4) these efficient paths become high-quality training examples for RL.

### Mechanism 3: Shortcut Search for Trajectory Compression
Shortcut search using learned action models compresses trajectories more effectively than loop removal alone. After removing loops, RAMP searches for action sequences that can be replaced by a single action where the action is applicable in the start state and directly reaches the end state according to the learned model. This is possible because NSAM may learn diagonal moves or other macro-actions that the RL policy discovered but doesn't systematically use.

## Foundational Learning

- **Concept: Numeric Planning (PDDL2.1)**
  - **Why needed here:** The entire approach hinges on representing domain dynamics with both discrete predicates (e.g., "at cell") and numeric fluents (e.g., inventory counts). Without understanding how numeric preconditions (linear inequalities) and effects (linear equations) differ from classical STRIPS, the NSAM outputs won't be interpretable.
  - **Quick check question:** Given an action "craft plank" that requires ≥1 wood and produces +4 planks while consuming -1 wood, write the precondition and effect formulas.

- **Concept: Safe Action Model Learning**
  - **Why needed here:** NSAM's safety guarantee is what makes shortcut search valid—if the model weren't safe, shortcuts could produce infeasible plans. Understanding this guarantee clarifies why RAMP can trust its learned model for trajectory compression.
  - **Quick check question:** If a learned model predicts action A is applicable in state S but executing A in S actually fails, does this violate the safety property or just completeness?

- **Concept: On-Policy RL with Action Masking (PPO)**
  - **Why needed here:** RAMP uses PPO with a specific action-masking modification to handle expert demonstrations. Standard PPO would struggle because its clipping mechanism nullifies gradient updates when the expert action has low probability under the current policy.
  - **Quick check question:** Why does masking invalid actions help PPO learn from expert demonstrations that differ substantially from its current policy distribution?

## Architecture Onboarding

- **Component map:**
  Environment (PAL/Minecraft) → Translation Layer → Gym format (for PPO) / PDDL format (for NSAM/Metric-FF) → RAMP Controller (PPO Agent + NSAM Learner + Metric-FF Planner + Shortcut Search)

- **Critical path:**
  1. Episode starts → RAMP attempts planning with current M_NSAM
  2. If no plan → PPO selects actions → trajectory collected
  3. If goal reached → NSAM updates M_NSAM from all trajectories
  4. Shortcut search compresses successful trajectory
  5. Compressed trajectory fed to PPO as training example
  6. Repeat until episode budget (B_i) exhausted, then advance to next problem instance

- **Design tradeoffs:**
  - **Planner call frequency:** The implementation skips planner calls until first goal reached and when M_NSAM unchanged. Trade-off: faster iterations vs. potentially missing early planning opportunities.
  - **Shortcut search complexity:** Current O(|T|) implementation only checks if a single action can replace a suffix. More sophisticated search could find non-contiguous shortcuts but would increase computational cost.
  - **Model-free baseline selection:** DQN/QR-DQN were abandoned after failing across all tasks despite hyperparameter tuning. This suggests the action space is too large for value-based methods without significant engineering.

- **Failure signatures:**
  - **NSAM returns overly restrictive model:** If training trajectories don't cover diverse state contexts, preconditions may be too specific. Planner fails to find plans for new instances.
  - **PPO doesn't improve from expert trajectories:** If action masking isn't implemented correctly, PPO's clipping will zero out gradients. Check that logits for non-expert actions are properly masked.
  - **Shortcut search produces invalid plans:** This indicates M_NSAM is not safe—likely due to noisy observations or violated NSAM assumptions.

- **First 3 experiments:**
  1. **Reproduce offline learning results on 6×6 maps:** Run NSAM(+p) and BC with 40, 100, 200 expert trajectories on Craft Wooden Sword. Verify BC learns faster initially but NSAM(+p) catches up.
  2. **Ablation on shortcut search components:** Run RAMP, RAMP(-p), and RAMP(-pn) on Craft Wooden Pogo 10×10. Quantify the contribution of loop removal only, model-based shortcuts, and full planning.
  3. **Test generalization with limited training data:** Train on 6×6 maps, test zero-shot on 15×15. Compare NSAM(+pt) vs. BC. A working implementation should show NSAM maintaining ~0.9+ success rate while BC degrades to ~0.2-0.4 on longer-horizon instances.

## Open Questions the Paper Calls Out

- **Can informed action selection policies be developed for numeric planning to intentionally improve the quality of the learned action model?** The authors identify this as a future direction, noting that while such policies exist for classical planning, they are missing for numeric planning. RAMP currently relies on standard RL exploration rather than actions chosen specifically to refine the model's understanding of numeric preconditions and effects.

- **Can integrating RAMP with the Go-Explore algorithm enable agents to find shortcuts to promising states more efficiently than standard memory mechanisms?** The authors propose replacing Go-Explore's memory component with an action model learning algorithm to find shortcuts to archived states. Go-Explore typically uses grid hashing for memory; leveraging a symbolic planner to find efficient paths back to these states is a proposed but untested integration.

- **Does performing "imagination" using a learned symbolic action model improve the efficiency of model-based RL algorithms like DreamerV3?** The authors suggest integrating RAMP with DreamerV3 to perform "imagination" using a learned symbolic model rather than neural network dynamics. It is unclear if symbolic planning models offer a computational or sample-efficiency advantage over the latent neural network dynamics currently used by DreamerV3 for lookahead.

## Limitations
- RAMP requires access to expert demonstrations or trajectories, limiting its applicability in purely online RL scenarios without any initial guidance
- The NSAM algorithm assumes noise-free observations and linear preconditions/effects, which may not hold in all real-world domains
- The