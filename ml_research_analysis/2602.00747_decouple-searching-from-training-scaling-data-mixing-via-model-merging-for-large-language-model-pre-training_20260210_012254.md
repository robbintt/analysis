---
ver: rpa2
title: 'Decouple Searching from Training: Scaling Data Mixing via Model Merging for
  Large Language Model Pre-training'
arxiv_id: '2602.00747'
source_url: https://arxiv.org/abs/2602.00747
tags:
- data
- arxiv
- training
- mixture
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of determining optimal data
  mixtures for large language model (LLM) pre-training, particularly when balancing
  general competence with proficiency on hard tasks like math and code. The proposed
  Decouple Searching from Training Mix (DeMix) framework uses model merging to construct
  proxy models that predict optimal data ratios without requiring additional training.
---

# Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training

## Quick Facts
- arXiv ID: 2602.00747
- Source URL: https://arxiv.org/abs/2602.00747
- Reference count: 40
- Primary result: Achieves 6.4× cost reduction in data mixture optimization by using model merging instead of training proxy models

## Executive Summary
This paper addresses the challenge of determining optimal data mixtures for large language model pre-training, particularly when balancing general competence with proficiency on hard tasks like math and code. The proposed Decouple Searching from Training Mix (DeMix) framework uses model merging to construct proxy models that predict optimal data ratios without requiring additional training. By training component models on candidate datasets and deriving proxy models via weighted model merging, DeMix enables unlimited evaluation of sampled mixtures without extra training costs. Extensive experiments demonstrate that DeMix achieves higher proxy accuracy and better mixture quality than training-based approaches while requiring significantly less computational budget.

## Method Summary
DeMix addresses data mixture optimization by training N component models on individual candidate datasets, then using weighted model merging to create proxy models for any mixture ratio without additional training. The framework operates on the "small-update assumption" where parameter deltas from individual dataset training are small enough to be linearly combined. An iterative search process samples mixture weights, builds proxy models via merging, evaluates them on benchmarks, and trains a LightGBM predictor to focus the search on promising regions. This approach achieves 6.4× cost reduction compared to training-based proxy methods while maintaining high proxy accuracy (Spearman's ρ of 0.81).

## Key Results
- Achieves 6.4× cost reduction: 212B training tokens vs 1344B tokens needed by conventional approaches
- Proxy accuracy: Macro-average Spearman's ρ of 0.81 with capability recovery rate of 0.85
- Mixture quality: Obtains optimal mixtures with higher benchmark performance at lower search cost
- Linear weighted merging outperforms complex alternatives (TIES, DARE) while being simpler and more robust

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighted model merging can approximate the parameters of a model trained on a mixed dataset, enabling proxy evaluation without retraining.
- **Mechanism:** The framework relies on the "small-update assumption." When parameter updates (weight deltas) from models trained on separate datasets are small relative to the base model, the arithmetic sum of these deltas approximates the delta from training on the union of the datasets. Therefore, linearly interpolating the weights of component models at a target ratio produces a proxy model close to one trained directly on that mixture.
- **Core assumption:** The magnitude of parameter updates remains small compared to the initialization scale (δ ≪ 1). This is stated as approximately 10% in the paper.
- **Evidence anchors:**
  - [abstract] "Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging."
  - [section 2.3] "as long as δ ≪ 1, the arithmetic sum of weight deltas from models trained on separate datasets closely approximates the weight delta obtained by training on their union [Equation 5]."
  - [corpus] Supported by related work on model merging (e.g., 'Merge to Mix'), which validates merging for dataset mixing, though DeMix extends this to continuous-weighted pre-training mixtures.
- **Break condition:** Fidelity degrades if component models are trained too long or diverge significantly, violating the small-update assumption. The linear approximation fails for non-linear training interactions.

### Mechanism 2
- **Claim:** Separating the search for an optimal mixture from the training of proxy models dramatically reduces the computational cost of mixture optimization.
- **Mechanism:** Traditional approaches like RegMix require training N proxy models for N candidates, costing O(N × proxy_training_budget). DeMix trains a fixed number k of component models once. Constructing any number of proxy models via merging is computationally trivial, changing the cost to O(k × component_training_budget + N × merging_cost), where the second term is negligible.
- **Core assumption:** The upfront investment in high-quality component models pays off by enabling a much more extensive and effective search.
- **Evidence anchors:**
  - [abstract] "DeMix achieves a macro-average Spearman's ρ of 0.81 and capability recovery rate of 0.85 with only 212B training tokens, outperforming training-based approaches that require 1344B tokens."
  - [section 4.1, Table 2] DeMix (212B tokens) achieves comparable proxy accuracy to a training-based approach (1344B tokens), a ~6.4x efficiency gain.
  - [corpus] This efficiency claim is a central contribution and is not directly refuted by related corpus papers, though 'Merge to Mix' argues for similar efficiency in fine-tuning.
- **Break condition:** The cost advantage disappears if the space of candidate datasets is very large, requiring an infeasible number of component models.

### Mechanism 3
- **Claim:** A predictor trained to map mixture weights to proxy model performance can effectively identify high-performing mixture configurations in a continuous search space.
- **Mechanism:** The continuous nature of mixture weights precludes exhaustive search. The framework uses an iterative process: 1) randomly sample weights, 2) build and evaluate proxy models, 3) train a LightGBM regressor to predict performance, and 4) use the predictor to focus search on promising regions.
- **Core assumption:** The relationship between mixture weights and performance is smooth enough to be learned by a regressor from a limited number of evaluated proxy points.
- **Evidence anchors:**
  - [abstract] "Extensive experiments demonstrate that DeMix... obtaining optimal mixtures with higher benchmark performance at lower search cost."
  - [section 2.4] Details the iterative prediction loop: "Using the collected pairs (αᵢʲ, rʲ), we train a predictor f that maps mixture weights to ranking scores."
  - [corpus] 'MixMin' frames data mixing as a bi-level optimization problem, suggesting a similar underlying assumption that the optimal mixture can be found via principled search.
- **Break condition:** The predictor may overfit to the sampled points, especially in high-dimensional mixture spaces or with few proxies. The paper notes a potential performance decline with an excessive number of proxies, hinting at overfitting risks.

## Foundational Learning

- **Concept: Spearman's Rank Correlation Coefficient (ρ)**
  - **Why needed here:** Used as the primary metric for "proxy accuracy" (Section 3.3.1). It measures how well the ranking of proxy models predicts the ranking of reference models. This is crucial because the goal is to select the best mixture, making rank preservation more important than matching absolute scores.
  - **Quick check question:** If a method achieves high capability recovery but low Spearman's ρ, why might it still fail to find the optimal mixture?

- **Concept: The Weight Delta (Task Arithmetic)**
  - **Why needed here:** The core mathematical justification for the model merging mechanism (Section 2.3). The framework operates on the principle that Θ trained = Θ base + Δ(data), and that these Δs are approximately additive. Understanding this is essential for grasping why linear merging can proxy for training on a data mixture.
  - **Quick check question:** Why does the "small-update assumption" (δ ≪ 1) matter for the validity of adding weight deltas? What happens if this assumption is violated?

- **Concept: Curse of Dimensionality in Search**
  - **Why needed here:** The paper explicitly reduces the number of candidate datasets to seven (Section A.1) to make the search tractable. This highlights a fundamental trade-off: searching over a mixture of 100 datasets is vastly harder than 7.
  - **Quick check question:** If you double the number of candidate datasets from 7 to 14, roughly how does this affect the volume of the mixture space that needs to be searched?

## Architecture Onboarding

- **Component map:**
  1. Data Curation Pipeline: Preprocesses and categorizes massive data into a manageable set of N high-quality candidate datasets
  2. Component Model Trainer: Trains N specialized models (component models) from a shared base model, each on one candidate dataset mixed with general data
  3. Proxy Factory: A training-free module that constructs proxy models on demand using weighted linear merging of the N component models
  4. Proxy Evaluator: Runs inference on proxy models against a fixed benchmark suite to generate performance scores
  5. Search Engine: Implements the iterative optimization loop: samples mixture weights, calls the Proxy Factory, gets scores from the Evaluator, trains a predictor, and uses it to suggest better mixture weights

- **Critical path:**
  1. Candidate Dataset Selection: This primary design decision defines the search space and has the largest impact on the final result
  2. Component Model Fidelity: Proxy accuracy is entirely dependent on the quality and training of the N component models. Under-training them produces poor proxies, invalidating the search
  3. Predictor Robustness: The LightGBM model must generalize from few evaluated proxies to the vast space of potential mixtures. Its failure leads to suboptimal mixtures

- **Design tradeoffs:**
  1. Number of Component Models (N) vs. Search Cost: Larger N allows for more granular control but increases upfront training cost and search complexity. The paper uses N=7 as a practical compromise
  2. Component Model Training Budget vs. Proxy Accuracy: More tokens improve fidelity but increase the fixed, upfront cost
  3. Merging Strategy Complexity vs. Simplicity: The paper finds simple linear merging to be highly effective (Table 4). More complex methods like TIES or DARE did not yield significant improvements and add hyperparameters

- **Failure signatures:**
  1. Low Proxy Accuracy (Spearman's ρ): Indicates merged proxies do not faithfully represent training dynamics. Root causes: under-trained component models or a violated small-update assumption (e.g., due to highly divergent data)
  2. High Variance in Predictor Performance: The search finds inconsistent mixtures across runs. This suggests too few proxy evaluations for the search space dimensionality, or predictor overfitting
  3. Final Model Underperforms on Specific Domains: The search overfitted to the benchmarks used in the Evaluator. The selected mixture is brittle and does not generalize to the full target capability set

- **First 3 experiments:**
  1. Validate the Core Assumption (Sanity Check): Train a component model on dataset A and another on B. Merge them at a 0.5/0.5 ratio. Separately, train a reference model from scratch on a 0.5/0.5 mixture of A and B. Compare benchmark performance. A close match validates the merging proxy.
  2. Ablate Merging Method: Using the DeMix setup, compare the proxy accuracy (Spearman's ρ) and capability recovery of linear merging against TIES and DARE. This reproduces Table 4 and confirms the choice of linear merging.
  3. Benchmark Mixture Quality: Run the full DeMix search pipeline with N=3 candidate datasets and a limited proxy budget. Compare the benchmark performance of a model trained on the DeMix-found mixture against a model trained on a uniform mixture. This tests the end-to-end system.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability uncertainty: The framework's performance with larger numbers of candidate datasets (N > 7) remains untested
- Generalization risk: Component models trained on specific domain mixtures may not accurately predict performance across all continuous mixture ratios
- Data dependency: Proxy accuracy depends heavily on the quality and representativeness of the candidate datasets selected

## Confidence

**High Confidence:** The efficiency advantage of model merging over training-based proxies (Section 4.1 results showing 6.4× reduction) is well-supported by controlled experiments. The choice of linear weighted merging over complex alternatives (Table 4) demonstrates robust empirical validation. The iterative predictor-based search methodology follows established practices in bi-level optimization.

**Medium Confidence:** The generalization of component models trained on specific domain mixtures (50% general + 50% domain-specific) to arbitrary mixture ratios remains somewhat untested. The framework assumes proxy models built from these components can accurately predict performance across the continuous mixture space, but edge cases where domains interact non-linearly could violate this assumption.

**Low Confidence:** The scalability claim to larger numbers of candidate datasets lacks empirical support. While the framework theoretically supports any N, the paper restricts experiments to N=7 without demonstrating how proxy accuracy degrades as N increases or how the search space complexity affects predictor performance.

## Next Checks

1. **Stress-test the small-update assumption:** Systematically vary component model training budgets (5B to 100B tokens) and measure degradation in proxy accuracy and capability recovery to identify the threshold where merging breaks down.

2. **Scale complexity experiment:** Replicate the DeMix pipeline with N=14 candidate datasets (doubling from the current 7) and measure how proxy accuracy, search efficiency, and final model performance change to validate scalability claims.

3. **Cross-domain generalization test:** Train component models on mixtures with varying general-to-domain ratios (25%/75%, 75%/25%) and evaluate whether proxies built from these can accurately predict performance of mixtures outside their training distribution.