---
ver: rpa2
title: 'PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on
  Scientific Literature'
arxiv_id: '2510.10909'
source_url: https://arxiv.org/abs/2510.10909
tags:
- reasoning
- arxiv
- tool
- agent
- paperarena
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PaperArena introduces a challenging benchmark for evaluating LLM-based
  agents on complex, tool-augmented reasoning tasks using scientific literature. Unlike
  prior benchmarks limited to single-paper, tool-free tasks, PaperArena requires agents
  to integrate information across multiple papers, interpret multimodal content, and
  orchestrate diverse external tools.
---

# PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature

## Quick Facts
- **arXiv ID**: 2510.10909
- **Source URL**: https://arxiv.org/abs/2510.10909
- **Authors**: Daoyu Wang; Mingyue Cheng; Shuo Yu; Zirui Liu; Ze Guo; Xin Li; Qi Liu
- **Reference count**: 36
- **Primary result**: PaperArena introduces a challenging benchmark for evaluating LLM-based agents on complex, tool-augmented reasoning tasks using scientific literature, achieving only 38.78% average accuracy even with top models like Gemini 2.5 Pro.

## Executive Summary
PaperArena introduces a challenging benchmark for evaluating LLM-based agents on complex, tool-augmented reasoning tasks using scientific literature. Unlike prior benchmarks limited to single-paper, tool-free tasks, PaperArena requires agents to integrate information across multiple papers, interpret multimodal content, and orchestrate diverse external tools. Constructed from a diverse corpus of recent AI papers, it features 784 expert-curated questions demanding cross-paper synthesis, chart analysis, code execution, and database querying. Experiments with nine leading LLMs show stark performance gaps: even top models like Gemini 2.5 Pro achieve only 38.78% average accuracy, dropping to 18.47% on hard tasks, far below human experts at 83.5%. Analysis reveals inefficient tool usage, fragile planning, and distinct failure patterns. PaperArena provides a robust platform for advancing agent capabilities in authentic scientific research scenarios.

## Method Summary
PaperArena is constructed from 100 sampled papers (from 14,435 recent AI papers) with 784 expert-curated questions categorized by difficulty (Easy/Medium/Hard) and feature type (Multi-Step Reasoning, Multimodal Understanding, Cross-Paper Integration, Database Interfacing). The evaluation platform, PaperArena-Hub, manages agent workflows in a sandboxed Docker environment with 8 tools: PDF Parser, Context Retriever, Table/Figure Analyzer, Cross-Ref Searcher, Web Searcher, Database Querier, and Code Executor. Agents operate via single-agent ReAct-style workflows or multi-agent centralized manager-worker architectures with up to 40 reasoning steps. Evaluation uses LLM-as-Judge (GPT-4o) for binary correctness, efficiency metrics based on tool chain overlap with minimal viable chains, and IRT analysis for latent ability estimation.

## Key Results
- Top models like Gemini 2.5 Pro achieve only 38.78% average accuracy, dropping to 18.47% on hard tasks
- Human experts achieve 83.5% accuracy, highlighting the significant gap between current agents and human performance
- Agents show inefficient tool usage patterns, overusing Web Searcher (2x minimal viable) and Code Executor (5x minimal viable)
- Multi-agent architectures improve hard-task performance but still fall far short of human-level reasoning

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Sampling for Evaluation Diversity
The hierarchical sampling strategy (K-Medoids + Farthest Point Sampling) creates a balanced paper subset that enables meaningful evaluation without topic bias. K-Medoids selects 50 prototype papers reflecting dominant research clusters, while Farthest Point Sampling extracts 50 boundary papers from sparse regions. This dual approach prevents over-representation of mainstream topics while ensuring the benchmark covers underrepresented research areas, creating questions that test diverse reasoning patterns.

### Mechanism 2: Manager-Worker Decoupling in Multi-Agent Architecture
Separating high-level planning (manager agent) from execution (worker agents) improves planning stability and task performance on complex, multi-tool scenarios. The manager agent operates in a clean context without execution noise, enabling coherent plan formulation. Worker agents execute tool invocations within isolated context spaces. This decoupling prevents the manager from being distracted by verbose tool outputs and error messages, maintaining reasoning coherence across long tool chains.

### Mechanism 3: IRT-Based Difficulty Calibration and Failure Attribution
Combining Item Response Theory (IRT) with failure attribution enables precise diagnosis of agent capabilities and identification of systematic bottlenecks beyond accuracy metrics. IRT estimates latent agent ability (θ) and question difficulty (β) from binary response matrices, revealing whether poor performance stems from agent limitations or question difficulty. Failure attribution categorizes errors into parameter errors, logical errors, redundant actions, and tool execution errors, isolating whether failures originate in planning, execution, or tool reliability.

## Foundational Learning

- **Concept: Tool-Augmented Reasoning (ReAct Paradigm)**
  - **Why needed here**: PaperArena's core contribution is evaluating agents that must interleave reasoning and tool use; understanding ReAct-style workflows is essential to interpret the single-agent baseline results.
  - **Quick check question**: Can you explain how a ReAct agent differs from a pipeline that executes a predetermined tool sequence?

- **Concept: Item Response Theory (IRT) Parameters (θ and β)**
  - **Why needed here**: The paper uses IRT to estimate latent ability (θ) and difficulty (β), which provides insight beyond raw accuracy. Understanding these parameters is necessary to interpret Figure 4 and the difficulty validation analysis.
  - **Quick check question**: If Agent A has θ = 1.5 and Agent B has θ = 0.8 on the same benchmark, what can you conclude about their relative capabilities?

- **Concept: Centralized vs. Decentralized Multi-Agent Coordination**
  - **Why needed here**: PaperArena-Hub implements a centralized manager-worker architecture. Understanding coordination patterns helps evaluate the tradeoffs described in Section 5.2 and the multi-agent performance gains.
  - **Quick check question**: What are the potential failure modes of a centralized multi-agent system compared to a decentralized peer-to-peer approach?

## Architecture Onboarding

- **Component map**:
  PaperArena (benchmark) -> PaperArena-Hub (platform) -> 8 tools (PDF Parser, Context Retriever, Table/Figure Analyzer, Cross-Ref Searcher, Web Searcher, Database Querier, Code Executor) -> Single-agent or Multi-agent workflows -> GPT-4o-as-Judge evaluation -> IRT analysis

- **Critical path**:
  1. Paper ingestion via MinerU → structured Markdown + figure/table extraction + citation parsing
  2. Question generation by 3 LLMs → human expert filtering/rewriting → annotation with ground truth + minimal viable tool chain + difficulty/type labels
  3. Agent execution in sandboxed Docker containers with 40-step limit → tool invocation with structured error messages
  4. Response evaluation by GPT-4o (binary correctness) → reasoning efficiency calculation → IRT analysis via MCMC

- **Design tradeoffs**:
  1. Single-agent vs. Multi-agent: Multi-agent improves hard-task accuracy but adds coordination complexity and context management overhead
  2. Automatic vs. Human evaluation: LLM-as-Judge achieves 98.5% agreement with humans but may miss subtle reasoning errors in open-ended questions
  3. Tool completeness vs. Attribution clarity: Providing all 8 tools enables authentic research scenarios but makes failure attribution harder
  4. Manual vs. IRT difficulty: Manual difficulty labels align well with IRT β values, but outliers reveal cases where humans misestimate agent difficulty

- **Failure signatures**:
  1. Tool usage imbalance: Agents overuse Web Searcher (2x minimal viable) and Code Executor (5x minimal viable), underuse specialized tools
  2. Planning fragility: Single-agent with explicit minimal viable tool chain prompting outperforms autonomous planning
  3. Redundant actions: Tendency toward over-verification, re-checking known information
  4. Parameter/logical errors: Higher rates of syntax faults and task dependency violations
  5. Execution inefficiency: Efficiency degrades with task complexity even with optimal plans

- **First 3 experiments**:
  1. Replicate the single-agent vs. multi-agent comparison on a 50-question subset, focusing on the hard subset to quantify the multi-agent advantage.
  2. Implement the "single-agent with minimal viable tool chain prompting" condition. Compare against baseline single-agent and multi-agent to quantify the planning ceiling and execution gap.
  3. Conduct manual failure attribution on 20 failed instances per model (5 models). Categorize into the 4 error types and compare patterns between closed-source thinking vs. open-source models.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark construction relied on three different LLMs for initial question generation followed by human expert filtering, introducing variability in question difficulty calibration that isn't fully quantified.
- The tool environment provides eight different tools, but the study doesn't fully explore whether this tool richness obscures whether failures stem from planning limitations versus tool selection challenges.
- The 784-question benchmark's generalizability to broader scientific domains beyond AI papers is uncertain.

## Confidence
- **High confidence**: The benchmark design methodology and the core finding that even top models struggle significantly on hard tasks are well-supported by the presented data.
- **Medium confidence**: The multi-agent architecture benefits and the failure attribution patterns are reasonable but would benefit from additional ablation studies.
- **Low confidence**: The generalizability of the benchmark to broader scientific domains and whether the observed performance gaps would persist with different paper corpora.

## Next Checks
1. **Human validation expansion**: Conduct human evaluation on 200+ randomly sampled questions across all difficulty levels to verify the LLM-as-Judge agreement holds for the full benchmark spectrum.
2. **Ablation on tool environment**: Create a minimal tool subset (PDF Parser + Context Retriever + Web Searcher) and compare agent performance to isolate whether tool richness affects failure attribution clarity.
3. **Cross-domain generalization**: Apply the benchmark methodology to non-AI scientific literature (e.g., biology or physics papers) and measure whether the same performance patterns and failure modes emerge.