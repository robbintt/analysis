---
ver: rpa2
title: Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation
  Steering
arxiv_id: '2511.00617'
source_url: https://arxiv.org/abs/2511.00617
tags:
- gid00001
- gid00068
- gid00083
- gid00064
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a unifying Bayesian model of how large language
  models (LLMs) can be controlled at inference time using both in-context learning
  (ICL) and activation steering. The key insight is that both methods operate by updating
  the model''s belief in latent concepts: ICL accumulates evidence through likelihood
  functions, while activation steering directly alters concept priors.'
---

# Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering

## Quick Facts
- arXiv ID: 2511.00617
- Source URL: https://arxiv.org/abs/2511.00617
- Reference count: 40
- This paper develops a unifying Bayesian model of how large language models (LLMs) can be controlled at inference time using both in-context learning (ICL) and activation steering.

## Executive Summary
This paper develops a unifying Bayesian model of how large language models (LLMs) can be controlled at inference time using both in-context learning (ICL) and activation steering. The key insight is that both methods operate by updating the model's belief in latent concepts: ICL accumulates evidence through likelihood functions, while activation steering directly alters concept priors. This leads to a closed-form model that predicts LLM behavior across both intervention types.

The model successfully explains and predicts several key phenomena: sigmoidal learning curves as ICL examples accumulate, predictable shifts in ICL behavior proportional to steering magnitude, and the additivity of both interventions in log-belief space. This additivity creates distinct phases where small changes in intervention controls can induce sudden behavioral shifts. The model achieves high predictive accuracy (r=0.98 average correlation) across multiple LLM architectures and datasets, including held-out predictions via cross-validation. This work offers both a theoretical foundation for understanding LLM control and a practical methodology for predicting intervention effects.

## Method Summary
The authors developed a Bayesian belief dynamics model to predict LLM behavior under combined in-context learning and activation steering interventions. They tested on five persona datasets (Psychopathy, Machiavellianism, Narcissism, Moral Nihilism, Life Has No Meaning) using Llama-3.1-8B-Instruct, Gemma-2-9B, and Qwen-2.5-7B models. CAA steering was applied at optimal layers (Llama-3.1-8B: layer 12, Gemma-2-9B: layer 20, Qwen-2.5-7B: layer 14) with steering magnitudes m ∈ [-10,+10] and ICL shots N ∈ {0,1,...,128}. The Bayesian model log o(c|x) = a·m + b + γN^(1-α) was fit using L-BFGS-B optimization with 10-fold cross-validation on held-out steering magnitudes.

## Key Results
- The model achieves r=0.98 average correlation between predicted and observed LLM behavior across multiple architectures and datasets
- ICL evidence accumulation follows a sub-linear power law (N^(1-α)), producing sigmoidal learning curves
- Activation steering operates by shifting concept priors linearly in log-odds space
- ICL and steering interventions are additive in log-belief space, creating predictable phase boundaries where small control changes cause sudden behavioral shifts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-Context Learning (ICL) updates model belief via a sub-linear accumulation of evidence (likelihood).
- **Mechanism:** As the number of in-context examples $N$ increases, the model accumulates evidence for a latent concept. This process is not linear; it scales as a power law ($N^{1-\alpha}$), resulting in sigmoidal learning dynamics where behavior changes slowly, then rapidly (transition point), then plateaus.
- **Core assumption:** The model functions as a Bayesian agent where the log-likelihood of a concept scales sub-linearly with context length, likely due to redundancy or noise in the context.
- **Evidence anchors:**
  - [Abstract]: "ICL accumulates evidence through likelihood functions... sigmoidal learning curves as ICL examples accumulate."
  - [Section 4.1]: "Evidence accumulation as sub-linear by multiplying the log-likelihood by a discount factor $\tau(N)$."
  - [Corpus]: Limited direct support in provided abstracts, though "The Shape of Beliefs" neighbor paper hints at "geometry... of posteriors."
- **Break condition:** This mechanism fails if context examples contradict each other or if the concept is not linearly accessible in the model's representation space (e.g., unlearned concepts).

### Mechanism 2
- **Claim:** Activation steering directly alters the model's prior belief in a concept linearly.
- **Mechanism:** By adding a vector $m \cdot d$ to a hidden representation $v$, the log-odds of the target concept increase linearly by $a \cdot m$. This is mathematically equivalent to shifting the prior probability $p(c)$ of the concept regardless of the input $x$.
- **Core assumption:** The Linear Representation Hypothesis (LRH) holds, meaning concepts are represented as linear directions in activation space and can be manipulated via vector arithmetic.
- **Evidence anchors:**
  - [Abstract]: "Steering operates by changing concept priors... [predicting] shifts in ICL behavior proportional to steering magnitude."
  - [Section 4.2]: "Steering yields a constant shift in the model log-posterior odds... best described as alteration of a model's prior beliefs."
  - [Corpus]: "Manipulating Transformer-Based Models" discusses "principled interventions," consistent with the feasibility of such steering.
- **Break condition:** Breaks if steering magnitude $m$ is too large, causing the representation to leave the valid support (manifold) where linear structure holds, leading to incoherent outputs or convergence to random chance.

### Mechanism 3
- **Claim:** ICL and steering interventions are additive in log-belief space, creating predictable "phase boundaries."
- **Mechanism:** Because both interventions affect the log-odds (one via likelihood, one via prior), their effects sum: $\text{log\_odds} = \text{steering\_shift} + \text{ICL\_evidence}$. This allows one to mathematically cancel the other, shifting the transition point (inflection point) of behavior.
- **Core assumption:** The two interventions target the same latent concept variable and do not destructively interfere within the tested ranges.
- **Evidence anchors:**
  - [Abstract]: "Additivity of both interventions in log-belief space... sudden and dramatic behavioral shifts can be induced by slightly changing intervention controls."
  - [Section 4.3 (Eq 8)]: Derives the additive model $\log o(c|x) = a \cdot m + b + \gamma N^{1-\alpha}$.
  - [Corpus]: General consensus on "interventions" in neighbor papers, but specific additivity claim is unique to this work.
- **Break condition:** If the "Linear Representation Hypothesis" fails or the model saturates (probabilities near 0 or 1), the additive relationship in probability space becomes decoupled from log-odds, though the underlying mechanism may persist.

## Foundational Learning

- **Concept:** **Bayesian Posterior Odds (Log-Odds)**
  - **Why needed here:** The entire framework relies on converting model probabilities (0-1) into log-odds ($-\infty$ to $+\infty$) to model the additive effects of context and steering. Without this, the "additivity" mechanism is opaque.
  - **Quick check question:** If the log-odds of a behavior are +2, is the probability > 90% or < 90%? (Answer: > 90%, roughly $e^2/(1+e^2)$).

- **Concept:** **Linear Representation Hypothesis (LRH)**
  - **Why needed here:** This is the enabling assumption for why adding a vector to an activation *works* at all. It posits that high-level concepts (like "persona") are encoded as directions in vector space.
  - **Quick check question:** If LRH holds, should steering a model with vector $v + 2d$ make the concept twice as "strong" in logit space as $v + d$?

- **Concept:** **Sigmoidal Dynamics / Phase Transitions**
  - **Why needed here:** The paper explains "sudden" learning not as magic, but as a property of probability accumulation crossing a threshold (0.5 probability) rapidly.
  - **Quick check question:** Does a linear increase in evidence ($N$) cause a linear increase in probability $p(y|x)$? (No, it causes a linear increase in log-odds, resulting in a sigmoidal curve in probability).

## Architecture Onboarding

- **Component map:**
  - **Interventions:** Two levers control the "Belief State" (Log-Odds).
    1.  *Context Length ($N$):* Feeds into **Likelihood Function** (Power Law scaling).
    2.  *Steering Vector ($m$):* Feeds into **Prior Shift** (Linear scaling).
  - **Model Core:** Hidden representations ($v$) in specific layers (e.g., Layer 12 for Llama-3.1-8B) where steering is most effective.
  - **Output:** Sigmoid function converting Log-Odds to Probability.

- **Critical path:**
  1.  Identify the optimal steering layer (where the steering vector $d$ maximally affects behavior).
  2.  Fit the scaling parameters ($\alpha, \gamma$) for ICL dynamics.
  3.  Fit the steering coefficient ($a$) for the magnitude response.
  4.  Combine to predict the transition point $N^*$ for a given steering magnitude.

- **Design tradeoffs:**
  - **Context vs. Steering:** You can achieve the same behavior change (log-odds shift) by either adding context (slower, more expensive inference) or increasing steering magnitude (faster, but risks "LRH breakdown" / incoherence at high $m$).
  - **Layer Selection:** Steering is often localized to specific middle layers (e.g., 12/32 for Llama); steering earlier or later may have negligible or distinct effects.

- **Failure signatures:**
  - **Steering Breakdown:** At extreme magnitudes ($|m| > 3$ typically), model outputs become incoherent or converge to random choice ($p=0.5$). This indicates leaving the linear subspace of the concept.
  - **Non-Additivity:** If a concept is not linearly represented (e.g., some models in the appendix like phi-4), steering has no effect, breaking the additive model.

- **First 3 experiments:**
  1.  **Validate Sigmoidal ICL:** Plot probability of a behavior vs. $N^{1-\alpha}$ (or log $N$) without steering. Verify the "sudden rise" curve fits the model.
  2.  **Validate Linear Steering:** At fixed $N$ (e.g., 0-shot), vary steering magnitude $m$. Plot $p(y|x)$. Verify it looks sigmoidal in $m$ (implying linear in log-odds).
  3.  **Predict Phase Shift ($N^*$):** Fit the model on small steering magnitudes ($m \in [-1, 1]$). Predict the new number of shots $N^*$ required to achieve 50% probability when steering at $m = -2$ (opposing direction). Verify if the model correctly predicts the "rightward shift" of the curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is belief representation truly linear throughout the representation space, or only linear within specific subspaces, with non-linear representations elsewhere?
- Basis in paper: [inferred] The authors observe that steering effects break down at large magnitudes and that some models (e.g., phi-4-mini-instruct) show no clear steering response, suggesting potential boundaries to linear belief representation.
- Why unresolved: The current experiments only test a limited range of steering magnitudes and do not systematically probe whether representations become non-linear outside certain regions.
- What evidence would resolve it: Systematic mapping of representation spaces across varying magnitudes, layers, and model architectures to identify boundaries where linear additivity fails.

### Open Question 2
- Question: How and where are concept likelihood functions implemented mechanistically in the network architecture?
- Basis in paper: [explicit] The authors ask "are concept likelihood functions implemented in a non-linear way, and are they implemented in earlier or later layers relative to this linear belief representation?"
- Why unresolved: The model captures behavioral predictions but does not identify which layers or circuits implement likelihood computation vs. belief storage.
- What evidence would resolve it: Layer-wise probing, ablation studies, and causal intervention experiments targeting likelihood computation separate from prior modification.

### Open Question 3
- Question: Why do steering vectors computed over multiple ICL shots produce weaker effects than single-shot vectors when normalized to equal magnitude?
- Basis in paper: [explicit] The authors report this counter-intuitive finding in Appendix D and state "Further work is required to explain this effect."
- Why unresolved: The empirical observation is documented but lacks theoretical explanation within the belief dynamics framework.
- What evidence would resolve it: Analysis of how context accumulation changes the direction and composition of steering vectors, testing whether multi-shot vectors capture different concept dimensions.

### Open Question 4
- Question: Does the belief dynamics model generalize to non-binary concept spaces with multiple competing concepts?
- Basis in paper: [explicit] The authors state "One limitation of this work is that we only consider binary concepts" and "Future work may explore how our theory and model generalize to non-binary concept spaces."
- Why unresolved: Current formalization assumes a target concept c and its complement c', which may not capture scenarios with three or more mutually exclusive concepts.
- What evidence would resolve it: Extending the mathematical framework to multi-concept settings and testing predictions on datasets with non-binary classification tasks.

## Limitations

- The Bayesian belief dynamics framework relies heavily on the Linear Representation Hypothesis (LRH), which may break down for complex concepts or at high steering magnitudes
- The five persona-based datasets represent a narrow slice of potential ICL and steering applications, limiting generalizability
- Steering breakdown at extreme magnitudes suggests the linear model may be valid only within a narrow operational window

## Confidence

- **High Confidence:** The additivity of ICL and steering effects in log-odds space is well-supported by the mathematical derivation and empirical validation. The sigmoidal nature of both ICL learning curves and steering response functions is clearly demonstrated.
- **Medium Confidence:** The power-law scaling of ICL evidence accumulation (N^(1-α)) is empirically well-fit but not theoretically grounded. The claim that steering fundamentally operates by shifting priors is logically consistent with the Bayesian framework but relies on the strong LRH assumption.
- **Low Confidence:** The universality of the 4-parameter Bayesian model across all tested architectures and tasks is asserted but not rigorously proven. The paper does not explore edge cases where the model might fail, such as steering conflicting concepts or ICL with contradictory examples.

## Next Checks

1. **Concept Conflict Test:** Apply steering and ICL that target opposite concepts (e.g., steer toward "honest" while providing ICL examples of "dishonest" behavior). Verify whether the additive model still predicts the transition point, or if destructive interference occurs, breaking the assumption that both interventions affect the same latent variable.

2. **Cross-Task Generalization:** Test the model's predictive accuracy on a fundamentally different task type, such as mathematical reasoning or code generation, where the latent concepts are more procedural than persona-based. This would validate whether the 4-parameter model is a general solution or overfit to the persona domain.

3. **Steering Layer Ablation:** Systematically vary the steering layer across the full depth of the model (not just the single optimal layer) for a single task. Map the steering effectiveness as a function of layer depth to test whether the model's "belief state" is truly localized or distributed, and whether the Bayesian model needs layer-specific parameters.