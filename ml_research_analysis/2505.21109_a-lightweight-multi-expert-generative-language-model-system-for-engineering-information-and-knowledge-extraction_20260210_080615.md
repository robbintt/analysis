---
ver: rpa2
title: A Lightweight Multi-Expert Generative Language Model System for Engineering
  Information and Knowledge Extraction
arxiv_id: '2505.21109'
source_url: https://arxiv.org/abs/2505.21109
tags:
- language
- llama-3
- engineering
- data
- b-instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Small Language Graph (SLG), a lightweight
  multi-expert generative language model system designed to improve domain-specific
  information extraction in engineering contexts while reducing hallucinations and
  computational overhead. SLG organizes fine-tuned small language models (1B parameters)
  as graph nodes, with an orchestrator routing user queries to the most relevant expert.
---

# A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction

## Quick Facts
- arXiv ID: 2505.21109
- Source URL: https://arxiv.org/abs/2505.21109
- Authors: Bogdan Bogachov; Yaoyao Fiona Zhao
- Reference count: 40
- Primary result: SLG outperforms an 8B parameter standalone model by 3x on Exact Match accuracy while being 1.7x faster to fine-tune

## Executive Summary
This study introduces the Small Language Graph (SLG), a lightweight multi-expert generative language model system designed to improve domain-specific information extraction in engineering contexts while reducing hallucinations and computational overhead. SLG organizes fine-tuned small language models (1B parameters) as graph nodes, with an orchestrator routing user queries to the most relevant expert. This architecture mitigates knowledge overlap and data overshadowing, which are common causes of hallucinations in larger models. Experimental results show that SLG outperforms a larger standalone model (8B parameters) by 3x on Exact Match accuracy, while being 1.7x faster to fine-tune. The system can be trained and run on a single RTX 4090 GPU, making it accessible to small-to-medium engineering firms without expensive compute infrastructure.

## Method Summary
The SLG system uses Llama-3.2-1B-Instruct as the base model for both orchestrator and expert nodes, with LoRA fine-tuning applied to all components. The engineering documentation (Cessna aircraft Structural Repair Manual) is split into non-overlapping subsections, with each expert fine-tuned on a specific chunk. An orchestrator node routes queries to appropriate experts, while LangGraph manages the connections. Training uses LoRA with rank 16, learning rate 1e-3, and early stopping. The system generates synthetic QA pairs using Llama-3.3-70B-Instruct and evaluates using ROUGE-L, Exact Match, and METEOR metrics.

## Key Results
- SLG achieves 3x better Exact Match accuracy compared to Llama-3.1-8B-Instruct
- Training process is 1.7x faster than fine-tuning a larger standalone model
- The entire system can be trained and deployed on a single RTX 4090 GPU
- Demonstrates effectiveness in reducing hallucinations through data isolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating training data into non-overlapping chunks may reduce hallucinations caused by knowledge overshadowing.
- Mechanism: Each expert model is fine-tuned exclusively on a narrow, logically bounded data segment, preventing the model from learning blended patterns between similar inputs with different correct outputs.
- Core assumption: Hallucinations in engineering domains are partially caused by probabilistic blending of overlapping training contexts, not just model scale or architecture.
- Evidence anchors: [abstract] "The method isolates training data into non-overlapping chunks to prevent knowledge overshadowing"; [PAGE 3] "knowledge overshadowing [16]...describes how overlapping contexts in the training data can blend together"
- Break condition: If hallucinations persist despite data isolation, the root cause may not be overlap but rather model capacity limitations or architectural constraints.

### Mechanism 2
- Claim: A graph-based orchestrator routing queries to specialized experts can outperform a single larger model on exact match accuracy.
- Mechanism: The orchestrator (also a fine-tuned small LM) is trained to output expert names rather than answers, directing queries to the appropriate specialist. Each expert then generates responses from its narrow knowledge base.
- Core assumption: Queries can be cleanly mapped to discrete knowledge domains, and routing errors are less costly than generation errors from a monolithic model.
- Evidence anchors: [abstract] "employs a graph-based architecture with an orchestrator routing queries to the most relevant expert"; [PAGE 4] "the orchestrator node does not always direct user queries to an appropriate expert...success rate...is approximately 70%"
- Break condition: If routing accuracy falls below a threshold where misrouted queries produce worse outcomes than a single model's average performance, the architecture overhead negates its benefits.

### Mechanism 3
- Claim: Multiple small fine-tuned models (1B parameters each) trained in parallel can achieve faster aggregate training time than a single larger model (8B parameters).
- Mechanism: LoRA fine-tuning on small models reduces trainable parameters. Each expert trains independently on a small data chunk, enabling parallelization and lower per-model compute requirements.
- Core assumption: Training time scales with model size and data volume, and the overhead of managing multiple models does not exceed the savings from smaller individual training runs.
- Evidence anchors: [abstract] "fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model"; [PAGE 5, TABLE 5] SLG: 3475 seconds; Llama-3.1-8B: 5891 seconds
- Break condition: If orchestration overhead, model loading costs, or sequential dependencies between experts exist, aggregate time savings diminish or reverse.

## Foundational Learning

- Concept: Knowledge Overshadowing in LLMs
  - Why needed here: Understanding that hallucinations can stem from overlapping training contexts—not just model size—is central to why SLG isolates data.
  - Quick check question: Can you explain how two training examples with identical prefixes but different endings might confuse a model during inference?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: All SLG components use LoRA for fine-tuning; understanding its bottleneck architecture (rank, alpha) is necessary to interpret hyperparameter choices.
  - Quick check question: What does LoRA rank control, and how does increasing it affect trainable parameter count?

- Concept: Graph-Based Orchestration vs. Monolithic Inference
  - Why needed here: SLG's architecture depends on separating routing (orchestrator) from generation (experts); this differs fundamentally from single-model inference pipelines.
  - Quick check question: In a graph-based system, what happens if the orchestrator's routing confidence is low—should it default to a generic expert or reject the query?

## Architecture Onboarding

- Component map: Orchestrator Node -> Expert Nodes (Wing Damage Classification, Fuselage Repairs, etc.)
- Critical path:
  1. Prepare dataset: Split engineering documentation into non-overlapping subsections
  2. Generate QA pairs per chunk using a larger LLM
  3. Fine-tune orchestrator (questions → expert names) and experts (questions → domain answers) separately
  4. Connect nodes via LangGraph
  5. Inference: orchestrator routes query → expert generates response
- Design tradeoffs:
  - Routing accuracy vs. coverage: 70% routing success (per paper) leaves 30% of queries potentially misrouted; an aggregator or fallback expert could mitigate this
  - Data granularity: Finer chunks reduce overlap but increase expert count and orchestration complexity
  - Model size: 1B experts are lightweight but may lack reasoning depth; larger experts improve quality but reduce parallelization benefits
- Failure signatures:
  - Orchestrator outputs incorrect expert name → wrong domain knowledge applied
  - Expert trained on overlapping data → hallucinations persist despite isolation strategy
  - Query spans multiple chunks → no single expert has complete context; response is partial or contradictory
  - LoRA rank too low → underfitting on narrow domain; too high → overfitting, slower training
- First 3 experiments:
  1. Baseline comparison: Fine-tune a single Llama-3.2-1B-Instruct on all data; measure EM, ROUGE-L, METEOR against SLG to isolate the effect of the graph architecture
  2. Routing stress test: Submit queries with deliberately ambiguous phrasing; measure orchestrator accuracy and compare end-to-end performance when routing fails
  3. Chunk granularity sweep: Vary subsection split size; observe impact on EM and training time to find optimal isolation granularity

## Open Questions the Paper Calls Out

- Question: How does the SLG system performance compare against Retrieval-Augmented Generation (RAG) techniques and significantly larger models like Llama-3.33.70B-Instruct?
- Basis in paper: [explicit] The authors explicitly state a limitation to comparing only two smaller models and identify comparing against RAG and Llama-3.33.70B as planned future work.
- Why unresolved: Current experiments only established baselines against Llama-3.1-8B and Llama-3.2-1B to prove the viability of the lightweight approach.
- What evidence would resolve it: Benchmark results (ROUGE-L, Exact Match, METEOR) run on the same Cessna dataset comparing SLG against a RAG implementation and the 70B parameter model.

- Question: To what extent does the SLG architecture reduce hallucinations compared to standard fine-tuning when evaluated using human assessment or fact-checking metrics?
- Basis in paper: [explicit] The authors acknowledge a "limited hallucinations check" relying solely on the Exact Match (EM) metric.
- Why unresolved: While EM suggests a reduction in hallucinations, the authors note that human evaluation and fact-checking would be a "more exhaustive way" to estimate reliability.
- What evidence would resolve it: A study incorporating qualitative human evaluation or specific hallucination detection metrics alongside existing automated metrics.

- Question: Can the integration of conversational memory and an aggregator node improve the orchestrator's routing accuracy beyond the current approximate success rate of 70%?
- Basis in paper: [explicit] The limitations section notes the orchestrator occasionally misroutes queries and lacks conversational context, proposing memory and aggregator nodes as potential solutions.
- Why unresolved: The current implementation treats queries as stand-alone inputs, limiting the orchestrator's ability to utilize clarifying context or split complex queries effectively.
- What evidence would resolve it: Ablation studies measuring routing accuracy and response quality in a modified system equipped with memory and output aggregation capabilities.

## Limitations

- The orchestrator's 70% routing accuracy leaves 30% of queries potentially misrouted, with no clear fallback mechanism specified
- The system's performance on queries spanning multiple domains or requiring cross-expert reasoning remains untested
- Exact dataset splits, number of expert nodes, and prompt templates for synthetic QA generation are not fully specified
- Dependency on a larger LLM (70B) for generating training data may limit generalizability to domains with limited resources

## Confidence

- High Confidence: Computational efficiency claims (1.7x faster training, single-GPU deployment) are well-supported by direct timing comparisons
- Medium Confidence: 3x improvement in Exact Match accuracy is compelling but may be sensitive to dataset splits and routing accuracy
- Low Confidence: Generalizability to domains outside structured engineering documentation and robustness to ambiguous queries remain untested

## Next Checks

1. **Routing Robustness Test**: Systematically generate ambiguous or cross-domain queries to measure orchestrator failure rates and end-to-end performance degradation
2. **Cross-Domain Generalization**: Apply SLG to a different structured knowledge domain (e.g., medical protocols or legal documents) to test whether data isolation and routing architecture transfer effectively
3. **Hybrid Query Handling**: Implement and evaluate a fallback or aggregator mechanism for queries that span multiple experts, measuring whether this improves overall accuracy and robustness without sacrificing efficiency