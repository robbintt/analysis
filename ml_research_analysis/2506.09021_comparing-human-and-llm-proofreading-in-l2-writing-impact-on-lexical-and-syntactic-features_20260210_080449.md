---
ver: rpa2
title: 'Comparing human and LLM proofreading in L2 writing: Impact on lexical and
  syntactic features'
arxiv_id: '2506.09021'
source_url: https://arxiv.org/abs/2506.09021
tags:
- proofreading
- lexical
- writing
- syntactic
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared human and LLM proofreading in L2 writing, focusing
  on lexical and syntactic interventions. Using the ICNALE dataset, original and professionally
  proofread essays were compared with outputs from three LLMs (ChatGPT-4o, Llama3.1-8b,
  Deepseek-r1-8b).
---

# Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features

## Quick Facts
- arXiv ID: 2506.09021
- Source URL: https://arxiv.org/abs/2506.09021
- Reference count: 17
- Human and LLM proofreading both improve L2 writing, but LLMs make more extensive edits affecting vocabulary diversity, sophistication, and syntactic complexity

## Executive Summary
This study compares human professional proofreading with LLM proofreading (ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b) on L2 English writing from the ICNALE dataset. Both human and LLM proofreading improved lexical coherence, but LLMs made more extensive edits, increasing vocabulary diversity and sophistication while also increasing syntactic complexity through more noun phrases, adjective modifiers, and nominalizations. LLM outputs showed reduced use of past tense and main verb "be" forms compared to human proofreading.

## Method Summary
The study used the ICNALE dataset containing original and professionally proofread essays from L2 English learners. These were compared with outputs from three LLM models. The researchers analyzed changes in lexical features (diversity, sophistication, coherence) and syntactic complexity using computational linguistic metrics including bigram coherence, vocabulary analysis, and syntactic structure counting.

## Key Results
- Both human and LLM proofreading improved bigram lexical coherence
- LLMs made more extensive edits than human proofreaders, increasing vocabulary diversity and sophistication
- LLM outputs showed increased syntactic complexity (more noun phrases, adjective modifiers, nominalizations) and reduced past tense/main verb "be" use
- Across three LLM models, lexical and syntactic changes were highly consistent (Cronbach's α: 0.83 and 0.81)

## Why This Works (Mechanism)
Assumption: LLMs likely make more extensive edits due to their ability to process entire texts holistically and apply pattern-based corrections at scale, while human proofreaders tend to focus on discrete errors and follow editorial guidelines more conservatively.

## Foundational Learning
Unknown: The paper does not explicitly address how these proofreading findings relate to broader learning mechanisms in L2 writing acquisition or pedagogical implications.

## Architecture Onboarding
Unknown: The paper does not provide specific guidance on how to integrate LLM proofreading tools into existing writing instruction or professional editing workflows.

## Open Questions the Paper Calls Out
None explicitly identified in the paper, though several follow-up research directions could be inferred from the findings.

## Limitations
- Cross-sectional design comparing different texts rather than tracking same texts through multiple proofreading stages
- Reliance on computational metrics that may not capture nuanced quality improvements
- Limited to three specific LLM models rather than broader representation
- Single professional proofreading standard rather than multiple editorial approaches
- Did not include human rating studies to validate whether observed complexity metrics translate to perceived writing quality improvements

## Confidence
- **High confidence**: Consistent patterns across three LLM models (Cronbach's α: 0.83 and 0.81), increased lexical diversity and syntactic complexity in LLM outputs
- **Medium confidence**: Claims about enhanced "fluency" require human judgment validation
- **Medium confidence**: Speculation about "inflating perceived proficiency" and "altering authorial voice" needs additional empirical support

## Next Checks
1. Conduct human rating studies with L2 writing experts to validate whether LLM-proofread texts genuinely demonstrate improved writing quality beyond complexity metrics
2. Replicate with longitudinal dataset tracking same texts through human and LLM proofreading stages to establish causal relationships
3. Test additional LLM models including newer versions to determine if observed patterns generalize across broader LLM landscape