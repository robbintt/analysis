---
ver: rpa2
title: 'Let''s Play Across Cultures: A Large Multilingual, Multicultural Benchmark
  for Assessing Language Models'' Understanding of Sports'
arxiv_id: '2510.01247'
source_url: https://arxiv.org/abs/2510.01247
tags:
- question
- uni00000013
- across
- sports
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CultSportQA, the first multilingual and multicultural
  benchmark for evaluating language models' understanding of traditional sports across
  60 countries, 6 continents, and 11 languages. The dataset comprises 33,000 multiple-choice
  questions across text and image modalities, covering history-based, rule-based,
  and scenario-based question types.
---

# Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports

## Quick Facts
- **arXiv ID:** 2510.01247
- **Source URL:** https://arxiv.org/abs/2510.01247
- **Reference count:** 40
- **Primary result:** Introduces CultSportQA, a 33,000-question multilingual/multicultural sports benchmark testing LLMs across 11 languages, 84 traditional sports, and text/image modalities.

## Executive Summary
This paper introduces CultSportQA, the first multilingual and multicultural benchmark for evaluating language models' understanding of traditional sports across 60 countries, 6 continents, and 11 languages. The dataset comprises 33,000 multiple-choice questions across text and image modalities, covering history-based, rule-based, and scenario-based question types. Comprehensive evaluations of 8 large language models, 5 small language models, and 4 multimodal models using zero-shot, few-shot, and chain-of-thought prompting reveal significant performance gaps in reasoning about culturally nuanced sports queries. GPT-4o and Llama-3.1-70B achieved the highest accuracies at 0.87 and 0.84 respectively, while smaller models showed lower performance across all settings. The study highlights the need for more diverse training data and improved cultural awareness in AI systems.

## Method Summary
The CultSportQA benchmark evaluates language models using 33,000 multiple-choice questions across 11 languages and 84 traditional sports from 60 countries. The evaluation employs zero-shot, few-shot (3-shot), and chain-of-thought prompting strategies with temperature=0 and greedy decoding. Models tested include 8 large language models (LLMs), 5 small language models (SLMs), and 4 multimodal models (MLLMs). The dataset features text and image modalities, with accuracy as the primary metric. Native speakers were used for data annotation to ensure cultural authenticity. The benchmark is publicly available at https://github.com/M-Groot7/CultSportQA.

## Key Results
- GPT-4o achieved the highest accuracy at 0.87, followed by Llama-3.1-70B at 0.84
- Smaller language models showed consistently lower performance across all prompting strategies
- Multimodal models (InstructBLIP) achieved ~49% accuracy, significantly lower than top text-only models
- Chain-of-thought prompting improved scenario-based question performance across all model sizes
- Performance gaps were most pronounced for culturally nuanced sports queries compared to globally popular sports

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Performance differentials between global and traditional sports appear contingent upon the statistical representation of cultural knowledge in pre-training corpora.
- **Mechanism:** LLMs retrieve information based on token co-occurrence frequencies during training. Culturally niche or regionally specific sports likely have lower document frequencies than global sports, resulting in weaker latent associations and higher error rates.
- **Core assumption:** The pre-training datasets were not specifically augmented with curated sources used in CultSportQA.
- **Evidence anchors:** Abstract states LLMs are primarily evaluated on globally popular sports, overlooking regional traditions; section 6.2 shows errors driven by limited knowledge of culturally nuanced sports.

### Mechanism 2
- **Claim:** Chain-of-Thought (CoT) prompting improves accuracy on scenario-based questions by forcing intermediate reasoning steps that surface obscure cultural rules.
- **Mechanism:** Scenario-based questions require conditional logic. CoT prompts the model to articulate underlying rules before decision-making, reducing guessing based on superficial associations.
- **Core assumption:** Models possess latent capacity to reason about rules but fail to access them in zero-shot settings.
- **Evidence anchors:** Section 5.2 shows evaluation used zero-shot, few-shot, and CoT prompting; table 2 shows consistent accuracy improvements for CoT over zero-shot.

### Mechanism 3
- **Claim:** Multimodal evaluation reveals a distinct "visual-culture" gap where models struggle to map visual features of traditional equipment or attire to specific cultural concepts.
- **Mechanism:** Multimodal models must fuse visual embeddings with textual semantic space. Standard models may generalize well on modern sports uniforms but fail to identify specific cultural artifacts due to lack of visual diversity in training data.
- **Core assumption:** Visual encoders have limited exposure to high-resolution, diverse images of these specific 84 traditional sports.
- **Evidence anchors:** Section 6.1 shows MLLM performance is significantly lower than top LLMs; section 6.2 highlights confusion caused by ambiguous or overlapping answer options in image-based queries.

## Foundational Learning

### Cultural Bias in LLMs
- **Why needed:** To understand why a model might know football rules but not "Kho-Kho" despite both being sports
- **Quick check:** Can you explain why "Western-centric" training data leads to lower accuracy on Amharic or Thai sports queries?

### Prompt Engineering (Zero/Few/CoT)
- **Why needed:** The paper relies heavily on these strategies to elicit performance; understanding them is required to interpret results tables
- **Quick check:** How does providing 3 examples (few-shot) change the model's attention mechanism compared to zero-shot?

### MCQ Evaluation Metrics
- **Why needed:** The entire benchmark is formatted as Multiple Choice Questions (MCQs) with accuracy as the sole metric
- **Quick check:** Why might "Accuracy" be a misleading metric if the options are too easy to distinguish linguistically without knowing the sport?

## Architecture Onboarding

### Component map:
- **Data Source** (CultSportQA dataset) -> **Prompt Interface** (Zero-shot, Few-shot, CoT templates) -> **Model Layer** (LLM/MLLM APIs) -> **Evaluation Engine** (Accuracy calculation)

### Critical path:
1. Load CultSportQA subset (e.g., Image-based Rule questions for Asia)
2. Apply Prompt Template (e.g., CoT)
3. Inference using specific decoding strategy (Temperature=0, Greedy)
4. Parse output to extract option token
5. Compute accuracy vs. metadata (Language, Continent, Question Type)

### Design tradeoffs:
- **Translation vs. Native:** Used native speakers for annotation, trading cost/speed for high cultural fidelity
- **MCQ vs. Generation:** Chose MCQ for scalable, objective evaluation but may limit depth of reasoning assessment compared to open-ended VQA

### Failure signatures:
- **Hallucination of Rules:** Model invents rules for traditional sports based on similar global sports
- **Visual Misclassification:** MLLMs identifying specific cultural weapon as generic "stick" or "sword"
- **Language Collapse:** In low-resource languages (Amharic), model may revert to English reasoning or fail to parse query

### First 3 experiments:
1. **Baseline Establishment:** Run Zero-shot evaluation on GPT-4o and Llama-3.1-70B to confirm reported 0.87 and 0.84 accuracy baselines
2. **Modality Ablation:** Compare Text-only LLM performance on sports where image data is available to see if visual context aids or confuses text-only reasoning
3. **Language Robustness Test:** Evaluate Mistral-7B across all 11 languages specifically on "Rule-based" questions to identify which linguistic families show steepest performance drop-off

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does expanding the benchmark to include additional low-resource languages and regions affect cross-lingual transfer capabilities of LLMs?
- **Basis:** Authors explicitly state dataset is limited to 11 languages and countries, and future work should enhance diversity by including more low-resource languages
- **Why unresolved:** Current study scope is restricted to specific countries in Asia, Africa, and Europe, leaving performance on broader linguistic set untested
- **Evidence needed:** Re-evaluating leading models on expanded version including low-resource languages not present in original training data

### Open Question 2
- **Question:** To what extent would incorporating video or temporal modalities improve model assessment on scenario-based reasoning tasks?
- **Basis:** Authors acknowledge dataset includes only text and image modalities, noting expanding to other modalities would enhance scope and depth
- **Why unresolved:** Scenario-based questions often involve dynamic motion which static images cannot fully capture, potentially limiting assessment of true reasoning capabilities
- **Evidence needed:** Comparative study evaluating MLLMs on scenario-based questions using video clips versus current static frames

### Open Question 3
- **Question:** Can Small Language Models achieve cultural reasoning capabilities comparable to LLMs when specifically fine-tuned on CultSportQA dataset?
- **Basis:** Paper evaluates SLMs and LLMs using zero-shot, few-shot, and CoT prompting, revealing performance gap, but does not explore whether fine-tuning SLMs on training data could bridge this gap
- **Why unresolved:** Experimental setup relies solely on prompting techniques; potential for knowledge injection via fine-tuning to improve cultural awareness in smaller models remains untested
- **Evidence needed:** Experiment fine-tuning SLMs (e.g., Mistral-7B or Gemma-7B) on CultSportQA training set and comparing results against few-shot performance of LLMs like GPT-4o

## Limitations
- Dataset representativeness uncertainty: Sample may not capture full diversity of indigenous sports globally due to selection process not being fully transparent
- Performance attribution ambiguity: Gaps could stem from multiple factors beyond cultural knowledge deficit (tokenization issues, image recognition limitations)
- Generalization gap: Benchmark evaluates specific traditional sports but does not establish whether improved performance would translate to broader cultural competence across other domains

## Confidence
- **High Confidence:** CultSportQA is first benchmark for multilingual/multicultural sports understanding; significant performance differences exist between global and traditional sports queries; GPT-4o and Llama-3.1-70B demonstrate substantially higher accuracy than smaller models
- **Medium Confidence:** Performance gap primarily driven by cultural knowledge deficits rather than technical limitations; CoT prompting provides consistent benefits for scenario-based questions; MLLMs underperform due to visual-culture gaps in training data
- **Low Confidence:** Specific magnitude of cultural knowledge gaps across different linguistic families; whether observed patterns would hold with different question formats; relative importance of prompt engineering versus model size in addressing cultural competency gaps

## Next Checks
1. **Ablation Study on Cultural Knowledge Sources:** Systematically remove different types of cultural information (history, rules, visual features) from dataset to quantify which knowledge types most impact model performance and isolate failure modes

2. **Cross-Domain Cultural Transfer Evaluation:** Test whether models performing well on CultSportQA also show improved performance on culturally specific queries in unrelated domains (traditional medicine or indigenous art) to validate broader cultural competence generalization

3. **Multimodal Model Architecture Comparison:** Compare different multimodal model architectures (vision transformer encoders, CLIP variants, custom cultural encoders) to determine whether visual-culture gap stems from general vision model limitations or specific architectural choices in current MLLMs