---
ver: rpa2
title: 'CLF-RL: Control Lyapunov Function Guided Reinforcement Learning'
arxiv_id: '2508.09354'
source_url: https://arxiv.org/abs/2508.09354
tags:
- reward
- tracking
- control
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of reward design in reinforcement
  learning (RL) for bipedal locomotion, which often leads to unstable policies and
  poor sim-to-real transfer. The authors propose a structured reward shaping framework
  called CLF-RL that integrates control Lyapunov functions (CLFs) with model-based
  reference planning.
---

# CLF-RL: Control Lyapunov Function Guided Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2508.09354
- **Source URL**: https://arxiv.org/abs/2508.09354
- **Reference count**: 35
- **Primary result**: CLF-based reward shaping improves bipedal locomotion stability and sim-to-real transfer by embedding Lyapunov decay conditions into RL training

## Executive Summary
This paper addresses the challenge of reward design in reinforcement learning for bipedal locomotion, which often leads to unstable policies and poor sim-to-real transfer. The authors propose a structured reward shaping framework called CLF-RL that integrates control Lyapunov functions (CLFs) with model-based reference planning. The method uses two types of reference trajectories—H-LIP for reduced-order online generation and HZD for full-order offline optimization—to construct CLF-based rewards that penalize tracking error and enforce convergence. The CLF decay condition is embedded into the RL reward to encourage stable behavior, reducing the need for extensive manual reward tuning.

## Method Summary
The CLF-RL framework combines model-based reference planning with reinforcement learning by constructing CLF-based rewards from H-LIP or HZD reference trajectories. The method generates reference outputs and computes Lyapunov functions relative to these references, then shapes the RL reward with both tracking error penalties and decay condition enforcement. The approach is validated on a Unitree G1 robot in simulation and hardware experiments, demonstrating improved velocity tracking and robustness under model perturbations.

## Key Results
- CLF-RL policies achieve up to 25% improvement in mean velocity tracking error compared to baseline RL methods
- Policies show significantly reduced variance under model perturbations and payload disturbances
- Hardware tests demonstrate robust performance across diverse flat-ground terrains

## Why This Works (Mechanism)

### Mechanism 1
Embedding the CLF decay condition into the reward signal guides policies toward stable limit cycles more effectively than instantaneous tracking error alone. Standard tracking rewards treat states with identical error magnitudes equally, even if one is diverging and the other converging. The CLF decay reward ($r_{\dot{v}}$) penalizes the system when the Lyapunov derivative $\dot{V}$ exceeds the decay rate $-\lambda V$. This provides a dense learning signal that explicitly rewards the dynamics of stabilization (convergence) rather than just positional correctness.

### Mechanism 2
Model-based reference trajectories reduce the exploration burden by restricting the policy search to the basin of attraction around a known stable gait. The framework uses H-LIP (reduced-order) or HZD (full-order) to generate reference outputs $y_d$. By constructing the CLF $V(\eta)$ relative to these references, the RL agent focuses on stabilizing a feasible motion rather than discovering the concept of walking from scratch.

### Mechanism 3
Structured CLF rewards lower performance variance under model perturbation compared to heuristic reward shaping. Heuristic rewards often contain conflicting gradients (e.g., "move fast" vs. "don't fall"). The CLF provides a mathematically consistent scalar metric of stability. By optimizing this single, theoretically grounded potential (plus minimal auxiliary rewards), the policy learns a more robust feedback controller that generalizes better to unseen mass distributions and disturbances.

## Foundational Learning

- **Control Lyapunov Functions (CLFs)**
  - Why needed here: The core innovation relies on translating the control-theoretic guarantee of stability ($\dot{V} < 0$) into a reinforcement learning reward.
  - Quick check question: If $\dot{V} > 0$, is the system becoming more or less stable relative to the reference?

- **Hybrid Zero Dynamics (HZD) & H-LIP**
  - Why needed here: The paper uses two distinct methods to generate the "reference." Understanding the trade-off between the reduced-order LIP model and the full-order HZD is critical for implementation.
  - Quick check question: Does H-LIP assume massless legs and constant Center of Mass (CoM) height?

- **Reward Shaping vs. Potential-Based Shaping**
  - Why needed here: The authors explicitly distinguish their method from potential-based shaping. Understanding that they add the CLF as an additional reward layer rather than replacing the value function is key to reproducing the training pipeline.
  - Quick check question: Why did the authors find that replacing the tracking reward entirely with potential shaping caused the policy to fail to learn?

## Architecture Onboarding

- **Component map**: Reference Generator -> CLF Calculator -> Reward Aggregator -> RL Loop (Isaac Lab PPO agent)
- **Critical path**: Generating the HZD gait library requires solving a nonlinear program (NLP) for the periodic orbit before training can begin. If this gait is infeasible, the CLF rewards will be noisy or misleading.
- **Design tradeoffs**:
  - *H-LIP vs. HZD*: H-LIP allows online velocity variation but assumes simplified dynamics. HZD offers high fidelity but requires a pre-computed library, limiting velocity agility.
  - *Hard Constraint vs. Soft Reward*: The paper uses soft rewards (CLF terms) rather than hard constraints, allowing the policy to momentarily violate stability conditions to learn recovery behaviors.
- **Failure signatures**:
  - Policy Freezes/Static Stand: If the decay reward weight ($w_{\dot{v}}$) is too high or the reference is unreachable, the policy may learn that doing nothing minimizes the penalty for divergence.
  - Potential Shaping Collapse: Replacing the tracking reward entirely with a Lyapunov potential caused learning failure ("policy never learned to walk").
  - Jitter/High Gain Behavior: If the P-matrix (from CARE) is tuned for overly aggressive convergence, the policy may output high-frequency joint corrections.
- **First 3 experiments**:
  1. Ablation on Decay Term: Train two policies, one with $r_{\dot{v}}$ and one without. Compare velocity tracking error variance on flat ground.
  2. Reference Fidelity Test: Train one policy with H-LIP references and one with HZD references. Deploy on hardware with added payload to see which generalizes better to mass uncertainty.
  3. Sim-to-Sim Stress Test: Apply domain randomization (friction, mass displacement) to the trained policy in a different physics engine to verify that the CLF reward structure robustly handles modeling errors better than a heuristic baseline.

## Open Questions the Paper Calls Out
1. Can formal stability guarantees be derived for the hybrid system when the CLF condition is enforced only as a soft reward constraint?
2. Does the CLF-RL framework generalize to non-flat or highly unstructured terrains where the assumptions of periodic orbits and constant CoM height are violated?
3. How sensitive is the training convergence to the quality and physical feasibility of the pre-computed reference trajectories?

## Limitations
- The framework's effectiveness is tightly coupled to the feasibility of the reference trajectory; if the H-LIP or HZD model cannot generate a dynamically feasible gait, the CLF rewards will be meaningless.
- The reliance on offline HZD gait optimization is a significant bottleneck, requiring substantial prior knowledge and computational effort before training can begin.
- The approach is currently validated only on flat terrain, leaving open questions about performance on unstructured environments.

## Confidence
- **High**: CLF decay reward provides a more informative learning signal than tracking error alone (supported by ablation and Table II).
- **Medium**: Model-based reference trajectories consistently improve sim-to-real transfer across all perturbation types (hardware results are promising but limited to flat terrain).
- **Low**: The Lyapunov function derived from the nominal model remains a valid stability certificate for the perturbed system under large structural uncertainties (assumption not explicitly validated beyond moderate mass perturbations).

## Next Checks
1. **Sim-to-Sim Stress Test**: Apply domain randomization (friction, mass displacement, motor noise) to the trained policy in a different physics engine to verify robust performance outside the training distribution.
2. **Sim-to-Real Transfer on Uneven Terrain**: Deploy the policy on hardware with varying ground heights and slopes to test stability beyond flat ground.
3. **Reference Trajectory Sensitivity**: Systematically degrade the accuracy of the H-LIP/HZD reference and measure the degradation in tracking performance and stability.