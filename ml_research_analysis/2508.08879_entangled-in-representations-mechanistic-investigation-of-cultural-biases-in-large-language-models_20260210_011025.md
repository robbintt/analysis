---
ver: rpa2
title: 'Entangled in Representations: Mechanistic Investigation of Cultural Biases
  in Large Language Models'
arxiv_id: '2508.08879'
source_url: https://arxiv.org/abs/2508.08879
tags:
- cultural
- knowledge
- llms
- culture
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cultural biases in large language models
  (LLMs) by introducing Culturescope, a mechanistic interpretability method that probes
  internal cultural knowledge representations. The authors propose a cultural flattening
  score to quantify how distinctive cultural knowledge is misrepresented through dominant
  cultures.
---

# Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models

## Quick Facts
- arXiv ID: 2508.08879
- Source URL: https://arxiv.org/abs/2508.08879
- Reference count: 40
- Key outcome: Introduces Culturescope to probe internal cultural knowledge representations, finding Western-dominance bias and cultural flattening in LLMs across three models and two datasets

## Executive Summary
This paper investigates cultural biases in large language models through mechanistic interpretability, introducing Culturescope to probe how cultural knowledge is represented internally. The authors find that LLMs exhibit Western-dominance bias and cultural flattening, where distinctive cultural knowledge becomes conflated with dominant cultures. Using activation patching and attention analysis on carefully constructed multiple-choice questions with hard negative options, they reveal asymmetric knowledge transfer patterns and show that low-resource cultures show weaker bias susceptibility primarily due to insufficient parametric knowledge rather than genuine fairness.

## Method Summary
The paper introduces Culturescope, a three-stage mechanistic interpretability pipeline that extracts internal cultural knowledge representations. First, an LLM answers open-ended cultural questions to generate hidden states. Second, a weighted sum of noun/verb hidden states is patched into an inspection prompt to elicit culture-specific knowledge. Third, generated concepts are filtered by semantic similarity to isolate culture-relevant knowledge. The method computes a Cultural Flattening (CF) Score using chi-square contributions to quantify how distinctive knowledge from target cultures appears in source cultures. For bias validation, the paper constructs MCQs with hard negative options from different resource levels and regions, then analyzes attention patterns from the final input token to option tokens to measure internal bias.

## Key Results
- Culturescope-augmented inputs achieve 0.5462 accuracy vs 0.4848 baseline on BLEnD cultural QA
- High-resource cultures show stronger cultural flattening (asymmetric knowledge transfer toward Western cultures)
- Attention contribution scores show 0.44 higher bias toward high-resource options when gold answers are low-resource
- Low-resource cultures show weaker susceptibility to cultural biases, attributed to insufficient parametric knowledge

## Why This Works (Mechanism)

### Mechanism 1: Culturescope Activation Patching Pipeline
- Claim: Internal cultural knowledge representations can be decoded by patching weighted hidden representations into an inspection prompt
- Mechanism: Three-stage pipeline: (1) Inference generates answer, (2) Scoping-in computes weighted sum of noun/verb hidden states and patches into inspection prompt, (3) Filtering removes non-cultural knowledge using semantic similarity thresholds
- Core assumption: Inspection prompt can elicit culture-specific knowledge used during inference
- Evidence anchors: [section 4.1] Bronzini et al. (2024) approach for condensing multi-token cultural answers; [section 7] 0.5462 vs 0.4848 accuracy; [corpus] cross-lingual alignment warns of cultural erasure
- Break condition: Patched representations produce generic/unrelated concepts even with high similarity thresholds

### Mechanism 2: Cultural Flattening Score (CF Score)
- Claim: Asymmetric chi-square contributions quantify how distinctive knowledge from target culture appears in source culture
- Mechanism: Compute chi-square contribution X_{k,y} = (max(0, g_{k,y} - E_{k,y}))^2 / E_{k,y}, sum over overlapping knowledge: F(y_t → y_s) = Σ X_{k,y_t} · I[X_{k,y_s} ≠ 0]
- Core assumption: Higher chi-square contributions indicate culturally distinctive knowledge
- Evidence anchors: [section 4.2] Higher values indicate large fraction of target culture's distinctive concepts in source culture; [section 6.1.1] Figure 3 shows asymmetric connections aligned with geographic proximity; [corpus] weak corpus evidence for this specific metric
- Break condition: Cultures with limited parametric knowledge appear "less biased" only due to sparse knowledge generation

### Mechanism 3: Attention Contribution to Hard Negative Options
- Claim: Attention weights from final input token to MCQ option tokens reveal internal bias toward dominant cultures during incorrect predictions
- Mechanism: Compute layer-wise attention a_{tc,ts}^l from final token t_s to option tokens t_c, average across layers, z-score normalize per sample
- Core assumption: Final input token serves as meaningful anchor for decision-relevant attention patterns
- Evidence anchors: [section 5.1] Yuksekgonul et al. (2024) highlights final input token as meaningful anchor; [section 6.1.2] Figure 4 shows significant higher attention to high-resource options (0.44 vs baseline); [corpus] Camellia, MCEval confirm LLMs favor Western-associated entities
- Break condition: Attention patterns don't correlate with behavioral bias

## Foundational Learning

- Concept: **Activation Patching / Patchscope**
  - Why needed here: Core technique for extracting internal representations without predefined probing classes
  - Quick check question: Can you explain why Patchscope is preferred over linear probes for multi-token cultural knowledge?

- Concept: **Chi-Square Test for Independence**
  - Why needed here: Statistical foundation for CF score—distinguishing culturally distinctive knowledge from shared knowledge
  - Quick check question: How does the paper's use of only positive residuals (max(0, g - E)) affect what the CF score measures?

- Concept: **Hard Negative Mining for MCQs**
  - Why needed here: Prevents models from exploiting surface-level elimination; reveals genuine cultural confusion
  - Quick check question: What distinguishes C_resource from C_region hard negatives, and what bias does each probe?

## Architecture Onboarding

- Component map: Inference stage -> Representation condensation -> Scoping-in stage -> Filtering -> CF computation -> Attention analysis
- Critical path:
  1. Run inference on cultural QA → extract hidden states at each layer
  2. For each layer, patch condensed representation → decode with inspection prompt
  3. Filter generated concepts by semantic similarity to original question
  4. Accumulate concepts per culture → compute pairwise CF scores
  5. Separately, run MCQ inference → extract attention patterns → aggregate by resource/region groups
- Design tradeoffs:
  - Patching all layers vs. specific components: Patches all layers to capture distributed cultural knowledge, trading precision for coverage
  - Noun/verb weighting vs. uniform: Focuses on semantic content but may miss culturally significant function words or proper nouns not tagged correctly
  - MCQ hard negatives vs. open-ended evaluation: Hard negatives provide controlled bias probing but may not reflect real-world generation patterns
- Failure signatures:
  - Low semantic similarity scores in filtering (Table 8: irrelevant patching ~0.14 vs. English ~0.32) indicates patching failed to elicit cultural knowledge
  - CF scores showing near-zero flows for low-resource cultures may indicate knowledge sparsity, not bias resistance
  - Attention heatmaps with uniform scores across options suggest model isn't distinguishing culturally
- First 3 experiments:
  1. Validation run: Apply Culturescope to single culture (e.g., Greece), manually inspect decoded concepts at layers 5, 10, 15, 20 to verify cultural relevance
  2. Irrelevant patching baseline: Patch Gaussian noise vectors with same inspection prompt; confirm semantic similarity scores are significantly lower than real patches (target: <0.20 vs ~0.30+ for real)
  3. Attention correlation check: For 50 MCQ instances, correlate attention contribution scores with model's final choice; verify hard negative options receive higher attention when model answers incorrectly

## Open Questions the Paper Calls Out

- **Open Question 1**: How can tailored mitigation approaches be developed that account for the distinct effects of bias and resource levels on LLMs' cultural understanding?
  - Basis: Conclusion states future work should develop approaches considering bias and resource levels impacts
  - Why unresolved: High-resource and low-resource cultures exhibit different bias patterns requiring differentiated intervention strategies
  - What evidence would resolve it: Comparative experiments applying different mitigation techniques separately to high- and low-resource culture representations

- **Open Question 2**: Do cultural bias patterns identified in 7-8B parameter models scale consistently to larger models?
  - Basis: Limitations section notes reporting results with 8B models due to computational constraints
  - Why unresolved: Cultural knowledge representation may scale non-linearly with model size
  - What evidence would resolve it: Replication of Culturescope analysis on models of varying scales (70B, 405B parameters)

## Limitations

- **Knowledge Sparsity vs. Bias Resistance**: Low-resource cultures showing weaker bias susceptibility could reflect genuine resistance or simply insufficient parametric knowledge, making it difficult to distinguish absence from resistance
- **Layer Selection Ambiguity**: The Culturescope methodology requires selecting specific layers for representation extraction, but the paper doesn't clearly specify which layers were used or whether results are sensitive to this choice
- **Attention Anchor Validity**: The use of final input token as attention anchor assumes this token captures decision-relevant information, but this assumption isn't rigorously validated

## Confidence

- **High Confidence**: Culturescope methodology works as described (Section 7 validation shows 0.5462 accuracy vs 0.4848 baseline), and basic finding of Western-dominance bias in MCQ performance is well-supported
- **Medium Confidence**: CF score effectively quantifies asymmetric cultural knowledge transfer patterns, though interpretation of low-resource cultures being "less susceptible" should be qualified as potentially reflecting knowledge sparsity
- **Low Confidence**: Mechanistic claim that attention from final input token causally drives cultural bias in MCQ predictions, as this requires stronger causal validation than provided correlation evidence

## Next Checks

1. **Knowledge Sparsity Control**: Run Culturescope on artificially knowledge-limited versions of high-resource cultures to determine whether reduced CF scores reflect true bias resistance or simply insufficient knowledge to exhibit bias

2. **Layer Sensitivity Analysis**: Systematically vary the patching layer across all available layers and measure how CF scores and semantic similarity scores change; plot these as layer-wise curves to identify whether cultural knowledge concentrates at specific depths

3. **Attention Anchor Validation**: For 100 MCQ instances, extract attention patterns from multiple anchor tokens (first, middle, last) and correlate each with model predictions; if final token doesn't show consistently stronger correlation with behavioral bias than other positions, attention contribution methodology needs revision