---
ver: rpa2
title: Leveraging Axis-Aligned Subspaces for High-Dimensional Bayesian Optimization
  with Group Testing
arxiv_id: '2504.06111'
source_url: https://arxiv.org/abs/2504.06111
tags:
- active
- dimensions
- optimization
- gtbo
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GTBO, a Bayesian optimization algorithm designed
  for high-dimensional problems with axis-aligned active subspaces. The key innovation
  is a group testing phase that identifies which input dimensions significantly influence
  the objective function before optimization begins.
---

# Leveraging Axis-Aligned Subspaces for High-Dimensional Bayesian Optimization with Group Testing

## Quick Facts
- arXiv ID: 2504.06111
- Source URL: https://arxiv.org/abs/2504.06111
- Authors: Erik Hellsten; Carl Hvarfner; Leonard Papenmeier; Luigi Nardi
- Reference count: 40
- Primary result: Introduces GTBO, a Bayesian optimization algorithm that uses group testing to identify active dimensions in high-dimensional problems, achieving strong performance on synthetic and real-world benchmarks while providing interpretability through explicit dimension identification.

## Executive Summary
This paper introduces GTBO, a Bayesian optimization algorithm designed for high-dimensional problems with axis-aligned active subspaces. The key innovation is a group testing phase that identifies which input dimensions significantly influence the objective function before optimization begins. GTBO adapts group testing theory to continuous domains by evaluating the function at perturbed points and using probabilistic analysis to determine which dimensions are active. The algorithm first runs a testing phase using sequential Monte Carlo to estimate the probability that each dimension is active, then performs Bayesian optimization focusing on the identified active subspace with shorter length scales for active dimensions and much longer ones for inactive dimensions.

## Method Summary
GTBO operates in two phases: first, a group testing phase uses sequential Monte Carlo to identify active dimensions by evaluating the function at perturbed points and computing mutual information to guide group selection; second, a Bayesian optimization phase uses a Gaussian process surrogate with asymmetric lengthscale priors (short for active, extremely long for inactive dimensions) to optimize the objective function. The method was evaluated on synthetic benchmarks extended to 300 dimensions and two real-world benchmarks, successfully identifying all active dimensions with a false positive rate of only 0.05% across benchmarks.

## Key Results
- GTBO successfully identified all active dimensions with a false positive rate of only 0.05% across benchmarks
- On Mopta08 benchmark, GTBO outperformed state-of-the-art methods (SAASBO, BAxUS, TuRBO) after the testing phase
- The method achieved strong performance on synthetic benchmarks but faced challenges on LassoDNA where dimensions may not be strictly active or inactive
- GTBO provides improved interpretability by explicitly identifying relevant dimensions while maintaining competitive optimization performance

## Why This Works (Mechanism)

### Mechanism 1: Group Testing for Active Subspace Identification
GTBO evaluates a default point and tests groups of dimensions by perturbing them. If the function value changes significantly, the group likely contains active dimensions. A probabilistic model uses two Gaussian distributions: one for noise (inactive groups) and one for signal (active groups). Sequential Monte Carlo maintains particles representing possible active dimension configurations, and mutual information guides group selection to maximize learning per evaluation.

### Mechanism 2: Lengthscale Prior Specialization
After identifying active dimensions, GTBO uses asymmetric lengthscale priors to focus optimization on the active subspace. The GP surrogate model receives short lengthscale priors for active dimensions (allowing rapid variation) and extremely long lengthscale priors for inactive dimensions (effectively "turning them off"). This concentrates model capacity where it matters, alleviating the curse of dimensionality.

### Mechanism 3: Two-Phase Budget Allocation
Separating dimension identification from optimization allows each phase to be optimized for its specific objective. Phase 1 (Group Testing) uses information-theoretic acquisition (mutual information maximization) to identify active dimensions. Phase 2 (Bayesian Optimization) uses standard BO acquisition (qLogNoisyExpectedImprovement) for optimization. The GT phase terminates when marginal probabilities converge to confidence thresholds.

## Foundational Learning

- Concept: **Group Testing Theory** - Why needed: Understanding how to efficiently identify "defective" elements (active dimensions) by testing pools rather than individuals. Quick check: Can you explain why testing groups of dimensions together can be more efficient than testing each dimension individually when active dimensions are rare?
- Concept: **Sequential Monte Carlo (SMC) Samplers** - Why needed: GTBO uses SMC to represent the posterior distribution over active dimension configurations, which is intractable to compute exactly (2^D states). Quick check: How do weighted particles in an SMC sampler approximate a posterior distribution, and why is resampling necessary?
- Concept: **Gaussian Process Lengthscale Priors** - Why needed: Understanding how lengthscale hyperparameters control smoothness of GP predictions and how priors encode structural assumptions. Quick check: What happens to GP predictions if you set a very long lengthscale for a dimension versus a very short one?

## Architecture Onboarding

- Component map: Default point evaluator -> Variance estimator -> Group selector (MI maximization) -> Function evaluator -> SMC posterior updater -> Convergence checker -> GP surrogate with specialized lengthscales -> qLogNEI acquisition optimizer -> Standard BO loop
- Critical path: 1. Initialize by evaluating default point multiple times (n_def evaluations) 2. Estimate noise and signal variance by perturbing along dimension bins 3. Iterate: select group maximizing MI, evaluate, update SMC particles 4. Terminate GT when all marginals converge to confidence thresholds 5. Classify dimensions active if marginal > η (default 0.5) 6. Initialize BO with GT phase data (deduplicate near-identical points) 7. Run standard BO with asymmetric lengthscale priors
- Design tradeoffs: max_act parameter must exceed true number of active dimensions for noise estimation; prior probability q=0.05 balances exploration vs exploitation in GT; convergence thresholds C_lower=5e-3, C_upper=0.9; batch size up to 5 groups per batch for parallel evaluation
- Failure signatures: LassoDNA pattern (many dimensions marginally active), high noise (signal-to-noise ratio too low), too many active dimensions (> √D active), non-axis-aligned subspace (active subspace involves rotations)
- First 3 experiments: 1. Replicate Figure 2 on Branin2-300D: Verify GT correctly identifies 2 active dimensions 2. Ablate max_act parameter: Run Griewank8-100D with max_act ∈ {5, 10, 15, 20} 3. Test on Mopta08: Run full GTBO with 1000 evaluations and compare performance post-GT phase against random search baseline

## Open Questions the Paper Calls Out
1. How can the group testing framework be modified to maintain robustness and accuracy in environments with very high observation noise? The authors intend to investigate the impact of their GT framework in the presence of noise.
2. Can the methodology be extended to handle dimensions with continuous or "soft" influence rather than strictly binary active/inactive states? GTBO faced challenges on LassoDNA where dimensions may not be strictly active or inactive.
3. Can an early-stopping mechanism be developed to detect violations of the axis-aligned subspace assumption and abort the testing phase to save the evaluation budget? The method might waste budget identifying all variables as relevant when the sparsity assumption is violated.

## Limitations
- Performance relies heavily on the axis-aligned active subspace assumption, which is not universally applicable
- Requires specifying max_act (maximum active dimensions) in advance, and underestimation leads to catastrophic failure
- The GT phase consumes substantial budget before optimization begins, making it less suitable for problems where early solutions are valuable

## Confidence
- High confidence: The core mechanism of group testing for active dimension identification works as described for problems with strictly axis-aligned subspaces
- Medium confidence: The asymmetric lengthscale priors effectively focus optimization on the active subspace
- Medium confidence: The two-phase approach provides better overall performance than interleaved methods

## Next Checks
1. Run GTBO on benchmarks with rotated active subspaces (not axis-aligned) to quantify performance degradation when the core assumption fails
2. Systematically vary max_act relative to true active dimension count across multiple benchmarks to identify failure thresholds
3. Compare performance when reducing GT phase budget versus total evaluation budget to understand optimal phase balance for different problem types