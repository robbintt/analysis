---
ver: rpa2
title: Geometry of Decision Making in Language Models
arxiv_id: '2511.20315'
source_url: https://arxiv.org/abs/2511.20315
tags:
- llama2
- gpt-2
- gpt-neo
- llama3
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Large Language Models (LLMs) make decisions
  by analyzing the intrinsic dimension (ID) of hidden representations across layers.
  Using 28 open-weight transformer models, the authors estimate ID at each layer with
  multiple estimators (MLE, TwoNN, GRIDE) while tracking performance on multiple-choice
  question answering (MCQA) tasks.
---

# Geometry of Decision Making in Language Models

## Quick Facts
- arXiv ID: 2511.20315
- Source URL: https://arxiv.org/abs/2511.20315
- Reference count: 40
- Key outcome: Analysis reveals "hunchback" intrinsic dimension pattern in LLM hidden states that correlates with decision-making and prediction confidence

## Executive Summary
This study investigates how Large Language Models make decisions by analyzing the intrinsic dimension of hidden representations across layers. Using 28 open-weight transformer models and multiple ID estimators, the authors identify a consistent "hunchback" pattern where intrinsic dimension rises in early layers, peaks in middle layers, and declines in later layers. This peak coincides with the onset of confident predictions, suggesting ID as a geometric marker of decision-making. The research reveals that MLP outputs show sharper ID transitions than residual post-activations, indicating their role in injecting task-specific refinements. Few-shot prompting accelerates compression, reducing final-layer ID and improving accuracy, especially in larger models.

## Method Summary
The authors analyze 28 open-weight transformer models across multiple families and scales, estimating intrinsic dimension at each layer using MLE, TwoNN, and GRIDE estimators. They track performance on multiple-choice question answering tasks while monitoring ID changes. The study examines both residual post-activation states and MLP outputs, comparing how different components contribute to representational geometry. Few-shot prompting experiments are conducted to assess how additional context affects ID dynamics and model performance.

## Key Results
- Consistent "hunchback" intrinsic dimension pattern across all models: rising in early layers, peaking in middle layers, declining in later layers
- Peak ID values coincide with onset of confident predictions, establishing geometric marker of decision-making
- MLP outputs show sharper ID transitions than residual post-activations, suggesting MLPs inject task-specific refinements
- Few-shot prompting accelerates compression, reducing final-layer ID and improving accuracy in larger models

## Why This Works (Mechanism)
The hunchback pattern emerges from the interplay between representation expansion (early layers exploring diverse features) and compression (later layers converging to decision-relevant manifolds). As models process information, they initially increase representational capacity to capture relevant features, then progressively compress these representations into lower-dimensional manifolds that support confident predictions. MLP layers appear to drive sharper compression by injecting task-specific refinements that prune irrelevant dimensions. Few-shot prompting provides additional context that guides this compression process more efficiently, particularly benefiting larger models with greater representational capacity.

## Foundational Learning

**Intrinsic Dimension Estimation**: Understanding how to quantify the true dimensionality of data manifolds within high-dimensional spaces - needed to measure representational efficiency and compression patterns across layers.

**Transformer Architecture**: Knowledge of residual connections, layer normalization, and MLP/attention mechanisms - required to interpret where and how ID changes occur within the model.

**Information Bottleneck Principle**: Understanding how models balance compression and prediction - crucial for interpreting why ID decreases in later layers despite maintaining performance.

**Quick Check**: Verify that ID estimators consistently capture relative patterns despite absolute value variations across different methods.

## Architecture Onboarding

**Component Map**: Input -> Embedding -> Multiple Transformer Layers (Attention + MLP) -> Output Head -> Prediction

**Critical Path**: Input tokens flow through embedding layer, then sequential transformer layers where ID dynamics occur, culminating in output head that produces predictions based on compressed representations.

**Design Tradeoffs**: Balance between representational capacity (higher ID allowing more complex features) and computational efficiency (lower ID enabling faster inference and better generalization).

**Failure Signatures**: Flattened ID curves suggest underfitting; excessively high final-layer ID indicates failure to compress to decision-relevant manifolds; mismatched peaks between layers suggest architectural issues.

**First Experiments**: 1) Compare ID patterns across different model scales to identify scaling effects. 2) Test whether removing MLPs affects ID compression sharpness. 3) Examine ID dynamics on non-MCQA tasks to assess pattern generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on MCQA tasks may limit generalizability to other task types with different decision-making processes
- Focus on open-weight models leaves uncertainty about whether patterns hold for proprietary architectures
- Lack of mechanistic explanation for why MLPs specifically drive sharper ID compression compared to attention mechanisms

## Confidence
**High confidence**: Hunchback pattern as geometric marker of decision-making across diverse models and datasets
**Medium confidence**: MLP outputs driving sharper ID transitions compared to residual activations
**Medium confidence**: Few-shot prompting accelerating compression and improving performance correlation

## Next Checks
1. Test hunchback pattern persistence across diverse task types (generation, classification, reasoning) beyond MCQA to establish generality
2. Conduct ablation studies isolating MLP and attention contributions to determine which components drive ID compression patterns
3. Perform intervention experiments constraining ID at different layers to measure direct effects on prediction confidence and accuracy