---
ver: rpa2
title: 'MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics,
  Reinforcement Learning, and Category Theory'
arxiv_id: '2507.11821'
source_url: https://arxiv.org/abs/2507.11821
tags:
- semantic
- dataset
- learning
- hierarchical
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MNIST-Gen, a modular framework for generating
  MNIST-style datasets using hierarchical semantic categorization, CLIP-based semantic
  understanding, and reinforcement learning. The system addresses the lack of domain-specific,
  lightweight datasets by automating image retrieval, preprocessing, and intelligent
  categorization with multiple processing modes.
---

# MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory

## Quick Facts
- arXiv ID: 2507.11821
- Source URL: https://arxiv.org/abs/2507.11821
- Reference count: 37
- Primary result: Framework for generating domain-specific MNIST-style datasets with 85% automatic categorization accuracy and 80% time savings versus manual approaches

## Executive Summary
MNIST-Gen is a modular framework that generates domain-specific MNIST-style datasets using hierarchical semantic categorization, CLIP-based semantic understanding, and reinforcement learning. The system automates image retrieval, preprocessing, and intelligent categorization while ensuring flexibility through category-theoretic composable morphisms. By modeling the pipeline as composable morphisms, it enables easy modification of individual processing steps without breaking the overall workflow.

## Method Summary
The framework uses hierarchical semantic graphs to define dataset categories, then retrieves images via Unsplash API or Kaggle. A CLIP-based semantic module scores images against category definitions using text similarity, characteristic matching, and visual attribute compatibility. A Deep Q-Network agent reviews these scores to make Keep/Discard/Review decisions, optimizing for dataset balance and quality. The system outputs 28x28 grayscale images after applying sequential transformations including resize, crop, and binarization. The approach was validated on Tree-MNIST (4 classes) and Food-MNIST (10 classes), demonstrating significant efficiency improvements over manual annotation.

## Key Results
- Achieved 85% automatic categorization accuracy using hierarchical semantic filtering
- Reduced dataset generation time by 80% compared to manual annotation approaches
- CNN classifiers achieved 86.51% accuracy on Tree-MNIST and 73.84% on Food-MNIST

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Filtering
The system computes a semantic score by aggregating CLIP text-to-image similarity, characteristic matching, and visual attribute compatibility. This hierarchical approach routes images into specific subcategories or rejects them, achieving 85% automatic categorization accuracy. The method fails when visual characteristics overlap heavily across subcategories or when keyword ambiguity propagates through CLIP embeddings.

### Mechanism 2: RL-Driven Adaptive Pruning
A Deep Q-Network agent observes image states and selects actions (Keep, Discard, Review) based on a reward function that maximizes semantic confidence and class distribution entropy while minimizing redundancy. The RL + Semantic version showed 5-8% improvement over baseline by removing noisy or ambiguous images. Instability occurs if reward weights are misspecified or the replay buffer lacks diversity.

### Mechanism 3: Composable Morphisms (Pipeline Modularity)
The pipeline is formalized as a functor mapping raw image categories to MNIST-style datasets, with transformations composed sequentially. This ensures changes to one stage don't propagate side effects to downstream formatting. The abstraction leaks if transformations require shared global state not captured in morphism definitions.

## Foundational Learning

- **Concept: CLIP (Contrastive Language-Image Pre-training)**
  - Why needed here: CLIP maps images and text into the same vector space, enabling the semantic scoring math to work.
  - Quick check question: If two images have different visual content but similar CLIP embeddings, what does that imply about their semantic meaning to the model?

- **Concept: Category Theory (Functors & Morphisms)**
  - Why needed here: The paper structures its code architecture around this abstraction for modularity.
  - Quick check question: If you replace the "Grayscale" morphism with a "Color Jitter" morphism, does the Functor still hold, and why?

- **Concept: Deep Q-Learning (DQN) Basics**
  - Why needed here: The adaptive filtering relies on an agent learning a policy through state-action-reward loops.
  - Quick check question: What happens to the agent's behavior if λ₄ (Redundancy penalty) is set too high?

## Architecture Onboarding

- **Component map**: Retrieval Module → Semantic Module → RL Module → Transformation Module → Processing Modes
- **Critical path**: The Semantic Module is the bottleneck; noisy CLIP similarity scores propagate errors through the entire pipeline.
- **Design tradeoffs**: Resolution vs. Speed (28x28 grayscale allows rapid benchmarking but destroys fine detail); Automation vs. Accuracy ("Fast Batch" reduces human decisions by 90% but drops accuracy by ~6%).
- **Failure signatures**: Semantic Drift (CLIP filter passes mismatched categories); RL Collapse (agent discards everything to maximize balance reward).
- **First 3 experiments**:
  1. Generate Tree-MNIST using default config to verify ~86% CNN accuracy.
  2. Run Food-MNIST generation using "Smart Batch" vs. "Fast Batch" to quantify efficiency/accuracy trade-off.
  3. Introduce noisy keywords into hierarchy to test CLIP scoring's ability to filter mismatches.

## Open Questions the Paper Calls Out

### Open Question 1
Can the RL agent's selection policies generalize across diverse domains without requiring domain-specific reward function tuning? The current RL module requires reward function tuning and semantic feature engineering per domain; cross-domain transfer mechanisms remain unexplored.

### Open Question 2
How does categorization accuracy and downstream model performance scale as the number of hierarchical categories increases beyond 10 classes? The framework has only been validated on 4-class and 10-class datasets; computational complexity suggests potential bottlenecks.

### Open Question 3
Does extending MNIST-Gen to configurable color and higher-resolution formats preserve the framework's modularity and 80% time savings while improving performance on fine-grained tasks? The current low-resolution grayscale format is insufficient for tasks requiring high visual fidelity.

## Limitations
- Semantic scoring weights (α, β, γ) are unspecified, introducing variability in filtering accuracy claims
- Category-theoretic abstraction may be more theoretical than practical, with unclear engineering benefits
- Superior RL + Semantic mode claims based on single dataset without ablation on reward weight tuning

## Confidence

- **High confidence**: Core modular pipeline (retrieval → semantic filtering → RL pruning → preprocessing) is well-specified and reproducible
- **Medium confidence**: Reported 85% categorization accuracy and 80% time savings are plausible but depend on unspecified parameters
- **Low confidence**: Claimed superiority of RL + Semantic mode over manual curation needs broader validation

## Next Checks

1. **Ablation on Semantic Weights**: Systematically vary α, β, γ in Eq. 4 to quantify their impact on categorization accuracy and downstream CNN performance.
2. **Reward Function Stability**: Test RL agent performance with fixed vs. periodically retrained lightweight models to assess sensitivity to reward estimation noise.
3. **Category-Theoretic Overhead**: Compare development time and code complexity with and without morphism/functor abstraction to validate practical modularity benefits.