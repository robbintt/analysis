---
ver: rpa2
title: Algorithm for Semantic Network Generation from Texts of Low Resource Languages
  Such as Kiswahili
arxiv_id: '2501.09326'
source_url: https://arxiv.org/abs/2501.09326
tags:
- language
- algorithm
- such
- languages
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research developed a rule-based algorithm to process low-resource
  languages like Kiswahili by mapping their subject-verb-object (SVO) structure into
  the subject-predicate-object (SPO) structure of semantic networks, bypassing the
  need for training data. The algorithm identifies verbs as anchors and extracts suitable
  subject-verb-object triples based on parts of speech, enabling practical NLP tasks
  such as question answering.
---

# Algorithm for Semantic Network Generation from Texts of Low Resource Languages Such as Kiswahili

## Quick Facts
- **arXiv ID:** 2501.09326
- **Source URL:** https://arxiv.org/abs/2501.09326
- **Reference count:** 10
- **One-line primary result:** Rule-based algorithm maps SVO POS tags to SPO triples without training data, achieving 78.6% EM accuracy on Kiswahili QA.

## Executive Summary
This paper presents a rule-based algorithm to generate semantic networks from low-resource languages like Kiswahili by leveraging their SVO structure. The method maps Subject-Verb-Object parts of speech to Subject-Predicate-Object triples without requiring training data. Tested on Swahili datasets (Tusome, TyDiQA, and KenSwQuAD), the approach achieves up to 78.6% exact match accuracy on question answering tasks. The algorithm uses verbs as anchors to extract subject-verb-object combinations, creating a graph structure suitable for machine processing of low-resource languages.

## Method Summary
The algorithm processes low-resource SVO languages by identifying verbs as structural anchors and extracting nouns from both sides to form SPO triples. It uses POS tagging to locate verbs, then mines nouns from the left (subjects) and right (objects) to generate a Cartesian product of possible triples. The method handles punctuation through heuristic splitting and recombination procedures, particularly for commas. Triples are stored in RDF format and queried via SPARQL for question answering. The approach bypasses the need for training data by relying on structural language properties rather than statistical models, making it suitable for languages with limited NLP resources.

## Key Results
- **78.6% EM accuracy** on KenSwQuAD and **64.8% EM accuracy** on TyDiQA for question answering tasks
- Successfully processes complex sentences by generating multiple SPO triples per verb through Cartesian product
- Demonstrates feasibility of semantic network generation without training data for low-resource languages
- Identifies specific challenges including named entity fragmentation and co-reference resolution gaps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping natural language (NL) to semantic networks is possible without training data by leveraging the structural isomorphism between Subject-Verb-Object (SVO) languages and Subject-Predicate-Object (SPO) triples.
- **Mechanism:** The algorithm treats the **Verb (V)** as a static anchor or pivot. It scans the Left Hand Side (LHS) of the verb to extract all Nouns (Subjects) and the Right Hand Side (RHS) to extract all Nouns (Objects). It then stitches every LHS noun to every RHS noun using the verb as the connector.
- **Core assumption:** The input language strictly follows SVO word order and that a reliable Part-of-Speech (POS) tagger exists for the target low-resource language.
- **Evidence anchors:**
  - [Abstract]: "...SVO parts of speech tags can map into a semantic network triple... bypassing the need for training data."
  - [Section 2.0]: "The algorithm therefore determines the position of the verb (V)... then does several iterations to mine the POS on the left of the ‘V’, to get all the subjects."
  - [Corpus]: Implicitly supported by general NLP literature on low-resource languages (e.g., paper 12093), though specific validation of this *specific* algorithm relies on the paper's internal results.
- **Break condition:** The mechanism fails if the sentence structure deviates significantly from SVO (e.g., SOV or VSO without adaptation) or if the POS tagger misidentifies the verb.

### Mechanism 2
- **Claim:** Semantic coverage for Question Answering (QA) can be maximized by generating a Cartesian product of all subject-object combinations surrounding a verb.
- **Mechanism:** Instead of selecting one specific subject or object, the algorithm creates an array of dimension LHS ($L$) $\times$ RHS ($R$). It generates $n$ triples for a single verb where $n = \text{count}(L) \times \text{count}(R)$. This ensures that if a query asks about any specific entity relationship in a complex sentence, the triple exists in the graph.
- **Core assumption:** Redundant or "correct but obvious" triples do not negatively impact the retrieval accuracy for the specific QA task tested.
- **Evidence anchors:**
  - [Section 2.0]: "A final array of dimension LHS (L) x RHS (R) is now created. Any combination of L(1..n) + V + R(1..n) are candidate S-V-O triples."
  - [Section 2.0]: Example provided where 2 subjects and 3 objects yield 6 possible SPO combinations.
- **Break condition:** Performance may degrade in sentences with many nouns where the combinatorial explosion creates noisy triples that obscure the correct answer.

### Mechanism 3
- **Claim:** Ambiguity in sentence boundaries caused by punctuation (specifically commas) can be resolved heuristically by splitting and recombining phrases.
- **Mechanism:** The algorithm includes a "Procedure COMMA_found" which splits the sentence at the comma. It processes the first phrase independently, then creates a "NEW_PHRASE" by combining the start of the sentence (up to the verb) with the text following the comma. This attempts to capture subject-verb linkages across interrupted clauses.
- **Core assumption:** Commas primarily function to separate enumerations or conjoined phrases that share a verb with the main subject.
- **Evidence anchors:**
  - [Fig. 2 / Section 2.0]: Lines 48-53 of the algorithm define the procedure for recombining sentence parts across commas.
  - [Section 4.0]: "One problem noted is the determination of intention of the COMMA... The algorithm has just modelled a few commonly used constructs."
- **Break condition:** Fails when commas are used for parenthetical remarks, complex nested clauses, or numeric formatting (e.g., "41,837") which the paper notes requires specific handling.

## Foundational Learning

- **Concept: Semantic Networks & RDF Triples**
  - **Why needed here:** The entire output of the system is a graph of triples (Subject-Predicate-Object), not a vector embedding. You must understand how data is stored as relationships (edges) rather than just text.
  - **Quick check question:** Can you distinguish between a "node" (entity) and an "edge" (relationship) in a graph of "Chelsea -> won -> Trophy"?

- **Concept: Part-of-Speech (POS) Tagging**
  - **Why needed here:** The algorithm is purely syntactic; it does not "understand" semantics. It relies 100% on the accuracy of the POS tagger to identify NOUNs and VERBs to build the graph.
  - **Quick check question:** If a tagger mislabels the noun "Google" as a verb, how would that break the Subject-Object extraction?

- **Concept: Low-Resource Language Constraints**
  - **Why needed here:** This architecture is a direct response to the inability to use Large Language Models (LLMs) due to lack of training data. Understanding this constraint explains *why* the authors chose a rule-based system over a neural one.
  - **Quick check question:** Why can't we just fine-tune BERT for Kiswahili in this context?

## Architecture Onboarding

- **Component map:** Input: Raw text (e.g., Kiswahili Wikipedia) -> Pre-processor: Splits text by sentences and punctuation (Commas) -> Tagger: Part-of-Speech (POS) tagger (external dependency) -> Extractor: The Core Algorithm (Fig 2) that finds Verbs and arrays of Nouns -> Generator: Creates RDF Triples (LHS + V + RHS) -> Store: Triple store (RDF database) -> Query Interface: SPARQL endpoint for Question Answering

- **Critical path:** The **Verb Identification (Line 14-17)**. If the algorithm fails to find a verb in a phrase, it falls back to "Other Rules" (like 'is-a' relationships). If these fail, the sentence is skipped entirely, resulting in lost information.

- **Design tradeoffs:**
  - **Precision vs. Recall:** The Cartesian product mechanism (Mechanism 2) prioritizes high recall (generating many triples) at the risk of creating false or nonsensical relationships (low precision).
  - **Rule-based vs. ML:** The system trades the nuance and adaptability of ML models for the "zero-shot" capability of working without training data.

- **Failure signatures:**
  - **Named Entity Fragmentation:** "Chelsea Football Club" might be split into three separate nodes ("Chelsea", "Football", "Club") linked to the object, rather than one single node, because the POS tagger sees three proper nouns. (Section 4.0).
  - **Coreference Gaps:** The graph will appear disconnected if Sentence 1 uses "Chelsea" and Sentence 2 uses "it", as the algorithm does not resolve "it" back to "Chelsea". (Section 4.0).
  - **Static Punctuation Rules:** Sentences with colons, semicolons, or complex numbering may be parsed incorrectly or skipped.

- **First 3 experiments:**
  1. **Unit Test the Extractor:** Input the specific phrase "Chelsea Football Club ni klabu..." and verify if the output matches the T1-T3 triples in Table 1 exactly.
  2. **Comma Stress Test:** Input a sentence with a non-enumeration comma (e.g., "However, the team...") and observe if the "Procedure COMMA_found" creates a false triple or crashes.
  3. **SPARQL Validation:** Load the generated RDF triples into a standard triple store (like Apache Jena or RDFLib) and run the query provided in Section 3.4 to confirm the answer "1905" is retrievable.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the algorithm be adapted to process Subject-Object-Verb (SOV) structured languages effectively?
- **Basis in paper:** [explicit] The authors state, "An immediate research opportunity is using the algorithm to process SOV-type low-resource languages."
- **Why unresolved:** The current implementation is optimized for SVO languages (like Kiswahili), anchoring on the verb to find subjects and objects, whereas SOV languages require a different parsing logic.
- **What evidence would resolve it:** Successful generation of semantic networks from SOV language corpora with comparable accuracy to the current Kiswahili results.

### Open Question 2
- **Question:** Does integrating Named Entity Recognition (NER) significantly improve the semantic accuracy of the extracted triples?
- **Basis in paper:** [explicit] The authors note a shortcoming where named entities like "Chelsea Football Club" are incorrectly decomposed into separate nouns because the system lacks NER tools.
- **Why unresolved:** Without NER, the algorithm treats multi-word proper nouns as distinct, unrelated subjects, diluting the semantic precision of the network.
- **What evidence would resolve it:** A comparative study measuring triple accuracy before and after applying NER to the pre-processing pipeline.

### Open Question 3
- **Question:** To what extent does co-reference resolution improve the connectivity of the generated semantic networks?
- **Basis in paper:** [explicit] The paper identifies that graphs are often disconnected because the algorithm fails to resolve pronouns (e.g., "it") to their specific antecedents.
- **Why unresolved:** The current rule-based approach processes tokens locally, lacking the broader context required to link pronouns in later sentences to subjects in earlier ones.
- **What evidence would resolve it:** Visualization of fully connected graphs and improved QA scores on multi-sentence contexts after implementing a co-reference module.

## Limitations
- **Named entity fragmentation:** Multi-word proper nouns like "Chelsea Football Club" are split into separate triples, reducing semantic precision.
- **Punctuation heuristic fragility:** Comma handling relies on rules that may misinterpret complex clause structures or numeric formatting.
- **Co-reference resolution gaps:** The algorithm fails to link pronouns across sentences, resulting in disconnected graphs and missed information.

## Confidence
- **High confidence:** The core mechanism of mapping SVO structures to SPO triples using verbs as anchors is well-supported by the algorithm description and example outputs.
- **Medium confidence:** The reported QA accuracy results are plausible given the methodology, though the manual SPARQL generation process for evaluation raises questions about reproducibility.
- **Low confidence:** The generalizability of this approach to non-SVO languages or more complex linguistic phenomena (nested clauses, coreference) remains unproven.

## Next Checks
1. **Reimplement the core extraction algorithm** using a standard Swahili POS tagger and verify it produces the exact T1-T3 triples from the Chelsea example.
2. **Test comma handling robustness** by evaluating performance on sentences with parenthetical clauses versus enumeration lists to quantify the impact of the COMMA_found procedure.
3. **Assess named entity fragmentation** by measuring how often multi-word entities (organizations, locations) are split into separate triples and estimating the precision cost.