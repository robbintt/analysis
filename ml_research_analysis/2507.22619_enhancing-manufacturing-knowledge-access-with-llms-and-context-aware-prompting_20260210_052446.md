---
ver: rpa2
title: Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting
arxiv_id: '2507.22619'
source_url: https://arxiv.org/abs/2507.22619
tags:
- sparql
- llms
- ontology
- queries
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of enabling non-experts to query
  manufacturing knowledge graphs (KGs) without requiring expertise in SPARQL. It proposes
  an LLM-based framework that translates natural language questions into SPARQL queries
  by providing the model with context-aware KG schema information.
---

# Enhancing Manufacturing Knowledge Access with LLQLMs and Context-aware Prompting

## Quick Facts
- arXiv ID: 2507.22619
- Source URL: https://arxiv.org/abs/2507.22619
- Reference count: 29
- One-line primary result: Context-aware schema reduction and domain-specific examples improve LLM-based SPARQL query generation accuracy by 20-30% and 5-8% respectively for manufacturing knowledge graphs.

## Executive Summary
This paper addresses the challenge of enabling non-experts to query manufacturing knowledge graphs without requiring expertise in SPARQL. It proposes an LLM-based framework that translates natural language questions into SPARQL queries by providing the model with context-aware KG schema information. Experiments using GPT-3.5 and GPT-4 on two manufacturing ontologies (LIS and CIMM) demonstrate that context-aware content selection significantly reduces hallucination and improves accuracy by 20-30%. Including domain-specific examples in prompts yields an additional 5-8% accuracy gain. The framework shows that carefully curated KG context and targeted prompting techniques enable LLMs to generate correct and complete SPARQL queries for complex manufacturing queries.

## Method Summary
The framework translates natural language questions into SPARQL queries through a multi-stage pipeline. First, a content selection module uses semantic embeddings to identify the top-k relevant classes and properties from the ontology. Second, the system enriches this reduced schema with semantic definitions (axioms, domain/range, comments). Third, the formatted schema (Graph/Table/Text) is combined with the user question and domain-specific examples in a prompt. Finally, an LLM (GPT-3.5 or GPT-4) generates the SPARQL query, which is validated against the ontology to measure hallucination accuracy. The approach is evaluated on 17 benchmark questions from manufacturing ontologies (LIS and CIMM) using four content selection variants, three representation formats, and three prompting strategies.

## Key Results
- Context-aware schema reduction (OntC) significantly reduces hallucination rates compared to naive reduction (OntA), improving accuracy by 20-30%.
- Providing semantic definitions alongside schema lists further improves performance by leveraging additional ontology semantics.
- Including a single domain-specific example in prompts yields an additional 5-8% accuracy gain over generic examples.
- The framework successfully handles domain-specific ambiguities like distinguishing between logical and physical equipment concepts.

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Schema Reduction
Reducing the ontology to a question-relevant subset significantly reduces hallucination rates and improves query accuracy. A RAG step extracts semantic embeddings from the user's question to identify and isolate only the top-k relevant classes and properties. By restricting the input context window to high-relevance schema elements, the LLM is forced to construct SPARQL queries using valid domain terms rather than inventing relationships. The core assumption is that semantic similarity search correctly identifies necessary schema elements without excluding critical components.

### Mechanism 2: Semantic Enrichment via Ontological Definitions
Providing rich semantic definitions (axioms, domain/range, comments) alongside class/property lists allows the LLM to resolve domain-specific ambiguities better than simple label lists. Raw schema defines what exists, but semantic definitions explain how entities relate. For example, distinguishing between a "Physical Machine" and "Logical Equipment" requires processing rdfs:comment or owl:inverseOf axioms. The LLM uses these logical constraints to align user intent with the specific modeling paradigm of the KG.

### Mechanism 3: In-Context Learning via Domain-Specific Examples
Including a single domain-specific question-query pair in the prompt yields higher accuracy (5-8%) than generic examples or simple instructions. Few-shot prompting with domain-specific examples conditions the LLM on the specific structural "dialect" of the target SPARQL endpoint. It demonstrates preferred prefix usage, filtering patterns, and variable naming conventions specific to that manufacturing KG, reducing the cognitive load on the model to infer these standards from scratch.

## Foundational Learning

- **Knowledge Graph Triples & SPARQL**: KGs store data as Subject-Predicate-Object triples and SPARQL matches patterns against these triples. Why needed: The entire framework is a translation layer from natural language to SPARQL. Quick check: Can you explain why a SPARQL query needs a WHERE clause with triple patterns to retrieve data?

- **Token Limits & Context Windows**: LLMs have fixed context windows (e.g., 32K tokens). Why needed: The paper explicitly addresses "Content Selection" because an entire manufacturing ontology typically exceeds this limit. Quick check: Why does increasing the size of the ontology context (OntA → OntC) not always lead to better results, considering token noise?

- **Hallucination in Generative AI**: LLMs generate probable text, not verified facts. Why needed: The paper defines a specific "Hallucination Accuracy" metric. Quick check: If the LLM generates a SPARQL query using a predicate hasMachine that sounds correct but does not exist in the ontology, what specific failure mode is this?

## Architecture Onboarding

- **Component map**: User Interface → Content Selection Module → Content Enrichment Module → Representation Layer → Prompt Constructor → LLM Engine → Validator/Executor

- **Critical path**: The Content Selection (Context-based Reduction) step is the primary performance driver. If this step retrieves the wrong classes (low recall), no amount of prompt engineering can recover the missing data.

- **Design tradeoffs**:
  - Naive vs. Context Reduction: Naive (random/top-down subset) is faster to implement but risks missing key entities. Context-based (embedding search) adds latency and infrastructure but significantly boosts accuracy.
  - Graph vs. Table Representation: Graph (Turtle) preserves relationships but consumes more tokens/complexity. Table is token-efficient but may lose structural nuance.
  - Generic vs. Domain Examples: Domain examples improve accuracy but require maintenance as schema evolves. Generic examples are robust but less performant.

- **Failure signatures**:
  - High Hallucination Rate: Often caused by "Naive Reduction" where the model invents properties to bridge gaps left by the truncated schema.
  - Syntactically Correct but Semantically Wrong: The query runs but returns empty results; usually caused by misinterpreting class hierarchies due to lack of semantic enrichment.
  - Token Overflow: Occurs if "Entire Ontology" is used on complex industrial schemas without reduction.

- **First 3 experiments**:
  1. Baseline Establishment: Run benchmark questions using Naive Reduction (OntA) with Simple Prompt to measure baseline hallucination rate.
  2. A/B Test Context Reduction: Implement Context-based Reduction (OntC) using vector index on ontology. Compare "Hallucination Accuracy" against baseline.
  3. Prompt Strategy Comparison: Keeping content selection fixed (OntC), swap between Generic and Domain-Specific prompts to measure the 5-8% accuracy lift.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the framework be extended to handle complex, multi-constraint questions involving production requirements? The authors state they "envision to handle more complex questions," specifically queries like "List all lines that fulfill the requirements for the production of a given product."

- **Open Question 2**: Can ensemble generation and answer rating mechanisms effectively mitigate the non-determinism inherent in LLM-based query generation? The authors note that "non-determinism and noise in answer generation need to be addressed" and propose investigating ensemble generation and answer rating.

- **Open Question 3**: How do open-source models (e.g., Llama 3, Falcon) compare to proprietary GPT models in context-aware SPARQL generation? The authors list "utilization of Open Language Models... and compare their performance to GPT-based LLMs" as a future research avenue.

- **Open Question 4**: To what extent can fine-tuning improve performance compared to the context-aware prompting strategies evaluated in this study? The authors admit they "did not delve into the realm of fine-tuning" due to data scarcity.

## Limitations
- Specific embedding model and similarity threshold for context-based reduction are not specified, making it difficult to reproduce the 20-30% accuracy improvement claim.
- The heuristic rules for ontology-based content enrichment (OntD) are not detailed, limiting understanding of the full methodology.
- The exact prompt templates are only partially provided, preventing faithful replication of the domain-specific example approach.
- The LIS ontology is proprietary, limiting external validation to only the CIMM ontology.

## Confidence
- **High Confidence**: The general framework design (content selection → enrichment → prompt generation → SPARQL output) is well-specified and supported by experimental results.
- **Medium Confidence**: The 20-30% accuracy improvement from context-aware reduction and 5-8% gain from domain examples are credible but depend on unreported implementation details.
- **Low Confidence**: Claims about handling domain-specific ambiguities (e.g., Physical vs. Logical Equipment) lack systematic evaluation beyond expert qualitative review.

## Next Checks
1. Reproduce Hallucination Reduction: Implement context-based reduction using sentence-transformers/all-MiniLM-L6-v2 and measure hallucination accuracy on CIMM ontology with 10-15 benchmark questions.

2. A/B Test Prompt Strategies: Fix the content selection method and systematically compare generic vs. domain-specific examples across multiple query complexities to validate the 5-8% accuracy claim.

3. Stress Test Edge Cases: Design benchmark questions that require joining concepts across distant ontology sections to evaluate whether context-based reduction reliably surfaces all necessary schema elements.