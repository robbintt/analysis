---
ver: rpa2
title: 'KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction'
arxiv_id: '2512.17917'
source_url: https://arxiv.org/abs/2512.17917
tags:
- uni00000013
- cache
- tokens
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of Key-Value (KV) cache memory
  explosion in large language models (LLMs) with long context lengths. Traditional
  compression methods permanently evict or merge tokens, causing "Contextual Amnesia"
  where important information is irretrievably lost.
---

# KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction

## Quick Facts
- arXiv ID: 2512.17917
- Source URL: https://arxiv.org/abs/2512.17917
- Reference count: 13
- Key outcome: Reversible KV cache compression achieves 10% cache budget with identical accuracy for 2k contexts and 25% cache with ~2% accuracy loss for 32k contexts

## Executive Summary
KVReviver addresses the memory explosion problem in LLM KV caches for long contexts by introducing a reversible compression method that eliminates "Contextual Amnesia" - the permanent loss of token information from traditional eviction or merging approaches. Instead of permanently discarding tokens, KVReviver stores them in a compressed sketch structure while maintaining recent and high-attention tokens uncompressed. The method reconstructs compressed tokens during computation using a modified Count Sketch data structure, enabling full attention computation with minimal information loss while achieving significant memory savings.

## Method Summary
KVReviver implements a three-part storage system for each attention layer: a Recent Part (queue for newest tokens), a Candidate Part (min-heap for high-attention tokens), and a Vague Part (compressed sketch for other tokens). The compression flow moves tokens from Recent to Candidate to Vague as budget constraints require. During reconstruction, the Vague Part uses a modified Count Sketch with r=3 buckets, where tokens are inserted using hash functions and retrieved via median queries. The method maintains cumulative attention scores to determine which tokens stay uncompressed, enabling reversible compression that preserves the ability to reconstruct any token's KV values from compressed storage.

## Key Results
- Achieves identical end-to-end accuracy using only 10% of KV cache budget for 2k-length contexts
- Maintains comparable accuracy (~2% loss) using merely 25% of KV cache budget for 32k-length contexts
- Effectively eliminates Contextual Amnesia by enabling token reconstruction from compressed storage
- Tested on short-text (XSum, CNNDM) and long-text (NIAH, Longbench) benchmarks

## Why This Works (Mechanism)
The key insight is that not all tokens need to be stored uncompressed in the KV cache. By maintaining recent tokens and those with high cumulative attention scores in uncompressed form, while compressing less important tokens using a sketch data structure, KVReviver achieves memory savings without permanently losing information. The sketch allows reconstruction of compressed tokens when needed for attention computation, preserving the model's ability to access any context token while dramatically reducing memory requirements.

## Foundational Learning
- **Count Sketch Data Structure**: A probabilistic data structure for dimensionality reduction that maps high-dimensional vectors to lower-dimensional representations using hash functions. Needed because it enables compression of KV pairs while allowing approximate reconstruction through median queries across multiple hash buckets.
- **Min-Heap for Token Prioritization**: A priority queue that efficiently tracks tokens by their cumulative attention scores. Required to identify which tokens to keep uncompressed versus compress, enabling the trade-off between memory savings and accuracy.
- **Attention Score Accumulation**: The process of summing attention weights across layers to identify important tokens. Critical for determining which tokens deserve uncompressed storage, as tokens frequently attended to are more likely to be important for downstream tasks.
- **Bucket Collision Management**: The probability and impact of multiple tokens mapping to the same sketch bucket. Important because collision rates directly affect reconstruction accuracy and must be controlled through appropriate bucket sizing and hash function selection.

## Architecture Onboarding

### Component Map
Recent Part -> Candidate Part -> Vague Part (Count Sketch with r=3 buckets) -> Attention Computation

### Critical Path
During forward pass: Attention scores accumulate per token → Tokens overflow Recent Part → Highest-attention tokens move to Candidate Part → Remaining tokens compress into Vague Sketch → During attention computation, all three parts reconstruct KV values → Full attention matrix computed with reconstructed tokens.

### Design Tradeoffs
- **Memory vs. Accuracy**: Higher compression (smaller cache budget) increases reconstruction error from sketch collisions but saves memory. The r=3 hash functions balance collision probability against storage overhead.
- **Recent vs. Candidate Allocation**: Equal allocation (0.45/0.45 at 10% budget) assumes uniform importance distribution; skewed allocation could optimize for specific task characteristics but reduces generalizability.
- **Hash Function Selection**: Simple hash functions reduce computation but increase collision probability; cryptographic hashes reduce collisions but add overhead. The paper's unspecified choice represents a critical implementation detail.

### Failure Signatures
- **High Accuracy Degradation**: Indicates excessive hash collisions in Vague Sketch (load factor too high) or insufficient Recent/Candidate allocation for the task.
- **Throughput Bottlenecks**: Suggests non-tensorized Vague operations or inefficient median computation across sketch buckets.
- **Memory Budget Overflow**: Means Recent Part slack parameter too small or ReplaceRate inappropriate for the compression ratio.

### First Experiments
1. Test Recent Part queue behavior with varying token insertion rates to verify overflow triggers Candidate compression at expected rates.
2. Validate Vague Sketch reconstruction accuracy by comparing original vs. reconstructed KV pairs for tokens with varying attention scores.
3. Profile the full pipeline on XSum at 10% budget to verify the claimed 2× slowdown and measure actual memory savings.

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Hash function implementation details unspecified, creating uncertainty about collision rates and reconstruction quality
- Budget allocation scaling beyond 10% compression rate not fully detailed, particularly for the 25% budget used in long-context experiments
- Slack parameter for Recent Part overflow control not provided, affecting cache management behavior

## Confidence

| Claim Area | Confidence Level |
|------------|------------------|
| Core conceptual approach | High |
| Three-part storage architecture | High |
| Sketch-based compression mechanism | Medium |
| Exact performance numbers | Medium |
| Implementation details for hash functions | Low |

## Next Checks
1. Implement hash collision analysis by tracking Vague load factor and measuring accuracy degradation as a function of collision rate; compare against the claimed 10% and 25% budget scenarios.
2. Validate budget allocation scaling by testing KVReviver at intermediate compression rates (e.g., 15%, 20%, 30%) to verify that the Recent/Candidate/Vague ratios scale proportionally as implied by the paper.
3. Profile the Vague Sketch operations (insertion and query) at batch sizes of 1, 8, and 32 to verify the claimed 2× slowdown at batch=1 and assess whether the batch processing optimizations are correctly implemented.