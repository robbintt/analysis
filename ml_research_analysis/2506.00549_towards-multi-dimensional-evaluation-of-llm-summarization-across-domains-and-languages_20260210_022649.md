---
ver: rpa2
title: Towards Multi-dimensional Evaluation of LLM Summarization across Domains and
  Languages
arxiv_id: '2506.00549'
source_url: https://arxiv.org/abs/2506.00549
tags:
- sentence
- summary
- chinese
- document
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MSumBench introduces a multi-dimensional, multi-domain summarization
  benchmark for English and Chinese, addressing the lack of domain-specific evaluation
  criteria and bilingual coverage in existing benchmarks. The benchmark employs a
  multi-agent debate system to enhance human annotation quality by providing structured
  arguments with contrasting perspectives, reducing bias and cognitive load.
---

# Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages

## Quick Facts
- **arXiv ID**: 2506.00549
- **Source URL**: https://arxiv.org/abs/2506.00549
- **Reference count**: 40
- **Primary result**: Introduces MSumBench, a multi-domain, bilingual summarization benchmark with enhanced human annotation via multi-agent debate system

## Executive Summary
MSumBench addresses critical gaps in LLM summarization evaluation by introducing a multi-dimensional, multi-domain benchmark covering English and Chinese across six diverse domains. The benchmark employs a novel multi-agent debate system that presents structured arguments with contrasting perspectives to human annotators, reducing bias and cognitive load compared to traditional single-view assistance. Comprehensive evaluation of eight modern summarization models reveals distinct performance patterns across domains and languages, with proprietary LLMs generally outperforming open-source alternatives. The work also identifies a strong correlation (ρ=0.71) between summarization and evaluation performance among LLMs, while uncovering systematic bias in self-evaluation that varies by model.

## Method Summary
The authors developed MSumBench by collecting 300 documents across six domains (News, Medical Literature, Report, Booking, Meeting, Interview) in both English and Chinese. For each document, they extracted domain-specific key-facts using GPT-4o and validated them through three-LLM majority voting. Eight summarization models (2 non-LLM, 3 open-source, 3 proprietary) generated summaries, which were evaluated through a multi-agent debate system where Advocate and Skeptic agents present evidence-based arguments to human annotators assisted by an Adjudicator. Human annotators performed fact verification and key-fact alignment tasks, with performance measured through percentage scores for faithfulness, completeness, and conciseness.

## Key Results
- Multi-agent debate achieves 12.76% higher balanced accuracy compared to UniSumEval's single-view assistance
- Domain-specific key-fact extraction yields 50% higher preference rate than generic approaches
- Strong correlation (ρ=0.71) between summarization and evaluation performance among LLMs
- Proprietary LLMs outperform open-source models, with 23% completeness degradation English→Chinese for open-source vs. 8% for proprietary
- High inter-annotator agreement scores of 0.58 (fact verification) and 0.79 (key-fact alignment)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-agent debate with structured contrasting perspectives reduces annotator bias and improves annotation accuracy.
- **Mechanism**: Three LLM agents (Advocate, Skeptic, Adjudicator) present opposing evidence-based arguments. Annotators receive balanced viewpoints rather than a single LLM recommendation, preventing blind endorsement. The Adjudicator ensures arguments remain grounded in source text, reducing superficial objections.
- **Core assumption**: Annotators can effectively synthesize competing arguments when presented with structured evidence and sentence-level citations.
- **Evidence anchors**:
  - [abstract] "multi-agent debate system to enhance human annotation quality by providing structured arguments with contrasting perspectives, reducing bias and cognitive load"
  - [section 4.2] "our debating system produces more accurate human labels, achieving a 12.76%p higher balanced accuracy compared to UniSumEval"
  - [corpus] Limited corpus support; neighboring papers focus on evaluation methods but not multi-agent annotation specifically.
- **Break condition**: If annotators cannot process multiple arguments within cognitive limits, or if debate outputs are overly verbose, quality gains may reverse.

### Mechanism 2
- **Claim**: Domain-specific key-fact categories improve evaluation relevance by capturing domain-unique priorities.
- **Mechanism**: Instead of uniform criteria, each domain receives 5–7 tailored key-fact categories (e.g., "Main Topic" and "Counter Arguments" for News; "Research Finding" and "Medical Prevention" for Medical Literature). Key-facts are extracted via GPT-4o, then cross-validated by three LLMs using majority voting.
- **Core assumption**: Domain-specific categories derived from reference summary patterns reflect what humans consider essential in each domain.
- **Evidence anchors**:
  - [section 3.2] "we derive approximately 5–7 recurring categories of essential information"
  - [section 4.2.1] "MSumBench's approach yields a 50%p higher preference rate than UniSumEval"
  - [corpus] Weak corpus support; neighboring papers address multilingual evaluation but not domain-specific criteria design.
- **Break condition**: If categories are too narrow or derived from non-representative reference summaries, they may exclude important domain information.

### Mechanism 3
- **Claim**: LLM summarization performance correlates with evaluation performance, but self-evaluation introduces systematic bias that varies by model.
- **Mechanism**: LLMs that summarize well also evaluate well (ρ=0.71), suggesting shared underlying capabilities. However, when evaluating their own summaries, models show inconsistent bias—Claude-3.5-Sonnet favors its own work, while Llama-3.1-70B rates its own work less favorably.
- **Core assumption**: The composite score (average of faithfulness, completeness, conciseness) captures meaningful cross-task capability.
- **Evidence anchors**:
  - [abstract] "strong correlation between summarization and evaluation performance (ρ=0.71), but also reveals systematic bias in self-evaluation"
  - [section 6.3] "self-evaluation bias does not always manifest as self-preference, as it can manifest as either over-rating or under-rating their own work"
  - [corpus] Weak direct support; corpus papers on LLM-as-judge focus on multilingual settings, not self-evaluation bias specifically.
- **Break condition**: If correlation is driven by a third factor (e.g., training data overlap), improving summarization may not improve evaluation.

## Foundational Learning

- **Concept**: **Key-fact atomicity**
  - **Why needed here**: Each key-fact must contain 2–3 entities maximum and express one action/event. This enables fine-grained NLI-style alignment between summaries and reference information.
  - **Quick check question**: Given "The resolution authorizes one winter shelter from December 1st to March 31st," how many atomic key-facts should this produce?

- **Concept**: **Coefficient of Variation (CV) for stability measurement**
  - **Why needed here**: Domain and language stability are computed using CV (σ/μ × 100), then rescaled to 100/(1 + Instability). This normalizes performance variability across dimensions.
  - **Quick check question**: If faithfulness scores across 6 domains have σ=8 and μ=85, what is the instability score?

- **Concept**: **Inter-Annotator Agreement (IAA) via Krippendorff's α**
  - **Why needed here**: IAA quantifies annotation reliability. The paper reports α=0.58 for fact verification and α=0.79 for key-fact alignment, indicating acceptable to high consistency.
  - **Quick check question**: Why might high IAA not guarantee dataset quality, and how does the paper address this?

## Architecture Onboarding

- **Component map**:
  - Dataset Collection: 6 domains × 25 documents × 2 languages (EN/ZH), with multi-step translation validation
  - Key-Fact Extraction: Category identification → GPT-4o extraction → 3-LLM cross-validation (majority vote)
  - Summary Generation: 8 models (2 non-LLM, 3 open-source, 3 proprietary) → 2,250 total summaries
  - Multi-Agent Debate: Advocate + Skeptic (Llama-3.1-70B) + Adjudicator → structured arguments → human annotation
  - Evaluation Pipeline: Fact verification (sentence-level) + Key-fact alignment (NLI-assisted) → percentage scores

- **Critical path**:
  1. Source document → domain-specific key-fact extraction → validation
  2. Summary generation → multi-agent debate assistance → human annotation
  3. Annotation aggregation → percentage score computation → benchmarking

- **Design tradeoffs**:
  - Single-round debate vs. multi-round: Single-round chosen to reduce cognitive load; may sacrifice depth
  - Llama-3.1-70B for all agents: Consistency vs. potential model-specific blind spots
  - Chinese summaries evaluated via translation: Enables bilingual coverage but introduces translation artifacts

- **Failure signatures**:
  - 95.31% of UniSumEval's incorrect annotations aligned with single-view assistance labels (blind endorsement pattern)
  - Open-source LLMs show 23% completeness degradation English→Chinese vs. 8% for proprietary
  - Medical Literature domain shows lowest faithfulness IAA (0.46), suggesting domain complexity challenges

- **First 3 experiments**:
  1. **A/B test multi-agent vs. single-view assistance** on 100 samples: Measure balanced accuracy difference and annotation time
  2. **Domain-specific vs. generic key-fact extraction** on 2 domains: Compare human preference rates and coverage of ground-truth facts
  3. **Self-evaluation bias audit** across 4 LLM evaluators: Calculate bias rate (self - peer average) with statistical significance testing

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can evaluation frameworks define purpose-specific key-fact categories to align summaries with varied objectives within a single domain?
- **Basis in paper**: [Explicit] The authors note their framework does not address variations in summary purpose within a domain (e.g., medical literature shifting focus from clinical treatments to epidemiological data).
- **Why unresolved**: Current evaluation relies on static domain-specific categories that do not account for dynamic summarization goals or perspectives.
- **What evidence would resolve it**: A benchmark incorporating variable ground-truth key-facts based on explicit summarization objectives or user intents.

### Open Question 2
- **Question**: How effectively do current LLMs handle summarization and evaluation tasks in underrepresented or lower-resource languages compared to English and Chinese?
- **Basis in paper**: [Explicit] The authors state that evaluating how LLMs handle other lower-resource languages remains an open question given the current bilingual focus.
- **Why unresolved**: MSumBench is limited to English and Chinese, leaving the generalizability of the observed LLM performance and evaluation correlations to other languages unverified.
- **What evidence would resolve it**: Expansion of the MSumBench dataset to include lower-resource languages with corresponding human annotations and model evaluations.

### Open Question 3
- **Question**: Can enhancing the multi-agent debate framework with dynamic rebuttals or multi-round interactions further improve the effectiveness of AI-human collaboration for summary annotation?
- **Basis in paper**: [Explicit] The authors suggest that exploring other forms of collaboration, such as enhancing debates with dynamic rebuttals, could further improve the effectiveness of AI-human collaboration.
- **Why unresolved**: The current system uses a single-round setup to reduce cognitive load, potentially missing the benefits of deeper, iterative reasoning.
- **What evidence would resolve it**: A comparative study measuring annotation accuracy and annotator cognitive load between single-round and multi-round debate setups.

## Limitations
- **Translation artifacts**: Chinese summaries were generated in English and translated, potentially introducing noise that may disadvantage open-source models (observed 23% completeness drop vs. 8% for proprietary models)
- **Single-round debate**: Multi-agent debate is limited to one round, potentially missing deeper argumentative insights that could further reduce bias
- **Self-evaluation bias**: Systematic bias in LLM-as-judge evaluation was observed, with inconsistent patterns (some models over-rate, others under-rate their own work), limiting reliability of automated assessment

## Confidence
- **High confidence**: Domain-specific key-fact extraction methodology and inter-annotator agreement scores (0.58 and 0.79 for core tasks) are well-supported by systematic validation
- **Medium confidence**: Multi-agent debate effectiveness claims (12.76% higher balanced accuracy) are supported but could benefit from larger-scale A/B testing
- **Medium confidence**: Correlation between summarization and evaluation performance (ρ=0.71) is statistically significant but may be influenced by shared training data rather than true capability overlap

## Next Checks
1. **A/B test multi-agent vs. single-view assistance** on 100 samples to measure balanced accuracy difference and annotation time, directly validating the bias reduction claim
2. **Domain-specific vs. generic key-fact extraction** on 2 domains to compare human preference rates and coverage of ground-truth facts, testing the domain-specificity hypothesis
3. **Self-evaluation bias audit** across 4 LLM evaluators, calculating bias rate (self - peer average) with statistical significance testing to quantify and characterize the systematic bias patterns observed