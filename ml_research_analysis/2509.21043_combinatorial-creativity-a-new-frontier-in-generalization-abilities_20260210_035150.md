---
ver: rpa2
title: 'Combinatorial Creativity: A New Frontier in Generalization Abilities'
arxiv_id: '2509.21043'
source_url: https://arxiv.org/abs/2509.21043
tags:
- creativity
- creative
- combinatorial
- novelty
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces combinatorial creativity as a new form of
  generalization in LLMs, where models generate novel ideas by combining familiar
  concepts. The authors develop a theoretical framework and algorithmic task to evaluate
  creativity through degrees of novelty and utility, using synthetic graphs where
  models must find valid paths between nodes while satisfying logical constraints.
---

# Combinatorial Creativity: A New Frontier in Generalization Abilities

## Quick Facts
- **arXiv ID:** 2509.21043
- **Source URL:** https://arxiv.org/abs/2509.21043
- **Reference count:** 26
- **Primary result:** Introduces combinatorial creativity as a new form of LLM generalization, where models generate novel ideas by combining familiar concepts.

## Executive Summary
This paper introduces combinatorial creativity as a distinct form of generalization in large language models, where models generate novel ideas by combining familiar concepts. The authors develop a theoretical framework and algorithmic task to evaluate creativity through degrees of novelty and utility, using synthetic graphs where models must find valid paths between nodes while satisfying logical constraints. Through extensive experiments across 1M-100M parameter models, they reveal three key findings: creativity improves predictably with model size and compute, optimal architectural configurations exist (wider, shallower models outperform deeper, narrower ones for fixed compute budgets), and the ideation-execution gap in scientific idea generation can be explained by a fundamental novelty-utility tradeoff that persists across all scales studied.

## Method Summary
The authors construct a synthetic conceptual graph with 17,576 nodes (AAA-ZZZ) and ~52,728 edges, then train GPT-2 decoder-only transformers to generate labeled paths between nodes while satisfying inclusion/exclusion constraints on edge labels. The training procedure uses size-dependent learning rates with AdamW optimizer and cosine learning rate decay over 16 epochs. Models are evaluated using greedy decoding on held-out query-path pairs, with creativity measured as the product of novelty (based on path length and label surprise) and utility (constraint satisfaction). The study sweeps model depths (2-12 layers), widths (constant parameter count), and head counts across 1M, 10M, and 100M parameter scales.

## Key Results
- Creativity improves predictably with model size and compute, showing clear scaling laws
- Optimal architectural configurations exist—for fixed compute budgets, wider, shallower models generally outperform deeper, narrower ones
- A fundamental novelty-utility tradeoff persists across all model scales, explaining the ideation-execution gap in scientific idea generation

## Why This Works (Mechanism)

### Mechanism 1: Depth-Width Tradeoff for Creative Reasoning
- Claim: For fixed compute budgets, there exists an optimal depth-width ratio that maximizes combinatorial creativity, with wider, shallower models generally outperforming deeper, narrower ones.
- Mechanism: Creativity requires balancing (1) sufficient depth for sequential reasoning to make distant constrained associations, and (2) sufficient width for parallel representational capacity to hold diverse concepts simultaneously.
- Core assumption: Assumption: Both capacities are necessary and neither alone is sufficient for combinatorial creativity tasks.
- Evidence anchors:
  - [abstract]: "for fixed compute budgets, there exist optimal model depths and widths for creative ability, with wider, shallower models generally outperforming deeper, narrower ones"
  - [section 5]: "For the 100M models, this peak is clearly visible around 8 layers... The optimal E/L ratio occurs between 200 and 300 for all three model sizes"
  - [corpus]: Weak direct corroboration; neighboring papers focus on creativity evaluation/definition rather than architectural scaling effects
- Break condition: If task complexity fundamentally changes (e.g., requires purely sequential reasoning or pure associative memory), the tradeoff structure may shift or disappear.

### Mechanism 2: Scale-Invariant Novelty-Utility Tradeoff
- Claim: A fundamental tradeoff exists between generating novel artifacts and satisfying utility constraints, persisting across all tested model scales.
- Mechanism: Information-theoretic limits (Varshney, 2019) constrain simultaneous maximization of novelty and utility; increasing constraint count systematically reduces achievable novelty.
- Core assumption: Creativity is multiplicatively composed of novelty × utility, making both dimensions necessary.
- Evidence anchors:
  - [abstract]: "fundamental novelty-utility tradeoff that persists across all model scales studied"
  - [section 5, Figure 4]: "across all three scales, as the number of utility constraints increases, the novelty of artifacts exhibits a clear downward trend"
  - [corpus]: Peeperkorn et al. (2024) found analogous temperature-coherence tradeoff; Shashidhar et al. (2025) documented validity-diversity tradeoff in LLM evaluation questions
- Break condition: If architectural innovations or training objectives fundamentally alter how constraints are processed, tradeoff severity may change.

### Mechanism 3: Error-Type Shift with Scale
- Claim: As models scale, hallucination errors decrease while logical inconsistency errors become proportionally more prominent.
- Mechanism: Scaling improves surface-level validity (recognizing valid tokens/structures) but semantic constraint satisfaction improves more slowly.
- Evidence anchors:
  - [section 5]: "At smaller scales (1M, 10M), hallucinations dominate by several orders of magnitude... at the 100M scale, hallucinations decline sharply and 'invalid path' errors rise to become nearly equal in frequency"
  - [Figure 5]: Log-scale error distribution showing hallucination decline and invalid path increase
  - [corpus]: Weak; neighboring hallucination paper doesn't address architectural scaling effects on error types
- Break condition: If scaling laws change at frontier-model scale (billions of parameters), error distribution may shift again; unknown without further study.

## Foundational Learning

**Concept: Combinatorial Creativity vs Compositional Generalization**
- Why needed: The paper deliberately distinguishes CC from CG-S (systematicity) and CG-P (productivity); understanding this distinction is essential for interpreting why CC requires open-ended evaluation.
- Quick check: Why does CC require measuring "degrees of novelty" while CG uses binary correctness evaluation?

**Concept: Open-Ended Evaluation**
- Why needed: Unlike closed-ended tasks with single ground-truth targets, CC accepts multiple valid solutions evaluated by continuous novelty/utility scores.
- Quick check: What happens if you evaluate creativity against fixed targets according to the paper's framework?

**Concept: Inclusion/Exclusion Constraints as Utility Abstraction**
- Why needed: The paper maps real-world LLM failure modes (unrealistic assumptions, missing baselines, vague plans) to these abstract constraint types.
- Quick check: Which constraint type would represent "must include a baseline comparison"?

## Architecture Onboarding

**Component map:**
- Conceptual Graph: Erdős-Rényi-like graph; 17,576 nodes (AAA-ZZZ), ~52,728 edges, 26 edge labels (a-z)
- Model: GPT-2 decoder-only Transformer
- Tokenizer: Custom vocabulary (3-letter uppercase node tokens, single lowercase edge labels, special syntax: `:`, `[`, `]`, `<eos>`)
- Training: AdamW, cosine LR decay, 16 epochs, size-dependent learning rates

**Critical path:**
1. Graph construction → 2. Query-path pair generation (BFS with constraints) → 3. Train from scratch → 4. Evaluate with greedy decoding

**Design tradeoffs:**
- Depth vs Width: E/L ratio 200-300 optimal; ~8 layers for 100M models
- Holdout: Strict separation—any (u,v) pair in evaluation excluded from training
- Max path length: h_train_max = 10 hops

**Failure signatures:**
- Hallucination: Invalid edges/nodes (dominant at small scale)
- Invalid path: Connected but semantically incorrect (increases with scale)
- Exclusion violated: Used forbidden labels
- Missing required labels: Inclusion set not satisfied

**First 3 experiments:**
1. Reproduce depth sweep: Train 10M models with 2-12 layers at constant FLOPs; verify peak creativity around optimal depth
2. Constraint scaling: Plot normalized novelty vs. constraint count (1-5) to confirm tradeoff slope
3. Error classification: Categorize all utility failures by type across 1M/10M/100M to reproduce hallucination-to-invalid-path shift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the fundamental novelty-utility tradeoff persist or diminish in Large Language Models (LLMs) scaled to billions of parameters?
- Basis in paper: [explicit] The authors note that while the tradeoff persists up to 100M parameters, "frontier models today are well into the billions," and explicitly state that a "large-scale study pretraining at frontier-model scale should be performed to validate this explicitly."
- Why unresolved: The study was limited to models up to 100M parameters due to computational constraints, leaving the behavior of frontier-scale models unverified.
- What evidence would resolve it: Applying the proposed algorithmic framework to pre-trained models in the billion-parameter range to observe if the inverse relationship between constraint satisfaction and novelty weakens or holds.

### Open Question 2
- Question: How do alternative pre-training objectives, such as multi-token prediction (MTP), impact combinatorial creativity compared to standard next-token prediction?
- Basis in paper: [explicit] The authors identify the "effect of pre-training objective on combinatorial creativity" as a "promising future direction," citing evidence that MTP may improve creative generalization.
- Why unresolved: The experiments relied exclusively on standard GPT-2 architectures using next-token prediction.
- What evidence would resolve it: A comparative study evaluating the creativity scores (novelty and utility) of models trained with MTP versus standard objectives on the same conceptual graph task.

### Open Question 3
- Question: Does the novelty-utility tradeoff characteristic of combinatorial creativity also constrain exploratory and transformational creativity in AI models?
- Basis in paper: [explicit] The paper restricts its scope to combinatorial creativity and explicitly states, "Future work can study the scaling behavior of exploratory and transformational creativity," specifically regarding the tradeoff.
- Why unresolved: The theoretical framework and algorithmic task were designed specifically for combining familiar concepts, not for exploring or transforming the rules of a conceptual space.
- What evidence would resolve it: Adapting frameworks for transformational creativity (e.g., conceptual blending) to test if increasing utility constraints similarly degrades the novelty of generated artifacts.

### Open Question 4
- Question: Do the optimal architectural depth-to-width ratios (E/L) for creativity found in synthetic graphs transfer to semantically grounded, real-world conceptual spaces?
- Basis in paper: [inferred] The authors acknowledge that their "empirical results relied on synthetic data, which may not be fully representative of the complexity of real-world data," suggesting the specific architectural "sweet spots" (e.g., 8 layers, E/L ratio of 200-300) may not generalize.
- Why unresolved: Real-world semantic graphs possess structures and complexities not present in the Erdős-Rényi-like synthetic graphs used in the study.
- What evidence would resolve it: Conducting the same architectural sweep on real-world knowledge graphs (e.g., scientific citation networks) to see if the optimal width-depth balance remains consistent.

## Limitations

- **Synthetic Task Limitation:** Results rely on synthetic conceptual graphs, which may not capture the complexity and structure of real-world semantic relationships.
- **Model Scale Constraint:** Experiments were limited to models up to 100M parameters, leaving the behavior of frontier-scale models (billions of parameters) unverified.
- **Evaluation Specificity:** The algorithmic task focuses specifically on combinatorial creativity, not exploring or transforming conceptual rules.

## Confidence

| Claim | Confidence | Evidence |
| --- | --- | --- |
| Optimal depth-width ratio exists for creativity | High | Multiple experiments across 3 model scales show consistent peak creativity at specific E/L ratios |
| Fundamental novelty-utility tradeoff persists across scales | High | Clear inverse relationship observed across all three model sizes with statistical significance |
| Creativity improves with model size and compute | High | Consistent upward trend in creativity scores with parameter count and training compute |
| Scaling shifts error types from hallucination to logical inconsistency | Medium | Clear trend observed but could be specific to synthetic task structure |

## Next Checks

1. **Replicate depth-width tradeoff:** Train 10M parameter models with varying depths (2-12 layers) at constant FLOPs to verify the peak creativity occurs around 8 layers with E/L ratio of 200-300.

2. **Validate novelty-utility tradeoff:** Conduct experiments varying constraint counts from 1-5 and plot normalized novelty scores to confirm the inverse relationship persists across all scales.

3. **Characterize error distribution shift:** Classify all utility failures by type (hallucination, invalid path, constraint violation) across 1M, 10M, and 100M parameter models to reproduce the transition from hallucination-dominated to logical inconsistency-dominated errors.