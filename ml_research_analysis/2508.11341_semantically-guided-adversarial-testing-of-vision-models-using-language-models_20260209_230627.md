---
ver: rpa2
title: Semantically Guided Adversarial Testing of Vision Models Using Language Models
arxiv_id: '2508.11341'
source_url: https://arxiv.org/abs/2508.11341
tags:
- adversarial
- similarity
- target
- vision
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes using language models to guide the selection
  of target labels in adversarial attacks on vision models. Instead of random or model-dependent
  target selection, the authors leverage semantic similarity from pretrained models
  (BERT, TinyLLAMA, CLIP) to select most and least semantically related labels to
  the ground truth.
---

# Semantically Guided Adversarial Testing of Vision Models Using Language Models

## Quick Facts
- arXiv ID: 2508.11341
- Source URL: https://arxiv.org/abs/2508.11341
- Reference count: 40
- Primary result: Language models can guide adversarial target selection, with CLIP outperforming text-only models for worst-case attacks

## Executive Summary
This work proposes using language models to guide the selection of target labels in adversarial attacks on vision models. Instead of random or model-dependent target selection, the authors leverage semantic similarity from pretrained models (BERT, TinyLLAMA, CLIP) to select most and least semantically related labels to the ground truth. This yields interpretable, reproducible, and standardized testing scenarios. Experiments on three vision models and five attack methods show that text-image models like CLIP outperform static lexical databases like WordNet, especially in identifying distant (worst-case) adversarial targets. Similarity-based target selection also correlates with attack severity and can be statically assessed without requiring image inputs.

## Method Summary
The method encodes ImageNet class labels using pretrained language models, computes cosine similarity between embeddings, and selects Most Similar (MS) and Least Similar (LS) target labels for each class. These precomputed lookup tables guide targeted adversarial attacks (FGSM, PGD, C&W, MIM, SPSA) on three vision models (MobileNetV2, EfficientNetV2, ResNet50). A static Dissimilarity Metric (DM) computed from vision model weights predicts attack severity without requiring image inputs. CLIP-based embeddings show superior performance for LS target selection compared to text-only models and WordNet.

## Key Results
- CLIP outperforms BERT and TinyLLAMA for worst-case (LS) adversarial target selection
- Static DM computed from class label embeddings correlates with post-attack severity
- Text-only models better estimate local similarity while text-image models excel at global semantic distance
- Semantic similarity sources can be assessed a priori for compatibility with target vision models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretrained language models can generate semantically meaningful target labels for adversarial testing without requiring image data or access to the target vision model.
- Mechanism: Class label text is encoded into embeddings (e.g., BERT, CLIP text encoder). Cosine similarity between embeddings ranks all classes by semantic proximity to ground truth, producing Most Similar (MS/best-case) and Least Similar (LS/worst-case) target lookup tables.
- Core assumption: Text embeddings from pretrained models capture semantic relationships that meaningfully correspond to the difficulty of adversarial label transitions in vision models.
- Evidence anchors:
  - [abstract] "We evaluate several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity sources to select the most and least semantically related labels"
  - [Section 3.2] "For each class c ∈ C, we consider its textual name (e.g., 'school bus') and encode it using pretrained models... semantic similarity between two classes is assessed using the cosine similarity function"
- Break condition: If vision model class boundaries are determined primarily by visual features not reflected in class name semantics (e.g., "tench" vs. "goldfish"), text-based similarity may misrank difficulty.

### Mechanism 2
- Claim: Vision-language models (CLIP) outperform text-only models and static lexical databases (WordNet) for identifying semantically distant adversarial targets.
- Mechanism: CLIP's joint text-image training creates embeddings that bridge visual and semantic spaces. For LS targets, CLIP better captures global semantic distance, yielding lower TSR (harder attacks) while matching WordNet for MS targets.
- Core assumption: Cross-modal training aligns CLIP's text embeddings with visual similarity structures relevant to vision model decision boundaries.
- Evidence anchors:
  - [Section 4.1] "CLIP yields the lowest average TSR, followed very closely by LLAMA, indicating that its worst-case selections are maximally challenging"
  - [Section 4.2] "Text-image models and WordNet-based measures can be slightly better for local similarity estimation... text and text-language models are superior at estimating global similarity"
- Break condition: If target vision model's training distribution diverges significantly from CLIP's pretraining data (e.g., medical imaging, satellite imagery), alignment may weaken.

### Mechanism 3
- Claim: Static Dissimilarity Metric (DM) computed between class labels without images predicts post-attack severity and can assess similarity source compatibility a priori.
- Mechanism: DM measures normalized rank distance in target model's output weight space. Computing DM between ground truth and target labels (no images) produces scores that correlate with actual attack outcomes, enabling pre-testing assessment.
- Core assumption: Vision model's final layer weights encode class relationships that approximate perceptual similarity, and this structure can be queried statically.
- Evidence anchors:
  - [Section 4.4] "These results suggest that static DM can assess, a priori, the alignment between semantic similarity sources and a vision model's internal class structure"
  - [Section 3.3] "Suppose the standard and dynamic DM results are similar. In that case, it means that we can assess the potential vulnerability of the model to given targets... with no image instances a priori to testing"
- Break condition: If attack success depends heavily on image-specific artifacts (textures, backgrounds) rather than semantic class relationships, static predictions will underperform.

## Foundational Learning

- **Targeted vs. Untargeted Adversarial Attacks**
  - Why needed here: The entire framework depends on understanding that targeted attacks aim for specific misclassifications; target selection directly determines attack difficulty and interpretability.
  - Quick check question: Explain why "least likely class" selection is image-dependent and how semantic selection differs.

- **Cosine Similarity in Embedding Space**
  - Why needed here: Core technical operation for ranking class relationships; misunderstanding this breaks target selection logic.
  - Quick check question: Given two embedding vectors [0.8, 0.6] and [0.6, 0.8], compute cosine similarity.

- **CLIP Architecture (Contrastive Language-Image Pretraining)**
  - Why needed here: Explains why CLIP outperforms text-only models for worst-case targets—joint training creates cross-modal alignment.
  - Quick check question: Why would a model trained on both text and images better predict "hard" adversarial targets for a vision classifier?

## Architecture Onboarding

- **Component map:**
  Similarity Source Models -> Text Embeddings -> Cosine Similarity Matrix -> (MS, LS) Lookup Tables -> Attack Methods -> Vision Models -> FR/TSR/DM Metrics

- **Critical path:**
  1. Extract class label strings from dataset
  2. Encode labels via similarity source -> embeddings
  3. Compute pairwise cosine similarity matrix
  4. Rank and select MS/LS targets -> save lookup table
  5. Run attacks using targets -> compute FR/TSR/DM
  6. Optionally: compute static DM to validate source compatibility

- **Design tradeoffs:**
  - WordNet vs. embedding models: WordNet requires manual mapping, produces discrete similarities; embeddings are automatic, continuous, but require model selection
  - CLIP vs. text-only: CLIP better for LS (worst-case), text-only faster and smaller; choose based on testing priority
  - MS vs. LS testing: MS reveals easier attack vectors; LS stress-tests robustness more severely

- **Failure signatures:**
  - High variance in TSR across attack methods -> similarity source poorly aligned with target model
  - Static DM doesn't correlate with post-attack DM -> vision model's class space not captured by source
  - MS targets harder than LS targets -> ranking inversion, check embedding quality or class label ambiguity

- **First 3 experiments:**
  1. Replicate FR/TSR results for one vision model (ResNet50V2) using CLIP similarity source with PGD attack on NIPS 2017 subset; verify MS TSR > LS TSR pattern.
  2. Compare static DM vs. post-attack DM for all similarity sources on one vision model; confirm correlation (implies a priori assessment validity).
  3. Test on out-of-distribution classes (subset of ImageNet not well-covered in CLIP pretraining) to evaluate break condition for cross-modal alignment.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How can the framework be extended to incorporate contextual and image-conditioned semantics for target selection?
  - Basis in paper: [explicit] The conclusion states the authors aim to "extend this framework to adjust contextual and image-conditioned semantics."
  - Why unresolved: The current method relies on static lookup tables based solely on class labels, which ignores the specific visual content of the input image that might influence the feasibility or relevance of an adversarial target.
  - What evidence would resolve it: A modified framework that dynamically adjusts target selection based on image embeddings (e.g., using CLIP's visual encoder) alongside text embeddings, showing improved correlation with attack success compared to static tables.

- **Open Question 2**
  - Question: Can semantic similarity guidance be effectively adapted for multi-label and open-vocabulary classification scenarios?
  - Basis in paper: [explicit] The conclusion explicitly identifies the need to "adapt it to multi-label and open vocabulary classification" as a future direction.
  - Why unresolved: The current experimental design is restricted to single-label classification with a fixed set of 1000 ImageNet classes, leaving the behavior of similarity-based targets in more complex output spaces unexplored.
  - What evidence would resolve it: Experiments demonstrating successful target selection and attack generation on datasets like MS-COCO (multi-label) or using open-vocabulary models without fixed output layers.

- **Open Question 3**
  - Question: Does the superiority of text-image models over static lexical databases hold in specialized domains outside of general object recognition?
  - Basis in paper: [inferred] The paper relies entirely on the NIPS 2017/ImageNet dataset (general objects). The authors note WordNet's limitations, but it is unclear if pretrained models like CLIP or BERT possess sufficient domain-specific semantic fidelity for specialized tasks (e.g., medical or satellite imagery).
  - Why unresolved: Pretrained models may lack the fine-grained semantic precision required for specialized domains, potentially performing worse than curated, domain-specific lexical databases.
  - What evidence would resolve it: A comparative benchmark of CLIP vs. WordNet (or domain ontologies) for adversarial testing on specialized datasets, measuring the correlation between static similarity scores and dynamic attack success rates.

## Limitations
- Attack hyperparameters (epsilon, iterations, step sizes) are not specified, potentially affecting reproducibility of FR/TSR results
- Cross-modal alignment assumption may break for specialized domains (medical imaging, satellite imagery) outside CLIP's pretraining distribution
- Static DM prediction validity depends on vision model weight space capturing semantic relationships, which may fail for texture-based attacks

## Confidence
- **High Confidence**: Mechanism 1 (language model-based target selection) - core idea is clearly specified and technically sound; only implementation details missing.
- **Medium Confidence**: Mechanism 2 (CLIP superiority for LS targets) - supported by experimental results but assumes cross-modal pretraining alignment holds generally.
- **Medium Confidence**: Mechanism 3 (static DM prediction) - theoretically plausible and experimentally supported, but no independent corroboration in corpus.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Run PGD attacks with varying epsilon/step sizes on ResNet50V2 using CLIP similarity source; verify TSR patterns (MS > LS) persist across parameter ranges.
2. **Cross-Modal Break Condition Test**: Evaluate CLIP-based target selection on a vision model trained on medical imaging data; measure TSR degradation compared to ImageNet-trained models.
3. **Static vs Dynamic DM Validation**: Compute correlation between static DM and post-attack DM for all similarity sources on MobileNetV2; test if correlation drops when using adversarial examples with texture-based perturbations rather than semantic shifts.