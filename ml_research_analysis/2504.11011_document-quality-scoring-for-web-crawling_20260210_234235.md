---
ver: rpa2
title: Document Quality Scoring for Web Crawling
arxiv_id: '2504.11011'
source_url: https://arxiv.org/abs/2504.11011
tags:
- quality
- pages
- page
- crawling
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low-quality content in web
  search by introducing a neural quality scoring approach to improve crawling prioritisation.
  The authors build on prior work using neural semantic quality estimators and extend
  it to crawling, showing that prioritising high-quality pages during crawling improves
  downstream search effectiveness.
---

# Document Quality Scoring for Web Crawling

## Quick Facts
- arXiv ID: 2504.11011
- Source URL: https://arxiv.org/abs/2504.11011
- Reference count: 20
- Primary result: Neural quality scoring improves crawling prioritisation and downstream retrieval recall.

## Executive Summary
This paper addresses the challenge of low-quality content in web search by introducing a neural quality scoring approach for crawling prioritisation. The authors extend neural semantic quality estimators to the crawling domain, demonstrating that prioritising high-quality pages during crawling improves downstream retrieval effectiveness. They implement a Docker container for document quality scoring and integrate it into the Resilipipe pipeline for OWS datasets. Experiments on ClueWeb22-B show that an oracle crawler using quality scores outperforms BFS and DFS baselines in early retrieval recall (R@100).

## Method Summary
The method uses a QT5-small model fine-tuned on MS MARCO Web Search data, where documents with positive relevance labels serve as high-quality examples. The model outputs a log-probability score indicating likelihood of being relevant to at least one query, which serves as a proxy for semantic quality. The quality scorer is integrated into the Resilipipe pipeline after plaintext extraction from WARC files. The approach is evaluated through crawling simulations that compare quality-prioritized (QOracle) vs. BFS/DFS baselines on ClueWeb22-B.

## Key Results
- Oracle crawler using quality scores outperforms BFS and DFS baselines in early retrieval recall (R@100)
- Positive correlation (Pearson=0.286) between page quality and mean quality of linked pages
- Approach generalizes to OWS datasets

## Why This Works (Mechanism)

### Mechanism 1
Neural quality scorers trained on relevance signals can estimate semantic quality of uncrawled pages. QT5-small model trained on MS MARCO Web Search data learns to output log-probability scores indicating likelihood of being relevant. Core assumption: relevance judgments correlate with semantic quality. Evidence: 9.1M positive docs from MS MARCO used for training.

### Mechanism 2
Pages of similar semantic quality link to each other at above-chance rates. High-quality pages create discoverable clusters through their linking behavior. Core assumption: link behavior reflects semantic coherence. Evidence: Pearson correlation coefficient of 0.286 between page quality and mean outlink quality.

### Mechanism 3
Prioritizing high-quality pages during crawl improves downstream retrieval recall. Quality-prioritized crawling discovers relevant pages earlier, enriching the corpus for retrieval. Core assumption: Quality scores provide sufficient signal to identify relevant pages before content is seen. Evidence: QOracle outperforms BFS/DFS in R@100 on ClueWeb22-B.

## Foundational Learning

- Concept: Static index pruning and its relationship to quality scoring
  - Why needed here: Quality scorer originates from pruning work; understanding pruning explains why scorer targets semantic quality vs. link-based quality
  - Quick check question: Can you explain why a quality scorer trained for pruning might differ from one trained for crawling prioritization?

- Concept: Oracle functions in crawling simulations
  - Why needed here: QOracle experiments assume access to page content before crawling, which is unrealistic
  - Quick check question: In a real crawler, what signals could approximate the oracle without pre-downloading content?

- Concept: Early retrieval metrics (R@100)
  - Why needed here: Paper evaluates crawlers by downstream recall at fixed corpus sizes
  - Quick check question: Why is R@100 more informative than precision@100 for evaluating crawl prioritization?

## Architecture Onboarding

- Component map: WARC files -> Plaintext extraction -> Quality scoring -> Parquet metadata -> Crawl simulation -> Indexing -> BM25 retrieval

- Critical path:
  1. Extract plaintext from WARC files (Resilipipe pre-processing)
  2. Score documents with QT5-small model
  3. Add quality scores to metadata (parquet)
  4. (Experimental) Simulate crawl using quality-ranked queue
  5. Index crawled subset; evaluate R@100

- Design tradeoffs:
  - Oracle vs. real crawling: QOracle is proof-of-concept; real systems must approximate quality from inlink quality or other signals
  - Batch scoring vs. streaming: QT5-small scores efficiently in batches, but real crawlers may need incremental scoring as pages arrive
  - Threshold-based pruning vs. prioritization: Paper focuses on prioritization; pruning is a harder decision with higher risk

- Failure signatures:
  - Score distribution collapse: Most pages receive similar scores, losing prioritization signal
  - Domain-level bias: Certain domains dominate high-quality scores, over-focusing crawler
  - Query drift: Quality scorer trained on MS MARCO may not generalize to specialized domains

- First 3 experiments:
  1. Validate score distribution on your corpus: Run Docker scorer on sample; compare histogram to ClueWeb22-B distributions
  2. Test inlink-quality approximation: Compute correlation between page quality and mean outlink quality on your web graph
  3. A/B crawl simulation: Compare BFS/DFS vs. quality-prioritized queue on subset with ground-truth relevance judgments

## Open Questions the Paper Calls Out

- How can neural semantic quality scores be effectively combined with other relevance signals to improve crawling effectiveness?
  - Basis: Authors conclude that "relevance signals coming from quality scoring should be combined with other signals"
  - Why unresolved: Observed overlap between relevant and high-quality distributions suggests quality scores alone are insufficient
  - What evidence would resolve it: Hybrid crawling strategy integrating quality scores with connectivity metrics demonstrating superior early recall

- Can a realistic crawler using only approximated quality scores (derived from in-links) match the performance of the proposed QOracle?
  - Basis: Primary experiments relied on an "Oracle" function allowing access to page text before download
  - Why unresolved: Weak positive correlation (r=0.286) found between page quality and neighbors, but non-oracle testing not performed
  - What evidence would resolve it: Experimental results showing non-oracle crawler based on parent page quality outperforms BFS/DFS

- What is the computational trade-off between the latency of neural quality scoring and the efficiency gains in downstream crawling?
  - Basis: Method uses LLM-based scoring but provides no analysis of inference latency or throughput costs
  - Why unresolved: Unclear if computational overhead of neural model outweighs resource savings from skipping low-quality documents
  - What evidence would resolve it: Comparison of wall-clock time and resource consumption between neural scoring and traditional heuristic crawlers

## Limitations

- Oracle assumption: QOracle simulation requires pre-downloading page content, which is not feasible in real-time crawling
- Domain generalization: Quality scorer trained on MS MARCO Web Search may not generalize to specialized domains or languages
- Limited empirical validation: Generalization to OWS datasets claimed but not empirically validated beyond ClueWeb22-B

## Confidence

- High: Positive correlation (Pearson=0.286) between page quality and outlink quality is empirically demonstrated and directly measured
- Medium: Superiority of QOracle over BFS/DFS baselines in simulated early retrieval recall (R@100) is well-supported but oracle assumption limits real-world applicability
- Low: Generalization to OWS datasets is claimed but not empirically validated; paper does not report results on non-ClueWeb22-B corpora

## Next Checks

1. Test the quality scorer on a non-English or specialized domain corpus to assess cross-domain generalization
2. Implement a non-oracle quality approximation (e.g., inlink-based scoring) and compare its crawling effectiveness to QOracle in simulation
3. Evaluate the impact of quality-based prioritization on precision@100 to determine if early recall gains come at the cost of precision