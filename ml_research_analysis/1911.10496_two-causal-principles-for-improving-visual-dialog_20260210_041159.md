---
ver: rpa2
title: Two Causal Principles for Improving Visual Dialog
arxiv_id: '1911.10496'
source_url: https://arxiv.org/abs/1911.10496
tags:
- visual
- causal
- dialog
- visdial
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two causal principles to improve Visual Dialog
  (VisDial) models, leading to significant performance gains across multiple baseline
  models. Principle 1 removes direct dialog history input to the answer model to eliminate
  harmful shortcut biases, while Principle 2 addresses unobserved confounders (user
  preferences) that create spurious correlations.
---

# Two Causal Principles for Improving Visual Dialog

## Quick Facts
- **arXiv ID**: 1911.10496
- **Source URL**: https://arxiv.org/abs/1911.10496
- **Reference count**: 40
- **Primary result**: Causal principles (removing direct history input, addressing unobserved confounders) improve VisDial model performance across multiple baselines, with an ensemble of simple models outperforming the 2019 challenge winner.

## Executive Summary
This paper introduces two causal principles to address fundamental biases in Visual Dialog models. Principle 1 eliminates direct dialog history input to the answer model, preventing shortcut biases that bypass visual reasoning. Principle 2 treats training as causal intervention rather than likelihood estimation, addressing unobserved confounders (user preferences) that create spurious correlations. The authors propose specific algorithms including question type analysis, answer score sampling, and hidden dictionary learning to implement Principle 2. Applied to four representative baseline models, these principles consistently improve NDCG performance, demonstrating that causal reasoning can significantly enhance VisDial systems beyond standard training approaches.

## Method Summary
The authors propose two causal principles: (1) removing direct dialog history input to the answer decoder to eliminate harmful shortcut biases, and (2) treating training as causal intervention ($do$-calculus) rather than likelihood estimation to block backdoor paths created by unobserved user preferences. They implement Principle 2 through three approaches: Question Type approximation (using 55 manually defined types), Answer Score Sampling (using dense human relevance annotations), and Hidden Dictionary Learning (approximating the confounder with learned embeddings). The method is applied to baseline models by modifying their architecture to exclude history from the final multimodal fusion, then training with specialized loss functions that account for the confounder distribution.

## Key Results
- The simple LF model with these principles outperformed the 2019 Visual Dialog Challenge winner
- Consistent NDCG improvements across four baseline models (LF, HCIAE, CoAtt, RvA)
- Answer Score Sampling and Hidden Dictionary methods provided significant boosts over baseline
- Removing history input prevented models from relying on answer length correlations with history

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing direct history input to the answer decoder prevents shortcut biases that bypass visual reasoning
- **Mechanism:** This eliminates the causal path H→A, forcing the model to rely on mediated paths (H→Q→A) and visual knowledge (V→A), aligning with how annotators actually answered based on question and image rather than copying history
- **Core assumption:** Question Q sufficiently captures history information through co-references, making direct H→A redundant and harmful
- **Evidence anchors:** The VisDial annotators were not allowed to copy from previous Q&A but were encouraged to ask consecutive questions, indicating answers should be based on Q and I only
- **Break condition:** If Q fails to resolve co-references from H, the model lacks fallback context to answer correctly

### Mechanism 2
- **Claim:** Causal intervention (do-calculus) training blocks backdoor paths created by unobserved user preferences
- **Mechanism:** Standard likelihood training captures spurious correlations via confounder U. The backdoor adjustment formula P(A|do(I,H,Q)) = Σ_u P(A|Q,H,I,u)P(u|H) isolates the causal effect of question and image on answer
- **Core assumption:** An unobserved confounder U (user preference) influences both question distribution and answer annotations, creating spurious correlations
- **Evidence anchors:** The authors identify U as causing spurious correlations from training data, with do-calculus making training fundamentally different from traditional likelihood estimation
- **Break condition:** If U doesn't exist or has negligible impact, the complexity of causal intervention is unnecessary

### Mechanism 3
- **Claim:** The unobserved confounder U can be approximated using Answer Score Sampling or a Hidden Dictionary
- **Mechanism:** Since U is unobserved, the summation Σ_u P(u|H) is approximated using human relevance scores or a learned dictionary D_u to model the prior distribution
- **Core assumption:** Sampled relevance scores or learned dictionary vectors serve as sufficient proxies for the complex hidden user preference variable
- **Evidence anchors:** The authors use normalized ground-truth NDCG scores to approximate P(a_i|H) and show that Answer Score Sampling and Hidden Dictionary provide significant boosts
- **Break condition:** Without dense annotations, Answer Score Sampling is impossible, requiring reliance on weaker Question Type approximation or learned dictionary

## Foundational Learning

- **Concept: Structural Causal Models (SCMs)**
  - **Why needed here:** To model the data generation process as a directed acyclic graph and identify harmful vs. natural paths
  - **Quick check question:** Can you draw the causal graph representing the baseline model (I, H, Q → A) versus the proposed graph (adding U and removing H → A)?

- **Concept: Confounders and Backdoor Paths**
  - **Why needed here:** To understand why spurious correlations exist (U affects both Q and A) and why standard training fails to distinguish true causation from association
  - **Quick check question:** In the graph Q ← U → A, if high Q values correlate with high A values, can we conclude Q causes A? Why or why not?

- **Concept: Intervention (do-calculus) vs. Observation**
  - **Why needed here:** To understand the mathematical shift from P(A|Q,H,I) to P(A|do(Q),H,I), which is the core operation required to de-confound the model
  - **Quick check question:** How does the do(Q) operator graphically modify the causal graph compared to simply observing Q?

## Architecture Onboarding

- **Component map:** I, H, Q → Encoder → Attention → V → Decoder (fuses V and Q) → A
- **Critical path:**
  1. Feature Extraction: I → V_feat, H → H_feat, Q → Q_feat
  2. Attention: Generate V using H and Q context (keep H in attention, only remove from final decoder input)
  3. Decoder Input: Fuse V and Q (exclude H_feat from final multimodal fusion vector)
  4. Loss Calculation: Use output logits and dense annotations/dictionary to compute Weighted Softmax or Ranking Loss

- **Design tradeoffs:**
  - P1 vs. Information Loss: Gains robustness against shortcut biases but loses ability to access history facts not resolved in Q or V
  - P2 (Score Sampling) vs. Data Cost: Requires expensive dense annotations; Hidden Dictionary is cheaper at inference but needs training stability

- **Failure signatures:**
  - P1 Failure: Model ignores context implied by previous turns (e.g., answering "Yes" when previous turn established object doesn't exist)
  - P2 Failure: Model overfits to specific answer frequencies or user styles, indicating poor confounder approximation

- **First 3 experiments:**
  1. Ablate P1: Run baseline with and without H→A link, compare NDCG and MRR
  2. Implement Question Type Approximation: Verify training loop handles weighted loss
  3. Visualize Shortcut Bias: Plot answer length distributions to verify P1 reduces history correlation

## Open Questions the Paper Calls Out

- **Embodied visual dialog tasks:** The authors intend to discover other potential causalities hidden in embodied Q&A and conversational visual dialog tasks beyond the static image history and user preference addressed here
- **Additional causal principles:** What principles are required to effectively model embodied visual dialog tasks with dynamic physical interactions and action-result loops?
- **Hidden dictionary convergence:** Can the Hidden Dictionary Learning implementation theoretically converge to the distribution of the true unobserved confounder?

## Limitations

- The causal framework assumes a single unobserved confounder U is sufficient, though real datasets may contain multiple interacting confounders
- The approach relies on expensive dense relevance annotations that are unavailable in standard VisDial settings
- The specific list of 55 question types for confounder approximation is mentioned but not provided, making exact replication challenging

## Confidence

- **High confidence** in Principle 1 (removing direct history input) as it addresses a well-defined shortcut bias with clear causal mechanism and empirical support
- **Medium confidence** in Principle 2's overall effectiveness, as the causal intervention approach is theoretically sound but implementation details significantly affect performance
- **Medium confidence** in specific implementations (Question Type, Answer Score Sampling, Hidden Dictionary) as the paper demonstrates improved results but lacks ablation studies isolating each method's contribution

## Next Checks

1. **Ablation study on Principle 2 variants:** Implement and compare all three confounder approximation methods on the same baseline model to determine which contributes most to performance gains

2. **Real-world deployment test:** Evaluate the proposed principles on a VisDial dataset without dense annotations to assess whether the Hidden Dictionary method alone maintains performance improvements

3. **Alternative causal modeling:** Test whether simpler debiasing approaches (adversarial removal of style features, data augmentation) achieve comparable results to the full causal intervention framework