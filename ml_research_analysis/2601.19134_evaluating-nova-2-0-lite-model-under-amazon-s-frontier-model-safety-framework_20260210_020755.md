---
ver: rpa2
title: Evaluating Nova 2.0 Lite model under Amazon's Frontier Model Safety Framework
arxiv_id: '2601.19134'
source_url: https://arxiv.org/abs/2601.19134
tags:
- nova
- lite
- safety
- amazon
- frontier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Amazon evaluated Nova 2.0 Lite under its Frontier Model Safety
  Framework (FMSF), focusing on CBRN, offensive cyber, and AI R&D risks. The evaluation
  combined automated benchmarks (WMDP, PROTOCOLQA, CyBench, RE-Bench) and expert red-teaming
  with independent audits.
---

# Evaluating Nova 2.0 Lite model under Amazon's Frontier Model Safety Framework

## Quick Facts
- arXiv ID: 2601.19134
- Source URL: https://arxiv.org/abs/2601.19134
- Reference count: 14
- Nova 2.0 Lite passed Amazon's Frontier Model Safety Framework with enhanced factual accuracy but triggered additional safeguards in radiological domains

## Executive Summary
Amazon conducted a comprehensive safety evaluation of Nova 2.0 Lite using its Frontier Model Safety Framework, focusing on CBRN, offensive cyber, and AI R&D risks. The evaluation combined automated benchmarks (WMDP, PROTOCOLQA, CyBench, RE-Bench) with expert red-teaming and independent audits. While Nova 2.0 Lite showed significant improvements in factual accuracy over previous versions, particularly in chemistry (0.71 WMDP-CHEM) and biology (0.82 WMDP-BIO), it required additional radiological safeguards and demonstrated limitations in autonomous exploit development and research capabilities.

## Method Summary
The evaluation employed a multi-layered approach combining automated benchmarks with human expert assessment. Key methodologies included WMDP for chemical and biological knowledge assessment, PROTOCOLQA for procedural understanding, CyBench for cybersecurity capabilities, and RE-Bench for AI research and development evaluation. Red-teaming exercises were conducted by subject matter experts across all three risk domains, supplemented by independent reviews from Nemesys and METR. The framework established specific threshold criteria for each domain, with additional safeguards implemented for radiological domains where thresholds were exceeded.

## Key Results
- Nova 2.0 Lite achieved 0.71 on WMDP-CHEM and 0.82 on WMDP-BIO, showing improved factual accuracy over Nova 1.0 Pro and Premier
- The model demonstrated 7.5% uplift in CyBench performance while remaining below critical risk thresholds
- Independent reviews from Nemesys and METR confirmed safety compliance, supporting deployment with enhanced monitoring and filters

## Why This Works (Mechanism)
The evaluation's effectiveness stems from its layered approach combining quantitative benchmarks with qualitative expert assessment, creating multiple validation checkpoints that reduce the likelihood of safety gaps.

## Foundational Learning
- **WMDP benchmarking methodology** - Standardized assessment of factual knowledge in specialized domains; needed for consistent measurement across models
- **Red-teaming frameworks** - Structured adversarial testing approaches; needed to identify potential misuse scenarios
- **Risk threshold definitions** - Clear criteria for acceptable vs. concerning model behavior; needed for objective safety decisions
- **Independent audit processes** - Third-party verification protocols; needed for unbiased safety validation
- **CBRN safety assessment** - Specialized evaluation of chemical, biological, radiological, and nuclear risks; needed for high-stakes domain compliance
- **Cybersecurity capability evaluation** - Assessment of offensive security knowledge; needed for preventing misuse in cyber contexts

## Architecture Onboarding
- **Component map**: Nova 2.0 Lite model -> Benchmark suite (WMDP, PROTOCOLQA, CyBench, RE-Bench) -> Red-teaming layer -> Independent audit layer -> Safety decision
- **Critical path**: Model performance evaluation → Threshold comparison → Expert review → Independent validation → Deployment decision
- **Design tradeoffs**: Automated benchmarks provide scalability but may miss nuanced risks; expert review adds depth but is resource-intensive
- **Failure signatures**: Exceeding thresholds in specific domains (radiological) indicates need for additional safeguards; poor performance on factual benchmarks suggests knowledge gaps
- **First experiments**: 1) Run baseline model comparisons with statistical significance testing, 2) Conduct adversarial testing in radiological domain, 3) Validate independent review methodology with control scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Absence of statistical significance testing for performance improvements between models
- Limited transparency about independent reviewer pool size and diversity
- Heavy reliance on automated benchmarks with unclear real-world scenario coverage
- Radiological domain exception suggests potential blind spots in safety framework

## Confidence
- **Nova 2.0 Lite safety compliance**: Medium
- **Performance improvements over Nova 1.0**: Medium
- **Independent review reliability**: Low

## Next Checks
1. Conduct statistical significance testing on all benchmark comparisons between Nova 2.0 Lite and baseline models, including confidence intervals and effect sizes to determine practical relevance of performance differences.

2. Implement adversarial testing specifically targeting the radiological domain where thresholds were exceeded, using diverse expert teams to identify whether this represents a systematic vulnerability or isolated finding.

3. Publish detailed methodology documentation for independent reviewers, including reviewer qualifications, testing protocols, and potential conflicts of interest, to enable external validation of the safety assessment process.