---
ver: rpa2
title: Explaining Grokking and Information Bottleneck through Neural Collapse Emergence
arxiv_id: '2509.20829'
source_url: https://arxiv.org/abs/2509.20829
tags:
- training
- neural
- collapse
- theorem
- grokking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explains two late-phase deep learning phenomena\u2014\
  grokking and the information bottleneck (IB) principle\u2014through the lens of\
  \ neural collapse, which characterizes the geometry of learned representations.\
  \ The authors identify the contraction of population within-class variance as a\
  \ key factor underlying both phenomena."
---

# Explaining Grokking and Information Bottleneck through Neural Collapse Emergence

## Quick Facts
- arXiv ID: 2509.20829
- Source URL: https://arxiv.org/abs/2509.20829
- Authors: Keitaro Sakamoto; Issei Sato
- Reference count: 40
- Primary result: Neural collapse's within-class variance contraction explains both grokking and information bottleneck dynamics through unified theoretical framework

## Executive Summary
This paper provides a unified explanation for two late-phase deep learning phenomena—grokking (delayed generalization) and information bottleneck compression—through the lens of neural collapse geometry. The key insight is that the contraction of population within-class variance in learned representations underlies both phenomena. For grokking, the authors derive a generalization error bound in terms of this variance, while for IB dynamics, they show superfluous information is bounded by the same quantity. The theoretical findings are validated across multiple datasets and architectures, providing practical insights for training strategies through weight decay control.

## Method Summary
The authors analyze neural collapse dynamics using a 4-layer MLP [784,200,200,200,10] trained on MNIST with 1000 samples, squared loss, AdamW optimizer, and varying weight decay. They measure RNC1 (rescaled within-class variance) and track its relationship to test accuracy improvement. The theoretical framework connects variance contraction to generalization bounds via Cantelli's inequality and to IB compression via mutual information bounds. Time-scale separation analysis shows that neural collapse emergence (τ₂) occurs on a distinct timescale from training loss convergence (τ₁), particularly when weight decay is small.

## Key Results
- Generalization error is bounded by population within-class variance, with RNC1 serving as empirical proxy
- Superfluous information I(Z;X|Y) in IB dynamics is upper-bounded by the same within-class variance quantity
- Time-scale separation between training loss convergence (τ₁) and neural collapse emergence (τ₂) is controlled by weight decay λ
- Theoretical predictions validated across MNIST, Fashion-MNIST, and CIFAR10 with both MLPs and CNNs

## Why This Works (Mechanism)

### Mechanism 1: Within-Class Variance Contraction Enables Generalization
- **Claim:** Reduction of population within-class variance tightens generalization error bounds and drives grokking transition.
- **Mechanism:** Theorem 3.2 establishes test error bound as function of within-class variance; RNC1 decrease tightens margin-based bound.
- **Core assumption:** Last-layer classifier fixed; representations become linearly separable as variance decreases.
- **Evidence anchors:** [abstract] "For grokking, they derive a generalization error bound in terms of this variance"; [Section 3.1, Theorem 3.2] full derivation via Cantelli's inequality.
- **Break condition:** If within-class variance doesn't decrease post-interpolation, or class means fail to separate.

### Mechanism 2: Variance Reduction Bounds Superfluous Information
- **Claim:** Superfluous information I(Z;X|Y) discarded in IB compression is upper-bounded by population within-class variance.
- **Mechanism:** Theorem 3.4 shows I(Z;X|Y) ≤ (1/2σ²) E[||g̃(X) - E[g̃(X)|Y]||²]; as variance contracts, bound forces information downward.
- **Core assumption:** Small Gaussian noise added to make MI well-defined; tight when Z|Y is Gaussian.
- **Evidence anchors:** [abstract] "for IB dynamics, they show that superfluous information is bounded by the same quantity"; [Section 3.2, Theorem 3.4] derivation via differential entropy bounds.
- **Break condition:** If representations don't collapse (RNC1 stays high), bound remains loose.

### Mechanism 3: Time-Scale Separation via Weight Decay Control
- **Claim:** Training loss convergence (τ₁) and neural collapse emergence (τ₂) proceed on distinct time scales controlled by weight decay λ.
- **Mechanism:** Theorem 4.3 shows τ₁ = Ω((1/η)log(1/ε₁)) for loss but τ₂ = Ω((1/λη)log(1/ε₂)) for RNC1; small λ creates delay between fitting and generalization/compression.
- **Core assumption:** Pyramidal network architecture, smooth activation, specific initialization conditions; squared loss with weight decay.
- **Evidence anchors:** [Section 4.2, Theorem 4.3] explicit time-scale derivation with λ and η dependence; [Figure 3] empirical validation with higher λ causing earlier RNC1 decrease.
- **Break condition:** If weight decay too large or architecture violates assumptions, time-scale separation may not hold.

## Foundational Learning

- **Concept: Neural Collapse (NC1, NC2, RNC1)**
  - **Why needed here:** Core geometric phenomenon being analyzed; RNC1 measures within-class variance rescaled by output norm.
  - **Quick check question:** Can you explain why RNC1 isolates within-class variance better than NC1 when all features collapse to a single point?

- **Concept: Information Bottleneck Principle**
  - **Why needed here:** Theoretical framework for compression vs. prediction tradeoff; mutual information terms track representation quality.
  - **Quick check question:** Why does I(Z;X) potentially become infinite for deterministic networks with continuous representations, and how does noise injection address this?

- **Concept: Cantelli's Inequality**
  - **Why needed here:** Mathematical tool used in Theorem 3.2 to derive variance-based tail probability bounds for generalization.
  - **Quick check question:** How does Cantelli's inequality differ from Chebyshev's, and why is it more suitable for one-sided generalization bounds?

## Architecture Onboarding

- **Component map:** Input X → Feature Extractor g(x) → [Representation Space] → Classifier W → Output f(x) → RNC1 Calculation: Tr(Σ_W) / B²_g → Neural Collapse Dynamics (τ₁ vs τ₂)

- **Critical path:**
  1. Train network with squared loss + weight decay λ
  2. Monitor RNC1 = (1/N) Σ_c Σ_i ||g̃(x_i) - μ̃_c||² over training
  3. Track alignment between RNC1 decrease and test accuracy improvement
  4. Vary λ to validate time-scale separation predictions

- **Design tradeoffs:**
  - **Higher λ:** Faster collapse/earlier generalization but may sacrifice final accuracy
  - **Larger initialization scale:** Promotes grokking behavior but increases training instability
  - **Pyramidal vs. uniform width:** Pyramidal enables theoretical guarantees but may limit expressivity

- **Failure signatures:**
  - RNC1 plateaus without decreasing → weight decay too weak or architecture incompatible
  - Test accuracy improves before RNC1 decreases → check NC2 (class separation) as compensating factor
  - Grokking absent in CNNs/ViT → inductive biases may collapse representations during fitting phase

- **First 3 experiments:**
  1. Replicate MNIST MLP grokking: 4-layer MLP, 1000 samples, weight decay ∈ {0.01, 0.1, 0.3}, track accuracy + RNC1 + NC2 synchrony
  2. Time-scale validation: Measure τ₁ (training loss < ε₁) and τ₂ (RNC1 < ε₂) across λ values; confirm τ₂ ∝ 1/λ relationship
  3. IB dynamics estimation: Use PCA compression + Kozachenko-Leonenko estimator to track I(Z;X) - I(Z;Y) alongside RNC1; verify correlated decrease

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the time-scale analysis of neural collapse be extended to architectures beyond MLPs, such as CNNs or Transformers, and to different initialization methods?
- **Basis in paper:** [explicit] The conclusion states: "A natural next step is to extend the time-scale analysis of neural collapse to other architectures beyond MLP or to different initialization methods."
- **Why unresolved:** The theoretical analysis (Theorem 4.3) relies on pyramidal network assumptions and smooth activations; empirical results show CNNs and Transformers exhibit different behavior without grokking.
- **What evidence would resolve it:** Theoretical bounds on RNC1 convergence for CNN/Transformer architectures, or counterexamples showing fundamental differences in collapse dynamics.

### Open Question 2
- **Question:** Can neural collapse emerge and progress on different time scales without explicit weight decay regularization?
- **Basis in paper:** [explicit] The conclusion states: "Another interesting direction is to analyze the possibility of neural collapse that implicitly arises without weight decay."
- **Why unresolved:** Theorem 4.3 shows RNC1 convergence time scales inversely with weight decay (τ₂ = Ω(1/(λ η))), but the role of implicit regularization from optimization alone is unclear.
- **What evidence would resolve it:** Empirical demonstration of delayed neural collapse dynamics without weight decay, or theoretical analysis of implicit regularization effects on within-class variance.

### Open Question 3
- **Question:** Do the theoretical connections between neural collapse, grokking, and IB dynamics hold under cross-entropy loss rather than squared loss?
- **Basis in paper:** [inferred] The theoretical analysis uses squared loss "as standard in this line of work" while acknowledging cross-entropy is more commonly used in practice; the relationship between loss function choice and collapse dynamics remains unaddressed.
- **Why unresolved:** The proofs rely on specific properties of squared loss; it's unclear whether the generalization bound (Theorem 3.2) and IB bound (Theorem 3.4) transfer directly.
- **What evidence would resolve it:** Theoretical derivation of equivalent bounds under cross-entropy, or systematic empirical comparison of RNC1 dynamics across loss functions.

## Limitations
- Theoretical analysis relies on strong assumptions including pyramidal architectures and smooth activations that may not hold for modern architectures like ResNets or Transformers.
- Mutual information bounds require Gaussian assumptions that are difficult to verify empirically in high-dimensional settings.
- Experiments show limited grokking behavior in CNNs and ViTs, suggesting the core mechanism may be architecture-dependent rather than universal.

## Confidence
**High Confidence:** The RNC1 metric as measure of within-class variance contraction and its empirical relationship to generalization timing is well-supported across multiple architectures and datasets.

**Medium Confidence:** The claim that time-scale separation controlled by weight decay explains grokking is supported empirically but relies on restrictive theoretical assumptions.

**Low Confidence:** The universal applicability of neural collapse-based explanations across all architectures, particularly for CNNs and ViTs where grokking is minimal, remains uncertain.

## Next Checks
1. **Architecture Transfer Test:** Systematically test the RNC1-generalization relationship on architectures beyond MLPs, particularly focusing on why CNNs/ViTs show limited grokking despite exhibiting neural collapse metrics.

2. **Noise Sensitivity Analysis:** Quantify how sensitive the theoretical bounds and empirical observations are to the choice of noise variance σ in mutual information estimation, and test alternative MI estimation methods.

3. **Early-Bird Phenomenon Investigation:** Examine cases where test accuracy improves before RNC1 decreases to identify compensatory mechanisms (such as NC2/class separation) and refine the variance-based explanation.