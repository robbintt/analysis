---
ver: rpa2
title: Learning to explore when mistakes are not allowed
arxiv_id: '2502.13801'
source_url: https://arxiv.org/abs/2502.13801
tags:
- safety
- policy
- safe
- exploration
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles the challenge of enabling goal-conditioned reinforcement
  learning (GCRL) agents to explore safely, without risking harmful mistakes. The
  authors propose a two-phase method: first, pretraining a safety policy via safe
  RL and distributional techniques to avoid failures; second, using this policy alongside
  a goal-conditioned policy during exploration, with an action-selection mechanism
  that switches to the safety policy when risk is high.'
---

# Learning to explore when mistakes are not allowed

## Quick Facts
- arXiv ID: 2502.13801
- Source URL: https://arxiv.org/abs/2502.13801
- Reference count: 40
- Primary result: Two-phase safe exploration method drastically reduces goal-conditioned RL failures while maintaining coverage

## Executive Summary
This paper addresses the challenge of safe exploration in goal-conditioned reinforcement learning (GCRL), where agents must explore to reach diverse goals without making catastrophic mistakes. The authors propose a two-phase approach: first pretraining a safety policy using distributional RL and reachability critics to avoid failures, then using this policy alongside a goal-conditioned policy during exploration with an action-selection mechanism that switches to safety when risk is high. Evaluated on CartPoleGC and SkydioX2GC environments, the method achieves near-zero mistakes in some runs while maintaining reasonable goal-space coverage.

## Method Summary
The method consists of two phases. Phase 1 pretrains a safety policy using TQC-style distributional RL with M=5 critics and N=25 quantile atoms, plus reachability critics with max-operator targets that estimate proximity to unsafe states. The actor loss combines Q-values and reachability critics with multiplier λ=100. Phase 2 freezes the safety policy and trains a goal-conditioned policy with SAC-N + HER. During exploration, an action-selection mechanism samples actions from both policies and uses worst-case quantiles (τ=0.9) to estimate risk, switching to the safety policy when the risk exceeds threshold th_GC→S. The approach assumes environment dynamics are spatially uniform, allowing the safety policy to generalize globally.

## Key Results
- Drastically reduces mistake occurrence compared to standard GCRL, achieving near-zero errors in some runs
- Maintains reasonable goal-space coverage while preventing failures
- Ablation studies show importance of reachability critics and threshold settings for balancing safety and exploration
- Critic disagreement spikes before failures, indicating undertrained safety policy regions

## Why This Works (Mechanism)

### Mechanism 1
A pretrained safety policy prevents catastrophic failures during goal-conditioned exploration by arbitrating actions based on distributional risk estimates. The safety policy is trained first using distributional RL and reachability critics, then used during exploration to switch actions when risk is high. Core assumption: environment dynamics are spatially uniform, so a policy trained for safety without exploration goals can still be exploited globally. Evidence: uniform dynamics assumption stated in abstract; policy optimized with reachability critic in section 4.2. Break condition: if dynamics are highly goal-dependent, safety policy won't transfer; if safety policy is poorly trained in certain state regions, critic disagreement spikes and failures occur.

### Mechanism 2
Reachability critics with max-operator targets provide a continuous distance metric to unsafe states, enabling earlier intervention than cumulative-cost critics. The reachability critic target propagates worst-case constraint values forward, creating a value proportional to proximity to failure boundaries. Core assumption: the constraint function h(s) is continuous, allowing generalization from visited safe states to nearby unvisited states. Evidence: continuity of h(s) crucial for generalization (section 4.1); explicit RCRL target formulation with max operator (section 4.2, equation 2); ablation shows L&S best for CartPoleGC, S best for SkydioX2GC (section 5.3.1). Break condition: if h(s) is discontinuous or poorly defined, generalization fails; if λ is too low, policy ignores reachability; if too high, instability during training.

### Mechanism 3
Distributional critics using worst-case quantiles (τ = 0.9) enable risk-averse action selection that anticipates rare failure modes. Rather than using expected return, the risk function uses the mean of quantiles corresponding to cumulative probabilities > τ (worst 10% of outcomes). Core assumption: the safety policy's return distribution can be adequately approximated by N quantiles (N=25 in experiments). Evidence: relative positions of quantiles with respect to threshold allow risk level control (section 3.4); three strategies defined using quantile-based risk estimation (section 4.3). Break condition: if distribution has heavy tails not captured by N quantiles, or if τ is set too low (ignores real risks) or too high (over-conservative, reduces coverage).

## Foundational Learning

- **Distributional Reinforcement Learning (Quantile Regression)**
  - Why needed here: Understanding how quantile critics approximate return distributions (not just means) is essential for grasping why worst-case quantiles capture risk
  - Quick check question: Can you explain why the τ-th quantile of a return distribution represents a value that is exceeded with probability (1-τ)?

- **Goal-Conditioned RL with Hindsight Experience Replay (HER)**
  - Why needed here: The GC policy uses SAC+HER to learn from sparse rewards; understanding relabeling is critical for the safe exploration phase
  - Quick check question: How does the "future" relabeling strategy in HER convert a failed trajectory into useful training data?

- **Reachability Theory and Constrained MDPs**
  - Why needed here: The reachability critic is fundamentally different from standard value functions; it estimates proximity to constraint violation via max propagation
  - Quick check question: Why does a max-operator target (vs. sum) prevent "compensating" safe behavior with earlier risky behavior?

## Architecture Onboarding

- **Component map:**
  Phase 1 (Pretraining): Safety Policy π_φS with Ensemble of M distributional critics Z_ψ1...M (TQC-style) and Ensemble of M reachability critics R_ξ1...M (RCRL-style)
  Phase 2 (Safe Exploration): Load frozen: π_φS, all critics; Initialize: GC Policy π_φGC (SAC-N with N critics); Action Selection: Sample a_GC ~ π_φGC(s,g), a_S ~ π_φS(s); Compute risk σ̂(s, a) for both actions; Switch based on thresholds th_GC→S, th_S→GC

- **Critical path:**
  1. Define constraint function h(s) carefully—it must be continuous and h(s)>0 iff s is terminal
  2. Pretrain safety policy with "reset anywhere" to cover diverse states
  3. Validate safety policy quality BEFORE safe exploration (high variance in pretraining causes variance in exploration safety)
  4. Set thresholds: start with hysteresis (th_GC→S > th_S→GC), tune based on safety-coverage tradeoff

- **Design tradeoffs:**
  | Decision | Safe Choice | Coverage Choice | Paper Finding |
  |----------|-------------|-----------------|---------------|
  | λ in actor loss | λ=100 | λ=0 | L&S best for CartPole; S alone best for Skydio |
  | Thresholds | Low (e.g., 30/30) | High with hysteresis (70/30) | Hysteresis helps CartPole; equal better for Skydio |
  | Risk strategy | Time-constraint | Time only | Constraint-only catastrophically bad |

- **Failure signatures:**
  - Critic disagreement spike before failure: If L1 disagreement between ensemble critics suddenly increases, the safety policy is in an unfamiliar state region—expect imminent failure
  - Safety policy active during failure: If failures occur while safety_flag=1, switching was too late; reduce th_GC→S
  - Zero coverage for edge goals: If goals near environment bounds are never reached, thresholds are too conservative or safety policy is over-protective

- **First 3 experiments:**
  1. Sanity check safety pretraining: Run 4 pretraining seeds, evaluate each on held-out initial states. Measure variance in episodes-to-failure. Discard high-variance seeds before Phase 2.
  2. Threshold sweep on single seed: Fix one pretraining seed, sweep (th_GC→S, th_S→GC) ∈ {(30,30), (50,50), (70,30), (70,70)}. Plot safety vs. coverage Pareto frontier.
  3. Ablate reachability usage: Compare L&S vs. L vs. S vs. None on both environments. Confirm environment-specific optimal configurations before committing to full runs.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating critic disagreement explicitly into the action selection mechanism prevent failures in under-explored regions of the state space? Basis: The authors identify that failures are "characterized by a huge disagreement between critics before the mistake happens" and propose "taking disagreement into account in the switching mechanism" as a research direction. Unresolved because the current mechanism only uses quantile-based risk estimates, not ensemble disagreement. Evidence needed: Experiments comparing failure rates when switching thresholds depend on both risk estimates and critic disagreement.

### Open Question 2
Can fine-tuning the safety policy during the safe exploration phase improve coverage while maintaining safety? Basis: The authors state that "fine-tuning the safety policy during the safe exploration phase could expand the set of reachable states, further enhancing exploration" and confirm this with supplementary analysis. Unresolved because the safety policy remains fixed during exploration. Evidence needed: A method that safely updates safety critics online, with experiments showing increased goal-space coverage without increased mistakes.

### Open Question 3
How can the high variance in safety policy pre-training quality be reduced, and what evaluation metrics predict downstream safe exploration performance? Basis: Figure 9 shows high variance in pretraining outcomes affecting downstream safety. Unresolved because pretraining uses standard TQC without targeted exploration of states relevant to downstream GCRL tasks. Evidence needed: Identification of pre-training metrics that correlate with downstream safety performance, or a curriculum that reduces variance across seeds.

## Limitations

- Environment generalization: The safety policy assumes spatially uniform dynamics, but this may not hold in more complex tasks where risk is state-dependent rather than globally transferable.
- Threshold sensitivity: The safety-coverage tradeoff is highly dependent on threshold settings (th_GC→S, th_S→GC), with different optimal settings for different environments but limited sensitivity analysis.
- Critic ensemble reliability: The method relies on ensemble disagreement detection to identify unfamiliar states, but this assumes disagreement is a reliable indicator of safety policy uncertainty.

## Confidence

- **High confidence**: The two-phase architecture (pretraining safety policy, then exploring with arbitration) is well-specified and the experimental results on the tested environments are clearly presented.
- **Medium confidence**: The distributional critics using worst-case quantiles effectively capture risk is supported by ablation studies, but the claim that this captures "rare failure modes" is less certain without explicit analysis of the quantile distribution shapes.
- **Low confidence**: The safety policy trained without exploration goals generalizes globally due to uniform dynamics is an assumption that works for the tested environments but lacks broader validation across diverse task structures.

## Next Checks

1. **Generalization test**: Evaluate the method on an environment with non-uniform dynamics where risk varies by state (e.g., varying friction coefficients or wind fields). This would test whether the safety policy's global transferability assumption holds beyond the current setup.

2. **Ensemble disagreement validation**: Systematically analyze whether critic ensemble disagreement actually correlates with safety policy uncertainty by: (a) computing disagreement in known safe vs risky regions, and (b) comparing disagreement patterns to actual failure rates across the state space.

3. **Threshold robustness analysis**: Perform a comprehensive grid search over threshold combinations across multiple seeds and plot the full safety-coverage Pareto frontier. This would reveal whether the reported "best" thresholds are truly optimal or just locally good, and how sensitive the tradeoff is to hyperparameter choice.