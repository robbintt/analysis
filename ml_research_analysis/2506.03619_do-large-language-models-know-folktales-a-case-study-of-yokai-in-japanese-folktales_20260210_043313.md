---
ver: rpa2
title: Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese
  Folktales
arxiv_id: '2506.03619'
source_url: https://arxiv.org/abs/2506.03619
tags:
- yokai
- japanese
- yokaieval
- llms
- folktales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well large language models understand
  Japanese folktales by introducing YokaiEval, a benchmark of 809 multiple-choice
  questions about yokai (supernatural creatures from Japanese folklore). Questions
  are generated from Wikipedia articles, filtered, and manually verified.
---

# Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales

## Quick Facts
- **arXiv ID**: 2506.03619
- **Source URL**: https://arxiv.org/abs/2506.03619
- **Reference count**: 40
- **Primary result**: Japanese-CPT models outperform English-centric models on cultural folktale knowledge

## Executive Summary
This study evaluates how well large language models understand Japanese folktales by introducing YokaiEval, a benchmark of 809 multiple-choice questions about yokai (supernatural creatures from Japanese folklore). Questions are generated from Wikipedia articles, filtered, and manually verified. The dataset covers various regions of Japan and references works by both historical and modern folklorists. Evaluations of 31 models show that Japanese-centric models, especially those with continued pretraining on Japanese data (e.g., Llama-3 variants), outperform English-centric models. This highlights the importance of community-specific language resources for cultural knowledge. Fine-tuning alone (SFT or preference learning) shows limited impact on folktale understanding, while continued pretraining is key. The study emphasizes careful material selection in cross-cultural NLP and provides a dataset and code for future research.

## Method Summary
The authors created YokaiEval by generating questions from 1,054 Wikipedia articles about Japanese yokai using GPT-4o, filtering with GPT-4o-mini, and manually verifying references and coherence. The final dataset contains 809 multiple-choice questions with 4 options each. They evaluated 31 models including both English-centric and Japanese-centric variants, measuring accuracy and comparing against JMT-Bench scores. To check for data leakage, they compared model perplexity on original versus paraphrased Wikipedia articles. The evaluation used temperature=0.1, top_p=1.0, and max_new_tokens=128, with GPT-4o judging correctness rather than lexical matching.

## Key Results
- Japanese-centric models with continued pretraining on Japanese data significantly outperform English-centric models on yokai knowledge
- Continued pretraining is more effective than standard fine-tuning (SFT or preference learning) for acquiring cultural knowledge
- Models perform worse on yokai from Okinawa (40% accuracy) compared to other regions, suggesting regional cultural distinctiveness affects knowledge acquisition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pretraining on community-specific language resources is more effective for acquiring deep cultural knowledge than standard fine-tuning (SFT or preference learning).
- Mechanism: Pretraining on a large corpus of Japanese text exposes the model to a high volume of cultural information (like folktales and yokai) in context, allowing the model to encode these facts into its parameters. Fine-tuning on generic instruction datasets shapes the model's behavior for existing knowledge but does not add new cultural knowledge.
- Core assumption: The knowledge gap is due to a lack of exposure to cultural data, not an inability to learn from it. The datasets used for SFT and preference learning in the study were not specifically designed to teach yokai lore.
- Evidence anchors:
  - [abstract] "Fine-tuning alone (SFT or preference learning) shows limited impact on folktale understanding, while continued pretraining is key."
  - [section 5.2] "While SFT leads to a significant improvement in JMT-Bench scores, its impact on YokaiEval is marginal... suggesting that SFT may have a limited effect on acquiring knowledge of Japanese folktales."
  - [corpus] Related work ("Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset") discusses cultural evaluation, but the specific mechanism comparing pretraining vs. fine-tuning is a primary contribution of this paper.
- Break condition: If fine-tuning datasets were curated to explicitly include yokai-related knowledge, SFT could become a more effective pathway for knowledge acquisition.

### Mechanism 2
- Claim: A strong base model's general capabilities can be efficiently transferred to a new cultural domain via continued pretraining.
- Mechanism: Starting with a model like Llama-3 (which has strong general language understanding) provides a robust foundation. Continued pretraining in Japanese then specializes this model, filling its knowledge gaps with Japanese cultural information while retaining its core reasoning abilities. This avoids the immense cost of training from scratch.
- Core assumption: The model's architecture and general learning capacities are sufficient; only the data (and thus cultural knowledge) is lacking.
- Evidence anchors:
  - [abstract] "...particularly those based on Llama-3, performing especially well."
  - [section 5.2] "...continually pretraining a highly capable English-centric model (e.g., the Llama-3 family) on Japanese documents appears to be important for acquiring knowledge of Japanese folktales."
  - [corpus] A related paper ("Harnessing PDF Data for Improving Japanese Large Multimodal Models") supports leveraging existing powerful models and focusing on data for cultural adaptation in other modalities.
- Break condition: If the base model's architecture were fundamentally incompatible with the nuances of the target language or cultural concepts, continued pretraining might be less effective than training from scratch.

### Mechanism 3
- Claim: Careful curation and manual verification of a benchmark dataset are critical to avoid data leakage and ensure the test truly measures knowledge.
- Mechanism: The authors generate questions from Wikipedia articles using GPT-4o, automatically filter them with GPT-4omini, and then manually verify references and coherence. To mitigate the risk of evaluating on memorized data, they check for data leakage by comparing model perplexity on original vs. paraphrased articles.
- Core assumption: Even sophisticated LLMs can generate flawed or ungrounded questions, and a benchmark's validity rests on the quality of its questions and answers. Low perplexity on a source text is a signal of memorization.
- Evidence anchors:
  - [section 4.1] "We manually check all the references of the QAs, as we find that GPT-4o often fails to identify the correct references."
  - [section 5.2] "Figure 6 shows the perplexity... no significant difference is observed... This suggests no clear evidence of data leakage in either case."
  - [corpus] The corpus does not contain papers specifically on benchmark creation methodology for folktales, so this is an internal mechanism of the study.
- Break condition: If a large portion of the yokai-related web corpus (beyond the sampled Wikipedia articles) was already memorized by the models, the perplexity check might not be sensitive enough to detect all leakage.

## Foundational Learning

### Concept: Continued Pretraining (CPT)
- Why needed here: The paper identifies this as the *key* mechanism for cultural knowledge acquisition, contrasting it with SFT.
- Quick check question: How does updating a model's weights on a large, unstructured corpus of Japanese text differ in its effect from training on a smaller, labeled dataset of question-answer pairs?

### Concept: Data Leakage and Perplexity
- Why needed here: The study uses perplexity as a key diagnostic tool to validate its benchmark. Understanding this concept is crucial for interpreting the results.
- Quick check question: If an LLM has "memorized" a Wikipedia article, what would you expect to happen to the model's perplexity score when evaluating that article compared to a paraphrased version of it?

### Concept: Cross-Cultural Transfer
- Why needed here: This is the core problem the paper addresses. Understanding why it's harder than cross-lingual transfer frames the motivation for the work.
- Quick check question: Why is it challenging for a model trained predominantly on English data to accurately answer questions about specific Japanese cultural concepts, even if it can translate between the languages?

## Architecture Onboarding

### Component map:
Base Model (Llama-3) -> CPT Data (Japanese corpus) -> SFT/PL Data (instruction datasets) -> YokaiEval (809 MCQs)

### Critical path:
Start with a strong base model (Llama-3) → Perform Continued Pretraining on Japanese data → Apply SFT/PL for general alignment. The knowledge for YokaiEval is acquired almost entirely at step 2.

### Design tradeoffs:
Training a model from scratch allows for full control but is prohibitively expensive. CPT on a strong base model is far more efficient and, as shown, highly effective for cultural knowledge, but it inherits any biases or limitations of the original model.

### Failure signatures:
- High JMT-Bench score but low YokaiEval score: Indicates the model can speak Japanese well but lacks deep cultural knowledge.
- Low YokaiEval score after SFT/PL: Confirms that these post-training methods don't significantly inject new knowledge, highlighting the need for CPT.
- High accuracy on YokaiEval but low perplexity difference on original vs. paraphrased articles: Would signal potential data leakage, invalidating the benchmark.

### First 3 experiments:
1. **Replicate the main result:** Evaluate a strong English-centric base model (e.g., Llama-3) and its Japanese-CPT counterpart (e.g., a Swallow model) on YokaiEval. Confirm the performance gain.
2. **Ablate the training steps:** Take a base model, perform only SFT, and evaluate. Then perform CPT and evaluate. Compare the gains to isolate the contribution of each step as done in the paper.
3. **Test for data leakage:** Follow the methodology in Section 5.2. Select a few yokai articles, generate paraphrased versions with GPT-4o, and compare the perplexity scores of the model being evaluated on both. Verify the "no leakage" conclusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do the findings from YokaiEval generalize to folktales from other cultures or to other domains of cultural knowledge beyond Japanese yokai?
- Basis in paper: [explicit] The limitations section states: "Our study focuses on Yokai in Japanese folktales, and the findings may differ when applied to other folktales, communities, or cultural domains. Our methodology may be extended to other cultural contexts in future research."
- Why unresolved: The benchmark and analysis are specific to Japanese yokai folklore. It is unclear if the relative importance of continued pretraining versus fine-tuning applies equally to other cultural knowledge domains with different characteristics (e.g., oral traditions with less digital presence).
- What evidence would resolve it: Replication of the methodology (benchmark construction + model evaluation) for folktales from other cultures (e.g., African, Celtic, Indigenous American), followed by comparative analysis of which training procedures are most effective across different cultural contexts.

### Open Question 2
- Question: What explains the significantly lower performance of Japanese LLMs on yokai from Okinawa (40% accuracy) compared to other regions, and how does regional cultural distinctiveness affect LLM knowledge?
- Basis in paper: [explicit] The error analysis notes: "It may be because Okinawa has a historically distinct background compared to the other regions, as it was once the Ryukyu Kingdom and the United States. In-depth analysis is required to analyze the effect of the regions to the knowledge of the LLMs."
- Why unresolved: The paper documents the performance gap but does not investigate its causes. Potential explanations include distinct linguistic traditions, different prevalence in training corpora, or unique cultural syncretism not captured in mainland-focused resources.
- What evidence would resolve it: Targeted analysis of training data composition for regional Japanese content; testing models on Okinawan-specific cultural benchmarks; controlled experiments with models fine-tuned on regionally balanced yokai corpora.

### Open Question 3
- Question: Can Retrieval-Augmented Generation (RAG) achieve comparable or superior performance to continued pretraining for acquiring cultural folktale knowledge?
- Basis in paper: [explicit] The limitations section mentions: "Our analysis is limited to evaluating the knowledge embedded in the LLMs themselves. However, it may not be the only way to achieve culturally aware LLMs. One potential approach for acquiring cultural knowledge is Retrieval-Augmented Generation (RAG)."
- Why unresolved: The paper only evaluates parametric knowledge. RAG might be more practical and updatable for niche cultural domains, but no comparison is provided.
- What evidence would resolve it: Experiments comparing parametric-only models against RAG-augmented models on YokaiEval, measuring both accuracy and cost/efficiency tradeoffs, particularly for yokai with limited training data presence.

## Limitations
- The study relies on GPT-4o for generating and filtering benchmark questions, which introduces potential model bias into the dataset construction process.
- The paper assumes that continued pretraining on Japanese text automatically leads to better folktale knowledge, but does not directly analyze what specific cultural knowledge is acquired versus general Japanese language proficiency.
- The perplexity-based leakage detection, while showing no significant differences, may not be sensitive enough to detect subtle memorization patterns across a large web corpus.

## Confidence
- **High confidence** in the main empirical finding that Japanese-CPT models outperform English-centric models on YokaiEval, supported by systematic evaluation of 31 models.
- **Medium confidence** in the conclusion that continued pretraining is more effective than SFT for cultural knowledge acquisition, as the study does not directly compare these approaches on identical base models or training regimes.
- **Medium confidence** in the benchmark quality, as the manual verification process is thorough but the initial question generation relies on another LLM.

## Next Checks
1. **Direct pretraining vs. fine-tuning comparison**: Train two identical base models on the same Japanese corpus—one with continued pretraining and one with SFT on question-answer pairs—then evaluate both on YokaiEval to isolate the knowledge acquisition mechanism.
2. **Cross-cultural generalization test**: Evaluate the same models on folklore benchmarks from other cultures (e.g., European fairy tales) to determine if the performance gains are specific to Japanese cultural knowledge or reflect general improvements in cultural reasoning.
3. **Knowledge tracing analysis**: Use probing techniques or intermediate checkpoints during continued pretraining to track when and how yokai-related knowledge emerges, distinguishing between memorization and true understanding.