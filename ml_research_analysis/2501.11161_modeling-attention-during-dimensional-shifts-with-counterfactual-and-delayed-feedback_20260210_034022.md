---
ver: rpa2
title: Modeling Attention during Dimensional Shifts with Counterfactual and Delayed
  Feedback
arxiv_id: '2501.11161'
source_url: https://arxiv.org/abs/2501.11161
tags:
- feedback
- learning
- shift
- human
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares two methods for modeling human attention during
  dimensional shifts in contextual bandit tasks with immediate, delayed, and counterfactual
  feedback. One method uses reward prediction errors (RPE) to iteratively update attention
  weights, while the other calculates mutual information over a memory of past experiences.
---

# Modeling Attention during Dimensional Shifts with Counterfactual and Delayed Feedback

## Quick Facts
- arXiv ID: 2501.11161
- Source URL: https://arxiv.org/abs/2501.11161
- Reference count: 10
- Primary result: WIBL method better captures human-like attention shifts in dimensional change tasks

## Executive Summary
This study investigates how humans allocate attention during dimensional shifts in contextual bandit tasks, comparing two computational approaches: reward prediction error (RPE) updates versus mutual information calculations over past experiences. The research focuses on tasks where the relevant dimensions for decision-making can shift, requiring adaptive attention allocation. The study examines performance under immediate, delayed, and counterfactual feedback conditions to understand how different feedback structures affect attention dynamics during these shifts.

## Method Summary
The research compares two computational models for attention allocation during dimensional shifts in contextual bandit tasks. The first approach uses reward prediction errors to iteratively update attention weights, while the second (WIBL) calculates mutual information between past experiences and current choices. Both methods were evaluated across three feedback conditions: immediate feedback (standard RL setting), delayed feedback (reward arrives after a delay), and counterfactual feedback (information about unchosen options). The study used simulations to compare how each method performs during intra-dimensional shifts (when the relevant dimension remains the same) versus extra-dimensional shifts (when the relevant dimension changes).

## Key Results
- WIBL method shows worse initial performance but superior asymptotic performance compared to RPE-based approach after extra-dimensional shifts
- WIBL demonstrates more pronounced differences between intra-dimensional and extra-dimensional shifts under delayed and counterfactual feedback conditions
- Information-theoretic metrics (WIBL) may better predict human attention patterns than reward prediction errors in decision-making tasks

## Why This Works (Mechanism)
The mutual information-based approach (WIBL) better captures attention dynamics because it considers the statistical relationships between past experiences and current choices, rather than relying solely on reward prediction errors. This allows the model to more effectively detect when dimensional shifts occur and adapt attention allocation accordingly. The method's sensitivity to the informational content of past experiences, rather than just reward signals, enables more nuanced attention shifts that better match human behavior patterns during complex decision-making scenarios.

## Foundational Learning
- **Dimensional Shifts**: Changes in which task dimensions are relevant for optimal decision-making - needed to understand when attention must be reallocated
- **Contextual Bandits**: Sequential decision-making problems where actions are taken based on contextual information - needed as the task framework
- **Reward Prediction Errors**: The difference between expected and actual rewards used to update value estimates - needed to understand the baseline comparison method
- **Mutual Information**: Statistical measure of dependence between variables - needed to understand the WIBL approach
- **Counterfactual Feedback**: Information about outcomes that would have occurred from unchosen options - needed to understand the feedback conditions tested
- **Attention Allocation**: The process of selectively focusing cognitive resources on relevant information - needed as the core phenomenon being modeled

## Architecture Onboarding
**Component Map**: Contextual Input -> Attention Module (RPE or WIBL) -> Action Selection -> Feedback Processing -> Attention Update

**Critical Path**: The attention module receives contextual information, generates attention weights, influences action selection, receives feedback, and updates attention weights for the next trial.

**Design Tradeoffs**: RPE-based approach offers computational simplicity and faster initial learning but struggles with dimensional shifts. WIBL provides better asymptotic performance and shift detection but requires maintaining and processing historical experience data, increasing computational overhead.

**Failure Signatures**: RPE-based models show persistent poor performance after dimensional shifts due to continued weight updates based on outdated reward expectations. WIBL may initially underperform due to the time required to accumulate sufficient experience for accurate mutual information calculations.

**3 First Experiments**: 1) Compare attention weight trajectories during dimensional shifts between methods, 2) Test sensitivity to varying delay lengths in delayed feedback condition, 3) Evaluate performance when dimensionality of the task increases.

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies entirely on simulated human behavior without empirical validation
- WIBL's memory requirements may not scale efficiently to real-world applications
- Results limited to specific feedback structures within contextual bandit framework

## Confidence
- **High confidence**: Technical implementation and performance differences between RPE-based and WIBL methods in simulation
- **Medium confidence**: Predicted superiority of information-theoretic metrics for modeling attention patterns
- **Low confidence**: Direct claims about human attention behavior without empirical validation

## Next Checks
1. Conduct human behavioral experiments using the same task paradigm to empirically test whether participants show the predicted patterns of initial performance drops and asymptotic improvements under extra-dimensional shifts.

2. Implement the WIBL algorithm in larger-scale reinforcement learning problems to evaluate computational efficiency and scalability beyond the contextual bandit framework.

3. Compare model predictions against human data across different feedback delays and counterfactual scenarios to determine which method more accurately captures human attention dynamics in practice.