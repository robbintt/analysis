---
ver: rpa2
title: Extracting Information from Scientific Literature via Visual Table Question
  Answering Models
arxiv_id: '2508.18661'
source_url: https://arxiv.org/abs/2508.18661
tags:
- table
- document
- information
- documents
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explored three approaches for processing table data
  in scientific papers to improve extractive question answering for systematic reviews.
  The methods evaluated were: (1) Optical Character Recognition (OCR) for text extraction,
  (2) Pre-trained vision-language models for document visual question answering, and
  (3) Table detection and structure recognition models to extract and merge table
  content with surrounding text.'
---

# Extracting Information from Scientific Literature via Visual Table Question Answering Models

## Quick Facts
- arXiv ID: 2508.18661
- Source URL: https://arxiv.org/abs/2508.18661
- Reference count: 10
- This study evaluated three approaches for extracting table data in scientific papers to improve extractive QA for systematic reviews.

## Executive Summary
This paper explores three approaches for processing table data in scientific papers to improve extractive question answering for systematic reviews. The methods evaluated were: (1) Optical Character Recognition (OCR) for text extraction, (2) Pre-trained vision-language models for document visual question answering, and (3) Table detection and structure recognition models to extract and merge table content with surrounding text. Using ten RF-EMF-related documents and seven predefined question-answer pairs, the results showed that approaches preserving table structure outperformed others in representing and organizing table content. Accurately recognizing specific notations and symbols emerged as critical for improved results.

## Method Summary
The study compares three approaches for extracting information from tables in scientific documents. The first uses OCR (Tesseract and Pix2tex) to extract text, converting it to pseudo-text or LaTeX format before feeding it to GPT-4 for QA. The second employs pre-trained end-to-end vision-language models (Donut, Pix2Struct, and GPT-4 Vision) that directly process document images. The third approach uses a table structure-aware pipeline with the PubTables-1M Table Transformer for detection and structure recognition, followed by cell-level OCR and GPT-4 QA. The evaluation uses 10 RF-EMF scientific documents with 7 predefined QA pairs covering experiment details, SAR values, frequencies, and temperature controls.

## Key Results
- Approaches preserving table structure outperformed others in representing and organizing table content
- Accurately recognizing specific notations and symbols within documents emerged as a critical factor for improved results
- GPT-4 demonstrated the highest scores in sample test cases among end-to-end models
- OCR-based approaches were affected by complexity of documents containing specific symbols like exponential and temperature symbols

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preserving table structure improves extractive QA accuracy for scientific documents
- Mechanism: Structure-aware representations maintain row/column/cell relationships that encode semantic meaning, enabling models to retrieve correct cells rather than fragmented text
- Core assumption: Tables in scientific documents follow conventions where spatial relationships carry semantic weight
- Evidence anchors:
  - "approaches preserving table structure outperform the others, particularly in representing and organizing table content"
  - "pseudo-text outperformed expectations in understanding structured data, which resulted from including escape characters and mathematical symbols within the text such as '\t', '\n'"
  - Related work on layout-enhanced LVLMs confirms layout modality improves TQA on structured reports

### Mechanism 2
- Claim: OCR quality on scientific notation directly bounds downstream QA performance
- Mechanism: OCR errors on temperature symbols (°), exponents, and units cascade into the QA module, which cannot recover missing or corrupted symbols without external knowledge
- Core assumption: Scientific symbols are low-frequency tokens that general OCR models under-represent
- Evidence anchors:
  - "Accurately recognizing specific notations and symbols within the documents emerged as a critical factor for improved results"
  - "OCR-based approaches are affected by the complexity of documents containing specific symbols... struggle to represent exponential symbols, temperature symbols, and structural information"
  - Weak direct evidence on symbol-specific OCR failure rates in corpus

### Mechanism 3
- Claim: End-to-end vision-language models outperform OCR pipelines when pre-training covers document layouts
- Mechanism: Models like GPT-4 jointly encode visual layout and textual semantics, bypassing OCR error propagation; however, performance degrades with low-quality inputs
- Core assumption: Pre-training data includes sufficient scientific document diversity
- Evidence anchors:
  - "GPT-4 demonstrates the highest scores in our sample test cases... slight challenges remain where the model generated incorrect answers in Case 1-b... performance is still susceptible to degradation due to the quality of input images"
  - "Donut and Pix2Struct models perform poorly... require fine-tuning to understand the specific terms"
  - Multimodal LLMs on chemical tables show similar variability based on pre-training coverage

## Foundational Learning

- Concept: Document Visual Question Answering (DocQA)
  - Why needed here: This paper extends VQA to documents where layout, not just pixels, carries meaning
  - Quick check question: Can you explain why OCR-free models (Donut) might fail where OCR-based pipelines succeed?

- Concept: Table Structure Recognition (TSR)
  - Why needed here: TSR identifies row/column hierarchy; without it, QA models treat tables as unstructured text
  - Quick check question: What distinguishes a "trivial" vs. "non-trivial" table layout in this paper's taxonomy?

- Concept: Error Propagation in Multi-stage Pipelines
  - Why needed here: Table structure-aware approach chains TSR → OCR → QA; each stage compounds uncertainty
  - Quick check question: If TSR misses a merged cell, how does that affect downstream QA on that row?

## Architecture Onboarding

- Component map: Document image → Table Transformer (detection + structure) → Cell-level OCR → GPT-4 QA
- Critical path: Image quality → OCR/TSR accuracy → Structured representation → QA response correctness
- Design tradeoffs:
  - OCR-based: Interpretable intermediate output but brittle on symbols
  - End-to-end: Simpler pipeline but requires domain fine-tuning; GPT-4 performs best but is closed-source
  - Structure-aware: Preserves table semantics but fails on non-trivial layouts and dense cells
- Failure signatures:
  - Case 1-b errors (table-only answers): Model extracts from surrounding text instead of table cells
  - Hallucinated symbols: OCR misreads exponents; QA generates plausible but incorrect units
  - Dense structure failures: Cell borders interfere with OCR; merged cells break row/column assumptions
- First 3 experiments:
  1. Baseline OCR quality test: Run Tesseract vs. Pix2tex on 5 sample tables; measure symbol error rate on °, W/kg, Hz notation
  2. End-to-end model comparison: Evaluate Donut, Pix2Struct, GPT-4 on the 7 predefined QA pairs; record Case 1-b accuracy
  3. Structure-aware pipeline stress test: Apply Table Transformer to tables with merged cells; measure cell-level OCR accuracy vs. full-document OCR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can table structure recognition models be improved to reliably handle non-trivial layouts with merged cells and irregular row/column structures in scientific documents?
- Basis in paper: The authors explicitly note that their table structure-aware approach faced challenges with "non-trivial tabular layout" where merged cells disrupted standard extraction, and that specialized algorithms are needed for irregular structures.
- Why unresolved: Current transformer-based table detection models assume regular grid structures; merged or spanned cells break row/column alignment assumptions, causing extraction failures in multi-stage pipelines.
- What evidence would resolve it: Development of a model that achieves >90% cell-level F1 score on tables with merged cells, validated on a benchmark dataset containing diverse irregular layouts from scientific literature.

### Open Question 2
- Question: Would fine-tuning Donut and Pix2Struct on domain-specific scientific documents (RF-EMF literature) significantly improve their extractive QA performance to match or exceed GPT-4?
- Basis in paper: The authors state these models "require fine-tuning to address the diverse and intricate nature of scientific tables" and identify this as a primary future research direction.
- Why unresolved: Pre-trained models were tested without domain adaptation; it remains unknown whether their architectural limitations or simply lack of scientific domain knowledge caused poor performance.
- What evidence would resolve it: A comparative study showing fine-tuned Donut/Pix2Struct performance metrics (exact match accuracy, F1) against zero-shot GPT-4 on the same RF-EMF document test set.

### Open Question 3
- Question: Can integrating OCR bounding box location data into the question-answering pipeline improve structural understanding and extraction accuracy for densely packed tables?
- Basis in paper: The authors propose that "integrating OCR location data and refining the processing stages in table recognition could significantly enhance the accuracy and reliability of extractive question-answering tasks."
- Why unresolved: Current approaches treat OCR output as pure text, discarding spatial coordinates that could help distinguish adjacent cells and maintain table structure in dense content.
- What evidence would resolve it: An ablation study comparing QA accuracy with and without spatial OCR features on tables with high cell density (>15 cells per square inch).

## Limitations

- Small test corpus of only 10 documents and 7 question-answer pairs may not capture full variability of scientific table structures
- Evaluation focuses specifically on RF-EMF literature, raising questions about generalizability to other scientific domains
- Paper does not report statistical significance testing or confidence intervals for performance differences between approaches

## Confidence

- High confidence: OCR quality directly impacts QA performance for scientific symbols, supported by clear failure examples
- Medium confidence: Ranking of approaches (OCR-based > End-to-end > Structure-aware) due to limited test cases and potential overfitting to specific document domain
- Low confidence: Specific claim that "structure-aware representations maintain semantic relationships" - while paper shows improved performance, mechanism is not explicitly validated through ablation studies

## Next Checks

1. Replicate the study with a larger, diverse corpus of 50+ scientific documents across multiple domains (biomedical, physics, chemistry) to test generalizability
2. Conduct ablation studies where table structure elements (headers, cell boundaries, merged cells) are systematically removed to quantify their individual contributions to QA accuracy
3. Implement domain-specific OCR fine-tuning for scientific notation and compare performance against general-purpose OCR models on symbol recognition tasks