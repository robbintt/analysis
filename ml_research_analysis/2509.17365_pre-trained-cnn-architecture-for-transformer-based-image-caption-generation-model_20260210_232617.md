---
ver: rpa2
title: Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation
  Model
arxiv_id: '2509.17365'
source_url: https://arxiv.org/abs/2509.17365
tags:
- image
- transformer
- caption
- captioning
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating descriptive captions
  for images, a task that combines computer vision and natural language processing.
  The study highlights limitations of traditional RNN-based models, such as slow training
  and poor handling of long sequences, and proposes using Transformer-based models
  with self-attention mechanisms for more efficient parallelization.
---

# Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model

## Quick Facts
- arXiv ID: 2509.17365
- Source URL: https://arxiv.org/abs/2509.17365
- Reference count: 21
- Primary result: Transformer-based image captioning model using pre-trained EfficientNetB0 achieves moderate BLEU scores on Flickr30k dataset

## Executive Summary
This paper addresses image captioning by proposing a Transformer-based architecture that leverages pre-trained EfficientNetB0 for feature extraction. The study identifies limitations of traditional RNN-based models, particularly slow training and poor handling of long sequences, and demonstrates how Transformers with self-attention mechanisms enable efficient parallelization while capturing both short- and long-range dependencies. Experiments on Flickr30k show that the model produces grammatically valid captions with moderate BLEU scores, validating the effectiveness of the proposed architecture.

## Method Summary
The methodology combines a pre-trained EfficientNetB0 CNN (ImageNet weights, classification head removed) for image feature extraction with a Transformer encoder-decoder architecture for caption generation. The model processes 299×299 RGB images through EfficientNetB0 to obtain visual features, which are then passed through a Transformer encoder with 1 multi-head attention layer and a decoder with 2 multi-head attention layers. Training uses the Adam optimizer with categorical cross-entropy loss, batch size 128, and early stopping based on BLEU-4 validation performance. The Flickr30k dataset provides the training corpus with 20,915 training images, 5,124 validation images, and 105 test images.

## Key Results
- Transformer-based model achieves moderate BLEU scores (1-4) on Flickr30k dataset
- Validation accuracy improves consistently over training epochs with early stopping
- Model demonstrates effective parallelization compared to sequential RNN approaches
- BLEU-1 and BLEU-2 scores indicate vocabulary coverage or feature alignment challenges

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained CNNs provide transferable visual features that reduce training requirements for image captioning. EfficientNetB0 encodes input images (299×299) into vector representations; the final softmax layer is removed since the task is feature extraction rather than classification. The pre-trained weights provide initial visual representations that the Transformer decoder conditions on for caption generation. Core assumption: Features learned during ImageNet-scale pre-training transfer effectively to the captioning domain without extensive fine-tuning.

### Mechanism 2
Transformer self-attention enables parallel processing and improved long-range dependency modeling compared to sequential RNNs. Self-attention computes relationships across all positions simultaneously rather than sequentially, allowing parallel training. Positional encodings are added to preserve sequence order information. The encoder uses 1 multi-head attention layer; the decoder uses 2 multi-head attention layers with layer normalization. Core assumption: Parallelization and global attention provide sufficient inductive bias for learning image-text alignments without recurrence.

### Mechanism 3
Multi-head attention captures diverse feature relationships that single attention heads cannot. Multiple attention heads operate in parallel, each potentially learning different aspects of image-text correspondence. The decoder's 2 attention heads attend to encoder outputs and previously generated tokens. Core assumption: Single attention-weighted values are insufficient; diversity of attention patterns is necessary for quality captions.

## Foundational Learning

- **Concept: Encoder-Decoder Architecture**
  - Why needed here: Understand the separation between visual feature extraction (encoder) and sequence generation (decoder)
  - Quick check question: Why does the decoder receive captions during training but not during inference?

- **Concept: Self-Attention and Positional Encoding**
  - Why needed here: Grasp how transformers replace recurrence with attention and maintain sequence order
  - Quick check question: What would happen if positional encodings were removed?

- **Concept: Transfer Learning with Pre-trained CNNs**
  - Why needed here: Recognize why pre-trained weights help and when they might not
  - Quick check question: Why is the final softmax layer removed from EfficientNetB0?

## Architecture Onboarding

- **Component map**: Input (299×299 RGB images) -> EfficientNetB0 (pre-trained, classification head removed) -> Transformer Encoder (1 multi-head attention + layer normalization) -> Transformer Decoder (2 multi-head attention layers + 3 normalization layers) -> Caption tokens

- **Critical path**: 1. Preprocess images (decode, resize to 299×299) 2. Extract features via EfficientNetB0 3. Add positional encodings to feature sequence 4. Pass through Transformer encoder 5. Feed caption tokens to decoder with masking 6. Compute next-token prediction via softmax

- **Design tradeoffs**: EfficientNetB0 vs. larger variants (B1–B7): simplicity and speed vs. potential accuracy gains; Single encoder attention layer vs. deeper stacks: computational efficiency vs. representation capacity; 24-token max sequence length: sufficient for Flickr30k captions but may truncate longer descriptions

- **Failure signatures**: BLEU-1/BLEU-2 consistently low (paper reports these as "not very good"): vocabulary coverage or feature alignment issues; Validation loss diverges from training loss: overfitting risk on small dataset; Repetitive or incoherent captions: decoder failing to learn proper sequential dependencies

- **First 3 experiments**: 1. Baseline replication: Implement EfficientNetB0 + Transformer with paper-specified hyperparameters on Flickr30k; report BLEU-1 through BLEU-4 and training time per epoch. 2. Encoder ablation: Replace EfficientNetB0 with ResNet-50 or VGG-16, keeping all else constant; compare BLEU scores and feature extraction time. 3. Attention head sensitivity: Vary encoder attention heads (1 vs. 2 vs. 4) to test the multi-head benefit claim; monitor for redundancy via attention visualization.

## Open Questions the Paper Calls Out

### Open Question 1
How does the model's performance scale when substituting EfficientNetB0 with higher-capacity variants (e.g., EfficientNetB4 to B7)? The study restricted its experimentation to the B0 variant to balance simplicity and performance, leaving comparative analysis with B4 or B7 on the same dataset unexplored.

### Open Question 2
Does the proposed architecture maintain its efficiency and accuracy when applied to larger, more complex datasets like MS COCO? The current validation is limited to the Flickr30k dataset, leaving scalability to larger corpora untested.

### Open Question 3
What linguistic or structural factors caused higher-order n-gram scores (BLEU-3, BLEU-4) to exceed lower-order scores (BLEU-1, BLEU-2) in the evaluation? The paper reports the BLEU scores but does not analyze the textual content or failure modes leading to this specific scoring distribution.

## Limitations

- Several key hyperparameters remain unspecified, including attention head count, feed-forward dimension, dropout rates, and learning rate schedule
- Study uses Flickr30k, a small dataset compared to contemporary alternatives, raising questions about model generalization
- Evaluation relies primarily on BLEU scores without comparison to modern captioning metrics (METEOR, CIDEr, SPICE)

## Confidence

**High confidence**: Transformer parallelization and self-attention mechanisms work as described, given established literature on transformer architectures and clear implementation of positional encodings.

**Medium confidence**: Pre-trained EfficientNetB0 provides beneficial visual features, supported by general success of transfer learning but lacking specific ablation studies in this work.

**Low confidence**: Multi-head attention provides significant benefits over single attention, as this claim lacks direct experimental evidence or ablation studies within the paper.

## Next Checks

1. **Encoder ablation study**: Replace EfficientNetB0 with ResNet-50 and a randomly initialized CNN, keeping all other parameters constant, to quantify the exact contribution of pre-training to caption quality and training efficiency.

2. **Attention head sensitivity analysis**: Systematically vary the number of attention heads in both encoder and decoder (1, 2, 4, 8) while monitoring BLEU scores, training convergence speed, and attention head redundancy through visualization.

3. **Cross-dataset generalization test**: Train the same architecture on MS-COCO and compare performance, training dynamics, and BLEU scores to Flickr30k results to assess scalability and dataset dependency.