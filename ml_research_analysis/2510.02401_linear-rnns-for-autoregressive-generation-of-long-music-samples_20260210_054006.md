---
ver: rpa2
title: Linear RNNs for autoregressive generation of long music samples
arxiv_id: '2510.02401'
source_url: https://arxiv.org/abs/2510.02401
tags:
- linear
- state
- training
- learning
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HarmonicRNN, a linear RNN architecture with pooling layers, achieves
  state-of-the-art performance on raw audio generation tasks, including the SC09,
  Beethoven, and YouTubeMix datasets. By using complex gated linear recurrent units
  (CG-LRU) and multi-host context parallelism, HarmonicRNN can train on sequences
  up to 1 million tokens (1 minute of audio at 16 kHz).
---

# Linear RNNs for autoregressive generation of long music samples

## Quick Facts
- **arXiv ID**: 2510.02401
- **Source URL**: https://arxiv.org/abs/2510.02401
- **Reference count**: 1
- **Primary result**: HarmonicRNN achieves state-of-the-art performance on raw audio generation tasks, training on sequences up to 1 million tokens with superior log-likelihood and perceptual metrics.

## Executive Summary
HarmonicRNN is a linear RNN architecture with pooling layers that achieves state-of-the-art performance on raw audio generation tasks including SC09, Beethoven, and YouTubeMix datasets. By using complex gated linear recurrent units (CG-LRU) and multi-host context parallelism, the model can train on sequences up to 1 million tokens (1 minute of audio at 16 kHz). The architecture attains superior log-likelihood and perceptual metrics (FID, IS) compared to prior methods like SaShiMi and Mamba. Key design choices include sinusoidal embeddings for faster convergence and grouped convolutions for stable pooling.

## Method Summary
HarmonicRNN uses a hierarchical U-Net architecture with down-pooling and up-pooling layers to generate coherent long-term structure in audio samples. The core component is the Complex Gated Linear Recurrent Unit (CG-LRU) that enables efficient parallel training on extremely long sequences through associative scan algorithms. The model employs non-learned sinusoidal embeddings for quantized audio inputs and uses grouped convolutions in pooling layers for stability. Training is performed on 8-bit quantized audio using μ-law encoding, with the model outputting categorical distributions over 256 tokens.

## Key Results
- HarmonicRNN achieves state-of-the-art log-likelihood and perceptual metrics (FID, IS) on SC09, Beethoven, and YouTubeMix datasets
- The model can train on sequences up to 1 million tokens (1 minute of audio at 16 kHz) using context-parallelism
- Hierarchical pooling improves perceptual quality significantly while maintaining similar NLL performance
- Sinusoidal embeddings accelerate convergence compared to learned embeddings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Linear recurrent units (CG-LRU) enable efficient parallel training on extremely long sequences (up to 1M tokens) while maintaining O(T) inference complexity.
- **Mechanism:** The recurrence relation $h_t = a(u_t; \theta) \odot h_{t-1} + b(u_t; \theta)$ allows the temporal dimension to be parallelized using an associative scan algorithm during training, bypassing the sequential bottleneck of traditional RNNs.
- **Core assumption:** The linear recurrence approximation is sufficient to model the complex temporal dependencies of raw audio waveforms without the non-linearities found in LSTMs/GRUs.
- **Evidence anchors:** [Abstract] mentions context-parallelism for 1M token training; [Section 2] defines the linear recurrence form and cites Blelloch (1991); [Corpus] supports linear recurrences for long-range tasks.

### Mechanism 2
- **Claim:** Hierarchical down-pooling and up-pooling are required to generate coherent long-term structure (measured by FID/IS), beyond what a flat sequence model achieves.
- **Mechanism:** Down-pooling (strided convolutions) reduces sequence length to allow the RNN to process "compressed" time windows, effectively increasing the receptive field for global structure, while up-pooling reconstructs the high-frequency waveform.
- **Core assumption:** Important acoustic features exist on multiple timescales, and compressing the sequence allows the model to "see" further into the past with fixed compute.
- **Evidence anchors:** [Section 1] states pooling is necessary for coherent samples; [Section 4.4] Table 2 shows pooling removal worsens FID from 0.46 to 2.95 and IS from 6.46 to 3.33.

### Mechanism 3
- **Claim:** Non-learned sinusoidal embeddings accelerate convergence and improve final performance compared to learned lookup tables for quantized audio.
- **Mechanism:** Mapping input integers to sinusoidal continuous vectors provides a structured, deterministic representation that is easier for the network to decode than arbitrary one-hot or learned vector indices.
- **Core assumption:** The temporal structure of audio benefits more from the smooth inductive bias of sinusoidal encodings than the flexibility of learned parameters.
- **Evidence anchors:** [Section 2] found faster training with non-learned embeddings; [Section 4.3] Figure 2 shows sinusoidal embeddings outperforming learned embeddings.

## Foundational Learning

- **Concept: Associative Scan (Parallel Prefix Sum)**
  - **Why needed here:** Understanding how the paper trains on 1M tokens requires knowing that linear recurrences can be unrolled in parallel, rather than step-by-step.
  - **Quick check question:** Explain why a standard RNN cannot be parallelized over the time dimension during training, but a linear RNN can.

- **Concept: Hierarchical U-Net Architecture**
  - **Why needed here:** The HarmonicRNN is not a flat stack; it relies on a U-shaped structure (Down-Pool → Bottleneck → Up-Pool) to balance local fidelity and global coherence.
  - **Quick check question:** Why does reducing the sequence length via pooling help a recurrent model learn long-range dependencies?

- **Concept: μ-law Encoding**
  - **Why needed here:** The paper quantizes 16-bit audio to 8-bit using μ-law. Without this, modeling raw 16-bit values would require massive output vocabularies (65,536 classes).
  - **Quick check question:** How does μ-law companding change the distribution of audio signal values compared to linear quantization?

## Architecture Onboarding

- **Component map:** Input (8-bit quantized audio) → Sinusoidal Embedding → Temporal Blocks (CG-LRU + Norm + Linear + GeLU) → Down-Pool (Strided Conv, factors [2,4,4,5]) → Bottleneck → Up-Pool (Dilated Conv) → Output (Categorical over 256 tokens)

- **Critical path:** The pooling layer configuration (specifically using `groups > 1` in convolutions) is cited as critical for stability (Table 1), and the CG-LRU kernels are required for context parallelism on TPUs.

- **Design tradeoffs:**
  - Pooling vs. Flat: Pooling drastically improves perceptual metrics (FID/IS) and speed, though flat models may capture marginally better NLL (Table 2).
  - Embedding: Sinusoidal embeddings trade parameter flexibility for faster convergence (Figure 2).
  - Conv Groups: Dense pooling (1 group) increases parameter count and instability; grouped convolutions (128 groups) are more efficient and stable (Table 1).

- **Failure signatures:**
  - Instability during training: Likely caused by using dense (group=1) pooling layers instead of grouped convolutions.
  - Poor perceptual quality (High FID): Indicates the model has not learned long-range structure; check if pooling layers were removed or sequence length was truncated.
  - Slow training: Check if the associative scan implementation is falling back to sequential execution or if context parallelism is not enabled for 1M token sequences.

- **First 3 experiments:**
  1. **Embedding A/B Test:** Verify Figure 2 by training on SC09 with Linear Scaling vs. Sinusoidal Embeddings (should show divergence in loss curves around epoch 100).
  2. **Ablate Pooling:** Run the baseline vs. "No pooling" on SC09; confirm that NLL remains similar but FID degrades significantly (Table 2).
  3. **Pooling Group Sweep:** Test `groups=1` vs `groups=4` vs `groups=128` on a short run to observe training stability and parameter count changes (Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can HarmonicRNN maintain its efficiency and sample quality when scaled to more complex, polyphonic, or multi-instrumental music generation tasks?
- **Basis in paper:** The Conclusion states the authors look forward to "future work scaling the approach up to more challenging audio generation tasks" beyond the "initial results" presented.
- **Why unresolved:** The experiments were restricted to relatively simple datasets (speech digits and solo piano) and small parameter counts (approx. 7M).
- **What evidence would resolve it:** Evaluation of larger HarmonicRNN models on diverse, high-fidelity datasets (e.g., full orchestral or multi-track pop music).

### Open Question 2
- **Question:** Why does the hierarchical pooling structure improve perceptual metrics so drastically while having negligible impact on the negative log-likelihood (NLL)?
- **Basis in paper:** Table 2 shows that removing pooling degrades the Inception Score (IS) from 6.46 to 3.33, yet the test NLL remains nearly identical (1.854 vs 1.852).
- **Why unresolved:** The paper demonstrates the necessity of pooling for quality but does not investigate the divergence between the likelihood objective and the perceptual metrics.
- **What evidence would resolve it:** A study of the latent space structure or feature distributions in pooled vs. non-pooled models to identify what acoustic features the likelihood misses.

### Open Question 3
- **Question:** Do sinusoidal input embeddings retain their advantage over learned embeddings as model capacity and dataset complexity increase?
- **Basis in paper:** Section 4.3 and Figure 2 show sinusoidal embeddings outperform learned embeddings, but this is tested only on the SC09 dataset (speech digits) with a small model.
- **Why unresolved:** The finding contradicts the common transformer practice of using learned embeddings; it is unclear if this result generalizes to the "more challenging" tasks proposed in the conclusion.
- **What evidence would resolve it:** Re-running the ablation from Section 4.3 on the larger YouTubeMix or Beethoven datasets with increased model width.

## Limitations
- Evaluation relies heavily on perceptual metrics (FID, IS) that correlate imperfectly with human judgments of audio quality
- Performance demonstrated only on three relatively simple music datasets (speech digits, solo piano)
- Architectural improvements trade off some log-likelihood accuracy for perceptual quality, suggesting domain-specific optimization

## Confidence
**High confidence:** The linear recurrence parallelization mechanism is well-supported by theoretical foundations and explicit implementation details. The associative scan algorithm and context parallelism claims are directly verifiable.

**Medium confidence:** The hierarchical pooling mechanism shows strong empirical support through ablation studies, but limited external validation exists for this specific architectural choice in audio generation.

**Medium confidence:** The sinusoidal embedding claims are supported by training curves, but the corpus provides weak external validation and the advantage over learned embeddings may not generalize to more complex tasks.

## Next Checks
1. **Human Perceptual Validation:** Conduct blind listening tests comparing HarmonicRNN outputs against SaShiMi and Mamba baselines using standardized MOS ratings to verify automated metrics correlate with human judgments.

2. **Cross-Domain Generalization:** Evaluate HarmonicRNN on non-music audio domains including speech synthesis (LJ Speech) and environmental sounds (ESC-50) to assess whether architectural advantages transfer beyond music datasets.

3. **Pooling Sensitivity Analysis:** Systematically vary pooling factors and group configurations beyond tested ranges to establish optimal parameters and identify potential failure modes in the architectural design.