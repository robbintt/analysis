---
ver: rpa2
title: 'Human Cognitive Biases in Explanation-Based Interaction: The Case of Within
  and Between Session Order Effect'
arxiv_id: '2512.04764'
source_url: https://arxiv.org/abs/2512.04764
tags:
- order
- participants
- images
- accuracy
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates order effects in explanatory interactive
  learning (XIL), where users debug AI models by correcting their explanations. Two
  user studies (n=713) tested within- and between-session order effects by manipulating
  the sequence of correct and incorrect model explanations.
---

# Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect

## Quick Facts
- **arXiv ID:** 2512.04764
- **Source URL:** https://arxiv.org/abs/2512.04764
- **Reference count:** 29
- **Primary result:** Order effects have limited impact on users' agreement with the model and feedback quality in explanatory interactive learning.

## Executive Summary
This study investigates order effects (primacy and recency) in Explanatory Interactive Learning (XIL), where users debug AI models by correcting their explanations. Two user studies (n=713) tested within- and between-session order effects by manipulating the sequence of correct and incorrect model explanations. Results showed that order effects had limited impact on users' agreement with the model and feedback quality. Within sessions, a small primacy effect was observed for difficult images, with early exposure to errors reducing reliance on the model. Between sessions, no significant order effects were found. Overall, order effects do not pose a significant barrier to successful XIL implementation.

## Method Summary
The study used a face detection task with blurred images and bounding box explanations. Participants (n=359 for Experiment 1, n=354 for Experiment 2) corrected model explanations within a 6-second time limit per image. Experiment 1 manipulated order within single sessions (increasing, constant, or decreasing accuracy), while Experiment 2 used two sessions with an explicit model update notification between them. Analysis employed mixed linear models with random intercepts for participants and images.

## Key Results
- Within sessions, a small primacy effect was observed for difficult images, with early exposure to errors reducing reliance on the model.
- Between sessions, no significant order effects were found when users were informed of a model update.
- The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments.

## Why This Works (Mechanism)

### Mechanism 1: Expectation Reset Between Sessions
When users are explicitly informed that an AI model has been updated between debugging sessions, their expectations and trust calibration reset, mitigating order effects from prior interactions. The explicit notification of a model update acts as a cognitive "boundary," causing users to treat the new session as a fresh evaluation rather than a continuation of prior judgments.

### Mechanism 2: Primacy Effect in Ambiguous Within-Session Stimuli
Early exposure to model errors within a single session reduces user reliance on the model's explanations, particularly for ambiguous or difficult cases. When incorrect model outputs are encountered early, users form a lower initial trust estimate that translates into less agreement with the model's incorrect placements for ambiguous stimuli.

### Mechanism 3: Behavioral Feedback Quality is Robust to Order
The quality of user corrective feedback is largely robust to order effects, both within and between sessions. Feedback quality is primarily driven by perceptual task demands and the correctness of the model's explanation, not by presentation sequence.

## Foundational Learning

- **Concept:** Order Effects (Primacy vs. Recency)
  - **Why needed here:** Understanding how sequence influences trust and reliance is critical for designing XIL presentation strategies and anticipating when user calibration may be biased.
  - **Quick check question:** If a debugging session starts with several correct model explanations, what is the likely impact on user reliance for subsequent ambiguous cases?

- **Concept:** Explanatory Interactive Learning (XIL) Framework
  - **Why needed here:** XIL is the core paradigm being studied; onboarding requires grasping the loop of explanation presentation, user feedback, and model update.
  - **Quick check question:** What is the primary role of the user in the XIL loop?

- **Concept:** Behavioral vs. Self-Report Measures of Trust
  - **Why needed here:** The paper relies on behavioral agreement as a trust proxy; self-report questionnaires showed no effects, underscoring the importance of measure selection.
  - **Quick check question:** Why might self-reported trust fail to capture subtle biases that behavioral agreement reveals?

## Architecture Onboarding

- **Component map:**
  - Item Selector -> Explanation Generator -> User Interface -> Feedback Processor -> Trust/Perception Questionnaire

- **Critical path:**
  1. Item Selector determines sequence based on experimental condition
  2. Explanation Generator produces bounding box for each image
  3. User Interface presents image + box; user provides feedback within time limit
  4. Feedback Processor computes accuracy and agreement
  5. After session(s), Trust Questionnaire is administered

- **Design tradeoffs:**
  - Task Difficulty vs. Sensitivity: Increasing blur and difficulty makes the task more sensitive to order effects but risks higher exclusion rates if too hard
  - Within- vs. Between-Session Focus: Randomizing item order within sessions mitigates primacy effects but does not address between-session anchoring without explicit update notifications
  - Behavioral vs. Self-Report: Behavioral measures are more sensitive to order effects but harder to collect; self-reports are easy but may miss subtle biases

- **Failure signatures:**
  - High exclusion rates (>10%) suggest task is too difficult or time limit is too short
  - No difference in agreement across conditions despite manipulated accuracy suggests the manipulation failed or stimuli are too easy
  - Significant order effects in self-report but not behavior may indicate demand characteristics or social desirability bias

- **First 3 experiments:**
  1. Replicate Experiment 1 with varied time limits (e.g., 4s, 6s, 8s) to test whether reduced cognitive load attenuates the primacy effect for difficult images
  2. Extend to a text classification XIL task (e.g., sentiment analysis with highlight explanations) to test generalizability across modalities and explanation types
  3. Introduce a "no-update notification" control in Experiment 2 to verify whether the between-session order effect is truly mitigated by the update notification mechanism

## Open Questions the Paper Calls Out
- Do order effects influence user feedback quality and trust in Explanatory Interactive Learning (XIL) systems when applied to non-visual tasks (e.g., text classification) or when utilizing concept-level explanations rather than visual bounding boxes?
- Does the sequence in which items of varying difficulty are presented (e.g., hard-to-easy vs. easy-to-hard) modulate user confidence and feedback accuracy in XIL debugging sessions?
- Are there other cognitive biases distinct from order effects that significantly impede the reliability of user feedback during explanatory interactive learning?

## Limitations
- Findings are limited to a specific XIL paradigm (bounding-box correction for facial recognition on blurred images) and may not generalize to other tasks or explanation modalities.
- The "reset" mechanism in between-session order effects relies on users noticing and believing the update notification; if this assumption fails, the observed null effect may not replicate.
- The difficulty stratification process used to select stimuli is a potential source of variability, as the exact thresholds are not reported.

## Confidence
- **Within-Session Order Effects:** Medium confidence - evidence supports a small primacy effect for difficult images, but the effect size is modest and driven by a specific subset of stimuli.
- **Between-Session Order Effects:** High confidence - evidence strongly supports the absence of significant order effects between sessions, contingent on the update notification being perceived as meaningful.
- **Behavioral vs. Self-Report Measures:** High confidence - the paper clearly demonstrates that behavioral agreement is more sensitive to order effects than self-reported trust.

## Next Checks
1. Conduct a similar order effects study in a text-based XIL task (e.g., sentiment analysis with highlighted rationales) to test whether the primacy effect for difficult stimuli generalizes across modalities.
2. In Experiment 2, include a control condition where users are *not* informed of a model update between sessions to verify whether the between-session reset is truly driven by the notification mechanism.
3. Vary the time limit per image (e.g., 4s, 6s, 8s) to test whether reducing cognitive load attenuates the primacy effect for difficult images, providing insight into the cognitive mechanisms underlying the observed order effects.