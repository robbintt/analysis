---
ver: rpa2
title: 'Causal discovery for linear causal model with correlated noise: an Adversarial
  Learning Approach'
arxiv_id: '2601.01368'
source_url: https://arxiv.org/abs/2601.01368
tags:
- causal
- structure
- distribution
- data
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fGAN-CD, a novel method for causal structure
  learning in linear systems with unmeasured confounding. The approach reformulates
  structure learning as minimizing Bayesian free energy, which is shown to be equivalent
  to minimizing the KL divergence between the true and model-generated data distributions.
---

# Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach

## Quick Facts
- arXiv ID: 2601.01368
- Source URL: https://arxiv.org/abs/2601.01368
- Reference count: 6
- Method recovers binary causal structure in linear systems with confounding using adversarial optimization

## Executive Summary
This paper introduces fGAN-CD, a novel method for causal structure learning in linear systems with unmeasured confounding. The approach reformulates structure learning as minimizing Bayesian free energy through an adversarial optimization framework. By leveraging the f-GAN framework with Gumbel-Softmax relaxation, the method transforms the discrete graph search problem into a differentiable min-max optimization task. The key innovation is simultaneously learning both the directed acyclic graph structure and confounding correlation structure, addressing a limitation of existing differentiable approaches that struggle with high confounding scenarios.

## Method Summary
fGAN-CD addresses causal structure learning in linear systems with correlated noise by minimizing Bayesian free energy through adversarial optimization. The method parameterizes the causal structure using a Gumbel-Softmax relaxation of the adjacency matrix, enabling gradient-based search in the discrete graph space. The optimization alternates between a discriminator network that distinguishes true data from model-generated data, and a generator that learns the causal structure parameters. The approach jointly learns the DAG structure and confounding correlations, with the adversarial game converging to a solution where the model distribution matches the true data distribution. The method focuses on recovering binary causal structure rather than specific weight parameters, making it suitable for scenarios where confounding effects are significant.

## Key Results
- fGAN-CD achieves lower structural Hamming distance (2.17 vs 3.4) compared to ABIC baseline in 4-node graph with confounding
- Higher arrowhead F1 score (0.667 vs 0.1) demonstrates better recovery of causal directions
- Correctly identifies conditional independence constraints in high-confounding scenarios where ABIC fails

## Why This Works (Mechanism)
The method works by reformulating causal structure learning as an adversarial game where the generator learns to produce data matching the true distribution, while the discriminator tries to distinguish real from generated data. This creates a min-max optimization problem where the generator improves its causal structure estimate until the discriminator can no longer distinguish between real and generated data. The Bayesian free energy minimization objective ensures that the learned structure captures the essential dependencies in the data while accounting for confounding correlations.

## Foundational Learning
- **Linear Causal Models**: Why needed - Provides mathematical framework for modeling cause-effect relationships with additive noise. Quick check - Verify system follows linear structural equation model assumptions.
- **f-GAN Framework**: Why needed - Enables flexible divergence measures for distribution matching in adversarial learning. Quick check - Confirm generator-discriminator game converges to optimal solution.
- **Gumbel-Softmax Relaxation**: Why needed - Makes discrete graph structure selection differentiable for gradient-based optimization. Quick check - Validate temperature scheduling maintains valid DAG constraints.
- **Bayesian Free Energy**: Why needed - Connects causal structure learning to information-theoretic principles. Quick check - Verify KL divergence minimization corresponds to structure learning objective.
- **Confounding Correlation**: Why needed - Accounts for unmeasured common causes affecting observed variables. Quick check - Ensure method can detect and model correlated noise structures.
- **Structural Hamming Distance**: Why needed - Provides quantitative metric for evaluating causal structure recovery accuracy. Quick check - Compare SHD values across different methods and noise levels.

## Architecture Onboarding
**Component Map**: Data -> Generator (Gumbel-Softmax + Structure) <-> Discriminator -> KL Divergence Minimization
**Critical Path**: Structure parameters → Data generation → Discriminator evaluation → Gradient computation → Structure update
**Design Tradeoffs**: Linear assumption enables tractable optimization but limits real-world applicability; Gumbel-Softmax enables differentiability but requires careful temperature scheduling
**Failure Signatures**: Poor convergence with high confounding; inability to recover cyclic structures; sensitivity to initialization
**First Experiments**: 1) Test on synthetic linear systems with known DAG and confounding structure; 2) Vary confounding strength to assess robustness; 3) Compare structural recovery across different noise distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted to linear causal models, limiting applicability to nonlinear real-world systems
- Does not estimate causal effect magnitudes or specific parameter values
- Experimental validation limited to synthetic data without real-world benchmarking
- Only compares against one differentiable baseline (ABIC) without broader method comparison

## Confidence
- High Confidence: Theoretical equivalence between Bayesian free energy minimization and KL divergence is mathematically sound
- Medium Confidence: Synthetic experimental results show superior performance over ABIC, but limited to controlled scenarios
- Low Confidence: Claims about handling high confounding lack real-world validation and broader benchmarking

## Next Checks
1. Evaluate fGAN-CD on real-world datasets with known causal structures to assess practical performance and robustness to linear assumption violations
2. Benchmark against a broader set of causal discovery methods, including nonlinear approaches and constraint-based algorithms
3. Conduct sensitivity analysis by varying noise distributions and non-linear causal relationships to quantify method limitations