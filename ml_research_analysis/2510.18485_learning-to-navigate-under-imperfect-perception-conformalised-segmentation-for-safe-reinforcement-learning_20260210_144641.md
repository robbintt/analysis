---
ver: rpa2
title: 'Learning to Navigate Under Imperfect Perception: Conformalised Segmentation
  for Safe Reinforcement Learning'
arxiv_id: '2510.18485'
source_url: https://arxiv.org/abs/2510.18485
tags:
- hazard
- learning
- coppol
- conformal
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: COPPOL addresses the challenge of safe navigation in safety-critical
  environments where perception errors can lead to catastrophic failures. It introduces
  a conformal prediction-based approach that integrates distribution-free, finite-sample
  safety guarantees into semantic segmentation, producing calibrated hazard maps with
  rigorous bounds on missed detections.
---

# Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2510.18485
- **Source URL:** https://arxiv.org/abs/2510.18485
- **Reference count:** 40
- **Primary result:** COPPOL increases hazard coverage by up to 6× and reduces hazardous violations by up to 50% in safe navigation tasks.

## Executive Summary
COPPOL addresses safe navigation in environments where perception errors can lead to catastrophic failures by integrating conformal prediction into semantic segmentation. The approach produces calibrated hazard maps with rigorous, distribution-free guarantees on missed detections, which are then used to construct risk-aware cost fields for reinforcement learning planning. Evaluated on satellite-derived benchmarks (DeepMoon and YTU-WaterNet), COPPOL demonstrates significant improvements in hazard detection coverage while maintaining robust navigation performance, achieving near-complete detection of unsafe regions and reducing hazardous violations during navigation by up to 50%.

## Method Summary
COPPOL combines conformal risk control with semantic segmentation and reinforcement learning for safe navigation. First, a U-Net is trained to segment hazards from satellite imagery. Then, a conformal calibration step computes a threshold on a held-out set to bound the false negative rate at a user-defined level α. This calibrated threshold is used to binarize segmentation outputs into hazard masks, which are incorporated into the agent's observation and reward function. The RL agent is trained on a grid-world MDP where entering hazardous regions incurs penalties, encouraging conservative navigation that maintains clearance from potential dangers.

## Key Results
- Increases hazard coverage by up to 6× compared to baselines
- Reduces hazardous violations during navigation by up to 50%
- Maintains robustness to distributional shift and state-wise noise
- RL agents spend approximately 50% less time in hazardous areas
- Agents maintain 2-3 times greater distance from unsafe terrain compared to non-conformal baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distribution-free calibration bounds false negative rate, ensuring RL agent perceives superset of true risks.
- **Mechanism:** COPPOL uses Conformal Risk Control (CRC) to compute threshold $\hat{\lambda}$ on calibration set, expanding predicted hazardous region until empirical risk falls below $\alpha$.
- **Core assumption:** Calibration and test data are exchangeable.
- **Evidence anchors:** Abstract states "distribution-free, finite-sample safety guarantees"; Section 4.1 defines threshold search and guarantees $\mathbb{E}[L^{FNR}_{N+1}(\hat{\lambda})] \le \alpha$.
- **Break condition:** Exchangeability violation (domain shift) invalidates coverage guarantee.

### Mechanism 2
- **Claim:** Calibrated uncertainty shapes RL policy gradients to favor conservative clearance without manual reward engineering.
- **Mechanism:** Conformal hazard mask induces reward structure where entering hazardous pixels triggers high penalty, teaching agent to maintain larger distances.
- **Core assumption:** RL agent can converge despite potentially more sparse safe state space.
- **Evidence anchors:** Section 4.2 explains reward structure; results show reduced time in hazardous regions and greater clearance.
- **Break condition:** Excessive false positives from overly conservative threshold trap agent or prevent path finding.

### Mechanism 3
- **Claim:** Expanded spatial buffers provide robustness to observation noise and distributional shift.
- **Mechanism:** Optimizing for coverage over precision creates "safety buffer" around hazards, dampening impact of noisy state estimates.
- **Core assumption:** Safety buffer width exceeds magnitude of state perturbations.
- **Evidence anchors:** Section 5.2 shows COPPOL remains stable under noise while baselines spike in hazard time.
- **Break condition:** Adversarial or systematic noise pushes agent outside pre-computed buffer.

## Foundational Learning

- **Concept:** **Conformal Prediction (CP) & Risk Control**
  - **Why needed here:** Mathematical engine guaranteeing COPPOL isn't "guessing" thresholds but statistically bounding error rate.
  - **Quick check question:** If calibration set has 100 images, does $\alpha=0.1$ ensure zero hazards missed in 101st image?

- **Concept:** **Precision-Recall Trade-off**
  - **Why needed here:** COPPOL deliberately sacrifices precision to maximize coverage; understanding this prevents misdiagnosing "bloated" masks as failure.
  - **Quick check question:** In safety-critical system, is high precision (few false alarms) or high recall (few missed hazards) better?

- **Concept:** **Markov Decision Process (MDP) & Reward Shaping**
  - **Why needed here:** System encodes conformal mask into observation and reward function; understanding reward-signal-to-policy translation is essential.
  - **Quick check question:** How does penalty term in Eq. (8) change gradient update for action moving agent closer to mask boundary?

## Architecture Onboarding

- **Component map:** Sensor Input -> Perception Core (U-Net) -> Raw Probabilities -> Calibration Module (CRC) -> Threshold $\hat{\lambda}$ -> State Constructor (Binarizes) -> Planning Image (Cost Field) -> Policy Optimizer (RL Agent)

- **Critical path:** The **Calibration Module**. Incorrect FNR calculation or unrepresentative calibration set invalidates safety guarantees for downstream RL agent.

- **Design tradeoffs:**
  - **Risk Parameter ($\alpha$):** Lower $\alpha$ = stricter safety = larger masks = potentially blocked paths; higher $\alpha$ = looser safety = smaller masks = higher risk.
  - **State Representation:** Image View (CNN policy, spatial reasoning) vs. Vector View (MLP, faster, simpler but requires hand-crafted features).

- **Failure signatures:**
  - **"Freezing" Agent:** Agent refuses to move; likely $\alpha$ too low causing merged masks that block all paths.
  - **Guarantee Violation:** Agent hits hazard not in mask; check for data leakage or extreme domain shift.
  - **Slow Convergence:** Image View with high-dimensional input without sufficient training steps.

- **First 3 experiments:**
  1. **Calibration Validity Check:** Verify $\hat{\lambda}$ on $D_{cal}$ satisfies $L^{FNR} \le \alpha$ on held-out test set.
  2. **Sweep Risk Levels:** Run COPPOL with $\alpha \in \{0.05, 0.1, 0.2\}$ to visualize task success vs. time in hazard trade-off.
  3. **Noise Injection:** Replicate "State-wise Noisy" experiment by adding Gaussian noise to vector view features to confirm conformal buffer robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can COPPOL be effectively extended to multi-agent reinforcement learning settings with shared or conflicting safety constraints?
- **Basis in paper:** [explicit] Conclusion states future work will "explore extending COPPOL to multi-agent... RL agents."
- **Why unresolved:** Current methodology evaluated on single-agent tasks; multi-agent environments introduce dynamics where one agent's hazard detection may invalidate safety guarantees of another.
- **What evidence would resolve it:** Empirical results from multi-agent navigation benchmark showing bounded conformal coverage and safety violations with simultaneous agent interaction.

### Open Question 2
- **Question:** How can COPPOL be adapted for partially observable environments without global planning image?
- **Basis in paper:** [explicit] Authors identify extending to "partially-observable RL agents" as future work.
- **Why unresolved:** Current architecture assumes global view for risk-aware cost field; robotic agents typically rely on local, egocentric sensors.
- **What evidence would resolve it:** Implementation using memory-based policy (recurrent networks) that accumulates hazard uncertainty over time while demonstrating retained safety guarantees.

### Open Question 3
- **Question:** Can online calibration and adaptive risk estimation be integrated to maintain safety guarantees without fixed pre-deployment calibration set?
- **Basis in paper:** [explicit] Conclusion cites "investigating online calibration and adaptive risk estimation" as necessary for scalability and operational reliability.
- **Why unresolved:** Current approach relies on static calibration dataset to determine threshold $\hat{\lambda}$, which may fail to adapt to real-time distributional shifts or sensor degradation.
- **What evidence would resolve it:** Modified framework that dynamically updates calibration parameters using incoming data streams while formally verifying preserved finite-sample coverage bounds.

## Limitations
- **Exchangeability Assumption:** Core safety guarantee relies on calibration and test data being exchangeable; domain shifts could invalidate FNR bounds.
- **Threshold Stability:** Conformal threshold is data-dependent and sensitive to calibration set size and composition.
- **Reward Shaping Sensitivity:** Specific values of $\kappa$ and $\beta$ are critical for balancing task success and safety; sensitivity analysis not provided.

## Confidence
- **High Confidence:** Mechanism by which COPPOL increases hazard coverage (up to 6×) and reduces violations (up to 50%) is well-supported by experimental results in Tables 1 and 2.
- **Medium Confidence:** Claim of maintaining robustness to distributional shift is supported by noise injection experiment, but extent in severe shifts unexplored.
- **Medium Confidence:** Claim that conformal buffer provides robustness to state-wise noise is demonstrated, but paper doesn't test adversarial noise models.

## Next Checks
1. **Calibration Set Sensitivity:** Systematically vary calibration set size and composition to quantify impact on threshold stability and safety guarantees.
2. **Domain Shift Stress Test:** Evaluate performance when test environment exhibits significant domain shift from calibration data to rigorously test exchangeability assumption.
3. **Reward Hyperparameter Sweep:** Perform grid search over reward parameters ($\kappa$, $\beta$) to identify Pareto frontier between task success rate and safety.