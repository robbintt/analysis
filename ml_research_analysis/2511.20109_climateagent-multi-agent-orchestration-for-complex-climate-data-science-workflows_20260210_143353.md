---
ver: rpa2
title: 'CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows'
arxiv_id: '2511.20109'
source_url: https://arxiv.org/abs/2511.20109
tags:
- data
- climate
- code
- agent
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ClimateAgent is a multi-agent framework that automates complex
  climate science workflows by decomposing user queries into specialized tasks and
  coordinating specialized agents for data acquisition, analysis, and reporting. It
  uses dynamic API introspection and built-in self-correction to handle heterogeneous
  climate datasets and external tools.
---

# CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows

## Quick Facts
- **arXiv ID:** 2511.20109
- **Source URL:** https://arxiv.org/abs/2511.20109
- **Reference count:** 40
- **Primary result:** Achieved 100% task completion and 8.32 report quality score on Climate-Agent-Bench-85, outperforming baselines by 2.5x

## Executive Summary
ClimateAgent is a multi-agent framework that automates complex climate science workflows by decomposing user queries into specialized tasks and coordinating specialized agents for data acquisition, analysis, and reporting. It uses dynamic API introspection and built-in self-correction to handle heterogeneous climate datasets and external tools. Evaluated on Climate-Agent-Bench-85 with 85 real-world tasks across six climate phenomena, ClimateAgent achieved 100% task completion and an 8.32 report quality score, outperforming GitHub-Copilot (6.27) and GPT-5 baseline (3.26), demonstrating reliable end-to-end automation for climate data science.

## Method Summary
ClimateAgent employs a three-layer hierarchical architecture where an ORCHESTRATE-AGENT manages workflow execution and a PLAN-AGENT decomposes user queries into JSON-formatted subtasks. Specialized DATA-AGENTs use Selenium automation to introspect climate APIs (CDS/ECMWF) and generate validated download scripts, while CODING-AGENTs produce and execute Python code for analysis and visualization. The system maintains persistent context across all subtasks, serialized as JSON, and employs multi-strategy self-correction including multi-candidate generation (m=8), iterative refinement (Rmax=3), and semantic validation to achieve reliable end-to-end automation.

## Key Results
- 100% task completion rate on 85 real-world climate science tasks across six phenomena
- Report Quality Score of 8.32 (1-10 scale) vs. 6.27 for GitHub-Copilot and 3.26 for GPT-5 baseline
- Complete automation from natural language query to final scientific report
- Eliminated 17% of baseline failures caused by API parameter validation errors

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition with Specialized Agent Assignment
- Decomposing complex climate queries into agent-specialized subtasks improves end-to-end report quality compared to monolithic single-model approaches.
- PLAN-AGENT breaks user queries into ordered subtasks P = [s₁, s₂, ..., sₙ], each assigned to specialized agents (DATA-AGENT for acquisition, CODING-AGENT for analysis).
- Core assumption: Climate workflows have decomposable structure that maps to distinct expertise types; LLMs perform better with constrained, role-specific prompts than open-ended orchestration.
- Evidence: ClimateAgent achieving 7.85-9.15 across domains vs. GPT-5's 0.00-7.87, with largest gains in multi-stage Tropical Cyclone (7.85 vs. 0.00).

### Mechanism 2: Dynamic API Introspection via Runtime Metadata Extraction
- Extracting real-time API metadata before code generation prevents parameter validation errors that cause 17% of baseline failures.
- DATA-AGENTs use Selenium/Chrome automation to scrape current API parameters from Copernicus and ECMWF portals, construct JSON metadata, and inject into LLM prompts.
- Core assumption: Climate APIs have stable web interfaces but evolving parameter schemas; runtime introspection captures current state better than static documentation.
- Evidence: 17% of baseline failures stem from API parameter errors; ClimateAgent eliminates these through dynamic metadata extraction.

### Mechanism 3: Persistent Context Accumulation for Cross-Step Coordination
- Monotonic context accumulation prevents the "context drift" that causes baseline failures in multi-stage workflows.
- Shared context Cᵢ = {task, plan, code, data, results, logs} persists across agent transitions, serialized to JSON after each subtask.
- Core assumption: Workflow artifacts have well-defined schemas that can be serialized/deserialized; agents can parse prior outputs reliably.
- Evidence: Baseline monolithic scripts "overwrite earlier logic without updating dependent steps" while ClimateAgent maintains "explicit, validated handoffs."

### Mechanism 4: Multi-Strategy Self-Correction (Generation, Refinement, Semantic Validation)
- Combining three complementary error-recovery strategies achieves 100% task completion where single-strategy baselines fail on 26% of tasks.
- (1) Multi-candidate generation produces m=8 download scripts with varying parameter interpretations; (2) Iterative refinement retries CODING-AGENT up to Rmax=3 times; (3) LLM-based semantic validation catches scientifically invalid outputs.
- Core assumption: Error modes are heterogeneous—some require exploration (API parameter spaces), others require debugging (implementation bugs), others require domain knowledge (scientific correctness).
- Evidence: Error distribution in GPT-5 baseline: Data/Array Shape (26%), Data Request (17%), Syntax (11%), Timeout (11%), Type (11%).

## Foundational Learning

- **Concept: Multi-Agent Orchestration Patterns**
  - Why needed: ClimateAgent's three-layer hierarchy (Planning → Data/Coding → Visualization) requires understanding when to decompose vs. centralize.
  - Quick check: Given a climate task requiring both ERA5 reanalysis and TempestExtremes tool execution, which agents would be invoked and in what order?

- **Concept: Climate Data API Constraints**
  - Why needed: 17% of baseline failures stem from API parameter errors. Understanding cdsapi vs. ecmwf-api-client conventions is prerequisite to debugging DATA-AGENT failures.
  - Quick check: For an ERA5 pressure-level request requiring humidity and wind at 1000/850/500 hPa for 2022-12-19 to 2022-12-25, what parameter format would the CDSAPI-AGENT generate?

- **Concept: xarray/NetCDF Coordinate Systems**
  - Why needed: 26% of failures are data/array shape errors. Understanding coordinate alignment is essential for interpreting CODING-AGENT outputs.
  - Quick check: If a baseline script attempts `ivt[:, mask]` where `mask` is 2D and `ivt` is 3D, what shape mismatch occurs and how would ClimateAgent's validation catch it?

- **Concept: Self-Correction Loop Design**
  - Why needed: ClimateAgent's three-strategy recovery system requires tuning thresholds (m=8, Rmax=3).
  - Quick check: If download scripts consistently fail on API timeout rather than parameter errors, which self-correction strategy should be prioritized and how would you adjust parameters?

## Architecture Onboarding

- **Component map:**
  User Query → ORCHESTRATE-AGENT → PLAN-AGENT → Subtask Router → DATA-AGENT (CDSAPI/ECMWF) → CODING-AGENT (Programming) → CODING-AGENT (Visualization) → Final Report
  Key files to trace: `exp_dir/context.json`, `data/`, `code_output/`, `final_report.md`

- **Critical path:**
  1. PLAN-AGENT decomposition correctness—errors here propagate to all downstream agents
  2. DATA-AGENT metadata extraction and parameter validation—17% of baseline failures occur here
  3. Context persistence between CODING-AGENT stages—miscoordinated file paths break visualization
  4. Self-correction loop convergence—non-terminating retries indicate systematic errors

- **Design tradeoffs:**
  - **Specialization vs. generality:** Climate-specific agents (CDSAPI, ECMWF) encode domain knowledge but require maintenance as APIs evolve; generic agents adapt freely but lack validation. Paper claims 2.5x quality improvement (8.32 vs. 3.26) justifies specialization overhead.
  - **Context accumulation vs. token limits:** Monotonic Cᵢ growth ensures completeness but may exceed LLM context windows for long workflows; paper doesn't address pruning strategies.
  - **Exploration (m=8 candidates) vs. efficiency:** Multi-candidate generation increases success probability but 8x API calls per data subtask; paper doesn't report latency/cost metrics.
  - **Orchestration complexity vs. debugging difficulty:** Three-layer hierarchy with five agent types enables modular failure isolation but requires tracing across multiple JSON logs and agent outputs.

- **Failure signatures:**
  - **PLAN-AGENT failures:** Subtask list missing critical steps → empty `final_report.md`; check `context.json` plan field
  - **DATA-AGENT failures:** Download script exits with API error code → empty `data/` directory; check error_history in context for parameter mismatches
  - **CODING-AGENT failures:** Analysis code raises runtime exception → missing intermediate files in `code_output/`; check README.md auto-generated by agent for expected outputs
  - **Context drift:** Downstream agent references non-existent file → path mismatch in generated code; compare file paths in Cᵢ vs. actual filesystem
  - **Self-correction non-convergence:** Retry count reaches Rmax=3 without success → systematic error (e.g., missing dependency, API outage); check final error_history entry

- **First 3 experiments:**
  1. **Run single easy task** (e.g., SST anomaly visualization) with full logging enabled; trace `ORCHESTRATE → PLAN → DATA → CODING → Visualization` pipeline; verify context.json evolves correctly across 4-5 subtasks; check that final_report.md contains expected figure paths and matches reference output
  2. **Inject controlled failure** (modify ERA5 API response to return invalid date format); observe DATA-AGENT multi-candidate generation (m=8 scripts); verify retry loop attempts alternative parameterizations; measure number of candidates tried before success or Rmax exhaustion
  3. **Compare baseline vs. ClimateAgent** on 5 medium-complexity tasks (e.g., atmospheric river detection requiring IVT computation, thresholding, and trajectory analysis); quantify failure modes using Table 3 taxonomy (data/array, API, syntax, type errors); correlate error types with missing self-correction strategies in baseline

## Open Questions the Paper Calls Out

- **Generalization to new phenomena:** The benchmark covers only six climate domains; no evaluation of out-of-domain generalization to phenomena like monsoons or El Niño teleconnections.
- **Computational cost trade-offs:** The paper reports quality improvements but provides no analysis of execution time, API call counts, or resource requirements for the multi-candidate generation approach.
- **LLM evaluation reliability:** The 1–10 scoring is entirely LLM-mediated; no human expert scores are reported for comparison to validate correlation with scientific quality assessment.
- **API introspection robustness:** The Selenium-based mechanism is vulnerable to UI changes; no evaluation of resilience to structural portal changes or longitudinal testing over extended periods.

## Limitations

- **Benchmark accessibility:** Climate-Agent-Bench-85 dataset is not publicly available for independent validation of the 100% task completion claim.
- **Baseline comparison ambiguity:** Unclear whether ClimateAgent uses the same underlying LLM as GPT-5 baseline or a different model, making direct comparison difficult.
- **Selenium brittleness:** Dynamic API introspection via Selenium introduces dependencies on external website structures that could break with minor interface changes.

## Confidence

- **High confidence:** Hierarchical task decomposition mechanism and persistent context accumulation approach are well-specified and technically sound with clear implementation details.
- **Medium confidence:** 2.5x quality improvement over baselines (8.32 vs 3.26) is well-supported by evaluation methodology, though limited by unavailable benchmark dataset.
- **Low confidence:** Claims about 17% of baseline failures from API parameter errors and dynamic introspection eliminating these errors lack direct empirical validation without baseline failure analysis data.

## Next Checks

1. **Replicate baseline comparison** using publicly available climate API tasks to verify ClimateAgent achieves superior success rates compared to monolithic GPT-4 prompting on multi-stage workflows.
2. **Stress-test API introspection** by modifying the Copernicus CDS website structure and measuring whether ClimateAgent's Selenium scraper fails or produces invalid parameter metadata, comparing success rates before and after interface changes.
3. **Analyze context accumulation limits** by running increasingly complex workflows (10+ subtasks) and measuring whether token overflow occurs, documenting the point at which context serialization exceeds typical LLM context windows (e.g., 128K tokens).