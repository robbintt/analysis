---
ver: rpa2
title: Do Generalisation Results Generalise?
arxiv_id: '2512.07832'
source_url: https://arxiv.org/abs/2512.07832
tags:
- olmo2
- snli
- mnli
- in-domain
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether out-of-distribution (OOD) generalisation
  results generalise across different testsets and model configurations. The authors
  propose measuring partial OOD correlations, which quantify the correlation between
  OOD performances while controlling for in-domain performance.
---

# Do Generalisation Results Generalise?

## Quick Facts
- arXiv ID: 2512.07832
- Source URL: https://arxiv.org/abs/2512.07832
- Authors: Matteo Boglioni; Andrea Sgobbi; Gabriel Tavernini; Francesco Rita; Marius Mosbach; Tiago Pimentel
- Reference count: 40
- Key outcome: OOD generalisation results do not generalise—no consistent trends exist in partial OOD correlations across testsets and model configurations.

## Executive Summary
This paper investigates whether out-of-distribution (OOD) generalisation results transfer across different testsets and model configurations. The authors propose measuring partial OOD correlations, which quantify the correlation between OOD performances while controlling for in-domain performance. They evaluate OLMo2 and OPT models fine-tuned on SNLI and MNLI using various shot settings. Results show no consistent trends: whether OOD performances correlate positively or negatively depends strongly on the specific model and training dataset. For example, OPT 30B on SNLI shows strong positive correlations for some testsets but negative for others, while OLMo2 32B exhibits different patterns. These findings highlight that generalisation evaluation must span multiple OOD testsets, as no single testset reliably predicts performance on others.

## Method Summary
The method measures partial OOD correlations between dataset pairs by regressing out in-domain performance. For each OOD dataset, a regression model predicts OOD performance from in-domain performance. Residuals (actual minus predicted OOD performance) are computed and correlated across OOD testsets. The approach uses NLI task data with training on SNLI or MNLI, evaluating across 7 OOD testsets (MNLI, SNLI, WNLI, SciTail, RTE, HANS, ANLI, PAWS). Models include OPT (2.7B–30B) and OLMo2 (7B–32B) with few-shot settings (32/64/128 examples). Pattern-based finetuning with LoRA is used, evaluating at multiple checkpoints to capture training dynamics.

## Key Results
- No consistent trends: partial OOD correlations vary dramatically across model-dataset combinations
- Model-dependent patterns: OPT 30B on SNLI shows positive correlations for some testsets, negative for others
- Different architectures exhibit different correlation patterns (OPT vs OLMo2)
- Results emphasize need for multiple OOD testsets in generalisation evaluation

## Why This Works (Mechanism)

### Mechanism 1: Partial OOD Correlation Isolation
Controlling for in-domain performance reveals whether OOD generalisation transfers across testsets, isolating "true" generalisation signals from trivial performance correlations. The method trains a regression model $f^d$ that predicts OOD performance from in-domain performance, then computes residuals $e^d_t = s^{ood:d}_t - f^d(s^{in}_t)$. These residuals represent how much better/worse a checkpoint performs on each OOD testset than expected. Correlating residuals across OOD testsets ($\rho_{d_1,d_2} = \text{corr}(e^{d_1}, e^{d_2})$) quantifies whether improvements on one OOD testset predict improvements on another, independent of overall task mastery.

### Mechanism 2: Checkpoint-Level Trajectory Analysis
Evaluating models across multiple training checkpoints captures the dynamics of generalisation emergence, revealing whether OOD improvements co-occur. Rather than evaluating a single final checkpoint, the method samples model performance at regular intervals throughout fine-tuning (0k-600k steps), creating time-series vectors for each testset. The partial correlation captures whether, when a checkpoint performs unexpectedly well on one OOD testset, it also tends to perform unexpectedly well on another—controlling for learning trajectory position.

### Mechanism 3: Multi-Testset OOD Diversity
Using multiple qualitatively different OOD testsets (adversarial vs standard vs different domains) exposes whether generalisation is a unified capability or testset-specific artifacts. The study uses 8 NLI datasets spanning adversarial challenge sets (HANS, ANLI, PAWS) designed to expose heuristic shortcuts, and standard evaluation sets (WNLI, RTE, SciTail) representing domain shifts. Computing partial correlations across all pairs reveals whether a model that "truly generalises" does so uniformly.

## Foundational Learning

- **Partial Correlation / Residualisation:**
  - Why needed here: The paper's entire contribution relies on understanding how to "control for" a confounding variable (in-domain performance) when correlating two others.
  - Quick check question: Given variables A, B, and C, if you want to know whether A and B correlate beyond their shared dependence on C, what transformation do you apply?

- **Natural Language Inference (NLI) Task:**
  - Why needed here: Experiments are NLI-specific; understanding what the task requires (entailment, contradiction, neutral relationships between premise-hypothesis pairs) is necessary to interpret why certain OOD testsets might be harder.
  - Quick check question: In NLI, if the premise is "The cat sat on the mat" and the hypothesis is "There is an animal on the mat", what label should this receive?

- **Fine-Tuning Dynamics (LoRA, Few-Shot, Pattern-Based):**
  - Why needed here: The method samples checkpoints during fine-tuning; understanding how models change during fine-tuning (vs. pre-training) clarifies what the trajectories represent.
  - Quick check question: What is the key difference between pattern-based fine-tuning (used here) and training with a randomly initialized classifier head, and why might this affect generalisation?

## Architecture Onboarding

- **Component map:** Training Loop -> Checkpoint Sampler -> Multi-Testset Evaluation -> Regression Module -> Residual Computer -> Correlation Engine -> Heatmap Visualization
- **Critical path:** Training → Checkpoint Sampling → Multi-Testset Evaluation → Regression Fitting → Residual Computation → Pairwise Partial Correlations → Heatmap Visualization (Figure 3)
- **Design tradeoffs:**
  - GAM vs Linear Regression: GAM captures nonlinear in-domain/OOD relationships but adds complexity; paper finds minimal difference
  - Checkpoint frequency: Too sparse misses dynamics; too dense increases compute
  - Testset selection: NLI-only limits generalisability of conclusions
  - Few-shot setting: 128/64/32 examples represents realistic deployment constraints but may limit performance ceiling
- **Failure signatures:**
  - Uniform positive correlations: Would suggest OOD testsets are redundant (not observed)
  - Uniform near-zero correlations: Would suggest OOD performance is pure noise
  - Model-dependent pattern reversals (observed): Indicates generalisation is not a unified capability
  - Regressor overfitting: If GAM fits noise, residuals become uninformative
- **First 3 experiments:**
  1. Reproduce on a single model/dataset pair: Take OPT-2.7B on MNLI-128shot, compute partial correlations across all 7 OOD testsets, verify you see mixed positive/negative correlations matching Figure 3
  2. Ablate regression method: Compare GAM vs linear regression partial correlations on same data to verify the choice of regressor has minor impact
  3. Extend to different task: Apply the same methodology to a non-NLI task with multiple OOD testsets to test whether findings generalise beyond NLI

## Open Questions the Paper Calls Out

- **Do the inconsistent partial OOD correlation trends observed in NLI generalise to other natural language tasks?**
  - Basis in paper: [explicit] The authors state their experiments "focus exclusively on NLI" due to a lack of OOD testsets for other tasks, leaving the generality of their findings uncertain.
  - Why unresolved: The current study could not verify if the model-dependent correlation variance is a universal phenomenon or specific to the NLI task structure.
  - What evidence would resolve it: Replicating the partial correlation analysis across diverse tasks (e.g., question answering, summarization) using equivalent suites of OOD testsets.

- **Do partial OOD correlation patterns change significantly for models exceeding 30 billion parameters?**
  - Basis in paper: [explicit] The authors note that "due to limited compute resources, it was impractical for us to include models larger than 30B parameters" and suggest investigating larger sizes.
  - Why unresolved: It is unclear if the observed instability in correlations is an artifact of model scale or if larger models exhibit more consistent generalisation behavior.
  - What evidence would resolve it: Evaluating partial OOD correlations on models of 70B parameters and above using the same NLI benchmarks.

- **To what extent does pre-training data contamination affect the observed partial OOD correlations?**
  - Basis in paper: [explicit] The authors "are not sure if our studied models... were exposed to the analysed testsets during pretraining" as contamination checks were inconclusive.
  - Why unresolved: If models memorised testsets during pre-training, the measured OOD performances and their correlations might be skewed, invalidating the generalisation assessment.
  - What evidence would resolve it: Conducting controlled experiments comparing models trained on verified clean data against those with known contamination on these specific benchmarks.

## Limitations

- **NLI-specific focus:** Experiments focus exclusively on NLI due to lack of OOD testsets for other tasks, limiting generalizability
- **Model size constraints:** Limited to models up to 30B parameters due to compute constraints
- **Potential contamination:** Uncertainty about whether studied models were exposed to analysed testsets during pretraining

## Confidence

- **High confidence:** The core finding that OOD generalisation results do not consistently transfer across testsets is well-supported by the empirical data
- **Medium confidence:** The partial correlation methodology itself is sound, though the choice of regression model could influence results in edge cases
- **Low confidence:** Whether the observed patterns would persist in non-NLI domains, given that NLI-specific properties may not translate to other tasks

## Next Checks

1. Apply the same partial correlation methodology to a completely different task domain (e.g., computer vision classification with multiple OOD testsets) to test whether the "no consistent trends" finding generalises beyond NLI
2. Systematically vary the regression model complexity (linear, GAM, neural network) and measure sensitivity of partial correlation results to capture any method-dependent artifacts
3. Conduct a controlled experiment where synthetic OOD testsets with known relationships to the in-domain testset are created, then verify that partial correlations correctly recover these known relationships