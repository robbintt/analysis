---
ver: rpa2
title: 'Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and
  Query-Based Attacks using Hyperparameter Tuning'
arxiv_id: '2511.13654'
source_url: https://arxiv.org/abs/2511.13654
tags:
- attacks
- learning
- robustness
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive analysis of how training\
  \ hyperparameters\u2014learning rate, weight decay, momentum, and batch size\u2014\
  affect robustness against both transfer-based and query-based black-box attacks.\
  \ The study covers various ML deployment settings including centralized, ensemble,\
  \ and distributed learning, with both i.i.d."
---

# Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning

## Quick Facts
- arXiv ID: 2511.13654
- Source URL: https://arxiv.org/abs/2511.13654
- Reference count: 40
- One-line primary result: Hyperparameter tuning significantly improves robustness against both transfer-based and query-based black-box attacks, outperforming JPEG compression and adversarial training with fewer training epochs.

## Executive Summary
This paper presents the first comprehensive analysis of how training hyperparameters—learning rate, weight decay, momentum, and batch size—affect robustness against both transfer-based and query-based black-box attacks. The study covers various ML deployment settings including centralized, ensemble, and distributed learning, with both i.i.d. and non-i.i.d. data distributions. Using NSGA-II optimization, the authors identified hyperparameter configurations that strike an effective balance, enhancing robustness against both attack types. These configurations outperform state-of-the-art defenses like JPEG compression and adversarial training, achieving up to 43% improvement in transfer attack robustness and 10% improvement in query attack robustness, with only 4% reduction in clean accuracy.

## Method Summary
The study uses Stochastic Gradient Descent (SGD) with momentum and weight decay to train models across different deployment scenarios: centralized (N=1), deep ensembles (N=3,5,7), and distributed IID/Non-IID ensembles. Hyperparameter search space includes learning rate (0.0001-0.4), weight decay (1e-6-0.01), momentum (0.8-0.99), and batch size (32-2048). The authors evaluate robustness using Common Weakness (transfer attack with 10-surrogate ensemble) and SquareAttack (query-based attack with Q=500) on CIFAR-10 and ImageNet datasets. NSGA-II optimization is used to find Pareto-optimal configurations maximizing clean accuracy and minimum robust accuracy against both attack types.

## Key Results
- Decreasing learning rate significantly enhances robustness against transfer-based attacks by up to 64%
- Increasing learning rate improves robustness against query-based attacks by up to 28%
- Distributed models achieve the best tradeoff by simultaneously mitigating both attack types more effectively than other training setups
- Recommended hyperparameters outperform JPEG compression and adversarial training, requiring significantly fewer training epochs (average 41 vs 186)

## Why This Works (Mechanism)

### Mechanism 1
Training hyperparameters (learning rate, weight decay, momentum, batch size) influence model robustness against black-box attacks by controlling the smoothness of the learned loss landscape, creating an inherent tension between robustness to transfer-based and query-based attacks. Hyperparameters act as implicit regularizers. Lower learning rates and weight decay lead to sharper minima (less smooth models), which reduce gradient similarity between surrogate and target models, hindering transfer-based attacks. Conversely, higher learning rates lead to smoother loss landscapes, requiring larger perturbations for successful query-based attacks, as the estimated gradient is bounded by the true model gradient. The relationship between parameter space sharpness and input space smoothness holds (σ(∇²θ L) ∼ σ(∇²x L)), and the transferability upper bound (Proposition 1) accurately reflects real-world attack behavior.

### Mechanism 2
Distributed machine learning setups inherently offer better robustness trade-offs against both attack types compared to centralized or deep ensembles, due to increased model diversity. In distributed ML, models are trained on disjoint data subsets. This promotes heterogeneity among ensemble members, leading to lower gradient similarity between them. This diversity simultaneously lowers the upper bound on transferability (by misaligning surrogate and target gradients) and provides a smoother overall decision boundary for the ensemble, improving query-based robustness. The benefits of disjoint data (diversity) outweigh the potential performance degradation (lower clean accuracy) from reduced per-node data volume.

### Mechanism 3
A multi-objective genetic algorithm (NSGA-II) can effectively identify a single set of hyperparameters that provides a Pareto-optimal trade-off between clean accuracy and robustness against both transfer and query-based attacks. NSGA-II explores the hyperparameter space to maximize an objective function that balances clean accuracy (CA) and the minimum of robust accuracies against both attack types (min(RA_T, RA_Q)). This finds configurations on the Pareto frontier that are superior to default settings and simpler defenses like JPEG compression. The Pareto frontier found via NSGA-II generalizes beyond the tested datasets (CIFAR-10, ImageNet) and model architectures (ResNet, MobileNetV2).

## Foundational Learning

- Concept: Black-Box Adversarial Attacks (Transfer vs. Query-based)
  - Why needed here: The core contribution of the paper is the analysis of these two distinct attack types. Understanding their mechanisms (surrogate model similarity vs. boundary estimation) is prerequisite to understanding why hyperparameters affect them differently.
  - Quick check question: What is the primary information an attacker uses in a transfer-based attack versus a query-based attack?

- Concept: Model Smoothness and Sharpness (Hessian Eigenvalues)
  - Why needed here: The theoretical foundation relies on the link between hyperparameters, the geometry of the loss landscape (smoothness/sharpness), and robustness. This concept is the key explanatory variable.
  - Quick check question: How does a lower learning rate affect the largest eigenvalue of the Hessian, and what does this imply for model smoothness?

- Concept: Implicit Regularization
  - Why needed here: The paper's central premise is that training hyperparameters are not merely optimization settings but act as implicit regularizers that shape the model's robustness properties.
  - Quick check question: How does weight decay function as an implicit regularizer, and what effect does increasing it have on the learned solution's complexity?

## Architecture Onboarding

- Component map: Training Pipeline (SGD with hyperparameters) -> Deployment Scenarios (Centralized, Deep Ensemble, Distributed IID/Non-IID) -> Defense Mechanism (NSGA-II-optimized hyperparameters) -> Evaluation (RA against transfer and query attacks)

- Critical path:
  1. Define deployment scenario (e.g., Distributed IID with N nodes)
  2. Set hyperparameter search space (Table 1 ranges)
  3. Run NSGA-II search to find Pareto-optimal hyperparameter set H
  4. Train models with recommended H
  5. Evaluate CA, RA_T, and RA_Q

- Design tradeoffs:
  - Transfer vs. Query Robustness: Tuning for high transfer robustness (low η) tends to hurt query robustness, and vice-versa. A trade-off must be managed.
  - Robustness vs. Clean Accuracy: Aggressively tuning for robustness can slightly degrade performance on clean data (e.g., ~4% drop). Distributed setups trade clean accuracy for higher robustness due to data splits.
  - Efficiency: Hyperparameter tuning is computationally cheaper than adversarial training (avg 41 epochs vs. 186) but more expensive than a single training run with default parameters.

- Failure signatures:
  - Tuning for only one attack type leads to high vulnerability on the other
  - In distributed non-i.i.d. settings, aggressive tuning may cause significant CA drops (>5%)
  - Using default hyperparameters results in suboptimal robustness for both attack types in ensemble/distributed settings

- First 3 experiments:
  1. Ablation Study: Train a centralized model on CIFAR-10, varying only the learning rate from 1e-4 to 0.4 while keeping other parameters default. Measure and plot CA and RA_T/RA_Q to reproduce the core dichotomy.
  2. Pareto Frontier Search: Set up an NSGA-II search for a Distributed IID Ensemble with N=3 on CIFAR-10. Objective: maximize min(RA_T, RA_Q) subject to CA > 0.9. Identify the top 3 hyperparameter configurations on the Pareto front.
  3. Comparative Evaluation: Train a model with the best-found configuration from Experiment 2 and compare its RA against a model with default parameters and a model defended with JPEG compression, using the same Common Weakness and SquareAttack.

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed robustness trade-offs persist when using adaptive optimizers like Adam or RMSProp instead of Stochastic Gradient Descent (SGD)? The study exclusively analyzes SGD hyperparameters and relies on SGD-specific theory linking sharpness to learning rate. Adaptive optimizers often implicitly adjust learning rates and might alter the relationship between hyperparameters and model smoothness. Repeating the ablation study using Adam and RMSProp across the same centralized and distributed settings would verify if lower learning rates still uniquely enhance transfer robustness.

### Open Question 2
Does the defense remain effective against "adaptive" transfer attacks where the surrogate model is trained to be sharp (non-smooth)? The authors note that adversarial examples generated on "typically smooth" surrogates transfer well to smooth targets, but the target is robust if it is less smooth. This defense strategy assumes a standard surrogate training procedure; it is unclear if an attacker can bypass this by tuning their surrogate to be sharp to match the target's loss landscape geometry. An experiment where transfer attacks are generated using surrogates trained with high learning rates (sharp minima) would test this.

### Open Question 3
Can hyperparameter tuning be effectively combined with adversarial training to yield compounding robustness benefits? The paper positions hyperparameter tuning as a more efficient alternative to adversarial training. However, it does not evaluate if the robustness gains from sharp/smooth tuning are orthogonal to those gained from adversarial training. Training models using the authors' recommended "balanced" hyperparameters while simultaneously applying adversarial training data augmentation would test for potential compounding effects.

## Limitations
- The study focuses on specific datasets (CIFAR-10, ImageNet) and architectures (ResNet, MobileNetV2), limiting generalizability
- Non-IID distributed settings show potential trade-offs with clean accuracy that could limit practical applicability
- The theoretical explanation relies on assumptions about the relationship between parameter space sharpness and input space smoothness that could be further validated

## Confidence

- **High Confidence**: The empirical observation that decreasing learning rate improves transfer attack robustness and increasing learning rate improves query attack robustness is strongly supported by the ablation study and ablation study plots (e.g., Figure 2, Figure 3).
- **Medium Confidence**: The theoretical explanation that hyperparameters act as implicit regularizers controlling loss landscape smoothness is plausible and grounded in the sharpness/transferability bound, but the exact causal mechanism and the assumption about the relationship between parameter space sharpness and input space smoothness (σ(∇²θ L) ∼ σ(∇²x L)) could be further validated.
- **Medium Confidence**: The claim that distributed ML setups offer the best robustness trade-offs is well-supported by the experiments, but the non-IID results show a potential trade-off with clean accuracy that could limit practical applicability.

## Next Checks

1. **Hyperparameter Isolation Study**: Design an ablation study that systematically varies one hyperparameter at a time (e.g., learning rate, weight decay) while keeping others fixed, to quantify the individual contribution of each hyperparameter to robustness against both attack types.

2. **Generalization Test**: Validate the Pareto-optimal hyperparameter configurations found on CIFAR-10 by applying them to a significantly different dataset (e.g., a smaller dataset like CIFAR-100 or a larger one like JFT-300M) and a different architecture (e.g., Vision Transformer) to assess the robustness of the tuning approach.

3. **Smoothness Validation**: Empirically measure the Hessian spectrum of models trained with different hyperparameter configurations to directly verify the claimed relationship between learning rate and loss landscape sharpness, and whether this correlates with the observed robustness differences.