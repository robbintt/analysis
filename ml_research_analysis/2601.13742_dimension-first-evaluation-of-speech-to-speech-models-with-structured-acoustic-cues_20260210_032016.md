---
ver: rpa2
title: Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic
  Cues
arxiv_id: '2601.13742'
source_url: https://arxiv.org/abs/2601.13742
tags:
- audio
- judge
- overall
- trace
- both
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRACE, a novel framework that enables large
  language model (LLM) judges to evaluate speech-to-speech (S2S) models by reasoning
  over structured audio cues rather than raw audio. The authors address the limitation
  of existing S2S evaluation methods that rely on either opaque and expensive audio
  language models (ALMs) or transcript-only LLM judges that miss crucial non-linguistic
  speech cues.
---

# Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues

## Quick Facts
- arXiv ID: 2601.13742
- Source URL: https://arxiv.org/abs/2601.13742
- Reference count: 40
- Primary result: TRACE achieves higher agreement with human raters than both ALMs and transcript-only LLM judges while being 3× cheaper than ALMs across S2S evaluation benchmarks

## Executive Summary
This paper introduces TRACE, a novel framework that enables large language model (LLM) judges to evaluate speech-to-speech (S2S) models by reasoning over structured audio cues rather than raw audio. The authors address the limitation of existing S2S evaluation methods that rely on either opaque and expensive audio language models (ALMs) or transcript-only LLM judges that miss crucial non-linguistic speech cues. TRACE extracts inexpensive audio signals—including transcriptions, voice quality metrics, and paralinguistic features—into a structured textual blueprint, which an LLM then uses to produce dimension-wise judgments (content, voice quality, paralinguistics) that are deterministically fused into an overall rating. To support this evaluation, the authors introduce a Human Chain-of-Thought (HCoT) annotation protocol that provides dimension-first pairwise judgments aligned with ITU-T standards.

## Method Summary
TRACE addresses the challenge of evaluating speech-to-speech models by extracting structured acoustic cues from audio rather than using raw audio or transcripts alone. The framework processes audio to extract transcriptions, voice quality metrics, and paralinguistic features, then structures these into a textual blueprint. An LLM reasons over this structured information to produce dimension-wise judgments across three categories: content, voice quality, and paralinguistics. These dimension-specific scores are then deterministically fused to generate an overall evaluation score. The approach leverages a Human Chain-of-Thought (HCoT) annotation protocol for creating high-quality reference judgments, where human annotators provide dimension-first pairwise judgments following ITU-T standards. This enables LLM judges to align with human evaluation criteria while maintaining cost-effectiveness compared to ALM-based approaches.

## Key Results
- TRACE achieves higher agreement with human raters than both ALMs and transcript-only LLM judges
- The framework is approximately 3× cheaper than ALMs while maintaining evaluation quality
- Demonstrates improved performance across benchmarks like SPEAKBENCH and S2S-ARENA

## Why This Works (Mechanism)
The mechanism succeeds by decomposing the complex evaluation task into structured, interpretable components that align with how humans assess speech quality. By extracting specific acoustic features (transcriptions, voice quality metrics, paralinguistic features) and presenting them as structured textual blueprints, TRACE enables LLMs to perform explicit reasoning about each dimension of speech quality. This structured approach allows the model to focus on specific aspects of speech evaluation rather than processing raw audio signals, making the evaluation both more transparent and computationally efficient. The dimension-first approach ensures that each aspect of speech quality is evaluated independently before being combined, mirroring human evaluation protocols and enabling more nuanced assessment than monolithic scores.

## Foundational Learning

**Audio Feature Extraction**: Extracting transcriptions, voice quality metrics, and paralinguistic features from raw audio - needed to convert unstructured audio into analyzable components; quick check: verify feature extraction pipeline produces consistent, interpretable outputs across diverse speech samples.

**Structured Textual Blueprints**: Converting extracted audio features into a textual format that LLMs can process - needed to bridge the gap between audio signals and language model reasoning; quick check: validate that blueprint format preserves all necessary information for accurate evaluation.

**Dimension-wise Evaluation**: Breaking down speech quality assessment into content, voice quality, and paralinguistic dimensions - needed to enable targeted evaluation and prevent information loss; quick check: confirm that dimensions are sufficiently independent and collectively comprehensive.

**Deterministic Fusion**: Combining dimension-specific scores into an overall rating through predefined rules - needed to maintain transparency and reproducibility in evaluation; quick check: verify fusion logic produces consistent results across different input combinations.

## Architecture Onboarding

**Component Map**: Audio Input -> Feature Extraction -> Structured Textual Blueprint -> LLM Reasoning -> Dimension Scores -> Deterministic Fusion -> Final Score

**Critical Path**: The evaluation flow proceeds from audio input through feature extraction to structured blueprint generation, followed by LLM reasoning to produce dimension scores, which are then deterministically fused into a final rating.

**Design Tradeoffs**: The framework trades the potentially higher accuracy of raw audio processing for the cost-effectiveness and interpretability of structured feature-based evaluation. While this may miss some subtle acoustic nuances captured by ALMs, it achieves significantly better agreement with human judgments at lower computational cost.

**Failure Signatures**: Potential failures include: (1) Feature extraction errors that propagate through the pipeline, (2) LLM reasoning that fails to properly weight structured information, (3) Fusion logic that produces inconsistent results for edge cases, (4) Structured blueprints that inadequately capture complex paralinguistic features.

**3 First Experiments**:
1. Evaluate TRACE on a simple controlled dataset with known ground truth to verify basic functionality
2. Compare dimension-specific scores against human annotations on the same samples to validate alignment
3. Perform ablation studies by removing individual feature types to assess their contribution to overall evaluation quality

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is primarily benchmarked on SPEAKBENCH and S2S-ARENA, which may not represent the full diversity of S2S applications
- The dimension-first approach assumes three categories (content, voice quality, paralinguistics) are sufficient and orthogonal, which may not hold across all S2S use cases
- The Human Chain-of-Thought annotation protocol relies on pairwise judgments that may not fully capture the complexity of real-world speech evaluation scenarios

## Confidence
- **High confidence**: The cost advantage over ALMs (approximately 3× cheaper) and the basic methodology of structured feature extraction
- **Medium confidence**: The improved agreement with human raters, as this depends on the quality and representativeness of the HCoT annotations
- **Medium confidence**: The dimension-first evaluation approach, pending broader validation across diverse S2S applications

## Next Checks
1. Test TRACE on diverse S2S tasks beyond SPEAKBENCH and S2S-ARENA, including cross-lingual translation and expressive speech generation scenarios
2. Conduct ablation studies to determine which structured features contribute most to evaluation quality versus cost
3. Compare TRACE against human evaluations on a held-out test set not used in training or fine-tuning to assess generalizability