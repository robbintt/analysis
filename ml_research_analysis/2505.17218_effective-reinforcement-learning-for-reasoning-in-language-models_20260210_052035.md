---
ver: rpa2
title: Effective Reinforcement Learning for Reasoning in Language Models
arxiv_id: '2505.17218'
source_url: https://arxiv.org/abs/2505.17218
tags:
- dash
- gradient
- reasoning
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies reinforcement learning (RL) algorithms
  for improving the reasoning capabilities of small language models (LMs), focusing
  on accuracy and computational efficiency. It compares supervised fine-tuning (SFT)
  and on-policy RL, finding that on-policy RL significantly outperforms SFT for the
  small models studied.
---

# Effective Reinforcement Learning for Reasoning in Language Models

## Quick Facts
- **arXiv ID**: 2505.17218
- **Source URL**: https://arxiv.org/abs/2505.17218
- **Reference count**: 27
- **Primary result**: DASH algorithm reduces RL training time by 83% without accuracy loss

## Executive Summary
This paper systematically evaluates reinforcement learning algorithms for improving reasoning in small language models, focusing on the Llama-3 8B architecture. The authors demonstrate that on-policy RL significantly outperforms supervised fine-tuning for reasoning tasks, with the key bottleneck being the sampling procedure. They identify that optimal batch sizes differ between inference and backpropagation, leading to computational inefficiency. To address this, they propose DASH (Decoupled Asynchronous Sampling for High-efficiency RL), which uses preemptive sampling and gradient filtering to achieve substantial computational savings while maintaining or improving accuracy.

## Method Summary
The authors compare supervised fine-tuning (SFT) and on-policy RL algorithms including GRPO and PPO for reasoning tasks on small LMs. They identify that sampling efficiency is the primary bottleneck, as optimal batch sizes differ between inference (requires large batches for better estimates) and backpropagation (benefits from small batches for frequent updates). DASH addresses this through two mechanisms: (1) preemptive sampling - performing large-batch inference while maintaining small-batch gradient updates, and (2) gradient filtering - dropping samples with small advantage estimates to reduce computation. The algorithm is evaluated on GSM8K and MATH reasoning benchmarks.

## Key Results
- DASH reduces training time by 83% compared to GRPO while maintaining accuracy
- On-policy RL outperforms SFT for small models on reasoning tasks
- Removing KL divergence regularization leads to more concise generations and higher accuracy
- DASH shows improved stability compared to PPO-based approaches

## Why This Works (Mechanism)
DASH works by decoupling the sampling process from the gradient update frequency. During RL training for reasoning, the model generates multiple candidate solutions before receiving rewards, creating a bottleneck when using synchronous sampling. By preemptively generating samples in large batches while updating with small gradient batches, DASH maintains computational efficiency. The gradient filtering mechanism further improves efficiency by eliminating samples with negligible learning signals, focusing computation on informative examples.

## Foundational Learning
- **Reinforcement Learning Basics**: Understanding reward maximization and policy optimization is essential for grasping why RL can improve reasoning beyond supervised learning
  - *Why needed*: RL allows exploration of solution strategies rather than just pattern matching
  - *Quick check*: Can you explain the difference between policy gradient and value-based RL?

- **Advantage Estimation**: The core mechanism for determining which actions lead to better outcomes relative to average performance
  - *Why needed*: Advantage filtering in DASH relies on accurate advantage computation
  - *Quick check*: What is the mathematical definition of advantage in RL?

- **KL Regularization**: A constraint that prevents policies from deviating too far from reference distributions during training
  - *Why needed*: Understanding why removing KL can improve generation quality is central to the paper's findings
  - *Quick check*: How does KL regularization typically affect policy optimization?

## Architecture Onboarding

**Component Map**: Llama-3 8B -> RL Policy Optimizer -> Reward Model -> Reasoning Dataset

**Critical Path**: Sample generation → Reward evaluation → Advantage computation → Gradient filtering → Parameter update

**Design Tradeoffs**: 
- Synchronous vs asynchronous sampling (computational efficiency vs gradient staleness)
- Large vs small batch sizes (inference quality vs update frequency)
- KL regularization (stability vs generation quality)

**Failure Signatures**: 
- Degraded accuracy when gradient filtering threshold is too aggressive
- Training instability without KL regularization
- Memory bottlenecks when preemptive sampling exceeds GPU capacity

**First Experiments**:
1. Compare training time and accuracy between DASH and baseline GRPO on GSM8K
2. Evaluate impact of KL regularization removal on generation length and accuracy
3. Test different gradient filtering thresholds to find optimal balance between efficiency and performance

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to larger models (Llama-3 70B+) remains untested, with potential memory constraints
- Long-term stability of models trained without KL regularization needs evaluation across multiple reasoning tasks
- Computational efficiency gains may not translate directly when GPU memory becomes more severely constrained

## Confidence
- **High**: 83% training time reduction claim, comparative evaluations against GRPO and PPO
- **Medium**: KL regularization impact on generation quality, findings specific to tested reasoning datasets
- **Low**: Scalability to different model architectures beyond Llama-3 8B

## Next Checks
1. Test DASH on larger language models (e.g., Llama-3 70B) to verify scalability and evaluate memory efficiency trade-offs
2. Conduct ablation studies with alternative reward functions and different KL regularization coefficient schedules
3. Perform long-term stability analysis on models trained without KL regularization, including evaluation across multiple reasoning tasks over extended inference periods