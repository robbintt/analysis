---
ver: rpa2
title: 'CaRT: Teaching LLM Agents to Know When They Know Enough'
arxiv_id: '2510.08517'
source_url: https://arxiv.org/abs/2510.08517
tags:
- termination
- diagnosis
- information
- cart
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of teaching large language\
  \ models (LLMs) to know when to stop gathering information during multi-step reasoning\
  \ tasks. The core idea is to fine-tune LLMs using counterfactual pairs of trajectories\u2014\
  one where termination is appropriate and a minimally modified version where it is\
  \ not\u2014combined with explicit reasoning traces that justify the termination\
  \ decision."
---

# CaRT: Teaching LLM Agents to Know When They Know Enough

## Quick Facts
- **arXiv ID**: 2510.08517
- **Source URL**: https://arxiv.org/abs/2510.08517
- **Reference count**: 40
- **Primary result**: CaRT improves LLM termination decisions using counterfactual pairs and reasoning traces, achieving 0.36 FRQ success rate in medical diagnosis and outperforming baselines in math reasoning.

## Executive Summary
This paper addresses the challenge of teaching large language models to know when to stop gathering information during multi-step reasoning tasks. The core innovation is CaRT (Counterfactuals and Reasoning for Termination), which fine-tunes LLMs using counterfactual trajectory pairs and explicit reasoning traces that justify termination decisions. The method is evaluated in two domains: interactive medical diagnosis (explicit Q&A) and math problem solving (implicit thinking). Results demonstrate that CaRT improves both the efficiency of information gathering and task success rates compared to other fine-tuning approaches.

## Method Summary
CaRT fine-tunes LLMs to make optimal termination decisions by training on counterfactual trajectory pairs—one where termination is appropriate and a minimally modified version where it is not—combined with reasoning traces explaining the termination decision. The process involves generating trajectories with labeled prefixes, identifying breakpoints where success probability jumps by ≥50%, creating hard negative counterfactuals by perturbing questions until success drops below 30%, and generating reasoning traces via GPT-4o. The fine-tuning uses SFT on these counterfactual pairs, with optional GRPO reinforcement learning post-training using binary rewards for correct/incorrect termination decisions.

## Key Results
- CaRT achieves up to 0.36 FRQ success rate in medical diagnosis, outperforming other fine-tuning methods
- Models trained with CaRT show improved efficiency in information gathering across both medical and math domains
- Ablation studies confirm that both counterfactual data and reasoning traces are essential for optimal performance

## Why This Works (Mechanism)
CaRT works by providing LLMs with contrastive examples that highlight the difference between appropriate and inappropriate termination decisions. The counterfactual pairs create a clear signal about when additional information gathering is beneficial versus when sufficient information has been collected. The reasoning traces force the model to explicitly articulate the rationale behind termination decisions, promoting more deliberate and accurate judgment. This combination addresses the fundamental challenge of teaching models to balance the trade-off between gathering sufficient information and avoiding unnecessary queries.

## Foundational Learning
- **Counterfactual learning**: Why needed - Provides contrastive examples that highlight decision boundaries; Quick check - Verify success rate drops below 0.3 after perturbation
- **Reasoning trace generation**: Why needed - Forces explicit articulation of termination rationale; Quick check - Assess coherence and domain-appropriateness of generated traces
- **Breakpoint identification**: Why needed - Identifies optimal stopping points where additional information provides diminishing returns; Quick check - Validate ≥50% success jump threshold
- **Success rate estimation**: Why needed - Provides ground truth for training signal; Quick check - Monitor variance across 50 generations per prefix

## Architecture Onboarding

**Component Map**: Trajectory Generation -> Breakpoint Identification -> Counterfactual Creation -> Reasoning Trace Generation -> SFT Fine-tuning -> (Optional GRPO)

**Critical Path**: The core innovation lies in the counterfactual generation and reasoning trace components, which provide the contrastive learning signal that distinguishes CaRT from standard fine-tuning approaches.

**Design Tradeoffs**: CaRT trades increased training complexity (requiring multiple LLM calls for counterfactuals and reasoning) for improved termination decision quality. The method assumes access to external reward models for success rate estimation, which may limit applicability in domains without reliable automatic evaluation.

**Failure Signatures**: 
- SFT without counterfactuals learns length-based heuristics (terminates when conversation long, ignoring content)
- Base model never terminates (consistently low termination rates)
- Counterfactual generation fails to drop success below 0.3 threshold

**3 First Experiments**:
1. Generate 5-10 medical conversations with GPT-4o and label with Llama-3.1-8B-Instruct to verify success rate estimation process
2. Create counterfactual pairs for 3 example prefixes and verify success rate drops below 0.3 after perturbation
3. Fine-tune Qwen2.5-3B-Instruct on a small set of counterfactual pairs (50 examples) and evaluate termination behavior on held-out conversations

## Open Questions the Paper Calls Out
None

## Limitations
- Key prompt templates for counterfactual generation and reasoning trace creation are not provided, making faithful reproduction challenging
- The method requires access to GPT-4o for simulation and reasoning trace generation, which may not be feasible for all users
- Episode segmentation logic for math domain relies on detecting "logic/strategy change sentences" without explicit definition

## Confidence
**High Confidence**: The core conceptual framework and evaluation metrics are well-articulated and theoretically sound
**Medium Confidence**: The overall methodology is described with sufficient detail for reproducibility, though key prompts are missing
**Low Confidence**: Exact implementation details for critical components (prompts, segmentation logic, base model training) are insufficiently specified

## Next Checks
1. Validate that success rate consistently drops below 0.3 after counterfactual perturbation by tracking success rates across multiple iterations on held-out prefixes
2. Plot termination rates across conversation steps on 3-5 example conversations to verify content-aware termination vs length-based heuristics
3. Evaluate 50 random reasoning traces for coherence and domain-appropriateness, checking for systematic quality patterns