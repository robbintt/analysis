---
ver: rpa2
title: Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards
arxiv_id: '2509.24981'
source_url: https://arxiv.org/abs/2509.24981
tags:
- rover
- policy
- arxiv
- reasoning
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROVER (Random Policy Valuation for Diverse
  Reasoning), a minimalist reinforcement learning method for improving LLM reasoning
  with verifiable rewards. The key insight is that in deterministic, tree-structured
  MDPs with binary terminal rewards, the optimal action can be recovered from Q-values
  of a fixed uniform random policy, eliminating the need for iterative policy evaluation-improvement
  loops.
---

# Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards

## Quick Facts
- **arXiv ID**: 2509.24981
- **Source URL**: https://arxiv.org/abs/2509.24981
- **Reference count**: 34
- **Primary result**: +8.2 pass@1 and +16.8 pass@256 improvements over baselines on competition-level math reasoning tasks

## Executive Summary
This paper introduces ROVER (Random Policy Valuation for Diverse Reasoning), a reinforcement learning method that leverages random policy evaluation to improve LLM reasoning with verifiable rewards. The key insight is that in deterministic, tree-structured MDPs with binary terminal rewards, the optimal action can be recovered from Q-values of a fixed uniform random policy, eliminating the need for iterative policy evaluation-improvement loops. ROVER achieves superior performance on competition-level math reasoning tasks while maintaining higher diversity compared to PPO/GRPO baselines.

## Method Summary
ROVER evaluates a uniformly random policy and samples actions via softmax over its Q-values, achieving optimality while preserving diversity. The method avoids the instability and diversity collapse common in PPO/GRPO methods by using a fixed random policy for Q-value estimation. The Q-function is parameterized intrinsically through the LLM, and low-variance rewards are computed using group reward centering. This approach demonstrates that in deterministic tree-structured MDPs with binary terminal rewards, random policy evaluation is sufficient to recover the optimal policy without iterative improvement.

## Key Results
- Achieves +8.2 improvement on pass@1 and +16.8 on pass@256 over baselines on competition-level math reasoning tasks
- Improves diversity by +17.6% while maintaining mathematical correctness
- Maintains higher entropy throughout training and discovers novel reasoning strategies absent from base models
- Robust to temperature settings and scales well at test-time with majority voting across multiple model sizes (4B-8B parameters)

## Why This Works (Mechanism)
ROVER exploits the mathematical property that in deterministic tree-structured MDPs with binary terminal rewards, the optimal policy can be recovered from the Q-values of a fixed random policy. By evaluating a uniform random policy once and using its Q-values for action selection, ROVER avoids the instability and diversity collapse associated with iterative policy improvement methods like PPO/GRPO. The softmax sampling over Q-values naturally preserves exploration and diversity while still converging to optimal solutions.

## Foundational Learning
- **Deterministic MDPs**: Why needed - ensures consistent Q-value estimates from random policy; Quick check - verify tree structure and deterministic transitions in math reasoning tasks
- **Binary terminal rewards**: Why needed - simplifies value estimation to success/failure outcomes; Quick check - confirm all math problems have clear correct/incorrect outcomes
- **Q-value sufficiency**: Why needed - proves random policy can recover optimal policy without iteration; Quick check - validate Q-values from random policy match optimal Q-values in small tree problems
- **Group reward centering**: Why needed - reduces variance in reward estimation across solution paths; Quick check - compare variance with and without centering on sample problems
- **Temperature scaling**: Why needed - controls exploration vs exploitation trade-off in softmax sampling; Quick check - measure diversity metrics across different temperature values
- **Majority voting**: Why needed - improves reliability at test-time by aggregating diverse solutions; Quick check - verify voting increases pass@256 metrics across benchmarks

## Architecture Onboarding

**Component Map**: LLM (Q-function) -> Random Policy Evaluation -> Softmax Action Sampling -> Verifiable Reward Computation -> Group Reward Centering

**Critical Path**: The critical path flows from the LLM's Q-value estimation through random policy evaluation to softmax action sampling. The Q-function is parameterized intrinsically through the LLM's internal representations, making the LLM the central component that must maintain stable Q-value estimates across the solution space.

**Design Tradeoffs**: ROVER trades computational efficiency (single random policy evaluation vs. iterative PPO) for theoretical guarantees that hold specifically in deterministic tree-structured domains. The method sacrifices generality for the domains where its assumptions hold, but gains stability and diversity preservation that iterative methods lose.

**Failure Signatures**: The method will fail when the MDP assumptions break down - specifically in non-deterministic environments, continuous reward spaces, or non-tree-structured problems where optimal policies cannot be recovered from random policy Q-values. Early warning signs include divergence between random policy Q-values and observed optimal outcomes.

**First 3 Experiments**: 1) Validate Q-value sufficiency in small deterministic tree problems by comparing random policy Q-values against optimal Q-values; 2) Test temperature scaling effects on diversity metrics across the full benchmark suite; 3) Conduct ablation study removing group reward centering to quantify its contribution to variance reduction.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section raises implicit questions about extending the method to non-tree-structured problems and continuous reward domains.

## Limitations
- Theoretical guarantees rely heavily on deterministic, tree-structured MDP assumptions that may not hold in more complex domains
- Performance on non-tree-structured problems or tasks with continuous reward signals remains unexplored
- The relative importance of group reward centering and temperature scaling versus the core random policy valuation insight is not systematically quantified
- Qualitative assessment of reasoning quality and mathematical correctness of diverse solutions is limited

## Confidence

| Claim | Confidence |
|-------|------------|
| +8.2 pass@1 and +16.8 pass@256 improvements on competition math benchmarks | High |
| Random policy sufficiency in deterministic tree-structured MDPs | Medium |
| Improved diversity metrics (+17.6%) | Medium |
| Robustness to temperature settings and scalability across model sizes | Medium |

## Next Checks
1. Test ROVER on non-tree-structured reasoning tasks with longer solution paths to evaluate whether random policy valuation remains sufficient when the optimal policy assumption breaks down
2. Conduct ablation studies isolating the contribution of group reward centering and temperature scaling from the core random policy valuation mechanism
3. Perform human evaluation studies comparing the mathematical correctness and reasoning quality of solutions generated by ROVER versus PPO/GRPO baselines across the diverse solution space