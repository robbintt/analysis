---
ver: rpa2
title: Reranking with Compressed Document Representation
arxiv_id: '2505.15394'
source_url: https://arxiv.org/abs/2505.15394
tags:
- compressed
- document
- reranking
- arxiv
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RRK, a compressed representation for reranking
  that significantly improves efficiency while maintaining effectiveness. The method
  uses the PISCO compression model to convert documents into fixed-size embedding
  representations, which are then used to train a reranker through distillation from
  a state-of-the-art cross-encoder.
---

# Reranking with Compressed Document Representation

## Quick Facts
- arXiv ID: 2505.15394
- Source URL: https://arxiv.org/abs/2505.15394
- Reference count: 40
- Primary result: RRK achieves up to 16x speedup compared to text-based rerankers while maintaining effectiveness on standard IR benchmarks

## Executive Summary
This paper introduces RRK, a compressed representation approach for efficient document reranking. The method uses PISCO compression to convert documents into fixed-size embedding representations, which are then used to train a reranker through distillation from a state-of-the-art cross-encoder. RRK reduces input length to 32 tokens (8 memory tokens plus query), achieving significant speedup while matching the effectiveness of much slower text-based rerankers on standard IR benchmarks like BeIR and TREC.

## Method Summary
RRK compresses documents offline using PISCO, a RAG-focused compression model that appends learnable memory tokens to each document and extracts their final hidden states as fixed-size embeddings. At inference, queries are combined with these pre-computed document embeddings (32 tokens total) and fed to a Mistral-7B decoder with LoRA, trained via distillation to predict relevance scores from a teacher cross-encoder. This approach enables constant-time reranking independent of document length.

## Key Results
- Achieves up to 16x speedup compared to text-based rerankers
- Matches effectiveness of ModernBERT and Mistral-7B on BeIR and TREC benchmarks
- Maintains efficiency gains for long documents where other methods degrade
- Index size of 270GB is larger than alternatives but shared with RAG generation

## Why This Works (Mechanism)

### Mechanism 1: Memory Token Compression Preserves Relevance Signals
- Documents represented as 8 fixed-size embedding vectors retain sufficient information for relevance scoring
- PISCO appends learnable memory tokens; final hidden states of these tokens become document embeddings
- Core assumption: semantic information necessary for relevance judgment is compressible into low-dimensional subspace
- Break condition: documents with sparse, distributed relevance signals that cannot be captured in 8 tokens

### Mechanism 2: Distillation Transfers Cross-Encoder Knowledge to Compressed Input Space
- Student reranker on compressed inputs approximates teacher cross-encoder scores through MSE regression
- Teacher (Naver-DeBERTa) produces relevance scores; student (Mistral-7B) receives query + 8 compressed document embeddings
- Core assumption: mapping from compressed representation to relevance score is learnable
- Break condition: when teacher and student input representations diverge too strongly

### Mechanism 3: Fixed Input Length Decouples Inference Cost from Document Length
- Constraining input to 32 tokens yields constant-time reranking independent of document size
- All documents pre-compressed to identical 8-token representation; only query tokenization varies
- Core assumption: query length remains bounded at ~24 tokens
- Break condition: long queries break efficiency assumption

## Foundational Learning

- **Knowledge Distillation (Teacher-Student Training)**
  - Why needed: RRK's training depends on transferring knowledge from text-based cross-encoder to compressed-input student
  - Quick check: Can you explain why MSE loss might be preferred over KL divergence for score regression in this context?

- **Cross-Encoder vs. Late-Interaction Architectures**
  - Why needed: Understanding attention patterns is essential for grasping why compression changes efficiency equation
  - Quick check: How does cross-encoder attention differ from ColBERT's late interaction, and where does RRK's compressed representation fit on this spectrum?

- **Soft Prompt / Memory Token Mechanisms**
  - Why needed: PISCO's compression uses memory tokens—learnable embeddings that absorb document information
  - Quick check: What is the difference between hard prompts (text) and soft prompts (continuous embeddings), and why might soft prompts enable compression?

## Architecture Onboarding

- **Component map:**
  Document Corpus -> PISCO Compressor (offline) -> Compressed Embeddings Index -> Query + Top-k Retrieved Docs -> Lookup Embeddings -> Mistral-7B Decoder + LoRA -> Relevance Score

- **Critical path:**
  1. Offline: Compress all documents once using frozen PISCO model
  2. Retrieval: SPLADE-v3 retrieves top-50 candidates
  3. Lookup: Fetch pre-computed 8-token embeddings for each candidate
  4. Inference: Concatenate query tokens + 8 doc embeddings (32 total) -> decoder -> scalar score

- **Design tradeoffs:**
  - Storage vs. Speed: Index is 270GB (larger than ColBERT's 154GB), but shared with RAG generation step
  - Effectiveness vs. Efficiency: RRK-16 layers faster but -2 pts nDCG; full RRK closer to teacher but slower
  - Model Size: Authors attempted smaller models (1B) but failed—larger decoder may be necessary for compressed-input reasoning

- **Failure signatures:**
  - Specific datasets underperform: FEVER and Touché show >1 pt degradation vs. text-based rerankers
  - Long-query scenarios: Breaks efficiency assumption
  - Very long documents (>768 tokens): PISCO trained on 128 tokens; performance degrades at 1K tokens

- **First 3 experiments:**
  1. Sanity check compression quality: Compare teacher scores on raw text vs. teacher scores on PISCO-reconstructed text for sample query-document pairs
  2. Teacher-student score correlation: Plot teacher scores vs. student scores on held-out queries; compute Pearson/Spearman correlation
  3. Latency profiling across document lengths: Measure end-to-end reranking time for documents at 256, 512, 768, 1024 tokens for RRK vs. ModernBERT vs. Mistral-text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does RRK underperform on FEVER and Touché datasets compared to text-based rerankers?
- Basis in paper: Authors explicitly state they cannot explain this weakness
- Why unresolved: The authors observe consistent performance gaps but do not investigate the root cause
- What evidence would resolve it: Ablation studies analyzing document characteristics of FEVER/Touché

### Open Question 2
- Question: Can effective compressed representation rerankers be trained using smaller backbone models (<7B parameters)?
- Basis in paper: Initial attempts with 1B parameter models failed miserably
- Why unresolved: Authors tried Qwen 3B and Llama 1B but failed; no root cause analysis
- What evidence would resolve it: Systematic experiments with varying model sizes and configurations

### Open Question 3
- Question: Does training the compressor and reranker with longer input sequences (>128 tokens) improve performance on long documents?
- Basis in paper: PISCO and RRK trained with maximum input length of 128, limiting effectiveness for long documents
- Why unresolved: No experiments with longer training sequences conducted
- What evidence would resolve it: Retraining with 256, 512, or 768 token inputs and evaluating on long-document benchmarks

## Limitations
- Performance degradation on FEVER and Touché datasets remains unexplained
- Compression quality uncertainty for documents longer than 128 tokens (training-test mismatch)
- Substantial index size of 270GB may limit deployment in resource-constrained environments

## Confidence

**High Confidence (8/10):**
- RRK achieves 16x speedup with minimal effectiveness loss across BeIR datasets
- Compressed representation maintains efficiency gains for long documents
- Distillation training successfully transfers knowledge from teacher to student

**Medium Confidence (6/10):**
- RRK matches ModernBERT and Mistral-7B effectiveness on standard benchmarks
- 32-token input constraint provides constant-time inference
- Compressed representation captures sufficient semantic information

**Low Confidence (4/10):**
- RRK's performance on FEVER and Touché datasets (significant underperformance unexplained)
- PISCO compression quality for documents longer than 128 tokens
- Scalability to much larger document collections

## Next Checks
1. Compression Quality Analysis: Compare teacher cross-encoder scores on raw text versus reconstructed text from PISCO embeddings for 100 query-document pairs
2. Dataset-Specific Debugging: Conduct ablation studies on FEVER and Touché to identify performance degradation causes
3. Long Document Robustness: Systematically evaluate RRK performance on documents of increasing length (256, 512, 768, 1024 tokens)