---
ver: rpa2
title: 'Multi-document Summarization through Multi-document Event Relation Graph Reasoning
  in LLMs: a case study in Framing Bias Mitigation'
arxiv_id: '2506.12978'
source_url: https://arxiv.org/abs/2506.12978
tags:
- event
- graph
- bias
- relation
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses media bias mitigation by proposing a multi-document
  event relation graph reasoning framework for neutralized summarization. The approach
  constructs a graph containing events as nodes, moral opinions as attributes, four
  types of in-document event relations (temporal, causal, subevent, coreference),
  and cross-document coreference relations to capture narrative framing and selection
  biases.
---

# Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation

## Quick Facts
- arXiv ID: 2506.12978
- Source URL: https://arxiv.org/abs/2506.12978
- Reference count: 40
- Primary result: Framework achieves lowest polarization/arousal scores while maintaining high Rouge/BLEU scores in neutralized multi-document summarization

## Executive Summary
This paper addresses media bias mitigation by proposing a multi-document event relation graph reasoning framework for neutralized summarization. The approach constructs a graph containing events as nodes, moral opinions as attributes, four types of in-document event relations (temporal, causal, subevent, coreference), and cross-document coreference relations to capture narrative framing and selection biases. Two strategies—graph textualization and graph prompt tuning—are used to incorporate the graph into large language models. Automatic and human evaluations show the method effectively reduces both lexical and informational bias while improving content preservation.

## Method Summary
The framework builds a multi-document event relation graph for each document cluster, then incorporates it into LLMs through two complementary strategies. The graph contains event nodes extracted via Longformer classifier, moral opinion attributes from Longformer+BiLSTM classifier, in-document relations extracted via joint learning, and cross-document coreference links. Graph textualization converts this structure into natural language prompts, while graph prompt tuning encodes it as GNN embeddings projected into LLM space. The method is evaluated on the NeuS dataset with Llama-2 and LED backbones, measuring content preservation (Rouge, BLEU) and bias mitigation (polarization, arousal).

## Key Results
- Llama-2 + graph model achieves highest Rouge scores, BLEU score, and lowest polarization score
- LED + graph model shows lowest arousal scores (better lexical neutrality)
- Framework consistently outperforms baseline models and commercial LLMs in bias mitigation
- Ablation studies show both graph textualization and graph prompt tuning contribute to performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-document event coreference exposes content selection bias by distinguishing commonly-reported events from selectively-reported events.
- Mechanism: Events appearing across multiple ideological sources are identified as coreferent and prioritized as neutral ground truth. Events unique to a single source are flagged as potentially biased selections. The graph connects documents through these coreference edges, enabling the model to compute "consensus" vs. "outlier" events.
- Core assumption: Events reported across ideological divides are more likely to be factual and neutral than events reported by only one perspective.
- Evidence anchors: Cross-doc event coreference relation to reveal content selection bias; cross-document event coreference relation connects multiple single-document event relation graphs; Narrative Consolidation work on unifying multi-perspective accounts.

### Mechanism 2
- Claim: Moral opinion attributes on event nodes enable detection of opinionated framing bias.
- Mechanism: Events are classified into Moral Foundation Theory categories (Care/Harm, Fairness/Cheating, etc.). When the same event receives different moral valences across documents, this signals framing bias. The model learns to downweight morally-charged descriptions in favor of neutral language.
- Core assumption: Moral sentiment is a reliable indicator of ideological framing in news text.
- Evidence anchors: Event-level moral opinions to highlight opinionated framing bias; Figure 1 showing different moral descriptions of related events.

### Mechanism 3
- Claim: Combining hard prompts (textualized graph) with soft prompts (learned graph embeddings) provides complementary structural and semantic information.
- Mechanism: Textualized graph explicitly describes events/relations in natural language, leveraging frozen LLM's language understanding. Graph prompt tuning encodes relational structure via GNN attention, then projects embeddings into LLM's representation space for fine-tuning. Hard prompts convey structure; soft prompts enable gradient-based optimization.
- Core assumption: LLMs can reason over structured graph descriptions when presented textually; GNN embeddings capture topology not easily expressed in text.
- Evidence anchors: Hard and soft prompts complement each other; ablation study showing both components contribute; Neutralizing Bias in LLM Reasoning using Entailment Graphs using graph structures for bias mitigation.

## Foundational Learning

- Concept: **Moral Foundation Theory (MFT)**
  - Why needed here: Provides the 10-class taxonomy (Care/Harm, Fairness/Cheating, etc.) for classifying event-level moral opinions. Required to understand bias signal design.
  - Quick check question: Can you explain why "challenges constitutional checks" maps to the "subversion" moral foundation?

- Concept: **Graph Attention Networks (GAT)**
  - Why needed here: Used to encode event relations with attention over neighbors. Relation-aware attention (Eq. 9-12) aggregates information based on relation type.
  - Quick check question: How does relation-aware attention differ from standard GAT when processing "temporal" vs. "causal" edges?

- Concept: **Soft Prompt Tuning**
  - Why needed here: Enables injecting learned graph embeddings into frozen LLMs. Requires understanding how to project external embeddings into LLM's token space.
  - Quick check question: Why must the projection layer output match the LLM's embedding dimension?

## Architecture Onboarding

- Component map: Event Extraction (Longformer) -> Moral Classifier (Longformer+BiLSTM) -> Relation Extractors (Joint learning) -> Cross-doc Coreference -> Graph Construction -> Textualization + GNN Encoding -> LLM Generation

- Critical path: Event extraction → Relation extraction → Cross-doc coreference → Graph construction → Textualization + GNN encoding → LLM generation. Errors propagate forward; coreference failures are most impactful.

- Design tradeoffs:
  - Llama-2 (decoder-only): Higher Rouge/BLEU, lower polarization; requires LoRA fine-tuning
  - LED (encoder-decoder): Lower arousal scores (better lexical neutrality); handles longer inputs natively
  - Textual-only vs. embedding-only: Textual is interpretable; embedding captures implicit patterns

- Failure signatures:
  - Hallucinated locations → Coreference or content recovery failure
  - Biased phrasing retained → Moral classifier missed opinionated event
  - Low Rouge with low polarization → Over-neutralization, losing content

- First 3 experiments:
  1. Baseline reproduction: Run LED and Llama-2 baselines without graph on NeuS test set. Verify polarization ~30 and Rouge-1 ~40 to match Table 1.
  2. Ablation by graph component: Test with only moral attributes, only in-doc relations, only cross-doc coreference. Identify which component reduces polarization most.
  3. Coreference error injection: Manually corrupt 10% of cross-doc coreference links and measure polarization/bias metric degradation. Quantifies robustness of Mechanism 1.

## Open Questions the Paper Calls Out

The paper identifies enhancing the extraction of implicit event relations as necessary future work, noting the current graph construction performance is "not perfect" for cases that state event relations in an implicit way without explicit discourse connectives.

## Limitations

- Cross-document coreference resolution has ~12% error rate that could propagate to bias detection, with the assumption that cross-ideological events are inherently neutral not always holding.
- Moral foundation classification achieves only 41.13% F1, introducing substantial noise into the bias signal and potentially causing the model to miss biased events or flag neutral ones as biased.
- The framework is specifically designed for and evaluated on media framing bias in US political news, with uncertain generalizability to other domains or multilingual contexts.

## Confidence

- **Bias Mitigation Effectiveness**: High confidence - framework consistently outperforms baselines across multiple bias metrics while maintaining content preservation.
- **Graph Reasoning Mechanism**: Medium confidence - theoretical framework is well-articulated with ablation support, but low component performance suggests benefits come from combined effects.
- **Generalizability Beyond Media Bias**: Low confidence - method relies on Moral Foundation Theory and cross-document coreference that may not transfer to domains with uniform event descriptions.

## Next Checks

1. **Coreference Error Sensitivity Analysis**: Systematically inject controlled errors into cross-document coreference links (e.g., 5%, 10%, 15% corruption rate) and measure degradation in polarization and content preservation metrics.

2. **Moral Classifier Error Attribution**: Conduct error analysis on moral foundation classifications to determine whether errors are random or systematic (e.g., consistently misclassifying Care/Harm as Fairness/Cheating).

3. **Zero-Shot Transfer to Different Domains**: Apply the framework to a different domain with known framing issues (e.g., scientific controversies, policy debates) without fine-tuning. Measure whether the same graph construction and reasoning approach generalizes or requires domain adaptation.