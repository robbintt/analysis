---
ver: rpa2
title: 'Hadamax Encoding: Elevating Performance in Model-Free Atari'
arxiv_id: '2505.15345'
source_url: https://arxiv.org/abs/2505.15345
tags:
- encoder
- learning
- hadamax
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes Hadamax, a novel encoder architecture for model-free
  Atari reinforcement learning. Hadamax applies max-pooling to Hadamard products between
  parallel GELU-activated hidden layers, replacing convolutional strides with max-pooling
  and introducing Hadamard representations to increase effective rank without scaling
  network size.
---

# Hadamax Encoding: Elevating Performance in Model-Free Atari

## Quick Facts
- **arXiv ID:** 2505.15345
- **Source URL:** https://arxiv.org/abs/2505.15345
- **Reference count:** 40
- **Primary result:** Hadamax-PQN achieves 80% performance gain over vanilla PQN and surpasses Rainbow-DQN on Atari-57 using only 90M frames

## Executive Summary
Hadamax introduces a novel encoder architecture for model-free Atari reinforcement learning that replaces convolutional strides with max-pooling and incorporates Hadamard products between parallel GELU-activated hidden layers. The design aims to increase effective rank without scaling network size. Applied to PQN without hyperparameter modifications, Hadamax-PQN achieves 80% performance gain over vanilla PQN and matches Rainbow-DQN's performance at 90M frames instead of 260M. The encoder also generalizes to C51, improving performance by ~70% on Atari-10.

## Method Summary
Hadamax is a 3-block convolutional encoder for Atari that uses parallel branches of stride-1 convolutions with LayerNorm and GELU activation, followed by element-wise Hadamard products and max-pooling for downsampling. The final block includes a 512-unit dense layer before the output. The architecture maintains higher effective rank during training compared to baselines, enabling richer representations without increased dimensionality. It was tested with PQN (no replay buffer, no target network, λ-return) and C51 algorithms.

## Key Results
- 80% performance gain over vanilla PQN on Atari-57 benchmark
- Surpasses Rainbow-DQN performance at 90M frames (vs Rainbow's 260M)
- ~70% improvement over C51 on Atari-10
- Effective rank maintained at higher levels during training
- Dead neuron fraction remains under 8% after 200M frames

## Why This Works (Mechanism)

### Mechanism 1: Max-Pooling Downsampling for Feature Selection
- **Claim:** Replacing strided convolutions with max-pooling preserves denser feature representations while emphasizing the strongest signals
- **Evidence:** Max-pooling ablation causes largest performance decay; emphasizes strongest signals over averaging
- **Core assumption:** Important features are spatially localized and benefit from selection
- **Break condition:** Tasks requiring precise spatial relationships may lose critical positional information

### Mechanism 2: Hadamard Products for Higher Effective Rank
- **Claim:** Element-wise multiplication of parallel hidden layers increases representation capacity without increasing layer dimensions
- **Evidence:** Effective rank plots show Hadamax maintains higher rank in Conv2/Conv3; dead neuron fraction under 8%
- **Core assumption:** Training stability keeps dead neurons low enough for successful Hadamard multiplication
- **Break condition:** High dead neuron fraction could amplify sparsity issues

### Mechanism 3: GELU for Smoother Gradient Flow
- **Claim:** GELU activation improves over ReLU by allowing small negative values to pass through softly
- **Evidence:** GELU is least critical component but still contributes notably over ReLU
- **Core assumption:** Softer negative handling justifies computational cost
- **Break condition:** Computational budget may not justify marginal gains over ReLU

## Foundational Learning

- **Effective Rank of Neural Representations**
  - Why needed: Used to diagnose capacity collapse and explain Hadamax's advantage
  - Quick check: Given singular values [10, 5, 0.1, 0.01, 0.001], would effective rank (δ=0.01) be closer to 2 or 5?

- **Hadamard Product (Element-wise Multiplication)**
  - Why needed: Core architectural innovation multiplies parallel activations element-wise
  - Quick check: For a=[1,2] and b=[3,4], what is their Hadamard product? How does this differ from dot product?

- **Max-Pooling vs. Strided Convolution vs. Average-Pooling**
  - Why needed: Paper argues max-pooling preserves salient features better for RL value functions
  - Quick check: For 2×2 patch [1, 3; 2, 4], what does max-pooling return? What about average-pooling?

## Architecture Onboarding

- **Component map:** Input (4×64×64 stacked frames) → normalize → Block 1: parallel Conv(32, 8×8, stride=1) → LayerNorm → GELU → Hadamard → MaxPool(4×4, stride=4) → Block 2: parallel Conv(64, 4×4, stride=1) → LayerNorm → GELU → Hadamard → MaxPool(2×2, stride=2) → Block 3: parallel Conv(64, 3×3, stride=1) → LayerNorm → GELU → Hadamard → MaxPool(3×3, stride=1) → Flatten → Dense(512) → LayerNorm → GELU → Dense(n_actions)

- **Critical path:** Parallel convolution branches remain independent until Hadamard multiplication; LayerNorm applied before activation; max-pooling handles all spatial downsampling; final pooling before flattening is also max-pooling

- **Design tradeoffs:** ~2× computational overhead due to parallel branches; 45 min vs PQN's 20 min for 40M frames; depth scaling to 5-7 layers showed no improvement; works with PQN and C51 but untested on complex model-based agents

- **Failure signatures:** High dead neuron fraction (>15%); effective rank collapse in early layers; no improvement on games requiring fine spatial discrimination

- **First 3 experiments:**
  1. Implement Hadamax on PQN-Atari-10 (40M frames) expecting ~50-70% improvement
  2. Run ablation sweep (full, no-GELU, no-Hadamard, no-max-pool) to verify max-pooling > Hadamard > GELU importance
  3. Log effective rank per layer during training to confirm Hadamax maintains higher rank in Conv2/Conv3

## Open Questions the Paper Calls Out

- **Can the Hadamax encoder be effectively scaled in width or depth to yield further improvements?**
  - Authors note that scaling to 5 or 7 layers failed; optimal scaling strategy unknown

- **Does the Hadamax encoder improve performance in sample-efficient or model-based algorithms like BBF or DreamerV3?**
  - Paper primarily validates on model-free methods with high environment interaction counts

- **Can Hadamax-PQN be augmented with exploration techniques to solve hard-exploration games?**
  - Current epsilon-greedy exploration insufficient for sparse-reward environments like Montezuma's Revenge

## Limitations

- Limited testing on complex model-based algorithms (BBF, DreamerV3)
- No exploration of optimal scaling strategies (width/depth)
- Limited theoretical grounding for why specific architectural choices outperform alternatives
- Untested on transformer-based architectures

## Confidence

- **High confidence:** Performance claims (80% gain, 90M frames to match Rainbow), effective rank measurements, dead neuron monitoring
- **Medium confidence:** Ablation ordering (max-pooling most important), GELU benefit over ReLU, Hadamard product capacity increase
- **Low confidence:** Theoretical justification for why these specific architectural choices work better than alternatives

## Next Checks

1. Apply Hadamax to a transformer-based RL agent (e.g., TransDreamerV3) to verify encoding benefits transfer beyond convolutional architectures
2. Systematically vary learning rate and LayerNorm parameters to identify thresholds where Hadamax's rank advantage degrades
3. Test on games requiring precise spatial relationships (Amidar, Hero) where max-pooling might discard critical positional information