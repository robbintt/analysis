---
ver: rpa2
title: 'Personality Expression Across Contexts: Linguistic and Behavioral Variation
  in LLM Agents'
arxiv_id: '2602.01063'
source_url: https://arxiv.org/abs/2602.01063
tags:
- personality
- high
- agents
- across
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how personality-conditioned LLM agents\
  \ adapt their linguistic, behavioral, and emotional expressions across different\
  \ conversational contexts. The research uses identical personality prompts across\
  \ four dialogue settings\u2014ice-breaking, negotiation, survival planning, and\
  \ empathy tasks\u2014and evaluates both linguistic style (via LIWC and personality\
  \ classifiers) and behavioral outcomes (agreement rates, concession patterns)."
---

# Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents

## Quick Facts
- arXiv ID: 2602.01063
- Source URL: https://arxiv.org/abs/2602.01063
- Authors: Bin Han; Deuksin Kwon; Jonathan Gratch
- Reference count: 40
- One-line primary result: Personality expression in LLM agents varies systematically across contexts, with cooperative tasks eliciting clearer trait differentiation and competitive tasks reducing personality separation.

## Executive Summary
This study investigates how personality-conditioned LLM agents adapt their linguistic, behavioral, and emotional expressions across different conversational contexts. The research uses identical personality prompts across four dialogue settings—ice-breaking, negotiation, survival planning, and empathy tasks—and evaluates both linguistic style (via LIWC and personality classifiers) and behavioral outcomes (agreement rates, concession patterns). Results show that personality expression varies systematically with context: cooperative tasks (ice-breaking, survival) elicit clearer personality differentiation and more positive emotional tone, while competitive or emotionally sensitive tasks (negotiation, empathy) reduce trait separation. Behavioral analysis confirms that personality traits influence cooperation, with agreeable and open agents showing greater concession and agreement rates in collaborative settings. These findings support a context-sensitive model of personality expression, consistent with psychological theories like Whole Trait Theory, rather than fixed trait manifestation.

## Method Summary
The study evaluates personality-conditioned LLM agents across four dialogue contexts: ice-breaking, negotiation, survival planning, and empathy tasks. Personality prompts describe High/Low levels of Big Five traits, with agents interacting against a generic baseline partner. The evaluation pipeline combines linguistic analysis (LIWC features, BERT-based personality classifier predictions, LLM-based Big Five ratings), behavioral metrics (agreement rates, concession curves), and emotional assessment (valence-arousal dimensions). Agent-agent dialogues are recorded and analyzed to determine how personality traits manifest differently across cooperative versus competitive contexts.

## Key Results
- Personality expression varies systematically by context, with cooperative tasks showing clearer trait differentiation than competitive ones
- Agreeable and open agents demonstrate higher concession and agreement rates specifically in collaborative settings
- Emotional tone and valence-arousal patterns differ across contexts, with positive and engaged affect co-occurring with clearer personality expression
- LIWC linguistic features show expected trait-related differences, but context-sensitive LLM evaluation reveals when surface-level markers don't align with deeper behavioral intent

## Why This Works (Mechanism)

### Mechanism 1: Contextual Trait Modulation
- **Claim:** If an LLM is conditioned with a specific personality, the *intensity* and *observability* of that trait appear to modulate based on the social goal of the conversation (e.g., competition vs. affiliation), rather than remaining static.
- **Mechanism:** The model likely retrieves trait-relevant behaviors but filters them through the "social goal" (Affiliation, Power, Achievement) implied by the context. In cooperative settings (Ice-breaking), the model expands its behavioral space, amplifying trait differences. In restrictive or high-stakes settings (Negotiation/Empathy), the task constraints dominate, suppressing trait differentiation.
- **Core assumption:** LLMs implicitly encode "social goals" associated with dialogue contexts and prioritize these over explicit persona instructions when conflict arises.
- **Evidence anchors:**
  - [abstract] "Cooperative tasks... elicit clearer personality differentiation... while competitive or emotionally sensitive tasks... reduce trait separation."
  - [section] Page 5, Section 5.3: "Trait differentiation was strongest in cooperative settings... and diminished under competitive or emotionally sensitive contexts."
  - [corpus] Related work (Profile-LLM) suggests dynamic profile optimization is needed for realistic expression, implying static prompting is insufficient for consistent expression across varied contexts.
- **Break condition:** This mechanism may fail if the "context" is novel or ambiguous, causing the model to default to generic helpfulness or safety refusals, effectively erasing the persona.

### Mechanism 2: Conditional Behavioral Consistency
- **Claim:** Personality traits manifest as *distributions of behavioral states* (concessions, agreement rates) rather than fixed actions, consistent with Whole Trait Theory.
- **Mechanism:** "High Agreeableness" does not strictly mean "always agrees." Instead, it increases the *probability* of cooperative moves (concession) specifically in collaborative tasks. In competitive tasks, the same trait may manifest as a different strategy (e.g., maintaining relationship harmony without full capitulation).
- **Core assumption:** The LLM has learned the statistical correlation between personality adjectives and game-theoretic outcomes from training data.
- **Evidence anchors:**
  - [abstract] "Agreeable and open agents showing greater concession and agreement rates in collaborative settings."
  - [section] Page 6, Figure 6a: Shows High Agreeableness agents concede ~40% in negotiation vs <10% for Low Agreeableness, indicating traits drive relative behavioral shifts even in conflict.
  - [corpus] "The Personality Illusion" warns of dissociation between self-reports and behavior, suggesting behavioral metrics are more robust indicators of agent "personality" than text analysis alone.
- **Break condition:** If the task logic requires strict rationality, personality effects on behavior may vanish as the model prioritizes logical correctness over social emulation.

### Mechanism 3: Affective Alignment via Valence-Arousal
- **Claim:** Personality expression is mediated by an "affective tone" (Valence/Arousal) that is jointly determined by the personality prompt and the emotional requirements of the context.
- **Mechanism:** The prompt sets a baseline (e.g., Extraversion → High Arousal/Positive), but the context imposes constraints (e.g., Empathy → Low Arousal). The output is a compromise: Extraverted agents in Empathy tasks may be warmer (higher valence) but still respect the low-arousal constraint of the sad context.
- **Core assumption:** The model represents personality not just as lexical choices, but as distinct affective states (Vector combinations of Valence and Arousal).
- **Evidence anchors:**
  - [section] Page 5, Section 5.4: "Contexts with greater personality differentiation also showed broader emotional variance... positive and engaged affect co-occurred with clearer personality expression."
  - [abstract] "Contextual cues systematically influence both personality expression and emotional tone."
  - [corpus] "Can LLMs Express Personality Across Cultures?" suggests trait expression interacts with cultural affective norms, reinforcing that affect is a modulatory layer.
- **Break condition:** Breaks down if the emotional context is extremely negative, where safety alignment overrides personality-affective coupling.

## Foundational Learning

- **Concept: Whole Trait Theory**
  - **Why needed here:** This is the theoretical lens the authors use to explain *why* LLMs vary behavior across contexts. It posits that traits are "density distributions" of states, not fixed labels. Without this, the variation looks like inconsistency/hallucination; with it, it looks like adaptability.
  - **Quick check question:** If an "Extraverted" agent becomes quiet during a library-themed roleplay, is that a failure of the prompt or a success of Whole Trait Theory logic?

- **Concept: LIWC (Linguistic Inquiry and Word Count)**
  - **Why needed here:** The study uses LIWC to provide a "context-free" baseline of personality expression. Understanding that this tool looks for specific lexical categories helps distinguish *surface-level* word choice from *deep* behavioral intent.
  - **Quick check question:** Why might LIWC be a poor measure for detecting personality in a negotiation task where "swear words" are pragmatically suppressed?

- **Concept: Agent-Agent Simulation**
  - **Why needed here:** The methodology relies on a "Personality Agent" interacting with a "Generic Agent." This setup isolates the personality effect by keeping the interlocutor constant, preventing feedback loops where two strong personalities escalate in unpredictable ways.
  - **Quick check question:** What is the primary tradeoff of using a Generic Agent instead of a Human in the evaluation loop regarding ecological validity?

## Architecture Onboarding

- **Component map:** Personality Prompt -> Context Encoder -> Agent Core -> Evaluation Suite (LIWC, BERT-Classifier, LLM-Evaluator, Behavioral Metrics)
- **Critical path:** The interaction between the **Context Encoder** and **Personality Prompt**. If the context description does not explicitly imply a "Social Goal," the personality trait may fail to activate distinct behavioral strategies.
- **Design tradeoffs:**
  - **Context-Free vs. Context-Sensitive Evaluation:** LIWC allows standardized comparison but misses intent. LLM-based evaluation captures context-sensitivity but introduces evaluator bias. The paper suggests using both to triangulate the "truth."
  - **Generic vs. Personality-Matched Partner:** Using a generic partner isolates the variable but removes the "personality-personality" dynamics found in real human interaction.
- **Failure signatures:**
  - **Trait Collapse:** Observing low differentiation between High and Low conditions in specific contexts, suggesting the task constraint is "stronger" than the persona.
  - **Behavioral Incoherence:** Linguistic markers contradicting behavioral outcomes.
- **First 3 experiments:**
  1. **Context Sensitivity Test:** Run the same "High Agreeableness" prompt across the 4 tasks. Verify if concession rates drop in Negotiation despite the "agreeable" instruction. (Expected: Yes, per Figure 6a).
  2. **Classifier Disagreement:** Compare LIWC scores vs. LLM-based evaluation scores. Identify cases where LIWC sees the trait but the LLM evaluator judges the expression as "out of context."
  3. **Partner Dynamics:** Replace the "Generic Agent" with an "Antagonistic Agent" in the Survival task. Check if "High Agreeableness" maintains high concession or adapts (defects).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed context-sensitive personality variations in LLMs result in functionally adaptive outcomes comparable to human personality adjustments?
- **Basis in paper:** [explicit] The Conclusion states, "further work is needed to determine whether these context-sensitive changes are functionally adaptive in the same way that human personality operates."
- **Why unresolved:** The study demonstrates that personality expression varies by context (supporting the descriptive part of Whole Trait Theory), but it does not validate if these variations improve interaction success or goal achievement (the functional part).
- **What evidence would resolve it:** Experiments correlating specific context-dependent personality shifts with objective metrics of task success, negotiation payoff, or partner satisfaction compared against human baselines.

### Open Question 2
- **Question:** How do interaction dynamics and personality expression change when two distinct personality-conditioned agents interact, rather than a personality agent interacting with a generic one?
- **Basis in paper:** [explicit] The Conclusion notes that the generic partner might have influenced dynamics and proposes to "explore interactions between agents with differing personalities to examine personality–personality dynamics."
- **Why unresolved:** The current experimental design held the partner's personality constant (neutral/generic) to isolate the primary agent's traits, masking any reciprocal effects that occur in human-human dyads.
- **What evidence would resolve it:** Simulations pairing agents with contrasting personalities (e.g., High Agreeable vs. Low Agreeable) to analyze emergent behavioral patterns and concession dynamics.

### Open Question 3
- **Question:** To what extent do LLM-generated behavioral trajectories align with human conversational data in similar contexts?
- **Basis in paper:** [explicit] The Conclusion proposes plans to "compare LLM-generated behaviors with human conversational data to improve real-world validity."
- **Why unresolved:** The study relies on agent-agent simulations and automated evaluation metrics, leaving the "ground truth" similarity to actual human behavior unverified.
- **What evidence would resolve it:** A comparative study benchmarking the linguistic features and concession curves of the LLM agents against human dyad datasets performing the same four tasks.

## Limitations

- The study's reliance on agent-agent simulations rather than human evaluators limits ecological validity and misses complex bidirectional personality dynamics
- Without knowing the specific base LLM model, temperature settings, and exact evaluation prompts, reproducibility faces significant challenges
- The study doesn't address potential safety alignment overrides that could suppress personality expression in emotionally sensitive contexts

## Confidence

- **High Confidence:** The finding that personality expression varies systematically with context (cooperative vs. competitive settings) is well-supported by behavioral metrics and linguistic analysis across multiple evaluation methods
- **Medium Confidence:** The claim that personality traits manifest as "distributions of behavioral states" rather than fixed actions is plausible given the behavioral data but relies on interpretation through Whole Trait Theory
- **Low Confidence:** The affective alignment mechanism (Valence-Arousal mediation) is the most speculative, as it depends on LLM-based emotion recognition which may not reliably capture nuanced emotional states

## Next Checks

1. **Cross-LLM Validation:** Replicate the study using different base models (e.g., GPT-4, Claude, open-source alternatives) to determine if context-sensitive personality expression is model-dependent or a general LLM property
2. **Human Evaluation Benchmark:** Replace the Generic Agent with human confederates in one task context to measure whether personality-conditioned LLMs produce outputs that align with human personality expression patterns in similar contexts
3. **Safety Override Detection:** Systematically test whether personality expression breaks down in contexts involving safety-sensitive content (e.g., suicidal ideation scenarios) to quantify how much safety alignment suppresses personality-driven affective expression