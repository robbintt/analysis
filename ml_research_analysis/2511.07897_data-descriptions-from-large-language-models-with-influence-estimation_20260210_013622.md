---
ver: rpa2
title: Data Descriptions from Large Language Models with Influence Estimation
arxiv_id: '2511.07897'
source_url: https://arxiv.org/abs/2511.07897
tags:
- descriptions
- images
- texts
- image
- proponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to understand data in deep
  learning via textual descriptions generated by large language models (LLMs). It
  proposes a pipeline to generate class-specific descriptions using Wikipedia and
  external knowledge, then employs influence estimation and CLIP scores to identify
  the most informative "proponent texts" for model training.
---

# Data Descriptions from Large Language Models with Influence Estimation

## Quick Facts
- arXiv ID: 2511.07897
- Source URL: https://arxiv.org/abs/2511.07897
- Reference count: 31
- Key outcome: Novel method for understanding image data through LLM-generated class descriptions, using influence estimation and CLIP scores to select informative texts for zero-shot and cross-modal transfer classification

## Executive Summary
This paper introduces a pipeline to generate human-readable textual descriptions for image classes using LLMs, then selects the most informative "proponent texts" via a novel Influence score For Texts (IFT) that combines influence estimation with CLIP similarity. The method leverages cross-modal transferability in CLIP's shared embedding space to train classifiers on image embeddings, then refine them with weighted text embeddings. Experiments across nine image classification datasets show the approach outperforms baseline methods in both zero-shot and cross-modal transfer settings, demonstrating improved model performance and interpretability through language-based explanations.

## Method Summary
The method generates class-specific descriptions using a two-stage LLM prompting process with Wikipedia URLs for external knowledge grounding. It then computes IFT scores by combining image influence scores (via TracIn) with CLIP similarity scores, selecting top-10 proponent texts per class. A linear classifier is first trained on CLIP image embeddings, then further trained on weighted proponent text embeddings using the normalized IFT scores as weights. This cross-modal transfer leverages CLIP's shared embedding space to enable text-based refinement without architectural changes.

## Key Results
- Proposed method achieves higher accuracy than baseline approaches across nine datasets in both zero-shot and cross-modal transfer settings
- Cross-modal transfer classification accuracy improves significantly when using proponent texts selected by IFT vs. random or CLIP-only selection
- GPT-4o evaluation confirms generated descriptions are more helpful, informative, and relevant compared to alternatives
- Ablation studies show both influence estimation and Wikipedia grounding contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1: Influence-Guided Text Selection (IFT Score)
The IFT score combines image influence scores (measuring causal relevance to classification) with CLIP similarity scores (measuring semantic alignment). This dual criterion identifies texts that are both semantically relevant and causally informative for the target task. The method assumes influence patterns generalize from image-to-image transfer to text-to-image utility within CLIP's shared embedding space.

### Mechanism 2: Cross-Modal Transfer in Shared Embedding Space
CLIP's contrastive pre-training creates a joint embedding space where image and text representations share the same manifold. This enables a linear classifier trained on image embeddings to be refined using text embeddings without architectural changes, as decision boundaries learned in one modality generalize to the other.

### Mechanism 3: External Knowledge Grounding for Hallucination Reduction
Providing Wikipedia URLs during LLM prompting constrains the generated descriptions to verifiable facts and visually-grounded attributes, reducing hallucinated content that could mislead the vision model. The external knowledge acts as a fact-checking mechanism for the LLM outputs.

## Foundational Learning

- **CLIP Vision-Language Embeddings**: CLIP produces shared D-dimensional embeddings where cosine similarity reflects semantic relatedness. Why needed: All text-image alignment and cross-modal transfer depends on this shared space. Quick check: Given a Blue Jay image and three texts ("a blue bird," "a red car," "quantum mechanics"), can you predict the CLIP similarity ranking?

- **Influence Functions (TracIn)**: Gradient-based influence quantifies a training sample's contribution to validation loss. Why needed: The method extends influence from images to texts; you must understand how gradient-based influence works. Quick check: If a training image has high positive influence for a validation sample, does increasing its weight increase or decrease validation loss?

- **Zero-Shot Classification with Text Prompts**: Baseline methods (Menon, CuPL, LaBo) operate in zero-shot settings. Why needed: The improvement claim is relative to these methods. Quick check: In zero-shot CLIP classification, how is the predicted class determined from class name strings and an input image?

## Architecture Onboarding

- **Component map**: LLM Description Generator (GPT-3.5 + Wikipedia) → Influence Estimator (TracIn) → CLIP Encoder → IFT Scorer → Linear Classifier (image → text refinement)
- **Critical path**: Generate descriptions → Compute influence scores → IFT ranking → Cross-modal transfer training
- **Design tradeoffs**: Checkpoint frequency (every 10 epochs for TracIn), number of proponent texts (fixed at 10), LLM choice (GPT-3.5 vs GPT-4), frozen vs. fine-tuned CLIP
- **Failure signatures**: Non-visual proponent texts, cross-modal training degradation, hallucination persistence with Wiki
- **First 3 experiments**: 1) Baseline sanity check: Train linear classifier on CLIP image embeddings only. 2) IFT vs. ablations: Compare cross-modal transfer accuracy using different selection methods. 3) Embedding alignment visualization: Plot t-SNE of image vs. text embeddings for 3 classes.

## Open Questions the Paper Calls Out

1. **Computational cost reduction**: Can influence estimation be made efficient for large-scale datasets? (Section 6 states computational load increases with dataset size)
2. **Prompt and LLM sensitivity**: How robust is the pipeline to different prompts and LLM choices? (Section 6 notes descriptions vary with prompts and models)
3. **Bias mitigation**: Does Wikipedia grounding effectively reduce deeper representational or social biases? (Section 6 acknowledges external knowledge may not fully resolve embedded biases)

## Limitations

- Computational cost scales poorly with dataset size due to influence estimation requirements
- Performance depends on Wikipedia coverage, with potential degradation for classes lacking detailed articles
- Method may not fully resolve deeper representational or social biases embedded in LLM pretraining data

## Confidence

- **High confidence**: IFT selection + cross-modal transfer works better than baselines (supported by Table 3)
- **Medium confidence**: Influence component adds value beyond CLIP similarity (IFT outperforms CLIP-only but influence-only also works)
- **Low confidence**: Wikipedia grounding's specific contribution vs. general LLM quality (difference is consistent but modest)

## Next Checks

1. **Influence ablation study**: Compare IFT vs. CLIP-only vs. influence-only selection across all 9 datasets to quantify influence component contribution
2. **Embedding alignment quantification**: Measure class-wise cosine similarity distributions between image and proponent text embeddings to verify cross-modal alignment quality
3. **Knowledge grounding stress test**: Identify classes with minimal Wikipedia coverage and measure performance degradation compared to well-covered classes