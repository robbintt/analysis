---
ver: rpa2
title: Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization
arxiv_id: '2506.00448'
source_url: https://arxiv.org/abs/2506.00448
tags:
- hallucinations
- clinical
- dataset
- facts
- transcript
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination detection in clinical text summarization
  by introducing a fact-controlled Leave-N-out dataset and a natural hallucination
  dataset annotated by clinical experts. It evaluates existing methods, finding that
  general-domain detectors struggle with clinical hallucinations, and proposes fact-based
  approaches that count hallucinations with greater explainability.
---

# Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization

## Quick Facts
- **arXiv ID:** 2506.00448
- **Source URL:** https://arxiv.org/abs/2506.00448
- **Reference count:** 0
- **Primary result:** Fact-based hallucination detection methods (particularly fact alignment) outperform general-domain detectors on clinical text summarization, achieving correlations up to 0.43 on synthetic data and 0.37 on real-world data.

## Executive Summary
This paper addresses hallucination detection in clinical text summarization by introducing a fact-controlled Leave-N-out dataset and a natural hallucination dataset annotated by clinical experts. It evaluates existing methods, finding that general-domain detectors struggle with clinical hallucinations, and proposes fact-based approaches that count hallucinations with greater explainability. The authors develop LLM-based methods, particularly fact alignment, which outperform traditional metrics and generalize well to real-world clinical hallucinations. Their methods achieve correlations up to 0.43 on synthetic data and 0.37 on real-world data, providing a suite of specialized metrics supported by expert-annotated datasets to advance faithful clinical summarization systems.

## Method Summary
The paper develops hallucination detection methods that extract atomic facts from both source transcripts and generated summaries, then align these facts to identify unsupported claims. The core approach uses Claude Sonnet 3.5 to decompose medical summaries and transcripts into atomic facts, then applies either LLM-based alignment (prompting the model to match summary facts against transcript facts) or embedding-based alignment (using MiniLM or BioBERT with a 0.75 cosine similarity threshold). The system counts unsupported summary facts as hallucinations, providing an explainable count rather than aggregate scores. The authors evaluate their methods on both synthetic hallucinations (generated by systematically removing facts from source dialogues) and expert-annotated natural hallucinations from real clinical summaries.

## Key Results
- General-domain hallucination detectors (SummaC, FactScore, AlignScore) show weak correlation on clinical data, with FENICE dropping from 0.45 to 0.10 when moving from synthetic to natural hallucinations.
- Fact-based methods, particularly fact alignment, outperform traditional metrics with correlations up to 0.43 on synthetic data and 0.37 on natural hallucinations.
- Fact alignment performs best by focusing on core facts, reducing noise from other textual information, and simplifying analysis.
- The paper provides expert-annotated datasets (LNO and NH) that enable systematic evaluation of hallucination detection methods in clinical settings.

## Why This Works (Mechanism)

### Mechanism 1
Systematic fact removal from source transcripts creates controlled, reproducible hallucination scenarios for detector development. An LLM identifies atomic facts from the summary, selects N orthogonal facts, then rewrites the transcript to remove these facts while preserving conversational flow. The summary remains unchanged, creating a guaranteed hallucination when the model references removed information. Core assumption: synthetic hallucinations induced by fact removal approximate the characteristics of naturally occurring hallucinations sufficiently for detector training.

### Mechanism 2
Decomposing evaluation into atomic fact verification with explicit alignment provides more robust and explainable hallucination detection than aggregate scoring metrics. Extract atomic facts from transcript and summary separately → align each summary fact against transcript facts using LLM judgment or embedding similarity → count unsupported facts as hallucinations. This reduces noise by narrowing the comparison scope to factual claims. Core assumption: Atomic facts can be reliably extracted and unambiguously matched; partial matches can be distinguished from hallucinations.

### Mechanism 3
Domain-specific clinical hallucination detection requires specialized approaches; general-domain detectors fail to capture clinically relevant errors. General-domain detectors trained on news or open-domain text lack sensitivity to medical terminology relationships, clinical reasoning patterns, and the severity hierarchy of different error types. Core assumption: Clinical hallucinations have domain-specific characteristics that general detectors are not trained to recognize.

## Foundational Learning

- **Concept: SOAP Note Structure (Subjective, Objective, Assessment, Plan)**
  - Why needed here: The ACI-Bench dataset uses this structured clinical documentation format. Understanding that transcripts contain layman's terms while summaries use medical terminology helps explain the alignment challenge.
  - Quick check question: Can you identify which section of a SOAP note would contain "patient reports knee pain" vs. "differential diagnosis includes osteoarthritis"?

- **Concept: Hallucination Taxonomy (Hallucination, Inference, Misunderstanding, No Factual Error)**
  - Why needed here: The Natural Hallucination dataset uses this four-category annotation scheme. Not all errors are equal; inferences may be clinically reasonable while hallucinations are fabrications.
  - Quick check question: If a summary states "patient likely has diabetes" based on family history discussion but no explicit diagnosis, is this a hallucination or inference?

- **Concept: High-Severity vs. Low-Severity Clinical Errors**
  - Why needed here: The paper evaluates detection specifically on high-severity categories (Diagnosis, Exam Findings, Labs, Medical History, Symptoms, Treatment Plan) excluding Age & Sex. This reflects clinical risk prioritization.
  - Quick check question: Why might mistaking a patient's age be less clinically dangerous than fabricating a treatment plan?

## Architecture Onboarding

- **Component map:** ACI-Bench (source) → LNO Generator (LLM fact removal + human correction) / NH Generator (multi-model summaries + expert annotation) → Fact Extraction Layer (LLM-based atomic fact decomposition) → Alignment Layer (LLM Alignment or Embedding Alignment) → Counting Layer (Unsupported fact enumeration) → Evaluation Layer (Pearson correlation against ground truth)

- **Critical path:** 1. Start with LNO dataset to validate monotonic response (metric should increase/decrease with N) 2. Test generalization on NH dataset (real-world performance) 3. Validate on XSum cross-domain (generalizability beyond medicine)

- **Design tradeoffs:** MiniLM vs. BioBERT: MiniLM (80MB, 6 layers) offers faster inference; BioBERT (400MB, 12 layers) provides domain-specific understanding. Paper found comparable performance, favoring efficiency. Single Prompt vs. Chain-of-Prompts: Single prompt overloads context; chain-of-prompts separates extraction and alignment for better accuracy at cost of latency. Fact Extraction vs. Full Transcript Lookup: Full transcript retains context but degrades performance (0.22 vs. 0.43 correlation), likely due to reduced information density.

- **Failure signatures:** Metric shows weak correlation on NH (< 0.15) despite strong LNO performance → detector overfits to synthetic patterns. High variance across trials (large S.D.) → unstable prompt sensitivity or randomness in LLM extraction. Undercounting hallucinations → fact extraction prompt failing to decompose compound statements.

- **First 3 experiments:** 1. Replicate baseline correlations on LNO using SummaC and AlignScore to establish performance floor before implementing fact-based methods. 2. Implement Summary + Transcript Fact Extract + LLM Alignment pipeline; compare correlation against Single Prompt Counting on both LNO and NH. 3. Ablate embedding threshold (try 0.65, 0.75, 0.85) on held-out split to validate 0.75 threshold generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
How can contextual information be more effectively incorporated into hallucination detection methods to improve the performance of transcript lookup approaches? Basis in paper: Section 4.3 states that using the full transcript led to poorer results than expected and that "Further research is needed to explore alternative approaches for incorporating context that might improve the performance of transcript lookup methods."

### Open Question 2
Do fact-based hallucination detection metrics generalize to clinical document formats beyond SOAP notes, such as discharge summaries or progress reports? Basis in paper: Section 6 (Limitations) notes that the study "focuses solely on SOAP notes, excluding other clinical documents like discharge summaries or progress reports."

### Open Question 3
How can the definition and extraction of "atomic facts" be improved to prevent the undercounting of hallucinations in complex medical statements? Basis in paper: Section 6 (Limitations) states that "Fact based analysis is limited by the definition of what is a fact" and notes that prompting models to extract atomic facts "often didn't work leading to undercounting of hallucinations."

## Limitations
- The study focuses solely on SOAP notes, excluding other clinical documents like discharge summaries or progress reports.
- Fact-based analysis is limited by the definition of what constitutes an atomic fact, leading to potential undercounting of hallucinations.
- Performance on synthetic hallucinations (LNO) does not reliably predict effectiveness on natural hallucinations (NH).

## Confidence

- **High confidence**: The core observation that general-domain detectors underperform on clinical hallucinations. This is directly supported by quantitative results showing FENICE's correlation drops from 0.45 to 0.10 when moving from LNO to NH datasets.
- **Medium confidence**: The proposed fact-based methods' superiority over traditional metrics. While correlations (0.43 on LNO, 0.37 on NH) exceed existing methods, the gap between LNO and NH performance suggests the approach may be overfitting to synthetic patterns.
- **Medium confidence**: The effectiveness of fact alignment over embedding alignment. The paper reports fact alignment performs best, but provides limited ablation studies comparing different alignment strategies systematically.

## Next Checks
1. **Cross-dataset transferability test**: Apply the fact-based hallucination detection pipeline to a non-medical summarization dataset (e.g., news articles) to assess whether the approach generalizes beyond clinical domains or is specifically tuned to medical fact structures.
2. **Expert annotation consistency study**: Have multiple clinical experts independently annotate the same set of natural hallucinations to measure inter-annotator agreement and assess whether the four-category scheme provides sufficient granularity for reliable detection.
3. **Threshold sensitivity analysis**: Systematically vary the cosine similarity threshold (0.65, 0.70, 0.75, 0.80, 0.85) on the LNO dataset and plot detection performance curves to identify optimal thresholds and assess robustness to parameter selection.