---
ver: rpa2
title: Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning
arxiv_id: '2601.12816'
source_url: https://arxiv.org/abs/2601.12816
tags:
- fisher
- task
- fopng
- learning
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FOPNG (Fisher-Orthogonal Projected Natural
  Gradient Descent), a continual learning optimizer that reduces catastrophic forgetting
  by projecting gradients onto the Fisher-orthogonal complement of previous task gradients.
  Unlike existing methods that operate in Euclidean parameter space, FOPNG enforces
  orthogonality constraints in the Fisher-Riemannian manifold, providing reparameterization
  invariance and better preservation of prior task performance.
---

# Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning

## Quick Facts
- arXiv ID: 2601.12816
- Source URL: https://arxiv.org/abs/2601.12816
- Authors: Ishir Garg; Neel Kolhe; Andy Peng; Rohan Gopalam
- Reference count: 21
- This paper proposes FOPNG, a continual learning optimizer that significantly outperforms baselines including EWC and OGD in most cases, with particularly strong performance on tasks where consecutive tasks have distributional similarity.

## Executive Summary
This paper introduces Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG), a continual learning method that projects gradients onto the Fisher-orthogonal complement of previous task gradients. Unlike existing methods that operate in Euclidean parameter space, FOPNG enforces orthogonality constraints in the Fisher-Riemannian manifold, providing reparameterization invariance and better preservation of prior task performance. The method combines natural gradient descent with orthogonal gradient methods within an information-geometric framework, using a diagonal Fisher approximation for efficiency.

## Method Summary
FOPNG operates by computing natural gradients on new-task data, then projecting these gradients onto the Fisher-orthogonal complement of gradients stored from previous tasks. The method maintains an exponentially weighted moving average of the Fisher information matrix across tasks and stores a fixed number of gradients per task. Each update is normalized to satisfy a trust-region constraint in Fisher norm, bounding the KL divergence induced by the update. The diagonal Fisher approximation enables tractable computation while maintaining the geometric benefits of natural gradient methods.

## Key Results
- FOPNG significantly outperforms baselines including EWC and OGD on standard benchmarks (Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100)
- Particularly strong performance on tasks where consecutive tasks have distributional similarity
- Remarkably stable across hyperparameter choices, requiring minimal tuning
- Shows robustness to Fisher batch sizes as small as 16 samples

## Why This Works (Mechanism)

### Mechanism 1
Projecting gradients onto the Fisher-orthogonal complement of previous task gradients reduces catastrophic forgetting while allowing new task acquisition. The Fisher information matrix defines a Riemannian metric where Fisher norm directly approximates KL divergence. By constraining updates to directions Fisher-orthogonal to prior task gradients, the method limits changes to prior task output distributions.

### Mechanism 2
Trust region normalization in Fisher norm bounds distribution shift per update step. By enforcing ||v||_F_new = η, each update step has bounded KL impact on the current model distribution, providing stability across task transitions regardless of gradient magnitude.

### Mechanism 3
Reparameterization invariance makes optimization robust to architecture-specific parameter scaling. Fisher norm is invariant under smooth reparameterizations, so FOPNG behaves consistently regardless of how parameters are scaled or transformed.

## Foundational Learning

- **Fisher Information Matrix**
  - Why needed here: Core geometric structure defining all projections and norms. Implementation uses diagonal approximation for tractability.
  - Quick check question: Why might diagonal Fisher fail to capture parameter correlations that affect forgetting?

- **Natural Gradient Descent**
  - Why needed here: FOPNG builds on natural gradient as the base descent direction before applying orthogonal projection.
  - Quick check question: How does natural gradient differ from Euclidean gradient in convergence behavior on ill-conditioned loss surfaces?

- **Orthogonal Projection in Metric Spaces**
  - Why needed here: The projection operator P projects onto Fisher-orthogonal complement, which differs from Euclidean orthogonality.
  - Quick check question: Given metric F, write the projection of v onto the F-orthogonal complement of subspace spanned by columns of G.

## Architecture Onboarding

- **Component map:**
  - Gradient memory buffer -> Fisher module -> Projection operator P -> Trust region normalizer -> Model parameters

- **Critical path:**
  1. Task 1: Train with standard optimizer (SGD/Adam)
  2. After Task 1: Compute Fold, store initial gradients
  3. For each subsequent task:
     - Recompute Fnew per epoch
     - Per batch: gradient → project via P → normalize → update
     - Post-training: update Fold, append gradients to buffer

- **Design tradeoffs:**
  - Diagonal vs. KFAC Fisher: diagonal is O(p) memory but loses correlations; KFAC captures structure but adds complexity
  - Gradients per task (k): larger k improves projection accuracy but increases memory and computation
  - Fisher batch size: larger improves estimation but slows training; FOPNG is robust to batch sizes as small as 16
  - Regularization λ (inversion damping): too small → numerical instability; too large → distorts geometry

- **Failure signatures:**
  - NaN/Inf in update: matrix inversion on ill-conditioned G^T F_old F_new^{-1} F_old G → increase λ
  - Rapid forgetting on dissimilar tasks: projection may over-constrain; consider hybrid approaches
  - Training time grows linearly with task count: gradient buffer operations don't scale to 100+ tasks

- **First 3 experiments:**
  1. Reproduce Split-MNIST or Rotated-MNIST (5 tasks, MLP) to validate implementation matches paper accuracy curves
  2. Ablate Fisher batch size [1, 16, 64, 256, full] on Split-CIFAR10 to confirm robustness claim
  3. Compare FOPNG vs FOPNG-PreFisher on 10-task Split-CIFAR100 to measure computational tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can the gradient buffer required by FOPNG be compressed via low-rank approximations to use constant space without degrading performance? The current method stores a fixed number of gradients per task, causing memory to scale linearly with the number of tasks.

### Open Question 2
Do non-diagonal Fisher approximations, such as KFAC or EKFAC, significantly improve the curvature modeling of FOPNG compared to the diagonal approximation? The paper only implements the diagonal Fisher due to computational constraints.

### Open Question 3
Why does FOPNG underperform on benchmarks like Permuted-MNIST, and does this indicate a fundamental limitation with Fisher-orthogonality on highly dissimilar task distributions? The paper observes poor performance but does not confirm the geometric cause.

## Limitations
- Scalability to very long task sequences (>100 tasks) due to linear growth of gradient buffer
- Diagonal Fisher approximation may miss important parameter correlations that affect forgetting
- Performance sensitivity to task similarity, particularly poor results on highly dissimilar tasks like Permuted-MNIST

## Confidence

- **High Confidence**: Reparameterization invariance (Theorem 2.1 proof is complete and rigorous)
- **High Confidence**: Trust region mechanism bounds KL divergence per update (Theorem 3.2 provides formal guarantee)
- **Medium Confidence**: Diagonal Fisher approximation provides sufficient geometry for effective projection (empirical evidence strong but no theoretical bounds)
- **Medium Confidence**: Performance gains over EWC and OGD baselines (robust across benchmarks but sensitivity to task similarity exists)

## Next Checks

1. Test FOPNG on 50-100 task sequences (e.g., Split-CIFAR100 with 50 tasks) to measure scalability and identify performance degradation points
2. Compare diagonal vs. KFAC Fisher approximation on a moderate-sized benchmark (10 tasks) to quantify impact of parameter correlation modeling
3. Conduct ablation study on Fisher batch size (1, 8, 16, 32, 64) across all benchmark tasks to map the tradeoff between estimation accuracy and computational efficiency