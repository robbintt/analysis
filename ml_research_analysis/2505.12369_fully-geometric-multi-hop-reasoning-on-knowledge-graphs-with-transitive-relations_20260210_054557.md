---
ver: rpa2
title: Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations
arxiv_id: '2505.12369'
source_url: https://arxiv.org/abs/2505.12369
tags:
- embedding
- queries
- geometric
- transitive
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GeometrE introduces a fully geometric approach to multi-hop reasoning\
  \ on knowledge graphs, addressing the interpretability gap in existing methods that\
  \ rely on neural networks for logical operations. The core innovation lies in mapping\
  \ all logical operations\u2014intersection, projection, and negation\u2014to pure\
  \ geometric transformations using box embeddings, eliminating the need for learnable\
  \ neural components."
---

# Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations

## Quick Facts
- arXiv ID: 2505.12369
- Source URL: https://arxiv.org/abs/2505.12369
- Authors: Fernando Zhapa-Camacho; Robert Hoehndorf
- Reference count: 30
- Primary result: Achieves state-of-the-art MRR on WN18RR-QA (52.6 on 1-hop queries) using purely geometric operations

## Executive Summary
GeometrE introduces a fully geometric approach to multi-hop reasoning on knowledge graphs, addressing the interpretability gap in existing methods that rely on neural networks for logical operations. The core innovation lies in mapping all logical operations—intersection, projection, and negation—to pure geometric transformations using box embeddings, eliminating the need for learnable neural components. Additionally, GeometrE incorporates a transitive loss function that preserves the logical rule ∀a, b, c : r(a, b) ∧ r(b, c) → r(a, c) by enforcing order-preserving embeddings along specific dimensions. Experiments on standard benchmarks (WN18RR-QA, NELL-QA, FB15k-237) demonstrate that GeometrE outperforms state-of-the-art methods across most query types, achieving up to 52.6 MRR on 1-hop queries in WN18RR. The transitive loss function further improves embedding quality for transitive relations, with Spearman correlation scores increasing from 0.32 to 0.88 for hypernym chains. While the negation approximation works well for WN18RR and NELL, results on FB15k-237 indicate dataset-dependent limitations, suggesting future work on more robust negation modeling.

## Method Summary
GeometrE represents entities as n-dimensional boxes (center, offset) and relations as 4-tuple transformations (r₁, r₂, r₃, r₄) that scale and translate these boxes. The method implements geometric intersection via coordinate-wise max/min operations, projection through hybrid additive-multiplicative transformations, and negation through approximation mappings. A transitive loss function enforces order-preserving embeddings along dedicated dimensions for transitive relations, preserving logical chain properties. The model trains using margin-based ranking loss with negative sampling, achieving state-of-the-art performance while maintaining full geometric interpretability without learnable neural operators.

## Key Results
- Achieves 52.6 MRR on 1-hop queries in WN18RR, outperforming state-of-the-art methods
- Spearman correlation for hypernym chains improves from 0.32 to 0.88 with transitive loss
- Geometric intersection provides O(k·n) complexity versus O(k·d²) for MLP-based approaches
- Negation approximation works well on WN18RR and NELL but shows limitations on FB15k-237

## Why This Works (Mechanism)

### Mechanism 1: Fixed Geometric Intersection
- Claim: Intersection operations can be computed via coordinate-wise max/min without learnable parameters, preserving full geometric interpretability.
- Mechanism: Given query boxes {B₁, ..., Bₙ}, the intersection I = (max(l_B₁, ..., l_Bₙ), min(u_B₁, ..., u_Bₙ)). This is a closed-form operation with O(k·n) complexity, compared to O(k·(n·d + L·d²)) for MLP-based intersections.
- Core assumption: Query regions that are semantically related will have overlapping box representations in the embedding space.
- Evidence anchors:
  - [abstract] "does not require learning the logical operations and enables full geometric interpretability"
  - [section 4, Equation 3] Defines intersection formula explicitly; claims "geometric definition also provides direct geometric interpretability, which is not the case with neural networks"
  - [corpus] Corpus provides weak direct evidence; Paper 2537 notes existing methods "rely on embedding entities and relations into continuous geometric spaces" but does not evaluate fixed geometric operators specifically.
- Break condition: If two semantically related queries yield disjoint boxes, the intersection returns an empty region regardless of their logical relationship.

### Mechanism 2: Hybrid Additive-Multiplicative Relation Projection
- Claim: Relation embeddings with both multiplicative (r₁, r₃) and additive (r₂, r₄) components improve embedding quality over purely additive projections.
- Mechanism: Given box B = (c_B, o_B), the transformation T_r produces B' = (r₁ ⊗ c_B + r₂, |r₃ ⊗ o_B + r₄|). Query2Box uses the restricted form r = (1, r₂, 1, r₄).
- Core assumption: Multiplicative scaling of centers and offsets captures relation-specific transformations that pure translation cannot.
- Evidence anchors:
  - [section 4, Equation 2] Explicit formula for geometric projection
  - [section 5.2, Table 6] Ablation shows GeometrE-m (multiplicative only) achieves 50.5 vs 36.3 for GeometrE-a (additive only) on WN18RR 1p queries; full model achieves 52.6
  - [corpus] No direct corpus evidence for this specific mechanism; related work on geometric embeddings (Paper 2537) does not compare additive vs. multiplicative components.
- Break condition: If multiplicative factors r₁ or r₃ become too large or too small, boxes may collapse or expand excessively, destabilizing training.

### Mechanism 3: Dimension-Reserved Transitive Loss
- Claim: Assigning each transitive relation a dedicated ordering dimension with an order-preserving loss function enables the embedding space to capture transitive chains.
- Mechanism: For transitive relation r with assigned dimension i, the loss includes dist_ordering(q[i], a[i]) = ||max(c'_a[i] - c_q[i] + λ, 0)||₁, enforcing a_answer[i] > b_answer[i] > c_answer[i] for chain a → b → c. The transitive loss L_{r_i} = ||r̂₁ᵢ - 1||₁ + ||r̂₃ᵢ - 1||₁ + ||r̂₂ᵢ||₁ + ||r̂₄ᵢ||₁ pushes T_r toward identity in dimension i.
- Core assumption: Transitive chains can be linearly ordered along a single dimension while other dimensions capture standard box containment.
- Evidence anchors:
  - [section 4.1, Equations 7-9, 11] Formal definition of dist_box_tr and transitive loss
  - [section 5.2, Table 5] Spearman correlation for hypernym chains improves from 0.32 (no transitive loss) to 0.88 (with transitive loss); for has_part: 0.27 to 0.73
  - [section 5.2, Theorem 1] Proves that under the transitive loss, if dist_box_tr(a_query, b_answer) ≈ 0 and dist_box_tr(b_query, c_answer) ≈ 0, then dist_box_tr(a_query, c_answer) ≈ 0
  - [corpus] Paper 58918 notes "robust reasoning about transitive relations is instrumental in many settings" but does not address geometric embedding approaches to transitivity.
- Break condition: If the number of transitive relations exceeds embedding dimensions, or if chain depth exceeds the separation capacity (λ spacing), ordering constraints may conflict.

## Foundational Learning

- Concept: **Box Embeddings**
  - Why needed here: GeometrE represents all queries and entities as n-dimensional boxes (center, offset) rather than points, enabling set-like operations (containment, intersection).
  - Quick check question: Can you explain why a box representation naturally supports multi-answer queries better than point embeddings?

- Concept: **First-Order Logic Queries on KGs**
  - Why needed here: The method handles DNF queries with ∃, ∧, ∨, ¬ operators; understanding query structure (anchor entities, bound variables, target variable) is prerequisite for implementing the geometric pipeline.
  - Quick check question: Given a 2p query "entities connected to anchor a via relation r₁, then via r₂," what would the computation graph look like?

- Concept: **Idempotent Transformations**
  - Why needed here: Transitivity (r ∘ r ≡ r) requires relation transformations that produce identical results when applied multiple times; understanding idempotency is essential for the transitive loss design.
  - Quick check question: Why does the paper restrict idempotent transformations to identity or constant functions (Lemma 1)?

## Architecture Onboarding

- Component map: Entity embeddings (center, offset) → Relation projections (multiplicative/additive) → Geometric intersection (max/min) → Negation approximation → Union (DNF) → Distance scoring (dist_box or dist_box_tr)
- Critical path: 1) Parse query into computation graph 2) Initialize anchor embeddings → apply relation projections → compute intersections → handle negations via approximation (Table 1) 3) Score candidate answers via dist_box or dist_box_tr 4) Backprop through entity/relation embeddings (operators are fixed)
- Design tradeoffs:
  - Box model vs. cones: Boxes naturally support intersection but not negation (complement is not a box); cones support negation but intersection may produce disconnected regions
  - Separate vs. shared answer embeddings: WN18RR benefits from separate (Table 9: 52.5 vs 52.0 on 1p); NELL/FB237 prefer shared
  - Geometric vs. neural operators: Gains interpretability and O(k·n) complexity, but loses flexibility for complex operation composition
- Failure signatures:
  - Empty intersection regions for semantically related queries → check box size and overlap
  - Poor negation query performance → verify approximation mapping (Table 1) is implemented correctly
  - Transitive chain ordering failures → check dist_ordering term is non-zero and λ>0
- First experiments: 1) Verify geometric intersection returns correct coordinates for simple 2-box overlap 2) Test relation projection T_r preserves box properties (positive offsets) 3) Confirm transitive loss increases Spearman correlation for hypernym chains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can geometric embedding methods achieve competitive performance on negation-heavy queries without relying on neural networks or approximations?
- Basis in paper: [explicit] The authors state: "our method has limitations, particularly for negation queries, where future work should explore transitions to cones or alternative geometric representations that naturally support negation operations." The mixed FB237 negation results (Table 3) further highlight this limitation.
- Why unresolved: Box models cannot represent exact negation since the complement of a box is not a box. GeometrE relies on an approximation where negated queries followed by intersection are simplified, which may not capture the full semantics of negation.
- What evidence would resolve it: Developing a hybrid geometric model (e.g., combining boxes with cones) that achieves consistent improvements across all negation query types (2in, 3in, inp, pin, pni) on all three benchmarks without neural components.

### Open Question 2
- Question: Does the transitive loss function generalize effectively to other relational properties such as symmetry, invertibility, and general composition?
- Basis in paper: [inferred] The paper mentions four relational properties (symmetry, invertibility, composition, transitivity) in Section 3 but only addresses transitivity. The authors note: "A promising direction for future research would be to investigate different transformation types and loss functions in alternative geometric spaces to further improve the representation of relational properties."
- Why unresolved: The transitive loss function is specifically designed for chain-like ordering structures. It remains unclear whether similar geometric constraints can encode symmetry (bidirectional equivalence) or general composition (r ∘ s = t) while maintaining the idempotency properties.
- What evidence would resolve it: Extending GeometrE with specialized loss functions for symmetry and composition, then demonstrating improved MRR on relation-specific query subsets containing symmetric or compositional patterns.

### Open Question 3
- Question: What is the trade-off between the number of dedicated transitive dimensions and embedding quality as the number of transitive relations scales?
- Basis in paper: [inferred] The paper assigns "each transitive relation a different projection dimension" for the ordering constraint, but the experiments only evaluate two transitive relations (hypernym, has_part). With many transitive relations, dimension allocation may become a bottleneck.
- Why unresolved: The method requires at least one dedicated dimension per transitive relation (or relation-inverse pair). The impact on embedding capacity and performance when handling dozens of transitive relations remains unexplored.
- What evidence would resolve it: Experiments on knowledge graphs with varying numbers of transitive relations (e.g., 5, 10, 20, 50), analyzing both MRR degradation and the Spearman correlation of transitive chains as dimension scarcity increases.

## Limitations

- Negation approximation shows poor performance on FB15k-237, suggesting dataset-specific limitations
- Transitive relation identification and dimension assignment strategy are underspecified, potentially affecting reproducibility
- Fixed geometric operators may limit the model's ability to learn complex logical compositions

## Confidence

- High: Geometric intersection mechanism and its computational advantages (O(k·n) vs O(k·d²))
- Medium: Hybrid additive-multiplicative relation projection performance gains
- Medium: Transitive loss function effectiveness, though implementation details are sparse

## Next Checks

1. Implement ablation studies on FB15k-237 to characterize negation approximation failures across different query types
2. Test multiple dimension assignment strategies for transitive relations to identify optimal configuration
3. Evaluate the impact of geometric operator rigidity by comparing against learned neural operators on complex query compositions