---
ver: rpa2
title: 'ScheduleMe: Multi-Agent Calendar Assistant'
arxiv_id: '2509.25693'
source_url: https://arxiv.org/abs/2509.25693
tags:
- agent
- user
- calendar
- event
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ScheduleMe, a multi-agent calendar assistant
  leveraging large language models to manage Google Calendar events through natural
  language. The system employs a graph-structured coordination mechanism where a central
  supervisory agent delegates tasks to specialized agents (scheduling, editing, deletion,
  availability checking) via LangGraph.
---

# ScheduleMe: Multi-Agent Calendar Assistant

## Quick Facts
- arXiv ID: 2509.25693
- Source URL: https://arxiv.org/abs/2509.25693
- Reference count: 19
- Multi-agent system achieves 65-100% task success across 6 languages

## Executive Summary
ScheduleMe presents a multi-agent calendar assistant leveraging large language models and graph-structured coordination to manage Google Calendar events through natural language. The system uses a central supervisory agent to delegate tasks to specialized agents (scheduling, editing, deletion, availability checking) via LangGraph, with each agent employing ReAct reasoning to invoke Google Calendar API tools. Zero-shot multilingual evaluation across six languages shows high task success rates, with strongest performance in English and European languages. A user study confirms high usability and satisfaction scores, demonstrating how structured agent coordination can enhance calendar management tools.

## Method Summary
The system implements a graph-structured coordination mechanism using LangGraph where a central supervisory agent delegates to specialized agents for calendar operations. Each agent follows the ReAct paradigm, combining reasoning with tool invocation of Google Calendar API wrappers. The architecture uses GPT-4o mini via LangChain, FastAPI backend, and Streamlit frontend with OAuth 2.0 authentication. Implementation requires setting up Google Cloud credentials, implementing five calendar tools, constructing the LangGraph StateGraph with supervisor routing, building API endpoints, and creating the UI interface.

## Key Results
- Task success rates range from 65-100% across six languages, with European languages performing best
- User study (n=20) shows high usability (SUS 82.5), trust (4.3/5), and satisfaction (4.6/5)
- Zero-shot multilingual capability demonstrates generalization across languages without task-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Graph-Structured Agent Orchestration
- Claim: State-aware graph coordination improves task routing and error recovery in multi-agent systems
- Mechanism: LangGraph represents agents as nodes with the supervisor parsing user intent and routing to specialized agents based on parameters and state
- Core assumption: User intent maps cleanly to single-agent capabilities; ambiguity can be resolved through supervisor-mediated clarification
- Evidence anchors: Abstract confirms graph coordination via LangGraph; section 3.1 details StateGraph structure; Auto-SLURP validates multi-agent frameworks

### Mechanism 2: ReAct-Based Tool Invocation
- Claim: ReAct enables agents to decompose calendar operations into tool calls with intermediate reasoning
- Mechanism: Agents reason about required tools, invoke Google Calendar API wrappers, and return results with mandatory workflow steps
- Core assumption: ReAct prompts generalize zero-shot across languages without fine-tuning
- Evidence anchors: Section 3.1 describes ReAct paradigm; section 5.3 reveals language-dependent reliability; AURA confirms ReAct effectiveness for tool use

### Mechanism 3: Centralized Supervision with Clarification Loops
- Claim: Single-point user interaction with supervised delegation improves transparency and reduces ambiguity
- Mechanism: Supervisor collects all required parameters before delegating, triggering clarification loops for missing information
- Core assumption: Users provide accurate information in follow-up questions; session context persists across turns
- Evidence anchors: Section 3.1 details supervisor-mediated communication; Appendix A shows clarification prompt; ECLAIR addresses clarification in enterprise assistants

## Foundational Learning

- **Concept: LangGraph StateGraph**
  - Why needed here: Provides stateful, cyclic workflows for multi-turn scheduling dialogs enabling backtracking and conditional branching
  - Quick check question: Can you sketch a 3-node graph with a conditional edge that loops back to the supervisor on missing parameters?

- **Concept: ReAct Prompting**
  - Why needed here: Enables agents to reason about tool selection rather than blindly calling APIs, reducing scheduling errors
  - Quick check question: Given a prompt enforcing "check conflicts before scheduling," what happens if the conflict tool returns an error?

- **Concept: OAuth 2.0 + API Wrapper Pattern**
  - Why needed here: Secure calendar access requires token-based authentication; wrappers decouple agent logic from API changes
  - Quick check question: If an agent's tool call fails due to expired OAuth token, where should retry logic live—in the agent or the tool wrapper?

## Architecture Onboarding

- **Component map:**
  User (Streamlit UI) → Supervisor Agent (GPT-4o mini + LangChain) → Specialized Agents [Scheduler | Editor | Deleter | Availability Checker] → Tool Layer [check_conflicts, create_event, update_event, delete_event, check_availability] → Google Calendar API (OAuth 2.0)

- **Critical path:** User query → Supervisor intent classification → Parameter extraction → Agent delegation → Tool execution → API call → Response formatting → User confirmation. Conflict-checking step in scheduling is mandatory.

- **Design tradeoffs:**
  - Centralized supervisor simplifies debugging but creates bottleneck
  - JSON-based state persistence (single-user) vs. Redis (multi-user) adds operational complexity
  - Zero-shot multilingual capability trades off with accuracy in non-Latin scripts

- **Failure signatures:**
  - Translation drift: Non-English event titles normalized to English, causing retrieval failures
  - Excessive clarification: Ambiguous temporal expressions trigger multiple follow-up questions
  - Entity confusion: Similar event titles lead to wrong event edits/deletions

- **First 3 experiments:**
  1. Intent routing validation: Submit 20 queries per intent type in English, verify agent delegation accuracy
  2. Conflict detection stress test: Attempt to schedule overlapping events, confirm mandatory conflict check
  3. Multilingual degradation probe: Run 5 scheduling requests each in Tamil and Sinhala, compare error patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can semantic drift in non-Latin scripts be mitigated to improve entity matching accuracy in editing and deletion tasks?
- Basis in paper: Section 5.3 identifies "Translation-Induced Errors" and semantic drift as primary failure modes for Tamil, Sinhala, and Chinese
- Why unresolved: Current normalization processes often translate event titles to English internally, causing mismatches during follow-up retrieval operations
- What evidence would resolve it: Evaluation of a cross-lingual entity resolution module that preserves original non-Latin event titles or embeddings throughout the agent workflow

### Open Question 2
- Question: Can persistent user profiling be integrated to enable predictive scheduling without compromising the system's current modularity?
- Basis in paper: Section 7 lists "limited personalization and adaptivity" as a key limitation, noting the system does not learn user preferences or recurring patterns
- Why unresolved: The current architecture is largely stateless, preventing the accumulation of historical data needed for predictive features
- What evidence would resolve it: Demonstration of an augmented architecture that successfully ingests user history to proactively suggest events while maintaining agent decoupling

### Open Question 3
- Question: What are the performance trade-offs when implementing local or hybrid LLM inference to satisfy enterprise privacy requirements?
- Basis in paper: Section 7 highlights privacy as a concern since sensitive calendar data is processed in the cloud without differential privacy or on-device inference
- Why unresolved: Moving computation to the edge or using hybrid approaches may introduce latency or reduce reasoning capability compared to cloud-hosted GPT-4o
- What evidence would resolve it: Comparative benchmarks of latency and task success rates between the current cloud implementation and a privacy-preserving local model configuration

## Limitations
- Performance variance across languages with non-Latin scripts showing lower accuracy (65-75% vs 85-100% for European languages)
- Central supervisor bottleneck limits scalability and creates single point of failure
- Lack of persistent memory across sessions prevents context continuity and personalization

## Confidence
- **High Confidence:** Graph-structured coordination mechanism using LangGraph, ReAct-based tool invocation patterns, and core modular architecture design
- **Medium Confidence:** Multilingual performance claims and user study results given small sample size and lack of long-term evaluation
- **Low Confidence:** Claims about zero-shot generalizability across all languages without task-specific fine-tuning, and scalability assertions

## Next Checks
1. **Scalability stress test:** Deploy multiple supervisor instances with Redis session management and measure throughput under concurrent multi-user load
2. **Long-term user study:** Conduct 4-week field study with 50+ participants to assess learning effects and satisfaction stability
3. **Non-Latin script robustness:** Systematically test 100+ scheduling queries each in Tamil and Sinhala, comparing error patterns against section 5.3 analysis