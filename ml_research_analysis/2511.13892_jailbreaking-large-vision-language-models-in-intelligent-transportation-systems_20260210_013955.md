---
ver: rpa2
title: Jailbreaking Large Vision Language Models in Intelligent Transportation Systems
arxiv_id: '2511.13892'
source_url: https://arxiv.org/abs/2511.13892
tags:
- lvlms
- attack
- jailbreaking
- arxiv
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel jailbreaking attack against Large
  Vision Language Models (LVLMs) integrated in Intelligent Transportation Systems
  (ITS) using image typography manipulation and multi-turn prompting. The attack hides
  adversarial goals within image captions and employs a gradual multi-turn conversation
  to mislead LVLMs into generating harmful responses.
---

# Jailbreaking Large Vision Language Models in Intelligent Transportation Systems

## Quick Facts
- arXiv ID: 2511.13892
- Source URL: https://arxiv.org/abs/2511.13892
- Reference count: 40
- Key outcome: Novel jailbreaking attack against LVLMs in ITS using image typography manipulation and multi-turn prompting, achieving 75-92% success rates

## Executive Summary
This paper presents a novel jailbreaking attack against Large Vision Language Models (LVLMs) integrated into Intelligent Transportation Systems (ITS) through image-based text manipulation and multi-turn prompting strategies. The attack embeds adversarial goals within image captions and employs gradual conversational techniques to mislead LVLMs into generating harmful responses. The authors create a new dataset of harmful transportation-related queries and test their attack against three state-of-the-art LVLMs (LLaVa-1.6, Qwen-2, and GPT-4o-mini). They also propose a multi-layer defense mechanism that combines pattern-based filtering with zero-shot classification, demonstrating significant reduction in attack success rates.

## Method Summary
The attack methodology consists of two main components: image typography manipulation and multi-turn prompting. The authors embed harmful queries within image captions using techniques like rotation, scaling, and color adjustment to evade detection while maintaining readability. These manipulated images are then used in a gradual multi-turn conversation where the attacker progressively steers the model toward generating harmful content. The proposed defense mechanism operates at multiple layers, first applying pattern-based filtering to detect suspicious text patterns in images, followed by zero-shot classification to evaluate the intent of the conversation. The system is trained on a custom dataset of 100 harmful transportation-related queries covering various scenarios like traffic accidents, emergency vehicle manipulation, and pedestrian safety violations.

## Key Results
- Attack achieves 75-92% success rates in generating harmful content across three SOTA LVLMs
- Defense mechanism reduces attack success by 44-87% across tested models
- Proposed attack outperforms existing methods in terms of success rate and stealthiness

## Why This Works (Mechanism)
The attack exploits the dual-modality processing capabilities of LVLMs by embedding adversarial text within images that the model must interpret alongside visual content. By carefully manipulating typography and employing multi-turn prompting strategies, the attacker can gradually bypass the model's safety mechanisms that might otherwise detect harmful intent in a single-turn interaction. The gradual nature of the attack allows the model to build context incrementally, making it more likely to generate harmful responses as the conversation progresses.

## Foundational Learning
- **Image-based adversarial text embedding**: Needed to hide malicious intent within visual content; quick check: verify text remains readable after manipulation
- **Multi-turn conversation manipulation**: Required to gradually bypass safety mechanisms; quick check: measure context accumulation across conversation turns
- **Zero-shot classification for intent detection**: Essential for defense without requiring labeled training data; quick check: evaluate classification accuracy on known harmful vs. benign conversations
- **Pattern-based filtering techniques**: Necessary for initial detection of suspicious image content; quick check: measure false positive and false negative rates
- **LVLM dual-modality processing**: Critical understanding of how models interpret text within images; quick check: test model responses to same text in different visual contexts
- **Safety mechanism bypassing strategies**: Key to understanding attack effectiveness; quick check: compare single-turn vs. multi-turn attack success rates

## Architecture Onboarding

### Component Map
Image Processing Module -> Typography Manipulation Engine -> Multi-turn Conversation Manager -> LVLM Interface -> Output Evaluation Module

### Critical Path
1. Image Processing Module receives original image and embeds harmful text using typography manipulation
2. Multi-turn Conversation Manager initiates dialogue with LVLM, gradually introducing manipulated content
3. LVLM Interface facilitates communication between attacker and model
4. Output Evaluation Module assesses whether harmful content was successfully generated

### Design Tradeoffs
- **Stealth vs. Readability**: Higher manipulation (rotation, scaling) increases stealth but may reduce text readability
- **Conversation Length vs. Success Rate**: Longer conversations may increase success but also raise detection risk
- **Defense Complexity vs. Performance**: More sophisticated defenses may reduce attack success but increase computational overhead

### Failure Signatures
- Early detection of embedded text patterns in images
- Sudden shifts in conversation tone or topic
- Inconsistent responses to similar prompts across conversation turns
- Recognition of harmful intent despite gradual introduction

### First Experiments
1. Test typography manipulation impact on text readability and detection rates
2. Evaluate multi-turn vs. single-turn attack success rates
3. Measure defense effectiveness against varying levels of image manipulation

## Open Questions the Paper Calls Out
None

## Limitations
- Attack methodology relies on specific assumptions about LVLM processing that may not generalize
- Success rates may be inflated due to controlled experimental conditions and small dataset size
- Defense mechanism tested against single attack vector may not withstand more sophisticated strategies

## Confidence
- **High confidence**: Fundamental vulnerability of LVLMs to image-based text manipulation attacks
- **Medium confidence**: Specific effectiveness of multi-turn prompting strategy and proposed defense mechanism
- **Medium confidence**: Generalizability of attack to real-world ITS deployments

## Next Checks
1. Test attack and defense mechanisms against broader range of LVLMs, including models not trained on ITS-specific data
2. Conduct user study with domain experts to evaluate practical impact and detectability of generated harmful content
3. Implement and evaluate attack in simulated ITS environment with multiple interacting LVLMs to assess defense robustness under realistic conditions