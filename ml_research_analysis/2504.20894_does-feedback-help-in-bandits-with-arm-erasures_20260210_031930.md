---
ver: rpa2
title: Does Feedback Help in Bandits with Arm Erasures?
arxiv_id: '2504.20894'
source_url: https://arxiv.org/abs/2504.20894
tags:
- erasure
- learner
- feedback
- regret
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the multi-armed bandit (MAB) problem with arm
  erasure channels, where a learner communicates arm choices to an agent through a
  channel that may erase transmissions. The key question is whether erasure feedback
  (knowing when transmissions are erased) improves regret performance compared to
  no-feedback settings.
---

# Does Feedback Help in Bandits with Arm Erasures?

## Quick Facts
- arXiv ID: 2504.20894
- Source URL: https://arxiv.org/abs/2504.20894
- Reference count: 40
- This paper proves erasure feedback doesn't improve fundamental regret bounds in bandit problems with communication erasures, but enables simpler adaptive algorithms

## Executive Summary
This paper studies multi-armed bandit problems where a learner communicates arm choices to an agent through an erasure channel. The key question is whether knowing when transmissions are erased (erasure feedback) can improve regret performance compared to no-feedback settings. The authors establish that erasure feedback does not improve the worst-case regret order, proving matching lower and upper bounds of Θ(√KT + K/(1-ϵ)). They propose a "Stop-on-Success" (SoS) algorithm that leverages feedback to adaptively stop repetitions after successful transmissions, achieving near-optimal regret bounds. While the fundamental regret bounds remain unchanged, empirical results show SoS provides constant-factor improvements in practice.

## Method Summary
The authors analyze the multi-armed bandit problem with arm erasure channels, where each arm pull may be erased with probability ϵ. They establish a regret lower bound of Ω(√KT + K/(1-ϵ)) for single-agent M bandits with erasure feedback. The proposed Stop-on-Success (SoS) algorithm replaces fixed repetition schedules with adaptive repetition: each arm pull is repeated until a successful transmission occurs or a timeout is reached. When applied to the Successive Arm Elimination algorithm, SoS achieves a regret bound of Õ(√KT + K/(1-ϵ)). The algorithm's key innovation is using feedback to terminate repetitions immediately after success, rather than continuing with fixed schedules.

## Key Results
- Proves regret lower bound of Ω(√KT + K/(1-ϵ)) for erasure feedback settings, matching no-feedback lower bounds up to logarithmic factors
- SoS algorithm achieves Õ(√KT + K/(1-ϵ)) regret when applied to Successive Arm Elimination, which is near-optimal
- Empirical evaluation shows constant-factor improvements in regret over no-feedback algorithms, particularly as erasure probability increases
- Feedback enables simpler algorithm designs that adapt to erasure events without requiring knowledge of erasure probability

## Why This Works (Mechanism)
The Stop-on-Success algorithm works by adaptively terminating repetitions once a successful transmission occurs, rather than continuing with fixed schedules. This mechanism exploits the feedback signal to reduce unnecessary repetitions when erasures don't occur, while still ensuring successful communication when needed. The algorithm maintains the same worst-case regret order as no-feedback approaches but achieves practical improvements through reduced communication overhead in favorable conditions.

## Foundational Learning
- **Multi-armed bandit problem**: Sequential decision-making framework where a learner selects arms to maximize cumulative reward
  - Why needed: Core problem setting for understanding exploration-exploitation tradeoffs
  - Quick check: Can model any sequential decision problem with partial information

- **Erasure channels**: Communication channels that either successfully transmit a message or completely erase it
  - Why needed: Models unreliable communication between learner and agent in distributed settings
  - Quick check: Erasure probability ϵ determines fraction of lost transmissions

- **Regret analysis**: Performance metric measuring cumulative reward difference from optimal policy
  - Why needed: Standard framework for evaluating bandit algorithm efficiency
  - Quick check: Lower regret indicates better algorithm performance

- **Successive Arm Elimination**: Bandit algorithm that iteratively eliminates suboptimal arms based on statistical confidence
  - Why needed: Provides structure for applying SoS in practical bandit settings
  - Quick check: Combines exploration with aggressive arm elimination

## Architecture Onboarding

Component map: Learner -> Erasure Channel -> Agent -> Feedback -> Learner

Critical path: Arm selection → Communication → Erasure decision → Feedback reception → Repetition decision

Design tradeoffs:
- Fixed vs. adaptive repetition schedules: Fixed schedules simpler but waste transmissions when successful
- Feedback vs. no-feedback: Feedback enables adaptation but requires communication overhead
- Exploration vs. exploitation: Must balance learning arm values with maximizing rewards

Failure signatures:
- High erasure rates cause excessive repetitions and increased regret
- Poor timeout selection leads to either insufficient repetitions or wasted communications
- Incorrect confidence bounds result in premature arm elimination

First experiments:
1. Test SoS on single-armed bandit with varying erasure probabilities to verify basic functionality
2. Compare SoS vs. fixed-repetition algorithm on 2-armed bandit with known optimal arm
3. Evaluate sensitivity to timeout parameter across different erasure probabilities

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting that while erasure feedback doesn't improve fundamental regret bounds, it enables simpler algorithm designs. The empirical results suggest practical value that warrants further investigation, particularly regarding constant-factor improvements and performance under non-i.i.d. erasure patterns.

## Limitations
- Analysis assumes i.i.d. erasure channels, while real-world channels may exhibit temporal correlations or bursty erasures
- The constant factors in regret bounds are not characterized, leaving a gap between theoretical and practical performance
- Empirical evaluation is limited to specific problem instances and erasure probabilities

## Confidence

Theoretical results (High): The matching of lower and upper bounds is rigorous and establishes that erasure feedback doesn't improve fundamental regret order.

Empirical results (Medium): While showing constant-factor improvements, the evaluation is limited in scope and doesn't characterize performance across diverse erasure patterns.

Practical implications (Medium): The value of SoS depends on how well constant-factor improvements translate to real-world performance gains, which remains to be fully validated.

## Next Checks

1. Test SoS on erasure patterns with bursty or temporally correlated erasures to assess robustness beyond i.i.d. assumptions

2. Characterize the constant factors in regret bounds through tighter analysis or empirical measurement across different problem scales

3. Compare SoS performance against no-feedback algorithms in scenarios where erasure probability varies over time or differs across arms