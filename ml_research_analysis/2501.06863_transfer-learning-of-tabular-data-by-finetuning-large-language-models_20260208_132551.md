---
ver: rpa2
title: Transfer Learning of Tabular Data by Finetuning Large Language Models
arxiv_id: '2501.06863'
source_url: https://arxiv.org/abs/2501.06863
tags:
- learning
- data
- tabular
- transfer
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying deep learning to
  tabular data, which is often less effective than traditional machine learning due
  to its heterogeneous feature space and limited sample sizes. The authors propose
  an end-to-end fine-tuning approach for large language models (LLMs) to perform transfer
  learning on tabular data classification tasks.
---

# Transfer Learning of Tabular Data by Finetuning Large Language Models

## Quick Facts
- arXiv ID: 2501.06863
- Source URL: https://arxiv.org/abs/2501.06863
- Authors: Shourav B. Rabbani; Ibna Kowsar; Manar D. Samad
- Reference count: 32
- Primary result: LLM-based transfer learning outperforms state-of-the-art methods on tabular data with <10 features while using significantly less computational resources

## Executive Summary
This paper addresses the challenge of applying deep learning to tabular data, which is often less effective than traditional machine learning due to its heterogeneous feature space and limited sample sizes. The authors propose an end-to-end fine-tuning approach for large language models (LLMs) to perform transfer learning on tabular data classification tasks. The method involves converting tabular data into text prompts, tokenizing them, and fine-tuning a pre-trained LLM like DistilGPT2 with a classifier head. The proposed approach is evaluated on ten benchmark datasets and compared against state-of-the-art machine and deep learning methods. Results show that the LLM-based transfer learning method outperforms other approaches, especially on datasets with fewer than ten features, and achieves competitive or superior classification performance while using significantly less computational resources than deep learning or API-based solutions.

## Method Summary
The method converts tabular data rows into text prompts by serializing feature-value pairs (e.g., "Age is 25. Sex is male."), then tokenizes these prompts for processing by DistilGPT2. A classifier head is appended to the LLM to map the 768-dimensional output embedding to class logits. The entire model is fine-tuned end-to-end using cross-entropy loss with AdamW optimizer (learning rate 5e-5, weight decay 0.01), batch size 16, and early stopping (patience 10). The approach is evaluated using 5-fold cross-validation with AUC as the primary metric, using 1/8 of training data for validation. The method is tested on 10 benchmark datasets ranging from 5 to 44 features.

## Key Results
- End-to-end fine-tuning significantly outperforms frozen-weight approaches (average rank 3.2 vs 5.3 across all datasets)
- LLM-based transfer learning achieves competitive or superior performance on datasets with fewer than 10 features
- The method uses significantly less computational resources than deep learning baselines while maintaining strong classification performance

## Why This Works (Mechanism)

### Mechanism 1
Converting tabular data to text prompts enables cross-domain transfer learning from pre-trained linguistic knowledge to tabular classification tasks. Data serialization transforms feature-value pairs into tokenized sequences that DistilGPT2 can process, allowing the model's pre-trained representations to be repurposed for tabular classification via an appended classifier head trained with cross-entropy loss. This works when feature names have semantic meaning that overlaps with pre-training corpus knowledge.

### Mechanism 2
End-to-end fine-tuning with all trainable weights substantially outperforms frozen-weight approaches for tabular classification. Unfrozen weights allow the transformer layers to adapt representations specifically for tabular patterns, whereas frozen weights constrain the model to fixed linguistic features that may not align with tabular decision boundaries. This is particularly important when tabular data patterns differ significantly from natural language patterns.

### Mechanism 3
LLM-based transfer learning shows competitive or superior performance specifically on low-dimensional tabular data (fewer than 10 features). Smaller feature sets produce shorter token sequences that fit within DistilGPT2's 1024-token context window without truncation, preserving full feature information and reducing sequence fragmentation. Performance degrades as feature count increases due to tokenization constraints.

## Foundational Learning

- **Transfer Learning Paradigm**: Understanding how knowledge from pre-training on text corpora transfers to tabular domains is essential for grasping why this approach works. Quick check: Can you explain why a model pre-trained on Wikipedia might have useful representations for predicting diabetes from clinical features?

- **Transformer Tokenization and Context Windows**: The 1024-token limit directly constrains how many features and samples can be processed, and understanding token-to-embedding conversion is essential for debugging serialization issues. Quick check: Given a tabular row with 15 features averaging 8 tokens each, will it fit in DistilGPT2's context window with room for a classification prompt?

- **Fine-Tuning Strategies (Frozen vs. Full)**: The paper demonstrates dramatic performance differences between frozen and trainable approachesâ€”practitioners must understand when each is appropriate. Quick check: If validation loss plateaus early while training loss continues decreasing, what does this suggest about your fine-tuning configuration?

## Architecture Onboarding

- **Component map**: Data Serializer -> Tokenizer -> DistilGPT2 Backbone -> Classifier Head -> Training Loop
- **Critical path**: Serialization format -> Tokenization padding strategy -> Classifier head initialization -> Learning rate selection
- **Design tradeoffs**: Frozen weights offer faster training but cause underfitting (loss plateau >0.5); end-to-end training provides superior performance but requires careful regularization and early stopping
- **Failure signatures**: Validation loss plateau >0.5 with frozen weights indicates switch to trainable weights needed; training loss near zero but validation increasing after epoch 30 suggests overfitting; AUC <0.6 on all datasets indicates serialization/tokenizer issues
- **First 3 experiments**: 1) Baseline sanity check: Replicate frozen vs. end-to-end comparison on Heart dataset (expect ~0.87 AUC frozen vs ~0.92 AUC end-to-end); 2) Feature ablation: Test serialization with vs without feature metadata descriptions on Blood-transfusion dataset; 3) Context window stress test: Incrementally increase feature count on synthetic dataset to identify truncation threshold

## Open Questions the Paper Calls Out

1. What is the optimal balance between frozen and trainable LLM weights for tabular data classification? The paper only experiments with fully frozen (underfitting) and fully trainable (overfitting) approaches without exploring intermediate configurations.

2. Can LLM-based transfer learning scale effectively to tabular datasets with significantly more than 10 features? The paper demonstrates success on low-dimensional datasets but doesn't evaluate high-dimensional (>100 features) tabular data.

3. What specific linguistic or semantic knowledge from text pre-training transfers to tabular data classification? The paper shows it works but doesn't investigate which linguistic knowledge is being leveraged or whether semantically meaningless feature names would degrade performance.

4. Would LLMs specifically pre-trained on tabular data outperform general-purpose text LLMs for tabular classification? The paper uses DistilGPT2 trained on Wikipedia/BookCorpus without exploring domain-specific pre-training.

## Limitations

- Evaluation relies on 10 public datasets with moderate sample sizes (366-2164) and feature counts (5-44), potentially not representing industrial-scale problems with millions of rows or hundreds of features
- Critical details about prompt template design, feature metadata inclusion, and handling of categorical vs. numerical features are underspecified, creating implementation variance risk
- Mixed cross-dataset transfer learning results (Dermatology: 0.64 AUC) suggest the "knowledge base" benefit may be domain-specific rather than universal

## Confidence

- **High Confidence**: End-to-end fine-tuning approach outperforms frozen-weight approaches (average rank 3.2 vs 5.3 across all datasets)
- **Medium Confidence**: LLM-based transfer learning shows competitive or superior performance specifically on low-dimensional tabular data (<10 features)
- **Low Confidence**: Cross-dataset transfer learning effectiveness due to inconsistent performance and lack of systematic analysis

## Next Checks

1. Implement and test three different serialization formats (minimal, semantic, hybrid) across all 10 datasets to quantify the impact of prompt design on classification performance

2. Systematically generate synthetic tabular datasets with 5, 10, 20, 30, and 40 features while holding sample size constant, then measure AUC degradation relative to GBT baselines to identify the precise feature count threshold where LLM approaches lose competitiveness

3. Select dataset pairs with varying domain similarity and measure AUC performance when training on one and testing on the other, analyzing feature correlation matrices and data distributions to identify predictive factors for successful transfer learning