---
ver: rpa2
title: 'Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via
  Deriving Real Calls into Virtual Trajectories'
arxiv_id: '2601.15120'
source_url: https://arxiv.org/abs/2601.15120
tags:
- tool
- rise
- intent
- data
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles intent deviation in tool-using LLM agents,
  where actions appear valid but misalign with user intent (e.g., wrong tool or parameter).
  RISE introduces a Real-to-Virtual method: starting from verified real tool primitives,
  it synthesizes virtual execution trajectories and user requests in reverse, then
  generates intent-aligned negatives via mutations on critical parameters.'
---

# Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories

## Quick Facts
- **arXiv ID**: 2601.15120
- **Source URL**: https://arxiv.org/abs/2601.15120
- **Reference count**: 12
- **Primary result**: RISE achieves 35.28% improvement in Acc_task and 23.27% in Acc_intent over state-of-the-art baselines by grounding synthetic tool-use data in real tool primitives.

## Executive Summary
This paper addresses intent deviation in LLM tool-using agents, where actions appear valid but misalign with user intent (e.g., wrong tool or parameter). RISE introduces a Real-to-Virtual method that starts from verified real tool primitives, synthesizes virtual execution trajectories and user requests in reverse, then generates intent-aligned negatives via mutations on critical parameters. A two-stage training (SFT + DPO) fine-tunes LLMs for intent alignment. RISE achieves significant improvements in task completion and intent alignment over baselines, with robust generalization on unseen datasets.

## Method Summary
RISE tackles intent deviation through a pipeline starting with real MCP tool collection and validation, followed by backward trajectory synthesis from verified primitives. The method identifies Intent-aware Critical Parameters (ICPs) and generates negative samples via six mutation types. A two-stage training approach (SFT then DPO) fine-tunes LLMs, where SFT establishes basic tool invocation competence before DPO optimizes for intent alignment. The method uses 728 tools across 19 fields, generates synthetic trajectories and requests, and applies stratified sampling for mutation diversity.

## Key Results
- 35.28% improvement in Acc_task (task completion) over state-of-the-art baselines
- 23.27% improvement in Acc_intent (intent alignment) over baselines
- +7.14% to +22.37% gains on OOD datasets (ToolBench, DICE-Bench, ACEBench)

## Why This Works (Mechanism)

### Mechanism 1
Grounding synthetic trajectories in verified real tool primitives reduces distribution shift compared to fully virtual simulation. RISE collects tools from MCP.so, generates candidate tool primitives using GPT-4o, then validates executability against real MCP servers. Only calls returning structured results without protocol errors are retained. Trajectories are synthesized backward from these verified primitives to user requests. Core assumption: Real tool primitives preserve parameter patterns that LLM-only simulation degrades.

### Mechanism 2
Explicit identification of Intent-aware Critical Parameters (ICPs) enables targeted mutation that creates semantically meaningful negative samples. ICPs are parameters whose values derive from user key values. RISE uses LLM-guided semantic mapping to identify ICPs, then applies six mutation types (hypernym/hyponym, co-hyponym, irrelevance, antonym, lexical similarity, combination) to generate diverse intent-deviation negatives. Core assumption: Intent deviation primarily manifests through incorrect values in parameters tied to user constraints.

### Mechanism 3
Two-stage training (SFT → DPO) separates foundational tool-use capability from intent alignment, preventing DPO instability on uncalibrated models. Stage 1 (SFT) trains on positive samples only, establishing basic tool invocation competence. Stage 2 (DPO) uses preference pairs to fine-tune intent alignment via the loss function L_DPO. Core assumption: Syntactically valid tool calls can still deviate from intent; DPO requires a competent base model to avoid degenerate solutions.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Understanding how L_DPO optimizes policy π_θ to prefer correct trajectories over mutated negatives without explicit reward modeling. Quick check: Can you explain why DPO requires a reference model π_ref and what β controls?
- **Model Context Protocol (MCP)**: RISE's tool collection and validation depend on MCP server/client architecture; understanding the invocation protocol is prerequisite for reproducing Tool Ground Initialization. Quick check: What does it mean for an MCP tool call to be "valid" per section 3.1?
- **Intent Deviation vs. Execution Failure**: The paper distinguishes covert intent misalignment from overt invocation errors; this distinction drives the negative sampling strategy. Quick check: Give an example where Acc_task is high but Acc_intent is low.

## Architecture Onboarding

- **Component map**: Tool Ground Initialization (MCP.so sampling → BERT deduplication → 728 tools) → Real-to-Virtual Synthesis (GPT-4o primitives → MCP validation → Trajectory synthesis → Request synthesis) → Negatives Expansion (ICP identification → Six-type mutation → Stratified binning) → Training Pipeline (SFT → DPO)
- **Critical path**: Tool primitive executability validation → ICP identification accuracy → Mutation diversity → DPO pair quality
- **Design tradeoffs**: Real vs. Virtual (real primitives ensure executability but limit scalability), Mutation complexity (higher complexity filters trivial errors but may miss subtle deviations), Stratified binning (ensures diversity but requires sufficient samples per cluster)
- **Failure signatures**: Low Acc_ICP with high Acc_task (model completes tasks but ignores user constraints), High Acc_calling with low Acc_task (model invokes tools correctly but selects wrong tools), DPO divergence (loss increases if reference model is too distant)
- **First 3 experiments**: 1) Reproduce Tool Ground Initialization on a single MCP server and validate executability checks and ICP identification, 2) Generate 10 negative samples via each mutation type for a single ICP and verify semantic validity and compute complexity scores, 3) Run ablation comparing Raw→SFT→DPO vs. Raw→DPO directly on a held-out test split

## Open Questions the Paper Calls Out
None

## Limitations
- **Data Synthesis Scalability**: Reliance on real MCP server validation creates a bottleneck—scaling to thousands of tools would face exponential validation time and server availability constraints.
- **Generalization Boundary**: Performance gains on OOD datasets are based on relatively small test sets without addressing fundamentally different tool distributions or user intent expression patterns.
- **DPO Stability**: Paper doesn't report DPO loss curves or stability metrics, leaving robustness across different β values and reference model choices unclear.

## Confidence
- **High Confidence**: Grounding synthetic data in real tool primitives (Mechanism 1) is well-supported by 70% element absence in virtual-only tools and executability validation results.
- **Medium Confidence**: ICP mutation strategy (Mechanism 2) shows theoretical soundness but lacks qualitative examples showing generated negatives capture realistic intent deviation scenarios.
- **Low Confidence**: "Robust generalization" claim on unseen datasets is based on small test sets without ablation studies showing which components drive generalization versus dataset-specific artifacts.

## Next Checks
1. Execute the Tool Ground Initialization pipeline on a single MCP server and verify that primitives pass executability checks while identifying ICPs for a sample request.
2. Generate 10 negative samples via each mutation type for a single ICP and verify semantic validity (same part-of-speech, same data type) while computing complexity scores.
3. Run the ablation study comparing Raw→SFT→DPO vs. Raw→DPO directly on a held-out test split and confirm the Table 1 pattern where skipping SFT degrades Acc_calling.