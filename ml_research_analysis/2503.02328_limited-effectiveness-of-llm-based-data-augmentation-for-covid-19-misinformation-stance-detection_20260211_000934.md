---
ver: rpa2
title: Limited Effectiveness of LLM-based Data Augmentation for COVID-19 Misinformation
  Stance Detection
arxiv_id: '2503.02328'
source_url: https://arxiv.org/abs/2503.02328
tags:
- data
- augmentation
- misinformation
- detection
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the effectiveness of LLM-based data augmentation
  for COVID-19 misinformation stance detection. The authors simulate low-resource
  scenarios and compare controllable misinformation generation (CMG) using LLMs against
  traditional augmentation methods like EDA, AEDA, and backtranslation.
---

# Limited Effectiveness of LLM-based Data Augmentation for COVID-19 Misinformation Stance Detection

## Quick Facts
- arXiv ID: 2503.02328
- Source URL: https://arxiv.org/abs/2503.02328
- Reference count: 29
- Primary result: LLM-based controllable misinformation generation (CMG) provides minimal performance gains for COVID-19 stance detection due to safety alignment causing refusals and task-flipping

## Executive Summary
This paper investigates whether LLM-based data augmentation can improve COVID-19 misinformation stance detection in low-resource scenarios. The authors compare controllable misinformation generation (CMG) using instruction-tuned LLMs against traditional augmentation methods like EDA, AEDA, and backtranslation across multiple dataset sizes (100-5000 claim-tweet pairs). They find that LLM-based approaches provide minimal performance gains, primarily due to built-in safety safeguards causing high refusal rates and task-flipping behaviors. Traditional methods consistently outperform CMG approaches, demonstrating that less resource-intensive methods are more effective for this task.

## Method Summary
The authors employ a two-step finetuning approach using a DeBERTa-v3-base cross-encoder: (1) pretraining on SNLI, MNLI, and FEVER-NLI datasets, followed by (2) finetuning on COVID-19 stance detection data with 4x augmentation. They compare CMG using GPT-4o, GPT-3.5-Turbo, LLaMa-3.1-8B-Instruct, and Qwen2.5-7B-Chat against traditional methods (EDA, AEDA, backtranslation). CMG outputs are filtered using a DistilRoBERTa rejection classifier with a 0.9 threshold. Experiments test scenarios from 100 to 5000 training pairs, with results averaged over 5 runs per configuration.

## Key Results
- LLM-based CMG provides minimal performance gains over traditional augmentation methods in low-resource stance detection
- Safety alignment in instruction-tuned LLMs causes high refusal rates (3.8-18.7%) and task-flipping behaviors that degrade augmentation quality
- Less resource-intensive methods like backtranslation and EDA consistently outperform CMG approaches across all dataset sizes tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety alignment in instruction-tuned LLMs degrades their utility for misinformation data augmentation.
- Mechanism: LLMs aligned to refuse harmful or deceptive content exhibit two failure modes when prompted for CMG: (1) explicit refusals that reduce effective augmentation volume, and (2) task-flipping where models generate content contradicting the requested stance label.
- Core assumption: Refusal and task-flipping behaviors are causally linked to safety alignment rather than general model incapacity.
- Evidence anchors:
  - [abstract] "performance gains over traditional augmentation methods are often minimal and inconsistent—primarily due to built-in safeguards within LLMs"
  - [Section 3.2.1] Refusal rates varied significantly: GPT-4o (3.8%), GPT-3.5-Turbo (18.7%), LLaMa-3.1-8B-Instruct (4.6%), Qwen2.5-7B-Chat (12.6%)
  - [Section 3.2.2] "These two kinds of behaviors, refusals, and task flipping, are most likely to be byproducts of aligning LLMs to establish safety guards against generating harmful and deceptive content"
  - [corpus] Related work (Chen & Shu 2023, Satapara et al. 2024) documents similar LLM caution with sensitive misinformation datasets, providing convergent evidence

### Mechanism 2
- Claim: Traditional augmentation methods (backtranslation, EDA) provide more consistent low-resource gains than CMG because they preserve label fidelity with lower variance.
- Mechanism: Paraphrase-based methods (backtranslation) and simple perturbations (EDA) transform surface form while maintaining semantic content and stance labels, avoiding the noise introduced by generative model refusals and task-flips.
- Core assumption: Label preservation in traditional methods exceeds CMG quality after rejection filtering.
- Evidence anchors:
  - [abstract] "less resource-intensive methods like backtranslation and EDA consistently outperformed CMG approaches"
  - [Section 3.2.2, Table 2] Backtranslation achieved F1_Macro=0.789 vs. GPT-4o CMG at 0.793—marginal difference at 4x augmentation
  - [Section 3.2.2, Figure 4] Performance gains from CMG remain inconsistent across varying data sizes (100-5000 pairs)

### Mechanism 3
- Claim: Two-step finetuning (NLI pretraining → domain-specific SD finetuning) provides stronger initialization for stance detection than direct finetuning.
- Mechanism: NLI pretraining on SNLI, MNLI, and FEVER-NLI establishes general textual inference capabilities that transfer to claim-tweet relationship classification.
- Core assumption: NLI entailment/contradiction labels map cleanly to stance support/oppose labels.
- Evidence anchors:
  - [Section 2] "By aligning NLI labels with SD labels, we leverage large-scale NLI datasets to improve SD performance"
  - [Section 3] "We employed a two-step finetuning process to leverage broader NLI capabilities before focusing on COVID-19-specific stance detection"

## Foundational Learning

- Concept: **Stance Detection vs. Sentiment Analysis**
  - Why needed here: Stance detection classifies the relationship between a claim and a response (support/oppose/neither), not general sentiment polarity. Confusing these leads to incorrect task formulation.
  - Quick check question: Given claim "Vaccines cause autism" and tweet "This is dangerous pseudoscience," what is the stance label? (Answer: oppose)

- Concept: **Data Augmentation for Class Imbalance**
  - Why needed here: The paper explicitly addresses imbalanced stance distributions and uses augmentation (not resampling) to expand training data while preserving original class ratios.
  - Quick check question: Why might oversampling the minority class hurt generalization compared to augmentation? (Answer: Oversampling duplicates examples, potentially overfitting; augmentation creates novel variations)

- Concept: **NLI-to-SD Label Mapping**
  - Why needed here: The two-step finetuning relies on mapping entailment→support, contradiction→oppose, neutral→neither. Understanding this mapping is essential for replicating the pipeline.
  - Quick check question: What NLI label corresponds to a tweet that neither supports nor opposes a false claim? (Answer: neutral)

## Architecture Onboarding

- Component map:
  Base encoder (DeBERTa-v3-base Cross-encoder) -> Stage 1 pretraining (SNLI+MNLI+FEVER-NLI) -> Stage 2 finetuning (augmented COVID-19 SD data) -> Augmentation modules (CMG, EDA, AEDA, Backtranslation) -> Rejection filter (DistilRoBERTa) -> Evaluation (F1_macro, accuracy, per-class F1)

- Critical path:
  1. Prepare claim-tweet pairs with stance labels; split with 90% overlap threshold to prevent leakage
  2. Run Stage 1 pretraining on NLI corpora
  3. Generate augmented samples via selected method; filter CMG outputs with rejection classifier
  4. Finetune on augmented COVID-19 SD data
  5. Evaluate on held-out test set using F1_macro, accuracy, and per-class F1

- Design tradeoffs:
  - CMG vs. traditional augmentation: CMG offers potential diversity but introduces refusal/flip noise; traditional methods are deterministic but limited in variation
  - Rejection threshold: Higher threshold (0.9) reduces false negatives but may let subtle refusals through; lower threshold loses more valid data
  - Augmentation multiplier (4x): Paper chose 4x; higher multipliers may amplify noise in CMG

- Failure signatures:
  - High refusal rate (>15%): Model likely over-aligned; consider less restrictive model or different prompting
  - Task-flip detection: Generated tweet contradicts requested stance label; manual inspection of samples required
  - No augmentation gain: Augmented data may be too similar to original (EDA) or too noisy (CMG with poor filtering)

- First 3 experiments:
  1. Baseline replication: Finetune DeBERTa-v3-base on unaugmented COVID-19 SD data (N=5000) to establish pretrained→finetuned gap (expected: ~0.77 F1_macro per Table 2)
  2. Traditional augmentation comparison: Run EDA vs. backtranslation augmentation at 4x on N=1000 subset; compare F1_macro to validate paper's finding that traditional methods match or exceed CMG
  3. CMG refusal analysis: Prompt GPT-4o and LLaMa-3.1-8B-Instruct for 100 generations each; compute refusal rate with DistilRoBERTa classifier and manually inspect 20 samples for task-flips

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's attribution of CMG limitations to safety alignment remains correlative rather than experimentally validated—testing with non-aligned models would strengthen this causal claim
- The rejection classifier threshold (0.9) and its impact on false negatives versus true negatives is not fully explored, potentially affecting the quality assessment of CMG outputs
- The relative performance gain of two-step finetuning over single-step is not explicitly quantified against a direct finetuning baseline

## Confidence
- **High confidence**: Traditional augmentation methods (backtranslation, EDA) consistently outperforming CMG in low-resource scenarios—supported by multiple dataset sizes and clear performance metrics
- **Medium confidence**: Safety alignment as primary cause of CMG limitations—supported by refusal rate data but lacks control experiments with non-aligned models
- **Medium confidence**: Two-step finetuning providing stronger initialization than direct finetuning—mechanism is sound but relative performance gain not explicitly quantified against single-step baseline

## Next Checks
1. Run CMG with a model lacking safety alignment (e.g., LLaMa-3.1-8B-Instruct without chat alignment) to isolate whether refusal/flip behaviors are truly caused by safety features versus model architecture
2. Systematically vary the rejection classifier threshold (0.7, 0.8, 0.9, 0.95) and measure impact on final stance detection performance to quantify trade-off between filtering false negatives and preserving augmentation quality
3. Examine per-class F1 scores across all dataset sizes to determine if CMG shows differential effectiveness for specific stance classes (support/oppose/neither), which aggregated metrics might mask