---
ver: rpa2
title: 'Semantic Gravity Wells: Why Negative Constraints Backfire'
arxiv_id: '2601.08070'
source_url: https://arxiv.org/abs/2601.08070
tags:
- target
- failure
- failures
- instruction
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper conducts the first comprehensive mechanistic investigation\
  \ of why negative constraints (e.g., \"do not use word X\") fail in large language\
  \ models. The study introduces semantic pressure, a measure of the model's intrinsic\
  \ probability of generating the forbidden token, and demonstrates that violation\
  \ probability follows a tight logistic relationship with pressure (p=\u03C3(-2.40+2.27\xB7\
  P0), n=40,000 samples, slope 95% CI [2.21,2.33])."
---

# Semantic Gravity Wells: Why Negative Constraints Backfire

## Quick Facts
- arXiv ID: 2601.08070
- Source URL: https://arxiv.org/abs/2601.08070
- Reference count: 5
- First comprehensive mechanistic investigation of why negative constraints fail in LLMs

## Executive Summary
This paper conducts the first comprehensive mechanistic investigation of why negative constraints (e.g., "do not use word X") fail in large language models. The study introduces semantic pressure, a measure of the model's intrinsic probability of generating the forbidden token, and demonstrates that violation probability follows a tight logistic relationship with pressure (p=σ(-2.40+2.27·P0), n=40,000 samples, slope 95% CI [2.21,2.33]). Through layer-wise analysis using the logit lens technique, the research reveals that suppression signals from negative instructions are present but systematically weaker in failures (5.2 percentage points vs. 22.8 in successes, a 4.4× asymmetry). Two mechanistically distinct failure modes are identified: priming failure (87.5% of violations), where the instruction's mention of the forbidden word paradoxically activates it, and override failure (12.5%), where late-layer feed-forward networks generate strong positive contributions toward the target (+0.39 vs. +0.10 in successes), overwhelming earlier suppression signals. Activation patching confirms that layers 23-27 are causally responsible for failures, as replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

## Method Summary
The study employed a large-scale empirical investigation with 40,000 samples across various negative constraints, measuring violation probabilities against semantic pressure values. The logit lens technique was used for layer-wise analysis, examining activation patterns across transformer layers to identify where suppression signals originate and why they fail. Activation patching experiments were conducted by replacing activations from specific layers to establish causal relationships. The analysis focused on identifying the timing and location of both suppression signals and override mechanisms, distinguishing between priming and override failure modes through systematic comparison of successful versus failed constraint enforcement.

## Key Results
- Violation probability follows a tight logistic relationship with semantic pressure (p=σ(-2.40+2.27·P0), slope 95% CI [2.21,2.33])
- Suppression signals in failures are systematically weaker (5.2 pp vs. 22.8 pp in successes, 4.4× asymmetry)
- Two distinct failure modes identified: priming failure (87.5%) and override failure (12.5%)
- Layers 23-27 are causally responsible for failures, confirmed by activation patching

## Why This Works (Mechanism)
The failure of negative constraints operates through a fundamental mechanism where the instruction itself creates a semantic gravity well that pulls the model toward the forbidden content. When a model receives "do not use word X," the mention of X in the instruction creates a priming effect that activates the semantic representation of X, even before the negation signal can fully suppress it. This creates a race condition where the priming signal and suppression signal compete, with the priming typically winning due to the model's autoregressive nature and the way transformers process language.

The logit lens analysis reveals that this competition occurs across multiple transformer layers, with early layers showing suppression signals that are systematically weaker in failures compared to successes. The critical insight is that the model's representation of the forbidden word becomes activated through the instruction itself, creating a self-defeating constraint. The override failure mode demonstrates that even when early suppression works, later layers can generate strong positive signals that overwhelm the initial negation.

## Foundational Learning
- **Semantic Pressure**: The intrinsic probability of generating a forbidden token before instruction processing. Why needed: Provides a quantitative measure of how strongly the model is predisposed to generate the forbidden content. Quick check: Compare baseline generation probabilities with and without negative instructions.
- **Logit Lens Technique**: Layer-wise analysis of transformer activations by examining logits at each layer. Why needed: Enables identification of where in the network suppression signals originate and fail. Quick check: Verify that activation patterns differ systematically between successful and failed constraints.
- **Activation Patching**: Experimental technique where activations from one layer are replaced to test causal relationships. Why needed: Establishes which layers are causally responsible for constraint failures rather than merely correlated. Quick check: Confirm that replacing layers 23-27 flips the sign of constraint effects.
- **Priming Effect**: The paradoxical activation of forbidden content through its mention in the instruction. Why needed: Explains why negative constraints often backfire by making the forbidden content more salient. Quick check: Measure violation rates when the forbidden word is mentioned versus when it's implied without naming.
- **Feed-Forward Network Override**: Late-layer mechanisms that can overwhelm earlier suppression signals with strong positive contributions. Why needed: Identifies how constraints can fail even when early suppression works. Quick check: Compare feed-forward contributions in successful versus failed constraints.

## Architecture Onboarding
- **Component Map**: Input tokens -> Embedding layer -> Transformer blocks (24 layers) -> Feed-forward networks -> Output logits -> Probability distribution
- **Critical Path**: Token embedding → Multi-head attention → Feed-forward network → Layer normalization → Residual connections → Next layer (repeated across 24 layers)
- **Design Tradeoffs**: The autoregressive architecture optimizes for coherent generation rather than constraint satisfaction, creating tension between natural language flow and negative instructions.
- **Failure Signatures**: Weak suppression signals in early layers (5.2 pp vs. 22.8 pp), strong positive feed-forward contributions in failures (+0.39 vs. +0.10), activation patterns in layers 23-27 that favor generation over suppression.
- **First Experiments**: 1) Measure baseline generation probabilities for forbidden tokens, 2) Apply logit lens to identify suppression signal locations, 3) Conduct activation patching on layers 23-27 to test causality.

## Open Questions the Paper Calls Out
Major uncertainties remain around the generalizability of these findings across model architectures and instruction types. The study focuses exclusively on a single model family (likely GPT-like transformers) and simple negation constraints, leaving open questions about whether these mechanisms apply to other architectures or more complex negative instructions. The 40,000-sample size, while substantial, covers only a limited instruction vocabulary and task types.

## Limitations
- Study focuses exclusively on a single model family, limiting generalizability across architectures
- Limited to simple negation constraints, leaving open questions about more complex negative instructions
- Layer-wise causal analysis cannot definitively establish whether identified layers are sole causal mechanisms
- Logistic model parameters show excellent fit for tested conditions but validity across different prompting strategies remains untested

## Confidence
- High: Core empirical observations (logistic relationship, systematic weakness of suppression signals, two distinct failure modes)
- Medium: Layer-wise mechanistic interpretations (depend on specific analytical assumptions)
- Low: Generalizability claims about negative constraint design principles across diverse applications

## Next Checks
1. Test whether the logistic pressure-violation relationship holds across different model families (e.g., Claude, LLaMA) and instruction types beyond simple negation
2. Conduct ablation studies removing the forbidden word from instructions to quantify the priming effect's contribution to failure rates
3. Apply the logit lens analysis to non-negation constraint types (e.g., "use formal language," "avoid slang") to determine if similar layer-wise asymmetries exist