---
ver: rpa2
title: Reinforcement Learning for Self-Improving Agent with Skill Library
arxiv_id: '2512.17102'
source_url: https://arxiv.org/abs/2512.17102
tags:
- skill
- library
- task
- agent
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving LLM-based agents'
  ability to continuously learn and adapt when deployed in new environments, specifically
  through skill libraries that allow agents to learn, validate, and apply new skills.
  The authors propose a reinforcement learning framework called Skill Augmented GRPO
  for self-Evolution (SAGE) that incorporates a novel Sequential Rollout process and
  Skill-integrated Reward.
---

# Reinforcement Learning for Self-Improving Agent with Skill Library

## Quick Facts
- arXiv ID: 2512.17102
- Source URL: https://arxiv.org/abs/2512.17102
- Reference count: 36
- LLMs improve continuously in new environments via skill libraries using SAGE framework

## Executive Summary
This paper addresses the challenge of enabling LLM-based agents to continuously learn and adapt when deployed in new environments. The authors propose a reinforcement learning framework called Skill Augmented GRPO for self-Evolution (SAGE) that incorporates a novel Sequential Rollout process and Skill-integrated Reward. The framework allows agents to learn, validate, and apply new skills through a skill library mechanism. Experiments on the AppWorld dataset demonstrate substantial improvements over baseline methods in both accuracy and efficiency.

## Method Summary
The SAGE framework trains agents through a two-phase process: first, supervised fine-tuning (SFT) on expert trajectories collected via rejection sampling with Claude 3.5 Sonnet V2; second, reinforcement learning using GRPO with Sequential Rollout and Skill-integrated Reward. The Sequential Rollout iteratively deploys agents across chains of similar tasks, accumulating skills from previous tasks for subsequent tasks. The Skill-integrated Reward provides additional incentives for high-quality skill generation and utilization beyond standard outcome-based rewards. The framework is evaluated on the AppWorld dataset using metrics including Task Goal Completion, Scenario Goal Completion, average steps, and average tokens.

## Key Results
- Achieved 8.9% higher Scenario Goal Completion compared to existing approaches
- Required 26% fewer interaction steps while maintaining performance
- Generated 59% fewer tokens, demonstrating significant efficiency improvements

## Why This Works (Mechanism)
The mechanism works by creating a feedback loop where agents generate reusable skill functions during task execution, store them in a skill library, and then retrieve and apply these skills for subsequent similar tasks. The Sequential Rollout process ensures that skills learned from one task are immediately available for the next task in a chain, creating a compounding learning effect. The Skill-integrated Reward provides explicit incentives for both skill generation and effective utilization, encouraging the agent to invest in building its skill library rather than just completing individual tasks.

## Foundational Learning
- **Skill Library Concept**: Why needed - Enables persistent knowledge accumulation across tasks; Quick check - Can the agent successfully store and retrieve skills between task executions
- **Sequential Rollout Process**: Why needed - Allows skill transfer between related tasks; Quick check - Does skill usage improve when tasks are processed in chains
- **Skill-integrated Reward Design**: Why needed - Provides incentives for both skill generation and utilization; Quick check - Are agents generating more skills when this reward is active
- **GRPO Optimization**: Why needed - Handles the non-differentiable nature of skill execution; Quick check - Does the training converge with the skill-based reward signals
- **Tool-using Agent Framework**: Why needed - Enables interaction with external APIs and functions; Quick check - Can the agent successfully execute generated skill functions
- **Context Window Management**: Why needed - Ensures skill library fits within model constraints; Quick check - Are skills being properly retrieved from library within context limits

## Architecture Onboarding

Component Map:
Qwen2.5-32B-Instruct -> SFT Training -> SAGE RL Training -> Skill Library Agent -> AppWorld Environment

Critical Path:
Expert Trajectory Collection -> SFT on 1,129 examples -> SAGE with Sequential Rollout (G=8) -> Skill-integrated Reward Computation -> GRPO Updates -> Skill Library Updates -> Next Task Execution

Design Tradeoffs:
- Chain length K=2 vs K>2: Shorter chains provide better reward stability and gradient variance
- Reward design: Skill-integrated vs Outcome-based vs Chain-based - Skill-integrated shows 2×+ improvement in skill usage
- Context window size: 28,048 tokens balances skill library capacity with computational efficiency
- Batch size 384: Chosen for stable training on 4×H100 8-GPU nodes with Ulysses sequence parallel

Failure Signatures:
- Low skill execution success rate: Indicates SFT data quality or prompt format issues
- Degraded performance on longer task chains: Suggests reward distribution imbalance
- Minimal skill usage despite rewards: Points to skill library retrieval or context management problems

First Experiments:
1. Test skill function generation and execution rates on sample tasks to verify prompt format
2. Compare skill usage frequency between Skill-integrated and Outcome-based reward variants
3. Validate 2-task vs 3-task chain performance differences to confirm Sequential Rollout design

## Open Questions the Paper Calls Out
- How effectively does the SAGE framework generalize to other tool-using agent datasets or environments with different structural constraints beyond AppWorld?
- Can the development of specialized skill or tool-specific retrievers significantly improve performance over standard text-embedding methods for skill library management?
- How can the Sequential Rollout process be modified to maintain training stability and performance when extending task chains beyond two examples?

## Limitations
- Missing complete Skill Library Agent prompt template prevents exact reproduction
- Skill library state management implementation details remain underspecified
- AppWorld evaluation harness specifics, particularly the "manually written program" for stage-based testing, lack sufficient detail

## Confidence
- High Confidence: Core experimental results showing SAGE's superiority over baseline reward approaches
- Medium Confidence: Reported efficiency gains (59% fewer tokens, 26% fewer steps) are plausible but require verification
- Low Confidence: Generalizability of results beyond AppWorld dataset remains unproven

## Next Checks
1. Implement the complete Skill Library Agent prompt based on Figure 9 structure and test skill function generation and execution rates on a small sample of AppWorld tasks. Target: >80% successful skill generation and execution.

2. Reproduce the Sequential Rollout reward comparison by implementing Outcome-based and Chain-based reward variants, then measure skill usage frequency and success rates on the Dev set. Target: Skill-integrated Reward should show 2× improvement over Outcome-based.

3. Validate the 2-task vs 3-task chain performance difference by running both configurations on the Test-Normal set. Target: 3-task chains should show significantly lower SGC than 2-task chains, confirming the reward imbalance concern.