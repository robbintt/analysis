---
ver: rpa2
title: Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language
  Model Explanations
arxiv_id: '2510.00047'
source_url: https://arxiv.org/abs/2510.00047
tags:
- image
- counterfactual
- explanation
- answer
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Explanation-Driven Counterfactual Testing
  (EDCT), an automated framework to evaluate the faithfulness of vision-language models'
  explanations. EDCT treats a model's own explanation as a falsifiable hypothesis
  by generating targeted counterfactual edits to visual concepts cited in the explanation,
  then measuring whether the model's answers and explanations change consistently.
---

# Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations

## Quick Facts
- **arXiv ID:** 2510.00047
- **Source URL:** https://arxiv.org/abs/2510.00047
- **Reference count:** 29
- **Primary result:** Automated framework evaluates vision-language model explanation faithfulness by testing counterfactual edits to cited visual concepts

## Executive Summary
This paper introduces Explanation-Driven Counterfactual Testing (EDCT), a novel automated framework for evaluating the faithfulness of vision-language model explanations. The approach treats model explanations as falsifiable hypotheses by generating targeted counterfactual edits to visual concepts mentioned in explanations, then measuring whether model predictions and explanations change consistently. EDCT extracts visual concepts from explanations, creates minimal image edits using generative inpainting, and computes a Counterfactual Consistency Score (CCS) through LLM-assisted analysis of prediction and explanation changes.

The framework is evaluated on 120 OK-VQA examples across multiple VLMs, revealing substantial faithfulness gaps between different models. Gemini 2.5 Flash achieves the highest CCS score of 0.674, while open-source models score lower. The approach provides regulator-aligned audit artifacts for verifying causal claims in model explanations and offers a principled method for assessing whether visual concepts mentioned in explanations actually causally influence model predictions.

## Method Summary
EDCT operates through a pipeline that begins with extracting visual concepts from a VLM's natural language explanation using LLM-assisted analysis. These concepts are then used to generate counterfactual images through controlled inpainting, creating minimal edits that remove or alter the cited visual elements. The original and counterfactual images are then presented to the VLM, and both predictions and explanations are compared. An LLM judge evaluates the consistency between changes in predictions and changes in explanations, producing a Counterfactual Consistency Score (CCS) that quantifies explanation faithfulness. The framework uses segmentation masks to guide edits and employs an ensemble-judge variant to improve robustness, though it acknowledges that scores can vary based on judge selection and prompting.

## Key Results
- Gemini 2.5 Flash achieves the highest Counterfactual Consistency Score (CCS) of 0.674 among tested models
- Open-source vision-language models score significantly lower on faithfulness metrics compared to proprietary models
- The framework successfully identifies specific faithfulness gaps in model explanations across 120 OK-VQA examples
- Automated counterfactual testing reveals that many VLMs fail to maintain consistent reasoning when visual concepts in their explanations are altered

## Why This Works (Mechanism)
The framework works by treating explanations as scientific hypotheses that can be empirically tested through controlled intervention. By systematically removing or altering visual concepts that models claim are important for their reasoning, EDCT can directly test whether these claims correspond to actual causal relationships. The use of generative inpainting allows for minimal, targeted modifications that isolate specific visual elements, while the LLM judge provides automated consistency analysis between prediction changes and explanation changes. This creates a scalable, automated approach to faithfulness evaluation that moves beyond static correlation-based metrics to active causal testing.

## Foundational Learning
- **Counterfactual testing:** Creating modified inputs where specific features are altered to test causal relationships; needed to empirically validate explanation claims, quick check: can you explain why counterfactuals are more powerful than correlation analysis for testing explanations?
- **Vision-language model explanations:** Natural language rationales generated by VLMs for their predictions; needed as the target of faithfulness evaluation, quick check: what distinguishes faithful from plausible explanations in VLMs?
- **Generative inpainting:** AI-based image editing that can remove or modify specific visual elements; needed to create controlled counterfactuals, quick check: how does inpainting quality affect counterfactual validity?
- **LLM-based consistency scoring:** Using language models to evaluate logical consistency between predictions and explanations; needed for automated faithfulness assessment, quick check: what are the limitations of using LLMs as judges for explanation quality?
- **Visual concept extraction:** Identifying specific objects, attributes, or relationships mentioned in explanations; needed to guide counterfactual generation, quick check: how might noisy concept extraction propagate errors through the pipeline?

## Architecture Onboarding

**Component Map:** Visual Concept Extraction -> Counterfactual Image Generation -> VLM Prediction/Explanation -> LLM Consistency Scoring -> CCS Calculation

**Critical Path:** The core pipeline flows from concept extraction through counterfactual generation to consistency evaluation. Each stage depends on the previous: accurate concept extraction is essential for meaningful counterfactuals, which in turn must be realistic enough to produce valid predictions, and the LLM judge must correctly assess consistency for reliable CCS scores.

**Design Tradeoffs:** The framework trades computational cost and complexity for rigorous faithfulness evaluation. Using generative models for counterfactuals provides more realistic edits than simple image manipulation but introduces uncertainty about edit quality. LLM judges enable scalable evaluation but may conflate semantic understanding with faithfulness assessment. The approach requires multiple model calls per example but provides more actionable insights than correlation-based metrics.

**Failure Signatures:** Low CCS scores indicate explanations that don't causally align with predictions, suggesting the model is relying on spurious correlations or using different reasoning than stated. Poor inpainting quality can lead to invalid counterfactuals that don't isolate target concepts properly. Noisy concept extraction can propagate errors through the pipeline. LLM judge inconsistency may indicate prompt quality issues or ambiguity in explanation interpretation.

**First 3 Experiments:** 1) Run EDCT on a simple VLM with known faithfulness issues to validate the framework detects expected problems. 2) Test the framework on cases where explanations are known to be faithful to establish baseline CCS values. 3) Perform ablation studies varying the LLM judge and prompting to quantify their impact on CCS scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can EDCT be adapted to handle temporal consistency in video or dialogue-based VLM applications?
- **Basis in paper:** The authors note the current scope is limited to "VQA-style NLEs" and that "extensions to dialog/video require temporal edits and persistence checks."
- **Why unresolved:** The existing pipeline relies on static image inpainting, which cannot assess faithfulness in dynamic scenes where reasoning occurs over time.
- **What evidence would resolve it:** An extension of the framework incorporating temporal counterfactuals applied to video-VQA benchmarks.

### Open Question 2
- **Question:** To what extent does the choice of LLM judge introduce variance into the Counterfactual Consistency Score (CCS)?
- **Basis in paper:** The paper acknowledges that "scores can vary by judge and prompting" and suggests mitigating this with "robustness checks and an ensemble-judge variant."
- **Why unresolved:** While ablations show LLM choice impacts scores, the sensitivity of the final faithfulness assessment to specific judge biases remains unquantified.
- **What evidence would resolve it:** A comparative study correlating LLM-judge scores with human expert evaluations across a diverse set of explanations.

### Open Question 3
- **Question:** Does guiding counterfactual generation with segmentation masks significantly improve the validity of faithfulness tests?
- **Basis in paper:** The authors list using "segmentation masks to guide edits" as a specific method to ensure counterfactual images are realistic and localized.
- **Why unresolved:** Without strict localization, generative models may inadvertently alter unintended visual features, making it unclear if a failed test is due to model unfaithfulness or poor edit quality.
- **What evidence would resolve it:** Ablation experiments comparing mask-guided vs. text-only editing using pixel-level metrics (e.g., LPIPS) to measure edit locality.

## Limitations
- Framework relies on generative inpainting that may introduce confounding artifacts when altering visual concepts
- LLM-based consistency assessment depends heavily on scoring prompt quality and may conflate semantic understanding with faithfulness
- Method assumes extracted visual concepts are accurate, but noisy concept extraction could propagate errors
- Study limited to 120 OK-VQA examples, potentially missing diversity of explanation types across VLMs

## Confidence
- **High Confidence:** The core methodology of using counterfactual testing to evaluate explanation faithfulness is sound and represents a novel contribution to the field
- **Medium Confidence:** The Counterfactual Consistency Score (CCS) as a quantitative metric shows promise but requires further validation across diverse datasets and explanation types
- **Medium Confidence:** The framework's effectiveness in identifying specific faithfulness gaps in VLMs is demonstrated, but practical implications for improving model explanations need more investigation

## Next Checks
1. Conduct ablation studies on the LLM scoring prompt and evaluation criteria to quantify their impact on CCS results and establish more rigorous validation protocols
2. Expand testing to include a broader range of VQA datasets and explanation types, including negative cases and more complex reasoning chains, to assess the framework's generalizability
3. Implement human evaluation studies to compare LLM-based consistency scoring with expert judgments, establishing the correlation between automated metrics and human assessment of explanation faithfulness