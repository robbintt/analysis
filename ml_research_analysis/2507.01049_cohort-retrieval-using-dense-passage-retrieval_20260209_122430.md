---
ver: rpa2
title: Cohort Retrieval using Dense Passage Retrieval
arxiv_id: '2507.01049'
source_url: https://arxiv.org/abs/2507.01049
tags:
- retrieval
- clinical
- tasks
- cohort
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies Dense Passage Retrieval (DPR) to the task of
  patient cohort retrieval in echocardiography. The authors transform unstructured
  EHR reports into a query-passage dataset, design clinical evaluation sets, and train
  a custom DPR model with modified loss for improved performance.
---

# Cohort Retrieval using Dense Passage Retrieval

## Quick Facts
- arXiv ID: 2507.01049
- Source URL: https://arxiv.org/abs/2507.01049
- Authors: Pranav Jadhav
- Reference count: 16
- This paper applies Dense Passage Retrieval (DPR) to the task of patient cohort retrieval in echocardiography, showing strong performance in held-out and paraphrased query tasks but struggling with out-of-distribution conditions.

## Executive Summary
This paper applies Dense Passage Retrieval (DPR) to the task of patient cohort retrieval in echocardiography. The authors transform unstructured EHR reports into a query-passage dataset, design clinical evaluation sets, and train a custom DPR model with modified loss for improved performance. They compare their model against traditional and off-the-shelf methods, showing strong performance in held-out and paraphrased query tasks. However, the model struggles with out-of-distribution conditions, indicating challenges in generalizing to rare cases. The work establishes a framework for domain-specific cohort retrieval, with potential adaptation to other medical domains.

## Method Summary
The authors create a query-passage dataset from echocardiography reports by parsing unstructured findings into ~400 unique queries and extracting summary passages from patient reports. They implement a modified Multiple Negatives Ranking Loss that masks in-batch positives to prevent misleading gradients when queries share relevant passages. The model is trained on msmarco-bert-base-dot-v5 with triplet sampling including hard negatives from different subcategories within the same condition, plus template-based augmentation for numerical queries. Three variants are evaluated: v0 (base), v1 (+numerical/paraphrase augmentation), and v2 (+MLM pretraining).

## Key Results
- Strong in-distribution performance: v2 achieves P@10 of 0.87 on held-out evaluation
- Significant improvement on numerical queries: v1 achieves P@10 of 0.98 vs 0.25 for v0
- Poor out-of-distribution generalization: all variants achieve near-zero P@10 (0.00-0.06) on rare conditions

## Why This Works (Mechanism)

### Mechanism 1
Modifying Multiple Negatives Ranking Loss to mask in-batch positives improves retrieval when queries share relevant passages. Standard MNR treats all other queries' positive passages as in-batch negatives, but in cohort retrieval, multiple queries can legitimately share the same relevant passage. The modification sets similarity scores of any passage that is a positive for any query in the batch to -∞, excluding them from gradient computation.

### Mechanism 2
Hard negative sampling from different subcategories within the same clinical condition creates more discriminative embeddings. Rather than random negatives, sampling from the same condition but different subcategory forces the model to distinguish clinically similar concepts that share vocabulary, improving fine-grained retrieval.

### Mechanism 3
Template-based augmentation for numerical queries teaches the model to associate quantitative constraints with passage content. Creating query templates with placeholders for operators and values, sampled randomly during training, enables the model to learn to map textual numerical expressions to relevant passages through repeated exposure.

## Foundational Learning

- **Dense Passage Retrieval (DPR) dual-encoder architecture**: Core paradigm where queries and passages are independently encoded into dense vectors; relevance computed via similarity. Paper uses shared encoder rather than separate. Quick check: Why might a shared encoder underperform separate encoders for asymmetric query-passage lengths?
- **In-batch negative sampling**: MNR loss implicitly treats other samples' positives as negatives. Understanding this is essential to grasp why shared passages corrupt gradients. Quick check: In a batch of [(q1, p1), (q2, p2)], what does standard MNR treat as negatives for q1?
- **Out-of-distribution (OOD) generalization in retrieval**: All model variants fail on rare conditions (0.00-0.06 P@10). Understanding why MLM pre-training didn't help informs realistic expectations. Quick check: Why does BM25 outperform DPR on OOD (0.48 vs 0.00) despite being simpler?

## Architecture Onboarding

- **Component map**:
  ```
  Data Layer
  ├── Query extraction: heuristic parsing of Findings → ~400 unique statements
  ├── Passage corpus: Summary sections → 42,996 passages
  └── Annotation: condition → subcategory → statement mapping (LLM-assisted)

  Training Layer
  ├── Triplet sampler: (query, positive passage, hard negative)
  ├── Modified MNR Loss: mask in-batch positives via -∞ similarity
  └── Variants: v0 (base) → v1 (+numerical/paraphrase aug) → v2 (+ MLM pre-train)

  Retrieval Layer
  ├── Encoder: msmarco-bert-base-dot-v5 (shared, 768-dim)
  ├── Pooling: mean pooling
  └── Search: dot product similarity
  ```

- **Critical path**: Query-passage mapping quality → Hard negative construction → Modified loss implementation
- **Design tradeoffs**: Shared encoder reduces parameters but may limit query/passage specialization; MLM pre-training adds cost without OOD benefit; template-based numerical handling works for predictable expressions but won't generalize to complex reasoning
- **Failure signatures**: OOD failure (0.00-0.06 P@10): Rare conditions completely missed; DPR has no term-match fallback. v0 numerical failure (0.25 P@10): Resolved by template augmentation in v1 (0.98)
- **First 3 experiments**:
  1. Reproduce v0 on held-out set: Expect P@10 ~0.85. Validates pipeline.
  2. Ablate hard negatives: Compare same-condition subcategory negatives vs random vs neutral. Hypothesis: hard negatives improve R-Precision on similar conditions.
  3. Hybrid retrieval for OOD: Combine DPR with BM25 (lexical fallback). If OOD is critical, this may recover rare-condition retrieval without sacrificing dense retrieval on in-distribution queries.

## Open Questions the Paper Calls Out

- **How can DPR-based cohort retrieval models be trained to better incorporate out-of-distribution (OOD) knowledge and improve generalization to rare or previously unseen conditions?**: MLM pre-training with OOD conditions did not transfer effectively to fine-tuned models, suggesting the standard pre-training/fine-tuning paradigm is insufficient for rare condition generalization in dense retrieval.
- **What is the impact of different model architectures and parameter scales on cohort retrieval performance in medical domains?**: This study only tested one architecture (msmarco-bert-base-dot-v5 with ~110M parameters), leaving the scalability and architectural sensitivity of the approach unknown.
- **How does extending cohort retrieval beyond a single cardiac anatomy affect model performance and retrieval complexity?**: Multi-anatomy retrieval may introduce cross-anatomy reasoning challenges that single-anatomy training does not address.
- **Can the proposed modified MNR loss and training framework effectively generalize to cohort retrieval tasks in other medical domains?**: Different medical domains have distinct terminology, note structures, and condition co-occurrence patterns that may require domain-specific adaptations.

## Limitations

- Custom annotation pipeline mapping unstructured findings to condition/subcategory labels is LLM-assisted but not fully specified, creating reproducibility challenges.
- OOD generalization failure (0.00-0.06 P@10 on rare conditions) suggests the model may be memorizing training distribution rather than learning transferable representations.
- The confidential nature of annotations prevents direct validation of the mapping methodology.

## Confidence

- **High confidence**: In-distribution retrieval performance (P@10: 0.85-0.87), modified MNR loss improves training, hard negative sampling enhances discrimination
- **Medium confidence**: Template-based numerical query augmentation efficacy (only one comparison point), template coverage for all numerical patterns
- **Low confidence**: MLM pre-training value (v2 underperforms v1 in some metrics despite additional complexity), scalability to other clinical domains without custom annotation pipelines

## Next Checks

1. **Ablation study on hard negatives**: Compare same-condition subcategory negatives vs random vs neutral negatives to quantify contribution to R-Precision improvement
2. **Hybrid retrieval for OOD**: Combine DPR with BM25 as lexical fallback to assess if rare-condition retrieval can be recovered without sacrificing in-distribution performance
3. **Template coverage audit**: Systematically test numerical query templates against diverse LVEF expressions in the corpus to identify gaps in the augmentation strategy