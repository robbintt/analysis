---
ver: rpa2
title: 'Calibration Is Not Enough: Evaluating Confidence Estimation Under Language
  Variations'
arxiv_id: '2601.08064'
source_url: https://arxiv.org/abs/2601.08064
tags:
- confidence
- answer
- methods
- prompt
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies that existing confidence estimation (CE)
  methods for large language models (LLMs) often fail under language variations, despite
  achieving good calibration or discrimination metrics. The authors propose a comprehensive
  evaluation framework that measures CE quality beyond calibration, focusing on three
  new aspects: robustness to prompt perturbations, stability across semantically equivalent
  answers, and sensitivity to semantically different answers.'
---

# Calibration Is Not Enough: Evaluating Confidence Estimation Under Language Variations

## Quick Facts
- arXiv ID: 2601.08064
- Source URL: https://arxiv.org/abs/2601.08064
- Reference count: 40
- The paper identifies that existing confidence estimation methods often fail under language variations despite achieving good calibration or discrimination metrics.

## Executive Summary
This paper reveals that current confidence estimation (CE) methods for large language models (LLMs) fail to maintain quality under language variations, despite strong calibration scores. The authors propose a comprehensive evaluation framework measuring CE quality through three new aspects: robustness to prompt perturbations, stability across semantically equivalent answers, and sensitivity to semantically different answers. Experiments with five CE methods across nine LLMs show that methods achieving good Brier scores or AUROC can still perform poorly on robustness or sensitivity, highlighting the need for more nuanced evaluation beyond traditional calibration metrics.

## Method Summary
The study evaluates five confidence estimation methods: Probability (token probability normalization), Platt Scaling (scalar calibration), Verbalized Confidence (direct prompting), P(True) (self-evaluation), and Calib-1-Focal (BERT-based classifier with focal loss). Using four QA datasets and nine LLMs, the framework measures three aspects beyond calibration: P-RB (prompt robustness), A-STB (answer stability), and A-SST (answer sensitivity). Semantic equivalence is judged by GPT-4o. The study reveals fundamental trade-offs between stability and sensitivity, with post-hoc and auxiliary methods showing high robustness and stability but low sensitivity, while logit-based and self-evaluation methods show the opposite pattern.

## Key Results
- Methods with strong Brier scores or AUROC can still perform poorly on robustness or sensitivity
- P(True) is highly sensitive to prompt changes
- Platt Scaling and Calib-1 lack sensitivity to answer changes
- Verbalized Confidence is robust for large models but not for smaller ones
- There exists a fundamental trade-off between stability and sensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence estimation methods that rely directly on prompting (VC, P(True)) are more susceptible to prompt perturbations than post-hoc or auxiliary methods.
- Mechanism: Verbalized confidence and self-evaluation methods must parse and interpret the prompt's structure to generate confidence outputs. Changes in wording, scale format, or linguistic framing alter the internal representations that produce confidence scores.
- Core assumption: The prompt directly influences the model's confidence generation process, and this influence varies by model size and capability.
- Evidence anchors:
  - [abstract]: "Specifically, P(True) is highly sensitive to prompt changes"
  - [Section 6, Finding 2]: "VC methods, which directly rely on prompting, are the most sensitive to prompt perturbations"
  - [corpus]: Related work (arXiv:2501.03991) confirms prompt style affects calibration generalization

### Mechanism 2
- Claim: Calibration metrics (Brier score, ECE) can be gamed by predictors that learn input difficulty rather than answer-specific correctness signals.
- Mechanism: A "constant predictor" that estimates question difficulty independently of the actual generated answer can achieve strong calibration scores while providing no useful information about whether a specific answer is correct.
- Core assumption: CE methods may capture statistical patterns in the data distribution rather than grounding confidence in the semantic content of generated answers.
- Evidence anchors:
  - [Section 3.2]: "fconstant... achieves perfect ECE, a relatively low Brier score of 0.17 and an AUROC of 0.83" while ignoring answer correctness
  - [Section 6, Finding 1]: "methods that achieve good performance on calibration or discrimination are not robust to prompt variations or are not sensitive to answer changes"

### Mechanism 3
- Claim: There is a fundamental trade-off between stability (consistency across equivalent answers) and sensitivity (differentiation across distinct answers).
- Mechanism: Methods that smooth confidence estimates to reduce variance achieve high stability but may fail to distinguish semantically different answers. Conversely, highly sensitive methods respond to surface-level changes that shouldn't affect confidence.
- Core assumption: Confidence should be invariant to semantic-preserving transformations but responsive to meaning changes—a hard constraint for neural methods.
- Evidence anchors:
  - [Section 6, Finding 3]: "Calib1, VC and PS are the most stable... but only P(True) and Prob. effectively distinguish different answers"
  - [Figure 4]: Stability and sensitivity show -0.62 Pearson correlation

## Foundational Learning

- Concept: **Expected Calibration Error (ECE) vs. Brier Score**
  - Why needed here: The paper critiques calibration-only evaluation; understanding these metrics reveals why they're insufficient.
  - Quick check question: Why does the paper argue that ECE is not a "proper scoring rule"?

- Concept: **Semantic Equivalence Detection**
  - Why needed here: The stability and sensitivity metrics require determining whether two answers have the same meaning.
  - Quick check question: How does the paper operationalize semantic equivalence in their experiments?

- Concept: **Post-hoc Calibration (Platt Scaling)**
  - Why needed here: PS represents a fundamentally different approach to CE than prompting-based methods.
  - Quick check question: Why does Platt Scaling achieve high robustness but low sensitivity?

## Architecture Onboarding

- Component map:
  - Five CE methods (Prob., PS, VC, P(True), Calib-1) that accept (answer, input, prompt) and output scalar confidence
  - GPT-4o-as-judge for grouping answers by semantic equivalence
  - Evaluation framework computing P-RB, A-STB, A-SST, plus traditional Brier/AUROC

- Critical path:
  1. Generate answers with temperature=0.7 for stability/sensitivity metrics (sampling)
  2. Generate answers with temperature=0 for robustness metrics (greedy)
  3. Apply CE method to each (answer, prompt) pair
  4. Compute metrics via standard deviation across groups

- Design tradeoffs:
  - Post-hoc methods (PS, Calib1): High robustness + stability, low sensitivity—choose when prompt consistency matters more than detecting answer changes
  - Logit/self-evaluation methods (Prob., P(True)): High sensitivity, moderate robustness—choose when distinguishing answers is critical
  - VC: Model-size dependent; reliable only for large models (>70B parameters per results)

- Failure signatures:
  - P(True) showing >0.5 confidence variance across prompt variants → prompt sensitivity failure
  - PS assigning nearly identical confidence to "Ovary" vs. "Fallopian tube" → sensitivity failure
  - VC confidence fluctuating wildly on smaller models (OLMo-7B, OLMo-13B) → model capability mismatch

- First 3 experiments:
  1. **Baseline audit**: Run all five CE methods on your deployment model with 10 answer-elicit prompt variants; compute P-RB to identify robustness failures before production.
  2. **Sensitivity stress test**: Sample 10 answers per question at temperature=0.7, manually verify that semantically different answers receive meaningfully different confidence scores (A-SST > 0.1).
  3. **Model-size validation for VC**: If using verbalized confidence, validate on your specific model; the paper shows VC robustness drops sharply below 32B parameters.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the observed trade-offs between stability and sensitivity persist in multilingual settings or open-ended tasks like reasoning and dialogue?
  - Basis in paper: [explicit] The authors state in the Limitations section that the analysis is "limited to English and fact-based question answering tasks" and that extending to "multilingual or open-ended domains... may reveal different findings."
  - Why unresolved: The current study only validates the proposed metrics on fact-based QA in English, leaving the behavior of CE methods on other languages and generation tasks unknown.
  - What evidence would resolve it: An evaluation of the five CE methods using the P-RB, A-STB, and A-SST metrics on multilingual benchmarks (e.g., MGSM) and open-ended generation datasets (e.g., MT-Bench).

- **Open Question 2**: Can a unified confidence estimation method be developed to optimize robustness, stability, and sensitivity simultaneously?
  - Basis in paper: [inferred] The paper concludes that "method choice should depend on application priorities" due to trade-offs; for example, finding that methods with high stability (PS, Calib1) often lack sensitivity, while sensitive methods (P(True)) lack robustness.
  - Why unresolved: The results show that existing methods occupy different points on the trade-off spectrum, but the paper does not propose or test a method that balances all three aspects effectively.
  - What evidence would resolve it: The development of a new CE method or an ensemble approach that achieves top-tier performance across all three proposed metrics (P-RB, A-STB, A-SST) in addition to calibration.

- **Open Question 3**: How do the proposed metrics behave in dynamic or interactive settings where models can revise their confidence based on feedback?
  - Basis in paper: [explicit] The authors note in the Limitations that "our evaluation is static" and suggest that "future work could explore dynamic or interactive settings where models can revise their confidence based on feedback or uncertainty cues."
  - Why unresolved: The current framework assumes a single-pass generation and evaluation process, whereas real-world use cases often involve iterative refinement or dialogue where confidence might need to be updated.
  - What evidence would resolve it: A study applying the P-RB, A-STB, and A-SST metrics to a conversational agent that updates its confidence estimates after receiving user feedback or self-correction cues.

- **Open Question 4**: How sensitive are the stability and sensitivity metrics to the choice of the semantic equivalence judge model?
  - Basis in paper: [inferred] The paper relies on GPT-4o to determine semantic equivalence for grouping answers (Section 5), but acknowledges in the Limitations that "it may introduce model-specific biases" and that this is a potential weakness.
  - Why unresolved: If the judge model systematically misclassifies semantic equivalence, the stability (A-STB) and sensitivity (A-SST) scores for the evaluated CE methods could be artificially inflated or deflated.
  - What evidence would resolve it: A comparative analysis of metric scores when using different judge models (e.g., Claude 3.5, Gemini) or human annotations to define the ground truth semantic groups $\hat{Y}$.

## Limitations

- The study's experimental design relies on a relatively small set of nine LLMs from five families, which may not fully represent the diversity of modern language models.
- The semantic equivalence judgments from GPT-4o introduce potential subjectivity that could affect the stability and sensitivity metrics.
- The Calib-1 method's training details (learning rate, epochs, focal loss parameter) remain underspecified, which may impact reproducibility and comparisons with other methods.

## Confidence

- **High confidence**: The fundamental observation that calibration metrics (Brier score, ECE) can be gamed by methods that ignore answer content while still appearing well-calibrated. The experimental evidence with the "constant predictor" and the correlation between traditional metrics and robustness/sensitivity are robust findings.
- **Medium confidence**: The characterization of trade-offs between robustness, stability, and sensitivity across different CE method families. While the experimental results support this, the generalizability to models outside the tested set and to other domains requires further validation.
- **Medium confidence**: The specific performance rankings of methods (e.g., VC being robust only for large models) are well-supported within the tested conditions but may not generalize to all deployment scenarios or model architectures.

## Next Checks

1. **Cross-dataset validation**: Test the five CE methods on an additional domain (e.g., medical diagnosis or legal reasoning) with 500+ samples to verify whether the robustness-stability-sensitivity trade-offs hold across different knowledge domains.

2. **Extended model coverage**: Evaluate the same CE methods on at least three additional LLM families (including small models <7B parameters and specialized models like Claude or Gemini) to test whether the size-dependent VC findings generalize beyond the tested OLMo models.

3. **Judge reliability assessment**: Conduct a small-scale human evaluation (n=50 samples) comparing GPT-4o's semantic equivalence judgments against human annotators to quantify the potential noise introduced by automated semantic equivalence detection in the stability and sensitivity metrics.