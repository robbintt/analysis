---
ver: rpa2
title: Compartmentalised Agentic Reasoning for Clinical NLI
arxiv_id: '2509.10222'
source_url: https://arxiv.org/abs/2509.10222
tags:
- reasoning
- solver
- premise
- statement
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Schema collapse limits large language model (LLM) reliability in
  clinical natural language inference (NLI). Models encode relevant facts but fail
  to apply reasoning-type-specific inferential schemas, leading to fluent yet invalid
  judgments.
---

# Compartmentalised Agentic Reasoning for Clinical NLI

## Quick Facts
- arXiv ID: 2509.10222
- Source URL: https://arxiv.org/abs/2509.10222
- Reference count: 22
- Schema collapse limits LLM reliability in clinical NLI; compartmentalised reasoning improves accuracy from ~23% to ~57%

## Executive Summary
Large language models struggle with clinical natural language inference due to schema collapse, where they encode relevant facts but fail to apply reasoning-type-specific inferential schemas, leading to fluent yet invalid judgments. CARENLI addresses this by decomposing inference into routing, solver, verifier, and refiner agents, enforcing schema-constrained decision procedures. Evaluated on an expanded CTNLI benchmark with 200 instances across four reasoning families, CARENLI achieves an average improvement of roughly 34 percentage points in macro accuracy.

## Method Summary
CARENLI implements a multi-agent architecture that compartmentalises clinical NLI into distinct reasoning phases. The system routes premises and hypotheses to appropriate reasoning agents based on inferred reasoning types, then applies schema-constrained inference procedures within each agent. A verifier checks intermediate outputs, and a refiner adjusts final judgments. The approach is evaluated on an expanded CTNLI benchmark with 200 instances across four reasoning families (Causal Attribution, Compositional Grounding, Epistemic Verification, Risk State Abstraction), comparing performance against baseline LLMs.

## Key Results
- Macro accuracy improves from approximately 23% to approximately 57%
- Average gain of roughly 34 percentage points across four LLMs
- Largest gains for structurally demanding families: Risk State Abstraction (+51.9%), Epistemic Verification (+40%)

## Why This Works (Mechanism)
CARENLI works by explicitly compartmentalising reasoning into distinct phases with schema-constrained procedures. This prevents the schema collapse that occurs when LLMs try to handle all reasoning types with a single general approach. By routing to specialised agents and enforcing reasoning-type-specific schemas, the system maintains structural integrity of inferences while still leveraging LLM capabilities for pattern recognition and language understanding.

## Foundational Learning
- Schema-constrained reasoning: Why needed - prevents collapse of distinct reasoning types into single patterns; Quick check - verify agent routing correctly identifies reasoning family
- Multi-agent verification: Why needed - catches errors in individual reasoning steps; Quick check - measure improvement when verifier is active vs disabled
- Compositional grounding: Why needed - handles complex clinical relationships requiring multiple inferential steps; Quick check - test on nested premise-hypothesis pairs
- Epistemic verification: Why needed - distinguishes between fact and possibility in clinical assertions; Quick check - evaluate performance on explicitly uncertain statements
- Risk state abstraction: Why needed - handles probabilistic reasoning about patient outcomes; Quick check - test on comparative risk statements

## Architecture Onboarding

Component Map:
Premise -> Router -> Solver -> Verifier -> Refiner -> Final Judgment

Critical Path:
Router → Solver → Verifier → Refiner

Design Tradeoffs:
- Specialisation vs. generalisation: Schema-constrained agents sacrifice some flexibility for reliability
- Complexity vs. interpretability: Multi-agent system is more auditable but computationally heavier
- Verification overhead: Verifier adds latency but catches critical errors

Failure Signatures:
- Incorrect routing to wrong reasoning family
- Solver applying wrong schema for the reasoning type
- Verifier missing subtle logical inconsistencies
- Refiner overcorrecting valid judgments

First Experiments:
1. Route simple premise-hypothesis pairs to verify correct reasoning family identification
2. Test solver on single-step reasoning to validate schema application
3. Verify verifier catches obvious logical contradictions

## Open Questions the Paper Calls Out
None

## Limitations
- Generalisability beyond tested reasoning families remains uncertain
- Synthetic data augmentation may not capture real-world clinical text variability
- Multi-agent architecture resource and latency implications not evaluated

## Confidence
High: Schema collapse limits LLM reliability and compartmentalisation improves performance within benchmark
Medium: Magnitude of improvement across reasoning families given synthetic evaluation data
Low: Generalisability to broader clinical reasoning tasks and real-world deployment feasibility

## Next Checks
1. Evaluate CARENLI on real-world clinical corpus with diverse reasoning types beyond four families tested
2. Conduct human evaluation comparing CARENLI's inferences against clinical expert judgments
3. Measure computational overhead and latency of multi-agent architecture versus baseline approaches