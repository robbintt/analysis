---
ver: rpa2
title: 'Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and
  Proprietary Models'
arxiv_id: '2502.15155'
source_url: https://arxiv.org/abs/2502.15155
tags:
- speech
- extreme
- llama
- hate
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses extreme speech classification by evaluating
  Large Language Models (LLMs) on the Indian subset of the Xtreme Speech Dataset.
  The authors compare zero-shot inference, fine-tuning, preference optimization, and
  ensembling approaches using open-source Llama models (1B to 70B parameters) and
  closed-source GPT models.
---

# Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models

## Quick Facts
- arXiv ID: 2502.15155
- Source URL: https://arxiv.org/abs/2502.15155
- Reference count: 36
- Primary result: Fine-tuning Llama models significantly improves extreme speech classification accuracy, matching or exceeding previous state-of-the-art methods

## Executive Summary
This paper evaluates Large Language Models (LLMs) for extreme speech classification on the Indian subset of the Xtreme Speech Dataset. The authors compare zero-shot inference, fine-tuning, preference optimization, and ensembling approaches across open-source Llama models (1B to 70B parameters) and closed-source GPT models. While pre-trained LLMs show moderate zero-shot performance, fine-tuning significantly enhances classification accuracy across all models. GPT models slightly outperform Llama models in zero-shot settings, but this gap disappears after fine-tuning. Fine-tuned Llama models, even smaller variants, outperform previous state-of-the-art methods (SVM, langBERT, mBERT).

## Method Summary
The study evaluates extreme speech classification using the Indian subset of the Xtreme Speech Dataset (4,933 samples across three categories: derogatory, exclusionary, dangerous). The authors employ zero-shot inference with Chain-of-Thought prompting on Llama and GPT models, then perform Supervised Fine-Tuning (SFT) using only labels (not justifications). They also test Direct Preference Optimization (DPO) on SFT-ed models and ensemble multiple fine-tuned models through weighted voting. Performance is measured using F1-macro and per-class F1 scores on a held-out test set.

## Key Results
- Fine-tuning LLMs significantly improves extreme speech classification performance across all model sizes and types
- GPT models outperform Llama models in zero-shot settings, but this gap disappears after fine-tuning
- Fine-tuned Llama models, even smaller variants, outperform previous state-of-the-art methods (SVM, langBERT, mBERT)
- Preference optimization and ensembling did not provide additional performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning aligns model weights with specific socio-cultural definitions of "extreme speech" that general pre-training fails to capture
- Mechanism: Pre-trained LLMs possess general linguistic knowledge but lack specific boundary definitions for nuanced categories like "derogatory" vs. "exclusionary" speech in the Indian context. Supervised Fine-Tuning (SFT) adjusts weights to minimize classification error on these specific categories, effectively injecting domain-specific cultural context
- Core assumption: The performance gain is driven by the acquisition of cultural nuance rather than mere memorization of the dataset's linguistic patterns
- Evidence anchors: [abstract] "fine-tuning with domain-specific data significantly enhances performance, highlighting their adaptability to linguistic and contextual nuances"; [section 5.2] "incorporating cultural context helps models better identify extreme speech in the Indian dataset"

### Mechanism 2
- Claim: Constrained output spaces (label-only) optimize classification accuracy better than generative explanations during fine-tuning
- Mechanism: In zero-shot settings, generating justifications (CoT) helps the model "reason" toward a label. However, during SFT, including justifications splits the model's capacity between generating text and predicting the class. Training with only the label allows the loss function to exclusively minimize the error on the class token
- Core assumption: The model has sufficient pre-existing knowledge to map the input text to the class label without needing to generate intermediate reasoning steps during training
- Evidence anchors: [section 4.2] "the variant with justifications underperformed compared to the label-only approach... training with only labels directly minimizes classification error"

### Mechanism 3
- Claim: Ensembling fails to yield gains when individual models are trained on identical data distributions, leading to correlated error patterns
- Mechanism: Ensembling improves performance primarily when individual models make uncorrelated errors (diversity). When models (even of different sizes like 1B vs 70B) are fine-tuned on the same dataset, they learn identical biases and failure modes. Consequently, the "vote" adds no new information
- Core assumption: The homogeneity introduced by SFT overrides the diversity inherent in different model architectures/sizes
- Evidence anchors: [section 5.3] "various fine-tuned models exhibit similar strengths and weaknesses... homogeneity in the fine-tuned models' performance limits the ensemble's ability"

## Foundational Learning

- **Concept: Zero-Shot vs. Fine-Tuning**
  - Why needed here: The paper establishes a massive performance delta between these two modes. Understanding that LLMs are generalists by default but specialists after SFT is critical for interpreting the results
  - Quick check question: Does the model need to generalize to a new task it has never seen (zero-shot), or do we have labeled examples to teach it a specific behavior (fine-tuning)?

- **Concept: F1-Score (Macro vs. Weighted)**
  - Why needed here: The dataset is imbalanced (e.g., Derogatory: 2190 samples vs. Dangerous: 1346). Accuracy would be misleading; F1-macro treats all classes equally, exposing performance on minority classes
  - Quick check question: If a model predicts the majority class 100% of the time and ignores the minority class, is F1-macro high or low?

- **Concept: Quantization (4-bit)**
  - Why needed here: The authors used 4-bit quantization for Llama models. This reduces memory usage but introduces a trade-off in precision/capacity, potentially impacting the nuance required for "exclusionary" speech detection
  - Quick check question: Why might a compressed model struggle more with subtle linguistic nuances than a full-precision model?

## Architecture Onboarding

- **Component map:** Text sample -> Llama/GPT encoder -> Classification head -> Class probability
- **Critical path:** Dataset prep (train/test/ensemble split) -> Zero-shot baseline with CoT -> SFT using labels only -> Evaluation with F1-macro
- **Design tradeoffs:** GPT offers superior zero-shot reasoning (higher API cost, data privacy concerns) vs. Llama requiring upfront compute for fine-tuning but offering data sovereignty and matching performance post-training
- **Failure signatures:** Confusion between Derogatory and Exclusionary (relying on "humor" or "intent"); DPO degradation for pure classification tasks
- **First 3 experiments:** (1) Zero-shot baseline: Run CoT inference on Llama 3.1 8B and GPT-4o-mini, record F1 gap; (2) SFT ablation: Fine-tune Llama 3.2-3B, compare "Label-only" vs "Justification+Label" training; (3) Benchmark comparison: Evaluate fine-tuned 3B model against SVM/mBERT baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on Indian subset limits generalizability to other cultural contexts or languages
- Evaluation only considers three speech categories, potentially missing nuances in other harmful speech types
- 4-bit quantization for open-source models could impact performance compared to full-precision implementations
- Absence of cost-benefit analysis comparing fine-tuning expenses against performance gains

## Confidence
- **High Confidence:** Fine-tuning LLMs consistently improves extreme speech classification performance across all model sizes and types
- **Medium Confidence:** Smaller fine-tuned Llama models can match or exceed larger models in zero-shot settings; open-source models match closed-source models post-fine-tuning
- **Low Confidence:** Ensembling provides no benefit; mechanism explaining DPO's lack of improvement is speculative

## Next Checks
1. **Cross-Cultural Validation:** Evaluate the best-performing fine-tuned model (Llama 3.1 8B) on extreme speech datasets from different cultural contexts to assess transferability
2. **Quantization Impact Analysis:** Compare 4-bit quantized Llama 3.1 8B against full-precision implementation on the same test set to quantify the performance gap
3. **Ensemble Diversity Experiment:** Create ensemble combinations using models trained on different data subsets rather than identical datasets to test whether model diversity through varied training data is necessary for ensemble gains