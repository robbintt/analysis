---
ver: rpa2
title: Using Large Language Models for Abstraction of Planning Domains - Extended
  Version
arxiv_id: '2510.20258'
source_url: https://arxiv.org/abs/2510.20258
tags:
- domain
- abstraction
- room
- actions
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates using GPT-4o to automatically generate
  abstract PDDL planning domains and problem instances from concrete ones based on
  a given abstraction purpose. Three categories of abstractions are explored: abstracting
  over alternative concrete actions, sequences of actions, and action/predicate parameters.'
---

# Using Large Language Models for Abstraction of Planning Domains - Extended Version

## Quick Facts
- arXiv ID: 2510.20258
- Source URL: https://arxiv.org/abs/2510.20258
- Reference count: 40
- Primary result: GPT-4o successfully generates abstract PDDL domains with near 100% accuracy for parameter abstraction, but performance degrades with increasing complexity, especially for action sequence abstraction

## Executive Summary
This paper investigates the capability of GPT-4o to automatically generate abstract PDDL planning domains and problem instances from concrete specifications based on given abstraction purposes. The study explores three categories of abstractions: over alternative concrete actions, sequences of actions, and action/predicate parameters. Through experiments with 35 benchmark examples, the authors demonstrate that LLMs can effectively produce useful planning abstractions in simple settings, with particularly strong performance in parameter abstraction (near 100% accuracy), though quality degrades as abstraction complexity increases.

## Method Summary
The study uses GPT-4o accessed via Azure OpenAI service to generate abstract PDDL domains and problem instances from concrete ones. The method employs a two-stage generation process: first abstracting the domain, then the problem instance. For parameter abstraction, zero-shot prompting is used, while one-shot prompting (with an example case) is employed for sequence and alternative abstractions. The prompts incorporate chain-of-thought reasoning and role-play, with the LLM acting as a PDDL expert. Five runs are performed for each example, and outputs are validated using the VAL plan validator and Fast Downward planner, with human evaluation providing correctness scores (Changes Needed and Avoid Unnecessary Changes).

## Key Results
- GPT-4o achieves near 100% accuracy for parameter abstraction tasks
- Performance degrades with increasing abstraction complexity, with action sequence abstraction being most challenging
- Two-stage prompting (domain then problem) proves effective for abstraction generation
- LLM-generated abstractions are mostly correct for simple settings but make more mistakes as difficulty increases

## Why This Works (Mechanism)
GPT-4o's strong performance on planning domain abstraction stems from its ability to understand natural language abstraction purposes and map them to formal PDDL transformations. The model leverages its general reasoning capabilities and knowledge of planning domain structures to identify which actions, predicates, and parameters can be abstracted while maintaining the essential planning problem characteristics. The use of chain-of-thought prompting and role-play helps guide the model's reasoning process, while the two-stage approach allows for more systematic handling of domain-level and problem-level abstractions separately.

## Foundational Learning
**PDDL Planning Language**: Standard language for representing planning problems with domains and instances. Why needed: Provides the formal target format for LLM-generated abstractions. Quick check: Can the LLM parse and generate syntactically correct PDDL with actions, predicates, and parameters?

**Planning Domain Abstraction**: Process of generating a simpler abstract domain that preserves essential properties of the original. Why needed: Core task being evaluated - measuring LLM's ability to identify what can be safely abstracted. Quick check: Does the abstract domain allow solution of the original problem?

**STRIPS vs ADL Fragments**: STRIPS uses simple preconditions/effects; ADL allows disjunctive preconditions and conditional effects. Why needed: Study focuses on STRIPS, but ADL support is identified as future work. Quick check: Can the LLM handle conditional effects if present?

**Chain-of-Thought Prompting**: Technique where model explains reasoning before generating output. Why needed: Improves accuracy by making reasoning explicit. Quick check: Does adding intermediate reasoning steps improve output quality?

**Zero-shot vs One-shot Learning**: Zero-shot uses no examples; one-shot uses one example. Why needed: Different prompting strategies are tested for different abstraction types. Quick check: Does one-shot prompting improve results for complex abstractions?

## Architecture Onboarding

**Component Map**: Human benchmark examples -> GPT-4o API (Azure) -> PDDL outputs -> VAL validator + Fast Downward planner -> Human evaluation

**Critical Path**: Prompt construction → LLM generation → Syntax validation (VAL) → Plan generation (Fast Downward) → Human correctness evaluation

**Design Tradeoffs**: Zero-shot vs one-shot prompting trades generality for accuracy; two-stage generation adds complexity but improves systematic handling; manual human evaluation provides gold standard but limits scalability

**Failure Signatures**: Syntax errors detected by VAL indicate structural issues; planning failures indicate semantic problems; human CN/AUC scores identify over-abstracting or unnecessary changes

**First Experiments**: 1) Validate the 4 provided benchmark examples end-to-end using VAL and Fast Downward planner; 2) Conduct prompt engineering ablation study on available examples; 3) Test LLM on 5-10 new STRIPS domains to assess generalizability

## Open Questions the Paper Calls Out
1. Can automated validation mechanisms be developed to formally verify the correctness of LLM-generated abstract domains without relying on human expert evaluation?
2. Can LLMs be guided to generate formal refinement mappings alongside abstract domains to verify soundness and completeness?
3. Does the performance of LLMs degrade when generating abstractions in the more expressive ADL fragment of PDDL compared to the currently tested STRIPS fragment?

## Limitations
- Limited benchmark availability with only 4 of 36 examples fully specified in the appendix
- No exploration of parameter tuning or alternative prompting strategies beyond zero-shot and one-shot
- Focus on GPT-4o without comparison to other LLM models or fine-tuned planning systems
- Reliance on human evaluation for correctness assessment limits scalability

## Confidence
- High confidence: GPT-4o can successfully generate abstract PDDL domains for parameter abstraction (near 100% accuracy demonstrated)
- Medium confidence: GPT-4o performance degrades with increasing abstraction complexity, with sequence abstraction being most challenging
- Medium confidence: Two-stage prompting (abstract domain first, then problem instance) is an effective approach
- Low confidence: Results generalize to domains beyond the provided examples due to limited benchmark availability

## Next Checks
1. Replicate the 4 provided benchmark examples (rover domain, blocksworld, scout-rover, one-dimensional motion planning) with full end-to-end validation using VAL and Fast Downward planner
2. Conduct ablation study on prompt engineering - test zero-shot vs one-shot, different system prompts, and parameter tuning on the available examples
3. Extend validation to a small set of new STRIPS domains (5-10 additional examples) to assess generalizability beyond the original benchmark set