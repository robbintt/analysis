---
ver: rpa2
title: 'CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics
  Research'
arxiv_id: '2510.11985'
source_url: https://arxiv.org/abs/2510.11985
tags:
- evidence
- code
- variant
- task
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CGBENCH evaluates language models\u2019 ability to interpret scientific\
  \ literature for clinical genetics tasks like variant and gene curation. Built from\
  \ ClinGen\u2019s expert-annotated Evidence Repository, it tests three tasks: extracting\
  \ experimental evidence, scoring evidence strength, and verifying evidence codes\
  \ under precise protocols."
---

# CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research

## Quick Facts
- **arXiv ID**: 2510.11985
- **Source URL**: https://arxiv.org/abs/2510.11985
- **Reference count**: 40
- **Primary result**: CGBENCH benchmark reveals significant gaps in language models' ability to precisely interpret clinical genetics literature for variant and gene curation tasks

## Executive Summary
CGBENCH is a new benchmark designed to evaluate language models' ability to interpret scientific literature for clinical genetics research. Built from ClinGen's expert-annotated Evidence Repository, it tests three core tasks: extracting experimental evidence, scoring evidence strength, and verifying evidence codes under precise protocols. The benchmark reveals substantial performance gaps in precise literature interpretation, particularly for specialized instructions, highlighting critical challenges for future AI research in biomedicine.

## Method Summary
The authors developed CGBENCH using data from ClinGen's Evidence Repository, which contains expert-annotated articles with gene-disease assertions and associated evidence. The benchmark evaluates three tasks: evidence extraction (identifying relevant experimental findings), evidence scoring (rating the strength of evidence), and evidence code verification (checking if evidence meets specific criteria). They tested 8 language models, including both reasoning models (Deepseek-R1, o4-mini) and non-reasoning models, using both zero-shot and few-shot prompting approaches. The evaluation also employed LM-as-a-judge methodology to assess explanation quality.

## Key Results
- Reasoning models (Deepseek-R1, o4-mini) excel at fine-grained evidence extraction but struggle with high-level code scoring
- Non-reasoning models perform better at overall classification tasks compared to reasoning models
- LM-as-a-judge evaluation revealed that even correct classifications often have hallucinated or misinterpreted explanations

## Why This Works (Mechanism)
The benchmark works by testing language models on real-world clinical genetics curation tasks using expert-annotated data, which ensures the tasks reflect actual research needs and complexity. The multi-task structure (extraction, scoring, verification) captures different aspects of scientific reasoning required in clinical genetics.

## Foundational Learning
- **Gene-disease validity assertions**: Understanding how genes relate to diseases is fundamental to clinical genetics curation. This knowledge is needed to interpret the core task of the benchmark.
- **Evidence strength scoring**: Models must evaluate the quality and reliability of scientific findings, a critical skill for clinical decision-making.
- **Evidence code verification**: Checking if experimental evidence meets specific criteria ensures precision and reliability in genetic interpretation.

## Architecture Onboarding
- **Component map**: ClinGen Evidence Repository -> CGBENCH dataset -> Three task modules (extraction, scoring, verification) -> LM evaluation framework
- **Critical path**: Data collection → Task formulation → Model prompting → Performance evaluation → Analysis of errors
- **Design tradeoffs**: Expert-annotated data ensures quality but limits dataset size; multi-task structure captures complexity but increases evaluation difficulty
- **Failure signatures**: Hallucinated explanations in correct classifications, poor performance on specialized instructions, inconsistency between reasoning and non-reasoning models
- **First experiments**: 1) Test zero-shot performance on each task separately, 2) Compare reasoning vs non-reasoning models on extraction task, 3) Evaluate LM-as-a-judge reliability

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (70 expert-annotated articles) may limit generalizability
- Focus on gene-disease validity assertions may miss other clinical genetics applications
- Single-expert annotations may not capture the full complexity of real-world curation

## Confidence
- **Benchmark design methodology**: High
- **Generalizability across clinical genetics**: Medium
- **Cross-model comparisons**: Medium

## Next Checks
1. Expand evaluation to include a larger, more diverse corpus of clinical genetics literature
2. Conduct inter-annotator agreement studies using multiple clinical genetics experts
3. Implement systematic hallucination detection methods beyond LM-as-a-judge evaluation