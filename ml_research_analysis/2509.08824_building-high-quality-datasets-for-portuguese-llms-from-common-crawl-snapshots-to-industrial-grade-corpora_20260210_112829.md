---
ver: rpa2
title: 'Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots
  to Industrial-Grade Corpora'
arxiv_id: '2509.08824'
source_url: https://arxiv.org/abs/2509.08824
tags:
- arxiv
- pages
- language
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ClassiCC-PT, a 120B token Portuguese corpus
  built from Common Crawl, achieving performance comparable to industry-grade datasets
  like ClueWeb22-A. The authors introduce a preprocessing pipeline that includes HTML
  text extraction with Trafilatura, intra-crawl deduplication, and rule-based and
  neural-based filtering.
---

# Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora

## Quick Facts
- arXiv ID: 2509.08824
- Source URL: https://arxiv.org/abs/2509.08824
- Reference count: 40
- Primary result: ClassiCC-PT corpus enables Curió-1.1B to achieve 27.1 NPM on Poeta benchmark, outperforming scratch training

## Executive Summary
This paper presents ClassiCC-PT, a 120B token high-quality Portuguese corpus constructed from Common Crawl data. The authors develop a comprehensive preprocessing pipeline including language filtering, HTML extraction with Trafilatura, intra-crawl deduplication, and neural-based content classification. Through continual pretraining experiments, they demonstrate that Curió-1.1B, initialized from TinyLlama-1.1T and trained on ClassiCC-PT, significantly outperforms models trained from scratch in Portuguese, achieving state-of-the-art results on the Poeta benchmark.

## Method Summary
The authors construct ClassiCC-PT through a multi-stage pipeline starting with Common Crawl snapshots. After language filtering using CLD2, they extract text content using Trafilatura, apply intra-crawl deduplication with MinHash, and implement rule-based and neural filtering. Three BERTimbau classifiers (educational, STEM, toxic) are trained using synthetic GPT-4o annotations. The corpus is then used for continual pretraining of Curió-1.1B from the TinyLlama-1.1T checkpoint, with extensive ablation studies comparing different filtering strategies and training approaches.

## Key Results
- ClassiCC-PT achieves 27.1 NPM on Poeta benchmark, outperforming scratch training (14.9 NPM) despite using half the tokens
- Intra-crawl deduplication removes 40% of pages while improving model quality
- Neural classifiers trained on synthetic GPT-4o annotations outperform English-trained classifiers for Portuguese data filtering
- Continual pretraining from English-initialized weights provides 2x token efficiency over training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific neural classifiers outperform English-trained classifiers for non-English data filtering
- Mechanism: BERTimbau-based classifiers trained on synthetic GPT-4o annotations in Portuguese capture language-specific quality signals that English classifiers miss
- Core assumption: GPT-4o annotations transfer reliably to smaller classifier training
- Evidence anchors: FineWeb-Edu classifier performs poorly in Portuguese; FineWeb2 validates multilingual pipeline adaptation

### Mechanism 2
- Claim: Continual pretraining from English-initialized weights outperforms training from scratch for language adaptation
- Mechanism: English pretraining learns transferable linguistic representations; target-language training specializes without discarding learned structure
- Core assumption: Cross-lingual knowledge transfer exceeds catastrophic forgetting costs at moderate data scales
- Evidence anchors: From-scratch on 240B PT tokens achieves 14.9 NPM; continual pretraining on 120B PT tokens achieves 27.1 NPM

### Mechanism 3
- Claim: Intra-crawl deduplication combined with content-aware HTML extraction improves token efficiency
- Mechanism: Trafilatura removes boilerplate; MinHash deduplication reduces memorization of repetitive content, both reducing noisy gradient updates
- Core assumption: Deduplication benefits (reduced overfitting, bias) outweigh loss of potentially useful repeated exposure
- Evidence anchors: Trafilatura + intra-crawl deduplication shows consistent NPM improvement; AICC paper challenges Trafilatura

## Foundational Learning

- Concept: **Continual pretraining vs. fine-tuning distinction**
  - Why needed here: Paper uses continual pretraining (same objective, new data distribution) not fine-tuning (new objective); confusion leads to wrong hyperparameter choices
  - Quick check question: Can you explain why the paper uses Adafactor with 1e-3 learning rate rather than typical fine-tuning rates?

- Concept: **Token efficiency and compute-quality tradeoffs**
  - Why needed here: Paper shows 120B high-quality tokens outperform 240B lower-quality tokens; understanding this informs data budgeting
  - Quick check question: Given limited compute, would you prioritize more tokens or better filtering?

- Concept: **Synthetic annotation transfer**
  - Why needed here: Classifiers trained on GPT-4o labels must generalize; understanding teacher-student gaps is critical
  - Quick check question: What distribution shift might occur between GPT-4o's scoring and true document quality?

## Architecture Onboarding

- Component map: Common Crawl → Language Filter (CLD2) → HTML Extraction (Trafilatura) → MinHash Deduplication → Rule Filters (C4/MassiveWeb) → Neural Classifiers (edu/STEM/toxic) → ClassiCC-PT corpus → TinyLlama-1T checkpoint → Continual Pretraining → Curió-1.1B

- Critical path: Classifier training is the bottleneck—requires 110k+ GPT-4o annotations per classifier, BERTimbau fine-tuning infrastructure, and validation set

- Design tradeoffs:
  - C4 rules (aggressive, -43% docs) vs. MassiveWeb rules (conservative, -20%): Paper shows C4 helps early training, MassiveWeb may over-filter
  - Intra-crawl only vs. inter-crawl deduplication: Paper cites FineWeb finding that inter-crawl can hurt by keeping high-entropy noise
  - Scratch vs. continual: ~2x token efficiency gain, but requires English checkpoint availability

- Failure signatures:
  - Classifier F1 < 0.5 on held-out test: Synthetic annotations not transferring
  - NPM plateaus or decreases after 1 epoch: Data quality insufficient or learning rate too high
  - Training from scratch underperforms TinyLlama-1T baseline: Likely data规模 inadequate for architecture capacity

- First 3 experiments:
  1. Validate classifier quality: Train ClassiCC-PT-edu on 10k subset, measure F1 vs. GPT-4o labels before full annotation spend
  2. Ablate deduplication: Train with/without MinHash on 10B token subset, measure NPM delta at 5B and 10B tokens
  3. Compare rule sets: Run C4 vs. MassiveWeb vs. no rules on 20B subset, identify early-training gains vs. late-training plateau

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why did the MassiveWeb filtering rules result in a performance decline compared to the unfiltered baseline, contradicting previous literature?
- **Basis in paper:** Section 4.2.1 states that applying MassiveWeb rules led to a "slight decline in performance," a finding that "conflicts with previous studies [Rae et al., 2021; Pires et al., 2023]."
- **Why unresolved:** The paper identifies the discrepancy but does not isolate the specific cause (e.g., excessive removal of useful minority language data vs. model capacity issues).
- **What evidence would resolve it:** Ablation studies applying MassiveWeb rules to models of varying sizes (e.g., 1.1B vs. 7B) and with different pretraining histories (scratch vs. multilingual init) to see if the negative effect persists.

### Open Question 2
- **Question:** What is the optimal strategy for integrating high-quality educational subsets with general web data to avoid the temporary performance degradation observed during training?
- **Basis in paper:** Figure 4 shows that a mixture of ClueWeb and high-quality ClassiCC data underperformed the ClueWeb-only baseline for nearly 90 billion tokens before eventually surpassing it at the end of the second epoch.
- **Why unresolved:** The paper demonstrates that high-quality data wins eventually but does not explain the "dip" in the middle or how to prevent it.
- **What evidence would resolve it:** Experiments using curriculum learning (starting with raw text, moving to educational) or varying the mixing ratios (e.g., upsampling educational data early vs. late) to see if the performance dip can be smoothed out.

### Open Question 3
- **Question:** Can the neural-based classifier pipeline be effectively adapted for languages with significantly less data than Portuguese (e.g., under 1 billion tokens)?
- **Basis in paper:** The authors claim the methods are "applicable to other languages," but the pipeline relies on training BERT-based classifiers on 110,000 annotated samples.
- **Why unresolved:** Portuguese is a mid-to-high resource language in Common Crawl (~2%); the scalability of the specific neural filtering approach to low-resource settings remains untested.
- **What evidence would resolve it:** Applying the ClassiCC pipeline to a low-resource language (e.g., with only 500M tokens available) to determine if the neural classifiers remain robust and beneficial.

## Limitations

- Classifier reliability under distribution shift remains untested; evaluation was on same distribution as training data
- Claims about universal methodology applicability lack evidence beyond Portuguese language
- Computational cost trade-offs not fully explored; 2x token efficiency gain requires access to large English pretrained model

## Confidence

- **High confidence**: Core experimental results showing Curió-1.1B's 27.1 NPM performance on Poeta benchmark compared to from-scratch alternatives
- **Medium confidence**: Claims about superiority of continual pretraining over training from scratch for low-resource languages
- **Low confidence**: Claims about universal applicability of the proposed methodology to any language

## Next Checks

1. **Cross-lingual classifier validation**: Test the Portuguese classifiers on a held-out sample of Portuguese documents from a different time period or domain than the training data. Measure F1 scores and identify systematic failure modes. Additionally, apply the same classification pipeline to a small sample of Spanish or Italian web documents to assess cross-linguistic transfer potential.

2. **Inter-crawl vs. intra-crawl deduplication ablation**: Conduct a controlled experiment comparing models trained on datasets with only intra-crawl deduplication versus those with both intra-crawl and inter-crawl deduplication. Measure not just final performance but training dynamics (loss curves, gradient variance) to understand whether the FineWeb finding about inter-crawl potentially hurting applies to Portuguese data specifically.

3. **From-scratch scaling experiment**: Train multiple models from scratch using increasing amounts of Portuguese data (e.g., 60B, 120B, 240B, 480B tokens) to determine whether the continual pretraining advantage persists at larger scales. This would establish whether the 2x efficiency gain is a fundamental property of the approach or simply reflects insufficient data for the from-scratch baseline.