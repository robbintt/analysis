---
ver: rpa2
title: 'Dale meets Langevin: A Multiplicative Denoising Diffusion Model'
arxiv_id: '2510.02730'
source_url: https://arxiv.org/abs/2510.02730
tags:
- multiplicative
- mnist
- samples
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a biologically inspired generative model
  based on geometric Brownian motion (GBM) and multiplicative score-matching. The
  key idea is to connect exponentiated gradient descent, which respects Dale's law
  and produces log-normally distributed synaptic weights, to sampling from GBM.
---

# Dale meets Langevin: A Multiplicative Denoising Diffusion Model

## Quick Facts
- arXiv ID: 2510.02730
- Source URL: https://arxiv.org/abs/2510.02730
- Authors: Nishanth Shetty; Madhava Prasath; Chandra Sekhar Seelamantula
- Reference count: 40
- Primary result: Novel multiplicative denoising diffusion model connecting exponentiated gradient descent to GBM sampling

## Executive Summary
This paper introduces a biologically inspired generative model that bridges Dale's law-compliant neural network updates with sampling from geometric Brownian motion (GBM). The authors develop a reverse-time stochastic differential equation (SDE) framework for GBM and propose a multiplicative denoising score-matching loss function. The model generates diverse, reasonable-quality samples on MNIST variants while maintaining biological plausibility through log-normal weight distributions and multiplicative updates.

## Method Summary
The core innovation connects exponentiated gradient descent, which produces log-normally distributed synaptic weights while respecting Dale's law, to sampling from GBM distributions. The authors derive a reverse-time SDE framework specifically for GBM and introduce a multiplicative denoising score-matching loss that generalizes Hyvärinen's approach for non-negative data. This framework enables generative modeling based on biologically motivated optimization principles while maintaining competitive performance on standard image benchmarks.

## Key Results
- Generated samples are diverse and of reasonable quality on MNIST, Fashion-MNIST, and Kuzushiji MNIST
- Fréchet Inception Distance (FID) scores: 28.96 (MNIST), 116.15 (Fashion-MNIST), 50.78 (Kuzushiji MNIST)
- Kernel Inception Distance (KID) scores also reported
- Model demonstrates competitive generative performance while maintaining biological plausibility

## Why This Works (Mechanism)
The model works by establishing a mathematical connection between exponentiated gradient descent (which naturally produces log-normal distributions and respects Dale's law) and the sampling process of geometric Brownian motion. The multiplicative denoising score-matching loss function enables learning the reverse-time SDE for GBM, allowing generation of samples that follow the learned log-normal distribution. This approach combines biological plausibility with effective generative modeling through the multiplicative update structure.

## Foundational Learning
- **Geometric Brownian Motion (GBM)**: A continuous-time stochastic process where the logarithm of the variable follows Brownian motion with drift. *Why needed*: Forms the mathematical foundation for the sampling process and connects to log-normal weight distributions. *Quick check*: Verify the GBM SDE correctly models the exponentiated gradient descent dynamics.
- **Dale's Law**: The principle that neurons are either excitatory or inhibitory, but not both. *Why needed*: Provides the biological constraint that motivates using exponentiated gradient descent for weight updates. *Quick check*: Confirm the log-normal distribution satisfies Dale's law constraints.
- **Multiplicative Score-Matching**: An extension of score-matching that handles non-negative data through multiplicative updates. *Why needed*: Enables learning the score function for GBM, which is essential for the reverse-time SDE. *Quick check*: Validate the loss function properly estimates the gradient of the log-density for non-negative data.
- **Reverse-Time SDE Framework**: The mathematical framework for generating samples by solving the SDE backward in time. *Why needed*: Provides the mechanism for transforming noise into structured samples following the learned distribution. *Quick check*: Test the numerical stability of the reverse-time integration scheme.

## Architecture Onboarding

### Component Map
Noise distribution -> Exponentiated gradient descent -> Multiplicative denoising network -> Reverse-time GBM SDE -> Generated samples

### Critical Path
Noise → Score network → Reverse-time integration → Sample generation

### Design Tradeoffs
- Multiplicative updates provide biological plausibility but may face numerical stability issues
- Log-normal distributions satisfy Dale's law but may limit expressiveness compared to unconstrained distributions
- The reverse-time SDE approach enables exact sampling but requires careful numerical integration

### Failure Signatures
- Numerical instability during exponentiation leading to NaN values
- Mode collapse resulting in poor sample diversity
- Poor FID/KID scores indicating failure to capture data distribution
- Training instability due to multiplicative gradient updates

### First Experiments
1. Verify the log-normal distribution of weights after exponentiated gradient descent training
2. Test the reverse-time SDE integration on synthetic GBM data before training
3. Evaluate the multiplicative score-matching loss on simple non-negative distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical connection relies on specific assumptions about exponentiated gradient descent dynamics that may not generalize to all architectures
- Empirical evaluation limited to simple image datasets (MNIST variants), scalability to complex data remains untested
- Biological plausibility claims are speculative and require further validation for direct mapping to neural network performance
- Multiplicative updates may face numerical stability challenges during exponentiation steps

## Confidence

- Theoretical framework and SDE derivation: **High**
- Mathematical novelty of multiplicative score-matching: **High**
- Empirical performance on benchmark datasets: **Medium**
- Biological plausibility claims: **Low**
- Scalability to complex data: **Low**

## Next Checks

1. Test the model on more challenging datasets (e.g., CIFAR-10, CelebA) to evaluate scalability and robustness to complex image distributions
2. Conduct ablation studies to isolate the contribution of the multiplicative score-matching loss versus standard denoising diffusion approaches
3. Perform sensitivity analysis on the exponentiation parameters to assess numerical stability and optimization behavior across different learning rates and network architectures