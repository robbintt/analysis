---
ver: rpa2
title: 'eMargin: Revisiting Contrastive Learning with Margin-Based Separation'
arxiv_id: '2507.14828'
source_url: https://arxiv.org/abs/2507.14828
tags:
- learning
- time
- contrastive
- representation
- margin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effect of introducing an adaptive margin
  into the contrastive loss function for time series representation learning. The
  method, called eMargin, adjusts the margin based on a predefined similarity threshold
  to improve the separation between adjacent but dissimilar time steps.
---

# eMargin: Revisiting Contrastive Learning with Margin-Based Separation

## Quick Facts
- arXiv ID: 2507.14828
- Source URL: https://arxiv.org/abs/2507.14828
- Reference count: 32
- Primary result: Adaptive margin improves clustering metrics but fails in downstream classification

## Executive Summary
This paper investigates whether introducing an adaptive margin into contrastive loss functions can improve time series representation learning. The method, called eMargin, adjusts the margin based on a predefined similarity threshold to explicitly separate adjacent but dissimilar time steps. While eMargin consistently improves unsupervised clustering metrics across three benchmark datasets, it fails to deliver competitive results in downstream classification tasks with linear probing. The key finding is that high scores on unsupervised clustering metrics (DBI, Silhouette) do not necessarily imply that the learned embeddings are meaningful or effective for task-specific performance.

## Method Summary
eMargin modifies the InfoNCE contrastive loss by introducing an adaptive margin that depends on raw data similarity between adjacent time steps. The method computes a pseudo-label by comparing cosine similarity of raw input vectors against a threshold. If similarity is low, a margin penalty is applied to the embedding similarity matrix, forcing the optimization to push apart representations of adjacent steps that differ significantly in the input space. The approach uses STFT-preprocessed time series as input and employs a 3-layer 1D CNN backbone with a projection head. The method is evaluated on three datasets (HARTH, ECG, SleepEeg) using both unsupervised clustering metrics and downstream classification via linear probing.

## Key Results
- eMargin achieves superior clustering metrics: On ECG dataset, eMargin achieves DBI of 0.59 and Silhouette score of 0.80
- eMargin fails in downstream classification: On SleepEeg dataset, eMargin shows among the lowest precision and recall despite strong clustering scores
- The study demonstrates that improved unsupervised clustering metrics do not necessarily correlate with gains in task-specific performance

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Label Driven Adaptive Separation
The method computes a pseudo-label by comparing cosine similarity of raw input vectors against a threshold. If similarity is low, a margin penalty is applied to the embedding similarity matrix, forcing the optimization to push apart representations of adjacent steps that differ significantly in the input space. This assumes local discontinuity in the raw data space corresponds to a semantic boundary that should be preserved in the embedding space.

### Mechanism 2: Geometric Compaction via Squared Similarity
When samples are marked as highly similar, the mechanism squares the similarity score, aggressively emphasizing high similarities and pulling already-close points tighter together. When combined with margin-based separation for dissimilar points, this results in distinct, dense geometric structures (spirals) that artificially inflate unsupervised clustering scores.

### Mechanism 3: Metric-Task Misalignment
The loss function optimizes for local geometry by pulling neighbors close and pushing dissimilar neighbors apart. However, this creates spiral manifolds that have low intra-cluster distance (good Silhouette score) but do not map linearly to class labels, causing linear classifiers to fail. This demonstrates that high clustering quality scores serve as unreliable proxies for downstream utility.

## Foundational Learning

- **Concept: InfoNCE Loss**
  - Why needed: eMargin is an augmentation of standard InfoNCE loss; understanding baseline pull/push dynamics is required to grasp how the margin modifies the push force
  - Quick check: How does InfoNCE treat two adjacent time steps in a standard time series setup?

- **Concept: Short-Time Fourier Transform (STFT)**
  - Why needed: The paper uses STFT for preprocessing rather than raw signals; understanding that the input is actually a time-frequency representation is critical for interpreting the raw data similarity calculation
  - Quick check: Why might calculating similarity on STFT features yield different pseudo-labels than calculating it on raw amplitude data?

- **Concept: Linear Probing**
  - Why needed: This is the specific failure mode of the paper's method; one must understand that linear probing tests if classes can be separated by a hyperplane, which fails if the embeddings form complex non-linear manifolds
  - Quick check: If representations form a spiral where Class A transitions continuously into Class B, why would a linear classifier struggle?

## Architecture Onboarding

- **Component map:** Input STFT processed time series (B×T×D) -> 3-block 1D CNN with BatchNorm and ReLU -> Projection head -> eMargin loss module -> Backpropagation
- **Critical path:** Calculate cosine similarity of raw STFT inputs -> Generate binary pseudo-label Y using threshold -> Apply conditional equation to modify embedding similarity matrix -> Backpropagate through adjusted InfoNCE loss
- **Design tradeoffs:**
  - Threshold Sensitivity: Fixed threshold (e.g., 0.4) - too high treats everything as "dissimilar" (over-separation), too low margin rarely applied
  - Metric vs. Utility: Architecture explicitly optimizes for DBI/Silhouette scores, which the paper proves comes at the cost of classification accuracy
- **Failure signatures:**
  - The "Spiral" Trap: t-SNE showing tight, spiral-like continuous arcs rather than discrete blobs
  - Precision/Recall Collapse: High Silhouette score (>0.80) coinciding with low Precision (<0.20) in linear evaluation
- **First 3 experiments:**
  1. Baseline Verification: Run standard InfoNCE (no margin) vs. InfoNCE + eMargin on validation set; plot DBI vs. Accuracy to confirm divergence
  2. Threshold Sweep: Vary similarity threshold (0.2 to 0.8) to observe how spiral tightness and classification accuracy shift
  3. Visualization Check: Generate t-SNE plots for baseline and eMargin; if eMargin shows spirals and baseline shows blobs, the mechanism is working as described

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating semi-supervised objectives or specialized inductive biases align eMargin's compact geometric structures with task-relevant semantics?
- Basis: Authors state in Discussion that future work should focus on explicitly aligning these structures with task-relevant semantics through semi-supervised objectives or better inductive biases
- Why unresolved: Current work demonstrates misalignment but stops short of modifying framework to bridge this gap
- Evidence needed: Demonstrating that semi-supervised variant of eMargin can maintain high Silhouette scores while simultaneously improving linear probing accuracy

### Open Question 2
- Question: Why do the "tight spiral" manifolds produced by eMargin result in high unsupervised clustering scores but poor linear separability?
- Basis: Discussion notes emergence of "tight, spiral-like structures" that are visually well-separated but create "biases in evaluation metrics"
- Why unresolved: Paper identifies visual phenomenon and metric discrepancy but does not provide theoretical explanation for why this specific geometry forms
- Evidence needed: Geometric analysis showing standard clustering metrics fail to penalize elongated/continuous manifolds, combined with evidence that non-linear classifiers can successfully leverage these spiral embeddings

### Open Question 3
- Question: How robust is the eMargin mechanism to the choice of the predefined similarity threshold in datasets with varying noise levels?
- Basis: Method relies on fixed threshold (0.4) to generate pseudo-labels based on raw data similarity; paper notes that "naively contrasting adjacent timestamps... ignores their temporal dependencies"
- Why unresolved: Study uses fixed thresholds but does not analyze how sensitive "dissimilar" pseudo-labels are to noise or signal variance
- Evidence needed: Ablation study showing correlation between signal-to-noise ratio, threshold value, and resulting downstream classification performance

## Limitations

- **Dataset Size:** The datasets used are relatively small (e.g., 1000 samples per class), raising questions about generalizability to larger, more diverse datasets
- **Threshold Sensitivity:** Method's performance is highly sensitive to the threshold hyperparameter, with no sensitivity analysis provided to show how different thresholds affect the trade-off between clustering and classification performance
- **Theoretical Gap:** The core finding is well-demonstrated empirically but lacks deeper theoretical explanation for why the spiral manifold forms

## Confidence

- **High Confidence:** The empirical observation that eMargin improves clustering metrics while harming linear probing accuracy is directly supported by reported results across three datasets
- **Medium Confidence:** The mechanism explaining how adaptive margins create spiral manifolds is plausible based on modified similarity matrix formulation, but exact geometric dynamics are not rigorously proven
- **Low Confidence:** The generalizability of findings to other domains, thresholds, or larger datasets is not established

## Next Checks

1. **Threshold Sensitivity Analysis:** Run eMargin with thresholds ranging from 0.2 to 0.8; plot clustering metrics (DBI, Silhouette) against classification accuracy to quantify the trade-off and identify optimal threshold for each dataset

2. **Dataset Scaling Test:** Apply eMargin to a larger time series dataset (e.g., UCR archive) with 10+ classes; compare clustering vs. classification performance to test if spiral trap phenomenon persists at scale

3. **Alternative Manifold Visualization:** Generate t-SNE plots for both eMargin and baseline embeddings; if eMargin consistently produces spirals and baseline produces blobs across datasets, it strengthens the claim that adaptive margin inherently biases representations toward geometrically compact but semantically entangled manifolds