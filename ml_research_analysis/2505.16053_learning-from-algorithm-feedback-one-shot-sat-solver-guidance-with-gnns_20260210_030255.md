---
ver: rpa2
title: 'Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs'
arxiv_id: '2505.16053'
source_url: https://arxiv.org/abs/2505.16053
tags:
- solver
- variable
- instances
- training
- solvers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel reinforcement learning from algorithm
  feedback (RLAF) paradigm to guide SAT solver branching heuristics using Graph Neural
  Networks (GNNs). The key idea is to inject variable-wise weights and polarities
  into existing SAT solvers in a single forward pass, enabling the GNN to influence
  every branching decision.
---

# Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs

## Quick Facts
- arXiv ID: 2505.16053
- Source URL: https://arxiv.org/abs/2505.16053
- Reference count: 40
- One-line primary result: RLAF-trained GNN policies significantly reduce SAT solver runtime, achieving >2x speedup in some cases, generalizing to larger instances, and outperforming expert-supervised approaches.

## Executive Summary
This paper introduces a novel reinforcement learning paradigm for guiding SAT solver branching heuristics using Graph Neural Networks (GNNs). The method, called RLAF (Reinforcement Learning from Algorithm Feedback), trains a GNN to predict variable weights and polarities in a single forward pass, which are then injected into existing SAT solvers to influence every branching decision. By framing this as a reinforcement learning problem with the solver's computational cost as the sole reward signal, the authors demonstrate that RLAF policies significantly outperform both vanilla solvers and expert-supervised approaches across diverse SAT problem distributions.

## Method Summary
The method trains a GNN to predict variable weights and polarities for SAT instances. The GNN takes a literal-clause graph representation of the CNF formula and outputs a distribution of weights for each variable. These weights are injected into the branching heuristic of existing SAT solvers (Glucose/March) via multiplicative scaling of internal scores. The training uses GRPO with the solver's decision count as the reward signal. The entire process—from GNN inference to solver execution—occurs in a single forward pass, making it computationally efficient.

## Key Results
- RLAF policies achieve more than 2x speedup in mean solve times across diverse SAT problem distributions.
- The approach generalizes effectively to larger and harder problems than those seen during training.
- RLAF consistently outperforms expert-supervised approaches based on learning handcrafted weighting heuristics.

## Why This Works (Mechanism)

### Mechanism 1: Multiplicative Branching Score Modulation
Injecting learned weights into existing branching heuristics improves search efficiency by prioritizing variables that maximize a learned importance signal alongside the solver's native score. The method modifies the standard branching selection to scale the internal VSIDS activity increment by the predicted weight, amplifying the "activity" of structurally important variables. This assumes the optimal variable for branching can be determined by static structural analysis encoded as a multiplicative scalar.

### Mechanism 2: Reinforcement Learning from Algorithm Feedback (RLAF)
Policy optimization using the solver's computational cost as a reward signal allows the GNN to discover non-obvious heuristics that outperform human-designed proxies. The training loop treats the SAT instance as a state and the variable parameterization (weights/polarities) as a single action. GRPO updates the GNN to favor parameterizations that yielded lower costs, navigating the high-dimensional space of variable weights without intermediate supervision.

### Mechanism 3: One-Shot Static Parameterization
A single forward pass to determine variable weights is computationally cheaper than per-step neural inference, allowing the overhead to be amortized over the entire search. Unlike previous approaches that query a neural network at every branching decision, this method computes all weights once at the start, enabling the solver to run at native speed with merely multiplied internal scores.

## Foundational Learning

- **Concept: Conflict-Driven Clause Learning (CDCL) & VSIDS**
  - Why needed here: The mechanism relies on modifying the VSIDS activity score. You cannot understand how the weight injection works without understanding that VSIDS tracks how often a variable appears in conflicts.
  - Quick check question: If a variable has a high VSIDS score but the GNN assigns it a low weight $w(x) \approx 0$, will it be chosen for branching?

- **Concept: Policy Gradient Methods (specifically GRPO)**
  - Why needed here: The training loop uses Group Relative Policy Optimization. You need to understand that GRPO simplifies PPO by estimating the baseline from group rewards rather than a value network.
  - Quick check question: In RLAF, does the GNN learn to predict the *value* of a state or the *probability* of an action (weight assignment)?

- **Concept: Literal-Clause Graphs**
  - Why needed here: The GNN input is not raw text; it is a bipartite graph connecting literals to clauses.
  - Quick check question: How does the graph structure handle the relationship between a literal $x$ and its negation $\neg x$? (Answer: They are connected by specific edges).

## Architecture Onboarding

- **Component map:** CNF Formula $\to$ Bipartite Literal-Clause Graph $\to$ 10-layer GNN Encoder $\to$ MLP Decoder $\to$ Variable Weights/Polarities $\to$ Modified SAT Solver $\to$ Solver Cost $\to$ GRPO Optimizer

- **Critical path:** The GRPO update step. Calculating the group-relative advantage requires sampling $M$ parameterizations per instance. If the solver is not batched efficiently or the cost variance is too high, this step fails to provide a learning signal.

- **Design tradeoffs:**
  - **One-Shot vs. Iterative:** The paper chooses One-Shot (fast, static) over Iterative (slow, dynamic).
  - **Reward Signal:** The paper chooses "Number of Decisions" (stable, deterministic for a given seed) over "CPU Time" (noisy, hardware-dependent).

- **Failure signatures:**
  - **GNN Overhead Dominates:** If instances are too easy (solved in milliseconds), the 0.1s GNN forward pass makes the total runtime slower than the baseline.
  - **Mode Collapse:** The GNN assigns identical weights to all variables, effectively becoming the baseline solver.

- **First 3 experiments:**
  1. **Sanity Check (Random 3SAT):** Train on 3SAT(200), test on 3SAT(400). Verify that speedup increases with instance difficulty (as seen in Table 1).
  2. **Baseline Comparison (UNSAT Core):** Compare RLAF against a supervised GNN predicting UNSAT core membership. Confirm RLAF > Supervised on structured instances (Crypto/3COL).
  3. **Weight Analysis:** Visualize learned weights vs. UNSAT core membership. Check if the GNN "rediscovers" the UNSAT core heuristic (Figure 5 shows high correlation for Crypto/3COL).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RLAF methodology be effectively translated to other combinatorial optimization domains like Mixed-Integer Programming (MIP) or Constraint Satisfaction Problems (CSP)?
- Basis in paper: [explicit] The authors state that applying the method to other domains "remains as future work."
- Why unresolved: The paper restricts empirical validation to Boolean Satisfiability (SAT) solvers.
- What evidence would resolve it: Successful training of RLAF policies that reduce solve times for MIP or CSP solvers.

### Open Question 2
- Question: Can RLAF scale to industrial-sized SAT instances by leveraging distributed computing for feedback collection?
- Basis in paper: [explicit] The discussion notes the current system is a "small-scale prototype" and expanding to distributed resources is "important future work."
- Why unresolved: The authors rely on small instances solvable in milliseconds due to single-machine hardware constraints.
- What evidence would resolve it: Training convergence and speedups on large, industrial benchmarks (e.g., SAT Competition instances) using a distributed cluster.

### Open Question 3
- Question: Can more expressive GNN architectures be integrated without compromising the efficiency of the one-shot guidance?
- Basis in paper: [explicit] The authors note their model is bounded by color refinement, leaving "highly symmetric patterns indistinguishable," and combining expressivity with scalability is a "critical challenge."
- Why unresolved: Standard message-passing GNNs may fail to capture complex symmetries in structured problems.
- What evidence would resolve it: Experiments using higher-order GNNs (e.g., PPGN) that show improved speedups on symmetric instances without significant latency overhead.

## Limitations
- The approach's effectiveness hinges on the assumption that structural properties encoded in a single GNN pass remain relevant throughout the search.
- While GRPO successfully learns from solver cost, the variance in solver runtime due to randomization or instance difficulty could potentially destabilize the learning signal.
- The one-shot parameterization efficiency claim is validated on tested instances, but its limitations on extremely dynamic problem classes remain theoretical.

## Confidence

- **High confidence:** The core mechanism of injecting learned weights into branching heuristics (Mechanism 1) is well-supported by the mathematical formulation and empirical results showing consistent speedups across problem distributions.
- **Medium confidence:** The RLAF training paradigm (Mechanism 2) demonstrates effectiveness, but the sensitivity to solver variance and the convergence behavior across different problem classes warrants deeper investigation.
- **Medium confidence:** The one-shot parameterization efficiency claim (Mechanism 3) is validated on tested instances, but its limitations on extremely dynamic problem classes remain theoretical.

## Next Checks

1. **Dynamic Adaptation Test:** Design a controlled experiment with a problem class where variable importance shifts dramatically during search (e.g., through crafted instances with hidden dependencies revealed late). Compare RLAF's static weights against a hypothetical dynamic oracle to quantify the performance gap.

2. **Solver Variance Sensitivity:** Systematically vary the solver's randomization parameters (rnd-freq, random restarts) and measure the impact on GRPO training stability and final policy performance. Quantify the noise-to-signal ratio in the cost-based reward.

3. **Cross-Architecture Generalization:** Train RLAF policies on Glucose and directly evaluate them on a different CDCL solver (e.g., MapleSAT or CryptoMiniSat) without fine-tuning. Measure the degradation in performance to assess the robustness of the learned weights across solver architectures.