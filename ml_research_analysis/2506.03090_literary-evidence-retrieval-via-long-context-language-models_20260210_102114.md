---
ver: rpa2
title: Literary Evidence Retrieval via Long-Context Language Models
arxiv_id: '2506.03090'
source_url: https://arxiv.org/abs/2506.03090
tags:
- literary
- quotation
- ground
- truth
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a long-context benchmark for evaluating how
  well language models understand literary fiction by repurposing the RELiC dataset
  of literary criticism. The task requires models to retrieve a missing quotation
  from a novel that supports a given excerpt of literary analysis, simulating human
  literary reasoning.
---

# Literary Evidence Retrieval via Long-Context Language Models

## Quick Facts
- arXiv ID: 2506.03090
- Source URL: https://arxiv.org/abs/2506.03090
- Authors: Katherine Thai; Mohit Iyyer
- Reference count: 19
- Models like Gemini Pro 2.5 exceed human expert performance with 62.5% accuracy versus 50% for humans

## Executive Summary
This paper introduces a long-context benchmark for evaluating how well language models understand literary fiction by repurposing the RELiC dataset of literary criticism. The task requires models to retrieve a missing quotation from a novel that supports a given excerpt of literary analysis, simulating human literary reasoning. A high-quality subset of 292 examples was curated through extensive filtering and human verification. Experiments show that state-of-the-art reasoning models like Gemini Pro 2.5 exceed human expert performance with 62.5% accuracy versus 50% for humans, while open-weight models lag significantly at 29.1% accuracy. Models consistently overgenerate and struggle with nuanced literary cues, highlighting the need for both long-context capacity and interpretive reasoning.

## Method Summary
The paper repurposes the RELiC dataset of literary criticism to create a benchmark where models must retrieve quotations from novels that support given excerpts of literary analysis. The researchers curated a high-quality subset of 292 examples through extensive filtering and human verification. The task simulates human literary reasoning by requiring models to understand nuanced literary analysis and locate supporting evidence within long-context novels. Models are evaluated based on their ability to accurately retrieve the correct quotation that corresponds to the analysis provided.

## Key Results
- Gemini Pro 2.5 achieves 62.5% accuracy, exceeding human expert performance of 50%
- Open-weight models significantly underperform at 29.1% accuracy
- Models consistently overgenerate and struggle with nuanced literary cues

## Why This Works (Mechanism)
The benchmark leverages the RELiC dataset's structure, which pairs literary analysis excerpts with corresponding quotations from novels. This design creates a natural task where models must understand both the analytical context and the source material to identify supporting evidence. The long-context requirement forces models to process extended narrative passages rather than isolated text segments, better simulating the interpretive reasoning that human literary scholars employ when analyzing fiction.

## Foundational Learning
The task builds on the fundamental capability of language models to process and understand extended contexts, requiring both factual retrieval and interpretive reasoning. Success depends on models learning to bridge the gap between abstract literary analysis and concrete textual evidence, a skill that mirrors human literary criticism. The benchmark implicitly tests whether models can develop a form of literary comprehension that goes beyond surface-level pattern matching to grasp thematic and contextual relationships.

## Architecture Onboarding
The evaluation framework can be applied to any long-context language model regardless of specific architecture, as it primarily tests retrieval and reasoning capabilities rather than architectural innovations. Models need sufficient context window capacity to process entire novel passages, and effective performance likely requires attention mechanisms that can track thematic and contextual relationships across long text spans. The task may particularly benefit from architectures that excel at cross-attention between analysis excerpts and source material.

## Open Questions the Paper Calls Out
The paper raises questions about whether current evaluation metrics adequately capture the quality of literary evidence retrieval, given that semantically equivalent but lexically different quotations might satisfy the same analytical context. There are also open questions about how well results from the curated 292-example subset generalize to the full RELiC dataset and whether the benchmark can be extended to capture more sophisticated forms of literary reasoning beyond quotation retrieval.

## Limitations
The most significant limitation is the relatively small size of the curated dataset (292 examples) despite extensive filtering, which may constrain statistical robustness and generalizability of findings. The human expert performance benchmark is based on a single annotator, raising concerns about inter-annotator reliability and potential subjectivity in literary interpretation. The evaluation methodology relies on exact matching of retrieved quotations to ground truth, which may not fully capture semantically equivalent but lexically different valid retrievals that could still satisfy the literary analysis context.

## Confidence
- Gemini Pro 2.5 exceeds human expert performance: High confidence (62.5% vs 50% accuracy)
- Open-weight models significantly underperform: Medium confidence (29.1% accuracy, small sample size)
- Models overgenerate and struggle with nuanced literary cues: Medium confidence (qualitative behavior patterns)

## Next Checks
1. Expand the human expert benchmark to include multiple annotators to establish inter-annotator agreement and more robust human performance baselines.
2. Test model performance on the full RELiC dataset before filtering to understand how the curated subset differs from the original distribution and whether results generalize.
3. Implement semantic similarity evaluation metrics beyond exact matching to capture paraphrases and semantically equivalent quotations that satisfy the literary analysis context.