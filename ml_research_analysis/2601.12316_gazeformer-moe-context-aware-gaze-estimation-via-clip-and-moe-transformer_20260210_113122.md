---
ver: rpa2
title: 'GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer'
arxiv_id: '2601.12316'
source_url: https://arxiv.org/abs/2601.12316
tags:
- gaze
- estimation
- transformer
- clip
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses appearance-based 3D gaze estimation from
  facial images, focusing on robustness under varying illumination, head pose, and
  background. The proposed GazeFormer-MoE model combines three core ideas: (1) semantic
  conditioning using CLIP global features enriched with learnable prototype banks
  for illumination, head pose, background, and direction; (2) unified multi-scale
  token fusion that jointly attends prototype-enriched vectors, CLIP patch tokens,
  and high-resolution CNN features within a single Transformer; and (3) a routed and
  shared Mixture of Experts (MoE) Transformer that increases conditional capacity
  by engaging specialized experts for rare appearance sub-distributions.'
---

# GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer

## Quick Facts
- arXiv ID: 2601.12316
- Source URL: https://arxiv.org/abs/2601.12316
- Reference count: 0
- State-of-the-art angular errors: 2.49° (MPIIFaceGaze), 3.22° (EYEDIAP), 10.16° (Gaze360), 1.44° (ETH-XGaze)

## Executive Summary
This paper introduces GazeFormer-MoE, a transformer-based model for appearance-based 3D gaze estimation from facial images. The approach tackles robustness under varying illumination, head pose, and background by integrating semantic conditioning, multi-scale token fusion, and a Mixture-of-Experts (MoE) architecture. By leveraging CLIP global features and learnable prototype banks, the model dynamically adapts to diverse appearance sub-distributions. Evaluated on four benchmark datasets, GazeFormer-MoE achieves state-of-the-art angular errors and demonstrates up to a 64% relative improvement over prior methods.

## Method Summary
The GazeFormer-MoE model addresses appearance-based 3D gaze estimation by combining semantic conditioning, multi-scale token fusion, and a routed MoE Transformer. Semantic conditioning uses CLIP global features and learnable prototypes for illumination, head pose, background, and direction. Multi-scale token fusion jointly attends prototype-enriched vectors, CLIP patch tokens, and high-resolution CNN features within a single Transformer. The MoE module increases conditional capacity by routing specialized experts for rare appearance sub-distributions. The model is trained and evaluated on MPIIFaceGaze, EYEDIAP, Gaze360, and ETH-XGaze, achieving state-of-the-art angular errors.

## Key Results
- State-of-the-art angular errors: 2.49° (MPIIFaceGaze), 3.22° (EYEDIAP), 10.16° (Gaze360), 1.44° (ETH-XGaze)
- Up to 64% relative improvement over prior work
- Ablations confirm gains from prototype conditioning, cross-scale token fusion, and MoE routing

## Why This Works (Mechanism)
GazeFormer-MoE improves gaze estimation by explicitly modeling context via semantic conditioning and increasing model capacity for rare conditions through MoE routing. Prototype banks allow the model to capture and adapt to illumination, pose, background, and direction sub-distributions, while multi-scale token fusion enables joint reasoning over global semantic and local fine-grained features. The MoE architecture engages specialized experts only when needed, improving robustness to diverse and rare appearance patterns without excessive computational cost during inference.

## Foundational Learning
- **3D Gaze Estimation**: Estimating the 3D direction of eye gaze from facial images. Needed because accurate gaze direction is crucial for human-computer interaction and behavioral studies. Quick check: model outputs 3D gaze vectors in radians or degrees.
- **CLIP Global Features**: Global semantic embeddings from CLIP that capture contextual scene information. Needed to provide rich semantic priors for gaze prediction. Quick check: CLIP embeddings are 512-dimensional and pre-trained on large image-text pairs.
- **Learnable Prototype Banks**: Sets of trainable vectors representing sub-distributions like illumination, pose, background, and direction. Needed to allow the model to adapt dynamically to varying conditions. Quick check: number of prototypes per category is fixed and learnable.
- **Multi-scale Token Fusion**: Combining tokens from different scales (global CLIP patches, local CNN features) within a single Transformer. Needed to integrate fine-grained and semantic information. Quick check: all tokens are attended jointly in the Transformer layers.
- **Mixture-of-Experts (MoE)**: Routing mechanism that activates specialized expert networks for different input patterns. Needed to increase model capacity for rare or challenging conditions. Quick check: only a subset of experts is active per input during training.
- **Routed Attention**: Selective activation of experts based on input gating. Needed to balance model complexity and specialization. Quick check: gating network determines expert selection per token.

## Architecture Onboarding
- **Component Map**: Input Image -> CNN Backbone -> Local Features; CLIP Backbone -> Global Features; Prototype Bank (Illumination, Pose, Background, Direction) -> Enriched Vectors; Token Fusion Layer -> MoE Transformer -> Gaze Output
- **Critical Path**: Image features → Prototype conditioning → Multi-scale fusion → MoE routing → Gaze regression
- **Design Tradeoffs**: Increased conditional capacity (MoE) vs. deployment complexity; richer context (prototypes) vs. model size; multi-scale fusion vs. computational overhead
- **Failure Signatures**: Over-reliance on a single expert in MoE; inadequate prototype coverage for rare conditions; poor fusion leading to loss of local detail
- **First 3 Experiments**: 1) Ablate prototype conditioning to measure context impact; 2) Disable MoE routing to assess capacity gains; 3) Test on a large, in-the-wild dataset (e.g., RT-GENE) to validate generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to in-the-wild datasets and cross-dataset transfer not verified
- Fixed prototype sets may not cover all rare or unseen conditions
- Deployment trade-offs (latency, memory) for MoE architecture not addressed

## Confidence
- Core methodological innovations (prototype conditioning, multi-scale fusion, MoE routing): High
- Robustness under real-world, unseen scenarios: Medium
- Sensitivity to hyperparameters (number of experts, prototypes): Medium

## Next Checks
1. Evaluate the model on a large, in-the-wild gaze dataset (e.g., RT-GENE or OpenEDS) to test robustness beyond controlled benchmarks.
2. Conduct ablation studies varying the number of experts and prototypes to assess sensitivity and necessity of each design choice.
3. Measure inference time and memory usage for the MoE variant, comparing against non-MoE baselines to quantify practical deployment costs.