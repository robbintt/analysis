---
ver: rpa2
title: What Matters to an LLM? Behavioral and Computational Evidences from Summarization
arxiv_id: '2602.00459'
source_url: https://arxiv.org/abs/2602.00459
tags:
- importance
- samsum
- across
- summarization
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how Large Language Models (LLMs) prioritize
  information during summarization by combining behavioral and computational analyses.
  Behaviorally, the study generates length-controlled summaries and derives empirical
  importance distributions based on how often each token is selected.
---

# What Matters to an LLM? Behavioral and Computational Evidences from Summarization

## Quick Facts
- arXiv ID: 2602.00459
- Source URL: https://arxiv.org/abs/2602.00459
- Reference count: 40
- Primary result: LLMs develop consistent importance patterns that differ from pre-LLM baselines and cluster more by architecture family than model size.

## Executive Summary
This work investigates how Large Language Models (LLMs) prioritize information during summarization by combining behavioral and computational analyses. Behaviorally, the study generates length-controlled summaries and derives empirical importance distributions based on how often each token is selected. These reveal that LLMs develop consistent importance patterns that differ from pre-LLM baselines and cluster more by architecture family than model size. Computationally, the research identifies specific attention heads that align with these importance distributions and finds that middle-to-late transformer layers are most predictive of importance. The findings suggest that importance encoding is robust across model scales but highly task-dependent, being more predictable in conversational dialogues than in long-form news. Together, these insights provide initial evidence for how LLMs internally represent and compute information importance, opening pathways for interpreting and controlling their summarization behavior.

## Method Summary
The study combines behavioral analysis with computational probing to understand how LLMs determine information importance. The behavioral approach generates k=10 summaries per document at varying lengths (10-100 words) to create empirical importance distributions based on token selection frequency. The computational approach uses linear probes on hidden states and analyzes attention head alignments to identify where importance is encoded. The research examines 7 LLMs across CNN/DailyMail, SAMSum, and DECODA datasets, training one-hidden-layer MLP probes with KL divergence loss over 20 epochs with early stopping.

## Key Results
- LLMs develop consistent importance patterns that differ from pre-LLM baselines and cluster more by architecture family than model size
- Middle-to-late transformer layers are most predictive of importance, while specific attention heads in deeper layers align well with empirical importance distributions
- Importance encoding is robust across model scales but highly task-dependent, being more predictable in conversational dialogues than in long-form news

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If you force an LLM to summarize the same document at varying lengths, the frequency of information unit selection serves as a proxy for the model's internal importance ranking.
- **Mechanism:** The method generates k summaries with different length constraints (e.g., 10, 20, ... 100 words). It calculates the selection frequency of tokens across these summaries. Units appearing in short summaries and persisting in longer ones receive higher scores, creating an "Empirical Importance Distribution" (I_M(D)).
- **Core assumption:** Information units have a hierarchical utility; critical units are retained even under extreme compression, while peripheral units are dropped first.
- **Evidence anchors:**
  - [abstract] "...derive empirical importance distributions based on how often each information unit is selected."
  - [section 3, Eq 1] Defines the empirical selection frequency calculation.
  - [corpus] The related work "Behavioral Analysis of Information Salience in Large Language Models" supports the use of behavioral probes to derive salience.
- **Break condition:** If the length-controlled prompts do not strictly force the model to compress hierarchically (e.g., the model hallucinates new content or ignores length constraints), the frequency distribution will not reflect true internal salience.

### Mechanism 2
- **Claim:** Information importance is not uniformly encoded across the network but is most readable in the middle-to-late layers of the transformer.
- **Mechanism:** Probing classifiers (small MLPs) are trained on hidden states from specific layers to predict the I_M(D) scores. Performance peaks in middle-to-late layers, suggesting these layers act as the locus where "importance" is computationally resolved before generation.
- **Core assumption:** Linear separability in a hidden state implies that the concept (importance) is causally or representationally encoded at that layer.
- **Evidence anchors:**
  - [abstract] "...middle-to-late layers are strongly predictive of importance."
  - [section 6.2.1] "Performance rises in early layers and peaks in middle-to-late layers..."
  - [corpus] Corpus evidence regarding specific layer-wise salience encoding is weak or inferred; related titles focus on general benchmark validity rather than this specific architectural locus.
- **Break condition:** If a probe trained on randomized weights ("dead salmon" baseline) achieves similar performance to the trained model, the mechanism is broken (the probe is learning the dataset distribution, not reading the model state).

### Mechanism 3
- **Claim:** Specific attention heads, particularly in deeper layers, function as specialized "importance predictors," though this alignment varies significantly by model family and dataset type.
- **Mechanism:** The paper aggregates raw attention weights received by tokens and compares this distribution to I_M(D) using NDCG@10. It finds that specific heads (e.g., in Layer 13 for Llama-3.2-1B) strongly align with behavioral importance, whereas early layers show negligible correlation.
- **Core assumption:** Attention weights can be interpreted as token-level importance scores (a proxy for salience), rather than just mixing coefficients for representation building.
- **Evidence anchors:**
  - [abstract] "...certain attention heads align well with empirical importance distributions..."
  - [section 5] "98.6% of attention heads... achieve NDCG@10 ≥ 0.5... Performance peaks in different layers across models."
  - [corpus] Corpus evidence for this specific "head-as-salience-predictor" mechanism is limited in the provided neighbors; existing neighbors focus on benchmark validity or domain-specific summarization.
- **Break condition:** If the dataset is highly abstractive (like CNN/DailyMail), attention alignment degrades significantly (Section 5.1 notes "CNN/DailyMail shows weak alignment"), suggesting attention alone cannot explain importance in complex generation tasks.

## Foundational Learning

- **Concept:** **Empirical Importance Distribution (I_M(D))**
  - **Why needed here:** This is the ground truth label for all experiments. Unlike standard summarization which uses a single reference, this method requires building a frequency map of what the *specific model* prioritizes.
  - **Quick check question:** If you generate 10 summaries and a token appears in all 10, but another appears only in the longest one, which has the higher I_M(D) score?

- **Concept:** **Linear Probing (with "Dead Salmon" Controls)**
  - **Why needed here:** To verify that the model *internally* represents importance. The "Dead Salmon" baseline (randomized weights) is critical to ensure you aren't just observing the probe's ability to memorize data.
  - **Quick check question:** If a probe achieves 80% accuracy on both a trained LLM and a randomized-weight LLM, can you claim the trained LLM encodes the concept?

- **Concept:** **Model Family vs. Scale**
  - **Why needed here:** The paper finds behavior clusters by *family* (Llama vs. Qwen) more than size. Understanding this helps in selecting models for specific summarization behaviors.
  - **Quick check question:** According to the paper, should you expect a 14B parameter model from Family A to behave more like a 1B model from Family A or an 8B model from Family B?

## Architecture Onboarding

- **Component map:**
  1. Input: Source Document
  2. Behavioral Pipeline: Length-variant Prompt Strategy → Summary Generator → Frequency Counter → I_M(D) Vector
  3. Computational Pipeline:
     - *Attention Path:* Hook on Attention Heads → Aggregation → NDCG Comparison
     - *Probing Path:* Hook on Hidden States → Probe MLP → Prediction vs I_M(D)
  4. Baselines: TextRank, Token Frequency, Randomized Weights (Dead Salmon)

- **Critical path:** The alignment between the **Behavioral Pipeline** (generating the ground truth importance) and the **Computational Pipeline** (extracting internal representations). If the length constraints in the prompt are not enforced (e.g., the model ignores "exactly 10 words"), the downstream alignment analysis is invalid.

- **Design tradeoffs:**
  - **Token-level vs. Word-level:** The paper maps sub-word tokens to words for importance scoring. This simplifies analysis but may lose nuance in morphological richness.
  - **Dataset Specificity:** SAMSum (dialogue) shows strong attention-importance alignment; CNN/DailyMail (news) does not. *Design implication:* Do not assume a mechanism discovered in dialogue tasks transfers to long-form news without validation.

- **Failure signatures:**
  - **Probe Overfitting:** High probe accuracy on trained weights *and* randomized weights (Section 6.1.1).
  - **Metric Saturation:** If Spearman correlation is flat across all layers, the probe is likely not reading hierarchical features.
  - **Length Leakage:** If summaries are not actually length-controlled, the importance distribution will be noisy.

- **First 3 experiments:**
  1. **Baseline Validation:** Implement the "Dead Salmon" baseline. Train a probe on a fully randomized version of your model to establish the floor for predictive performance. If your real model probe doesn't beat this significantly, stop.
  2. **Head Hunting:** Extract attention weights for a single layer (e.g., middle layer) on a small batch (100 samples). Calculate Spearman correlation with word frequency in the ground truth summary to see if attention aligns with surface-level importance.
  3. **Behavioral Clustering:** Generate 10 summaries per document for 2 different model families (e.g., Llama & Qwen) on the same input. Visualize the MDS plot of their importance distributions to verify the "Family > Scale" clustering claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can causal manipulation of identified attention heads or hidden state representations enable direct control over information selection in generated summaries?
- **Basis in paper:** [explicit] The conclusion states: "A key gap remains: while models maintain consistent internal hierarchies, we lack direct methods to access or control them. Future work should bridge this gap through causal manipulation experiments for output control."
- **Why unresolved:** The study identifies where importance is encoded (middle-to-late layers, specific attention heads) but does not test whether intervening on these components can steer summarization behavior.
- **What evidence would resolve it:** Experiments that modify activations in identified attention heads or layers and measure resulting changes in summary content selection.

### Open Question 2
- **Question:** How is the abstract concept of information importance mechanistically encoded across the full transformer architecture during summarization?
- **Basis in paper:** [explicit] Section 2.1 states: "A key open question remains how this abstract concept is mechanistically encoded within models."
- **Why unresolved:** The study shows that certain heads and layers predict importance, but the full computational circuit—how importance emerges from interactions across components—remains uncharacterized.
- **What evidence would resolve it:** Mechanistic interpretability methods (e.g., activation patching, circuit analysis) tracing the flow of importance-related information through the network.

### Open Question 3
- **Question:** Would using semantic information units (rather than tokens) improve the alignment between behavioral importance distributions and internal representations?
- **Basis in paper:** [inferred] The Limitations section notes "the imperfect alignment between annotated summary words and source tokens results in a limited set of word–hidden-state pairs for probe training. Future work could improve this mapping by incorporating semantic similarity, moving beyond exact lexical matching."
- **Why unresolved:** Token-level analysis may miss semantically coherent units that better capture what LLMs prioritize.
- **What evidence would resolve it:** Repeating the probing experiments using phrase-level or semantic-unit granularity and comparing predictive performance against token-level baselines.

## Limitations

- **Length control reliability:** Generated summaries do not strictly adhere to requested token counts, introducing noise into the empirical importance distribution.
- **Task transfer validity:** Strong attention-importance alignment in SAMSum (dialogue) versus weak alignment in CNN/DailyMail (news) suggests mechanisms are highly task-specific.
- **Architectural locus ambiguity:** While middle-to-late layers are most predictive, the paper cannot definitively determine whether importance is computed in these layers or merely readily accessible there.

## Confidence

- **Behavioral importance distributions are consistent and family-clustered (High):** Multiple behavioral experiments show clear clustering by model family over scale, supported by consistent MDS visualizations and ablation studies.
- **Middle-to-late layers are most predictive of importance (Medium):** Probing results consistently peak in middle-to-late layers, but the causal interpretation (computation vs. accessibility) remains uncertain without intervention studies.
- **Specific attention heads predict importance (Low-Medium):** While NDCG@10 scores show strong alignment for certain heads, the mechanism is brittle across datasets and the interpretation of attention as importance is debated in the literature.

## Next Checks

1. **Dead Salmon Baseline Replication:** Implement randomized-weight probes to establish the floor for importance prediction. If trained probes do not significantly outperform this baseline, the claimed internal encoding is not validated.
2. **Length Adherence Filtering:** Measure actual vs. requested summary lengths across all length-variant prompts. Remove or re-generate summaries that deviate significantly (>20% from target) to reduce noise in the empirical importance distribution.
3. **Cross-Dataset Mechanism Transfer:** Test whether attention-importance alignment mechanisms discovered in SAMSum transfer to another conversational dataset (e.g., MultiWOZ). If alignment breaks, this confirms the task-specific nature of the mechanism and limits generalizability claims.