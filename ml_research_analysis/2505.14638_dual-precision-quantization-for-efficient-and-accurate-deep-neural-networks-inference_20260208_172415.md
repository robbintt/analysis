---
ver: rpa2
title: Dual Precision Quantization for Efficient and Accurate Deep Neural Networks
  Inference
arxiv_id: '2505.14638'
source_url: https://arxiv.org/abs/2505.14638
tags:
- quantization
- weights
- arxiv
- weight
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dual Precision Quantization (DPQ), a novel
  quantization scheme for efficient deep neural network inference. The method combines
  4-bit integer weight storage (W4) with 8-bit floating-point computation (A8) to
  optimize both memory-bound and compute-bound inference regimes.
---

# Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference

## Quick Facts
- arXiv ID: 2505.14638
- Source URL: https://arxiv.org/abs/2505.14638
- Reference count: 40
- One-line primary result: Up to 3x speed-up for memory-bound inference while maintaining competitive accuracy with 4-bit weight storage and 8-bit floating-point computation

## Executive Summary
This paper introduces Dual Precision Quantization (DPQ), a novel quantization scheme that combines 4-bit integer weight storage with 8-bit floating-point computation to optimize deep neural network inference efficiency. The method uses a two-step quantization process with Hessian-based error compensation and Group-Aware Reordering to maintain accuracy while achieving significant memory and compute savings. Experiments demonstrate up to 3x throughput improvements on large language and vision-language models while preserving competitive accuracy compared to full-precision models.

## Method Summary
DPQ implements a W4A8 quantization scheme where weights are stored offline in 4-bit integer format and dequantized on-the-fly to 8-bit floating-point for computation. The method employs a two-step quantization algorithm that accounts for cumulative quantization errors using Hessian matrix information, distributing errors based on weight importance. Group-Aware Reordering (GAR) reorders weights within quantization groups before compression to prioritize important weights without introducing inference overhead. The approach is implemented in Intel's Neural Compressor framework and optimized for both memory-bound and compute-bound inference regimes.

## Key Results
- Achieves up to 3x throughput improvement compared to 16-bit inference
- Maintains competitive accuracy on Llama-2, Llama-3, and Qwen2-VL models
- Outperforms existing W4A8 schemes while reducing memory footprint by 4x
- Particularly effective for memory-bound scenarios with small batch sizes
- Successfully quantizes models from 7B to 72B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPQ improves inference efficiency by decoupling storage precision from compute precision, optimizing for both memory-bound and compute-bound regimes.
- Mechanism: Weights are stored offline in 4-bit integer format (INT4), reducing memory footprint by 4x compared to BF16. During inference, these weights are dequantized on-the-fly to FP8. Simultaneously, input activations are quantized to FP8. The core matrix multiplication is then performed using native FP8 arithmetic, which is approximately 2x faster than 16-bit operations on supported hardware.
- Core assumption: The target hardware accelerator has native support for FP8 matrix multiplication (e.g., NVIDIA H100, Intel Gaudi 2/3, AMD MI300X) and that the overhead of INT4-to-FP8 dequantization is negligible compared to memory bandwidth savings or compute gains.
- Evidence anchors:
  - [abstract]: "...W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations..."
  - [section 1]: "...FP8 can be advantageous over INT8 in terms of model accuracy, as INT8 represents a uniform distribution of values... FP8, with its non-uniform distribution and higher dynamic range, better accommodates such data..."
  - [corpus]: Related work like QServe and FlexiQ also explore mixed-precision (e.g., W4A8), confirming the viability of hybrid schemes. The corpus notes "adaptive mixed-precision quantization" as a key trade-off.
- Break condition: This mechanism fails if the hardware lacks efficient FP8 compute units, forcing a fallback to higher-precision arithmetic, or if the dynamic range of FP8 (E4M3) proves insufficient for certain outlier-heavy layers, leading to numerical instability.

### Mechanism 2
- Claim: A two-step, Hessian-based error compensation algorithm minimizes the accuracy degradation caused by cascaded quantization.
- Mechanism: The algorithm accounts for the cumulative error introduced by the BF16→FP8→INT4 quantization chain. It quantizes weights one-by-one. After each weight is quantized and then dequantized back to BF16, the resulting error is calculated. This error is then distributed to the remaining, not-yet-quantized weights in the row. The distribution is weighted by the inverse of the Hessian matrix, which acts as a measure of each weight's importance to the final output.
- Core assumption: The Hessian matrix, computed from a calibration dataset, accurately captures the sensitivity of the loss function to weight perturbations, and the calibration data is representative of the real input distribution.
- Evidence anchors:
  - [abstract]: "...develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ)... without introducing additional inference overhead."
  - [section 3.1]: "Our objective is to find a 4-bit weight matrix, such that its dequantized 16-bit matrix, ˆW 16, minimizes the squared error... ˆW 16 incorporates the two-step dequantization process... This ensures that the error... accounts for the cumulative error introduced by both quantization levels..."
  - [corpus]: Corpus signals for GPTQ and AWQ (related methods) confirm the use of second-order information (Hessian) for error compensation is a standard but evolving practice. The specific contribution here is the dual-precision error compensation.
- Break condition: The mechanism breaks if the Hessian approximation is poor or if the layer's output is highly sensitive to a few specific weights whose errors cannot be sufficiently compensated by later weights.

### Mechanism 3
- Claim: Group-Aware Reordering (GAR) improves quantization accuracy by protecting important weights without introducing inference-time memory overhead.
- Mechanism: Before quantization, weights are reordered based on their importance (measured by the Hessian diagonal). However, this reordering is constrained to occur only within a quantization group or by moving entire groups. After quantization, the weights are re-permuted to their original order. Because groups were kept contiguous, each group still shares a single scale factor and zero-point. This allows for efficient vectorized dequantization at inference, unlike unconstrained reordering which would require scattering/gathering individual scale factors.
- Core assumption: The most important weights in a layer can be effectively prioritized by ranking entire groups (e.g., by the max Hessian value in a group), not just individual weights.
- Evidence anchors:
  - [abstract]: "...Group-Aware Reordering (GAR), a weight reordering technique that prioritizes important weights without introducing inference overhead."
  - [section 3.2]: "GAR reorders weights under the constraint that weights can be permuted in one of two ways - within a group, and/or by moving whole groups. With GAR, scales operate on consecutive weights in the original order."
  - [corpus]: Corpus evidence is weak for this specific technique. The signal mentions "adaptive mixed-precision," but not constrained reordering. Therefore, GAR appears to be the novel, paper-specific contribution.
- Break condition: The method's benefit diminishes if the "important" weights are diffusely spread across many groups, making group-level reordering ineffective at protecting them.

## Foundational Learning

- Concept: **Hessian Matrix in Second-Order Optimization**
  - Why needed here: The DPQ algorithm uses the inverse Hessian to determine how to distribute quantization error. Understanding that the Hessian represents the curvature of the loss function (weight sensitivity) is key to understanding *why* error is distributed the way it is.
  - Quick check question: In the context of the DPQ algorithm, what does a large value on the diagonal of the Hessian matrix indicate about the corresponding weight?

- Concept: **Memory-Bound vs. Compute-Bound Regimes**
  - Why needed here: The W4A8 scheme is explicitly designed to optimize for both regimes. Grasping this distinction is crucial for understanding the paper's core motivation: W4 for memory bandwidth, A8 for compute throughput.
  - Quick check question: In an autoregressive decoding phase with a batch size of 1, is the inference typically memory-bound or compute-bound, and which part of the W4A8 scheme provides the primary benefit?

- Concept: **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: The paper operates in the PTQ setting, meaning the model is not retrained. This constraint is what necessitates the sophisticated error-compensation algorithm; we can't "train away" the noise.
  - Quick check question: What is the primary practical advantage of using PTQ over QAT for a large language model with billions of parameters?

## Architecture Onboarding

- Component map: DPQ Quantization Engine (Offline) -> Inference Runtime Kernel -> GAR Module (Offline)
- Critical path: The **accuracy preservation** path is most critical. The two-step error compensation must successfully account for quantization noise. If the Hessian-based compensation is miscalibrated, the model's accuracy will degrade irreversibly, rendering the speed improvements useless.
- Design tradeoffs:
  - **FP8 vs. INT8 for Compute:** Choosing FP8 (E4M3) for activation compute offers higher dynamic range and better handles outliers but may have slightly lower precision for small, uniform values compared to INT8. The paper claims FP8 is superior for typical neural network distributions.
  - **GAR vs. Full Reordering:** GAR trades a potential minor increase in accuracy (from full reordering) for zero inference-time overhead. The design decision is to prioritize inference speed and simplicity.
  - **Group Size:** The choice of group size (e.g., 128) is a trade-off. Smaller groups provide finer-grained scaling (better accuracy) but increase the overhead of storing and applying scale factors.
- Failure signatures:
  - **Accuracy Collapse:** A sudden, catastrophic drop in accuracy on specific benchmarks (e.g., MMLU) likely indicates the Hessian approximation failed for certain critical layers or the calibration data was unrepresentative.
  - **Latency Plateau:** If throughput does not scale with batch size as predicted, it suggests the implementation is not fully leveraging the FP8 compute units or that another bottleneck (e.g., memory bandwidth for activations) has been introduced.
  - **NaN/Inf Errors:** The appearance of NaNs during inference suggests the FP8 dynamic range (E4M3) was exceeded, likely in a layer with extreme activation outliers not handled by the static quantization scales.
- First 3 experiments:
  1.  **End-to-End Validation:** Run the official DPQ implementation on a standard model (e.g., Llama-2-7B). Compare perplexity and task accuracy against the BF16 baseline and a standard W4A16 GPTQ baseline. Measure throughput (tokens/sec) across various batch sizes.
  2.  **Ablation Study:** Disable key components one by one: (a) Run without the dual-precision error compensation (naive BF16→FP8→INT4), (b) Run without GAR. This quantifies the individual contribution of each mechanism to the final result.
  3.  **Profiling Analysis:** Use hardware profiling tools (e.g., Intel VTune or equivalent) during inference to measure the actual time spent in INT4-to-FP8 dequantization vs. the FP8 GeMM operation. This validates the assumption that dequantization overhead is negligible.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What alternative ranking criteria for Group-Aware Reordering (GAR) could further improve accuracy compared to using the maximum Hessian diagonal element per group?
- Basis in paper: [explicit] The authors state: "We leave the exploration of other ranking strategies for future work," and suggest alternatives such as "ranking groups by the average of the top 10% largest elements before sorting."
- Why unresolved: Only one ranking criterion (maximum diagonal element) was tested; the paper does not compare against alternative strategies.
- What evidence would resolve it: Systematic ablation comparing multiple ranking criteria (e.g., top-k average, variance-based, entropy-based) across Llama and Qwen2-VL models with accuracy metrics.

### Open Question 2
- Question: Can combining DPQ with orthogonal quantization techniques such as AWQ, QuaRot, or SmoothQuant further reduce accuracy degradation?
- Basis in paper: [explicit] "Our quantization method is orthogonal to many existing quantization strategies, such as AWQ, QuaRot, and others. Incorporating these techniques may further enhance our results. We leave this for future work."
- Why unresolved: No experiments integrate DPQ with these complementary methods; their combined effect is unknown.
- What evidence would resolve it: Experiments applying DPQ alongside AWQ/QuaRot on shared benchmarks (e.g., WikiText-2, MMLU) reporting perplexity and accuracy deltas.

### Open Question 3
- Question: How does DPQ generalize to model architectures beyond Transformer-based LLMs and VLMs, such as Mixture-of-Experts or diffusion models?
- Basis in paper: [inferred] Evaluation is limited to Llama-2, Llama-3, and Qwen2-VL families; architectural diversity is not explored.
- Why unresolved: Different architectures have distinct weight/activation distributions and sparsity patterns that may affect the W4A8 scheme and GAR effectiveness.
- What evidence would resolve it: Benchmarks on MoE models (e.g., Mixtral) and diffusion transformers, comparing DPQ vs. W8A8 and W4A16 baselines.

## Limitations

- The effectiveness of DPQ is contingent on hardware with efficient FP8 matrix multiplication support; on hardware lacking this capability, the proposed speedups may not materialize.
- The two-step error compensation relies on an accurate Hessian approximation from a limited calibration dataset; if this data is unrepresentative, the error compensation may fail, leading to significant accuracy degradation.
- Group-Aware Reordering (GAR) introduces a trade-off between complexity and benefit; its advantage over unconstrained reordering is primarily inference-time efficiency, but if important weights are not well-aligned with group boundaries, its impact on accuracy preservation may be limited.

## Confidence

- **High Confidence:** The paper's core motivation (optimizing for memory-bound vs. compute-bound regimes) and the general approach of decoupling storage and compute precision are well-established in the quantization literature.
- **Medium Confidence:** The specific implementation details of the dual-precision error compensation and the constrained reordering (GAR) are novel contributions whose effectiveness depends on careful implementation and parameter tuning.
- **Medium Confidence:** The reported throughput improvements (up to 3x) are promising but are based on experiments using Intel's Neural Compressor and Gaudi 2/3 accelerators; generalizability to other hardware requires further validation.

## Next Checks

1. **Hardware Portability Test:** Implement DPQ on a different hardware platform with native FP8 support (e.g., NVIDIA H100). Measure and compare the actual throughput improvement against the reported Gaudi results. This validates the claim that the benefits are not tied to a single hardware vendor.

2. **Ablation on Error Compensation:** Perform a detailed ablation study isolating the contribution of the Hessian-based error compensation. Run the same model with (a) standard W4A8 without dual-precision compensation, and (b) with a simplified, uniform error distribution. This quantifies the specific value added by the second-order information.

3. **Robustness to Calibration Data:** Quantize the same model using DPQ but with two different, diverse calibration datasets (e.g., one from a different domain). Compare the final accuracy. This tests the sensitivity of the Hessian approximation and the overall method to the choice of calibration data.