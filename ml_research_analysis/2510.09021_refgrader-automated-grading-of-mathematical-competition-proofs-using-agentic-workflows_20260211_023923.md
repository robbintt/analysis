---
ver: rpa2
title: 'RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic
  Workflows'
arxiv_id: '2510.09021'
source_url: https://arxiv.org/abs/2510.09021
tags:
- solution
- strategic
- problem
- step
- insights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of accurately grading mathematical
  competition proofs, moving beyond simple correctness to assess partial credit fairly.
  It introduces agentic workflows that automatically derive problem-specific rubrics
  by analyzing reference solutions and extracting key "aha moments." The approach
  improves grading consistency and agreement with human judges across diverse metrics
  such as Pearson/Spearman correlation, MAE/RMSE, QWK, and AC2, compared to single-turn
  grading.
---

# RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows

## Quick Facts
- **arXiv ID**: 2510.09021
- **Source URL**: https://arxiv.org/abs/2510.09021
- **Reference count**: 40
- **Primary result**: Automated grading of mathematical competition proofs with agentic workflows outperforms single-turn grading across multiple metrics and better detects partial credit.

## Executive Summary
This paper introduces RefGrader, an automated grading system for mathematical competition proofs that moves beyond simple correctness to assess partial credit fairly. The system uses agentic workflows to automatically derive problem-specific rubrics by analyzing reference solutions and extracting key "aha moments." The approach significantly improves grading consistency and agreement with human judges across diverse metrics compared to single-turn grading, while also better detecting partial progress in incomplete solutions. The methods are cacheable for efficiency and released publicly.

## Method Summary
RefGrader employs a five-stage agentic workflow using Gemini 2.5 Pro to grade mathematical competition proofs. The workflow first clusters reference solutions by approach, matches student solutions to the closest reference cluster, analyzes references to extract key "aha moments" or main steps, auto-generates a rubric allocating points across these steps, and finally grades by detecting errors and mapping them to the rubric. The system is evaluated on two datasets: IMO Shortlist (90 problems with Gemini-generated solutions) and MathArena (385 solutions from 12 models). The approach includes variants focusing on approachability (difficulty-weighted rubrics) and milestones (structural step completion), with a hybrid option that underperformed individual methods.

## Key Results
- Agentic workflows significantly outperform single-turn grading across Pearson/Spearman correlation, MAE/RMSE, QWK, and AC2 metrics
- Better detection of partial progress in incomplete solutions compared to baseline approaches
- Ablation studies show that adding reference solutions and rubrics significantly boosts performance, while sampling/averaging within a method does not
- Cacheable workflow steps (clustering, rubric generation) improve efficiency for repeated grading tasks

## Why This Works (Mechanism)
The agentic workflow succeeds by incorporating domain expertise through reference solution analysis rather than relying solely on pattern matching. By clustering reference solutions and extracting "aha moments," the system creates tailored rubrics that reflect the actual solution space for each problem. This approach captures the hierarchical nature of mathematical proofs—where certain key insights unlock subsequent steps—allowing for nuanced partial credit assessment. The workflow's ability to analyze solution approaches rather than just final answers enables it to recognize meaningful mathematical progress even in incomplete solutions.

## Foundational Learning

**Mathematical Proof Structure**: Understanding that proofs have hierarchical, logical dependencies where certain key insights (lemmas, constructions) enable subsequent steps. Why needed: Essential for designing rubrics that allocate credit appropriately across proof components. Quick check: Can the grader correctly identify the critical steps in a simple proof and their logical dependencies?

**Partial Credit Assessment**: Recognizing that mathematical work has varying degrees of correctness and completeness, requiring granular evaluation beyond binary right/wrong judgments. Why needed: Enables fair evaluation of incomplete or partially correct solutions common in competition settings. Quick check: Does the grader assign intermediate scores (e.g., 3/7) to solutions that demonstrate some correct reasoning but have critical errors?

**Agentic Workflow Design**: Breaking down complex tasks into sequential reasoning steps where each agent builds on previous outputs. Why needed: Allows systematic decomposition of the grading task into manageable subtasks (clustering, matching, rubric generation, grading). Quick check: Can each workflow stage be executed independently and produce consistent outputs when given the same inputs?

## Architecture Onboarding

**Component Map**: Reference Solution Scraping -> Solution Clustering -> Student Solution Matching -> Reference Analysis -> Rubric Generation -> Error Detection -> Final Grading

**Critical Path**: The core evaluation pipeline follows: Student Solution + Reference Cluster -> Approachability/Milestone Analysis -> Rubric Application -> Final Score. This path must be efficient for practical deployment.

**Design Tradeoffs**: The system trades initial computational overhead (clustering, rubric generation) for improved grading accuracy and partial credit detection. Caching intermediate results mitigates this cost for repeated grading of the same problem.

**Failure Signatures**: Over-optimistic grading of low-quality solutions (rightward bias in confusion matrix), poor clustering leading to mismatched reference-student pairs, or rubric generation that fails to capture essential proof steps. These manifest as systematic grade inflation or inability to distinguish between fundamentally different solution approaches.

**Three First Experiments**:
1. Run single-turn grading baseline on IMO Shortlist data using provided prompts to establish performance floor
2. Implement 5-step Ref-Grader (Approachability) workflow and compare against baseline on correlation metrics
3. Test cache efficiency by measuring grading time with cached vs. non-cached intermediate steps

## Open Questions the Paper Calls Out

**Open Question 1**: What systematic ensembling strategies best leverage the complementary strengths of different RefGrader variants? The paper only performed simple averaging across methods and did not explore weighted voting, meta-graders, or confidence-based selection.

**Open Question 2**: Why does the hybrid combination of approachability and milestone-based rubrics degrade performance compared to individual methods? The paper identifies a conflict between difficulty-weighting and structural goals but does not propose reconciliation methods.

**Open Question 3**: Can the RefGrader workflow function effectively as a generative reward model for reinforcement learning? The paper proposes this application but provides no experimental validation of using the workflow as feedback for training a prover model.

**Open Question 4**: How robust is the grading workflow when high-quality reference solutions are scarce or must be generated by the model itself? The paper does not ablate performance when references are missing or low-quality, limiting applicability to novel problems.

## Limitations

- Heavy dependence on reference solution quality and coverage, which may vary significantly across problems and introduce bias
- Focus on well-defined proof problems with clear logical steps, potentially limiting generalization to open-ended or creative proof tasks
- Use of a single model (Gemini 2.5 Pro) raises concerns about model-specific biases and generalizability to other architectures

## Confidence

- **High Confidence**: Claims about improved grading consistency and better detection of partial progress are well-supported by multiple metrics across two datasets
- **Medium Confidence**: Claims about significant improvement over single-turn grading are supported, but some metrics show modest gains and generalizability to non-IMO problems is untested
- **Low Confidence**: Claims about scalability are not substantiated with runtime or cost analysis, and caching efficiency is mentioned but not benchmarked

## Next Checks

1. **Reference Solution Quality Audit**: Manually review a sample of scraped AoPS reference solutions for correctness and representativeness, then correlate solution quality with grading accuracy

2. **Cross-Model Grading Consistency**: Implement the same agentic workflows using different frontier models (e.g., GPT-4o, Claude 3.5) and measure inter-model agreement to assess model-specific bias

3. **Runtime and Cost Benchmarking**: Measure wall-clock time and API costs for initial clustering/rubric generation versus cached grading to quantify scalability claims