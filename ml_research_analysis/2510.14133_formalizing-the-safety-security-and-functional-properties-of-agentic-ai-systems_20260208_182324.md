---
ver: rpa2
title: Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems
arxiv_id: '2510.14133'
source_url: https://arxiv.org/abs/2510.14133
tags:
- agent
- task
- state
- systems
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the critical challenge of ensuring the safety,
  security, and functional properties of agentic AI systems that leverage multiple
  autonomous agents and Large Language Models (LLMs). The authors introduce a modeling
  framework that consists of two foundational models: the Host Agent model, which
  formalizes the top-level entity that interacts with users and orchestrates task
  execution, and the Task Lifecycle model, which details the states and transitions
  of individual sub-tasks from creation to completion.'
---

# Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems

## Quick Facts
- arXiv ID: 2510.14133
- Source URL: https://arxiv.org/abs/2510.14133
- Reference count: 40
- This paper introduces a modeling framework for formally verifying safety, security, and functional properties of agentic AI systems using temporal logic.

## Executive Summary
This paper addresses the critical challenge of ensuring the safety, security, and functional properties of agentic AI systems that leverage multiple autonomous agents and Large Language Models (LLMs). The authors introduce a modeling framework that consists of two foundational models: the Host Agent model, which formalizes the top-level entity that interacts with users and orchestrates task execution, and the Task Lifecycle model, which details the states and transitions of individual sub-tasks from creation to completion. Grounded in these models, the paper defines 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. These properties are expressed in temporal logic, enabling formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. The paper presents the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.

## Method Summary
The authors formalize agentic AI systems using two state machine models: the Host Agent model (H = (Agents, Entities, Registry, Orchestrator)) and the Task Lifecycle model (L = (S_t, s_0, E_t, δ)). These models abstract the system architecture into discrete states and transitions, enabling rigorous analysis. The framework defines 31 temporal logic properties (17 Host Agent, 14 Task Lifecycle) expressed in CTL/LTL notation, covering liveness (e.g., eventual response), safety (e.g., privilege escalation prevention), completeness, and fairness. Properties are verified using formal methods, with assumptions about correct behavior of the Validation Module and external entities. The approach enables detection of coordination failures, deadlocks, and security vulnerabilities across heterogeneous protocols like MCP and A2A.

## Key Results
- Introduced the first domain-agnostic modeling framework for verifying safety, security, and functional properties of agentic AI systems
- Defined 31 formal temporal logic properties (17 Host Agent, 14 Task Lifecycle) to ensure liveness, safety, completeness, and fairness
- Demonstrated how unified semantic modeling enables detection of cross-protocol coordination failures and prevents deadlocks through DAG-based dependency tracking
- Provided a foundation for formal verification of agentic AI systems using established computer science techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A unified semantic framework enables detection of cross-protocol coordination failures that isolated protocol analysis cannot capture.
- Mechanism: The Host Agent model abstracts both MCP (vertical tool access) and A2A (horizontal agent coordination) into a single state space (S_H), allowing temporal logic properties to reason across protocol boundaries. The Orchestrator's DAG structure (D = (V, S)) enforces dependency ordering regardless of whether sub-tasks invoke tools or delegate to agents.
- Core assumption: External entities (agents and tools) adhere to their advertised capability profiles; the Validation Module (VM) correctly identifies malicious or unreliable entities.
- Evidence anchors:
  - [abstract] "This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues."
  - [Page 3, Section III] Describes task handoff failures and inconsistent state management as consequences of lacking unified semantics.
  - [corpus] "A Safety and Security Framework for Real-World Agentic Systems" (FMR=0.69) supports that safety emerges from dynamic interactions among orchestrators, tools, and data.
- Break condition: If the VM is compromised or capability profiles are falsified, the trust anchoring fails and safety properties cannot be guaranteed.

### Mechanism 2
- Claim: Finite state machines with explicit transition functions enable formal verification of task execution integrity.
- Mechanism: The Task Lifecycle model (L = (S_t, s_0, E_t, δ)) constrains sub-task behavior to 11 discrete states with deterministic transitions. Safety properties like TL6 ("sub-task may only enter COMPLETED if previously IN PROGRESS") become temporal logic invariants (G((state=COMPLETED) → (previous_state=IN_PROGRESS))) that model checkers can verify.
- Core assumption: The transition function δ correctly captures all valid state changes; no undefined transitions occur at runtime.
- Evidence anchors:
  - [Page 6] Formal definition of L = (S_t, s_0, E_t, δ) with explicit state set enumeration.
  - [Page 9, Table II] 14 temporal logic properties covering liveness, safety, fairness, and specific flow paths.
  - [corpus] Limited direct corpus evidence for FSM-based agentic verification; this appears to be a novel contribution.
- Break condition: If agents can spontaneously enter states outside S_t or if δ permits transitions not in the model, verification guarantees are void.

### Mechanism 3
- Claim: DAG-based task decomposition with dependency tracking prevents deadlocks and privilege escalation through causal isolation.
- Mechanism: The Orchestrator constructs a Directed Acyclic Graph where edges (v_i, v_j) ∈ S enforce that v_j executes only after v_i completes. Property HP10 (AG(CL.invoke(EE, protocol, sub_task_i)) → dependencies[sub_task_i] = ∅) ensures no sub-task runs before its dependencies resolve, preventing circular delegation loops and containing fault propagation.
- Core assumption: The LLM correctly decomposes user intent into a valid DAG without cycles or spurious dependencies; the Orchestrator faithfully executes the DAG structure.
- Evidence anchors:
  - [Page 5] "A directed edge (v_i, v_j) ∈ S implies that sub-task v_j executes only after sub-task v_i successfully completes."
  - [Page 3, Section III] Identifies circular delegation loops and privilege escalation as key coordination issues.
  - [corpus] "Extending the OWASP Multi-Agentic System Threat Modeling Guide" (FMR=0.56) addresses unreliable delegation as a critical risk category.
- Break condition: If the LLM generates a cyclic graph or if runtime conditions allow bypassing dependency checks, deadlocks and race conditions may occur.

## Foundational Learning

- Concept: **Linear Temporal Logic (LTL) and Computation Tree Logic (CTL)**
  - Why needed here: All 31 formal properties are expressed in CTL/LTL notation (AG, AF, G, EX, AX operators). Understanding these is required to interpret, modify, or verify the safety/liveness guarantees.
  - Quick check question: What does AG(Req_U → AF Resp_H) mean, and how does it differ from G(Req_U → F Resp_H)?

- Concept: **Finite State Machines with Output (Mealy/Moore Machines)**
  - Why needed here: The Task Lifecycle model is essentially an FSM where transitions depend on both current state and external events (E_t). Understanding state reachability and invariant preservation is essential.
  - Quick check question: Given the state set S_t, is ERROR a sink state? Which properties guarantee this?

- Concept: **Directed Acyclic Graphs (DAGs) for Task Scheduling**
  - Why needed here: The Orchestrator uses DAGs to represent task dependencies. Topological ordering, parallel execution of independent nodes, and deadlock freedom all derive from DAG properties.
  - Quick check question: How would you detect if an LLM-generated task graph contains a cycle? What recovery mechanism does the model propose?

## Architecture Onboarding

- Component map:
  - Host Agent Core (HAC) -> Registry -> Orchestrator -> Communication Layer (CL) -> External Entities
  - Validation Module (VM) enforces trust anchoring

- Critical path: User prompt → HAC intent resolution (LLM + DM) → Registry discovery → Orchestrator DAG construction → CL sub-task dispatch → External entity execution → Result aggregation → Final response

- Design tradeoffs:
  - Dynamic vs. static workflows: Model supports dynamic DAG generation via LLM (flexible) but risks invalid graphs (safety cost)
  - Centralized vs. distributed validation: Registry centralizes trust anchoring (simpler verification) but creates single point of failure
  - Recovery complexity: RETRY SCHEDULED and FALLBACK SELECTED states improve liveness but expand state space for verification

- Failure signatures:
  - Circular delegation: Sub-tasks remain in DISPATCHING or AWAITING_DEPENDENCY indefinitely (violates TL10, TL11)
  - Privilege escalation: Unvalidated EE invoked (violates HP9); check VM logs
  - Intent drift: HAC fails to clarify ambiguous requests (violates HP12); user receives no response or incorrect task execution

- First 3 experiments:
  1. Implement a minimal Host Agent with hardcoded DAG for a 2-sub-task workflow; verify HP1 (eventual response) and HP10 (dependency ordering) hold under simulated network delays.
  2. Extend Task Lifecycle model with a timeout transition from IN_PROGRESS to FAILED; verify TL1 (eventual termination) holds even when external entities hang.
  3. Inject a malicious external entity that advertises false capabilities; test whether VM validation (HP9) prevents invocation or if the system violates the safety property.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can property violations be automatically detected in implemented Agentic AI systems by deriving formal models directly from source code?
- Basis in paper: [explicit] The conclusion states that future work will focus on "automatically detecting property violations in coded Agentic AI systems by deriving a formal model from the code and model-checking specified properties against it."
- Why unresolved: The current paper defines the theoretical framework, properties, and abstract models but does not provide the tooling or methodology to bridge the gap between code implementations and formal verification.
- What evidence would resolve it: A software pipeline or static analysis tool that ingests agent code (e.g., LangChain implementations) and outputs verification results for the 31 defined properties.

### Open Question 2
- Question: What specific mechanisms are required to construct the "Validation Module" (VM) assumed to be responsible for the trust anchoring of external entities?
- Basis in paper: [inferred] Section V.A explicitly states, "For the formal analysis, we assume the existence of a correct and functioning VM," relying on it as a precondition for Safety property HP9 without defining its implementation.
- Why unresolved: The security of the Host Agent depends on this module successfully validating schemas and behaviors, but the paper leaves the complex problem of validating potentially malicious or stochastic agents as an assumption.
- What evidence would resolve it: A formal specification or prototype of the VM that can robustly validate dynamic agent capabilities and credentials against adversarial manipulation.

### Open Question 3
- Question: How does the Task Lifecycle Model's deterministic transition function (δ) reconcile with the inherent stochasticity and uncertainty of LLM-based reasoning?
- Basis in paper: [inferred] Section IV.B defines the state transition function δ: S_t × E_t → S_t as deterministic, whereas the Background section emphasizes that agents "exhibit adaptive behavior" and make decisions based on "incomplete or uncertain information."
- Why unresolved: Standard temporal logic assumes deterministic or explicitly probabilistic transitions; the paper does not address how the high variance of LLM outputs is mapped to discrete, verifiable state changes.
- What evidence would resolve it: An extension of the framework using probabilistic model checking (e.g., PCTL) or empirical validation showing that LLM behavior conforms to the defined discrete transitions.

## Limitations

- The framework relies on idealized assumptions about external entity behavior and the capability of the Validation Module to correctly identify trustworthy agents and tools
- The paper abstracts away implementation details of critical components like the LLM-based task decomposition algorithm and the Registry's entity registration process
- No empirical validation or case studies demonstrate the framework's effectiveness in detecting coordination failures or preventing security vulnerabilities in actual agentic AI systems

## Confidence

- **High confidence**: The formal modeling approach using temporal logic is sound and well-established in computer science. The state machine definitions for both Host Agent and Task Lifecycle are clearly specified and internally consistent.
- **Medium confidence**: The 31 properties specified in temporal logic appear comprehensive for the stated categories (liveness, safety, completeness, fairness), but their practical effectiveness depends on correct implementation of the underlying components and accurate modeling of real-world failure modes.
- **Low confidence**: The paper does not provide empirical validation or case studies demonstrating the framework's effectiveness in detecting coordination failures or preventing security vulnerabilities in actual agentic AI systems. The recovery mechanisms for circular delegation and privilege escalation are mentioned but not detailed.

## Next Checks

1. Implement a formal verification environment (e.g., TLA+ or NuSMV) for the Host Agent and Task Lifecycle models, then test the 31 temporal logic properties against adversarial scenarios including falsified capability profiles and cyclic dependency graphs.
2. Design and execute a prototype implementation that simulates multiple autonomous agents with varying reliability levels, measuring how effectively the framework prevents deadlocks and contains fault propagation compared to baseline approaches.
3. Conduct a threat modeling exercise using the OWASP Multi-Agentic System Threat Modeling Guide to identify gaps between the formal properties and practical security concerns that may arise in production deployments.