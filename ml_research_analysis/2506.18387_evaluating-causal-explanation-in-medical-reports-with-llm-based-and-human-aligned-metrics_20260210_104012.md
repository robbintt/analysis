---
ver: rpa2
title: Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned
  Metrics
arxiv_id: '2506.18387'
source_url: https://arxiv.org/abs/2506.18387
tags:
- evaluation
- report
- metrics
- causal
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the quality of causal explanations in automatically\
  \ generated medical reports using six metrics: BERTScore, Cosine Similarity, BioSentVec,\
  \ GPT-White, GPT-Black, and expert qualitative assessment. Across two input types\u2014\
  observation-based and multiple-choice-based\u2014GPT-Black and GPT-White demonstrated\
  \ the strongest alignment with expert judgments and highest discriminative power\
  \ for identifying clinically valid causal reasoning."
---

# Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics

## Quick Facts
- arXiv ID: 2506.18387
- Source URL: https://arxiv.org/abs/2506.18387
- Reference count: 8
- LLM-based evaluators (GPT-Black, GPT-White) outperformed similarity metrics in aligning with expert judgments for medical report causal reasoning quality

## Executive Summary
This study evaluates causal explanation quality in automatically generated medical reports using six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment. Across observation-based and multiple-choice-based inputs, GPT-Black and GPT-White demonstrated strongest alignment with expert judgments and highest discriminative power for identifying clinically valid causal reasoning. Similarity-based metrics showed limited sensitivity to diagnostic coherence and often diverged from clinical quality assessments. GPT-Black's rule-based structure and wide score range made it particularly effective for evaluating logical integrity in medical reasoning.

## Method Summary
The study evaluates five externally generated diagnostic report models (A-E) from the NTCIR-18 Hidden-Rad shared task using six evaluation metrics. Reports were generated from two input types: observation-based (chest radiograph findings) and multiple-choice-based (physician QA justifications). Three similarity metrics (BERTScore, Cosine Similarity, BioSentVec) computed against reference reports, while GPT-White applied a 100-point rubric and GPT-Black used a rule-based bonus/penalty system. Expert qualitative assessments served as the ground truth. Two weighting schemes aggregated scores: task-prioritized (25/25/20/20/5/5% for GPT-White/GPT-Black/BioSentVec/expert/BERTScore/Cosine Similarity) and equal (16.67% each).

## Key Results
- GPT-Black and GPT-White showed strongest alignment with expert judgments and highest discriminative power for clinical causal reasoning
- GPT-Black demonstrated the broadest score range (0.136 between top and bottom models), underlining its effectiveness in assessing causal explanation depth
- Similarity-based metrics (BERTScore, Cosine Similarity, BioSentVec) showed limited sensitivity to diagnostic coherence and often diverged from clinical quality assessments
- Task-prioritized weighting favored models with stronger reasoning capabilities, while equal weighting obscured clinically relevant distinctions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evaluators (GPT-Black, GPT-White) capture causal reasoning quality better than embedding-similarity metrics
- Mechanism: GPT-Black applies a rule-based bonus/penalty system (+0.2/−0.2/−0.1) targeting logical consistency, diagnostic accuracy, and causal completeness, while GPT-White uses a 100-point rubric scoring contextual similarity, diagnosis-centric focus, and causal explanation clarity. Both accept structured prompts with reference and generated reports, outputting numeric scores that reflect clinical reasoning coherence
- Core assumption: The LLM's parametric knowledge includes sufficient clinical reasoning patterns to discriminate valid vs. invalid causal narratives; prompt design reliably elicits this capacity
- Evidence anchors:
  - [abstract] "GPT-Black and GPT-White demonstrated the strongest alignment with expert judgments and highest discriminative power for identifying clinically valid causal reasoning"
  - [section 4.3] "GPT-Black showed the broadest score range (0.136 between top and bottom models), underlining its effectiveness in assessing the depth and structure of causal explanation"
  - [corpus] Related work (arXiv:2502.20635) explores LLM-assisted evaluation of ML explanation quality, suggesting transferability of LLM-as-evaluator approaches
- Break condition: If prompts lack domain-specific criteria or the LLM has insufficient medical knowledge, scores may dissociate from clinical validity

### Mechanism 2
- Claim: Similarity-based metrics (BERTScore, Cosine Similarity) fail to capture diagnostic reasoning because they conflate lexical/semantic overlap with clinical coherence
- Mechanism: These metrics compute token-level or sentence-level vector alignments without modeling the directional, conditional structure of causal explanations. A generated report can achieve high similarity by matching surface terms while omitting or misordering causal links
- Core assumption: High semantic similarity correlates with factual correctness; this assumption does not hold when diagnostic logic requires non-local dependencies
- Evidence anchors:
  - [abstract] "Similarity-based metrics (BERTScore, Cosine Similarity, BioSentVec) showed limited sensitivity to diagnostic coherence and often diverged from clinical quality assessments"
  - [section 4.3] "Model C's highest BERTScore (0.224) in the multiple-choice task did not translate into high GPT-Black (0.723) or expert (0.783) scores, underscoring the disconnect between surface similarity and causal soundness"
  - [corpus] Weak direct evidence on this specific failure mode; related papers focus on evaluation metrics but not the similarity-causality gap explicitly
- Break condition: If generated reports are near-duplicates of references, similarity metrics may coincidentally align with quality; divergence emerges when paraphrasing or reordering occurs

### Mechanism 3
- Claim: Weighting schemes that prioritize causal metrics over surface metrics change model rankings and better reflect clinical utility
- Mechanism: Task-prioritized weights (25% each for GPT-White/GPT-Black, 20% for BioSentVec/expert, 5% for BERTScore/Cosine Similarity) amplify differences in reasoning quality, while equal weights (16.7% each) allow surface metrics to obscure clinically relevant distinctions
- Core assumption: The selected weights meaningfully reflect task priorities; expert qualitative assessment is a reliable ground truth for clinical reasoning
- Evidence anchors:
  - [section 4.1] "Under task-prioritized weights, Model B slightly outperformed Model C (0.680 vs. 0.678), while the order reversed under equal weighting due to stronger BERTScore and Cosine Similarity from Model C"
  - [section 5] "Task-specific weighting schemes—prioritizing causal relevance and clinical interpretability—favored models with stronger reasoning capabilities"
  - [corpus] No direct corpus evidence on weighting schemes for medical evaluation; this appears underexplored
- Break condition: If weights are set arbitrarily without domain expert input, rankings may not generalize across clinical contexts

## Foundational Learning

- Concept: **Causal explanation vs. surface description in clinical text**
  - Why needed here: The paper's core thesis is that diagnostic reports must link observations to interpretations; evaluators must assess this linkage, not just term overlap
  - Quick check question: Given "bilateral lower lung opacity" as an observation, what additional statement transforms it into a causal explanation?

- Concept: **LLM-as-judge evaluation paradigm**
  - Why needed here: GPT-Black and GPT-White instantiate this paradigm; understanding prompt design, rubric structure, and output constraints is essential for replication or extension
  - Quick check question: What output format constraint does GPT-Black enforce to ensure reproducibility?

- Concept: **Metric discriminative power (score range variance)**
  - Why needed here: The paper uses score spread between top and bottom models as a proxy for metric sensitivity; this is a non-obvious but critical evaluation criterion
  - Quick check question: Why might a metric with narrow score variance be less useful for ranking models even if it correlates with quality?

## Architecture Onboarding

- Component map:
  External LLM systems (Model A-E) -> Generated diagnostic reports -> Six evaluation metrics (BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, Expert) -> Two weighting schemes -> Composite scores

- Critical path:
  1. Receive generated reports from external systems
  2. Apply similarity metrics via embedding comparison against reference reports
  3. Query GPT-White with rubric-based prompts; query GPT-Black with rule-based bonus/penalty prompts
  4. Collect expert qualitative assessments (independent, blinded)
  5. Compute weighted composite scores under both weighting schemes
  6. Compare metric-level discriminative power via score range analysis

- Design tradeoffs:
  - GPT-White offers transparent, rubric-aligned scores but requires manual rubric design; GPT-Black offers finer-grained causal assessment but depends on prompt engineering quality
  - Similarity metrics are computationally cheap and deterministic but insensitive to reasoning
  - Expert evaluation is gold-standard but unscalable and subject to inter-rater variance (not measured in this study)

- Failure signatures:
  - High similarity scores + low GPT/expert scores → generated report mimics reference wording without coherent reasoning
  - Narrow score variance across models → metric lacks discriminative power; consider exclusion or reweighting
  - GPT-Black/GPT-White scores diverge from expert judgment → possible prompt misalignment or LLM knowledge gap

- First 3 experiments:
  1. Replicate GPT-Black evaluation on a held-out set of reports with modified prompts (e.g., remove penalty rules) to measure sensitivity to prompt design
  2. Add inter-rater agreement analysis for expert evaluations (e.g., Cohen's kappa) to quantify qualitative benchmark reliability
  3. Test BioSentVec vs. general-domain embeddings to isolate whether domain-specific training improves causal sensitivity or merely tightens score distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on pre-generated reports from external models, limiting control over generation quality and diversity
- GPT-White and GPT-Black evaluation prompts are not fully specified, making exact replication challenging
- Expert qualitative assessments serve as ground truth but lack reported inter-rater reliability measures
- Analysis focuses on discriminative power through score range rather than correlation coefficients

## Confidence
- **High Confidence**: GPT-Black and GPT-White alignment with expert judgments for causal reasoning quality; superiority over similarity-based metrics in clinical coherence assessment
- **Medium Confidence**: Task-prioritized weighting scheme effectiveness; BioSentVec domain-specific advantage claims (limited direct evidence)
- **Low Confidence**: Exact prompt specifications for LLM evaluators; generalizability of findings to other medical report types or evaluation contexts

## Next Checks
1. Conduct inter-rater reliability analysis on expert assessments to establish qualitative benchmark stability
2. Replicate GPT-Black evaluation with modified prompts (e.g., removing penalty rules) to test sensitivity to prompt design
3. Compare BioSentVec against general-domain embeddings on the same corpus to isolate domain-specific effects on causal reasoning detection