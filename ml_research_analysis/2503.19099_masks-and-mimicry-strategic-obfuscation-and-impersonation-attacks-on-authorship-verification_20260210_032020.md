---
ver: rpa2
title: 'Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship
  Verification'
arxiv_id: '2503.19099'
source_url: https://arxiv.org/abs/2503.19099
tags:
- authorship
- author
- obfuscation
- attacks
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluated the robustness of an authorship verification
  model to two types of adversarial attacks: authorship obfuscation and authorship
  impersonation. The study used several large language models, including Mistral and
  DIPPER, to perform these attacks on a high-performing authorship verification model.'
---

# Masks and Mimicry: Strategic Obfuscation and Impersonation Attacks on Authorship Verification

## Quick Facts
- arXiv ID: 2503.19099
- Source URL: https://arxiv.org/abs/2503.19099
- Reference count: 40
- Attack success rates reached 92% for obfuscation and 78% for impersonation

## Executive Summary
This work evaluates the robustness of authorship verification models against two LLM-based adversarial attacks: authorship obfuscation (masking author identity while preserving semantics) and authorship impersonation (mimicking a target author's style). Using the PAN20 FanFiction and CelebTwitter datasets with ~250-token truncation, the study tests DIPPER, PEGASUS, and Mistral-based attacks against a BigBird AV model. Obfuscation attacks successfully flipped true author predictions to false with up to 92% success rate, while impersonation attacks achieved 78% success by making authors' styles indistinguishable. The results reveal significant vulnerabilities in AV systems to these attacks while maintaining semantic coherence through BLEU, ROUGE, and BERTScore metrics.

## Method Summary
The study evaluates two types of LLM-based attacks on a BigBird authorship verification model. For obfuscation, DIPPER (T5-XXL 11B), PEGASUS, and Mistral-7B-Instruct-v0.1 generate paraphrases under zero-shot and author-profile-aware prompts to flip true-author predictions to false. For impersonation, a Mistral+RAG approach extracts target author style from retrieved documents using MPNET embeddings and chain-of-thought prompting, while STRAP employs GPT-2 fine-tuned for style transfer. The evaluation uses PAN20 FanFiction (1595 authors) and CelebTwitter (129 authors) datasets with 250-token truncation at sentence boundaries. Success is measured via Attack Success Rate (ASR) with threshold at EER score = 0.29, and semantic preservation is validated through BLEU, ROUGE, and BERTScore metrics.

## Key Results
- Authorship obfuscation attacks achieved maximum 92% ASR, successfully masking author identity while preserving semantic content
- Authorship impersonation attacks reached 78% ASR by making target authors' writing styles indistinguishable from source authors
- Semantic preservation metrics remained strong: BLEU ~70+, ROUGE ~50+, BERTScore ~75+ despite successful adversarial perturbations

## Why This Works (Mechanism)
The attacks exploit the fact that authorship verification models rely on statistical patterns in writing style that can be disrupted or mimicked through controlled text transformations. Obfuscation works by paraphrasing text while preserving meaning but altering stylistic fingerprints, while impersonation works by extracting and applying target author characteristics through retrieval-augmented generation and style transfer techniques.

## Foundational Learning
- **Authorship Verification (AV)**: Binary classification task determining if two documents share the same author based on stylistic features. Why needed: Defines the target model being attacked and success criteria for both attack types.
- **Attack Success Rate (ASR)**: Fraction of true trials flipped to false trials (obfuscation) or false trials flipped to true trials (impersonation). Why needed: Primary metric quantifying attack effectiveness against AV models.
- **Semantic Preservation Metrics**: BLEU, ROUGE, and BERTScore measuring content similarity between original and perturbed texts. Why needed: Ensures attacks maintain semantic integrity while achieving stylistic manipulation.
- **Decision Thresholds in AV**: EER (Equal Error Rate) score of 0.29 used to classify author similarity. Why needed: Critical parameter affecting both baseline model performance and attack success rates.
- **RAG-based Style Extraction**: Retrieval-augmented generation using MPNET embeddings to identify target author characteristics. Why needed: Core mechanism enabling effective impersonation attacks.
- **Style Transfer Models**: Fine-tuned models like STRAP (GPT-2) that learn to apply specific writing styles to text. Why needed: Alternative approach for generating stylistically consistent adversarial examples.

## Architecture Onboarding

**Component Map**: Datasets (PAN20, CelebTwitter) -> Preprocessing (250-token truncation) -> BigBird AV Model -> LLM-based Attacks (DIPPER, Mistral, STRAP) -> Perturbed Texts -> ASR Calculation -> Semantic Preservation Metrics

**Critical Path**: Data preparation → AV model inference → LLM attack generation → Re-inference → ASR calculation → Semantic evaluation

**Design Tradeoffs**: High ASR vs semantic preservation balance; model size and computational cost (11B DIPPER vs 7B Mistral) vs attack effectiveness; prompt engineering complexity vs retrieval-based approaches.

**Failure Signatures**: Over-aggressive paraphrasing achieving high ASR but destroying semantics (low BLEU/ROUGE/BERTScore); attacks succeeding on short texts but failing on longer documents; model-specific vulnerabilities that don't generalize across different AV architectures.

**3 First Experiments**:
1. Baseline AV performance verification on PAN20 and CelebTwitter with 0.29 threshold to ensure proper calibration
2. DIPPER attack implementation with both prompt variants and semantic preservation validation before ASR measurement
3. Paired statistical comparison of ASR across attack methods with confidence intervals

## Open Questions the Paper Calls Out
- **Targeted vs Random Obfuscation**: Whether targeting stylistically "impactful" text segments improves obfuscation efficiency compared to random paraphrasing, as current study only evaluates random percentage-based approaches.
- **Long-form Document Robustness**: How LLM-based attacks perform on long-form documents versus the short/medium texts (250 tokens) tested, given longer texts provide larger stylometric fingerprints.
- **PEFT Performance Gains**: Whether parameter-efficient fine-tuning (PEFT) could significantly increase impersonation success rates compared to current prompting and RAG-based approaches.

## Limitations
- Results may not generalize to long-form data as study focuses on short texts (~250 tokens) rather than full-length documents
- Missing statistical significance tests and confidence intervals across multiple runs, leaving result stability unverified
- Critical implementation details for BigBird model weights, Mistral RAG pipeline specifics, and STRAP fine-tuning parameters are not fully specified

## Confidence
- **High confidence**: Conceptual framework and attack methodology are clearly defined and reproducible in principle
- **Medium confidence**: Attack success rate measurements and semantic preservation metrics are reported but lack statistical validation
- **Low confidence**: Comparative claims about attack effectiveness cannot be independently verified due to insufficient methodological detail

## Next Checks
1. Reconstruct complete AV pipeline by obtaining BigBird model weights from Nguyen et al. (2023) and verifying 0.29 threshold produces balanced FPR/FNR
2. Implement DIPPER attack with exact prompt variants and verify semantic preservation metrics (BLEU~70+, ROUGE~50+, BERTScore~75+) before measuring ASR
3. Run each attack method across 5 random seeds with paired statistical tests and report confidence intervals to establish result significance