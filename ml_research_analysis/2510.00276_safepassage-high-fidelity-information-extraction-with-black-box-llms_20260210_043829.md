---
ver: rpa2
title: 'SafePassage: High-Fidelity Information Extraction with Black Box LLMs'
arxiv_id: '2510.00276'
source_url: https://arxiv.org/abs/2510.00276
tags:
- information
- entity
- context
- llms
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of hallucinations in black-box
  LLM information extraction by introducing a "safe passage" concept and a three-step
  SAFEPASSAGE pipeline: (1) structured generation of entities and context, (2) string-based
  alignment to verify grounding in the document, and (3) scoring to ensure the context
  supports the extracted information. The pipeline reduces hallucinations by up to
  85% on legal information extraction tasks with minimal false positives.'
---

# SafePassage: High-Fidelity Information Extraction with Black Box LLMs

## Quick Facts
- arXiv ID: 2510.00276
- Source URL: https://arxiv.org/abs/2510.00276
- Reference count: 14
- Primary result: Reduces LLM hallucination in information extraction by up to 85% on legal documents using a three-step "safe passage" pipeline

## Executive Summary
This paper addresses the critical problem of hallucinations in black-box LLM information extraction systems by introducing a "safe passage" concept that requires extracted entities to be grounded in source documents. The SAFEPASSAGE pipeline implements this through three steps: structured entity generation with context, string-based alignment to verify grounding, and scoring to ensure the context supports the extracted information. The approach significantly reduces hallucinations while maintaining precision, and surprisingly shows that a small transformer encoder fine-tuned on minimal human annotations outperforms expensive LLM scorers by roughly 2,000x in cost and latency.

## Method Summary
The SAFEPASSAGE pipeline addresses LLM hallucinations through a three-step process. First, an LLM generates structured entities with type, payload, and context using a JSON schema prompt. Second, a character-level Smith-Waterman alignment (via BioPython) verifies that the extracted context has a strong match (s ≥ 0.6) to the source document, filtering out mistranscriptions and hallucinated spans. Third, an NLI-based scorer (or LLM grader) evaluates whether the aligned context supports the extracted entity, predicting entailment. The NLI scorer uses DeBERTaV3-base, fine-tuned first on LLM-generated silver labels from 900 documents, then on 500 human-annotated examples using Sentence Transformers with early stopping on validation F1. The method requires only 1-2 hours of human annotation to achieve strong performance.

## Key Results
- Reduces hallucinations by up to 85% on legal information extraction tasks with minimal false positives
- Small transformer encoder fine-tuned on 1-2 hours of human annotations outperforms LLM grader by ~2,000x in cost and latency
- SAFEPASSAGE scores enable reliable offline evaluation of LLM extraction quality
- High precision (few false positives) in flagging hallucinations when entity contexts are not aligned to source documents

## Why This Works (Mechanism)
The pipeline works by introducing multiple independent verification steps that catch different types of hallucinations. The structured generation ensures consistent entity extraction, while the string-based alignment catches mistranscriptions, insertions, and contextual hallucinations by requiring exact document matches. The scoring step then verifies semantic consistency between the entity and its supporting context. The key insight is that requiring "safe passages" - grounded evidence from source documents - is more effective than trying to detect hallucinations after the fact. The surprising finding that a small encoder outperforms expensive LLMs demonstrates that the task can be solved efficiently with proper fine-tuning on domain-specific data.

## Foundational Learning
**Structured Generation (Why needed: Ensures consistent entity extraction format)**
Quick check: LLM outputs entities in Entity[type, payload, context] JSON format with no missing fields

**Smith-Waterman Alignment (Why needed: Catches mistranscriptions and contextual hallucinations)**
Quick check: Alignment scores s ≥ 0.6 for correctly grounded passages, low scores for hallucinated content

**NLI Scoring (Why needed: Verifies semantic consistency between entity and context)**
Quick check: Model predicts "ENTAILS" for supported entities, "NOT ENTAILS" for hallucinations

**Silver Label Generation (Why needed: Scales training data efficiently)**
Quick check: LLM grader produces consistent labels across similar entity-context pairs

**Early Stopping (Why needed: Prevents overfitting on small human-annotated datasets)**
Quick check: Validation F1 plateaus before training F1 reaches maximum

## Architecture Onboarding

**Component Map:** LLM Extraction -> Alignment Filter -> Scorer -> Output
```
[LLM Generator] -> [Smith-Waterman Aligner] -> [NLI Scorer] -> [Safe/Hallucinated]
```

**Critical Path:** The alignment filter is the critical path - if context doesn't align to document (s < 0.6), the passage is flagged as unsafe regardless of scorer output. This provides a strong, interpretable first-line defense against hallucinations.

**Design Tradeoffs:** Alignment threshold τ = 0.6 balances precision and recall; higher thresholds increase precision but may miss borderline hallucinations. Using a small encoder instead of LLM grader trades some potential accuracy for 2,000x cost reduction and lower latency, which is acceptable given the strong performance. Silver labels from LLM graders enable scaling to 40,000 examples while maintaining quality through noise reduction.

**Failure Signatures:** Low alignment scores indicate mistranscriptions, OCR fixes, capitalization changes, or context hallucinations. Scorer errors (false positives/negatives) suggest insufficient training examples for specific entity types or formatting issues in hypothesis/premise construction. Both can be diagnosed by inspecting failed examples and adding targeted training data.

**Three First Experiments:**
1. Run LLM extraction on 10 documents, manually inspect outputs for formatting consistency and context quality
2. Test alignment on known good vs. hallucinated passages to verify τ = 0.6 threshold performance
3. Fine-tune NLI scorer on 50 human-labeled examples and evaluate on a held-out set to verify training pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a single legal-domain dataset (ASYLex), limiting generalizability
- Reported "2,000x cost reduction" depends on specific LLM pricing models and may vary significantly
- Smith-Waterman alignment threshold (τ = 0.6) was empirically selected for legal documents and may need tuning for other domains
- NLI fine-tuning procedure uses unspecified hyperparameters and validation strategies

## Confidence

**High Confidence:**
- Core methodological innovation (structured generation + alignment + scoring pipeline)
- NLI fine-tuning approach using silver labels + human annotations

**Medium Confidence:**
- Quantitative claims (85% hallucination reduction, cost/latency comparisons) due to dataset-specific evaluation
- Generalizability of results across domains and document types
- Exact reproducibility without specified training hyperparameters

**Low Confidence:**
- Optimal alignment threshold for non-legal domains
- Long-term stability of LLM-generated silver labels

## Next Checks

1. **Domain Transfer Validation**: Apply SAFEPASSAGE to a non-legal dataset (e.g., medical records or financial documents) and report hallucination detection rates, comparing to the original legal domain results

2. **Cost Sensitivity Analysis**: Measure actual latency and compute costs across different LLM providers and batch sizes to verify the claimed 2,000x reduction against real-world pricing

3. **Alignment Robustness Test**: Systematically vary the Smith-Waterman threshold (τ = 0.5, 0.6, 0.7) on held-out data to determine sensitivity and optimal thresholds for different document types and languages