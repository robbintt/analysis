---
ver: rpa2
title: 'Detection of Disengagement from Voluntary Quizzes: An Explainable Machine
  Learning Approach in Higher Distance Education'
arxiv_id: '2507.02681'
source_url: https://arxiv.org/abs/2507.02681
tags:
- students
- disengagement
- learning
- engagement
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an explainable machine learning framework
  to detect student disengagement from voluntary quizzes in distance education. Using
  Moodle log data from 42 courses, eight machine learning models were trained and
  compared, with neural networks achieving the highest accuracy at 91% (AUC = 0.96).
---

# Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education

## Quick Facts
- **arXiv ID:** 2507.02681
- **Source URL:** https://arxiv.org/abs/2507.02681
- **Reference count:** 40
- **Primary result:** Neural networks detect disengagement from voluntary quizzes with 91% accuracy (AUC = 0.96) using interpretable SHAP analysis to map behavioral patterns to risk levels

## Executive Summary
This study presents an explainable machine learning framework for detecting student disengagement from voluntary quizzes in distance education using Moodle log data. The framework combines temporal and activity-based features from 42 courses (565 students) with neural network classification and SHAP interpretability to identify three behavioral patterns—erratic, delayed, and irregular—mapped to high, medium, and low disengagement risk levels. The approach enables targeted intervention strategies including structured learning plans, gamification, and personalized scheduling, providing instructors with data-driven insights for timely student support in online learning environments.

## Method Summary
The framework processes Moodle quiz and log tables by joining on attemptID and aggregating into daily records with 16 features including activity counts by time period, inactive days, and temporal statistics (mean, median, SD, skewness, kurtosis of time between interactions). A neural network with 2 hidden layers of 100 nodes each is trained using semester-based temporal splitting (Semesters 1-3 for training, Semester 4 for testing) with 4-fold cross-validation maximizing AUC. SHAP values are computed on test predictions to identify behavioral patterns, which are then mapped to risk levels using rule-based classification for targeted intervention assignment.

## Key Results
- Neural networks achieved 91% balanced accuracy and 0.96 AUC in detecting disengagement from voluntary quizzes
- SHAP analysis identified three behavioral patterns—erratic, delayed, and irregular—mapped to high, medium, and low disengagement risk levels
- The framework enables interpretable predictions that support targeted intervention strategies including structured learning plans, gamification, and personalized scheduling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Neural networks can detect disengagement from voluntary quizzes with high accuracy using temporal and activity-based features extracted from LMS logs.
- **Mechanism:** The model processes features derived from student interaction sequences—including activity counts by time period, inactive days, and temporal statistics (mean, median, SD, skewness, kurtosis of time between interactions)—to classify engagement status on a per-day basis. The neural network (2 hidden layers, 100 nodes each) captures non-linear relationships between these features and submission outcomes.
- **Core assumption:** Behavioral patterns observable in LMS log data reliably predict whether a student will disengage from voluntary quizzes.
- **Evidence anchors:**
  - [abstract] "neural networks achieving the highest accuracy at 91% (AUC = 0.96)"
  - [Section IV-B] "The NN and GBM models outperform all other models, achieving an accuracy of 91% and an AUC of 0.96"
  - [corpus] Neighbor paper on early warning signals (arXiv:2602.00021) supports the broader premise that disengagement signals appear before drop-out, though corpus evidence for this specific quiz-based detection approach is limited.
- **Break condition:** If students exhibit engagement patterns that differ substantially from the training population (e.g., different course structures, cultural contexts, or non-Moodle platforms), model performance may degrade.

### Mechanism 2
- **Claim:** SHAP values can be aggregated by feature category to identify three interpretable behavioral patterns: erratic, delayed, and irregular.
- **Mechanism:** SHAP decomposes each prediction into feature contributions. Features are grouped: activity count features (workday/weekend, morning/afternoon/evening) indicate erratic behavior when their SHAP sum is negative; `days_inactive` indicates delayed behavior when its SHAP value is negative; `stat_mean` and `stat_sd` together indicate irregular behavior when their SHAP sum is negative.
- **Core assumption:** The mapping from SHAP value patterns to behavioral categories (erratic/delayed/irregular) aligns with pedagogically meaningful constructs.
- **Evidence anchors:**
  - [abstract] "SHAP analysis identified three behavioral patterns—erratic, delayed, and irregular—which were mapped to high, medium, and low disengagement risk levels"
  - [Section IV-C] "By selecting the top 10 most influential features from this figure, we can define erratic behavior based on the number of views during weekdays and weekends... Delayed behavior is linked to days inactive, while the irregular behaviour is captured by stat mean and stat sd"
  - [corpus] No direct corpus validation of this specific SHAP-to-behavior mapping was found.
- **Break condition:** If feature importance rankings shift significantly across different student populations or course types, the rule-based behavioral classification may require recalibration.

### Mechanism 3
- **Claim:** Combining detected behavioral patterns into a three-tier risk categorization enables targeted intervention assignment.
- **Mechanism:** Risk levels are rule-based: High = erratic AND delayed; Medium = erratic XOR delayed; Low = irregular only; Engaged = none. Each tier maps to intervention strategies from literature (e.g., structured learning plans for high risk, gamification for medium risk, self-reflection dashboards for low risk).
- **Core assumption:** The proposed interventions, though literature-grounded, will effectively reduce disengagement when deployed—an untested hypothesis in this study.
- **Evidence anchors:**
  - [abstract] "Targeted intervention strategies were proposed for each risk category, including structured learning plans, motivational messages, gamification, and personalized scheduling"
  - [Section V-C] "these interventions (though grounded in literature) have not yet been tested in live learning settings, leaving their practical effectiveness unverified"
  - [corpus] Neighbor paper on gamification in education (arXiv:2504.02962) provides tangential support for gamification-based engagement strategies.
- **Break condition:** If students receive interventions but behavioral change does not follow (e.g., due to external constraints like work or family obligations), the risk-to-intervention mapping fails operationally.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** The paper relies on SHAP to transform neural network predictions into interpretable behavioral patterns; without understanding SHAP, the explainability layer is opaque.
  - **Quick check question:** Given a feature with a negative SHAP value for a prediction, does this push the model toward predicting "disengaged" or "engaged"?

- **Concept: Class imbalance and balanced accuracy**
  - **Why needed here:** The paper reports balanced accuracy (91%) rather than raw accuracy, which matters because engagement/disengagement classes may be imbalanced across time windows.
  - **Quick check question:** Why would balanced accuracy (average of TPR and TNR) be preferred over simple accuracy when classes are imbalanced?

- **Concept: Time-series feature engineering from event logs**
  - **Why needed here:** The model's input features are derived from timestamped LMS events; understanding how raw logs become features (e.g., `stat_mean`, `days_inactive`) is critical for replication.
  - **Quick check question:** If a student has high variance in time-between-activities (`stat_sd`), what behavioral pattern might this indicate?

## Architecture Onboarding

- **Component map:**
  ```
  Moodle LMS → Log tables (quiz, logs) → Preprocessing (daily aggregation, feature computation) → Feature vector → Neural Network classifier → SHAP explainer → Behavioral pattern rules → Risk level → Intervention strategy mapping
  ```

- **Critical path:**
  1. Extract `quiz` and `logs` tables from Moodle, joining on `attemptID`
  2. Aggregate to daily records with features: activity counts by time period, `days_inactive`, temporal statistics
  3. Train/test split by semester (train: semesters 1–3; test: semester 4)
  4. Train NN (2 hidden layers, 100 nodes each) with 4-fold CV to maximize AUC
  5. Apply SHAP to test predictions; compute SHAP sums per feature group
  6. Apply behavioral rules: if erratic_sum < 0 AND delayed_SHAP < 0 → High risk; else if erratic_sum < 0 OR delayed_SHAP < 0 → Medium risk; else if irregular_sum < 0 → Low risk
  7. Map risk level to intervention strategy

- **Design tradeoffs:**
  - NN vs. GBM: NN chosen despite GBM having marginally higher NPV because NN is computationally cheaper for grid-search and deployment
  - Semester-based split vs. random split: Semester split tests generalization across academic terms but may introduce temporal drift
  - Rule-based risk classification vs. direct ML prediction: Rules add interpretability but may not capture edge cases as well as a learned classifier

- **Failure signatures:**
  - Model predicts engagement but student disengages: Check if feature values fall outside training distribution (e.g., extreme `days_inactive` > 20)
  - SHAP explanations contradict intuition: Verify feature engineering pipeline (e.g., `stat_mean` computed correctly from timestamps)
  - Risk classification produces unexpected counts: Re-check rule logic (AND vs. OR conditions for high/medium risk)

- **First 3 experiments:**
  1. **Feature ablation study:** Remove each feature category (attempts-related, performance-related, activity-related) and measure impact on balanced accuracy and AUC to validate feature importance ranking.
  2. **Threshold sensitivity analysis:** Vary the SHAP value threshold for classifying negative/positive contribution (currently implicit at 0) and observe stability of risk-level assignments.
  3. **Cross-context validation:** Train on a subset of courses (e.g., Mathematics only) and test on others (e.g., Natural Sciences) to assess generalization across subject domains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the proposed intervention strategies effectively reduce disengagement and improve learning outcomes when deployed in live courses?
- **Basis in paper:** [explicit] Section V.C states that the proposed interventions are grounded in literature but "have not yet been tested in live learning settings, leaving their practical effectiveness unverified."
- **Why unresolved:** The study successfully developed the detection and classification framework but stopped short of empirically validating the suggested support strategies (e.g., structured plans, gamification) with actual students.
- **What evidence would resolve it:** A controlled pilot study measuring changes in quiz submission rates and final grades for students who received the targeted interventions compared to a control group.

### Open Question 2
- **Question:** How does the integration of forum-based or contextual data impact the transparency and stability of the SHAP-based risk categorization?
- **Basis in paper:** [explicit] Section V.C notes that future work involving richer data sources "may enhance classification performance but could impact the transparency and consistency of the behavioral risk categorization."
- **Why unresolved:** The current framework relies solely on quiz log features. It is unknown if adding unstructured or complex data would obscure the clear "erratic," "delayed," and "irregular" behavioral patterns identified.
- **What evidence would resolve it:** Retraining the model with additional data sources and analyzing the resulting SHAP value distributions to see if the existing behavioral rules remain interpretable.

### Open Question 3
- **Question:** Can the diagnostic framework be evolved into a fully predictive system that accurately forecasts disengagement earlier in the semester?
- **Basis in paper:** [inferred] The conclusion explicitly lists evolving the "diagnostic framework into a fully predictive... system" as a key step for future work.
- **Why unresolved:** The current model achieves high accuracy (91%) based on accumulated activity logs, functioning more as a diagnostic tool than an early-warning system for future behavior.
- **What evidence would resolve it:** Longitudinal validation showing that the model maintains high accuracy when predicting disengagement using only data from the early weeks of a course.

## Limitations
- Behavioral patterns identified via SHAP lack external validation and may not generalize across different educational contexts
- Intervention strategies are theoretically grounded but remain untested in practice
- Semester-based temporal split may introduce data drift due to changing course structures or student populations

## Confidence

- **High confidence:** Neural network model performance (91% balanced accuracy, 0.96 AUC) based on specified metrics and methodology
- **Medium confidence:** The three behavioral patterns identified through SHAP analysis, though their pedagogical validity and generalizability remain uncertain
- **Low confidence:** Effectiveness of proposed interventions for each risk category, as they are not empirically tested

## Next Checks

1. Conduct cross-validation with course-level grouping to ensure predictions are not based on data from the same course
2. Test the behavioral pattern classification rules on a held-out dataset to verify their stability and accuracy
3. Implement a small-scale pilot study to evaluate the effectiveness of proposed interventions for each risk category