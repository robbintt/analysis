---
ver: rpa2
title: A Statistical Method for Attack-Agnostic Adversarial Attack Detection with
  Compressive Sensing Comparison
arxiv_id: '2510.02707'
source_url: https://arxiv.org/abs/2510.02707
tags:
- adversarial
- attack
- class
- network
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an attack-agnostic adversarial detection method
  that leverages behavior differences between compressed and uncompressed neural network
  pairs. The core idea involves building class identities using statistical measures
  (KL divergence, Mann-Whitney U test) on feature maps from clean data, then detecting
  adversarial samples by measuring disagreement between the two networks' outputs.
---

# A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison

## Quick Facts
- **arXiv ID:** 2510.02707
- **Source URL:** https://arxiv.org/abs/2510.02707
- **Reference count:** 2
- **Primary result:** Near-perfect adversarial detection (100% on most attack-dataset combinations) while maintaining 0% false positive rate on clean samples

## Executive Summary
This work presents an attack-agnostic adversarial detection method that leverages behavior differences between compressed and uncompressed neural network pairs. The core idea involves building class identities using statistical measures (KL divergence, Mann-Whitney U test) on feature maps from clean data, then detecting adversarial samples by measuring disagreement between the two networks' outputs. The method requires a simple pre-deployment training process and uses a runtime metric based on L2 norm differences between class distribution distances. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet datasets against five attack types (FGSM, PGD, Square Attack, DeepFool, CW), the method achieves near-perfect detection rates across all attacks while maintaining low false positive rates.

## Method Summary
The method uses dual ResNet18 networks (raw and JPEG2000-compressed) to detect adversarial samples by comparing their behavior. During pre-deployment, Algorithm 1 builds class identities using KL divergence and Mann-Whitney U test statistics on feature maps from clean training data. At runtime, Algorithm 2 computes a detection metric (P_A) by measuring L2 norm differences between class distribution distances from both networks. The method includes noise augmentation to improve statistical stability and uses empirically determined thresholds for detection decisions.

## Key Results
- 100% detection accuracy for most attack-dataset combinations (CIFAR-10, CIFAR-100, TinyImageNet)
- 0% false positive rate on clean samples across all tested datasets
- Significantly outperforms existing attack-agnostic methods while requiring no attack-specific training
- Robust across diverse attack types including gradient-based (FGSM, PGD), score-based (Square Attack), decision-based (DeepFool), and optimization-based (CW) attacks

## Why This Works (Mechanism)

### Mechanism 1: Compression-Induced Behavioral Divergence
Adversarial perturbations are suppressed by lossy compression more aggressively than clean image features, creating measurable disagreement between raw and compressed network outputs. JPEG2000 compression treats adversarial noise as imperceptible information and discards it during quantization, causing the compressed network to produce different feature representations for adversarial inputs compared to the raw network. Clean images exhibit consistent behavior across both networks, enabling reliable detection.

### Mechanism 2: Statistical Distribution Identity Profiling
Class-specific statistical profiles built from KL divergence and Mann-Whitney U tests on feature map distributions enable detection of out-of-distribution (adversarial) samples without attack-specific training. The method partitions clean train/test data, computes average feature vectors from the penultimate layer, calculates bidirectional KL divergences, and stores p-values from Mann-Whitney U tests across 50 iterations to create a histogram-based fingerprint for each class.

### Mechanism 3: Augmented Sample Enrichment for Robust Matching
Adding random noise to input images creates enriched sample representations that improve detection reliability by reducing single-sample variance. The method generates multiple augmented images per input by adding uncorrelated random noise, forming a batch that is compared against class distributions via the same KL divergence pipeline. This enrichment reduces the impact of any single perturbation direction and provides a more stable estimate of the sample's relationship to class identities.

## Foundational Learning

- **KL Divergence (Kullback-Leibler Divergence)**: Used to measure distributional distance between feature vectors from train and test partitions. Bidirectional KL (averaged) provides symmetric comparison for building class identities. *Quick check:* Can you explain why KL divergence is non-commutative and why the paper uses the averaged bidirectional form?

- **Mann-Whitney U Test (Wilcoxon Rank-Sum Test)**: Non-parametric test to determine if two distributions come from the same population. Generates p-values that form the class identity histogram. *Quick check:* What assumptions does the Mann-Whitney U test make about the underlying distributions, and why might a non-parametric test be preferred over t-tests here?

- **Lossy Image Compression (JPEG2000)**: Provides the denoising mechanism that differentially affects adversarial vs. clean images. Understanding quantization and wavelet-based compression helps explain why adversarial noise is suppressed. *Quick check:* How does JPEG2000's wavelet-based approach differ from DCT-based JPEG, and what implications might this have for adversarial noise patterns?

## Architecture Onboarding

- **Component map:** Base CNN -> Compressed CNN -> Pre-deployment module -> Runtime detection module -> Augmentation engine -> Threshold comparator
- **Critical path:** 1) Pre-deployment: Train both networks → Run Algorithm 1 on clean data → Store class identities 2) Runtime: Receive input → Augment → Forward pass through both networks → Extract penultimate features → Compute distance vectors V_R and V_C → Calculate L2 norm → Compare to threshold
- **Design tradeoffs:** Compression quality vs. detection margin (lower quality increases suppression but degrades clean accuracy); iteration count I vs. profile precision (higher I improves reliability but increases pre-deployment computation); augmentation count N vs. latency (more samples improve stability but multiply inference cost); threshold T vs. false positive/negative balance (conservative thresholds reduce false positives but may miss subtle attacks)
- **Failure signatures:** High false positives: Check if clean training data contained perturbations; verify compression quality matches pre-deployment settings. Low detection rate on specific attacks: Inspect whether attack perturbation magnitude is below compression's suppression threshold. Inconsistent results across datasets: Threshold T requires recalibration per dataset; class identity quality depends on representative training data. Memory overflow during identity building: Reduce I parameter or batch sizes; disk-cache intermediate distributions.
- **First 3 experiments:** 1) Baseline validation: Replicate CIFAR-10 results with FGSM attack (ε=0.02). Verify 100% detection rate and 0% false positive rate. 2) Threshold sensitivity analysis: Sweep threshold T from 1.0 to 10.0 on validation set. Plot ROC curve and identify optimal operating point. 3) Compression quality ablation: Test JPEG2000 quality factors (50%, 65%, 80%, 95%) on PGD attack detection. Measure impact on clean accuracy degradation, adversarial suppression effectiveness, and detection metric separation.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the detection method against adaptive adversaries who possess full knowledge of the compression mechanism and statistical comparison logic? The paper evaluates standard attacks but does not address adaptive attacks designed specifically to bypass a dual-network defense that relies on JPEG compression.

### Open Question 2
To what extent does the inclusion of poisoned or perturbed samples in the training data degrade the accuracy of the pre-deployment class identities? Section 3.1 states that train and test sets must be strictly free of perturbations, but the impact of contaminated baselines is unverified.

### Open Question 3
Does the method maintain near-perfect detection rates when applied to high-resolution datasets or alternative compression architectures beyond JPEG2000? The authors use a truncated version of ImageNet and rely specifically on JPEG2000, leaving scalability to full high-resolution datasets and generalizability to other denoising methods unverified.

## Limitations
- Implementation details for key parameters (partition sizes, noise augmentation specifics, feature normalization) are not specified, creating significant barriers to faithful reproduction
- Reliance on JPEG2000 compression assumes adversarial perturbations exhibit noise-like statistical properties that compression will suppress, but this may not hold for all attack generation methods
- Performance on high-resolution datasets beyond TinyImageNet remains unverified, and computational overhead of dual-network inference plus augmentation may limit practical deployment

## Confidence

- **High confidence:** The core detection mechanism (L2 norm comparison between compressed/uncompressed networks) and baseline results on CIFAR-10 are well-specified and reproducible
- **Medium confidence:** The statistical foundation (KL divergence + Mann-Whitney U test for class identity building) is sound, but implementation details are incomplete
- **Low confidence:** The noise augmentation strategy's effectiveness is weakly supported in the literature, and specific parameters are not specified

## Next Checks

1. **Implementation fidelity verification:** Reproduce CIFAR-10 FGSM results with complete parameter specification (partition sizes, noise augmentation details, normalization methods). Document any deviations from claimed performance and identify whether gaps stem from missing details or fundamental limitations.

2. **Attack-specific failure analysis:** Systematically evaluate detection performance across all five attack types with varying perturbation magnitudes. Identify the detection threshold where each attack type becomes undetectable and analyze whether this correlates with compression's noise suppression limits.

3. **Compression quality tradeoff study:** Conduct a comprehensive ablation study varying JPEG2000 quality from 50% to 95% on PGD attacks. Measure the impact on clean accuracy degradation, adversarial attack success rate on compressed network, detection metric separation, and computational overhead. Determine the optimal quality parameter for practical deployment.