---
ver: rpa2
title: 'Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional
  Models?'
arxiv_id: '2506.12744'
source_url: https://arxiv.org/abs/2506.12744
tags:
- hate
- speech
- detection
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models (LLMs) against traditional
  transformer models for hate speech detection, particularly in multilingual and code-mixed
  settings. The authors introduce IndoHateMix, a novel dataset capturing Hindi-English
  code-mixing and transliteration in Indian social media, to address the gap in existing
  benchmarks.
---

# Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?

## Quick Facts
- arXiv ID: 2506.12744
- Source URL: https://arxiv.org/abs/2506.12744
- Authors: Daman Deep Singh; Ramanuj Bhattacharjee; Abhijnan Chakraborty
- Reference count: 40
- Primary result: LLMs, especially LLaMA-3.1-8B, consistently outperform fine-tuned BERT-based models for hate speech detection, even with minimal training data

## Executive Summary
This study benchmarks large language models against traditional transformer models for hate speech detection, introducing IndoHateMix, a novel dataset capturing Hindi-English code-mixing and transliteration in Indian social media. Comprehensive experiments across three datasets demonstrate that LLMs, particularly LLaMA-3.1-8B, consistently outperform fine-tuned BERT-based models, even when trained on just 10% of the data. The findings suggest that future hate speech detection efforts should prioritize developing diverse datasets and leveraging general-purpose LLMs over task-specific models.

## Method Summary
The study introduces IndoHateMix, a novel dataset for Hindi-English code-mixed hate speech detection, to address the gap in existing benchmarks. The authors conduct comprehensive experiments across three datasets—HateXplain, ImplicitHate, and IndoHateMix—comparing LLMs against traditional transformer models. They evaluate zero-shot and few-shot learning scenarios, demonstrating that LLMs consistently outperform fine-tuned BERT-based models, even with minimal training data.

## Key Results
- LLaMA-3.1-8B outperforms BERT models trained on full datasets when fine-tuned on just 10% of the data
- LLMs show superior handling of indirect, sarcastic, and code-mixed expressions compared to traditional models
- IndoHateMix dataset captures unique challenges of Hindi-English code-mixing in Indian social media contexts

## Why This Works (Mechanism)
LLMs demonstrate superior performance due to their larger context understanding, ability to capture nuanced linguistic patterns, and better generalization from limited examples. The models' pretraining on diverse web data enables them to recognize indirect hate speech expressions, sarcasm, and code-mixed language patterns that traditional models often miss due to their reliance on surface-level features and task-specific fine-tuning.

## Foundational Learning
- **Transformer architecture**: The core mechanism enabling sequence modeling and attention-based context understanding, necessary for processing social media text with complex linguistic patterns
- **Code-mixing phenomena**: Understanding how languages blend in multilingual contexts, crucial for developing effective detection models for diverse linguistic communities
- **Few-shot learning**: The ability to learn from minimal examples, essential for real-world deployment where labeled data is scarce
- **Cross-lingual transfer**: How models leverage knowledge across languages, important for generalizing hate speech detection beyond English
- **Contextual word representations**: Dynamic meaning based on surrounding context, critical for identifying hate speech that depends on nuanced interpretation
- **Implicit hate detection**: Recognizing hate speech that doesn't use explicit slurs or offensive language, increasingly important as hate speech evolves

## Architecture Onboarding

**Component Map**
LLM backbone (e.g., LLaMA-3.1-8B) -> Prompt engineering module -> Classification head -> Output layer

**Critical Path**
Input text -> Tokenization -> Context embedding generation -> Prompt processing -> Classification decision

**Design Tradeoffs**
- Computational efficiency vs. detection accuracy: LLMs require more resources but achieve better performance
- Generalization vs. specificity: General-purpose LLMs adapt to various contexts versus task-specific models optimized for particular datasets
- Data efficiency vs. model size: Larger models need less fine-tuning data but require more computational resources

**Failure Signatures**
- Misclassification of novel hate speech patterns not seen during pretraining
- Over-sensitivity to certain linguistic features leading to false positives
- Difficulty with extremely short or ambiguous text snippets

**First Experiments**
1. Compare zero-shot performance of different LLM sizes on IndoHateMix
2. Evaluate fine-tuning efficiency across 1%, 10%, and 100% data splits
3. Test cross-dataset generalization by training on one dataset and evaluating on others

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset scope limited to three datasets, raising questions about generalizability across diverse hate speech contexts and non-Indo-European languages
- No exploration of intermediate fine-tuning strategies that might bridge performance gaps between traditional models and LLMs
- Error analysis lacks quantitative metrics and statistical significance testing to substantiate qualitative observations

## Confidence

**Major Uncertainties and Limitations**
- LLM superiority claims (High confidence): Consistent outperformance of LLaMA-3.1-8B across all datasets well-supported
- Generalization to low-resource settings (Medium confidence): 10% fine-tuning results compelling but need broader validation
- Traditional model limitations (Medium confidence): Claims about surface-level sensitivity supported qualitatively but lack rigorous analysis

## Next Checks
1. **Cross-linguistic validation**: Evaluate models on hate speech datasets from non-Indo-European language families to test universality of LLM advantages
2. **Cost-benefit analysis**: Measure computational requirements, inference latency, and deployment costs across different hardware configurations
3. **Error pattern quantification**: Conduct systematic error analysis with inter-annotator agreement and statistical significance testing to quantify LLM advantages