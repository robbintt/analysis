---
ver: rpa2
title: 'Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical
  Reasoning'
arxiv_id: '2506.10903'
source_url: https://arxiv.org/abs/2506.10903
tags:
- uni00000013
- formal
- gpt-4
- uni00000011
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating autoformalization
  in formal mathematical reasoning, a task that is traditionally time-consuming and
  requires domain expertise, especially for complex statements. The authors introduce
  a fine-grained, interpretable taxonomy grounded in epistemic and formal criteria,
  including logical preservation, mathematical consistency, formal validity, and formal
  quality.
---

# Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning

## Quick Facts
- **arXiv ID:** 2506.10903
- **Source URL:** https://arxiv.org/abs/2506.10903
- **Reference count:** 40
- **Primary result:** LLM ensemble judges with fine-grained taxonomy correlate more strongly with human assessments (up to 0.662 for Isabelle/HOL) than coarse-grained or direct judgment methods.

## Executive Summary
This paper introduces a novel evaluation framework for autoformalization in formal mathematical reasoning, addressing the challenge of assessing translations from natural language to formal languages like Isabelle/HOL and Lean4. The authors develop a fine-grained, interpretable taxonomy based on epistemic and formal criteria, operationalized through 12 atomic properties. These properties are evaluated by an ensemble of LLM judges, whose judgments are aggregated using learned weights and combined with theorem prover verification. Human evaluation shows that this approach achieves higher correlation with human assessments than traditional metrics or direct LLM judgment, demonstrating that smaller LLMs guided by well-defined criteria can reliably assess formalization quality.

## Method Summary
The method employs an epistemically and formally grounded (EFG) ensemble of LLM judges to evaluate autoformalization quality. It decomposes evaluation into 12 Operable Atomic Properties (OAPs) across four core aspects: Logical Preservation (LP), Mathematical Consistency (MC), Formal Validity (FV), and Formal Quality (FQ). LLMs judge LP, MC, and FQ properties; theorem provers verify FV. OAP judgments are aggregated per core aspect using weighted averaging, then combined into an overall score via linear model: SOA = 0.25×LP + 0.19×MC + 0.32×FV + 0.24×FQ. The approach was validated on Isabelle/HOL and Lean4 datasets using human-annotated samples.

## Key Results
- EFG ensemble with weighted OAP aggregation achieves correlation of 0.662 (Isabelle/HOL) and 0.479 (Lean4) with human assessments
- Outperforms traditional metrics (BLEU, ChrF, RUBY) and direct LLM judgment methods
- GPT-4.1-Mini with OAP weighted averaging (OAP-WA) outperforms GPT-4.1 direct judgment on overall assessment
- Hybrid approach combining theorem prover verification with LLM semantic judgment provides more reliable assessment than either alone

## Why This Works (Mechanism)

### Mechanism 1
Decomposing evaluation into fine-grained, interpretable atomic properties improves LLM judge alignment with human assessment compared to coarse-grained direct judgment. The taxonomy operationalizes abstract quality concepts into 12 concrete properties, reducing ambiguity and enabling smaller LLMs to make more consistent binary judgments that can be aggregated into reliable scores.

### Mechanism 2
Weighted aggregation of atomic property judgments outperforms both direct LLM judgment and simple AND-based synthesis. A linear model with learned weights combines judgments into an overall score, allowing partial credit rather than requiring all properties to pass, enabling more nuanced discrimination between formalizations.

### Mechanism 3
Hybrid evaluation—using theorem provers for formal validity and LLMs for semantic aspects—provides more reliable assessment than either alone. This division of labor leverages provers' deterministic guarantees for validity while using LLMs' language understanding for nuanced semantic judgment.

## Foundational Learning

**Concept: Autoformalization**
- **Why needed here:** The entire evaluation framework is designed for this task—translating natural language mathematical statements into formal languages like Isabelle/HOL or Lean4.
- **Quick check question:** Given "The sum of two even numbers is even," can you sketch two different valid Lean4 formalizations?

**Concept: Theorem Provers (Isabelle/HOL, Lean4)**
- **Why needed here:** The framework relies on theorem provers for formal validity checks.
- **Quick check question:** What is the key difference between a prover's validity check and an LLM's semantic judgment?

**Concept: LLM-as-a-Judge Paradigm**
- **Why needed here:** The core contribution applies this paradigm to formal mathematics.
- **Quick check question:** Why might an LLM judge show higher correlation with human judgments on one formal language (Isabelle) than another (Lean4)?

## Architecture Onboarding

**Component map:** Natural language statement (s) + candidate formalization (φ) → Theorem prover check → binary SFV + LLM Judges (12 OAP prompts) → binary judgments + explanations → OAP Aggregation (weighted average per core aspect) → Linear Ensemble (wLP=0.25, wMC=0.19, wFV=0.32, wFQ=0.24) → overall score SOA ∈ [0,1]

**Critical path:** 1. Define and validate the taxonomy of atomic properties (currently 12 OAPs) 2. Calibrate aggregation weights via human evaluation data 3. Select and configure LLM judges (temperature, prompting strategy) 4. Integrate theorem prover checks as a separate, authoritative validity signal

**Design tradeoffs:**
- Smaller vs. larger LLM judges: GPT-4.1-Mini with OAPs outperforms GPT-4.1 direct, but larger models may have higher precision on alignment aspects
- OAP-based vs. direct judgment: OAPs require more API calls (12 per evaluation) but provide interpretable feedback; direct judgment is cheaper but less explainable
- AND vs. weighted synthesis: AND is strict; weighted average allows partial credit and correlates better with humans

**Failure signatures:**
- Low correlation with human judgment: May indicate taxonomy gaps, weight overfitting, or LLM judge bias
- High variance across temperatures: OA scores should be stable; if LP/MC fluctuate significantly, the judge is unreliable
- Systematic misalignment between LLM and theorem prover: LLM judges should not be used as proxies for provers; kappa < 0.30 indicates fundamental limitation

**First 3 experiments:**
1. Replicate correlation analysis on a held-out subset of the human-annotated data to validate generalizability
2. Test the framework on a new formal language (e.g., Coq) to assess taxonomy and weight transferability
3. Ablate individual OAPs to determine which properties contribute most to human alignment

## Open Questions the Paper Calls Out

**Open Question 1:** Can fine-tuning smaller, open-source LLMs serve as specialized judges that outperform general-purpose prompted models in evaluating autoformalization?
- **Basis in paper:** [explicit] The conclusion states: "Further directions could focus on fine-tuning smaller open-sourced LLMs specialized as judges for autoformalization evaluation."

**Open Question 2:** Can the natural language explanations generated by LLM judges be effectively utilized to iteratively improve the quality of autoformalization outputs?
- **Basis in paper:** [explicit] The conclusion suggests: "Further directions could focus on... using LLM explanations of judgments to improve autoformalization performance."

**Open Question 3:** Can LLM-based evaluation frameworks be adapted to reliably approximate theorem prover verification for formal validity, particularly for complex languages like Lean4?
- **Basis in paper:** [inferred] The authors find in Appendix D that "LLM-judges are not reliable substitutes for theorem provers" and specifically note poor agreement on Lean4 validity.

## Limitations
- Taxonomy completeness and generalizability to other formal languages remains uncertain
- Weight calibration may be dataset-specific and require re-calibration for new contexts
- Human evaluation sample size is limited (100 samples total, particularly small for Lean4)

## Confidence
**High confidence:** Fine-grained OAP decomposition improves LLM judge alignment with human assessment compared to direct judgment (correlation: 0.662 vs 0.599 Isabelle, 0.479 vs 0.424 Lean4)

**Medium confidence:** Weighted aggregation of OAPs outperforms simple AND-based synthesis, though additive linear model assumption needs more testing

**Low confidence:** Independence assumption underlying hybrid evaluation approach; Cohen's kappa results (<0.30) suggest fundamental limitations in using LLMs as theorem prover substitutes

## Next Checks
1. Apply the EFG ensemble framework to a new formal language (e.g., Coq) and evaluate whether existing OAP taxonomy and weights maintain comparable human correlation without modification
2. Systematically remove each OAP from the evaluation pipeline and measure impact on human correlation to identify critical vs redundant properties
3. Perform bootstrap resampling on human evaluation data to establish confidence intervals for reported correlations and test statistical significance of method differences at p<0.05