---
ver: rpa2
title: PAC-Private Responses with Adversarial Composition
arxiv_id: '2601.14033'
source_url: https://arxiv.org/abs/2601.14033
tags:
- privacy
- noise
- adversary
- responses
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a PAC-private response framework that enables
  linear composition under adversarial querying, addressing the challenge of protecting
  machine learning models from membership inference attacks when deployed behind APIs.
  The core idea is to adaptively calibrate noise based on the posterior distribution
  of the secret data given the interaction history, achieving instance-dependent privacy
  guarantees that exploit the stability of model responses.
---

# PAC-Private Responses with Adversarial Composition

## Quick Facts
- arXiv ID: 2601.14033
- Source URL: https://arxiv.org/abs/2601.14033
- Reference count: 40
- Key outcome: Introduces PAC-private response framework enabling linear composition under adversarial querying with instance-dependent noise calibration, achieving high utility (87.79% accuracy on CIFAR-10 at b=2⁻³²) while provably limiting MIA success rates.

## Executive Summary
This paper addresses membership inference attacks on ML APIs by introducing a PAC-private response framework that calibrates noise based on posterior distributions of secret data. Unlike prior work that yields quadratic composition bounds, this method achieves linear accumulation of mutual information guarantees under adaptive and adversarial querying. The framework is instantiated for ML response privacy, showing that even with extremely small per-query privacy budgets, it maintains high utility across tabular, vision, and NLP tasks while provably limiting MIA success rates. Additionally, a private model distillation protocol is proposed to enable unlimited inference while inheriting the same privacy guarantees.

## Method Summary
The method constructs a secret space S with m subsets where each data point appears in exactly half of them (50% marginal probability). Offline, m models are trained in parallel on these subsets. Online, for each query, one-hot predictions from all m models are used to compute a covariance matrix under the current posterior distribution. Gaussian noise is calibrated via SVD of this covariance and added to the predictions. The posterior is then updated via Bayes' rule, and mutual information bounds accumulate linearly. For distillation, a filtering mechanism uses hypothesis testing on noisy responses to retain only confidently labeled examples for training a student model.

## Key Results
- Achieves 87.79% accuracy on CIFAR-10 with b=2⁻³², supporting one million queries with MIA success bounded to 51.08%
- Maintains 95% accuracy on distilled student models using filtered private labels
- Demonstrates linear composition of mutual information under adversarial querying, overcoming prior quadratic bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive noise calibration based on posterior distributions enables linear composition of mutual information guarantees under adversarial querying, overcoming prior work's quadratic bounds.
- **Mechanism:** The curator maintains belief state P_t (posterior distribution of secret S given transcript T_t). At each step, noise covariance Σ_t is calibrated to the current belief P_{t-1} rather than fixed prior P_S, then beliefs update via Bayes' rule. This bounds conditional MI I(S; R_t | T_{t-1}) ≤ b_t exactly, enabling linear accumulation: I(S; R_1,...,R_T) ≤ Σb_t.
- **Core assumption:** Adversary's query M_t depends on S only through transcript (M_t ⊥ S | T_{t-1}); no side-channel access.
- **Evidence anchors:**
  - [Abstract]: "prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying"
  - [Section 3.2, Theorem 3.3]: Formal proof that I(S; R_1,...,R_T) ≤ B_T = Σb_t
  - [Appendix A]: Analysis showing static composition either yields quadratic bounds or DP-like input-independent noise
- **Break condition:** If adversary gains side-channel access to S bypassing the transcript, or if S is re-sampled per query rather than persistent, posterior tracking fails and composition guarantee collapses.

### Mechanism 2
- **Claim:** Privatizing hard one-hot predictions rather than soft confidence scores achieves superior utility at tight privacy budgets by exploiting output stability.
- **Mechanism:** When models trained on different S∈S agree on prediction class, one-hot outputs have near-zero variance, requiring minimal noise. After adding Gaussian noise to one-hot vector, argmax yields final prediction.
- **Core assumption:** Classification task exhibits sufficient stability—models on different training subsets converge to similar predictions on most inputs.
- **Evidence anchors:**
  - [Section 4.2.4]: "discrete one-hot encodings are highly stable... This minimizes variance across S and requires significantly less noise"
  - [Table 6]: CIFAR-10 at b=2^-32: one-hot achieves 87.79% vs confidence scores at 33.41%
  - [Corpus]: Limited direct evidence; neighboring papers (AlignDP, Private Linear Regression) address different mechanisms
- **Break condition:** High prediction variance across S (e.g., CIFAR-100 shows larger utility drop) or tasks requiring probabilistic outputs undermine stability advantages.

### Mechanism 3
- **Claim:** Hypothesis testing on noisy responses provably bounds mislabeling probability, enabling high-quality student model distillation.
- **Mechanism:** For privately labeled sample (x, ỹ), test whether response r is significantly more likely under ỹ than any alternative j≠ỹ using known noise distribution Σ. Test statistic T_j(r) ≥ Φ^{-1}(1-α) ensures Pr(retain | mislabeled) ≤ α.
- **Core assumption:** Non-private prediction is one-hot; noise distribution Σ is known.
- **Evidence anchors:**
  - [Proposition 5.1]: Formal guarantee with explicit test statistic formula
  - [Section 6.4]: 95% accuracy on retained ImageNet examples vs 58.66% on full set
  - [Corpus]: No direct corpus evidence on this protocol
- **Break condition:** Excessive noise (large Σ) makes filtering too aggressive; loose α contaminates training with mislabeled samples.

## Foundational Learning

- **Mutual Information as Privacy Metric**
  - Why needed here: PAC privacy bounds adversary success via I(S; M(S)). Understanding MI-to-attack-success translation is essential for budget selection.
  - Quick check question: Given I(S; M(S)) ≤ B = 2^-32 with 50% prior, what's the MIA success bound?

- **Bayesian Posterior Updates**
  - Why needed here: Algorithm 1 requires maintaining P_t and updating via P_t(s) ∝ exp(-½(R_t - M_t(s))^T Σ_t^{-1}(R_t - M_t(s))) · P_{t-1}(s).
  - Quick check question: After observing response R_t with covariance Σ_t, how do you update posterior P_t given prior P_{t-1}?

- **Composition Under Adaptivity**
  - Why needed here: Standard non-adaptive composition (linear) fails when queries depend on prior outputs; understanding why posterior-aware calibration restores linearity is critical.
  - Quick check question: Why does calibrating noise to fixed prior P_S at every step fail to bound I(S; R_t | T_{t-1})?

## Architecture Onboarding

- **Component map:**
  ```
  Offline: U → construct S={S_1,...,S_m} → train m models
  
  Online loop:
    Query q_t → inference on m models → compute covariance under P_{t-1} 
              → calibrate Σ_t → release R_t → update P_t → record T_t
  
  Distillation:
    D_pub → query mechanism → confidence filter → train student
  ```

- **Critical path:**
  1. Secret space construction (m=128, each u∈U in exactly m/2 subsets)
  2. Offline training of m models (parallelizable, O(m·training_cost))
  3. Per-query: m inferences + O(md²) covariance + O(d²) noise sampling + O(md²) posterior update
  4. Budget tracking: B_t = Σb_i vs target threshold

- **Design tradeoffs:**
  - **m (secret space size):** Larger m reduces baseline guessing (1/m) but increases offline/online cost; paper uses 128
  - **Per-step budget b:** Tighter (2^-32) supports more queries but requires stable models; looser exhausts faster
  - **Output type:** One-hot maximizes stability; soft scores preserve information but require more noise
  - **Distillation data:** In-distribution improves quality; out-of-distribution may be easier to obtain

- **Failure signatures:**
  - Quadratic composition growth if posterior updates skipped
  - Vacuous bounds if subsampling error >10% (unstable predictions)
  - MIA success approaching bound indicates budget exhaustion
  - Filtering retaining <10% samples signals excessive noise or distribution shift

- **First 3 experiments:**
  1. **Synthetic composition validation:** m=16 subsets, simple classifier, 10K adaptive queries at b=2^-16. Verify empirical MIA stays below theoretical bound—tests composition correctness in isolation.
  2. **Output stability ablation:** On CIFAR-10 at b=2^-32, compare one-hot vs top-3 softmax vs full softmax. Measure output variance across m models and utility—quantifies Section 4.2.4 claims.
  3. **Distillation noise sweep:** CINIC-ImageNet public data, CIFAR-10 teacher, vary b from 2^-16 to 2^-32. Measure student accuracy and retained fraction; verify Proposition 5.1 label accuracy bound holds on retained samples.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be extended to efficiently handle continuous or extremely large secret spaces where exact posterior tracking is infeasible?
- **Basis:** [Explicit] The authors state in Section 3.2.2 and Section 8 that maintaining the full posterior distribution is currently intractable for continuous spaces, leaving efficient estimation for such spaces to future work.
- **Why unresolved:** Algorithm 1 explicitly updates beliefs over a discrete, finite support $m$, which becomes computationally impossible if the secret space is infinite.
- **What evidence would resolve it:** A mechanism using variational inference or Monte Carlo methods that bounds mutual information without explicit enumeration of the posterior.

### Open Question 2
- **Question:** Can the adversarial composition theorem be generalized to other $f$-divergences beyond KL-divergence?
- **Basis:** [Explicit] Section 8 lists "privacy accounting under $f$-divergence" as a potential theoretical extension.
- **Why unresolved:** The current composition bounds rely on the chain rule of Mutual Information (KL-divergence) as established in Theorem 3.3.
- **What evidence would resolve it:** A generalized proof demonstrating linear composition properties for alternate divergences under the adaptive noise calibration framework.

### Open Question 3
- **Question:** How can this PAC-private response mechanism be adapted for the sequential, high-dimensional output generation typical of Large Language Models (LLMs)?
- **Basis:** [Explicit] The conclusion identifies "applying response privacy to LLM inference" as a promising future avenue.
- **Why unresolved:** The current instantiation focuses on classification tasks with low-dimensional outputs (e.g., one-hot vectors), whereas LLMs involve vast vocabularies and sequential dependencies.
- **What evidence would resolve it:** An empirical evaluation on a generative task showing the framework maintains utility while bounding leakage over long interaction sequences.

## Limitations
- The framework assumes adversary's queries depend only on the transcript, with no side-channel access to the secret data
- Maintaining exact posterior distributions becomes computationally intractable for continuous or extremely large secret spaces
- Performance depends critically on prediction stability across different training subsets, which may not hold for all tasks

## Confidence

- **High confidence:** The core composition theorem (Theorem 3.3) and its proof; the utility-privacy tradeoff measurements on CIFAR-10, tabular datasets, and text classification tasks; the distillation protocol's theoretical guarantee (Proposition 5.1)
- **Medium confidence:** Generalization of results to more complex model architectures (e.g., transformers on vision tasks); stability advantages of one-hot vs soft outputs across diverse tasks; practical effectiveness of distillation filtering in noisy regimes
- **Low confidence:** Real-world applicability when side-channel information exists; scalability to massive secret spaces (m >> 128); performance under adaptive adversaries that violate the transcript-only assumption

## Next Checks

1. **Side-channel robustness test:** Implement an adversary that has access to both the mechanism's transcript and an independent estimate of S (e.g., from model weights). Measure whether MIA success exceeds the theoretical bound, identifying the breakdown point.

2. **Composition stress test:** Run synthetic experiments with varying m (16, 64, 256) and b values, tracking empirical MIA success vs theoretical bound across 100K+ adaptive queries. Verify linear accumulation holds under extreme query volumes.

3. **Distribution shift evaluation:** Apply the distillation protocol to out-of-distribution public data (e.g., CIFAR-10 teacher with SVHN public data). Measure student accuracy and filtering retention rate to assess robustness to domain mismatch.