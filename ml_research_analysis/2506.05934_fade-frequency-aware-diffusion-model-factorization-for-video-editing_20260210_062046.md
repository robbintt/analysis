---
ver: rpa2
title: 'FADE: Frequency-Aware Diffusion Model Factorization for Video Editing'
arxiv_id: '2506.05934'
source_url: https://arxiv.org/abs/2506.05934
tags:
- video
- editing
- blocks
- diffusion
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FADE addresses the challenge of editing video content, particularly\
  \ for motion adjustments, by leveraging frequency-aware factorization of pre-trained\
  \ text-to-video (T2V) diffusion models. The method analyzes attention patterns within\
  \ the T2V model to identify how video priors are distributed across components,\
  \ enabling a factorization strategy that optimizes each block\u2019s specialized\
  \ role."
---

# FADE: Frequency-Aware Diffusion Model Factorization for Video Editing

## Quick Facts
- arXiv ID: 2506.05934
- Source URL: https://arxiv.org/abs/2506.05934
- Reference count: 40
- Key outcome: Training-free video editing framework using frequency-aware factorization of pre-trained T2V models, achieving CLIP 0.3561 and OSV 43.57 on DAVIS

## Executive Summary
FADE introduces a training-free approach for video editing that leverages frequency-aware factorization of pre-trained text-to-video diffusion models. The method identifies specialized roles of different model blocks through attention pattern analysis, designating sketching blocks for structural guidance and sharpening blocks for detail enhancement. By applying spectrum-guided modulation with 3D DFT-based low-pass filtering, FADE prevents information leakage while enabling both appearance and motion edits with preserved temporal coherence.

## Method Summary
FADE analyzes attention patterns within pre-trained T2V diffusion models to identify how video priors are distributed across components, enabling a factorization strategy that optimizes each block's specialized role. The method performs DDIM inversion to obtain the initial latent trajectory, then applies spectrum-guided modulation using frequency domain cues from sketching blocks (first 4 transformer blocks). During sampling, a low-pass filter isolates structural information while preventing high-frequency leakage, with guidance applied during the first 60% of steps and local editing with masks during the first 80%. The approach operates without training and completes edits in approximately 3 minutes per video.

## Key Results
- CLIP score of 0.3561 and OSV of 43.57 on DAVIS benchmark
- High input fidelity: Mask-PSNR of 21.54 and LPIPS of 0.3297
- Strong human preference scores for edited videos
- Consistent temporal coherence and realistic editing results

## Why This Works (Mechanism)
FADE works by exploiting the inherent frequency decomposition present in diffusion models. By analyzing attention patterns, it identifies that early transformer blocks capture low-frequency structural information (sketching) while later blocks handle high-frequency details (sharpening). The spectrum-guided modulation applies 3D DFT to attention outputs, using low-pass filtering to preserve structural priors while preventing source appearance leakage during editing. This frequency-aware factorization enables precise control over both spatial and temporal aspects of video editing without requiring additional training.

## Foundational Learning
1. **DDIM Inversion** - Generates initial latent trajectory from real video
   - Why needed: Provides starting point for editing without retraining
   - Quick check: Verify z*_T matches original video reconstruction quality

2. **Attention Pattern Analysis** - Identifies block specialization through frequency decomposition
   - Why needed: Enables targeted modulation of sketching vs sharpening blocks
   - Quick check: Confirm first 4 blocks show low-frequency dominance in attention maps

3. **3D DFT Spectrum Processing** - Transforms attention outputs to frequency domain
   - Why needed: Enables frequency-aware separation of structure vs details
   - Quick check: Verify low-pass filter preserves ~2/3 of frequency components

4. **Temporal Coherence Maintenance** - Ensures smooth frame transitions
   - Why needed: Prevents flickering and temporal artifacts
   - Quick check: Measure OSV score across different video types

5. **Guidance Gradient Application** - Modulates latent trajectory based on target prompt
   - Why needed: Drives editing toward desired appearance/motion changes
   - Quick check: Validate CLIP score improvement after guidance application

6. **Mask-based Local Editing** - Enables region-specific modifications
   - Why needed: Provides precise control over editing scope
   - Quick check: Confirm Mask-PSNR and LPIPS metrics meet expectations

## Architecture Onboarding

Component Map:
Input Video → DDIM Inversion → Initial Trajectory {z*_t} → Spectrum Analysis → Factorization (Sketching Blocks 1-4) → Frequency Modulation → Guidance Application → Edited Trajectory → VAE Decoder → Output Video

Critical Path:
The core editing pipeline runs through: (1) DDIM inversion to obtain z*_T and trajectory, (2) attention analysis to identify sketching blocks, (3) spectrum-guided modulation during [0, 0.6T] with low-pass filtering, and (4) mask-based local editing during [0, 0.8T]. The method uses CogVideoX-5b with 48 transformer blocks, but only the first 4 are modulated for guidance.

Design Tradeoffs:
- Training-free vs. potential quality gains from fine-tuning
- Runtime efficiency (~3 min/video) vs. more extensive editing capabilities
- Frequency-based factorization vs. alternative architectural modifications
- Low-pass filtering cutoff selection balancing preservation vs. editing flexibility

Failure Signatures:
- High-frequency leakage → excessive source appearance preservation
- Temporal inconsistency → incorrect block selection or guidance interval
- Slow inference → modulating too many blocks instead of just sketching ones
- Incorrect object orientation → lack of spatial constraints in guidance

First Experiments:
1. Test DDIM inversion with different step counts (25, 50, 75) to verify reconstruction quality vs. computational cost
2. Validate low-pass filter cutoff effects by testing 50%, 66%, and 75% frequency preservation on editing quality
3. Compare CLIP scores and OSV metrics across different guidance weights (λ = 10, 12, 15) to optimize editing performance

## Open Questions the Paper Calls Out
1. **Heavy Occlusions and Complex Temporal Reasoning**: The method struggles with heavy occlusions requiring advanced temporal reasoning, as frequency-aware modulation may fail to infer hidden object states or continuity across frames when visual evidence is missing.

2. **Architecture Consistency Across Models**: It remains unclear whether the sketching vs sharpening block factorization is consistent across different video diffusion architectures (UNet vs DiT) and varying model depths, as the current analysis is limited to CogVideoX-5b.

3. **Spatial Constraint Enforcement**: The spectrum-guided modulation lacks specific constraints to prevent artifacts such as random object orientation, suggesting the need for spatial alignment losses or pose constraints for consistent object placement.

## Limitations
- Struggles with heavy occlusions requiring advanced temporal reasoning
- Unknown optimal low-pass filter cutoff frequency specification
- Mask generation process lacks detailed integration specifications

## Confidence
- **High confidence** in core methodology of frequency-aware factorization and spectrum-guided modulation
- **Medium confidence** in practical implementation feasibility given runtime estimates
- **Medium confidence** in "without training" claim due to underspecified implementation details

## Next Checks
1. Implement and test multiple low-pass filter cutoff frequencies (50%, 66%, 75%) to verify optimal balance between preservation and editing quality
2. Reproduce the mask generation pipeline using referenced detection method [38] and evaluate its impact on local editing fidelity
3. Validate temporal consistency claim by measuring OSV scores across different video content types and editing durations