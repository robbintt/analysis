---
ver: rpa2
title: 'Graph Repairs with Large Language Models: An Empirical Study'
arxiv_id: '2507.03410'
source_url: https://arxiv.org/abs/2507.03410
tags:
- graph
- repair
- repairs
- llms
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of six open-source large
  language models (LLMs) for automated property graph repair. Using a synthetic medical
  dataset with artificial inconsistencies, the research tests different encoding methods
  and prompt configurations to guide LLMs in identifying and correcting violations
  of graph denial constraints.
---

# Graph Repairs with Large Language Models: An Empirical Study

## Quick Facts
- **arXiv ID**: 2507.03410
- **Source URL**: https://arxiv.org/abs/2507.03410
- **Reference count**: 40
- **Primary result**: LLMs can follow structured repair formats with high success (72%-96%) but moderate accuracy (42%-74%) for automated property graph repair

## Executive Summary
This study evaluates six open-source large language models (LLMs) for automated property graph repair, focusing on fixing Graph Denial Constraint violations in synthetic medical data. Using a controlled dataset with artificial inconsistencies, the research tests different encoding methods and prompt configurations to guide LLMs in identifying and correcting violations where patients are prescribed medications containing ingredients they're allergic to. Results demonstrate that while all tested models can follow the required output format with high success rates, their accuracy in generating correct repairs remains moderate, with DeepSeek-R1 achieving the highest repair validity at significant computational cost.

The study systematically evaluates encoding methods (node-edge, template-based, LLM-generated descriptions) and few-shot configurations across six LLMs, revealing important tradeoffs between accuracy, computational efficiency, and repair validity. Models like Phi4 and Gemma2 show potential for balancing accuracy and efficiency under appropriate configurations, while others exhibit patterns of over-eager generation or valid-but-inaccurate repairs. The research establishes a foundational framework for LLM-based graph repair while highlighting critical limitations that require further investigation, particularly regarding hallucinations and the need for human-in-the-loop workflows.

## Method Summary
The study uses a synthetic medical dataset (Synthea++) with artificial inconsistencies to evaluate LLM-based graph repair. Six open-source LLMs (Llama3.2-7B, Mistral-7B, Phi4-14B, Gemma2-9B, Qwen2.5-7B, DeepSeek-R1-7B) are tested via Ollama interface on a graph containing 1,430 nodes and 1,222 edges with ~165 introduced inconsistencies. Three graph encoding methods are evaluated: M1 (node-edge), M2 (template-based), and M3 (LLM-generated description), across five few-shot configurations. The evaluation measures format adherence (F), repair validity (V), accuracy (A), and computational cost using structured XML-like output formats for repair operations.

## Key Results
- All tested LLMs achieved high format adherence (72%-96%) in following structured repair output formats
- DeepSeek-R1 achieved highest repair validity (84%) but with 11x longer evaluation time than faster models
- Models like Llama3.2 showed excessive repair operations (over-eager generation) while Mistral produced valid-but-inaccurate repairs
- Phi4 and Gemma2 demonstrated potential for balancing accuracy and efficiency under optimal configurations (V=100%, A=32-38%)

## Why This Works (Mechanism)
The approach works by leveraging LLMs' ability to understand natural language descriptions of graph structures and constraint violations, combined with structured output formatting to ensure predictable repair operations. The encoding methods translate graph data into formats LLMs can process, while few-shot prompts provide examples of desired output patterns. The combination of graph structure understanding and instruction-following capabilities enables LLMs to identify violations and propose corrective operations that eliminate constraint violations, even if the proposed repairs don't always match ground truth solutions.

## Foundational Learning
**Graph Denial Constraints (GDCs)**: Logical rules that define forbidden patterns in graph structures (why needed: core problem being solved; quick check: can identify patient-allergy-medication violation patterns). **Graph Encoding Methods**: Different approaches to represent graph data for LLM processing (why needed: LLMs cannot directly process graph structures; quick check: M1/M2/M3 produce different token counts). **Format Adherence vs. Accuracy**: Distinction between following output format and generating correct repairs (why needed: high F doesn't guarantee high A; quick check: models can format correctly while suggesting wrong operations). **Repair Validity**: Whether proposed repairs eliminate constraint violations (why needed: measures effectiveness regardless of ground truth match; quick check: V=100% means violation eliminated). **Hallucination Detection**: Identifying when LLMs generate content not present in input data (why needed: major failure mode causing invalid repairs; quick check: Phi4 V=83%, A=0% indicates hallucinations).

## Architecture Onboarding
**Component Map**: Graph Database (Neo4j) -> Cypher Queries (GDC detection) -> LLM (repair generation) -> Output Parser (format validation) -> Evaluation Metrics (F/V/A)
**Critical Path**: Cypher query execution → graph encoding → LLM prompt generation → repair suggestion → format validation → accuracy evaluation
**Design Tradeoffs**: Encoding method complexity vs. token efficiency (M3 most expressive but highest token cost); model size vs. computational efficiency (DeepSeek-R1 most accurate but slowest); few-shot examples vs. prompt length (more examples improve accuracy but increase tokens)
**Failure Signatures**: Over-eager generation (excessive operations, low accuracy); valid-but-inaccurate repairs (high V, low A); format violations (low F); hallucinations (repair suggests nodes/edges not in input)
**3 First Experiments**: 1) Run M1 encoding with no few-shot examples on all six models to establish baseline performance; 2) Test M3 encoding with 2-mixed few-shot examples on Gemma2 to verify optimal configuration; 3) Compare token counts and evaluation times between DeepSeek-R1 and Qwen2.5 to quantify efficiency tradeoff

## Open Questions the Paper Calls Out
1. Can retrieval-augmented generation (RAG) or domain-specific fine-tuning reduce LLM hallucinations in graph repair while maintaining computational efficiency?
2. How can human-in-the-loop workflows be optimally designed to validate LLM repair suggestions with minimal user burden?
3. Do explicit reasoning traces (e.g., DeepSeek-R1's `<thoughts>` tags) causally improve repair accuracy, or are they epiphenomenal overhead?
4. Will LLM graph repair performance generalize to real-world graphs and non-medical domains?

## Limitations
- Ground truth repair dataset not fully specified in paper text, requiring external repository access
- Prompt templates for encoding modes M2 and M3 incompletely described, complicating exact replication
- Computational cost findings may be model-version specific and not generalize to newer LLM versions
- Synthetic dataset with controlled inconsistencies may not reflect real-world noise patterns or constraint complexity
- Evaluation focuses on single constraint type (allergy-medication), limiting conclusions about broader applicability

## Confidence
- **High confidence**: LLMs can follow structured output formats for graph repairs with consistently high success rates across all tested models (72%-96% format adherence)
- **Medium confidence**: All tested LLMs can detect and propose repairs for graph inconsistencies, but accuracy remains moderate (42%-74%) and varies significantly by model and configuration
- **Medium confidence**: DeepSeek-R1 achieves the highest repair validity (84%) but requires substantially higher computational resources compared to other models
- **Low confidence**: The optimal configuration for balancing accuracy and efficiency can be universally determined—results show Phi4 and Gemma2 can achieve good performance under specific configurations, but this may not generalize across different constraint types or datasets

## Next Checks
1. Replicate with expanded constraint types: Test the same LLMs and encoding methods on additional graph denial constraints beyond allergy-medication violations to assess generalizability across different inconsistency patterns.
2. Ground truth verification: Obtain and validate the complete labeled repair dataset from the repository, then compare the published accuracy metrics against independent evaluation of the same dataset using the described methodology.
3. Real-world data testing: Apply the best-performing configurations to a real medical dataset with naturally occurring inconsistencies rather than synthetic data with controlled error injection, to assess practical utility.