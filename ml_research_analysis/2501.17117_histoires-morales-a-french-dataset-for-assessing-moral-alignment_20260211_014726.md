---
ver: rpa2
title: 'Histoires Morales: A French Dataset for Assessing Moral Alignment'
arxiv_id: '2501.17117'
source_url: https://arxiv.org/abs/2501.17117
tags:
- moral
- french
- translation
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces HISTOIRES MORALES, the first dataset for assessing
  moral alignment of language models in French, consisting of 12,000 stories derived
  from the English MORAL STORIES dataset through translation and cultural adaptation.
  The translation pipeline employs demonstrations with error explanations to ensure
  high-quality, culturally appropriate outputs.
---

# Histoires Morales: A French Dataset for Assessing Moral Alignment

## Quick Facts
- arXiv ID: 2501.17117
- Source URL: https://arxiv.org/abs/2501.17117
- Reference count: 40
- This work introduces HISTOIRES MORALES, the first dataset for assessing moral alignment of language models in French, consisting of 12,000 stories derived from the English MORAL STORIES dataset through translation and cultural adaptation.

## Executive Summary
This paper introduces HISTOIRES MORALES, a French dataset for assessing moral alignment of language models, created by translating and culturally adapting the MORAL STORIES dataset. The translation pipeline uses demonstrations with error explanations to ensure high-quality outputs. Evaluation shows that while multilingual models generally align with human moral norms, they exhibit sensitivity to alignment shifts, with models being influenced by as few as 84 examples to prefer either moral or immoral actions. Experiments reveal that models perform better in English than French for moral decision-making, and robustness of alignment varies across languages. The dataset provides a foundation for comparative analysis of moral reasoning across languages and highlights challenges in ensuring consistent alignment.

## Method Summary
The method involves creating HISTOIRES MORALES by translating MORAL STORIES from English to French using a three-stage pipeline: simple prompt, cultural adaptation prompt, and demonstrations with error explanations. Evaluation uses perplexity comparison between moral and immoral actions (PPLM vs PPLI) and declarative prompting. Robustness is tested via DPO fine-tuning toward moral or immoral preferences using QLoRA with specific hyperparameters. Models tested include Mistral-7B, CroissantLLM, and LLaMA-3.1-8B-Instruct. The dataset consists of 12,000 stories with 7 components each, and experiments use training sizes from 8 to 8400 examples with 5 random seeds.

## Key Results
- Translation quality achieved COMET KIWI 22 score of 0.851
- Models show language-dependent moral alignment with better performance in English than French
- DPO experiments demonstrate sensitivity to alignment shifts with as few as 84 examples
- LLaMA-3.1-8B-Instruct disproportionately blocks French stories compared to English

## Why This Works (Mechanism)
The translation pipeline ensures high-quality outputs through iterative refinement using demonstrations with error explanations. Cultural adaptation addresses nuances that direct translation might miss. Perplexity-based evaluation provides a quantitative measure of moral alignment by comparing the likelihood of moral versus immoral actions. DPO fine-tuning demonstrates the sensitivity of alignment to small training sets, showing how models can be steered toward specific moral preferences.

## Foundational Learning
- **Translation quality metrics**: Why needed - to ensure cultural adaptation preserves meaning; Quick check - verify COMET KIWI 22 scores exceed 0.85
- **Perplexity evaluation**: Why needed - to quantitatively compare moral vs immoral action likelihood; Quick check - confirm PPLM < PPLI indicates moral preference
- **DPO fine-tuning**: Why needed - to test alignment robustness and sensitivity; Quick check - measure PPL shift across training sizes 8, 84, 840, 8400
- **Cross-lingual evaluation**: Why needed - to identify language-dependent alignment differences; Quick check - compare performance across French and English story pairs

## Architecture Onboarding

**Component Map**: 
Dataset Creation -> Translation Pipeline -> Evaluation Models -> DPO Experiments

**Critical Path**: 
Download MORAL STORIES -> Run Translation Pipeline -> Evaluate Perplexity -> Run DPO Fine-tuning -> Analyze Results

**Design Tradeoffs**: 
Uses GPT-3.5-turbo-16k for translation (ensures quality but introduces potential bias) vs. human translators (would be more accurate but impractical at scale); Perplexity-based metrics (scalable but may conflate fluency with morality) vs. direct human judgment (more accurate but expensive).

**Failure Signatures**: 
Perplexity values similar for moral/immoral actions suggests translation or evaluation issues; Disproportionate blocking of French stories by LLaMA-3 indicates language-specific bias; Alignment shifts disappearing with larger training sets suggests limited robustness.

**First Experiments**:
1. Verify dataset creation by checking that HISTOIRES MORALES contains 12k stories with 7 components each
2. Run perplexity evaluation on a small subset (100 stories) to confirm PPLM < PPLI pattern
3. Execute DPO with smallest training size (8 examples) to verify alignment shift detection

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Translation pipeline relies heavily on GPT-3.5-turbo-16k, introducing potential model bias
- DPO effects diminish with larger training sets, suggesting limited practical robustness
- Perplexity-based metrics may conflate language modeling fluency with genuine moral reasoning
- Cultural adaptation improvements demonstrated but not comprehensively quantified

## Confidence

**High confidence**: The translation pipeline design and dataset creation methodology are well-documented and reproducible. The core finding that multilingual models show language-dependent moral alignment is supported by multiple evaluation methods.

**Medium confidence**: The DPO experiments demonstrating alignment sensitivity are reproducible, but the practical significance of small training set effects versus large-scale robustness remains uncertain. Cultural adaptation improvements are demonstrated but not comprehensively quantified.

**Low confidence**: Claims about cross-cultural moral differences between French and English datasets are difficult to verify due to potential translation artifacts and the lack of direct human moral judgment comparisons across languages.

## Next Checks
1. Replicate the full DPO experiment pipeline across all training sizes (8, 84, 840, 8400) with the exact demonstration examples and seeds to verify the reported alignment sensitivity and effect size decay.
2. Conduct human evaluation studies comparing moral judgments on French versus English story pairs to distinguish genuine cultural differences from translation artifacts or model bias.
3. Test additional French-language models (e.g., French-specific LLaMAs or open-source alternatives) to determine whether observed alignment differences are language-specific or model-dependent.