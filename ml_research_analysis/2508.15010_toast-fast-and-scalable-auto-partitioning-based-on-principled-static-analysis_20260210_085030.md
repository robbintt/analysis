---
ver: rpa2
title: 'TOAST: Fast and scalable auto-partitioning based on principled static analysis'
arxiv_id: '2508.15010'
source_url: https://arxiv.org/abs/2508.15010
tags:
- sharding
- dimension
- dimensions
- figure
- toast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TOAST is an automatic sharding tool for large ML models that combines
  static compiler analysis with Monte Carlo Tree Search. It discovers sharding strategies
  by identifying tensor dimensions requiring identical sharding and resolving sharding
  conflicts through a novel static analysis.
---

# TOAST: Fast and scalable auto-partitioning based on principled static analysis

## Quick Facts
- arXiv ID: 2508.15010
- Source URL: https://arxiv.org/abs/2508.15010
- Reference count: 40
- TOAST outperforms state-of-the-art methods like Alpa and AutoMap across diverse models and hardware platforms, consistently finding better partitioning strategies

## Executive Summary
TOAST is an automatic sharding tool for large ML models that combines static compiler analysis with Monte Carlo Tree Search (MCTS). It discovers sharding strategies by identifying tensor dimensions requiring identical sharding and resolving sharding conflicts through a novel static analysis. The tool outperforms state-of-the-art methods like Alpa and AutoMap across diverse models (T2B, T7B, GNS, U-Net, ITX) and hardware platforms (TPUv3, A100, P100 GPUs), consistently finding better partitioning strategies while being 25x faster than AutoMap and avoiding out-of-memory errors.

## Method Summary
TOAST uses Named Dimension Analysis (NDA) to identify tensor dimensions that must be sharded identically through identity constraints and sharding rules, reducing the exponential search space to tractable subsets. Sharding conflicts (same dimension appearing multiple times) are grouped into compatibility sets that share resolution decisions. MCTS then explores action space using a cost model combining analytical roofline estimates with memory penalties, with state represented as an in-memory map of dimension sharding configurations to eliminate duplicate states.

## Key Results
- TOAST consistently discovers superior partitioning strategies compared to manual, Alpa, and AutoMap baselines across T2B, T7B, GNS, U-Net, and ITX models
- Search is 25x faster than AutoMap while avoiding out-of-memory errors that plague other methods
- Enables hardware fungibility by finding near-optimal strategies across different accelerator architectures with minimal re-tuning

## Why This Works (Mechanism)

### Mechanism 1: Named Dimension Analysis (NDA) for Exponential Pruning
Static analysis of tensor dimension relationships prunes the search space by identifying dimensions that must be sharded identically, reducing the exponential decision space to a tractable subset. NDA assigns unique dimension names to tensor operands and results based on operation-specific sharding rules, then discovers equivalence classes of dimensions ("colors") that must share sharding decisions through identity constraints and definition-to-use maps.

### Mechanism 2: Conflict Compatibility Sets for Resolution Space Reduction
Sharding conflicts (same dimension name appearing multiple times in a tensor) are grouped into compatibility sets that share resolution decisions, preventing combinatorial explosion. The analysis identifies "compatible" conflicts forming "box" patterns in the dimension graph—pairs connected through definition-use relationships where resolving them identically avoids resharding communication. Transitive closure of compatibility creates sets resolved by single bits.

### Mechanism 3: Color-Aware State Representation for MCTS Efficiency
Representing MCTS state as the sharding configuration of all dimensions eliminates duplicate states by construction and enables efficient in-memory tracking. Each action is a tuple (dim_name/color, resolution_order bitstring, device axis), and the state is a map recording which dimensions are sharded and how. Since state is defined by final configuration—not action history—any trajectory reaching identical sharding resolves to the same tree node.

## Foundational Learning

- **SPMD Partitioning and Device Meshes**: TOAST operates on sharding tensors across n-dimensional device meshes. Without understanding that sharding dimension X along axis b means each device holds disjoint slices, NDA output is uninterpretable.
  - Quick check: Given a [1024, 512] tensor sharded along dimension 0 across 4 devices on axis 'b', what shape does each device hold locally?

- **Sharding Propagation in Compilers**: NDA formalizes what propagation-based systems do implicitly. Understanding propagation—how sharding one tensor constrains others via dataflow—explains why NDA's identity constraints work.
  - Quick check: If operand A to a matmul is sharded on dimension 0, and the matmul rule says result dimension 0 matches operand A dimension 0, what sharding does the result inherit?

- **Monte Carlo Tree Search (Selection, Expansion, Simulation, Backpropagation)**: TOAST uses MCTS to explore sharding trajectories. Without understanding the exploration-exploitation tradeoff and how UCT balances it, search behavior is opaque.
  - Quick check: In MCTS, if a node has high visit count but average reward, vs. a node with low visit count but high reward, which does UCT prioritize and why?

## Architecture Onboarding

- **Component map**: Frontend (JAX/PyTorch) → StableHLO IR → Named Dimension Analysis → Action Space Constructor → MCTS Engine → Cost Model → Argument Grouper → Output (sharded StableHLO) → XLA backend → execution

- **Critical path**: NDA correctness is foundational—bugs propagate to invalid actions; conflict compatibility set computation determines search space size; cost model accuracy determines MCTS convergence quality

- **Design tradeoffs**: Pre-computed actions vs. runtime generation (trades memory for speed); compatibility heuristics vs. exhaustive resolution (reduces search but may miss optimal strategies); cost model precision vs. speed (fast but may miss runtime effects)

- **Failure signatures**: OOM during search (cost model memory penalty too low); slow convergence on new architectures (missing operation sharding rules); suboptimal results vs. manual (conflict compatibility heuristics over-constraining); different results on GPU vs. TPU (cost model device parameters misconfigured)

- **First 3 experiments**:
  1. Reproduce MLP example: Run TOAST on 2-layer MLP, verify NDA produces colors B, X, U, W matching Figure 4c, and batch partitioning is discovered
  2. Single attention layer conflict analysis: Run NDA on isolated attention, verify conflict compatibility set identification, measure actual runtime vs. cost model estimate
  3. Scaling study on T2B: Reproduce Figure 10 trajectory, plot step time and search time vs. sequence length, identify when TOAST first finds superior strategies

## Open Questions the Paper Calls Out

- **Learned sharding prediction**: Can NDA features train a model to predict optimal sharding configurations without search? The paper states future work will leverage NDA to train a model capable of predicting optimal sharding immediately given colors to shard.

- **Compatibility heuristic completeness**: Do heuristics enforcing uniform conflict resolution across compatible sets or isomorphic layers ever exclude globally optimal strategies? The paper doesn't prove optimal solutions never require asymmetric resharding.

- **Cost model limitations**: How much does reliance on approximate analytical cost models limit discovery of truly optimal strategies versus measurement-based profiling? The model may fail to capture complex hardware interactions like network congestion.

## Limitations

- Cost model relies on analytical approximations that may poorly capture dynamic runtime effects like network contention, potentially causing MCTS to converge to suboptimal strategies
- Conflict compatibility heuristics are based on empirical observation rather than formal correctness guarantees, risking missed optimization opportunities
- NDA assumes operation sharding rules fully capture all valid parallelization patterns; missing or incorrect rules for new operation types would break the analysis

## Confidence

- **High**: NDA's exponential pruning effect through dimension equivalence classes—formally specified and algorithmically sound
- **Medium**: Conflict compatibility set computation—heuristic-based with no corpus validation but matches empirical observations on standard Transformers
- **Medium**: MCTS convergence to superior strategies—demonstrated empirically but dependent on cost model accuracy

## Next Checks

1. **Cost model calibration validation**: Run TOAST on single attention layer example with each conflict resolution applied, measuring actual step time versus cost model predictions to quantify systematic bias

2. **Cross-platform generalization test**: Evaluate TOAST on T2B across TPUv3, A100, and P100 using identical cost model parameters to identify whether platform-specific parameters need tuning

3. **Conflict compatibility stress test**: Construct synthetic models with non-box-compatible conflicts to verify whether compatibility heuristic correctly identifies when conflicts must be resolved independently versus grouped