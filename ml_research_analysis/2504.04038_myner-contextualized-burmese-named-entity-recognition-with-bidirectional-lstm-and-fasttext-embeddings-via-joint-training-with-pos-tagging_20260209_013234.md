---
ver: rpa2
title: 'myNER: Contextualized Burmese Named Entity Recognition with Bidirectional
  LSTM and fastText Embeddings via Joint Training with POS Tagging'
arxiv_id: '2504.04038'
source_url: https://arxiv.org/abs/2504.04038
tags:
- embeddings
- joint
- fasttext
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents myNER, a word-level Named Entity Recognition
  (NER) corpus for the Myanmar (Burmese) language, which includes a 7-tag annotation
  scheme and Part-of-Speech (POS) tagging. The authors evaluated multiple NER models,
  including Conditional Random Fields (CRF) with and without fastText embeddings,
  and Bidirectional LSTM (BiLSTM)-CRF models with random, frozen, and fine-tuned fastText
  embeddings, both in single-task and joint POS+NER training settings.
---

# myNER: Contextualized Burmese Named Entity Recognition with Bidirectional LSTM and fastText Embeddings via Joint Training with POS Tagging

## Quick Facts
- arXiv ID: 2504.04038
- Source URL: https://arxiv.org/abs/2504.04038
- Authors: Kaung Lwin Thant; Kwankamol Nongpong; Ye Kyaw Thu; Thura Aung; Khaing Hsu Wai; Thazin Myint Oo
- Reference count: 10
- CRF with fastText features achieved 0.9818 accuracy and 0.9811 weighted F1 score (0.7429 macro F1 score)

## Executive Summary
This paper presents myNER, a word-level Named Entity Recognition corpus for the Myanmar (Burmese) language, featuring a 7-tag annotation scheme and Part-of-Speech tagging. The authors evaluated multiple NER models including Conditional Random Fields with and without fastText embeddings, and Bidirectional LSTM-CRF models with different embedding configurations. The research demonstrates that joint training with POS tagging improves performance, particularly for BiLSTM-based models, while CRF-based models outperform Softmax-based models due to their ability to capture label dependencies.

## Method Summary
The research employed both traditional machine learning (CRF with CRFsuite) and deep learning (BiLSTM with PyTorch) approaches for Burmese NER. CRF models used handcrafted features including word context, prefixes/suffixes, and position indicators, with optional fastText embeddings. BiLSTM models were tested with random, frozen, and fine-tuned fastText embeddings, using either CRF or Softmax inference layers. Joint training with POS tagging was implemented for BiLSTM models. The myNER corpus (16,605 sentences) was derived from myPOS v3, with pre-trained Burmese fastText embeddings (300-dim, Wikipedia-trained) serving as the primary feature source.

## Key Results
- CRF model with fastText features achieved highest performance: 0.9818 accuracy and 0.9811 weighted F1 score
- BiLSTM-CRF with fine-tuned fastText embeddings achieved best deep learning result: 0.9791 accuracy and 0.9776 weighted F1 score
- Joint training with POS tagging improved BiLSTM macro F1 from 0.7154 to 0.7395
- CRF-based models outperformed Softmax-based models due to better handling of label dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRF inference layers outperform Softmax for NER by explicitly modeling valid label transitions.
- Mechanism: CRF learns transition probabilities between consecutive labels alongside emission probabilities from the encoder, preventing invalid sequences (e.g., "I-PER" cannot follow "B-LOC"). Softmax predicts each token independently, ignoring sequential constraints.
- Core assumption: Entity label sequences follow predictable patterns that can be learned from training data.
- Evidence anchors:
  - [abstract] "The CRF-based models outperformed Softmax-based models due to their ability to capture label dependencies."
  - [section III.D] "The CRF layer models dependencies between consecutive labels by considering transition probabilities between labels and emission probabilities... making it particularly effective for tasks like NER where label transitions follow specific patterns."
  - [corpus] Related work "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF" validates BiLSTM-CRF architectures for sequence labeling tasks.
- Break condition: If training data is insufficient to learn reliable transition patterns, or if computational budget cannot support CRF's ~7.5x longer training time (257.90s vs 34.58s reported).

### Mechanism 2
- Claim: FastText embeddings improve NER for Burmese by leveraging subword information to handle OOV words and morphological complexity.
- Mechanism: FastText represents words as bags of character n-grams, averaging constituent n-gram embeddings. This captures morphological patterns and enables representations for unseen words based on their character composition.
- Core assumption: Subword morphology in Burmese correlates with semantic meaning and entity type signals.
- Evidence anchors:
  - [abstract] "The research demonstrates the effectiveness of contextualized word embeddings... for improving NER performance in low-resource languages."
  - [section III.A] "This approach is particularly useful for morphologically rich languages such as the Myanmar language and improves the representation of rare words."
  - [corpus] Evidence is limited; neighbor paper on Hungarian embeddings discusses FastText for morphology but for a different language family.
- Break condition: If pre-trained fastText has poor coverage of domain-specific vocabulary, or if entity recognition relies more on word-level semantics than morphological patterns.

### Mechanism 3
- Claim: Joint training with POS tagging improves BiLSTM-based NER by sharing syntactic representations across tasks.
- Mechanism: Multi-task learning forces the encoder to learn representations useful for both POS and NER, providing implicit syntactic context that helps disambiguate entity boundaries and types.
- Core assumption: POS information provides complementary signal that correlates with entity recognition accuracy.
- Evidence anchors:
  - [abstract] "Joint training with POS tagging improved model performance, particularly for BiLSTM-based models."
  - [section V] "Joint training with BiLSTM models provided a more noticeable improvement in performance, particularly when fine-tuned embeddings were used."
  - [corpus] Related work on semantic type dependencies for clinical NER shows similar multi-task benefits, though in a different domain.
- Break condition: If POS and NER tasks have conflicting gradient signals, or if POS annotations are noisy/unavailable for target domain.

## Foundational Learning

- Concept: **Conditional Random Fields (CRFs)**
  - Why needed here: CRF is the inference layer that achieved the best results; understanding transition matrices is essential for debugging invalid label sequences.
  - Quick check question: Why should the transition probability B-PER → I-LOC be near zero in a well-trained NER model?

- Concept: **Subword Embeddings (FastText)**
  - Why needed here: The paper compares random, frozen, and fine-tuned fastText; understanding OOV handling is critical for low-resource language deployment.
  - Quick check question: Given the word "ရန်ကုန်တက္ကသိုလ်" (Yangon University) not in vocabulary, how would fastText still generate an embedding?

- Concept: **Multi-task Learning**
  - Why needed here: Joint POS+NER training improved BiLSTM macro F1 from 0.7154 to 0.7395; understanding task interference helps architecture decisions.
  - Quick check question: If POS tag distribution differs significantly between training and inference domains, would you expect joint training to help or hurt NER performance?

## Architecture Onboarding

- Component map:
  Input -> Embedding Layer (300d fastText, random/frozen/fine-tuned options) -> Encoder (BiLSTM: 128 hidden units for random, 256 for fastText) -> Inference (CRF or Softmax) -> Output (BIOES tags)

- Critical path:
  1. Initialize with pre-trained Burmese fastText (300d, Wikipedia-trained)
  2. Set BiLSTM to 256 hidden units, batch size 64, dropout 0.5
  3. Use CRF inference layer (not Softmax)
  4. Enable fine-tuning of embeddings during training
  5. Train jointly with POS tags (15 tag sets from myPOS)
  6. Apply early stopping on validation loss (max 50 epochs)

- Design tradeoffs:
  - CRF vs Softmax: +7% macro F1 but 7.5x training time
  - Fine-tuned vs frozen embeddings: +2.4% macro F1 for BiLSTM-CRF joint
  - Traditional CRF vs BiLSTM-CRF: Traditional CRF + fastText features achieves marginally best results (0.7429 vs 0.7395 macro F1) but requires manual feature engineering

- Failure signatures:
  - Low F1 on rare tags: B-NUM (0.42), E-NUM (0.42) due to only 151 training examples
  - Softmax produces invalid sequences (I-X without preceding B-X)
  - Macro F1 (0.74) much lower than weighted F1 (0.98) indicates poor minority class performance masked by dominant "O" tag (167,547 instances)

- First 3 experiments:
  1. Replicate baseline: BiLSTM-Softmax + random embeddings, single-task NER (expect ~0.65 macro F1)
  2. Embedding ablation: Switch to fine-tuned fastText (expect ~0.67 macro F1)
  3. Full configuration: BiLSTM-CRF + fine-tuned fastText + joint POS (target: ~0.74 macro F1 per paper)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would the integration of transformer-based architectures outperform the current BiLSTM-CRF baseline on the myNER corpus?
- Basis in paper: [explicit] The authors explicitly list "exploring more advanced neural network architectures" as a primary direction for future work.
- Why unresolved: The study focused only on traditional machine learning (CRF) and recurrent neural networks (BiLSTM), leaving newer architectures untested.
- What evidence would resolve it: A performance comparison (Accuracy, F1) between the current best models and transformer-based models (e.g., BERT) trained on the same myNER dataset.

### Open Question 2
- Question: Can advanced loss functions or data resampling techniques significantly improve the recall for low-frequency tags like NUM and TIME?
- Basis in paper: [inferred] The Limitations section notes that the model struggles with "low-frequency tags" due to class imbalance and suggests "more advanced loss functions" or "over-sampling" as potential remedies.
- Why unresolved: The experiments utilized standard training procedures without specific strategies to mitigate the identified class imbalance.
- What evidence would resolve it: Experimental results showing improved Macro F1-scores for minority classes after applying techniques like focal loss or SMOTE to the training data.

### Open Question 3
- Question: How robust are the proposed models when applied to specialized domains outside of the current myPOS-derived dataset?
- Basis in paper: [explicit] The Conclusion states that future efforts should focus on "expanding datasets to cover a wider range of domains."
- Why unresolved: The current dataset is relatively small and may not represent the full linguistic variety or domain-specific entities found in the broader Burmese language.
- What evidence would resolve it: Evaluation of the fine-tuned models on a new, domain-specific test set (e.g., medical or legal texts) demonstrating sustained performance.

## Limitations
- Corpus availability: myNER corpus source and exact train/validation/test splits remain unspecified, limiting independent validation
- Architecture details: Joint POS+NER implementation specifics (layer sharing, loss weighting) are not fully detailed
- Feature engineering transparency: Exact integration of handcrafted features with fastText embeddings lacks sufficient detail

## Confidence
**High Confidence**: CRF inference layers outperform Softmax for NER by explicitly modeling label dependencies and valid transitions; FastText embeddings improve NER performance through subword information handling for OOV words; CRF+fastText handcrafted features achieves the highest overall performance

**Medium Confidence**: Joint training with POS tagging improves BiLSTM-based NER performance, particularly with fine-tuned embeddings; The reported improvement from random embeddings to fine-tuned fastText demonstrates the effectiveness of contextualized embeddings; The gap between macro F1 (0.74) and weighted F1 (0.98) indicates significant class imbalance affecting minority tag performance

**Low Confidence**: The specific contribution of each component (fastText vs joint training vs CRF) to the overall performance improvement; The scalability of these approaches to other low-resource languages with different morphological characteristics; The computational efficiency comparison between traditional CRF and BiLSTM-CRF given the 7.5x training time difference

## Next Checks
1. Replicate the baseline BiLSTM-Softmax model with random embeddings using the same hyperparameters (hidden=128, batch=32, dropout=0.5) and verify the macro F1 score (~0.65) to establish a baseline for comparison with paper results

2. Implement the exact CRF feature set described (word context, 1-3 character prefixes/suffixes, hyphen/numeric indicators, position) and compare performance with and without fastText embeddings as handcrafted features to validate the reported 0.7429 macro F1 achievement

3. Obtain access to the myNER corpus and verify the train/validation/test split ratios and random seeds used in the paper. Test the joint POS+NER implementation with specified hyperparameters (hidden=256, batch=64, fine-tuned embeddings) to confirm the 0.7395 macro F1 result for BiLSTM-CRF joint training