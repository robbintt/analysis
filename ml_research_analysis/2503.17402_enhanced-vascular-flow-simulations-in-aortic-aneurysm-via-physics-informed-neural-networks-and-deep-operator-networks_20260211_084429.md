---
ver: rpa2
title: Enhanced Vascular Flow Simulations in Aortic Aneurysm via Physics-Informed
  Neural Networks and Deep Operator Networks
arxiv_id: '2503.17402'
source_url: https://arxiv.org/abs/2503.17402
tags:
- data
- training
- deeponet
- e-02
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of Physics-Informed Neural Networks
  (PINNs), Deep Operator Networks (DeepONets), and their Physics-Informed extensions
  (PI-DeepONets) for predicting vascular flow in 3D Abdominal Aortic Aneurysm (AAA)
  models. The methods are adapted to incorporate Navier-Stokes equations as physical
  laws governing fluid dynamics.
---

# Enhanced Vascular Flow Simulations in Aortic Aneurysm via Physics-Informed Neural Networks and Deep Operator Networks

## Quick Facts
- **arXiv ID**: 2503.17402
- **Source URL**: https://arxiv.org/abs/2503.17402
- **Reference count**: 40
- **Primary result**: (PI-)DeepONet inference runtime is 22.5× faster than CFD simulations for 3D AAA flow prediction

## Executive Summary
This study explores Physics-Informed Neural Networks (PINNs), Deep Operator Networks (DeepONets), and their Physics-Informed extensions (PI-DeepONets) for predicting vascular flow in 3D Abdominal Aortic Aneurysm (AAA) models. The methods incorporate Navier-Stokes equations as physical laws governing fluid dynamics. PINNs demonstrate resilience to noisy data and can operate without labeled data, while (PI-)DeepONets efficiently handle parametric PDEs and generalize across varying boundary conditions without retraining. These approaches offer promising alternatives to computationally expensive CFD simulations, with potential applications in patient-specific cardiovascular modeling.

## Method Summary
The framework adapts PINNs and DeepONets to predict steady 3D vascular flow in an idealized AAA geometry using Navier-Stokes equations. PINNs combine data and physics loss terms, showing resilience to noise and ability to function without labeled data. DeepONets learn operators mapping boundary conditions to full flow fields, achieving 22.5× faster inference than CFD. The architecture employs Modified-MLPs with dual encoders and gating mechanisms, while Grad Norm dynamically balances loss terms. Training uses Adam optimizer with exponential learning rate decay, and the models are validated against CFD simulations across varying inlet velocities.

## Key Results
- PINNs show resilience to noisy data and can operate without labeled data
- (PI-)DeepONets handle parametric PDEs efficiently and generalize across boundary conditions without retraining
- (PI-)DeepONet inference runtime is 22.5× faster than CFD simulations
- Selective data usage during training and transfer learning strategies enhance model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Physics-informed loss constraints enable accurate reconstruction of flow fields from sparse or noisy data
- **Mechanism**: Encoding Navier-Stokes equations into the loss function acts as a regularizer, preventing overfitting to noisy data and allowing inference in unobserved regions
- **Core assumption**: Fluid dynamics adhere to steady, incompressible, Newtonian Navier-Stokes equations
- **Evidence anchors**: PINNs outperform standard DeepNNs as noise levels increase; physics residuals are crucial for convergence in operator learning

### Mechanism 2
- **Claim**: Operator learning decouples training cost from inference speed by learning boundary-to-field mappings
- **Mechanism**: DeepONet learns a non-linear operator mapping input functions (boundary conditions) to output functions (flow fields) via separate Branch and Trunk networks
- **Core assumption**: Test boundary conditions lie within training parameter distribution (interpolation)
- **Evidence anchors**: 22.5× speedup demonstrated; model generalizes to unseen inlet velocities within training bounds

### Mechanism 3
- **Claim**: Architectural modifications and adaptive loss balancing mitigate spectral bias and optimization pathologies
- **Mechanism**: Modified-MLP with dual encoders captures complex flow patterns; Grad Norm equalizes gradient norms of different loss terms
- **Core assumption**: Optimization landscape allows gradient-based methods to find global minimum if scales are balanced
- **Evidence anchors**: Modified-MLP contributes most to minimizing relative error; standard PINNs often face optimization difficulties

## Foundational Learning

- **Concept: Navier-Stokes Equations (NSE)**
  - **Why needed here**: Define the "physics" in Physics-Informed Neural Networks; required for implementing the L_phy loss term
  - **Quick check question**: Can you write the residual for the conservation of momentum for an incompressible fluid?

- **Concept: Automatic Differentiation (AD)**
  - **Why needed here**: Model calculates physics loss by differentiating network output with respect to spatial coordinates
  - **Quick check question**: Do you understand how to compute second-order derivatives (∇²v) of a neural network output with respect to inputs using PyTorch?

- **Concept: Operator Regression**
  - **Why needed here**: DeepONet learns mappings between infinite-dimensional function spaces rather than finite vectors
  - **Quick check question**: Can you explain the difference between learning a function f(x) and learning an operator G(u)(x) where u is an input function?

## Architecture Onboarding

- **Component map**: Boundary parameters → Branch Net → Embedding A; Spatial coordinates → Trunk Net → Embedding B; Embeddings A and B → Dot product → 4D output vector (v₁, v₂, v₃, p)

- **Critical path**: 1) Data Stratification: Split mesh data into Inlet, Outlet, Wall, and Volume strata; 2) Loss Construction: Combine L_data, L_bc, and L_phy; 3) Balancing: Apply Grad Norm to weigh physics vs. boundary vs. data losses

- **Design tradeoffs**: PINN vs. DeepONet - use PINN for single scenarios with sparse/noisy data (slower train), use DeepONet for parametric studies (slower initial train, massive inference speedup); Modified-MLP increases accuracy but requires more memory and compute

- **Failure signatures**: Pressure Drift (constant offset in data-less PINN modes), Spectral Bias (smooth, blurred velocity fields), Extrapolation Collapse (non-physical results for velocities outside training range)

- **First 3 experiments**: 1) Sanity Check: Compare standard DeepNN vs. PINN on sparse longitudinal slice; 2) Noise Resilience: Add 10% Gaussian noise and compare test error; 3) Operator Generalization: Train Multi-Input DeepONet on 5 velocities, test on 2 withheld velocities

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the frameworks be extended to handle unsteady, transient flows and moving boundaries in patient-specific vascular geometries?
- **Open Question 2**: To what extent can meta-learning techniques optimize hyper-parameters more effectively than manual adjustment?
- **Open Question 3**: How can (PI-)DeepONet architecture be modified to reliably generalize for inlet velocities outside training range?

## Limitations
- Exact mathematical definition of idealized AAA bulge geometry not provided, requiring assumptions
- Specific indices for cross-sectional and longitudinal training data subsets not included
- CFD setup details (exact outlet pressure values) are implicit

## Confidence
- **High Confidence**: Physics-informed loss mechanism's ability to regularize noisy data
- **Medium Confidence**: 22.5× speedup claim for PI-DeepONet inference
- **Medium Confidence**: Effectiveness of Modified-MLP and Grad Norm for optimization

## Next Checks
1. **Geometry sensitivity**: Reproduce key results with alternative AAA bulge geometries to assess robustness
2. **Extrapolation limits**: Systematically test DeepONet performance on inlet velocities progressively further outside training range
3. **Real patient data**: Apply trained models to 3D vascular geometries derived from actual patient imaging to evaluate clinical transferability