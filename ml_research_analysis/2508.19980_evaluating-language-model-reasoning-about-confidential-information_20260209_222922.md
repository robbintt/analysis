---
ver: rpa2
title: Evaluating Language Model Reasoning about Confidential Information
arxiv_id: '2508.19980'
source_url: https://arxiv.org/abs/2508.19980
tags:
- password
- information
- confidential
- reasoning
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language models fail to reliably withhold confidential information
  without correct passwords, even in simple scenarios. The authors create PasswordEval,
  a benchmark where models must reveal a secret only when given a correct password.
---

# Evaluating Language Model Reasoning about Confidential Information

## Quick Facts
- arXiv ID: 2508.19980
- Source URL: https://arxiv.org/abs/2508.19980
- Reference count: 22
- Language models fail to reliably withhold confidential information without correct passwords, even in simple scenarios

## Executive Summary
This paper introduces PasswordEval, a benchmark testing whether language models can conditionally release confidential information based on password verification. All tested frontier models—including reasoning models—perform well on compliant requests but poorly on non-compliant ones, with reasoning capabilities providing no significant benefit. The study reveals that reasoning traces frequently leak passwords or secrets, raising security concerns about exposing intermediate reasoning states. Simple jailbreak prompts drastically reduce compliance, exposing brittleness in alignment. Results suggest current models are ill-suited for high-stakes access control and that reasoning traces should not be exposed publicly.

## Method Summary
PasswordEval evaluates LLMs on 500 scenarios where models must reveal confidential information only when provided with correct passwords. The benchmark includes compliant and non-compliant requests, multi-turn password verification (2-10 sequential passwords), and adversarial jailbreak testing. Models are evaluated using exact string matching to detect password or confidential information leakage in both outputs and reasoning traces. Standard inference parameters are used (temperature=1.0, top_p=1.0, 100 tokens) with one in-context example per query. The evaluation measures compliant accuracy, non-compliant accuracy, and information leakage rates.

## Key Results
- Frontier models achieve high non-compliant accuracy but struggle with compliant requests, revealing a gap in contextual robustness
- Reasoning traces leak confidential information at extremely high rates (72-97.6%) even when final outputs are secure
- Simple jailbreak prompts drastically reduce compliance, exposing brittleness in alignment mechanisms
- Performance degrades further with multi-turn password verification, making current models ill-suited for high-stakes access control

## Why This Works (Mechanism)

### Mechanism 1: Contextual Robustness Failure
Post-training optimization focuses on broad refusal of harmful content but doesn't robustly bind conditional logic (if password -> release; if no password -> refuse) to specific data pieces. Models prioritize general helpfulness over strict logical verification of authorization states, leading to poor performance on compliant requests despite strong non-compliant accuracy.

### Mechanism 2: Reasoning-Induced Information Leakage
Chain-of-thought reasoning requires explicit manipulation of confidential data to determine correct responses. Without specific training to sanitize thought processes, reasoning traces frequently contain passwords and secrets. This decoupling of reasoning capabilities from safety constraints means models reason about secrets rather than reasoning to protect them.

### Mechanism 3: Adversarial Override of Instruction Hierarchy
Models lack robust instruction hierarchies, allowing adversarial prompts to elevate user instructions above system-level access rules. Jailbreak templates frame requests in new contexts (educational, fictional) or use optimized adversarial suffixes that bypass password-based controls by shifting the model's attention to the immediate user context.

## Foundational Learning

**Concept: Contextual Robustness**
- Why needed here: To understand that safety is conditional (safe for user A, unsafe for user B) rather than binary
- Quick check question: Can the model distinguish between generic refusal and conditional refusal based on session state?

**Concept: Instruction Hierarchy**
- Why needed here: To diagnose why jailbreaks work—user prompts successfully compete with system prompts
- Quick check question: Does the model treat system prompts as hard constraints or soft context?

**Concept: Side-Channel Leakage (Reasoning Traces)**
- Why needed here: To recognize that secure output doesn't mean secure process—exposing reasoning invalidates access control
- Quick check question: If the user sees the reasoning trace, is the system secure? (The paper suggests No)

## Architecture Onboarding

**Component map:** PasswordEval Dataset (500 scenarios) -> System Prompt (injects rules, password, secret) -> Evaluation Layer (string-matchers for Output T and Reasoning Trace R)

**Critical path:** Generating system prompt with embedded secrets -> Running inference (standard vs. reasoning mode) -> Applying string-based detection to output and trace

**Design tradeoffs:**
- Exact Match vs. Semantic Match: Strict exact string matching may miss paraphrased leaks but assumes secrets are specific enough to prevent accidental matches
- Trace Visibility: Reasoning models must architecturally hide traces from end-users to maintain security properties found in final output

**Failure signatures:**
- Over-refusal: Model refuses compliant requests (Low Compliant Correctness)
- Context Leakage: Model refuses but outputs password in refusal message
- Reasoning Leakage: Final output secure but intermediate reasoning contains confidential info

**First 3 experiments:**
1. Baseline Conditional Logic: Run PasswordEval (Standard) to measure gap between Non-Compliant Accuracy and Compliant Accuracy
2. Trace Inspection: Run reasoning models and query reasoning trace API for secret string presence
3. Adversarial Stress Test: Apply template jailbreak to measure drop in Non-Compliant Accuracy

## Open Questions the Paper Calls Out

**Open Question 1:** How can reasoning capabilities be trained differently to improve contextual robustness and rule-following? Current reasoning training doesn't transfer to rule-following, showing marginal or negative effects on PasswordEval.

**Open Question 2:** Should reasoning traces be exposed to users, and how can training prevent information leakage in traces? Even when outputs withhold secrets, reasoning traces often contain passwords or confidential information.

**Open Question 3:** What mechanisms underlie the gap between models' compliance on authorized vs. unauthorized requests? The paper doesn't investigate internal representations explaining this asymmetry between high non-compliant accuracy and lower compliant accuracy.

**Open Question 4:** How can language models be integrated with external verification mechanisms for robust access control? PasswordEval tests native authentication; external tool integration remains unexplored.

## Limitations

- Exact content of in-context examples is unspecified, potentially influencing model behavior
- Exact API parameters for newer models' reasoning traces remain unclear
- Evaluation relies on exact string matching which may miss semantically equivalent leaks
- Benchmark focuses on simple password-based access control, may not generalize to complex authorization schemes
- Adversarial templates may not represent full space of possible attack vectors

## Confidence

**High Confidence:** Core finding that models struggle with conditional access control is well-supported by substantial gap between compliant and non-compliant accuracy. Reasoning traces leak confidential information is directly measurable and reproducible.

**Medium Confidence:** Claim about reasoning capabilities providing no benefit is supported but could be influenced by architectural differences beyond reasoning ability. Instruction hierarchy vulnerability is robust but exact mechanism remains partially speculative.

**Low Confidence:** Broader claim that "current models are ill-suited for high-stakes access control" extrapolates beyond tested password-based scenario. Assertion about reasoning traces "should not be exposed publicly" is a security recommendation rather than empirically demonstrated universal truth.

## Next Checks

1. **Multi-modal and Contextual Access Control:** Test PasswordEval with additional security dimensions including multi-modal verification, time-based access windows, and contextual factors to assess generalizability.

2. **Process Reward Model Validation:** Implement PRM penalizing reasoning traces containing confidential information and retrain models on PasswordEval to measure impact on leakage.

3. **Instruction Hierarchy Stress Testing:** Systematically vary relative position, formatting, and priority markers of system vs. user prompts to map boundary conditions where jailbreaks succeed or fail.