---
ver: rpa2
title: 'OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning'
arxiv_id: '2510.24636'
source_url: https://arxiv.org/abs/2510.24636
tags:
- reward
- openrm
- arxiv
- training
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OpenReward, a tool-augmented reward model
  for evaluating long-form agentic tasks by using external tools to gather evidence.
  The authors address the challenge of knowledge-intensive tasks requiring grounding
  beyond internal model knowledge by training with reinforcement learning on over
  27K synthetic pairwise examples.
---

# OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.24636
- Source URL: https://arxiv.org/abs/2510.24636
- Reference count: 9
- Primary result: Achieves 91.33% average accuracy across three in-domain tasks and 78.54% on out-of-domain benchmarks

## Executive Summary
OpenReward is a tool-augmented reward model that evaluates long-form agentic tasks by gathering external evidence through retrieval tools. The model uses reinforcement learning (specifically GRPO) to jointly supervise intermediate tool usage and final judgment accuracy, addressing the challenge of knowledge-intensive tasks that require grounding beyond internal model knowledge. Extensive experiments demonstrate superior performance over existing reward models on both in-domain and out-of-domain benchmarks.

## Method Summary
OpenReward employs a two-stage pipeline: (1) data synthesis generating over 27K pairwise examples by prompting an LLM to create responses with/without reference documents, and (2) GRPO training with a composite reward function combining final judgment accuracy and intermediate tool selection. The model acts as an agent that iteratively decides which tools to invoke (e.g., Wikipedia, arXiv) to gather evidence for verifying candidate responses. The training objective jointly optimizes tool usage strategy and final judgment accuracy through group-relative advantage computation.

## Key Results
- Achieves 91.33% average accuracy across three in-domain evaluation tasks
- Maintains 78.54% accuracy on out-of-domain benchmarks (RewardBench)
- Demonstrates consistent gains when integrated into inference-time response selection and training-time data selection for LLM alignment
- Outperforms existing reward models across all evaluation settings

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Evidence Grounding
The reward model overcomes limitations of static internal knowledge by dynamically retrieving evidence through external tools. This creates a verification loop where the model can ground its judgments in real-time, retrieved evidence for knowledge-intensive, long-form tasks. The effectiveness depends on tool availability and reliability—poor retrieval can introduce noise and degrade performance.

### Mechanism 2: Group Relative Policy Optimization (GRPO)
GRPO trains both tool-use strategy and judgment accuracy simultaneously through group-relative advantages. By comparing trajectory rewards to group means, the method provides stable learning signals that reinforce behaviors leading to better-than-average outcomes. The composite reward combines final outcome reward with intermediate tool-use reward, with tool rewards only granted when final judgments are correct.

### Mechanism 3: Controllable Synthetic Data Synthesis
The framework generates large-scale, high-quality pairwise training data without human annotation by exploiting LLM performance gaps between responses with and without reference document access. This controllable synthesis creates targeted quality differences between positive and negative pairs while maintaining domain diversity across Wikipedia, scientific, and medical domains.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Reward (RLVR)**
  - Why needed here: Uses automatically verifiable outcomes as final rewards instead of subjective human preferences, enabling scalable training without costly annotations
  - Quick check question: How does the composite reward function provide a dense, verifiable learning signal compared to sparse binary rewards?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Uses group-wise comparisons instead of separate value functions, providing built-in baselines and stabilizing the notoriously unstable RL training process
  - Quick check question: How does GRPO compute trajectory advantages and why is the group-relative approach more stable than absolute scalar rewards?

- **Concept: LLM-as-a-Judge and Generative Reward Models**
  - Why needed here: Generates textual reasoning and final judgments rather than scalar scores, providing interpretability while requiring generative training approaches
  - Quick check question: How does OpenReward's tool-augmented generative approach improve upon traditional scalar reward models?

## Architecture Onboarding

- **Component map:** Backbone LLM (Qwen-2.5-7B-Instruct) -> Tool Interface (WIKI(), ARXIV()) -> Prompting System (system prompt with role, tools, output format) -> Training Pipeline (GRPO implementation)

- **Critical path:** 1) Build data synthesis pipeline using strong LLM for target-aware queries and pos/neg pairs, 2) Set up retrieval tools (ColBERT-v2.0 for Wikipedia, LitSearch for arXiv), 3) Implement GRPO with correct group-relative advantage calculation and composite reward function, 4) Evaluate on synthesized test sets and RewardBench

- **Design tradeoffs:** Synthetic data offers scalability but may lack human nuance; tool dependency couples performance to external tool quality and introduces latency; λ weighting balances tool use vs accuracy but performance robust to exact value; 7B model balances capability with training cost

- **Failure signatures:** Lazy searching (avoiding tools, relying on internal knowledge), over-searching/reward hacking (excessive irrelevant tool calls), training instability from miscalibrated KL penalty or clip range

- **First 3 experiments:** 1) Ablation study comparing models trained with only R_EM, only R_tool, and full composite reward to verify failure modes, 2) Baseline comparison against strong LLM prompted to use tools to isolate RL training benefits, 3) Downstream DPO test using OpenReward as data selector for alignment training and comparing to baseline RM

## Open Questions the Paper Calls Out

- Can tool-augmented reward models be effectively extended to multimodal settings involving visual or tabular evidence? The current text-only implementation has yet to be adapted for vision or structured data.

- How does the reliability and latency of external tools affect the robustness and practical deployment of tool-augmented reward models? The effectiveness relies on tool availability and quality, which may introduce bias or latency.

- Does synthetic pairwise data from controlled document access generalize to real-world human preference distributions? The synthetic positive-negative pairs may not capture nuanced human judgments where both responses have mixed strengths.

## Limitations

- Performance depends heavily on external tool availability and quality, creating potential single points of failure
- Synthetic data generation may not capture all nuances of real human preferences despite controlled design
- Specific parameter values (λ weighting, max tool calls, R_tool definition) are not fully disclosed, limiting exact reproduction

## Confidence

- **High confidence:** Tool-augmented evidence grounding approach is well-supported by methodology and experimental design; GRPO framework is theoretically sound
- **Medium confidence:** Superiority over existing models demonstrated but synthetic evaluation data and missing parameters reduce exact reproduction confidence
- **Low confidence:** Scalability to unseen domains and long-term robustness with changing tool APIs remain unclear

## Next Checks

1. **Real-world preference validation:** Collect human preference annotations on test cases to validate synthetic evaluation methodology and ensure claimed accuracy improvements translate to actual human preferences

2. **Tool failure simulation:** Systematically test performance degradation when tools return irrelevant/noisy results or are unavailable, quantifying dependency risk and establishing degraded condition baselines

3. **Cross-domain generalization test:** Evaluate on completely unseen domains (legal, financial, technical) without additional training to assess generalization limits and identify domain-specific failure patterns