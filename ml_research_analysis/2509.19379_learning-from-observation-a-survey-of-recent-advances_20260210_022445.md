---
ver: rpa2
title: 'Learning from Observation: A Survey of Recent Advances'
arxiv_id: '2509.19379'
source_url: https://arxiv.org/abs/2509.19379
tags:
- expert
- learning
- imitator
- state
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys recent advances in learning from observation
  (LfO), where agents learn to imitate expert behavior using only state information,
  without requiring action data. The authors present a novel taxonomy categorizing
  LfO methods based on expert identity, trajectory collection methods (first-person
  vs.
---

# Learning from Observation: A Survey of Recent Advances

## Quick Facts
- arXiv ID: 2509.19379
- Source URL: https://arxiv.org/abs/2509.19379
- Reference count: 40
- Key outcome: Presents a taxonomy categorizing LfO methods based on expert identity, trajectory collection methods, dataset types, and algorithmic design choices, organizing approaches into supervised, goal-based, reward engineering, and distribution matching categories.

## Executive Summary
This survey provides a comprehensive overview of Learning from Observation (LfO), a paradigm where agents learn to imitate expert behavior using only state information without requiring action data. The authors develop a novel taxonomy that classifies LfO methods based on four key dimensions: expert identity, trajectory collection methods (first-person vs. third-person viewpoint), dataset types, and algorithmic design choices. They organize existing LfO approaches into four main categories: supervised methods, goal-based methods that decompose imitation into selecting and reaching goals, reward engineering that derives imitation rewards from demonstrations, and distribution matching methods that align state or state-transition distributions between expert and imitator.

## Method Summary
The authors conducted a systematic literature review of 40 recent papers on Learning from Observation, analyzing algorithms through a taxonomic lens based on three trajectory construction questions (Who is the expert? How are trajectories collected? What datasets?) and four algorithmic design choices (Supervised, Goal-based, Reward Engineering, Distribution Matching). The survey method involved extracting algorithm details for each paper into a matrix and mapping them to the defined categories by identifying their loss functions, objective formulations, and dataset assumptions. The authors also connect LfO to related fields like offline RL, model-based RL, and hierarchical RL while identifying open problems such as learning from diverse expert skills and developing better evaluation metrics.

## Key Results
- LfO enables learning from expert demonstrations without action labels by leveraging state-only trajectories
- Four main algorithmic categories emerge: supervised methods (BCO), goal-based methods (decoupling meta-policy and low-level controller), reward engineering, and distribution matching (GAIfO)
- Distribution matching methods can handle test-time distribution shift better than supervised approaches but require complex adversarial training
- LfO faces challenges including dynamics mismatch, third-person viewpoint adaptation, and lack of standardized evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Inverse Dynamics Modeling for Action Label Synthesis
- **Claim:** If an agent can learn the causal link between state transitions and actions, it can convert state-only observations into a standard supervised learning problem.
- **Mechanism:** The agent first interacts with the environment using an exploratory policy to train an Inverse Dynamics Model (IDM) (predicting $a_t$ given $s_t, s_{t+1}$). Once trained, this model infers the likely actions the expert took to generate the observed state sequences. The agent then applies Behavior Cloning (BC) on these synthetic state-action pairs.
- **Core assumption:** The expert and imitator share identical dynamics ($T_E = T_I$), meaning the transitions observed in the demonstration are physically realizable by the imitator.
- **Break condition:** This mechanism fails under test-time distribution shift. If the BC policy makes a small error and visits a state off the expert distribution, the policy may hallucinate actions, compounding errors.

### Mechanism 2: Adversarial Distribution Matching (GAIfO)
- **Claim:** An agent can learn to imitate by maximizing the confusion of a discriminator trained to distinguish between its own state transitions and the expert's, thereby aligning occupancy measures without action labels.
- **Mechanism:** This utilizes a Generative Adversarial Network (GAN) framework. A discriminator is trained to output "expert" vs. "imitator" based on state-transition pairs $(s_t, s_{t+1})$. The generator (the policy) is updated using reinforcement learning to maximize the discriminator's error (i.e., generate transitions that look like expert transitions).
- **Core assumption:** The divergence between state-transition occupancy measures is a sufficient proxy for behavioral similarity. It assumes that matching the probability of visiting state pairs forces behavioral alignment.
- **Break condition:** Fails when dynamics mismatch exists ($T_E \neq T_I$). If the expert transitions are physically impossible for the imitator, the algorithm cannot converge to the expert distribution.

### Mechanism 3: Goal-Conditioned Decoupling
- **Claim:** Imitation is more robust if separated into "where to go" (meta-policy) and "how to get there" (low-level controller).
- **Mechanism:** The policy is split into two sub-modules. A meta-policy (goal predictor) analyzes the expert observation to select a target state. A low-level controller (often a goal-conditioned policy or IDM) executes primitive actions to reach that target.
- **Core assumption:** Expert behavior can be decomposed into reachable sub-goals. It assumes the imitator has the capacity to reach the goals selected by the meta-policy.
- **Break condition:** Feasibility errors. If the meta-policy selects an expert state that is visually similar but kinematically infeasible for the imitator (e.g., due to morphology differences), the low-level controller will fail.

## Foundational Learning

- **Concept:** Occupancy Measure ($\mu^\pi$)
  - **Why needed here:** The mathematical foundation of LfO (specifically distribution matching) relies on matching the stationary distribution of states or state-transitions, not just single-step rewards.
  - **Quick check question:** Can you explain why matching the state-occupancy measure $\mu(s)$ might result in an imitator spinning counter-clockwise while the expert spun clockwise?

- **Concept:** Inverse Dynamics Models (IDM)
  - **Why needed here:** IDMs are the bridge from observation to action. They allow the system to infer the missing action labels required for supervised learning or goal-conditioned control.
  - **Quick check question:** If an agent has never visited a specific state $s_{rare}$ during its exploration, how will its IDM perform when the expert visits $s_{rare}$?

- **Concept:** Offline RL & DICE (Distribution Correction Estimation)
  - **Why needed here:** To prevent the high cost of online environmental interaction, recent LfO methods utilize offline datasets. Understanding how to correct distribution shifts without online data is critical for modern implementations.
  - **Quick check question:** Why does offline LfO require "additional proxy state-action trajectories" from the imitator, as opposed to just expert state trajectories?

## Architecture Onboarding

- **Component map:** Input Buffer -> Domain Adapter (Optional) -> Action Inference Engine -> Policy Network -> Environment
- **Critical path:**
  1. Data Prep: Collect state-only demos (video/sensors)
  2. Dynamics Learning (if Supervised): Train IDM via random exploration
  3. Reward/Label Engineering: Use IDM to label data (Supervised) OR Train Discriminator to output $P(expert|s,s')$ (Adversarial)
  4. Policy Optimization: Update $\pi$ using BC loss or RL reward signal
- **Design tradeoffs:**
  - Supervised (BCO) vs. Adversarial (GAIfO): BCO is sample-efficient but brittle to distribution shift. GAIfO is robust but requires complex adversarial training and is prone to instability.
  - Online vs. Offline: Online allows learning the IDM on the fly. Offline requires pre-existing proxy datasets to ground the policy.
- **Failure signatures:**
  - Action Hallucination: Policy takes erratic actions in states slightly off the expert trajectory (Supervised approach)
  - Reward Hacking: Agent finds a way to visit expert states in a different order or context to fool the discriminator (Adversarial approach)
  - Domain Confusion: Agent imitates background pixels rather than the task-relevant object (3rd-person viewpoint failure)
- **First 3 experiments:**
  1. Validation of IDM (Supervised Baseline): Train an IDM on random transitions. Test its accuracy on a hold-out set of state-transitions.
  2. Behavior Cloning from Observation (BCO): Run the BCO algorithm on a simple control task (e.g., CartPole or Ant) to establish a baseline.
  3. GAIfO Stress Test: Implement GAIfO and test it on a "Half-Cheetah" vs "Full-Cheetah" scenario (dynamics mismatch).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can evaluation metrics be developed to distinguish between successful task completion via environmental reward and accurate behavioral imitation?
- **Basis in paper:** The authors note that most studies evaluate performance using "return at the end of an episode," which does not reflect imitation quality because "the imitator may achieve a similar return through different behaviors."
- **Why unresolved:** Relying solely on return fails to capture whether the agent is mimicking the expert's strategy or simply finding an alternative, potentially exploitative, solution.
- **What evidence would resolve it:** The development and adoption of a standardized metric that correlates with behavioral similarity (e.g., trajectory distribution overlap) independent of task reward.

### Open Question 2
- **Question:** How can LfO algorithms be modified to prioritize safety constraints over the replication of expert behavior in real-world applications?
- **Basis in paper:** The survey highlights that "adhering to safety constraints often takes precedence over merely copying the expert's behavior," identifying "Safe Imitation" as a distinct area for future research.
- **Why unresolved:** Current research predominantly centers on replicating expert behavior, often ignoring the safety trade-offs required in physical or high-stakes environments.
- **What evidence would resolve it:** Algorithms that demonstrate the ability to deviate from expert trajectories to satisfy safety constraints while still successfully completing the task.

### Open Question 3
- **Question:** How can LfO methods be adapted to handle the severe viewpoint variations and domain shifts found in internet-scale video data?
- **Basis in paper:** The authors argue that to utilize "internet scale passive resources like YouTube videos," researchers must solve LfO where demonstrations are available only in a third-person viewpoint with significant domain shifts.
- **Why unresolved:** Existing methods can generally only handle minor camera angle changes or require extensive trimming of irrelevant visual data, failing to generalize to diverse, uncurated video sources.
- **What evidence would resolve it:** An LfO model trained on diverse, uncurated third-person internet videos that successfully transfers skills to a first-person agent without manual data alignment.

### Open Question 4
- **Question:** Can foundational robotic models be effectively pre-trained using only state-only trajectories to leverage easier-to-collect data?
- **Basis in paper:** The paper suggests adapting foundation models to learn from state-only trajectories, noting that "collecting state-only trajectories are easier" than collecting the state-action pairs currently required by these models.
- **Why unresolved:** Current foundation models for robotics rely heavily on action labels, and it is unclear if the performance benefits of large-scale pre-training can be retained when action information is absent.
- **What evidence would resolve it:** A large-scale foundation model trained predominantly on state-only data that achieves competitive performance on downstream tasks compared to action-labeled counterparts.

## Limitations
- The survey's primary limitation is its dependence on the completeness of the literature corpus with only 40 references, potentially omitting recent or emerging LfO methods.
- The taxonomy, while comprehensive, may not capture all edge cases or hybrid approaches that combine multiple algorithmic strategies.
- The survey assumes the four main categories are mutually exclusive and collectively exhaustive, which may not hold for all implementations.

## Confidence
- **High Confidence:** The mathematical foundations of LfO (occupancy measures, inverse dynamics models) are well-established and accurately described.
- **Medium Confidence:** The taxonomy construction is logical but may require refinement as new methods emerge that blur category boundaries.
- **Medium Confidence:** The connections to related fields (offline RL, hierarchical RL) are well-grounded but may benefit from more empirical validation.

## Next Checks
1. **Literature Gap Analysis:** Perform a comprehensive search to identify LfO methods published after the survey's cutoff date to assess completeness.
2. **Taxonomy Stress Test:** Apply the four-category framework to recent hybrid LfO methods to identify potential classification ambiguities.
3. **Empirical Benchmark:** Implement representative methods from each category (e.g., BCO, GAIfO, and a goal-based approach) on a standard benchmark suite to validate the practical distinctions between categories.