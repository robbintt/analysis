---
ver: rpa2
title: Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement
  Learning
arxiv_id: '2510.23038'
source_url: https://arxiv.org/abs/2510.23038
tags:
- reasoning
- response
- arxiv
- learning
- judges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TIR-Judge, a framework that trains LLM judges
  with tool-integrated reinforcement learning to improve accuracy in evaluating responses.
  By combining code execution with iterative RL, TIR-Judge outperforms strong reasoning-based
  judges by up to 6.4% (pointwise) and 7.7% (pairwise), and matches 96% of Claude-Opus-4's
  performance in listwise ranking with only 8B parameters.
---

# Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.23038
- Source URL: https://arxiv.org/abs/2510.23038
- Reference count: 25
- Primary result: TIR-Judge trained with tool-integrated RL outperforms text-only baselines by 4-8% on verifiable tasks, and TIR-Judge-Zero (no distillation) matches distilled variants

## Executive Summary
This paper introduces TIR-Judge, a framework that trains LLM judges with tool-integrated reinforcement learning to improve accuracy in evaluating responses. By combining code execution with iterative RL, TIR-Judge outperforms strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and matches 96% of Claude-Opus-4's performance in listwise ranking with only 8B parameters. Notably, TIR-Judge-Zero, trained without distilled data, matches or exceeds its distilled variant, demonstrating that tool-augmented judges can self-improve through RL. The approach achieves consistent gains across seven benchmarks, showing strong generalization and efficiency.

## Method Summary
TIR-Judge integrates a Python executor into LLM judges and trains via tool-integrated reinforcement learning. The framework uses a three-component reward function (correctness, format compliance, tool execution success) with DAPO optimization. Training proceeds through iterative cycles of RL with group rollouts, rejection sampling to curate valid trajectories, and SFT consolidation. The approach handles both verifiable domains (math, coding) requiring tool use and non-verifiable domains (safety, helpfulness) where text reasoning suffices. TIR-Judge-Zero demonstrates that models can self-improve without distillation by bootstrapping tool use through RL alone.

## Key Results
- Outperforms best text-only reasoning-based judges by 4.1% on PPE, 6.4% on pointwise, 7.7% on pairwise evaluations
- Matches 96% of Claude-Opus-4's performance in listwise ranking with only 8B parameters
- TIR-Judge-Zero (no distillation) matches or exceeds distilled variant on 4/6 benchmarks
- Shows consistent gains across seven benchmarks spanning verifiable and non-verifiable domains
- Tool integration provides particular advantage on tasks requiring counting, arithmetic, and structural validation

## Why This Works (Mechanism)

### Mechanism 1: Code Execution Enables Verifiable Grounding for Judgments
Integrating a Python executor allows judges to verify constraints that text-only reasoning handles unreliably (counting, arithmetic, structural validation). The judge generates interleaved reasoning-text and code blocks; code executes in a sandbox and returns exact outputs that replace fuzzy textual estimation. This grounds decisions in executable evidence rather than parametric inference. The bottleneck is not reasoning capacity per se, but the inability of text-only models to reliably perform symbolic operations and exact checks. TIR-Judge uses `response.count('O')` to get correct counts vs text-only miscounting.

### Mechanism 2: Iterative RL Bootstraps Tool-Use Without Distillation
Models can self-improve tool-integrated reasoning through rejection sampling + SFT + RL cycles, matching or exceeding distillation-based cold starts. RL improves policy → rejection sampling curates correct trajectories → SFT consolidates → RL refines again. This progressively teaches when/how to invoke tools and when text reasoning suffices. The base model has sufficient latent tool-calling and reasoning capability; RL unlocks and shapes it rather than injecting new skills from scratch.

### Mechanism 3: Mixed Verifiable/Non-Verifiable Training Induces Conditional Tool Use
Training on both tool-appropriate (math, instruction-following) and tool-unnecessary (safety, chat) domains teaches the model to invoke tools selectively. The reward design grants format reward for no-tool responses on safety/helpfulness tasks, creating negative pressure against gratuitous tool calls. Verifiable tasks provide positive signal for tool use. Models learn a meta-policy over when to tool-call based on task type signals embedded in prompts.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO/DAPO)**
  - Why needed: TIR-Judge uses DAPO for multi-turn RL with tool execution; understanding advantage estimation from group rollouts is essential for debugging reward signals
  - Quick check: Given 4 rollouts with rewards [0, 1, 1, 1], what is the advantage for the first rollout?

- **Tool-Integrated Reasoning (TIR) Trajectories**
  - Why needed: The training data format interleaves `(reasoning, code, output)` tuples; masking execution outputs during loss computation is critical for preventing memorization
  - Quick check: Why should interpreter output tokens be masked during SFT loss computation?

- **Rejection Sampling for Self-Improvement**
  - Why needed: TIR-Judge-Zero relies on filtering correct trajectories for SFT; understanding what makes a trajectory "valid" (correct answer + format + no execution errors) is key to reproducing results
  - Quick check: If a trajectory produces the correct judgment but calls tools 5 times (exceeding the 3-call budget), should it be retained for SFT?

## Architecture Onboarding

- **Component map**: Preference pairs (HelpSteer3, UltraInteract, etc.) + synthetic verifiable pairs → ~26k total → DAPO RL with sandboxed Python executor → rejection sampling → SFT consolidation → repeat → Qwen3-8B/4B backbone

- **Critical path**: 1) Curate diverse (prompt, responses) pairs spanning verifiable and non-verifiable domains 2) Initialize from base model (Qwen3-4B/8B); if using TIR-Judge-Zero, skip distillation 3) Run RL with group rollouts (G=4–8), computing advantage across trajectories 4) Filter valid trajectories via rejection sampling (correct + format + no errors + within budget) 5) SFT on filtered trajectories, masking interpreter outputs 6) Iterate steps 3–5, selecting best checkpoint on held-out validation

- **Design tradeoffs**: Distill vs. Zero (distillation provides faster convergence but requires teacher access; Zero enables self-bootstrapping but needs more RL iterations), Tool budget (higher budgets allow complex verification but increase latency and error surface; paper uses max 3 calls), Data mixture (specialized mixtures improve domain accuracy but hurt cross-domain generalization)

- **Failure signatures**: Tool over-reliance (model calls tools for safety/chat tasks where text suffices → check format reward enforcement), Interaction collapse (model generates excessive tool calls without terminating → check tool-specific reward and max-call enforcement), Memorization of outputs (model overfits to specific execution results → verify output masking in loss), Stagnant bootstrap (TIR-Judge-Zero shows no improvement across iterations → check base model code capability and rejection sampling yield)

- **First 3 experiments**: 1) Ablate tool integration: Train identical RL setup with tools disabled; expect ~4–8% drop on PPE/IFBench, slight gain on Chat/Safety 2) Validate iterative benefit: Compare TIR-Judge-Zero after 0, 1, 2 RL iterations; expect progressive gains 3) Test data mixture sensitivity: Train on single-domain subsets; expect poor transfer and confirm unified mixture necessity

## Open Questions the Paper Calls Out

- **Can TIR-Judge be used to enhance policy model training (e.g., as a reward signal for RLHF), and does tool-integrated judging improve downstream policy performance compared to text-only judges?**
  - Basis: The conclusion states: "In future work, we aim to... explore using TIR-Judge to enhance policy model training"
  - Unresolved: Paper evaluates TIR-Judge on judging benchmarks but does not study its integration into the full RLHF training loop for policy optimization

- **How does expanding beyond Python code execution to additional tools (e.g., web search, database queries, formal verifiers) affect TIR-Judge's accuracy and generalization?**
  - Basis: The conclusion states: "In future work, we aim to expand the range of tools and training tasks used in RL"
  - Unresolved: Current framework only integrates a Python code executor; other tool types remain unexplored

- **Under what conditions does TIR-Judge-Zero (pure RL without distillation) outperform TIR-Judge-Distill, and can we predict when distillation is unnecessary or harmful?**
  - Basis: Table 1 shows TIR-Judge-Zero outperforms the distilled variant on 4/6 benchmarks but underperforms on 2/6, with no clear pattern explained
  - Unresolved: Paper demonstrates this phenomenon empirically but does not analyze which task characteristics determine whether distillation helps

## Limitations

- Training data heavily emphasizes verifiable domains (math, coding, instruction-following) comprising ~18k of 26k pairs, raising questions about robustness to novel evaluation scenarios
- Paper does not quantify computational overhead from sandbox execution, where each tool call introduces latency and potential failure modes
- TIR-Judge-Zero's success assumes base Qwen3 models possess latent tool-calling capability that RL can unlock, untested on base models with weaker code generation abilities

## Confidence

- **High confidence**: Core empirical claims showing TIR-Judge outperforming text-only baselines by 4-8% on verifiable tasks, and demonstration that TIR-Judge-Zero matches distilled variants
- **Medium confidence**: Generalization claims across all seven benchmarks, given the domain imbalance in training data
- **Medium confidence**: The mechanism explaining why tool integration helps (verification vs parametric inference), as this is inferred from design rather than directly measured

## Next Checks

1. **Domain Transfer Test**: Evaluate TIR-Judge on held-out non-verifiable domains not present in training data to quantify true generalization beyond the documented seven benchmarks.

2. **Overhead Characterization**: Measure inference latency and failure rate per tool call across different evaluation scenarios to establish the practical cost-benefit tradeoff.

3. **Base Model Sensitivity**: Repeat TIR-Judge-Zero training with base models of varying code generation capabilities (e.g., Llama-3, Gemini-1.5-Flash) to test the bootstrap mechanism's robustness to architectural differences.