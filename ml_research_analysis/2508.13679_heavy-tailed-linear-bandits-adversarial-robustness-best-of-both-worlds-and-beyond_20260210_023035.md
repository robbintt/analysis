---
ver: rpa2
title: 'Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds,
  and Beyond'
arxiv_id: '2508.13679'
source_url: https://arxiv.org/abs/2508.13679
tags:
- heavy-tailed
- regret
- adversarial
- linear
- bandits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing algorithms for
  adversarial heavy-tailed bandits, a setting where losses can be heavy-tailed and
  chosen adversarially. Existing approaches struggle with this problem due to difficulties
  in handling the potential bias introduced by loss clipping, particularly on the
  optimal arm, and the lack of finite support of the losses.
---

# Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond

## Quick Facts
- arXiv ID: 2508.13679
- Source URL: https://arxiv.org/abs/2508.13679
- Reference count: 40
- This paper proposes a general framework for adversarial heavy-tailed bandits that achieves best-of-both-worlds regret bounds for multi-armed bandits and linear bandits.

## Executive Summary
This paper addresses the challenge of designing algorithms for adversarial heavy-tailed bandits, where losses can be heavy-tailed and chosen adversarially. Existing approaches struggle with bias introduced by loss clipping, particularly on the optimal arm, and the lack of finite support of the losses. The authors propose a general framework using follow-the-regularized-leader (FTRL) over clipped loss estimates with a carefully designed bonus function that cancels bias while keeping regret nearly optimal.

The key contributions include the first FTRL-type best-of-both-worlds algorithm for heavy-tailed MABs without truncated non-negativity assumption, achieving improved regret bounds in both adversarial and stochastic regimes, and the first algorithm for adversarial heavy-tailed linear bandits with finite arm sets that matches the best-known worst-case regret bound in stochastic regimes.

## Method Summary
The paper proposes a general framework for adversarial heavy-tailed bandits that performs follow-the-regularized-leader (FTRL) over clipped loss estimates shifted by a carefully designed bonus function. The bonus function is crucial as it helps cancel the bias introduced by loss clipping, especially on the optimal arm, while keeping its magnitude controlled to ensure overall regret remains (nearly) optimal. The framework is applied to both multi-armed bandits and linear bandits, with specific algorithms developed for each setting that achieve improved regret bounds compared to existing approaches.

## Key Results
- Heavy-tailed MABs: First FTRL-type best-of-both-worlds algorithm achieving $\widetilde{O}(T^{1/\varepsilon})$ worst-case regret in adversarial regime and $\widetilde{O}(\log T)$ gap-dependent regret in stochastic regime, improving current best results by factors of $O(\log K \cdot \log^4 T)$ and $O(\log K \cdot \log^3 T)$ respectively.
- Adversarial heavy-tailed linear bandits: First algorithm with finite arm sets achieving $\widetilde{O}(d^{1/2}T^{1/\varepsilon})$ regret, matching the best-known worst-case regret bound in stochastic regimes.
- Heavy-tailed Noise Aware Stability-Penalty Matching (HT-SPM): General data-dependent bound framework for the problem.

## Why This Works (Mechanism)
The mechanism works by using FTRL with clipped loss estimates and a carefully designed bonus function. The bonus function cancels the bias introduced by clipping, particularly on the optimal arm, while being small enough to maintain optimal regret bounds. For linear bandits, the framework extends by incorporating the linear structure into the bonus function design while maintaining the bias cancellation property.

## Foundational Learning

1. **Follow-the-Regularized-Leader (FTRL)**
   - Why needed: Core algorithmic framework for bandit optimization
   - Quick check: Verify FTRL update rule with regularization term

2. **Loss Clipping in Heavy-tailed Settings**
   - Why needed: Robustness to extreme values in adversarial setting
   - Quick check: Confirm clipping threshold selection preserves statistical properties

3. **Bias Cancellation Techniques**
   - Why needed: Essential for maintaining optimal regret despite clipping
   - Quick check: Validate bonus function correctly cancels clipping-induced bias

4. **Best-of-Both-Worlds Regret Analysis**
   - Why needed: Achieving optimal performance in both adversarial and stochastic regimes
   - Quick check: Verify gap-dependent and gap-independent regret bounds

5. **Linear Bandit Structure**
   - Why needed: Extending results from MAB to linear case
   - Quick check: Confirm linear structure is properly incorporated into algorithm

## Architecture Onboarding

Component Map: Clipped Losses -> Bonus Function -> FTRL Update -> Action Selection

Critical Path: Data Collection → Loss Clipping → Bonus Computation → FTRL Update → Action Selection → Regret Calculation

Design Tradeoffs: The framework balances between aggressive clipping for robustness and careful bonus design for bias cancellation, trading off computational complexity for improved regret bounds.

Failure Signatures: If bonus function is poorly designed, bias cancellation fails leading to suboptimal regret; if clipping is too aggressive, statistical efficiency is lost.

First 3 Experiments:
1. Verify MAB algorithm achieves claimed regret bounds on synthetic heavy-tailed data
2. Test linear bandit algorithm on finite arm set problems with varying dimensions
3. Compare HT-SPM framework against existing heavy-tailed bandit approaches on standard benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- The linear bandit results require finite arm sets, limiting applicability to more general linear bandit settings
- The framework relies on complex bonus function designs without full algorithmic details provided
- Improvement factors over existing methods lack clear specification of comparison baselines

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| General framework structure and MAB applications | High |
| Specific regret bounds and improvement factors | Medium |
| Technical details of bonus function design and HT-SPM mechanism | Low |

## Next Checks

1. Request full algorithmic pseudocode for the bonus function design to verify the claimed bias cancellation properties
2. Compare the linear bandit results against existing stochastic and adversarial linear bandit algorithms to validate the claimed matching of best-known bounds
3. Examine the HT-SPM framework's performance on standard benchmark problems to assess its generality claims beyond the specific applications presented