---
ver: rpa2
title: 'Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods
  for Fairness Exploration'
arxiv_id: '2505.00802'
source_url: https://arxiv.org/abs/2505.00802
tags:
- race
- fairness
- status
- marital
- occupation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of local post-hoc explanation methods
  to detect and interpret algorithmic bias. The authors propose a pipeline that generates
  local explanations for demographic groups and aggregates them to derive fairness-related
  insights.
---

# Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration

## Quick Facts
- arXiv ID: 2505.00802
- Source URL: https://arxiv.org/abs/2505.00802
- Authors: Vasiliki Papanikou; Danae Pla Karidi; Evaggelia Pitoura; Emmanouil Panagiotou; Eirini Ntoutsi
- Reference count: 40
- Primary result: Local post-hoc explanation methods can detect and interpret algorithmic bias by aggregating individual explanations across demographic groups

## Executive Summary
This paper investigates whether local post-hoc explanation methods (LIME, SHAP, DiCE) can effectively detect algorithmic bias by analyzing feature contributions across demographic groups. The authors propose a pipeline that generates local explanations for individual instances, aggregates them by demographic group, and compares these insights against traditional distributive fairness metrics. Using datasets like Adult and COMPAS, they find that explanation methods can reveal procedural unfairness consistent with statistical fairness violations, though the choice of aggregation strategy significantly impacts interpretation. The study demonstrates both the potential and limitations of XAI for bias detection, highlighting critical design considerations for fairness analysis.

## Method Summary
The proposed pipeline integrates local post-hoc explanation methods to derive fairness-related insights from black-box models. The approach involves training a Random Forest classifier on datasets with protected attributes (sex, race), then applying LIME, SHAP, and DiCE to generate explanations for individual instances. These local explanations are aggregated by demographic group using various strategies (mean, mean absolute, burden) to reveal feature contribution disparities. The aggregated insights are compared against distributive fairness metrics (Demographic Parity, Equal Opportunity, Equalized Odds). The study also evaluates explanation method robustness using AOPC (Area Over Perturbation Curve) and investigates how removing protected attributes shifts influence to correlated proxy features.

## Key Results
- Explanation methods successfully detected procedural unfairness that correlated with distributive fairness violations across demographic groups
- Removing protected attributes from models shifted feature contributions to correlated proxy features, often maintaining bias indirectly
- Different aggregation strategies (mean vs. mean absolute) significantly impacted the interpretation of fairness disparities
- AOPC evaluation showed explanation methods are reliable but require careful application for bias detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating local post-hoc explanations by demographic group can reveal procedural unfairness that mirrors distributive fairness violations.
- Mechanism: The pipeline generates local explanations (feature attributions or counterfactuals) for each instance in a protected and non-protected group. These local insights are then aggregated (e.g., by averaging contributions or measuring feature change frequencies) to provide a group-level view of which features drive model decisions for each group. A systematic disparity in feature contributions (e.g., a protected attribute or a proxy feature consistently pushing a protected group toward unfavorable outcomes) signals a procedural fairness issue.
- Core assumption: The post-hoc explanation methods (like LIME, SHAP, DiCE) provide a faithful and consistent approximation of the model's decision-making logic for individual instances.
- Evidence anchors:
  - [abstract] "We propose a pipeline that integrates local post-hoc explanation methods to derive fairness-related insights."
  - [section 4.2] Describes how LIME/SHAP feature contributions are aggregated and compared between groups (e.g., male/female), showing protected attributes having polarized contributions consistent with distributive fairness violations.
- Break condition: Explanation methods produce inconsistent or highly divergent results for similar instances within the same group.

### Mechanism 2
- Claim: Removing a protected attribute from a model's inputs often shifts its influence to correlated proxy features, a form of indirect discrimination that explanation methods can help detect.
- Mechanism: A model is trained with and without the protected attribute (e.g., 'sex'). Explanation methods are then applied to both models. By comparing the feature attributions, one can observe if the contribution of the removed protected attribute has been redistributed to other features that are highly correlated with it (e.g., 'marital status' as a proxy for 'sex'). This reveals that the model relies on the information from the protected attribute indirectly.
- Core assumption: Proxy features exist in the dataset and are sufficiently correlated with the protected attribute.
- Evidence anchors:
  - [abstract] "Removing protected attributes shifts feature contributions to correlated proxy features, often maintaining bias."
  - [section 4.3] "We notice an increase in the contributions of features that are correlated with sex... such as Marital Status and those indirectly reflecting gender, such as Relationship."
- Break condition: The correlations between the protected attribute and other features are extremely weak.

### Mechanism 3
- Claim: Evaluating the quality and robustness of explanation methods (e.g., using AOPC) provides a necessary, though not sufficient, basis for trusting their insights for fairness analysis.
- Mechanism: To ensure explanations are meaningful, they are subjected to an evaluation like the Area Over the Perturbation Curve (AOPC). This involves ranking features by their attributed importance from an explanation and then sequentially removing (perturbing) them from the input to measure the drop in model performance.
- Core assumption: Perturbation-based metrics like AOPC are valid proxies for explanation quality and faithfulness, despite known limitations like generating out-of-distribution samples.
- Evidence anchors:
  - [section 4.4] "We observe that all methods outperform the random baseline, with LIME and SHAP having steeper slopes... This suggests that the features identified by the explanations are indeed important for the model, providing evidence that we can trust the feature rankings."
- Break condition: The explanation method is susceptible to adversarial attacks or can be easily manipulated to produce misleading rankings while still performing well on AOPC.

## Foundational Learning

- **Group Fairness Metrics (Distributive Fairness)**
  - Why needed here: To establish a ground truth for bias. The paper compares insights from explanations (procedural fairness) against these standard statistical metrics (Demographic Parity, Equal Opportunity, Equalized Odds) to see if they tell the same story.
  - Quick check question: If a model has Equal Opportunity, what must be true about the True Positive Rate (TPR) for Group A and Group B?

- **Post-hoc Explanation Methods (LIME, SHAP, DiCE)**
  - Why needed here: These are the core tools used in the paper to "open the black box." Understanding their distinct approaches—approximation (LIME), game-theoretic contribution (SHAP), and minimal change (DiCE)—is essential to interpret the experimental results.
  - Quick check question: Which explanation method would tell a user *what to change* about their loan application to get approved?

- **Procedural vs. Indirect Discrimination**
  - Why needed here: The paper's RQ2 investigates the link between them. Understanding that indirect discrimination is bias carried by proxy features is key to interpreting the experiment where a protected attribute is removed.
  - Quick check question: If we remove 'race' from a model but it still discriminates based on 'zip code,' what is 'zip code' an example of?

## Architecture Onboarding

- **Component map**: Black-Box Model -> Explanation Generator -> Group Aggregator -> Fairness Comparator
- **Critical path**: The most sensitive path is from Explanation Generator to Group Aggregator. The choice of explanation method and, crucially, the aggregation strategy (e.g., mean vs. mean absolute) determines whether directional bias is visible or hidden.
- **Design tradeoffs**:
  - Choice of Explanation Method: LIME/SHAP show contribution magnitude/direction; DiCE shows actionable recourse. The paper finds LIME often shows more pronounced disparities.
  - Aggregation Strategy: Averaging signed contributions (mean) reveals whether a feature helps or hurts a group, but magnitudes may cancel out. Averaging absolute values (mean abs) shows overall influence but hides direction.
  - Perturbation for AOPC: The paper replaces removed features with their mean. This is simple but can create unrealistic data points.
- **Failure signatures**:
  - Procedural-Distributive Mismatch: Explanations show no feature attribution disparity, but distributive fairness metrics show strong group outcome disparities.
  - Aggregation Washout: Group-level graphs show similar feature contributions, but a scatter plot of individual explanations would show a bimodal distribution (some helped, some hurt) within the group.
  - Explanation Instability: Applying the explanation method to the same instance yields highly different feature rankings.
- **First 3 experiments**:
  1. **Baseline Correlation Check**: Reproduce the RQ1 experiment. Train a classifier on a dataset with known bias (e.g., Adult), compute distributive fairness metrics, and then generate and aggregate LIME explanations for protected vs. non-protected groups.
  2. **Proxy Feature Audit**: Reproduce the RQ2 experiment. Retrain the model after removing the protected attribute. Use SHAP to generate explanations for the new model and specifically check if the contribution of the top correlated proxy features has increased.
  3. **Aggregation Sensitivity Analysis**: For the same set of explanations, generate group-level fairness reports using both the mean and the mean absolute value of feature contributions.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the presentation of aggregated local explanations influence user perception of procedural unfairness compared to standard metrics?
  - Basis in paper: The authors explicitly state that "conducting a user study would be valuable for assessing procedural unfairness from the users’ perspective."
  - Why unresolved: The current study relies on algorithmic and statistical correlations; human interpretation of feature attributions involves subjective cognitive factors and trust dynamics not captured by computational metrics.
  - What evidence would resolve it: Results from controlled user studies measuring perceived fairness and trust after participants view aggregated explanations versus standard distributive fairness reports.

- **Open Question 2**: Can the fairness of the explanation methods themselves be formally defined and measured?
  - Basis in paper: The authors list a need to "evaluate the fairness of the explanations themselves" as a distinct area of future work.
  - Why unresolved: While the paper shows explanations can detect model bias, the explanation generation algorithms (LIME, SHAP, DiCE) are susceptible to data biases and lack ground truth, raising concerns about their own impartiality.
  - What evidence would resolve it: The development of metrics to quantify bias within the explanation generation process, validating that explanations do not systematically disadvantage specific demographic groups.

- **Open Question 3**: Do group counterfactuals or global explanation methods provide more consistent fairness insights than aggregated local explanations?
  - Basis in paper: The authors propose to "explore additional explanation methods, such as group counterfactuals and global explanations" to extend their pipeline.
  - Why unresolved: The current pipeline relies on aggregating individual instance-level explanations, which may smooth out disparities or miss structural biases that are inherently global or group-based.
  - What evidence would resolve it: Comparative experiments analyzing the stability and detection accuracy of bias when using global/group-based methods versus the local aggregation strategies tested in this paper.

## Limitations

- The reliance on the faithfulness of post-hoc explanation methods, with AOPC evaluation using mean imputation that can generate out-of-distribution samples
- Small sample sizes (100 instances per group) for explanation generation may not capture full model complexity
- Focus on tabular datasets with binary protected attributes limits generalizability to more complex scenarios

## Confidence

- **High Confidence**: The core mechanism that aggregated local explanations can reveal procedural fairness issues consistent with distributive fairness violations. The Adult dataset experiments provide clear evidence of this relationship.
- **Medium Confidence**: The claim about protected attributes shifting to proxy features when removed. While the paper shows evidence in the Adult dataset, the effect size and generalizability across different model types and datasets require further validation.
- **Medium Confidence**: The AOPC-based evaluation of explanation method quality. While all methods outperform random baselines, the perturbation approach has known limitations, and high AOPC scores don't guarantee reliable bias detection.

## Next Checks

1. **Cross-dataset validation**: Replicate the main experiments (RQ1-RQ3) on a different biased dataset (e.g., COMPAS) to verify if the observed relationships between procedural and distributive fairness hold across domains.

2. **Aggregation strategy impact analysis**: Systematically compare how different aggregation methods (mean, median, trimmed mean, distribution-based approaches) affect the detection of bias, particularly for cases where feature contributions cancel out in simple averaging.

3. **Robustness testing under adversarial conditions**: Test whether explanation methods can be manipulated to hide bias by adding correlated noise features or by training models that explicitly minimize explanation fidelity while maintaining biased outcomes.