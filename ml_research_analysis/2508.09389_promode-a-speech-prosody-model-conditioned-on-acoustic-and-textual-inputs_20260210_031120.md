---
ver: rpa2
title: 'ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs'
arxiv_id: '2508.09389'
source_url: https://arxiv.org/abs/2508.09389
tags:
- prosody
- speech
- promode
- energy
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMode, a zero-shot, stand-alone prosody
  model built on the Perceiver IO architecture for speech synthesis. ProMode takes
  masked acoustic and textual features as input and predicts F0 and energy for masked
  regions, leveraging contextual information from unmasked segments.
---

# ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs
## Quick Facts
- arXiv ID: 2508.09389
- Source URL: https://arxiv.org/abs/2508.09389
- Reference count: 0
- Zero-shot prosody model using Perceiver IO with dual decoders achieves RPA of 43.9%, RCA of 43.6%, and RMSE of 21.5% for F0 prediction on GigaSpeech

## Executive Summary
This paper introduces ProMode, a zero-shot prosody model built on the Perceiver IO architecture for speech synthesis. ProMode takes masked acoustic and textual features as input and predicts F0 and energy for masked regions, leveraging contextual information from unmasked segments. Unlike many existing prosody models tightly coupled with specific downstream tasks, ProMode is designed to be general-purpose and can be used in various applications like TTS.

The model employs a dual-decoder approach in the Perceiver-based architecture, combining both conditional and unconditional decoders along with an auxiliary acoustic-only loss to enhance its ability to capture prosodic variations. Experimental results on GigaSpeech demonstrate ProMode's superior performance compared to several baselines in predicting both F0 and energy. When integrated into a TTS system, ProMode leads to significantly better prosody perception of the synthesized speech.

## Method Summary
ProMode uses a Perceiver IO encoder with dual decoders (conditional and unconditional) to predict F0 and energy for masked speech regions. The model takes masked acoustic features (mel-spectrogram bins <380Hz, denoised energy) and textual features (phoneme durations from MFA, speaker embeddings from ECAPA2) as input. Training uses 60% phoneme-boundary-aligned masking with combined losses (L1 for F0, MSE for mel-energy, BCE for VUV) from both decoders. The auxiliary acoustic-only loss (AOL) from the unconditional decoder is critical to prevent decoder collapse. Model is trained for 250k iterations on GigaSpeech M subset with batch size 32.

## Key Results
- RPA of 43.9%, RCA of 43.6%, and RMSE of 21.5% for F0 prediction on GigaSpeech test set
- Superior performance compared to baselines including ACN-F0 and TTS-T0-F0 across all metrics
- Consistent improvements at different levels of granularity (frame/phone levels with DTW alignment)
- Subjective listening tests show strong preference for ProMode-integrated TTS over baseline systems

## Why This Works (Mechanism)
The dual-decoder architecture with auxiliary acoustic-only loss prevents the conditional decoder from collapsing into text-only attention patterns. The Perceiver IO's ability to handle variable-length inputs efficiently allows effective masking and reconstruction of prosodic features. The modified adaLN-zero mechanism with cross-attention enables fine-grained control over prosody prediction by learning scale and shift parameters per timestep.

## Foundational Learning
**Perceiver IO architecture**: Neural network that can handle arbitrary input and output modalities through cross-attention mechanisms; needed for flexible masking and reconstruction of speech features; quick check: verify encoder-decoder attention patterns during training.
**Dual-decoder training**: Using both conditional (text-aware) and unconditional (text-agnostic) decoders with shared encoder; needed to provide auxiliary supervision and prevent collapse; quick check: monitor decoder outputs during ablation of AOL.
**Length-regulated text features**: Converting variable-length text to frame-level features matching audio duration; needed for proper alignment between text and acoustic inputs; quick check: verify phoneme durations match ground truth after MFA processing.

## Architecture Onboarding
**Component map**: Audio preprocessing -> ConvNeXt V2 feature extraction -> Perceiver IO encoder -> Dual decoders (PD1 unconditional, PD2 conditional) -> F0/energy/VUV prediction
**Critical path**: Input masking (60% phoneme-boundary) -> ConvNeXt V2 preprocessing -> Perceiver encoder with cross-attention -> Dual decoder outputs with AOL loss -> Combined loss optimization
**Design tradeoffs**: Uses ground truth durations instead of predicted ones to avoid error propagation; trades off some real-world applicability for cleaner evaluation; employs dual decoders to prevent collapse rather than stronger regularization alone.
**Failure signatures**: Decoder collapse (ignores prosody embeddings, attends only to text); training instability from improper loss weighting; poor F0 prediction from misaligned durations.
**First experiments**: 1) Train with AOL ablation to confirm ~26% performance drop; 2) Test modified adaLN-zero sensitivity to layer depth; 3) Verify masking ratio alignment with phoneme boundaries.

## Open Questions the Paper Calls Out
- How does ProMode perform on tonal languages where pitch carries lexical meaning rather than just prosodic information?
- How robust is the model when integrated into a fully generative pipeline that relies on predicted rather than ground-truth phoneme durations?
- Can the proposed architecture generalize to speech editing tasks where the acoustic context is discontinuous or synthesized?

## Limitations
- Exact implementation details of modified adaLN-zero cross-attention mechanism are underspecified
- Experimental validation relies heavily on automated metrics that may not fully capture perceptual quality
- Uses ground truth durations instead of predicted ones, limiting real-world applicability

## Confidence
- High confidence in core architectural contribution and empirical finding that AOL is critical for preventing decoder collapse
- Medium confidence in quantitative results given strong baseline comparisons but limited evaluation methodology detail
- Low confidence in precise implementation details required for exact reproduction

## Next Checks
1. **AOL ablation verification**: Train ProMode with and without the auxiliary acoustic-only loss from the unconditional decoder (PD1) while keeping all other components identical. Measure the degradation in RPA, RCA, and RMSE metrics to confirm whether the ~26% performance drop reported in the paper is reproducible.

2. **Cross-attention architecture isolation**: Implement a minimal version of the modified adaLN-zero mechanism with controlled variations in network depth (2-4 layers) and width (128-512 dimensions) to determine the sensitivity of performance to these architectural choices and identify the critical components.

3. **Baseline reproduction and comparison**: Implement the two strongest baselines (ACN-F0 and TTS-T0-F0) following the same training protocol and evaluation setup as ProMode, using identical preprocessing, masking ratios, and loss functions to verify the claimed performance improvements are robust across different model architectures.