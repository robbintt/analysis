---
ver: rpa2
title: Reinforcement Learning for Long-Horizon Multi-Turn Search Agents
arxiv_id: '2510.24126'
source_url: https://arxiv.org/abs/2510.24126
tags:
- search
- learning
- tool
- answer
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that reinforcement learning (RL) can significantly
  improve the performance of LLM agents on complex multi-turn document search tasks.
  Using a legal document search benchmark, the authors train a 14B parameter model
  with RL from verifiable rewards, achieving 85% accuracy - outperforming frontier
  API-based models (78%) and the base model (53%).
---

# Reinforcement Learning for Long-Horizon Multi-Turn Search Agents

## Quick Facts
- arXiv ID: 2510.24126
- Source URL: https://arxiv.org/abs/2510.24126
- Reference count: 27
- Primary result: RL-trained 14B parameter model achieves 85% accuracy on legal document search, outperforming frontier API models (78%) and base model (53%)

## Executive Summary
This paper demonstrates that reinforcement learning significantly improves LLM agents' performance on complex multi-turn document search tasks. Using a legal document search benchmark with verifiable rewards, the authors train a 14B parameter model and achieve state-of-the-art results. The study explores turn-restricted regimes during both training and inference, revealing that RL-trained agents benefit more from longer multi-turn interactions than prompt-based approaches alone.

## Method Summary
The authors employ reinforcement learning from verifiable rewards to train a 14B parameter model on a legal document search benchmark. The RL approach uses a reward signal based on the accuracy of document retrieval across multiple search turns. The training explores different turn-restricted regimes to understand how limiting the number of search turns affects learning efficiency and final performance. The reward function provides direct feedback on retrieval accuracy, enabling the model to learn effective exploration strategies for complex multi-turn searches.

## Key Results
- RL-trained model achieves 85% accuracy on legal document search benchmark
- Outperforms frontier API-based models (78%) and base model (53%)
- RL agents show greater benefit from longer multi-turn interactions compared to prompt-based approaches

## Why This Works (Mechanism)
The paper demonstrates that RL enables agents to develop superior exploration strategies and better utilize multi-turn capabilities. The verifiable reward signal provides direct feedback on retrieval accuracy, allowing the model to learn which search strategies are most effective. During turn-restricted training, the authors find that limited turns create insufficient positive trajectories for comparison, suggesting that trajectory diversity is crucial for effective learning. The mechanism appears to involve the model learning to balance exploration and exploitation across multiple search turns to maximize retrieval accuracy.

## Foundational Learning
- **Reinforcement Learning from Verifiable Rewards**: Needed to train models when ground truth answers are available but the path to discovery is uncertain. Quick check: Verify reward signal provides sufficient gradient for learning complex multi-turn strategies.
- **Multi-turn Search Strategy**: Required for navigating complex document collections where single queries are insufficient. Quick check: Ensure turn transitions preserve relevant context for subsequent searches.
- **Exploration-Exploitation Tradeoff**: Essential for balancing between trying new search strategies and refining known successful approaches. Quick check: Monitor whether the model explores sufficiently diverse search paths during training.
- **Trajectory Diversity in RL Training**: Critical for providing the model with varied examples of successful and unsuccessful search patterns. Quick check: Analyze trajectory distribution to ensure adequate positive examples across different search strategies.

## Architecture Onboarding

**Component Map**: Document Retriever -> Action Generator -> Search Environment -> Reward Calculator -> RL Optimizer

**Critical Path**: Search Query Generation -> Document Retrieval -> Result Evaluation -> Reward Assignment -> Policy Update

**Design Tradeoffs**: The paper uses a single 14B parameter model for both search strategy and query generation, trading off computational efficiency for unified training. This choice enables end-to-end optimization but requires significant computational resources.

**Failure Signatures**: Models trained with limited turns fail to learn effectively due to insufficient positive trajectories. This manifests as poor performance regardless of inference turn allowance. Additionally, reward signal design must be carefully calibrated to provide meaningful gradients for multi-turn decision making.

**First 3 Experiments**:
1. Baseline comparison: Evaluate base model performance without RL training
2. Turn-restricted training analysis: Train models with 1, 2, and 3-turn limits to measure learning efficiency
3. Inference turn allowance study: Test RL-trained models with varying inference turn limits to assess multi-turn benefit utilization

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to document domains with different structures or reward signal properties
- 14B parameter model size represents significant computational investment
- Single-task optimization approach may not transfer to related search tasks
- Mechanism behind multi-turn benefit utilization remains under-specified

## Confidence
- RL improves multi-turn search performance: **High confidence** (controlled benchmark, verifiable rewards)
- Turn-restricted training effects: **Medium confidence** (relationship between trajectory diversity and convergence unclear)
- Multi-turn benefit mechanism: **Medium confidence** (empirical support but mechanism under-specified)

## Next Checks
1. Test RL approach on diverse document domains (medical literature, technical documentation) to assess domain transfer
2. Implement parameter-efficient variant (LoRA or adapters) to evaluate performance gains with lower computational cost
3. Conduct ablation studies comparing different trajectory sampling strategies during turn-restricted training to isolate positive trajectory diversity effects on learning efficiency