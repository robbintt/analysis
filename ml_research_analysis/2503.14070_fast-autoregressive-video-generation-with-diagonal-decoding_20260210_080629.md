---
ver: rpa2
title: Fast Autoregressive Video Generation with Diagonal Decoding
arxiv_id: '2503.14070'
source_url: https://arxiv.org/abs/2503.14070
tags:
- video
- diagd
- generation
- decoding
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Diagonal Decoding (DiagD) is a training-free acceleration algorithm\
  \ for autoregressive video generation that generates tokens along diagonal paths\
  \ in the spatial-temporal token grid, enabling parallel decoding within each frame\
  \ and partially overlapping across consecutive frames. The method exploits spatial\
  \ and temporal correlations in videos to achieve up to 10\xD7 speedup compared to\
  \ naive sequential decoding while maintaining comparable visual fidelity."
---

# Fast Autoregressive Video Generation with Diagonal Decoding

## Quick Facts
- arXiv ID: 2503.14070
- Source URL: https://arxiv.org/abs/2503.14070
- Reference count: 40
- Primary result: DiagD achieves up to 10× speedup for autoregressive video generation by decoding tokens along diagonal paths in the spatial-temporal token grid.

## Executive Summary
Diagonal Decoding (DiagD) is a training-free acceleration algorithm for autoregressive video generation that generates tokens along diagonal paths in the spatial-temporal token grid, enabling parallel decoding within each frame and partially overlapping across consecutive frames. The method exploits spatial and temporal correlations in videos to achieve up to 10× speedup compared to naive sequential decoding while maintaining comparable visual fidelity. Experiments on multiple autoregressive video generation models (Cosmos, WHAM, and MC-AR) and datasets demonstrate DiagD's versatility across different tasks (video continuation, text-to-video generation), model scales, and tokenizers.

## Method Summary
DiagD modifies the standard sequential next-token prediction paradigm by generating tokens along diagonal paths in the spatial-temporal token grid. Instead of generating tokens left-to-right, top-to-bottom sequentially, the model generates all tokens where `i + j = constant` simultaneously within each frame. The method introduces two key parameters: `k` (spatial neighbors) controlling how many spatial predecessors are available, and `d` (temporal delay) controlling how many diagonal steps must complete in frame t before starting frame t+1. The approach includes a cost-effective fine-tuning strategy that aligns attention patterns with the decoding order, particularly beneficial for small-scale models.

## Key Results
- Achieves up to 10× speedup compared to naive sequential decoding (e.g., Cosmos-12B from 7.68k to 0.18k steps)
- Maintains comparable visual fidelity across diverse settings and model scales
- Reduces inference latency while preserving generation quality across video continuation and text-to-video tasks
- Fine-tuning for ~1k steps significantly reduces performance degradation for small models

## Why This Works (Mechanism)

### Mechanism 1: Spatial Parallel Decoding via Diagonal Token Groups
- Claim: Tokens along spatial diagonals within a frame can be generated in parallel because spatial neighbors exhibit stronger dependencies than sequential (raster-scan) neighbors.
- Mechanism: Instead of generating tokens left-to-right, top-to-bottom sequentially, the model generates all tokens where `i + j = constant` simultaneously. Token `ct,i,j` depends primarily on already-generated tokens satisfying `ct,≤i,≤j+k`.
- Core assumption: Visual patches correlate more strongly with spatially adjacent patches than with their sequential predecessor in raster order.
- Evidence anchors: [abstract]: "generates tokens along diagonal paths in the spatial-temporal token grid, enabling parallel decoding within each frame"; [section 3.2]: "the first patch in each row is more related to the first patch in the previous row than to the last patch in the same row"

### Mechanism 2: Temporal Overlap via Cross-Frame Diagonal Extension
- Claim: Temporal redundancy in videos allows initiating generation of frame t+1 before frame t is complete, because early tokens in frame t+1 depend less on late tokens in frame t.
- Mechanism: The temporal delay parameter `d` controls how many diagonal steps must complete in frame t before starting frame t+1. Setting `d < s_spatial` enables overlapping generation across frames.
- Core assumption: Consecutive video frames at the same spatial position are highly correlated.
- Evidence anchors: [abstract]: "partially overlapping across consecutive frames"; [section 3.2]: "due to the temporal redundancy of videos, patches from consecutive frames that occupy similar relative positions are highly likely to be similar"

### Mechanism 3: Attention Alignment via Lightweight Fine-tuning
- Claim: Fine-tuning with diagonal-aligned attention masks for ~1k steps reduces training-inference distribution mismatch, improving quality for smaller models.
- Mechanism: Standard causal attention masks train models to attend to all previous tokens in raster order. Diagonal decoding changes the available context at inference. Fine-tuning with masks reflecting diagonal generation order aligns attention patterns with the new decoding trajectory.
- Core assumption: The training-inference gap matters more for smaller models with limited capacity to generalize across context patterns.
- Evidence anchors: [abstract]: "cost-effective finetuning strategy that aligns the attention patterns of the model with our decoding order, particularly beneficial for small-scale models"; [section 3.3]: "finetuning for just 1k steps significantly reduces performance degradation"

## Foundational Learning

- **Autoregressive Token Prediction**: Why needed: DiagD modifies the standard sequential next-token prediction paradigm; understanding how autoregressive models factor joint distributions is prerequisite.
  - Quick check: Can you explain why Equation 2 (`pθ(c) = ∏ pθ(ci | c1, ..., ci-1)`) implies sequential generation, and how DiagD modifies which conditioning tokens are available?

- **Visual Tokenization (VAE/VQ-VAE)**: Why needed: Videos are encoded into discrete tokens before autoregressive modeling; the compression ratio directly affects the speedup potential.
  - Quick check: Given a 16-frame video at 640×1024 resolution with 16× spatial and 8× temporal compression, how many tokens does it produce? (Answer from paper: 12,800 tokens)

- **Attention Masking in Transformers**: Why needed: The fine-tuning strategy modifies causal attention masks; understanding how masks control token visibility is essential.
  - Quick check: What does a standard causal mask allow token at position i to attend to, and how would a diagonal-aware mask differ?

## Architecture Onboarding

- **Component map**: Raw video -> Discrete VAE/VQ-VAE -> Token grid (T frames × h × w tokens) -> Autoregressive Transformer with causal attention -> DiagD Scheduler -> Generated tokens -> VAE decoder -> Reconstructed video

- **Critical path**:
  1. Initialize with prompt tokens (conditioning frames + optional text)
  2. At each DiagD step, identify tokens satisfying diagonal condition: generate all `ct,i,j` where `(t-1)*d + (i-1)*k + j == current_step`
  3. Forward pass generates all eligible tokens in parallel
  4. Repeat until all T×h×w tokens generated
  5. Decode tokens to video frames

- **Design tradeoffs**:
  - **k=1 vs k=2**: k=1 maximizes speedup (~10×) but may degrade quality on smaller models; k=2 provides quality buffer at ~5× speedup
  - **d = k×h vs d = s_spatial**: Full temporal overlap (d=k×h) maximizes speedup; d=s_spatial disables temporal acceleration (spatial-only variant for multimodal outputs like WHAM)
  - **With vs without fine-tuning**: 1k-step fine-tuning helps small models; large models (>10B) typically don't need it

- **Failure signatures**:
  - **Blurry frames**: May indicate training-inference gap; try increasing k or applying fine-tuning
  - **Temporal inconsistency**: d set too low for high-motion content; increase d
  - **Quality drop on small models**: Expected per paper; apply fine-tuning or increase k
  - **No speedup**: Verify batch size=1 and that parallel token generation is actually occurring

- **First 3 experiments**:
  1. **Sanity check**: Run NTP (naive next-token prediction) baseline on Cosmos-12B video continuation, measure FVD and FPS. Then apply DiagD with k=2, d=k×h. Expect ~10× FPS increase with FVD within 1-2 points of baseline.
  2. **Hyperparameter sweep**: On a smaller model (e.g., 4B), test k∈{1,2,4} with d=k×h. Plot FVD vs FPS to identify acceptable speed-quality operating points.
  3. **Fine-tuning validation**: Train MC-AR 300M model, compare DiagD k=2 with and without 1k-step diagonal attention fine-tuning. Expect ~5-10 FVD improvement from fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can models trained from scratch using the DiagD attention mask outperform pre-trained models that are only fine-tuned with DiagD?
- **Basis in paper:** [explicit] The paper states in the contributions that fine-tuning "provides inspiration for training video generation models in the future work."
- **Why unresolved:** The current study focuses on DiagD as a training-free inference acceleration or a light fine-tuning strategy on existing checkpoints, but does not explore training initialization with the diagonal attention pattern.
- **What evidence would resolve it:** Comparative experiments training an autoregressive video model from scratch with diagonal masking versus standard causal masking, evaluated on visual fidelity metrics.

### Open Question 2
- **Question:** What is the minimum model capacity required to maintain visual fidelity with aggressive diagonal decoding settings (e.g., $k=1$) without fine-tuning?
- **Basis in paper:** [inferred] Table 1 shows the 4B Cosmos model degrades significantly at $k=1$ while the 12B model does not, and the analysis notes that larger models better "tolerate the training-inference gap."
- **Why unresolved:** While the paper demonstrates that scale mitigates the gap, it does not define the specific scaling laws or parameter thresholds that ensure stability for high-speed configurations.
- **What evidence would resolve it:** An ablation study sweeping model sizes (e.g., 300M to 12B) on the same dataset to identify the precise inflection point where aggressive DiagD matches NTP quality.

### Open Question 3
- **Question:** How does the error accumulation of DiagD compare to sequential decoding when generating videos significantly longer than the tested 100 frames?
- **Basis in paper:** [inferred] The introduction highlights the ability of AR models to generate "arbitrary length" videos, but the WHAM experiments are limited to 100 frames and 10 seconds.
- **Why unresolved:** DiagD relies on partial temporal context (controlled by $d$) to predict future frames; it is unclear if this approximation causes divergence or drift in ultra-long video generation compared to full-context NTP.
- **What evidence would resolve it:** Evaluation of long-form video generation (e.g., 500+ frames) analyzing temporal consistency and error propagation rates relative to ground truth.

## Limitations

- The spatial correlation assumption may not hold universally across all video content types
- Temporal correlation generalization is limited for high-motion scenes
- Fine-tuning necessity is only validated on limited model scales
- Speedup quantification may not account for all computational overheads

## Confidence

**High Confidence**: The basic mechanism of diagonal token generation and its ability to enable parallel decoding within frames is well-supported by the paper's mathematical formulation and attention visualizations.

**Medium Confidence**: The extension to temporal overlap and the effectiveness of the fine-tuning strategy are supported by experiments but have limited validation scope.

**Low Confidence**: The paper's claims about the generality of spatial and temporal correlation assumptions across all video types, and the assertion that fine-tuning is "cost-effective" without comparing to alternative approaches, are not well-established.

## Next Checks

1. **Content Sensitivity Analysis**: Systematically evaluate DiagD performance across diverse video categories (high-motion sports, complex textures, static scenes, low-resolution content) to quantify how spatial and temporal correlation assumptions affect quality.

2. **Alternative Alignment Methods**: Compare the proposed fine-tuning strategy against other attention alignment approaches (adapter-based fine-tuning, prompt tuning, or training-time curriculum learning) on a broader range of model scales and architectures.

3. **Implementation-Independent Speedup Validation**: Conduct controlled experiments varying batch size, sequence length, and hardware to establish the relationship between theoretical token-level parallelism and actual wall-clock speedup.