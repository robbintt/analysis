---
ver: rpa2
title: Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs
arxiv_id: '2506.12182'
source_url: https://arxiv.org/abs/2506.12182
tags:
- arxiv
- reasoning
- biomedical
- prompting
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how prompt design and lightweight fine-tuning\
  \ affect the performance of open-source large language models on biomedical question\
  \ answering. The study focuses on two prompting strategies\u2014standard instruction\
  \ prompts and Chain-of-Thought (CoT) prompts\u2014combined with parameter-efficient\
  \ instruction tuning using QLoRA on the PubMedQA dataset."
---

# Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs

## Quick Facts
- arXiv ID: 2506.12182
- Source URL: https://arxiv.org/abs/2506.12182
- Reference count: 31
- Primary result: CoT prompting improves zero-shot reasoning but may degrade performance when combined with fine-tuning in larger models

## Executive Summary
This paper investigates how prompt design and lightweight fine-tuning affect the performance of open-source large language models on biomedical question answering. The study focuses on two prompting strategies—standard instruction prompts and Chain-of-Thought (CoT) prompts—combined with parameter-efficient instruction tuning using QLoRA on the PubMedQA dataset. Experiments across multiple model families and sizes reveal that CoT prompting alone can improve reasoning in zero-shot settings, while instruction tuning consistently boosts accuracy. However, fine-tuning on CoT prompts does not universally enhance performance and may even degrade results for certain larger models. These findings suggest that the benefits of reasoning-aware prompts are model- and scale-dependent, highlighting the need for nuanced prompt-model alignment in medical question answering applications.

## Method Summary
The study employs QLoRA with 4-bit quantization for parameter-efficient fine-tuning on PubMedQA, a multiple-choice biomedical QA dataset. Four model sizes (Llama-3.1-8B, Llama-3.3-70B, Qwen2.5-7B, Qwen2.5-14B) are trained for one epoch using AdamW optimizer with learning rate 2×10⁻⁴ and linear decay. Two prompt formats are tested: standard instruction prompts and Chain-of-Thought prompts with "Think:" prefix. Performance is evaluated using accuracy and weighted F1 score on the test set, with final letter answers extracted from model outputs.

## Key Results
- CoT prompting alone improves zero-shot F1 in 3 out of 4 models
- Instruction tuning consistently increases accuracy across all models (+1.0% to +8.0%)
- Fine-tuning on CoT prompts may degrade performance in larger models (e.g., Qwen2.5-14B F1 drops from 0.6760 to 0.6087)
- Smaller models (8B) benefit most from combined CoT+SFT approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought (CoT) prompting scaffolds latent reasoning capabilities in base models during zero-shot inference.
- Mechanism: CoT prompts introduce a "Think:" prefix that structures intermediate reasoning steps before the final answer. This external scaffolding guides the model through explicit decomposition without modifying weights, activating reasoning pathways that may otherwise remain dormant in standard instruction prompts.
- Core assumption: Models possess latent multi-step reasoning capabilities that can be elicited through prompt structure alone.
- Evidence anchors:
  - [abstract] "CoT prompting alone can improve reasoning in zero-shot settings"
  - [section IV.C] "In 3 out of 4 models, adding CoT to base models increases F1, suggesting that CoT can help models handle ambiguous or nuanced biomedical questions"
  - [corpus] Related work on Short-Path Prompting (FMR=0.50) corroborates reasoning instability analysis in CoT approaches, suggesting scaffolded prompting affects reasoning reliability
- Break condition: When models already internalize reasoning processes (observed in Llama-3.3-70B), CoT scaffolding provides diminishing or negative returns.

### Mechanism 2
- Claim: Parameter-efficient instruction fine-tuning via QLoRA consistently improves task alignment across model families and scales.
- Mechanism: QLoRA quantizes base model weights to 4-bit precision while learning low-rank adaptation matrices. This enables domain-specific adaptation to biomedical QA patterns using limited supervised data, shifting model behavior toward instruction-following without catastrophic forgetting or prohibitive compute costs.
- Core assumption: The PubMedQA training distribution contains sufficient signal for biomedical reasoning alignment without requiring full parameter updates.
- Evidence anchors:
  - [abstract] "instruction tuning significantly boosts accuracy"
  - [section IV.C] "All models show performance gains after SFT, with accuracy increases ranging from +1.0% (Qwen2.5-7B CoT) to +8.0% (Llama-3.1-8B Default)"
  - [corpus] MedBioLM paper (FMR=0.53) demonstrates similar gains from fine-tuned LLMs for medical QA, supporting transferability of this mechanism
- Break condition: None observed for instruction tuning alone; gains were consistent across all tested configurations.

### Mechanism 3
- Claim: Combining CoT-style supervision with instruction fine-tuning produces model- and scale-dependent effects that may degrade performance in larger models.
- Mechanism: Fine-tuning on CoT-formatted data imposes explicit reasoning structure on model representations. For smaller models (e.g., Llama-3.1-8B), this creates synergistic alignment. For larger models with stronger internal reasoning (e.g., Llama-3.3-70B, Qwen2.5-14B), externally imposed reasoning formats may conflict with learned representations, causing misalignment and reduced generalization.
- Core assumption: Larger models develop internal reasoning schemas during pre-training that can conflict with explicitly supervised CoT patterns.
- Evidence anchors:
  - [abstract] "fine-tuning on CoT prompts does not universally enhance performance and may even degrade it for certain larger models"
  - [section IV.D] "Qwen2.5-14B shows a drop in F1 from 0.6760 (Base CoT) to 0.6087 (CoT + SFT), indicating potential misalignment between CoT-style reasoning and instruction-tuned representations"
  - [section IV.E] "For larger models, the interaction can be neutral or even detrimental... Over-constraining a model with verbose or unnatural reasoning patterns during fine-tuning may hinder its generalization ability"
  - [corpus] Mixture of Reasonings paper (FMR=0.54) discusses adaptive reasoning strategies, suggesting fixed CoT formats may not suit all model configurations
- Break condition: Observed break condition—the mechanism fails when scale exceeds a threshold where internal reasoning is already sufficient.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Core intervention tested; understanding how explicit reasoning scaffolding differs from direct answering is essential for interpreting results.
  - Quick check question: Given a medical QA prompt, can you distinguish between a standard instruction format and one that elicits step-by-step reasoning?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT) / QLoRA**
  - Why needed here: The paper uses QLoRA exclusively; understanding low-rank adaptation and quantization is prerequisite to reproducing or extending this work.
  - Quick check question: How does 4-bit quantization with learned low-rank adapters differ from full fine-tuning in memory footprint and update capacity?

- Concept: **Prompt-Model Alignment**
  - Why needed here: The central finding—that CoT benefits are scale-dependent—requires understanding how prompt structure interacts with model capacity and pre-training.
  - Quick check question: Why might a 70B parameter model respond differently to CoT supervision than an 8B model?

## Architecture Onboarding

- Component map:
  - PubMedQA dataset (Context + Question + Options) -> Prompt formatting (Standard or CoT) -> QLoRA model (4-bit quantized) -> Fine-tuning (1 epoch) -> Evaluation (Accuracy + F1)

- Critical path:
  1. Format PubMedQA training samples into Standard or CoT prompt structure
  2. Apply QLoRA configuration to target model
  3. Fine-tune for one epoch (assumption: early stopping not used based on config description)
  4. Run inference with matching prompt format (Standard or CoT)
  5. Extract final letter answer; discard reasoning output for scoring

- Design tradeoffs:
  - **Standard vs. CoT prompts**: CoT improves zero-shot but adds inference overhead and may conflict with SFT in larger models
  - **Model scale selection**: Larger models (70B) achieve higher base performance but show diminishing or negative returns from CoT+SFT; smaller models (8B) benefit most from combined approach
  - **Single-epoch training**: Reduces overfitting risk but may underutilize training signal; paper does not report multi-epoch ablations

- Failure signatures:
  - F1 drop after CoT+SFT compared to Base CoT (observed in Qwen2.5-14B: 0.6760 → 0.6087)
  - CoT+SFT underperforming Default+SFT in same model (observed in Llama-3.3-70B: 0.7366 vs 0.7420)
  - Large accuracy variance across prompt formats post-tuning indicates prompt-model misalignment

- First 3 experiments:
  1. **Baseline replication**: Run zero-shot inference with Standard prompts on all four model sizes; record Accuracy and F1 to establish base performance floor.
  2. **CoT zero-shot test**: Add CoT prompting to baseline; identify which models show F1 improvement (expect 3/4 per paper findings) to confirm reasoning scaffolding effect.
  3. **Scale-dependent ablation**: Fine-tune smallest (8B) and largest (70B) models using QLoRA on CoT-formatted data; compare Default+SFT vs CoT+SFT deltas to validate the degradation hypothesis for larger scales.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a multi-stage training regime involving CoT pretraining prior to full instruction tuning mitigate the performance degradation observed in larger models?
- Basis in paper: [explicit] The authors explicitly list "Multi-stage training with CoT pretraining before full instruction tuning" as a target for future work.
- Why unresolved: The current study employed a single-stage tuning process, which resulted in performance drops for larger models (e.g., Qwen2.5-14B) when CoT was used, suggesting potential misalignment.
- What evidence would resolve it: Comparative experiments showing that separating CoT pretraining from instruction tuning restores or improves F1 scores in larger parameter models compared to simultaneous tuning.

### Open Question 2
- Question: Is the reasoning generated in the "Think:" sections faithful to the model's actual decision-making process, or is it post-hoc rationalization?
- Basis in paper: [explicit] The conclusion identifies the need for "Faithfulness and reasoning quality evaluation for Think: outputs" as a critical next step.
- Why unresolved: The current methodology evaluates only the final answer accuracy, leaving the validity and logical consistency of the intermediate reasoning steps unverified.
- What evidence would resolve it: Human or automated evaluation of the generated reasoning chains to ensure they logically support the correct answer without hallucinations.

### Open Question 3
- Question: How does the integration of Retrieval-Augmented Generation (RAG) interact with CoT prompting in grounded biomedical QA?
- Basis in paper: [explicit] The authors propose exploring "Combining retrieval-based methods (RAG) with CoT prompting for grounded biomedical QA."
- Why unresolved: This study relied on fixed contexts provided by the dataset; it remains unclear how dynamic retrieval affects the stability and accuracy of CoT reasoning.
- What evidence would resolve it: Experiments benchmarking standard vs. CoT prompting on PubMedQA where context is dynamically retrieved rather than pre-provided.

## Limitations

- Mechanism 3 scale threshold is empirically observed but not theoretically grounded - The paper identifies a performance degradation for CoT+SFT in larger models but does not establish a clear theoretical boundary or scaling law that predicts when this effect occurs.
- Single-epoch training regime limits generalization insights - With only one epoch of fine-tuning, the study cannot distinguish between transient effects and stable performance patterns.
- CoT training data provenance is unspecified - The paper states CoT-formatted data was used for fine-tuning but provides no details on whether these reasoning steps were manually annotated or automatically generated.

## Confidence

- High confidence: Mechanism 2 (instruction tuning consistently improves performance) - This finding is directly supported by quantitative results showing gains across all model sizes and prompt types.
- Medium confidence: Mechanism 1 (CoT scaffolding benefits zero-shot reasoning) - Supported by empirical improvements in 3/4 models, but lacks theoretical explanation for why scaffolding works.
- Low confidence: Mechanism 3 (CoT+SFT degrades larger models) - Based on limited observations in specific model pairs; could reflect data quality issues, hyperparameter sensitivity, or prompt-model misalignment rather than a fundamental scaling phenomenon.

## Next Checks

1. **Hyperparameter sensitivity ablation**: Re-run fine-tuning experiments with varied LoRA ranks (8, 16, 32) and alphas (16, 32, 64) to determine whether CoT-induced degradation in larger models persists across different parameter-efficient fine-tuning configurations.

2. **Multi-epoch training study**: Extend fine-tuning to 3-5 epochs for both small and large models under all prompt conditions, tracking validation performance curves to distinguish transient from stable effects and identify optimal stopping points.

3. **Reasoning quality evaluation**: Implement automated scoring of intermediate reasoning traces (e.g., using semantic similarity to ground truth reasoning patterns) to determine whether CoT degradation in larger models reflects poorer reasoning quality or simply format misalignment with expected answer patterns.