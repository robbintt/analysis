---
ver: rpa2
title: Learning from Discriminatory Training Data
arxiv_id: '1912.08189'
source_url: https://arxiv.org/abs/1912.08189
tags:
- learning
- discrimination
- data
- protected
- discriminatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of preventing discrimination in
  supervised learning models when training data may be tainted by historical bias.
  The authors formalize this as a dataset shift problem where training data (potentially
  discriminatory) differs from test data (non-discriminatory).
---

# Learning from Discriminatory Training Data

## Quick Facts
- arXiv ID: 1912.08189
- Source URL: https://arxiv.org/abs/1912.08189
- Reference count: 40
- Key outcome: OIM method achieves higher accuracy (up to 0.7) and lower demographic disparity than state-of-the-art fair learning methods across synthetic and real-world datasets

## Executive Summary
This paper addresses the challenge of preventing discrimination in supervised learning models when training data contains historical bias. The authors formalize this as a dataset shift problem where training data may be discriminatory while test data is expected to be fair. They propose the Optimal Interventional Mixture (OIM) method, which probabilistically intervenes on protected attributes to minimize cross-loss between potentially biased training data and fair test data. The method provably minimizes error under additive discriminatory concept shifts and handles multiple protected attributes and intersectionality.

The OIM achieves superior performance compared to existing fair learning approaches, demonstrating higher accuracy and lower demographic disparity across multiple datasets including COMPAS, German Credit, and CelebA. The method is computationally lightweight and designed to be compatible with existing legal frameworks while avoiding discrimination through proxy variables.

## Method Summary
The OIM method addresses dataset shift between potentially discriminatory training data and non-discriminatory test data by modeling the shift as an additive concept shift on protected attributes. It learns a probabilistic intervention policy that maps training samples to counterfactual fair samples, then trains on this mixture of original and intervened data. The method optimizes for minimal cross-loss between training and test distributions while maintaining fairness constraints. Unlike existing approaches that rely on regularization or adversarial debiasing, OIM directly models the intervention mechanism and provides theoretical guarantees under additive concept shift assumptions.

## Key Results
- Achieves up to 0.7 higher accuracy compared to state-of-the-art fair learning methods
- Demonstrates lower demographic disparity across all evaluated datasets
- Outperforms baseline methods on both synthetic and real-world datasets (COMPAS, German Credit, CelebA)
- Successfully handles multiple protected attributes and intersectionality without performance degradation

## Why This Works (Mechanism)
The OIM works by explicitly modeling the causal relationship between protected attributes and outcomes, then intervening on these attributes to create a counterfactual fair distribution. By training on a mixture of original and intervened samples, the model learns representations that generalize to both biased and fair distributions. The probabilistic intervention allows smooth interpolation between preserving useful signal from the original data and enforcing fairness constraints.

## Foundational Learning
- Dataset shift theory: Why needed - understanding how training and test distributions differ; Quick check - verify KL divergence between distributions is non-zero
- Causal inference fundamentals: Why needed - modeling the intervention mechanism; Quick check - validate causal graph structure matches domain knowledge
- Counterfactual reasoning: Why needed - generating fair alternative samples; Quick check - ensure intervened samples maintain logical consistency
- Mixture model training: Why needed - combining original and intervened data; Quick check - monitor mixture weight convergence during training
- Cross-validation under distribution shift: Why needed - evaluating generalization to fair test data; Quick check - compare training and test loss curves

## Architecture Onboarding
Component map: Raw Data -> Protected Attribute Detection -> Intervention Policy Learning -> Mixture Generation -> Model Training -> Fair Predictions

Critical path: The intervention policy learning stage is critical as it determines how effectively the model can bridge the gap between biased and fair distributions. Poor intervention policies lead to either insufficient fairness or loss of predictive signal.

Design tradeoffs: The method trades computational complexity for theoretical guarantees. While more computationally intensive than simple regularization approaches, it provides provable error bounds under specific shift assumptions.

Failure signatures: The method may fail when the additive concept shift assumption is violated, when protected attributes have complex non-linear relationships with outcomes, or when the intervention policy over-corrects and removes useful predictive information.

Three first experiments:
1. Validate the intervention policy by visualizing the distribution shift before and after intervention
2. Test model performance sensitivity to mixture weight hyperparameters
3. Compare cross-loss convergence between OIM and baseline fair learning methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Theoretical guarantees assume additive concept shifts which may not hold in all real-world scenarios
- Focus on classification tasks limits applicability to regression or ranking problems
- Assumes test data is non-discriminatory, which may not reflect persistent systemic bias
- Does not address trade-offs between fairness and other performance metrics beyond accuracy

## Confidence
- High confidence in mathematical formulation and algorithmic approach
- Medium confidence in empirical results given the mix of synthetic and real-world datasets
- Low confidence in legal framework compatibility claims without detailed legal analysis

## Next Checks
1. Test OIM on larger-scale datasets with more complex feature spaces to verify computational efficiency claims
2. Conduct ablation studies to quantify the impact of each component of the OIM method
3. Perform a more rigorous comparison with non-probabilistic fair learning methods to establish the unique value proposition of the OIM approach