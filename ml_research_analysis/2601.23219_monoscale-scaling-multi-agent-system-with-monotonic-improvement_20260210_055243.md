---
ver: rpa2
title: 'MonoScale: Scaling Multi-Agent System with Monotonic Improvement'
arxiv_id: '2601.23219'
source_url: https://arxiv.org/abs/2601.23219
tags:
- agent
- agents
- system
- router
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MonoScale tackles the cold-start misrouting problem in multi-agent
  systems during agent expansion. It proactively synthesizes agent-conditioned warm-up
  tasks to probe new agents' capabilities and failure modes, then distills evidence
  into natural-language routing memories.
---

# MonoScale: Scaling Multi-Agent System with Monotonic Improvement

## Quick Facts
- arXiv ID: 2601.23219
- Source URL: https://arxiv.org/abs/2601.23219
- Authors: Shuai Shao; Yixiang Liu; Bingwei Lu; Weinan Zhang
- Reference count: 40
- Primary result: Prevents performance collapse when scaling MAS from 3 to 10 agents by proactively synthesizing agent-conditioned warm-up tasks and distilling execution evidence into routing memories.

## Executive Summary
MonoScale addresses the cold-start misrouting problem in multi-agent systems during agent expansion. It proactively synthesizes agent-conditioned warm-up tasks to probe new agents' capabilities and failure modes, then distills evidence into natural-language routing memories. These memories guide future routing decisions while preventing performance collapse. Experiments on GAIA and HLE benchmarks show that MonoScale enables a smaller open-weight router to outperform proprietary baselines and achieve stable improvements as the agent pool grows from 3 to 10 agents, while naive scale-up exhibits performance degradation.

## Method Summary
MonoScale implements a four-stage expansion protocol: (1) Agent-conditioned task synthesis generates ~50 warm-up tasks per new agent via planner-executor-validator loop; (2) Tasks execute in parallel, filtering unsolvable ones; (3) Success/failure evidence distills into structured routing memories ("do/don't" principles); (4) Trust-region memory update selects optimal routing policy with conservative fallback ensuring monotonic non-decreasing performance. The router is a frozen LLM whose behavior is modulated by an editable text memory, shifting policy through prompt injection rather than weight updates.

## Key Results
- Prevents performance collapse when scaling MAS from 3 to 10 agents (naive scaling shows accuracy degradation)
- Smaller open-weight router (Qwen-30B) outperforms proprietary baselines on GAIA and HLE benchmarks
- Robust to noisy agent pools with malfunctioning workers through memory-encoded "do not use" constraints

## Why This Works (Mechanism)

### Mechanism 1: Agent-Conditioned Warm-up Probing
- Synthesizing tasks tailored to a new agent's description exposes capability boundaries and failure modes before live deployment, reducing cold-start errors.
- The synthesized task distribution is assumed to be a valid proxy for real deployment contexts regarding the new agent's usage.
- Core evidence: "proactively synthesizes agent-conditioned warm-up tasks to probe new agents' capabilities and failure modes" (abstract).

### Mechanism 2: Natural Language Memory Distillation
- Distilling interaction traces into structured natural-language memory allows a frozen router LLM to dynamically adjust routing policy without weight updates.
- Assumes the frozen LLM has sufficient instruction-following capabilities to adhere to complex, textual routing constraints consistently.
- Core evidence: "distills evidence into natural-language routing memories. These memories guide future routing decisions" (abstract).

### Mechanism 3: Conservative Trust-Region Updates
- Enforcing a trust-region constraint on memory updates and including a "disable new agent" fallback guarantees performance does not degrade (theoretically).
- Assumes expansions are "non-interfering"—adding a new agent does not alter the execution environment for existing agents.
- Core evidence: Theorem 4.3 proves monotonicity using the conservative fallback and KL constraint.

## Foundational Learning

- **Contextual Bandits:** The paper frames MAS expansion as a sequence of contextual bandit problems where the action space (agents) grows. Quick check: How does the "action space" change when a new agent is added, and why does that destabilize a static router?
- **Trust Region Optimization (TRPO):** The memory update logic borrows the surrogate objective and KL constraint from TRPO. Quick check: Why is minimizing KL-divergence between the old and new routing policy critical for preventing collapse during expansion?
- **Cold Start in Routing:** The core failure mode is the router not knowing the new agent's reliability. Quick check: Why is a new agent's "Agent Card" insufficient for safe routing without execution-grounded evidence?

## Architecture Onboarding

- **Component map:** New Agent Card -> Task Synthesizer -> Warm-up Tasks -> Execution Traces -> Memory Manager -> Routing Memories -> Router
- **Critical path:** Input: New Agent Card + Existing Memory → Synthesis: Generate 50 warm-up tasks → Execution: Run tasks, collect success/fail traces → Distillation: Extract "do/don't" principles → Update: Optimize memory selection
- **Design tradeoffs:** Warm-up cost (~50 tasks per agent), memory size (must fit context window), conservatism (high reliance on fallback limits aggressive use of new capabilities but ensures safety)
- **Failure signatures:** Collapse (accuracy drops as N increases), mis-routing (delegating video tasks to audio-only agents), context loss (forgetting constraints from earlier expansion rounds)
- **First 3 experiments:** 1) Reproduce Collapse: Scale agent pool from 3 to 10 on GAIA using static router; 2) Ablation (Memory): Run MonoScale with warm-up but without distilling memory; 3) Noisy Pool Test: Inject malfunctioning agent to verify memory encodes "do not use" constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MonoScale be adapted to support parallel onboarding for agent catalogs containing thousands to millions of agents under a strict cost budget?
- Basis: The authors state continuous and parallel onboarding agents at Web scale with a controllable cost budget remains an open challenge.
- Why unresolved: Current framework validated only on small pools (3–10 agents) with sequential expansion.
- What evidence would resolve it: Experiments demonstrating efficient warm-up and stable scaling in a system managing thousands of agents.

### Open Question 2
- Question: Does integrating retrieval-based routing with MonoScale create an effective "retrieve–route–calibrate" loop for long-tail agents?
- Basis: Page 8 asks whether retrieval-based routing should be integrated to prioritize calibration for highly uncertain agents.
- Why unresolved: Current method assumes router has direct access to full agent pool rather than retrieving from massive catalog.
- What evidence would resolve it: Hybrid architecture where retrieval uncertainty directly triggers MonoScale warm-up protocol.

### Open Question 3
- Question: How does the monotonic performance guarantee hold if new agents interfere with the execution reliability of existing agents?
- Basis: Theoretical proof relies on Assumption B.1 (non-interfering expansion).
- Why unresolved: Real-world systems may suffer from resource contention or cascading errors where adding an agent impacts others.
- What evidence would resolve it: Stress tests in environments where agent addition degrades shared resources.

## Limitations
- Exact KL constraint δ value and semantic trust-region compatibility computation mechanism are unspecified
- Task synthesis prompts and memory distillation templates are referenced but not fully provided
- Memory retrieval mechanism for matching incoming tasks to stored routing principles at inference time is unspecified

## Confidence
- **High confidence:** Core mechanism of using agent-conditioned warm-up tasks to probe capabilities before deployment
- **Medium confidence:** Effectiveness of natural language memory distillation for preventing scale-up collapse
- **Medium confidence:** Theoretical monotonicity guarantee under non-interfering expansion assumption

## Next Checks
1. **KL constraint sensitivity:** Test how different δ values affect the trade-off between exploration of new agent capabilities and maintenance of baseline performance
2. **Memory retrieval ablation:** Compare performance with full memory injection versus routing solely on agent descriptions
3. **Interfering expansion stress test:** Design experiments where new agents share resources with existing ones to validate fallback mechanism under Assumption B.1 violations