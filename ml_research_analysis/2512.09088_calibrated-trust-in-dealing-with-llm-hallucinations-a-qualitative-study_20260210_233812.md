---
ver: rpa2
title: 'Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study'
arxiv_id: '2512.09088'
source_url: https://arxiv.org/abs/2512.09088
tags:
- trust
- llms
- hallucinations
- participants
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This qualitative study with 192 participants investigated how users
  perceive and respond to hallucinations in large language models (LLMs). Results
  showed that hallucination experiences do not lead to blanket mistrust but instead
  trigger context-sensitive trust calibration.
---

# Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study

## Quick Facts
- arXiv ID: 2512.09088
- Source URL: https://arxiv.org/abs/2512.09088
- Reference count: 0
- Key outcome: Hallucinations trigger context-sensitive trust calibration, not blanket mistrust; users adjust trust based on risk, stakes, and intuition.

## Executive Summary
This qualitative study with 192 participants investigated how users perceive and respond to hallucinations in large language models (LLMs). Results showed that hallucination experiences do not lead to blanket mistrust but instead trigger context-sensitive trust calibration. Users reported adjusting their trust and verification behavior based on perceived risk, decision stakes, and their own domain knowledge. A new trust-related factor—intuition—was identified, complementing established factors like expectancy, prior experience, and user expertise. Trust was found to be dynamic and recursive, shaped by continuous interaction with LLMs. Based on these insights, five practical recommendations for responsible LLM use were proposed.

## Method Summary
An online survey was conducted with 192 German-speaking participants (187 LLM users) from December 2024 to January 2025. The survey included 4 multiple-choice and 11 open-ended questions. Qualitative content analysis using Mayring's approach was applied, with iterative theory-driven coding and double-checking by another author. The anonymized dataset is publicly available.

## Key Results
- Users calibrate trust based on context-specific risk and decision stakes, not blanket mistrust
- Intuition emerges as a new factor for hallucination detection alongside expectancy, prior experience, and domain knowledge
- Trust calibration is a recursive, dynamic process shaped by continuous interaction and reflection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucination exposure triggers context-sensitive trust calibration rather than blanket mistrust.
- Mechanism: Users evaluate perceived risk and decision stakes per task, adjusting trust upward for low-consequence contexts (e.g., brainstorming) and downward for high-stakes domains (e.g., academic, medical). This calibration prevents both overtrust (uncritical acceptance) and undertrust (missed utility).
- Core assumption: Users possess sufficient metacognitive awareness to distinguish task-relevant risk levels and adjust behavior accordingly.
- Evidence anchors:
  - [abstract] "hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration"
  - [section IV.C] "Trust according to relevance" category; participants reported adjusting trust based on perceived consequences
  - [corpus] Weak direct support; neighbor papers address trust in adjacent domains (data visualization, higher education) without validating this specific calibration mechanism
- Break condition: If users lack domain knowledge to assess risk, or if hallucinations are linguistically indistinguishable from correct outputs, calibration fails and misplaced trust persists.

### Mechanism 2
- Claim: Intuition functions as a rapid plausibility filter for hallucination detection when systematic verification is infeasible.
- Mechanism: Drawing on Kahneman's System 1 processing, users apply fast, experience-based pattern recognition (e.g., linguistic coherence, internal consistency, common sense violations) to flag implausible outputs. This supplements slower, analytical verification (System 2).
- Core assumption: Users have accumulated sufficient prior LLM experience to develop reliable intuitive pattern-matching for hallucination cues.
- Evidence anchors:
  - [abstract] "identify intuition as an additional factor relevant for hallucination detection"
  - [section IV.D] Participants reported relying on intuition: "If the answers seem strange to me, I follow my gut instinct" (A8)
  - [corpus] No direct corpus validation for intuition as a trust factor in LLM contexts
- Break condition: Intuition is prone to "illusion of validity" (Kahneman), where confident-sounding outputs feel correct regardless of accuracy. Novel domains without prior experience yield unreliable intuitive judgments.

### Mechanism 3
- Claim: Trust calibration operates as a recursive process where each evaluation loop incrementally builds AI literacy.
- Mechanism: Users cycle through intention → trust decision → action → evaluation. Each loop refines expectancy, enriches prior experience, and sharpens domain knowledge. Over time, this produces more calibrated trust as a learned competence.
- Core assumption: Users reflect on outcomes and consciously update their mental models of LLM capabilities and limitations.
- Evidence anchors:
  - [abstract] "validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition"
  - [section V.C] Figure 2 visualizes the recursive loop; "calibrated trust can be understood as a user competence shaped by various cognitive and behavioral factors"
  - [corpus] Weak; no neighbor papers explicitly validate recursive trust dynamics in LLM use
- Break condition: If users experience no negative consequences from hallucinations, or lack reflective capacity, the loop fails to update trust appropriately, potentially entrenching overtrust.

## Foundational Learning

- Concept: **Calibrated trust (vs. overtrust/undertrust)**
  - Why needed here: Understanding that appropriate trust is context-dependent, not binary, is prerequisite to designing or evaluating any user-facing LLM system.
  - Quick check question: Can you explain why a user might appropriately trust an LLM for email drafting but distrust it for medical diagnosis?

- Concept: **Hallucinations as system-inherent properties**
  - Why needed here: Hallucinations cannot be fully eliminated (cited theoretical work [26], [27]); mitigation strategies must account for this irreducibility.
  - Quick check question: Why might higher linguistic fluency increase hallucination detection difficulty?

- Concept: **Dual-process theory (System 1/System 2)**
  - Why needed here: The paper explicitly maps intuition to System 1 and verification to System 2; understanding this distinction is necessary to interpret trust calibration mechanisms.
  - Quick check question: What cognitive biases might arise from over-reliance on System 1 when evaluating LLM outputs?

## Architecture Onboarding

- Component map: User intention → risk/context assessment → trust decision (intuition + analytical factors) → action (use or verify) → outcome evaluation → model update

- Critical path: User intention → risk/context assessment → trust decision (incorporating intuition + analytical factors) → action (use or verify) → outcome evaluation → model update. Failure at any stage breaks calibration.

- Design tradeoffs:
  - Prompting for explicit confidence scores may reduce overtrust but adds cognitive load
  - System-level hallucination warnings could induce undertrust and disengagement
  - Supporting intuitive detection (e.g., highlighting inconsistencies) risks false positives that erode appropriate trust

- Failure signatures:
  - Overtrust: High-stakes decisions made without verification; user cites linguistic fluency as trust basis
  - Undertrust: Avoidance of LLM use even in low-risk, high-utility scenarios
  - Stagnant calibration: No trust adjustment despite repeated hallucination exposure (broken feedback loop)

- First 3 experiments:
  1. Log verification rates by task type (high vs. low decision stakes) to validate context-sensitive verification behavior.
  2. Present participants with controlled hallucination examples and measure whether intuitive vs. analytical detection pathways yield different accuracy rates.
  3. Run a longitudinal trust calibration study tracking how trust levels evolve across multiple hallucination encounters over time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does trust calibration evolve over time with repeated LLM use, software updates, and educational interventions?
- Basis in paper: [explicit] The authors explicitly call for longitudinal studies in the Future Research section to track trust dynamics, as their data reflects only a single point of observation.
- Why unresolved: The study establishes trust as a recursive process but relies on a cross-sectional survey, which cannot measure how trust stabilizes or erodes over months or years.
- What evidence would resolve it: Longitudinal tracking of user trust levels and verification behaviors correlated with specific interaction milestones or software changes.

### Open Question 2
- Question: To what extent do self-reported verification strategies align with actual user behavior in chat logs?
- Basis in paper: [explicit] The paper notes the need to triangulate survey results with objective sources like chat logs, especially since some participants contradicted themselves regarding verification habits.
- Why unresolved: Qualitative surveys rely on subjective self-reporting, which may not capture the gap between a user's stated intention to verify and their actual real-time actions.
- What evidence would resolve it: A comparative study analyzing participants' survey claims against objective interaction logs or screen recordings of LLM usage.

### Open Question 3
- Question: How do emotional reactions (e.g., frustration, irony) specifically influence the trust calibration loop compared to cognitive assessments?
- Basis in paper: [explicit] The Future Research section suggests further investigation into emotional responses, noting that trust is shaped by emotion as well as cognition.
- Why unresolved: While the study observed emotional reactions in 9% of participants, it did not isolate the specific causal weight of these emotions on subsequent trust decisions.
- What evidence would resolve it: Experimental or qualitative analysis distinguishing the predictive power of emotional affect versus rational evaluation on future reliance behavior.

## Limitations
- Sample (N=192) consists primarily of German-speaking users recruited via convenience sampling, limiting cross-cultural applicability
- Reliance on self-reported data may introduce social desirability bias, particularly given the survey's explicit focus on hallucination experiences
- The reactive awareness effect—where participants' trust decreased after being exposed to hallucination examples during the survey—may have influenced responses

## Confidence
- **High confidence**: Users adjust trust based on context-specific risk assessment; trust calibration operates recursively over time
- **Medium confidence**: Intuition functions as a valid trust factor in hallucination detection; expectancy, prior experience, and domain knowledge influence trust decisions
- **Low confidence**: The relative weighting of trust factors across different user populations; the durability of calibration patterns across longer time periods

## Next Checks
1. Replicate the study with diverse, cross-cultural samples to assess generalizability of trust calibration patterns and intuition as a detection factor.

2. Conduct controlled experiments comparing intuitive vs. analytical detection pathways for hallucinations, measuring accuracy rates and time efficiency.

3. Implement longitudinal tracking of user trust levels across multiple hallucination encounters to validate the recursive calibration mechanism and identify potential threshold effects.