---
ver: rpa2
title: Tug-of-war between idioms' figurative and literal interpretations in LLMs
arxiv_id: '2506.01723'
source_url: https://arxiv.org/abs/2506.01723
tags:
- layers
- idiom
- interpretation
- figurative
- literal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how large language models (LLMs) process
  idiomatic expressions, which have figurative meanings that diverge from their literal
  interpretations. Using causal tracing methods, the researchers analyze a pretrained
  transformer model's mechanisms for idiom comprehension.
---

# Tug-of-war between idioms' figurative and literal interpretations in LLMs

## Quick Facts
- arXiv ID: 2506.01723
- Source URL: https://arxiv.org/abs/2506.01723
- Reference count: 40
- The study identifies three key mechanisms for idiom processing in transformers, revealing how models handle competing figurative and literal interpretations

## Executive Summary
This study investigates how large language models process idiomatic expressions through causal tracing methods applied to a pretrained transformer. The research reveals that early layers and specific attention heads retrieve figurative meanings while suppressing literal ones, with contextual disambiguation occurring immediately and later layers refining interpretations when context conflicts with retrieved meaning. The findings demonstrate that transformers employ competing pathways for idiom comprehension - an intermediate pathway prioritizing figurative meaning and a parallel direct route favoring literal interpretation.

## Method Summary
The researchers employed causal tracing methodology to analyze idiom processing mechanisms in a pretrained transformer model. This approach involved systematically identifying which components (attention heads and MLPs) contribute to figurative versus literal interpretation retrieval. The study examined how information flows through different layers, particularly focusing on early-layer processing for initial meaning retrieval and middle-layer refinement based on contextual evidence. The causal tracing method allowed researchers to map the computational pathways involved in resolving the tug-of-war between competing interpretations.

## Key Results
- Early layers and specific attention heads retrieve figurative interpretations while suppressing literal ones
- Contextual disambiguation occurs immediately, with later layers refining interpretation when context conflicts with retrieved meaning
- Competing pathways carry both interpretations - an intermediate pathway prioritizes figurative meaning while a parallel direct route favors literal interpretation

## Why This Works (Mechanism)
The mechanism works because transformers can simultaneously maintain and process multiple interpretations of ambiguous expressions. Early attention heads and MLPs activate relevant semantic features associated with both literal and figurative meanings, while later layers use contextual signals to resolve ambiguity. The model's architecture allows for parallel processing of competing interpretations, with specific pathways becoming dominant based on contextual evidence and learned associations from pretraining data.

## Foundational Learning

**Causal Tracing Methodology**: Why needed - To identify specific model components responsible for different aspects of idiom processing; Quick check - Can we systematically remove or modify identified components to test their causal role in figurative interpretation.

**Transformer Attention Mechanisms**: Why needed - Understanding how attention heads process ambiguous expressions; Quick check - Which attention heads show increased activity for figurative versus literal interpretations.

**Layer-wise Processing in Transformers**: Why needed - To understand how meaning disambiguation evolves across layers; Quick check - At which layers does contextual refinement of idiom interpretation occur.

**Autoregressive Generation**: Why needed - Context matters for idiom interpretation in sequential processing; Quick check - How does left-to-right generation affect competition between figurative and literal meanings.

## Architecture Onboarding

Component map: Input tokens -> Early attention heads/MLPs (figurative retrieval) -> Middle layers (contextual refinement) -> Output layer

Critical path: Token embedding → Early attention heads → Early MLPs → Middle layer contextual processing → Output prediction

Design tradeoffs: The model must balance immediate figurative interpretation retrieval with contextual disambiguation, creating competing pathways that may increase computational cost but improve accuracy.

Failure signatures: Incorrect idiom interpretation when contextual signals are weak, over-reliance on literal meaning in ambiguous contexts, failure to suppress dominant literal interpretations.

First experiments: 1) Remove identified crucial attention heads to test causal role in figurative interpretation, 2) Ablate middle-layer contextual processing to assess disambiguation impact, 3) Test with idiom variants to evaluate robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on a single pretrained transformer model without specifying architecture or size, limiting generalizability
- Causal tracing methodology may not capture all relevant mechanisms for idiom comprehension
- Analysis of "competing pathways" remains somewhat speculative with limited empirical validation
- Findings may not translate well to real-world applications with varied contexts and idiom frequencies

## Confidence
- Early-layer processing mechanisms: Medium
- Attention head involvement in figurative interpretation: Medium
- Competing pathway dynamics: Low
- Generalizability across architectures: Low

## Next Checks
1. Replicate the causal tracing analysis across multiple LLM architectures (GPT, BERT, T5 variants) to assess generalizability of the identified mechanisms
2. Conduct ablation studies removing specific attention heads and MLPs identified as crucial to test their causal role in figurative interpretation
3. Test the model's performance on idiom variants and novel contexts to evaluate robustness of the proposed processing mechanisms beyond the studied corpus