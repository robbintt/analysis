---
ver: rpa2
title: 'Semantic Deception: When Reasoning Models Can''t Compute an Addition'
arxiv_id: '2512.20812'
source_url: https://arxiv.org/abs/2512.20812
tags:
- llms
- level
- reasoning
- semantic
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) can\
  \ perform symbolic reasoning when faced with semantically deceptive prompts. Researchers\
  \ introduced four levels of semantic load\u2014ranging from meaningless symbols\
  \ to meaningful questions\u2014while keeping the underlying arithmetic task constant."
---

# Semantic Deception: When Reasoning Models Can't Compute an Addition

## Quick Facts
- **arXiv ID**: 2512.20812
- **Source URL**: https://arxiv.org/abs/2512.20812
- **Reference count**: 21
- **Key outcome**: Semantic load significantly degrades arithmetic accuracy in LLMs, with reasoning models most vulnerable

## Executive Summary
This study investigates whether large language models (LLMs) can perform symbolic reasoning when faced with semantically deceptive prompts. Researchers introduced four levels of semantic load—ranging from meaningless symbols to meaningful questions—while keeping the underlying arithmetic task constant. Four LLMs were tested on simple addition problems presented in altered notation, with repeated trials to ensure robustness. Results showed that as semantic load increased, model accuracy in performing the correct calculation significantly declined, even for reasoning models designed to handle complex tasks. While most models correctly identified the presence of an addition at low semantic loads, they increasingly failed at execution as context became more semantically meaningful. These findings challenge claims of human-like reasoning in LLMs and highlight their vulnerability to learned linguistic patterns.

## Method Summary
The researchers conducted experiments on four large language models using a controlled addition task with varying semantic loads. The study employed four semantic load levels: completely meaningless symbols, partially meaningful notation, meaningful questions, and direct instructions. Each model was tested on the same arithmetic problems (addition of two numbers) presented with different semantic contexts. The experiments were repeated five times per condition to ensure robustness, and accuracy was measured both for task recognition (identifying the arithmetic operation) and task execution (computing the correct result). The semantic load manipulation was carefully designed to isolate the effect of linguistic context on computational performance.

## Key Results
- Model accuracy in arithmetic computation decreased significantly as semantic load increased, even when models correctly identified the addition task
- Reasoning models with Chain-of-Thought (CoT) integration showed particular vulnerability, sometimes answering the embedded question instead of computing the sum
- The study revealed a dissociation between task recognition and task execution, where models could identify arithmetic operations but failed to compute them correctly under high semantic load
- Performance differences between models suggested that CoT may amplify rather than mitigate semantic interference effects

## Why This Works (Mechanism)
The mechanism appears to involve interference between semantic processing and arithmetic computation pathways in LLMs. When faced with semantically loaded prompts, models may activate language understanding mechanisms that conflict with or override their arithmetic processing capabilities. The presence of question formatting, meaningful context, and instruction phrases seems to trigger pattern matching against learned linguistic associations rather than pure computational reasoning. This interference is particularly pronounced in reasoning models that employ Chain-of-Thought, suggesting that the iterative reasoning process may reinforce misleading semantic patterns rather than resolve them.

## Foundational Learning
- **Semantic load**: The degree to which linguistic context provides meaning beyond the core task. Why needed: Understanding how context affects model behavior is crucial for real-world applications. Quick check: Can you identify semantic load levels in different prompts?
- **Chain-of-Thought reasoning**: An approach where models generate intermediate reasoning steps before final answers. Why needed: Many modern reasoning models use CoT, making its limitations important to understand. Quick check: Does the model show intermediate reasoning steps in its responses?
- **Task recognition vs. execution**: The ability to identify a task differs from performing it correctly. Why needed: Models may appear to understand a task without being able to execute it. Quick check: Does the model correctly identify the operation but fail at computation?
- **Arithmetic computation in LLMs**: The capacity to perform mathematical operations. Why needed: Core functionality that should be independent of semantic context. Quick check: Can the model compute simple math with neutral prompts?
- **Linguistic pattern matching**: Models' tendency to respond based on learned language patterns rather than logical reasoning. Why needed: Explains why semantic deception works. Quick check: Does changing prompt wording affect numerical answers?

## Architecture Onboarding

**Component map**: Input text -> Semantic processing layer -> Arithmetic computation module -> Output generation

**Critical path**: Semantic context (prompt) → Pattern matching → Internal representation → Calculation attempt → Answer generation

**Design tradeoffs**: The study highlights a fundamental tradeoff between linguistic understanding and mathematical computation. Models optimized for natural language understanding may sacrifice computational purity, while those focused on arithmetic may lack semantic flexibility.

**Failure signatures**: Models correctly identifying addition tasks but providing incorrect results, CoT models generating question answers instead of computations, accuracy dropping with meaningful context despite constant mathematical complexity.

**First 3 experiments**:
1. Test whether semantic load affects other arithmetic operations (subtraction, multiplication) similarly
2. Compare CoT vs non-CoT models on identical tasks to isolate CoT's specific contribution to interference
3. Conduct ablation studies removing individual semantic markers to identify most problematic components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does semantic load degrade calculation accuracy even after models correctly identify the arithmetic task?
- Basis in paper: "This result is then very interesting, as it shows that the semantic deception is applicable to the execution of the addition too. It indicates that there are hidden effects, as the semantic cues still interfere with the accuracy of the generated result."
- Why unresolved: The authors expected semantic load to only affect task recognition, not execution; the mechanism causing interference during computation remains unexplained.
- What evidence would resolve it: Attention analysis or probing studies showing how semantic representations interact with arithmetic computation within the model's forward pass.

### Open Question 2
- Question: Under what conditions does Chain-of-Thought amplifying semantic interference become counterproductive rather than beneficial?
- Basis in paper: "The fact that only o1 and r1—the two models with integrated CoT—sometimes generate answers to the sentence, and the fact that v3 is more accurate than r1 when computing the addition, may indicate that CoT could have a bad influence."
- Why unresolved: The paper observes this phenomenon but cannot determine whether repetition in CoT reinforces misleading patterns or whether other architectural factors are responsible.
- What evidence would resolve it: Controlled experiments comparing CoT vs. non-CoT models on the same task, combined with ablation studies on CoT content.

### Open Question 3
- Question: Do individual semantic markers (punctuation, question formatting, instruction phrases like "answer in one word") contribute differently to semantic deception?
- Basis in paper: "It is uncertain whether or not all symbols, like dots or questions marks, or certain combination of words, are equivalent... Further test could be done with the presence and not of symbols like dots, question marks or 'answer in one word' to investigate their impact."
- Why unresolved: The experimental design varied semantic load levels holistically without isolating individual contributors.
- What evidence would resolve it: Ablation experiments systematically adding or removing specific semantic markers while holding other factors constant.

## Limitations
- The study focuses narrowly on addition operations, which may not generalize to other mathematical tasks or broader reasoning domains
- Limited sample size with only four LLMs tested, potentially missing variations in architecture or training approaches
- Does not account for different prompting strategies, temperature settings, or training data exposure that might affect vulnerability to semantic deception

## Confidence

**High confidence**: The core finding that semantic load affects arithmetic computation accuracy is well-supported by the experimental design and repeated trials across multiple models.

**Medium confidence**: The claim that reasoning models with Chain-of-Thought are particularly vulnerable to semantic interference, as this requires more nuanced interpretation of the results and may depend on specific implementation details.

**Low confidence**: The broader implications for human-like reasoning in LLMs and the suggestion that these findings challenge current understanding of LLM capabilities, as these extend beyond the specific experimental scope.

## Next Checks
1. Replicate the study with additional mathematical operations (subtraction, multiplication, division) and more complex mathematical reasoning tasks to assess generalizability of the semantic interference effect
2. Test a wider range of LLMs with varying architectures, training approaches, and reasoning capabilities to determine if the semantic load effect is consistent across the broader LLM landscape
3. Conduct ablation studies to isolate which components of semantic load (meaningfulness, question format, etc.) contribute most significantly to performance degradation, and whether models can be fine-tuned to resist such interference