---
ver: rpa2
title: Automatic Classification of Arabic Literature into Historical Eras
arxiv_id: '2601.16138'
source_url: https://arxiv.org/abs/2601.16138
tags:
- eras
- arabic
- classification
- class
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the first comprehensive analysis of automatic
  temporal classification of Arabic literature beyond poetry. Using neural networks
  and deep learning, the authors classify Arabic texts into predefined historical
  eras and custom time periods.
---

# Automatic Classification of Arabic Literature into Historical Eras

## Quick Facts
- arXiv ID: 2601.16138
- Source URL: https://arxiv.org/abs/2601.16138
- Reference count: 15
- Primary result: F1-scores range from 0.83 (binary classification) to 0.18 (15-class classification) depending on dataset and setup

## Executive Summary
This study introduces the first comprehensive analysis of automatic temporal classification of Arabic literature beyond poetry. Using neural networks and deep learning, the authors classify Arabic texts into predefined historical eras and custom time periods. Experiments on two public datasets (OpenITI and APCD) reveal that classification performance ranges from F1-scores of 0.83 (binary classification) to 0.18 (15-class classification), depending on the dataset and setup. Fully connected feedforward networks outperform recurrent models in most cases. Notably, authorial style significantly impacts classification, with author-disjoint splits yielding lower accuracy than author-merged splits. Lemmatization improves performance for poetry but not for non-poetry texts. Confusion matrices show that eras are often misclassified with adjacent periods, suggesting gradual linguistic evolution rather than sharp historical boundaries.

## Method Summary
The authors classify Arabic literature into historical eras using neural networks on two datasets: OpenITI (prose) and APCD (poetry). They preprocess text by removing HTML tags, symbols, and punctuation, normalizing diacritics and kashida, and optionally lemmatizing with Farasa. The study uses ANN models with TF-IDF features and RNN models with word/char sequences. Two evaluation protocols are employed: author-disjoint (no author overlap) and author-merged (authors can appear in both splits). Performance is measured using macro-averaged F1-score, precision, recall, and confusion matrices. The experiments test binary classification, five predefined eras, and 15 custom 100-year bins.

## Key Results
- Classification performance ranges from F1-scores of 0.83 (binary classification) to 0.18 (15-class classification)
- Fully connected feedforward networks outperform recurrent models in most cases
- Authorial style significantly impacts classification, with author-disjoint splits yielding lower accuracy than author-merged splits
- Lemmatization improves performance for poetry but not for non-poetry texts
- Confusion matrices show adjacent eras are frequently misclassified, suggesting gradual linguistic evolution

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Distribution Shift Detection
- Claim: Neural networks can learn temporal signals by capturing systematic vocabulary distribution changes across historical periods.
- Mechanism: Models associate word frequency patterns and lexical choices with specific eras through supervised training on temporally-labeled texts. The ANN architecture with BoW/TF-IDF features directly encodes these distributional patterns.
- Core assumption: Temporal linguistic changes are sufficiently systematic and consistent to be captured as statistical patterns distinguishable from noise.
- Evidence anchors:
  - [abstract]: "The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage."
  - [section]: Table 1 documents the semantic evolution of the word "adab" across seven distinct meanings from pre-Islamic to modern eras.
  - [corpus]: Weak corpus evidence—neighbor "Fann or Flop" benchmark addresses Arabic poetry understanding across eras but focuses on LLM evaluation, not mechanism analysis.
- Break condition: If vocabulary distributions are highly similar across adjacent eras or highly variable within eras, the mechanism fails to produce sharp temporal boundaries.

### Mechanism 2: Authorial Style as Confounding Temporal Signal
- Claim: Models partially learn author-specific stylometric features that correlate with time periods rather than purely period-level linguistic signals.
- Mechanism: When authors appear in both training and evaluation splits, models leverage individual authorial patterns (vocabulary preferences, syntactic habits) as proxy features for era classification.
- Core assumption: Authors have consistent stylistic signatures within their historical context, and author distribution is not uniform across eras.
- Evidence anchors:
  - [abstract]: "authorial style significantly impacts classification, with author-disjoint splits yielding lower accuracy than author-merged splits"
  - [section]: Section 6.2 shows binary classification F1 improves from 0.668 to 0.829 on OpenITI when authors are merged across splits.
  - [corpus]: No direct corpus evidence on authorial style mechanisms in temporal classification.
- Break condition: If authors write across multiple eras or exhibit highly variable styles within their corpus, this mechanism provides unreliable and potentially misleading temporal signals.

### Mechanism 3: Adjacent Era Confusion as Evidence of Gradual Linguistic Evolution
- Claim: The pattern of systematic confusion between adjacent historical periods reflects genuine gradual linguistic change rather than solely model inadequacy.
- Mechanism: Neural models naturally produce higher misclassification rates between consecutive eras because these periods share transitional linguistic features, making them inherently difficult to distinguish with hard boundaries.
- Core assumption: Historical era boundaries are artificial constructs imposed on a continuous linguistic evolution process.
- Evidence anchors:
  - [abstract]: "Confusion matrices show that eras are often misclassified with adjacent periods, suggesting gradual linguistic evolution rather than sharp historical boundaries"
  - [section]: Section 6.3 reports that merging adjacent eras increases accuracy from 43.6% to 76.6% (OpenITI) and from 65.4% to 93.1% (APCD), supporting the continuum hypothesis.
  - [corpus]: No direct corpus evidence on this specific mechanism.
- Break condition: If certain era transitions correspond to genuine discontinuities (e.g., major political disruptions, language contact events), this mechanism may incorrectly smooth over real linguistic breaks.

## Foundational Learning

- Concept: **Sparse Text Representations (BoW and TF-IDF)**
  - Why needed here: The ANN baseline uses these representations to encode vocabulary distributions as temporal signals; understanding their properties is essential for interpreting why simpler architectures outperform sequence models.
  - Quick check question: Why would TF-IDF potentially capture era-specific vocabulary better than raw word occurrence counts?

- Concept: **Author-Disjoint vs. Author-Merged Evaluation Protocols**
  - Why needed here: This experimental design isolates whether models learn genuine temporal features versus memorizing author-specific patterns; it's critical for valid performance claims.
  - Quick check question: If a model performs identically on author-disjoint and author-merged splits, what does that suggest about what temporal features it has learned?

- Concept: **Classification Difficulty Scaling with Class Granularity**
  - Why needed here: The paper demonstrates dramatic performance degradation as class count increases (F1: 0.83 → 0.18); understanding this scaling is essential for setting realistic expectations and choosing appropriate periodization schemes.
  - Quick check question: Why would 100-year custom bins (15 classes) perform worse than linguistically-defined five eras, despite both being multi-class problems over the same time span?

## Architecture Onboarding

- Component map:
  - **Preprocessing Pipeline**: Text cleaning (HTML, symbols removal) → Normalization (diacritics, kashida removal) → Optional lemmatization (Farasa) → Tokenization (BoW/TF-IDF for ANN; word/char sequences for RNN) → Padding (RNN only)
  - **ANN Architecture**: Input(V-dimensional sparse vector) → Dense(32, ReLU) → Dropout(0.7) → [Dense-Dropout repeated] → Output(sigmoid/softmax)
  - **RNN Architecture**: Input(sequence) → Embedding(learned) → BiGRU(32 units, 2 layers) → Dropout(0.7) → Dense → Output
  - **Training Infrastructure**: RmsProp/Adam optimizer, 10 epochs max, early stopping on validation accuracy, batch size 128 (RNN) to 512 (ANN), vocabulary cap 15K-80K

- Critical path:
  1. Arabic-specific normalization (diacritics, kashida) must be applied consistently—these affect token matching
  2. Vocabulary size limit must be set before training; larger vocabularies increase sparsity and overfitting risk
  3. Author split protocol must be determined at dataset creation time—cannot be changed after splitting
  4. All reported results use retained stop words (removal hurt performance)

- Design tradeoffs:
  - **ANN vs. RNN**: ANNs are faster to train and outperform RNNs on most configurations, but discard word order; RNNs capture sequence but require more data and tuning
  - **Word vs. Character tokenization**: Word-level has shorter sequences but OOV issues; character-level handles unseen words but needs deeper architectures for adequate context
  - **Lemmatization**: Benefits poetry (F1: 0.651 vs. lower without) but not prose—likely because morphological variation carries temporal signal in non-poetry texts
  - **Class granularity**: Finer periodization (100-year bins) enables detailed analysis but sacrifices accuracy; coarser eras are more reliable

- Failure signatures:
  - **Systematic adjacent-era confusion**: Islamic→Abbasid, Ottoman→Modern misclassifications are expected and indicate gradual transitions, not model failure
  - **Edge-era underperformance**: First and last eras (Islamic, Modern) show lowest recall—boundary periods lack distinguishing features on one side
  - **Author-disjoint performance drop**: If author-merged F1 exceeds author-disjoint by >0.15, the model has learned author style rather than temporal features
  - **Lemmatization backfire on prose**: If lemmatization reduces non-poetry accuracy, morphological variation carries signal that should be preserved

- First 3 experiments:
  1. **Binary classification baseline with author-disjoint splits**: Train ANN with BoW features on the binary (classical vs. modern) setup; this establishes minimum viable performance and validates the preprocessing pipeline (target: F1 > 0.66 on OpenITI, > 0.78 on APCD per Tables 10 and 13).
  2. **Five-era classification with lemmatization ablation**: Run the five-era setup with and without Farasa lemmatization on both datasets; expect improvement only on APCD (poetry), confirming dataset-specific preprocessing needs.
  3. **Author-split impact quantification**: Train the best-performing five-era model on both author-disjoint and author-merged splits; measure the performance gap to quantify how much the model relies on authorial style versus temporal features (expect larger gap on OpenITI prose than APCD poetry).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pretrained transformer models (e.g., AraBERT, MARBERT) improve temporal classification accuracy despite potential biases from being trained predominantly on modern data?
- Basis in paper: [explicit] The authors explicitly identify evaluating pretrained transformers as a direction for future work.
- Why unresolved: The study only utilized ANNs, RNNs, and CNNs. The authors note that the dominance of modern-era data in existing Arabic transformers might introduce temporal bias that hinders historical classification.
- What evidence would resolve it: Benchmarking experiments comparing transformer performance against the established ANN baselines on the OpenITI and APCD datasets.

### Open Question 2
- Question: Does removing "text-reuse" (boilerplate) passages from the OpenITI corpus enhance the reliability of automatic era classification?
- Basis in paper: [explicit] The future work section proposes examining how the de-reused corpus version released by Belinkov et al. affects classification performance.
- Why unresolved: The current study used the standard corpus, which contains frequent, reused phrases (e.g., "peace be upon him"). These may provide spurious signals that inflate or confuse model performance rather than reflecting genuine stylistic evolution.
- What evidence would resolve it: A comparative study evaluating model accuracy when trained on the standard OpenITI corpus versus the filtered version with reused passages removed.

### Open Question 3
- Question: Can incorporating Named Entity Recognition (NER) to link historical figures with biographical data improve classification accuracy?
- Basis in paper: [explicit] The authors suggest that incorporating external knowledge about named persons mentioned in the text may help resolve classification challenges.
- Why unresolved: Current models rely strictly on lexical and stylometric features (e.g., word frequencies). They lack the capacity to utilize concrete historical metadata, such as the lifespan of a person mentioned in the text, which could anchor the document to a specific era.
- What evidence would resolve it: Implementation of a hybrid model that uses NER to extract person names and links them to known biographical dates to constrain or weight the classification predictions.

### Open Question 4
- Question: Does developing a lemmatization tool specifically optimized for Classical Arabic improve classification performance for non-poetry texts?
- Basis in paper: [explicit] The authors note that future work includes improving lemmatization for Classical Arabic, as current tools proved less effective for non-poetry texts.
- Why unresolved: The study found that lemmatization improved results for poetry (APCD) but not for prose (OpenITI). This suggests that existing tools (like Farasa) may not adequately handle the morphological complexity or orthography of Classical Arabic prose.
- What evidence would resolve it: Experiments re-training and testing the models on OpenITI using a domain-specific lemmatizer designed for Classical Arabic.

## Limitations
- The paper's core findings rest on relatively small datasets (1.8M poetry verses vs 1.5B words prose), raising questions about generalizability to larger-scale literary corpora.
- The binary and five-era setups achieve strong performance (F1: 0.83-0.75), but the 15-class setup drops to F1: 0.18, suggesting the method may not scale to finer temporal granularity without substantial architectural or methodological changes.
- Author-specific stylistic features appear to be learned alongside genuine temporal signals, making it difficult to determine whether the model truly captures era-level linguistic evolution or merely author-era correlations.

## Confidence

- **High Confidence**: The core experimental results (performance metrics across setups, confusion patterns, author-split effects) are reproducible and align with the presented methodology.
- **Medium Confidence**: The interpretation that adjacent-era confusion reflects genuine gradual linguistic evolution is plausible but not definitively proven; the observed patterns could also reflect model limitations or dataset biases.
- **Low Confidence**: The claim that lemmatization benefits poetry but not prose requires further validation across diverse poetry/prose datasets to rule out corpus-specific effects rather than general linguistic principles.

## Next Checks
1. **Author-confounding isolation**: Re-run the five-era classification on OpenITI with strict author-disjoint splits (no author overlap) and compare against the author-merged results to quantify how much performance depends on authorial style versus genuine temporal features.
2. **Adjacent-era distinction stress test**: Create a controlled experiment where adjacent eras are artificially merged (e.g., Islamic+Abbasid, Ottoman+Modern) and measure classification accuracy improvements to test whether the confusion reflects genuine gradual evolution or model inadequacy.
3. **Vocabulary signal decomposition**: Analyze the top-100 TF-IDF features per era to determine what proportion are genuinely era-specific vocabulary versus author-preferred terms, providing empirical evidence for or against the authorial style confounding hypothesis.