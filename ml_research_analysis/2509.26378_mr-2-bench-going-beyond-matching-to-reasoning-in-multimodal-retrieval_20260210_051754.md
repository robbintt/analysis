---
ver: rpa2
title: 'MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval'
arxiv_id: '2509.26378'
source_url: https://arxiv.org/abs/2509.26378
tags:
- multimodal
- retrieval
- visual
- reasoning
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MR2-Bench, a benchmark for evaluating multimodal
  retrieval systems on reasoning-intensive tasks. Unlike existing benchmarks that
  focus on surface-level semantic matching, MR2-Bench requires models to perform logical,
  spatial, and causal inference over complex multimodal data including natural images,
  diagrams, and visual puzzles.
---

# MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval

## Quick Facts
- arXiv ID: 2509.26378
- Source URL: https://arxiv.org/abs/2509.26378
- Reference count: 39
- Introduces MR2-Bench requiring logical, spatial, and causal inference over complex multimodal data

## Executive Summary
This paper introduces MR2-Bench, a benchmark for evaluating multimodal retrieval systems on reasoning-intensive tasks. Unlike existing benchmarks that focus on surface-level semantic matching, MR2-Bench requires models to perform logical, spatial, and causal inference over complex multimodal data including natural images, diagrams, and visual puzzles. The benchmark contains 1,309 queries across 12 sub-tasks spanning three meta-tasks: multimodal knowledge retrieval, visual illustration search, and visual relation reasoning. Despite strong performance on existing benchmarks like MMEB (Recall@1 of 77.78%), the leading Seed1.6-Embedding model achieves only 9.91% Recall@1 on MR2-Bench, highlighting the increased difficulty.

## Method Summary
MR2-Bench is constructed with 1,309 queries across 12 sub-tasks spanning three meta-tasks: multimodal knowledge retrieval, visual illustration search, and visual relation reasoning. The benchmark requires models to perform logical, spatial, and causal inference over complex multimodal data including natural images, diagrams, and visual puzzles. The authors evaluate multiple state-of-the-art multimodal retrieval models including Seed1.6-Embedding, LLaVA-NeXT-72B-0324, Qwen2.5-VL-72B-0324, and InternVL-34B-0324. They also explore techniques like query rewriting and reranking to improve performance on the benchmark.

## Key Results
- Leading model Seed1.6-Embedding achieves 9.91% Recall@1 on MR2-Bench compared to 77.78% on MMEB
- Reasoning-oriented text retrievers and multimodal retrievers outperform matching-centric models
- Query rewriting and reranking techniques can significantly improve performance on MR2-Bench
- Current multimodal retrievers struggle with reasoning-intensive tasks despite strong performance on matching tasks

## Why This Works (Mechanism)
MR2-Bench works by requiring models to engage in deeper reasoning rather than simple semantic matching. The benchmark's effectiveness stems from its focus on logical, spatial, and causal inference across multimodal inputs. By including complex visual puzzles, diagrams, and natural images that require reasoning beyond surface-level understanding, MR2-Bench exposes the limitations of current multimodal retrieval models that excel at matching but struggle with inference. The benchmark's structure, with three meta-tasks and 12 sub-tasks, provides comprehensive coverage of different reasoning capabilities needed for real-world applications.

## Foundational Learning

**Multimodal Retrieval** - Systems that retrieve relevant information from multiple modalities (text, image, video) based on queries. Needed to understand the broader context of multimodal systems that MR2-Bench aims to evaluate.

**Reasoning vs Matching** - Matching focuses on finding semantically similar items, while reasoning requires logical inference and understanding relationships. This distinction is crucial for understanding why MR2-Bench represents a significant challenge beyond existing benchmarks.

**Visual Puzzles and Diagrams** - Complex visual representations that require spatial reasoning and understanding of relationships. These are key components of MR2-Bench that test advanced reasoning capabilities.

**Query Rewriting** - The process of reformulating queries to improve retrieval performance, particularly useful for complex reasoning tasks where initial queries may not capture the full intent.

**Reranking** - Post-retrieval re-ordering of results based on additional criteria or models, used to improve performance on challenging benchmarks like MR2-Bench.

## Architecture Onboarding

**Component Map**
Seed1.6-Embedding -> MMEB Benchmark -> 77.78% Recall@1
Seed1.6-Embedding -> MR2-Bench -> 9.91% Recall@1
Reasoning-Oriented Models -> MR2-Bench -> Better Performance
Matching-Centric Models -> MR2-Bench -> Poor Performance

**Critical Path**
1. Query input (multimodal)
2. Model processing (reasoning vs matching)
3. Retrieval of relevant items
4. Evaluation using Recall@1 metric
5. Benchmark classification (MMEB vs MR2-Bench)

**Design Tradeoffs**
The benchmark trades scale (1,309 queries) for depth of reasoning requirements. This creates a more challenging but potentially less statistically robust evaluation compared to larger matching-focused benchmarks.

**Failure Signatures**
Models that rely primarily on surface-level semantic matching fail dramatically on MR2-Bench, dropping from ~77% to ~10% Recall@1. This indicates inability to perform logical, spatial, and causal inference.

**First 3 Experiments**
1. Compare performance of reasoning-oriented vs matching-centric models on MR2-Bench
2. Test query rewriting techniques to improve performance on reasoning tasks
3. Apply reranking methods to enhance initial retrieval results

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small scale with only 1,309 queries may limit statistical significance
- Substantial performance gap raises questions about benchmark calibration and difficulty
- Limited detailed ablation studies on individual reasoning components
- Distribution balance across meta-tasks not explicitly discussed

## Confidence

**High Confidence**: The claim that MR2-Bench requires deeper reasoning than existing multimodal retrieval benchmarks is well-supported by the significant performance drop observed across multiple state-of-the-art models.

**Medium Confidence**: The assertion that query rewriting and reranking techniques can significantly improve performance is supported but would benefit from more detailed quantitative analysis.

**Low Confidence**: The paper's implications about the benchmark's ability to guide development of more capable multimodal retrievers for real-world applications is somewhat speculative given current performance limitations.

## Next Checks

1. Conduct comprehensive statistical power analysis to determine minimum number of queries needed across different reasoning types for reliable performance measurements.

2. Perform detailed error analysis on the 9.91% Recall@1 achieved by Seed1.6-Embedding to identify specific reasoning capabilities that are most challenging for current models.

3. Design and execute controlled experiments comparing reasoning-oriented versus matching-centric approaches across the three meta-tasks to quantify the relative importance of different reasoning components.