---
ver: rpa2
title: GenPlanX. Generation of Plans and Execution
arxiv_id: '2506.10897'
source_url: https://arxiv.org/abs/2506.10897
tags:
- planning
- state
- genplanx
- data
- goals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenPlanX integrates Large Language Models (LLMs) with classical
  AI planning to automate office-related tasks. It interprets natural language requests,
  generates structured planning problems, solves them using an AI planner, and executes
  the resulting plans while monitoring for success or failure.
---

# GenPlanX. Generation of Plans and Execution

## Quick Facts
- arXiv ID: 2506.10897
- Source URL: https://arxiv.org/abs/2506.10897
- Authors: Daniel Borrajo; Giuseppe Canonaco; Tomás de la Rosa; Alfredo Garrachón; Sriram Gopalakrishnan; Simerjot Kaur; Marianela Morales; Sunandita Patra; Alberto Pozanco; Keshav Ramani; Charese Smiley; Pietro Totis; Manuela Veloso
- Reference count: 40
- Key outcome: GenPlanX integrates LLMs with classical AI planning to automate office tasks, generating optimal plans where LLM-only approaches fail.

## Executive Summary
GenPlanX is a system that automates office-related tasks by combining Large Language Models (LLMs) with classical AI planning. It interprets natural language requests, generates structured planning problems, solves them using an AI planner, and executes the resulting plans while monitoring for success or failure. The system successfully translates natural language to executable plans and reliably executes tasks such as reading files, querying databases, creating charts, and managing appointments. In user evaluations, GenPlanX demonstrated superior performance compared to LLM-only planning, producing optimal solutions for complex office workflows.

## Method Summary
GenPlanX interprets natural language requests through an LLM, which translates them into structured JSON problem descriptions containing objects, initial states, and goals. This JSON is compiled into PDDL (Planning Domain Definition Language) and solved by a classical planner to generate an optimal action sequence. The system then executes these actions using Python functions, monitoring each step for success or failure. If execution fails or new goals emerge, GenPlanX replans from the updated state. The architecture includes entity extraction to ground the LLM prompt with domain-specific knowledge, and uses few-shot examples to guide the LLM's output format.

## Key Results
- GenPlanX generated optimal plans for complex office workflows involving data manipulation, presentation creation, and scheduling
- Outperformed LLM-only planning, producing optimal solutions where LLMs often failed or returned suboptimal plans
- Successfully executed tasks such as reading files, querying databases, creating charts, and managing appointments

## Why This Works (Mechanism)

### Mechanism 1
Separating language understanding from plan generation yields valid and optimal plans where LLM-only approaches fail. The LLM translates natural language to structured JSON, which is compiled to PDDL and solved by a classical planner. The LLM handles semantic ambiguity while the planner handles combinatorial optimization.

Core assumption: The LLM can reliably extract intents and state literals when given domain types, predicates, and few-shot examples in the prompt.

Evidence anchors:
- Experiments demonstrate GenPlanX's ability to generate optimal plans where LLM-only approaches fail or produce suboptimal results, such as efficiently reading from multiple databases with different access costs.
- LLM-only planning: GPT-4O achieved 4/5 correct plans with hints, but 0/5 optimal plans without hints; GenPlanX achieved optimal plans consistently.

Break condition: If the LLM outputs malformed JSON or hallucinates predicates not in the domain, compilation fails and no plan is generated.

### Mechanism 2
Domain-specific entity extraction grounds the LLM prompt in organizational knowledge not available to general-purpose models. An ensemble module extracts entities (e.g., client IDs, ISINs, trade dates) and their types before prompt construction, injecting them into the prompt so the LLM can reference correct object types and values.

Core assumption: Entity types are stable and known a priori; extraction accuracy is sufficient for the target domain.

Evidence anchors:
- The entity extraction module builds upon the capabilities outlined in [21]... This is particularly crucial in the finance industry, where entities extracted include unique identifiers...
- The prompt provides schemas for relevant SORs... These schemas outline the available data fields, guiding the LLM in selecting the appropriate data sources.

Break condition: If entity extraction misclassifies types or misses entities, the LLM may produce incorrect object bindings, leading to infeasible plans.

### Mechanism 3
Execution monitoring with replanning on failure provides robustness to real-world execution errors. Each PDDL action maps to a Python function; after execution, a boolean monitor checks expected effects. On failure—or when new goals emerge from action outcomes—the system regenerates the planning problem and invokes the planner again.

Core assumption: Monitors can reliably detect failures; the planner can be re-invoked quickly enough for interactive use.

Evidence anchors:
- In case the execution of any action fails, GenPlanX replans. Another reason for replanning consists on of an action execution adding a new goal to the execution state.
- Experiments demonstrate GenPlanX's ability to generate optimal plans where LLM-only approaches fail or produce suboptimal results, ensuring correctness through replanning on failures.

Break condition: If monitors produce false negatives (miss failures) or false positives (flag success as failure), the system either proceeds with invalid state or replans unnecessarily.

## Foundational Learning

**PDDL (Planning Domain Definition Language)**
- Why needed here: GenPlanX uses PDDL as the interchange between LLM output and classical planners. Understanding types, predicates, actions, preconditions, and effects is required to extend domains or debug problems.
- Quick check question: Given a simple domain with `(:action move :parameters (?a - agent ?l1 - location ?l2 - location) :precondition (at ?a ?l1) :effect (and (at ?a ?l2) (not (at ?a ?l1))))`, what is the precondition for `move(agent1, roomA, roomB)`?

**Classical AI Planning Optimality**
- Why needed here: The paper explicitly claims optimality via cost-aware planners (e.g., choosing db2 with optimized query over db1 with basic query). Understanding action costs and planner guarantees helps diagnose when plans are suboptimal.
- Quick check question: If action A costs 2 and action B costs 5, and both achieve the same goal, which will a cost-optimal planner choose?

**Few-Shot Prompting**
- Why needed here: The LLM is guided by a prompt containing domain types, predicates, actions, and example intent→output mappings. Effective prompt design is critical for reliable JSON problem generation.
- Quick check question: What is the risk of providing examples that conflict with the current user request's intent structure?

## Architecture Onboarding

**Component map:**
Entity Extraction -> Generate Prompt -> LLM -> Compiler -> Planner -> Execution -> Response Generation

**Critical path:**
1. User request in natural language
2. Entity extraction identifies typed entities
3. Prompt constructed with domain info + examples
4. LLM outputs JSON with objects, init_state, goals
5. Compiler transforms JSON -> PDDL problem
6. Planner (Fast-Downward via Unified Planning) generates plan
7. Executor maps each PDDL action to Python function, monitors success
8. On failure or new goals -> replan from updated state

**Design tradeoffs:**
- JSON vs PDDL output from LLM: JSON is easier for LLMs to generate and can carry execution-time values (file paths, email addresses); requires a compiler step
- Planner-independent architecture: Flexibility to swap planners; adds dependency on PDDL standard compliance
- Monitoring at execution level only: Currently does not translate low-level state back to high-level PDDL state for monitoring (noted as limitation)

**Failure signatures:**
- Compilation error -> LLM output missing required keys or using unknown predicates
- Planner returns no solution -> goals unreachable given initial state and action preconditions
- Execution failure -> monitor function returns False; triggers replan
- Infinite replanning loop -> goals unachievable or monitors consistently failing

**First 3 experiments:**
1. Replicate Example 1.2 with two databases of different costs; verify the planner chooses db2 + optimized query (cost 4) over db1 + basic query (cost 6)
2. Introduce an unknown predicate in the LLM prompt examples; observe whether the compiler catches it or the planner fails
3. Simulate an execution failure (e.g., corrupt the input CSV file); verify replanning is triggered and the response reflects recovery or graceful failure

## Open Questions the Paper Calls Out

**Open Question 1**
Can action models be learned automatically from observations in the GenPlanX framework, and how would learned actions compare in reliability to manually-defined PDDL actions?

Basis in paper: Future work states: "we would like to expand the set of actions considered... This process could be either manual as we are currently doing; or automated, by learning action models from observations [2, 25, 14]."

Why unresolved: The paper only demonstrates manually-defined actions; no experiments or methodology exist for automated action learning within the system.

What evidence would resolve it: A comparative study showing success rates of plans using learned vs. manually-defined actions on the same office tasks.

**Open Question 2**
How can GenPlanX automatically generate and prioritize new goals during replanning based on environmental changes or detected opportunities?

Basis in paper: Future work proposes providing "goal reasoning capabilities" to "automatically generate new goals upon replanning... when monitoring detects opportunities upon changes in the environment, by analyzing the structure of the goals, or by predicting the appearance of new goals."

Why unresolved: Current replanning only handles failures or new goals from action execution (e.g., read-email generating goals); no proactive goal generation mechanism exists.

What evidence would resolve it: Demonstrations of GenPlanX autonomously identifying and pursuing beneficial sub-goals not explicitly requested by users.

**Open Question 3**
What is the impact of integrating high-level PDDL state monitoring on replanning accuracy and execution efficiency?

Basis in paper: Section 7.2 states: "currently GenPlanX only monitors the low-level state without translating it back to the high-level PDDL state."

Why unresolved: The gap between low-level execution state and high-level planning representation could cause replanning decisions based on incomplete or misaligned information.

What evidence would resolve it: Ablation experiments comparing task success rates and replanning frequency with and without high-level state translation.

**Open Question 4**
How does GenPlanX's performance scale with the complexity and ambiguity of natural language requests, particularly when users provide underspecified or conflicting intents?

Basis in paper: The system relies on LLMs correctly mapping natural language to structured dictionaries with mandatory keys, but no analysis addresses handling vague requests or intent ambiguity.

Why unresolved: The system's robustness to ambiguous or conflicting natural language requests is not evaluated.

What evidence would resolve it: Benchmarking GenPlanX on a dataset of ambiguous or multi-intent requests, measuring clarification-seeking behavior and plan correctness.

## Limitations
- Entity extraction module details are underspecified, with only citation to prior work [21] and no provided models or accuracy metrics
- Execution monitoring system's reliability is untested beyond basic success/failure flags, with no discussion of false positive/negative rates
- Paper lacks ablation studies showing the marginal benefit of each component (e.g., comparing entity extraction + LLM vs. LLM alone)

## Confidence
- **High Confidence**: The separation of LLM-based language understanding from classical planner-based optimization is well-established in planning literature and logically sound. The empirical comparison showing LLM-only planning failing to produce optimal solutions is clearly demonstrated in Table 8.
- **Medium Confidence**: The entity extraction mechanism's effectiveness in grounding the LLM prompt with domain-specific knowledge is supported by implementation details but lacks validation through ablation or error analysis.
- **Low Confidence**: The robustness claims for execution monitoring and replanning are supported only by mechanism description, not by systematic evaluation of failure recovery rates or replanning loop termination conditions.

## Next Checks
1. **Entity Extraction Validation**: Test the entity extraction module on a diverse set of finance-domain requests with varying entity complexity; measure precision/recall against ground truth entity annotations to establish baseline accuracy before prompt generation.
2. **Replanning Robustness Test**: Systematically inject different types of execution failures (precondition violations, resource unavailability, data corruption) and measure: (a) detection rate by monitors, (b) success rate of subsequent replans, and (c) termination conditions to prevent infinite replanning loops.
3. **Compiler Error Handling Evaluation**: Generate LLM outputs with controlled errors (missing keys, unknown predicates, type mismatches) and measure: (a) compiler detection rate, (b) quality of error messages, and (c) fallback behavior when compilation fails.