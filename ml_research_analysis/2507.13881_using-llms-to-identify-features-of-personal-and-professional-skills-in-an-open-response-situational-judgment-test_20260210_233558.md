---
ver: rpa2
title: Using LLMs to identify features of personal and professional skills in an open-response
  situational judgment test
arxiv_id: '2507.13881'
source_url: https://arxiv.org/abs/2507.13881
tags:
- features
- responses
- feature
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  to automatically identify construct-relevant features in responses to an open-response
  situational judgment test (SJT) designed to measure personal and professional skills.
  The study builds on prior work identifying nine features that influence human ratings
  of SJT responses and uses a dataset of 162 responses from the Casper SJT, rated
  by two human annotators.
---

# Using LLMs to identify features of personal and professional skills in an open-response situational judgment test

## Quick Facts
- arXiv ID: 2507.13881
- Source URL: https://arxiv.org/abs/2507.13881
- Reference count: 10
- Primary result: LLMs achieve moderate agreement (κ up to 0.404) with human raters in classifying construct-relevant features from open-response SJTs, with performance improving significantly when prompts include detailed level descriptions

## Executive Summary
This paper investigates the use of large language models (LLMs) to automatically identify construct-relevant features in responses to an open-response situational judgment test (SJT) designed to measure personal and professional skills. The study builds on prior work identifying nine features that influence human ratings of SJT responses and uses a dataset of 162 responses from the Casper SJT, rated by two human annotators. Using zero-shot prompting, five LLMs (including GPT-4o-mini, Claude Sonnet 4, and o4-mini) were compared against human annotations, achieving moderate agreement (κ up to 0.404 for the best models). Performance improved significantly when prompts included detailed descriptions of feature levels, especially for o4-mini (e.g., DISRES improved from κ = 0.171 to 0.377). The results demonstrate that LLMs can reliably extract certain features, such as statements of insufficient information, and that targeted prompt engineering enhances performance. The findings suggest LLMs hold promise for scalable, automated scoring of SJTs, with implications for broader use in evaluating open-response assessments of personal and professional competencies.

## Method Summary
The study uses zero-shot prompting to classify seven construct-relevant features from 162 open-response SJT answers rated by two human annotators. Five LLMs (GPT-4o-mini, DeepSeek-R1, Llama 4 Maverick, o4-mini, Claude Sonnet 4) were tested using a system prompt template that includes scenario context, questions, and feature descriptions. The models return JSON with "decision" and "reasoning" fields. Cohen's κ (quadratic-weighted for ordinal, unweighted for binary) was used to measure agreement between LLM and human classifications. The researchers compared zero-shot prompting against prompts with detailed level descriptions to assess the impact of explicit inclusion/exclusion criteria.

## Key Results
- LLMs achieved moderate agreement with human raters (κ up to 0.404) across seven features
- LACKINF feature detection was highly reliable (κ = 0.658 for GPT-4o-mini) using zero-shot prompting
- Performance improved significantly with detailed level descriptions (e.g., DISRES improved from κ = 0.171 to 0.377 for o4-mini)
- Reasoning models (o4-mini, Claude Sonnet 4) generally outperformed standard models on complex features requiring social inference
- CREAT feature detection remained challenging across all models (κ = 0.006-0.145)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot prompting enables LLMs to extract nuanced, construct-relevant features from open-response SJTs without training examples.
- Mechanism: Pre-trained language models leverage semantic understanding and contextual reasoning to classify complex behavioral indicators (e.g., empathy, justification quality, perspective-taking) when provided with feature descriptions and response context.
- Core assumption: The latent knowledge encoded in LLMs during pre-training includes representations of social-emotional reasoning that align with human rater interpretations of professional competencies.
- Evidence anchors:
  - [abstract] "Using zero-shot prompting, five LLMs... achieving moderate agreement (κ up to 0.404 for the best models)"
  - [section 5.1] "For four out of seven features, the top performing LLM achieved κ > 0.4"
  - [corpus] Related work (Measure what Matters: Psychometric Evaluation of AI with SJTs) explores AI psychometrics using SJTs, suggesting cross-validation of the approach, but direct corpus evidence for zero-shot efficacy on this specific construct is limited.
- Break condition: If construct definitions diverge significantly from common language usage (e.g., domain-specific terminology), zero-shot performance may degrade substantially.

### Mechanism 2
- Claim: Providing explicit level descriptions in prompts shifts LLM classification thresholds toward human rater distributions.
- Mechanism: Detailed inclusion/exclusion criteria for each ordinal level reduce ambiguity in boundary cases, constraining the model's tendency to default to middle categories and aligning its decision boundaries with human operationalizations.
- Core assumption: The gap between LLM and human agreement stems primarily from threshold misalignment rather than fundamental misunderstanding of the construct.
- Evidence anchors:
  - [abstract] "Performance improved significantly when prompts included detailed descriptions of feature levels, especially for o4-mini (e.g., DISRES improved from κ = 0.171 to 0.377)"
  - [section 5.2] "o4-mini tended to provide more classifications in the middle of an ordinal scale than we observed with human raters"
  - [corpus] No direct corpus evidence on level-description prompting for SJTs; this appears novel to this application domain.
- Break condition: If features require subjective judgment that resists explicit articulation (e.g., "creativity"), level descriptions may yield diminishing returns (CREAT showed minimal improvement: 0.054 → 0.145).

### Mechanism 3
- Claim: Reasoning-optimized models outperform standard LLMs on features requiring multi-step inference about social dynamics.
- Mechanism: Models with explicit reasoning capabilities (o4-mini, Claude Sonnet 4) better handle features requiring integration of contextual information, perspective attribution, and evaluation of argument quality—tasks that benefit from chain-of-thought-like processing.
- Core assumption: Construct-relevant features in SJTs (e.g., "grasps complex social and emotional dynamics") require decompositional reasoning rather than pattern matching.
- Evidence anchors:
  - [section 5.1] "Claude Sonnet 4 generally outperforms the other models, achieving the highest agreement with human raters on four out of seven features"
  - [section 5.1] "reasoning models like OpenAI's o4-mini and Anthropic's Claude Sonnet 4 generally performed best at identifying these complex and nuanced constructs"
  - [corpus] Related work (Artificial Intelligence Quotient) frames human-AI collaborative intelligence measurement, but corpus evidence directly comparing reasoning vs. non-reasoning models on SJT feature extraction is absent.
- Break condition: For features identifiable via surface patterns (e.g., LACKINF with phrases like "gather more information"), reasoning models offer no advantage—GPT-4o-mini achieved κ = 0.658, matching human agreement (κ = 0.640).

## Foundational Learning

- Concept: **Cohen's κ with quadratic weighting**
  - Why needed here: Primary metric for measuring inter-rater reliability between LLMs and human annotators; quadratic weighting penalizes larger disagreements more heavily on ordinal scales.
  - Quick check question: If an LLM classifies a response as "Excellent Interpretation" when humans rated it "Limited Interpretation," does quadratic κ penalize this more than a one-level error?

- Concept: **Construct validity in automated scoring**
  - Why needed here: Prior NLP scoring systems failed for SJTs because they used features (grammar, coherence) unrelated to personal/professional competencies; valid scoring requires construct-relevant inputs.
  - Quick check question: Why would an NLP system optimized for essay coherence fail when applied to SJT responses about ethical reasoning?

- Concept: **Zero-shot vs. few-shot prompting**
  - Why needed here: This study uses zero-shot prompting (no examples in prompt); understanding the distinction is critical for interpreting results and designing improvements.
  - Quick check question: What additional information would a few-shot prompt include compared to the zero-shot approach used here?

## Architecture Onboarding

- Component map:
  - Input layer: Scenario context (text summary), questions, respondent answers
  - Prompt constructor: Injects feature description + level definitions into system prompt template
  - LLM classifier: Processes prompt, returns JSON with "decision" and "reasoning" fields
  - Aggregation layer: Computes κ agreement between LLM outputs and human annotations
  - Output: Feature-level classifications per response

- Critical path:
  1. Human raters classify responses → establish ground truth
  2. Construct feature descriptions (from prior work: Iqbal et al.)
  3. Format prompt with context + feature description
  4. LLM generates classification + reasoning
  5. Compare LLM classifications to human labels via Cohen's κ

- Design tradeoffs:
  - **Single LLM vs. ensemble**: Paper suggests using different LLMs for different features (DeepSeek-R1 excels at CREAT; Claude Sonnet 4 at INT/PERSP) vs. operational simplicity of one model
  - **Zero-shot simplicity vs. prompt-engineering investment**: Zero-shot requires no tuning but achieves lower κ; detailed level descriptions improve performance but require domain expertise to author
  - **Reasoning models vs. throughput/cost**: o4-mini and Claude Sonnet 4 perform best but have higher latency and cost than GPT-4o-mini

- Failure signatures:
  - **Middle-category collapse**: LLM over-classifies to middle ordinal levels (e.g., "Reasonable Justification" at 59.9% vs. humans' 35.8-46.9%) → indicates threshold misalignment
  - **Feature-specific breakdown**: CREAT feature shows near-zero detection (o4-mini: 0.006 True vs. humans: 0.17-0.20) → may require fundamentally different approach
  - **κ < 0.2**: For VAGUE (κ = 0.07-0.19 across models), feature definition may be too subjective for reliable extraction

- First 3 experiments:
  1. **Baseline replication**: Run zero-shot prompts across all 5 LLMs on a held-out subset of responses to verify κ ranges match paper (0.05-0.66 depending on feature/model)
  2. **Level-description ablation**: For one feature (e.g., DISRES), systematically add/remove level criteria to measure Δκ contribution of each component
  3. **Cross-feature model selection**: Test ensemble approach—assign best-performing LLM per feature based on Table 3 results—and compute composite agreement score

## Open Questions the Paper Calls Out

- Would few-shot prompting or fine-tuning improve LLM-human agreement beyond the gains achieved with detailed level descriptions?
  - Basis in paper: [explicit] "Future work could explore other approaches to prompt engineering including few-shot prompting as well as fine-tuning to further improve performance."
  - Why unresolved: Only zero-shot prompting with level descriptions was tested; few-shot and fine-tuning remain unexplored.
  - What evidence would resolve it: Compare κ agreement across zero-shot, few-shot, and fine-tuned conditions on the same feature extraction task.

- Can an ensemble approach using multiple LLMs with a voting system outperform single-model classification?
  - Basis in paper: [explicit] "We could also consider using multiple LLMs for the same criteria and instituting a voting system resembling traditional machine learning ensemble methods to produce more accurate and reliable results."
  - Why unresolved: The study only evaluated individual LLMs; ensemble methods were proposed but not tested.
  - What evidence would resolve it: Train an ensemble of diverse LLMs on the same features and measure whether voting improves κ over the best single model.

- Can LLM-generated "reasoning" outputs be used to provide valid, personalized formative feedback to respondents?
  - Basis in paper: [explicit] "Future work could use these 'reasoning' fields to generate personalized and direct feedback for respondents."
  - Why unresolved: Reasoning outputs were collected but not inspected or evaluated for feedback utility.
  - What evidence would resolve it: Have experts evaluate the accuracy and pedagogical value of LLM-generated feedback derived from reasoning outputs.

- Does using feature-specific LLMs (different models for different features) yield better overall classification than a single universal LLM?
  - Basis in paper: [explicit] "A future automated scoring solution... may be best served by using different LLMs for different features rather than forcing a single universal LLM."
  - Why unresolved: Each LLM performed best on different features, but a feature-specific deployment strategy was not empirically tested.
  - What evidence would resolve it: Compare overall classification accuracy when assigning the best-performing LLM per feature versus using one LLM across all features.

## Limitations
- Dataset availability: The 162 annotated SJT responses and human rater judgments are not publicly available, preventing independent validation of the exact κ values reported.
- Prompt engineering details: While the zero-shot template is provided, the specific level descriptions that improved DISRES from κ = 0.171 to 0.377 are not included in the paper.
- Model version stability: Different model snapshots may yield different performance, particularly for newer reasoning models like o4-mini.

## Confidence
- High confidence: LLMs can reliably identify the LACKINF feature (κ up to 0.658) using zero-shot prompting, and performance improves with detailed level descriptions for certain features like DISRES.
- Medium confidence: The general pattern that reasoning models (o4-mini, Claude Sonnet 4) outperform standard models on complex features requiring social inference, though absolute κ values may vary with dataset.
- Low confidence: The claim that CREAT detection remains challenging (κ = 0.006) due to feature-specific limitations—this may reflect dataset characteristics or definition clarity rather than inherent model incapacity.

## Next Checks
1. **Replicate core findings**: Run zero-shot prompts using o4-mini and Claude Sonnet 4 on a held-out subset of 50-100 responses to verify κ ranges (0.05-0.66) match paper results.
2. **Prompt engineering ablation**: Systematically add/remove level criteria for DISRES feature to quantify each component's contribution to the κ improvement from 0.171 to 0.377.
3. **Ensemble approach validation**: Test model selection strategy by assigning best-performing LLM per feature (per Table 3) and computing composite agreement score against human annotations.