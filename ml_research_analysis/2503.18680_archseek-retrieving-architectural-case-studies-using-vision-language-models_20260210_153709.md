---
ver: rpa2
title: 'ArchSeek: Retrieving Architectural Case Studies Using Vision-Language Models'
arxiv_id: '2503.18680'
source_url: https://arxiv.org/abs/2503.18680
tags:
- design
- search
- architectural
- system
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently searching for
  architectural case studies, as traditional text-based search tools struggle to capture
  the inherently visual and complex nature of architectural knowledge. ArchSeek, an
  innovative case study search system with recommendation capability, is introduced
  to overcome this limitation.
---

# ArchSeek: Retrieving Architectural Case Studies Using Vision-Language Models

## Quick Facts
- **arXiv ID**: 2503.18680
- **Source URL**: https://arxiv.org/abs/2503.18680
- **Reference count**: 21
- **Primary result**: ArchSeek outperforms text-only and image-only retrieval methods on architectural case studies using VLM-based text augmentation and cross-modal fusion.

## Executive Summary
This paper addresses the challenge of efficiently searching for architectural case studies, as traditional text-based search tools struggle to capture the inherently visual and complex nature of architectural knowledge. ArchSeek, an innovative case study search system with recommendation capability, is introduced to overcome this limitation. Powered by vision-language models and cross-modal embeddings, ArchSeek enables text and image queries with fine-grained control and interaction-based design case recommendations. The system offers architects a more efficient, personalized way to discover design inspirations, with potential applications across other visually driven design fields.

## Method Summary
ArchSeek constructs a database of 54 architectural cases with text descriptions and images, then uses GPT-4-Vision to extract structured textual analyses (form, style, material, relations to context, passive design, etc.) from each image. These analyses are embedded using text-embedding-3-large, while images are embedded using ImageBind. For retrieval, text queries and image queries are processed in parallel through cosine similarity matching, then combined using Reciprocal Rank Fusion (RRF) with parameter c=10. The system also implements implicit preference learning by augmenting queries with text descriptions from user-liked cases.

## Key Results
- ArchSeek outperforms text-only and image-only retrieval methods on architectural case studies
- Vision-language model augmentation improves retrieval quality by capturing domain-specific architectural features
- Cross-modal fusion via RRF provides better results than either modality alone
- Implicit preference learning through query augmentation shows visible effects on personalized recommendations

## Why This Works (Mechanism)

### Mechanism 1: VLM-based Text Augmentation
Extracting structured textual analysis from architectural images enables more effective semantic matching than raw image embeddings alone. GPT-4-Vision processes each image with domain-specific prompts to generate structured JSON critiques, which are embedded and used for similarity matching alongside original text. This guides the model toward attending to relevant architectural visual elements that general embedding models may miss.

### Mechanism 2: Cross-modal Embedding Fusion via Reciprocal Rank Fusion
Combining text-embedding search and image-embedding search via RRF provides better retrieval than either modality alone. Text captures conceptual/technical aspects while images capture aesthetic/spatial properties. RRF (with c=10) aggregates the two ranked lists to leverage complementary information from both modalities.

### Mechanism 3: Implicit Preference Learning via Query Augmentation
Augmenting queries with text descriptions from user-liked cases enables personalized recommendations without explicit preference modeling or collaborative filtering. When users click "like" on a design case, its text description is added to the query set, shifting results toward liked-case attributes while maintaining retrieval efficiency.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Essential for extracting architectural meaning from images. Understanding VLM capabilities and limitations is critical for effective implementation.
  - *Quick check*: Given an architectural photograph, what structured attributes would you prompt a VLM to extract to maximize downstream retrieval utility?

- **Cross-modal Embeddings**: The system maps text and images into shared embedding spaces and compares via cosine similarity. Understanding embedding alignment and cross-modal transfer is critical.
  - *Quick check*: Why might ImageBind's image-to-text similarity differ from text-embedding-3-large's text-to-text similarity for the same semantic concept?

- **Reciprocal Rank Fusion (RRF)**: RRF (parameter c=10) combines ranked lists from text and image searches. Understanding how rank-based fusion differs from score-based fusion informs tuning decisions.
  - *Quick check*: If text search returns 10 results and image search returns 100, does RRF inherently favor the longer list? How does the constant c affect this?

## Architecture Onboarding

- **Component map**: Database Construction Pipeline (raw images/text → GPT-4-Vision analysis → Embedding generation → Vector storage) → Query Processing Layer (text/image input → parallel embedding → cosine similarity → RRF fusion → ranked results) → Recommendation Module ("Like" event listener → query augmentation → re-retrieval) → Web UI (text input, image upload, analysis weight sliders, like buttons, result cards)

- **Critical path**: Verify VLM API access and rate limits for database construction (54 cases processed; scaling requires cost/time estimation), confirm embedding dimensionality compatibility, validate RRF implementation with synthetic ranked lists, test cosine similarity numerical stability on high-dimensional vectors

- **Design tradeoffs**: Survey-informed prompts vs. generic prompts (domain-specific vs. field-transferability), text augmentation cost vs. retrieval quality (API cost and latency vs. quality drops without augmentation), database scope (54-case evaluation limits generalizability vs. broader architectural categories)

- **Failure signatures**: Retrieval returns irrelevant results for style-specific queries (inspect VLM prompt outputs), recommendations converge too narrowly after multiple "likes" (query augmentation over-constrains), image query fails on abstract drawings (VLM struggles with non-photographic inputs)

- **First 3 experiments**: Prompt ablation study (survey-informed vs. generic prompts on precision@5), RRF parameter sweep (test c ∈ {1, 5, 10, 20, 50, 100}), recommendation diversity measurement (compute intra-list diversity of top-10 recommendations after 1, 3, 5 simulated likes)

## Open Questions the Paper Calls Out

- **Open Question 1**: How does ArchSeek's performance scale when applied to a larger, more diverse dataset of architectural styles and building types beyond art galleries and museums?
- **Open Question 2**: What is the impact on design efficiency when integrating ArchSeek directly into computational design workflows, such as plugins for Rhino or Grasshopper?
- **Open Question 3**: How can the implicit recommendation algorithm be refined to prevent the occasional reduction in result relevance observed after user interactions?

## Limitations

- Dataset scale and diversity limited to 54 cases primarily focused on art galleries and museums
- VLM dependency introduces API costs and potential rate limits that could hinder scaling
- Evaluation scope doesn't cover abstract drawings, sketches, or non-photographic architectural representations where VLMs struggle

## Confidence

- **High confidence**: RRF fusion mechanism and cross-modal embedding approach are well-established in literature; evaluation methodology (precision/recall on human-labeled queries) is standard and reproducible
- **Medium confidence**: VLM-based text augmentation shows promising results in ablation studies but relies on domain-specific prompts whose effectiveness outside architecture is unproven
- **Medium confidence**: Implicit preference learning via query augmentation is intuitively sound and shows visible effects but lacks quantitative evaluation of recommendation diversity or long-term user satisfaction

## Next Checks

1. **Prompt ablation study**: Test survey-informed architectural prompts against generic VLM prompts on precision@5 to quantify domain-specific benefit (expected: 15-25% improvement)
2. **RRF parameter sensitivity**: Sweep c values {1, 5, 10, 20, 50, 100} to identify optimal balance between text and image modality contributions for different query types
3. **Recommendation diversity measurement**: Simulate user interactions (1, 3, 5 likes) and compute intra-list diversity (embedding variance) of top-10 recommendations to assess long-term personalization quality