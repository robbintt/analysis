---
ver: rpa2
title: Evaluating Nova 2.0 Lite model under Amazon's Frontier Model Safety Framework
arxiv_id: '2601.19134'
source_url: https://arxiv.org/abs/2601.19134
tags:
- nova
- lite
- safety
- amazon
- frontier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Amazon evaluated Nova 2.0 Lite under its Frontier Model Safety
  Framework (FMSF), focusing on CBRN, offensive cyber, and AI R&D risks. The evaluation
  combined automated benchmarks (WMDP, PROTOCOLQA, CyBench, RE-Bench) and expert red-teaming
  with independent audits.
---

# Evaluating Nova 2.0 Lite model under Amazon's Frontier Model Safety Framework

## Quick Facts
- arXiv ID: 2601.19134
- Source URL: https://arxiv.org/abs/2601.19134
- Reference count: 14
- Primary result: Nova 2.0 Lite achieved improved factual accuracy over Nova 1.0 but did not cross critical risk thresholds in Amazon's FMSF evaluation

## Executive Summary
Amazon conducted a comprehensive safety evaluation of Nova 2.0 Lite under its Frontier Model Safety Framework, focusing on CBRN, offensive cyber, and AI R&D risks. The evaluation combined automated benchmarks (WMDP, PROTOCOLQA, CyBench, RE-Bench) with expert red-teaming and independent audits. Nova 2.0 Lite demonstrated enhanced factual accuracy with 0.71 on WMDP-CHEM and 0.82 on WMDP-BIO, along with a 7.5% improvement on CyBench, while remaining below critical risk thresholds in most domains. Independent reviews from Nemesys and METR confirmed safety findings, supporting deployment with additional safeguards in radiological domains and enhanced monitoring.

## Method Summary
The evaluation employed a multi-layered approach combining automated benchmarks and expert assessment. Automated testing included WMDP for chemical/biological knowledge, PROTOCOLQA for CBRN protocol extraction, CyBench for cybersecurity capabilities, and RE-Bench for AI research automation. Red-teaming exercises were conducted by domain experts who tested model responses across risk categories. Independent audits by Nemesys and METR provided external validation. The framework established specific risk thresholds that models must not cross to ensure safe deployment, with additional safeguards implemented where thresholds were approached.

## Key Results
- Nova 2.0 Lite achieved 0.71 on WMDP-CHEM and 0.82 on WMDP-BIO, showing improved factual accuracy over Nova 1.0 Pro and Premier
- The model showed a 7.5% uplift on CyBench but remained below autonomous exploit development capability
- Independent audits by Nemesys and METR confirmed safety findings, supporting deployment with enhanced monitoring and filters

## Why This Works (Mechanism)
The multi-layered evaluation approach combines quantitative benchmarks with qualitative expert assessment, providing both measurable performance metrics and nuanced risk evaluation. Automated benchmarks establish baseline capabilities while red-teaming reveals practical vulnerabilities. Independent audits add credibility through external validation. The framework's risk threshold approach ensures deployment decisions are based on objective safety criteria rather than subjective judgment alone.

## Foundational Learning

**Risk threshold methodology**: Establishes clear boundaries for safe deployment by defining specific metrics that must not be exceeded, preventing subjective risk assessment.

Quick check: Verify that threshold values are based on empirical evidence and expert consensus rather than arbitrary selection.

**Benchmark standardization**: Uses established metrics like WMDP and CyBench to enable consistent comparison across model versions and competitors.

Quick check: Ensure benchmark versions and scoring methodologies are documented and reproducible.

**Red-teaming protocols**: Employs domain experts to simulate real-world adversarial scenarios that automated tests might miss.

Quick check: Validate that red-teamers have appropriate credentials and follow standardized testing procedures.

## Architecture Onboarding

**Component map**: Automated benchmarks -> Red-teaming assessment -> Independent audit review -> Risk threshold evaluation -> Deployment decision

**Critical path**: Automated benchmark testing (WMDP, CyBench) → Red-teaming exercises → Independent audit review → Final risk assessment and deployment decision

**Design tradeoffs**: Automated benchmarks provide scalability but may miss nuanced risks; red-teaming offers depth but lacks standardization; independent audits add credibility but may have methodological limitations.

**Failure signatures**: Model exceeding risk thresholds in any domain, inconsistent audit findings, or failure to identify known risk scenarios during testing.

**First experiments**:
1. Run automated benchmarks (WMDP, CyBench) to establish baseline performance metrics
2. Conduct initial red-teaming session with domain experts to identify potential risk areas
3. Compare model responses against established safety guidelines to identify compliance gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on human judgment in red-teaming exercises may introduce subjectivity in risk assessment
- Evaluation scope limited to specific CBRN, offensive cyber, and AI R&D domains, potentially missing other critical risk areas
- Lack of detailed methodology for independent audits makes assessment of their rigor difficult

## Confidence

**High**: The model's improved factual accuracy and performance on established benchmarks (WMDP, CyBench) are well-documented and reproducible.

**Medium**: The independent audit findings (Nemesys, METR) support the safety claims, but the lack of detailed audit methodologies introduces uncertainty.

**Low**: The assessment of risk thresholds and the effectiveness of additional safeguards in radiological domains rely heavily on expert judgment, which may vary across evaluators.

## Next Checks

1. Conduct a follow-up evaluation with expanded adversarial testing to assess the model's resilience against novel attack vectors not covered in the initial study.

2. Perform a longitudinal study to monitor the model's safety performance and risk mitigation effectiveness during real-world deployment over an extended period.

3. Implement a transparent, peer-reviewed framework for independent audits to enhance the credibility and reproducibility of safety assessments.