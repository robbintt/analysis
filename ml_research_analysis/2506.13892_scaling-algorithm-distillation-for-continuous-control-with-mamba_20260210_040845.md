---
ver: rpa2
title: Scaling Algorithm Distillation for Continuous Control with Mamba
arxiv_id: '2506.13892'
source_url: https://arxiv.org/abs/2506.13892
tags:
- learning
- context
- tasks
- in-context
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Algorithm Distillation (AD) enables in-context reinforcement learning
  (ICRL) by modeling across-episodic training histories with a causal transformer,
  but its quadratic complexity limits it to short-horizon tasks. This work leverages
  Mamba, a Selective Structured State Space (S6) model with linear scaling, to scale
  AD to continuous control environments with long time horizons.
---

# Scaling Algorithm Distillation for Continuous Control with Mamba

## Quick Facts
- arXiv ID: 2506.13892
- Source URL: https://arxiv.org/abs/2506.13892
- Reference count: 39
- Primary result: Mamba-based Algorithm Distillation achieves better asymptotic performance than Decision Transformer across four continuous control tasks, with improvements ranging from slight to significant.

## Executive Summary
Algorithm Distillation (AD) enables in-context reinforcement learning by modeling across-episodic training histories with a sequence model. Traditional transformer-based AD suffers from quadratic complexity, limiting it to short-horizon tasks. This work introduces Mamba-based AD, leveraging the Selective Structured State Space (S6) model's linear scaling to handle longer contexts necessary for complex continuous control tasks. Mamba-AD demonstrates superior or competitive performance compared to Decision Transformer baselines and existing meta-RL methods across four MuJoCo environments.

## Method Summary
Mamba-based AD models cross-episodic learning histories by training a Selective Structured State Space model to autoregressively predict actions from transition tuples (state, action, reward, next-state). The model is pre-trained offline on trajectories generated by source RL algorithms (SAC, PPO, DroQ) with downsampling to reduce redundancy. At test time, the model performs in-context reinforcement learning without parameter updates, using its own generated trajectories to populate context. Transition-level tokenization concatenates complete state-action-reward-next-state transitions into single tokens, enabling longer effective horizons within computational constraints.

## Key Results
- Mamba-AD outperforms Decision Transformer in asymptotic performance across all four tested environments (Reacher-Goal, Pusher-Goal, Half-Cheetah-Vel, Ant-Dir)
- Longer context lengths improve performance in more complex tasks, demonstrating the importance of modeling long-range dependencies
- Mamba-AD achieves competitive results with online meta-RL (MQL) and outperforms offline meta-RL (MACAW) baselines while training purely offline
- Performance gains range from slight to significant depending on task complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training a sequence model to autoregressively predict actions from cross-episodic learning histories enables in-context reinforcement learning without parameter updates at test time.
- **Mechanism:** The model learns to imitate the exploration, credit assignment, and policy improvement behaviors present in the source RL algorithm's training data. At inference, it populates its own context through environment interaction, effectively "running" the distilled algorithm in-context.
- **Core assumption:** Learning histories contain sufficient signal about policy improvement dynamics; the model can approximate the source algorithm's policy improvement operator through behavior cloning alone.
- **Evidence anchors:**
  - [abstract]: "modeling across-episodic training histories autoregressively with a causal transformer model"
  - [section 3]: "correct action prediction given a learning history requires the model to approximate the policy improvement operator of the source algorithm"
  - [corpus]: RLBenchNet confirms architecture choice significantly impacts RL task performance across different network types.

### Mechanism 2
- **Claim:** Mamba's S6 architecture enables efficient scaling to long context lengths that are necessary for complex meta-RL tasks.
- **Mechanism:** Selective structured state space models scale linearly O(n) in sequence length versus attention's O(n²), allowing training with full or near-full learning trajectories (thousands of steps) rather than severely truncated contexts.
- **Core assumption:** Complex tasks with higher-dimensional state-action spaces and longer time horizons require longer-range credit assignment, which benefits from longer context windows.
- **Evidence anchors:**
  - [abstract]: "Mamba offers linear scaling with sequence length and better handling of long-range dependencies"
  - [section 5.3]: "more complex tasks require longer contexts, which highlights the importance of efficiently modeling long sequences"
  - [corpus]: Mamba-Shedder and ms-Mamba papers corroborate SSM efficiency for long-sequence modeling tasks.

### Mechanism 3
- **Claim:** Transition-level tokenization (concatenating state, action, reward, next-state into single tokens) preserves learning dynamics while reducing sequence length by 3×.
- **Mechanism:** Rather than tokenizing each (s, a, r) separately, the model processes complete transitions as single tokens, enabling longer effective horizons within the same compute budget.
- **Core assumption:** Per-step granularity within a transition is less critical than capturing the cross-episodic improvement patterns.
- **Evidence anchors:**
  - [section 5.2]: "we concatenate one-step transitions into a single token such that ci = (sit, ait, rit, sit+1)"
  - [section 5.2]: "both approaches were competitive, but the former entails using a context 3× longer, which exceeds our compute budget"
  - [corpus]: Weak/missing—no direct corpus evidence on tokenization strategies for RL sequence modeling.

## Foundational Learning

- **Concept: In-Context Learning vs. In-Weights Learning**
  - **Why needed here:** AD relies entirely on in-context adaptation—no gradient updates at test time. Understanding this distinction is essential to grasp why the model can adapt to new tasks without fine-tuning.
  - **Quick check question:** If you update model weights at test time, are you doing in-context learning or in-weights learning?

- **Concept: State Space Models (SSMs) and the S6 Layer**
  - **Why needed here:** Mamba's core building block. You need to understand how the recurrent state hk = Ahk−1 + Bxk enables linear scaling and fast inference.
  - **Quick check question:** Why does a recurrent formulation with a fixed state dimension scale linearly with sequence length while attention scales quadratically?

- **Concept: Meta-Reinforcement Learning Task Distribution**
  - **Why needed here:** AD is evaluated on a distribution of related tasks (pT). The model must generalize to unseen tasks from the same distribution, not just memorize training tasks.
  - **Quick check question:** If a task's goal were fully observable in the state, would meta-learning still be required?

## Architecture Onboarding

- **Component map:** Transition tokens (s_t, a_t, r_t, s_{t+1}) with Gaussian noise augmentation -> Mamba/S6 layers (6-8 layers, 384-512 dim) -> Continuous action prediction via MSE loss -> Adam optimizer with cosine learning rate schedule

- **Critical path:**
  1. Generate learning histories using source RL algorithms (PPO/SAC/DroQ) on training tasks
  2. Downsample by keeping 1 in k episodes (k=4–10 depending on environment)
  3. Pre-train Mamba-AD autoregressively on cross-episodic sequences
  4. Evaluate on unseen test tasks by letting the model interact and populate its own context

- **Design tradeoffs:**
  - Longer context → better performance on complex tasks but higher compute/memory
  - Higher downsampling rate (k) → faster training but risk of unstable ICRL if histories become too sparse
  - Transition-level tokenization → 3× more efficient than per-element tokenization but potentially loses fine-grained signal

- **Failure signatures:**
  - Model plateaus early without improvement → context may be too short for the task complexity
  - Erratic/volatile inference behavior → source algorithm may have been unstable (e.g., SAC with sharp drops)
  - Decision Transformer significantly underperforms Mamba → suggests task requires longer-range dependencies than attention can efficiently model

- **First 3 experiments:**
  1. **Baseline comparison:** Train Mamba-AD and Decision Transformer (same params, same data) on Reacher-Goal; compare asymptotic performance and training stability.
  2. **Context length ablation:** Train Mamba-AD on Half-Cheetah-Vel with context lengths ranging from 1 episode to full downsampled history; plot ICRL emergence vs. context size.
  3. **Generalization test:** After meta-training on 15–30 tasks, evaluate on held-out tasks (unseen velocities/directions) to confirm adaptation is not memorization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the specific distribution and sampling scheme of source data (e.g., stability vs. sample efficiency of the source algorithm) shape the in-context reinforcement learning abilities of the distilled model?
- **Basis in paper:** [explicit] Section 5.2 discusses the trade-off between using PPO (stable but slow) versus SAC/DroQ (efficient but volatile) for data generation, and explicitly states, "Characterizing the impact of data distribution is an interesting and important question which let for future work."
- **Why unresolved:** The authors chose different source algorithms for different environments to ensure data quality but did not ablate how the specific characteristics of the source algorithm's learning history (e.g., volatility vs. monotonic improvement) affect the distilled policy.
- **What evidence:** A comparative study where Mamba-AD is trained on datasets generated by different source algorithms (e.g., PPO vs. SAC) for the same environment, analyzing the resulting variance in ICRL performance.

### Open Question 2
- **Question:** Is there a formal relationship or scaling law connecting optimal context length to task complexity metrics such as state-action dimensionality and time-horizon?
- **Basis in paper:** [explicit] Section 5.3 observes that "optimal context length seems related to the task complexity," noting that simpler tasks preferred shorter contexts while complex tasks required full histories, but leaves the precise relationship undefined.
- **Why unresolved:** The experiments showed varying correlations (e.g., Half-Cheetah needed full history, Reacher-Goal did not), but the study did not quantify "complexity" to predict the necessary context window for a new task.
- **What evidence:** A systematic evaluation across a wider range of environments with varying state dimensions and horizon lengths to identify a function that predicts the optimal context length $C$ given task parameters.

### Open Question 3
- **Question:** Can Mamba-based Algorithm Distillation effectively generalize to environments with significantly longer horizons and higher exploration requirements than the continuous control benchmarks used in this study?
- **Basis in paper:** [explicit] Section 6 (Limitations) states: "Evaluating AD on environments with potentially very long horizon and/or which requires extensive exploration is worth while and an interesting extension to this work."
- **Why unresolved:** The selected Mujoco environments (episodes of 50-200 steps) are considered manageable even for transformers; it remains untested if Mamba's linear scaling provides a distinct qualitative advantage in "very long horizon" settings.
- **What evidence:** Benchmarking Mamba-AD on tasks specifically designed for long-term credit assignment (e.g., sparse reward navigation or hierarchical tasks) where sequence lengths exceed the effective limits of transformer attention.

## Limitations
- The study is limited to continuous control environments with relatively short horizons (50-200 steps), leaving open questions about performance in truly long-horizon settings.
- Optimal context length regimes are not precisely quantified, with exact absolute context lengths per task unspecified.
- Generalization boundaries to distribution shifts beyond training task parameters remain unexplored.

## Confidence
- **High confidence:** Mamba-AD outperforms Decision Transformer in asymptotic performance across tested tasks. This claim is directly supported by experimental results and the mechanism (linear vs quadratic scaling) is well-established in the SSM literature.
- **Medium confidence:** Longer context lengths improve performance in more complex tasks. While supported by results, the relationship between task complexity and required context length is not precisely quantified.
- **Medium confidence:** Mamba-AD is competitive with online meta-RL (MQL) and outperforms offline meta-RL (MACAW). This claim is based on relative performance comparisons, though the exact evaluation protocols for baselines are not fully specified.

## Next Checks
1. **Context length sensitivity analysis:** Systematically vary context lengths (from minimal to full downsampled history) for each task and measure ICRL emergence threshold and asymptotic performance to identify optimal context regimes per task complexity level.
2. **Distribution shift robustness test:** Evaluate Mamba-AD on tasks that systematically deviate from training task distributions (e.g., Reacher-Goal with novel goal positions beyond training range) to determine generalization boundaries.
3. **Cross-algorithm distillation comparison:** Train Mamba-AD using different source RL algorithms (PPO, SAC, DroQ) on the same tasks and compare ICRL performance to isolate the impact of source algorithm stability on distilled behavior quality.