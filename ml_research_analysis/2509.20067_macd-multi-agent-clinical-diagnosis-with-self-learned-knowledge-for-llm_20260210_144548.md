---
ver: rpa2
title: 'MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM'
arxiv_id: '2509.20067'
source_url: https://arxiv.org/abs/2509.20067
tags:
- uni00000048
- uni00000044
- uni00000011
- uni00000013
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the MACD framework, a multi-agent system
  that enables LLMs to self-learn clinical knowledge by simulating physician expertise
  development through experience accumulation. The framework comprises knowledge summarizer
  and refiner agents that distill and optimize diagnostic insights from real-world
  cases, and a diagnostician agent that applies this Self-Learned Knowledge to improve
  diagnostic accuracy.
---

# MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM

## Quick Facts
- arXiv ID: 2509.20067
- Source URL: https://arxiv.org/abs/2509.20067
- Reference count: 40
- LLM framework achieves up to 22.3% improvement in primary diagnostic accuracy compared to clinical guidelines using self-learned knowledge

## Executive Summary
This study introduces MACD (Multi-Agent Clinical Diagnosis), a framework that enables LLMs to self-learn clinical knowledge through experience accumulation, simulating physician expertise development. The system comprises knowledge summarizer and refiner agents that distill diagnostic insights from real-world cases, and diagnostician agents that apply this Self-Learned Knowledge to improve diagnostic accuracy. Evaluated on 4,390 real-world patient cases across seven diseases using Llama-3.1 (8B/70B) and DeepSeek-R1-Distill-Llama (70B), MACD significantly improves primary diagnostic accuracy by up to 22.3% compared to clinical guidelines. When compared to physician-only diagnosis, MACD achieves comparable or superior performance with up to 16% improvement.

## Method Summary
MACD uses a multi-agent pipeline where knowledge summarizer agents extract diagnostic concepts from correctly diagnosed cases in a sampling set, then knowledge refiner agents apply dual-filtering (redundancy removal via BioBERT embeddings and causal intervention via concept ablation) to produce structured Self-Learned Knowledge. Diagnostician agents use this knowledge as prompt context to diagnose test cases. The framework employs Llama-3.1-8B, Llama-3.1-70B, and DeepSeek-R1-Distill-Llama-70B models with temperature=0.01, top_k=1, top_p=0.05, and context window=16,384. Evaluation uses exact term matching and tolerant matching based on anatomical location for seven diseases (appendicitis, diverticulitis, pancreatitis, cholecystitis, pneumonia, pulmonary embolism, aortic dissection).

## Key Results
- Self-learned knowledge achieves up to 22.3% improvement in primary diagnostic accuracy compared to clinical guidelines
- MACD-human collaborative workflow achieves 18.6% improvement over physician-only diagnosis
- Model-specific knowledge personalization: each diagnostician agent achieves peak performance using self-generated knowledge, with accuracy drops of 7.2% when using cross-model knowledge

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation Through Multi-Agent Pipeline
LLMs perform better with knowledge representations that match their pretraining distribution and reasoning mechanisms rather than human-optimized clinical text. The knowledge summarizer extracts diagnostic concepts from correctly diagnosed cases, while the knowledge refiner applies dual-filtering to produce structured knowledge aligned with LLM reasoning patterns. Break condition: if sampling set contains predominantly atypical cases, distilled knowledge may encode incorrect patterns.

### Mechanism 2: Model-Specific Knowledge Personalization
Different LLMs develop unique linguistic styles and reasoning patterns during pretraining; self-generated knowledge matches these "cognitive styles," creating a comprehension advantage over externally-generated knowledge. Each diagnostician agent achieves optimal performance only when using knowledge generated by its own agent team, with significant accuracy drops when using cross-model knowledge. Break condition: transferability may increase when model architectures are highly similar.

### Mechanism 3: Collaborative Consensus With Human Escalation
Three diagnostician agents using different base models provide independent diagnoses; an evaluator agent checks for semantic consensus using BioBERT embeddings and cosine similarity threshold; cases without consensus are escalated to human physicians who receive agents' outputs as decision support. Model diversity reduces individual model blind spots. Break condition: if all agents share similar failure modes, consensus may be wrong.

## Foundational Learning

- **In-context knowledge injection vs. parametric fine-tuning**
  - Why needed: Framework relies on injecting self-learned knowledge as prompt context rather than updating model weights, affecting updatability, interpretability, and deployment complexity
  - Quick check: Can you articulate why the authors claim this approach is more "lightweight, flexible, and secure" than fine-tuning methods?

- **Causal intervention for concept importance**
  - Why needed: Knowledge refiner agent uses systematic concept ablation to measure each concept's impact on diagnostic accuracy, removing concepts that harm performance
  - Quick check: If removing concept X improves accuracy, what label does the system assign to concept X, and what action is taken?

- **Semantic consensus detection**
  - Why needed: MACD-human workflow doesn't use simple string matching; it normalizes diagnoses using tolerant matching rules then applies semantic similarity (BioBERT embeddings, cosine similarity threshold) to detect clinically meaningful consensus
  - Quick check: Why might "pericard effusion" be considered a tolerantly accurate diagnosis for "pericarditis" in this system's evaluation framework?

## Architecture Onboarding

- **Component map**: Sampling Cases → Knowledge Summarizer → Raw Concepts → Knowledge Refiner → Self-Learned Knowledge → Diagnostician Agent → Diagnosis + Rationale
- **Critical path**: Knowledge quality depends entirely on sampling set case quality and refiner's causal intervention accuracy. If sampling set contains misdiagnosed cases or causal intervention miscalculates concept importance, downstream diagnostician performance degrades.
- **Design tradeoffs**: Sampling set size vs. knowledge quality; consensus threshold vs. escalation rate; self-knowledge vs. cross-model transfer storage overhead
- **Failure signatures**: Low consensus rate (<50% initial) suggests poorly aligned knowledge sources; accuracy drops with cross-model knowledge indicate semantic style mismatch; effective opinion rate <80% suggests systematic blind spots
- **First 3 experiments**: 1) Baseline calibration: Run diagnostician agents with zero knowledge, professional guidelines, and Mayo Clinic guidelines on held-out test set; 2) Knowledge transfer matrix: Generate self-learned knowledge with each model, then test each model with each knowledge source (3×3 matrix); 3) Consensus threshold sweep: Vary semantic similarity threshold (0.7, 0.8, 0.9) and measure impact on consensus rate, accuracy, and human escalation frequency

## Open Questions the Paper Calls Out

- Can the MACD framework maintain its performance when extended to multimodal inputs (direct image interpretation) and applied to non-English or geographically diverse clinical datasets? Current evaluation relies on pre-processed text reports from single geographic/language source.
- How can Self-Learned Knowledge be optimally leveraged within the MACD-human collaborative consultation setting to maximize the framework's full potential? Current study evaluates based on whether at least one agent provides correct diagnosis as reference.
- To what degree can the current structured, manually-guided MACD workflow be transitioned into a fully-automated agent system without compromising diagnostic safety? Current framework uses structured pipeline that may require human oversight.

## Limitations
- Performance depends critically on quality of cases in sampling set - diagnostic errors or atypical presentations may encode incorrect patterns
- Causal intervention mechanism for filtering misleading concepts requires careful calibration to avoid removing important diagnostic context
- Semantic consensus mechanism relies on arbitrary BioBERT similarity threshold (0.8) that may not capture clinically meaningful diagnostic equivalence across all disease categories

## Confidence

**High confidence**: Model-specific personalization finding (Diagnostician agents achieve peak performance with self-generated knowledge), supported by systematic testing across three models with significant accuracy drops when using cross-model knowledge

**Medium confidence**: Collaborative consensus mechanism's superiority over physician-only diagnosis (18.6% improvement claim depends on specific evaluation methodology and may not generalize to all clinical contexts)

**Low confidence**: Generalizability of self-learned knowledge approach to diseases outside abdominal/thoracic focus and to clinical settings with different data distributions or resource constraints

## Next Checks

1. **Knowledge Transferability Stress Test**: Systematically test whether self-learned knowledge transfers across disease categories (e.g., using abdominal knowledge for thoracic diagnosis) and measure accuracy degradation patterns to establish true generalization boundaries

2. **Clinical Expert Validation**: Have board-certified physicians independently evaluate a stratified sample of self-learned knowledge concepts and diagnostic rationales for clinical accuracy, relevance, and potential safety concerns before deployment

3. **Temporal Robustness Evaluation**: Test whether self-learned knowledge maintains diagnostic accuracy when applied to cases from different time periods within MIMIC-IV to assess temporal generalizability and potential concept drift over time