---
ver: rpa2
title: On the use of graph models to achieve individual and group fairness
arxiv_id: '2601.08784'
source_url: https://arxiv.org/abs/2601.08784
tags:
- fairness
- which
- graph
- page
- sheaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fair Sheaf Diffusion (FSD), a unified framework
  that leverages cellular sheaf theory to achieve both individual and group fairness
  in machine learning. The method encodes fairness constraints as algebraic equations
  in the kernel of a linear map, enabling projection of data into a bias-free space.
---

# On the use of graph models to achieve individual and group fairness

## Quick Facts
- arXiv ID: 2601.08784
- Source URL: https://arxiv.org/abs/2601.08784
- Reference count: 40
- Primary result: FSD framework achieves both individual and group fairness via sheaf diffusion with closed-form SHAP values

## Executive Summary
This paper introduces Fair Sheaf Diffusion (FSD), a unified framework that leverages cellular sheaf theory to achieve both individual and group fairness in machine learning. The method encodes fairness constraints as algebraic equations in the kernel of a linear map, enabling projection of data into a bias-free space. Multiple network topologies are defined to handle different fairness metrics, allowing flexible integration as pre-, in-, or post-processor. The approach provides closed-form SHAP values, enhancing interpretability. Experiments on synthetic and real-world datasets demonstrate that FSD effectively improves fairness metrics—reducing independence bias by up to 50% and consistency bias by 33%—while maintaining competitive accuracy.

## Method Summary
FSD uses cellular sheaf theory to encode fairness constraints as algebraic equations in the kernel of a sheaf Laplacian. The framework defines multiple graph topologies (subset, kNN, unit ball) that induce specific fairness metrics through signal diffusion. The diffusion ODE dx/dt = -α∆Fx drives signals toward ker(∆F), projecting data onto a bias-free subspace. The method supports identity sheaves for pre/post-processing and vector sheaves for in-processing, with linear combinations enabling simultaneous satisfaction of multiple constraints. Implementation uses discrete diffusion for stability, and experiments compare five topologies across synthetic and real-world datasets.

## Key Results
- kNN topology achieves best combined fairness-accuracy trade-off, reducing independence bias by up to 50% and consistency bias by 33%
- Mixed topology (kNN + subset) provides superior individual fairness while maintaining group fairness
- Unit ball topology with small δ improves Lipschitz constant by 20% without significant accuracy loss
- FSD provides closed-form SHAP values, enabling direct interpretability of fairness corrections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sheaf diffusion projects data onto a subspace encoding fairness constraints with minimal information loss.
- Mechanism: A cellular sheaf defines a sheaf Laplacian ∆F whose kernel H⁰(G;F) contains signals satisfying fairness constraints. The diffusion ODE dx/dt = -α∆Fx drives signals toward ker(∆F) as t→∞. Theorem 3.1 guarantees convergence to the orthogonal projection of the initial signal, preserving maximal information.
- Core assumption: Fairness constraints can be algebraically encoded as F_{v≤e}xᵥ = F_{u≤e}xᵤ for edges in a constructed graph topology.
- Evidence anchors:
  - [abstract] "encodes fairness constrains [sic] as algebraic equations in the kernel of a linear map, enabling projection of data into a bias-free space"
  - [Section 3.4.2] Theorem 3.1: "In the limit t→∞ the signal xᵗ tends to the orthogonal projection of x⁰ onto ker ∆F"
  - [corpus] No direct corpus evidence for this specific sheaf-based mechanism; related work (FairAD, arXiv:2510.27136) uses algebraic distance for graph clustering fairness but not sheaf theory.
- Break condition: When the sheaf Laplacian is poorly conditioned (e.g., high-degree aggregator nodes in subset topology), numerical instability may prevent convergence; discrete implementation with large α can also cause instability.

### Mechanism 2
- Claim: Different graph topologies induce specific fairness metrics by structuring which nodes communicate through the diffusion process.
- Mechanism: The subset topology connects all observations within each group to a virtual representative node; fully connecting representatives enforces equal group averages → Independence. The kNN topology enforces xᵥ ≈ mean(xₙₑᵢ₉ₕbₒᵣₛ) → Consistency. The unit ball topology smooths within local radius δ → reduces local Lipschitz constant.
- Core assumption: Fairness corresponds to signal smoothness under specific neighborhood structures; neighbors should have similar predictions.
- Evidence anchors:
  - [Section 4.2.2] "When using a fully connected graph on the partition induced by a sensitive attribute... the subsequent scores are independent of the sensitive attribute"
  - [Section 4.2.3] kNN configuration "aggregates the signal over the k closest individuals to v, thus encouraging consistency"
  - [corpus] Fair graph clustering work (FairAD, arXiv:2510.27136) similarly uses graph topology for fairness but via algebraic distance rather than sheaf kernels.
- Break condition: When neighbors are not diverse (e.g., segregated communities), group fairness metrics may not improve despite local smoothing.

### Mechanism 3
- Claim: Linear combination of sheaves achieves simultaneous satisfaction of multiple fairness constraints.
- Mechanism: Given sheaves Fᵢ with Laplacians ∆Fᵢ, the combined sheaf has Laplacian ∆F = Σwᵢ∆Fᵢ. Lemma 4.6 proves ker(∆F) = ∩ker(∆Fᵢ), so diffusion converges to signals satisfying all constraints.
- Core assumption: Fairness constraints are compatible (intersection is non-empty); infeasible constraint combinations will yield poor accuracy.
- Evidence anchors:
  - [Section 4.2.5] Lemma 4.6 and Definition 4.5 formalize sheaf combination
  - [Section 5.4.2] Mixed configurations show trade-offs; "the best results are obtained when giving equal weight to both topologies"
  - [corpus] Trade-offs paper (arXiv:2602.00094) surveys individual-group fairness trade-offs broadly but not this intersection mechanism.
- Break condition: When wₛᵤbₛₑₜ approaches 1 in mixed configurations, numerical instability emerges due to high-degree aggregator nodes; continuous implementation shows chaotic behavior.

## Foundational Learning

- Concept: Graph Laplacian and Heat Diffusion
  - Why needed here: Sheaf diffusion generalizes graph heat diffusion; understanding L = D - A and how dx/dt = -Lx smooths signals is prerequisite.
  - Quick check question: Given a 3-node path graph, what is the kernel of its Laplacian?

- Concept: Group vs Individual Fairness Metrics
  - Why needed here: FSD targets IND (independence), SEP (separation), CON (consistency), LIP (Lipschitz); each topology optimizes different metrics.
  - Quick check question: If P(Ŷ=1|A=1) = 0.6 and P(Ŷ=1|A=0) = 0.4, what is IND? Is this fair under independence?

- Concept: Linear Algebra Projection Theory
  - Why needed here: The mechanism relies on orthogonal projection onto ker(∆F); understanding why this minimizes information loss requires knowing projection matrices preserve components in the target subspace.
  - Quick check question: If P is a projection onto subspace V, what is ||x - Px||² + ||Px||²?

## Architecture Onboarding

- Component map:
  - Input: Dataset D = (X, Y, A) with sensitive attribute A
  - Topology Constructor: Builds graph G based on chosen fairness target (subset/kNN/unit ball)
  - Sheaf Definition: F with restriction maps encoding constraints (identity sheaf for pre/post-processing; β-vector sheaf for in-processing)
  - Diffusion Engine: Computes D = exp(-αt∆F) [continuous] or D = (I - α∆F)ⁿ [discrete]
  - Output: Transformed signal xᵗ = Dx⁰ → linear classifier → predictions

- Critical path:
  1. Choose fairness target (IND → subset, CON → kNN, LIP → unit ball, multiple → mixed)
  2. Construct topology from data (requires computing distances for local topologies)
  3. Build sheaf Laplacian (identity sheaf: L₁ = Lw ⊗ I; vector sheaf: Lβ = Lw ⊗ ββᵀ)
  4. Run discrete diffusion (recommended over continuous due to stability)
  5. Train logistic regression on diffused features or apply to logits

- Design tradeoffs:
  - Discrete vs Continuous: Discrete (more stable, predictable); Continuous (chaotic, longer training)
  - α strength: Higher α → more fairness, lower accuracy; instability above α≈1 for subset topology
  - Topology depth (n layers): More layers → more fairness; local topologies converge to trivial solution if too deep
  - Mixed weight wₛᵤbₛₑₜ: Equal weights (0.5) recommended; high weights cause instability

- Failure signatures:
  - Subset topology instability: Rapid accuracy drop, exploding metrics when α > 1 or n large
  - Continuous mode chaos: Unpredictable fairness-accuracy trade-offs, metrics do not monotonically improve
  - Sparse unit ball: When δ too small, graph has few edges → behaves like logistic regression with minimal fairness improvement
  - Segregated neighbors: kNN improves consistency but not independence if k-neighbors lack group diversity

- First 3 experiments:
  1. **Sanity check on synthetic data**: Generate data per Section 5.1 with known bias structure; verify kNN topology reduces CON by ~30% while subset topology reduces IND (check if matching Figure 3 distributions).
  2. **Hyperparameter sweep on German dataset**: Fix kNN topology, vary α ∈ {0.1, 0.3, 0.5} and n ∈ {5, 10, 20}; plot Pareto frontier of ACC vs CON to validate trade-off curves in Figure 5.
  3. **Topology comparison on Compas**: Run all five topologies (kNN, mixed-kNN, unit, mixed-unit, subset) with default hyperparameters from Table A.7; verify kNN achieves best combined IND+CON improvement per Table B.11 pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hand-crafted sheaves in FSD be replaced with learned sheaves in an end-to-end framework while maintaining fairness guarantees?
- Basis in paper: [explicit] "Our approach is similar to Hansen and Gebhart (2020) in that we propose a set of hand-crafted sheaves, leaving the question on how to extend our methodology to an end-to-end framework like Bodnar et al. (2022) for future research."
- Why unresolved: Learning sheaf parameters end-to-end introduces optimization complexity; it is unclear whether fairness constraints encoded in the kernel would remain satisfiable during gradient-based training.
- What evidence would resolve it: An end-to-end trainable FSD model achieving comparable or better fairness-accuracy trade-offs than hand-crafted versions on standard benchmarks.

### Open Question 2
- Question: Would non-linear sheaf diffusion better model non-linear fairness constraints and metrics that linear FSD cannot capture?
- Basis in paper: [explicit] "It could be the case that non-linear SD might help with this goal, but this approach is outside the scope of the current work" and "...the use of non-linear sheaf diffusion (Zaghen et al., 2024) to model non-linear constraints and metrics..."
- Why unresolved: The Dirichlet energy minimization guarantees for linear SD do not directly extend to non-linear variants, making theoretical fairness guarantees unclear.
- What evidence would resolve it: Theoretical analysis establishing convergence properties for non-linear FSD, plus empirical gains on fairness metrics that linear models struggle with.

### Open Question 3
- Question: Can concatenating multiple sheaf diffusion stages create a hybrid processor with superior fairness-accuracy trade-offs compared to single-stage FSD?
- Basis in paper: [explicit] "...the concatenation of sheaf diffusion processes is an inherently hybrid fairness method combining different kinds of processors, being, to our knowledge, the first of its kind which will be the object of future study."
- Why unresolved: Multiple diffusion stages may over-smooth signals or introduce compounding approximation errors; optimal ordering and combination rules remain undefined.
- What evidence would resolve it: Empirical evaluation of staged FSD pipelines showing Pareto-dominant solutions over single-stage alternatives.

### Open Question 4
- Question: Would constructing network topologies using alternative distance metrics (e.g., Mahalanobis distance) improve consistency under the Euclidean metric used for evaluation?
- Basis in paper: [explicit] "For example, creating network topologies based on the Mahalanobis distance might lead to a better consistency result with the euclidean metric. Nonetheless, this connection is not obvious to us and we relegate it to future work."
- Why unresolved: The relationship between the metric used for graph construction and the metric used for fairness evaluation is theoretically unclear.
- What evidence would resolve it: Systematic experiments varying construction metrics while holding evaluation metrics constant, with analysis of transfer effects.

## Limitations

- Numerical instability in subset topology when α > 1 or with many diffusion layers due to high-degree aggregator nodes
- Continuous diffusion implementation exhibits chaotic behavior, requiring stable discrete alternative despite theoretical convergence guarantees
- O(N²) complexity for kNN and unit ball topologies may limit scalability to large graphs

## Confidence

- **High Confidence**: The core mechanism of sheaf diffusion as a projection onto fairness-constrained subspaces (Theorem 3.1); effectiveness of kNN topology for consistency improvement; linear combination of sheaves achieving multiple fairness constraints (Lemma 4.6).
- **Medium Confidence**: The practical implementation details for subset topology virtual nodes; numerical stability thresholds for different topologies; exact Pareto-optimal hyperparameter settings across datasets.
- **Low Confidence**: Generalization to non-tabular data; scalability to large graphs with millions of nodes; performance in highly imbalanced or multi-class settings.

## Next Checks

1. **Numerical Stability Analysis**: Systematically test the discrete implementation's stability bounds across α ∈ [0.1, 1.0] and n ∈ [1, 50] for all topologies, measuring divergence metrics and accuracy degradation to validate the instability claims for subset topology.

2. **Scalability Benchmarking**: Evaluate FSD on synthetic datasets scaled from 1K to 1M nodes, measuring diffusion runtime and memory usage to confirm whether the O(N²) neighbor computation becomes prohibitive as claimed.

3. **Transfer to Non-Standard Data**: Apply FSD to image or text classification tasks (e.g., fair facial recognition or sentiment analysis) using graph constructions based on feature similarity, testing whether the sheaf-theoretic framework generalizes beyond tabular data.