---
ver: rpa2
title: Policy Gradient with Second Order Momentum
arxiv_id: '2505.11561'
source_url: https://arxiv.org/abs/2505.11561
tags:
- policy
- gradient
- learning
- hessian
- second-order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Policy Gradient with Second-Order Momentum
  (PG-SOM), a lightweight extension of the REINFORCE algorithm that incorporates diagonal
  Hessian information to improve optimization stability and sample efficiency in reinforcement
  learning. PG-SOM maintains exponentially weighted averages of both the gradient
  and a diagonal approximation of the Hessian, using the latter as a per-parameter
  preconditioner in an Adam-style adaptive update.
---

# Policy Gradient with Second Order Momentum

## Quick Facts
- arXiv ID: 2505.11561
- Source URL: https://arxiv.org/abs/2505.11561
- Authors: Tianyu Sun
- Reference count: 19
- Key outcome: PG-SOM achieves up to 2.1× faster learning and substantially lower variance compared to first-order and Fisher-matrix baselines on standard control benchmarks

## Executive Summary
This paper introduces Policy Gradient with Second-Order Momentum (PG-SOM), a lightweight extension of the REINFORCE algorithm that incorporates diagonal Hessian information to improve optimization stability and sample efficiency in reinforcement learning. PG-SOM maintains exponentially weighted averages of both the gradient and a diagonal approximation of the Hessian, using the latter as a per-parameter preconditioner in an Adam-style adaptive update. The authors prove that the diagonal Hessian estimator is unbiased and that the resulting update direction is a descent direction in expectation under mild assumptions. Experiments on standard control benchmarks show that PG-SOM achieves up to 2.1× faster learning and substantially lower variance compared to first-order and Fisher-matrix baselines, while only requiring O(D) additional memory for a D-parameter policy. Ablation studies confirm that gradient clipping is crucial for stabilizing training, with the Runge-Kutta variant (PG-SOM) plus clipping delivering the best overall performance.

## Method Summary
PG-SOM extends REINFORCE by incorporating diagonal Hessian information as a curvature-based preconditioner. The method computes an unbiased estimate of the diagonal Hessian through score-function decomposition, maintains it as an exponentially weighted average alongside the gradient, and uses the inverse diagonal Hessian to adaptively rescale each parameter's update. The algorithm follows an Adam-style framework with bias correction and includes gradient clipping as a critical stabilizer. Three variants are tested: vanilla policy gradient, Hessian policy gradient, and a Runge-Kutta style update that combines gradients at current and lookahead parameter values. The approach maintains O(D) additional memory for the diagonal Hessian and requires approximately 1.8× more computation per update due to the second backward pass needed for Hessian computation.

## Key Results
- PG-SOM achieves up to 2.1× faster learning compared to first-order and Fisher-matrix baselines
- Hessian preconditioning alone reduces variance but can underperform vanilla PG without stabilization
- Gradient clipping is essential: Hessian PG + clipping improves mean return from 147.0 to 392.8 (+167%) while reducing std from 24.5 to 0.6
- The best variant (Runge-Kutta + clipping) achieves ~383 mean return with std ~7 across 5 seeds
- PG-SOM requires O(D) additional memory and approximately 1.8× more computation per update

## Why This Works (Mechanism)

### Mechanism 1: Diagonal Hessian Preconditioning
The algorithm computes a diagonal approximation of the Hessian ∇²J(θ), maintains it as an exponentially weighted average, and uses its inverse as a preconditioner: θ_{t+1} ← θ_t + η · ĥ^{-1} ⊙ ĝ. This adaptively rescales each coordinate based on local curvature, accelerating convergence by normalizing gradient magnitudes across dimensions with vastly different sensitivities.

### Mechanism 2: Unbiased Hessian Estimator with Score-Function Decomposition
The Hessian can be estimated from on-policy trajectories without knowledge of environment dynamics using the decomposition ∇²J(θ) = E[∇²Ψ + (∇ ln p)(∇Ψ)]. Lemma 3.2 shows ∇θ ln p(τ) = Σ ∇θ ln π(a_h|s_h), which depends only on policy log-probabilities, enabling computation purely from sampled rollouts.

### Mechanism 3: Gradient Clipping as Critical Stabilizer
Clipping bounds gradient magnitude, preventing rare large updates. Ablation shows Hessian PG + clipping improves mean return from 147.0 to 392.8 (+167%) while reducing std from 24.5 to 0.6, demonstrating that curvature-aware updates alone do not prevent catastrophic gradient spikes.

## Foundational Learning

- **Concept: REINFORCE / Policy Gradient Theorem**
  - Why needed here: PG-SOM builds directly on REINFORCE; understanding ∇J(θ) = E[∇ ln π(a|s) · Q(s,a)] is prerequisite to grasping how Hessian terms extend it
  - Quick check question: Can you derive why the score function ∇θ ln π(a|s) appears in the policy gradient?

- **Concept: Exponential Moving Averages with Bias Correction**
  - Why needed here: PG-SOM uses Adam-style moments (m_t, v_t) with bias correction terms (1-β^t); misunderstanding these leads to incorrect early-training behavior
  - Quick check question: Why does bias correction matter more in early iterations than late training?

- **Concept: Hessian as Curvature / Second-Order Optimization**
  - Why needed here: The core innovation is using diagonal Hessian as preconditioner; intuition for why curvature affects learning rate per dimension is essential
  - Quick check question: In a 2D loss landscape, what happens to gradient descent if one direction has 100× larger curvature than the other?

## Architecture Onboarding

- **Component map:** Trajectory Sampler → [states, actions, rewards] → Gradient Estimator → ∇θ J(θ) via REINFORCE → Hessian Estimator → diag(∇²θ J(θ)) via automatic differentiation → Moment Buffers → g_t (gradient EMA), h_t (Hessian EMA) → Bias Correction → ĝ_t, ĥ_t → Preconditioned Update → θ_{t+1} = θ_t + η · ĥ_t^{-1} ⊙ ĝ_t → [Optional] Gradient Clipping → applied before or after preconditioning

- **Critical path:** The Hessian diagonal computation (second backward pass) is the new bottleneck. Ensure autodiff correctly computes diag(∇²) without materializing full Hessian.

- **Design tradeoffs:**
  - Memory vs. accuracy: Full diagonal is O(D) memory, cheaper than K-FAC's block structure but less expressive
  - Wall-clock vs. sample efficiency: ~1.8× slower per update but ~2.1× fewer samples needed
  - Clipping threshold: Paper uses 50 with doubled learning rate; too low constrains exploration, too high loses stability

- **Failure signatures:**
  - Exploding variance early in training → check Hessian diagonal for near-zero values causing division instability; add ε to denominator
  - No improvement over vanilla REINFORCE → verify Hessian is being computed (not just gradient); check β₂ is not too high (stale curvature)
  - Seed-to-seed variance remains high → clipping may be disabled or threshold too permissive

- **First 3 experiments:**
  1. **Baseline replication:** Implement vanilla REINFORCE on CartPole; confirm ~350 episodes to reach reward 200 as reference
  2. **Ablation sweep:** Test PG-SOM with/without clipping, with/without Hessian (ablate to Adam-style only), measuring both sample efficiency and variance across 5 seeds
  3. **Hyperparameter sensitivity:** Vary β₂ (0.9, 0.99, 0.999) and clipping threshold (10, 50, 100) to identify stable operating region before scaling to harder environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PG-SOM be effectively extended to continuous action spaces with neural network policies?
- Basis in paper: Section 5 states "extending the proposed approach to handle environments with continuous action spaces holds significant promise" and "Integrating second-order momentum with neural network-based policies tailored for continuous action spaces presents a compelling avenue for exploration."
- Why unresolved: Current experiments are limited to CartPole (discrete actions); continuous spaces require different policy parameterizations that may affect the diagonal Hessian estimator's unbiasedness and positive-definiteness guarantees
- What evidence would resolve it: Evaluation on MuJoCo locomotion benchmarks showing comparable sample efficiency gains with continuous policies

### Open Question 2
- Question: Can dynamically adapting momentum parameters β₁ and β₂ during training improve PG-SOM's convergence?
- Basis in paper: Section 5 proposes "exploring dynamic adaptation of momentum techniques" to let "the algorithm adapt more efficiently to changing environments and improve convergence properties."
- Why unresolved: Fixed momentum factors may not suit non-stationary RL objectives where gradient statistics shift as the policy improves
- What evidence would resolve it: Comparison of fixed vs. scheduled/adaptive β₁, β₂ across environments with varying reward sparsity

### Open Question 3
- Question: How does PG-SOM scale to high-dimensional policies and complex environments beyond simple control tasks?
- Basis in paper: Experiments are limited to CartPole; the 1.8× wall-clock overhead and diagonal approximation accuracy remain untested on high-dimensional networks typical in modern RL
- Why unresolved: The diagonal Hessian may become less informative as parameter count grows, and computational overhead may compound
- What evidence would resolve it: Evaluation on high-dimensional benchmarks (e.g., Humanoid-v4) with scaling analysis

## Limitations
- The diagonal Hessian approximation may be ineffective in complex environments with strong parameter correlations
- Study is confined to low-dimensional control tasks without validation on high-dimensional image-based domains
- No comparison against modern second-order methods like K-FAC or AdaHessian in RL contexts

## Confidence
- **High Confidence:** The unbiasedness of the diagonal Hessian estimator and its implementation via score-function decomposition are rigorously proven and well-specified. The computational/memory cost claims (O(D) additional memory, 1.8× slower per step) are plausible given the second backward pass requirement.
- **Medium Confidence:** The empirical gains in sample efficiency (2.1× faster learning) and variance reduction are demonstrated on standard benchmarks but lack statistical power analysis. The necessity of gradient clipping is strongly supported by ablation, but the specific threshold (50) and doubled learning rate (0.004) appear tuned rather than derived from theory.
- **Low Confidence:** Generalization to more complex environments is speculative. The paper does not test against modern second-order methods like K-FAC or AdaHessian in RL contexts, nor does it explore adaptive clipping or curvature-based learning rate schedules.

## Next Checks
1. **Off-diagonal impact test:** Compare PG-SOM against a block-diagonal Hessian approximation on a low-dimensional task to quantify the cost of ignoring parameter correlations
2. **High-dimensional stress test:** Evaluate PG-SOM on a pixel-based continuous control task (e.g., DeepMind Control Suite) to assess scalability and robustness to high gradient variance
3. **Ablation of stabilization techniques:** Systematically remove entropy regularization, baseline, and clipping in a factorial design to isolate each component's contribution to stability and performance