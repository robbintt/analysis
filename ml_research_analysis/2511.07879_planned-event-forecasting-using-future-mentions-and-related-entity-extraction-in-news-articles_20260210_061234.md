---
ver: rpa2
title: Planned Event Forecasting using Future Mentions and Related Entity Extraction
  in News Articles
arxiv_id: '2511.07879'
source_url: https://arxiv.org/abs/2511.07879
tags:
- related
- entities
- event
- entity
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a method to forecast planned civil unrest events
  by analyzing future mentions and related entities in news articles. It addresses
  the challenge of identifying not just any entities in a news article, but those
  specifically involved in the event (termed "Related Entities").
---

# Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles

## Quick Facts
- arXiv ID: 2511.07879
- Source URL: https://arxiv.org/abs/2511.07879
- Authors: Neelesh Kumar Shukla; Pranay Sanghvi
- Reference count: 2
- One-line primary result: Method forecasts planned civil unrest events with 85% precision and 69.5% recall for relevant article identification

## Executive Summary
This work develops a method to forecast planned civil unrest events by analyzing future mentions and related entities in news articles. It addresses the challenge of identifying not just any entities in a news article, but those specifically involved in the event (termed "Related Entities"). The approach uses word2vec and topic modeling to filter relevant articles, then applies Named Entity Recognition and a novel "Related Entity Extraction" method to identify key event attributes: date, location, person, and organization. The system provides geographically independent, generalized modeling for forecasting planned social unrest events.

## Method Summary
The method employs a multi-stage pipeline to forecast planned civil unrest events from news articles. First, word2vec embeddings expand seed keywords ("protest", "demonstration") to capture domain-specific terminology including local language terms. Articles are then filtered through keyword presence and LDA topic modeling to isolate protest-related content. Named Entity Recognition extracts entity types (Person, Organization, Location, Date), while a relation extraction component builds triplets to identify entity relationships. Finally, a window-based "Related Entity Extraction" method uses lexicon patterns and spatial locality to identify entities specifically involved in the event, distinguishing them from general entity mentions.

## Key Results
- Relevant document identification: 85% precision, 69.5% recall, 76.6% F-measure
- Related entity extraction: 64.3% precision, 63% recall, 87% accuracy
- Successfully extracts event attributes: date, location, person, and organization
- System operates across geographically independent news sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual vocabulary expansion captures domain-specific terminology that synonym-based approaches miss.
- Mechanism: Word2vec embeddings trained on the target corpus identify words sharing context with seed terms ("protest", "demonstration"), surfacing both synonyms and domain-specific loanwords (e.g., "bandh", "dharna") that would be missed by WordNet synonyms alone.
- Core assumption: Articles about civil unrest use a distinguishable lexical pattern that concentrates around seed terminology.
- Evidence anchors: [abstract] "uses word2vec and topic modeling to filter relevant news articles" [section 3.2] "We took 'protest' and 'demonstration' as target words and generated words having similar context. We...were able to get the words in local language like 'bandh', 'dharna'"

### Mechanism 2
- Claim: Two-stage filtering (keyword presence + LDA topic modeling) reduces false positives from polysemous keywords.
- Mechanism: Initial keyword filtering casts a wide net. LDA then models document-topic distributions to isolate articles where protest-related terms appear in coherent topical context rather than tangential mentions (e.g., "surgical strike" filtered out from "strike").
- Core assumption: Protest-related articles form a distinct topic cluster separable from general political news.
- Evidence anchors: [abstract] "uses topic modeling and word2vec to filter relevant news articles" [section 4.2] "LDA was returning all the political articles which were noise for us. To resolve this issue, we focused on improving the density of protest articles"

### Mechanism 3
- Claim: Related entities co-occur in local text windows and relation triplets more frequently than unrelated entities.
- Mechanism: Named entities are extracted via NER. Relation extraction builds triplets (<Entity1, Verb Phrase, Entity2>). A lexicon-based pattern matcher identifies "anchor" relations containing one known event-related entity (e.g., "call on <DATE>"). A sliding window then captures nearby relations, extracting co-occurring entities as "related." Article titles are checked separately as high-signal zones.
- Core assumption: Spatial locality in text correlates with semantic relatedness to the event.
- Evidence anchors: [abstract] "proposes a method to extract them, referred to as Related Entity Extraction" [section 3.3] "By analyzing the articles we found that related entities occur near by...if we can locate one related entity, we can locate other related entities nearby"

## Foundational Learning

- Concept: **Word2vec skip-gram embeddings**
  - Why needed here: Expands seed vocabulary using distributional semantics; requires understanding that words appearing in similar contexts embed near each other in vector space.
  - Quick check question: Given a corpus, can you explain why "bandh" would embed near "protest" even though they share no lexical overlap?

- Concept: **Latent Dirichlet Allocation (LDA)**
  - Why needed here: Models documents as mixtures of topics; used to filter articles where protest terms appear in coherent topical context.
  - Quick check question: If LDA returns too many general political articles, what parameter or preprocessing step would you adjust first?

- Concept: **Named Entity Recognition (NER) with CRF sequence models**
  - Why needed here: Extracts structured entity types (Person, Organization, Location, Date) from unstructured text; Stanford NER uses linear-chain CRFs.
  - Quick check question: Why might a CRF-based NER tag "February 16" as a DATE but fail to normalize "next Wednesday"?

## Architecture Onboarding

- Component map:
  RSS Crawler -> Preprocessor -> Word2vec Vocabulary Learner -> Keyword Filter -> LDA Topic Model -> Stanford NER -> Stanford Relation Extractor -> Window-Based Related Entity Extractor -> Time Normalizer

- Critical path: Word2vec -> Keyword Filter -> LDA -> NER -> Relation Extraction -> Window-Based Extraction. Errors propagate; early false negatives cannot be recovered.

- Design tradeoffs:
  - Precision vs. recall in keyword cutoff (0.68 similarity threshold chosen empirically)
  - LDA topic count selection (tested 50, 100, 150) trades granularity vs. topic coherence
  - Ignoring coreference and entity disambiguation simplifies implementation but reduces accuracy on long documents

- Failure signatures:
  - High recall, low precision on article filtering -> keyword cutoff too permissive or LDA under-separates topics
  - Missing related entities -> window size too narrow or lexicon patterns incomplete
  - False related entities -> anchor patterns match non-event contexts (e.g., "called on" in non-protest contexts)

- First 3 experiments:
  1. **Validate vocabulary expansion**: Train word2vec on sample corpus; manually inspect top-20 neighbors of "protest" and "demonstration" for relevance and domain coverage.
  2. **Benchmark LDA filtering**: Apply LDA to keyword-filtered corpus with varying topic counts; measure precision/recall against human-labeled relevance judgments on a held-out sample.
  3. **Ablate window size**: Run Related Entity Extraction with window sizes of 1, 3, 5 relations; compare precision/recall on relation triplet relevance to identify optimal locality assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can text summarization techniques be effectively integrated to forecast the underlying purpose or cause of the planned civil unrest event?
- Basis in paper: [explicit] The Conclusion states, "apart from giving entities involved in the event, purpose of that event could also be forecasted by using text summarization techniques."
- Why unresolved: The current system focuses exclusively on extracting attributes (Who, When, Where) and explicitly does not extract the "Why" (purpose) of the event.
- What evidence would resolve it: A modified system architecture that includes a summarization module capable of extracting event purpose with measurable precision and recall.

### Open Question 2
- Question: To what extent can sentiment analysis accurately define the intensity of a social unrest event to aid administrative planning?
- Basis in paper: [explicit] The authors suggest in the Conclusion that "sentiment analysis techniques [could be applied] to define the intensity of the event, which can help authorities to plan according to that."
- Why unresolved: The current work filters articles based on keywords and topics but does not measure the severity or emotional intensity of the forecasted events.
- What evidence would resolve it: Experimental results correlating sentiment scores derived from news articles with ground-truth measures of event intensity (e.g., crowd size or disruption level).

### Open Question 3
- Question: How does the incorporation of entity co-referencing and disambiguation impact the accuracy of the Related Entity Extraction model?
- Basis in paper: [explicit] The paper notes that the method "ignored co-reference and entity disambiguation issues" and concludes that these could be "incorporated later for further improvement."
- Why unresolved: The system currently treats entity mentions in isolation or simple window relations, potentially missing connections between entities referred to by pronouns or aliases.
- What evidence would resolve it: A comparison of F-measure scores for the current "Related Entity Extraction" method against a version enhanced with a coreference resolution layer.

### Open Question 4
- Question: Does the "geographically independent" model maintain performance when applied to news sources outside of the specific Indian context tested?
- Basis in paper: [inferred] The abstract claims the model is "geographically independent," but the experiment relies solely on articles from four specific Indian news outlets (Times of India, Hindustan Times, etc.) collected over a two-month period.
- Why unresolved: The lexicon and word2vec models may be overfitted to the specific political terminology and linguistic nuances of the Indian dataset (e.g., local terms like 'Dharna', 'Bandh'), leaving the generalizability to other democracies unproven.
- What evidence would resolve it: Evaluation of the system's precision and recall on a dataset of news articles from a different geographical region, such as political news from North America or Europe.

## Limitations

- The methodology makes strong assumptions about textual locality and topical coherence that may not generalize beyond Indian civil unrest news.
- The approach does not handle entity disambiguation or coreference resolution, which could lead to false positives when the same entity name appears in multiple contexts.
- The LDA topic modeling parameters (particularly the threshold for assigning articles to the "protest" topic) are not clearly defined, introducing potential variability in document filtering performance.

## Confidence

- **High Confidence**: The vocabulary expansion mechanism using word2vec to capture domain-specific terminology (bandh, dharna) from seed words - this is well-supported by the method description and aligns with established distributional semantics principles.
- **Medium Confidence**: The two-stage filtering approach (keyword presence + LDA topic modeling) for reducing false positives - while the method is clearly described, the effectiveness depends heavily on un-specified parameters like topic filtering thresholds.
- **Low Confidence**: The window-based related entity extraction performance metrics - the 64.3% precision and 63% recall are reported without clear explanation of how the evaluation set was constructed or how closely the test conditions matched the training context.

## Next Checks

1. **Vocabulary Expansion Validation**: Train word2vec on a sample of the corpus and manually inspect the top-20 nearest neighbors of "protest" and "demonstration" to verify they capture relevant domain-specific terms (bandh, dharna) rather than irrelevant synonyms.

2. **Topic Filtering Threshold Test**: Apply LDA to a keyword-filtered subset of articles with varying topic counts (50, 100, 150) and systematically test different probability thresholds for assigning documents to the protest topic, measuring precision/recall against human-labeled relevance judgments.

3. **Window Size Ablation Study**: Run the related entity extraction pipeline with multiple window sizes (1, 3, 5 relations) on a held-out test set and compare precision/recall to empirically determine the optimal locality assumption for this domain.