---
ver: rpa2
title: 'Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via
  Difference Vectors'
arxiv_id: '2511.17987'
source_url: https://arxiv.org/abs/2511.17987
tags:
- task
- difference
- dv-basi
- vectors
- arithmetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DV-BASI, a novel multi-step optimization\
  \ framework that extends task arithmetic by leveraging difference vectors as directed\
  \ perturbations to overcome optimization stagnation. The key idea is to use difference\
  \ vectors\u2014generalized task vectors derived from historical weight movements\u2014\
  to continuously escape local optima through anisotropic scaling, without relying\
  \ on additional model components."
---

# Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors

## Quick Facts
- **arXiv ID:** 2511.17987
- **Source URL:** https://arxiv.org/abs/2511.17987
- **Authors:** Jinping Wang; Zhiqiang Gao; Dinggen Zhang; Zhiwu Xie
- **Reference count:** 18
- **Primary result:** DV-BASI improves multi-task model merging accuracy by up to 2.1% over existing methods using difference vectors for iterative optimization

## Executive Summary
This paper introduces DV-BASI, a novel multi-step optimization framework that extends task arithmetic by leveraging difference vectors as directed perturbations to overcome optimization stagnation. The key idea is to use difference vectors—generalized task vectors derived from historical weight movements—to continuously escape local optima through anisotropic scaling, without relying on additional model components. Empirically, DV-BASI achieves state-of-the-art performance in both supervised and unsupervised settings, improving multi-task model merging accuracy by up to 2.1% over existing methods. Notably, the average performance of multi-task models merged via DV-BASI can even exceed individually fine-tuned models, and the method is also effective for enhancing single-task models post-fine-tuning. The framework is scalable, integrates seamlessly with advanced optimization techniques, and demonstrates robust results across diverse datasets and architectures.

## Method Summary
DV-BASI extends task arithmetic by introducing difference vectors (δ = θ - θ_pre) as generalized task vectors derived from historical weight movements. The method uses these vectors as directed perturbations to escape local optima through iterative optimization. At each iteration, when optimization stagnates, a new difference vector is computed from the current model weights, and an anisotropic scaling matrix Λ is optimized via gradient descent to modulate the vector's effect. This process repeats until convergence or a maximum number of iterations. The framework supports both supervised (cross-entropy) and unsupervised (entropy minimization) settings, and integrates with existing merging techniques like aTLAS. The key innovation is replacing scalar coefficients with block-diagonal anisotropic scaling, allowing differentiated modulation of parameter groups based on their functional roles.

## Key Results
- DV-BASI achieves state-of-the-art performance in multi-task model merging, improving accuracy by up to 2.1% over existing methods
- The average performance of multi-task models merged via DV-BASI can exceed individually fine-tuned models
- Difference vectors provide directional advantage over random perturbations, with random directions yielding only 5.6% accuracy vs. 83.9% for difference vectors on ViT-B/32
- Anisotropic scaling (83.9%) outperforms isotropic scalar scaling (83.3%) on ViT-B/32, demonstrating the importance of per-block modulation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Difference vectors provide directional advantage over random perturbations for escaping local optima.
- **Mechanism:** The difference vector δ = θ* - θ_pre encodes cumulative optimization history as a sum of successful weight updates. When model weights stagnate at a local optimum, this vector provides a structured direction pointing toward previously beneficial regions of parameter space, rather than unstructured random noise.
- **Core assumption:** The aggregated historical direction retains meaningful geometric structure that correlates with loss reduction, even when instantaneous gradients vanish.
- **Evidence anchors:**
  - [abstract]: "difference vectors—generalized task vectors derived from historical weight movements—to continuously escape local optima"
  - [section 4, Table 1]: Random perturbation yields 5.6% accuracy vs. 83.9% for difference vectors on ViT-B/32, demonstrating catastrophic failure of random directions
  - [corpus]: Weak direct evidence; neighbor papers focus on task vector composition rather than perturbation analysis
- **Break condition:** If loss landscapes become highly non-convex with disconnected basins, historical directions may mislead rather than guide.

### Mechanism 2
- **Claim:** Anisotropic scaling of difference vectors enables more expressive exploration than scalar coefficients.
- **Mechanism:** Decomposing δ into parameter blocks (δ^(1), δ^(2), ..., δ^(n)) with independent learnable scaling coefficients λ^(i) allows per-block modulation. This creates a block-diagonal scaling matrix Λ that can emphasize or suppress different parameter groups based on their functional roles.
- **Core assumption:** Parameter blocks have semantically distinct roles (e.g., early vs. late layers) that benefit from differentiated scaling.
- **Evidence anchors:**
  - [abstract]: "anisotropic scaling" explicitly mentioned as enabling factor
  - [section 3, Eq. 3-4]: Formal definition of Λ structure with per-block λ^(i)I^(i)
  - [section 4, Table 1]: Anisotropic scaling (83.9%) outperforms isotropic scalar scaling (83.3%) on ViT-B/32
  - [corpus]: Assumption supported indirectly by "Layer-Aware Task Arithmetic" neighbor paper discussing layer-specific knowledge
- **Break condition:** If blocks are too fine-grained, overfitting risk increases; if too coarse, expressiveness degrades to scalar case.

### Mechanism 3
- **Claim:** Iterative difference vector updates enable continuous optimization beyond single-step stagnation.
- **Mechanism:** At each iteration j, when optimization stalls, δ_j = θ_j - θ_pre is recomputed from the current local optimum. This refreshed direction incorporates all accumulated improvements, then Λ_j is re-optimized via gradient descent on the loss L(f(x; θ_j + Λ_j δ_j), y). The cycle repeats: stagnation → new δ → new Λ → progress.
- **Core assumption:** Local optima are not isolated; better solutions exist along reachable paths (citing Garipov et al. 2018 on mode connectivity).
- **Evidence anchors:**
  - [section 3, Algorithm 1]: Formal iteration structure with δ update at line 2 and θ update at line 8
  - [section 4, Figure 2a-b]: Shows stepwise relative accuracy increasing across iterations, exceeding 100% (outperforming fine-tuned models) by iteration 4
  - [corpus]: "Variational Task Vector Composition" neighbor paper addresses composition but not iteration; limited direct corpus support
- **Break condition:** If each iteration converges to progressively worse local minima, or if computational budget prevents sufficient iterations.

## Foundational Learning

- **Concept: Task Arithmetic**
  - **Why needed here:** DV-BASI extends task arithmetic; understanding τ = θ_ft - θ_pre as the fundamental operation is prerequisite.
  - **Quick check question:** Given pre-trained weights θ_pre = [1.0, 2.0] and fine-tuned weights θ_ft = [1.2, 1.8], what is the task vector?

- **Concept: Local Optima and Gradient Vanishing**
  - **Why needed here:** The core problem DV-BASI solves is stagnation where ∇L ≈ 0 but global optimum remains unreached.
  - **Quick check question:** Why can't standard gradient descent escape a local minimum where gradients vanish?

- **Concept: Block-wise Parameter Grouping**
  - **Why needed here:** Anisotropic scaling requires decomposing model weights into meaningful blocks (e.g., by layer or attention head).
  - **Quick check question:** How might you partition a 12-layer transformer for block-wise scaling?

## Architecture Onboarding

- **Component map:**
  - θ_pre (frozen): Pre-trained weights serving as reference point
  - θ_0 (input): Initial merged model from baseline method (e.g., aTLAS)
  - δ_j (computed): Difference vector at iteration j
  - Λ_j (learnable): Block-diagonal anisotropic scaling matrix
  - θ_j → θ_{j+1}: Iterative weight update via θ + Λδ

- **Critical path:**
  1. Obtain θ_0 from existing merging method
  2. Compute initial δ_0 = θ_0 - θ_pre
  3. Initialize Λ (typically λ^(i) ≈ 0.1-1.0 per block)
  4. Optimize Λ via gradient descent on target loss
  5. Detect stagnation (patience-based early stopping)
  6. Update δ and repeat from step 3

- **Design tradeoffs:**
  - Block granularity: More blocks → more expressive but higher overfitting risk and compute
  - Iteration count M: More iterations → potential improvement but diminishing returns
  - Supervised vs. unsupervised: Supervised uses cross-entropy; unsupervised uses entropy minimization (no labels required but potentially less stable)

- **Failure signatures:**
  - Accuracy degrades catastrophically (similar to random perturbation in Table 1): Check that δ is computed correctly, not randomized
  - No improvement across iterations: Λ may be initialized poorly; try smaller learning rate η
  - Overfitting on validation: Reduce M or increase block size (fewer learnable λ parameters)
  - Storage explosion: Method requires storing 4-6 models; ensure sufficient GPU memory for target scale

- **First 3 experiments:**
  1. Reproduce Table 1 on a single backbone (ViT-B/32) comparing random perturbation vs. isotropic vs. anisotropic scaling to validate directional advantage claim
  2. Run 4-iteration DV-BASI starting from aTLAS merged model on 3 datasets, plotting relative accuracy per epoch (reproduce Figure 2a) to verify escapability
  3. Extend to single-task fine-tuning enhancement: Take a fine-tuned checkpoint, apply DV-BASI, and check if accuracy improves (reproduce Table 5 subset)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can DV-BASI be effectively generalized to Large Language Models (LLMs) or Convolutional Neural Networks (CNNs) outside the CLIP ViT framework?
- **Basis in paper:** [inferred] The "Experiments" section explicitly states the evaluation focuses on "computer vision task" using only "ViT-B/32, ViT-B/16, and ViT-L/14 architectures in CLIP" as backbones.
- **Why unresolved:** While the authors claim the framework is "scalable," the empirical validation is restricted to Vision Transformers. The behavior of difference vectors in CNNs (lacking patch embeddings) or generative LLMs (with distinct attention patterns) remains unverified.
- **What evidence would resolve it:** Empirical results applying DV-BASI to encoder-decoder or decoder-only LLMs (e.g., LLaMA) and standard CNN architectures (e.g., ResNet).

### Open Question 2
- **Question:** What are the theoretical limits and convergence properties of applying DV-BASI to single-task fine-tuning?
- **Basis in paper:** [explicit] The Conclusion states, "edit and tune models with the difference vectors is very promising and has great potential for future development," following the observation that DV-BASI improves fine-tuned models.
- **Why unresolved:** The paper demonstrates feasibility on 20 datasets (Table 5) but does not characterize if the iterative improvements plateau, regress, or if the method introduces overfitting risks over extended iterations.
- **What evidence would resolve it:** An ablation study analyzing single-task performance and generalization gap as the number of iterations ($M$) increases significantly beyond the standard 3-5 steps.

### Open Question 3
- **Question:** How does the choice of anisotropic block partitioning affect the optimization trajectory compared to scalar scaling?
- **Basis in paper:** [inferred] The method uses a block diagonal scaling matrix $\Lambda$ inspired by prior work, but the paper does not ablate *how* parameters should be grouped into blocks (e.g., layer-wise vs. channel-wise).
- **Why unresolved:** The performance gain of anisotropic scaling over isotropic scaling is shown (Table 1), but the sensitivity of this result to the specific granularities of the parameter blocks ($\delta^{(1)}, \dots, \delta^{(n)}$) is not discussed.
- **What evidence would resolve it:** A comparative analysis of merging performance using different blocking strategies (e.g., per-layer, per-module, or random grouping) to validate the robustness of the anisotropic assumption.

## Limitations

- The method's performance depends heavily on the quality of the initial merged model θ_0, making it less effective when starting from poor baselines
- Computational overhead increases with iteration count and block granularity, potentially limiting scalability to larger models
- The precise block decomposition strategy for anisotropic scaling is not specified, requiring empirical tuning for optimal performance

## Confidence

- **High confidence**: Directional advantage of difference vectors over random perturbations (supported by Table 1 comparison showing 78.3% absolute improvement)
- **Medium confidence**: Anisotropic scaling consistently outperforms isotropic scaling (supported by Table 1, though block decomposition details are unspecified)
- **Medium confidence**: Iterative updates enable escaping local optima (supported by Figure 2a-b showing progressive improvement, though causality vs. continued training is unclear)

## Next Checks

1. Implement DV-BASI with different block decomposition strategies (layer-wise, attention-head-wise, mixed) to determine optimal granularity and verify anisotropic advantage holds across variants
2. Test DV-BASI on non-CLIP architectures (e.g., ResNet, Swin Transformer) to assess architectural generalization beyond the reported CLIP ViT-B/32, B/16, L/14 models
3. Compare DV-BASI against alternative optimization methods like Sharpness-Aware Minimization and Lookahead within the same multi-task merging framework to isolate the specific contribution of difference vectors