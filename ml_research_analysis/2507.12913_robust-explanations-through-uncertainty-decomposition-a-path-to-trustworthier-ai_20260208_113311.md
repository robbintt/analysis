---
ver: rpa2
title: 'Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier
  AI'
arxiv_id: '2507.12913'
source_url: https://arxiv.org/abs/2507.12913
tags:
- uncertainty
- explanations
- aleatoric
- epistemic
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework that uses uncertainty decomposition
  to guide the selection of appropriate explanations in explainable AI (XAI), distinguishing
  between aleatoric (data-related) and epistemic (model-related) uncertainty. Epistemic
  uncertainty is used as a rejection criterion for unreliable explanations, while
  aleatoric uncertainty determines whether to use feature-importance explanations
  or counterfactual explanations.
---

# Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI

## Quick Facts
- arXiv ID: 2507.12913
- Source URL: https://arxiv.org/abs/2507.12913
- Authors: Chenrui Zhu; Louenas Bounia; Vu Linh Nguyen; Sébastien Destercke; Arthur Hoarau
- Reference count: 19
- Primary result: Framework uses uncertainty decomposition to select appropriate explanations (feature-importance vs counterfactual) and reject unreliable ones, validated on 10 UCI datasets and MNIST.

## Executive Summary
This paper introduces a framework that leverages uncertainty decomposition to enhance the reliability and effectiveness of explainable AI (XAI). By distinguishing between aleatoric (data-related) and epistemic (model-related) uncertainty, the framework guides the selection of appropriate explanations—using epistemic uncertainty as a rejection criterion and aleatoric uncertainty to choose between feature-importance and counterfactual explanations. Experiments demonstrate that this approach improves explanation trustworthiness and attainability in both classical ML and deep learning contexts.

## Method Summary
The framework employs uncertainty decomposition via three methods: entropy-based ensemble, centroid-based, and evidential K-NN. For a given input, epistemic uncertainty serves as a rejection criterion—if too high, the model lacks knowledge to explain. Aleatoric uncertainty then determines the explanation type: low aleatoric favors feature-importance (e.g., SHAP) due to robustness, while high aleatoric favors counterfactual explanations due to their attainability. The approach was validated on 10 UCI datasets using K-NN and Random Forest classifiers, and on MNIST with a LeNet architecture.

## Key Results
- Aleatoric uncertainty correlates negatively with counterfactual explanation similarity (higher AU → closer counterfactuals)
- Aleatoric uncertainty correlates positively with feature-importance robustness (lower AU → more stable SHAP)
- Epistemic uncertainty successfully identifies out-of-distribution instances where explanations are unreliable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: High aleatoric uncertainty indicates regions where counterfactual explanations are more attainable and stable.
- **Mechanism**: AU captures inherent data ambiguity (e.g., class overlap). In these regions, distinct classes exist close together, so the nearest neighbor with a different class label is likely spatially close, resulting in a low distance metric (high attainability).
- **Core assumption**: Lower distance to a counterfactual instance correlates with higher user acceptability and explanation utility.
- **Evidence anchors**: [abstract] "aleatoric uncertainty correlates negatively with counterfactual explanation similarity"; Section 3.1.1 shows negative Spearman’s correlations confirming higher AU leads to closer counterfactuals.
- **Break condition**: If the decision boundary is extremely sharp or non-linear in high-AU regions, the nearest neighbor might be a misleading proxy for a causal counterfactual.

### Mechanism 2
- **Claim**: Low aleatoric uncertainty favors feature-importance explanations because the local model behavior is more robust.
- **Mechanism**: Low AU implies the instance is in a "safe" region (dense, well-separated class). In these regions, the model's gradient or local surrogate is stable, meaning the explanation does not change drastically with small perturbations (low Lipschitz constant).
- **Core assumption**: Robustness (stability against perturbation) is the primary marker of a "good" feature-importance explanation.
- **Evidence anchors**: [abstract] "[AU correlates] positively with feature-importance robustness"; Figure 4 and Table 2 suggest low AU corresponds to more stable explanation behavior.
- **Break condition**: If the model exhibits high-curvature or jitter even in low-AU regions (e.g., adversarial vulnerability), feature importance may still be misleading despite "robustness."

### Mechanism 3
- **Claim**: Epistemic uncertainty serves as a reliable rejection criterion for explanations when the model is effectively ignorant.
- **Mechanism**: EU measures a lack of knowledge (e.g., out-of-distribution or sparse training data). If EU is high, the model's prediction is arbitrary. Explaining an arbitrary prediction is logically flawed; thus, the high EU itself becomes the explanation ("Model lacks training data").
- **Core assumption**: It is better to provide a meta-explanation of ignorance than a detailed but hallucinated explanation of a low-confidence prediction.
- **Evidence anchors**: [abstract] "Epistemic uncertainty successfully identifies out-of-distribution instances where explanations are unreliable"; Figure 5c visualizes a high-EU instance as an outlier in the training space.
- **Break condition**: If the EU estimator is miscalibrated (e.g., ensembles disagree for the wrong reasons), valid predictions might be rejected unnecessarily.

## Foundational Learning

**Concept**: Aleatoric vs. Epistemic Uncertainty
- **Why needed here**: The entire framework depends on distinguishing *data noise* (Aleatoric) from *model ignorance* (Epistemic) to route the explanation.
- **Quick check question**: Can you explain why a single noisy data point creates Aleatoric uncertainty but a lack of data creates Epistemic uncertainty?

**Concept**: Lipschitz Continuity (Robustness)
- **Why needed here**: Used to quantify the stability of SHAP explanations. A high Lipschitz constant means the explanation changes wildly with tiny input changes.
- **Quick check question**: If an input $x$ is perturbed by $\epsilon$, does the explanation vector $\phi(x)$ change by a small or large amount?

**Concept**: Attainability (Counterfactuals)
- **Why needed here**: Measures the feasibility of a counterfactual. An explanation is useless if it requires changing a feature to an impossible value.
- **Quick check question**: Does the generated counterfactual $\tilde{x}$ reside within the training data distribution?

## Architecture Onboarding

**Component map**: UQ Module -> EU Gate -> AU Router -> [Counterfactual Generator | Feature Attribution Engine]

**Critical path**: The UQ Module must execute first. If the EU Gate triggers, the pipeline terminates immediately to save compute and prevent bad explanations.

**Design tradeoffs**:
- *Accuracy vs. Latency*: The paper uses Evidential KNN and Ensembles. KNN is slow at inference time for large datasets; Ensembles are computationally heavy.
- *Threshold Sensitivity*: The choice of $\tau_{reject}$ (rejection rate) trades off coverage (how many instances you explain) against trustworthiness.

**Failure signatures**:
- *High EU in dense regions*: Indicates a miscalibrated UQ module or model underfitting.
- *Contradictory routing*: If AU is high (noisy data) but the Counterfactual distance is massive (unattainable), the AU estimator or distance metric is flawed.

**First 3 experiments**:
1. **Correlation Check**: Scatter plot AU vs. Counterfactual Distance on a validation set. Verify negative correlation.
2. **Rejection Validation**: Inject out-of-distribution (OOD) noise. Verify that the EU Gate rejects $>90\%$ of OOD samples while retaining $>80\%$ of in-distribution samples.
3. **Robustness Comparison**: Compare SHAP stability (Lipschitz constant) in High-AU vs. Low-AU buckets. Verify that Low-AU buckets have lower variance in explanation vectors.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unaddressed:

1. Does the uncertainty-guided selection of explanation types significantly improve end-user comprehension or trust compared to providing a single explanation type uniformly?
2. Does the proposed framework generalize effectively to large-scale, modern deep learning architectures (e.g., Transformers) trained on massive datasets?
3. Can a theoretically grounded, automated threshold be established for the epistemic uncertainty rejection criterion?

## Limitations
- The framework assumes uncertainty decomposition methods are well-calibrated, but no validation of calibration is reported
- KNN-based counterfactuals may not capture causal relationships, limiting practical applicability
- The rejection threshold $\tau_{reject}$ is dataset-dependent and requires careful tuning
- Results are primarily on classical ML models (KNN, RF) with limited deep learning validation beyond MNIST

## Confidence
- **High confidence**: Aleatoric uncertainty correlation with counterfactual attainability; epistemic uncertainty effectiveness at identifying OOD instances
- **Medium confidence**: Feature-importance robustness in low-aleatoric regions; general framework applicability across domains
- **Low confidence**: Specific threshold recommendations; counterfactuals as meaningful causal explanations

## Next Checks
1. Validate calibration of uncertainty decomposition methods across different datasets using proper scoring rules
2. Test framework with complex deep learning models on real-world tabular data with known feature interactions
3. Conduct user studies comparing acceptance of aleatoric-based counterfactuals versus standard counterfactuals