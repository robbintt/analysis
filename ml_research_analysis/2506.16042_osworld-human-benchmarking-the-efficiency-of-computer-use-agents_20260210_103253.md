---
ver: rpa2
title: 'OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents'
arxiv_id: '2506.16042'
source_url: https://arxiv.org/abs/2506.16042
tags:
- steps
- agents
- task
- agent
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first in-depth study on the temporal performance
  of computer-use agents, focusing on the latency implications of state-of-the-art
  agents on the OSWorld benchmark. Through detailed analysis of Agent S2, the leading
  open-source system, the authors find that large model calls for planning and reflection
  account for the majority (75%-94%) of overall latency, with per-step latency growing
  3x longer as tasks progress.
---

# OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents

## Quick Facts
- **arXiv ID**: 2506.16042
- **Source URL**: https://arxiv.org/abs/2506.16042
- **Reference count**: 8
- **Primary result**: Computer-use agents take 1.4-2.7x more steps than necessary, with planning/reflection LLM calls accounting for 75%-94% of latency

## Executive Summary
This paper presents the first comprehensive temporal performance analysis of computer-use agents (CUAs) on the OSWorld benchmark. Through detailed latency profiling of Agent S2, the leading open-source system, the authors discover that large model calls for planning and reflection dominate execution time, comprising 75%-94% of total latency. They find that per-step latency grows up to 3x longer as tasks progress due to accumulating context in prompts. To address the gap between agent and human efficiency, the paper introduces OSWorld-Human, a manually annotated dataset containing human-determined minimal trajectories for all 369 OSWorld tasks. Evaluation of 16 agents reveals that even high-scoring systems take 1.4-2.7x more steps than necessary, highlighting significant efficiency gaps that need addressing for practical deployment.

## Method Summary
The study analyzes Agent S2 on 37 OSWorld tasks to profile per-step latency across six components: planning, reflection, retrieval, grounding, screenshot capture, and action execution. The framework uses GPT-4.1 for planning, reflection, and retrieval, while grounding uses UI-TARS-7B-DPO hosted on NVIDIA A6000 with SGLang. Human baseline trajectories are constructed by manually performing each task and recording minimal action sequences, both as single atomic actions and grouped actions where multiple actions share a visual observation. Sixteen agents are evaluated on the full OSWorld-Human dataset using Weighted Efficiency Score (WES+) for success-weighted efficiency and WES− for failure penalty, with a maximum 50-step limit per task.

## Key Results
- Planning and reflection LLM calls account for 75%-94% of total task latency
- Per-step latency grows up to 3× longer as task trajectories lengthen
- Even highest-scoring agents take 1.4-2.7× more steps than human-determined minimal trajectories
- Accessibility tree generation adds 3-26 seconds per step but can reduce step count for some applications
- Agent S2 achieves 42.5% success rate on OSWorld but only 23.7% single-action WES+ and 14.3% grouped-action WES+

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-step latency grows substantially as task trajectories lengthen, with later steps taking up to 3× longer than early steps.
- Mechanism: Most CUA frameworks include full trajectory history in each prompt. As step count increases, prompt tokens accumulate, and LLM latency—dominated by the prefill stage for long contexts—grows proportionally.
- Core assumption: The underlying LLM serving infrastructure does not implement context caching or prefix-sharing across sequential calls.
- Evidence anchors: [abstract] "...as an agent uses more steps to complete a task, each successive step can take 3× longer than steps at the beginning of a task." [Section 3.2] "when Agent S2 gets to the later steps, LLM calls take longer and have longer prompts. This is due to the prompting mechanism for most CUAs: at each step, the prompt sent to the LLM includes the history of all previous steps."

### Mechanism 2
- Claim: Planning and reflection LLM calls account for 75%-94% of total task latency, making them the dominant bottleneck.
- Mechanism: These components invoke large foundation models with long prompts. Grounding uses smaller models with fixed-size outputs, and screenshot/action execution require no GPU computation. Latency is therefore concentrated in large-model inference.
- Core assumption: Planning and reflection require large models; smaller or distilled models would degrade task success rates.
- Evidence anchors: [abstract] "...large model calls for planning and reflection account for the majority (75%–94%) of overall latency..." [Section 3.1, Table 1] Breakdown by application shows Planning (48.9%–59.2%) + Reflection (15.9%–45.2%) dominating.

### Mechanism 3
- Claim: State-of-the-art agents take 1.4×–2.7× more steps than human-determined minimal trajectories, even when successful.
- Mechanism: Agents lack task-specific procedural knowledge, leading to suboptimal action sequences, redundant explorations, or inefficient recovery from minor errors. They do not "know" shortcuts or grouped actions that humans execute fluently.
- Core assumption: The human trajectories in OSWorld-Human represent near-optimal step counts.
- Evidence anchors: [abstract] "...even the highest-scoring agents on OSWorld take 1.4–2.7× more steps than necessary..." [Section 5, Table 4] Top agent (UI-TARS-1.5) achieves 42.5% success on OSWorld but only 23.7% single-action WES+ and 14.3% grouped-action WES+.

## Foundational Learning

- **LLM Inference Stages (Prefill vs. Decode)**:
  - Why needed here: The paper attributes planning/reflection latency to the prefill stage due to large prompt tokens, while retrieval latency is decode-bound. Understanding this distinction is essential for targeting optimizations.
  - Quick check question: Given a prompt with 15,000 tokens and an expected output of 50 tokens, which inference stage dominates latency for a transformer model without caching?

- **Agent Observation Modalities (Screenshot, A11y Tree, Set-of-Marks)**:
  - Why needed here: Section 3.3 shows that including accessibility trees can add 3–26 seconds per step and increase prompt tokens substantially, while SoM adds minimal overhead. Choice of modality directly affects latency and step count.
  - Quick check question: Why might adding an A11y tree reduce step count for some applications (e.g., GIMP) but increase it for others (e.g., LibreOffice)?

- **Action Grouping in CUAs**:
  - Why needed here: OSWorld-Human introduces grouped-action trajectories where multiple atomic actions execute per observation. This concept is central to understanding potential efficiency gains.
  - Quick check question: What criteria determine whether "click text field → type text → press Enter" can be grouped into a single agent step?

## Architecture Onboarding

- **Component map**: Screenshot/A11y+SoM → Planning → Grounding → Action execution → Reflection
- **Critical path**: Planning → Grounding → Reflection (repeats each step). Planning and reflection together form the latency bottleneck; grounding is fast but necessary for coordinate precision.
- **Design tradeoffs**:
  - **A11y trees**: Provide structured UI information that can reduce steps for some apps but add significant generation time and token overhead—best suited for applications with sparse visual elements.
  - **SoM overlays**: Low overhead, often reduces step count, but may not help when UI elements are dense or unlabeled.
  - **Action grouping**: Can reduce step count (and thus LLM calls) by 1.3–2.3× depending on application, but requires the model to predict multi-action sequences reliably.
- **Failure signatures**:
  - Trajectories approaching the 50-step limit with high per-step latency (30–60s/step) indicate context explosion.
  - Success rates below 20% with low WES− (near −0.5 or worse) suggest agents are failing slowly rather than failing fast.
  - Large gaps between single-action and grouped-action WES+ indicate missed opportunities for action batching.
- **First 3 experiments**:
  1. **Latency profiling by step index**: Run Agent S2 on the 37-task subset, log per-step latency and prompt token count. Verify the 3× growth claim and identify the step index where latency inflection occurs.
  2. **A11y ablation by application**: For each application, run tasks with screenshot-only, screenshot+A11y, and screenshot+A11y+SoM. Measure both end-to-end latency and step count to confirm which applications benefit from additional modalities.
  3. **Action-grouping simulation**: Using OSWorld-Human grouped trajectories, compute theoretical WES+ if agents could execute grouped actions. Estimate latency reduction assuming one planning+reflection cycle per group instead of per atomic action.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agent architectures be optimized to minimize the number of steps to match human efficiency, rather than just maximizing success rates?
- Basis in paper: [explicit] The paper notes that even the highest-scoring agents "take 1.4-2.7x more steps than necessary" and identifies "minimizing the number of steps for a task" as a primary approach for speeding up execution.
- Why unresolved: Current state-of-the-art agents focus primarily on accuracy, often utilizing trial-and-error or sub-optimal paths that result in the significant efficiency gap documented in the OSWorld-Human evaluation.
- What evidence would resolve it: The development of an agent that achieves a Step Efficiency Score (t_exp/t_actual) closer to 1.0 on OSWorld-Human while maintaining high task success rates.

### Open Question 2
- Question: How can the latency growth caused by accumulating prompt history be mitigated in long-horizon tasks?
- Basis in paper: [explicit] The study finds that "each successive step can take 3x longer than steps at the beginning of a task" because the prompt includes the history of all previous steps, causing the prefill stage to dominate latency.
- Why unresolved: The standard "observe-call-act" loop relies on providing the model with the full trajectory history for context, creating a computational bottleneck as the context window fills with screenshots and logs.
- What evidence would resolve it: A context management technique (e.g., summarization or selective memory) that maintains flat per-step latency over 50+ steps without degrading planning accuracy.

### Open Question 3
- Question: What is the optimal perceptual strategy to balance the trade-off between the high cost of accessibility tree generation and its benefits for step reduction?
- Basis in paper: [inferred] The paper notes that while accessibility trees lower the number of steps for some applications (OS, GIMP), they "drastically increase the per-task latency" for others (LibreOffice) due to generation time and token overhead.
- Why unresolved: There is currently no dynamic mechanism to toggle observation types; using A11y trees is beneficial for grounding but computationally expensive, and the paper highlights that this overhead can negate efficiency gains in complex UIs.
- What evidence would resolve it: A dynamic perception module that selectively enables or disables the A11y tree based on real-time UI complexity to minimize end-to-end latency.

## Limitations

- The study's reliance on Agent S2 as the primary latency analysis subject may limit generalizability to other CUA architectures
- The 37-task subset represents only 10% of the full OSWorld benchmark, potentially missing edge cases
- Human trajectory annotations may not represent true optimal solutions for all tasks
- The benefits of action grouping vary significantly across applications without clear predictors

## Confidence

**High confidence**: The claim that planning and reflection LLM calls dominate latency (75%-94%) is supported by direct timing measurements across multiple applications. The step-inefficiency finding (1.4-2.7× more steps than human trajectories) is corroborated by multiple WES+ calculations across 16 agents.

**Medium confidence**: The 3× per-step latency growth as tasks progress is well-documented for Agent S2 but may not hold for agents using different prompting strategies or context management. The A11y tree impact (3-26s additional latency) shows application-specific variability.

**Low confidence**: The generalizability of grouped-action efficiency gains to all applications is uncertain, as the paper notes "significant variation" in benefits across applications without providing a clear predictor.

## Next Checks

1. **Cross-architecture latency validation**: Run the same latency profiling methodology on UI-Evol and OpenCUA frameworks using the 37-task subset to verify whether planning/reflection still accounts for 75%-94% of latency and whether step latency grows 3× across different CUA implementations.

2. **A11y tree application classification**: Conduct systematic experiments to classify which application categories (document editors, image editors, IDEs, etc.) benefit from A11y trees versus those that suffer from the additional latency, creating guidelines for when to enable this modality.

3. **Grouped-action robustness testing**: Test Agent S2's performance on the full OSWorld-Human dataset with both single-action and grouped-action modes across all applications, measuring not just WES+ but also success rate stability to determine if grouping introduces new failure modes.