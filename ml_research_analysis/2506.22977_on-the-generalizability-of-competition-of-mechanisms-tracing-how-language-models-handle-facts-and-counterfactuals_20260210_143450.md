---
ver: rpa2
title: 'On the Generalizability of "Competition of Mechanisms: Tracing How Language
  Models Handle Facts and Counterfactuals"'
arxiv_id: '2506.22977'
source_url: https://arxiv.org/abs/2506.22977
tags:
- factual
- attention
- layer
- counterfactual
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reproduces and extends "Competition of Mechanisms:
  Tracing How Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024).
  The authors successfully replicate the original study''s findings on GPT-2 small
  and Pythia 6.9B, confirming that factual and counterfactual information compete
  through specialized attention heads, with attention blocks dominating MLP blocks
  in mechanism competition.'
---

# On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"

## Quick Facts
- arXiv ID: 2506.22977
- Source URL: https://arxiv.org/abs/2506.22977
- Reference count: 40
- This paper successfully reproduces and extends "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), confirming that factual and counterfactual information compete through specialized attention heads in GPT-2 small and Pythia 6.9B, while revealing that prompt structure, domain bias, and architecture significantly affect these mechanisms.

## Executive Summary
This paper reproduces and extends the original "Competition of Mechanisms" study, confirming that factual and counterfactual information compete through specialized attention heads in GPT-2 small and Pythia 6.9B. The authors extend these findings by testing Llama 3.1 8B, which shows reduced attention head specialization and delayed activation patterns. The study also reveals that prompt structure significantly affects counterfactual predictions, with question-answering formats reducing counterfactual outcomes compared to the original "Redefine" prompts. Domain analysis shows that the original findings are heavily influenced by dataset bias toward automotive and technology domains.

## Method Summary
The study reproduces the original logit lens analysis to trace factual and counterfactual information flow through transformer layers, identifying specialized attention heads that mediate mechanism competition. It extends the methodology by testing Llama 3.1 8B, applying different prompt structures (QnA format vs original "Redefine" prompts), and conducting domain-specific analysis. Attention head ablation is used to validate the causal role of identified heads, with scaling factors applied to test their sufficiency in determining competition outcomes.

## Key Results
- Successfully reproduces original findings on GPT-2 small and Pythia 6.9B regarding attention block dominance and specialized head identification
- Llama 3.1 8B shows reduced attention head specialization and delayed activation patterns compared to smaller models
- Question-answering prompt formats significantly reduce counterfactual predictions compared to original "Redefine" prompts
- Domain analysis reveals findings are heavily influenced by dataset bias toward automotive and technology domains

## Why This Works (Mechanism)

### Mechanism 1: Factual-Counterfactual Competition via Attention Blocks
- **Claim:** Factual recall and counterfactual copying compete through specialized attention heads that attend to the attribute position, with attention blocks dominating MLP blocks in determining the outcome.
- **Mechanism:** Attention heads in later layers aggregate information from different token positions. Heads supporting factual outcomes suppress the counterfactual logit rather than directly promoting the factual token. The competition is mediated by scaling attention weights at critical heads (e.g., L10H7, L11H10 in GPT-2).
- **Core assumption:** The logit lens method faithfully captures intermediate representations and their contributions to final predictions.
- **Evidence anchors:**
  - [abstract] "attention blocks dominating MLP blocks in mechanism competition"
  - [Section 4.1] "attention blocks show stronger contributions with ∆cofa peaks of approximately 1.3 in layers 7 and 9, compared to more modest MLP block contributions"
  - [corpus] Related work confirms attention head specialization in factual recall across architectures, though corpus evidence for the specific competition mechanism is limited.
- **Break condition:** Mechanism degrades when prompt structure breaks verbatim repetition (QnA format), or when domain changes cause different attention heads to activate. Llama 3.1 8B shows this mechanism may not scale—delayed activation and reduced specialization suggest architectural differences.

### Mechanism 2: Counterfactual Copying as Repetition Heuristic
- **Claim:** The counterfactual mechanism primarily relies on pattern completion from repetitive prompt structure rather than genuine counterfactual comprehension.
- **Mechanism:** When the prompt repeats the subject-relation structure verbatim ("Redefine: iPhone was developed by Google. iPhone was developed by"), models complete using the immediately preceding attribute token through a copy mechanism.
- **Core assumption:** Models treat the "Redefine" context as a completion task rather than an instruction to adopt alternative facts.
- **Evidence anchors:**
  - [abstract] "question-answering formats significantly reduce counterfactual predictions compared to the original 'Redefine' prompts"
  - [Section 4.3] "the repetitive structure of the original prompt conditions the models to repeat the whole sentence verbatim"
  - [corpus] Related paper "Tracing Facts or just Copies?" directly investigates this distinction, supporting the interpretation that copying rather than counterfactual reasoning dominates.
- **Break condition:** Reformulating to QnA format reduces counterfactual prediction significantly. Effect is model-size dependent—larger models show smaller reductions, suggesting better semantic extraction.

### Mechanism 3: Domain-Specific Attention Head Activation
- **Claim:** Which attention heads mediate the factual-counterfactual competition depends on the knowledge domain, making universal ablation strategies ineffective.
- **Mechanism:** Different domains activate different attention head patterns. Overrepresented domains (Autos, Technology) in training data develop specialized heads; underrepresented domains show distributed or different activation patterns.
- **Core assumption:** The observed domain effects reflect genuine differences in how knowledge is stored/retrieved rather than confounds from prompt wording.
- **Evidence anchors:**
  - [abstract] "effectiveness of attention head ablation varying dramatically across different knowledge domains"
  - [Section 4.5] "prompts from 'Autos and Vehicles'...product names predominantly begin with the name of the company (i.e. the factual token) itself" vs. "Arts and Entertainment...do not contain the factual token explicitly"
  - [corpus] Weak corpus evidence on domain-specific mechanisms; this appears to be a novel finding requiring further validation.
- **Break condition:** Ablation of heads identified from one domain (e.g., L10H7, L11H10) fails to shift predictions in other domains. The identified heads are task- and domain-dependent, not universal.

## Foundational Learning

- **Concept: Logit Lens and Residual Stream Interpretation**
  - **Why needed here:** The paper's core methodology relies on projecting intermediate residual stream states through the unembedding matrix to trace how information flows through layers.
  - **Quick check question:** Can you explain why applying the unembedding matrix at layer 5 instead of the final layer might reveal different information than the final prediction?

- **Concept: Attention Head Ablation/Patching**
  - **Why needed here:** The causal intervention method—scaling attention weights to specific positions—tests whether identified heads are necessary/sufficient for mechanism competition.
  - **Quick check question:** If ablating head L10H7 reduces counterfactual predictions, does this prove L10H7 "causes" counterfactual outcomes? What alternative explanations exist?

- **Concept: Distributional Dataset Bias**
  - **Why needed here:** The paper's key finding is that mechanism claims depend heavily on dataset composition. Understanding how overrepresented domains can create spurious mechanistic conclusions is critical.
  - **Quick check question:** If 50% of your test prompts contain the answer token in the subject, what mechanistic conclusions might be artificially strengthened?

## Architecture Onboarding

- **Component map:** Residual stream -> Layer-wise attention blocks (with multiple attention heads) -> MLP blocks -> Final unembedding matrix
- **Critical path:**
  1. Early layers (0-4 in GPT-2): Information spread across positions, minimal competition
  2. Middle-to-late layers (5-11 in GPT-2): Competition emerges, attention blocks dominate
  3. Final positions: Specialized heads aggregate to last token, write factual or counterfactual information
  4. **Llama 3.1 8B deviation:** Minimal activity until layer 15-20, sharp final-layer contribution (~50% of logit values), reduced head specialization
- **Design tradeoffs:**
  - **GPT-2 small / Pythia 6.9B:** Clear specialization enables targeted ablation but may overfit to dataset biases
  - **Llama 3.1 8B:** Distributed processing resists simple interventions but may reflect more robust architectures
  - **Prompt structure:** Repetitive formats strengthen copy mechanism (easier to study but less ecologically valid); QnA formats reduce copy bias but complicate mechanistic analysis
- **Failure signatures:**
  - **Logit lens breakdown:** Minimal early-layer activations in Llama 3.1 8B suggest the method may fail for larger/different architectures (see Belrose et al., 2023 on tuned lens)
  - **Dataset leakage:** Factual token appearing in subject (e.g., "Honda Aviator is produced by") creates spurious subject-position importance
  - **Domain mismatch:** Ablation strategies trained on one domain fail to transfer
- **First 3 experiments:**
  1. **Reproduce positional encoding visualization** (Figures 6, 9): Run logit inspection on GPT-2 small with filtered dataset, verify factual information concentrates in later layers at the last position. Check whether subject-position importance replicates or if relation positions dominate.
  2. **Test attention head ablation transfer**: Apply the paper's identified heads (L10H7, L11H10) to prompts from underrepresented domains (Arts, Law). Measure ablation effectiveness—if it drops significantly, domain-specificity is confirmed.
  3. **Compare prompt structure sensitivity**: Run the same factual-counterfactual pairs through both "Redefine" and "QnA" formats on all three models. Quantify the reduction in counterfactual prediction rate to understand how much of the "competition" is genuine vs. copy-artifact.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do larger language models distribute mechanistic competition more evenly across attention heads, or does the logit lens methodology fail to capture emerging patterns in newer architectures?
- **Basis:** [explicit] Section 5.4 states "it is not clear whether larger models like Llama 3.1 8B distribute their work more evenly through early attention heads, leading to less specialization, or whether our logit lens analysis is simply insufficient to capture some other emerging pattern."
- **Why unresolved:** Llama 3.1 8B showed delayed activations, reduced specialization, and a sharp final-layer effect that the current methodology cannot explain.
- **What evidence would resolve it:** Application of circuit analysis or tuned lens methods to larger models, comparing across interpretability techniques.

### Open Question 2
- **Question:** What drives domain-dependent variation in attention head activation—subject token leakage, topic vocabulary shifts, or inherent task structure differences across domains?
- **Basis:** [explicit] Section 5.4 states "it remains to be seen to what degree the source of the discrepancy lies within the subject containing the factual token, the shift of topic words in the prompts, or within the task of the prompt itself, which also varies with the domain."
- **Why unresolved:** Automotive/technology prompts contained factual tokens in subject names, but causality remains unclear.
- **What evidence would resolve it:** Controlled experiments isolating each factor through systematically constructed prompts across diverse domains.

### Open Question 3
- **Question:** Do mechanism competition patterns generalize from base models to fine-tuned and instruction-tuned language models?
- **Basis:** [explicit] Section 5.3 states "we are wary of extending our conclusions to fine-tuned models. Future work could try to address this gap."
- **Why unresolved:** User-facing LLMs undergo fine-tuning and RL that may fundamentally alter how mechanisms compete.
- **What evidence would resolve it:** Replication of experiments on fine-tuned variants, comparing attention head specialization and ablation effectiveness against base models.

## Limitations

- **Dataset composition effects:** Mechanism competition findings are heavily influenced by dataset bias toward automotive and technology domains, with unclear causal relationship between domain representation and mechanism effectiveness
- **Generalizability to other architectures:** Findings from GPT-2 small/Pythia 6.9B may not extend to modern architectures like Llama 3.1 8B, which show different activation patterns
- **Prompt structure artifact concerns:** Dramatic reduction in counterfactual predictions under QnA formatting suggests significant portion of observed "competition" may be prompt-structure artifact rather than genuine counterfactual reasoning

## Confidence

**High confidence:** The reproduction successfully replicates original findings on GPT-2 small and Pythia 6.9B regarding attention block dominance and specialized head identification. The domain bias analysis and prompt structure sensitivity findings are methodologically sound and clearly demonstrated.

**Medium confidence:** The extension to Llama 3.1 8B reveals meaningful architectural differences, though the interpretation that these reflect fundamental differences in mechanism competition rather than scale/architecture effects requires further validation. The domain-specific head activation claims are plausible but lack comprehensive cross-domain ablation testing.

**Low confidence:** The precise boundary between genuine counterfactual comprehension and copy mechanism artifact remains unclear. While QnA formatting reduces counterfactual predictions, the extent to which this represents "better" reasoning versus different prompt conditioning is uncertain.

## Next Checks

1. **Systematic domain bias quantification:** Re-run the analysis with domain-balanced prompt sampling (equal representation from each domain) to determine whether the original mechanism findings hold under controlled domain composition. This would clarify whether domain effects reflect genuine architectural differences or dataset artifacts.

2. **Architecture ablation transferability:** Test whether heads identified as critical in GPT-2 small/Pythia 6.9B can effectively modulate counterfactual predictions in Llama 3.1 8B (and vice versa). This would determine whether mechanism competition is truly architecture-dependent or merely head-specific.

3. **Extended prompt structure analysis:** Systematically vary prompt formats beyond "Redefine" and QnA (e.g., instruction-based prompts, multi-turn conversations) to map the boundary between copy mechanisms and genuine counterfactual reasoning. This would quantify how much of the observed competition is prompt-structure dependent versus inherent to model behavior.