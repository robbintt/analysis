---
ver: rpa2
title: 'ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared
  task: LLM-based question generation and selection'
arxiv_id: '2506.14371'
source_url: https://arxiv.org/abs/2506.14371
tags:
- questions
- critical
- scheme
- llmj
- llmq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work tackles the challenge of generating critical questions
  to foster deeper reasoning in LLM-based education, countering concerns about superficial
  learning. The authors propose a two-step framework: a small open-source LLM (Questioner)
  generates multiple candidate questions from debate interventions, and a second small
  LLM (Judge) selects the three most useful ones.'
---

# ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection

## Quick Facts
- arXiv ID: 2506.14371
- Source URL: https://arxiv.org/abs/2506.14371
- Reference count: 11
- Primary result: Achieved 62.4% useful question rate using small LLMs, ranking first in CQs-Gen 2025 shared task

## Executive Summary
This work addresses the challenge of generating critical questions to foster deeper reasoning in LLM-based education. The authors propose a two-step framework using small open-source language models: a Questioner generates multiple candidate questions from debate interventions, and a Judge selects the three most useful ones. Using Llama 3.1 8B as Questioner and Gemma 2 9B as Judge—with optional incorporation of argumentation schemes—they achieved 62.4% useful question rate on test data, ranking first in the CQs-Gen 2025 shared task. The approach demonstrates that small, locally deployable models can effectively support critical thinking question generation when generation and evaluation are separated into specialized roles.

## Method Summary
The method employs a two-stage pipeline for critical question generation. First, Llama 3.1 8B (Questioner) generates eight candidate questions per debate intervention—four using argumentation schemes and four without—in a single batch prompt to minimize redundancy. Second, Gemma 2 9B (Judge) selects the three most useful questions from the candidate pool based on relevance and non-redundancy criteria. The approach leverages predefined argumentation schemes (Ad Hominem, Cause to Effect, Expert Opinion, etc.) as structural priors, though selective integration in the "Both" configuration yields optimal results. No fine-tuning is performed; the system relies entirely on zero-shot prompting and cosine similarity evaluation against reference questions.

## Key Results
- Achieved 62.4% useful question rate on test data, ranking first in CQs-Gen 2025 shared task
- LLM-as-Judge selection improves performance by 3.4 percentage points over random selection (p < 0.05, McNemar's test)
- "Both" configuration (selective argumentation scheme integration) achieved optimal performance with 81% of selected questions generated using schemes
- Small models (8B-9B) successfully deployed locally, demonstrating feasibility of private, accessible critical question generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating generation from evaluation improves critical question quality compared to direct generation
- Mechanism: A Questioner LLM generates diverse candidate questions without quality filtering pressure, while a Judge LLM evaluates and selects the most useful ones. This division allows each model to specialize—creative generation vs. critical evaluation—rather than forcing one model to both generate and self-assess simultaneously.
- Core assumption: Judge LLM can reliably distinguish useful from unhelpful questions; the evaluation criteria are learnable from prompt instructions alone.
- Evidence anchors:
  - [abstract] "a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones"
  - [section 4.4] "LLMJ achieves a usefulness rate that is 3.4 percentage points higher than random selection, a statistically significant improvement (p < 0.05, McNemar's test)"
  - [corpus] Weak direct corpus support for mechanism; related work (DayDreamer at CQs-Gen 2025) uses chain-of-thought prompting but doesn't compare generation-selection architectures
- Break condition: When Judge LLM performance degrades significantly below random selection threshold, or when candidate pool has insufficient useful questions for Judge to identify.

### Mechanism 2
- Claim: Selective integration of argumentation schemes balances structural guidance against creative diversity
- Mechanism: Providing argumentation scheme definitions and question templates guides LLMQ toward argument-theoretic patterns, but strict enforcement reduces question diversity. The "Both" configuration—merging scheme-based and scheme-free generations—allows structural guidance while preserving exploration of non-scheme-grounded questions.
- Core assumption: Argumentation schemes capture meaningful patterns in political debate interventions; templates transfer across contexts without over-constraining generation.
- Evidence anchors:
  - [abstract] "optionally incorporating argumentation schemes"
  - [section 4.2] "the best performance is achieved in the Both configuration... 81% of the questions selected by LLMJ were generated with the argumentation scheme in the prompt"
  - [corpus] No direct corpus comparison of selective vs. systematic scheme integration; insufficient external validation
- Break condition: When scheme definitions don't match intervention argumentation patterns, or when template questions become formulaic and fail to challenge context-specific claims.

### Mechanism 3
- Claim: Batch generation in single prompts reduces redundancy compared to iterative single-question prompting
- Mechanism: Generating N questions in one forward pass allows the model to maintain coherence across candidates and avoid repetition, as the model can see its prior outputs within the same context. Sequential prompting without access to previous outputs increases duplicate likelihood.
- Core assumption: Model's attention across generated questions within single prompt is sufficient to track and avoid repetition.
- Evidence anchors:
  - [section 3.1] "each prompt is designed to elicit N questions in a single generation step, rather than prompting the model N times for one question at a time. This strategy effectively reduces question repetition"
  - [corpus] No corpus papers test this batch vs. iterative mechanism directly
- Break condition: When batch size exceeds model's effective context tracking, leading to late-prompt questions ignoring earlier ones.

## Foundational Learning

- Concept: **Argumentation Schemes (Walton et al.)**
  - Why needed here: The framework uses predefined schemes (Ad Hominem, Cause to Effect, Expert Opinion, etc.) as structural priors for question generation. Understanding these patterns is essential for designing effective prompts and interpreting why certain questions are generated.
  - Quick check question: Can you identify which argumentation scheme applies to: "We should ban X because countries that banned X saw crime drop 20%"?

- Concept: **LLM-as-a-Judge Paradigm**
  - Why needed here: The Judge LLM evaluates candidate questions without gold-standard references during inference. Understanding limitations—subjectivity, calibration, prompt sensitivity—is critical for interpreting "Useful" labels.
  - Quick check question: What failure modes might occur when an LLM judges its own outputs versus another model's outputs?

- Concept: **Cosine Similarity for Question Evaluation**
  - Why needed here: The shared task uses semantic similarity to reference questions as the automatic evaluation metric. Many valid questions receive "Not able to evaluate" labels when no similar reference exists.
  - Quick check question: Why might a semantically novel but valid critical question score poorly under this metric?

## Architecture Onboarding

- Component map: Input → LLMQ (Questioner) → Candidate pool merger → LLMJ (Judge) → Output
- Critical path:
  1. Prompt engineering for LLMQ (role definition + scheme definitions + templates + task objective)
  2. Prompt engineering for LLMJ (strict judge role + redundancy-aware selection instruction)
  3. Inference pipeline orchestration (batch generation → merge → selection)
  4. Evaluation against reference questions using cosine similarity threshold ≥0.6
- Design tradeoffs:
  - Small models (8B-9B) vs. large models (GPT-4o): Small models enable local deployment and privacy; GPT-4o achieved higher test scores (50.0% vs. 62.4% useful on validation, but final manual scoring favored small model approach at 67.6%)
  - Scheme integration: Full integration reduces diversity; partial integration (Both) yields best results
  - Candidate count: More candidates increase computation but provide better selection pool (4+4 per prompt optimal in experiments)
- Failure signatures:
  - High "Invalid" rate: LLMQ generates questions that cannot challenge any argument in intervention
  - High "Not able to evaluate" rate: Generated questions don't match reference distribution; may indicate overfitting or genuine novelty
  - Judge selects redundant questions: Prompt instruction to "select the most relevant one" among redundant candidates not followed
  - Fine-tuning divergence: Attempts to fine-tune on small dataset (74 interventions) produced outputs diverging from instructions
- First 3 experiments:
  1. **Baseline establishment**: Run LLMQ alone (Llama 3.1 8B, no Judge) generating 3 questions per intervention; measure Useful/Invalid/NoEval rates to establish generation quality floor
  2. **Judge ablation**: Compare LLMJ (Gemma 2 9B) selection against random selection and oracle (human-labeled upper bound) using identical candidate pools; quantify Judge value-add
  3. **Scheme sensitivity**: Test four configurations (Without, With-one, With-multiple, Both) on held-out validation set; identify which scheme integration strategy suits your target domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the two-step Questioner-Judge framework generalize effectively to non-political domains such as educational texts, scientific discourse, or everyday argumentation?
- Basis in paper: [explicit] The Limitations section states: "Future work should aim to evaluate the proposed framework on broader and more diverse datasets to assess its robustness across different domains, like education."
- Why unresolved: The current dataset comprises only political debate interventions, which may have distinctive argumentative patterns not representative of other contexts.
- What evidence would resolve it: Systematic evaluation on datasets from diverse domains (education, science, legal, social media) showing comparable useful question rates to the 62.4% achieved on political discourse.

### Open Question 2
- Question: How can the Judge model (LLMJ) be improved to close the substantial performance gap with the oracle selector?
- Basis in paper: [explicit] The Limitations section notes that "LLMJ shows a substantial gap compared to the oracle... suggesting significant potential for future improvements."
- Why unresolved: The Judge achieves 59.3% usefulness versus 93.5% for the oracle, indicating it fails to consistently identify useful questions among candidates.
- What evidence would resolve it: Modified Judge architectures or training approaches that achieve >75% usefulness rate while maintaining the constraint of using small, open-source models.

### Open Question 3
- Question: What alternative evaluation metrics could reduce the high proportion of "Not able to evaluate" labels while maintaining alignment with human judgment of question usefulness?
- Basis in paper: [explicit] The Limitations section highlights that "Many generated questions did not align with any reference, despite being potentially useful, and hence were labeled as Not able to evaluate."
- Why unresolved: Cosine similarity thresholding against reference questions fails to capture semantic equivalence or novelty, potentially penalizing valid creative outputs.
- What evidence would resolve it: New evaluation approaches (e.g., LLM-based semantic matching, entailment classifiers) that reduce NoEval rates below 25% while showing high correlation with human annotations.

### Open Question 4
- Question: Does fine-tuning on larger, balanced datasets enable small LLMs to outperform the current prompting-based approach?
- Basis in paper: [inferred] The paper reports that fine-tuning attempts yielded "inconclusive results" and models "failed to outperform a random baseline, likely due to task complexity combined with the limited size of the training dataset."
- Why unresolved: The current dataset (189 interventions, heavily imbalanced toward "Useful" labels) may be insufficient for effective fine-tuning, leaving the potential of trained models unexplored.
- What evidence would resolve it: Fine-tuning experiments on expanded, label-balanced datasets (>1000 interventions) showing statistically significant improvements over the prompting baseline.

## Limitations

- The approach's effectiveness with selective argumentation scheme integration (81% scheme-based questions) may reflect dataset bias rather than universal optimization, requiring validation on non-political domains.
- The Judge model achieves only a modest 3.4 percentage point improvement over random selection (p < 0.05), suggesting limited value-add that may not scale to more complex reasoning tasks.
- Cosine similarity thresholding for evaluation systematically disadvantages semantically valid but structurally novel questions, creating potential evaluation bias against creative critical questions.

## Confidence

- **High confidence**: The separation of generation and evaluation tasks demonstrably improves performance over single-model approaches, supported by statistical significance testing and consistent with established LLM-as-a-Judge paradigms.
- **Medium confidence**: Selective argumentation scheme integration shows promise, but the 81% scheme-based selection rate may reflect dataset-specific characteristics rather than universal optimization. External validation across domains is needed.
- **Low confidence**: The batch generation mechanism for reducing redundancy lacks direct empirical validation beyond single-dataset observations. The claim that maintaining coherence across generated questions prevents repetition needs systematic testing.

## Next Checks

1. **Cross-domain validation**: Apply the framework to non-political debate contexts (e.g., scientific discourse, legal arguments) to test whether selective scheme integration generalizes beyond the political debate dataset used in CQs-Gen 2025.

2. **Judge reliability testing**: Conduct human evaluation studies comparing LLMJ selections against human judgments across multiple annotators to quantify inter-rater reliability and identify systematic Judge biases.

3. **Evaluation metric stress test**: Generate questions that deliberately challenge the cosine similarity threshold by being semantically valid but structurally dissimilar to reference questions, measuring how many legitimate critical questions are incorrectly labeled "Not able to evaluate."