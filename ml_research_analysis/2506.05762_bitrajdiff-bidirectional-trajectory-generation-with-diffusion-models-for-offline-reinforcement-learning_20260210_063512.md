---
ver: rpa2
title: 'BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for
  Offline Reinforcement Learning'
arxiv_id: '2506.05762'
source_url: https://arxiv.org/abs/2506.05762
tags:
- bitrajdiff
- offline
- trajectories
- trajectory
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of data augmentation for offline
  reinforcement learning (RL), where static datasets often exhibit distribution bias,
  limiting generalizability. The authors propose Bidirectional Trajectory Diffusion
  (BiTrajDiff), a novel framework that generates both forward-future and backward-history
  trajectories from intermediate states using diffusion models.
---

# BiTrajDiff: Bidirectional Trajectory Generation with Diffusion Models for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.05762
- Source URL: https://arxiv.org/abs/2506.05762
- Reference count: 22
- Outperforms state-of-the-art data augmentation baselines across D4RL benchmark suite

## Executive Summary
This paper addresses the challenge of data augmentation for offline reinforcement learning (RL), where static datasets often exhibit distribution bias that limits generalizability. The authors propose BiTrajDiff, a novel framework that generates both forward-future and backward-history trajectories from intermediate states using diffusion models. This bidirectional approach enables the synthesis of novel trajectories connecting previously unreachable states, unlike existing methods that only generate local, forward-only trajectories. BiTrajDiff employs two diffusion models, an inverse dynamics model, and a reward model, followed by filtering mechanisms to ensure high-quality augmented data.

## Method Summary
BiTrajDiff generates trajectories through two independent diffusion processes: forward diffusion predicts future dynamics from intermediate anchor states, while backward diffusion reconstructs essential history transitions. These bidirectional trajectories are then stitched at shared anchors to create novel paths connecting previously unreachable state pairs. An inverse dynamics model and reward model convert the generated state-only trajectories into complete RL transitions with actions and rewards. The framework includes a two-stage filtering process - first removing out-of-distribution trajectories using Isolation Forest, then retaining top-performing trajectories based on cumulative rewards. The augmented dataset is mixed with original data at a 30% ratio for training downstream RL algorithms.

## Key Results
- Consistent improvement over state-of-the-art data augmentation baselines across D4RL locomotion, navigation, and manipulation tasks
- Bidirectional generation significantly outperforms unidirectional approaches (forward-only, backward-only) in ablation studies
- Filtering mechanisms are critical - removing either OOD or greedy filter degrades performance
- Optimal augmentation ratio identified at 30%, with instability observed at 100% synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Bidirectional trajectory generation enables connecting states that are not directly linked in the original offline dataset
- Two independent diffusion models generate forward trajectories (predicting future dynamics) and backward trajectories (reconstructing history) conditioned on shared intermediate anchor states
- Stitching at these anchors creates novel paths connecting previously unreachable state pairs
- Break condition: If generated forward/backward trajectories are physically inconsistent at the anchor state (large dynamic error), stitching produces invalid transitions

### Mechanism 2
- Inverse dynamics model and reward model convert state-only trajectories into complete RL transitions
- IDM infers action from consecutive states; reward model assigns rewards, both trained via supervised learning on original dataset
- Core assumption: IDM generalizes to diffusion-generated state pairs that may lie outside training distribution but remain physically plausible
- Break condition: Accumulated IDM errors compound over trajectory length, degrading n-step TD estimation quality

### Mechanism 3
- Two-stage filtering removes low-quality trajectories while preserving diversity gains
- OOD filter (Isolation Forest) removes trajectories with high anomaly scores; greedy filter retains top trajectories by cumulative reward
- Sequential application ensures both plausibility and quality
- Break condition: Over-aggressive OOD filtering may reject diverse trajectories; insufficient greedy filtering retains suboptimal behaviors that misguide policy learning

## Foundational Learning

- **Diffusion Models (Denoising Process)**: BiTrajDiff uses diffusion to generate trajectories via iterative denoising from noise to structured sequences
  - Quick check question: Can you explain how the forward noising process q(x_k|x_{k-1}) relates to the reverse denoising process p_θ(x_{k-1}|x_k)?

- **Offline RL Distribution Bias**: The motivation for data augmentation stems from static datasets constraining learned policies to biased behavior distributions
  - Quick check question: Why does constraining policy to the offline dataset distribution improve stability but limit generalizability?

- **Classifier-Free Guidance (CFG)**: BiTrajDiff conditions trajectory generation on return and state using CFG
  - Quick check question: How does the guidance weight ω in ε̂ = ω · ε_θ(x_k, k, y) + (1-ω) · ε_θ(x_k, k, ∅) trade off condition adherence vs. diversity?

## Architecture Onboarding

- Component map: Forward diffusion denoiser ε_θ_f → Backward diffusion denoiser ε_θ_b → Inverse Dynamics Model f_ψ → Reward Model r_φ → OOD filter (Isolation Forest) → Greedy filter

- Critical path: Train diffusion models on offline data → Train IDM/RM → Sample anchor states → Generate bidirectional trajectories → Complete with actions/rewards → Filter → Augment dataset → Train downstream RL

- Design tradeoffs: Horizon H controls trajectory length vs. error accumulation; augmentation ratio σ (30% optimal) balances diversity vs. noise; filter thresholds C_ood, C_greedy control quality vs. quantity

- Failure signatures: (1) Unstable training with high σ (>50%) due to synthetic noise; (2) Erratic policy updates without OOD filter; (3) Performance degradation in long-horizon n-step TD from IDM error accumulation

- First 3 experiments:
  1. Ablation on directionality: Compare forward-only, backward-only, and bidirectional generation on IQL/TD3BC to verify bidirectional contribution
  2. Filter ablation: Test None / OOD-only / greedy-only / both filters on walker2d-medium-replay to isolate filter effects
  3. Augmentation ratio sweep: Vary σ ∈ {10%, 20%, 30%, 40%, 50%, 100%} to find stable operating point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the bidirectional generation paradigm be effectively extended to multi-task offline RL settings where behavioral patterns are significantly broader and more diverse?
- Basis in paper: The conclusion explicitly states, "Future work will explore bidirectional generation for offline RL under multi-task settings with broader behavioral patterns."
- Why unresolved: Current study limits evaluation to single-task datasets where dynamics and reward structures are isolated
- What evidence would resolve it: Performance benchmarks on multi-task datasets (e.g., Meta-World) demonstrating that bidirectional stitching improves generalization across distinct tasks

### Open Question 2
- Question: How does BiTrajDiff perform in high-dimensional visual domains where state reconstruction fidelity is more difficult to maintain?
- Basis in paper: All experiments are conducted on low-dimensional state-vector tasks, yet the method relies on precise state generation to enable trajectory stitching via an inverse dynamics model
- Why unresolved: Diffusion models for images introduce different artifacts and computational costs than state vectors, potentially causing IDM to hallucinate invalid actions
- What evidence would resolve it: Empirical results on image-based benchmarks (e.g., DMControl from pixels) validating the pipeline's ability to generate physically consistent visual trajectories

### Open Question 3
- Question: Does the greedy trajectory filter inadvertently discard optimal long-horizon behaviors that appear suboptimal in the short term?
- Basis in paper: The "Greedy Trajectory Filter" selects generated trajectories based on highest sum of rewards, assuming the learned reward model accurately captures long-term value
- Why unresolved: In sparse reward settings, a greedy filter might prune trajectories essential for reaching goal states but with low intermediate rewards
- What evidence would resolve it: An ablation study comparing current greedy filter against a value-function-based filter in sparse-reward navigation tasks

## Limitations

- Performance degradation observed with increased n-step TD, suggesting IDM error accumulation limits applicability to long-horizon tasks
- Method relies on reliable stitching at intermediate anchors, but optimal anchor selection and temporal coherence validation are not fully specified
- Evaluation limited to D4RL benchmarks with state-vector observations; scalability to high-dimensional visual domains remains unproven

## Confidence

- Bidirectional generation mechanism: Medium confidence - ablation studies demonstrate superiority over unidirectional approaches, but analysis limited to fixed horizon H=5
- Filtering mechanism effectiveness: Medium confidence - ablation confirms both filters are necessary, but parameter tuning and interaction effects require further investigation
- State connectivity claims: Low confidence in generality - evidence shows effectiveness on D4RL tasks, but robustness across diverse distributions and theoretical guarantees remain unclear

## Next Checks

1. Test BiTrajDiff with horizon H=10+ to quantify error accumulation and identify breaking points for n-step TD learning
2. Evaluate performance on sparse-reward and long-horizon manipulation tasks where state connectivity is critical
3. Compare against DAWM (which generates actions directly) to isolate benefits of state-only diffusion + IDM vs. integrated action generation