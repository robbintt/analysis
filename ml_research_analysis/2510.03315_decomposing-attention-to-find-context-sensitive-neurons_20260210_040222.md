---
ver: rpa2
title: Decomposing Attention To Find Context-Sensitive Neurons
arxiv_id: '2510.03315'
source_url: https://arxiv.org/abs/2510.03315
tags:
- token
- contributions
- neuron
- explanation
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to decompose attention heads in transformer
  models into positional and content-dependent components, focusing on "stable" heads
  with broad attention patterns. By sampling softmax denominators from a representative
  calibration text, the authors approximate the combined output of these heads as
  a linear summary of the surrounding text.
---

# Decomposing Attention To Find Context-Sensitive Neurons

## Quick Facts
- **arXiv ID:** 2510.03315
- **Source URL:** https://arxiv.org/abs/2510.03315
- **Reference count:** 38
- **Primary result:** Method to find context-sensitive neurons in transformers by decomposing attention into positional and content components, achieving median correlation 0.9485 with true activations

## Executive Summary
This paper introduces a method to discover neurons in transformer models that respond to high-level contextual properties by decomposing attention heads into positional and content-dependent components. The key insight is that certain "stable" attention heads with broad positional kernels have softmax denominators that concentrate around their expected values, enabling accurate approximation of their combined effect as a linear summary of surrounding text. This allows identification of context-sensitive neurons without requiring large-scale corpus activations.

## Method Summary
The method works by first approximating attention keys in the first layer as a sum of positional and content-dependent components using LayerNorm decoupling. It identifies "stable" heads with broad positional kernels and weak content dependence, then samples their softmax denominators from a representative calibration text. The combined output of these heads is approximated as a positionally-weighted linear combination of token contributions, enabling discovery of context-sensitive neurons by analyzing which neurons respond strongly to individual token contributions.

## Key Results
- Contextual circuit approximation achieves median correlation 0.9485 with true neuron activations across 1000 texts
- Median Fraction of Variance Unexplained (FVU) is 0.1426, demonstrating strong accuracy
- Method identifies hundreds of first-layer neurons responding to high-level properties like Commonwealth vs American English, astronomy, and programming
- Single calibration text suffices for general analysis, avoiding need for large-scale corpus activations

## Why This Works (Mechanism)

### Mechanism 1: LayerNorm-Decoupled Key Decomposition
First-layer attention keys can be approximated as the sum of positional and content-dependent components despite LayerNorm's nonlinear mixing. The decomposition separates keys into E[n, xi] (content-dependent term using attending position's LayerNorm scale) and P[n, i] (positional term using approximate orthogonal decomposition of WE and Wpos). Adding/subtracting Wpos[n]WK reduces absolute error for nearby positions where most attention mass concentrates.

### Mechanism 2: Softmax Denominator Concentration for "Stable" Heads
Heads with broad positional kernels and weak content dependence have softmax denominators that concentrate around their expected value when token distribution is fixed. By modeling surrounding tokens as i.i.d. random variables, concentration inequalities (Hoeffding/Chebyshev) bound denominator variance. The key term Σ(pos²) is small when positional kernels spread attention broadly, reducing variance regardless of individual token content scores.

### Mechanism 3: Contextual Circuit as Linear Text Summary
The combined output of 6 slowly-decaying first-layer heads can be approximated as a positionally-weighted linear combination of token contributions. After approximating head-specific positional kernels by a shared median kernel and sampling denominators from a representative calibration text, the circuit becomes: Σᵢ posᵢ × contribution[j, xi]. This exposes which MLP neurons are context-sensitive by analyzing sensitivity to individual token contributions.

## Foundational Learning

- **OV (Output-Value) Circuit from Mathematical Framework for Transformer Circuits**
  - Why needed: The entire contextual circuit builds on decomposing how attention head outputs flow through V and W matrices to influence downstream neurons
  - Quick check: Can you explain why V^Oh(i, xi) represents the contribution of token xi through head h to the residual stream?

- **Concentration Inequalities (Hoeffding/Chebyshev)**
  - Why needed: The theoretical justification for denominator stability relies on proving that Σᵢ posᵢ × contentᵢ concentrates around its mean
  - Quick check: Given n independent random variables in [a,b], what does Hoeffding's inequality tell you about P(|ΣXᵢ - E[ΣXᵢ]| ≥ t)?

- **Total Variation Distance**
  - Why needed: The paper uses TV distance to quantify approximation quality between true and approximate attention patterns
  - Quick check: If two distributions p and q have TV distance 0.1, what's the maximum difference in probability assigned to any single event?

## Architecture Onboarding

- **Component map:** GPT-2 Small, Layer 0 attention heads → 6 "slowly decaying" heads (0, 2, 6, 8, 9, 10) with broad positional kernels → Layer 0 MLP → ~100+ context-sensitive neurons → Single calibration text → sampled denominators [denom_h] → Contribution matrix contribution[j, t]

- **Critical path:** Load GPT-2 Small with TransformerLens → Identify slowly decaying heads via positional kernel visualization → Compute TV distance between true and approximate attention (target: ~0.05) → Sample denominators from calibration text at position n_ctx/2 with token "the" → Compute contribution[j, t] for all neurons j and tokens t → Filter neurons by top[j] threshold to find context-sensitive candidates → Validate via correlation with true activations on held-out texts

- **Design tradeoffs:** Calibration text selection (prose works generally; domain-specific needs domain-matched calibration); Threshold θ (lower finds more neurons but increases false positives; paper uses θ≈5.0 for ~100 neurons); Head selection (more heads increases coverage but may introduce unstable heads)

- **Failure signatures:** Systematic underestimation of contextual contribution (check calibration text stop-word density); Low correlation (<0.8) between approximation and truth (verify head selection and LayerNorm approximation); Neurons activating on glitch tokens only (increase θ threshold or manually filter)

- **First 3 experiments:**
  1. Reproduce Figure 3-4 stability plots: Compute denom[h, n, 'the'] across 100+ texts for each candidate head to verify concentration
  2. Validate approximation accuracy on 10 known context-sensitive neurons: Run full contextual circuit vs. true activation on 100 texts; target median r > 0.9
  3. Cross-domain calibration test: Use prose calibration text to predict neurons on programming/religious texts; quantify systematic bias

## Open Questions the Paper Calls Out

- Can the contextual circuit approximation be effectively applied to deeper layers of transformer models?
- Can the decomposition method be adapted for models using Rotary Positional Embeddings (RoPE)?
- Does clustering calibration texts based on softmax denominator values improve approximation accuracy?
- Do models with relative positional encodings like T5 and ALiBi utilize similar stable heads for contextual processing?

## Limitations

- The LayerNorm approximation assumes WE and Wpos are approximately orthogonal, which may not hold for all embedding schemes
- The "stable head" criterion is restrictive, identifying only 6 out of 144 layer 0 heads as suitable
- The i.i.d. assumption for token sampling may fail for texts with strong local dependencies (code, poetry, dialogue)
- Domain-specific analysis requires domain-matched calibration texts, adding complexity

## Confidence

- **High Confidence:** Key decomposition accuracy (TV ~0.05), denominator concentration for stable heads, contextual circuit correlation (median 0.9485), identified neurons respond to contextual properties
- **Medium Confidence:** LayerNorm decoupling mechanism, six heads sufficient for layer 0, single calibration text for general analysis
- **Low Confidence:** Generalization to deeper layers/other architectures, concentration inequalities fully justify stability, optimal threshold θ = 5.0

## Next Checks

1. **LayerNorm Approximation Sensitivity Analysis:** Systematically vary the constant C in Equation 1 across its plausible range and measure how TV distance between true and approximate attention patterns changes.

2. **Cross-Architecture Generalization Test:** Apply the decomposition method to GPT-2 Medium and GPT-Neo to determine whether the same heads exhibit stability and whether the LayerNorm approximation remains valid.

3. **Concentration Assumption Validation:** For each identified stable head, measure the empirical distribution of softmax denominators across 1000+ texts and compare against theoretical bounds predicted by Hoeffding/Chebyshev inequalities.