---
ver: rpa2
title: 'IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal
  Retrieval'
arxiv_id: '2504.00954'
source_url: https://arxiv.org/abs/2504.00954
tags:
- image
- retrieval
- data
- multimodal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Instance-Driven Multimodal Image Retrieval
  (IDMR), a new task requiring models to retrieve images containing the same instance
  as a query image while matching a text-described scenario. The authors develop IDMR-bench
  using real-world object tracking and first-person video data and propose a cross-domain
  synthesis method to create 557K training samples by cropping objects from detection
  datasets.
---

# IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval

## Quick Facts
- arXiv ID: 2504.00954
- Source URL: https://arxiv.org/abs/2504.00954
- Reference count: 40
- Primary result: MLLM-based retrieval model trained on 1.2M samples outperforms state-of-the-art on both traditional benchmarks and zero-shot IDMR-bench

## Executive Summary
This paper introduces Instance-Driven Multimodal Image Retrieval (IDMR), a new task requiring models to retrieve images containing the same instance as a query image while matching a text-described scenario. The authors develop IDMR-bench using real-world object tracking and first-person video data and propose a cross-domain synthesis method to create 557K training samples by cropping objects from detection datasets. Their Multimodal Large Language Model (MLLM)-based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and zero-shot IDMR-bench. Experimental results demonstrate significant improvements over previous models, with the 26B model achieving 69.2% average precision on MMEB and substantial gains on IDMR tasks, highlighting MLLM's potential for advanced retrieval applications.

## Method Summary
The authors develop IDMR-bench using real-world object tracking (LaSOT) and first-person video (EPIC-KITCHENS-100) data. They propose a cross-domain synthesis method that creates 557K training samples by cropping objects from detection datasets (COCO, Objects365, Open Images) using bounding box annotations. An MLLM (Qwen2VL-72B-Instruct) generates location-oriented captions for these cropped instances by overlaying green bounding boxes. The retrieval model uses InternVL2.5 as backbone with LoRA adaptation (rank 8), training on both synthetic and real data (1.2M total samples) using InfoNCE contrastive loss. The model is evaluated on traditional benchmarks (MMEB) and zero-shot IDMR-bench.

## Key Results
- 26B model achieves 69.2% average precision on MMEB, outperforming previous state-of-the-art
- Significant improvements on IDMR-bench zero-shot evaluation (46-89% accuracy across models)
- In-domain test accuracy reaches 78-92% for different model sizes
- Smaller batch sizes (128/GPU) outperform larger ones (1024/GPU) by ~2% on average, contradicting typical contrastive learning assumptions

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Synthesis Creates Valid Instance-Level Training Signal
- Claim: Cropping objects from detection datasets and pairing them with their source images provides a scalable training signal for instance-level retrieval.
- Mechanism: The method uses bounding box annotations from object detection datasets to create (cropped_instance, location_description, full_image) triplets. The cropped region serves as the query image, forcing the model to recognize the specific instance rather than relying on global scene features.
- Core assumption: Objects within a single image that are cropped and re-presented as queries maintain sufficient visual identity for the model to learn instance-specific features.
- Evidence anchors: [abstract]: "cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets"; [section 4.2]: "We construct training data based on three large scale object detection datasets... We hope the model can learn to recognize the same objects (instance-driven abilities) by studying the relationship between cropped images of objects and the corresponding original full images."

### Mechanism 2: Location-Oriented Captions Provide Contextual Grounding
- Claim: MLLM-generated descriptions of object surroundings force the model to jointly encode instance identity and scene context.
- Mechanism: A green bounding box is overlaid on the image, and Qwen2VL-72B-Instruct generates captions describing position/surroundings without describing the object itself. This creates the query text that must be satisfied along with instance matching.
- Core assumption: The MLLM can generate spatial descriptions that are both informative enough to disambiguate scenes and general enough to avoid overfitting to specific training images.
- Evidence anchors: [section 4.2]: "We prompt the MLLM with a visual marker on the input image, which is a green bounding box that frames the target. The generated caption is used to build query text in our training data."; [figure 3]: Shows the pipeline from detection data through MLLM reasoning to final training triplets.

### Mechanism 3: Contrastive Learning with InternVL2.5 Backbone
- Claim: Fine-tuning a pretrained MLLM with InfoNCE loss on instance-retrieval tasks improves fine-grained visual correspondence beyond what category-level training provides.
- Mechanism: Uses InternVL2.5 as backbone with LoRA adaptation (rank 8). The last token's hidden state serves as the multimodal embedding. Training combines 557K synthetic samples with 662K MMEB samples.
- Core assumption: The pretrained MLLM has sufficient visual discrimination capability that can be redirected toward instance-level matching without catastrophic forgetting of broader retrieval abilities.
- Evidence anchors: [section 4.3]: "We employ contrastive training to train an embedding model, utilizing the hidden state h of the last token as the multimodal embedding. The loss function is Noise Contrastive Estimation(InfoNCE) Loss"; [table 4]: Shows improvements on MMEB (69.2% avg) demonstrating retained general capabilities.

## Foundational Learning

- Concept: **Contrastive Learning with InfoNCE Loss**
  - Why needed here: The entire training pipeline depends on understanding how positive/negative sample pairs shape the embedding space. Without this, the loss function in Equation 4 is opaque.
  - Quick check question: If you double the batch size in contrastive learning, what happens to the number of negative samples per query?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The paper uses LoRA rank 8 on an 8B+ parameter model. Understanding why full fine-tuning isn't used is critical for resource planning.
  - Quick check question: What is the relationship between LoRA rank and the number of trainable parameters?

- Concept: **Object Detection Bounding Boxes as Instance Proxies**
  - Why needed here: The synthesis method assumes detection boxes correspond to meaningful instances. Understanding detection data limitations (occlusion, truncation, labeling noise) is essential for data quality assessment.
  - Quick check question: How might an object that's 80% occluded affect the quality of an instance-retrieval training sample?

## Architecture Onboarding

- Component map:
  - Detection datasets (COCO/Objects365/Open Images) -> CLIP filtering -> MLLM captioning -> Triplet construction -> InternVL2.5 backbone with LoRA -> InfoNCE training -> Last token embedding extraction

- Critical path:
  1. Detection dataset annotations must pass CLIP score threshold (>0.2)
  2. MLLM must generate location-oriented caption successfully
  3. Cropped query + caption + full image form valid triplet
  4. Contrastive training with proper negative sampling
  5. Embedding extraction from fine-tuned model

- Design tradeoffs:
  - Data volume vs. quality: CLIP filtering removes ~40% of samples but improves instance recognizability
  - Model scale vs. inference cost: 26B model shows ~10% improvement over 8B on IDMR-bench but requires 3x parameters
  - Batch size vs. generalization: Counterintuitively, smaller batches (128/GPU) outperform larger (1024/GPU) by 2% on average
  - Synthetic vs. real training data: 557K synthetic + 662K real outperforms 662K real alone, but zero-shot gap remains

- Failure signatures:
  - Category confusion: Model retrieves correct object class but wrong instance
  - Text over-reliance: Model matches caption semantics while ignoring instance identity
  - Small object failure: Crops below a certain size may lack discriminative features despite CLIP filtering
  - Domain shift: In-domain test shows 78-92% accuracy while zero-shot shows 46-89%, suggesting synthetic data doesn't fully transfer

- First 3 experiments:
  1. **Reproduce filtering threshold ablation**: Test CLIP thresholds of 0.1, 0.2, 0.3 to validate the 0.2 choice on a held-out subset
  2. **Batch size validation on your hardware**: Table 5 shows 128/GPU optimal, but verify this holds with your GPU memory configuration and gradient accumulation settings
  3. **Zero-shot probe before full training**: Evaluate pretrained InternVL2.5 on a small IDMR-bench subset to establish baseline and confirm the task is learnable before investing in full training runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does performance saturation occur when scaling beyond 1.2M samples and 26B parameters to the proposed 10M samples and 78B parameter models?
- Basis in paper: [explicit] The authors explicitly state in the conclusion that they plan to release 10M-scale datasets and 78B models to overcome current computational constraints.
- Why unresolved: Current scaling experiments (Fig 5) show linear improvements, but the trend is not validated at the significantly larger scale proposed for future work.
- What evidence would resolve it: Evaluation of a 78B parameter model trained on 10M samples on the MMEB and IDMR-bench to observe the scaling curve.

### Open Question 2
- Question: What specific mechanisms cause smaller batch sizes to outperform larger ones in this instance-driven contrastive learning setup?
- Basis in paper: [inferred] The authors observe in Section 5.3.2 that smaller batches (128) yield peak accuracy, hypothesizing that gradient noise prevents overfitting, but they do not validate this mechanism.
- Why unresolved: This finding contradicts the standard consensus that larger batches benefit contrastive learning, and the authors' hypothesis remains an unproven explanation.
- What evidence would resolve it: A detailed analysis of gradient distributions and embedding space geometry across different batch sizes for this specific task.

### Open Question 3
- Question: To what extent does the static nature of the synthetic training data limit the model's ability to handle temporal consistency and viewpoint changes in real-world video retrieval?
- Basis in paper: [inferred] The model is trained on static crops from object detection datasets (COCO, Objects365) but evaluated on video tracking datasets (LaSOT, EPIC-KITCHENS) containing dynamic viewpoints and temporal changes.
- Why unresolved: While zero-shot results are strong, it is unclear if the lack of temporal training data creates a performance ceiling for video-based instance matching.
- What evidence would resolve it: An ablation study comparing models trained on static crops versus those trained on temporal video frame pairs.

## Limitations

- Zero-shot performance on IDMR-bench varies significantly (46-89%) despite strong in-domain performance (78-92%), suggesting synthetic training data may not fully capture real-world instance-level retrieval complexity
- The CLIP filtering threshold (0.2) is empirically chosen but not theoretically justified, potentially affecting the quality and diversity of training samples
- The model relies on detection bounding boxes as instance proxies, which can be imprecise and may introduce noise in the training signal

## Confidence

**High Confidence Claims:**
- The IDMR task definition and benchmark construction using LaSOT and EPIC-KITCHENS-100 data is methodologically sound
- The contrastive learning framework with InfoNCE loss is correctly implemented
- The MLLM-based retrieval model demonstrably outperforms existing approaches on both traditional and IDMR-specific benchmarks

**Medium Confidence Claims:**
- The cross-domain synthesis method creates valid training signal for instance-level retrieval
- Location-oriented captions generated by MLLM effectively ground instance identity in scene context
- The 557K synthetic samples meaningfully contribute to model performance beyond real training data alone

**Low Confidence Claims:**
- The optimal CLIP filtering threshold is definitively 0.2 (requires ablation study)
- The last token hidden state is the optimal embedding choice for this task
- The batch size of 128/GPU is universally optimal across different hardware configurations

## Next Checks

1. **CLIP Filtering Threshold Ablation**: Systematically test CLIP score thresholds at 0.1, 0.2, 0.3, and 0.4 on a held-out validation subset to quantify the tradeoff between data quantity and training signal quality. Measure impact on both in-domain and zero-shot performance to determine if the 0.2 threshold is truly optimal.

2. **Embedding Strategy Comparison**: Replace the last token hidden state with alternative embeddings (mean pooling of all tokens, [CLS] token, or attention-weighted combinations) and evaluate performance impact on the IDMR task. This validates whether the current choice is truly optimal or merely convenient.

3. **Small Object Robustness Test**: Create a subset of IDMR-bench containing only queries where the instance occupies less than 5% of the image area. Evaluate model performance on this subset to quantify the impact of object size on retrieval accuracy and identify potential failure modes in the synthetic training pipeline.