---
ver: rpa2
title: 'Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for
  Real-Time Strategy Games'
arxiv_id: '2510.18395'
source_url: https://arxiv.org/abs/2510.18395
tags:
- state
- masmp
- machine
- agents
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Memory-Augmented State Machine Prompting (MASMP),
  a novel framework that integrates state machine prompting with memory mechanisms
  to address key challenges in LLM-based agents for real-time strategy games. MASMP
  guides LLMs to emulate finite state machines and behavior trees through natural
  language prompts while maintaining long-term tactical coherence via a lightweight
  memory module.
---

# Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games

## Quick Facts
- arXiv ID: 2510.18395
- Source URL: https://arxiv.org/abs/2510.18395
- Reference count: 13
- Primary result: 60% win rate vs. hardest built-in AI (Lv7) in StarCraft II

## Executive Summary
This paper introduces Memory-Augmented State Machine Prompting (MASMP), a framework that integrates state machine prompting with memory mechanisms to address key challenges in LLM-based agents for real-time strategy games. MASMP guides LLMs to emulate finite state machines and behavior trees through natural language prompts while maintaining long-term tactical coherence via a lightweight memory module. Experiments in StarCraft II demonstrate MASMP's 60% win rate against the hardest built-in AI (Lv7), significantly outperforming baselines at 0%.

## Method Summary
MASMP combines natural language state machine prompting with a lightweight memory module to enable reliable decision-making in StarCraft II. The framework uses a Macro-Strategic State Machine and Action Implementation Behavior Tree defined in prompts to constrain LLM outputs to valid actions. A memory module stores strategic variables across observation cycles, enabling non-Markovian decision-making. The system uses DeepSeek-V3 LLM with LLM-PySC2 for textual observations, extracts strategies via regex, and executes actions through Easy Build/Control Mode.

## Key Results
- 60% win rate against built-in AI Lv7 (baseline: 0%)
- 40.2% advanced unit production rate (baseline: 19.4%)
- Successfully addresses "Knowing-Doing Gap" through strict state-action mapping
- Resolves fragmented execution through strategic memory persistence

## Why This Works (Mechanism)

### Mechanism 1
Natural language state machine prompting constrains LLM outputs to valid, executable action spaces, reducing hallucinations. The framework defines a Macro-Strategic State Machine and Action Implementation Behavior Tree in the prompt, forcing the LLM to classify situations into pre-defined states and select actions from specific behavior tree nodes. This filters out impossible actions before execution. The mechanism breaks if game states become too complex for the prompt's token limit or if the LLM fails to respect formatting constraints.

### Mechanism 2
A lightweight memory module enables non-Markovian decision-making by persisting strategic variables across observation cycles. Standard LLM agents act solely on current observations, but MASMP introduces memory storing high-level variables like tactic and priority unit. Feeding previous memory back into prompts maintains tactical coherence, preventing abandonment of long-term build orders due to momentary fluctuations. The mechanism breaks if the StrategyExtractor fails to parse LLM output, causing memory loss and fragmented execution.

### Mechanism 3
Strict state-action mapping resolves the "Knowing-Doing Gap" by translating semantic reasoning into hard execution constraints. LLMs often reason correctly but execute poorly or greedily. MASMP defines specific unit ratios within state machine prompts, forcing resource allocation toward reasoned goals. The mechanism breaks if enemies adapt faster than fixed state transition logic allows, potentially locking the agent in suboptimal states.

## Foundational Learning

- **Finite State Machines (FSM) & Behavior Trees**: These structures define how traditional game AI uses states (Idle, Attack, Retreat) and transitions. Understanding this is crucial because MASMP uses prompts to emulate this structure. *Quick check*: Can you explain the difference between a transition condition and an action in a behavior tree?

- **Markov vs. Non-Markovian Decision Processes**: RTS games exhibit non-Markovian characteristics where history matters due to Fog of War. Standard LLMs treat each prompt as independent, failing to maintain long-term strategies. *Quick check*: Why would an agent acting only on the current visual frame fail to execute a "flanking maneuver" that takes 60 seconds?

- **Prompt Engineering as Architecture**: Here, prompts are not just questions but code/logic. The prompt structure itself is the state machine. *Quick check*: How does providing a "state definition" in a prompt change the LLM's output distribution compared to a zero-shot request?

## Architecture Onboarding

- **Component map**: LLM-PySC2 Textual Observation -> MemoryDB (Retrieves M_{t-1}) -> LLM (DeepSeek-V3) + State Machine Prompt -> StrategyExtractor (Regex) -> Easy Build/Control Mode (Executes actions)

- **Critical path**: The feedback loop between StrategyExtractor and MemoryDB. If the LLM outputs a strategy that Regex cannot parse, the memory loop breaks, and the agent reverts to a memoryless (Markovian) state, causing fragmentation.

- **Design tradeoffs**: Reliability vs. Creativity (FSM structure reduces hallucinations but may stifle creative strategies), Latency vs. Intelligence (large model provides reasoning but may introduce latency)

- **Failure signatures**: Greedy Spirals (agent produces only basic units despite advanced priority), State Flapping (agent rapidly toggles between aggressive and defensive states)

- **First 3 experiments**: 1) Sanity Check (Lv1-3): Verify state transitions like switching to aggressive when winning. 2) Ablation on Memory: Run without MemoryDB against Lv7 to confirm performance drops to near-baseline (0%). 3) Stress Test (Greedy Trap): Observe 7-minute mark resource allocation to verify ~40% advanced units production.

## Open Questions the Paper Calls Out

1. **Multi-agent coordination**: How can MASMP be extended to facilitate effective multi-agent coordination? The current implementation focuses on a single agent's strategic loop; managing multiple agents with shared or hierarchical goals requires architectural extensions.

2. **Dynamic prompt optimization**: Can dynamic prompt optimization during gameplay improve adaptability beyond static state machine definitions? The current framework relies on predefined natural language prompts that don't change autonomously during execution.

3. **Cross-domain applications**: Is the MASMP architecture transferable to complex decision-making domains outside of Real-Time Strategy games? The evidence provided is strictly limited to StarCraft II mechanics and environment.

## Limitations

- The exact implementation of natural language state machine and behavior tree prompts is not fully specified, requiring significant prompt engineering to reproduce
- The "Easy Build/Control Mode" abstraction layer obscures whether improvements come from better decision-making or simply better execution
- Performance against human opponents or adversarial reinforcement learning agents remains untested
- The framework may be overfitted to exploit specific weaknesses of the scripted built-in AI

## Confidence

- **High Confidence**: The 60% vs 0% win rate comparison against Lv7 AI is well-defined and directly measurable
- **Medium Confidence**: The "Knowing-Doing Gap" resolution mechanism is logically sound but lacks sufficient evidence for being the primary driver of performance differences
- **Medium Confidence**: The memory module's contribution to non-Markovian decision-making is theoretically justified but lacks ablation studies

## Next Checks

1. **Ablation Study**: Run MASMP without the memory component against Lv7 to quantify the exact contribution of the strategic memory module to the 60% win rate.

2. **Prompt Compliance Audit**: Log all LLM outputs and calculate the percentage that strictly follow the state machine format versus those that contain free-form reasoning or hallucinations.

3. **State Transition Analysis**: Instrument the agent to record every state transition with the reasoning and game context, then analyze whether transitions align with defined conditions or if the agent gets stuck in suboptimal states during complex scenarios.