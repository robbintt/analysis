---
ver: rpa2
title: 'ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback'
arxiv_id: '2509.05091'
source_url: https://arxiv.org/abs/2509.05091
tags:
- feedback
- agents
- agent
- protom
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProToM, a Theory of Mind-informed facilitator
  that promotes prosocial behaviour in multi-agent systems by providing targeted,
  context-sensitive feedback to individual agents. ProToM infers agents' goals using
  Bayesian inverse planning and selects feedback by maximizing expected utility conditioned
  on the inferred goal distribution.
---

# ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback

## Quick Facts
- arXiv ID: 2509.05091
- Source URL: https://arxiv.org/abs/2509.05091
- Authors: Matteo Bortoletto; Yichao Zhou; Lance Ying; Tianmin Shu; Andreas Bulling
- Reference count: 17
- One-line primary result: ProToM achieved perfect success rates and significantly higher task speedups compared to baselines while communicating fewer messages.

## Executive Summary
This paper introduces ProToM, a Theory of Mind-informed facilitator that promotes prosocial behaviour in multi-agent systems by providing targeted, context-sensitive feedback to individual agents. ProToM infers agents' goals using Bayesian inverse planning and selects feedback by maximizing expected utility conditioned on the inferred goal distribution. Evaluated in two multi-agent environments (Doors, Keys, and Gems, and Overcooked), ProToM achieved perfect success rates and significantly higher task speedups compared to baselines. It required minimal communication, averaging 0.70-0.81 messages per episode versus 1.00-1.50 for baseline models. Human participants consistently preferred ProToM's feedback as more helpful, appropriate, and better explained. The method demonstrates that integrating Theory of Mind reasoning with utility-based feedback selection enables more effective prosocial facilitation than current large language and reasoning models.

## Method Summary
ProToM is a facilitator that observes multiple agents with independent goals and provides targeted feedback to promote prosocial behavior. It maintains belief particles per agent (N=1 for mDKG, N=5 for Overcooked) and infers goals via Bayesian inverse planning. Feedback is selected by maximizing expected utility (step reduction) with thresholds (ϕ=0, ε=0.1 for mDKG; ϕ=2, ε=0.3 for Overcooked). The system only issues feedback when utility exceeds the threshold and predicted behavioral divergence exceeds ε. Implemented in Julia 1.11.5 for mDKG (using Gen.jl, PDDL.jl) and Python 3.10 for Overcooked.

## Key Results
- Achieved perfect success rates in both Doors, Keys, and Gems and Overcooked environments
- Communicated significantly fewer messages (0.70-0.81 per episode) compared to baselines (1.00-1.50)
- Human participants consistently preferred ProToM's feedback as more helpful, appropriate, and better explained
- Showed significantly higher task speedups compared to LLM and reasoning model baselines

## Why This Works (Mechanism)

### Mechanism 1: Goal Inference via Bayesian Inverse Planning
- **Claim:** Accurate prosocial feedback requires inferring the hidden goals of independent agents, which Bayesian inverse planning may provide more reliably than zero-shot LLM reasoning.
- **Mechanism:** The system maintains a belief distribution over agent goals using a particle filter. It updates the probability of a goal $g_i$ by computing the likelihood of the observed action $a_t$ given the agent's current belief state (Eq. 8). This allows the facilitator to distinguish between an agent exploring vs. executing a specific plan.
- **Core assumption:** Agents select actions according to an approximate rationality policy (e.g., Boltzmann-rational planning) based on their internal beliefs.
- **Evidence anchors:** [Section 3.1] Describes maintaining dynamic belief distributions $b^i_F$ and updating goal distributions via Bayesian inverse planning. [Abstract] "ProToM first infers agents' goals using Bayesian inverse planning..."
- **Break condition:** If agents act randomly or irrationally relative to their stated goals, the likelihood calculation $P(a_t|g_i, b^i_{S,k})$ may become uninformative, causing the particle filter to diverge.

### Mechanism 2: Utility-Driven Feedback Selection
- **Claim:** Selecting feedback based on expected marginal utility minimizes unnecessary communication while maximizing task efficiency.
- **Mechanism:** ProToM evaluates candidate feedback by computing the expected "speedup" (reduction in steps) it would cause. It simulates the agents' plans with and without the feedback. Feedback is selected only if the utility $U(f)$ exceeds a threshold $\phi$ and the expected behavioral divergence exceeds $\epsilon$ (Eq. 15).
- **Core assumption:** The utility function (step reduction) serves as a valid proxy for "prosociality" or helpfulness in the target environment.
- **Evidence anchors:** [Section 3.2] Defines the utility function $U(f)$ as the expected marginal improvement $\Delta C(f, g)$ (Eq. 12). [Results] Shows ProToM communicates fewer messages (0.81 vs >1.0 for baselines in Overcooked) while achieving higher speedup.
- **Break condition:** If the simulation of the "plan with feedback" is computationally expensive or inaccurate, the utility estimate will be noisy, potentially leading to poor feedback selection.

### Mechanism 3: Belief-Constrained Feedback Timing
- **Claim:** Providing feedback only when an agent's predicted path diverges significantly from the optimal prosocial path prevents "over-coaching."
- **Mechanism:** The system includes a divergence check ($div(f) > \epsilon$). If the agent is already likely to act optimally (or equivalently well) without intervention, the system withholds feedback. This targets the "communication overhead" problem seen in LLM baselines.
- **Core assumption:** The facilitator has a sufficiently accurate model of the environment dynamics to simulate future divergence.
- **Evidence anchors:** [Section 3.2] "ProToM communicates the feedback only when the divergence... is greater than a threshold $\epsilon$." [Figure 4] Qualitative example shows o3 providing verbose, unnecessary instructions while ProToM provides a single critical prompt.
- **Break condition:** In partially observable environments where the divergence is hard to estimate (e.g., Overcooked with $N=5$ particles), the system may miss opportunities or interrupt unnecessarily if the particle sample is unrepresentative.

## Foundational Learning

- **Concept: Bayesian Inverse Planning**
  - **Why needed here:** This is the engine of ProToM's "Theory of Mind." You cannot understand the feedback loop without understanding how $P(goal|action)$ is derived from $P(action|goal)$.
  - **Quick check question:** If an agent moves away from a door, how does a Bayesian planner update the probability that "unlocking the door" is their goal?

- **Concept: Particle Filters (Sequential Monte Carlo)**
  - **Why needed here:** ProToM uses particles to approximate the probability distribution of beliefs and goals, especially crucial in the partially observable Overcooked environment.
  - **Quick check question:** Why does the paper use $N=5$ particles for Overcooked but $N=1$ for mDKG?

- **Concept: Utility Functions in Multi-Agent Systems**
  - **Why needed here:** The system decides *what* to say based on a utility score. Understanding how "efficiency" is mathematically defined (steps saved) is key to reproducing the results.
  - **Quick check question:** Does the utility function reward the *act* of helping, or the *outcome* of saving time?

## Architecture Onboarding

- **Component map:** Observer -> Mental State Estimator -> Feedback Generator -> Utility Evaluator -> Decision Module -> Explainer
- **Critical path:** Action Observation → Particle Resampling → Goal Posterior Update → Candidate Construction → Utility Simulation → Threshold Check
- **Design tradeoffs:**
  - **Particle Count ($N$):** Higher $N$ improves robustness in partial observability but increases latency. The paper caps $N=5$ for real-time play.
  - **Thresholds ($\phi, \epsilon$):** High $\phi$ reduces noise but may miss subtle prosocial opportunities; low $\epsilon$ causes over-communication (the "LLM problem").
  - **Template vs. Generated Text:** ProToM uses templates for explanations (reliable) while baselines use generation (flexible but verbose/unpredictable).
- **Failure signatures:**
  - **Silent Failure:** Agents fail the task because ProToM's goal inference was wrong, leading to $U(f) < \phi$ for the correct feedback.
  - **Over-intervention:** The system repeatedly interrupts because the divergence metric is sensitive to minor path differences.
  - **Latency:** In complex environments, the "Utility Simulation" step (planning rollouts) may block the main loop.
- **First 3 experiments:**
  1. **Inference Validation:** Run ProToM on recorded trajectories where ground truth goals are known. Measure goal inference accuracy over time.
  2. **Parameter Sweep:** Vary $\phi$ (utility threshold) and $\epsilon$ (divergence threshold) in a simulation to map the Pareto frontier of Message Count vs. Task Speedup.
  3. **Ablation on Belief Modeling:** Compare ProToM ($N=5$) against a version with $N=1$ in the Overcooked environment to quantify the value of modeling partial observability.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can ProToM maintain its performance and utility when deployed in dynamic, real-world environments outside of constrained grid-world simulations? [explicit] The authors state in the conclusion, "We have not tested ProToM in real-world settings, which we aim to do in future work."
- **Open Question 2:** Does incorporating pragmatic communication, where the facilitator adapts language to inferred beliefs, improve the efficacy of prosocial feedback? [explicit] The authors list "pragmatic communication, where the facilitator adapts its language based on agents' inferred beliefs and goals" as a direction worth exploring.
- **Open Question 3:** How does the system perform in complex settings requiring recursive reasoning, such as when agents reason about the facilitator's intentions? [explicit] The conclusion suggests future work could consider "more complex settings where the facilitator agents may recursively reason about each other."

## Limitations
- **Unknown 1:** Exact implementation of Bayesian inverse planning—P(a_t | g_i, b_S,k) likelihood computation and prior initialization for goal distributions is not fully specified.
- **Unknown 2:** The utility function's focus on step reduction may not capture all forms of prosocial behavior, potentially limiting generalizability to environments where efficiency isn't the primary metric.
- **Unknown 3:** Real-world applications may feature more complex goal structures and stochastic agent behaviors that could degrade inference accuracy.

## Confidence
- **High Confidence:** The performance metrics showing perfect success rates and higher speedup in controlled environments (mDKG, Overcooked) are well-supported by the experimental design and baseline comparisons.
- **Medium Confidence:** The human preference results (18 participants) support the claim that ProToM's feedback is perceived as more helpful, though the sample size limits generalizability.
- **Medium Confidence:** The mechanism of utility-driven feedback selection reducing communication overhead is demonstrated, but the paper doesn't fully explore edge cases where this might fail.

## Next Checks
1. **Inference Validation:** Test ProToM's goal inference accuracy on recorded trajectories with known ground truth goals across varying levels of agent rationality and environmental complexity.
2. **Parameter Sensitivity Analysis:** Conduct a systematic sweep of utility (ϕ) and divergence (ε) thresholds across different scenario types to map the communication-efficiency Pareto frontier and identify optimal parameter settings.
3. **Cross-Environment Generalization:** Evaluate ProToM in a third, structurally distinct multi-agent environment (e.g., different grid-world or coordination task) to assess whether the Bayesian inverse planning approach generalizes beyond the two tested domains.