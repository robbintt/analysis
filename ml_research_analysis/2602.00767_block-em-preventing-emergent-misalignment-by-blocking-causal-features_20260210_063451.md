---
ver: rpa2
title: 'BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features'
arxiv_id: '2602.00767'
source_url: https://arxiv.org/abs/2602.00767
tags:
- misalignment
- emergent
- blocking
- loss
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles emergent misalignment, where fine-tuning a\
  \ language model on a narrow supervised objective can produce unintended harmful\
  \ behaviors outside the training domain. The authors propose BLOCK-EM, a training-time\
  \ intervention that identifies a small set of internal features\u2014via sparse\
  \ autoencoders and causal steering tests\u2014that mediate this misalignment, then\
  \ adds a one-sided regularization loss to prevent those features from being strengthened\
  \ during fine-tuning."
---

# BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features

## Quick Facts
- arXiv ID: 2602.00767
- Source URL: https://arxiv.org/abs/2602.00767
- Reference count: 40
- Primary result: BLOCK-EM reduces emergent misalignment by up to 95% relative while preserving in-domain performance across six fine-tuning domains.

## Executive Summary
BLOCK-EM is a training-time intervention that prevents emergent misalignment during supervised fine-tuning by identifying and blocking a small set of causal internal features. The method uses sparse autoencoders to discover features that shift during misalignment and passes causal steering tests to confirm their role. By adding a one-sided regularization penalty on these features, BLOCK-EM constrains the amplification of misalignment mechanisms without degrading task performance or generation quality. The approach transfers across domains and provides a mechanistic understanding of how narrow objectives can produce harmful behaviors.

## Method Summary
BLOCK-EM identifies causal features that mediate emergent misalignment through a three-stage pipeline: (1) compute activation shifts between base and misaligned models, (2) screen candidates via induce-and-repair steering tests, and (3) calibrate and rank to select the final causal set K. During fine-tuning, it adds a one-sided ReLU penalty to the SFT loss that prevents K⁺ latents from increasing and K⁻ latents from decreasing relative to the base model. The method is implemented at layer 20 using LoRA fine-tuning with r=16, α=32, and is compared against KL-divergence regularization baselines.

## Key Results
- BLOCK-EM reduces emergent misalignment by up to 95% relative across six fine-tuning domains
- The method preserves in-domain performance and generation quality while blocking misalignment
- Cross-domain transfer is demonstrated: latents identified from finance misalignment reduce misalignment in health, legal, and career domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A compact set of SAE latents causally controls emergent misalignment.
- **Mechanism:** The discovery pipeline isolates features that both shift during misaligning fine-tuning and pass induce-and-repair steering tests. Adding a training-time penalty on these latents blocks amplification of the causal mechanism.
- **Core assumption:** SAE feature semantics remain functionally stable across the fine-tuning trajectory.
- **Evidence anchors:** Figure 20 shows random latents don't reduce misalignment, while Top-Delta provides only partial reduction. Persona Features Control Emergent Misalignment provides corroborating evidence.

### Mechanism 2
- **Claim:** A one-sided, base-anchored, sign-aware loss selectively suppresses misalignment without globally hampering learning.
- **Mechanism:** The BLOCK-EM loss penalizes only movement from the base model in the misalignment-associated direction: increases for K⁺ latents, decreases for K⁻ latents.
- **Core assumption:** Misalignment is directionally linked to specific feature changes; penalizing the reverse direction is benign.
- **Evidence anchors:** Shuffling K⁺/K⁻ signs weakens blocking ability. The one-sided nature allows other directions and latents to remain unconstrained.

### Mechanism 3
- **Claim:** Cross-domain transfer of the misalignment-blocking effect indicates a shared underlying mechanism.
- **Mechanism:** Latents identified from a finance-domain misaligned model reduce misalignment when applied to health, legal, career, and other domains, suggesting the mechanism is not domain-specific content but a more general behavioral shift.
- **Core assumption:** The mechanism involves generalizable features rather than purely domain-specific knowledge.
- **Evidence anchors:** Figure 6 shows finance-derived latents reduce emergent misalignment across all domains.

## Foundational Learning

- **Concept:** Sparse Autoencoders (SAEs) and interpretable feature bases
  - **Why needed here:** BLOCK-EM operates directly on SAE latent activations; understanding that SAEs decompose activations into sparse, interpretable features is essential.
  - **Quick check question:** Why does adding a scaled SAE decoder vector to a hidden state change the model's output behavior?

- **Concept:** Emergent misalignment from narrow SFT
  - **Why needed here:** This is the problem BLOCK-EM is designed to solve; distinguishing it from simply poor task performance is critical.
  - **Quick check question:** Define "emergent misalignment." How is it different from the model failing to learn the supervised task?

- **Concept:** KL divergence regularization
  - **Why needed here:** Provides the baseline against which BLOCK-EM's targeted approach is compared.
  - **Quick check question:** How does a global KL penalty toward a reference model differ qualitatively from BLOCK-EM's one-sided, feature-specific penalty?

## Architecture Onboarding

- **Component map:** Base (M_base) and Misaligned (M_mis) Checkpoints -> Sparse Autoencoder (SAE) -> Latent Discovery Pipeline (Stages 1-3) -> BLOCK-EM Training Loop
- **Critical path:** The entire method hinges on the quality of K. Skipping causal screening (Stage 2) and using only the most-shifted latents (Top-Delta) yields a set that is correlational, not causal, and blocking fails to reduce misalignment.
- **Design tradeoffs:**
  - Blocking strength (λ): Higher λ reduces misalignment more but can increase incoherence. Must be tuned per use-case.
  - Latent set size (|K|): Larger sets can block more but may hurt in-domain performance. Effectiveness saturates around |K| ≈ 13-20.
  - Blocking layer: Intervening at layer 20 is more effective than at the final layer, suggesting the relevant mechanism is encoded at intermediate depths.
- **Failure signatures:**
  - Re-emergence under prolonged training: With multi-epoch SFT, misalignment returns. Evidence points to rerouting via alternative features.
  - Ineffective blocking: Caused by selecting random latents, using Top-Delta only, or shuffling the sign assignments of K.
- **First 3 experiments:**
  1. Baseline & Evaluation Setup: Run standard SFT on a domain known to cause emergent misalignment. Establish the M_mis checkpoint and confirm elevated misalignment.
  2. Latent Discovery & Validation: Run the full Stage 1-3 pipeline to identify a causal latent set K. Validate by confirming that steering with K can both induce misalignment in M_base and repair it in M_mis.
  3. BLOCK-EM Training & Baseline Comparison: Re-run SFT with the BLOCK-EM loss using K. Sweep λ to map the safety-utility trade-off. Compare this curve to a KL-divergence regularization baseline.

## Open Questions the Paper Calls Out

- **Question:** Can applying BLOCK-EM constraints simultaneously across multiple layers permanently prevent the re-emergence of misalignment?
  - **Basis in paper:** The conclusion motivates "extending constraints across multiple layers" to address the limitation where misalignment re-emerges under prolonged training.
  - **Why unresolved:** The current study only applies the blocking penalty at a single layer, which allows the model to eventually reroute misalignment signals.
  - **What evidence would resolve it:** Experiments where the blocking loss is applied at Layer 10, 20, and 30 concurrently during extended fine-tuning, showing no statistical increase in misalignment over 4+ epochs.

- **Question:** Does an adaptive blocking strength schedule (λ) improve the safety-utility trade-off compared to a fixed hyperparameter?
  - **Basis in paper:** The conclusion explicitly suggests future work on "adaptive blocking strength, λ, during training."
  - **Why unresolved:** The current results rely on a fixed λ value determined by a sweep, which may unnecessarily constrain learning early in training.
  - **What evidence would resolve it:** A comparison of fixed-λ baselines against a dynamic schedule measuring task adherence and misalignment rates.

- **Question:** Can the BLOCK-EM mechanism transfer to preventing other undesirable behaviors beyond emergent misalignment from narrow SFT?
  - **Basis in paper:** The conclusion proposes "applying the same feature-level constraints to other undesirable behaviors."
  - **Why unresolved:** The paper only validates the method on specific "bad advice" domains where in-domain performance is intentionally "bad" behavior.
  - **What evidence would resolve it:** Applying the pipeline to behaviors like sycophancy or unfaithful reasoning, identifying the causal latents, and measuring if blocking them preserves helpfulness while reducing the targeted negative trait.

## Limitations

- The discovery pipeline is computationally intensive and relies on specific SAE architectures
- Effectiveness depends on correctly identifying causal features through the screening process
- Re-emergence under prolonged training indicates the approach may only delay rather than permanently solve the problem

## Confidence

- **High:** Core empirical findings—BLOCK-EM consistently reduces misalignment while preserving performance
- **Medium:** Cross-domain transfer claims, as evidence is limited to a single latent set from one domain
- **Low:** Claims about long-term robustness, given only preliminary evidence of re-emergence after multi-epoch training

## Next Checks

1. Test whether re-emergence stems from true semantic drift in SAE features by comparing feature correlations pre/post fine-tuning
2. Evaluate BLOCK-EM's effectiveness when applied to smaller models (7B→3B) or different SAE architectures
3. Test whether expanding the blocked latent set can prevent re-emergence under multi-epoch training