---
ver: rpa2
title: 'The Confidence Trap: Gender Bias and Predictive Certainty in LLMs'
arxiv_id: '2601.07806'
source_url: https://arxiv.org/abs/2601.07806
tags:
- calibration
- bias
- gender
- confidence
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how the confidence scores of large language
  models (LLMs) align with gender bias judgments in pronoun resolution tasks. The
  authors introduce Gender-ECE, a new metric to measure gender-related calibration
  disparities.
---

# The Confidence Trap: Gender Bias and Predictive Certainty in LLMs

## Quick Facts
- arXiv ID: 2601.07806
- Source URL: https://arxiv.org/abs/2601.07806
- Reference count: 5
- This paper examines how the confidence scores of large language models (LLMs) align with gender bias judgments in pronoun resolution tasks. The authors introduce Gender-ECE, a new metric to measure gender-related calibration disparities. Using datasets like WinoBias, Winogender, and GenderLex, they evaluate six state-of-the-art open-weight LLMs. Results show that Gemma-2-9B exhibits the worst calibration and fairness, with high calibration error for female pronouns and poor human alignment. GPT-J-6B shows the best calibration overall. Most models are less calibrated on female pronouns, revealing a bias toward male references. Post-hoc calibration methods like Beta calibration significantly improve reliability. The findings highlight the importance of gender-aware evaluation metrics for fair and trustworthy LLM deployment.

## Executive Summary
This study introduces Gender-ECE, a novel metric to quantify gender-specific calibration disparities in LLMs during pronoun resolution tasks. By evaluating six open-weight models on datasets like WinoBias, Winogender, and GenderLex, the authors find that models consistently exhibit worse calibration for female pronouns, with Gemma-2-9B showing the poorest performance. GPT-J-6B demonstrates the best calibration overall. The research highlights the importance of gender-aware evaluation metrics for ensuring fair and trustworthy LLM deployment, and shows that post-hoc calibration methods like Beta calibration can significantly improve confidence reliability without altering model weights.

## Method Summary
The authors evaluate six state-of-the-art open-weight LLMs on gender bias in pronoun resolution tasks. They use deterministic forward passes to extract token probabilities at pronoun positions, then compute Expected Calibration Error (ECE) and introduce Gender-ECE to measure gender-specific calibration disparities. The models are tested on datasets including WinoBias, Winogender, and GenderLex. Post-hoc Beta calibration is applied to improve confidence reliability. The study also examines human alignment to assess how well model predictions match human bias judgments.

## Key Results
- Gemma-2-9B exhibits the worst calibration and fairness, with high calibration error for female pronouns and poor human alignment.
- GPT-J-6B shows the best calibration overall among the evaluated models.
- Most models are less calibrated on female pronouns, revealing a bias toward male references.
- Post-hoc calibration methods like Beta calibration significantly improve reliability, reducing ECE by approximately three times.
- Gender-ECE effectively captures gender-specific calibration disparities masked by aggregate ECE.

## Why This Works (Mechanism)

### Mechanism 1: Gender-Specific Calibration Decomposition
- Claim: Splitting calibration error by predicted gender pronoun reveals disparities masked by aggregate ECE.
- Mechanism: Gender-ECE partitions predictions into male/female groups based on the model's predicted label (not ground truth), computes ECE separately for each group using equal-width binning, then averages. This isolates whether a model's confidence reliably tracks correctness for each gender.
- Core assumption: The paper assumes that calibration gaps across gender groups reflect representational harm and training data skew, though this is framed as a measurement tool rather than a causal explanation.
- Evidence anchors:
  - [abstract] "we introduce a new calibration metric, Gender-ECE, designed to measure gender disparities in resolution tasks"
  - [section: Methods] "Gender-ECE divides data into two groups (i.e. male and female) based on the predicted label, not relying on the true label for the division"
  - [corpus] Related work on intersectional bias (arXiv:2508.07111) similarly uses confidence disparities in coreference, suggesting convergent validity of this approach
- Break condition: If predictions are tightly clustered around 0.5 probability for both genders, Gender-ECE becomes unstable due to sparse bins.

### Mechanism 2: Token Probability Extraction via Deterministic Forward Pass
- Claim: Direct logit extraction at pronoun positions yields reproducible confidence estimates without decoding variance.
- Mechanism: For sentence S with pronoun w_p at position k, extract softmax probability P(w_p | w_1, ..., w_{k-1}) = exp(z_{k-1,wp}) / Σ exp(z_{k-1,j}). No sampling parameters or temperature scaling applied.
- Core assumption: Assumes that next-token probability reflects the model's genuine confidence in gender assignment, not just surface-level word prediction.
- Evidence anchors:
  - [section: Methods] Equation 1 defines probability computation from logit scores
  - [section: Experimental Result] "we perform a deterministic forward pass (w/o decoding parameters involved) and utilize offset mappings for precise token alignment"
  - [corpus] Calibration research (arXiv:2504.13548) notes limitations of one-hot labels; token-level extraction avoids this by capturing continuous confidence
- Break condition: If pronoun tokenization is inconsistent across sentence variants, offset mappings may misalign.

### Mechanism 3: Post-Hoc Beta Calibration for Confidence Realignment
- Claim: Fitting a Beta calibration function on validation data reduces ECE by ~3× without changing model weights.
- Mechanism: Learn a parametric transformation that maps model confidence scores to empirical accuracy rates. For 100 predictions at 80% confidence, calibrator adjusts toward the observed correctness rate.
- Core assumption: Assumes validation set bias distribution approximates deployment distribution; calibration does not mitigate underlying bias, only improves confidence reliability.
- Evidence anchors:
  - [section: Discussion] "Beta calibration method is able to alleviate calibration issues with all the models, resulting in about three times lower calibration error (ECE)"
  - [section: Discussion] Figure 3 shows reliability diagrams before/after calibration
  - [corpus] Weak corpus signal on Beta calibration specifically; related work (arXiv:2511.00280) studies layer-wise calibration evolution
- Break condition: If validation set is too small (<150 instances per Table 7 ablation), ECE estimates become unstable, degrading calibration quality.

## Foundational Learning

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: ECE quantifies the gap between predicted confidence and observed accuracy; Gender-ECE extends this by conditioning on predicted gender.
  - Quick check question: Given a model that assigns 90% confidence to 100 predictions but is correct only 60% of the time, what is the calibration error for that bin?

- Concept: **Coreference Resolution in Winograd Schemas**
  - Why needed here: The benchmark tasks require resolving pronoun references using syntactic and semantic cues; gender bias emerges when models systematically prefer one resolution.
  - Quick check question: In "The developer argued with the designer and slapped her," what contextual information determines whether "her" refers to developer or designer?

- Concept: **Post-Hoc vs. In-Training Calibration**
  - Why needed here: Beta calibration is applied after inference; understanding this distinction clarifies why calibration improves confidence reliability without reducing underlying bias.
  - Quick check question: If a model is well-calibrated but makes biased predictions, does calibration alone address fairness concerns?

## Architecture Onboarding

- Component map:
  - Input layer -> Probability extractor -> Calibration evaluator -> Gender-ECE module -> Post-hoc calibrator

- Critical path:
  1. Load sentence pair variants (male/female pronoun versions)
  2. Run deterministic forward pass for each variant
  3. Extract pronoun token probability using offset mapping
  4. Compare probabilities against human-labeled bias ground truth
  5. Compute ECE, Gender-ECE, human alignment score
  6. (Optional) Fit Beta calibrator on validation, evaluate on test

- Design tradeoffs:
  - **Equal-width vs. equal-size binning**: Equal-width bins are interpretable but may have sparse bins; equal-size ensures statistical weight per bin but less intuitive probability ranges. Paper shows similar results for both (Figure 4).
  - **Predicted-label vs. true-label grouping**: Gender-ECE uses predicted labels (precision-sensitive); cc-ECE uses true labels (recall-sensitive). Choice depends on whether false positives or false negatives are more concerning.
  - **Template-based vs. free-form evaluation**: Templates provide controlled comparison but may not reflect real-world usage. Paper's ablation on COCO captions shows ECE becomes less sensitive in free-form settings (Table 8).

- Failure signatures:
  - **ECE ≈ 0.5 with clustered predictions**: Indicates model outputs near-uniform probabilities; ECE cannot discriminate calibration quality.
  - **Large Gender-ECE gap (e.g., Gemma-2-9B: 0.438 male vs. 0.156 female)**: Model is well-calibrated for one gender but poorly calibrated for the other; deployment risk for high-stakes decisions.
  - **Human alignment < 0.6**: Model predictions diverge significantly from human bias judgments; may indicate training data distribution mismatch or over-filtering.

- First 3 experiments:
  1. **Baseline calibration assessment**: Run all six models on GenderLex (last-cloze structure), compute ECE and Gender-ECE. Verify that GPT-J-6B yields lowest ECE (~0.076) and Gemma-2-9B yields highest (~0.327) as reported.
  2. **Beta calibration ablation**: Fit Beta calibrator on 385 validation instances from WinoBias, evaluate ECE reduction on 386 test instances. Target ~3× reduction as paper reports.
  3. **Gender-neutral term probe**: Replace occupational titles with "someone" or "person" in GenderLex, measure ECE increase. Expect highest calibration error with "someone" as Table 5 shows.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can post-hoc calibration methods be extended or adapted to serve as genuine bias mitigation techniques, rather than merely improving confidence reliability?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "calibration helps improve the trustworthiness of the model, these techniques do not serve as true bias mitigation strategies. In future work, we plan to address this by proposing post-processing mitigation techniques."
- Why unresolved: Post-hoc methods like Beta calibration adjust confidence scores to match empirical accuracy but do not alter the underlying decision boundaries or representation biases that cause disparate predictions across gender groups.
- What evidence would resolve it: Development and empirical validation of post-processing methods that reduce both calibration error and prediction disparities (e.g., equalized odds or demographic parity) on gender bias benchmarks.

### Open Question 2
- Question: Do the calibration and fairness findings for gender bias generalize to other social biases such as nationality, race, or disability?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "In future work... we aim to extend our work to address biases beyond gender, such as stereotypes related to nationality and disability."
- Why unresolved: The Gender-ECE metric and experimental setup are designed specifically for binary gender pronoun resolution; other bias dimensions involve different linguistic patterns, identity representations, and evaluation challenges.
- What evidence would resolve it: Replication of the calibration-fairness analysis on established benchmarks for other bias types (e.g., CrowS-Pairs, StereoSet for nationality/disability) using adapted grouping metrics.

### Open Question 3
- Question: How does calibration and gender-specific fairness transfer in cross-lingual settings, particularly for grammatically gendered or genderless languages?
- Basis in paper: [explicit] The authors explicitly note: "Future work will investigate Cross-Lingual Bias Transfer, where English is used as a pivot to train low-resource languages. This potentially propagates stereotypes into the target language, as seen in Estonian, where bias is encoded through lexical suffixes."
- Why unresolved: The current framework operates only on English with gendered pronouns; cross-lingual transfer introduces complexities such as grammatical gender systems, lack of direct pronoun equivalents, and cultural differences in bias expression.
- What evidence would resolve it: Evaluation of multilingual models' calibration on translated or native bias benchmarks across typologically diverse languages, with analysis of how gender bias manifests differently.

### Open Question 4
- Question: Do calibration and fairness disparities observed in template-based pronoun tasks persist in naturalistic, free-form text generation contexts?
- Basis in paper: [inferred] The ablation study using COCO image captions found that "ECE may not be sensitive enough to detect subtle differences in model calibration under this experimental setup," suggesting open questions about the ecological validity of template-based bias measurements.
- Why unresolved: Template-based methods provide controlled comparisons but may not capture how bias and miscalibration manifest in open-ended generation where context is richer and less structured.
- What evidence would resolve it: Design and deployment of gender-bias evaluation protocols using free-form generation tasks (e.g., story completion, persona descriptions) with confidence calibration metrics adapted for variable-length outputs.

## Limitations

- The Gender-ECE metric, while novel, relies on grouping by predicted labels rather than ground truth, which could conflate calibration with accuracy issues.
- The human alignment score is critical but its computation method is not explicitly defined, and the moderate inter-annotator agreement (Cohen's κ=0.51 for GenderLex) introduces uncertainty.
- The effectiveness of post-hoc Beta calibration is demonstrated, but the paper does not investigate whether calibration improves downstream task performance, only confidence reliability.
- The analysis is limited to English pronoun resolution tasks and a specific set of open-weight models, limiting generalization to other languages or proprietary models.

## Confidence

- **High Confidence:** The observation that most models exhibit worse calibration on female pronouns compared to male pronouns, and that Gemma-2-9B has the highest overall ECE while GPT-J-6B has the best calibration.
- **Medium Confidence:** The claim that Gender-ECE is a valuable and sensitive tool for detecting gender-specific calibration disparities.
- **Medium Confidence:** The assertion that Beta calibration can reduce ECE by approximately three times.

## Next Checks

1. **Validation of Human Alignment Computation:** Re-implement the human alignment score calculation using the provided datasets and human labels. Verify the score ranges and correlations reported in the paper, and assess the impact of the moderate inter-annotator agreement on the results.
2. **Ablation on Binning Strategy:** Repeat the ECE and Gender-ECE calculations using equal-size binning instead of equal-width binning to confirm that the reported disparities are robust to the binning method.
3. **Downstream Task Performance:** After applying Beta calibration, evaluate the models on a downstream pronoun resolution task (e.g., accuracy on WinoBias) to determine if improved calibration translates to better or worse task performance, or if it is purely a confidence reliability improvement.