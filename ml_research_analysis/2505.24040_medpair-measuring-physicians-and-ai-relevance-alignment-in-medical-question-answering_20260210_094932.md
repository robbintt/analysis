---
ver: rpa2
title: 'MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question
  Answering'
arxiv_id: '2505.24040'
source_url: https://arxiv.org/abs/2505.24040
tags:
- relevance
- high
- physician
- familiarity
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedPAIR introduces a dataset of 1,300 medical QA pairs with sentence-level
  relevance annotations from 36 physician trainees. It compares human expert relevance
  judgments with those of LLMs using both self-reporting and ContextCite attribution.
---

# MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering

## Quick Facts
- arXiv ID: 2505.24040
- Source URL: https://arxiv.org/abs/2505.24040
- Reference count: 40
- Key outcome: LLM relevance judgments frequently disagree with physicians on which medical information is diagnostically important

## Executive Summary
MedPAIR introduces a dataset of 1,300 medical QA pairs with sentence-level relevance annotations from 36 physician trainees. The study compares human expert relevance judgments with those of LLMs using both self-reporting and ContextCite attribution. Results show that LLMs frequently disagree with physicians on which information is relevant, and that removing physician-labeled irrelevant sentences improves LLM accuracy by 5-25 percentage points. GPT-4o outperforms open-source models while Qwen-72B shows the highest sensitivity to irrelevant content. The benchmark enables fine-grained analysis of LLM reasoning and alignment with clinical expertise.

## Method Summary
The paper creates MedPAIR by annotating 1,300 medical QA pairs with sentence-level relevance labels from physician trainees. Each QA pair is labeled by 3+ physicians who answer the question and then identify relevant sentences at three levels: high, low, and irrelevant. The dataset spans multiple sources including MMLU, JAMA, MedBullets, and MedXpertQA. LLMs are evaluated using both self-reported relevance and ContextCite attribution, with accuracy measured on both original and physician-pruned contexts.

## Key Results
- LLMs show less than 66% concordance with physician relevance judgments across all tested models
- Removing physician-labeled irrelevant sentences improves LLM accuracy by 5-25 percentage points
- GPT-4o achieves highest overall accuracy (39.3%) but lowest spurious rate (2.1-6.5%)
- Qwen-72B shows highest sensitivity to irrelevant content with spurious rates of 9.8-18.5%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing physician-identified irrelevant sentences improves both LLM and human QA accuracy.
- **Mechanism:** Context pruning reduces attention dilution—both humans and models allocate limited cognitive/computational capacity across input tokens. Irrelevant sentences introduce noise that competes with diagnostically relevant information for attention weights. Filtering concentrates processing on clinically salient features, reducing the search space and likelihood of spurious correlation.
- **Core assumption:** Physician trainees' majority-vote relevance labels represent a valid approximation of clinical reasoning priorities; sentences marked "high relevance" contain sufficient information for correct diagnosis when isolated.
- **Evidence anchors:**
  - [abstract] "After filtering out physician trainee-labeled irrelevant sentences, accuracy improves for both the trainees and the LLMs."
  - [Section 4.3] GPT-4o improved from 39.3% to 73.0% overall on MedXpertQA after filtering; physician trainee accuracy improved from 48.3% to 67.2% on the evaluation subset.
  - [corpus] Related work on clinical QA evaluation (LongQAEval) similarly emphasizes resource-constrained evaluation but does not address sentence-level relevance alignment specifically.
- **Break condition:** If physician annotations systematically exclude information that is diagnostically useful in edge cases, pruning could reduce recall on rare presentations. The paper notes 1.6-18.5% of correct answers flipped to incorrect after pruning (Table 3), indicating some spurious but functional correlations exist.

### Mechanism 2
- **Claim:** LLM relevance judgments (via ContextCite or self-report) do not reliably align with physician trainee judgments.
- **Mechanism:** ContextCite approximates attention distribution by measuring output sensitivity to input perturbations, but this captures statistical salience rather than clinical salience. LLMs may attend to surface patterns (extreme lab values, anatomical terms) that correlate with answers in training data but lack causal diagnostic value. Self-reported relevance inherits prompt sensitivity and lacks independent verification.
- **Core assumption:** Concordance rates between model and human relevance judgments meaningfully reflect reasoning alignment rather than superficial agreement; disagreement indicates different prioritization strategies rather than annotation noise.
- **Evidence anchors:**
  - [Section 4.2] "Although Llama-70B achieved the highest agreement rate at 65.9 percent, the concordance did not exceed two thirds of all instances."
  - [Section 4.4] When GPT-4o was evaluated on contexts pruned by its own self-reported labels, performance sometimes degraded (e.g., -1.8% on MMLU), whereas physician-pruned contexts yielded consistent gains.
  - [corpus] Weak corpus evidence on attribution reliability—related papers do not directly validate ContextCite-style attribution against human expert judgment in clinical domains.
- **Break condition:** If physician trainee annotations themselves exhibit high variance or systematic bias (e.g., over-weighting recent training material), then low concordance may reflect expert disagreement rather than model misalignment. The paper notes 58.6% of labelers characterized the relevance task as having "moderate ambiguity."

### Mechanism 3
- **Claim:** Models with higher Spurious Rate (SR) rely more heavily on information humans deem irrelevant.
- **Mechanism:** SR measures the proportion of correct answers that become incorrect when physician-labeled irrelevant sentences are removed. High SR indicates the model's correct predictions depend on spurious features—statistical correlations in training data that lack clinical validity. This creates brittle performance that may not generalize to distribution shifts.
- **Core assumption:** A correct answer that flips to incorrect after removing irrelevant context indicates reliance on spurious information rather than the model having learned an alternative valid reasoning path.
- **Evidence anchors:**
  - [Section 3.2] Formal definition: SR(f) counts cases where f(S) = Y but f(S+) ≠ Y, measuring reliance on S− (irrelevant sentences).
  - [Table 3] Qwen-14B showed SR of 9.8-18.5% across datasets; GPT-4o showed lowest SR (2.1-6.5%), suggesting more robust reasoning.
  - [corpus] No direct corpus validation of SR as a generalization predictor—this appears to be a novel metric introduced in this work.
- **Break condition:** If some "irrelevant" sentences contain implicit diagnostic cues that physicians consciously exclude but models legitimately utilize, then SR may overestimate spuriousness. The qualitative analysis (Appendix D) attempts to address this but cannot fully rule out valid alternative reasoning paths.

## Foundational Learning

- **Context Attribution Methods (ContextCite, influence functions):**
  - Why needed here: The paper uses ContextCite to generate model-side relevance scores; understanding attribution methods is prerequisite to interpreting why model and human relevance diverge.
  - Quick check question: Given an LLM response, can you explain how ContextCite would identify which input sentences most influenced the output, and why attention-based attribution may not capture clinical reasoning?

- **Inter-Rater Agreement and Majority Voting:**
  - Why needed here: The dataset uses 3+ physician annotations per QA with majority voting; understanding agreement metrics is essential for assessing annotation reliability and the "ground truth" quality.
  - Quick check question: If three annotators label a sentence as High, Low, and Irrelevant respectively, what are the tradeoffs of using majority voting vs. weighted averaging for the final label?

- **Clinical Vignette Structure and Diagnostic Reasoning:**
  - Why needed here: The paper focuses on patient case QA; understanding how clinicians synthesize demographic info, symptoms, exam findings, and history is necessary to interpret what "relevance" means in this context.
  - Quick check question: In a clinical vignette asking for a diagnosis, what types of information would typically be "high relevance" vs. "low relevance" vs. "irrelevant," and how might this differ from an LLM's attention pattern?

## Architecture Onboarding

- **Component map:**
  Data Sources (MMLU, JAMA, MedBullets, MedXpertQA) -> Sampling & Preprocessing (2,000 QA pairs → 1,300 with removable sentences) -> Annotation Pipeline: Physician trainees (Centaur Labs): Answer selection → Sentence-level relevance labels; LLMs: ContextCite scores (Qwen-14B/72B, Llama-70B) OR Self-report prompts (GPT-4o) -> Label Aggregation: Majority voting for physicians; score thresholding for ContextCite -> Evaluation Framework: Round 1: Original context accuracy; Round 2: Filtered context (physician-labeled high-relevance only); Metrics: Accuracy delta, Spurious Rate, concordance rates

- **Critical path:**
  1. **Annotation quality control** — Only annotations from physicians who answered correctly are retained; this filters out labels from incorrect reasoning but may introduce selection bias.
  2. **Relevance label mapping** — ContextCite numerical scores must be mapped to categorical labels using physician label count as budget (k sentences = k highest ContextCite scores labeled "high").
  3. **Paired evaluation** — Same QA must be evaluated in both original and filtered conditions to isolate pruning effect.

- **Design tradeoffs:**
  - **Physician trainees vs. board-certified physicians:** Trainees are more familiar with exam-style questions but may lack clinical experience; the paper argues exam familiarity outweighs this concern.
  - **ContextCite vs. self-report attribution:** ContextCite is model-intrinsic but may not capture deliberative reasoning; self-report is interpretable but prompt-sensitive and potentially unreliable (paper notes self-report "may lack reliability").
  - **Binary vs. trinary relevance labels:** Three levels (High/Low/Irrelevant) provide granularity but increase annotation complexity; aggregation to binary (High vs. not-High) used for evaluation.

- **Failure signatures:**
  - **High variance in physician labels:** If inter-annotator agreement is low (<0.5 kappa), majority voting may not represent consensus; check label distribution per sentence.
  - **ContextCite flat scores:** If ContextCite scores show minimal variance across sentences (paper notes scores "plateau between 0.20 and 0.25"), the method may not be capturing meaningful distinctions.
  - **Negative accuracy delta after pruning:** If filtering reduces accuracy for a specific model-dataset pair, investigate whether that model has high SR (legitimate reliance on spurious features) or whether annotations exclude valid cues.

- **First 3 experiments:**
  1. **Reproduce concordance analysis:** For a single dataset (e.g., JAMA), compute sentence-level agreement between physician majority labels and each LLM's ContextCite-derived labels. Report Cohen's kappa and per-category precision/recall.
  2. **Ablate filtering threshold:** Instead of using physician label count as budget, try fixed thresholds (top 50%, top 30% of ContextCite scores) and measure accuracy delta. This tests whether the alignment mechanism is specific to physician-guided budgets.
  3. **Cross-model SR correlation:** Compute SR for multiple models and correlate with their original accuracy. Test the hypothesis that higher-accuracy models have lower SR (more robust reasoning). The paper suggests this pattern (GPT-4o highest accuracy, lowest SR) but does not formally test it.

## Open Questions the Paper Calls Out

- [Section 5.1] Clinical relevance of disagreements between physician and LLM relevance judgments
- [Section 5.2] Applicability of MedPAIR framework to real-world clinical decision-making scenarios
- [Section 5.3] Impact of annotation ambiguity on benchmark reliability and validity

## Limitations

- **Annotation validity:** Physician trainee relevance judgments may not fully represent clinical expert reasoning, particularly given the 58.6% "moderate ambiguity" rating and the use of exam-style rather than clinical practice contexts.
- **ContextCite reliability:** The attribution method's ability to capture meaningful relevance signals is questionable given the reported score plateau (0.20-0.25) and lack of external validation against ground-truth clinical reasoning.
- **Generalizability:** The MedPAIR dataset focuses on board exam-style questions, which may not capture the complexity and uncertainty of real clinical decision-making scenarios.

## Confidence

- **High confidence:** The empirical finding that physician-pruned contexts improve both human and LLM accuracy (Section 4.3 results are consistent across multiple datasets and models).
- **Medium confidence:** The claim that LLM relevance judgments generally disagree with physicians (concordance rates below 66% are clearly demonstrated, but the clinical significance of this disagreement is less certain).
- **Medium confidence:** The assertion that SR identifies models relying on spurious features (the metric is novel and internally consistent, but lacks external validation as a generalization predictor).

## Next Checks

1. **External expert validation:** Have board-certified physicians independently review a stratified sample of cases where models showed high SR to determine whether the pruned content genuinely contains spurious information or represents valid alternative reasoning paths.

2. **Cross-domain generalization test:** Apply the MedPAIR evaluation framework to a dataset of clinical practice vignettes (rather than board exam questions) to assess whether the relevance alignment patterns hold in more realistic medical reasoning contexts.

3. **Temporal stability analysis:** Evaluate the same models on the same datasets after a 3-6 month interval to determine whether SR scores and relevance disagreements remain consistent or are influenced by model updates and evolving training data.