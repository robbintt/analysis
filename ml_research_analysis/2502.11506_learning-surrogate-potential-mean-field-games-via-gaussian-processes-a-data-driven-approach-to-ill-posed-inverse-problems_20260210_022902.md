---
ver: rpa2
title: 'Learning Surrogate Potential Mean Field Games via Gaussian Processes: A Data-Driven
  Approach to Ill-Posed Inverse Problems'
arxiv_id: '2502.11506'
source_url: https://arxiv.org/abs/2502.11506
tags:
- problem
- points
- recovered
- potential
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the ill-posed inverse problem in potential
  mean field games (MFGs), where the goal is to recover population, momentum, and
  environmental parameters from limited, noisy observations. The authors propose two
  Gaussian process (GP)-based frameworks: an inf-sup formulation and a bilevel approach.'
---

# Learning Surrogate Potential Mean Field Games via Gaussian Processes: A Data-Driven Approach to Ill-Posed Inverse Problems

## Quick Facts
- arXiv ID: 2502.11506
- Source URL: https://arxiv.org/abs/2502.11506
- Reference count: 40
- Primary result: Proposes GP-based inf-sup and bilevel frameworks for recovering MFG parameters from limited, noisy observations

## Executive Summary
This paper addresses the ill-posed inverse problem in potential mean field games, where the goal is to recover population, momentum, and environmental parameters from limited, noisy observations. The authors propose two Gaussian process-based frameworks: an inf-sup formulation for concave unknowns and a bilevel approach for general cases. Both methods leverage GP linearity to preserve convexity structure, enabling standard convex solvers. Numerical experiments demonstrate that sufficient prior information enables accurate recovery of unknown parameters, while limited priors result in surrogate MFG models that closely match observed data.

## Method Summary
The paper tackles MFG inverse problems by parameterizing unknown functions (spatial cost V, coupling F, metric Λ, exponent q, viscosity ν) with Gaussian processes. Two frameworks are proposed: an inf-sup formulation for concave unknowns using primal-dual algorithms, and a bilevel approach for general cases. The inner level solves the forward MFG problem via Chambolle-Pock algorithm, while the outer level optimizes parameters to minimize data misfit plus regularization. Unknown functions are approximated via GPs with Matérn kernels, and convexity of F is enforced through Monte Carlo penalties. Gradients are computed via automatic differentiation or adjoint methods for solver independence.

## Key Results
- GP-based inf-sup formulation successfully recovers spatial cost V when coupling function F is known and convex
- Bilevel framework recovers surrogate MFGs that reproduce observed data even when true parameters are unidentifiable
- Adjoint-based gradient computation offers robustness and independence from inner solver accuracy
- Sufficient prior information enables accurate parameter recovery; limited priors yield data-consistent surrogates

## Why This Works (Mechanism)

### Mechanism 1: GP Linearity Preserves Convexity Structure
Parameterizing unknown functions with Gaussian processes preserves the convexity-concavity structure of potential MFG optimization problems, enabling standard convex solvers. Since GPs are linear models, they transform the infinite-dimensional optimization into a finite-dimensional convex-concave saddle-point problem. The inf-sup formulation exploits this by treating unknown parameters as concave terms in the objective, allowing primal-dual algorithms with global convergence guarantees.

### Mechanism 2: Bilevel Optimization with Data-Consistent Surrogate Recovery
Even when the inverse problem is ill-posed (multiple parameter sets explain the same data), the bilevel framework recovers surrogate MFGs that reproduce observed data, if not true parameters. The inner level enforces MFG dynamics exactly via convex optimization. The outer level minimizes data misfit plus regularization. This decoupling guarantees that every outer candidate produces a valid MFG solution, ensuring data consistency regardless of identifiability.

### Mechanism 3: Adjoint-Based Gradient Computation for Solver Independence
The adjoint method computes exact outer-level gradients analytically, avoiding dependence on inner-solver accuracy and improving robustness over automatic differentiation. By deriving the adjoint equations from first-order optimality conditions of the inner problem, gradients of the outer objective with respect to parameters are expressed without explicit dv/dθ computation. This avoids backpropagation through iterative solvers, which can accumulate numerical errors.

## Foundational Learning

- **Concept: Potential Mean Field Games as Convex Optimization**
  - Why needed: The entire methodology hinges on reformulating MFGs as linear PDE-constrained convex minimization. Without this structure, neither inf-sup nor bilevel GP approaches would have convergence guarantees.
  - Quick check: Can you explain why the Lasry-Lions monotonicity condition ensures convexity of the coupling term F?

- **Concept: Gaussian Process Regression and RKHS**
  - Why needed: GPs provide the non-parametric function approximation framework, with the representer theorem enabling finite-dimensional reduction. Understanding kernel choice and hyperparameter tuning is critical for model flexibility vs. overfitting.
  - Quick check: How does the Matérn kernel relate to Sobolev regularity, and why might you choose it over a periodic kernel for spatial cost V?

- **Concept: Primal-Dual Algorithms for Saddle-Point Problems**
  - Why needed: The Chambolle-Pock algorithm solves both forward MFGs and the inf-sup inverse problem efficiently. Understanding convergence criteria and step-size selection is essential for implementation.
  - Quick check: What role does the strong convexity of the data-fidelity term play in primal-dual convergence rates?

## Architecture Onboarding

- **Component map:** Data observations -> GP latent variable initialization -> Inner MFG solve (Chambolle-Pock) -> Outer loss evaluation -> Gradient computation (adjoint preferred) -> Parameter update -> Convergence check

- **Critical path:** Data ingestion → GP latent variable initialization → Inner MFG solve (fixed parameters) → Outer loss evaluation → Gradient computation (adjoint preferred) → Parameter update → Convergence check

- **Design tradeoffs:**
  - Inf-sup vs. Bilevel: Inf-sup is faster and globally convergent but limited to concave unknowns (e.g., V only). Bilevel handles all unknowns but requires careful regularization and may find surrogates rather than true parameters.
  - Autodiff vs. Adjoint: Autodiff is easier to implement (PyTorch integration) but accuracy depends on inner-solver iterations. Adjoint is more complex to derive but solver-independent.
  - One-step vs. Two-step GP: One-step uses all collocation points (higher dimension); two-step uses pseudo-points (lower dimension, scalable).

- **Failure signatures:**
  - Non-converging inner MFG solver → check discretization stability (CFL condition) and primal-dual step sizes
  - Recovered F non-convex → increase convexity penalty α_fp or reduce Monte Carlo noise
  - Gradient explosion → verify adjoint derivation; check kernel matrix conditioning (add jitter)
  - Surrogate fits data but parameters wildly incorrect → ill-posedness; add prior information or more observations

- **First 3 experiments:**
  1. Inf-sup recovery of V only: Reproduce Section 7.1 (first-order stationary MFG, known F/Λ/q/ν). Validate L2 error convergence with observation density.
  2. Bilevel recovery of V + F (power function): Reproduce Section 7.2.1. Test sensitivity to β initialization and noise level γ.
  3. Full bilevel with adjoint gradient: Reproduce Section 7.4 (non-Euclidean Λ, unknown q, ν, F, V). Compare autodiff vs. adjoint gradient accuracy and wall-clock time.

## Open Questions the Paper Calls Out
None

## Limitations
- Ill-posedness prevents recovery of true parameters when insufficient prior information is available
- Regularization parameter selection lacks systematic criteria and significantly impacts results
- Numerical stability depends critically on kernel matrix conditioning and inner solver convergence accuracy

## Confidence

- **High Confidence**: GP linearity preserving convexity structure; adjoint-based gradient computation independence from inner solver
- **Medium Confidence**: Bilevel framework recovering surrogate MFGs when true parameters are unidentifiable; effectiveness of Monte Carlo convexity penalty for F
- **Low Confidence**: Generalization to time-dependent and non-stationary MFGs; scalability to high-dimensional problems

## Next Checks

1. **Solver Dependency Test**: Compare adjoint vs. automatic differentiation gradient accuracy and convergence rates across varying inner solver iteration counts and noise levels
2. **Identifiability Analysis**: Systematically vary observation density and noise levels to quantify the trade-off between parameter recovery accuracy and data fit quality
3. **Regularization Sensitivity**: Perform grid search over regularization parameters to establish selection criteria and assess impact on recovered parameter distributions across multiple random seeds