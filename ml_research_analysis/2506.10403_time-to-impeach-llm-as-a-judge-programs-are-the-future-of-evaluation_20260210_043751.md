---
ver: rpa2
title: 'Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation'
arxiv_id: '2506.10403'
source_url: https://arxiv.org/abs/2506.10403
tags:
- programs
- response
- arxiv
- evaluation
- pajama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PAJAMA synthesizes executable judging programs from LLM-generated\
  \ logic, reducing API costs by ~3500x compared to LLM-as-a-judge while improving\
  \ consistency by 15.83% and reducing biased responses by 23.7%. When distilled into\
  \ a reward model, PAJAMA outperforms LLM-as-a-judge-distilled models on RewardBench\u2019\
  s CHAT-HARD subset (+2.19% on Prometheus, +8.67% on JudgeLM) and maintains accuracy\
  \ across 52 programs, improving accuracy from 59% (3 programs) to 82.2% without\
  \ performance plateau."
---

# Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation

## Quick Facts
- **arXiv ID**: 2506.10403
- **Source URL**: https://arxiv.org/abs/2506.10403
- **Reference count**: 40
- **Primary result**: PAJAMA achieves ~3500x cost reduction vs. LLM-as-a-judge while improving consistency by 15.83% and reducing biased responses by 23.7%

## Executive Summary
PAJAMA proposes replacing expensive LLM-as-a-judge evaluations with executable judging programs synthesized from LLM-generated logic. The approach dramatically reduces API costs by ~3500x while improving evaluation consistency and reducing bias. By distilling these programs into a reward model, PAJAMA achieves state-of-the-art performance on RewardBench's CHAT-HARD subset, outperforming LLM-as-a-judge-distilled models by +2.19% on Prometheus and +8.67% on JudgeLM. The method scales effectively, with 52 programs achieving 82.2% accuracy compared to 59% with only 3 programs.

## Method Summary
PAJAMA synthesizes executable judging programs from LLM-generated logic to evaluate pairwise LLM response preferences. The process involves prompting GPT-4o to generate 52 judging programs across 6 criteria (Structure, Relevance, Readability, Bias, Factuality, Safety), executing these programs to score response pairs, discretizing scores to binary preferences, and aggregating via weak supervision using Snorkel. The aggregated preferences are then distilled into a Gemma-2B-it reward model. This approach replaces expensive LLM-as-a-judge API calls with deterministic program execution while maintaining or improving evaluation quality.

## Key Results
- **Cost reduction**: ~3500x reduction in API costs through program execution
- **Consistency improvement**: 15.83% improvement in evaluation consistency
- **Bias reduction**: 23.7% reduction in biased response wins
- **RewardBench performance**: +2.19% Prometheus, +8.67% JudgeLM on CHAT-HARD subset
- **Scalability**: 52 programs achieve 82.2% accuracy vs 59% with 3 programs

## Why This Works (Mechanism)
PAJAMA works by replacing the stochastic reasoning of LLM-as-a-judge with deterministic program execution. LLM-as-a-judge suffers from high variance, inconsistency, and API costs due to its black-box nature. PAJAMA addresses these by synthesizing interpretable programs that encode specific evaluation criteria, which can be executed cheaply and repeatedly with identical results. The weak supervision aggregation step learns to combine multiple imperfect program judgments into a more reliable consensus, while distillation into a reward model enables efficient inference without program execution overhead.

## Foundational Learning

**Program Synthesis for Evaluation** - Converting natural language evaluation criteria into executable code. *Why needed*: Enables deterministic, interpretable evaluation instead of stochastic LLM reasoning. *Quick check*: Can you write a simple program that scores response readability using textstat?

**Weak Supervision with Snorkel** - Using multiple imperfect labeling functions to generate high-quality training data without manual annotation. *Why needed*: Combines diverse program judgments into consensus preferences while learning program reliability. *Quick check*: Can you aggregate 3 program outputs using Snorkel's label model?

**Reward Model Distillation** - Fine-tuning a small model on synthetic preference data to replace expensive program execution. *Why needed*: Enables efficient inference while preserving program-based evaluation quality. *Quick check*: Can you fine-tune Gemma-2B-it on preference pairs using cross-entropy loss?

**Multi-Criteria Evaluation** - Decomposing judgment into independent criteria (structure, relevance, readability, etc.). *Why needed*: Allows targeted evaluation and modular program synthesis. *Quick check*: Can you define 3 separate judging criteria for response evaluation?

## Architecture Onboarding

**Component Map**: LLM Code Generator -> 52 Judging Programs -> Program Execution Engine -> Score Discretizer -> Snorkel Aggregator -> Distilled Reward Model

**Critical Path**: Query + Response Pair → Program Execution → Aggregation → Reward Model Inference → Preference Output

**Design Tradeoffs**: Programs offer determinism and cost savings but may lack nuance of LLM reasoning; weak supervision trades manual annotation for program diversity; distillation trades program flexibility for inference efficiency.

**Failure Signatures**: Programs fail to execute (missing dependencies); aggregation produces low-confidence labels (program diversity issues); distilled model underperforms on specific subsets (safety bias).

**First Experiments**:
1. Implement and test 6 judging programs (one per criterion) on sample inputs
2. Execute programs on small validation set and aggregate with Snorkel
3. Fine-tune Gemma-2B-it on aggregated labels and evaluate on CHAT subset

## Open Questions the Paper Calls Out

**Open Question 1**: Can improvements in LLM code generation capacity push PAJAMA's accuracy past the performance plateau observed in standard LLM-as-a-judge approaches? The paper notes that improving LLMs' capacity to generate more precise and comprehensive judging code has potential to push PAJAMA beyond current LLM-as-a-judge performance, but this upper bound remains untested.

**Open Question 2**: How can synthesized judging programs be adapted to effectively evaluate subjective domains like safety and emotional alignment, where they currently underperform? The authors report a performance drop on safety subset, stating synthesized rules may struggle to generalize to emotional or sentimental policies, suggesting current discrete logic is insufficient.

**Open Question 3**: Do sophisticated program synthesis techniques yield significantly more reliable judging logic than the simple prompting strategy currently employed? The authors mention that more sophisticated approaches to program synthesis can be seamlessly swapped in, but it's unclear if the simplicity of the prompt is a feature or limitation.

## Limitations

- Program synthesis quality depends on the generator's capability and prompt quality, which are not fully specified
- Safety and subjective evaluation domains remain challenging for programmatic approaches, with 2.7% performance drop noted
- Implementation fidelity affected by unspecified third-party models and Snorkel configuration details
- Weak supervision aggregation introduces complexity and potential for low-confidence labels

## Confidence

- **High confidence**: Cost reduction claims (~3500x) and consistency/bias improvements are directly measurable
- **Medium confidence**: RewardBench performance improvements depend on distillation hyperparameters
- **Medium confidence**: Scalability claims rely on weak supervision aggregation details

## Next Checks

1. **Program Synthesis Verification**: Implement the provided prompt template to generate at least 6 programs (one per criterion) and test their execution on sample inputs. Verify that programs produce reasonable scores in [0,1] and that discretization works as intended.

2. **Weak Supervision Validation**: Execute the 6 programs on a small validation set with known preferences. Use Snorkel to aggregate labels and verify that the label model produces higher-confidence labels than individual programs, checking if reliability weights θi make intuitive sense.

3. **Distillation Baseline Comparison**: Fine-tune Gemma-2B-it on aggregated labels from step 2. Evaluate on RewardBench's CHAT subset first (simpler than CHAT-HARD) and compare against LLM-as-a-judge baselines to verify the distillation process works before scaling to 52 programs.