---
ver: rpa2
title: 'MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text
  Categorization'
arxiv_id: '2601.13437'
source_url: https://arxiv.org/abs/2601.13437
tags:
- classes
- language
- learning
- test
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first multilingual open-set learning
  and discovery (OSLD) benchmark for text categorization by topic, comprising 960K
  data samples across 12 languages. The benchmark is constructed by rearranging existing
  datasets and collecting new data from the news domain, with 4 known classes and
  6-10 unknown classes introduced progressively across three test sets per language.
---

# MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization

## Quick Facts
- **arXiv ID:** 2601.13437
- **Source URL:** https://arxiv.org/abs/2601.13437
- **Reference count:** 21
- **Primary result:** 960K samples across 12 languages, 4 known + 6-10 unknown classes per language, OSLD framework achieves 90.6-90.2% accuracy on Arabic T1.

## Executive Summary
This paper introduces the first multilingual open-set learning and discovery (OSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. The benchmark is constructed by rearranging existing datasets and collecting new data from the news domain, with 4 known classes and 6-10 unknown classes introduced progressively across three test sets per language. The proposed OSLD framework integrates energy-based outlier detection, k-means clustering on [CLS] embeddings, TFIDF-based keyword extraction for class naming, and model retraining with pseudo-labeling. Two variants are evaluated: V1 uses standard cross-entropy, while V2 adds a contrastive loss term. Experimental results show the baseline BERT model achieves 51.7% accuracy on Arabic, dropping to 37.0% on later test sets, while the OSLD methods achieve 90.6-90.2% accuracy on Arabic T1, demonstrating effective discovery of new classes.

## Method Summary
The method fine-tunes language-specific BERT on 4 known classes, then uses energy-based outlier detection (top 15% high-energy samples) to identify potential unknown class samples. These outliers are clustered using k-means on [CLS] embeddings, with k selected via silhouette coefficient maximization. TFIDF keyword extraction identifies representative terms for each cluster, which are encoded to form centroids. The 40% of samples closest to each centroid are selected for pseudo-labeling. Retraining uses either V1 (cross-entropy) or V2 (cross-entropy + contrastive loss with λ=0.3). The process is applied progressively across three test sets (T1, T2, T3) with increasing numbers of unknown classes.

## Key Results
- Baseline BERT achieves 51.7% accuracy on Arabic, dropping to 37.0% on later test sets
- OSLD methods achieve 90.6-90.2% accuracy on Arabic T1 for unknown class discovery
- Performance on unknown classes reaches 84.5% accuracy on Arabic T1 for V1
- Progressive performance degradation from T1 to T3 observed across all languages
- GPT-4o shows data leakage tendencies, proposing nearly full label sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy-based scores enable detection of samples from unknown classes by exploiting prediction confidence differences.
- Mechanism: The energy function E(x) = -log(Σ exp(h_j(x))) aggregates classifier logits; samples from known classes produce lower energy due to concentrated probability mass on familiar categories, while outliers yield higher energy from distributed, uncertain predictions. The top 15% highest-energy samples are flagged as outliers.
- Core assumption: Known class samples induce higher confidence predictions in the frozen BERT classifier than unknown class samples.
- Evidence anchors:
  - [section] Page 3, Eq. (1) defines energy score and states: "samples from known classes yield lower energy scores due to higher confidence predictions, while outliers produce higher energy values."
  - [corpus] Liu et al. (2020) "Energy-based out-of-distribution detection" (NeurIPS) provides theoretical grounding for energy-based OOD detection.

### Mechanism 2
- Claim: K-means clustering on [CLS] embeddings discovers latent class structure among outlier samples.
- Mechanism: BERT's [CLS] token produces semantically meaningful document representations. K-means partitions outlier embeddings into k clusters, where k is selected via silhouette coefficient maximization. Each cluster corresponds to a discovered class.
- Core assumption: [CLS] embeddings from the same semantic category form compact neighborhoods in embedding space.
- Evidence anchors:
  - [section] Page 3: "we cluster outliers using k-means on [CLS] embeddings provided by the language model. The optimal k is determined by maximizing the silhouette coefficient."
  - [corpus] Related work on open-set recognition (Geng et al., 2021) leverages feature space clustering for novelty detection.

### Mechanism 3
- Claim: Contrastive loss grounds representations in cluster centroids, improving class discrimination.
- Mechanism: V2 adds L_CL = -1/|U| Σ log(exp(sim(e_i, ĉ_yi)/τ) / Σ exp(sim(e_i, c_j)/τ)), pulling sample embeddings toward their assigned cluster centroid while pushing away from others. This reinforces semantic coherence discovered through clustering.
- Core assumption: Cluster centroids (derived from TFIDF keyword embeddings) capture class-representative semantics.
- Evidence anchors:
  - [section] Page 4, Eq. (2-3): contrastive term "grounds representations in semantic information from clustering."
  - [abstract] Results show V2 provides improvements "particularly in early stages (T1 and T2)" for several languages.

## Foundational Learning

- Concept: **Energy-based out-of-distribution detection**
  - Why needed here: Core mechanism for separating known from unknown samples without labeled outlier data.
  - Quick check question: Can you explain why lower energy correlates with higher prediction confidence?

- Concept: **Pseudo-labeling with confidence filtering**
  - Why needed here: Enables semi-supervised learning on discovered clusters by selecting high-confidence samples (40% closest to centroids) for retraining.
  - Quick check question: Why might using all samples for retraining harm performance?

- Concept: **Silhouette coefficient for cluster validation**
  - Why needed here: Automatically determines the number of discovered classes without ground-truth labels.
  - Quick check question: What are failure modes of silhouette-based k selection on high-dimensional text embeddings?

## Architecture Onboarding

- Component map:
  Frozen BERT classifier → Energy scorer → [CLS] embedding extractor → K-means clustering → TFIDF keyword extractor → Centroid construction → Sample filtering → Retraining loop

- Critical path: Energy detection → clustering → centroid construction → sample selection → model update. Errors propagate downstream; poor outlier detection contaminates clustering.

- Design tradeoffs:
  - 15% outlier threshold: higher values capture more unknowns but increase false positives from known classes
  - 40% sample retention: balances noise filtering vs. training data quantity
  - λ=0.3 contrastive weight: empirical choice; higher values may over-constrain representations

- Failure signatures:
  - Unknown class accuracy drops sharply from T1 to T3 (observed in results) → cumulative discovery difficulty
  - Romanian/Turkish near-zero unknown accuracy → language-specific embedding quality issues
  - GPT-4o proposing near-complete label sets → data leakage, not true discovery

- First 3 experiments:
  1. **Ablate energy threshold**: Test 10%, 15%, 25% on Arabic T1 to characterize precision-recall tradeoff.
  2. **Vary sample retention**: Compare 20%, 40%, 60% pseudo-label filtering on French (high-performing language).
  3. **mBERT vs. language-specific BERT**: Replicate authors' preliminary finding that mBERT underperforms; quantify gap on 3 languages.

## Open Questions the Paper Calls Out
- What methods beyond the proposed V1/V2 framework can improve OSLD performance, particularly for later test stages (T2, T3)?
- How can large language models be fairly evaluated for OSLD when pre-training data may overlap with benchmark corpora?
- What causes the progressive performance degradation from T1 to T3, and can error accumulation in clustering/pseudo-labeling be mitigated?
- How sensitive are OSLD results to the fixed hyperparameter choices (15% outlier threshold, 40% sample selection for retraining)?

## Limitations
- Limited language diversity in foundational datasets, relying heavily on DBpedia and Wikipedia-derived sources
- Uncontrolled language-specific confounding factors affecting model performance variations
- Questionable realism of "open-set" framing with controlled unknown class discovery
- Sensitivity to hyperparameter choices without systematic sensitivity analysis

## Confidence
- **High confidence (8/10)**: Benchmark construction methodology, energy-based outlier detection, overall framework architecture
- **Medium confidence (6/10)**: Reported performance improvements, superiority over GPT-4o, contrastive loss benefits
- **Low confidence (3/10)**: Cross-lingual generalization claims, Hungarian matching evaluation assumptions

## Next Checks
- **Validation 1: Outlier detection robustness analysis**: Systematically vary the 15% outlier threshold across all languages and measure the tradeoff between known-class contamination and unknown-class discovery.
- **Validation 2: Language-specific confounding factors**: Re-run the complete pipeline using a single multilingual model (mBERT or XLM-R) across all 12 languages to isolate OSLD methodology effects.
- **Validation 3: Real-world semantic similarity stress test**: Construct synthetic test sets where unknown classes are deliberately chosen to be semantically close to known classes to measure performance degradation.