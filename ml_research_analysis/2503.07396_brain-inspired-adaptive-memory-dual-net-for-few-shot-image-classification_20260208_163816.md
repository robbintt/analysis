---
ver: rpa2
title: Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image Classification
arxiv_id: '2503.07396'
source_url: https://arxiv.org/abs/2503.07396
tags:
- memory
- few-shot
- learning
- systems
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SCAM-Net, a brain-inspired adaptive memory
  dual-network for few-shot image classification. The model addresses the challenge
  of supervision collapse in few-shot learning by simulating the human brain's complementary
  learning system.
---

# Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image Classification

## Quick Facts
- arXiv ID: 2503.07396
- Source URL: https://arxiv.org/abs/2503.07396
- Reference count: 40
- Primary result: State-of-the-art performance on four few-shot image classification benchmarks (miniImageNet, tieredImageNet, CIFAR-FS, FC100) using brain-inspired dual-network architecture

## Executive Summary
This paper proposes SCAM-Net, a brain-inspired adaptive memory dual-network architecture for few-shot image classification. The model addresses supervision collapse by simulating the human brain's complementary learning system through a Hippocampus-Neocortex dual network that consolidates structured information and an adaptive memory module that regulates representations based on prior knowledge. The approach successfully addresses the challenge of identifying semantically relevant features in few-shot scenarios while minimizing support set feature bias, achieving state-of-the-art performance on four benchmark datasets.

## Method Summary
SCAM-Net uses a ViT-S/16 backbone pretrained with Masked Image Modeling on training classes. The dual-network architecture consists of a Hippocampus model (updated via gradient descent) and a Neocortex model (updated via exponential moving average of Hippocampus weights). The model computes patch-level cosine similarities between support and query images, aggregates these similarities per class, and uses an adaptive memory dictionary to store and regulate class representations through averaged CLS tokens. Training involves three losses: classification loss, consistency loss between networks, and CLS token regulation loss.

## Key Results
- Achieves 75.93% accuracy on 1-shot miniImageNet and 89.75% on 5-shot miniImageNet
- State-of-the-art performance across miniImageNet, tieredImageNet, CIFAR-FS, and FC100 datasets
- Effectively addresses supervision collapse by identifying semantically relevant features
- Minimizes support set feature bias through adaptive memory regulation

## Why This Works (Mechanism)

### Mechanism 1: Systems Consolidation via Dual-Network EMA Transfer
The Hippocampus model learns rapidly from episode losses while the Neocortex model consolidates knowledge through exponential moving average updates (θN = αθN + (1-α)θH), with consistency loss constraining Hippocampus deviation. This separation preserves structured knowledge while allowing rapid task-specific learning.

### Mechanism 2: Adaptive Memory Regulation via CLS Token Averaging
The Neocortex maintains a memory dictionary mapping class labels to averaged CLS tokens. When classes appear, their CLS tokens are averaged with existing entries, creating more reliable class prototypes that reduce intra-class variability and feature bias.

### Mechanism 3: Dense Patch-Level Similarity Aggregation
Images are divided into patches via ViT, and pairwise cosine similarity is computed between all support-query patch pairs. Block-diagonal masking prevents self-matching, and LogSumExp aggregation with temperature scaling produces class logits, locating semantically relevant regions without explicit spatial alignment.

## Foundational Learning

- **Complementary Learning Systems (neuroscience)**: The dual-network architecture rests on this biological theory—hippocampus for rapid episodic learning, neocortex for slow semantic consolidation. Quick check: Why might two learning systems with different timescales outperform a single system with intermediate learning rate?

- **Vision Transformers and Patch Tokenization**: SCAM-Net uses ViT-S/16 backbone; understanding patch embeddings and [CLS] tokens is essential for similarity computation and memory mechanisms. Quick check: How does a ViT convert a 224×224 image into a sequence of tokens, and what semantic role does the [CLS] token play?

- **Exponential Moving Average (EMA) for Model Averaging**: Neocortex consolidation uses EMA; understanding how α controls stability-plasticity tradeoff is critical for tuning. Quick check: With α=0.99, what effective time constant does the Neocortex have for incorporating Hippocampus changes?

## Architecture Onboarding

- **Component map**: Input → Patch embedding + [CLS] token → Parallel Hippocampus/Neocortex branches → ViT encoder → patch similarity matrix → LogSumExp aggregation → class logits → Neocortex memory regulation → auxiliary CLS loss → combined loss

- **Critical path**: Self-supervised MIM pretraining → episode sampling → dual forward pass with similarity computation → memory lookup and CLS regulation → combined loss → Hippocampus gradient step → Neocortex EMA update → inference with Neocortex only

- **Design tradeoffs**: Dual network reduces training throughput (~4.18→2.68 it/s) but maintains inference speed; λ=0.2 balances global vs patch-level features; α=0.99 controls consolidation speed vs stability

- **Failure signatures**: Supervision collapse (visualize attention/similarity matrices); memory corruption (accuracy degrades across episodes); high consistency loss (Hippocampus diverging—reduce LR or increase α)

- **First 3 experiments**: 1) Ablate systems consolidation (single vs dual network) to verify ~5% gain; 2) Replicate Figure 4 memory regulation visualization—confirm CLS token clustering; 3) Sweep α∈[0.95, 0.99, 0.999] and λ∈[0.1, 0.2, 0.4] on validation

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several important unresolved issues emerge from the methodology and results. The approach's scalability to thousands of classes, performance on cross-domain few-shot tasks, potential improvements to the simple averaging memory mechanism, and comparison to single-network alternatives with consistency regularization remain unexplored areas requiring further investigation.

## Limitations

- **Training efficiency**: Dual-network design significantly reduces training speed (4.18→2.68 it/s) despite unchanged inference speed
- **Memory accumulation**: No analysis of memory growth or retrieval efficiency with large class counts, raising scalability concerns
- **Hyperparameter sensitivity**: EMA decay α and memory regulation parameters may require dataset-specific tuning for optimal performance

## Confidence

- **High Confidence**: Empirical performance claims are well-supported by experimental results on four benchmark datasets
- **Medium Confidence**: Biological plausibility of complementary learning systems framework relies on assumptions about representation consolidation not extensively validated
- **Medium Confidence**: Adaptive memory regulation effectiveness depends on class prototype stability over training, which isn't thoroughly examined for long-term scenarios

## Next Checks

1. **EMA Sensitivity Analysis**: Systematically vary α (0.95-0.999) and measure impact on performance and training stability across all four benchmark datasets
2. **Memory Corruption Test**: Run extended training with mixed task distributions and track accuracy degradation to quantify memory accumulation effects
3. **Mechanism Ablation Study**: Isolate contributions by testing single vs dual network, with/without adaptive memory regulation, and with/without patch-level similarity aggregation