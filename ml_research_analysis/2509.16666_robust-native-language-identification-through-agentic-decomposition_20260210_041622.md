---
ver: rpa2
title: Robust Native Language Identification through Agentic Decomposition
arxiv_id: '2509.16666'
source_url: https://arxiv.org/abs/2509.16666
tags:
- language
- text
- agentic
- linguistic
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of superficial bias in native language
  identification (NLI) by large language models (LLMs), which often rely on contextual
  clues like names and stereotypes rather than linguistic patterns. To solve this,
  the authors propose an agentic NLI pipeline inspired by forensic linguistics, where
  specialized agents independently analyze distinct linguistic features (syntax, lexical,
  idiomatic) before a coordinator synthesizes the evidence to make the final prediction.
---

# Robust Native Language Identification through Agentic Decomposition

## Quick Facts
- arXiv ID: 2509.16666
- Source URL: https://arxiv.org/abs/2509.16666
- Reference count: 26
- Primary result: Agentic decomposition improves robustness to contextual bias in NLI at the cost of peak accuracy

## Executive Summary
This paper addresses the problem of superficial bias in native language identification (NLI) by large language models (LLMs), which often rely on contextual clues like names and stereotypes rather than linguistic patterns. The authors propose an agentic NLI pipeline inspired by forensic linguistics, where specialized agents independently analyze distinct linguistic features (syntax, lexical, idiomatic) before a coordinator synthesizes the evidence to make the final prediction. This structured approach forces decisions to be grounded in linguistic evidence rather than superficial hints.

## Method Summary
The agentic pipeline decomposes NLI into specialized expert agents (Syntax, Lexical, Idiomatic) that analyze text independently and generate structured JSON reports of linguistic evidence. A coordinator agent synthesizes these expert analyses to make the final L1 prediction, never accessing the original text directly. This creates an information bottleneck that prevents shortcut circuits from activating on superficial features. The approach is compared against direct prompting and NER-redacted prompting on TOEFL4 and Write & Improve 2024 corpora, testing robustness across five conditions: no modification, supportive signature, supportive stereotype, misleading signature, and misleading stereotype.

## Key Results
- On TOEFL4, agentic approach achieved 73.7% macro-F1 vs. 96.5% for direct prompting, but with much lower volatility (2.5 vs 29.1 std dev)
- Misleading signatures/stereotypes reduced direct prompting accuracy to near-chance levels (13.6-24.6 F1), while agentic approach maintained stability
- Single-expert ablations show idiomatic expert alone achieves 70.9 F1, suggesting possible overlap with syntactic features
- Computational cost is 4x higher (4 LLM calls vs. 1) but provides significantly improved robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining the decision-maker's access to raw input forces reliance on extracted linguistic evidence.
- Mechanism: The coordinator agent receives only abstracted expert analyses, not the original text. This information bottleneck prevents shortcut circuits from activating on salient surface features (names, stereotypes) that would otherwise bias the decision.
- Core assumption: Expert agents extract linguistically relevant features that are sufficient for accurate classification; the bottleneck does not discard critical signal.
- Evidence anchors:
  - [abstract] "specialized agents accumulate and categorize diverse linguistic evidence before an independent final overall assessment"
  - [section 4.3.2] "Crucially, this expert does not have direct access to the original input text... This constraint ensures that the NLI decision is based only on the categorized linguistic features"
  - [corpus] No direct corpus evidence for this bottleneck mechanism in other NLI work; related decomposition papers focus on interpretability, not robustness.

### Mechanism 2
- Claim: Narrow expert scopes reduce interference from irrelevant contextual features during evidence extraction.
- Mechanism: Each expert is instructed to focus exclusively on one linguistic category (syntax, lexical, or idiomatic). This role specialization limits attention to a defined feature set, reducing the chance that salient cultural references contaminate the analysis.
- Core assumption: Experts follow their role constraints faithfully; LLMs do not "leak" attention to prohibited features within a single prompt.
- Evidence anchors:
  - [section 4.3.2] "The resulting analyses are compiled into a report, which is processed by a final expert tasked with synthesizing information"
  - [section 2.3] "task-agnostic (i.e., unaware of the final NLI goal) and shielded from potentially misleading global contextual clues"
  - [corpus] Weak evidence; neighbor papers on decomposition (e.g., JEDI, atomic hypothesis decomposition) address interpretability, not adversarial robustness.

### Mechanism 3
- Claim: Decoupling evidence extraction from classification reduces confirmation bias in the synthesis step.
- Mechanism: Inspired by forensic practice, preliminary analysis withholds final judgment. Evidence is accumulated and categorized before any L1 hypothesis is formed. The coordinator only sees categorized evidence, not raw text, preventing premature anchoring on stereotypical cues.
- Core assumption: The coordinator's synthesis is not itself biased by prior expectations about likely L1-labels given certain feature patterns.
- Evidence anchors:
  - [section 2.3] "Forensic linguists often deliberately withhold ultimate judgment during preliminary analysis... avoiding observer bias"
  - [section 4.3.2] "goal-aware coordinating agent synthesizes all evidence to make the NLI prediction"
  - [corpus] No corpus evidence directly validates this forensic-inspired decoupling for LLMs.

## Foundational Learning

- Concept: Cross-linguistic influence (L1 transfer)
  - Why needed here: NLI is grounded in the theory that an author's native language leaves distinctive traces in L2 production (syntax, lexical, error patterns).
  - Quick check question: Can you name three types of L1-induced errors (e.g., false cognates, word order, article omission)?

- Concept: Shortcut learning in LLMs
  - Why needed here: Models exploit superficial correlations (names, cultural markers) instead of task-relevant features, leading to brittle performance under distribution shift.
  - Quick check question: What happens to Llama-3.3-70B accuracy when a misleading Spanish signature is added to an Italian L1 text (Table 1)?

- Concept: Self-consistency vs. faithfulness
  - Why needed here: The paper measures whether model behavior aligns with its explicit instructions across conditions, not whether explanations reflect internal reasoning.
  - Quick check question: If a model ignores its instruction to "disregard names," what type of failure does the paper call this?

## Architecture Onboarding

- Component map: Input text → Syntax Expert → Lexical Expert → Idiomatic Expert → JSON reports → Coordinator Agent → L1 prediction

- Critical path: Input text → parallel expert prompts → JSON error arrays → coordinator prompt (with expert outputs) → L1 label

- Design tradeoffs:
  - Peak accuracy vs. robustness: Agentic approach lowers best-case F1 (73.7 vs. 96.5 on TOEFL4) but reduces volatility (2.5 vs. 33.5 std dev across conditions)
  - Compute cost: 4 LLM calls per input (3 experts + coordinator) vs. 1 call for direct prompting
  - Information loss: Clean texts yield sparse expert outputs, making final classification harder

- Failure signatures:
  - Near-chance accuracy with misleading signatures under direct prompting (13.6–24.6 F1 across models)
  - Hallucinated linguistic rules in explanations when models rationalize shortcut-driven predictions
  - Coordinator confusion between related languages (Italian frequently misidentified as Spanish/French)

- First 3 experiments:
  1. Replicate the misleading signature condition on TOEFL4 subset to confirm volatility in direct prompting baseline.
  2. Run single-expert ablations (syntax-only, lexical-only, idiomatic-only) to verify that each contributes signal and that full combination is best.
  3. Test on W&I corpus with stereotype hints to validate that agentic approach maintains low volatility across datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the agentic decomposition pipeline maintain robustness against adversarial attacks that actively mimic specific L1 linguistic features rather than just superficial contextual clues?
- Basis in paper: [explicit] The Limitations section explicitly identifies "sophisticated adversarial attacks" involving "linguistic impersonation" as a critical area for future work, distinguishing it from the superficial hints tested in this study.
- Why unresolved: The current study only evaluates robustness against "salient, content-based features" like signatures and stereotypes; it does not test scenarios where the linguistic features themselves are fabricated to mislead the model.
- What evidence would resolve it: Evaluation of the pipeline on a dataset of synthetic or human-written adversarial examples where non-native authors intentionally insert grammatical errors or idioms characteristic of a different L1.

### Open Question 2
- Question: Does equipping specialized agents with tools to query external linguistic knowledge bases significantly reduce hallucination rates in the final evidence synthesis?
- Basis in paper: [explicit] The Discussion section proposes that "agents could be equipped with tools to consult external knowledge bases" to verify evidence rather than relying solely on internal knowledge.
- Why unresolved: The results showed that baseline models often produced hallucinated linguistic rules; while the agentic approach reduced this, it relies on the model's internal parametric knowledge which can still be unreliable.
- What evidence would resolve it: A comparative experiment measuring the factual accuracy of the reasoning steps (evidence) and the final F1 scores between the current pipeline and a modified pipeline with retrieval-augmented generation (RAG) capabilities.

### Open Question 3
- Question: Can expert prompts be refined to ensure stricter disentanglement of linguistic features (syntax, lexical, idiomatic), or is there a necessary overlap for optimal performance?
- Basis in paper: [inferred] The ablation study (Table 2) showed the Idiomatic expert alone performed nearly as well as the full system, leading the authors to suspect "refinements to these expert prompts could result in stricter disentanglement."
- Why unresolved: The current prompts allow for overlap (e.g., the Idiomatic expert identifying word-order irregularities), making it unclear if the performance gain is due to complementary evidence or the Idiomatic agent acting as a proxy for other experts.
- What evidence would resolve it: An analysis of feature overlap between agents using stricter, mutually exclusive prompts, coupled with performance metrics to see if "disentanglement" improves or harms the final synthesis.

### Open Question 4
- Question: How can the agentic pipeline be adapted to maintain accuracy on "clean" texts that exhibit few linguistic irregularities or errors?
- Basis in paper: [inferred] The Results section notes that the agentic approach suffers a performance drop because "in cases where texts contain few irregularities, only limited informative fragments can be provided to the decision maker."
- Why unresolved: The current pipeline depends heavily on negative evidence (errors); it is unresolved how a "feature-first" approach can leverage positive evidence (native-like fluency) or stylistic nuances when explicit errors are absent.
- What evidence would resolve it: Testing the pipeline on high-proficiency learner corpora (e.g., CEFR C1/C2 levels) and analyzing the failure modes to determine if the lack of "errors" equates to a lack of "signal."

## Limitations

- Peak accuracy trade-off: Agentic approach achieves significantly lower best-case performance (73.7% vs 96.5% F1) compared to direct prompting
- Computational overhead: Requires 4x more LLM calls per input, increasing cost and latency
- Forensic framework generalization: No empirical validation that forensic linguistics principles transfer to LLM-based NLI across different language pairs

## Confidence

**High Confidence**: 
- Volatility reduction is well-supported (2.5 vs 29.1 std dev)
- Misleading cues significantly degrade direct prompting
- Combined expert approach outperforms single experts

**Medium Confidence**:
- Bottleneck mechanism prevents shortcut circuits
- Narrow expert scopes limit attention to prohibited features
- Forensic decoupling reduces confirmation bias

**Low Confidence**:
- Approach generalizability to non-Romance languages
- Effectiveness across different proficiency levels
- Scalability to larger-scale NLI applications

## Next Checks

1. Cross-linguistic generalization test: Apply the agentic pipeline to a dataset containing non-Romance language pairs (e.g., Chinese/Japanese/Korean) to verify that the decomposition approach generalizes beyond the specific L1s tested.

2. Proficiency level robustness: Test the approach on TOEFL11's full range of proficiency levels (from beginner to advanced) to determine whether the agentic pipeline maintains its robustness advantage across different writing quality levels.

3. Computational efficiency analysis: Conduct a detailed analysis comparing the computational cost (API calls, latency, tokens processed) of the agentic approach versus direct prompting, and model how this scales with dataset size to assess real-world feasibility.