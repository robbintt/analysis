---
ver: rpa2
title: 'LREF: A Novel LLM-based Relevance Framework for E-commerce'
arxiv_id: '2503.09223'
source_url: https://arxiv.org/abs/2503.09223
tags:
- relevance
- data
- search
- arxiv
- lref
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LREF, an LLM-based framework for e-commerce
  query-product relevance prediction. The framework addresses challenges in applying
  LLMs to relevance tasks, including data quality demands, reasoning optimization,
  and optimistic bias.
---

# LREF: A Novel LLM-based Relevance Framework for E-commerce

## Quick Facts
- arXiv ID: 2503.09223
- Source URL: https://arxiv.org/abs/2503.09223
- Authors: Tian Tang; Zhixing Tian; Zhenyu Zhu; Chenyang Wang; Haiqing Hu; Guoyu Tang; Lin Liu; Sulong Xu
- Reference count: 38
- Primary result: Novel LLM-based framework achieving 66.91% weighted F1 on e-commerce query-product relevance vs 64.78% for DeBERTa

## Executive Summary
This paper presents LREF, a three-stage LLM-based framework for e-commerce query-product relevance prediction that addresses key challenges in applying LLMs to this task. The framework employs targeted data selection to identify high-quality challenging samples, Multi-CoT tuning to guide domain-specific reasoning, and DPO de-biasing to mitigate over-recall issues. Evaluated on large-scale datasets and deployed via online A/B testing, LREF significantly outperforms BERT-like baselines with improvements in user conversion rate (+0.209%), click-through rate (+0.120%), and relevance satisfaction (+1.016%).

## Method Summary
LREF addresses LLM-based relevance prediction through a three-stage approach: (1) Data Selection using Challenge Identifier, Initial Model, and Mislabeled Supervisor to filter 5M raw samples down to ~0.5M high-quality challenging samples; (2) Multi-CoT Tuning with three sequential reasoning types - Expert Explaining (multi-perspective analysis), Rule Adherence (business rules), and Decision Reflection (error correction); (3) DPO De-biasing to reduce optimistic bias by constructing preference pairs from beam search errors. The framework uses LLaMA-2-7B as base model, fine-tuned with LM loss (epochs=8, lr=2e-5), then trained with DPO loss (epochs=2, α=0.65), and finally distilled to BERT for online serving to reduce latency.

## Key Results
- Weighted F1 score of 66.91% vs 64.78% for DeBERTa and 59.63% for LLM Base
- Online improvements: UCVR (+0.209%), UCTR (+0.120%), Relevance Satisfaction (+1.016%)
- Data selection stage improved WF1 from 59.63% to 65.07%
- DPO de-biasing reduced over-recall: Marginal recall improved from 50.13% to 64.12%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted data selection outperforms full-dataset fine-tuning for LLM-based relevance tasks.
- **Mechanism:** Three-stage filtering: (1) Challenge Identifier selects diverse query-product pairs across head/tail distribution; (2) Initial Model filters out easy samples it already predicts correctly; (3) Mislabeled Supervisor uses GPT-generated confounding labels to remove noisy annotations. Final dataset ~10% of original (0.5M from 5M samples).
- **Core assumption:** LLMs possess sufficient pre-trained text-matching capability that additional "easy" examples add noise rather than signal; challenge samples at decision boundaries provide higher learning value.
- **Evidence anchors:** [abstract] "high demand for data quality" as primary challenge; [Section 2.2] Equations 1-4 define selection logic; Table 3 shows progressive improvement: Full Data (59.63% WF1) → +IM (59.89%) → +CI (63.91%) → +MS (65.07%).
- **Break condition:** If your domain has high label noise or limited annotation budget, the Mislabeled Supervisor may filter legitimate edge cases; ablation shows MS contributes ~1.2% WF1 gain, so skipping it degrades but doesn't collapse performance.

### Mechanism 2
- **Claim:** Multi-CoT tuning improves reasoning by decomposing relevance into domain-specific analytical steps.
- **Mechanism:** Three CoT types trained sequentially: (1) EE-CoT generates multi-perspective analysis (brand, category, attributes) using GPT-labeled explanations; (2) RA-CoT applies business rules distinguishing product-level vs. modifier-level relevance; (3) DR-CoT trains on model's own incorrect predictions to learn error correction paths.
- **Core assumption:** Relevance judgment benefits from explicit reasoning traces; LLMs can internalize domain rules through structured chain supervision rather than end-to-end label prediction alone.
- **Evidence anchors:** [abstract] "Multi-CoT tuning to guide task-specific reasoning"; [Section 2.3] Describes all three CoT variants; Table 4 shows cumulative gains: w/o CoT (65.07%) → +EE (64.58%—slight drop) → +RA (65.27%) → +DR (66.03%).
- **Break condition:** EE-CoT alone degraded WF1 from 65.07% to 64.58%; rule-based RA-CoT appears necessary to constrain over-explaining. Deploy EE only if you have robust business rules to add immediately after.

### Mechanism 3
- **Claim:** DPO de-biasing reduces LLMs' optimistic tendency to over-classify marginal cases as significant.
- **Mechanism:** Identify samples where model predicts incorrectly but correct label appears in top-k beam search (80% of errors have correct answer at rank 2). Construct preference pairs: rejected = incorrect prediction, chosen = correct beam result. Train with DPO loss to shift preference toward cautious judgments.
- **Core assumption:** Optimistic bias stems from RLHF-style value alignment; this can be counteracted by explicit preference learning on ambiguous cases without full reward model training.
- **Evidence anchors:** [Section 2.4] "70% were misclassified as significant"; Table 5 shows Marginal recall improves from 50.13% (LLM Base) to 64.12% (LREF), Significant precision from 61.39% to 66.91%.
- **Break condition:** If your domain has legitimate high-recall requirements (e.g., new product discovery), DPO may over-suppress relevant-but-weak matches; monitor Marginal-class recall carefully post-deployment.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** LREF's third stage uses DPO for bias correction; requires understanding how DPO replaces RLHF's reward model with implicit preference learning through binary classification loss on chosen/rejected pairs.
  - **Quick check question:** Can you explain why DPO avoids training a separate reward model while still achieving preference alignment?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** Multi-CoT is central to LREF's stage 2; requires understanding how intermediate reasoning steps can be supervised via training data augmentation (not just inference-time prompting).
  - **Quick check question:** What is the difference between CoT prompting at inference vs. CoT tuning during training?

- **Concept: Data Selection / Curriculum Learning**
  - **Why needed here:** Stage 1 assumes challenge samples provide more learning signal than easy samples; requires understanding uncertainty-based sampling and noise filtering in annotation datasets.
  - **Quick check question:** Why might training on a curated 10% subset outperform full-dataset training for LLMs specifically?

## Architecture Onboarding

- **Component map:**
  Raw Training Data (5M pairs) -> [Stage 1] Data Selection Pipeline -> Curated Dataset (~0.5M pairs) -> [Stage 2] Multi-CoT Tuning -> SFT Model -> [Stage 3] DPO De-biasing -> Final LREF Model -> [Deployment] Knowledge Distillation -> BERT for serving

- **Critical path:** Stage 1 → Stage 2 → Stage 3 is sequential; cannot skip Stage 1 without degrading Stage 2 (noisy data undermines CoT learning), cannot skip Stage 2 without reducing DPO effectiveness (needs strong base model to generate meaningful beam candidates).

- **Design tradeoffs:**
  - Latency vs. accuracy: LREF uses 7B LLM offline, distills to BERT for online serving (Section 3.5.1). Direct LLM deployment would have ~100ms+ latency.
  - Data quantity vs. quality: Ablation shows 0.5M curated samples > 5M raw samples. Annotation cost may shift toward quality review rather than volume.
  - CoT complexity: EE-CoT alone degraded performance; RA-CoT required for constraints. More CoT types ≠ linear improvement.

- **Failure signatures:**
  - Over-filtering in Stage 1: If Mislabeled Supervisor is too aggressive, legitimate ambiguous cases are removed → model underperforms on boundary queries. Monitor: training set size should be 8-12% of raw data.
  - Insufficient beam search in Stage 3: If k is too small in DPO pair construction, correct answers won't appear in candidates → no preference pairs generated. Monitor: ≥70% of errors should have correct answer in top-3.
  - CoT hallucination at inference: If RA-CoT rules are too rigid, model may reject legitimate matches that don't fit template. Monitor: Exact-class recall shouldn't drop below baseline.

- **First 3 experiments:**
  1. **Data selection ablation:** Train three models (full data, CI-only, CI+IM+MS) on same base LLM. Compare WF1 on held-out test set. Expected: ~5% improvement from full to full selection pipeline.
  2. **CoT variant comparison:** Starting from Stage 1 checkpoint, train separate models with EE-only, EE+RA, EE+RA+DR. Log per-class precision/recall, especially Marginal vs. Significant. Expected: DR provides largest marginal gain.
  3. **DPO pair quality audit:** Before training Stage 3, manually review 100 preference pairs (chosen/rejected). Check that rejected answers are genuinely incorrect (not just different valid interpretations). Expected: <10% annotation disagreement rate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the LREF framework be adapted for direct real-time inference in high-traffic industrial search systems without relying on knowledge distillation to smaller models?
- **Basis in paper:** [Explicit] The paper states in Section 3.5.1: "To reduce the response latency of online deployment, we leverage the data distillation method... to a BERT-based model for efficient online serving."
- **Why unresolved:** The authors addressed latency by bypassing the LLM during serving time rather than optimizing the LLM inference itself. It is unclear if the full reasoning capabilities of LREF can be retained in a live setting without the latency penalty that necessitated the distillation step.
- **What evidence would resolve it:** Online A/B testing results demonstrating that a quantized or optimized LLM version of LREF can serve traffic directly with acceptable latency (e.g., < 20ms) and comparable conversion rates to the distilled version.

### Open Question 2
- **Question:** Does the use of LLM-based auxiliary models for data selection introduce a "model collapse" risk or systematic bias against out-of-distribution query patterns?
- **Basis in paper:** [Inferred] Section 2.2 describes using the "Mislabeled Supervisor" (MS) and "Challenge Identifier" (CI)—both fine-tuned LLMs—to filter data. While effective for noise reduction, relying on the model's own judgment to curate its training data could reinforce existing blind spots.
- **Why unresolved:** The paper evaluates the final accuracy but does not analyze the *rejected* data (false negatives) from the selection process. If the MS incorrectly rejects valid but rare user intents, the model's knowledge capacity would contract rather than expand.
- **What evidence would resolve it:** A detailed error analysis of the samples rejected by the Mislabeled Supervisor to verify that legitimate, diverse linguistic patterns are not being systematically excluded from the fine-tuning set.

### Open Question 3
- **Question:** Is the "Optimistic Bias" observed in LLM relevance tasks a trait of the specific model architecture (LLaMA-2 7B) or a general characteristic of instruction-tuned models?
- **Basis in paper:** [Explicit] Section 1 identifies "optimistic bias" as a key challenge where LLMs "tend to make more lenient and optimistic judgments." However, the experiments are limited to the LLaMA-2 7B architecture (Section 3.3).
- **Why unresolved:** Different base models or parameter scales (e.g., 70B models or different attention mechanisms) might exhibit varying degrees of over-recall. The reliance on DPO to fix this implies the bias is treated as an inevitability rather than an artifact of the specific base model.
- **What evidence would resolve it:** A comparative study applying the LREF framework without DPO to various base model sizes and families (e.g., Mistral, Qwen) to measure the baseline variance of optimistic bias before de-biasing is applied.

## Limitations
- Data selection stage contribution is modest (~1.2% WF1 gain from Mislabeled Supervisor), suggesting limited impact from noise filtering.
- EE-CoT alone degraded performance (64.58% vs 65.07% WF1), indicating rule-based constraints are essential but underspecified.
- Framework relies on 7B LLM offline and BERT online, limiting direct deployment of full reasoning capabilities in real-time systems.

## Confidence
- Data Selection Impact: Medium
- Multi-CoT Effectiveness: Medium
- DPO De-biasing Results: High
- Online Deployment Gains: High

## Next Checks
1. Conduct ablation study comparing full-data fine-tuning vs. each stage of LREF's data selection pipeline (IM-only, IM+CI, IM+CI+MS) to quantify individual contribution thresholds.
2. Test RA-CoT rule generalization by applying the same rule-based constraints to a different LLM architecture without re-training the rules.
3. Verify DPO pair quality by manually auditing 100 preference pairs to ensure rejected predictions are genuinely incorrect rather than valid alternative interpretations.