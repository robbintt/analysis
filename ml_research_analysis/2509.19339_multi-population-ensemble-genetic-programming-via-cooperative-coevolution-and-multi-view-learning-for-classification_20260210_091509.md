---
ver: rpa2
title: Multi-population Ensemble Genetic Programming via Cooperative Coevolution and
  Multi-view Learning for Classification
arxiv_id: '2509.19339'
source_url: https://arxiv.org/abs/2509.19339
tags:
- large
- megp
- small
- negligible
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-population Ensemble Genetic Programming
  (MEGP), a framework that integrates cooperative coevolution and multi-view learning
  to tackle high-dimensional classification challenges. MEGP partitions the input
  space into conditionally independent feature subsets, evolving multiple subpopulations
  in parallel.
---

# Multi-population Ensemble Genetic Programming via Cooperative Coevolution and Multi-view Learning for Classification

## Quick Facts
- arXiv ID: 2509.19339
- Source URL: https://arxiv.org/abs/2509.19339
- Reference count: 40
- Primary result: MEGP consistently outperforms baseline GP across eight datasets in Log-Loss, Precision, Recall, F1 score, and AUC while maintaining diversity and accelerating fitness gains

## Executive Summary
This paper introduces Multi-population Ensemble Genetic Programming (MEGP), a framework that integrates cooperative coevolution and multi-view learning to tackle high-dimensional classification challenges. MEGP partitions the input space into conditionally independent feature subsets, evolving multiple subpopulations in parallel. Each individual encodes multiple genes, with outputs aggregated via a differentiable softmax-based weighting layer. A hybrid selection mechanism, incorporating both isolated and ensemble-level fitness, promotes inter-population cooperation while preserving intra-population diversity. Experimental evaluations across eight benchmark datasets demonstrate that MEGP consistently outperforms a baseline GP model, achieving significant improvements in Log-Loss, Precision, Recall, F1 score, and AUC. MEGP also exhibits robust diversity retention and accelerated fitness gains throughout evolution, highlighting its effectiveness for scalable, ensemble-driven evolutionary learning.

## Method Summary
MEGP integrates cooperative coevolution and multi-view learning to address high-dimensional classification tasks. The method partitions input features into conditionally independent subsets, with each subset assigned to a separate subpopulation that evolves in parallel. Each individual encodes multiple genes, and their outputs are aggregated using a differentiable softmax-based weighting layer. A hybrid selection mechanism combines isolated fitness (within subpopulations) and ensemble-level fitness to promote cooperation while maintaining diversity. The framework was evaluated across eight benchmark datasets, comparing against a baseline GP model and measuring performance across multiple metrics including Log-Loss, Precision, Recall, F1 score, and AUC.

## Key Results
- MEGP achieves significant improvements over baseline GP in Log-Loss, Precision, Recall, F1 score, and AUC across all eight benchmark datasets
- The framework demonstrates robust diversity retention throughout the evolutionary process
- MEGP shows accelerated fitness gains compared to standard GP approaches

## Why This Works (Mechanism)
MEGP's effectiveness stems from its ability to decompose complex high-dimensional problems into conditionally independent subproblems that can be evolved in parallel. By partitioning features into subsets and evolving separate subpopulations, the framework reduces the complexity of each search space while maintaining overall problem coverage. The softmax-based weighting layer enables differentiable combination of subpopulation outputs, allowing gradient-like information to flow through the ensemble. The hybrid selection mechanism balances exploitation within subpopulations with cooperation across the ensemble, preventing premature convergence while encouraging beneficial feature interactions.

## Foundational Learning
- **Cooperative Coevolution**: Needed for decomposing complex problems into manageable subproblems; quick check: verify subpopulations evolve independently but contribute to unified solution
- **Multi-view Learning**: Required for handling high-dimensional feature spaces by treating different feature subsets as complementary views; quick check: ensure feature partitioning preserves conditional independence
- **Genetic Programming**: Fundamental evolutionary algorithm for evolving tree-based solutions; quick check: confirm standard GP operators work within each subpopulation
- **Softmax Aggregation**: Enables differentiable combination of ensemble outputs; quick check: verify gradient flow through the weighting layer
- **Ensemble Methods**: Leverages multiple diverse models for improved generalization; quick check: measure diversity between subpopulations
- **Feature Partitioning**: Critical for maintaining conditionally independent subproblems; quick check: validate statistical independence between feature subsets

## Architecture Onboarding

### Component Map
Input features -> Feature partitioning -> Multiple subpopulations (parallel evolution) -> Individual encoding (multiple genes) -> Softmax aggregation layer -> Ensemble output -> Hybrid selection (isolated + ensemble fitness) -> Next generation

### Critical Path
Feature partitioning → Subpopulation evolution → Gene encoding → Softmax aggregation → Hybrid selection → Generation update

### Design Tradeoffs
- Parallel evolution vs. communication overhead
- Feature subset independence vs. information sharing
- Diversity preservation vs. convergence speed
- Computational cost vs. performance gains

### Failure Signatures
- Premature convergence in subpopulations
- Poor feature partitioning leading to dependent subsets
- Weighting layer failing to capture optimal ensemble combinations
- Hybrid selection mechanism not balancing exploration/exploitation

### First Experiments
1. Validate feature partitioning maintains conditional independence across datasets
2. Test softmax aggregation layer with synthetic ensemble outputs
3. Evaluate hybrid selection mechanism on simple benchmark problems

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Computational overhead from maintaining multiple evolving subpopulations
- Sensitivity to feature partitioning strategy and its impact on performance
- Limited exploration of scalability to extremely high-dimensional datasets
- No detailed analysis of parameter sensitivity or hyperparameter optimization

## Confidence

| Claim | Confidence |
|-------|------------|
| MEGP outperforms baseline GP across all metrics | High |
| Framework maintains diversity throughout evolution | High |
| Softmax-based aggregation effectively combines subpopulation outputs | Medium |
| Feature partitioning strategy preserves conditional independence | Medium |
| Results generalize across different dataset characteristics | Medium |

## Next Checks
1. Conduct ablation studies to isolate contributions of cooperative coevolution vs. multi-view learning components
2. Test scalability on datasets with thousands of features to evaluate practical limitations
3. Perform parameter sensitivity analysis to identify optimal configuration ranges for different problem types