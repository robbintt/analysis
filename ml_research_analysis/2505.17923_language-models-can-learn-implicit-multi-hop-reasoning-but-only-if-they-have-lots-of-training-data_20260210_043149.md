---
ver: rpa2
title: Language models can learn implicit multi-hop reasoning, but only if they have
  lots of training data
arxiv_id: '2505.17923'
source_url: https://arxiv.org/abs/2505.17923
tags:
- training
- data
- reasoning
- entity
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models can learn implicit
  multi-hop reasoning without explicit chain-of-thought prompting. The authors train
  GPT2-style transformers from scratch on synthetic k-hop reasoning datasets (k=2,3,4),
  where models must combine multiple facts to answer questions.
---

# Language models can learn implicit multi-hop reasoning, but only if they have lots of training data

## Quick Facts
- arXiv ID: 2505.17923
- Source URL: https://arxiv.org/abs/2505.17923
- Reference count: 40
- Primary result: Language models can learn implicit k-hop reasoning, but training data requirements grow exponentially with k (×100 for 4-hop vs ×10 for 3-hop)

## Executive Summary
This paper investigates whether language models can learn implicit multi-hop reasoning without explicit chain-of-thought prompting. The authors train GPT2-style transformers from scratch on synthetic k-hop reasoning datasets (k=2,3,4), where models must combine multiple facts to answer questions. They find that while such models can indeed learn implicit k-hop reasoning, the required training data grows exponentially with k (e.g., ×100 data for 4-hop vs ×10 for 3-hop), and the number of transformer layers must grow linearly with k. Mechanistic interpretability reveals that models solve these tasks through layer-wise lookup of intermediate "bridge entities." Curriculum learning significantly mitigates but does not eliminate the data requirements. The study concludes that language models can perform implicit reasoning, but this capability comes at substantial computational costs.

## Method Summary
The authors train GPT-2 Small (12 layers, 768 dim, 12 heads) with RoPE positional embeddings from scratch on synthetic k-hop reasoning datasets. They generate controlled entity profiles and multi-hop questions with hierarchical structures, varying the hop count (k=2,3,4) and training data scale. The model is evaluated using exact-match accuracy on test sets, and mechanistic interpretability is performed using causal intervention (activation patching) and probing classifiers to identify how the model performs reasoning across layers and tokens.

## Key Results
- Models can learn implicit k-hop reasoning without chain-of-thought prompting
- Training data requirements grow exponentially with k (×100 for 4-hop vs ×10 for 3-hop)
- Number of transformer layers must grow linearly with k (Theorem 5.1)
- Models perform reasoning through layer-wise lookup of intermediate bridge entities
- Curriculum learning reduces but does not eliminate data requirements (×5 vs ×100 budget for 4-hop)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models perform k-hop reasoning through sequential layer-wise lookup of intermediate bridge entities.
- Mechanism: Each transformer layer (or small group of layers) resolves one hop in the reasoning chain. Layer 1 encodes/retrieves the 1-hop entity; layers 2–3 handle the 2-hop entity; deeper layers resolve higher-hop entities, culminating in the final answer.
- Core assumption: The attention pattern does not depend on the query entity e (Theorem 5.1); shortcut-free relation sets are assumed for worst-case depth bounds.
- Evidence anchors:
  - [abstract] "models perform reasoning via layer-wise lookup of intermediate bridge entities"
  - [Section 5.2] Causal intervention shows distinct layer effects per hop: corrupting 1-hop affects layer 1 most; 2-hop affects layers 2–3, etc.
  - [corpus] Weak direct corpus support; neighbor papers focus on implicit reasoning existence, not layer-wise mechanisms.
- Break condition: If attention can depend heavily on e or if relation compositions admit shortcuts, fewer layers may suffice; bound may not hold for all R.

### Mechanism 2
- Claim: All reasoning computation concentrates at the last token position (the whitespace before the answer).
- Mechanism: The hidden state at the final token aggregates and composes bridge entity information; preceding tokens contribute primarily relation context but not composition.
- Core assumption: Task structure ensures the model must combine relations and the source entity only after seeing the full query.
- Evidence anchors:
  - [Section 5.2] Probing shows only the <space> token consistently encodes all four bridge entities; other tokens do not.
  - [Section G.1] Activation patching at non-final tokens shows negligible causal effects on output probability.
  - [corpus] Not explicitly corroborated; neighbor papers do not analyze token-level reasoning loci.
- Break condition: If prompts are reformatted so composition occurs earlier (e.g., entity-first ordering) or if models use different positional strategies, reasoning locus may shift.

### Mechanism 3
- Claim: Curriculum learning reduces data needs by building higher-hop circuits on top of already-established lower-hop circuits.
- Mechanism: Stage-wise training (2-hop → 3-hop → 4-hop) allows 1-hop circuits to stabilize in shallow layers; subsequent stages extend rather than relearn these circuits.
- Core assumption: Circuits for different hops are partially compositional and reusable; training dynamics favor incremental construction over simultaneous discovery.
- Evidence anchors:
  - [Section 6.2] Curriculum learning enables 4-hop at ×5 budget vs ×100 baseline.
  - [Section G.2] Causal-effect-over-training shows progressive circuit construction with curriculum, simultaneous emergence without it.
  - [corpus] Neighbor papers suggest curriculum-like internalization strategies (CoT elision), but do not provide mechanistic circuit-building evidence.
- Break condition: If training budgets are extremely large, simultaneous emergence may still succeed; curriculum benefits may diminish when hop-specific data is plentiful.

## Foundational Learning

- Concept: Residual stream & activation patching
  - Why needed here: Causal intervention modifies residual streams at specific layers/tokens to test reliance on bridge entities.
  - Quick check question: Can you define the residual stream and explain how replacing it at a specific layer/token tests causal dependence?

- Concept: Composition of relations (k-hop queries)
  - Why needed here: The task is formally rk(…r1(e)…); understanding function composition clarifies why search space grows as |R|^k.
  - Quick check question: Given relations r1, r2 mapping entities to entities, what is the composed function r2∘r1 and its output for a given input entity?

- Concept: Curriculum learning
  - Why needed here: The method critically mitigates (but does not eliminate) exponential data growth by staged training.
  - Quick check question: How does presenting 2-hop tasks before 4-hop tasks change the circuit-building dynamics?

## Architecture Onboarding

- Component map: GPT-2 style transformer (12 layers, 12 heads, 768 dim) with RoPE positional embeddings -> Training on k-hop datasets -> Probing classifiers and activation patching hooks

- Critical path:
  1. Generate controlled k-hop dataset with hierarchical entity profiles
  2. Train model with specified data budget; log test accuracy
  3. Run probing and activation patching at final token to identify layer-hop correspondence
  4. Optionally apply curriculum learning and compare data budgets

- Design tradeoffs:
  - Depth vs. k: More hops require more layers (linear growth theoretically)
  - Data budget vs. k: Exponential growth; curriculum learning mitigates but does not eliminate
  - Precision/width/heads vs. depth: Theorem 5.1 implies a width-depth tradeoff; increasing d/H/p can reduce required L

- Failure signatures:
  - Accuracy ≈ random baseline despite training: insufficient data budget for given k
  - Probing fails to localize bridge entities to specific layers: model may be memorizing or using shortcuts
  - Curriculum learning shows no improvement: auxiliary lower-hop data may be insufficient or stages poorly tuned

- First 3 experiments:
  1. Replicate 2/3/4-hop training on k-hopsmall with ×1, ×5, ×20 budgets to confirm exponential data scaling
  2. Apply probing at each token position to verify reasoning concentration at the last token
  3. Compare baseline vs. curriculum learning on 4-hoplarge with budgets ×5 to ×100 to quantify curriculum benefit

## Open Questions the Paper Calls Out

- Question: Do the exponential training data requirements and layer-wise lookup mechanisms persist when applying this analysis to realistic, non-synthetic datasets?
  - Basis in paper: [explicit] The authors explicitly list the use of synthetic datasets as a limitation, stating "Applying the same analysis to realistic datasets is challenging."
  - Why unresolved: Realistic datasets introduce noise and variable fact densities that are excluded by the controlled templates used in this study.
  - What evidence would resolve it: Replicating the mechanistic interpretability experiments on a natural multi-hop QA corpus (e.g., HotpotQA) to observe if similar scaling laws apply.

- Question: Can the theoretical linear lower bound on model depth be proven without the constraint that attention patterns are independent of the query entity?
  - Basis in paper: [explicit] Appendix A.2 states regarding the proof of Theorem 5.1: "It remains open if this assumption [attention pattern does not depend on e] can be relaxed."
  - Why unresolved: The current proof relies on a communication complexity argument that necessitates restricting attention patterns to be entity-independent.
  - What evidence would resolve it: A formal mathematical proof extending the depth bound to transformers with query-dependent attention, or a constructive demonstration of a shallower architecture solving the task via dynamic attention.

- Question: Does scaling model parameters beyond GPT-2 Small mitigate the exponential data growth required for implicit multi-hop reasoning?
  - Basis in paper: [explicit] The Limitations section notes that experiments were restricted to GPT-2 small and the authors "do not extend our analysis to models with greater parameter sizes."
  - Why unresolved: It is unclear if the sample inefficiency is a fundamental property of the task or a limitation of the small model capacity used in the study.
  - What evidence would resolve it: Training larger transformer architectures (e.g., 1B+ parameters) on the same k-hop tasks to determine if the data scaling exponent decreases.

## Limitations

- The study uses synthetic datasets, limiting generalizability to real-world reasoning tasks
- Experiments are restricted to GPT-2 Small architecture, leaving questions about scalability to larger models
- The theoretical depth bounds assume worst-case relation sets without shortcuts, which may not reflect practical scenarios
- Exponential data growth may make k-hop reasoning infeasible for larger k values in practical applications

## Confidence

- High Confidence: The exponential data scaling with k-hop complexity is well-supported by multiple experimental conditions and directly observed in training curves
- Medium Confidence: The curriculum learning benefits are demonstrated but show significant variation across different hop levels
- Low Confidence: The theoretical bounds on depth requirements (Theorem 5.1) assume worst-case relation sets without shortcuts

## Next Checks

1. Apply the same layer-wise analysis methodology to a non-synthetic multi-hop reasoning dataset (e.g., HotpotQA) to determine if the layer-wise lookup mechanism generalizes beyond controlled synthetic environments.

2. Design experiments that introduce shortcut paths in the relation graph to test whether models abandon the layer-wise mechanism when easier solutions become available, and measure the impact on data efficiency.

3. Systematically vary the number of layers (L) and hidden dimension (d) while keeping computational budget constant to empirically validate the width-depth tradeoff suggested by Theorem 5.1 and determine optimal architectural configurations for different k values.