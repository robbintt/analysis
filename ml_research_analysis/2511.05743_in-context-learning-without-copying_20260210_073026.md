---
ver: rpa2
title: In-Context Learning Without Copying
arxiv_id: '2511.05743'
source_url: https://arxiv.org/abs/2511.05743
tags:
- heads
- induction
- copying
- vanilla
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Inductive copying by induction heads has been hypothesized as a
  foundation for in-context learning, but its necessity for abstractive capabilities
  remains unclear. To test this, we introduce HAPAX, a training regime that masks
  loss contributions from tokens predictable by induction heads, thereby suppressing
  inductive copying.
---

# In-Context Learning Without Copying

## Quick Facts
- arXiv ID: 2511.05743
- Source URL: https://arxiv.org/abs/2511.05743
- Reference count: 40
- Primary result: HAPAX models maintain or surpass vanilla performance on 13 of 21 abstractive ICL tasks despite 31.7% reduction in training tokens

## Executive Summary
This paper investigates whether inductive copying by induction heads is necessary for abstractive in-context learning (ICL) capabilities in transformers. The authors introduce HAPAX, a training regime that masks loss contributions from tokens predictable by induction heads, thereby suppressing inductive copying. Despite masking 31.7% of training tokens, HAPAX models maintain or surpass vanilla performance on 13 of 21 abstractive ICL tasks, with statistically significant gains on 9 tasks. The results indicate that inductive copying is not causally required for abstractive ICL and that reducing repetition incentives can even enhance certain capabilities.

## Method Summary
The HAPAX method masks loss contributions for tokens predictable by induction heads by identifying repeated n-grams within context windows and excluding these positions from loss computation. The approach uses a 1B parameter GPT-NeoX model trained on The Pile dataset with 20,000 steps, where HAPAX masks positions where bigrams repeat within the context window. The method also includes a thresholded variant using cosine similarity τ=0.3 in embedding space, which masks 52.5% of tokens. Vanilla models are trained on 40B tokens while HAPAX uses 28B tokens (31.7% masked).

## Key Results
- HAPAX models show 31.7% reduction in training tokens but maintain comparable or superior performance on 13 of 21 abstractive ICL tasks
- HAPAX performs better on 9 of 17 abstractive tasks with statistically significant differences
- HAPAX models show fewer and weaker induction heads but preserve ICL capabilities
- HAPAX achieves lower losses on non-extractive samples and generates more fluent natural text than vanilla model

## Why This Works (Mechanism)

### Mechanism 1: Gradient Isolation for Copyable Tokens
Masking loss on tokens predictable by induction heads reduces the model's incentive to develop strong inductive copying circuits by excluding positions where repeated n-grams appear in context from gradient computation.

### Mechanism 2: Functional Independence of Abstractive ICL from Inductive Copying
Abstractive ICL does not causally depend on inductive copying capabilities, as evidenced by HAPAX models showing degraded extractive performance but preserved or improved abstractive performance.

### Mechanism 3: Incidental Prefix-Matching from Architectural Priors
Some prefix-matching attention patterns emerge from the interaction between previous token heads and randomly initialized later-layer heads, explaining residual (weaker) induction behavior in HAPAX models.

## Foundational Learning

- **Induction Circuits**: Previous token heads → induction heads; needed to understand the specific circuit targeted by HAPAX intervention
  - Quick check: Given sequence [A, B, C, A, B], which token position does an induction head attend to when predicting the final token?

- **Token-Loss Difference (TLD) Metric**: Measures ICL capability by comparing losses at different positions; needed to evaluate context utilization
  - Quick check: If loss at position 500 is lower than position 50, what does positive TLD indicate about context utilization?

- **Mean Ablation for Causal Attribution**: Determines which attention heads causally contribute to copying vs abstract tasks
  - Quick check: After mean ablating a head, if target token probability decreases, what does this indicate about that head's role?

## Architecture Onboarding

- **Component map**: Loss masking module (identifies repeated n-grams) → Training loop modification (applies mask before loss aggregation) → Evaluation suite (28 extractive + 26 abstractive + 8 translation tasks) → Mechanistic analysis (prefix-matching scoring, mean ablation, logit lens)

- **Critical path**: Implement n-gram detection → Integrate masking into forward pass → Train from scratch on Pile dataset → Evaluate extractive vs abstractive ICL tasks

- **Design tradeoffs**: Exact matching (31.7% tokens masked) vs thresholded matching (52.5% masked); stricter masking reduces inductive copying more but harms some abstractive tasks

- **Failure signatures**: Random repetition accuracy remains high → masking not working correctly; Extractive tasks show no degradation → loss masking not applied

- **First 3 experiments**:
  1. Replicate HAPAX training on small scale (160M parameters) and verify random repetition accuracy drops by >60% vs vanilla
  2. Run ablation study on top-10 prefix-matching heads for both models; confirm HAPAX heads show more negative probability differences
  3. Evaluate on abstractive tasks with target-excluded few-shot examples to control for distributional copying

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms or attention heads replace induction heads to facilitate abstractive ICL in HAPAX models? The authors confirm reduction of induction heads and preservation of performance but do not isolate the specific circuitry that compensates for this reduction.

### Open Question 2
Why does the stricter similarity-thresholded HAPAX regime significantly improve translation tasks while degrading performance on other abstractive tasks? The authors hypothesize cross-language token pairs often fall below the cosine similarity threshold, but do not confirm if this is the sole cause.

### Open Question 3
Does the HAPAX training regime provide similar benefits for abstractive ICL in larger language models (>1B parameters)? The experimental scope is limited to 1B parameter models trained from scratch, leaving scaling dynamics unexplored.

## Limitations

- The masking mechanism may reduce overall training signal rather than truly isolating abstractive capabilities
- The confound between reduced training tokens (28B vs 40B) and the masking intervention itself is not systematically isolated
- The mechanistic analysis showing incidental prefix-matching from architectural priors remains speculative with limited experimental evidence

## Confidence

**High Confidence**: Core empirical finding that HAPAX models maintain abstractive ICL capabilities while reducing inductive copying
**Medium Confidence**: Claim that inductive copying is not causally required for abstractive ICL
**Low Confidence**: Mechanism of incidental prefix-matching from architectural priors

## Next Checks

1. Train HAPAX models at 160M, 400M, and 1.3B parameter scales to test whether the dissociation between extractive and abstractive performance persists across architectures

2. Implement targeted ablations of non-induction circuits (function vector heads, value induction heads) in HAPAX models to determine if these circuits specifically compensate for reduced copying capability

3. Track the emergence of abstractive vs extractive capabilities at fine-grained intervals (every 1K steps) to determine whether abstractive circuits develop concurrently with or subsequent to any residual copying behavior