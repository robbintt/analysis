---
ver: rpa2
title: Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent
  Multi-Turn Interaction
arxiv_id: '2510.20411'
source_url: https://arxiv.org/abs/2510.20411
tags:
- seqlen
- cefr
- progressive
- parlai
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONTINGENT CHAT is a teacher-student framework that benchmarks
  and improves multi-turn contingency in BabyLMs trained on 100M words. It uses a
  novel alignment dataset for post-training, enabling BabyLMs to generate more grammatical
  and cohesive responses through iterative trial-and-demonstration interactions with
  a larger teacher LLM.
---

# Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction

## Quick Facts
- **arXiv ID:** 2510.20411
- **Source URL:** https://arxiv.org/abs/2510.20411
- **Reference count:** 40
- **Key outcome:** CPO-based post-training improves multi-turn contingency in BabyLMs trained on 100M words through iterative trial-and-demonstration interactions with a larger teacher LLM.

## Executive Summary
CONTINGENT CHAT is a teacher-student framework that benchmarks and improves multi-turn contingency in BabyLMs trained on 100M words. It uses a novel alignment dataset for post-training, enabling BabyLMs to generate more grammatical and cohesive responses through iterative trial-and-demonstration interactions with a larger teacher LLM. Experiments with adaptive teacher decoding strategies showed limited additional gains, but CPO-based post-training significantly improved turn-level coherence, lexical continuity, and grammatical repair. Results demonstrate the positive benefits of targeted post-training for dialogue quality and indicate that contingency remains a challenging goal for BabyLMs.

## Method Summary
The CONTINGENT CHAT framework pre-trains a small BabyLM (OPT-125M) on 100M words from the STRICT BabyLM Corpus, then post-trains it using contrastive preference optimization (CPO) or offline reinforcement preference optimization (ORPO) with a larger teacher LLM. The alignment dataset is built from Switchboard conversations annotated with TAACO cohesion metrics, where student-generated responses are paired with teacher-improved versions. Post-training occurs over 5 disjoint iterations, with each iteration training on a different subset of the preference pairs. The framework also explores adaptive teacher decoding strategies using CEFR level constraints to control lexical complexity.

## Key Results
- CPO-based post-training significantly improved turn-level coherence, lexical continuity, and grammatical repair in BabyLMs
- Models trained on fewer than 100M words struggled to sustain post-training interactions, exhibiting unstable generation behaviors
- CEFR-constrained teacher decoding showed limited additional gains compared to standard CPO baseline
- Human evaluation revealed that BabyLMs passed grammaticality and cohesion checks but still struggled with conciseness and appropriateness

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Preference Optimization Anchors Learning in ZPD
- **Claim:** CPO-based post-training improves turn-level coherence and grammatical repair by constraining policy updates to remain close to teacher demonstrations.
- **Mechanism:** Preference pairs are formed from original BabyLM outputs (disfavoured) and teacher-improved continuations (favoured). CPO's contrastive loss penalizes adequate-but-suboptimal responses while rewarding responses that approach teacher quality, creating a scaffolded learning region.
- **Core assumption:** Teacher improvements represent achievable targets within the student's Zone of Proximal Development—not too distant from current capability.
- **Evidence anchors:** [abstract]: "CPO-based post-training significantly improved turn-level coherence, lexical continuity, and grammatical repair"

### Mechanism 2: Minimum Scale Threshold Enables Stable Post-Training
- **Claim:** Models require ~100M words of pre-training data before they can stably benefit from interactive preference-based post-training.
- **Mechanism:** Smaller models exhibit self-repetition and incoherent generation during post-training, creating unstable preference signals. Sufficient pre-training provides the grammatical foundation needed to meaningfully respond to scaffolding.
- **Core assumption:** A baseline level of formal linguistic competence is prerequisite to acquiring functional/pragmatic competence through interaction.
- **Evidence anchors:** [section 3]: "preliminary experiments indicate that models trained on fewer than 100M words struggle to sustain post-training interactions"

### Mechanism 3: CEFR-Constrained Teacher Decoding Shows Limited Effect
- **Claim:** Lexical complexity control via CEFR levels provides weak benefits for contingency learning without accompanying interactive feedback.
- **Mechanism:** A regressor predicts CEFR level of candidate messages; re-ranking combines original beam score with CEFR target. Progressive (A2→C2) and reverse curricula were tested.
- **Core assumption:** The Goldilocks Principle applies—learning is maximized when input difficulty matches learner proficiency.
- **Evidence anchors:** [abstract]: "Experiments with adaptive teacher decoding strategies showed limited additional gains"

## Foundational Learning

- **Concept: Contingency in Multi-turn Dialogue**
  - **Why needed here:** The paper's central goal is improving contingency—prompt, direct, meaningful exchanges between interlocutors. Grammatical responses can still be non-contingent (e.g., answering "What do you like about summer?" with "What's the matter?").
  - **Quick check question:** Can you explain why a response might be grammatical but fail to be contingent?

- **Concept: Preference-based Optimization (CPO vs. ORPO)**
  - **Why needed here:** The technical contribution relies on comparing CPO and ORPO for aligning student outputs with teacher preferences. CPO requires a reference model; ORPO integrates preference directly into SFT.
  - **Quick check question:** Which approach would you expect to produce more diverse but potentially noisier learning dynamics?

- **Concept: Cohesion Metrics (TAACO-based)**
  - **Why needed here:** Standard language model benchmarks (BLiMP, perplexity) don't capture dialogue quality. The paper uses TTR, content-word overlap, verb-tense repetition, and related metrics from TAACO.
  - **Quick check question:** Why might high lexical overlap between turns indicate either good cohesion OR problematic repetition?

## Architecture Onboarding

- **Component map:**
  - Student (BabyLM): OPT-125M pretrained on 100M words from STRICT BabyLM Corpus
  - Teacher (Experiment 1): Llama-3.1-8B-Instruct with anti-repetition/coherence instructions
  - Teacher (Experiment 2): BlenderBot 3B with CEFR-level regressor for adaptive decoding
  - Alignment Dataset: 30M words from Switchboard annotated with TAACO metrics
  - Post-training: CPO or ORPO over 5 disjoint iterations, carrying weights forward

- **Critical path:**
  1. Pretrain OPT-125M on 100M words (STRICT corpus)
  2. Sample 2-turn dialogues from Switchboard, compute cohesion annotations
  3. Generate student continuation → teacher improvement → filter low-quality outputs
  4. Train with CPO/ORPO for 5 iterations on disjoint dataset slices
  5. Evaluate on BabyLM benchmarks (BLiMP, EWoK) + cohesion metrics + human rubric

- **Design tradeoffs:**
  - CPO (stricter teacher adherence, higher grammaticality) vs. ORPO (more exploration, potentially noisier)
  - Sequence length 1024 vs. 4096: 1024 generally performed better in experiments
  - CEFR adaptation: theoretically motivated but empirically limited gains

- **Failure signatures:**
  - Self-repetition loop in generated dialogue turns
  - Responses that fail to directly address prompts (non-contingent)
  - Instability during post-training for models <100M words
  - Passing grammaticality/cohesion checks but failing conciseness/appropriateness (human eval shows this gap)

- **First 3 experiments:**
  1. Replicate baseline failure mode: Run pretrained OPT-125M through multi-turn dialogue with teacher; quantify self-repetition rate and non-contingent responses using cohesion metrics
  2. A/B test CPO vs. ORPO: Train both on identical preference pairs; compare on BLiMP + cohesion metrics to validate trade-off hypothesis
  3. Probe ZPD boundary: Vary teacher improvement quality (add noise to teacher outputs); measure at what point post-training degrades to identify scaffolding limits

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CONTINGENT CHAT framework effectively generalize to interactional contexts beyond adult telephone conversations, such as spontaneous child-directed speech?
- **Basis in paper:** [explicit] The authors state in the Limitations section that future work should extend the approach to corpora that "more closely resemble early caregiver–child interactions."
- **Why unresolved:** The current alignment dataset is derived exclusively from the Switchboard corpus, which represents a narrow sociolinguistic domain (adult telephone conversations) that differs significantly from the target L1 acquisition setting.
- **What evidence would resolve it:** Replicating the post-training pipeline using a child-directed speech corpus (e.g., CHILDES) and measuring resulting contingency improvements.

### Open Question 2
- **Question:** How does the choice of the Teacher LLM influence the robustness of the Student BabyLM's contingent behavior?
- **Basis in paper:** [explicit] The paper notes in the Limitations that all experiments used a single Teacher (Llama-3.1-8B-Instruct) and that "Investigation with more Teacher Models" is required.
- **Why unresolved:** It is unclear if the observed improvements represent generalized contingent competence or mere "stylistic imitation" of the specific Llama-3.1-8B-Instruct model's discourse patterns.
- **What evidence would resolve it:** Running the CONTINGENT CHAT framework with diverse Teacher LLMs and evaluating the cross-model consistency of the student's performance.

### Open Question 3
- **Question:** What specific types of adaptive feedback are most effective for helping BabyLMs internalize communicative forms and generalize beyond imitation?
- **Basis in paper:** [explicit] The Discussion notes that static demonstrations convey forms but not the adaptive feedback necessary for internalization, concluding "Further controlled experimentation is needed... investigating different types of adaptive feedback."
- **Why unresolved:** The study found limited gains from lexically-constrained demonstrations, suggesting the need for different feedback mechanisms to bridge the gap between imitation and generalization.
- **What evidence would resolve it:** Comparing the current preference-based feedback against dynamic reward signals that adjust based on the student's proficiency level within the ZPD.

## Limitations

- The framework relies on a single teacher model (Llama-3.1-8B-Instruct), limiting assessment of whether improvements generalize beyond that model's specific discourse patterns
- The 100M-word pre-training threshold for stable post-training is empirically observed but the theoretical basis for this specific number remains unexplored
- CEFR-based adaptive decoding showed limited effectiveness, suggesting the current curriculum design may not optimally align with the Zone of Proximal Development concept

## Confidence

- **High confidence:** The core finding that CPO-based post-training improves turn-level coherence, lexical continuity, and grammatical repair is well-supported by both automated metrics and human evaluation
- **Medium confidence:** The limited effectiveness of CEFR-constrained teacher decoding is demonstrated, but the inconsistent results across experiments suggest the mechanism may be more nuanced than reported
- **Low confidence:** The claim that teacher demonstrations represent achievable targets within the ZPD is theoretically sound but lacks direct empirical validation

## Next Checks

1. **ZPD boundary validation:** Systematically vary teacher improvement quality (from subtle corrections to wholesale rewrites) and measure at what point post-training degrades for different model sizes to empirically map the ZPD
2. **Beam size impact study:** Replicate the CEFR experiments with 20-beam decoding (matching Tyen et al. 2022) to isolate whether candidate diversity explains the limited gains
3. **Cross-corpus generalization:** Test CONTINGENT CHAT with BabyLMs trained on alternative 100M-word corpora to assess whether improvements transfer beyond the STRICT corpus domain