---
ver: rpa2
title: 'PEToolLLM: Towards Personalized Tool Learning in Large Language Models'
arxiv_id: '2502.18980'
source_url: https://arxiv.org/abs/2502.18980
tags:
- tool
- user
- history
- interaction
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of personalized tool learning for
  LLMs, addressing the need to handle implicit user preferences by integrating user
  interaction history. To evaluate this task, the authors construct PEToolBench, a
  benchmark featuring diverse user preferences under three personalized settings (preferred-only,
  rating-integrated, and chronological) and encompassing 7,454 tools across 46 categories.
---

# PEToolLLM: Towards Personalized Tool Learning in Large Language Models

## Quick Facts
- **arXiv ID:** 2502.18980
- **Source URL:** https://arxiv.org/abs/2502.18980
- **Reference count:** 27
- **Primary result:** PEToolLLaMA achieves over 50% improvement in tool accuracy by integrating user interaction history into LLM tool selection

## Executive Summary
This paper introduces the task of personalized tool learning for LLMs, addressing the need to handle implicit user preferences by integrating user interaction history. To evaluate this task, the authors construct PEToolBench, a benchmark featuring diverse user preferences under three personalized settings (preferred-only, rating-integrated, and chronological) and encompassing 7,454 tools across 46 categories. They propose PEToolLLaMA, a framework that adapts LLMs through supervised fine-tuning and direct preference optimization. Extensive experiments show that PEToolLLaMA significantly outperforms existing LLMs, achieving over 50% improvement in tool accuracy and demonstrating superior personalized tool-use capabilities.

## Method Summary
The PEToolLLaMA framework trains LLaMA-3.1-8B through a two-stage pipeline: Supervised Fine-Tuning (SFT) on 9,000 synthetic tool-use examples, followed by Direct Preference Optimization (DPO) using preferred vs. non-preferred tool call pairs. The method leverages user interaction history to infer implicit preferences, distinguishing tools by both functional attributes and non-functional attributes like usability and accessibility. PEToolBench is synthetically generated using GPT-4o-mini with RapidAPI tools, creating three history modes and 12,000 samples for evaluation.

## Key Results
- PEToolLLaMA achieves over 50% improvement in tool accuracy compared to baseline LLMs
- Significant improvements observed across all three history settings (preferred-only, rating-integrated, chronological)
- The model successfully handles 7,454 tools across 46 categories with strong performance in both tool selection and parameter filling accuracy

## Why This Works (Mechanism)

### Mechanism 1: Interaction History as Preference Signal
- Claim: Integrating user interaction history enables LLMs to infer implicit tool preferences not stated in current instructions.
- Mechanism: The model receives historical instruction-tool call pairs, conditioning its generation on patterns of past tool selection rather than only explicit requirements in the current query.
- Core assumption: Users demonstrate consistent, learnable preferences through historical tool usage patterns that generalize to new instructions within the same functional domain.
- Evidence anchors: [abstract] "...personalized tool learning... which integrates user's interaction history towards personalized tool usage." [section 3.1] Formally defines interaction history Hu = {h1_u, h2_u, ..., hM_u} as part of model input.
- Break condition: If user preferences are highly volatile or interaction history is too noisy (many non-preferred tools without clear signals), inferred preferences may misguide selection.

### Mechanism 2: Two-Stage Training (SFT + DPO) for Preference Alignment
- Claim: SFT establishes foundational tool-use capabilities; DPO explicitly aligns the model with user tool preferences by contrasting preferred vs. non-preferred tool calls.
- Mechanism: SFT trains correct tool selection and parameter filling. DPO constructs (preferred, non-preferred) tool call pairs for the same instruction, optimizing the policy to increase likelihood of preferred calls using the DPO loss (Eq. 3).
- Core assumption: High-quality preference pairs can be constructed, and DPO successfully generalizes the distinction between preferred and non-preferred tool calls to unseen instructions.
- Evidence anchors: [abstract] "...trained through supervised fine-tuning and direct preference optimization." [section 4] Describes Personalized SFT and Personalized DPO stages with explicit loss function.
- Break condition: If preference pairs are mislabeled or the model overfits to specific tool names rather than generalizing from attributes, DPO may degrade performance.

### Mechanism 3: Non-Functional Attribute Modeling for Tool Differentiation
- Claim: Distinguishing tools by non-functional attributes (usability, integrability, accessibility) enables selection among functionally equivalent tools based on inferred user preference.
- Mechanism: Extracts non-functional attributes from documentation, embeds them, and uses similarity to construct user preferences; model learns to attend to these during selection.
- Core assumption: Non-functional attributes are reliably extractable, relevant to user preference, and sufficiently discriminative.
- Evidence anchors: [section 1] "This underscores the need to consider non-functional tool attributes... which can better reflect user preferences." [section 3.2.1] "LLM is instructed to generate descriptions of the tool's functionality and non-functional attributes separately."
- Break condition: If documentation is sparse/ambiguous, attributes may be mischaracterized, introducing noise into preference signals.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed here: Establishes base capability to understand tool documentation, map instructions to tools, and generate valid tool calls (JSON format, correct parameters) before alignment.
  - Quick check question: Can the model, given an instruction and tool list, select a functionally appropriate tool and fill required parameters correctly without preference information?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Provides a computationally efficient method to align outputs with implicit preferences without a separate reward model, using paired preference data.
  - Quick check question: Can you construct a dataset of (instruction, history, preferred_tool_call, non_preferred_tool_call) tuples for your domain?

- **Concept: Tool Retrieval/Ranking**
  - Why needed here: Real deployments have thousands of tools; efficient retrieval ensures the LLM only considers a manageable, relevant subset.
  - Quick check question: How will you reduce a large tool corpus to a relevant candidate set for each query before LLM invocation?

## Architecture Onboarding

- **Component map:**
  1. Tool Preparation Module -> Preference Construction Module -> Interaction History Generator -> Instruction Generator -> PEToolLLaMA Training Pipeline -> Inference Routine

- **Critical path:**
  1. Validate tool understanding (correct attribute extraction)â€”downstream tasks depend on it.
  2. Check simulated histories for realism and preference consistency.
  3. Run SFT to convergence on tool-use correctness.
  4. Generate and filter preference pairs for DPO.
  5. Run DPO and evaluate on held-out test splits across all three history types.

- **Design tradeoffs:**
  - Synthetic vs. Real History: Synthetic enables scale/control but may lack real-world noise.
  - History Length: Longer histories provide more signal but increase context and may confuse the model (paper shows baseline performance drops with longer history).
  - Three History Types: Preferred-only is simplest; rating-integrated and chronological are more realistic but harder.
  - Single-tool vs. Multi-tool: Current framework handles single-tool scenarios only.

- **Failure signatures:**
  - Tool Preference Mismatch: High error rate selecting non-preferred tools despite history.
  - Tool Functionality Mismatch: Fails to select functionally correct tool for explicit requirements.
  - Invalid Format / Tool Hallucination: Malformed JSON or non-existent tool names.
  - Performance Collapse with Long History: Accuracy degrades significantly as history length increases.

- **First 3 experiments:**
  1. **Baseline SFT-only:** Train LLaMA-3.1-8B on PEToolBench using only SFT. Measure Tool Acc and Param Acc across all three history settings.
  2. **Ablate History Type:** Train PEToolLLaMA on data from only one history type (e.g., preferred-only) and evaluate on all three to assess generalization.
  3. **Probe Attribute Usage:** Mask or shuffle non-functional attributes in candidate tools for a subset of test cases. Measure drop in Tool Acc to confirm reliance on non-functional attributes.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can personalized tool learning be effectively extended to complex multi-tool scenarios requiring personalized planning and multi-round calling? Basis: The authors state, "Currently, our work is limited to tool-usage scenarios involving a single tool. In the future, we intend to expand to more complex personalized tool-usage, such as multi-tool scenarios." Unresolved because current framework only supports single-tool calls.

- **Open Question 2:** Can integrating heterogeneous personal data beyond interaction history, such as user profiles or personas, improve the accuracy of implicit preference inference? Basis: The authors propose to "explore more heterogeneous personal user data beyond interaction history, such as user profiles or personas." Unresolved because current study relies exclusively on interaction history.

- **Open Question 3:** To what extent does the reliance on LLM-synthesized interaction history compromise the authenticity and reliability of personalized tool learning evaluation? Basis: The Limitations section acknowledges that using LLMs to synthesize interaction history "may compromise the authenticity and reliability of the data." Unresolved because benchmark relies entirely on synthetic data without real user comparison.

## Limitations
- Reliance on synthetic interaction history rather than real user data may compromise authenticity and generalization to real-world scenarios.
- Current framework is limited to single-tool scenarios, unable to handle complex multi-tool workflows requiring personalized planning.
- Assumes perfect tool documentation and attribute extraction, though these could be noisy in real-world applications.

## Confidence
- **Synthetic Data Generalization:** Medium - Results show consistent improvements but lack validation on real user histories.
- **Two-Stage Training Effectiveness:** Medium - Demonstrated within controlled benchmark but potential biases in preference pair construction.
- **Non-Functional Attribute Modeling:** Low - Shows promise but lacks direct empirical validation of contribution versus other factors.

## Next Checks
1. **Real User History Validation:** Test PEToolLLaMA on a small dataset of actual user interaction histories to verify that synthetic training generalizes to real preference patterns.
2. **History Length Robustness:** Systematically evaluate model performance degradation as history length increases beyond the tested range to identify practical context window limits.
3. **Cross-Domain Transfer:** Evaluate whether personalized tool learning capabilities transfer to domains outside RapidAPI tools used in PEToolBench, testing ability to learn transferable preference inference patterns.