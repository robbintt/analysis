---
ver: rpa2
title: Empirical Likelihood for Random Forests and Ensembles
arxiv_id: '2511.13934'
source_url: https://arxiv.org/abs/2511.13934
tags:
- random
- forests
- jackknife
- empirical
- likelihood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops an empirical likelihood (EL) framework for
  random forests and related ensemble methods, enabling likelihood-based inference
  for statistical uncertainty. The core idea is to construct an EL statistic using
  jackknife pseudo-values derived from the incomplete U-statistic structure inherent
  in ensemble predictions.
---

# Empirical Likelihood for Random Forests and Ensembles

## Quick Facts
- arXiv ID: 2511.13934
- Source URL: https://arxiv.org/abs/2511.13934
- Reference count: 11
- This paper develops an empirical likelihood framework for random forests and related ensemble methods, enabling likelihood-based inference for statistical uncertainty.

## Executive Summary
This paper develops an empirical likelihood (EL) framework for random forests and related ensemble methods, enabling likelihood-based inference for statistical uncertainty. The core idea is to construct an EL statistic using jackknife pseudo-values derived from the incomplete U-statistic structure inherent in ensemble predictions. Under dense-subsampling asymptotics, the proposed EL procedure yields asymptotically pivotal inference, but tends to over-cover under sparser regimes due to loss of pivotality. To address this, a modified EL statistic is introduced that restores asymptotic pivotality in both dense- and sparse-subsampling settings.

## Method Summary
The method treats random forest predictions as incomplete U-statistics and constructs jackknife pseudo-values from subsampled trees. These pseudo-values are used to form an empirical likelihood statistic via constrained optimization. For sparse subsampling regimes where standard EL loses pivotality, a modified EL statistic is introduced that corrects for Efron-Stein bias using estimated variance components. The approach is computationally efficient, requiring only the trees already constructed during forest fitting, and retains key properties of conventional EL such as range preservation and data-driven confidence region shape.

## Key Results
- Standard EL yields asymptotically chi-squared inference under dense subsampling, but over-covers under sparser regimes
- Modified EL restores asymptotic pivotality in both dense and sparse subsampling settings
- Simulations demonstrate accurate coverage and practical reliability relative to existing methods like the infinitesimal jackknife
- The approach is computationally efficient and retains key properties of conventional EL

## Why This Works (Mechanism)

### Mechanism 1: Incomplete U-statistic Framework
Random forest predictions can be analyzed as incomplete generalized U-statistics, enabling asymptotic theory derivation for empirical likelihood. An ensemble with B_n trees averages over a subset of all possible subsample-based predictions rather than the complete combinatorial collection. The Hoeffding decomposition separates the main U-statistic U_n from the incomplete term Ũ_n. Under dense-subsampling asymptotics where n/N_n · ζ_{n,s}/(s·ζ_{n,1}) = o(1), the incomplete term Ũ_n becomes asymptotically negligible.

### Mechanism 2: Jackknife-after-subsampling Pseudo-values
Jackknife pseudo-values can be constructed efficiently from already-computed trees without requiring complete leave-one-out forests. For each observation i, define I_{1n}^{(i)} as the set of selected subsamples not containing observation i. The pseudo-value V_n^{(i)}(θ) = nS_n(θ) - (n-1)S_n^{(i)}(θ) averages only over trees excluding i. This exploits the jackknife-after-bootstrap principle, avoiding O(n·B_n) tree recomputations.

### Mechanism 3: Modified EL for Efron-Stein Bias Correction
A simple adjustment to pseudo-values restores asymptotic pivotality under both dense and sparse subsampling regimes. Under sparse subsampling, the jackknife variance overestimates the s-th order Hoeffding projection contribution by factor s (Efron-Stein bias). The modified pseudo-value Ṽ_n^{(i)}(θ) = V_n^{(i)}(θ̂_n) - [s·V̂_1/(V̂_1 + V̂_2)]·{V_n^{(i)}(θ̂_n) - V_n^{(i)}(θ)} corrects this without requiring leave-more-out estimators.

## Foundational Learning

- **U-statistics and Hoeffding Decomposition**: The entire framework treats random forests as incomplete U-statistics with kernels h_n. Understanding how U-statistics decompose into projections explains where bias arises and why the modification works. Quick check: For a complete U-statistic of order s, why does the variance decompose as Σ_{j=1}^s (s choose j)² V_j?

- **Empirical Likelihood (Owen, 1988)**: The method builds on jackknife EL by treating pseudo-values as estimating equations. Understanding constrained optimization for EL is essential for implementation. Quick check: Given pseudo-values V_n^{(i)}, how would you set up the EL optimization to test H₀: θ = θ₀?

- **Honest Random Forests (Wager & Athey, 2018)**: Low-level conditions in Theorem 4 require honesty (splitting and prediction use different subsamples) to establish bias bounds and variance ratios. Quick check: Why does honesty guarantee that prediction bias vanishes as s → ∞?

## Architecture Onboarding

- Component map: Input: (Z_1,...,Z_n), subsample size s, # trees B_n -> Tree Construction: Build B_n trees on subsamples ι_b -> Ensemble: θ̂_n = (1/B_n) Σ_b h_b -> Leave-one-out Sets: I_{1n}^{(i)} = {b: i ∉ ι_b} for each i -> Pseudo-values: V_n^{(i)}(θ) = nS_n(θ) - (n-1)S_n^{(i)}(θ) -> Correction: V̂_2 = n(s-1)/B_n · ζ̂_{n,s} -> Modified Pseudo-values: Ṽ_n^{(i)}(θ) with Efron-Stein correction -> EL Solve: Find λ via Σ_i Ṽ_n^{(i)}/(1 + λṼ_n^{(i)}) = 0 -> Statistic: ℓ̃(θ) = 2 Σ_i log(1 + λ̂(θ)·Ṽ_n^{(i)}) -> CI: {θ: ℓ̃(θ) ≤ χ²_{1,α}}

- Critical path:
  1. Computing I_{1n}^{(i)} correctly — determines which trees contribute to each pseudo-value
  2. Estimating V̂_2 = n(s-1)/B_n · ζ̂_{n,s} — essential correction for sparse regimes
  3. Solving one-dimensional optimization for λ — numerical stability when pseudo-values vary

- Design tradeoffs:
  - Dense vs. sparse subsampling: Larger N_n ensures standard EL works but increases computation; smaller N_n requires modified EL but is faster
  - Honesty vs. adaptivity: Honest trees enable theory but may sacrifice predictive power
  - Subsample size s: Larger s reduces bias but increases variance ratio, potentially requiring denser subsampling

- Failure signatures:
  - Over-coverage with standard EL: Indicates sparse regime; switch to modified EL
  - Negative pseudo-value variance: B_n too small; increase trees
  - Non-convergence in λ: Pseudo-values have extreme outliers; check data anomalies

- First 3 experiments:
  1. Calibration check: Generate data from MLR or MARS DGPs, compute 95% CI coverage for EL and mEL across n ∈ {200, 400, 800} with Large N (α=1.2) and Small N (α=1.1). Verify mEL achieves nominal coverage while standard EL over-covers under Small N.
  2. Sensitivity to N_n: Fix n=400, vary N_n from 10n to 100n, plot coverage vs. N_n for both methods to identify crossover where standard EL becomes calibrated.
  3. Comparison to IJ variance: Compare mEL intervals to Wald-type intervals using infinitesimal jackknife variance on same DGPs; assess coverage accuracy and interval width tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
Does the modified empirical likelihood (mEL) framework remain asymptotically pivotal for adaptive random forests (standard Breiman forests) where the "honesty" condition is violated? The theoretical validity of the method explicitly relies on Assumption 4 (Honesty) to ensure the bias bounds and variance ratio regularity conditions hold. Breiman's original forests use response variables for splitting, which violates the independence structure required for the honest forest theory utilized in the proofs.

### Open Question 2
Can the EL framework be extended to explicitly correct for estimation bias in settings where the bias of the forest estimator θ_n relative to the target θ_0 is non-negligible? Section 4 notes that θ_0 is used in the likelihood "owing to the bias bound established in Wager and Athey (2018)", implicitly assuming the bias vanishes asymptotically. If the subsample size s grows slowly or the true regression function is highly complex, the bias may not vanish, leading to shifted confidence regions that fail to cover θ_0 at the nominal rate.

### Open Question 3
How does the jackknife-after-subsampling EL extend to multivariate parameters (vector-valued θ), and does the adjustment for sparse subsampling remain effective? The paper restricts the parameter of interest θ_n to a scalar (Θ ⊂ ℝ) in Section 2, leaving the multivariate case unaddressed. Multivariate empirical likelihood involves a vector of Lagrange multipliers, and it is unclear if the specific correction for incomplete U-statistics generalizes to maintain pivotality in higher dimensions.

## Limitations
- The incomplete U-statistic framework depends critically on the dense-subsampling condition n/N_n · ζ_{n,s}/(s·ζ_{n,1}) = o(1), with no empirical guidance on identifying this threshold in practice
- The theoretical validity of the method explicitly relies on honesty assumptions that may not hold for standard adaptive random forests
- Extension to multivariate parameters θ ∈ ℝᵏ is not addressed, as the current theory covers only univariate case

## Confidence
- **High confidence**: Theoretical validity of mEL under dense subsampling (Theorem 1), correctness of jackknife-after-subsampling construction (Section 2.1), and simulation results showing mEL achieves nominal coverage under sparse regimes (Table 1, Figures 2-3)
- **Medium confidence**: General applicability to non-honest forests given Theorem 4's specialized assumptions; practical calibration of subsampling intensity N_n without theoretical guidance
- **Low confidence**: Extension to multivariate parameters θ ∈ ℝᵏ (current theory covers univariate case), and performance under extreme high-dimensional settings (d ≫ n)

## Next Checks
1. **Subsampling calibration experiment**: Systematically vary N_n across a wide range (10n to 100n) for fixed n, plot coverage probability of standard EL vs. mEL to empirically identify the crossover point where pivotality breaks down.

2. **Higher-dimensional parameter test**: Extend the simulation framework to construct joint confidence regions for θ = (θ₁, θ₂) and evaluate whether the χ²_k calibration remains valid under the mEL modification.

3. **Extreme nonlinearity stress test**: Generate data from a highly discontinuous function (e.g., step function with narrow jump regions) and assess whether Assumption 2 holds empirically, measuring the magnitude of higher-order Hoeffding projections.