---
ver: rpa2
title: 'UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model
  and Diffusion Model Experts'
arxiv_id: '2510.17937'
source_url: https://arxiv.org/abs/2510.17937
tags:
- image
- generation
- arxiv
- unified
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniRL-Zero, a unified reinforcement learning
  framework that jointly optimizes a multimodal language model and a diffusion model
  within a single architecture. It defines six reinforcement learning scenarios covering
  text understanding, multimodal reasoning, text-to-image generation, instructional
  image editing, chain-of-thought-enhanced generation, and reflective image generation.
---

# UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts

## Quick Facts
- arXiv ID: 2510.17937
- Source URL: https://arxiv.org/abs/2510.17937
- Reference count: 40
- Introduces a unified RL framework jointly optimizing a multimodal LM and diffusion model within a single architecture.

## Executive Summary
UniRL-Zero presents a unified reinforcement learning framework that jointly optimizes a multimodal language model (Qwen2.5-VL) and a diffusion model (SANA-1.6B) within a single architecture. The framework addresses six reinforcement learning scenarios covering text understanding, multimodal reasoning, text-to-image generation, instructional image editing, chain-of-thought-enhanced generation, and reflective image generation. Using a unified policy that combines discrete LM actions and continuous DM denoising steps, the approach leverages Group Relative Policy Optimization (GRPO) to improve performance across both modalities. Experiments demonstrate strong results on multimodal understanding benchmarks (MM-Vet 63.2) and generation tasks, validating the feasibility of end-to-end reinforcement learning in unified multimodal models.

## Method Summary
The method employs a unified architecture integrating a frozen multimodal LM (Qwen2.5-VL 3B) with a diffusion model expert (SANA-1.6B) connected via learnable meta-query tokens and a bidirectional connector transformer. The diffusion model uses a linear transformer architecture with v-prediction loss and sigmoid-sampled timesteps. Reinforcement learning is performed using Group Relative Policy Optimization (GRPO), which computes advantages by normalizing rewards within sampled trajectory groups. A cold-start fine-tuning phase first adapts the DM on 10K targeted samples before joint RL training. The unified policy combines discrete LM actions and continuous DM denoising steps, optimized end-to-end with joint surrogate losses and KL regularization against a reference policy.

## Key Results
- Achieves strong performance on multimodal understanding (MM-Vet 63.2) and generation tasks.
- RL training improves text-to-image generation quality and alignment with prompts.
- Enhanced reasoning-based prompts through chain-of-thought optimization.
- Enables reflection-based error correction for image generation.

## Why This Works (Mechanism)
The framework's effectiveness stems from jointly optimizing both modalities within a unified architecture rather than treating them as separate components. By formulating the diffusion denoising process as a reversal of a stochastic differential equation, the DM's continuous actions become compatible with the RL framework. The meta-query tokens and connector transformer enable seamless information flow between the frozen LM and the trainable DM, allowing the unified policy to leverage both reasoning capabilities and generative strength. GRPO's group-relative advantage computation eliminates the need for a separate value function while maintaining stable learning dynamics across both modalities.

## Foundational Learning

- **Concept: Unified Model Architecture**
  - **Why needed here:** This is the foundational substrate of UniRL-Zero. It's not a standard LLM or a diffusion model alone, but a single system containing both an LM expert (Qwen2.5-VL) and a DM expert (SANA-1.6B), connected via learnable "meta-query tokens" and a connector transformer. All RL mechanisms operate on this integrated structure.
  - **Quick check question:** Can you identify which part of the unified model handles the discrete reasoning and which part handles the continuous image generation?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** This is the core RL algorithm adapted for this task. Unlike standard PPO, GRPO computes advantages by sampling a group of trajectories for a given query and normalizing their rewards relative to the group mean and standard deviation. This avoids training a separate value function critic.
  - **Quick check question:** How does GRPO compute the advantage for a specific trajectory within a group?

- **Concept: Diffusion SDE (Stochastic Differential Equation) Sampling**
  - **Why needed here:** The paper's mechanism for applying RL to the diffusion model involves formulating the denoising process as a reversal of an SDE. Understanding this is key to grasping how the DM's "actions" (denoising steps) are defined and optimized.
  - **Quick check question:** In this formulation, what corresponds to the "action" in the diffusion model's RL process?

## Architecture Onboarding

- **Component map:**
  - Pretrained text-image pairs (1.8M) + instructional editing pairs (1.3M) -> Unified architecture: Qwen2.5-VL 3B (frozen LM) + SANA-1.6B (DM) + DC-VAE encoder + meta-query tokens + bidirectional connector transformer -> GRPO optimizer -> Improved multimodal capabilities

- **Critical path:**
  1. **Data Curation:** Create datasets for the target scenario (e.g., GenEval prompts for T2I, 200 curated samples with reverse instructions for Cycle Edit RL).
  2. **Cold Start Fine-tuning:** **Crucial first step.** Before RL, the DM is fine-tuned on a small, targeted dataset (e.g., 10k CoT-enhanced pairs) while the LM is frozen. This bootstraps the model's capability for the complex scenario.
  3. **RL Training Loop:**
      - **Forward Pass:** Sample a group of G trajectories (text -> reasoning -> image). For Cycle Edit RL, this includes the forward and reverse edit cycle.
      - **Reward Computation:** Calculate rewards based on the task (e.g., CLIP-based alignment for T2I, cycle consistency for editing, judge/correction accuracy for reflection).
      - **Advantage Calculation:** Normalize rewards within the group to get advantages.
      - **Loss & Backprop:** Compute clipped surrogate losses for both the LM and DM based on the shared advantages and update weights end-to-end.

- **Design tradeoffs:**
  - **Joint vs. Modular Optimization:** The paper argues for joint end-to-end optimization of LM and DM experts, claiming it leverages their synergistic strengths. The tradeoff is increased training complexity and potential for instability compared to training components in isolation.
  - **Reward Model Fidelity:** The authors use proxy rewards like GenEval scores, CLIP similarity, and even JPEG compressibility. These are imperfect proxies for true human preference. The tradeoff is between a simple, automated signal and a more complex, expensive, and potentially biased human feedback loop.
  - **Freezing the LM:** The "Cold Start" phase freezes the LM to preserve its reasoning capabilities. This simplifies training but may limit the extent to which reasoning can be adapted to the generative task.

- **Failure signatures:**
  - **Catastrophic Forgetting:** If the KL penalty is too weak or RL steps are too aggressive, the model may lose its base capabilities (e.g., language understanding degrades while image generation improves).
  - **Reward Hacking:** The model may learn to generate outputs that maximize the proxy reward (e.g., high CLIP similarity) without being semantically meaningful or perceptually high-quality.
  - **Instability in Joint Training:** Losses from the LM and DM may conflict, leading to unstable training or one modality dominating the other.

- **First 3 experiments:**
  1. **Sanity Check (JPEG Rewards):** Replicate the JPEG compressibility/incompressibility experiment (Section 5.2). This validates the basic GRPO pipeline and the model's ability to optimize a simple, opposing reward signal. Success is a clear, monotonic change in image file size.
  2. **Text-to-Image Alignment:** Run a small-scale RL experiment on the GenEval benchmark using a CLIP-based or GenEval reward. Compare the "Overall" and specific attribute scores against the base model to confirm improvements in alignment and compositionality.
  3. **Instructional Editing Pilot:** Test the Cycle Edit RL pipeline on a small set of curated image-instruction pairs. Measure the trade-off between CLIP image similarity and CLIP text-image direction similarity to verify the model is learning to follow instructions while preserving the original image, as shown in Figure 7.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the performance improvements of UniRL-Zero scale effectively to significantly larger model architectures (e.g., 7B+ parameters) and expanded training datasets?
  - **Basis in paper:** [explicit] The authors state that due to limited computational resources, experiments were conducted at a "relatively modest scale" in terms of data volume and model size, potentially underestimating the framework's potential.
  - **Why unresolved:** The current study validates the framework using a 1.6B parameter model, leaving the dynamics of large-scale joint optimization unexplored.
  - **What evidence would resolve it:** Demonstrating consistent or improved relative gains when applying the GRPO-based joint optimization to larger unified models trained on million-scale RL datasets.

- **Open Question 2:** How can reward functions be refined to capture complex visual attributes like scene geometry and long-range coherence without introducing optimization bias?
  - **Basis in paper:** [explicit] The limitations section notes that current rewards, such as CLIP-based alignment and GenEval metrics, do not cover all quality aspects (e.g., scene geometry, fine-grained attributes), suggesting more diverse rewards are needed.
  - **Why unresolved:** RL optimization currently relies on proxies that may ignore structural errors or subtle visual flaws not represented in the reward signal.
  - **What evidence would resolve it:** Identifying or constructing a multi-modal reward model that successfully penalizes geometric inconsistencies during joint LM-DM training.

- **Open Question 3:** Can the Language Model expert be updated directly via RL without suffering from catastrophic forgetting of its reasoning capabilities?
  - **Basis in paper:** [inferred] The authors note that in reflection tasks, "computing autoregressive (AR) loss for the LM leads to rapid degradation," leading them to freeze the LM during specific cold-start phases.
  - **Why unresolved:** It is unclear if the unified policy $\pi_\theta$ can be optimized end-to-end without restricting updates to the diffusion expert to preserve the LM's base intelligence.
  - **What evidence would resolve it:** A training curriculum where $\theta_{LM}$ is updated jointly with $\theta_{DM}$ and subsequently maintains or improves performance on text-only reasoning benchmarks.

## Limitations

- Several key hyperparameters for GRPO (group size, learning rates, KL penalty) and reward function weights are underspecified, limiting reproducibility.
- The framework relies on proxy rewards (GenEval scores, CLIP similarity) rather than direct human feedback, introducing uncertainty about true perceptual quality improvements.
- Performance gains are demonstrated on a relatively small 1.6B parameter model, leaving questions about scalability to larger architectures unanswered.

## Confidence

- **High Confidence:** The core technical approach—unifying a frozen multimodal LM with a diffusion expert and applying GRPO for joint optimization—is clearly described and internally consistent.
- **Medium Confidence:** The reported quantitative improvements on GenEval and multimodal benchmarks are plausible given the described methodology, but full verification requires access to all hyperparameters and exact reward implementations.
- **Low Confidence:** Claims about the superiority of joint end-to-end RL over modular or sequential fine-tuning are supported by ablation results, but lack direct comparisons to other unified RL methods or specialized pipelines.

## Next Checks

1. **Cold Start Fine-tuning Validation:** Replicate the cold-start DM-only fine-tuning on the 10K CoT or reflection pairs (LM frozen). Measure the baseline capability of the DM before RL (e.g., prompt following, generation quality) to confirm this step is necessary and effective for bootstrapping complex scenarios.

2. **Reward Function Ablation for Cycle Edit RL:** Systematically vary the λ₁, λ₂, λ₃ weights in the Cycle Edit RL reward (R(τ) = λ₁R_edit + λ₂R_cycle + λ₃R_quality) and measure the trade-off between instruction-following (text-image similarity) and image preservation (image similarity). This will clarify the importance of each reward component and the robustness of the editing capability.

3. **GRPO Hyperparameter Sensitivity:** Conduct a small-scale ablation study on key GRPO hyperparameters (e.g., group size G, KL penalty β, clip ratio ϵ) using a fixed scenario (e.g., T2I on GenEval). Measure the impact on convergence speed, final reward, and cross-modal stability to identify the most sensitive parameters and optimal ranges.