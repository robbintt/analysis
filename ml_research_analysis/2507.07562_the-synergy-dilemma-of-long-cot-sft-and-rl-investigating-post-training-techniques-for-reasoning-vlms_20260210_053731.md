---
ver: rpa2
title: 'The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques
  for Reasoning VLMs'
arxiv_id: '2507.07562'
source_url: https://arxiv.org/abs/2507.07562
tags:
- reasoning
- training
- arxiv
- accuracy
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the synergy between long chain-of-thought
  supervised fine-tuning (SFT) and reinforcement learning (RL) for enhancing reasoning
  capabilities in large vision-language models (VLMs). The study systematically explores
  how these two post-training techniques affect model performance across varying question
  difficulties.
---

# The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs

## Quick Facts
- arXiv ID: 2507.07562
- Source URL: https://arxiv.org/abs/2507.07562
- Reference count: 9
- Primary result: SFT and RL produce opposing effects on reasoning VLMs - SFT degrades simple question performance while improving complex ones, and RL provides consistent gains across all difficulty levels, but no combination strategy achieves true synergy.

## Executive Summary
This paper investigates the combination of long chain-of-thought supervised fine-tuning (SFT) and reinforcement learning (RL) for enhancing reasoning capabilities in large vision-language models (VLMs). The study systematically explores how these two post-training techniques affect model performance across varying question difficulties, revealing that they produce distinct trade-offs rather than synergistic gains. While SFT improves performance on harder questions through structured reasoning but degrades accuracy on simpler ones due to verbosity, RL provides consistent gains across all difficulty levels through concise responses. The paper explores various strategies to combine SFT and RL including two-stage, interleaved, progressive training, data mixing, and model merging, but finds that none produce true synergy.

## Method Summary
The study employs Qwen2.5-VL-7B with frozen visual encoders and investigates long-CoT SFT and RL using datasets s1/s1.1-R1 (1k textual samples distilled from DeepSeek-R1) and Eureka-Distill (34k multimodal samples constructed via self-distillation). SFT is performed with lr=1e-5, batch=32, 5-15 epochs, while RL uses GRPO with lr=1e-6, batch=128, KL=0.005, and a composite reward function. Models are evaluated across five benchmarks (MathVision, MathVerse, MathVista, MMMU val, MMStar val) with difficulty binning based on baseline pass rates. The evaluation uses temp=0.6, top_p=0.95, top_k=20, max_gen=24k, with 4-run averages.

## Key Results
- SFT degrades accuracy on L1-L3 questions (-2.8% to -3.5%) but improves L4-L5 questions (+4.1% to +10.1%) due to verbosity trade-offs
- RL provides consistent gains across all difficulty levels (+1.1% to +3.8%) through concise responses and maintains the shortest reasoning traces
- None of the combination strategies (two-stage, interleaved, progressive, data mixing, model merging) achieve true synergy between SFT and RL
- Response length and reasoning token frequency differ significantly: SFT increases verbosity, RL maintains conciseness, and combinations fall between these extremes

## Why This Works (Mechanism)
The paper reveals that SFT and RL operate through fundamentally different mechanisms in reasoning VLMs. SFT teaches structured, step-by-step reasoning patterns that excel at complex problems but introduce verbosity that hurts simple question performance. RL, through GRPO optimization, learns to generate concise responses that generalize well across difficulty levels. The "synergy dilemma" emerges because these mechanisms conflict - SFT's verbose structured reasoning interferes with RL's efficiency optimization, while RL's concise approach undermines the structured logic that SFT builds.

## Foundational Learning
- **Chain-of-Thought reasoning**: Step-by-step logical decomposition needed for complex multimodal problems; quick check: verify models can break down compound questions into sequential steps
- **Difficulty-level binning**: Classification of questions by baseline pass rates (L1-L5); quick check: ensure baseline distributions are stable across multiple runs
- **Self-distillation**: Creating training data by generating responses from SFT-ed models; quick check: verify self-generated data quality and consistency
- **GRPO optimization**: Gradient-based RL with KL penalty for stable training; quick check: monitor KL divergence to prevent reward hacking
- **Multimodal reasoning**: Integration of visual and textual understanding for problem-solving; quick check: test cross-modal alignment on mixed input types
- **Reward shaping**: Composite rewards balancing accuracy and format compliance; quick check: verify reward weights produce desired response characteristics

## Architecture Onboarding

**Component Map**: Qwen2.5-VL-7B (frozen ViT+MLP) -> SFT/RL Training Loop -> Evaluation Pipeline -> Benchmark Classification

**Critical Path**: Dataset Preparation -> SFT Training -> RL Training -> Difficulty-Based Evaluation -> Analysis

**Design Tradeoffs**: SFT prioritizes structured reasoning accuracy vs. verbosity; RL prioritizes generalization vs. detailed logic; combinations struggle to balance both

**Failure Signatures**: SFT collapse shows overfitting with poor simple question performance; RL collapse shows reward hacking or length spikes; combination failure shows regression on either easy or hard questions

**First Experiments**:
1. Baseline evaluation of Qwen2.5-VL-7B on difficulty-binned benchmarks to establish performance distributions
2. Isolated SFT training to measure accuracy changes across difficulty levels and response length impacts
3. Isolated RL training to establish baseline gains and response characteristics for comparison

## Open Questions the Paper Calls Out

**Open Question 1**: How can VLMs be equipped with adaptive frameworks to dynamically identify problem difficulty and switch between concise reasoning and verbose long-CoT? The conclusion states future research should prioritize "developing adaptive frameworks capable of accurately identifying problem difficulty, selecting optimal reasoning modes, and avoiding interference." This remains unresolved as current hybrid strategies apply training signals uniformly or heuristically, failing to discriminate between easy and hard inputs effectively.

**Open Question 2**: Can self-distilled or model-compatible datasets resolve the "synergy dilemma" by mitigating the catastrophic forgetting caused by external SFT data? The authors suggest "constructing model-compatible or self-distilled long-CoT datasets to mitigate data-model incompatibility or catastrophic forgetting." This is unresolved as the study shows external SFT data forces the model into rigid, verbose patterns that hurt generalization, which subsequent RL cannot fully correct.

**Open Question 3**: Can the trade-off between "structured reasoning" and "verbosity" be decoupled via length-penalized SFT? Section 2.3 identifies verbosity as the driver of performance degradation on easy questions, while noting SFT's unique ability to teach structured, step-by-step logic. It remains unclear if the "reasoning tokens" necessitate long responses, or if concise structured reasoning is learnable.

## Limitations
- The study is limited to specific model architectures (Qwen2.5-VL variants) and may not generalize to other VLM families
- Difficulty-binning methodology relies on baseline pass rates that may not capture true question complexity across different model families
- Self-distillation process for Eureka-Distill introduces potential biases that could affect RL training dynamics
- The search space for combination strategies was not exhaustive, so the claim of "no synergy" may be premature

## Confidence

**High confidence**: Core empirical finding that SFT and RL produce opposing effects on different difficulty levels is robust and reproducible
**Medium confidence**: Claim that no combination strategy achieves synergy is based on tested approaches but the search space was not exhaustive
**Low confidence**: Generalization of findings to other VLMs, reasoning tasks, or training paradigms beyond the specific benchmarks studied

## Next Checks

1. **Dataset composition validation**: Verify the exact composition and quality of the s1/s1.1-R1 and Eureka-Distill datasets, including their coverage across difficulty levels and potential biases

2. **Alternative combination strategy exploration**: Systematically test adaptive training schedules that dynamically switch between SFT and RL modes based on real-time difficulty estimation

3. **Cross-model generalization**: Replicate the core experiments on alternative VLM architectures (e.g., LLaVA, InternVL) to determine whether the observed trade-offs are model-specific or represent general phenomena