---
ver: rpa2
title: "B\xFCy\xFCk Dil Modelleri i\xE7in TR-MMLU Benchmark\u0131: Performans De\u011F\
  erlendirmesi, Zorluklar ve \u0130yile\u015Ftirme F\u0131rsatlar\u0131"
arxiv_id: '2508.13044'
source_url: https://arxiv.org/abs/2508.13044
tags:
- tr-mmlu
- benchmark
- gibi
- gerlendirme
- veri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces TR-MMLU, a Turkish benchmark for evaluating
  large language models (LLMs) on Turkish language understanding. Built on 6,200 multiple-choice
  questions from the Turkish education system across 62 categories, TR-MMLU addresses
  the lack of culturally and linguistically appropriate evaluation resources for Turkish
  NLP.
---

# Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları

## Quick Facts
- arXiv ID: 2508.13044
- Source URL: https://arxiv.org/abs/2508.13044
- Reference count: 0
- Key outcome: Turkish-specific MMLU benchmark reveals significant performance gaps for current LLMs, with GPT-4o achieving 84.84% accuracy while open models like Llama3.3 reach 79.42%

## Executive Summary
TR-MMLU is a Turkish benchmark designed to evaluate large language models on Turkish language understanding using 6,200 multiple-choice questions across 62 categories from the Turkish education system. The benchmark addresses the lack of culturally and linguistically appropriate evaluation resources for Turkish NLP by providing questions in their original form rather than through translation. When tested on 39 state-of-the-art models including GPT-4o, Claude-3.5, Llama3.3, and Gemma2, results show significant performance variations with GPT-4o achieving the highest accuracy at 84.84%. The study highlights challenges related to Turkish's agglutinative morphology and identifies opportunities for improved tokenization strategies and fine-tuning approaches.

## Method Summary
The evaluation methodology uses a Python script running on the Ollama platform with random seed=42 to assess model responses to TR-MMLU questions. Multiple Turkish prompt templates were tested, and responses were normalized using the "paraphrase-multilingual-mpnet-base-v2" model for semantic similarity matching rather than exact string comparison. The benchmark covers diverse categories including law, health, history, arts, TUS (medical specialization), KPSS (public personnel selection), driving exams, and AÖF (open education faculty) exams. Processing time per model was also measured as an additional evaluation dimension.

## Key Results
- GPT-4o achieved the highest accuracy at 84.84% among all tested models
- Llama3.3 reached 79.42% accuracy, outperforming other open models
- Performance varied significantly across categories, with models excelling in driving exams but facing challenges in complex domains like medical and legal
- Turkish's agglutinative structure caused tokenization problems, with models using advanced tokenization strategies performing better
- Fine-tuning on Turkish datasets improved performance but led to catastrophic forgetting of previously learned knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Turkish-specific benchmark design reduces cultural and linguistic mismatch compared to translated benchmarks.
- **Mechanism:** Questions sourced directly from the Turkish education system preserve native phrasing, cultural context, and curriculum-aligned knowledge distribution. This avoids translation artifacts that can obscure model capabilities in agglutinative languages where word order and suffixes carry dense semantic information.
- **Core assumption:** Translation-based benchmarks systematically under- or misrepresent Turkish LLM capabilities due to linguistic distortion rather than genuine knowledge gaps.
- **Evidence anchors:** [abstract] "TR-MMLU is based on a meticulously curated dataset comprising 6,200 multiple-choice questions across 62 sections within the Turkish education system." [Section II] "TR-MMLU, Türkçe için özgün olarak tasarlanmış... çeviri hataları ve kültürel uyumsuzlukları ortadan kaldırır."

### Mechanism 2
- **Claim:** Tokenization strategies adapted to Turkish morphology improve accuracy on knowledge-intensive tasks.
- **Mechanism:** Turkish's agglutinative structure allows single verb roots to encode tense, person, mood, and other features via suffixes. Subword tokenizers (BPE, WordPiece) trained on non-Turkish corpora fragment these morphologically-rich forms into semantically meaningless pieces, degrading comprehension.
- **Core assumption:** Tokenization quality directly affects downstream task performance rather than being compensated for by larger model capacity.
- **Evidence anchors:** [Section I] "Türkçede bir fiil kökü... sayısız kelime formu oluşturabilir. Bu çeşitlilik, hem tokenizasyon hem de anlamsal çözümlemeyi karmaşık hale getirir." [Section IV] "Türkçe morfolojisine uygun güçlü tokenizasyon stratejilerine sahip modellerin diğerlerinden daha iyi performans gösterdiğini ortaya koymuştur."

### Mechanism 3
- **Claim:** Prompt phrasing significantly affects Turkish model accuracy even for identical task structures.
- **Mechanism:** Turkish syntax and pragmatics interact with instruction phrasing. Minor rewordings (e.g., "sadece hangi seçeneğin doğru olduğunu yaz" vs. "sadece doğru şıkkın harfini söyle") alter how models parse task constraints, affecting output format compliance and answer selection.
- **Core assumption:** Observed accuracy differences stem from prompt comprehension rather than model randomness or question ordering effects.
- **Evidence anchors:** [Section IV] Gemma2:9b scored 63 vs. 55 correct across four prompt variants; Llama3.1 ranged 33-47 correct on same questions.

## Foundational Learning

- **Concept: Agglutinative Morphology**
  - **Why needed here:** Turkish suffixes stack to create words carrying dense grammatical and semantic information. Understanding this explains why standard tokenizers fail and why Turkish NLP requires specialized approaches.
  - **Quick check question:** Given the Turkish word "görüştürülebilmekteydik," can you identify at least three morphological features encoded in its suffixes?

- **Concept: Subword Tokenization (BPE/WordPiece)**
  - **Why needed here:** The paper identifies tokenization as a core bottleneck. You need to understand how BPE splits text into subword units and why morphological richness causes over-fragmentation.
  - **Quick check question:** How would BPE tokenize "anlayamadıklarımızdanmışçasına" if trained only on English text versus Turkish text?

- **Concept: Catastrophic Forgetting in Fine-tuning**
  - **Why needed here:** The paper notes fine-tuning on Turkish improves performance but risks losing previously learned capabilities. This tradeoff informs practical deployment decisions.
  - **Quick check question:** If you fine-tune a multilingual LLM on Turkish legal documents, what specific capabilities might degrade, and how would you measure that degradation?

## Architecture Onboarding

- **Component map:** TR-MMLU Dataset (6,200 MCQs, 62 categories) -> Ollama inference (Python evaluation script, seed=42) -> Response normalization (paraphrase-multilingual-mpnet-base-v2) -> Accuracy computation per category + aggregate -> Hugging Face leaderboard publication (3 datasets)

- **Critical path:** 1. Question formatting: Ensure MCQ structure (question + options A-D) matches prompt template 2. Response extraction: Handle paraphrased answers via semantic similarity, not exact string match 3. Category-wise analysis: TUS, KPSS, driving, and academic categories require separate tracking

- **Design tradeoffs:** Closed vs. open models: GPT-4o (84.84%) and Claude-3.5 (84.40%) outperform open models like Llama3.3 (79.42%), but open models enable controlled experiments and fine-tuning. Prompt selection: No single prompt dominates; testing multiple prompts increases evaluation cost but reveals robustness. Paraphrase matching vs. exact match: Semantic matching handles response variation but may accept edge cases as correct.

- **Failure signatures:** Models output explanations instead of letter options -> prompt constraint failure. Performance drops sharply on TUS (medical specialization) vs. driving exams -> domain knowledge gap. Fine-tuned models score high on TR-MMLU but fail general tasks -> catastrophic forgetting

- **First 3 experiments:** 1. Baseline replication: Run evaluation script on TR-MMLU with GPT-4o and Llama3.3; verify accuracy numbers match reported values (84.84% and 79.42%) using identical seed and prompts. 2. Tokenizer ablation: Compare same model family with Turkish-optimized vs. generic tokenizer on 500-question subset; measure accuracy delta and token count per question. 3. Prompt robustness test: Evaluate 3 models across all 4 documented prompts; compute variance and identify systematically harder/easier prompt formulations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can tokenization techniques be optimized specifically for Turkish's agglutinative morphology to improve LLM accuracy on knowledge-intensive benchmarks?
- **Basis in paper:** [explicit] The conclusion states that Turkish's agglutinative structure and complex morphology cause tokenization problems, and "future work should focus on advanced tokenization techniques addressing Turkish-specific linguistic complexities."
- **Why unresolved:** The paper identifies the problem and correlation with lower accuracy but does not propose or test specific tokenization solutions.
- **What evidence would resolve it:** A comparative study evaluating Turkish-specific tokenizers (e.g., morpheme-aware or BPE variants optimized for Turkish) against standard tokenizers, measuring accuracy gains on TR-MMLU.

### Open Question 2
- **Question:** What fine-tuning strategies can mitigate catastrophic forgetting when adapting models to Turkish-specific tasks?
- **Basis in paper:** [explicit] The paper reports that while fine-tuning on Turkish datasets improves performance, it leads to catastrophic forgetting where previously learned knowledge is lost, and calls for investigating more robust methods.
- **Why unresolved:** The phenomenon is observed but no solutions (e.g., replay buffers, parameter-efficient methods) are tested.
- **What evidence would resolve it:** Experiments comparing standard fine-tuning with techniques like LoRA or elastic weight consolidation, measuring both TR-MMLU performance and retention on general benchmarks.

### Open Question 3
- **Question:** What underlying factors explain performance disparities across TR-MMLU categories (e.g., driving exams vs. medical exams)?
- **Basis in paper:** [explicit] The conclusion notes that "performance differences observed in different categories require deeper investigation of the underlying causes of these variations."
- **Why unresolved:** The paper reports category-level scores but does not analyze correlations with linguistic complexity, domain specificity, or question format.
- **What evidence would resolve it:** A regression analysis linking category performance to features such as morphological complexity, vocabulary overlap with pretraining data, or reasoning depth required.

## Limitations
- Benchmark generalization: TR-MMLU's predictive validity for real-world Turkish NLP tasks remains untested, as the dataset originates from standardized educational materials
- Morphological tokenization claims: The paper asserts Turkish-specific tokenization significantly improves performance, but this relationship is inferred rather than experimentally validated
- Prompt sensitivity mechanisms: Observed variance in model performance across different prompt formulations is documented but not systematically analyzed

## Confidence
- **High confidence:** The benchmark construction methodology and dataset characteristics (6,200 questions, 62 categories, Turkish educational source) are well-documented and reproducible
- **Medium confidence:** The comparative performance rankings of models on TR-MMLU (GPT-4o > Claude-3.5 > Llama3.3) appear robust, though absolute accuracy values depend on evaluation parameters
- **Low confidence:** Claims about tokenization improvements and prompt sensitivity lack experimental validation within the paper itself

## Next Checks
1. **Cross-benchmark validation:** Test whether TR-MMLU performance correlates with performance on existing Turkish NLP benchmarks (Cetvel, TrGLUE) and whether it predicts success on real-world Turkish language tasks outside the educational domain
2. **Tokenization ablation study:** Systematically compare identical model architectures with Turkish-optimized versus generic tokenizers across a representative sample of TR-MMLU questions, measuring both accuracy and tokenization efficiency (tokens per question)
3. **Prompt sensitivity analysis:** Conduct controlled experiments varying single prompt elements (instruction phrasing, format requirements, tone) while holding questions constant, using multiple models to distinguish systematic effects from random variation