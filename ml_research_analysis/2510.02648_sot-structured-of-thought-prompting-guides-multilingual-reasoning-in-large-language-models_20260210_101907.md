---
ver: rpa2
title: 'SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large
  Language Models'
arxiv_id: '2510.02648'
source_url: https://arxiv.org/abs/2510.02648
tags:
- reasoning
- language
- multilingual
- llms
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Structured-of-Thought (SoT), a training-free
  method to improve multilingual reasoning in large language models. SoT addresses
  the challenge of transferring reasoning capabilities to non-high-resource languages
  by converting language-specific semantic information into language-agnostic structured
  representations through Language Thinking Transformation and Structured Knowledge
  Transformation.
---

# SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2510.02648
- Source URL: https://arxiv.org/abs/2510.02648
- Reference count: 30
- Primary result: Training-free method improves multilingual reasoning, achieving up to 85.4% accuracy on MSV AMP and 76.5% on MGSM

## Executive Summary
This paper introduces Structured-of-Thought (SoT), a training-free method that improves multilingual reasoning in large language models by converting language-specific semantic information into language-agnostic structured representations. SoT addresses the challenge of transferring reasoning capabilities to non-high-resource languages through a four-step prompting approach: Language Thinking Transformation, Structured Knowledge Extraction, Language-Specific Knowledge Injection, and Answer Generation. The method guides models to maintain consistent reasoning pathways across languages by extracting structured knowledge and incorporating language-specific knowledge, outperforming several strong baselines on multiple multilingual reasoning benchmarks when adapting to various LLMs.

## Method Summary
SoT is a four-step zero-shot prompting method that transforms language-specific queries into language-agnostic structured representations. The approach consists of: (1) Language Thinking Transformation - converting the reasoning process from low-resource to high-resource languages via cross-lingual transfer; (2) Structured Knowledge Extraction - performing NER-based entity/relationship extraction to create a structured intermediate representation; (3) Language-Specific Knowledge Injection - applying language-specific cultural/linguistic conventions to correct misinterpretations; and (4) Answer Generation - producing the final answer in the source language. The method is compatible with other training-free strategies like Chain-of-Thought and few-shot learning, and has been evaluated on mathematical and commonsense reasoning tasks across 11+ languages.

## Key Results
- Achieves 85.4% average accuracy on MSV AMP and 76.5% on MGSM benchmarks
- Outperforms several strong baselines on multiple multilingual reasoning benchmarks
- Step 2 (Structured Knowledge Extraction) and Step 3 (Language-Specific Knowledge Injection) are the most impactful components
- Compatible with various LLMs including gpt-3.5-turbo, Qwen2.5-7B-Instruct, and DeepSeek-R1-7B

## Why This Works (Mechanism)

### Mechanism 1: Language Thinking Transformation
Transferring reasoning pathways from lower-resource languages to English leverages LLMs' stronger proficiency in high-resource languages, improving comprehension before structured extraction. Given input X in source language Ls, the model generates intermediate reasoning steps R in English via implicit cross-lingual transfer, avoiding error accumulation from external translators. This operates as implicit cross-lingual transfer rather than explicit translation.

### Mechanism 2: Structured Knowledge Extraction via NER
Extracting entity-relationship patterns into explicit structured representations reduces noise and makes numerical/semantic relationships unambiguous across languages. The model performs Named Entity Recognition to identify values, units, and relationships, outputting structured knowledge K that creates a language-agnostic intermediate representation separating semantic understanding from surface linguistic variation.

### Mechanism 3: Language-Specific Knowledge Injection
Explicitly injecting language-specific cultural/linguistic conventions corrects misinterpretations that persist after English transfer and structured extraction. After structured extraction, the model applies language-specific knowledge KLs to revise interpretations (e.g., correcting "四五折" from 45% to 55% discount in Chinese), leveraging the model's latent language-specific knowledge through targeted prompting.

## Foundational Learning

- **Cross-lingual transfer in multilingual LLMs**: Why needed here - SoT relies on the premise that reasoning capability transfers across languages better than surface comprehension. Quick check: Can you explain why translating a query to English and reasoning in English might yield different results than reasoning directly in a lower-resource language?

- **Chain-of-Thought prompting and its variants**: Why needed here - SoT is positioned as a structured enhancement to CoT, compatible with but distinct from standard step-by-step reasoning. Quick check: What specific failure modes does standard CoT exhibit in multilingual mathematical reasoning, as shown in Figure 6?

- **Named Entity Recognition and information extraction**: Why needed here - Step 2 explicitly instructs LLMs to perform NER-like extraction. Quick check: In the example "0.75 cakes per guest" vs "1/4 of guests will not attend," what entities and relationships would NER identify, and how do they semantically align?

## Architecture Onboarding

- **Component map**: Input → Language Thinking (English reasoning) → Structured Knowledge (NER extraction) → Language-Specific Injection (cultural corrections) → Answer Generation

- **Critical path**: Steps 2 and 3 are the most impactful (Strategy 7 achieves 61.6%). Step 1 provides marginal improvement. For minimal viable implementation, prioritize structured extraction + language-specific injection.

- **Design tradeoffs**: Thinking vs. Translation (Step 1): Thinking is more robust but requires model's intrinsic cross-lingual capability; Explicit structure vs. implicit reasoning: Structured extraction adds ~0.1s decoding time but significantly improves accuracy.

- **Failure signatures**: Unit/convention confusion (e.g., "per dozen" vs "per egg"), relationship parsing failures (e.g., misidentifying entity ownership), and double-counting conditions (e.g., treating semantically equivalent constraints as separate).

- **First 3 experiments**:
  1. Ablation validation: Run all 8 strategy combinations on MGSM with gpt-3.5-turbo to verify Step 2+3 is the critical path
  2. Thinking vs. Translation comparison: Replicate Figure 5 analysis comparing implicit thinking, LLM translation, and external translator across 3-5 languages
  3. Language-specific knowledge stress test: Construct 20 questions per language with unambiguous language-specific conventions to isolate Step 3's contribution

## Open Questions the Paper Calls Out

1. **Optimal target language for thinking transformation**: How can the optimal target language for the "Language Thinking Transformation" step be dynamically determined for LLMs that are not English-centric? The authors acknowledge that selecting the target language remains an urgent issue, noting that some LLMs perform better in languages other than English.

2. **Naturally occurring datasets**: Does the "Language-Specific Knowledge Injection" step yield significant gains on naturally occurring datasets compared to the machine-translated benchmarks currently used? The authors acknowledge that existing multilingual benchmarks often rely on machine-translated text, potentially underestimating Step 3's impact.

3. **Error propagation in low-resource languages**: To what extent does error propagation in the LLM's self-extraction of structured knowledge affect the final reasoning accuracy for low-resource languages? The method relies on the LLM performing NER before final reasoning, and lower performance in low-resource languages may stem from extraction failures rather than reasoning failures.

## Limitations

- Current benchmarks (MGSM, MSV-AMP, XCOPA) are largely machine-translated and lack authentic cultural phenomena, potentially understating Step 3's contribution
- The method defaults to English as the pivot language, which may be suboptimal for non-English-centric LLMs
- Limited exploration of generalization to languages with different script systems or right-to-left writing

## Confidence

**High Confidence**: Step 2 (Structured Knowledge Extraction) is the primary driver of performance gains; SoT outperforms standard CoT and Direct approaches across all tested benchmarks; The compatibility of SoT with other training-free strategies like few-shot learning

**Medium Confidence**: Step 1 (Language Thinking Transformation) provides marginal but consistent improvements; Step 3 (Language-Specific Knowledge Injection) adds value beyond what Step 2 captures; English serves as an effective pivot language for cross-lingual reasoning transfer

**Low Confidence**: The optimal balance between Steps 1, 2, and 3 may vary by language resource level; SoT's effectiveness on languages with non-Latin scripts or right-to-left writing systems; The method's scalability to languages where the model has minimal pre-training exposure

## Next Checks

1. **Prompt Template Reconstruction**: Recreate the complete 4-step SoT prompt template using only the examples and descriptions provided in the paper. Run a small-scale experiment (3-5 languages, 20 questions each) to verify whether the reconstructed prompts achieve similar performance to reported results.

2. **Language-Specific Knowledge Stress Test**: Design a controlled experiment using 50 questions per language that contain unambiguous language-specific conventions (e.g., Chinese discount expressions, Thai numerical classifiers, Japanese counter words). Compare SoT performance with and without Step 3 to quantify its contribution beyond benchmark artifacts.

3. **Pivot Language Optimization**: Test alternative pivot languages for different LLM architectures. Run SoT with Chinese as the pivot for Chinese-centric models, Spanish for Spanish-centric models, and English as control. Measure whether pivot language selection affects performance based on model training corpus.