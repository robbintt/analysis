---
ver: rpa2
title: 'MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers'
arxiv_id: '2504.10551'
source_url: https://arxiv.org/abs/2504.10551
tags:
- shortcuts
- learning
- shortcut
- mimu
- multiple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes MiMu, a novel method to mitigate multiple
  shortcut learning behavior in Transformer-based models. The method consists of two
  strategies: self-calibration in the source model to prevent overconfident predictions,
  and self-improvement in the target model to reduce reliance on multiple shortcuts.'
---

# MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers

## Quick Facts
- arXiv ID: 2504.10551
- Source URL: https://arxiv.org/abs/2504.10551
- Reference count: 40
- Primary result: Improves ImageNet-9 Rand-B accuracy to 0.8021 (+0.0313 over ERM baseline) while mitigating shortcut learning

## Executive Summary
This paper introduces MiMu, a method targeting multiple shortcut learning in Transformer models through self-calibration and self-improvement strategies. The approach prevents overconfident predictions and reduces reliance on fixed shortcut regions using random masking and adaptive attention alignment. Experiments demonstrate significant improvements in out-of-distribution robustness while maintaining in-distribution accuracy across NLP and CV tasks.

## Method Summary
MiMu operates in two stages: first training a source model with self-calibration loss to prevent overconfidence, then training a target model that uses random masking to diversify attention and aligns to the calibrated source model's attention patterns. The method employs logistic regression post-hoc calibration and has been tested on BERT-base-uncased for NLP and ViT-Large for CV tasks.

## Key Results
- ImageNet-9 Rand-B accuracy improves to 0.8021 (+0.0313 over ERM baseline)
- Maintains strong in-distribution performance while improving OOD generalization
- Demonstrates effectiveness across both NLP (MNLI-Hans, QQP-PAWS) and CV (ImageNet variants) tasks

## Why This Works (Mechanism)

### Mechanism 1
Self-calibration prevents shortcut over-reliance by reducing prediction overconfidence through a combined mean squared error and cross-entropy loss. This aligns model confidence with accuracy, discouraging shortcut exploitation. Break condition: If calibration doesn't produce richer attention distributions, downstream improvement may inherit biased patterns.

### Mechanism 2
Random masking forces diversified attention beyond shortcut regions by obscuring 20% of tokens (NLP) or 10% of patches (CV) in attention weights. This prevents concentration on fixed shortcut-correlated regions. Break condition: If N is too large, the model cannot learn meaningful features; if too small, shortcuts remain.

### Mechanism 3
Adaptive attention alignment transfers calibrated attention patterns without external supervision by aligning target model's unmasked attention weights with the calibrated source model via top-M difference loss. This encourages richer attention distributions. Break condition: If the source model still contains shortcut-correlated attention, alignment may propagate rather than mitigate shortcut reliance.

## Foundational Learning

- **Shortcut learning**: Models exploit easy-to-learn but non-robust patterns between features and labels that generalize poorly to OOD data. Quick check: Can you explain why a model might classify images based on background rather than object shape?

- **Model calibration**: Alignment between predicted probabilities and actual correctness. Quick check: What does a high Expected Calibration Error (ECE) indicate about a model's predictions?

- **Self-distillation**: Using a model's own softened outputs and representations to train another instance. Quick check: Why might self-distillation preserve more information than label smoothing alone?

## Architecture Onboarding

- **Component map**: Source model (Fs) -> Random mask module -> Target model (Ft) -> Post-hoc calibration layer
- **Critical path**: Train Fs with calibration loss → Fix Fs parameters → Train Ft with masked attention and alignment → Apply post-hoc calibration → Inference with calibrated outputs
- **Design tradeoffs**: Mask ratio N (10-20% of sequence length); higher values disrupt learning, lower values leave shortcuts unaddressed. Top-M differences (50%) balance alignment strength with flexibility.
- **Failure signatures**: Significant IID accuracy drops suggest over-regularization; unchanged OOD performance indicates source model may still encode shortcuts; overly similar Ft and Fs attention patterns suggest insufficient randomization.
- **First 3 experiments**: 1) Replicate ERM baseline on MNLI-Hans to establish shortcut learning behavior, 2) Train source model with calibration loss only and evaluate ECE scores, 3) Compare MiMu full pipeline against ablations on Rand-B and Rand-W.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do semantic interactions between distinct shortcuts influence model reliance, potentially forming a unique "third" shortcut? This remains unresolved as MiMu operates at pixel/token level, potentially missing semantic relationships.

- **Open Question 2**: How do fine-grained attributes of shortcuts (size, complexity, color) quantitatively influence the degree of shortcut learning? Current benchmarks treat shortcuts as binary or generic categories without systematic testing of continuous attributes.

- **Open Question 3**: Can the MiMu framework effectively mitigate shortcut learning in Large Language Models (LLMs) to reduce hallucinations? The study is restricted to encoder-based architectures, and applicability to large generative models is unverified.

## Limitations

- The calibration loss formulation lacks detailed justification for specific coefficients used
- Random masking depends critically on mask ratio N, which varies between NLP and CV without clear explanation
- The adaptive attention alignment assumes top-M differences capture meaningful shortcut patterns without empirical validation

## Confidence

- **High Confidence**: OOD robustness improvements on watermarked ImageNet variants (0.8021 Rand-B accuracy, +0.0313 over ERM)
- **Medium Confidence**: Self-calibration preventing overconfident predictions lacks corpus evidence for direct causal link to shortcut mitigation
- **Low Confidence**: Maintaining IID accuracy while improving OOD performance requires careful examination due to modest improvements and potential sensitivity to unspecified hyperparameters

## Next Checks

1. **Ablation study verification**: Run experiments without random masking component to isolate whether attention alignment alone achieves similar OOD improvements.

2. **Calibration effectiveness audit**: Measure and compare Expected Calibration Error (ECE) scores on both IID and OOD test sets for source model versus baseline ERM model.

3. **Hyperparameter sensitivity analysis**: Systematically vary mask ratio N (10%, 15%, 25%, 30%) and attention alignment weight λ2 to determine robustness to critical hyperparameters.