---
ver: rpa2
title: Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and
  Feature Selection
arxiv_id: '2507.11136'
source_url: https://arxiv.org/abs/2507.11136
tags:
- tensor
- factor
- data
- posterior
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bayesian Tensor Network Kernel Machines (BTN-Kernel
  Machines), a probabilistic framework for kernel methods that leverages low-rank
  tensor networks to handle high-dimensional feature spaces and large-scale datasets.
  The key innovation is a fully Bayesian treatment that automatically infers model
  complexity, including tensor rank and feature dimensions, by using sparsity-inducing
  hierarchical priors on tensor network factors.
---

# Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection

## Quick Facts
- arXiv ID: 2507.11136
- Source URL: https://arxiv.org/abs/2507.11136
- Authors: Afra Kilic; Kim Batselier
- Reference count: 16
- Primary result: Bayesian Tensor Network Kernel Machines enable automatic rank and feature selection through sparsity-inducing hierarchical priors, achieving scalable high-dimensional kernel learning with uncertainty quantification

## Executive Summary
This paper introduces Bayesian Tensor Network Kernel Machines (BTN-Kernel Machines), a probabilistic framework that combines tensor networks with Bayesian inference to create interpretable kernel methods for high-dimensional data. The approach automatically determines model complexity through sparsity-inducing hierarchical priors on tensor network factors, enabling simultaneous rank selection and feature selection. By employing mean-field variational inference, the method achieves computational efficiency comparable to deterministic approaches while providing uncertainty quantification at no additional cost. The framework demonstrates superior performance across prediction accuracy, uncertainty quantification, interpretability, and scalability compared to state-of-the-art methods.

## Method Summary
The BTN-Kernel Machines framework integrates low-rank tensor networks with Bayesian inference to create a probabilistic kernel method. The core innovation lies in using hierarchical sparsity-inducing priors on tensor network factors, which automatically learns the optimal tensor rank and performs feature selection simultaneously. The method employs mean-field variational inference to approximate the posterior distributions, resulting in a Bayesian alternating least squares algorithm. This approach maintains the same computational complexity as its deterministic counterpart while enabling uncertainty quantification. The framework is specifically designed to handle high-dimensional feature spaces and large-scale datasets efficiently.

## Key Results
- Automatic rank and feature selection through sparsity-inducing hierarchical priors
- Scalable performance on high-dimensional datasets with uncertainty quantification at no extra computational cost
- Superior prediction accuracy and interpretability compared to state-of-the-art kernel methods
- Effective scaling to large datasets like Adult while maintaining competitive performance

## Why This Works (Mechanism)
The framework works by combining the representational power of tensor networks with Bayesian inference. Low-rank tensor networks efficiently represent high-dimensional feature interactions while keeping computational complexity manageable. The hierarchical sparsity-inducing priors act as automatic complexity controls, learning which features and tensor ranks are necessary for the task. Mean-field variational inference provides an efficient approximation to the posterior distributions, enabling tractable learning while maintaining uncertainty quantification. This combination allows the model to automatically balance model complexity with data fit, avoiding overfitting while maintaining predictive performance.

## Foundational Learning
- **Tensor networks**: Efficient representations of high-dimensional data using low-rank decompositions; needed for handling curse of dimensionality in kernel methods
  - Quick check: Verify rank bounds and computational complexity scaling
- **Hierarchical sparsity-inducing priors**: Bayesian priors that encourage feature selection and rank determination; needed for automatic model complexity control
  - Quick check: Confirm prior hyperparameters are appropriately tuned
- **Mean-field variational inference**: Approximation method for posterior distributions; needed for tractable Bayesian inference at scale
  - Quick check: Validate variational approximation quality through diagnostics
- **Alternating least squares**: Optimization technique for tensor factorization; needed for efficient parameter updates
  - Quick check: Monitor convergence and stability of optimization
- **Kernel methods**: Non-linear feature mapping through kernel functions; needed for capturing complex relationships
  - Quick check: Verify kernel function properties and hyperparameters
- **Uncertainty quantification**: Probabilistic predictions with confidence estimates; needed for reliable decision-making
  - Quick check: Evaluate calibration of predictive uncertainty

## Architecture Onboarding

**Component Map**
Data -> Feature Map -> Tensor Network -> Kernel Function -> Prediction + Uncertainty

**Critical Path**
1. Input features undergo non-linear mapping through kernel function
2. Mapped features are processed through low-rank tensor network
3. Tensor network outputs serve as kernel inputs for prediction
4. Bayesian inference provides uncertainty quantification

**Design Tradeoffs**
- Variational inference approximation vs. exact posterior computation
- Mean-field assumption vs. capturing posterior correlations
- Low-rank tensor approximation vs. full tensor representation
- Computational efficiency vs. modeling flexibility

**Failure Signatures**
- Poor uncertainty calibration indicating variational approximation breakdown
- Overfitting suggesting inadequate regularization or prior specification
- Computational instability during tensor factorization
- Degenerate feature selection patterns

**First Experiments**
1. Synthetic data with known ground truth to validate automatic feature selection
2. Medium-sized UCI dataset to benchmark against standard kernel methods
3. High-dimensional dataset to test scalability claims

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Mean-field variational inference may underestimate uncertainty due to factorized posterior assumptions
- Limited validation on extremely high-dimensional real-world applications beyond standard datasets
- Interpretability claims lack quantitative metrics and practical case studies for decision-making utility

## Confidence
- Uncertainty estimation quality: Medium - Mean-field approximation may miss posterior correlations
- High-dimensional scaling: Low - Limited empirical validation on extreme-scale real-world data
- Practical interpretability: Medium - Mechanism exists but utility not demonstrated through case studies

## Next Checks
1. Evaluate uncertainty calibration on datasets with known ground truth uncertainty structures, comparing mean-field variational inference against more expressive posterior approximations
2. Test scalability on high-dimensional real-world datasets (e.g., genomics, hyperspectral imaging) with >10,000 features to assess practical limitations
3. Conduct case studies with domain experts to quantify the practical interpretability and decision-making utility of the learned tensor structures