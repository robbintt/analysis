---
ver: rpa2
title: 'Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based
  Multi-Agent Systems'
arxiv_id: '2509.09629'
source_url: https://arxiv.org/abs/2509.09629
tags:
- agent
- grounding
- planning
- agents
- moat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MOAT, a Multi-Agent Joint Alignment Tuning
  framework to address capability gaps between planning and grounding agents in multi-agent
  systems. MOAT iteratively optimizes the planning agent to generate subgoals that
  better guide the grounding agent (using perplexity-based rewards and DPO), and improves
  the grounding agent's generalization by training on diverse subgoal-action pairs.
---

# Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems

## Quick Facts
- arXiv ID: 2509.09629
- Source URL: https://arxiv.org/abs/2509.09629
- Reference count: 40
- Primary result: Achieves average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks compared to state-of-the-art baselines

## Executive Summary
This paper addresses the capability gap between planning and grounding agents in multi-agent systems through a novel iterative alignment framework called MOAT. The approach aligns a planning agent's subgoal generation with a grounding agent's comprehension capabilities using perplexity-based rewards and Direct Preference Optimization (DPO). By training the grounding agent on diverse, critic-corrected subgoal-action pairs, MOAT improves both agents' performance while theoretical analysis proves the training process is non-decreasing and convergent.

## Method Summary
MOAT employs a three-phase approach: (1) Cold start SFT to initialize both agents on ground-truth trajectories, (2) Iterative alignment where the planning agent is optimized via DPO using the grounding agent's perplexity as a reward signal, and (3) Grounding agent improvement through SFT on critic-corrected subgoal-action pairs. The framework alternates between these stages for multiple iterations, using K=15 sampled subgoals per task and a strong critic model (DeepSeek-R1-Distill-Qwen-32B) for validation and correction.

## Key Results
- Achieves 3.1% average improvement on held-in tasks (GSM8K, StrategyQA, Mind2Web)
- Achieves 4.4% average improvement on held-out tasks (SVAMP, WebShop, HotpotQA)
- Outperforms state-of-the-art baselines including Lumos and Lats

## Why This Works (Mechanism)

### Mechanism 1
The planning agent is optimized using the grounding agent's perplexity as a reward signal, aligning subgoal generation with the executor's comprehension limits. The planner samples K subgoal sequences, the grounder computes PPL for each, and DPO aligns the planner to generate subgoals that yield low PPL (high confidence) in the grounder. This assumes PPL correlates positively with task success likelihood.

### Mechanism 2
The grounding agent is trained on diverse, critic-corrected subgoal-action pairs to bridge the capability gap. Instead of ground-truth subgoals, the agent learns from the planner's actual output distribution. A critic model verifies outputs and provides corrections when errors occur, filtering noise while adapting the grounder to the planner's "language."

### Mechanism 3
Iterative alternation between planning alignment and grounding improvement guarantees non-decreasing performance and convergence. This EM-like loop optimizes the plan for the current executor, then the executor for the current plan. Theoretical analysis proves updating one agent while holding the other fixed cannot decrease expected reward, ensuring convergence to a local equilibrium.

## Foundational Learning

**Direct Preference Optimization (DPO)**
- Why needed: Core engine for Planning Agent Alignment (Stage 1), using preference pairs (high/low perplexity samples) without explicit reward model
- Quick check: Why does the paper prefer DPO over RLHF for aligning the planner?

**Perplexity (PPL) as Surrogate Metric**
- Why needed: Evaluates subgoal quality as measure of "surprise" vs. "correctness"
- Quick check: If a subgoal has low PPL but leads to incorrect tool call, how would MOAT handle it?

**Cold Start SFT**
- Why needed: Initializes both agents with capability before iterative loop begins
- Quick check: What specific data is used for cold start phase, and what loss function is applied?

## Architecture Onboarding

**Component map:**
Planning Agent (LLM) -> Subgoals -> Grounding Agent (LLM) -> Tool Calls -> Tool Environment
                                      ↓
                                 Critic Model (validation/correction)

**Critical path:**
1. Cold Start: SFT both agents on ground-truth trajectories
2. Stage 1 (Alignment): Planner samples K subgoals → Grounder computes PPL → Construct (Win, Lose) pairs → DPO update Planner
3. Stage 2 (Improving): Planner generates subgoals → Grounder predicts actions → Critic corrects errors → SFT update Grounder
4. Repeat for N iterations

**Design tradeoffs:**
- Sample Count (K): Higher K improves quality but increases compute linearly
- Critic Strength: Stronger critic yields better performance than weaker one
- Iterations: Performance plateaus after 2-3 iterations

**Failure signatures:**
- High PPL/High Error: Planner generates subgoals outside grounder's distribution
- Model Collapse: Running Stage 2 without Critic causes degradation
- Overfitting: Excessive iterations on held-in tasks hurt held-out generalization

**First 3 experiments:**
1. Sanity Check: Evaluate initial Planner and Grounder separately on held-in benchmarks
2. Component Ablation: Run MOAT "w/o critic" to confirm correction step necessity
3. Hyperparameter Scan: Compare K=5 vs K=15 sampling to visualize diversity-performance relationship

## Open Questions the Paper Calls Out
The paper identifies several directions for future work: integrating visual models for multimodal agents, expanding to more than two agents with specialized modules (tool retrieval, reflection), and exploring mechanisms to reduce dependency on strong external critic models for grounding agent improvement.

## Limitations
- Relies on strong external critic model, creating resource overhead and potential ceiling limits
- Theoretical convergence proofs assume idealized conditions that may not hold under distribution shift
- Limited experimental scope with only 2-3 iterations tested on 6 benchmarks

## Confidence
- **High**: Iterative optimization framework and basic DPO implementation are well-established
- **Medium**: Theoretical convergence proofs are sound but assume idealized conditions
- **Low**: Long-term stability beyond 2-3 iterations is not established

## Next Checks
1. Stress Test the Critic Absence: Run MOAT without critic correction on all six benchmarks
2. Extended Iteration Testing: Run MOAT for 5-10 iterations on held-in tasks to identify stability patterns
3. Critic Strength Scaling: Replace 32B critic with progressively weaker models and measure performance impact