---
ver: rpa2
title: Informative Text-Image Alignment for Visual Affordance Learning with Foundation
  Models
arxiv_id: '2509.17074'
source_url: https://arxiv.org/abs/2509.17074
tags:
- affordance
- learning
- visual
- features
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an informative framework for text-guided affordance
  learning by incorporating mutual information-based constraints to achieve text-image
  alignment at feature level. The method introduces affordance mutual information
  constraint to align visual affordance areas with textual descriptions by maximizing
  mutual information between features, and object-level information constraint to
  maintain text-image alignment properties of foundation models.
---

# Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models

## Quick Facts
- **arXiv ID:** 2509.17074
- **Source URL:** https://arxiv.org/abs/2509.17074
- **Authors:** Qian Zhang; Lin Zhang; Xing Fang; Mingxin Zhang; Zhiyuan Wei; Ran Song; Wei Zhang
- **Reference count:** 21
- **Primary result:** Proposes an informative framework for text-guided affordance learning with foundation models using mutual information constraints, achieving SOTA performance on AGD20K dataset with KLD of 0.719, SIM of 0.581, and NSS of 1.787 on Seen split

## Executive Summary
This paper addresses the challenge of visual affordance learning by proposing an informative framework that achieves text-image alignment at the feature level using mutual information-based constraints. The method introduces two key components: an affordance mutual information constraint that aligns visual affordance areas with textual descriptions, and an object-level information constraint that maintains the text-image alignment properties of foundation models. The approach is evaluated on the AGD20K dataset and demonstrates state-of-the-art performance in one-shot affordance learning tasks.

## Method Summary
The framework employs mutual information maximization to align visual and textual features for affordance learning. It introduces an affordance mutual information constraint that specifically targets the alignment between visual affordance regions and their textual descriptions. Additionally, an object-level information constraint is incorporated to preserve the inherent text-image alignment capabilities of foundation models. This dual-constraint approach enables effective one-shot learning where the model can generalize affordance knowledge from limited examples by leveraging the semantic relationships between visual features and textual descriptions.

## Key Results
- Achieves state-of-the-art performance on AGD20K dataset with KLD of 0.719, SIM of 0.581, and NSS of 1.787 on Seen split
- Outperforms existing approaches in one-shot affordance learning tasks
- Demonstrates effective text-guided affordance learning through feature-level alignment

## Why This Works (Mechanism)
The method works by maximizing mutual information between visual and textual features, creating stronger semantic connections between what objects look like and what they can be used for. The affordance mutual information constraint specifically targets the alignment between visual regions that afford certain actions and their corresponding textual descriptions, while the object-level constraint ensures that the foundational language-vision alignment capabilities of pre-trained models are preserved and enhanced.

## Foundational Learning
- **Mutual Information Maximization**: Why needed - to create strong semantic connections between visual and textual features; Quick check - verify MI values increase during training
- **Text-Image Feature Alignment**: Why needed - to ensure visual regions correspond to textual descriptions; Quick check - measure alignment scores between visual and textual embeddings
- **Foundation Model Integration**: Why needed - to leverage pre-trained vision-language understanding; Quick check - confirm model maintains zero-shot capabilities on standard benchmarks
- **One-shot Learning**: Why needed - to enable affordance generalization from limited examples; Quick check - test performance with varying shot counts
- **Affordance Mapping**: Why needed - to identify which regions of objects support which actions; Quick check - visualize predicted affordance maps
- **Semantic Consistency**: Why needed - to ensure learned affordances match real-world functionality; Quick check - evaluate against human-labeled affordance datasets

## Architecture Onboarding
- **Component Map:** Foundation Model -> Feature Extractor -> Mutual Information Estimator -> Alignment Loss -> Affordance Predictor
- **Critical Path:** Input Image + Text Description → Visual and Textual Feature Extraction → Mutual Information Maximization → Affordance Prediction
- **Design Tradeoffs:** Balance between preserving foundation model properties versus specialized affordance learning; complexity of mutual information estimation versus performance gains
- **Failure Signatures:** Poor alignment between visual regions and textual descriptions; degradation of foundation model's general vision-language understanding; overfitting to specific object categories
- **First Experiments:** 1) Ablation study removing mutual information constraint, 2) Cross-dataset evaluation on non-AGD20K datasets, 3) Real-world robotic manipulation task testing

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation metrics (KLD, SIM, NSS) lack clear connection to practical robotic manipulation performance
- Heavy reliance on AGD20K dataset raises concerns about overfitting and generalization
- Performance on Seen split specifically questions whether method learns generalizable knowledge or memorizes dataset patterns
- Limited cross-dataset evaluation scope

## Confidence
- **Text-image alignment effectiveness:** Medium confidence - mutual information approach shows promise but lacks component isolation
- **Foundation model integration:** Medium confidence - claims preservation of properties but lacks cross-domain generalization evidence
- **State-of-the-art performance:** Medium confidence - superior metrics but limited to single dataset and split

## Next Checks
1. Conduct cross-dataset evaluation on multiple affordance datasets to verify generalization beyond AGD20K, including datasets with different object categories and visual styles
2. Perform ablation studies to isolate the contribution of the affordance mutual information constraint versus the object-level information constraint, and test performance with and without foundation model pretraining
3. Implement a real-world robotic manipulation test to validate whether the learned affordances translate to practical task performance, measuring success rates on actual grasping and manipulation tasks