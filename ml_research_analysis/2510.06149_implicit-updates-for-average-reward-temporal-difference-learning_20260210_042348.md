---
ver: rpa2
title: Implicit Updates for Average-Reward Temporal Difference Learning
arxiv_id: '2510.06149'
source_url: https://arxiv.org/abs/2510.06149
tags:
- average-reward
- step-size
- implicit
- learning
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Average-reward TD(\u03BB) methods are sensitive to step-size selection,\
  \ requiring careful tuning to balance convergence speed and numerical stability.\
  \ This work introduces average-reward implicit TD(\u03BB), which uses implicit fixed-point\
  \ updates to automatically stabilize learning across a wider range of step-sizes\
  \ without increasing computational complexity."
---

# Implicit Updates for Average-Reward Temporal Difference Learning

## Quick Facts
- **arXiv ID**: 2510.06149
- **Source URL**: https://arxiv.org/abs/2510.06149
- **Reference count**: 40
- **Primary result**: Average-reward implicit TD(λ) stabilizes learning across wider step-size ranges without increased computational cost.

## Executive Summary
Average-reward temporal difference learning is notoriously sensitive to step-size selection, often requiring careful tuning to balance convergence speed and numerical stability. This paper introduces average-reward implicit TD(λ), which employs implicit fixed-point updates to automatically stabilize learning across a broader range of step-sizes without increasing computational complexity. The method achieves this through a data-adaptive shrinkage factor that reduces the effective step-size when eligibility traces have large norms. Finite-time error bounds are established under both constant and diminishing step-size schedules, substantially relaxing prior step-size conditions. Empirically, the implicit variant maintains low loss and stability over broad step-size ranges, outperforming standard average-reward TD(λ) in policy evaluation and control tasks.

## Method Summary
The method reformulates the standard TD update as a fixed-point equation, where the next iterate is defined implicitly through a denominator term $(1 + \beta_t \|z_t\|^2)$. This creates a data-adaptive stabilization mechanism that automatically reduces the effective step-size when the eligibility trace has large norm, preventing divergence. The algorithm maintains estimates of both the average reward $\omega$ and differential value weights $\theta$, updating them using dampened multipliers derived from the current trace norm. A projection step ensures bounded iterates by constraining the combined parameter vector to a ball of radius $R_\Theta$, addressing the non-identifiability inherent in average-reward settings where value functions are only unique up to a constant shift.

## Key Results
- Implicit TD maintains stability and low loss across a much wider range of constant step-sizes compared to standard TD, which diverges at higher values
- Finite-time error bounds are established for polynomial step-size schedules ($\beta_t \propto t^{-s}$ for $s \in (0,1)$) not covered by prior work
- In control tasks using SARSA, the implicit variant achieves higher cumulative average reward and learns better policies faster, especially with larger initial step-sizes

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Effective Step-Size via Implicit Denominators
The implicit update automatically reduces the effective step-size when the eligibility trace has a large norm, preventing divergence. The standard TD update is reformulated as a fixed-point equation. Solving for the next iterate yields a denominator term $(1 + \beta_t \|z_t\|^2)$ (and similar for the average reward update). This acts as a data-dependent shrinkage factor; when the trace $z_t$ is large (indicating high variance or "momentum"), the update magnitude is dampened. Core assumption: The shrinkage factor effectively approximates the stability usually requiring manual step-size tuning. Break condition: If $\|z_t\|^2$ remains consistently small (near zero), the implicit update behaves identically to standard TD, offering no added stability benefits.

### Mechanism 2: Handling Non-Identifiability via Projection
The algorithm stabilizes the differential value estimation by projecting weight updates onto a bounded ball and accounting for the constant-shift invariance of the average-reward setting. Average-reward value functions are defined only up to an additive constant (the "constant-shift direction"). The theoretical analysis relies on an auxiliary iterate that projects onto the orthogonal complement of this direction ($O$). Practically, the algorithm projects the combined parameter vector $\Theta$ onto a ball of radius $R_\Theta$ to ensure bounded iterates required for the finite-time bounds. Core assumption: The projection radius $R_\Theta$ is chosen sufficiently large so that the optimal parameter $\Theta^*$ lies within the ball. Break condition: If the projection radius $R_\Theta$ is set too small, the algorithm will suffer from bias as it clips the optimal solution.

### Mechanism 3: Stability Under Aggressive Step-Sizes
The implicit formulation allows for significantly larger constant step-sizes or slower-decaying diminishing step-sizes (e.g., $\beta_t \propto t^{-s}$ for $s \in (0,1)$) without diverging. Standard TD requires restrictive step-size conditions to ensure the expected update operator is contractive. The implicit modification alters the dynamics such that the "effective" step-size is self-correcting, allowing the theoretical analysis to relax conditions on the raw step-size sequence $\beta_t$. Core assumption: The Markov chain is uniformly geometrically ergodic. Break condition: If the step-size is excessively large (approaching infinity) or the data is non-Markovian/non-ergodic, the stability guarantees may not hold.

## Foundational Learning

- **Average-Reward MDPs (Differential Value Functions)**: Why needed here: Unlike discounted settings, average-reward problems optimize the long-run rate of return. The value function $v^\mu$ represents the *relative* advantage of states (differential value), which is only unique up to a constant. You must understand this to interpret the projection logic and the definition of the TD error ($R_t - \hat{\omega}_t + \dots$). Quick check question: Why does the average-reward TD error subtract the current average reward estimate $\hat{\omega}_t$ rather than using a discount factor $\gamma$?

- **Eligibility Traces ($TD(\lambda)$)**: Why needed here: The core instability addressed by this paper is heavily influenced by the magnitude of the eligibility trace $z_t$. The implicit mechanism specifically scales the update based on $\|z_t\|^2$. You need to know how $z_t$ aggregates past feature vectors to understand why its norm grows and triggers stabilization. Quick check question: If $\lambda=0$, what does $z_t$ become, and how does that simplify the implicit update denominator?

- **Implicit Stochastic Approximation**: Why needed here: This is the mathematical engine of the paper. It involves formulating the update $\theta_{t+1} = \theta_t + \beta_t g(\theta_{t+1})$ instead of the explicit $g(\theta_t)$. Understanding this inversion (solving for $\theta_{t+1}$) is required to implement the algorithm correctly. Quick check question: In the implicit update $\theta_{t+1} = \theta_t + \beta \delta(\theta_{t+1}) z$, how do you mathematically isolate $\theta_{t+1}$ on the left-hand side?

## Architecture Onboarding

- **Component map**: Estimator (maintains $\hat{\omega}$ and $\hat{\theta}$) -> Trace Generator (updates $z_t \leftarrow \lambda z_{t-1} + \phi(S_t)$) -> Implicit Calculator (computes scaling factor) -> Projector (clamps $\Theta$ to radius $R_\Theta$)

- **Critical path**: 
  1. Observe transition $(S_t, R_t, S_{t+1})$
  2. Update eligibility trace $z_t$
  3. Compute TD error $\delta_t$ using *current* weights (standard TD error)
  4. **Crucial Step**: Calculate the effective step-size multiplier using the *current* trace norm
  5. Apply updates to $\hat{\omega}$ and $\hat{\theta}$ using the dampened multipliers
  6. Apply Projection (check if $\|\Theta\| > R_\Theta$)

- **Design tradeoffs**:
  - **Step-Size Ratio ($c_\alpha$)**: The paper sets $\alpha_t = c_\alpha \beta_t$. Tuning $c_\alpha$ is required to balance the learning speed of the average reward $\omega$ vs. the differential weights $\theta$. The paper suggests $c_\alpha \ge \Delta + \dots$ for theory, but uses 1.0 in experiments.
  - **Projection Radius ($R_\Theta$)**: A large radius (e.g., 5000) is safer for theory but less numerically restrictive; a smaller radius might regularize but introduces bias.
  - **Complexity**: The implicit method adds virtually no computational cost (just a scalar division) compared to standard TD.

- **Failure signatures**:
  - **Standard TD Divergence**: Oscillating or exploding loss values as step-size $\beta$ increases (Figure 1)
  - **Stagnation**: If $\|z_t\|$ is consistently huge, the effective step-size might become too small, slowing convergence significantly compared to a well-tuned standard TD
  - **Projection Bias**: If loss decreases then plateaus at a high value, check if $R_\Theta$ is too small

- **First 3 experiments**:
  1. **MRP Policy Evaluation (Sensitivity Sweep)**: Replicate Figure 2. Run both standard and implicit TD on a 100-state MRP. Sweep constant step-size $\beta$ from 0.1 to 3.0. Verify that standard TD loss explodes while implicit TD stays flat.
  2. **Boyan Chain (Diminishing Step-Size)**: Implement the decaying schedule $\beta_t = \beta_0 / (t+1)^{0.99}$. Check if the implicit version tracks the optimal weights faster and more stably than the standard version over 2000 steps.
  3. **Access-Control Queuing (Control)**: Integrate the implicit TD update into a SARSA loop. Compare the cumulative average reward achieved. Validate that the implicit agent can use a larger initial $\beta_0$ to learn a better policy faster without destabilizing the queue.

## Open Questions the Paper Calls Out
- **Can finite-time error bounds be established for average-reward implicit SARSA in control settings?**: The conclusion identifies a "full theoretical analysis of average-reward implicit SARSA" as a promising future direction. The paper provides theoretical guarantees only for policy evaluation; the empirical control results (SARSA) lack accompanying theoretical proofs of convergence or stability.
- **Do the stability benefits of implicit updates extend to two-time-scale average-reward TD(λ)?**: The authors list "rigorous two-time-scale extensions of both standard and implicit average-reward TD(λ)" as future work. The current analysis restricts itself to a single-time-scale setup ($\alpha_t = c_\alpha \beta_t$), explicitly stating that exploring distinct decay rates is "outside our scope."
- **Can implicit TD methods be constructed to estimate the asymptotic variance of cumulative reward?**: The conclusion proposes building "implicit TD methods to estimate the asymptotic variance of cumulative reward in the average-reward regime." Existing methods for variance estimation in this setting may suffer from step-size sensitivity similar to standard TD, but the implicit framework has not yet been applied to this specific problem.

## Limitations
- The projection step introduces a hyperparameter ($R_\Theta$) that must be set large enough to contain the optimal solution but not so large as to lose numerical benefits
- Theoretical bounds assume uniformly geometrically ergodic Markov chains, which may not hold in all practical RL environments
- The mechanism analysis relies on the implicit denominator acting as effective step-size shrinkage, but empirical validation of the *magnitude* of this effect across diverse trace norms is limited

## Confidence
- **High Confidence**: The core mechanism of implicit updates providing stability through adaptive effective step-sizes (Mechanism 1)
- **Medium Confidence**: The practical benefit of relaxed step-size conditions in finite-time performance
- **Medium Confidence**: The necessity and impact of the projection step for average-reward TD

## Next Checks
1. **Projection Radius Sensitivity**: Run MRP policy evaluation with $R_\Theta$ set to 100, 1000, 5000, and 10000. Measure final loss and check if results change significantly.
2. **Trace Norm Distribution**: For Boyan chain and MRP, record the distribution of $\|z_t\|^2$ during training. Verify that it spans a range where the implicit denominator meaningfully differs from 1 (i.e., is not always near 1).
3. **Non-Ergodic Environment Test**: Test the implicit TD method on a simple MRP with a "sticky" state (high self-transition probability). Observe if stability guarantees break down compared to the ergodic case.