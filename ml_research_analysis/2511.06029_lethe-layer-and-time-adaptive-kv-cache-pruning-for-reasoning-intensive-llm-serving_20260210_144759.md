---
ver: rpa2
title: 'Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM
  Serving'
arxiv_id: '2511.06029'
source_url: https://arxiv.org/abs/2511.06029
tags:
- lethe
- arxiv
- cache
- attention
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Lethe introduces a dynamic KV cache management framework for reasoning-intensive
  LLM inference. It addresses the challenge of rapidly growing memory during long-form
  generation by combining spatial and temporal adaptivity: layerwise sparsity-aware
  token allocation and recency-aware iterative pruning.'
---

# Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving

## Quick Facts
- arXiv ID: 2511.06029
- Source URL: https://arxiv.org/abs/2511.06029
- Reference count: 9
- Primary result: Up to 2.56× higher throughput and 91.7% reduction in KV cache memory while maintaining or improving accuracy on reasoning tasks

## Executive Summary
Lethe introduces a dynamic KV cache management framework for reasoning-intensive LLM inference. It addresses the challenge of rapidly growing memory during long-form generation by combining spatial and temporal adaptivity: layerwise sparsity-aware token allocation and recency-aware iterative pruning. The approach uses attention sparsity estimates to allocate token budgets per layer and a RASR mechanism to prune low-utility tokens over time. Evaluated across DeepSeek-R1-Distill models on Math500 and MMLU, Lethe achieves up to 2.56× higher throughput and 91.7% reduction in KV cache memory compared to full caching, while maintaining or improving accuracy over baselines such as H2O, StreamingLLM, and PyramidKV.

## Method Summary
Lethe combines spatial and temporal adaptivity for KV cache pruning during inference. Spatially, it estimates layerwise sparsity using the Hoyer metric and aggregates attention scores to allocate token budgets per layer, implementing layerwise token shrinking that retains top-k salient tokens plus a sink length and recent tokens. Temporally, it uses a Recency-Aware Sparsity Retention (RASR) mechanism that maintains decayed attention scores and periodically evicts low-utility tokens while preserving recent and sink tokens. The framework operates without model fine-tuning and is evaluated on DeepSeek-R1-Distill models across mathematical and MMLU benchmarks, demonstrating significant memory reduction and throughput gains.

## Key Results
- Achieves up to 2.56× higher throughput compared to full caching
- Reduces KV cache memory by up to 91.7% while maintaining accuracy
- Outperforms baseline methods including H2O, StreamingLLM, and PyramidKV on Math500 and MMLU benchmarks

## Why This Works (Mechanism)
Lethe addresses the inefficiency of static KV cache allocation by recognizing that reasoning-intensive models exhibit non-uniform, layer-dependent attention sparsity patterns. Traditional methods assume either uniform sparsity or pyramidal decay, but reasoning tasks show complex, fluctuating sparsity across layers and prompts. By estimating sparsity per layer using attention scores and allocating token budgets accordingly, Lethe avoids both over- and under-pruning. The temporal component (RASR) addresses the dynamic nature of attention over time, retaining tokens that continue to receive attention while evicting those that become irrelevant. This dual adaptivity allows Lethe to maintain accuracy while significantly reducing memory footprint and improving throughput.

## Foundational Learning
- **Attention Sparsity Estimation**: Measuring how much each token contributes to attention in each layer using metrics like Hoyer score. Why needed: To identify which tokens can be pruned without significant accuracy loss. Quick check: Verify that sparsity patterns differ across layers and prompts in reasoning tasks.
- **Layerwise Token Allocation**: Distributing available token slots across layers based on estimated sparsity rather than using uniform allocation. Why needed: Different layers have different tolerance for pruning; reasoning tasks show unpredictable sparsity patterns. Quick check: Compare accuracy when using uniform vs. layerwise allocation.
- **RASR Mechanism**: Maintaining decayed attention scores over time to identify low-utility tokens for eviction. Why needed: Attention relevance changes dynamically during generation, requiring time-adaptive pruning. Quick check: Monitor how token scores evolve over decoding steps.
- **Sink Token Preservation**: Always retaining the most recent tokens (sink) regardless of their attention scores. Why needed: Recent tokens are critical for context and cannot be safely pruned. Quick check: Verify that removing sink tokens causes accuracy to drop significantly.
- **Hoyer Metric for Sparsity**: Using L1/L2 norm ratio to quantify attention sparsity. Why needed: Provides a continuous measure of sparsity that can guide token allocation. Quick check: Confirm that Hoyer scores correlate with pruning tolerance in validation data.

## Architecture Onboarding

Component Map: Attention Scores -> Layerwise Sparsity Estimation -> Token Allocation Algorithm -> KV Cache Management -> Model Inference

Critical Path: During decoding, for each layer: compute attention scores → aggregate using Hoyer metric → allocate token budget → apply token shrinking algorithm → update KV cache → continue inference. RASR runs concurrently, maintaining decayed scores and triggering periodic pruning.

Design Tradeoffs: Lethe trades implementation complexity and hyperparameter tuning for significant memory and latency improvements. The layerwise allocation requires per-layer attention score computation and segmentation, while RASR adds state maintenance overhead. However, these costs are outweighed by the benefits of avoiding OOM errors and improving throughput, particularly for long-form reasoning tasks where traditional caching fails.

Failure Signatures: Accuracy degradation indicates over-pruning - likely due to aggressive τ values, insufficient token allocation per layer, or too frequent RASR eviction. OOM errors despite pruning suggest pruning isn't being triggered or isn't aggressive enough, or that attention score aggregation is incorrect. Suboptimal throughput gains may indicate pruning overhead exceeding benefits or inefficient token allocation.

First Experiments:
1. Implement Algorithm 1 with different τ values (1.5, 2.0, 2.5) and measure per-layer retention counts and accuracy on a small Math500 subset to find optimal configuration.
2. Run RASR with different decay rates (γ = 0.8, 0.9, 0.95) and pruning frequencies, logging cache size evolution and token scores to validate correct operation.
3. Compare Lethe against FullKV baseline on a single model (e.g., DeepSeek-R1-Distill-Qwen-7B) using Math500, measuring accuracy, latency, and peak memory to verify claimed improvements.

## Open Questions the Paper Calls Out
- **Quantization Synergy**: How does Lethe perform when combined with KV cache quantization methods like KIVI or KVQuant, and does joint optimization yield compounding or diminishing memory savings? The paper states these approaches are complementary but doesn't evaluate their combination.
- **Scaling to Full Models**: How does Lethe scale to the full DeepSeek-R1-671B model and other dense or mixture-of-experts architectures beyond the distilled 7B-70B variants? The paper used distilled variants due to hardware limitations.
- **Theoretical Sparsity Patterns**: What theoretical mechanisms explain the non-monotonic, prompt-specific layerwise sparsity patterns observed in reasoning models, and can they be predicted a priori? The paper observes these patterns but provides no theoretical account for why they emerge.
- **Generalization to Other Reasoning Tasks**: How does Lethe perform on reasoning-intensive tasks beyond mathematical and factual domains, such as multi-turn agentic planning, code generation with debugging traces, or long-horizon decision-making? Evaluation is limited to Math500 and 8 MMLU subjects.

## Limitations
- Exact hyperparameter values for τ, D, s_len, r, and γ are underspecified beyond loose bounds, requiring assumptions that may affect results
- The largest production-scale models (671B) are not evaluated due to hardware limitations, leaving scaling questions unanswered
- No evaluation on diverse reasoning benchmarks beyond mathematical and factual domains, limiting understanding of generalization

## Confidence
- High confidence: The core framework combining spatial and temporal adaptivity is clearly described and supported by hardware-level measurements and multiple baselines
- Medium confidence: Implementation details for Algorithm 1 and RASR are not fully specified, requiring assumptions about hyperparameters and pruning logic
- Medium confidence: Accuracy preservation claims are supported by benchmark results, but lack per-layer or per-model-size breakdowns for detailed validation

## Next Checks
1. Reproduce the layerwise token shrinking (Algorithm 1) with different τ and D values; measure how per-layer retention counts and overall accuracy vary to find the best configuration
2. Implement and test the RASR mechanism with varying decay rates γ and pruning frequencies; log cache size evolution and token scores to ensure correct operation
3. Benchmark Lethe against StreamingLLM and H2O on a held-out set of Math500 and MMLU samples, measuring not only accuracy and throughput but also per-layer latency and peak memory to verify claimed efficiency gains