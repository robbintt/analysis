---
ver: rpa2
title: Enhancing Fractional Gradient Descent with Learned Optimizers
arxiv_id: '2510.18783'
source_url: https://arxiv.org/abs/2510.18783
tags:
- l2o-cfgd
- fractional
- optimization
- gradient
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Enhancing Fractional Gradient Descent with Learned Optimizers

## Quick Facts
- **arXiv ID:** 2510.18783
- **Source URL:** https://arxiv.org/abs/2510.18783
- **Reference count:** 23
- **Primary result:** NA

## Executive Summary
This paper introduces L2O-CFGD, a meta-learning approach that dynamically tunes the hyperparameters (α, β, c) of Caputo Fractional Gradient Descent (CFGD) during optimization. The key insight is that static fractional orders limit convergence, while a learned optimizer can adapt these parameters to the evolving loss landscape. The method employs a coordinate-wise LSTM to output per-parameter hyperparameter schedules, using Hutchinson's method to approximate the Hessian diagonal for tractability. Empirical results on convex and non-convex tasks show that this dynamic tuning accelerates convergence compared to fixed-parameter CFGD and learned optimizers with fixed hyperparameters.

## Method Summary
The core method trains a coordinate-wise LSTM (the optimizer) to output hyperparameter schedules (α, β, c) for a Caputo fractional gradient update. The optimizee (function being minimized) computes loss and gradients, which are fed into the LSTM along with a time encoding to produce per-parameter α, β, and c values. The fractional gradient update is computed using 1-point Gauss-Jacobi quadrature for the Caputo integral and Hutchinson's method (3 steps) to approximate the Hessian diagonal term. Meta-training involves unrolling the optimization for a fixed number of steps (20-40), computing the sum of optimizee losses, and backpropagating to update the LSTM parameters using Adam (lr=0.001). The approach is evaluated on least-squares problems, synthetic neural network functions, and MNIST classification.

## Key Results
- L2O-CFGD achieves faster convergence than static NA-CFGD and AT-CFGD on least-squares and neural network tasks.
- Meta-learned α schedules show rapid initial increase followed by stabilization, contrasting with static methods that plateau.
- Layer-specific tuning of β reveals distinct history-dependence requirements for input-to-hidden versus hidden-to-output weights.
- The method generalizes from meta-training to unseen functions, outperforming black-box L2O with fixed hyperparameters.

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Regularization via Fractional Order Scheduling
Static fractional orders limit convergence; dynamic scheduling of α (fractional order) accelerates optimization by adapting the smoothness of the effective objective function over time. The meta-learner predicts a time-varying α(t). A lower α implies heavier regularization (smoothing) of the loss landscape, aiding early exploration. As optimization progresses, increasing α reduces smoothing, allowing for finer convergence. Core assumption: The optimal degree of fractional regularization changes predictably as the iterate moves from initial guess to local minimum. Evidence anchors: [abstract] Mentions the method "meta-learns how to dynamically tune the hyperparameters." [section 4.1] Figure 2 shows α increasing rapidly during the initial loss drop and stabilizing, contrasting with static CFGD which plateaus. [corpus] *Theoretical Framework for Tempered Fractional Gradient Descent* supports the general principle that modulating fractional parameters stabilizes convergence. Break condition: If the loss landscape is uniformly flat or uniformly convex, the benefits of dynamic smoothing may diminish, reducing the advantage over static methods.

### Mechanism 2: Coordinate-wise Heterogeneous History Dependence
Assigning distinct hyperparameters (β and c) to different parameter groups (e.g., layers) improves performance by tailoring the history-dependence of the fractional derivative to the specific topology of the network. The Caputo derivative integrates gradient history. The meta-learner learns separate schedules for input-to-hidden vs. hidden-to-output weights. Specifically, β (smoothing parameter) modulates how much second-order information (Hessian approximation) is mixed into the update, effectively varying the "memory" length per coordinate. Core assumption: Different layers or parameter groups exhibit distinct curvature or gradient noise characteristics requiring distinct fractional dynamics. Evidence anchors: [section 4.2.2] Figure 8 shows distinct β schedules for input-to-hidden versus hidden-to-output layers, suggesting layer-specific optimization strategies. [section 2.2] Equation 6 defines β as a vector, allowing per-coordinate variation. [corpus] *Effective Dimension Aware Fractional-Order Stochastic Gradient Descent* implies dimensional sensitivity in fractional methods, though this paper operationalizes it via meta-learning. Break condition: If the network is very shallow or parameters are homogeneously initialized and normalized (e.g., specific transformer architectures with weight tying), the benefit of coordinate-wise tuning may be outweighed by the computational overhead.

### Mechanism 3: Tractable Second-Order Approximation via Hutchinson's Method
The fractional gradient requires Hessian diagonal information; Hutchinson's method provides a sufficiently accurate approximation to drive the meta-learned updates without prohibitive computation. The Caputo fractional gradient update (Eq. 6) involves a term proportional to the Hessian diagonal. The method estimates this diagonal using random projection vectors (Hutchinson's estimator) rather than exact computation, making the update feasible for high-dimensional problems. Core assumption: The variance of the Hutchinson estimator does not destabilize the meta-learner's ability to predict hyperparameters. Evidence anchors: [section 3.2] Describes using Hutchinson's method to approximate the Hessian diagonal for Eq. 15. [section 4] Notes using "3 Hutchinson steps per iteration" for L2O-CFGD. [corpus] *More Optimal Fractional-Order Stochastic Gradient Descent* highlights the general difficulty of stabilizing fractional methods, which this approximation attempts to address computationally. Break condition: If the objective function is extremely noisy or non-smooth, the variance of the Hutchinson estimator may corrupt the gradient direction, causing divergence.

## Foundational Learning

- **Concept: Caputo Fractional Derivative**
  - **Why needed here:** This is the core mathematical operator replacing the standard gradient. You must understand that unlike integer-order derivatives, it depends on the integral of the function's history (via a weighted kernel), introducing non-local memory effects.
  - **Quick check question:** How does the Caputo derivative behave differently from the standard derivative when the function $f(x)$ is constant?

- **Concept: Meta-Learning / Learning to Optimize (L2O)**
  - **Why needed here:** The "L2O" in L2O-CFGD refers to training an RNN (the optimizer) to output hyperparameters for another network (the optimizee). You need to grasp the concept of "unrolling" the optimization loop to differentiate through the training trajectory.
  - **Quick check question:** In the L2O framework, what is the loss function used to update the optimizer's weights $\phi$?

- **Concept: Gauss-Jacobi Quadrature**
  - **Why needed here:** The paper uses this numerical integration technique to approximate the integral inside the Caputo derivative.
  - **Quick check question:** Why is Gauss-Jacobi quadrature specifically suited for the integral form seen in Equation 3 (involving terms like $(1-u)^{-\alpha}$)?

## Architecture Onboarding

- **Component map:** Optimizee -> Standard gradients -> LSTM (with time encoding) -> α, β, c -> Hutchinson Hessian approx -> Fractional gradient update -> Optimizee update
- **Critical path:** 1. Forward pass: Optimizee computes loss and standard gradients. 2. RNN inference: RNN takes gradients + time encoding → outputs α, β, c. 3. Fractional calc: Approximate Hessian diagonal (Hutchinson) → Compute fractional gradient (Eq. 15). 4. Update: Apply update to optimizee. 5. Backward pass (Meta-update): After u steps, backpropagate the unrolled loss to update the RNN weights φ.
- **Design tradeoffs:**
  - Unroll length (u): Longer unrolls capture longer-term optimization dynamics but explode memory usage and suffer from vanishing gradients.
  - Hutchinson steps: More steps yield a better Hessian approximation but cost O(steps × parameters) compute.
  - Quadrature points (s): The paper suggests s=1 is often sufficient; increasing this adds compute with diminishing returns.
- **Failure signatures:**
  - Convergence to non-critical points: If α remains too low, the method minimizes a smoothed proxy of the loss, not the actual loss (the "bias" issue mentioned in [section 1]).
  - Instability in early meta-training: The optimizer might output α ≈ 0 or large β, causing exploding updates.
  - Memory overflow: Occurs if the unroll length is too large relative to GPU memory during the meta-training backward pass.
- **First 3 experiments:**
  1. **Quadratic Baseline:** Implement the least-squares task (Section 4.1) to verify the implementation of Eq. 8 (closed form) vs. the meta-learned output.
  2. **Static vs. Dynamic Ablation:** Compare L2O-CFGD against NA-CFGD with the best fixed hyperparameters found via grid search on a simple convex function to confirm the "dynamic" advantage.
  3. **Neural Network Generalization:** Meta-train on the h₁ function and test on h₂ (as per Section 4.2.1) to ensure the RNN isn't just memorizing a specific loss landscape's curvature.

## Open Questions the Paper Calls Out
- **Open Question 1:** What is the theoretical and empirical relationship between neural network depth and the optimal history-dependence (governed by the β parameter) required for fractional gradient descent? [explicit] The authors state in Section 4.2.2 that the observation of separate β schedules for different layers "opens up an intriguing research direction to explore the relationship between neural network depth and the history-dependence required for the fractional differential used in optimizing the particular layer." Why unresolved: The current study only visualizes the differing schedules but does not formulate a generalized rule or theory connecting layer depth to the smoothing parameter β. What evidence would resolve it: A theoretical analysis or a comprehensive empirical study across various network architectures (depths/widths) that derives or identifies a consistent pattern linking layer depth to the optimal β schedule.

## Limitations
- The paper does not analyze how the stochastic noise from Hutchinson's approximation affects optimizer stability compared to exact second-order methods.
- The coordinate-wise heterogeneity mechanism assumes different layers have meaningfully distinct curvature, which may not hold for all architectures.
- The theoretical guarantee that dynamic tuning resolves the convergence bias of fixed-parameter CFGD (converging to a point that is not the minimum) is not established.

## Confidence
- Dynamic regularization mechanism: High
- Coordinate-wise heterogeneity: Medium
- Hutchinson approximation tractability: Medium

## Next Checks
1. Implement a controlled experiment varying the number of Hutchinson steps (1, 3, 5) to quantify the variance-bias tradeoff in the Hessian approximation and its impact on meta-learned hyperparameter stability.
2. Test the meta-learned optimizer on a non-convex function with known local minima structure to verify it actually converges to better minima rather than just faster to any minimum.
3. Perform an ablation study where the LSTM outputs only α (not β or c) to determine if the full coordinate-wise heterogeneity provides significant benefit over simpler dynamic scheduling.