---
ver: rpa2
title: Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for
  Just-in-Time Programming of AAC
arxiv_id: '2507.21811'
source_url: https://arxiv.org/abs/2507.21811
tags:
- hotspots
- vsds
- communication
- prototype
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether generative AI can assist non-experts,
  specifically pre-service speech-language pathologists, in creating effective visual
  scene displays (VSDs) for augmentative and alternative communication (AAC). The
  authors developed a prototype that automatically suggests hotspots for images using
  a multimodal large language model.
---

# Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC

## Quick Facts
- arXiv ID: 2507.21811
- Source URL: https://arxiv.org/abs/2507.21811
- Reference count: 40
- Primary result: GenAI assistance speeds up AAC visual scene display creation but reduces personalization and communication diversity

## Executive Summary
This paper explores whether generative AI can assist non-experts, specifically pre-service speech-language pathologists, in creating effective visual scene displays (VSDs) for augmentative and alternative communication (AAC). The authors developed a prototype that automatically suggests hotspots for images using a multimodal large language model. In a user study, participants created VSDs faster and with greater confidence when using the AI-assisted prototype compared to existing software. However, the quality of VSDs was mixed: while hotspots were more developmentally appropriate, participants used too many hotspots and included less socially-focused vocabulary. The study also found participants relied heavily on AI suggestions and created more homogeneous VSDs, raising concerns about personalization in AAC device design. The findings suggest AI can improve efficiency and confidence but requires careful implementation to avoid negative impacts on communication diversity and quality.

## Method Summary
The study developed a web application prototype that automatically suggests hotspots on images for VSD configuration using GPT-4o multimodal LLM. The prototype allowed participants to upload images, receive AI-generated hotspot suggestions, and manually edit or add hotspots. A between-subjects user study compared the prototype to existing manual VSD creation software (Snap Scene) with 33 pre-service speech-language pathology students. Participants created VSDs for two different images, with half using the prototype and half using manual tools. The study measured creation time, user confidence, hotspot quality, and semantic similarity between VSDs to assess efficiency, user experience, and personalization effects.

## Key Results
- VSD creation time reduced from 77 seconds (manual) to 63.5 seconds (AI-assisted)
- User confidence increased significantly with AI assistance
- AI-generated hotspots were more developmentally appropriate than manual ones
- Prototype VSDs were more semantically homogeneous across users compared to manual creation
- Participants accepted AI suggestions unmodified 61.8% of the time

## Why This Works (Mechanism)

### Mechanism 1: Efficiency via Reduced Cognitive Load
- Claim: GenAI suggestions reduce VSD configuration time by providing a starting point that users evaluate rather than generate from scratch.
- Mechanism: The system offloads vocabulary selection and hotspot identification to a multimodal LLM, converting a generative task into an evaluative one, which requires fewer cognitive resources and less domain expertise.
- Core assumption: Users can assess relevance and appropriateness of suggested hotspots more efficiently than creating them de novo.
- Evidence anchors:
  - Wilcoxon Signed-rank test shows statistically significant effect on time-to-create (W = 438.5, Z = 3.2635, p < 0.05) with medium effect size (r = 0.41)
  - Related work on automated vocabulary generation supports efficiency gains

### Mechanism 2: Confidence through Task Scaffolding
- Claim: Providing initial hotspot suggestions increases user confidence by reducing perceived task difficulty and ambiguity.
- Mechanism: The prototype reduces the "blank page" problem by offering concrete starting points, which improves self-efficacy—particularly for users unfamiliar with VSD design patterns.
- Core assumption: Confidence gains are not solely due to novelty effects or perceived AI authority, but to genuine reduction in task uncertainty.
- Evidence anchors:
  - Wilcoxon Signed-rank test shows significant effect of prototype on users' confidence (W = 5, Z = -2.8043, p < 0.05) with medium effect size (r = 0.496)
  - Prior work in assistive technology adoption links confidence to reduced abandonment rates

### Mechanism 3: Homogenization via Convergent AI Defaults
- Claim: AI-generated hotspot suggestions produce more semantically similar VSDs across users, reducing diversity of communication options.
- Mechanism: Without user-specific personalization data, the multimodal LLM generates contextually plausible but generic suggestions that cluster around common visual elements, producing outputs that converge toward "sensible defaults" rather than personalized configurations.
- Core assumption: Users' tendency to accept suggestions without modification (observed behavior) amplifies this convergence.
- Evidence anchors:
  - Statistically significant difference in homogeneity (t(31) = 3.4037, p < 0.05, d = 0.60); prototype VSDs more similar to average embedding (x̄ = 0.39 ± 0.09) vs. Snap Scene (x̄ = 0.46 ± 0.07)
  - 61.80% of generated hotspots used unmodified; 77.12% of all prototype hotspots were AI-generated (edited or not)

## Foundational Learning

- Concept: **Visual Scene Displays (VSDs)**
  - Why needed here: VSDs are the core artifact being configured—images with embedded interactive hotspots that map to spoken vocabulary, used by emergent communicators.
  - Quick check question: Can you explain why VSDs are preferred over grid displays for minimally verbal autistic children learning symbolic language?

- Concept: **Just-in-Time (JIT) Programming**
  - Why needed here: The prototype targets JIT scenarios where communication partners configure VSDs in real-time during spontaneous interactions, making speed and ease-of-use critical.
  - Quick check question: What are the tradeoffs between pre-configured VSDs and JIT-configured VSDs for end-user communication outcomes?

- Concept: **Light's Functions of Communication**
  - Why needed here: The paper uses this framework (expressing wants/needs, information transfer, social closeness, social etiquette) to analyze hotspot quality and identify that AI-assisted VSDs under-represent social closeness functions.
  - Quick check question: If a VSD has 10 hotspots focused on nouns but none supporting social closeness, what communication capability is being constrained?

## Architecture Onboarding

- Component map:
  Input Layer -> AI Inference Layer -> Suggestion Engine -> Editing Interface -> Preview Layer

- Critical path:
  1. User uploads/captures image → 2. System sends image + prompt to GPT-4o → 3. LLM returns hotspot suggestions → 4. User reviews, edits, deletes, or adds hotspots → 5. User draws hotspot regions on image → 6. Preview and finalize

- Design tradeoffs:
  - **Speed vs. Quality**: Automatic suggestions accelerate configuration but introduce irrelevant hotspots and reduce personalization.
  - **Scaffolding vs. Over-reliance**: Providing suggestions increases confidence but users often accept without modification (77.12% of hotspots AI-generated).
  - **Generalizability vs. Personalization**: Generic prompt (no user-specific data) enables any-image input but produces homogenized outputs; personalized prompting would require user modeling infrastructure.
  - **Hotspot Count**: Prompt did not limit hotspot count; median 5 hotspots (prototype) vs. 3 (manual) potentially overloads emergent communicators.

- Failure signatures:
  - **Excessive hotspots**: >4 hotspots risk overwhelming users (stimulus over-selectivity)
  - **Irrelevant suggestions**: Hotspots not related to scene people/activities (35.4% of generated hotspots)
  - **Social function deficit**: <1% of prototype hotspots served social closeness vs. 5.3% in manual condition
  - **Homogenized vocabulary**: High semantic similarity across users' outputs

- First 3 experiments:
  1. **Prompt iteration for hotspot count**: Modify prompt to explicitly cap suggestions at 3-4 hotspots; measure impact on quality metrics (relevance, developmental appropriateness) and user behavior (deletion rate).
  2. **Social closeness injection**: Add explicit prompt instruction to include at least one social closeness hotspot per VSD; analyze function distribution changes and user acceptance rates.
  3. **Cognitive forcing intervention**: Require users to modify or justify at least one AI suggestion before finalizing; measure impact on homogeneity and personalization while tracking time cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the use of GenAI-assisted VSD tools by untrained communication partners (e.g., parents) result in higher rates of over-reliance and homogenization compared to pre-service SLPs?
- Basis in paper: The authors explicitly state in the Limitations section that "Future work could also include other communication partners such as parents, caregivers, and friends" and hypothesize that "trends of over-reliance and homogenization would be more apparent with these communication partners."
- Why unresolved: The study was restricted to pre-service SLPs who possess domain-specific knowledge about language development, which may have acted as a buffer against the negative impacts observed.
- What evidence would resolve it: A comparative user study measuring acceptance rates of AI suggestions and semantic diversity of output between expert-in-training groups and layperson groups.

### Open Question 2
- Question: Can specific interaction design interventions, such as reflection prompts or validation steps, effectively mitigate over-reliance on AI suggestions without negating the efficiency benefits?
- Basis in paper: Page 19 suggests that "interaction methods... which can be implemented to apply checks to ensure appropriateness and quality" are needed, specifically proposing "prompting the users to reflect upon their created hotspots" as a potential solution.
- Why unresolved: The current prototype allowed users to accept suggestions passively (61.80% were accepted unmodified), and the authors did not test whether specific "guardrails" would force critical engagement or simply frustrate the user.
- What evidence would resolve it: An experimental study comparing a control interface against interfaces equipped with "cognitive forcing functions" or reflection prompts, measuring both task time and the quality of modifications.

### Open Question 3
- Question: How can AI-enabled AAC systems be designed to actively promote semantic diversity and personalization rather than converging on homogenized communication defaults?
- Basis in paper: While the authors note that "more research needs to be done to understand how to apply these potential solutions [to homogenization]," they do not propose a specific design solution for the AAC context.
- Why unresolved: The study demonstrated that GenAI suggestions caused a convergence in vocabulary (higher semantic similarity), effectively undermining the clinical goal of personalization, but the technical or interaction design solution to this specific problem remains unidentified.
- What evidence would resolve it: A study testing algorithms or interfaces that explicitly incentivize "out-of-the-box" vocabulary or require personal user data integration to verify if homogenization can be statistically reduced.

## Limitations
- The study was limited to pre-service SLPs rather than actual AAC end-users or communication partners
- The quality metrics focused on VSD configuration rather than actual communication outcomes or user engagement
- The prototype lacked integration with actual AAC devices and did not measure long-term effects on communication patterns

## Confidence

- **High Confidence**: Efficiency improvements (creation time reduction from 77s to 63.5s) and statistical significance of confidence gains
- **Medium Confidence**: The homogenization findings and their implications for personalization
- **Low Confidence**: Claims about the specific mechanisms driving user acceptance of AI suggestions and the long-term impact on communication diversity and quality

## Next Checks
1. **User Agency Measurement**: Conduct think-aloud protocols during VSD creation to distinguish between active selection and passive acceptance of AI suggestions, measuring the degree of critical evaluation applied to each suggestion.
2. **End-User Testing**: Replicate the study with actual AAC communication partners (parents, teachers) and measure not just VSD creation metrics but also subsequent communication acts using the created VSDs over multiple sessions.
3. **Longitudinal Homogenization Study**: Track VSD vocabulary diversity across multiple sessions and users over time to determine whether initial homogenization effects persist or whether users develop strategies to maintain personalization despite AI assistance.