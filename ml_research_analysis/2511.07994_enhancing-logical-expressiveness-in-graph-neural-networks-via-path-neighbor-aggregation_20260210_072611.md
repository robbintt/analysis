---
ver: rpa2
title: Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor
  Aggregation
arxiv_id: '2511.07994'
source_url: https://arxiv.org/abs/2511.07994
tags:
- logical
- pn-gnn
- c-gnn
- rule
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PN-GNN, a method to enhance the logical expressive
  power of graph neural networks by aggregating node-neighbor embeddings on reasoning
  paths. The authors theoretically analyze the limitations of existing C-GNN methods
  and demonstrate that PN-GNN has strictly stronger logical expressiveness than C-GNN.
---

# Enhancing Logical Expressiveness in Graph Neural Networks via Path-Neighbor Aggregation

## Quick Facts
- **arXiv ID:** 2511.07994
- **Source URL:** https://arxiv.org/abs/2511.07994
- **Reference count:** 38
- **Primary result:** PN-GNN achieves 100% accuracy on synthetic reasoning tasks and competitive performance on FB15K237/WN18RR while strictly outperforming C-GNN in logical expressiveness

## Executive Summary
This paper introduces PN-GNN, a method that enhances the logical expressiveness of graph neural networks by aggregating node-neighbor embeddings along reasoning paths. The authors theoretically demonstrate that PN-GNN can learn logical rule structures (specifically the U structure) that are beyond the capability of existing Conditional GNN methods. Experiments on synthetic and real-world datasets show that PN-GNN achieves perfect accuracy on synthetic tasks while maintaining strong performance on knowledge graph reasoning benchmarks without the generalization issues seen in constant-labeling approaches.

## Method Summary
PN-GNN builds upon Conditional GNNs by introducing path-neighbor aggregation. After computing standard C-GNN representations through L iterations of conditional message passing, PN-GNN identifies nodes w on paths between head entity u and candidate tail v. It pools representations of these path neighbors based on their distances (i,j) from (u,v), applies MLPs to each pooled vector, and fuses the results with the original C-GNN tail representation. This enables the model to capture structural patterns like the U rule that C-GNN cannot distinguish, while avoiding the generalization problems of constant-labeling tricks.

## Key Results
- PN-GNN achieves 100% accuracy on most synthetic reasoning tasks (C3, C4, I1, I2, T) compared to 60-80% for EL-GNN and 50-75% for NBFNet
- On the U dataset, PN-GNN reaches 69.9% accuracy versus 54.1% for NBFNet and 60.0% for EL-GNN
- Maintains strong generalization: 60.0% on T_label (unseen entities) versus 22.0% for EL-GNN
- Shows competitive performance on real-world datasets: FB15K237 (MRR 0.351) and WN18RR (MRR 0.515) comparable to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
Path-neighbor aggregation enables distinction of logical rule structures (like U) that C-GNN cannot differentiate. PN-GNN aggregates representations of intermediate nodes on paths between head entity u and tail entity v, specifically pooling neighbors at hop distances (i,j) from (u,v). For the U structure, this captures whether multiple edges originate from the same intermediate node—information lost in standard C-GNN message passing.

### Mechanism 2
Avoiding constant labels preserves generalization to unseen entities in inductive settings. Unlike labeling tricks that fix variables to specific constants (reducing rule coverage), PN-GNN uses variable-based aggregation. Path neighbors are identified by their structural position (distance i,j), not by entity-specific labels, maintaining full coverage of the original logical rules.

### Mechanism 3
Expressive power increases strictly with hop depth (k+1 > k). Higher hops aggregate neighbors at greater distances, enabling representation of longer-chain logical rules. The paper proves (k+1)-hop can represent all k-hop expressions plus additional rules unreachable at depth k, with the tradeoff between expressive power and computational cost being favorable up to k=2 for typical KG reasoning tasks.

## Foundational Learning

- **Concept: Counting Modal Logic (CML) in Knowledge Graphs**
  - Why needed here: The paper uses CML to formalize what logical rules C-GNN can learn and why U is beyond C-GNN's capacity. Understanding ∃^N (counting quantifiers) and recursive CML rules is essential to follow the expressiveness proofs.
  - Quick check question: Can you explain why the formula φ(x) = ∃^≥2 z(r(x,z) ∧ P(z)) differs from ∃z₁∃z₂(r(x,z₁) ∧ P(z₁) ∧ r(x,z₂) ∧ P(z₂)))?

- **Concept: Conditional Message Passing (C-GNN)**
  - Why needed here: PN-GNN builds on C-GNN representations h^(L)_v|u,q. You must understand how C-GNN conditions on (head entity, query relation) pairs and iteratively updates representations.
  - Quick check question: In Eq. (1), why does the initialization function INIT(u,v,q) mark the head entity u differently from other nodes?

- **Concept: Weisfeiler-Leman (WL) Testing and Graph Isomorphism**
  - Why needed here: The paper grounds expressiveness analysis in R-WL tests and relates logical expressivity to graph discrimination. This is the theoretical basis for comparing PN-GNN to C-GNN.
  - Quick check question: Why does the 1-WL test fail to distinguish certain non-isomorphic graphs, and how does this relate to GNN limitations?

## Architecture Onboarding

- **Component map:** C-GNN backbone -> Path neighbor pooler -> Fusion layer -> Scorer
- **Critical path:** 1) Run C-GNN for L iterations to get all node representations 2) For each candidate tail v, identify path neighbors w at distances (1,1), (1,2), (2,1) from (u,v) 3) Pool representations within each distance bucket 4) Fuse with C-GNN tail representation 5) Score and rank candidates
- **Design tradeoffs:**
  - Hop depth (d): Higher d → more expressive but O(|V|^d) path enumeration. Paper uses d=2 as default.
  - Pool function: Max preserves strongest signals; mean averages; min captures weakest. Paper tests max/mean/min without strong preference.
  - Fusion operator: Concatenation preserves full information but increases dimension; addition is cheaper but may lose signal.
- **Failure signatures:**
  - Over-smoothing: If paths are too long or graphs too dense, aggregated representations become uniform
  - Memory blowup: Storing all pairwise path neighbors scales poorly; need sampling or pruning
  - No improvement over C-GNN: Check if dataset contains U-like structures; if only chain rules (C_k), C-GNN is already sufficient
- **First 3 experiments:**
  1. Sanity check on synthetic C_3: Train PN-GNN and C-GNN on chain-rule dataset. Both should achieve ~100% Hits@1.
  2. Expressiveness test on U: Compare PN-GNN vs. NBFNet on U dataset. Expect ~15% improvement.
  3. Generalization test on T_label vs. T: Train EL-GNN and PN-GNN on T_label. EL-GNN should drop significantly (22.0 vs 100); PN-GNN should remain stable (60.0).

## Open Questions the Paper Calls Out

### Open Question 1
How can the computational efficiency of PN-GNN be optimized to handle larger graphs and deeper hop requirements without sacrificing logical expressiveness? The conclusion states that PN-GNN "faces increased computational costs in multi-hop scenarios" and lists enhancing "efficiency and expressiveness for larger, more complex tasks" as future work.

### Open Question 2
Can the negative impact of noise from multi-hop aggregation in sparse datasets be mitigated through adaptive hop selection? The ablation study notes that for WN18RR v1, "considering longer paths... may introduce noise," suggesting the fixed hop count strategy is suboptimal for all data distributions.

### Open Question 3
How does the choice of the pooling function (e.g., max, mean, min) theoretically impact the logical expressiveness of the path-neighbor aggregation? The method section mentions the pooling function "can be set to any commonly used function in GNN," but the paper does not analyze if different functions preserve the theoretical guarantees of CML expressiveness.

## Limitations
- Scalability concerns: The path-neighbor enumeration (O(|V|^d) for d=2) may become prohibitive for large graphs
- Limited generalization to non-U structures: The expressiveness gains are demonstrated primarily for U structures
- Fusion function sensitivity: The paper tests max/mean/min pooling and concatenation/addition fusion without identifying optimal configurations

## Confidence

- **High confidence** in theoretical contribution: The logical expressiveness proofs are rigorous and well-grounded in established CML and WL-testing literature
- **Medium confidence** in empirical claims: The synthetic dataset results are convincing (100% accuracy on T, strong U performance), but real-world datasets show more modest gains over strong baselines
- **Medium confidence** in mechanism explanations: While the path-neighbor mechanism is well-defined, the exact reasons for varying pooling function performance across datasets could be explored further

## Next Checks

1. **Scalability benchmark**: Measure memory and time complexity of PN-GNN on progressively larger graphs (10K → 100K → 1M entities) to identify practical limits
2. **Pattern coverage analysis**: Systematically test PN-GNN on synthetic datasets with different complex patterns (e.g., hypercubes, cliques) to map the boundaries of its expressiveness advantage
3. **Ablation on fusion design**: Conduct controlled experiments varying MLP depth, pooling functions, and fusion operators to identify optimal architectural choices for different KG types