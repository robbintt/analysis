---
ver: rpa2
title: 'BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation'
arxiv_id: '2512.19122'
source_url: https://arxiv.org/abs/2512.19122
tags:
- bangla
- code
- test
- generation
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BanglaForge, a framework for generating Python
  code from Bangla natural language descriptions. It addresses the low-resource challenge
  in Bangla code generation by combining retrieval-augmented prompting, bilingual
  translation with a controlled glossary, iterative self-refinement using execution
  feedback, and a dual-model coder-reviewer pipeline.
---

# BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation

## Quick Facts
- arXiv ID: 2512.19122
- Source URL: https://arxiv.org/abs/2512.19122
- Reference count: 40
- Primary result: Achieves 84% Pass@1 accuracy on BLP-2025 Bangla code generation benchmark using retrieval-augmented prompting and self-refinement

## Executive Summary
BanglaForge is a framework for generating Python code from Bangla natural language descriptions that addresses the low-resource challenge in Bangla code generation. The system combines retrieval-augmented prompting, bilingual translation with controlled glossary, iterative self-refinement using execution feedback, and a dual-model coder-reviewer pipeline. By retrieving relevant examples, generating initial code with a Coder LLM, and refining it with a Reviewer LLM until all test cases pass, BanglaForge achieves 84% Pass@1 accuracy on the BLP-2025 benchmark. The framework demonstrates that combining retrieval, model collaboration, and self-refinement can significantly improve performance in low-resource programming language settings.

## Method Summary
BanglaForge addresses the scarcity of Bangla programming resources through a multi-component approach. The framework uses retrieval-augmented prompting to find relevant code examples from a corpus, then employs bilingual translation with a controlled glossary to maintain semantic accuracy between Bangla prompts and code generation. The core innovation is a dual-model coder-reviewer pipeline where the Coder LLM generates initial Python code and the Reviewer LLM iteratively refines it based on execution feedback from test cases. This self-refinement loop continues until the generated code passes all test cases or reaches a maximum iteration limit. The framework is specifically designed to handle the challenges of low-resource Bangla code generation by leveraging both retrieved examples and collaborative refinement between specialized models.

## Key Results
- Achieves 84% Pass@1 accuracy on the BLP-2025 Bangla Code Generation benchmark
- Demonstrates 15% Pass@1 improvement when using retrieval-augmented prompting versus non-retrieval approaches
- Successfully generates Python code that passes all test cases through iterative self-refinement

## Why This Works (Mechanism)
BanglaForge leverages the complementary strengths of retrieval systems and large language models to overcome the scarcity of Bangla programming data. The retrieval component provides relevant context and examples that guide code generation, while the bilingual translation with controlled glossary ensures semantic accuracy between Bangla descriptions and generated code. The dual-model coder-reviewer pipeline allows for iterative refinement where the Reviewer LLM can identify and fix errors that the Coder LLM may have missed, particularly those that become apparent only through execution feedback. This self-refinement mechanism is particularly effective because it uses actual test case results as feedback, allowing the system to progressively improve code quality until it meets functional requirements.

## Foundational Learning
- **Retrieval-augmented generation**: Why needed - to provide relevant context when training data is scarce; Quick check - verify retrieved examples are semantically related to the prompt
- **Bilingual translation with controlled glossary**: Why needed - to maintain semantic accuracy between Bangla and programming languages; Quick check - ensure glossary terms are consistently translated across generations
- **Execution-based self-refinement**: Why needed - to iteratively improve code quality using concrete feedback; Quick check - confirm test cases effectively capture code correctness requirements
- **Dual-model coder-reviewer pipeline**: Why needed - to separate code generation from error detection and correction; Quick check - verify Reviewer LLM can identify and fix Coder LLM errors
- **Iterative refinement loops**: Why needed - to progressively improve solutions until they meet all requirements; Quick check - monitor convergence behavior and maximum iteration limits

## Architecture Onboarding

Component Map: Retrieval System -> Bilingual Translator -> Coder LLM -> Reviewer LLM -> Test Executor -> Refinement Loop

Critical Path: Bangla prompt → Retrieval → Translation → Code Generation → Execution Testing → Refinement (repeat until pass/fail)

Design Tradeoffs:
- Single-model vs dual-model approach: Dual-model provides specialized roles but increases computational cost
- Maximum iterations: Higher limits allow more refinement but increase latency and resource usage
- Test case quality: Comprehensive test cases enable effective self-refinement but require significant effort to create

Failure Signatures:
- Retrieval failure: Irrelevant examples lead to poor code generation
- Translation issues: Glossary limitations cause semantic drift between prompt and code
- Reviewer LLM limitations: Cannot identify or fix certain types of errors
- Test case inadequacy: Missing test cases prevent detection of some bugs

First Experiments:
1. Test retrieval effectiveness by comparing code generation with and without retrieved examples on identical prompts
2. Evaluate bilingual translation accuracy by checking glossary term consistency across multiple generations
3. Assess self-refinement efficiency by measuring iterations needed to reach convergence on various difficulty levels

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation limited to BLP-2025 benchmark with 280 samples, potentially not representing real-world diversity
- Reliance on execution-based self-refinement assumes availability of test cases, which may not be practical in all production environments
- Significant performance dependence on retrieval corpus quality and coverage, which is not thoroughly characterized

## Confidence
- High Confidence: The reported 84% Pass@1 accuracy on BLP-2025 is reproducible given the described methodology and benchmark specifications
- Medium Confidence: The effectiveness of retrieval-augmented approach is supported by comparative results, but impact of retrieval corpus quality remains uncertain
- Medium Confidence: Self-refinement loop's contribution to performance is demonstrated, but efficiency and limitations in complex scenarios are not fully explored

## Next Checks
1. Evaluate BanglaForge on additional Bangla code generation datasets or real-world programming tasks to assess generalizability beyond BLP-2025
2. Conduct ablation studies to quantify individual contributions of retrieval, bilingual translation, and self-refinement components
3. Test framework's robustness by introducing adversarial or ambiguous Bangla prompts and analyzing failure modes in coder-reviewer pipeline