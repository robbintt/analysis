---
ver: rpa2
title: 'AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning
  in Large Language Models'
arxiv_id: '2505.18978'
source_url: https://arxiv.org/abs/2505.18978
tags:
- mini
- deepseek
- mathematical
- llama
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AI4Math introduces a native Spanish benchmark of 105 university-level
  math problems across seven domains, addressing the lack of language-specific evaluation
  tools. The dataset, authored and peer-reviewed by Latin American STEM students,
  includes detailed step-by-step solutions and is designed to expose language-driven
  and domain-specific reasoning errors.
---

# AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2505.18978
- **Source URL:** https://arxiv.org/abs/2505.18978
- **Reference count:** 6
- **Key outcome:** Introduces native Spanish benchmark of 105 university-level math problems across seven domains, revealing persistent challenges in geometry, combinatorics, and probability while showing minimal language-dependent performance differences across tested models.

## Executive Summary
AI4Math presents a native Spanish benchmark specifically designed to evaluate large language models on university-level mathematical reasoning. The dataset comprises 105 carefully curated problems across seven mathematical domains, authored and peer-reviewed by Latin American STEM students to ensure cultural and linguistic authenticity. The benchmark addresses a critical gap in language-specific evaluation tools for mathematical reasoning, where most existing benchmarks are English-centric.

The evaluation of six prominent large language models under four different configurations (zero-shot and chain-of-thought, in both Spanish and English) revealed significant performance variations between models. While top performers like o3-mini, DeepSeek-R1 685B, and DeepSeek-V3 685B achieved over 70% accuracy, others such as LLaMA 3.3 70B and GPT-4o mini struggled below 40%. Notably, most models showed no significant performance degradation when switching between languages, with GPT-4o even performing better in Spanish during zero-shot prompting. The benchmark demonstrates the importance of native-language evaluation and highlights persistent challenges in specific mathematical domains.

## Method Summary
The AI4Math benchmark was developed through a community-driven approach involving STEM students from Latin American universities. The dataset contains 105 university-level mathematics problems distributed across seven domains: algebra, geometry, calculus, number theory, combinatorics, probability, and statistics. Each problem includes detailed step-by-step solutions and was subjected to multiple rounds of peer review to ensure accuracy and consistency. Six large language models were evaluated under four configurations: zero-shot and chain-of-thought prompting, in both Spanish and English languages. Performance was measured using standard accuracy metrics, with particular attention to domain-specific reasoning capabilities and potential language-dependent variations.

## Key Results
- Top models (o3-mini, DeepSeek-R1 685B, DeepSeek-V3 685B) achieved over 70% accuracy on the benchmark
- LLaMA 3.3 70B and GPT-4o mini remained below 40% accuracy, showing significant performance gaps
- Most models showed no significant performance drop between Spanish and English, with GPT-4o performing better in Spanish in zero-shot setting
- Geometry, Combinatorics, and Probability questions were persistently challenging across all models

## Why This Works (Mechanism)
The benchmark works by providing a standardized, culturally authentic evaluation framework that exposes both language-driven and domain-specific reasoning errors in large language models. By using native Spanish problems authored by Latin American students, the benchmark captures linguistic nuances and problem-solving approaches that may not be present in translated or English-centric datasets.

## Foundational Learning
- **Mathematical domain expertise** - Why needed: Ensures problems cover appropriate university-level concepts; Quick check: Verify problems align with standard undergraduate curricula
- **Spanish language proficiency** - Why needed: Captures authentic linguistic structures and terminology; Quick check: Confirm problems use regionally appropriate mathematical vocabulary
- **Peer review methodology** - Why needed: Maintains consistency and accuracy across problem solutions; Quick check: Establish inter-rater reliability metrics
- **Benchmark design principles** - Why needed: Ensures fair comparison across different model architectures; Quick check: Validate problem difficulty distribution
- **Large language model evaluation** - Why needed: Provides appropriate metrics for reasoning assessment; Quick check: Confirm alignment with established LLM evaluation standards
- **Cross-linguistic performance analysis** - Why needed: Identifies true language independence versus translation artifacts; Quick check: Test with controlled translation pairs

## Architecture Onboarding
**Component Map:** Problem dataset -> Model input configurations -> Prompt engineering -> Response generation -> Accuracy evaluation -> Domain-specific analysis
**Critical Path:** Problem selection → Peer review → Model configuration → Prompt application → Response evaluation → Domain analysis
**Design Tradeoffs:** Native Spanish problems provide authenticity but limit applicability to other languages; comprehensive domain coverage ensures thorough evaluation but increases dataset complexity; peer review ensures quality but adds development time
**Failure Signatures:** Domain-specific weaknesses (geometry, combinatorics, probability); language-dependent performance variations; significant accuracy gaps between top and bottom performing models
**First Experiments:** 1) Test individual domain performance to identify specific weaknesses, 2) Compare zero-shot versus chain-of-thought performance within each language, 3) Evaluate model performance on problems with similar mathematical concepts but different linguistic structures

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 105 problems remains relatively small for comprehensive benchmarking across all mathematical domains
- Peer review process lacks detailed documentation of reviewer selection criteria and inter-rater agreement metrics
- Performance differences between language settings may reflect ceiling effects rather than true language independence due to limited problem diversity

## Confidence
- **High:** Importance of native-language evaluation for mathematical reasoning
- **Medium:** Completeness of cross-linguistic performance comparisons
- **Low:** Generalization of findings to educational levels beyond university scope

## Next Checks
1. Expand the dataset to include at least 300 problems with broader domain coverage and difficulty ranges
2. Implement blind re-annotation of a subset of problems to establish inter-rater reliability metrics
3. Test additional specialized mathematical reasoning models and conduct ablation studies to isolate the impact of language-specific training versus general reasoning capabilities