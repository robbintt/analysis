---
ver: rpa2
title: Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty
  Estimations
arxiv_id: '2509.12661'
source_url: https://arxiv.org/abs/2509.12661
tags:
- uni00000011
- uni00000015
- uni00000014
- strategy
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of strategy preference bias in
  emotional support conversations generated by large language models (LLMs), where
  models show low accuracy in strategy selection and tend to over-rely on certain
  strategies, limiting adaptability to users' emotional needs. The core method idea
  is to identify and leverage the knowledge boundaries of LLMs by categorizing samples
  into highly known, weakly known, and unknown regions based on strategy selection
  accuracy and entropy.
---

# Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations

## Quick Facts
- arXiv ID: 2509.12661
- Source URL: https://arxiv.org/abs/2509.12661
- Reference count: 14
- Primary result: Significantly outperforms baselines on ESCov and ExTES datasets, improving strategy selection accuracy and reducing preference bias across multiple LLM backbones.

## Executive Summary
This paper addresses strategy preference bias in emotional support conversations generated by large language models, where models show low strategy selection accuracy and tend to over-rely on certain strategies. The authors identify that this bias stems from LLMs' internal knowledge distributions rather than just data imbalance during fine-tuning. They propose a knowledge boundary-aware reinforcement learning approach that categorizes samples into highly known, weakly known, and unknown regions based on strategy selection accuracy and entropy, then applies region-specific reward functions to optimize both proficiency and reduce preference bias.

## Method Summary
The approach uses a two-stage process: first, supervised fine-tuning on the full dataset to establish baseline proficiency; second, knowledge boundary delineation followed by reinforcement learning. For each input, K responses are sampled to compute accuracy (c_i) and entropy (e_i), categorizing samples into three regions. A dual reward function is then applied: for known/weakly known samples, rewards emphasize low entropy to reduce variability; for unknown samples, rewards encourage high entropy exploration. The method is trained using GRPO with a KL-divergence penalty to prevent policy drift, resulting in improved strategy selection accuracy and reduced preference bias across multiple LLM backbones.

## Key Results
- Significantly outperforms standard SFT and GRPO baselines on ESCov and ExTES datasets
- Achieves higher F1 scores and lower bias metrics (B) compared to baselines
- Consistent improvements in both strategy selection accuracy and reduced strategy overuse across different LLM sizes (7B-8B parameters)
- Ablation studies confirm the importance of weakly known samples and dual reward components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strategy preference bias in ESC originates from LLMs' internal knowledge distributions rather than solely from fine-tuning data imbalance
- Mechanism: The model exhibits confidence calibration based on underlying knowledge strength—over-relying on strategies within "known" regions while struggling with "weakly known" areas where partial knowledge creates inconsistent outputs
- Core assumption: Preference patterns in generation reflect the model's parametric knowledge boundaries acquired during pretraining
- Evidence anchors:
  - [abstract] "we first reveal the fundamental causes of the bias by identifying the knowledge boundaries of LLMs in strategy planning"
  - [section 1] "preference bias in ESC is not merely due to the data imbalance during the fine-tuning stage, but it is rooted in the internal knowledge distributions of LLMs in pretraining"
  - [corpus] Related work (DecoupledESC, Chain of Strategy Optimization) addresses similar bias issues but does not investigate knowledge boundary causes
- Break condition: If preference bias persists uniformly across all knowledge regions after intervention, the knowledge boundary hypothesis would be weakened

### Mechanism 2
- Claim: Weakly known samples are pivotal for bias mitigation because they represent partial knowledge where the model can be guided toward consistency without hallucination
- Mechanism: By computing accuracy-based confidence scores (c_i) from K repeated samplings, samples with 0 < c_i < 1 are identified as weakly known—the model has relevant knowledge but produces variable outputs, making them amenable to reward-guided convergence
- Core assumption: Sampling-based accuracy estimation reliably approximates the model's true knowledge state for each input
- Evidence anchors:
  - [abstract] "categorizing samples into highly known, weakly known, and unknown regions based on strategy selection accuracy and entropy"
  - [section 5.4.1] "training solely on weakly known samples reduces preference bias more effectively than the baseline, highlighting their value in reducing model overconfidence"
  - [corpus] Weak—related papers do not explicitly analyze weakly known samples as a mechanism for bias reduction
- Break condition: If models trained exclusively on weakly known samples show no bias reduction compared to full-dataset training, this mechanism would be challenged

### Mechanism 3
- Claim: Region-specific reward functions optimize differently based on knowledge certainty—encouraging convergence for known samples while promoting exploration for unknown samples
- Mechanism: For highly/weakly known samples, rewards maximize low entropy (r_known = 1 - e_i/log(|S|)) to reduce output variability. For unknown samples, rewards maximize high entropy (r_unknown = e_i/log(|S|)) to encourage exploration of potentially correct strategies
- Core assumption: Known samples should converge to single strategies while unknown samples benefit from exploration rather than forced convergence
- Evidence anchors:
  - [abstract] "reinforcement learning with a dual reward function that combines strategy accuracy and entropy-based confidence measures, tailored to each knowledge region"
  - [section 3.4] "For highly known and weakly known samples... we prioritize consistency by a reward that emphasizes low diversity... For unknown samples, where the model lacks proficiency, we encourage exploration"
  - [corpus] Related RL approaches (CARE, Future-Oriented Rewards) use reward shaping but without knowledge-region differentiation
- Break condition: If uniform reward functions across all regions achieve equivalent or better performance than region-specific rewards, the mechanism would be undermined

## Foundational Learning

- Concept: **Entropy as uncertainty measure**
  - Why needed here: Strategy entropy (e_i) quantifies output diversity across K samples, serving as the core metric for both knowledge categorization and reward calculation
  - Quick check question: Given K=10 sampled responses with strategy distribution {Question: 7, Affirmation: 2, Reflection: 1}, can you compute the empirical entropy?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL stage uses GRPO to optimize the dual reward function while maintaining stability via KL-divergence penalty
  - Quick check question: How does GRPO differ from standard PPO in handling group-based reward comparisons?

- Concept: **Knowledge boundary hypothesis**
  - Why needed here: The paper's core claim relies on LLMs having distinguishable regions of knowledge strength, borrowed from hallucination research
  - Quick check question: What behavior differences would you expect between a model's response to a "known" vs. "unknown" factual query?

## Architecture Onboarding

- Component map:
  - **Stage 1 (SFT)**: Pre-trained LLM → Task/format fine-tuning on full dataset D → Fine-tuned model π₀
  - **Stage 2 (Knowledge Delineation)**: For each h_i, generate K responses with few-shot prompting → Compute accuracy c_i and entropy e_i → Categorize into regions
  - **Stage 3 (RL)**: Initialize policy π = π₀ → For each batch, compute region-specific reward r(h_i, ȳ_i) = c_i + r_region - β·KL → Update via GRPO
  - **Inference**: Optimized policy generates response with strategy prefix

- Critical path:
  1. SFT quality determines initial strategy proficiency—poor SFT makes knowledge boundary estimation unreliable
  2. Sampling number K affects boundary precision—K=10 balances estimation accuracy with compute cost
  3. KL penalty coefficient β prevents policy drift—set to 0.001 in experiments

- Design tradeoffs:
  - Larger K improves boundary estimation but increases compute linearly
  - Training only on weakly known samples reduces bias but may sacrifice proficiency (Table 3 shows accuracy drops from 22.10 to 18.16 for Qwen)
  - Entropy-only rewards show unstable performance; accuracy-entropy combination is more robust (Figure 3b)

- Failure signatures:
  - Strategy F1 improves but bias metric B increases → Likely using uniform rewards instead of region-specific
  - High accuracy on validation but low human satisfaction scores → May be overfitting to specific strategies without genuine support quality
  - Unknown-region samples never improve → Exploration reward may be insufficient or KL penalty too high

- First 3 experiments:
  1. **Baseline replication**: Run SFT-only on ESCov with LLaMA-3.1-8B, measure macro F1 (Q) and bias (B) to establish starting point (~21.79 F1, 1.30 B expected per Table 1)
  2. **Ablation on K**: Test K ∈ {3, 5, 10} for knowledge boundary estimation to verify plateau behavior shown in Figure 4
  3. **Reward component isolation**: Compare accuracy-only, entropy-only, and dual rewards to confirm complementary effect in Figure 3b

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the knowledge boundary-aware RL approach effectively generalize to other strategy-centric dialogue tasks beyond emotional support, such as negotiation, persuasion, or tutoring?
- Basis in paper: [explicit] The authors state: "In the future, we will extend our approach to other dialogue tasks, and refine knowledge boundary definitions to adapt to different downstream tasks."
- Why unresolved: The current work only validates the method on ESC tasks (ESCov and ExTES datasets). Different dialogue tasks may have different strategy taxonomies, success metrics, and uncertainty patterns that could affect transferability.
- What evidence would resolve it: Systematic evaluation of the dual-reward approach on at least two other strategy-centric dialogue domains, comparing performance against domain-specific baselines using task-appropriate proficiency and preference metrics.

### Open Question 2
- Question: How can knowledge boundary definitions be dynamically refined to adapt to evolving model capabilities during training, rather than relying on fixed pre-training distributions?
- Basis in paper: [explicit] The conclusion explicitly calls for work to "refine knowledge boundary definitions to adapt to different downstream tasks."
- Why unresolved: The current method identifies knowledge boundaries from a pre-trained model before RL fine-tuning. However, as the model learns, its knowledge boundaries shift—samples initially classified as "unknown" may become "weakly known" or "highly known," but the current approach uses static categorization.
- What evidence would resolve it: Experiments comparing static vs. adaptive boundary re-estimation during RL training, measuring whether dynamic recategorization improves final proficiency and bias metrics on held-out test sets.

### Open Question 3
- Question: Does the approach scale effectively to larger LLMs (70B+ parameters) and maintain bias reduction benefits, or do emergent capabilities in larger models change the relationship between knowledge boundaries and strategy preference?
- Basis in paper: [inferred] The paper only tests on 7B-8B parameter models (Qwen2.5-7B-Instruct and LLaMA-3.1-8B-Instruct). Scaling behavior is not discussed, yet prior work suggests larger models exhibit different calibration and bias properties.
- Why unresolved: Larger models may have more nuanced knowledge boundaries (smaller "unknown" regions) or different uncertainty estimation properties, potentially requiring adjusted reward formulations or sampling strategies.
- What evidence would resolve it: Evaluation on at least one 70B+ parameter model using identical methodology, with analysis of how knowledge boundary distributions (proportions of highly known/weakly known/unknown samples) differ from smaller models.

## Limitations
- The knowledge boundary hypothesis relies on sampling-based accuracy and entropy measures that may not capture true knowledge states
- Weak correlation between accuracy and entropy suggests these metrics may not capture orthogonal aspects of knowledge as claimed
- Region-specific reward design for unknown samples uses pure entropy maximization, which may promote random output generation rather than genuine strategy discovery

## Confidence
- **High confidence**: The empirical results showing reduced preference bias and improved strategy selection accuracy across multiple LLM backbones and datasets
- **Medium confidence**: The knowledge boundary hypothesis as the fundamental cause of preference bias
- **Medium confidence**: The weakly known sample mechanism

## Next Checks
1. **Knowledge boundary sensitivity**: Systematically vary the sampling temperature T and number K to measure stability of knowledge region categorization
2. **Alternative bias sources**: Design controlled experiments where LLMs are fine-tuned on balanced datasets but with different pretraining domains
3. **Unknown region effectiveness**: Compare the proposed entropy-only exploration for unknown samples against alternative approaches including accuracy-entropy hybrid rewards and knowledge distillation from larger models