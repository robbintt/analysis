---
ver: rpa2
title: Block-wise Adaptive Caching for Accelerating Diffusion Policy
arxiv_id: '2506.13456'
source_url: https://arxiv.org/abs/2506.13456
tags:
- layers
- layer
- timestep
- block
- caching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Block-wise Adaptive Caching (BAC) accelerates transformer-based
  Diffusion Policy by caching intermediate action features at the block level. The
  method uses an Adaptive Caching Scheduler to identify optimal update timesteps for
  each block, maximizing feature similarity between cached and skipped features via
  dynamic programming.
---

# Block-wise Adaptive Caching for Accelerating Diffusion Policy

## Quick Facts
- **arXiv ID**: 2506.13456
- **Source URL**: https://arxiv.org/abs/2506.13456
- **Reference count**: 40
- **Primary result**: Up to 3× inference speedup on transformer-based Diffusion Policy with maintained performance

## Executive Summary
Block-wise Adaptive Caching (BAC) accelerates transformer-based Diffusion Policy by caching intermediate action features at the block level. The method uses an Adaptive Caching Scheduler to identify optimal update timesteps for each block, maximizing feature similarity between cached and skipped features via dynamic programming. To prevent error propagation between blocks—particularly in Feed-Forward Network layers—BAC employs a Bubbling Union Algorithm that forces upstream blocks with large errors to update before downstream FFNs. As a training-free plugin, BAC integrates seamlessly with existing transformer-based Diffusion Policy and vision-language-action models.

## Method Summary
BAC introduces two key innovations: an Adaptive Caching Scheduler (ACS) that uses dynamic programming to compute optimal cache update schedules based on cosine similarity between features at different timesteps, and a Bubbling Union Algorithm (BUA) that prevents error propagation by forcing upstream blocks with large caching errors to update before downstream FFNs. The method is training-free and works as a plugin to existing transformer-based Diffusion Policy models. ACS computes per-block update schedules offline by solving a DP optimization to maximize cumulative feature similarity, while BUA refines these schedules by identifying high-error blocks via L1 norm averaging and unioning their update sets with downstream FFN schedules.

## Key Results
- Achieves up to 3× inference speedup without performance degradation
- Maintains stable acceleration rates above 3.4× across multiple robotic benchmarks
- Improves success rates on challenging tasks where uniform caching fails
- Works as a training-free plugin to existing transformer-based Diffusion Policy

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Caching Scheduler (ACS)
Optimally selecting cache update timesteps via dynamic programming reduces reuse-induced error more effectively than uniform caching schedules. ACS computes interval similarity scores between timesteps using cosine similarity, then solves a DP optimization to maximize cumulative similarity across the denoising trajectory. This identifies timesteps where cached features remain most valid, allowing selective recomputation. Core assumption: feature similarity patterns are consistent within a task (episode homogeneity), so the schedule computed once remains valid across inference episodes.

### Mechanism 2: Bubbling Union Algorithm (BUA)
Forcing upstream blocks with large caching errors to update before downstream FFNs prevents error surge propagation. BUA identifies high-error blocks via L1 norm averaging across timesteps, then unions their update schedules with downstream FFN schedules. This ensures upstream errors don't compound during FFN updates (which lack intermediate normalization). Core assumption: FFN blocks propagate upstream errors linearly (validated via Proposition 3.1's first-order approximation), and the top-k error-prone blocks are the critical failure points.

### Mechanism 3: Block-wise Heterogeneous Scheduling
Different transformer blocks (Self-Attention, Cross-Attention, FFN) exhibit distinct temporal similarity patterns requiring independent cache schedules. Each block type receives its own update timestep set computed via ACS. Self-attention shows high early-timestep similarity, FFN shows lower and more variable similarity—justifying different caching frequencies per block. Core assumption: Block-wise feature statistics are stable enough that per-block schedules don't require frequent recomputation.

## Foundational Learning

- **Concept: Diffusion Policy denoising trajectory**
  - Why needed: BAC exploits redundancy across the K-step reverse diffusion process; understanding this is prerequisite to grasping why caching works.
  - Quick check: Can you explain why action features might be similar at timesteps t and t-10 in a denoising process?

- **Concept: Transformer residual connections and error propagation**
  - Why needed: BUA is specifically designed to handle how FFN blocks propagate upstream caching errors through residual summation.
  - Quick check: In a residual block `h_out = h_in + FFN(LayerNorm(h_in))`, how would an error δ in h_in affect h_out?

- **Concept: Dynamic programming for sequence optimization**
  - Why needed: ACS reformulates cache schedule selection as a DP problem to avoid exponential search.
  - Quick check: Given similarity scores between timesteps, how would you formulate a DP state to maximize cumulative similarity with exactly M updates?

## Architecture Onboarding

- **Component map**: Feature extractor -> Similarity matrix computation -> DP solver (ACS) -> Error analysis (BUA) -> Caching engine -> Accelerated inference
- **Critical path**:
  1. Offline: Compute similarity matrices from a few sample episodes
  2. Offline: Run ACS per block to get initial schedules
  3. Offline: Apply BUA to union upstream block schedules
  4. Online: During denoising, check if current timestep is in block's update set; if not, reuse cached features
- **Design tradeoffs**:
  - k (top-k error blocks): Higher k = better error truncation but lower speedup (paper uses k=5)
  - S (cache update budget): Higher S = better quality but lower speedup (paper tests S=7,10)
  - Similarity metric: Cosine outperforms MSE/L1/Wasserstein for directional consistency in high-dim features
- **Failure signatures**:
  - Error surge in FFN blocks: Sudden performance collapse on hard tasks → indicates BUA not applied or k too small
  - Schedule mismatch: Quality degradation on new task variants → recompute similarity matrices
  - Speedup below expected: Check if all blocks actually using cached features; verify update set sizes
- **First 3 experiments**:
  1. Baseline validation: Run DP-T full precision vs. BAC (S=10, k=5) on Square task; expect ~3.4× speedup with maintained success rate
  2. Ablation on k: Compare k=3 vs. k=5 on Tool_hang task; expect k=5 to recover performance k=3 loses
  3. Cross-architecture test: Apply BAC to a different DiT-based policy; verify similarity patterns hold before expecting gains

## Open Questions the Paper Calls Out

- **Open Question 1**: Does BAC maintain its "lossless" performance and speedup when deployed on physical robots using Vision-Language-Action (VLA) models?
  - Basis: The authors state in Section 5 that they have "not yet conducted real-world VLA experiments" due to equipment constraints.
  - Why unresolved: Current experiments are limited to simulated benchmarks (Robomimic, Push-T, etc.), and real-world latency and noise may affect the caching scheduler differently.
  - What evidence would resolve it: Evaluation of success rates and inference latency on physical hardware running VLA models integrated with BAC.

- **Open Question 2**: How does BAC interact with base models that have inherently low accuracy, and can this amplification of inaccuracy be mitigated?
  - Basis: Section 5 notes that the caching strategy "may inadvertently amplify this inaccuracy" when the base model's accuracy is very low.
  - Why unresolved: The paper does not provide a mechanism to detect or adjust caching behavior when the base model itself is unreliable.
  - What evidence would resolve it: An analysis of BAC's performance degradation on tasks where the baseline policy success rate is intentionally reduced (e.g., below 50%).

- **Open Question 3**: Is the assumption of "high episode homogeneity" valid for long-horizon or highly dynamic tasks where the pre-computed schedule might become obsolete?
  - Basis: Section 3.2 assumes high homogeneity to compute the schedule once before inference, but this may not hold if scene dynamics shift drastically during execution.
  - Why unresolved: The method relies on a static schedule derived offline; if the feature similarity distribution shifts online, the scheduler's optimality is not guaranteed.
  - What evidence would resolve it: A comparison of task success rates using the static BAC schedule versus a dynamic online-recalculated schedule on highly variable environments.

## Limitations
- Dynamic Programming Solver Precision: The exact backtracking implementation and boundary condition handling could affect schedule optimality.
- Cross-Task Generalization: Significant domain shifts between training and deployment tasks may invalidate precomputed schedules.
- Error Propagation Model Validity: Proposition 3.1's first-order error approximation assumes linear FFN behavior, which may not hold for deeper networks or extreme feature values.

## Confidence
- **High Confidence**: The core mechanism of block-wise adaptive caching (ACS + BUA) is well-validated through multiple robotic benchmarks with consistent 3× speedup and maintained performance.
- **Medium Confidence**: The claim that BUA prevents "error surge" is strongly supported by Pearson correlation evidence (r=0.9894) but relies on assumptions about error propagation.
- **Medium Confidence**: The task-specific scheduling approach assumes feature similarity patterns remain stable within tasks, which is reasonable but untested under significant environmental changes.

## Next Checks
1. **Solver Robustness Test**: Run ACS with different initial conditions and solver parameters across all benchmark tasks to verify schedule stability and convergence properties.
2. **Domain Shift Evaluation**: Apply BAC schedules from one task (e.g., Push-T) to significantly different tasks (e.g., Kitchen) to quantify performance degradation and identify schedule invalidation thresholds.
3. **Error Propagation Boundary Analysis**: Systematically vary feature magnitudes and network depths to test the validity limits of the first-order error propagation approximation used in BUA.