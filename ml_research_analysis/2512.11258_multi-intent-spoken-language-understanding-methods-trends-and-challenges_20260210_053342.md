---
ver: rpa2
title: 'Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges'
arxiv_id: '2512.11258'
source_url: https://arxiv.org/abs/2512.11258
tags:
- intent
- slot
- multi-intent
- language
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides the first comprehensive review of multi-intent\
  \ spoken language understanding (SLU) research since 2019, systematically analyzing\
  \ decoding paradigms and modeling approaches. It categorizes methods based on information\
  \ flow direction\u2014intent-guided slot filling, slot-guided intent detection,\
  \ and bidirectional interaction\u2014and evaluates their performance across classification-based\
  \ and generation-based frameworks."
---

# Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges

## Quick Facts
- arXiv ID: 2512.11258
- Source URL: https://arxiv.org/abs/2512.11258
- Reference count: 40
- This survey provides the first comprehensive review of multi-intent spoken language understanding (SLU) research since 2019, systematically analyzing decoding paradigms and modeling approaches

## Executive Summary
This survey comprehensively reviews multi-intent spoken language understanding (SLU) research from 2019 to 2024, addressing the growing need to process utterances containing multiple user intents. The authors systematically categorize decoding paradigms into three types: intent-guided slot filling, slot-guided intent detection, and bidirectional interaction models. They analyze both classification-based and generation-based frameworks, finding that bidirectional interactive models generally outperform unidirectional approaches, particularly when enhanced with pre-trained models (PTMs) and large language models (LLMs). While classification-based models offer computational efficiency, generation-based approaches show greater potential for accuracy. The survey identifies key challenges including limited real-world and multilingual datasets, interpretability issues, and practical deployment considerations.

## Method Summary
The authors conducted a comprehensive literature review of multi-intent SLU research published between 2019 and 2024, systematically analyzing papers across major venues including ACL, EMNLP, AAAI, and ICASSP. They categorized methods based on information flow direction (intent-to-slot, slot-to-intent, and bidirectional interaction) and framework type (classification-based vs. generation-based). The survey examined performance across multiple benchmarks including MixATIS, MixSNIPS, and MTOP datasets, comparing results across different decoding paradigms and modeling approaches. They also analyzed emerging trends in PTM and LLM integration, and identified current limitations and future research directions through synthesis of findings across all surveyed works.

## Key Results
- Bidirectional interactive models outperform unidirectional approaches in multi-intent SLU tasks
- PTMs and LLMs provide significant performance gains across all model categories
- Classification-based models are more computationally efficient while generation-based approaches show greater accuracy potential
- Current datasets are synthetic and fail to capture complex inter-intent dependencies present in real-world scenarios

## Why This Works (Mechanism)
Multi-intent SLU requires models to simultaneously detect multiple user intents and extract corresponding slot values from a single utterance. The complexity arises from the need to capture interactions between multiple intent-slot pairs, where the presence of one intent can influence slot filling for another. Bidirectional interaction models work better because they allow information to flow both from intents to slots and from slots to intents, creating a feedback loop that captures these complex dependencies. Pre-trained models provide strong semantic representations that help distinguish between similar intents and slots, while LLMs offer superior reasoning capabilities for handling rare or ambiguous cases. Generation-based approaches show more potential because they can explicitly model the relationship between intents and slots through their output structure, whereas classification-based methods treat them as separate prediction tasks.

## Foundational Learning
**Spoken Language Understanding (SLU)**: The task of converting natural language utterances into structured semantic representations. Why needed: Provides the foundation for understanding how machines interpret human speech. Quick check: Can identify intents and slots in a simple utterance like "