---
ver: rpa2
title: 'Readability Measures and Automatic Text Simplification: In the Search of a
  Construct'
arxiv_id: '2511.09536'
source_url: https://arxiv.org/abs/2511.09536
tags:
- readability
- simplification
- coca
- text
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Readability Measures and Automatic Text Simplification: In the Search of a Construct

## Quick Facts
- **arXiv ID:** 2511.09536
- **Source URL:** https://arxiv.org/abs/2511.09536
- **Authors:** Rémi Cardon; A. Seza Doğruöz
- **Reference count:** 0
- **Key outcome:** None

## Executive Summary
This paper investigates how readability measures correlate with human judgment and automatic text simplification (ATS) metrics in evaluating text simplification quality. The study computes readability features on both simplified texts and the delta between original and simplified texts, finding that delta-based computation yields higher correlations with human judgment and ATS metrics. The research reveals that traditional readability formulas, human judgment, and ATS metrics measure different aspects of simplification, suggesting a need for clearer construct definition in the field.

## Method Summary
The authors analyzed 1,066 readability features extracted using four TAA* tools (TAALED, TAALES, TAASSC, TAACO) plus 8 traditional metrics on English simplification datasets. They computed correlations between these features and human judgments (simplicity, fluency, meaning preservation) as well as ATS metrics (BLEU, SARI, BERTScore) across two conditions: features computed on simplified texts alone and features computed on the delta between original and simplified texts. The analysis covered both sentence-level (SimplicityDA dataset) and document-level (D-Wiki subsets) simplification.

## Key Results
- Readability measures computed on the delta between original and simplified texts correlate more strongly with human judgment and ATS metrics than when computed on simplified texts alone
- Traditional readability formulas correlate poorly with human judgment on simplification quality
- Three evaluation approaches (readability measures, human judgment, ATS metrics) exhibit rather low correlations with one another
- TTR-based features correlate better with sentence-level simplification than with document-level simplification

## Why This Works (Mechanism)

### Mechanism 1: Delta-Computation Improves Correlation Detection
Computing readability measures on the difference between original and simplified texts yields higher correlations with human judgment and ATS metrics than computing on simplified texts alone. The transformation magnitude captures the simplification operation rather than the absolute simplicity state, aligning better with human comparative evaluation processes.

### Mechanism 2: Construct Fragmentation Across Evaluation Paradigms
Readability formulas, human judgment, and ATS metrics measure non-overlapping aspects of a poorly-defined "simplicity" construct. Each approach captures different signals—traditional formulas measure surface features, ATS metrics measure n-gram overlap or semantic similarity, and humans judge functional accessibility—resulting in low inter-correlations.

### Mechanism 3: Scale-Dependent Feature Relevance
Lexical diversity features (TTR variants) correlate with sentence-level but not document-level simplification evaluation, indicating different linguistic phenomena dominate at different granularities. Sentence-level simplification primarily involves lexical substitution and shortening, while document-level simplification involves discourse-level restructuring not captured by sentence-bound features.

## Foundational Learning

- **Concept: Pearson correlation for metric validation**
  - **Why needed here:** The entire methodology rests on interpreting correlation coefficients between metrics
  - **Quick check question:** If readability measure A correlates r=0.43 with human judgment on "meaning preservation" (delta condition), would you recommend using A as a proxy for human evaluation?

- **Concept: Type-Token Ratio (TTR) and lexical diversity**
  - **Why needed here:** TTR variants appear as top-correlating features in sentence-level experiments
  - **Quick check question:** Why might TTR be problematic for comparing documents of different lengths?

- **Concept: Reference-based vs. reference-free evaluation**
  - **Why needed here:** BLEU and SARI require reference simplifications; BERTScore can be reference-free
  - **Quick check question:** If you have no human-written simplifications for your domain, which ATS metrics can you still compute?

## Architecture Onboarding

- **Component map:** Original text + simplified text -> TAA* tools (TAALED, TAALES, TAASSC, TAACO) -> 1,066 features + 8 traditional metrics -> Delta computation -> Correlation with human judgment + ATS metrics
- **Critical path:** Parse input texts -> Run all four TAA* tools on each text -> Compute delta features -> Load human judgment annotations -> Compute ATS metrics against references -> Calculate Pearson correlations with p-value filtering
- **Design tradeoffs:** Feature volume vs. interpretability (1,066 features provide comprehensive coverage but require post-hoc interpretation); Delta vs. absolute computation (delta captures simplification process but requires original text); Sentence vs. document granularity (feature relevance differs across granularities)
- **Failure signatures:** Non-significant correlations (p > threshold) -> insufficient data or no relationship; Correlation direction inversion between delta/simp modes -> measure responds differently to transformation vs. state; Feature groups clustering only with themselves -> features measure intra-construct rather than target construct
- **First 3 experiments:**
  1. Compute top-10 correlating features for your target domain using delta-mode; verify |r| > 0.2 exists
  2. Test whether TAALES features alone can predict human judgment for your granularity; compare against full 1,066-feature set
  3. Replace BLEU/SARI with LENS or a learned metric; check if correlation patterns with readability features shift meaningfully

## Open Questions the Paper Calls Out

### Open Question 1
How should the field of ATS define a unified "simplicity" construct that can reconcile the three evaluation approaches (readability measures, human judgment, and ATS metrics)? The authors conclude there is a need for a clear definition of the construct in ATS given the low inter-correlations among the three evaluation angles.

### Open Question 2
Why do TTR-based features correlate with sentence-level ATS evaluation but not document-level evaluation, despite TTR being commonly used to assess text complexity? The authors find this discrepancy but offer no theoretical explanation for why lexical diversity features behave differently across granularities.

### Open Question 3
To what extent do these correlation patterns generalize to languages other than English, given that readability features depend heavily on language-specific resources? The study is limited to English data, and readability tools require language-dependent resources such as reference corpora and syntactic analyzers.

### Open Question 4
Can behavioral measures such as eye-tracking provide a more reliable link between readability measures and human judgment in ATS evaluation? The authors cite eye-tracking as promising but did not investigate this method in the current work.

## Limitations

- The study relies on relatively small human-annotated datasets (SimplicityDA for sentences, D-Wiki subsets for documents), limiting statistical power
- The evaluation uses only Pearson correlations without examining which metrics fail to capture quality dimensions
- While 1,066 features are analyzed, the paper doesn't provide actionable guidance on which specific features practitioners should use
- The analysis uses TAA* tools from 2016-2017; linguistic patterns may have shifted with modern simplification systems

## Confidence

- **High confidence:** The observation that delta-based readability measures correlate more strongly with human judgment than absolute measures on simplified text alone
- **Medium confidence:** The claim that readability formulas, human judgment, and ATS metrics measure non-overlapping aspects of simplification quality
- **Low confidence:** The assertion that lexical diversity features correlate with sentence-level but not document-level simplification evaluation due to lack of theoretical grounding

## Next Checks

1. **Replication with modern tools:** Recompute the entire correlation analysis using current versions of readability tools and a larger human-annotated dataset to verify delta-computation advantage persists
2. **Feature reduction experiment:** Test whether a reduced feature set (top 20 features by correlation) performs as well as the full 1,066-feature set in predicting human judgment
3. **Cross-linguistic validation:** Apply the same delta-computation methodology to a non-English simplification corpus (e.g., German or Spanish) to test whether correlation patterns generalize beyond English