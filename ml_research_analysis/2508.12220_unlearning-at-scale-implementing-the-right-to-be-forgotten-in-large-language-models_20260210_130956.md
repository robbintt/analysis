---
ver: rpa2
title: 'Unlearning at Scale: Implementing the Right to be Forgotten in Large Language
  Models'
arxiv_id: '2508.12220'
source_url: https://arxiv.org/abs/2508.12220
tags:
- replay
- training
- exact
- optimizer
- deterministic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the right to be forgotten for large language models
  by framing unlearning as a reproducible systems problem. Our core method treats
  training as a deterministic program and logs minimal per-microbatch records (ordered
  ID hash, RNG seed, learning-rate value, optimizer-step counter, and accumulation
  boundary).
---

# Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models

## Quick Facts
- arXiv ID: 2508.12220
- Source URL: https://arxiv.org/abs/2508.12220
- Reference count: 40
- Primary result: Exact unlearning for LLMs via deterministic replay with minimal per-microbatch logging, achieving bit-identical parameters to training on retain set

## Executive Summary
This paper addresses the right to be forgotten for large language models by treating unlearning as a reproducible systems problem. The core innovation is a write-ahead logging approach that captures minimal training state (RNG seeds, learning rates, optimizer steps) per microbatch, enabling deterministic replay that filters forget samples while maintaining bit-identical parameters to oracle retraining. The approach provides three complementary fast paths - exact reverts via per-step deltas, cohort-scoped adapter deletion, and curvature-guided anti-updates - while maintaining auditability and compliance with GDPR Art. 17.

## Method Summary
The method treats training as a deterministic program, logging 32-byte per-microbatch records containing ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and accumulation boundary. Under pinned software/hardware stacks and deterministic kernels, replaying the training tail while filtering only the forget closure yields bit-identical parameters to training on the retain set. For operational constraints, the approach provides exact reverts via micro-checkpoints/dense per-step deltas, cohort-scoped adapter deletion, and curvature-guided anti-updates with audit-gated escalation. The system achieves exact unlearning with practical storage/latency budgets while maintaining auditability through structured logging and validation gates.

## Key Results
- Byte-identical equality of model and optimizer states demonstrated in controlled run
- Per-step dense parameter deltas enable exact reverts of recent steps in seconds-to-minutes
- Cohort-scoped LoRA adapter deletion removes parametric influence exactly under base freezing
- Storage overhead remains practical: 4.55M bytes for N=16 on tiny-gpt2 with compression ratio 0.70

## Why This Works (Mechanism)

### Mechanism 1
Replaying training from a checkpoint while filtering forget samples yields bit-identical parameters to training on the retain set when determinism preconditions hold. The system reconstructs microbatches from WAL records, removes samples in the forget closure, and re-executes the same Update function with identical seeds and LR values - by linearity of gradient accumulation with reduction=sum, removing addends yields identical gradients. Core assumption: deterministic kernels, fixed data order, logged RNG seeds with index-stable generation, exact checkpoint restoration, and loss reduction=sum.

### Mechanism 2
Per-step dense parameter deltas enable exact reverts of recent steps in seconds-to-minutes without replay. Store either bitwise XOR patches of raw tensor bytes or arithmetic deltas in training dtype. Ring buffer of size N bounds storage and latency for recent requests. Core assumption: Window N covers the forget request; storage available for N × model-size deltas; compression ratio stable.

### Mechanism 3
Deleting cohort-scoped LoRA adapters on a frozen base removes that cohort's parametric influence exactly. When base θ₀ is frozen during adapter training, only adapter parameters receive updates. Setting Pⱼ = AⱼBⱼᵀ = 0 removes all parameter dependence on cohort j. A short retain-tune smooths activation-level drift. Core assumption: Base strictly frozen; adapters never merged; no other parameters touched during cohort training.

## Foundational Learning

- **Write-Ahead Logging (WAL) in Databases**: The paper adapts ARIES-style redo logging from transaction processing to SGD. Understanding WAL helps grasp why logging seeds/LR before execution enables exact replay.
  - Quick check question: What minimal state must be logged before a transaction to enable deterministic redo?

- **Deterministic Training in PyTorch**: Bit-identical replay requires `torch.use_deterministic_algorithms(True)`, pinned cuDNN/cuBLAS configs, and disabled benchmarking. These are non-default and fragile.
  - Quick check question: Name three sources of nondeterminism in a multi-GPU training run and how to pin each.

- **Gradient Accumulation Semantics**: The WAL logs accumulation boundaries; replay must respect them. With reduction=sum, filtering samples removes addends cleanly; with reduction=mean, denominators diverge.
  - Quick check question: Why does reduction=sum enable exact filtering while reduction=mean does not?

## Architecture Onboarding

- **Component map**: [Trainer + WAL Writer] → [WAL segments] → [ReplayFilter] → [Checkpoint Store] ←→ [Dense Delta Ring Buffer] → [Adapter Registry] → [Controller] → [Audit Harness] → [Near-Dup Index] → [Signed Forget Manifest]

- **Critical path**: 1. Preflight CI gate: Train 100 steps twice, assert byte-identical; replay from checkpoint, assert byte-identical. 2. WAL integrity scan: Per-record CRC32, per-segment SHA-256, monotone opt_step_u32. 3. Controller routing: Adapter delete → Recent revert → Hot path → Exact replay (in priority order).

- **Design tradeoffs**: Checkpoint cadence K vs replay latency (larger K reduces storage but increases worst-case replay time); Ring buffer N vs instant revert coverage (larger N covers more recent requests but storage scales as N × 2P bytes); Exact vs approximate (hot path has lower latency but requires audit gates; exact replay has SLO-independent latency).

- **Failure signatures**: Determinism violation (CI preflight fails; replay refuses under pin drift); WAL corruption (CRC32 mismatch; segment SHA-256 mismatch; opt_step_u32 gaps); Graph mismatch (filtered microbatch hash doesn't match manifest; loader repacks samples instead of preserving slots); Adapter merge leakage (base updated during cohort phase; G2 guarantee void).

- **First 3 experiments**: 1. Validate CI gate: Run Alg. 5.1 on target hardware; confirm train-train and checkpoint-replay byte equality for 100 steps. 2. WAL overhead measurement: Log WAL size per 1M microbatches; measure compression; confirm 32B/record overhead is negligible vs model size. 3. Single-forget replay test: Insert known forget samples in training; run ReplayFilter from checkpoint that precedes their influence; verify parameter equality via hash comparison.

## Open Questions the Paper Calls Out

- **Distributed training**: Can the exact unlearning guarantee be preserved in distributed multi-GPU training environments where collective communication orders are difficult to pin? The authors state "scaling to distributed GPU is left for future work" and the method is validated only on a CPU toy model.

- **RLHF integration**: How can the write-ahead logging approach be adapted to support exact unlearning in RLHF or other interactive training pipelines? The conclusion notes that stages like RLHF "will require logging sampler and environment state in addition to the training log."

- **Software stack updates**: Is it possible to maintain bit-identical replay guarantees across minor software stack revisions without invalidating previous checkpoints? The discussion identifies "verified determinism across minor stack revisions" as a necessary direction for the community.

## Limitations
- The exact unlearning guarantee depends on a fragile stack of deterministic training preconditions that are difficult to maintain in production environments
- Dense-delta revert mechanism is limited by the ring buffer window N, creating coverage gaps between recent and requires-full-replay requests
- Critical hyperparameters for reproduction are missing, including LR schedule specifics, weight decay, batch size, and gradient-accumulation length
- The approach requires significant storage for WAL records and dense deltas, scaling with model size and forget frequency

## Confidence
- **Mechanism 1 (Deterministic Replay)**: Medium-High confidence. Theoretical framework is sound and byte-identical equality demonstrated in controlled run, but achieving deterministic training stacks is notoriously difficult in practice
- **Mechanism 2 (Dense Delta Reverts)**: Medium confidence. Arithmetic approach has proven accuracy bounds and demonstrates practical storage efficiency, but coverage is limited to recent steps
- **Mechanism 3 (Adapter Deletion)**: High confidence. LoRA unlearning mechanism has clear mathematical guarantee under frozen-base assumption, which is well-established in adapter literature

## Next Checks
1. **Production Determinism Test**: Run Algorithm 5.1 CI gate on production hardware with actual training hyperparameters. Measure actual storage/latency costs for WAL, dense deltas, and adapter deletion across different model sizes. Test failure modes (nondeterminism, corrupted WAL, scheduler calls during replay).

2. **Coverage Gap Analysis**: Systematically measure the proportion of forget requests falling within the dense-delta window N vs requiring full replay. Evaluate the cost-benefit tradeoff of increasing N vs accepting longer replay latencies.

3. **Audit Gateway Stress Test**: Implement and validate the curvature-guided anti-update + retain-tune path. Measure utility degradation vs latency gains. Test audit gate effectiveness at preventing privacy regressions.