---
ver: rpa2
title: 'Quantum Re-Uploading for Calorimetry: Optimized Architectures with Extended
  Expressivity'
arxiv_id: '2412.12397'
source_url: https://arxiv.org/abs/2412.12397
tags:
- loss
- learning
- accuracy
- quantum
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates single-qubit quantum re-uploading units (QRUs)
  for three-class calorimetry classification, comparing them to matched-parameter
  baselines. QRUs encode input features repeatedly across layers, expanding frequency
  support compared to mono-encoded variational quantum circuits (VQC).
---

# Quantum Re-Uploading for Calorimetry: Optimized Architectures with Extended Expressivity

## Quick Facts
- **arXiv ID**: 2412.12397
- **Source URL**: https://arxiv.org/abs/2412.12397
- **Reference count**: 40
- **Primary result**: Single-qubit QRU achieves ~98% accuracy on 3-class calorimetry task, outperforming matched-parameter VQC baseline (~93%)

## Executive Summary
This study evaluates single-qubit Quantum Re-Uploading Units (QRUs) for three-class particle identification in simulated calorimetry data. QRUs encode input features repeatedly across layers, expanding frequency support compared to mono-encoded Variational Quantum Circuits (VQCs). On a particle physics dataset, a depth-4 QRU achieved ~98% accuracy, outperforming the VQC baseline (~93%) and matching a small MLP. Theoretical analysis and spectral activation experiments confirm QRUs attain richer Fourier-frequency profiles, consistent with higher expressivity. A proof-of-execution deployed the trained QRU on a superconducting QPU via cloud workflow, demonstrating practical deployability under current NISQ constraints.

## Method Summary
The method uses a single-qubit Quantum Re-Uploading Unit with 10 layers where each layer applies R_x(θ) - R_y(θ × x) - R_x(θ) sequences for all three input features. Inputs (ECAL energy, shower length, HCAL std dev) are normalized to [-π, π]. The model outputs a scalar expectation value ⟨Z⟩ ∈ [-1, 1] which is thresholded at ±0.33 for three-class classification. Training uses Adam optimizer with learning rate 5×10^-5 and gradient accumulation over the full dataset (26k samples) for 10-30 epochs with Huber loss.

## Key Results
- Depth-4 QRU achieved ~98% test accuracy on 3-class calorimetry classification
- QRU outperformed matched-parameter 3-qubit VQC baseline (~93% accuracy)
- Performance gains plateau after depth 4 while execution time grows linearly
- QRU deployed on superconducting QPU demonstrated practical NISQ feasibility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Repeated data encoding extends functional expressivity by expanding accessible Fourier frequency spectrum
- **Mechanism**: QRUs re-encode data L times, theoretically allowing representation of trigonometric polynomials with frequency support up to order L, versus VQCs restricted to first harmonic
- **Core assumption**: Classification boundary requires higher-frequency Fourier components than single-encoding circuits can represent
- **Evidence anchors**: Abstract confirms richer Fourier-frequency profiles; Section 2 shows F_QRU ⊆ {-L, ..., L}; related work supports re-uploading-expressivity link
- **Break condition**: If target function is primarily low-frequency or noise drowns high-frequency components

### Mechanism 2
- **Claim**: Single-qubit QRU outperforms multi-qubit VQC baseline under matched-parameter constraints
- **Mechanism**: QRU decouples expressivity from qubit count by utilizing circuit depth and iterative parameterization, creating a trainable deep computation graph
- **Core assumption**: QRU optimization landscape is less prone to barren plateaus than 3-qubit VQC ansatz
- **Evidence anchors**: Abstract shows QRU attains highest test accuracy (0.987) vs VQC (0.927); Section 4.3 shows negative generalization gap for VQC; re-uploading acts as universal approximator
- **Break condition**: Advantage is conditional on matched-parameter setup; unconstrained qubit count or different VQC ansatz might shift performance

### Mechanism 3
- **Claim**: Performance gains saturate quickly (depth ≈ 4) while computational cost scales linearly
- **Mechanism**: Practical capacity to fit specific dataset saturates as problem complexity matches shallow circuit capacity, with diminishing returns beyond depth 4
- **Core assumption**: Optimizer finds global solution within shallow regime, rendering deeper circuits redundant for 3-class task
- **Evidence anchors**: Abstract shows accuracy gains plateau after depth 4; Section 4.1.1 shows loss stabilizes beyond depth 4; high expressivity doesn't guarantee predictive performance
- **Break condition**: If dataset complexity increases, saturation point would likely shift to deeper circuits

## Foundational Learning

- **Concept**: Quantum Fourier Expressivity
  - **Why needed here**: Paper relies on Fourier analysis to explain QRU superiority over VQCs; understanding frequency support argument requires knowing quantum circuits output trigonometric series
  - **Quick check question**: Does a quantum circuit with single encoding gate support frequencies higher than 1?

- **Concept**: NISQ Constraints & Qubit Efficiency
  - **Why needed here**: Core value proposition is achieving high accuracy (98%) on single qubit; understanding qubit scarcity and noise explains significance
  - **Quick check question**: Why is single-qubit solution potentially more deployable than 3-qubit solution on current hardware?

- **Concept**: Threshold-based Classification
  - **Why needed here**: Model outputs scalar ⟨Z⟩ ∈ [-1, 1] for 3-class problem; understanding this mapping is essential for interpreting accuracy metrics
  - **Quick check question**: How do you map continuous output range [-1, 1] to three discrete particle labels?

## Architecture Onboarding

- **Component map**: Input (3 features) -> Normalization to [-π, π] -> Core Loop (Layer: R_X(θ) -> R_Y(x_j θ) -> R_X(θ) repeated for features, repeated for L layers) -> Readout (Z expectation) -> Thresholding at ±0.33 for class assignment

- **Critical path**: Data normalization and rotation sandwich (R_X-R_Y-R_X) are primary drivers of stability; unnormalized inputs cause minimal rotations and failure to learn

- **Design tradeoffs**:
  - Depth vs. Time: Accuracy plateaus at L=4, execution time increases linearly; do not exceed depth 4 for this dataset
  - Sandwich vs. Triplet: Sandwich circuits (R_x-R_y-R_x) more stable than triplet (R_x-R_y-R_z) for this readout axis

- **Failure signatures**:
  - Stagnant Loss (~1.0): Learning rate too high (>0.5) or optimizer stuck (SGD performed poorly)
  - Random Guessing (~33% acc): Check input normalization; unnormalized data caused model failure
  - Slow Convergence: Learning rate too low (<10^-6) or effective batch size too small

- **First 3 experiments**:
  1. Sanity Check: Run QRU at depth 1 (mono-encoding) vs. depth 4 to verify performance jump
  2. Ablation: Train model with unnormalized inputs to observe failure mode of minimal rotations
  3. Generalization Test: Retrain on full dataset but validate on "E10-100_theta0" subset to check overfitting

## Open Questions the Paper Calls Out

- **Open Question 1**: Can multi-qubit QRUs with cross-entropy loss outperform single-qubit scalar thresholding approach on same classification task?
  - **Basis in paper**: [explicit] Conclusion states future work should "extend evaluation to multi-logit readouts"; Section 3.2 notes cross-entropy requires more qubits, "subject of future work"
  - **Why unresolved**: Current study uses single scalar expectation value mapped to three classes via fixed thresholds, limiting separability and comparability to softmax-based classifiers
  - **What evidence would resolve it**: Parameter-matched comparison between single-qubit regression model and multi-qubit QRU trained with cross-entropy loss

- **Open Question 2**: How does QRU performance scale when applied to less curated, higher-dimensional calorimetry inputs without extensive feature engineering?
  - **Basis in paper**: [explicit] Conclusion identifies "use of small set of engineered features" as key limitation; suggests extending evaluation to "less curated inputs"
  - **Why unresolved**: Current results based on three pre-selected high-level variables; unknown if single-qubit QRU can handle raw cell-level data complexity
  - **What evidence would resolve it**: Benchmarking QRU architecture on raw detector hits (increasing input dimension D) to observe if expressivity gains persist

- **Open Question 3**: Do noise-aware training strategies significantly improve QRU classifier accuracy deployed on NISQ hardware?
  - **Basis in paper**: [explicit] Conclusion explicitly calls for "noise-aware training and mitigation"; hardware results (Section 4.4) described as "proof-of-execution" on 10 samples rather than robust benchmark
  - **Why unresolved**: Hardware deployment showed fluctuations in expectation values (Δ ŷ), but undetermined if training with noise models would yield decision boundaries robust enough for large-scale inference
  - **What evidence would resolve it**: Study comparing models trained on noise-free simulators against those trained with simulated hardware noise or error mitigation layers

## Limitations
- QRU advantage tied to specific matched-parameter constraint; performance may degrade if VQC ansatz changes or qubit count is unconstrained
- Saturation at depth 4 may not generalize to more complex classification tasks or larger feature spaces
- Practical QPU deployment success depends on hardware connectivity and gate fidelity not fully characterized

## Confidence
- **High confidence** in expressivity mechanism (Mechanism 1) - Fourier analysis well-grounded with spectral experiments provided
- **Medium confidence** in comparative performance (Mechanism 2) - Results strong but depend on specific VQC baseline choice
- **Medium confidence** in efficiency claims (Mechanism 3) - Ablation shows clear plateau, but computational cost scaling assumes ideal simulator conditions

## Next Checks
1. Test QRU performance on multi-class problem with >3 classes to verify expressivity claims beyond current dataset
2. Implement QRU on different QPU architectures to validate practical deployability claims under varying noise models
3. Compare QRU against VQC with different qubit counts and ansatz designs to determine if advantage persists outside matched-parameter constraints