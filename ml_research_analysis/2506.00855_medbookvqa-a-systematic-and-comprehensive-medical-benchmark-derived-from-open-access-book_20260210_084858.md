---
ver: rpa2
title: 'MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from
  Open-Access Book'
arxiv_id: '2506.00855'
source_url: https://arxiv.org/abs/2506.00855
tags:
- medical
- figure
- modality
- wang
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces MedBookVQA, a comprehensive medical multimodal
  benchmark derived from open-access medical textbooks. It proposes a pipeline to
  extract medical figures and align them with contextual narratives, generating 5,000
  clinically relevant VQA questions across five categories: modality recognition,
  disease classification, anatomical identification, symptom diagnosis, and surgical
  procedures.'
---

# MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book

## Quick Facts
- arXiv ID: 2506.00855
- Source URL: https://arxiv.org/abs/2506.00855
- Reference count: 40
- Introduces MedBookVQA, a medical VQA benchmark with 5,000 questions across five clinical categories

## Executive Summary
MedBookVQA addresses the critical need for standardized evaluation of general multimodal large language models (MLLMs) in medical contexts by constructing a comprehensive benchmark from open-access medical textbooks. The benchmark employs a systematic pipeline to extract medical figures and align them with contextual narratives, generating clinically relevant visual question-answering (VQA) questions. Evaluation reveals significant performance gaps across diverse MLLM families, with proprietary models significantly outperforming open-sourced, medical-specific, and reasoning models, particularly in tasks requiring medical knowledge and cross-modal reasoning.

## Method Summary
The benchmark construction pipeline extracts figure-information pairs from DOAB medical textbooks using MinerU for document parsing, then employs LLM-based categorization to filter real-case medical figures. InternVL2.5-78B generates VQA questions across five types with multi-stage filtering to ensure clinical relevance and multimodal dependency. A hierarchical annotation system organizes the data by 42 imaging modalities, 125 anatomical structures, and 31 medical departments. The final benchmark contains 5,000 multiple-choice questions evaluated using temperature=0 settings across diverse MLLM families.

## Key Results
- Proprietary models (Gemini, GPT-4V) significantly outperform open-sourced models, medical-specific models, and reasoning models
- Performance gaps range from 22-34% across different medical knowledge types, with modality recognition being highest-performing (~90% for top models)
- Medical-specific MLLMs underperform general proprietary models (e.g., HealthGPT-L14: 68.6% vs. Gemini: 81.2%)
- Systematic weaknesses identified in Integumentary, Respiratory, and Digestive systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standardized pipelines for extracting figure-information pairs from medical textbooks enable scalable, diverse benchmark construction.
- Mechanism: The pipeline uses MinerU for document parsing, pattern-matching for caption recovery, and MLLM-based categorization (InternVL2-8B) to filter real-case medical figures. Contextual sentences are collected via explicit figure name references, creating paired visual-textual units for VQA generation.
- Core assumption: Medical textbooks contain systematically organized, clinically validated knowledge with figures that are contextually anchored by captions and nearby text.
- Evidence anchors: [abstract]: "propose a standardized pipeline for automated extraction of medical figures while contextually aligning them with corresponding medical narratives"; [section 3.1]: "MinerU is used to process the downloaded books by dividing each page into smaller regions and labeling them by type"; [corpus]: MM-Skin paper similarly uses textbook-derived datasets for dermatology VLMs.

### Mechanism 2
- Claim: LLM-generated VQAs with multi-stage filtering produce clinically relevant, multimodally-dependent questions.
- Mechanism: InternVL2.5-78B generates questions across five types with constraints (concise answers, image-relevance). Three-stage filtering removes: (1) non-diagnostic topics, (2) answerable-without-image questions, and (3) artifacts like incorrect answers or linguistic shortcuts.
- Core assumption: LLMs can generate medically appropriate questions when given figure-information pairs, and filtering stages reliably catch failure modes.
- Evidence anchors: [abstract]: "generated questions using large language models with multi-tier annotation"; [section 3.2]: "Qwen-VL-Max assesses suitability... excluding non-suitable types... DeepSeek-R1 to determine answerability without the image"; [corpus]: MedCaseReasoning benchmark similarly emphasizes filtering for clinically valid reasoning.

### Mechanism 3
- Claim: Hierarchical labeling by modality, anatomy, and department enables fine-grained capability gap analysis.
- Mechanism: Qwen-VL-72B assigns hierarchical labels (42 modalities, 125 anatomical structures, 31 departments). This allows performance disaggregation by medical subdomain, revealing systematic weaknesses.
- Core assumption: Hierarchical taxonomies capture clinically meaningful distinctions and models can reliably assign these labels.
- Evidence anchors: [abstract]: "multi-tier annotation system categorizes queries through hierarchical taxonomies"; [section 5.1.3]: "most models show noticeable weaknesses in the Integumentary, Respiratory, and Digestive systems".

## Foundational Learning

- **Concept: Medical imaging modalities** (CT, MRI, X-ray, histopathology, etc.)
  - Why needed here: Modality recognition is the highest-performing task (~90% for top models) and is foundational to downstream diagnosis.
  - Quick check question: Can you distinguish CT from MRI based on tissue contrast and image characteristics?

- **Concept: VQA task formulation** (open-ended vs. multiple-choice, multimodal dependency)
  - Why needed here: The benchmark uses MC format with distractors; understanding multimodal dependency filtering is critical for interpreting results.
  - Quick check question: Given a medical image and caption, can you write a question that requires seeing the image to answer?

- **Concept: MLLM evaluation protocols** (temperature=0, zero-shot, accuracy metrics)
  - Why needed here: The paper evaluates diverse models under standardized conditions; understanding these protocols enables fair comparison.
  - Quick check question: Why set temperature=0 for benchmark evaluation?

## Architecture Onboarding

- **Component map**: DOAB medical textbooks → MinerU parser → Figure-caption pairing → Categorization (InternVL2-8B) → FIGtexts collection → InternVL2.5-78B generation → Qwen-VL-Max distractors → Filtering (Suitability, Multimodality, Manual) → Hierarchical labeling (Qwen-VL-72B) → Evaluation

- **Critical path**: Textbook PDF parsing quality → Figure-caption alignment → VQA quality → Filtering thoroughness → Annotation accuracy. Errors cascade; early-stage parsing failures compound.

- **Design tradeoffs**:
  - LLM-generated VQAs vs. expert-authored: Scalable but introduces hallucination risk
  - 5,000 samples vs. larger scale: Manageable evaluation but limited coverage per subcategory
  - Textbook-derived vs. PubMed-derived: More clinically validated but potentially less cutting-edge

- **Failure signatures**:
  - Models performing well on modality recognition but poorly on disease diagnosis (observed: 22-34% gaps)
  - Medical-specific MLLMs underperforming general proprietary models (e.g., HealthGPT-L14: 68.6% vs. Gemini: 81.2%)
  - Reasoning models showing inconsistent gains on medical tasks

- **First 3 experiments**:
  1. **Reproduce baseline**: Evaluate InternVL3-78B on MedBookVQA subset (500 samples) with temperature=0; verify accuracy is ~73% overall.
  2. **Ablate filtering**: Generate VQAs without multimodality filtering; quantify how many become answerable from text alone.
  3. **Subdomain analysis**: Select one weak anatomical system (e.g., Integumentary) and one strong (e.g., Nervous); compare model performance patterns to identify systematic gaps.

## Open Questions the Paper Calls Out
None

## Limitations
- **Knowledge recency and coverage**: Benchmark relies on open-access medical textbooks, which may not reflect latest clinical guidelines, rare diseases, or emerging imaging modalities.
- **LLM generation quality and bias**: Benchmark validity fundamentally depends on LLM-generated questions, which may have systematic biases affecting representativeness.
- **Evaluation methodology constraints**: Temperature=0 and zero-shot evaluation may not reflect real-world deployment where models can be fine-tuned or use temperature for exploration.

## Confidence
- **High confidence**: Claims about performance gaps between model types (proprietary vs. open-sourced, general vs. medical-specific) are supported by extensive empirical evaluation across 16+ models.
- **Medium confidence**: Benchmark's clinical relevance and representativeness depend on DOAB textbook selection and LLM filtering pipeline effectiveness.
- **Low confidence**: Claims about pipeline's scalability and generalizability to other medical domains or non-textbook sources lack empirical validation.

## Next Checks
1. **Clinical expert validation**: Have board-certified physicians independently review a stratified sample (n=100) of VQAs across all five categories to assess clinical accuracy, relevance, and whether questions truly require multimodal reasoning.

2. **Temporal validation**: Evaluate model performance on textbook-derived questions versus contemporary clinical cases from recent medical conferences or journals to quantify the impact of knowledge recency on benchmark validity.

3. **Bias analysis**: Conduct systematic analysis of question distribution across anatomical systems, modalities, and departments to identify potential underrepresentation that could create blind spots in model evaluation.