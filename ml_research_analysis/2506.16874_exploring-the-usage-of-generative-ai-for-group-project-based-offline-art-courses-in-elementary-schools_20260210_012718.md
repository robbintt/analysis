---
ver: rpa2
title: Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses
  in Elementary Schools
arxiv_id: '2506.16874'
source_url: https://arxiv.org/abs/2506.16874
tags:
- students
- genais
- group
- dall-e
- askart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored how Generative AI can support elementary students
  in group project-based art courses. A four-phase field study was conducted with
  132 students and two teachers, introducing DALL-E and GPT via an interface called
  AskArt.
---

# Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools

## Quick Facts
- arXiv ID: 2506.16874
- Source URL: https://arxiv.org/abs/2506.16874
- Reference count: 40
- Students using AskArt reported significantly higher satisfaction (M=6.64) compared to those without (M=5.46, p=0.0005).

## Executive Summary
This study explored how Generative AI can support elementary students in group project-based art courses. A four-phase field study was conducted with 132 students and two teachers, introducing DALL-E and GPT via an interface called AskArt. Students benefited from AI-generated background information, inspiration, and guidance, but faced challenges in query formulation. AskArt's features, such as project-related introductions and suggested follow-ups, improved student interaction with AI. Students using AskArt reported significantly higher satisfaction (M=6.64) compared to those without (M=5.46, p=0.0005). The study highlights the potential of GenAI in enhancing creativity and engagement while emphasizing the need for tailored interfaces and teacher guidance to address misuse and improve usability.

## Method Summary
The study developed AskArt, a web interface combining OpenAI's GPT-4o for text-based ideation and DALL-E 3 for image generation, deployed in group settings for elementary students. The system featured audio-to-text input, project-related introductions, "Select and Generate" functionality to convert GPT responses into DALL-E prompts, and suggested follow-up questions. A four-phase field study compared three conditions: standard Chatbox, AskArt, and no-tool baseline across 132 students working on art projects like "The Nine Sons of the Dragon" and "Future City Poster." Data collection included questionnaires, interviews, screen recordings, and interaction logs.

## Key Results
- AskArt group reported significantly higher satisfaction (M=6.64) compared to baseline (M=5.46, p=0.0005).
- Students effectively used GPT for information seeking and DALL-E for visual inspiration in a sequential workflow.
- The "Select and Generate" feature helped students iterate queries and improved their interaction with AI models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing a sequential interaction flow where students first gather textual context (via LLM) and then visualize it (via Text-to-Image) may reduce the cognitive load of "blank page" ideation.
- **Mechanism:** The LLM acts as a semantic scaffold, externalizing abstract concepts into text, which is then used as a structured prompt for the image generator. This bypasses the student's potential lack of drawing skills or vocabulary for visual description.
- **Core assumption:** Students struggle more with initial concept articulation than with selecting/modifying generated content.
- **Evidence anchors:**
  - [abstract] "Students employed varied collaboration strategies... AskArt... tailored to support elementary school students."
  - [section 4.2.2 (Finding 2)] "Students use GPT for information seeking and DALL-E for visual inspiration sequentially... copied the complete answer from GPT and pasted it into the chat with DALL-E."
  - [corpus] Related work in higher education (arXiv:2505.00100) suggests structured scaffolding guides tool use, supporting the necessity of this mechanism, though K-12 specific data is sparser.
- **Break condition:** If the LLM provides hallucinated or overly complex text, the subsequent image generation fails, causing frustration rather than support.

### Mechanism 2
- **Claim:** "Select and Generate" features (transforming LLM output into Image Prompts) likely mitigate the prompt formulation barrier inherent in elementary education.
- **Mechanism:** By mapping specific text snippets to image generation parameters (Main Subject vs. Context Word), the system offloads the syntactic requirement of prompt engineering (e.g., "a photo of X in style Y") from the student to the interface logic.
- **Core assumption:** Elementary students can recognize a good description (text) even if they cannot synthesize one from scratch for an image model.
- **Evidence anchors:**
  - [section 5] "AskArt should support a seamless transition from the content generated by a text-based AI model to that produced by the image generator."
  - [section 6.2.3 (Finding 8)] "The 'Select and Generate' feature helped students iterate their queries to DALL-E... [students] utilized this feature to make use of answers in GPT."
  - [corpus] Insufficient specific evidence in corpus for "Select and Generate" UI patterns; corpus focuses more on prompt engineering skills (arXiv:2504.19673).
- **Break condition:** If the text selection is too ambiguous, the auto-generated prompt fails to render the student's intent, leading to "perceived as incorrect" responses.

### Mechanism 3
- **Claim:** AI integration in group settings increases *satisfaction* by providing immediate, on-demand support that a single teacher cannot offer to multiple groups simultaneously.
- **Mechanism:** The AI serves as a "always-available" teaching assistant for lower-level cognitive tasks (fact retrieval, sketching), freeing up the human teacher for higher-level guidance and keeping students in a state of flow.
- **Core assumption:** Student satisfaction correlates with reduced waiting time for help and access to visual inspirations.
- **Evidence anchors:**
  - [abstract] "Students using AskArt reported significantly higher satisfaction (M=6.64) compared to those without (M=5.46, p=0.0005)."
  - [section 1] "Teacher’s support may not be in time, especially when managing multiple groups... GenAI offers a promising alternative."
  - [corpus] arXiv:2508.11709 highlights the risk of "Individual Prompts" creating equity gaps; this mechanism works only if the group structure promotes collective intelligence rather than individual isolation.
- **Break condition:** If students engage in "social loafing" (letting one student handle the AI while others disengage), the satisfaction mechanism may not apply to all group members equally.

## Foundational Learning

- **Concept: Sequential Modality Logic (Text → Image)**
  - **Why needed here:** Students naturally confused the roles of GPT (text) and DALL-E (image), asking GPT for images or DALL-E for advice. Understanding that text models "think" and image models "visualize" is a prerequisite for effective tool use.
  - **Quick check question:** "Do you need an answer (definition/idea) or a picture (visual reference)?"

- **Concept: Prompt Refinement via Context**
  - **Why needed here:** Elementary students typically use short, uncontextualized queries (e.g., "dragon"). They must learn that adding context ("blue dragon", "cartoon style") changes the output.
  - **Quick check question:** "If you ask for 'a dragon', will the computer know you want it to look *friendly*?"

- **Concept: AI as a Tool, Not a Creator**
  - **Why needed here:** Teachers expressed concern about misuse (copying). Students need the prerequisite understanding that AI generates *references* or *materials*, not the final artwork itself.
  - **Quick check question:** "Are we going to paste this printout directly, or use it to help us draw our own version?"

## Architecture Onboarding

- **Component map:** Frontend (Vue.js) -> Backend (Python/Flask) -> OpenAI APIs (GPT-4o, DALL-E 3) -> Data Storage
- **Critical path:**
  1. **Cold Start:** Display project-specific intro prompts (Project-Related Introduction).
  2. **Ideation:** Student uses Audio/Text to query GPT -> GPT responds with text + auto-generated DALL-E prompt draft.
  3. **Visualization:** Student selects text -> Right clicks -> "Generate Image" -> DALL-E renders -> Image displays.
  4. **Iteration:** Student refines via "Suggested Follow-Ups" or adds "Context Words".
- **Design tradeoffs:**
  - **Audio Input:** Implemented to help low-literacy/typing-speed students, but resulted in "disorganized or incomplete" queries ([section 6.2.3, Finding 6]). *Tradeoff:* Ease of input vs. semantic precision.
  - **Shared Device:** One device per group fostered collaboration but risked "fixed member" usage where only one student controls the AI ([section 6.2.2, Finding 4]).
- **Failure signatures:**
  - **Empty Queries:** Voice input failing to transcribe elementary student speech patterns.
  - **Category Confusion:** Students asking "How to make..." in the image generator (DALL-E) instead of the text model.
  - **Generic Outputs:** Images that are "correct but not helpful" due to lack of specific context words in the prompt.
- **First 3 experiments:**
  1. **The "Cold Start" Test:** Verify that the "Project-Related Introduction" actually prevents category confusion (e.g., ensure no queries asking GPT for images in the first 5 minutes).
  2. **The "Select and Generate" Loop:** Measure the number of iterations required to get a "satisfactory" image when using the auto-generated prompt vs. manual typing.
  3. **Collaboration Dynamics:** Observe one group session to count how many unique students physically interact with the device vs. passively watching.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can GenAI interfaces effectively scaffold query articulation for elementary students to prevent disorganized or incomplete inputs?
- **Basis in paper:** [explicit] Finding 6 noted that the "Audio-to-Text Input" often produced disorganized results, and Section 7.2.3 states that future tools must scaffold students' ability to structure and clarify their thoughts.
- **Why unresolved:** Current voice input features failed to address the root cognitive challenge of articulation, and the specific design of effective scaffolds (e.g., guided templates, visual prompts) remains untested.
- **What evidence would resolve it:** A usability study evaluating new scaffolding features, such as interactive clarification dialogues, showing improved query specificity and higher success rates in response generation.

### Open Question 2
- **Question:** How does the impact of GenAI tools on student satisfaction and creativity compare to standard search engines in PBL art courses?
- **Basis in paper:** [explicit] The Limitations section states the experimental design lacked a comprehensive baseline comparison, noting that a "more systematic approach... would involve equipping baseline groups with computer interfaces that allow the use of search engines."
- **Why unresolved:** The baseline group had no digital tools, making it unclear if the higher satisfaction in the experimental group was due to GenAI specifically or simply access to digital assistance.
- **What evidence would resolve it:** A controlled study comparing learning outcomes and satisfaction between a GenAI condition and a search-engine-only condition.

### Open Question 3
- **Question:** How does the presence of teaching assistants (TAs) influence student collaboration strategies and interaction behaviors with GenAI?
- **Basis in paper:** [explicit] The Limitations section notes that the involvement of TAs "may create an environment that differs from a regular classroom setting," and suggests observing behaviors without TAs in the future.
- **Why unresolved:** TAs frequently assisted with typing and query formulation, potentially masking students' actual struggles with the interface and inflating engagement metrics.
- **What evidence would resolve it:** Longitudinal field studies in regular classrooms without dedicated research assistants to observe natural usage patterns and independent problem-solving.

## Limitations
- Findings based on single deployment in Chinese elementary classrooms with 132 students, limiting generalizability.
- Evaluation relies on self-reported satisfaction rather than objective measures of creative output quality or learning gains.
- System's reliance on paid API services raises scalability and accessibility concerns for resource-constrained schools.

## Confidence
- **High Confidence:** The observed satisfaction difference between AskArt and control groups (M=6.64 vs M=5.46, p=0.0005) is statistically robust and well-documented. The sequential text-then-image interaction pattern is consistently observed across multiple data sources.
- **Medium Confidence:** The mechanisms explaining why students benefit from the interface (cognitive scaffolding, reduced prompt formulation barriers) are logically sound but would benefit from additional experimental validation with different age groups or subject matters.
- **Low Confidence:** Claims about long-term creative development or the system's effectiveness without teacher guidance are speculative, as the study duration and scope don't support these conclusions.

## Next Checks
1. **Replication in Different Cultural Contexts:** Deploy AskArt in elementary classrooms with different cultural backgrounds and art curriculum structures to test generalizability of satisfaction and usability findings.
2. **Objective Creativity Metrics:** Implement blind assessments of final artwork quality and originality by art educators to complement self-reported satisfaction measures.
3. **Teacher Independence Test:** Conduct a controlled study comparing student outcomes with AskArt when teachers provide varying levels of guidance versus minimal intervention.