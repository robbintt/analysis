---
ver: rpa2
title: Federated Self-Supervised Learning for Automatic Modulation Classification
  under Non-IID and Class-Imbalanced Data
arxiv_id: '2510.04927'
source_url: https://arxiv.org/abs/2510.04927
tags:
- learning
- client
- data
- modulation
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatic modulation classification
  (AMC) under non-IID and class-imbalanced data distributions in federated learning
  settings, where privacy constraints and label scarcity are critical concerns. The
  authors propose FedSSL-AMC, a federated self-supervised learning framework that
  first trains a causal, time-dilated CNN encoder with triplet-loss self-supervision
  on unlabeled I/Q sequences across distributed clients, then adapts a per-client
  SVM classifier using limited labeled data.
---

# Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data

## Quick Facts
- arXiv ID: 2510.04927
- Source URL: https://arxiv.org/abs/2510.04927
- Authors: Usman Akram; Yiyue Chen; Haris Vikalo
- Reference count: 40
- Primary result: Proposes FedSSL-AMC, achieving up to 82.59% accuracy on MIGOU dataset under heterogeneous SNR, CFO, and non-IID conditions while reducing communication rounds by 100× compared to baselines.

## Executive Summary
This paper addresses automatic modulation classification (AMC) in federated learning settings where data is non-IID, class-imbalanced, and privacy-constrained. The authors propose FedSSL-AMC, a framework that first trains a shared causal CNN encoder using triplet-loss self-supervision on unlabeled I/Q sequences across clients, then adapts per-client SVM classifiers using limited labeled data. This approach decouples representation learning from classification, addressing both label scarcity and data heterogeneity. The method demonstrates consistent performance gains over supervised FL baselines on both synthetic and over-the-air datasets, achieving robust results even under extreme conditions including mobility-induced carrier frequency offset heterogeneity and model quantization differences.

## Method Summary
FedSSL-AMC operates in two phases: (1) federated representation learning where clients collaboratively train a causal, time-dilated CNN encoder using triplet-loss self-supervision on unlabeled I/Q sequences, and (2) per-client SVM adaptation where each client fits a linear classifier on frozen encoder features using its limited labeled data. The triplet loss objective trains the encoder to embed subsequences from the same I/Q signal close together while pushing subsequences from different signals apart, leveraging temporal consistency of wireless signals as an implicit supervisory signal. The method maintains privacy by never sharing raw data and addresses label scarcity through self-supervision, while handling non-IID distributions by learning a shared representation space before classification.

## Key Results
- Achieves 82.59% accuracy on MIGOU dataset and 55.41% on synthetic data under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions
- Maintains robustness under extreme conditions including mobility-induced CFO heterogeneity and model quantization differences
- Reduces communication rounds by 100× compared to baselines (10 rounds with 2,500 local steps vs. 1,000 rounds with 1 epoch)
- Demonstrates label efficiency with stable accuracy across 2,800-9,800 labeled examples per client
- Outperforms supervised FL baselines (FedAvg, FedeAMC, FedProx, FedDyn) on both synthetic and over-the-air datasets

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining on unlabeled I/Q streams aligns latent representations across clients before label-driven adaptation, mitigating representation drift under non-IID distributions. The triplet loss objective trains the encoder to embed positive pairs (subsequences from same I/Q signal) close together while pushing negative pairs (subsequences from different signals) apart, leveraging local temporal consistency of wireless signals as an implicit supervisory signal without requiring labels. This works because low-level temporal features of I/Q symbol sequences are shared across modulation classes and client distributions, enabling a common feature space to emerge from unlabeled data.

### Mechanism 2
Decoupling representation learning from downstream classification enables label-efficient personalization under severe class imbalance. A shared encoder is trained federated on abundant unlabeled data via triplet loss, then each client independently fits a lightweight SVM classifier on its small labeled subset. This restricts personalization to a low-dimensional output layer while keeping representations collaborative. The learned representation space is sufficiently discriminative that a linear SVM can separate modulation classes even with limited labeled examples per client.

### Mechanism 3
Time-dilated causal convolutions capture long-range temporal dependencies in I/Q sequences more efficiently than sequential models. Stacked convolutional layers with exponentially increasing dilation expand the receptive field exponentially with depth while maintaining O(χΨΛ) inference complexity. The causal constraint ensures each output depends only on current and past inputs. This works because modulation-relevant patterns in I/Q sequences span long temporal contexts (e.g., phase accumulation from carrier frequency offset over many samples).

## Foundational Learning

- **Concept:** Triplet loss / contrastive learning
  - **Why needed here:** Core self-supervised objective enabling representation learning from unlabeled I/Q sequences.
  - **Quick check question:** Can you explain why pulling positive pairs together and pushing negative pairs apart in embedding space creates useful representations?

- **Concept:** Federated averaging (FedAvg)
  - **Why needed here:** Aggregation mechanism for distributed encoder training across clients.
  - **Quick check question:** How does FedAvg aggregate client updates, and what happens when client data distributions diverge significantly?

- **Concept:** Carrier frequency offset (CFO) and I/Q signal structure
  - **Why needed here:** Understanding CFO-induced heterogeneity is essential for interpreting the mobility experiments and why representation alignment is challenging.
  - **Quick check question:** How does Doppler shift from relative motion manifest as a frequency offset in the baseband signal, and why does this create client heterogeneity?

## Architecture Onboarding

- **Component map:** Causal CNN encoder -> Triplet loss module -> FedAvg aggregator -> Per-client SVM

- **Critical path:**
  1. Initialize global encoder θ₀
  2. For T rounds: clients download θ_{t-1}, compute local triplet-loss gradients on unlabeled data, upload θ_t^c
  3. Server aggregates θ_t = Σ_c (n_c/n) θ_t^c
  4. After T rounds, each client fits SVM on labeled data using frozen encoder

- **Design tradeoffs:**
  - **Communication vs. convergence:** Paper uses T=10 rounds with 2,500 local steps vs. baselines at 1,000 rounds with 1 epoch—a 100× reduction in communication at similar or better accuracy
  - **Encoder capacity vs. compute:** Causal CNN has 0.247M parameters (5× smaller than baseline) but 473.56 MFLOPs (26× higher) due to triplet loss and larger receptive field
  - **SVM vs. neural classifier:** SVM provides label efficiency and personalization but cannot be fine-tuned end-to-end

- **Failure signatures:**
  - **Representation drift:** If local triplet losses diverge across clients, aggregated encoder may serve no one well—monitor per-client loss curves
  - **SVM underfitting:** If accuracy saturates with more labeled data, encoder features may lack discriminative power—check class separation in feature space via t-SNE
  - **CFO/mobility mismatch:** If clients experience CFO regimes unseen during encoder training, representations may fail—validate encoder on held-out CFO ranges

- **First 3 experiments:**
  1. **Ablate labeled data:** Replicate Table 3 by varying labeled examples per client (2,800 → 14,000) to confirm label efficiency claims
  2. **CFO heterogeneity stress test:** Replicate Figure 4 by assigning clients to different mobility regimes (ultra-low to high ∆f) to test robustness
  3. **Encoder-only transfer:** Train encoder on synthetic data, freeze, then fit SVM on MIGOU data to assess cross-dataset transfer without encoder fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
Can clustering clients based on their data distributions (e.g., shared SNR or CFO characteristics) enhance performance through group-wise contrastive learning? The current FedSSL-AMC framework aggregates all clients into a single global model, which may struggle with extreme statistical heterogeneity where distinct client groups exist. Experiments comparing global aggregation against clustered aggregation under high data skew would show improved convergence speed or accuracy for minority groups.

### Open Question 2
Do the convergence guarantees established for the linear representation model extend to the practical deep causal CNN architecture? Section 3.1 explicitly analyzes "a simplified setting where each client optimizes a linear representation model," while the proposed FedSSL-AMC actually utilizes a deep, time-dilated causal CNN. There is a theoretical gap between the convergence bounds derived for the linear model and the non-convex loss landscape of the deep CNN used in practice. Extension of Theorem 1 to multi-layer non-linear settings or empirical analysis showing the loss landscape satisfies smoothness bounds would resolve this.

### Open Question 3
Does the performance of the triplet loss saturate with random negative sampling, necessitating hard-negative mining strategies? Section 2.1 describes the implementation where negative examples are simply "drawn from a different I/Q sequence" without discussing the difficulty or quality of these negatives. Contrastive learning often benefits from hard negative mining; random sampling might yield "easy" negatives that provide weak gradient signals. An ablation study comparing random sampling against semi-supervised hard negative mining strategies on the MIGOU dataset would resolve this.

## Limitations
- Theoretical separability guarantee assumes Gaussian noise bounded by 1/√L, which may not hold for real wireless channels with impulsive noise or interference
- The claim that time-dilated convolutions are inherently superior for capturing long-range dependencies lacks direct comparative evidence in the corpus
- Complexity analysis focuses on inference FLOPs but omits training costs for the triplet loss, which requires mining 10 negatives per anchor and could dominate communication rounds

## Confidence

- **High confidence:** The label-efficient personalization mechanism (Mechanism 2) is well-supported by Theorem 2 and empirical evidence showing stable accuracy across 2,800–9,800 labeled examples
- **Medium confidence:** The convergence and separability theory (Theorem 1 and 2) provides theoretical grounding but relies on idealized assumptions about noise and separability that may not translate to practice
- **Low confidence:** The claim that time-dilated convolutions are inherently superior for capturing long-range dependencies in I/Q sequences lacks direct comparative evidence in the corpus

## Next Checks
1. **Noise sensitivity validation:** Test the encoder's robustness to non-Gaussian channel noise (e.g., impulsive noise, interference) beyond the theoretical Gaussian noise model to verify separability guarantees hold in practice
2. **Self-supervised objective ablation:** Replace triplet loss with alternative self-supervised objectives (e.g., contrastive loss, masked autoencoding) to determine if the performance gains are specific to the triplet formulation or general to self-supervised AMC
3. **Cross-dataset encoder transfer:** Train the encoder on synthetic data, freeze weights, and evaluate SVM performance on MIGOU without fine-tuning to quantify true representation quality independent of encoder adaptation