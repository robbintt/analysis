---
ver: rpa2
title: 'Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient
  Multi-Task Learners'
arxiv_id: '2505.23150'
source_url: https://arxiv.org/abs/2505.23150
tags:
- tasks
- learning
- multi-task
- figure
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of scaling value-based reinforcement\
  \ learning (RL) to high-capacity models trained across many diverse tasks. It identifies\
  \ that na\xEFvely scaling standard temporal-difference methods leads to instability\
  \ due to reward imbalances and gradient conflicts across tasks."
---

# Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners

## Quick Facts
- arXiv ID: 2505.23150
- Source URL: https://arxiv.org/abs/2505.23150
- Reference count: 40
- Primary result: High-capacity value networks trained with categorical cross-entropy loss and task embeddings enable stable, scalable multi-task RL across 283 tasks

## Executive Summary
The paper addresses the challenge of scaling value-based reinforcement learning to high-capacity models trained across many diverse tasks. Standard temporal-difference methods become unstable when naively scaled due to reward imbalances and gradient conflicts across tasks. The authors propose Bigger, Regularized, Categorical (BRC), which combines a high-capacity BroNet critic, cross-entropy loss via categorical Q-learning, and learnable task embeddings. This design stabilizes multi-task training and enables scaling to over 1 billion parameters. Experiments across 283 tasks from 5 benchmarks show BRC achieves state-of-the-art performance, outperforms single-task baselines, and enables sample-efficient transfer to new tasks. Multi-task training is also shown to be more compute-efficient than single-task learning.

## Method Summary
BRC builds on SAC and DrQ-ϵ, combining a high-capacity BroNet critic with layer normalization and residual connections, categorical Q-learning with 101 atoms over [-10,10], and learnable task embeddings (32-dim, L1-constrained). The key innovation is using cross-entropy loss instead of MSE for TD updates, combined with per-task return normalization that scales rewards by the running maximum Monte Carlo return. This stabilizes gradient magnitudes across tasks with different reward scales. The architecture scales from 64M to 1B parameters while maintaining training stability through the regularized residual design. Task embeddings are concatenated to state observations and updated via TD backpropagation, discovering latent task structure rather than using separate heads.

## Key Results
- BRC achieves state-of-the-art performance across 283 tasks from 5 benchmarks (MetaWorld, DMC, HumanoidBench, ShadowHand, Atari)
- Cross-entropy loss reduces variance in TD loss, gradient norms, and policy entropy across tasks compared to MSE
- Increasing model capacity from 64M to 1B parameters reduces gradient conflicts and improves transfer efficiency
- Multi-task pretraining with BRC enables sample-efficient transfer to new tasks, outperforming single-task pretraining
- Multi-task training is more compute-efficient than single-task learning across all model scales

## Why This Works (Mechanism)

### Mechanism 1: Cross-entropy loss balances multi-task gradient signals
Replacing MSE with cross-entropy via categorical Q-learning stabilizes multi-task TD learning by reducing variance in loss magnitudes, gradient norms, and policy entropy across tasks. MSE scales with reward magnitude, causing high-reward tasks to dominate gradient updates. Cross-entropy treats Q-value prediction as classification over fixed support atoms, removing scale dependence. When combined with per-task return normalization, all tasks produce bounded, comparable gradient signals. If reward scales vary by orders of magnitude and maximal returns are underestimated early in training, normalization may introduce bias; categorical representation with fixed support clips extreme values, potentially underestimating returns.

### Mechanism 2: High-capacity regularized networks reduce gradient conflicts
Increasing Q-network capacity (up to 1B parameters) with layer-normalized residual connections reduces inter-task gradient conflicts and enables positive transfer. Larger models have higher representational capacity, allowing separate "sub-networks" to specialize for different tasks while sharing low-level features. Layer normalization prevents activation explosion at depth, and residual connections maintain gradient flow. The paper empirically shows gradient conflict rate decreases as model size increases. If tasks are fundamentally incompatible (different action spaces, conflicting objectives), even infinite capacity may not resolve conflicts; computational costs scale quadratically with width in practice.

### Mechanism 3: Learnable task embeddings discover shared structure
Task embeddings learned via TD backpropagation outperform separate task heads by discovering latent task structure and enabling parameter-efficient sharing. Instead of separate output heads per task, embeddings are concatenated to state observations and updated via the same TD loss. This forces embeddings to encode task-relevant features that predict value, naturally clustering similar tasks. Task identity can be compressed into a low-dimensional embedding (32-dim used); the TD loss gradient provides sufficient signal to learn meaningful embeddings. If tasks require fundamentally different state processing (e.g., vision vs. proprioception), a shared encoder may bottleneck performance; embedding dimension may need scaling with task diversity.

## Foundational Learning

- **Temporal Difference (TD) Learning**:
  - Why needed here: BRC builds on SAC and DrQ-ϵ, both actor-critic methods using TD targets. Understanding bootstrapped value estimation is essential to grasp why gradient conflicts arise.
  - Quick check question: Can you explain why TD(0) updates use a target network and why this can cause instability in multi-task settings?

- **Distributional RL (Categorical Q-Learning)**:
  - Why needed here: BRC reframes Q-value regression as classification over atoms; understanding the projection step and cross-entropy loss is non-optional.
  - Quick check question: Given a Bellman update that shifts probability mass to a non-atom position, how does categorical distributional RL handle this?

- **Multi-Task Gradient Interference**:
  - Why needed here: The paper explicitly addresses gradient conflicts; understanding why task gradients can oppose each other motivates the architectural choices.
  - Quick check question: If task A's gradient is [1, -1] and task B's gradient is [-1, 1], what happens with naive gradient averaging?

## Architecture Onboarding

- **Component map**:
  - Replay Buffer -> Sampler (mixed tasks) -> Reward Normalization -> Categorical Projection -> Cross-Entropy Loss -> BroNet Critic (Residual MLP + LayerNorm) -> Task Embedding Concatenation -> Gaussian Actor -> Policy Update

- **Critical path**:
  1. Sample batch from replay buffer (mixed tasks)
  2. Normalize rewards using per-task `G_max` estimates
  3. Compute TD target: project `r + γ * V_target(s')` onto categorical support
  4. Compute cross-entropy loss between predicted and target distributions
  5. Backpropagate through critic, actor, and task embeddings jointly
  6. Update target networks via Polyak averaging

- **Design tradeoffs**:
  - Width vs. depth: Paper uses width scaling (256→8192) with fixed depth 2; deeper networks may require different regularization
  - Embedding dimension: 32-dim worked for 283 tasks; higher task diversity may require larger embeddings
  - Vmin/Vmax range: [-10, 10] assumes normalized returns; if normalization fails, values get clipped
  - Separate heads vs. embeddings: Embeddings share all parameters; separate heads share encoder only. Paper shows embeddings win at all scales.

- **Failure signatures**:
  - Training divergence with large models: Check layer norm is applied correctly; vanilla MLPs diverge per Figure 4
  - High variance in per-task metrics: Indicates reward normalization may be failing; verify `G_max` updates
  - Embeddings not clustering: May need longer training or more tasks; check embedding gradients are non-zero
  - Transfer performance degrading: Pretrained model may have overfit to training tasks; try smaller model or more diverse pretraining

- **First 3 experiments**:
  1. Single-task baseline: Run SAC+BRC on one task (e.g., HumanoidBench walk) with 64M parameter critic; verify performance matches or exceeds standard SAC
  2. Multi-task ablation: Train on 3-5 tasks from same benchmark with and without cross-entropy loss; plot per-task variance of gradient norms (replicate Figure 3 pattern)
  3. Embedding visualization: After multi-task training on MW+DMC, compute PCA of learned embeddings; verify clustering by embodiment type (replicate Figure 7 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific conditions determine whether tasks within a mixed-embodiment suite create positive synergy or fail to reinforce one another during multi-task training?
- Basis in paper: In the Limitations section, the authors state their results "do not yet clarify when tasks help each other and when they do not," noting that mixed-embodiment suites show synergy in some cases but none in others.
- Why unresolved: The authors observe that tasks sharing embodiments usually reinforce one another, but the outcome for mixed embodiments is inconsistent and unpredictable with the current methodology.
- What evidence would resolve it: A theoretical framework or a set of empirical metrics that can predict task interference or synergy prior to training across diverse robot morphologies.

### Open Question 2
- Question: Can a comprehensive theory of task similarity in RL be established to accurately predict how data diversity scales with model capacity?
- Basis in paper: Section 5 concludes that "establishing a more comprehensive theory of task similarity in RL can be an impactful avenue for future research."
- Why unresolved: The paper demonstrates empirically that increased data diversity leads to general features and sample-efficient transfer, but lacks a formal theoretical explanation for why specific task groupings scale better than others.
- What evidence would resolve it: A mathematical formulation of task similarity that correlates with the observed scaling laws and transfer efficiency improvements shown in Figure 10.

### Open Question 3
- Question: Why does weight initialization (model transfer) lead to faster convergence and higher returns than replay buffer initialization (data transfer) when fine-tuning on new tasks?
- Basis in paper: Section 5 notes that model transfer outperforms data transfer and states, "Although the exact reason for this requires further study, pretrained weights encode the dataset without inducing distribution mismatch when learning new tasks."
- Why unresolved: While the authors hypothesize that pretrained weights avoid distribution mismatch, they do not verify if this is the sole cause or if other factors, such as feature reuse versus data composition, play a larger role.
- What evidence would resolve it: An ablation study isolating the effects of distribution mismatch and feature maturity during the fine-tuning phases of both transfer methods.

## Limitations
- Computational accessibility: Scaling to 1B parameters requires specialized hardware (e.g., TPUv4 with 512GB memory) and complex distributed training setups
- Normalization assumptions: Return normalization depends on reliable estimation of maximal episodic returns per task, which may be unstable for highly stochastic or delayed rewards
- Task compatibility constraints: The framework assumes tasks share compatible state and action spaces with some underlying structure; fundamentally incompatible tasks may not benefit from shared representations

## Confidence

- **High confidence**: The empirical demonstration that categorical cross-entropy loss reduces gradient variance compared to MSE is well-supported by controlled experiments (Figure 3). The architectural scaling results showing reduced gradient conflicts with larger models are reproducible.

- **Medium confidence**: The claim that learnable task embeddings outperform separate task heads is supported by ablation studies, but the analysis focuses on specific benchmarks. The generalization to arbitrary task distributions requires further validation.

- **Medium confidence**: The computational efficiency claim (multi-task vs. single-task training) is supported by theoretical FLOPs analysis but may not fully capture practical considerations like distributed training overhead and hyperparameter tuning costs.

## Next Checks

1. **Normalization robustness test**: Design an experiment where return normalization fails due to highly stochastic rewards or delayed rewards. Measure how BRC's performance degrades compared to MSE-based methods and whether clipping artifacts emerge.

2. **Task compatibility boundary test**: Create a multi-task setup with intentionally incompatible tasks (e.g., different action dimensions or conflicting reward structures). Evaluate whether BRC's scaling and regularization strategies can handle such edge cases or if performance collapses.

3. **Transfer efficiency validation**: Implement the transfer learning protocol on a held-out set of tasks not seen during pretraining. Compare sample efficiency and final performance against single-task trained baselines, controlling for total compute budget to validate the computational efficiency claim.