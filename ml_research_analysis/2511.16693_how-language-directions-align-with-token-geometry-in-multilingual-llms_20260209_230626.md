---
ver: rpa2
title: How Language Directions Align with Token Geometry in Multilingual LLMs
arxiv_id: '2511.16693'
source_url: https://arxiv.org/abs/2511.16693
tags:
- language
- multilingual
- alignment
- token
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically analyzes how multilingual LLMs encode\
  \ language information across all transformer layers. Using linear and MLP probes\
  \ plus a novel Token\u2013Language Alignment metric, the authors find that language\
  \ separation emerges sharply in the first block (+76.4 percentage points from Layer\
  \ 0 to 1) and remains nearly fully linearly separable (99.8\xB10.1%) throughout\
  \ model depth."
---

# How Language Directions Align with Token Geometry in Multilingual LLMs

## Quick Facts
- arXiv ID: 2511.16693
- Source URL: https://arxiv.org/abs/2511.16693
- Reference count: 12
- This study systematically analyzes how multilingual LLMs encode language information across all transformer layers.

## Executive Summary
This study systematically analyzes how multilingual LLMs encode language information across all transformer layers. Using linear and MLP probes plus a novel Token–Language Alignment metric, the authors find that language separation emerges sharply in the first block (+76.4 percentage points from Layer 0 to 1) and remains nearly fully linearly separable (99.8±0.1%) throughout model depth. Linear probes match MLP performance within 1%, indicating language identity lies in a globally accessible subspace. The Token–Language Alignment reveals strong structural imprinting: Chinese-inclusive models show a ZH Match@Peak of 16.43%, while English-centric models achieve only 3.90%, a 4.21× difference. This shows that pretraining language composition shapes the geometry of latent representations, not just performance. The findings highlight that fairness and balance in multilingual models require careful pretraining data design, as post-hoc adjustments cannot fully undo these structural imprints.

## Method Summary
The authors analyze multilingual LLMs by probing language identity across transformer layers using linear and MLP classifiers. They introduce a novel Token–Language Alignment metric that measures structural correspondence between tokens and language-specific directions. The study examines how language separation emerges across layers and how pretraining composition affects geometric imprinting in latent representations.

## Key Results
- Language separation emerges sharply in the first block (+76.4 percentage points from Layer 0 to 1) and remains nearly fully linearly separable (99.8±0.1%) throughout model depth
- Linear probes match MLP performance within 1%, indicating language identity lies in a globally accessible subspace
- The Token–Language Alignment reveals strong structural imprinting: Chinese-inclusive models show a ZH Match@Peak of 16.43%, while English-centric models achieve only 3.90%, a 4.21× difference

## Why This Works (Mechanism)
The study demonstrates that language-specific geometric separation in multilingual LLMs emerges early in the transformer layers and persists throughout model depth. The mechanism appears to be that pretraining data composition directly shapes the geometry of latent representations, creating structural imprints that reflect the languages present in training data. This geometric separation is both linear and stable, suggesting that language identity is encoded in a globally accessible subspace rather than requiring complex non-linear transformations.

## Foundational Learning
- **Transformer architecture**: Why needed - To understand how language information flows through layers; Quick check - Can trace information from input to output through attention and feed-forward layers
- **Linear probe methodology**: Why needed - To measure linear separability of language representations; Quick check - Can distinguish between linear and non-linear encoding
- **Multilingual model pretraining**: Why needed - To understand how training data composition affects learned representations; Quick check - Can identify relationships between pretraining languages and model behavior
- **Vector geometry in embedding space**: Why needed - To interpret the Token-Language Alignment metric; Quick check - Can visualize and measure directional relationships between tokens and language centroids

## Architecture Onboarding

**Component map:**
Token embedding -> Transformer blocks (L0 → L1 → ... → Lfinal) -> Linear/MLP probes for language classification -> Token-Language Alignment analysis

**Critical path:**
Input tokens → Early transformer layers → Language-separated representations → Stable geometric encoding → Probed classification

**Design tradeoffs:**
- Simple linear probes vs. complex MLPs (minimal performance difference suggests simple separation)
- Single layer probing vs. cross-layer analysis (current study focuses on final layer of each block)
- Fixed k threshold (k=8) in Token-Language Alignment vs. adaptive methods

**Failure signatures:**
- If language separation doesn't emerge early, suggests poor pretraining or model architecture issues
- If linear and MLP performance differ significantly, suggests complex non-linear encoding patterns
- If Token-Language Alignment shows low correlation, suggests poor geometric imprinting

**First 3 experiments to run:**
1. Test linear vs. MLP probe performance on the same language-separated representations
2. Measure language separation emergence across transformer layers from L0 to Lfinal
3. Calculate Token-Language Alignment for different pretraining compositions

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses on token-level representations in the final layer of each transformer block, which may miss subtler cross-layer mixing effects
- The dataset composition (four languages with varying script families) limits claims about broader multilingual patterns
- The Token-Language Alignment metric depends on fixed top-k thresholds (k=8) that may not capture all meaningful alignment patterns

## Confidence
- Language separation emerges in early layers and remains stable: **High** - supported by consistent probe accuracy across layers
- Linear probes match MLP performance: **High** - statistical differences are minimal (within 1%)
- Pretraining composition shapes geometric imprinting: **Medium** - correlation is strong but causal mechanisms require further investigation
- Fairness implications require careful pretraining design: **Medium** - theoretical but not empirically validated through fairness metrics

## Next Checks
1. Test Token-Language Alignment across diverse language families and scripts to assess generalizability beyond the four languages studied
2. Evaluate whether geometric separations correlate with actual cross-lingual transfer performance on downstream tasks
3. Investigate intermediate token representations (not just final layer) to understand information flow and mixing patterns across transformer blocks