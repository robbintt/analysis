---
ver: rpa2
title: 'EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially
  Robust Vision-Language Models'
arxiv_id: '2502.14976'
source_url: https://arxiv.org/abs/2502.14976
tags:
- adversarial
- eigenshield
- causal
- distribution
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EigenShield addresses adversarial jailbreak vulnerabilities in
  Vision-Language Models (VLMs) by leveraging Random Matrix Theory (RMT) to filter
  adversarial perturbations at inference time. The method identifies and isolates
  causal eigenvectors using a Robustness-based Nonconformity Score (RbNS) and quantile-based
  thresholding, then projects embeddings onto the causal subspace to remove adversarial
  noise without modifying model parameters.
---

# EigenShield: Causal Subspace Filtering via Random Matrix Theory for Adversarially Robust Vision-Language Models

## Quick Facts
- **arXiv ID:** 2502.14976
- **Source URL:** https://arxiv.org/abs/2502.14976
- **Reference count:** 40
- **Primary result:** Reduces attack success rates from 81.61% to 19.20% on LLaV A-v1.5-7B using eigenvector filtering

## Executive Summary
EigenShield addresses adversarial jailbreak vulnerabilities in Vision-Language Models (VLMs) by leveraging Random Matrix Theory (RMT) to filter adversarial perturbations at inference time. The method identifies and isolates causal eigenvectors using a Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, then projects embeddings onto the causal subspace to remove adversarial noise without modifying model parameters. Evaluated across five state-of-the-art VLMs (LLaV A-v1.5-7B, MiniGPT-4, InstructBLIP, Qwen2-VL, Florence-2-large), EigenShield achieves significant improvements: reducing attack success rates (ASR) from 81.61% to 19.20% on LLaV A-v1.5-7B, from 37.20% to 20.37% on MiniGPT-4, and from 59.80% to 41.21% on InstructBLIP. Toxicity scores also decrease substantially, with LLaV A-v1.5-7B dropping from 77.93 to 13.28. EigenShield consistently outperforms adversarial training, UNIGUARD, CIDER, and diffusion-based defenses while maintaining computational efficiency and clean input integrity.

## Method Summary
EigenShield operates by analyzing the image feature matrix C and its eigenvalue decomposition to identify and isolate causal eigenvectors. The method computes the Robustness-based Nonconformity Score (RbNS) for each eigenvector, which measures its contribution to robustness against adversarial attacks. A quantile-based thresholding approach then selects eigenvectors with high RbNS values, representing the causal subspace that captures meaningful image features. During inference, EigenShield projects the image embeddings onto this causal subspace before passing them to the VLM, effectively removing adversarial perturbations while preserving essential visual information. This approach requires no model retraining or fine-tuning, making it computationally efficient and compatible with existing VLM architectures.

## Key Results
- Reduces attack success rates (ASR) from 81.61% to 19.20% on LLaV A-v1.5-7B
- Decreases toxicity scores from 77.93 to 13.28 on LLaV A-v1.5-7B
- Outperforms adversarial training, UNIGUARD, CIDER, and diffusion-based defenses

## Why This Works (Mechanism)
EigenShield leverages the observation that adversarial perturbations typically manifest as low-RbNS eigenvectors that can be safely removed without affecting the causal structure of the input. By using Random Matrix Theory to decompose the image feature matrix and identify the robust causal subspace, the method can effectively filter out noise while preserving the essential visual information needed for accurate VLM responses. The RbNS metric provides a principled way to distinguish between meaningful features and adversarial noise based on their contribution to model robustness.

## Foundational Learning
- **Random Matrix Theory (RMT):** Provides mathematical framework for analyzing eigenvalue distributions in high-dimensional matrices. Why needed: Enables principled decomposition of image feature matrix to identify causal structure. Quick check: Verify eigenvalue distribution follows expected RMT patterns.
- **Eigenvector filtering:** Technique for removing components from data based on eigenvector analysis. Why needed: Allows selective removal of adversarial perturbations while preserving meaningful features. Quick check: Confirm filtered embeddings maintain semantic content.
- **Robustness-based Nonconformity Score (RbNS):** Metric for measuring contribution to model robustness. Why needed: Provides principled criterion for distinguishing causal from adversarial eigenvectors. Quick check: Validate RbNS correlates with actual robustness improvements.
- **Quantile-based thresholding:** Statistical method for selecting top-k components. Why needed: Enables automatic selection of causal eigenvectors without manual tuning. Quick check: Test different quantile thresholds for optimal performance.
- **Causal subspace projection:** Mathematical operation for dimensionality reduction while preserving causal structure. Why needed: Allows efficient removal of adversarial components while maintaining model performance. Quick check: Verify projected embeddings maintain task accuracy.
- **Adversarial jailbreak attacks:** Targeted attacks designed to manipulate VLM outputs through input perturbations. Why needed: Defines the threat model that EigenShield aims to defend against. Quick check: Confirm attack success rates decrease after applying EigenShield.

## Architecture Onboarding
- **Component map:** Input image -> Feature extraction -> Eigenvalue decomposition -> RbNS calculation -> Quantile thresholding -> Causal subspace projection -> VLM processing
- **Critical path:** The eigenvalue decomposition and RbNS calculation steps are computationally intensive but can be optimized through incremental updates for streaming applications.
- **Design tradeoffs:** EigenShield trades minimal computational overhead at inference time for significant improvements in adversarial robustness, avoiding the need for expensive model retraining.
- **Failure signatures:** If RbNS thresholding is too aggressive, legitimate features may be removed causing performance degradation on clean inputs. If too conservative, adversarial perturbations may remain.
- **Three first experiments:** 1) Test RbNS threshold sensitivity on validation set, 2) Compare different eigenvalue decomposition methods for computational efficiency, 3) Evaluate performance on black-box attacks where attacker doesn't know EigenShield is deployed.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on assumption that adversarial perturbations manifest as low-RbNS eigenvectors, which may not hold for all attack types
- Computational overhead of eigenvalue decomposition may be prohibitive for very large VLMs
- Effectiveness may degrade against adaptive attacks specifically designed to evade eigenvector-based filtering

## Confidence
- **High Confidence:** Substantial reductions in attack success rates and toxicity scores across multiple VLM architectures
- **Medium Confidence:** Generalizability to different VLM architectures beyond those tested
- **Medium Confidence:** Practical effectiveness of RbNS thresholding across diverse datasets and attack methodologies

## Next Checks
1. **Cross-dataset robustness testing:** Evaluate EigenShield's performance on diverse datasets (e.g., ImageNet, COCO) beyond the PASCAL dataset to verify consistent defense effectiveness across different visual domains.
2. **Adaptive attack evaluation:** Test EigenShield against white-box attacks where adversaries are aware of the eigenvector filtering mechanism and optimize perturbations to evade the RbNS thresholding criteria.
3. **Scalability analysis:** Assess the computational overhead and defense effectiveness when applied to larger VLMs (e.g., 34B+ parameter models) to determine practical limitations in real-world deployment scenarios.