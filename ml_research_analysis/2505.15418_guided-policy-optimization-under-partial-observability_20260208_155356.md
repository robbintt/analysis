---
ver: rpa2
title: Guided Policy Optimization under Partial Observability
arxiv_id: '2505.15418'
source_url: https://arxiv.org/abs/2505.15418
tags:
- policy
- learning
- teacher
- guider
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Guided Policy Optimization (GPO), a framework
  for co-training a guider (with privileged information) and a learner in partially
  observable environments. The guider is trained via RL with alignment constraints
  to the learner, who is primarily trained via imitation learning.
---

# Guided Policy Optimization under Partial Observability

## Quick Facts
- **arXiv ID:** 2505.15418
- **Source URL:** https://arxiv.org/abs/2505.15418
- **Reference count:** 40
- **Primary result:** GPO co-trains a privileged-information guider and observation-based learner, achieving RL-level performance with imitation learning efficiency in partially observable environments.

## Executive Summary
This paper introduces Guided Policy Optimization (GPO), a framework for co-training a guider (with privileged information) and a learner in partially observable environments. The guider is trained via RL with alignment constraints to the learner, who is primarily trained via imitation learning. The method addresses the challenge of leveraging privileged information in POMDPs without requiring a pre-trained teacher. GPO is theoretically shown to achieve optimality comparable to direct RL, overcoming limitations of existing teacher-student approaches. Empirical evaluations demonstrate GPO's superior performance over baselines on partially observable continuous control tasks in Brax and memory-based tasks in POPGym, especially under noise and partial observability. The framework effectively uses privileged information to improve learning efficiency and robustness.

## Method Summary
GPO splits learning into two phases: a guider trained with RL using privileged state information, and a learner trained primarily via supervised imitation of the guider while constrained to stay close through KL divergence penalties. The guider's optimization includes an alignment constraint that prevents it from learning policies the learner cannot imitate, theoretically ensuring the learner achieves optimality comparable to direct RL. The framework can use either adaptive KL penalties (GPO-penalty) or clipping-based constraints (GPO-clip) to maintain alignment. Both components share a network backbone but receive different inputs—the guider gets full state plus observations, while the learner gets zeros plus observations—allowing parameter sharing while distinguishing information access.

## Key Results
- GPO outperforms standard PPO and PPO+BC baselines on partially observable continuous control tasks in Brax, especially under increasing noise levels.
- GPO-naive (pure imitation learning) achieves optimal reward in didactic TigerDoor task where PPO+BC fails, validating theoretical claims about overcoming the imitation gap.
- GPO demonstrates robust performance on POPGym memory tasks, though it shows limitations when the learner's memory architecture cannot retain sufficient information to track the guider.

## Why This Works (Mechanism)

### Mechanism 1: Policy Mirror Descent Equivalence
The framework enforces a constraint that aligns the guider's policy μ with the learner's policy π. Theoretically, iteratively optimizing the guider with RL and projecting the learner onto the guider's policy via KL divergence is equivalent to the learner performing constrained policy mirror descent directly on the environment. This overcomes the "imitation gap" of traditional Teacher-Student Learning. Break condition: If the guider learns too fast or the KL constraint is too loose, the guider exits the learner's "possibly good" region, causing divergence.

### Mechanism 2: Variance Reduction via Decomposed Optimization
Policy gradients under partial observability suffer from high variance. GPO assigns the high-variance RL task to the guider (with privileged state information), while the learner is trained primarily via supervised imitation learning (BC), which is lower variance. This allows the system to benefit from privileged info without the learner needing to learn a complex RL policy from scratch on noisy observations. Break condition: If the privileged state to observation mapping requires stochastic actions dependent on unobservable history, simple imitation fails.

### Mechanism 3: Asymmetric Sample Reuse
The framework uses a shared behavioral policy (the guider). By constraining the guider to stay close to the learner via KL penalty or clipping, the importance sampling ratios used for the learner's auxiliary RL update remain valid. This prevents the learner from lagging too far behind the guider's data distribution. Break condition: If the alignment penalty α is too weak, the guider drifts, and the learner's off-policy correction becomes too high-variance.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - **Why needed here:** To understand why the "state" available to the guider (s) and the "observation" available to the learner (o) result in different optimal policies, creating the "imitation gap."
  - **Quick check question:** Why does a deterministic policy trained on full state fail when mapped directly to partial observations in the TigerDoor task?

- **Concept: Imitation Learning & Behavioral Cloning (BC)**
  - **Why needed here:** The learner's core update is minimizing KL divergence to the guider. Understanding BC failure modes (covariate shift, "impossibly good teacher") explains why the *alignment constraint* is the novel contribution here.
  - **Quick check question:** In the TigerDoor example, why does standard BC fail to learn the "listen" action?

- **Concept: Trust Region Methods (TRPO/PPO)**
  - **Why needed here:** GPO builds explicitly on PPO. Understanding the clipping mechanism and KL penalties is required to implement the guider's constrained optimization.
  - **Quick check question:** How does the GPO "double-clip" (Eq. 9) differ from standard PPO clipping?

## Architecture Onboarding

- **Component map:** Guider (s,o,1) -> PPO Loss + KL Alignment -> Guider Update; Learner (0,o,0) -> BC Loss + PPO Aux -> Learner Update
- **Critical path:**
  1. Collect data using Guider (μ).
  2. Compute Guider PPO loss (L₁).
  3. Compute Alignment Penalty (L₃) and adjust α.
  4. Update Guider (L₁ - αL₃).
  5. Update Learner via BC (L₂) and auxiliary PPO (L₄) using the same batch of data.
- **Design tradeoffs:**
  - Shared vs. Separate Networks: Sharing saves memory but can hurt performance on high-dimensional observation spaces (e.g., HumanoidStandup).
  - GPO-penalty vs. GPO-clip: Penalty requires tuning adaptive coefficients (α,d); Clip is simpler but might require more aggressive backtracking if the gap widens.
- **Failure signatures:**
  - Lagging Learner: Learner reward plateaus while Guider continues improving. *Cause:* KL constraint (d) is too loose, or learning rate is too low relative to Guider.
  - Collapse to PPO: Performance matches PPO-asym. *Cause:* Guider constraint is too tight or imitation loss is zeroed out because the Guider is "impossibly good."
  - Memory Bottleneck: In POPGym tasks, GRU fails to retain history needed to bridge the gap between privileged info and observation.
- **First 3 experiments:**
  1. TigerDoor Validation: Implement TigerDoor-alt task to verify GPO-naive achieves optimal reward where PPO+BC fails.
  2. Ablation on Alignment: Run Brax Ant with and without backtrack constraint (L₃) to confirm unconstrained co-training leads to instability.
  3. Noise Robustness: Test on Brax with increasing noise σ (0.0 to 0.3). Expect GPO to maintain performance better than PPO-asym as noise increases.

## Open Questions the Paper Calls Out
- **Multi-Agent Extension:** Future work could explore extending guided policy optimization to the multi-agent setting, where agents utilize global information during training but are constrained to local observations during execution.
- **Memory Architecture Limits:** Tasks that strain memory models like GRU—when GRU fails to retain key information, the learner cannot follow the guider—suggesting a need for more expressive memory architectures.
- **Automatic Hyperparameter Adaptation:** Poorly tuned KL thresholds can cause failure modes, indicating a need for dynamic scheduling mechanisms for alignment thresholds that maintain performance across varying noise levels without manual per-task tuning.

## Limitations
- The framework's theoretical guarantees rely on assumptions about the learner's policy class expressiveness that aren't fully validated empirically across all tasks.
- Empirical evaluation, while comprehensive across 8 tasks, doesn't systematically vary the observation-to-state mapping complexity to test the framework's limits.
- The backtracking mechanism's robustness when the imitation gap becomes large isn't fully characterized, particularly when the guider might learn policies fundamentally impossible for the learner to imitate.

## Confidence
- **High Confidence:** The mechanism of variance reduction through decomposed optimization is well-supported by experimental results showing GPO outperforming standard PPO across multiple noise levels and partial observability settings.
- **Medium Confidence:** The theoretical equivalence to policy mirror descent is sound but relies on assumptions about the learner's policy class expressiveness that aren't fully validated empirically.
- **Medium Confidence:** The asymmetric sample reuse benefits are demonstrated but the analysis focuses on bounding the error rather than quantifying practical sample efficiency gains compared to alternative approaches.

## Next Checks
1. **Imitation Gap Stress Test:** Systematically vary the amount of privileged information available to the guider (e.g., gradually remove state dimensions) and measure at what point the alignment constraints fail to keep the guider within the learner's imitation capabilities.

2. **Alternative Backtracking Analysis:** Compare the proposed backtracking mechanism against simpler alternatives like KL penalty scheduling or trust region constraints that don't require the guider to explicitly account for the learner's policy during optimization.

3. **Memory Capacity Scaling:** In POPGym memory tasks, systematically vary the capacity of the learner's memory (e.g., LSTM/GRU size) to quantify how much the memory bottleneck limits GPO's ability to bridge the privileged information gap.