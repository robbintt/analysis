---
ver: rpa2
title: Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty
arxiv_id: '2505.03910'
source_url: https://arxiv.org/abs/2505.03910
tags:
- uncertainty
- deep
- predictive
- human
- chexpert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the relationship between predictive uncertainty,
  derived from Bayesian Deep Learning approximations, and human/linguistic uncertainty
  in chest radiograph interpretation. Using BERT with Monte Carlo Dropout and Deep
  Ensembles, the research evaluates different binarisation methods for uncertainty
  labels and investigates how well predictive uncertainty aligns with disagreements
  between rule-based labellers.
---

# Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty

## Quick Facts
- arXiv ID: 2505.03910
- Source URL: https://arxiv.org/abs/2505.03910
- Reference count: 0
- Primary result: Bayesian Deep Learning approximations (MC Dropout, Deep Ensembles) provide uncertainty estimates that modestly correlate with linguistic uncertainty in chest radiograph interpretation

## Executive Summary
This study investigates the relationship between predictive uncertainty from Bayesian Deep Learning and human/linguistic uncertainty in chest radiograph interpretation. Using BERT with Monte Carlo Dropout and Deep Ensembles, the research evaluates different binarisation methods for uncertainty labels and examines how well predictive uncertainty aligns with disagreements between rule-based labellers. Results show good model performance but modest correlation between predictive and linguistic uncertainty, with Monte Carlo Dropout capturing uncertainty better than Deep Ensembles. While Bayesian approximations provide valuable uncertainty estimates, further refinement is needed to fully capture human interpretation nuances in clinical applications.

## Method Summary
The study fine-tunes BERT on chest radiograph reports to classify Edema presence, using U-Random binarization to preserve uncertainty signals during training. Uncertainty is estimated using MC Dropout and Deep Ensembles, generating 10 predictions per study. Predictive Standard Deviation (PSD) captures epistemic uncertainty while Predictive Entropy (PE) captures total uncertainty. The correlation between these uncertainty measures and linguistic uncertainty (True Label Disagreement, CheXpert uncertain, NegBio uncertain) is evaluated using point-biserial correlation. Experiments are conducted on MIMIC-CXR-JPG dataset with 65,833 reports.

## Key Results
- U-Random binarization preserves uncertainty signals better than U-Ones/U-Zeros (Rpb ~0.35-0.85 vs ~0.02-0.16)
- Monte Carlo Dropout captures epistemic uncertainty more effectively than Deep Ensembles (Rpb ~0.35 vs ~0.16)
- Model achieves ~90% accuracy and ~88% F1 with U-Random, trading performance for uncertainty preservation
- Predictive Entropy correlates strongly with explicit uncertainty markers in rule-based labellers (~72-85% variance explained)

## Why This Works (Mechanism)

### Mechanism 1
Monte Carlo Dropout captures epistemic uncertainty more effectively than Deep Ensembles when correlating with linguistic disagreement. By applying dropout at both training and inference, MC Dropout generates multiple stochastic predictions per input from a single model. The standard deviation across these predictions (Predictive Standard Deviation, PSD) estimates epistemic uncertainty—model uncertainty that could theoretically be reduced with more data. This within-model variance better reflects the disagreement patterns between rule-based labellers than training multiple independent models.

### Mechanism 2
Predictive Entropy (PE) strongly correlates with explicit uncertainty markers in rule-based labellers, capturing total uncertainty across both aleatoric and epistemic components. PE aggregates multiple predictions through Shannon entropy: H(p̂) = -p̂ log(p̂) - (1-p̂)log(1-p̂). For binary classification, PE peaks at p̂=0.5 (decision boundary) and minimizes at confident predictions (p̂→0 or p̂→1). Since rule-based labellers like CheXpert explicitly flag uncertainty through lexical markers ("possibly", "cannot be ruled out"), cases labelled "uncertain" should exhibit higher entropy in model predictions.

### Mechanism 3
U-Random binarization (randomly mapping uncertain labels 50-50 to positive/negative) preserves uncertainty signals better than deterministic mapping strategies. U-Ones (uncertain→positive) and U-Zeros (uncertain→negative) systematically bias the model to treat uncertain cases as definitively one class, suppressing the uncertainty signal during training. U-Random introduces noise that prevents the model from learning a deterministic mapping for uncertain cases, forcing it to retain higher entropy for these inputs at inference.

## Foundational Learning

- **Epistemic vs. Aleatoric Uncertainty Decomposition**
  - Why needed here: The paper explicitly decomposes predictive uncertainty into epistemic (model/parameter uncertainty, reducible with data) and aleatoric (data noise, irreducible). PSD captures epistemic; PE captures total uncertainty. Without this distinction, you cannot interpret why MC Dropout outperforms Deep Ensembles on epistemic measures but they perform similarly on total uncertainty.
  - Quick check question: If you add more training data and a model's uncertainty decreases, was that epistemic or aleatoric uncertainty?

- **Point-Biserial Correlation Coefficient**
  - Why needed here: This is the paper's primary metric for evaluating alignment between continuous uncertainty measures (PE, PSD) and binary linguistic uncertainty labels (TLD, Chex Uncertain). Understanding that Rpb² represents proportion of variance explained is essential for interpreting the 72% variance explained claim.
  - Quick check question: What does Rpb = 0.35 tell you about the relationship between predictive standard deviation and true-label disagreement?

- **Bayesian Deep Learning Approximations**
  - Why needed here: True Bayesian inference over neural network weights is intractable. MC Dropout and Deep Ensembles are approximations with different computational profiles and theoretical guarantees. MC Dropout is cheaper (single model, multiple passes) while Deep Ensembles require training multiple models but may capture more diverse predictions.
  - Quick check question: Why might generating only 10 predictions per study (due to computational constraints) limit uncertainty estimation quality?

## Architecture Onboarding

- **Component map:**
```
Raw radiology report → Text preprocessing (NLTK/spaCy: stopword removal, lemmatization)
                     → BERT tokenizer (max_length=512)
                     → BERT fine-tuning on U-Random binarized labels
                     → Inference with MC Dropout (dropout_rate applied) OR Deep Ensembles (N models)
                     → Aggregate N predictions: mean → probability; std → PSD; entropy → PE
                     → Point-biserial correlation vs. linguistic uncertainty (TLD, CheXpert uncertain)
```

- **Critical path:**
  1. Label binarization strategy (U-Random vs U-Ones/U-Zeros) — determines whether uncertainty signal is preserved
  2. BDL approximation choice (MC Dropout vs Deep Ensembles) — determines epistemic uncertainty quality
  3. Number of predictions per study — determines reliability of variance/entropy estimates

- **Design tradeoffs:**
  - Classification performance vs uncertainty preservation: U-Ones/U-Zeros achieve ~97-98% F1 but near-zero uncertainty correlation; U-Random achieves ~88-89% F1 with meaningful correlation (Rpb 0.35-0.85 depending on metric)
  - Computational cost vs ensemble diversity: Deep Ensembles require training N independent models; MC Dropout uses single model with stochastic inference
  - Decision threshold: Lowering from 0.5 could increase recall (critical for medical screening) but would decrease precision

- **Failure signatures:**
  - Near-zero or negative point-biserial correlations (as seen with U-Ones/U-Zeros on NegBio uncertain labels) indicates uncertainty signal has been suppressed during training
  - Deep Ensembles showing much higher PSD than MC Dropout but lower correlation with linguistic uncertainty suggests ensemble diversity doesn't align with meaningful uncertainty
  - Model highly confident (p̂→0 or 1) on cases where labellers disagree suggests overfitting to spurious patterns

- **First 3 experiments:**
  1. Reproduce U-Random vs U-Ones/U-Zeros comparison on a different outcome (e.g., Cardiomegaly or Pleural Effusion) to validate whether the uncertainty-preservation effect generalizes beyond Edema
  2. Increase predictions per study from 10 to 50-100 for both MC Dropout and Deep Ensembles to test whether uncertainty correlation improves with more reliable variance estimates
  3. Vary the U-Random split ratio (e.g., 30-70, 70-30 instead of 50-50) to understand whether there's an optimal noise injection level that balances classification and uncertainty performance

## Open Questions the Paper Calls Out

### Open Question 1
Does increasing the number of predictions per study beyond 10 improve MC Dropout and Deep Ensembles' alignment with linguistic uncertainty? The study was limited to 10 predictions due to computational constraints; the optimal number for this clinical NLP task remains unknown.

### Open Question 2
Can Self-Training or 3-Class Classification approaches capture linguistic uncertainty better than U-Random binarisation? Only three binarisation methods were tested; U-Random yielded modest correlations (77–85% for predictive uncertainty, substantially lower for epistemic uncertainty).

### Open Question 3
How does the alignment between predictive and human uncertainty change when using actual multi-radiologist annotations instead of rule-based labeller disagreement as a proxy? Rule-based labellers only identify explicit uncertainty (missing implicit uncertainty in causality and medical opinion) and produce systematic errors.

## Limitations
- Modest correlation between predictive and linguistic uncertainty (Rpb ~0.35) suggests current Bayesian approximations may not fully capture human interpretation nuances
- U-Random binarization significantly reduces classification performance (~88% F1 vs ~98% F1 for U-Ones/U-Zeros), creating a fundamental trade-off
- Rule-based labellers only identify explicit uncertainty, missing implicit uncertainty in causality and medical opinion

## Confidence
- **High Confidence:** Model performance metrics (accuracy ~90%, F1 ~88%) and the superiority of U-Random binarization over deterministic alternatives
- **Medium Confidence:** The correlation between predictive uncertainty and linguistic disagreement (Rpb ~0.35), as this depends on the quality of rule-based uncertainty labels
- **Low Confidence:** The claim that MC Dropout captures epistemic uncertainty better than Deep Ensembles, as this requires careful interpretation of variance estimates

## Next Checks
1. Cross-Disease Validation: Reproduce the U-Random vs U-Ones/U-Zeros comparison on a different radiographic finding (e.g., Cardiomegaly) to determine whether uncertainty preservation generalizes beyond Edema
2. Prediction Quantity Sensitivity: Increase predictions per study from 10 to 50-100 for both MC Dropout and Deep Ensembles to test whether uncertainty correlation improves with more reliable variance estimates
3. Uncertainty Signal Robustness: Vary the U-Random split ratio (e.g., 30-70, 70-30) to identify whether there's an optimal noise injection level that balances classification performance with uncertainty preservation