---
ver: rpa2
title: 'WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data'
arxiv_id: '2509.01379'
source_url: https://arxiv.org/abs/2509.01379
tags:
- hate
- speech
- agent
- https
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents WATCHED, an AI agent system designed to assist
  human moderators in detecting and explaining hate speech on social media. The system
  combines a BERT-based classifier, a retrieval-augmented generation (RAG) module,
  Urban Dictionary lookups, a reasoning model, and platform guidelines to provide
  explainable classifications.
---

# WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data

## Quick Facts
- arXiv ID: 2509.01379
- Source URL: https://arxiv.org/abs/2509.01379
- Reference count: 40
- Primary result: AI agent system achieves macro F1 0.91 on hate speech detection

## Executive Summary
This paper presents WATCHED, an AI agent system designed to assist human moderators in detecting and explaining hate speech on social media. The system combines a BERT-based classifier, a retrieval-augmented generation (RAG) module, Urban Dictionary lookups, a reasoning model, and platform guidelines to provide explainable classifications. It also includes a human-in-the-loop feedback loop for continuous data expansion. Experiments on the MetaHate dataset show that WATCHED outperforms state-of-the-art baselines with a macro F1 score of 0.91. An ablation study confirms that the classifier, similar posts, and reasoning components are the most impactful for performance. The system balances speed and depth, enabling moderators to understand and act on hate speech more effectively.

## Method Summary
The WATCHED system uses an AI agent (qwen2.5:7b-instruct-q4_K_M) that orchestrates multiple specialized tools to classify hate speech. The core components include MetaHateBERT for fast classification, Agentic RAG over a Qdrant vector database containing 1.16M posts for contextual grounding, Urban Dictionary lookups for slang interpretation, and Llama-3-8B-Distil-MetaHate for chain-of-thought reasoning. The system ingests the MetaHate train split into the RAG database and evaluates on a reannotated 2001-instance subsample. Human feedback updates the RAG database for continuous learning.

## Key Results
- WATCHED achieves a macro F1 score of 0.9139, surpassing state-of-the-art baselines
- Ablation study shows classifier removal causes the largest performance drop (F1 to 0.8053)
- Human-in-the-loop feedback enables continuous data expansion and adaptability to new linguistic trends

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool orchestration by an AI agent outperforms standalone classifiers.
- Mechanism: The agent dynamically selects and invokes specialized tools based on input, synthesizing multiple evidence sources rather than relying on a single classification signal.
- Core assumption: Different hate speech instances require different analytical approaches; no single model captures all linguistic patterns.
- Evidence anchors:
  - [abstract] "surpassing existing state-of-the-art methods, reaching a macro F1 score of 0.91"
  - [Section 2.3.5] Full system F1 MACRO: 0.9139 vs. no tools: 0.8053 (10.8 point drop)
  - [corpus] Related work on LLM-C3MOD shows human-LLM collaboration improves cross-cultural moderation, supporting multi-component approaches.
- Break condition: If tool invocation overhead exceeds latency requirements for real-time moderation, the tradeoff shifts toward simpler pipelines.

### Mechanism 2
- Claim: The BERT classifier provides the strongest single signal; contextual retrieval provides secondary grounding.
- Mechanism: MetaHateBERT delivers fast, domain-specific classification; RAG retrieval of similar posts grounds decisions in precedent, reducing over-prediction on non-standard dialects.
- Core assumption: Hate speech patterns repeat across posts; similar historical examples improve classification confidence.
- Evidence anchors:
  - [Section 2.3.5] Removing classifier drops F1 MACRO to 0.8571 (largest single-tool ablation impact); removing Similar Posts drops to 0.8733
  - [Section 2.2.1] RAG database contains 1,164,586 examples; retrieves top 5 similar posts with labels
  - [corpus] Davidson et al. (2017) and Sap et al. (2019) document racial bias in hate speech detection—contextual grounding helps mitigate this.
- Break condition: If retrieval surface semantically similar but label-inconsistent examples, agent confusion increases rather than decreases.

### Mechanism 3
- Claim: Slang lookup and structured reasoning reduce false positives on informal language.
- Mechanism: Urban Dictionary definitions inject external knowledge for evolving slang; the reasoning model (Distil MetaHate) generates chain-of-thought justifications, making decisions interpretable and consistent.
- Core assumption: LLMs lack up-to-date slang knowledge; explicit definition injection improves calibration on informal text.
- Evidence anchors:
  - [Section 2.2.1] "models often over-predict hate speech when exposed to non-standard dialects, slang, or AAVE"
  - [Section 2.3.5] Urban Dictionary ablation: F1 MACRO drops to 0.8753; used in 69.6% of queries
  - [corpus] Evidence weak—no direct corpus comparison of slang-augmented vs. baseline systems found.
- Break condition: If Urban Dictionary definitions are noisy or offensive themselves, injection may degrade rather than improve reasoning.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core to the "Similar Posts" tool; agent retrieves labeled examples from Qdrant vector DB to ground decisions.
  - Quick check question: Can you explain why vector similarity search alone might retrieve semantically similar but label-inconsistent examples?

- Concept: AI Agent Tool Calling
  - Why needed here: The system uses pydantic-ai framework; agent decides which tools to invoke per query rather than running fixed pipeline.
  - Quick check question: What is the difference between a fixed pipeline and an agentic system in terms of tool selection?

- Concept: Model Distillation for Specialized Tasks
  - Why needed here: The reasoning tool uses "Llama-3-8B-Distil-MetaHate," a distilled model fine-tuned for hate speech reasoning.
  - Quick check question: Why would a distilled 8B model outperform a general 70B teacher on a specific task?

## Architecture Onboarding

- Component map:
  Agent LLM (qwen2.5:7b-instruct-q4_K_M) -> Orchestrates tool calls
  MetaHateBERT classifier -> Binary hate/non-hate classification (<0.1s)
  RAG Tool (Qdrant + jina-embeddings-v3) -> 1.16M posts, top-5 retrieval (~0.25s)
  Urban Dictionary API -> Slang term definitions (~1s)
  Llama-3-8B-Distil-MetaHate -> Chain-of-thought reasoning (~30s)
  Guidelines DB -> Platform policy documents for explanation grounding

- Critical path:
  1. User submits post → Agent receives input
  2. Agent invokes classifier (95.3% of queries), RAG (69.5%), Urban Dictionary (69.6%) in parallel where possible
  3. Agent optionally calls reasoning model (78.2%) for deeper analysis
  4. Agent synthesizes evidence → outputs label, confidence, explanation
  5. Guidelines retrieved to align explanation with platform policy
  6. User feedback (correct/incorrect) → RAG DB updated with new labeled example

- Design tradeoffs:
  - **Speed vs. depth**: Reasoning tool adds ~30s; used selectively. Classifier is fastest but less interpretable alone.
  - **Coverage vs. noise**: Urban Dictionary provides evolving slang but definitions may be unreliable or offensive.
  - **Static vs. adaptive**: Human feedback enables data expansion but introduces labeling noise if users err.

- Failure signatures:
  - High latency (>40s total): Likely over-reliance on reasoning tool; check tool invocation patterns.
  - Low confidence + conflicting tool outputs: RAG returning inconsistent labels; may need similarity threshold tuning.
  - Slang-heavy false positives: Urban Dictionary not invoked or definitions not incorporated into reasoning prompt.

- First 3 experiments:
  1. **Ablation validation**: Reproduce the ablation study (Table 3) to confirm tool contribution rankings on a held-out sample.
  2. **Latency profiling**: Measure end-to-end latency per query; identify if reasoning tool calls dominate and test early-exit heuristics (e.g., skip reasoning if classifier confidence >0.95).
  3. **Slang edge cases**: Construct a test set of AAVE and slang-heavy posts; compare performance with/without Urban Dictionary tool to quantify false-positive reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system's performance and tool-calling behavior change when utilizing different Large Language Models as the central agent?
- Basis in paper: [explicit] The authors state that "A comparative evaluation of alternative agent LLMs is considered out of scope, as our aim is to showcase the integration... rather than benchmark agent models."
- Why unresolved: The study restricts its scope to a single model (qwen2.5:7b-instruct-q4_K_M), leaving the sensitivity of the architecture to the agent's reasoning capabilities unknown.
- What evidence: Benchmarking results comparing the current setup against agents powered by larger or different model families (e.g., GPT-4, Llama 3 70B) on the same classification task.

### Open Question 2
- Question: Can the 30-second average latency of the reasoning tool be reduced to support real-time moderation without degrading classification accuracy?
- Basis in paper: [inferred] While the abstract emphasizes "speed and scale," the results section reveals the reasoning tool requires an average of 30 seconds per query, contrasting with sub-second times for other tools.
- Why unresolved: The paper confirms the "strategic balance" but does not explore optimization methods for the computationally intensive reasoning component required for live deployment.
- What evidence: Performance metrics from an optimized or distilled reasoning module demonstrating sub-second or near-real-time response times while maintaining a Macro F1 score comparable to 0.91.

### Open Question 3
- Question: How effectively does the human-in-the-loop data expansion mechanism mitigate concept drift and improve detection of evolving slang over time?
- Basis in paper: [explicit] The authors state the tool "supports studies on... AI adaptability to new linguistic and cultural trends" via the feedback loop, but the evaluation uses a static dataset.
- Why unresolved: The experimental validation relies on a fixed test split (reannotated eval subset) and does not measure the longitudinal impact of the data augmentation feature.
- What evidence: A temporal analysis tracking classification performance on newly emerged hate speech trends as the vector database is continuously updated with human-verified labels.

## Limitations

- The system relies on Urban Dictionary, which may provide unreliable or offensive definitions
- The human-in-the-loop feedback mechanism's quality and potential bias are not evaluated
- The agent's system prompt and reasoning model details are not fully specified in the paper

## Confidence

- **High**: Overall system performance and ablation study results are well-supported
- **Medium**: Urban Dictionary's contribution to reducing slang false positives relies on single data point
- **Low**: Long-term efficacy of human feedback mechanism not evaluated

## Next Checks

1. **Ablation Validation**: Reproduce the ablation study on a held-out sample to confirm the reported contribution rankings and magnitudes for each tool.

2. **Slang Edge Case Analysis**: Construct and test a targeted set of AAVE and slang-heavy posts to measure the actual false-positive reduction attributable to the Urban Dictionary tool.

3. **Human Feedback Quality Audit**: Design a small-scale experiment where independent annotators review a sample of user feedback updates to the RAG database, assessing for consistency, bias, and error propagation.