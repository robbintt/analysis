---
ver: rpa2
title: Towards a Japanese Full-duplex Spoken Dialogue System
arxiv_id: '2506.02979'
source_url: https://arxiv.org/abs/2506.02979
tags:
- dialogue
- japanese
- spoken
- speech
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents J-Moshi, the first publicly available full-duplex
  spoken dialogue model for Japanese. Built on Moshi's architecture, it was adapted
  to Japanese through pre-training on 69,000 hours of J-CHAT data followed by fine-tuning
  on 344 hours of stereo dialogue data.
---

# Towards a Japanese Full-duplex Spoken Dialogue System

## Quick Facts
- arXiv ID: 2506.02979
- Source URL: https://arxiv.org/abs/2506.02979
- Reference count: 0
- First publicly available full-duplex spoken dialogue model for Japanese

## Executive Summary
This paper presents J-Moshi, the first publicly available full-duplex spoken dialogue model for Japanese, adapted from Moshi's architecture through extensive pre-training and fine-tuning. The model demonstrates Japanese-specific turn-taking behaviors with higher speech overlap rates compared to English Moshi. Human evaluations show J-Moshi achieves 2.67/5 naturalness and 2.19/5 meaningfulness scores, outperforming the baseline dGSLM.

## Method Summary
J-Moshi was developed through a two-stage adaptation process: pre-training on 69,000 hours of J-CHAT data followed by fine-tuning on 344 hours of stereo dialogue data. The model uses a Japanese SentencePiece tokenizer and employs multi-stream TTS to generate 602 hours of synthetic dialogue data for augmentation. The RQ-Transformer architecture processes parallel user and system speech token streams to enable full-duplex capabilities including overlaps and backchannels.

## Key Results
- J-Moshi achieved 2.67/5 naturalness and 2.19/5 meaningfulness scores, outperforming dGSLM baseline (2.44/5 and 1.76/5)
- Generated 53.2 seconds of speech overlap per minute compared to 35.1 seconds for English Moshi
- J-Moshi-ext further improved meaningfulness to 2.30/5 with synthetic dialogue augmentation

## Why This Works (Mechanism)

### Mechanism 1: Parallel Token Stream Modeling for Full-Duplex Dialogue
Simultaneous bidirectional features (overlaps, backchannels) emerge from modeling user and system speech as parallel audio token streams processed at 12.5Hz. The RQ-Transformer processes 17 interleaved token sequences per timestep, with the Temporal Transformer modeling temporal dependencies and the Depth Transformer autoregressively generating audio tokens. Turn-taking behaviors are learned implicitly from temporal alignment without explicit rules.

### Mechanism 2: Hierarchical Semantic-Acoustic Token Separation
Separating semantic content (layer 1) from acoustic details (layers 2-8) stabilizes generation while preserving linguistic meaning. Mimi's residual vector quantizer produces 8 token layers per frame, with layer 1 trained for semantic information and layers 2-8 capturing acoustic properties. A one-timestep delay on acoustic tokens during training stabilizes output quality.

### Mechanism 3: Two-Stage Language Adaptation with Synthetic Augmentation
Large-scale monophonic pre-training establishes foundational speech patterns; stereo fine-tuning plus synthetic dialogue improves meaningfulness and naturalness. Stage 1 pre-trains on 69K hours J-CHAT data split into pseudo-stereo. Stage 2 fine-tunes on 344 hours genuine stereo dialogue plus 602 hours synthetic speech generated by multi-stream TTS with LLM-rewritten text dialogues.

## Foundational Learning

- **Full-duplex vs. Half-duplex Communication**: Understanding the distinction between simultaneous vs. alternating communication is prerequisite to grasping why parallel token streams matter. *Quick check*: Can you explain why a half-duplex system cannot produce backchannels while the user is still speaking?

- **Residual Vector Quantization (RVQ)**: Mimi encodes speech into 8 hierarchical token layers; understanding RVQ explains why semantic and acoustic information separate across layers. *Quick check*: If layer 1 captures semantics and layers 2-8 capture acoustics, what happens if you decode using only layer 1 tokens?

- **Autoregressive Modeling with Temporal Delays**: The paper applies explicit delays (1 timestep acoustic, 25-27 timesteps for TTS) to stabilize generation; understanding why delays help is critical. *Quick check*: Why might generating acoustic tokens without delay cause quality degradation?

## Architecture Onboarding

- **Component map**: User speech waveform → Mimi encoder → 8 user audio tokens per frame → Temporal Transformer (context) → Depth Transformer (per-frame generation) → 8 Moshi audio tokens → Mimi decoder → output speech waveform

- **Critical path**: The RQ-Transformer processes 17 interleaved token sequences per timestep—1 text token layer (inner monologue), 8 Moshi audio token layers, and 8 user audio token layers. The Temporal Transformer (7B LLM backbone) models temporal dependencies across timesteps, while the smaller Depth Transformer autoregressively generates the 16 audio tokens at each timestep.

- **Design tradeoffs**:
  - Japanese SentencePiece tokenizer requires random initialization of text embeddings, losing transfer from English pre-training
  - PAD token weighting: 88% of text tokens are PAD in Japanese (vs. 65% English) due to kanji density; loss weighting (50% reduction) prevents PAD dominance but may underweight silence modeling
  - Synthetic data WER: 24.6% WER in synthesized speech suggests quality-noise tradeoff; filtering by lowest WER mitigates but doesn't eliminate artifacts

- **Failure signatures**:
  - High perplexity (>300): Indicates fluency failure; check temperature setting (lower τ improves fluency)
  - Low meaningfulness (<2.0): Suggests semantic incoherence; may indicate tokenizer mismatch or insufficient pre-training
  - Abnormal overlap statistics: If overlap << ground truth, stereo fine-tuning may be insufficient; if >> ground truth, delay parameters may be wrong
  - Mimi resynthesis degradation: If >0.5 point drop from ground truth, codec may need Japanese adaptation

- **First 3 experiments**:
  1. **Mimi Japanese validation**: Resynthesize Japanese speech samples; compute WER and MOS degradation vs. original to confirm codec transferability (paper shows ~0.5 point drop)
  2. **Ablation of synthetic augmentation**: Train J-Moshi without 602 hours TTS data; compare meaningfulness scores to isolate augmentation contribution (paper shows +0.11 improvement)
  3. **Temperature sweep**: Generate samples at τ ∈ {0.8, 0.9, 1.0}; measure perplexity and human ratings to find optimal fluency-quality tradeoff (paper shows τ=0.8 optimal)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Mimi neural audio codec be specifically adapted to minimize the degradation of naturalness observed in Japanese speech re-synthesis?
- Basis: The conclusion states "Mimi will also need to be adapted to Japanese in the future," noting a 0.5-point drop in naturalness scores during the re-synthesis of ground-truth audio.
- Why unresolved: The current study freezes Mimi's parameters (trained primarily on English) to leverage its generalization capability, but this prevents the capture of Japanese-specific acoustic features.
- Evidence: Comparative evaluation of the current frozen Mimi against a version fine-tuned on Japanese speech data, measuring re-synthesis naturalness and MOS scores.

### Open Question 2
- Question: What modifications to loss weighting are required to optimize the RQ-Transformer for the high density of PAD tokens in Japanese text alignment?
- Basis: Section 5 notes that the PAD token ratio is 88% in Japanese data (vs. 65% in English) due to Kanji density, suggesting a need for "learning settings... such as adjusting the loss weights."
- Why unresolved: The current model inherits loss weighting strategies designed for English, which may cause the model to under-utilize the sparse but information-dense text tokens.
- Evidence: Ablation studies varying the penalty for PAD token loss and the ratio between semantic and acoustic token losses, tracking changes in perplexity and human evaluation scores.

### Open Question 3
- Question: Does the high Word Error Rate (24.6%) in the synthetic dialogue data impose a ceiling on the model's conversational meaningfulness?
- Basis: While J-Moshi-ext showed improved meaningfulness, the authors note the synthetic data has a 24.6% WER; the impact of this noise on the upper bound of performance is not isolated.
- Why unresolved: It is unclear if the model improves despite the noise or if the noise limits the model from achieving higher meaningfulness scores (currently 2.30/5).
- Evidence: Training models on synthetic data filtered for lower WER (e.g., <10%) and comparing the resulting meaningfulness scores against the current J-Moshi-ext baseline.

## Limitations
- The 344-hour fine-tuning corpus includes 58% in-house data (Casual Dialogue 148h, Consultation 53h) that cannot be accessed for replication, limiting external validation
- Human evaluation focused on naturalness and meaningfulness but didn't assess turn-taking appropriateness or task completion—critical dimensions for dialogue systems
- Claims about semantic-acoustic token separation mechanism lack direct corpus validation—the assumption that layer 1 captures semantics while layers 2-8 capture acoustics isn't empirically tested in Japanese

## Confidence
- **High confidence**: The core architectural adaptation (replacing English tokenizer with Japanese SentencePiece, pre-training on J-CHAT, fine-tuning on stereo dialogue) is well-documented and reproducible. The observed improvements over dGSLM baseline (2.67→2.44 naturalness, 2.19→1.76 meaningfulness) are statistically meaningful.
- **Medium confidence**: The contribution of synthetic data augmentation (+0.11 meaningfulness) is documented but the specific rewriting prompt and multi-stream TTS parameters remain unspecified. The PAD token handling strategy (50% loss weighting) appears empirically justified but may not generalize.
- **Low confidence**: Claims about semantic-acoustic token separation mechanism lack direct corpus validation—the assumption that layer 1 captures semantics while layers 2-8 capture acoustics isn't empirically tested in Japanese.

## Next Checks
1. **Codec transfer validation**: Resynthesize 100 Japanese speech samples using ground truth audio and measure WER/MOS degradation. Compare results to English Moshi baseline to confirm Mimi's semantic layer functions equivalently across languages. Target: <0.5 MOS drop from ground truth.

2. **Synthetic data contribution isolation**: Train two J-Moshi variants—one with only 344h stereo data, one with 344h + 602h synthetic. Measure meaningfulness difference and analyze overlap timing patterns. Target: Confirm >0.1 point meaningfulness improvement from augmentation.

3. **PAD token stability analysis**: Generate samples with varying PAD loss weights (0%, 50%, 100%) and measure gradient norms, perplexity, and meaningfulness. Target: Identify optimal weighting that maintains stability without underrepresenting silence/speech boundaries.