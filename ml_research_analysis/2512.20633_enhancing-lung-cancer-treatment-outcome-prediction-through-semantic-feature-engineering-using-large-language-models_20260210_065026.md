---
ver: rpa2
title: Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature
  Engineering Using Large Language Models
arxiv_id: '2512.20633'
source_url: https://arxiv.org/abs/2512.20633
tags:
- data
- clinical
- cancer
- were
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of predicting lung cancer treatment
  outcomes using sparse, heterogeneous clinical data. The authors introduce a framework
  using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to
  convert laboratory, genomic, and medication data into high-fidelity, task-aligned
  features.
---

# Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models

## Quick Facts
- arXiv ID: 2512.20633
- Source URL: https://arxiv.org/abs/2512.20633
- Reference count: 0
- Primary result: GKC framework achieved mean AUC-ROC of 0.803, significantly outperforming expert-engineered features (0.619) and generic embeddings (0.678)

## Executive Summary
This study introduces a novel framework for predicting lung cancer treatment outcomes by using Large Language Models as Goal-oriented Knowledge Curators (GKC) to transform sparse, heterogeneous clinical data into high-fidelity semantic features. The approach converts laboratory, genomic, and medication data into structured summaries via LLM-driven synthesis, then embeds these for downstream classification. Tested on a lung cancer cohort (N=184), GKC achieved superior predictive performance (AUC-ROC 0.803) compared to both expert-engineered features and generic embedding baselines. The framework is designed as an offline preprocessing step, making it practical for integration into clinical informatics pipelines.

## Method Summary
The framework operates by enriching raw clinical data (lab tests, genomic mutations, medications) with knowledge from external databases (DrugBank, KEGG, GO), then generating structured JSON summaries via modality-specific LLM prompts (Gemini 2.0 Flash). These summaries are embedded using Google's text-embedding-005 and fed to an XGBoost classifier. The process is deterministic and offline, enabling reproducible and interpretable predictions. Modality-specific prompts force orthogonal signal extraction, and ablation studies confirmed the complementary value of combining all three data streams.

## Key Results
- GKC achieved mean AUC-ROC of 0.803 (95% CI: 0.799-0.807), significantly outperforming expert-engineered features (0.619), contextual text embeddings (0.678), and end-to-end transformer baseline (0.675).
- Ablation study confirmed complementary value of combining all three modalities (Lab, Gene, Med).
- SHAP analysis showed balanced contribution across modalities (Medication 33.2%, Gene 33.9%, Lab 32.9%).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-aligned summarization outperforms generic embeddings because explicit knowledge curation preserves clinically salient relationships that self-attention alone misses.
- Core assumption: LLM's parametric knowledge correctly maps clinical relationships; hallucinations would propagate as noise.
- Evidence: GKC achieved AUC-ROC of 0.803 vs. contextual text embeddings at 0.678; SHAP attribution shows balanced modality contributions.

### Mechanism 2
- Claim: Modality-specific prompts force orthogonal signal extraction—each stream captures distinct prognostic dimensions (trajectory, blueprint, state).
- Core assumption: Prognostic signal is distributed across modalities rather than concentrated in one.
- Evidence: Ablation study confirmed complementary value; SHAP analysis shows no dominant modality.

### Mechanism 3
- Claim: Deterministic, offline summarization enables deployment in clinical workflows where end-to-end fine-tuning is impractical.
- Core assumption: Target population's feature distribution remains stable.
- Evidence: Framework operates as offline preprocessing; reported cost ~$0.001/patient, latency ~1.6 seconds.

## Foundational Learning

- **Concept: Task-aligned vs. generic embeddings**
  - Why needed: Central claim that embeddings optimized for prediction objective outperform generic biomedical embeddings.
  - Quick check: Can you explain why concatenating DrugBank descriptions underperforms LLM-generated summaries that explicitly reference "therapy implications"?

- **Concept: SHAP-based modality attribution**
  - Why needed: Validates model isn't relying on single data source; enables per-patient interpretability.
  - Quick check: If SHAP showed Gene contributing 80% of prediction signal, what would that imply about value of medication and lab summarization steps?

- **Concept: Small-sample regime overfitting**
  - Why needed: N=184 with high-dimensional embeddings; tree-based ensembles selected for built-in regularization.
  - Quick check: Why did Fully Connected Neural Networks show higher variance than XGBoost in this setting?

## Architecture Onboarding

- **Component map:** Raw data → Knowledge base enrichment → LLM summarization → Embedding → Classifier
- **Critical path:** Raw data → Knowledge base enrichment → LLM summarization (this is where predictive gain originates) → Embedding → Classifier
- **Design tradeoffs:** Proprietary LLM API vs. open-source; deterministic decoding vs. diversity; modality separation vs. early fusion
- **Failure signatures:** LLM outputs empty/malformed JSON; SHAP attribution collapses to single modality; train/validation AUC gap > 0.1
- **First 3 experiments:**
  1. Run ENF model (numeric features only) on your data to confirm AUC ~0.62
  2. Remove persona-based role-playing from prompts; measure AUC drop
  3. Swap text-embedding-005 for BioBERT; if AUC drops, embedding choice—not summarization—may drive gains

## Open Questions the Paper Calls Out

- **External validation:** Can the framework maintain predictive superiority when validated on larger, multi-institutional cohorts with heterogeneous EHR systems?
- **Unstructured data integration:** Does integration of unstructured clinical narratives or imaging data provide complementary predictive signal to current tri-modal approach?
- **Open-source reproducibility:** Can open-source LLMs effectively replace proprietary models as Knowledge Curators without compromising semantic representation quality?

## Limitations
- Study limited to single institution (N=184); external validation needed
- Proprietary LLM API usage raises reproducibility concerns
- Framework may not adapt well to changing clinical protocols or new biomarker discoveries

## Confidence
- High Confidence: Complementary value of multimodal integration supported by SHAP analysis and ablation study
- Medium Confidence: Deployment architecture advantage plausible but lacks direct clinical workflow validation
- Low Confidence: Superiority of task-aligned summarization could be influenced by specific embedding model choice

## Next Checks
1. Apply framework to temporal split of data to verify performance as clinical practices evolve
2. Implement automated checks for clinical consistency in LLM-generated summaries to detect hallucinations
3. Validate framework on independent lung cancer cohort from different hospital system to assess generalization