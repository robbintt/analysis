---
ver: rpa2
title: Revisiting Active Learning under (Human) Label Variation
arxiv_id: '2507.02593'
source_url: https://arxiv.org/abs/2507.02593
tags:
- label
- learning
- annotation
- variation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper examines how label variation\u2014especially human label\
  \ variation (HLV)\u2014affects active learning (AL). Traditional AL assumes a single\
  \ ground truth and a noiseless oracle, but these assumptions often fail in subjective\
  \ NLP tasks where HLV is present."
---

# Revisiting Active Learning under (Human) Label Variation)

## Quick Facts
- **arXiv ID:** 2507.02593
- **Source URL:** https://arxiv.org/abs/2507.02593
- **Reference count:** 23
- **Primary result:** The paper proposes decomposing label variation into signal (HLV) and noise, and adapting active learning for subjective NLP tasks with distributional loss and annotator selection.

## Executive Summary
This paper challenges the traditional assumptions of active learning (AL) by highlighting the prevalence of human label variation (HLV) in subjective NLP tasks. It argues that observed label disagreement often contains valid signal rather than pure noise, and proposes four key adaptations to the AL loop: using distributional loss measures instead of hard-label ones, modifying acquisition functions to handle multiple annotators and HLV, adding annotator selection strategies that can involve both humans and LLMs, and decomposing label variation into signal (HLV) and noise (annotation error). The work provides a conceptual foundation for future empirical research in HLV-aware AL, emphasizing the need to treat labels as distributions rather than discrete values.

## Method Summary
The paper is conceptual and does not provide a specific implementation or training procedure. It synthesizes literature from AL and HLV research to identify four key adaptations for HLV-aware AL: (C1) distributional training measures (KL divergence instead of cross-entropy), (C2) modified acquisition functions that consider multiple annotations per instance, (C3) annotator selection function to choose between human and LLM annotators, and (C4) noise vs. signal decomposition in labels. The minimum viable reproduction would require selecting a subjective NLP task with multi-annotator data, implementing distributional labels and loss functions, and modifying acquisition functions to account for HLV.

## Key Results
- Label variation should be decomposed into signal (HLV) and noise (annotation error) rather than treating all disagreement as error
- Distributional loss measures (e.g., KL divergence) are more appropriate than hard-label loss functions for HLV-aware AL
- Annotator selection strategies that can involve both humans and LLMs are needed to optimize budget and capture diverse perspectives
- Three core AL assumptions (single ground truth, noiseless oracle, equal cost) must be revisited for HLV settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing observed label variation into signal (HLV) and noise (annotation error) improves data utility compared to treating all disagreement as error.
- **Mechanism:** By distinguishing "plausible variability due to subjectivity" from "mistakes," the system retains informative differences in perspective that would otherwise be discarded, allowing the model to learn the underlying distribution of valid interpretations.
- **Core assumption:** Observed disagreement contains structured information about the task's subjectivity rather than being purely stochastic error.
- **Evidence anchors:** Abstract and Section 2 discussions of decomposing LV into signal and noise; corpus papers like EVADE and From Noise to Signal reinforce this conceptual shift.
- **Break condition:** If the task is objective with genuinely random noise, this decomposition adds unnecessary complexity.

### Mechanism 2
- **Claim:** Distributional loss measures (e.g., KL divergence) enable models to learn probability distributions of human judgments rather than forcing discrete outputs.
- **Mechanism:** Traditional cross-entropy optimizes for a single class, while KL divergence minimizes divergence between model uncertainty and collective human uncertainty, resulting in better calibration and representation of ambiguity.
- **Core assumption:** The "true" label is a continuous distribution or probability rather than a discrete variable.
- **Evidence anchors:** Abstract mention of distributional loss measures; Section 4.1 discussion of KL divergence and related measures; related corpus work on training with HLV.
- **Break condition:** If annotation budget allows only one label per instance, estimating a reliable distribution is impossible without strong priors.

### Mechanism 3
- **Claim:** Annotator Selection Functions enable strategic sampling of diverse perspectives rather than random annotator assignment.
- **Mechanism:** The system selects specific annotators based on their predicted contribution to diversity or expertise, ensuring the resulting label distribution reflects true variance in the population rather than the bias of a small subset.
- **Core assumption:** Annotators are not interchangeable and possess varying perspectives and levels of expertise.
- **Evidence anchors:** Abstract mention of annotator selection strategies; Section 4.3 discussion of choosing between human and LLM annotators; related corpus work on annotator-level analysis.
- **Break condition:** If annotator identity or characteristics are unavailable, or if selection costs outweigh marginal value.

## Foundational Learning

- **Concept: Label Types (Discrete vs. Probability vs. Distributional)**
  - **Why needed here:** The paper challenges the assumption that labels must be discrete. Understanding the statistical difference between $y=1$ (Binomial) and $y=Beta(\alpha, \beta)$ (Distributional) is required to implement proposed loss functions.
  - **Quick check question:** Can you explain why a "soft label" (Type b) is statistically distinct from a "distributional label" (Type c) in the context of model optimization?

- **Concept: Active Learning (AL) Assumptions**
  - **Why needed here:** The paper revisits AL by attacking three core assumptions (A1: Single Ground Truth, A2: Noiseless Oracle, A3: Equal Cost). You must understand these axioms to understand why proposed modifications are necessary.
  - **Quick check question:** In a standard AL loop, what is the role of the "Acquisition Function," and how does the authors' proposal modify this role?

- **Concept: Signal-Noise Decomposition in Annotation**
  - **Why needed here:** The central thesis relies on separating valid subjectivity (Signal/HLV) from mistakes (Noise). Without this concept, one might erroneously treat all disagreement as model failure or data garbage.
  - **Quick check question:** Does high inter-annotator disagreement always imply low data quality? Why or why not, according to the "Perspectivist" stance?

## Architecture Onboarding

- **Component map:** Unlabeled Pool -> Acquisition Function (Selects Instance) -> Annotator Selection Function (Selects Human/LLM) -> Oracle/Annotator (Provides Label/Distribution) -> Label Aggregator/Decomposer (Separates Signal/Noise) -> Training Loop (Distributional Loss)
- **Critical path:** The implementation hinges on the Training Measure (Loss Function). If the model cannot ingest distributional labels (Type b/c), upstream acquisition and annotator selection strategies cannot leverage the HLV signal.
- **Design tradeoffs:**
  - Annotator Pool Depth vs. Cost: Sourcing multiple annotators per instance to capture HLV increases costs
  - LLM vs. Human: LLMs offer cost efficiency and can output distributions cheaply but may hallucinate biases; humans provide ground-truth subjectivity but typically output discrete labels
- **Failure signatures:**
  - Mode Collapse: Model predicts uniform distributions for everything, failing to distinguish between "ambiguous input" and "model uncertainty"
  - Bias Amplification: Annotator Selection Function repeatedly selects the same "type" of annotator, resulting in a skewed distribution
- **First 3 experiments:**
  1. Baseline Comparison: Run standard AL (hard labels, single oracle) vs. HLV-aware AL (soft labels, multi-annotator) on a subjective dataset to measure performance gaps
  2. Ablation on Loss Functions: Compare models trained with Cross-Entropy vs. KL Divergence loss using the same distributional ground truth
  3. Annotator Strategy Test: Evaluate random annotator selection vs. diversity-maximizing annotator selection to validate if "who annotates" significantly changes the learned distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can acquisition functions be effectively redesigned to distinguish between model uncertainty and inherent human label variation (HLV) when selecting instances for annotation?
- **Basis in paper:** Section 4.2 states that classical informativeness sampling (e.g., entropy) is unsuitable for HLV because high entropy can be an integral property of the data rather than a signal of model uncertainty.
- **Why unresolved:** Standard AL strategies assume a single ground truth and treat high entropy as a reason to query. In HLV settings, querying high-entropy instances may be inefficient if the entropy represents legitimate subjectivity rather than lack of model knowledge.
- **What evidence would resolve it:** Development and empirical validation of novel acquisition functions that successfully down-weight instances with "stable" HLV while prioritizing instances where the model is uncertain relative to the human distribution.

### Open Question 2
- **Question:** What methodologies can effectively decompose observed label variation into annotation noise (error) and signal (plausible HLV) within the active learning loop?
- **Basis in paper:** Section 4.4 notes that "detecting annotation noise in HLV is a complex endeavor" and it remains "non-trivial to distinguish true label variation from noise."
- **Why unresolved:** While the paper argues that observed variation contains both signal and noise, current AL methods typically treat variation as entirely one or the other. Identifying the specific boundary requires validating annotator intent or "acceptable variation."
- **What evidence would resolve it:** A framework or heuristic that can automatically label instances or individual annotations as "error" or "HLV" with high precision, perhaps by utilizing annotator explanations or consistency checks.

### Open Question 3
- **Question:** How should an active learning framework dynamically select between human annotators and Large Language Models (LLMs) to optimize budget and label quality?
- **Basis in paper:** Section 4.3 calls for an "overarching annotator selection strategy" to evaluate whether a language model or a human should provide the label, and the Limitations section explicitly lists the "reliability of 'LLM-as-annotator'" as an open question.
- **Why unresolved:** LLMs offer cost efficiency and can output distributional labels directly, but they suffer from specific biases and a lack of accountability. It is unclear when an LLM's "judgment" is a sufficient proxy for human HLV versus when it introduces systematic noise.
- **What evidence would resolve it:** Comparative studies benchmarking the performance of models trained on AL loops using mixed annotator strategies (Human vs. LLM) against those using single-source annotators.

## Limitations
- This is a conceptual position paper without empirical validation; all proposed mechanisms remain theoretical
- Signal-noise decomposition lacks concrete operationalization for practical implementation
- No specific model architectures, hyperparameters, or training details provided
- Assumes availability of multiple annotators per instance, which may be prohibitively expensive

## Confidence
- **High Confidence:** The need to decompose label variation into signal and noise is well-supported by related work on HLV
- **Medium Confidence:** Distributional loss functions can better capture label uncertainty than hard-label approaches, based on existing literature
- **Low Confidence:** Practical implementation of annotator selection strategies that balance human and LLM perspectives without introducing new biases

## Next Checks
1. Conduct controlled experiments comparing standard AL with HLV-aware AL on a subjective NLP task (e.g., NLI) using multi-annotator datasets
2. Implement and test different distributional loss functions (KL divergence, Jensen-Shannon divergence) against aggregated soft labels
3. Evaluate acquisition functions that explicitly account for predicted inter-annotator disagreement versus traditional uncertainty sampling