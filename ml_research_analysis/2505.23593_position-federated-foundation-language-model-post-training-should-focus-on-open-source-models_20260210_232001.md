---
ver: rpa2
title: 'Position: Federated Foundation Language Model Post-Training Should Focus on
  Open-Source Models'
arxiv_id: '2505.23593'
source_url: https://arxiv.org/abs/2505.23593
tags:
- federated
- post-training
- language
- data
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that federated post-training of foundation language
  models should prioritize open-source models over black-box models. While black-box
  models like GPT-4 have driven recent advances in centralized post-training, their
  use in federated learning contradicts core principles of privacy, autonomy, and
  transparency.
---

# Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models

## Quick Facts
- arXiv ID: 2505.23593
- Source URL: https://arxiv.org/abs/2505.23593
- Reference count: 40
- Primary result: Federated post-training should prioritize open-source models over black-box models to preserve privacy, autonomy, and transparency

## Executive Summary
This paper argues that federated post-training of foundation language models should prioritize open-source models over black-box models. While black-box models like GPT-4 have driven recent advances in centralized post-training, their use in federated learning contradicts core principles of privacy, autonomy, and transparency. The authors analyze model openness across four categories (open-source, open-weight, gray-box, black-box) and demonstrate that only open models enable full control over inference, robust privacy mechanisms, and a complete range of post-training methods. They provide a detailed security and privacy analysis showing that open models allow clients to implement comprehensive defenses against poisoning, membership inference, and gradient inversion attacks, while black-box models force reliance on third-party providers and leak sensitive data through API interactions.

## Method Summary
The paper provides a conceptual analysis of federated foundation language model post-training across four model categories: open-source, open-weight, gray-box, and black-box. It classifies models based on access to weights, architecture, source code, and training data, then maps each category to available post-training methods (FFT, LoRA, adapters, RLHF, prompt tuning) and security/privacy implications. The analysis is structured around four dimensions: access, autonomy, post-training method availability, and security/privacy defenses. While no experimental procedure is specified, the paper provides decision frameworks (Table 1, Table 4, Figure 2) for selecting appropriate model openness levels based on privacy requirements and hardware constraints.

## Key Results
- Black-box models inherently leak sensitive data through API interactions, contradicting federated learning's core privacy guarantee
- Open-source and open-weight models enable full defense implementation (DP, secure aggregation, homomorphic encryption) against FL-specific attacks
- Model openness level deterministically constrains which post-training techniques are available (FFT/RLHF require full weights; prompt tuning is API-only)
- Open models preserve client autonomy by enabling local deployment and control over inference infrastructure

## Why This Works (Mechanism)

### Mechanism 1: Local Deployment Eliminates Third-Party Data Exposure
- Claim: Open-source models preserve FL's core privacy guarantee by enabling fully local training without external API calls.
- Mechanism: Weight availability → local deployment possible → no data transmitted to third-party servers → training data remains entirely on client infrastructure.
- Core assumption: Clients possess sufficient hardware (e.g., ~5-48 GB VRAM for 7B-65B models with QLoRA per Section 2.2) to run models locally.
- Evidence anchors:
  - [abstract] "black-box models force reliance on third-party providers and leak sensitive data through API interactions"
  - [Section 3.1] "Without open weights, clients cannot directly access the foundation model for post-training. Instead, they must communicate with the model through the provider's API. This puts the client's privacy at risk"
  - [corpus] Weak direct support; neighbor papers discuss FL applications but don't validate this specific privacy mechanism.
- Break condition: If clients lack compute resources for local deployment, they must use gray/black-box APIs regardless of privacy concerns.

### Mechanism 2: Weight Access Enables Client-Controlled Defense Implementation
- Claim: Open models allow clients to deploy the full defense stack (DP, secure aggregation, homomorphic encryption, Byzantine aggregation) against FL-specific attacks.
- Mechanism: Access to model internals → clients can apply noise injection, encrypted aggregation, and robust protocols to trainable parameters (full model, LoRA matrices, or adapters) → protection against poisoning, membership inference, and gradient inversion.
- Core assumption: Clients have technical expertise to implement and tune these defenses appropriately.
- Evidence anchors:
  - [abstract] "open models allow clients to implement comprehensive defenses against poisoning, membership inference, and gradient inversion attacks"
  - [Table 4] Shows FFT/LoRA/adapters support all four defense categories; prompt tuning has limited defense applicability.
  - [Section 3.4] "With open-source and open-weight models, there is a larger attack surface...clients are capable to apply and control defense mechanisms."
  - [corpus] No direct corpus validation of defense effectiveness in federated post-training specifically.
- Break condition: Aggressive defenses (especially DP) may degrade model utility to unacceptable levels.

### Mechanism 3: Openness Level Constrains Post-Training Method Space
- Claim: The degree of model openness deterministically limits which post-training techniques are available.
- Mechanism: Full weight access (open-source/open-weight) → all methods feasible (FFT, LoRA, adapters, RLHF, prompt tuning); partial API access (gray-box) → only provider-exposed methods; token-only access (black-box) → limited to textual prompt tuning.
- Core assumption: Method flexibility significantly impacts adaptation quality for diverse downstream tasks.
- Evidence anchors:
  - [Table 1] Clear mapping: open-source/open-weight = ✓ for all methods; gray-box = partial for LoRA/adapters; black-box = ✗ for FFT/RLHF, textual prompt tuning only.
  - [Section 2.1] "Open-source and open-weight models can be post-trained using all possible methods...Gray-box models may provide token-level logits or an adapter interface, but they can only be accessed through an API that limits the fine-tuning options"
  - [corpus] "LLM Post-Training" paper surveys techniques but doesn't directly address federated constraints.
- Break condition: If textual prompt tuning achieves task parity with parametric methods, openness constraints matter less.

## Foundational Learning

- **Federated Learning Fundamentals (FedAvg, cross-silo vs. cross-device)**
  - Why needed here: The entire position rests on whether black-box FL contradicts FL's core principles of privacy, autonomy, and heterogeneity.
  - Quick check question: Can you explain how FedAvg aggregates client model updates without accessing raw training data?

- **Parameter-Efficient Fine-Tuning (LoRA, Adapters, Prompt Tuning)**
  - Why needed here: PEFT methods are central to making federated post-training computationally feasible; Table 4's defense analysis is method-specific.
  - Quick check question: How many trainable parameters does LoRA add compared to full fine-tuning, and why does this matter for FL communication costs?

- **Privacy Attacks on Federated Learning (Membership Inference, Gradient Inversion, Poisoning)**
  - Why needed here: The security analysis (Table 4) assumes understanding of what each attack extracts and which defenses mitigate them.
  - Quick check question: What does a gradient inversion attack reconstruct, and why does differential privacy help defend against it?

## Architecture Onboarding

- **Component map:**
  - Foundation model layer: Open-source (GPT-J, Mistral-7B Apache license) vs. open-weight (Llama family) vs. gray-box (Together.AI) vs. black-box (GPT-4, Claude)
  - Post-training method layer: FFT → LoRA → Adapters → Instruction Tuning → RLHF → Prompt Tuning (ordered by parameter footprint)
  - FL infrastructure: Client nodes (local training), aggregation server (FedAvg or Byzantine-robust variants)
  - Defense layer: DP (noise injection), Secure Aggregation (MPC-based), Homomorphic Encryption, Byzantine Aggregation

- **Critical path:**
  1. Assess client hardware (VRAM, compute) against model size requirements (Section 2.2: 5-48 GB for 7B-65B with QLoRA)
  2. Select model openness level using Figure 2 decision tree (privacy/autonomy constraints → model type)
  3. Choose post-training method based on Table 1 compatibility
  4. Configure defenses per Table 4 (method-dependent)
  5. Deploy FL training loop with appropriate aggregation strategy

- **Design tradeoffs:**
  - Open-source + FFT: Maximum control, highest compute/communication cost, full defense applicability
  - Open-weight + LoRA: Good balance of control and efficiency, license restrictions may apply (e.g., Llama's 700M user threshold)
  - Gray-box + adapters: Lower compute, but trust provider with frozen base model; limited defense options
  - Black-box + prompt tuning: Minimal compute, but privacy fundamentally compromised via API data transmission

- **Failure signatures:**
  - License violation: Exceeding usage restrictions post-training (e.g., Llama 3 commercial scale)
  - Privacy breach via API: Sensitive data logged by provider during gray/black-box inference
  - Defense-induced utility collapse: Over-aggressive DP noise destroying model quality
  - Heterogeneity exclusion: Clients unable to participate due to license/geo restrictions

- **First 3 experiments:**
  1. **Baseline comparison:** Implement identical downstream task (e.g., medical text classification) using open-source model with LoRA vs. black-box with prompt tuning; measure task performance and privacy leakage.
  2. **Defense overhead analysis:** Apply DP and secure aggregation to LoRA-based federated post-training; quantify utility loss vs. privacy gain across epsilon values.
  3. **Cross-silo hardware validation:** Test QLoRA post-training on 7B model across clients with varying VRAM (8GB, 12GB, 24GB) to validate Section 2.2's memory claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the partial client control available in federated black-box learning sufficient to justify the use of Federated Learning over centralized alternatives?
- Basis in paper: [explicit] Section 3.2 states, "It raises the question whether the partial control that is left for the clients is sufficient to justify the use of FL in the first place."
- Why unresolved: While the authors argue that autonomy is compromised, they do not define a specific threshold of control required to validate the FL paradigm.
- What evidence would resolve it: A formal framework defining minimum autonomy thresholds that distinguishes valid federated learning applications from decentralized data processing.

### Open Question 2
- Question: How can privacy-enhancing technologies like secure aggregation be effectively adapted for the auxiliary models (reward/policy) required in federated RLHF?
- Basis in paper: [inferred] Table 4 indicates that standard defenses like Secure Aggregation and Homomorphic Encryption are not fully available for Instruction Tuning and RLHF.
- Why unresolved: The paper notes that reward and policy models must be protected if trained in a federated fashion, but does not propose technical solutions for the identified lack of standard defenses.
- What evidence would resolve it: A protocol implementation that successfully applies secure aggregation to both policy and reward model updates in a federated setting.

### Open Question 3
- Question: What is the comparative severity of data leakage via black-box APIs versus the attack surface in open-source federated training?
- Basis in paper: [inferred] The authors argue black-box usage "inherently leaks" data via APIs (Section 3.2) while simultaneously acknowledging open-source models have a "larger attack surface" (Section 3.4).
- Why unresolved: Without quantitative comparison, it remains difficult to definitively assess if the transparency benefits of open-source models outweigh the inherent leakage risks of black-box APIs.
- What evidence would resolve it: A comparative empirical study measuring membership inference and gradient inversion attack success rates in both open-source federated fine-tuning and black-box prompt tuning.

## Limitations
- Hardware Assumptions: Section 2.2's VRAM requirements (5-48GB for 7B-65B models) are theoretical estimates that may not hold across diverse client hardware configurations in real federated deployments.
- Defense Efficacy: Table 4 presents qualitative defense applicability but lacks quantitative validation of privacy-utility tradeoffs across different epsilon values.
- License Compliance: Open-weight models like Llama have complex usage restrictions that may inadvertently exclude participants in federated settings.

## Confidence
- **High Confidence**: The fundamental claim that black-box models compromise federated learning's privacy principles through API data transmission.
- **Medium Confidence**: The mapping between model openness levels and available post-training methods (Table 1).
- **Low Confidence**: The practical feasibility of implementing the full defense stack on open models in resource-constrained federated environments.

## Next Checks
1. **Hardware Validation**: Implement QLoRA post-training on a 7B model across clients with varying VRAM (8GB, 12GB, 24GB) and measure actual memory usage, training time, and communication overhead during federated aggregation.

2. **Privacy Quantification**: Conduct membership inference and gradient inversion attacks on both open-model (LoRA) and black-box (prompt tuning) federated post-training scenarios. Measure attack success rates with and without DP defense across different epsilon values.

3. **License Compliance Audit**: Analyze the operational complexity of license compliance in federated settings by implementing a mock deployment of Llama-7B with automated license usage tracking and violation detection mechanisms.