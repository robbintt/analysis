---
ver: rpa2
title: Benchmarking Small Language Models and Small Reasoning Language Models on System
  Log Severity Classification
arxiv_id: '2601.07790'
source_url: https://arxiv.org/abs/2601.07790
tags:
- system
- logs
- classification
- accuracy
- severity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates small language models (SLMs) and small reasoning
  language models (SRLMs) for system log severity classification, a task critical
  for automated monitoring and digital twin systems. Using real-world journalctl logs,
  we test nine models under zero-shot, few-shot, and retrieval-augmented generation
  (RAG) prompting.
---

# Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification

## Quick Facts
- **arXiv ID:** 2601.07790
- **Source URL:** https://arxiv.org/abs/2601.07790
- **Authors:** Yahya Masri; Emily Ma; Zifu Wang; Joseph Rogers; Chaowei Yang
- **Reference count:** 40
- **Primary result:** Qwen3-4B achieves 95.64% accuracy on system log severity classification using retrieval-augmented generation

## Executive Summary
This study evaluates nine small language models and small reasoning language models for system log severity classification using journalctl logs. The research tests models under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting conditions. Qwen3-4B achieves the highest accuracy of 95.64% with RAG, while demonstrating that smaller models like Qwen3-0.6B can reach 88.12% accuracy despite weaker performance without retrieval. However, several reasoning models show substantial degradation when paired with RAG. The study emphasizes practical deployment considerations for digital twin systems, measuring inference times and highlighting architectural design choices that affect model performance and efficiency.

## Method Summary
The research benchmarks nine small language models and reasoning models on system log severity classification using real-world journalctl logs. Models are evaluated under three prompting conditions: zero-shot, few-shot, and retrieval-augmented generation (RAG). The study measures both accuracy and inference efficiency, with inference times ranging from under 1.2 seconds per log for Gemma and Llama variants to over 228 seconds for Phi-4-Mini-Reasoning. The evaluation focuses on practical deployment scenarios for digital twin systems, emphasizing the importance of model size and computational efficiency for real-time monitoring applications.

## Key Results
- Qwen3-4B achieves 95.64% accuracy with RAG prompting on system log severity classification
- Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval
- Several SRLMs (Qwen3-1.7B, DeepSeek-R1-Distill-Qwen-1.5B) show substantial accuracy degradation when using RAG
- Gemma and Llama variants complete inference in under 1.2 seconds per log, while Phi-4-Mini-Reasoning exceeds 228 seconds per log with <10% accuracy

## Why This Works (Mechanism)
Assumption: The superior performance of Qwen3-4B with RAG likely stems from its larger parameter count enabling better pattern recognition across diverse log structures, combined with the retrieval mechanism providing contextual information that smaller models cannot generate independently. The accuracy degradation in reasoning models with RAG suggests potential architectural conflicts between chain-of-thought reasoning processes and retrieval-based contextual augmentation, though the exact mechanism remains unclear.

## Foundational Learning
Assumption: The models leverage foundational knowledge of system administration patterns, error message structures, and severity classification schemes learned during pretraining. The RAG component supplements this with domain-specific knowledge about journalctl log formats and severity hierarchies. However, the effectiveness depends heavily on the quality and relevance of the retrieval knowledge base, which the paper does not fully characterize.

## Architecture Onboarding
- **Component map:** System logs -> Preprocessing -> Tokenization -> Model inference -> Severity classification
- **Critical path:** Log ingestion → tokenization → model processing → classification output
- **Design tradeoffs:** Model size vs accuracy vs inference speed; reasoning capability vs RAG compatibility
- **Failure signatures:** RAG degradation in reasoning models; extreme latency in complex reasoning models; accuracy drops without retrieval augmentation
- **First experiments:** 1) Test models on single log sample to verify basic functionality; 2) Measure inference time for each model; 3) Validate classification accuracy on known log samples

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions, but critical unanswered questions include why SRLMs show RAG degradation, how different knowledge base qualities affect RAG performance, and whether these results generalize to non-journalctl log formats. The relationship between model architecture and RAG compatibility also requires further investigation.

## Limitations
- Evaluation relies on journalctl logs that may not represent full diversity of system log formats
- RAG effectiveness depends heavily on knowledge base quality, which remains unspecified
- Performance degradation in SRLMs with RAG lacks full explanation of underlying causes
- Inference time measurements don't account for preprocessing overhead or end-to-end pipeline latency
- Limited exploration of how model size affects RAG compatibility across different reasoning approaches

## Confidence
- Performance claims (95.64% accuracy for Qwen3-4B with RAG): High confidence
- RAG degradation patterns in SRLMs: Medium confidence
- Deployment feasibility assertions: Medium confidence
- Generalizability across log types and systems: Low confidence
- Explanations for RAG degradation mechanisms: Low confidence

## Next Checks
1. Cross-system validation: Test models on diverse log sources (Apache, Nginx, Windows Event Logs, custom application logs)
2. Knowledge base robustness assessment: Systematically vary RAG knowledge base quality and coverage
3. End-to-end pipeline benchmarking: Measure complete system performance under realistic concurrent workload scenarios
4. Architectural investigation: Analyze model internals to understand RAG compatibility issues in reasoning models
5. Format generalization: Test performance across different log formatting conventions and severity schemes