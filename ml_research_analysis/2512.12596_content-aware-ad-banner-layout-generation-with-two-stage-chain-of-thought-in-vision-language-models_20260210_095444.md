---
ver: rpa2
title: Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in
  Vision Language Models
arxiv_id: '2512.12596'
source_url: https://arxiv.org/abs/2512.12596
tags:
- layout
- image
- generation
- text
- placement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a vision-language model-based method for
  generating content-aware ad banner layouts. The approach addresses limitations of
  existing saliency-based methods by using a two-stage chain-of-thought prompting
  strategy: first generating a text-based placement plan through VLM analysis of image
  content and object relationships, then producing the final HTML layout based on
  that plan.'
---

# Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models

## Quick Facts
- arXiv ID: 2512.12596
- Source URL: https://arxiv.org/abs/2512.12596
- Authors: Kei Yoshitake; Kento Hosono; Ken Kobayashi; Kazuhide Nakata
- Reference count: 26
- Key outcome: Two-stage chain-of-thought VLM method for content-aware ad banner layouts, showing competitive performance vs baselines

## Executive Summary
This paper introduces a vision-language model-based method for generating content-aware ad banner layouts. The approach addresses limitations of existing saliency-based methods by using a two-stage chain-of-thought prompting strategy: first generating a text-based placement plan through VLM analysis of image content and object relationships, then producing the final HTML layout based on that plan. This separation enables more interpretable and controllable layout generation that better reflects image semantics. The method was evaluated on the PKU-PosterLayout dataset using standard metrics (Validity, Overlap, Alignment, Underlay, Utility, Occlusion, Unreadability) and VLM-based evaluation. Results show competitive or superior performance compared to baselines, particularly in avoiding salient regions and satisfying placement constraints.

## Method Summary
The method uses GPT-4o with a two-stage few-shot prompting approach. In Stage 1, the VLM analyzes the background image along with element constraints to generate a text-based placement plan that describes where and why elements should be positioned. In Stage 2, this plan is combined with the image and constraints to generate HTML layout code with `<div>` elements positioned using bounding box coordinates. The approach leverages VLM's semantic understanding of image content rather than relying on saliency maps, and the explicit planning stage improves interpretability and constraint satisfaction.

## Key Results
- Two-step (5-shot) achieved Occlusion 0.1619 vs one-step (5-shot) at 0.2502
- Underlay constraint satisfaction: Loose 0.9882 vs Strict 0.9932 in two-step approach
- VLM-based evaluation showed improved content-awareness and constraint satisfaction
- Qualitative examples demonstrated better visual appropriateness and semantic reasoning

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Chain-of-Thought Decomposition
Separating layout generation into explicit planning and execution phases improves content-awareness and constraint satisfaction. By forcing the model to first verbalize spatial reasoning before generating code, reasoning becomes explicit rather than post-hoc rationalization. The paper observed that in one-step generation, HTML layout was generated first, followed by a textual placement plan which may compromise both image awareness and constraint satisfaction.

### Mechanism 2: Semantic Image Understanding via VLM
VLMs provide richer semantic understanding of image content than saliency-based methods, enabling selective avoidance of truly important regions. VLMs identify objects and their semantic importance (e.g., distinguishing "rabbit's face" from generic salient regions), enabling context-aware placement that saliency maps cannot achieve.

### Mechanism 3: Few-Shot Prompting with Structured Exemplars
Providing structured input-output examples stabilizes VLM outputs and ensures consistent, interpretable format. Few-shot exemplars constrain the output space and demonstrate the desired reasoning pattern, reducing prompt interpretation variance.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Core technique enabling reasoning decomposition; understanding why "let's think step by step" improves complex tasks is essential. Quick check: Why might asking a model to explain its reasoning before giving an answer improve accuracy on spatial tasks?

- **HTML as Layout Representation**: The method encodes layouts as HTML `<div>` tags; you must understand this representation to debug outputs. Quick check: Given position (10, 50), width 80, height 20, write the corresponding HTML div element.

- **Saliency Maps vs. Semantic Understanding**: Understanding why saliency-based methods fail clarifies the VLM approach's value proposition. Quick check: A saliency map highlights 70% of an image as "salient." Why might this still fail to protect a product from being occluded?

## Architecture Onboarding

- **Component map**: Background image + element constraints + few-shot exemplars → VLM → Text placement plan → VLM → HTML layout → Renderer → Final rendered ad banner

- **Critical path**:
  1. Create manually-authored few-shot exemplars (placement plans + matching HTML)
  2. Construct Stage 1 prompt with image, constraints, and exemplars → call VLM API
  3. Feed Stage 1 output (plan) into Stage 2 prompt → call VLM API again
  4. Parse HTML output and render overlay on background image

- **Design tradeoffs**:
  - 5-shot vs 10-shot: Paper found no significant quantitative difference; 5-shot is more token-efficient
  - One-step vs two-step: Two-step requires 2× API calls but shows better content-awareness (lower Occlusion)
  - Model choice: Paper uses GPT-4o; other VLMs (LLaVA, etc.) untested—performance may vary

- **Failure signatures**:
  - Elements overlapping salient/important image regions → planning stage failed to identify constraints
  - Underlay elements not positioned beneath text/logo → constraint satisfaction failure
  - Placement plan is generic ("place elements in corners") → VLM not engaging with image content
  - Malformed HTML (missing tags, invalid coordinates) → parsing/validation failure

- **First 3 experiments**:
  1. Reproduce 0-shot baseline: Generate layouts without exemplars to confirm model's default behavior
  2. Ablate two-stage decomposition: Compare 1-step CoT vs 2-step CoT on 20-30 samples; measure Occlusion and inspect output order
  3. Validate constraint satisfaction: Automatically check underlay-text containment in generated HTML; compare violation rates against benchmarks

## Open Questions the Paper Calls Out

**Open Question 1**: How can VLM-based evaluation frameworks be improved to reliably assess graphic design layouts when elements are represented as bounding boxes rather than fully rendered graphics? The authors note that GPT-4o frequently failed to recognize visual relationships when evaluating box-based layouts, leading to low scores that did not match actual quality.

**Open Question 2**: Can quantitative benchmarks be developed to evaluate semantic content-awareness without relying on proxy metrics like saliency maps? The paper acknowledges their method underperforms on "Utility" and "Occlusion" metrics because these rely on saliency maps, highlighting a misalignment between model capability and measurement tools.

**Open Question 3**: To what extent can the intermediate "placement plan" be modified by humans to effectively correct or refine the final layout? The paper highlights "controllability" as a key advantage of the two-stage approach, yet experiments focus solely on end-to-end automatic generation without testing human intervention in the intermediate step.

## Limitations
- Unknown few-shot exemplars: Only 1 example per table shown; remaining exemplars not provided
- VLM evaluation protocol: Evaluation prompts referenced to unavailable appendix
- Dataset construction details: Exact element type distribution and constraint sampling not fully specified

## Confidence
- **High confidence**: Two-stage CoT decomposition works better than one-step generation (Occlusion 0.1619 vs 0.2502)
- **Medium confidence**: VLM semantic understanding provides advantage over saliency maps, based on qualitative observations
- **Low confidence**: VLM-based evaluation reliability, given opaque evaluation prompts and potential interpretation variance

## Next Checks
1. Reconstruct exemplar set: Manually create 5-10 few-shot exemplars matching the paper's format and test whether 5-shot vs 10-shot shows no significant difference as claimed.

2. Validate constraint satisfaction: Implement automatic HTML parsing to verify underlay-text containment rates; confirm violation rates align with Table 6 benchmarks.

3. Ablate planning stage: Generate 20-30 layouts using one-step CoT vs two-step CoT; verify the claimed mechanism that one-step generates code first then retroactively writes plans by analyzing output order.