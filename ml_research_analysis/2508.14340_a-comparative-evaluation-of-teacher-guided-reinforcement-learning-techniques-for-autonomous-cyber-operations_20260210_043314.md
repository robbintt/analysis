---
ver: rpa2
title: A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques
  for Autonomous Cyber Operations
arxiv_id: '2508.14340'
source_url: https://arxiv.org/abs/2508.14340
tags:
- teacher
- uni00000013
- uni00000003
- agent
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of slow convergence and poor
  early-stage performance in reinforcement learning for autonomous cyber operations
  by implementing teacher-guided techniques. Four distinct methods were integrated
  into the CybORG environment: feature space modification, reward shaping, action
  masking, and auxiliary loss, each leveraging a pretrained RL agent as a teacher.'
---

# A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations

## Quick Facts
- **arXiv ID:** 2508.14340
- **Source URL:** https://arxiv.org/abs/2508.14340
- **Reference count:** 17
- **Primary result:** Teacher-guided RL techniques (action masking and auxiliary loss) significantly improve early-stage performance and convergence speed in autonomous cyber operations compared to baseline PPO.

## Executive Summary
This study addresses the challenge of slow convergence and poor early-stage performance in reinforcement learning for autonomous cyber operations by implementing teacher-guided techniques. Four distinct methods were integrated into the CybORG environment: feature space modification, reward shaping, action masking, and auxiliary loss, each leveraging a pretrained RL agent as a teacher. The primary results showed that incorporating teacher guidance as an auxiliary loss signal and action masking yielded the most promising improvements, with action masking achieving high initial performance and auxiliary loss enabling quick convergence to the teacher's policy. Reward shaping and feature space modification provided no measurable benefits. Specifically, action masking achieved initial rewards of approximately 50, while auxiliary loss allowed agents to converge to the teacher's performance five times faster than baseline PPO.

## Method Summary
The paper evaluates four teacher-guided reinforcement learning techniques within the CybORG Cage Challenge 2 environment. A pretrained PPO agent (trained for 100 episodes) serves as the teacher to guide a student PPO agent. The methods include: 1) reward shaping (modifying scalar rewards), 2) action masking (restricting action space based on teacher recommendations), 3) auxiliary loss (adding teacher-guided loss term to PPO objective), and 4) feature space modification (appending teacher actions to state). Each technique was tested with gradual decay schedules or hard stops during transition to independence. Performance was measured over 10 independent runs of 500 episodes each, comparing mean reward curves and convergence speed against a baseline PPO agent.

## Key Results
- Action masking and auxiliary loss techniques significantly outperformed baseline PPO in both early-stage performance and convergence speed
- Action masking achieved initial rewards of approximately 50, with gradual decay providing the best balance
- Auxiliary loss enabled convergence to teacher performance by episode 20, five times faster than baseline PPO
- Reward shaping and feature space modification provided no measurable benefits over baseline
- Hard stops during transition caused performance collapse, while gradual decay mitigated this issue

## Why This Works (Mechanism)

### Mechanism 1: Direct Gradient Alignment via Auxiliary Loss
Incorporating teacher guidance as an auxiliary loss signal accelerates convergence by directly biasing the policy update toward the teacher's policy distribution during backpropagation. This method adds a specific term ($L_{Teacher}$) to the PPO actor loss, forcing the optimizer to align network weights with the teacher's policy immediately rather than waiting for sparse reward signals.

### Mechanism 2: Action Masking for Exploration Reduction
Action masking improves early-stage performance by restricting the agent's exploration space to a subset of high-probability actions defined by the teacher. This creates guided exploration where the agent samples from a narrowed distribution, reducing variance of policy gradient estimates and preventing hazardous random actions.

### Mechanism 3: Failure of Indirect Heuristics (Reward Shaping & Features)
Indirect guidance methods like reward shaping and feature augmentation failed to provide measurable benefits, likely due to signal dilution or the agent's inability to correlate features with actions. A scalar bonus may be too weak to shift policy gradients effectively compared to the direct force of a loss term.

## Foundational Learning

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** PPO is the base algorithm for all experiments. Understanding its actor-critic architecture and clipping objective is required to interpret how the auxiliary loss interacts with the primary loss.
  - **Quick check question:** How does the PPO clipping objective prevent large, destabilizing policy updates, and how might adding an auxiliary loss term interfere with this stability?

- **Concept:** Knowledge Distillation (Teacher-Student)
  - **Why needed here:** The core premise is transferring knowledge from a "frozen" pretrained agent to a student. You must distinguish between imitation learning and the guided exploration techniques used here.
  - **Quick check question:** In this paper, does the student learn from the teacher's weights, or via the teacher's outputs during environment interaction?

- **Concept:** Exploration vs. Exploitation in Cyber Operations
  - **Why needed here:** The paper frames the problem as "poor early-stage performance" due to random exploration. The mechanisms are essentially exploitation hacks to skip the random exploration phase.
  - **Quick check question:** Why is random exploration particularly costly in Autonomous Cyber Operations compared to a game like CartPole?

## Architecture Onboarding

- **Component map:** Environment (CybORG) -> Teacher (frozen PPO) -> Student (PPO) -> Intervention Layer (mask/loss logic) -> Update
- **Critical path:** 1) Teacher Inference: Pass current state to frozen Teacher to get recommended action. 2) Student Inference: Student generates distribution. 3) Intervention: Apply mask to distribution or store teacher action for loss. 4) Update: Compute PPO loss and teacher loss, combine using decay parameter, and backprop.
- **Design tradeoffs:** Gradual decay (soft transition) vs. hard stops; masking specific actions (higher initial performance but brittle) vs. masking hosts (robust but slower); teacher quality (weak teacher sufficient for bootstrap but may limit ceiling).
- **Failure signatures:** Flatlining (performance matches teacher but never surpasses), transition collapse (sharp drop when guidance removed), gradient conflict (PPO loss decreasing while auxiliary loss is unstable).
- **First 3 experiments:** 1) Baseline Reproduction: Train standard PPO agent for 500 episodes to establish slow convergence baseline. 2) Hard Stop Masking: Implement action masking with hard stop at interval 4 to observe performance drop. 3) Decayed Auxiliary Loss: Implement auxiliary loss with gradual decay and increasing entropy to verify convergence by episode 20.

## Open Questions the Paper Calls Out

### Open Question 1
Can combining successful teacher-guided techniques (e.g., auxiliary loss and action masking) yield additive improvements in convergence speed and final policy performance compared to single-method implementations? The paper evaluates each technique in isolation, identifying them as top performers but not experimenting with simultaneous application.

### Open Question 2
Can external knowledge sources, such as Large Language Models (LLMs) or expert heuristics, replace the pre-trained RL teacher without necessitating the initial round of training? The current methodology relies on a "bootstrapping" problem where a teacher must be trained via RL before it can guide the student.

### Open Question 3
To what extent does the quality and performance level of the teacher agent influence the success of the guidance techniques, particularly regarding the risk of limiting the student to the teacher's performance ceiling? The paper uses a teacher trained for only 100 episodes but does not test how teacher expertise affects student learning curves.

## Limitations
- Results are based on a single simulation environment (CybORG Cage Challenge 2) and relatively simple teacher policy (100 episodes training)
- External validity across different cyber ranges or more sophisticated teacher policies remains untested
- Absence of benefit from reward shaping and feature modification could reflect specific implementation choices rather than universal truths

## Confidence

**High confidence:** Action masking improves early performance; auxiliary loss accelerates convergence; reward shaping and feature modification show no measurable benefit in this specific setup.

**Medium confidence:** These results generalize to other ACO environments; the teacher's suboptimality doesn't limit the student's potential to exceed it.

**Low confidence:** The specific decay schedules and hyperparameters are optimal; performance collapse with action masking is unavoidable.

## Next Checks

1. **Cross-environment validation:** Test the two successful techniques (auxiliary loss, action masking) in at least one different cyber range or simulation environment to assess generalizability.

2. **Teacher quality sensitivity:** Repeat key experiments with teachers trained for varying episode counts (e.g., 50, 200, 500) to determine how teacher quality affects student learning curves and final performance.

3. **Hybrid approach testing:** Implement a combined technique that uses action masking for the first 10 episodes, then switches to auxiliary loss guidance, to test if staged guidance can preserve early performance while enabling smoother transitions.