---
ver: rpa2
title: Achieving Distributive Justice in Federated Learning via Uncertainty Quantification
arxiv_id: '2504.15924'
source_url: https://arxiv.org/abs/2504.15924
tags:
- fairness
- learning
- federated
- uncertainty
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors propose UDJ-FL, a federated learning framework that\
  \ achieves distributive justice-based fairness by leveraging aleatoric uncertainty\
  \ as client weights. The method enables practitioners to achieve egalitarian, utilitarian,\
  \ Rawls\u2019 difference principle, or desert-based fairness by adjusting hyperparameters."
---

# Achieving Distributive Justice in Federated Learning via Uncertainty Quantification

## Quick Facts
- arXiv ID: 2504.15924
- Source URL: https://arxiv.org/abs/2504.15924
- Authors: Alycia Carey; Xintao Wu
- Reference count: 40
- Primary result: Achieves four distributive justice fairness theories in FL using aleatoric uncertainty as client weights

## Executive Summary
This paper introduces UDJ-FL, a federated learning framework that achieves client-level distributive justice through uncertainty quantification. The method uses aleatoric uncertainty (data noise) as a pre-training proxy for client "advantage" and incorporates fairness theories via a unified objective function. By adjusting hyperparameters, the framework can optimize for egalitarian, utilitarian, Rawls' difference principle, or desert-based fairness. Experiments on Dirty-MNIST and CURE-TSR datasets demonstrate that UDJ-FL achieves fairness metrics equivalent to or better than existing methods while maintaining high global accuracy.

## Method Summary
UDJ-FL operates by first estimating each client's aleatoric uncertainty via softmax entropy during a pre-training phase. This uncertainty value serves as the client's weight in the federated aggregation. The framework uses a unified loss function based on α-fairness theory, which can be tuned to achieve different distributive justice objectives by adjusting hyperparameters β and γ. For training stability with high β values, the method employs gradient normalization using estimated Lipschitz constants. The approach assumes aleatoric uncertainty remains stable throughout training and that the mathematical mapping from fairness theory to FL optimization holds under non-convex loss landscapes.

## Key Results
- Achieved 94.26% global accuracy on Dirty-MNIST and 98.61% on CURE-TSR for utilitarian fairness
- Reduced standard deviation of client accuracies to 2.21% (Dirty-MNIST) and 0.94% (CURE-TSR) for egalitarian fairness
- Showed highest benefit to least advantaged clients for Rawls' difference principle
- Achieved most negative correlation between client accuracies and aleatoric uncertainty for desert fairness
- Demonstrated that using aleatoric uncertainty as weights prevents accuracy degradation when high-uncertainty clients have large datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Aleatoric uncertainty serves as a stable, pre-training proxy for client "advantage" or "contribution," replacing naive metrics like dataset size.
- **Mechanism**: The framework calculates the average softmax entropy (aleatoric uncertainty) for each client's data before FL training begins. This value ($v_i$) is injected into the aggregation weight. High $v_i$ implies noisy data (disadvantaged), while low $v_i$ implies clean data (high contributor). This prevents the "large client with bad data" failure mode of standard FedAvg.
- **Core assumption**: Aleatoric uncertainty is irreducible and reflects data quality consistently throughout the training timeline.
- **Evidence anchors**: Abstract mentions "aleatoric uncertainty-based client weighing"; Appendix C discusses fundamental limiting bounds; corpus links uncertainty to fairness generally.

### Mechanism 2
- **Claim**: A unified objective function derived from $\alpha$-fairness can switch between distinct distributive justice theories by adjusting hyperparameters ($\beta, \gamma$).
- **Mechanism**: The loss function $\sum v_i^\gamma H_i(\theta)^{r\beta}$ is manipulated. For Utilitarian fairness, $\gamma=-1$ (up-weight low uncertainty) and $\beta \to 0$. For Rawlsian fairness, $\gamma=1$ (up-weight high uncertainty) and $\beta \to \infty$ (min-max).
- **Core assumption**: The mathematical mapping from resource allocation theory to FL optimization holds under non-convex neural network loss landscapes.
- **Evidence anchors**: Abstract states framework achieves four fairness theories; Table 2 shows hyperparameter mappings.

### Mechanism 3
- **Claim**: Normalizing gradients using an upper bound on the local Lipschitz constant stabilizes training when using power-scaled losses ($H^{r\beta}$).
- **Mechanism**: Algorithm 1 computes a "pseudo-gradient" $\Delta$ and a normalization term $g$ based on Lemma 1. The update step is $\theta^{t+1} = \theta^t - \sum \Delta / \sum g$. This acts as a dynamic learning rate, preventing exploding gradients when $\beta$ is high (Rawlsian).
- **Core assumption**: The local Lipschitz constant estimation is accurate enough to prevent step-size pathologies.
- **Evidence anchors**: Section describes Algorithm 1 and Lipschitz estimation; corpus does not address this specific technique.

## Foundational Learning

- **Concept**: Aleatoric vs. Epistemic Uncertainty
  - **Why needed here**: UDJ-FL relies specifically on *aleatoric* uncertainty (data noise) because it is irreducible. Epistemic uncertainty reduces as the model learns, making it an unstable weight during training.
  - **Quick check question**: Can you reduce the uncertainty of a fair coin flip by observing more flips? (Yes -> Epistemic; No -> Aleatoric).

- **Concept**: Distributive Justice Theories (Rawls vs. Utilitarian)
  - **Why needed here**: To configure the hyperparameters correctly, one must understand the philosophy. E.g., optimizing for the client with the highest loss (Rawls') is distinct from maximizing the average accuracy (Utilitarian).
  - **Quick check question**: If a policy improves the average wealth of a society but lowers the status of the poorest group, which theory does it violate? (Rawls' Difference Principle).

- **Concept**: Alpha-Fairness in Resource Allocation
  - **Why needed here**: The math of UDJ-FL is borrowed from network bandwidth allocation. Understanding how the $\alpha$ parameter controls the trade-off between efficiency and fairness explains how $\beta$ functions in this context.
  - **Quick check question**: In network fairness, what happens to the resource allocation as the fairness parameter $\alpha \to \infty$? (It approaches Max-Min fairness, prioritizing the worst-off user).

## Architecture Onboarding

- **Component map**: Uncertainty Estimator -> Local Trainer -> Gradient Processor -> Global Aggregator
- **Critical path**:
  1. **Initialization**: All clients must train locally to estimate $v_i$ *before* federation starts.
  2. **Broadcast**: Server sends $\theta^t$ to clients.
  3. **Local Compute**: Clients calculate loss $H_i$, gradients, and Lipschitz bounds.
  4. **Aggregation**: Server applies the weighted sum based on the chosen fairness theory ($\gamma, \beta$).

- **Design tradeoffs**:
  - **Pre-calculation vs. Real-time**: Calculating $v_i$ once at the start saves communication/compute but assumes data distribution stationarity.
  - **Fairness vs. Global Accuracy**: Setting $\beta \to \infty$ (Rawls) empirically lowers global accuracy (Table 4: Global Acc drops from ~94% to ~85-90%) to help the worst client.

- **Failure signatures**:
  - **Exploding Gradients**: High $\beta$ values without proper Lipschitz scaling will cause NaN losses.
  - **Stagnation**: If $\gamma$ and $\beta$ oppose each other (e.g., trying to be Utilitarian but up-weighting high uncertainty clients), convergence slows significantly.
  - **Misidentified Disadvantage**: If aleatoric uncertainty is low but data is adversarially poisoned, UDJ-FL will incorrectly treat the client as "advantaged," degrading global performance.

- **First 3 experiments**:
  1. **Baseline Uncertainty Check**: Train 5 clients locally on Dirty-MNIST with varying clean/ambiguous ratios. Verify that calculated $v_i$ correlates with the expected ambiguity (Table 3 validation).
  2. **Hyperparameter Sweep**: Run UDJ-FL with $\beta=\{0, 1, 5, \text{neg}\}$ on a non-IID split. Plot the "Efficiency Frontier" (Fig 1) to verify Rawls/Utilitarian trade-offs match the paper's claims.
  3. **Ablation on Weights**: Compare using $v_i$ (uncertainty) weights vs. $n_i$ (data size) weights in a scenario where the largest client has the dirtiest data. Check if global accuracy degrades with $n_i$ weighting (as claimed in Abstract).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can UDJ-FL be effectively extended to cross-device federated learning settings where clients are unreliable and may drop out?
- **Basis in paper**: Explicit statement about future plans to extend to cross-device settings.
- **Why unresolved**: Current work assumes cross-silo settings with all clients participating every round. Cross-device settings introduce client unavailability, stragglers, and heterogeneous participation patterns that may invalidate the pre-computed aleatoric uncertainty weights.
- **What evidence would resolve it**: Empirical demonstration of UDJ-FL performance in cross-device simulations with client dropout rates, showing whether uncertainty-based weights remain stable and fairness guarantees hold under partial participation.

### Open Question 2
- **Question**: Does the assumption that aleatoric uncertainty accurately captures client contribution and advantage hold under highly noisy or adversarial data distributions?
- **Basis in paper**: Section 5.3 explicitly states this assumption may not hold in all cases.
- **Why unresolved**: If clients can manipulate their reported aleatoric uncertainty scores or if noise patterns differ from natural ambiguity, the weighting scheme could be gamed or produce unfair distributions.
- **What evidence would resolve it**: Experiments with adversarial clients who artificially inflate or deflate uncertainty estimates, and analysis of UDJ-FL's robustness to such manipulation.

### Open Question 3
- **Question**: Can differential privacy be integrated into UDJ-FL without degrading fairness guarantees?
- **Basis in paper**: Future plans to implement a differentially private version of UDJ-FL.
- **Why unresolved**: Differential privacy adds noise to gradients and model updates, which may interfere with the uncertainty quantification mechanism and the fairness-accuracy tradeoffs.
- **What evidence would resolve it**: Theoretical analysis and empirical evaluation of DP-UDJ-FL showing bounds on privacy-fairness-accuracy tradeoffs.

### Open Question 4
- **Question**: How does the source of aleatoric uncertainty (measurement error, inherent randomness, label noise) affect UDJ-FL's fairness outcomes?
- **Basis in paper**: Footnote 5 mentions this would be an interesting future study.
- **Why unresolved**: The current work treats aleatoric uncertainty monolithically, but different uncertainty sources may have different implications for what constitutes "fair" treatment.
- **What evidence would resolve it**: Controlled experiments isolating different uncertainty sources and measuring whether UDJ-FL's fairness guarantees hold uniformly across them.

## Limitations
- Framework assumes aleatoric uncertainty remains stable throughout training, which may not hold under significant data distribution drift
- Cannot simultaneously optimize for conflicting fairness goals due to single-objective formulation
- Generalizability to more than 5 clients or non-vision tasks has not been demonstrated
- Computational overhead and communication efficiency compared to standard FedAvg is not evaluated

## Confidence
- **High Confidence**: Core mechanism of using aleatoric uncertainty as client weights is well-supported by empirical results
- **Medium Confidence**: Assumption that pre-calculated uncertainty remains stable throughout training is reasonable but not fully validated
- **Low Confidence**: Framework's performance at scale and across different task domains remains untested

## Next Checks
1. **Distribution Drift Test**: Implement a data drift scenario where one client's data distribution changes significantly during training. Verify whether UDJ-FL's performance degrades compared to baseline methods when uncertainty estimates become stale.
2. **Multi-Objective Extension**: Modify the framework to handle simultaneous Rawlsian and Utilitarian fairness goals using a weighted sum approach. Test whether the framework can maintain reasonable performance on both objectives without significant degradation.
3. **Scale and Transfer Test**: Apply UDJ-FL to a larger-scale federated learning task (e.g., 50+ clients) using a non-vision dataset like Shakespeare or Stack Overflow. Verify whether the framework maintains its fairness benefits at scale and across different task domains.