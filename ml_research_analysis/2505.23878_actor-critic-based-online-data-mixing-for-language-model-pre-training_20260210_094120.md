---
ver: rpa2
title: Actor-Critic based Online Data Mixing For Language Model Pre-Training
arxiv_id: '2505.23878'
source_url: https://arxiv.org/abs/2505.23878
tags:
- data
- training
- domain
- domains
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient data mixing for large
  language model pretraining by developing an actor-critic based online data mixing
  (AC-ODM) method. The core method uses reinforcement learning with actor-critic networks
  to dynamically adjust domain weights during pretraining, considering intra-domain
  interactions through a gradient alignment-based reward function.
---

# Actor-Critic based Online Data Mixing For Language Model Pre-Training

## Quick Facts
- **arXiv ID:** 2505.23878
- **Source URL:** https://arxiv.org/abs/2505.23878
- **Reference count:** 9
- **Primary result:** AC-ODM-410M reaches optimal validation perplexity 71% faster than ODM baseline, improving zero-shot MMLU accuracy by 27.5% and achieving 2.23x better HumanEval pass@1.

## Executive Summary
This paper addresses efficient data mixing for large language model pretraining by developing an actor-critic based online data mixing (AC-ODM) method. The core innovation uses reinforcement learning with actor-critic networks to dynamically adjust domain weights during pretraining, considering intra-domain interactions through a gradient alignment-based reward function. To improve efficiency, the authors transfer the sampling strategy learned by a small proxy model (410M parameters) to pretrain a larger target model (1B parameters). Experimental results show AC-ODM-410M reaches the optimal validation perplexity of ODM 71% faster and improves performance on zero-shot MMLU benchmark by 27.5% accuracy and achieves about 2.23x better performance on the pass@1 metric of the HumanEval benchmark compared to ODM.

## Method Summary
The method uses a two-stage approach: first, a proxy model (410M parameters) is trained using actor-critic reinforcement learning with gradient alignment rewards, where the actor network learns to sample data according to training dynamics and the critic evaluates action quality. The reward function W_i = ⟨∇ℓ_i, Σ_j∇ℓ_j⟩ measures gradient alignment between domains to capture intra-domain interactions. After the proxy model converges, the learned actor network is frozen and used to determine domain weights for training the larger target model (1B parameters), bypassing the computational overhead of calculating rewards during target model training. Both models are trained for 41,667 steps on The Pile dataset using Pythia-style architectures with GPT-NeoX-20B tokenizer.

## Key Results
- AC-ODM-410M reaches optimal validation perplexity 71% faster than ODM baseline
- Improves zero-shot MMLU benchmark accuracy by 27.5% over ODM
- Achieves approximately 2.23x better HumanEval pass@1 metric compared to ODM
- Demonstrates superior perplexity across all 22 domains in Pile dataset
- Achieves 20.7% lower perplexity than original ODM baseline with 41,667 training steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rewarding domains based on gradient alignment scores theoretically accelerates convergence by prioritizing data that benefits multiple domains simultaneously.
- **Mechanism:** The system calculates a reward $W_i = \langle \nabla \ell_i, \sum \nabla \ell_j \rangle$ that measures alignment between a domain's gradient and the global gradient direction. High alignment implies that learning from this domain assists in learning others (intra-domain interaction).
- **Core assumption:** Domains with aligned gradients share lexical, syntactic, or semantic structures, and optimizing for this alignment early creates a better foundation for general knowledge.
- **Break condition:** If domain gradients are orthogonal or conflicting (negative transfer), the alignment score may fail to identify useful niche data, causing the model to overfit to dominant, high-overlap domains.

### Mechanism 2
- **Claim:** Using a Deep Deterministic Policy Gradient (DDPG) framework allows for fine-grained, continuous adjustment of domain weights, potentially outperforming discrete bandit selection.
- **Mechanism:** The problem is framed as a Markov Decision Process (MDP) where an Actor network outputs continuous domain weights $\alpha$, and a Critic network estimates the expected value of that weighting strategy, allowing smooth navigation of the continuous probability simplex $\Delta^k$.
- **Core assumption:** The optimal data mixture is not static and changes dynamically during training; a continuous policy can track these shifts better than discrete algorithms like EXP3.
- **Break condition:** If the replay buffer size is insufficient or the reward signal is too noisy, the Actor-Critic networks may suffer from training instability, leading to erratic sampling strategies.

### Mechanism 3
- **Claim:** Transferring a learned sampling strategy from a small proxy model to a large target model preserves the benefits of dynamic mixing while decoupling the computational overhead from the target model training.
- **Mechanism:** The method trains a small proxy model (e.g., 410M params) to convergence using AC-ODM, then freezes and transfers the trained Actor network to determine domain weights for the large target model (e.g., 1B params) without recalculating rewards.
- **Core assumption:** The "relative" utility of data domains (the sampling policy) learned on a small model transfers effectively to a larger architecture.
- **Break condition:** If the proxy model is too small (e.g., 70M), it may fail to capture the necessary complexity of the data relationships, resulting in a poor transfer strategy.

## Foundational Learning

- **Concept: Gradient Alignment / Cosine Similarity**
  - **Why needed here:** This is the core reward signal $r_t$. You must understand that gradient dot products measure the directional similarity of updates to understand why high alignment implies "shared learning."
  - **Quick check question:** If Domain A has a high loss but its gradient is perpendicular to the global gradient, would AC-ODM prioritize it? (Answer: No, because alignment score would be near zero).

- **Concept: Actor-Critic (RL) Architecture**
  - **Why needed here:** The system uses two separate networks. The Actor proposes a distribution; the Critic judges it. Understanding this separation is required to debug why the sampling weights might be collapsing or oscillating.
  - **Quick check question:** Which network determines the final domain weights used for the target LLM? (Answer: The Actor network $\mu_{\theta_A}$).

- **Concept: The Probability Simplex ($\Delta^k$)**
  - **Why needed here:** The output of the Actor is a vector of weights summing to 1. Understanding the constraints of this space (all weights $\ge 0$, sum $= 1$) is necessary to interpret the Actor's Softmax output.
  - **Quick check question:** Why is a Softmax activation used in the Actor's output layer? (Answer: To convert raw network outputs into a valid probability distribution).

## Architecture Onboarding

- **Component map:** Proxy LLM (Environment) -> Actor Network (outputs domain weights) -> Critic Network (evaluates value) -> Target LLM (uses frozen Actor)

- **Critical path:**
  1. Warm-up: Train Proxy LLM with fixed weights to populate Replay Buffer
  2. Proxy Training: Train Proxy LLM + Actor + Critic jointly. Compute expensive gradient alignment rewards here
  3. Policy Extraction: Save the trained Actor weights
  4. Target Training: Train Target LLM using the saved Actor to determine batch composition (no Critic/Reward overhead)

- **Design tradeoffs:**
  - Proxy Size: 410M is better than 160M/70M (Fig 4), but costs more pre-compute
  - State Representation: Using only partial gradients (last FC layers) for rewards reduces overhead but assumes those layers represent the model's global state

- **Failure signatures:**
  - Domain Collapse: If the Critic overestimates value for high-resource domains, the Actor may set $\alpha_i \approx 0$ for low-resource domains
  - Slow Convergence: If the proxy is too small (70M), the transfer fails, and the target model learns slower than baselines

- **First 3 experiments:**
  1. Sanity Check: Train a 70M proxy with AC-ODM and check if validation perplexity drops faster than uniform sampling on the proxy itself
  2. Reward Verification: Visualize the gradient alignment scores ($W$) vs. domain loss to verify high-loss domains don't always have high alignment
  3. Transfer Run: Take the Actor from Experiment 1, freeze it, and use it to drive data selection for a 160M target model

## Open Questions the Paper Calls Out
- What is the optimal scaling relationship between the proxy and target LLM sizes? The conclusion states future work will investigate this relationship to further improve online data mixing performance.
- Does the learned sampling strategy transfer effectively to target models that are orders of magnitude larger? The paper hypothesizes the performance gap between proxy sizes would widen with larger target models, but all results are limited to 1B parameter targets.
- How does the gradient alignment reward function handle domains with high noise or low quality? The method rewards domains based on gradient alignment, but doesn't analyze behavior when high-loss domains contain primarily noise rather than useful signal.

## Limitations
- The transfer of learned sampling policies from small proxy models to large target models is a critical but unproven assumption, with no ablation showing how much performance gain comes from proxy training versus RL algorithm itself
- The gradient alignment reward assumes domains with aligned gradients will benefit each other, but this may not hold for truly disjoint domains like legal documents vs. cooking recipes
- The computational savings claim relies on the assumption that the 410M proxy is sufficiently large to capture domain relationships, but this threshold is not rigorously established

## Confidence

- **High Confidence:** Basic actor-critic architecture implementation and its use of DDPG for continuous action spaces; experimental setup (Pythia models, Pile dataset, training steps) is clearly specified
- **Medium Confidence:** Gradient alignment reward mechanism and its claimed benefits for convergence; while theoretically sound, the empirical advantage over simpler bandit methods is modest (71% faster vs baseline)
- **Low Confidence:** Policy transfer mechanism's general applicability; the paper only tests one transfer (410M → 1B) and doesn't explore whether this scales to much larger models or different data distributions

## Next Checks

1. **Transfer Robustness Test:** Train AC-ODM with multiple proxy sizes (70M, 160M, 410M, 1B) and systematically evaluate transfer performance to target models of various scales to establish minimum viable proxy size and test transfer assumption across model sizes

2. **Reward Mechanism Isolation:** Create an ablation where the actor-critic uses random rewards versus gradient alignment rewards, while keeping all other components identical, to quantify how much of the performance gain comes specifically from the alignment-based reward versus the actor-critic framework itself

3. **Domain Diversity Analysis:** For a held-out test set, analyze which domains the actor-critic prioritizes at different training stages versus uniform sampling, and compute the overlap between gradient-aligned domains and domains that contribute most to downstream task performance (MMLU/HumanEval) to validate the intra-domain interaction hypothesis