---
ver: rpa2
title: 'SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism'
arxiv_id: '2510.21599'
source_url: https://arxiv.org/abs/2510.21599
tags:
- shap
- tensor
- complexity
- neural
- computing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a complexity-theoretic analysis of computing
  exact SHAP explanations for Tensor Networks (TNs), a broader class of models than
  previously studied. It introduces a general framework for computing exact SHAP for
  TNs with arbitrary structures and shows that when TNs are restricted to Tensor Train
  (TT) structures, SHAP computation can be performed in poly-logarithmic time using
  parallel computation.
---

# SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism

## Quick Facts
- arXiv ID: 2510.21599
- Source URL: https://arxiv.org/abs/2510.21599
- Reference count: 40
- Exact SHAP for Tensor Trains is in NC² (poly-logarithmic parallel time)

## Executive Summary
This paper provides a complexity-theoretic analysis of computing exact SHAP explanations for Tensor Networks (TNs), establishing that Tensor Train (TT) structures enable poly-logarithmic parallel computation of SHAP values. The work introduces a general framework for computing exact SHAP for TNs with arbitrary structures and demonstrates that width, rather than depth, is the primary computational bottleneck for SHAP in binarized neural networks. These results significantly tighten known complexity bounds and extend to various ML models including decision trees, tree ensembles, linear models, and linear RNNs.

## Method Summary
The method reformulates SHAP computation as tensor contractions by constructing a "Marginal SHAP Tensor" from two components: a modified Weighted Coalitional tensor (sparse TT) and a Marginal Value tensor. Algorithm 1 builds these tensors using specific core constructions - the coalitional tensor tracks feature indices and coalition sizes, while router tensors simulate interventional operators. For Tensor Trains, the resulting Marginal SHAP Tensor also admits a TT representation, enabling parallel computation via parallel scan strategies. The framework supports arbitrary distributions but achieves efficient complexity only when models and distributions are restricted to TT structures.

## Key Results
- Computing exact Marginal SHAP for Tensor Trains lies in NC² complexity class (poly-logarithmic parallel time)
- Width is identified as the primary computational bottleneck for SHAP in Binarized Neural Networks (BNNs), with fixed width ensuring Fixed-Parameter Tractability
- The framework extends exact SHAP computation to trees, tree ensembles, linear models, and linear RNNs beyond previous work

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exact SHAP values can be computed by contracting two specific tensors: a weighted coalitional tensor and a marginal value tensor.
- **Mechanism:** The paper reformulates the exponential SHAP sum into a tensor operation. By defining a "modified Weighted Coalitional tensor" ($\tilde{W}$) and a "Marginal Value tensor" ($V$), the SHAP value becomes a contraction of these two tensors (Prop 1). This moves the complexity from the summation loop to the structural properties of the tensors involved.
- **Core assumption:** The model and distribution can be represented as Tensor Networks (TNs), and the contraction of these specific tensors is computationally feasible.
- **Evidence anchors:**
  - [section] "The Marginal SHAP Tensor $T^{(M,P)}$ can be represented by a TT parametrized as..."
  - [abstract] "...introduces a general framework for computing provably exact SHAP explanations for general TNs..."
  - [corpus] Related work "Tractable Shapley Values..." supports the general viability of using TNs for SHAP, though this paper formalizes the exact contraction mechanism.

### Mechanism 2
- **Claim:** If the model and distribution are restricted to Tensor Train (TT) structures, SHAP computation is efficiently parallelizable (belongs to complexity class NC).
- **Mechanism:** When the model and distribution are TTs, the Marginal SHAP Tensor also admits a TT representation. Since matrix multiplication and TT contraction can be solved via parallel scan strategies, the entire SHAP computation can be performed in poly-logarithmic time relative to the input dimension, given enough parallel processors.
- **Core assumption:** The hardware supports massive parallelism (PRAM model), and the TT ranks (bond dimensions) remain manageable.
- **Evidence anchors:**
  - [section] Proposition 3 states "...Computing Marginal SHAP for the family of TTs lies in NC^2."
  - [abstract] "...SHAP computation can be performed in poly-logarithmic time using parallel computation."
  - [corpus] Evidence for parallelism is weak in provided neighbors; this specific complexity bound appears novel to this work.

### Mechanism 3
- **Claim:** Network width, rather than depth, is the primary computational bottleneck for SHAP in Binarized Neural Networks (BNNs).
- **Mechanism:** This is derived via reduction. The paper shows BNNs can be compiled into TNs. The analysis reveals that fixing width (and sparsity) renders the problem Fixed-Parameter Tractable (FPT), whereas fixing depth does not (remains para-NP-Hard). The "width" effectively controls the size of the state space in the equivalent automata/Tensor Train representation.
- **Core assumption:** The BNN uses a reified cardinality representation (weights $\pm 1$), and the analysis holds under specific distribution classes (empirical, independent, TT).
- **Evidence anchors:**
  - [section] Theorem 3, Item 3: "...computing SHAP is efficiently tractable... so long as these parameters [width and sparsity] remain small..."
  - [abstract] "...width — rather than depth — emerges as the primary computational bottleneck..."
  - [corpus] "Approximation Theory of Tree Tensor Networks" indirectly supports the idea that structure (like width/rank) dictates tractability.

## Foundational Learning

- **Concept:** **Tensor Trains (TT / Matrix Product States)**
  - **Why needed here:** This is the specific architecture for which the paper proves efficient parallel SHAP computation. Without understanding the linear chain structure of cores, the "NC complexity" result is opaque.
  - **Quick check question:** Can you explain why a linear chain of tensor contractions is easier to parallelize than a general graph contraction?

- **Concept:** **Parameterized Complexity (FPT vs. XP vs. para-NP)**
  - **Why needed here:** Crucial for interpreting the BNN results. The paper claims tractability not by saying "it's fast," but by classifying the problem based on structural parameters (width/depth).
  - **Quick check question:** If a problem is FPT with respect to width, what happens to the runtime if I increase depth but keep width constant?

- **Concept:** **The "Coalitional" Game in SHAP**
  - **Why needed here:** To understand why the "Weighted Coalitional Tensor" is necessary. It encodes the exponential number of feature subsets (coalitions) that usually makes SHAP expensive.
  - **Quick check question:** Why does the standard SHAP calculation require evaluating $2^N$ subsets, and how does a tensor representation avoid explicitly listing them?

## Architecture Onboarding

- **Component map:**
  Input Layer (Model $T_M$ + Distribution $T_P$) -> Tensorizer ($\tilde{W}$ + $V$) -> Contractor (TT Contraction) -> Interpreter (Extract SHAP Matrix $\Phi$)

- **Critical path:**
  The conversion of the input model into a **Tensor Train (TT)**. If the model cannot be reduced to a TT (or a product of LDFAs for BNNs) with low rank, the complexity guarantees fail. For BNNs, the *compilation* step (converting layers to TTs) is the bottleneck.

- **Design tradeoffs:**
  - **Exactness vs. Generality:** You get exact, parallel SHAP, but only for models that fit the TT structure (or BNNs with constrained width).
  - **Space vs. Time:** The parallel algorithm uses $O(n_{in}^5)$ processors to achieve $O(\log^2 n)$ time. It trades sequential time for massive hardware space.

- **Failure signatures:**
  - **Runtime Explosion:** Attempting this on a "dense" generic neural network without BNN constraints or TT structure will fail (Prop 2 says it is #P-Hard).
  - **Memory Overflow:** If BNN width is not fixed, the state space of the equivalent LDFA/TT grows exponentially, causing memory issues during compilation.

- **First 3 experiments:**
  1. **Sanity Check (Tree):** Convert a small Decision Tree into a TT and verify that the parallel SHAP output matches standard TreeSHAP.
  2. **Stress Test (BNN Width):** Compile BNNs of increasing width (fixed depth) to empirically validate the FPT runtime curve (should grow polynomially if width is fixed, exponentially otherwise).
  3. **Parallel Scaling:** Implement the TT contraction on a GPU/parallel hardware and measure the scaling against sequence length $n_{in}$ to verify the poly-logarithmic claim.

## Open Questions the Paper Calls Out
1. Can the complexity results, specifically efficient parallelizability (membership in NC), be extended from Tensor Trains to Tree Tensor Networks (TTNs)?
2. Do the tractability and parallelizability results for Marginal SHAP transfer to other explanation variants, such as Conditional SHAP or other attribution indices?
3. Is width the primary computational bottleneck for exact SHAP computation in non-binarized (standard) neural networks, similar to the findings for BNNs?

## Limitations
- The framework only achieves efficient complexity for Tensor Train structures, limiting applicability to specific model architectures
- Hardware assumptions (PRAM model with $O(n^5)$ processors) are highly idealized compared to current parallel architectures
- The BNN results rely on a reified cardinality representation that may not capture all practical BNN variants

## Confidence
- **High Confidence:** The complexity classification (TT ∈ NC²) and the fixed-parameter tractability results for BNNs are supported by formal proofs in the paper. The tensor formulation of SHAP is mathematically rigorous.
- **Medium Confidence:** The empirical evaluation is limited. While the theoretical framework is sound, practical performance on real-world models and the impact of numerical precision during tensor contractions remain unverified.
- **Low Confidence:** The paper's claims about "provably exact" SHAP for general TNs require careful scrutiny of the tensor representation step, as errors in compiling arbitrary models to TNs could propagate through the framework.

## Next Checks
1. **Empirical Complexity Validation:** Implement the TT contraction algorithm on synthetic models of varying width/depth to empirically verify the predicted complexity classes (polynomial in width, exponential in depth).
2. **Numerical Stability Test:** Evaluate the impact of floating-point precision on SHAP values for large input dimensions and deep TT structures, comparing against high-precision arithmetic.
3. **Cross-Model Consistency:** Verify that the SHAP values computed via the TN framework match ground-truth SHAP (brute-force or specialized algorithms) for a suite of models: small decision trees, linear models, and width-constrained BNNs.