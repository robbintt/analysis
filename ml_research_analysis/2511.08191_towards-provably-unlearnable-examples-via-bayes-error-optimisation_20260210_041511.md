---
ver: rpa2
title: Towards Provably Unlearnable Examples via Bayes Error Optimisation
arxiv_id: '2511.08191'
source_url: https://arxiv.org/abs/2511.08191
tags:
- data
- error
- bayes
- unlearnable
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of protecting user data from\
  \ being exploited in machine learning training, particularly when data is collected\
  \ without explicit consent. The authors propose a method for constructing unlearnable\
  \ examples that are provably harder for models to learn from by systematically maximizing\
  \ the Bayes error\u2014a measure of inherent classification difficulty."
---

# Towards Provably Unlearnable Examples via Bayes Error Optimisation

## Quick Facts
- arXiv ID: 2511.08191
- Source URL: https://arxiv.org/abs/2511.08191
- Reference count: 15
- Primary result: Achieves 28.12% test accuracy on CIFAR-10 (vs 95.12% baseline) by maximizing Bayes error under L∞ constraints

## Executive Summary
This paper addresses data protection in machine learning by proposing a method to construct provably unlearnable examples. The approach systematically maximizes the Bayes error—a theoretical measure of classification difficulty—using projected gradient ascent with norm-bounded constraints. By increasing the irreducible error floor, the method ensures that no classifier can achieve high accuracy on the perturbed distribution, even when unlearnable examples are mixed with clean data. The framework provides theoretical guarantees and maintains effectiveness against adversarial training countermeasures.

## Method Summary
The method constructs unlearnable examples by maximizing an estimated Bayes error under L∞-norm constraints. It uses projected gradient ascent to iteratively increase class overlap in the feature space while maintaining perceptual quality. The Bayes error is estimated via kernel-weighted local class frequency, enabling differentiable optimization. The approach guarantees monotonic increase in Bayes error and remains effective even when unlearnable examples are mixed with clean data. Experimental validation shows significant accuracy drops across CIFAR-10, CIFAR-100, and Tiny ImageNet datasets.

## Key Results
- Test accuracy drops from 95.12% to 28.12% when training on full unlearnable CIFAR-10
- Accuracy decreases from 91.16% to 69.68% when 50% of training data is replaced with unlearnable examples
- Consistently outperforms existing approaches across multiple datasets
- Maintains effectiveness against adversarial training (74.5% accuracy)
- Generalizes well across different model architectures

## Why This Works (Mechanism)

### Mechanism 1: Bayes Error as an Information-Theoretic Barrier
Maximizing Bayes error guarantees no classifier can achieve high accuracy on the perturbed distribution because Bayes error represents the irreducible error floor. By perturbing inputs to increase class overlap in the feature space, the method raises this theoretical ceiling, making effective learning impossible regardless of model capacity or training algorithm.

### Mechanism 2: Local Posterior Averaging for Differentiable Estimation
Bayes error can be estimated from finite samples via kernel-weighted local class frequency, enabling gradient-based optimization. For each sample x, class posteriors are estimated by averaging labels of nearby points using a similarity kernel (e.g., RBF). The resulting Bayes error estimate is differentiable with respect to input positions, allowing gradient ascent to push points toward class boundaries.

### Mechanism 3: Projected Gradient Ascent with Norm Constraints
Constrained optimization guarantees monotonic Bayes error increase while maintaining perceptual quality. Standard gradient ascent followed by projection onto the Lp-norm ball ensures perturbations remain imperceptible. The objective function has κ-Lipschitz gradients, enabling convergence guarantees with appropriate step size.

## Foundational Learning

**Bayes Error Rate**
- Why needed here: This is the core theoretical object being optimized; understanding it as the irreducible minimum error of any classifier is essential.
- Quick check question: Given a binary classification problem where P(y=0|x) = 0.6 in some region, what is the Bayes-optimal error contribution from that region?

**Projected Gradient Methods**
- Why needed here: The algorithm uses projection to enforce Lp constraints; understanding how projection works is necessary for implementation.
- Quick check question: If a gradient step takes you outside the L∞ ball of radius ε, what is the closed-form projection back?

**Kernel Density Estimation / Nadaraya-Watson Estimator**
- Why needed here: The local posterior averaging is a form of kernel regression; understanding bias-variance tradeoffs in bandwidth selection matters.
- Quick check question: As kernel bandwidth σ → 0 with fixed n, what happens to the variance of the posterior estimate?

## Architecture Onboarding

**Component map:**
Raw samples {(xᵢ, yᵢ)} with perturbation budget ε -> Feature extractor m: X → X_emb (e.g., ResNet-18) -> RBF similarity kernel s(x, x') = exp(-||x - x'||²/2σ²) computed in embedding space -> Posterior estimator (Algorithm 1) computes p̂(y|x) via weighted neighbor averaging -> Bayes error objective: β̂ = 1 - (1/n) Σᵢ max_c p̂(y=c|xᵢ) -> Projected gradient ascent (Algorithm 2) with step size η -> Lp-norm ball projection, typically L∞ with ε = 8/255

**Critical path:**
1. Extract embeddings for all samples (forward pass through frozen feature extractor)
2. Compute pairwise similarity matrix S_ij = s(m(xᵢ), m(xⱼ))
3. Estimate posteriors via Algorithm 1 (matrix operations, no learning)
4. Compute Bayes error objective and gradients w.r.t. input pixels
5. Update perturbations: δ ← δ + η·∇δ
6. Project: δ ← clip(δ, -ε, +ε) for L∞ constraint
7. Repeat for T iterations (typically T ≈ 50-100)

**Design tradeoffs:**
- Embedding choice: Fixed ResNet-18 provides transferability but limits effectiveness against architectures with different inductive biases. Consider ensemble embeddings if target model is unknown.
- Kernel bandwidth σ: Large σ smooths posteriors (stable gradients, coarse optimization); small σ captures local structure (noisy gradients, fine-grained perturbations).
- Perturbation budget ε: Larger ε increases effectiveness but risks perceptibility. Paper uses 8/255; face protection may require smaller bounds.
- Batch processing: Full pairwise similarity is O(n²). For large datasets, approximate nearest neighbors or mini-batch optimization is necessary.

**Failure signatures:**
- Test accuracy unexpectedly high (>80% on CIFAR-10): Check that perturbations are actually being applied; verify ε is not being reset during projection.
- Gradient explosion/NaN: Kernel bandwidth too small for embedding dimension; normalize embeddings or increase σ.
- Slow convergence: Step size η may be too conservative; try adaptive scheduling or increase if loss plateaus.
- Effective against clean training but not adversarial training: Expected behavior per Figure 6; consider increasing perturbation budget or combining with orthogonal defenses.
- Works on ResNet but not on transformer architectures: Embedding mismatch; consider using a transformer-based feature extractor.

**First 3 experiments:**
1. Toy validation: Replicate the Moons dataset experiment (Figure 2) with n=200 points and ε=0.15. Target: verify Bayes error increases by ~25-30% as reported. If not, debug kernel bandwidth and projection logic.
2. CIFAR-10 full set: Generate unlearnable examples for the full training set using ResNet-18 embeddings with ε=8/255. Train a fresh ResNet-18 on perturbed data for 100 epochs. Target: test accuracy ≤30% (paper achieves 28.12%). If >40%, check that gradient updates are flowing correctly.
3. Mixed-data scenario: Replace 50% of CIFAR-10 training data with unlearnable examples. Train and evaluate. Target: test accuracy ≤70% (paper achieves 69.68%). If clean+unlearnable exceeds clean-only accuracy (91.16%), perturbations are not effective—investigate embedding and kernel settings.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the Bayes error optimization framework be effectively extended to protect data from generative models, such as Generative Adversarial Networks (GANs)?
- Basis in paper: The conclusion explicitly states: "In the future, an interesting direction is to extend our framework to protect data from generative models, such as generative adversarial networks."
- Why unresolved: The current work focuses exclusively on discriminative tasks (classification) where the goal is to maximize the test error, whereas protection against generation involves preventing the synthesis of realistic data.
- What evidence would resolve it: Experiments demonstrating that models trained on protected images fail to generate high-fidelity reconstructions or representative synthetic samples.

**Open Question 2**
- Question: How sensitive is the method's effectiveness to the specific choice of the embedding function (m) used for similarity computation?
- Basis in paper: The method relies on mapping inputs to an embedding space using a fixed feature extractor (ResNet-18) to estimate local posteriors (Section 3.2), but assumes this space adequately represents the attacker's feature view.
- Why unresolved: If the embedding function used for perturbation generation does not align with the feature space of the target model (e.g., a Vision Transformer or different CNN architecture), the computed "unlearnability" in that subspace might not transfer.
- What evidence would resolve it: An ablation study analyzing the transferability of unlearnable examples when constructed using diverse or significantly different embedding functions.

**Open Question 3**
- Question: Can adaptive attacks specifically tailored to reverse the class-overlap objective fully restore model performance?
- Basis in paper: While the paper evaluates PGD-based adversarial training (Section 4.3), which improves accuracy to 74.5%, it leaves open the possibility of stronger countermeasures that explicitly identify and separate the overlapping classes induced by the perturbation.
- Why unresolved: The current defense increases Bayes error by overlapping class features; adaptive attacks might employ techniques to disentangle these specific perturbations more effectively than generic adversarial training.
- What evidence would resolve it: Evaluation against white-box adaptive attacks where the attacker has knowledge of the Bayes error maximization objective and designs specific defenses against it.

## Limitations

- The theoretical foundation assumes the Bayes error estimate is accurate enough to guide optimization, but kernel-based estimation in high-dimensional spaces may suffer from the curse of dimensionality, potentially weakening the connection between the estimated and true Bayes error.
- The use of a fixed feature extractor (ResNet-18) limits transferability guarantees across different model architectures, and the effectiveness against adaptive defenses beyond adversarial training remains untested.

## Confidence

**High Confidence**: The core algorithmic framework (projected gradient ascent for Bayes error maximization) is sound and well-specified. Experimental results showing dramatic accuracy drops (28.12% vs 95.12% on full CIFAR-10) are convincing and reproducible.

**Medium Confidence**: The theoretical guarantees (Theorem 3.4) depend on Lipschitz continuity assumptions that may not hold precisely in practice. The choice of hyperparameters (σ, η, T) significantly affects performance but lacks systematic tuning guidance.

**Low Confidence**: Claims about "provable" unlearnability rest on the assumption that the estimated Bayes error closely tracks the true Bayes error, which is not empirically validated across diverse datasets and embedding spaces.

## Next Checks

1. **Kernel Bandwidth Sensitivity**: Systematically vary σ across 2-3 orders of magnitude and measure the correlation between estimated and actual test accuracy degradation on CIFAR-10.

2. **Embedding Transferability**: Generate unlearnable examples using ResNet-50 embeddings and test against ResNet-18, ConvNeXt, and ViT architectures to quantify transferability claims.

3. **Adaptive Defense Evaluation**: Test against gradient masking defenses (e.g., randomized smoothing) and input transformation defenses (e.g., JPEG compression) to assess robustness claims beyond adversarial training.