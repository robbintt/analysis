---
ver: rpa2
title: Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs
arxiv_id: '2509.13664'
source_url: https://arxiv.org/abs/2509.13664
tags:
- question
- aens
- neurons
- ambiguity
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies a sparse set of neurons that encode question
  ambiguity signals in large language models. Using linear probes, the authors find
  that ambiguity is linearly separable in model activations, often concentrated in
  as few as one neuron per model.
---

# Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs

## Quick Facts
- arXiv ID: 2509.13664
- Source URL: https://arxiv.org/abs/2509.13664
- Reference count: 40
- Sparse neurons encode ambiguity signals with 90-97% linear probe accuracy

## Executive Summary
This work identifies a sparse set of neurons that encode question ambiguity signals in large language models. Using linear probes, the authors find that ambiguity is linearly separable in model activations, often concentrated in as few as one neuron per model. These "Ambiguity-Encoding Neurons" (AENs) generalize across datasets and models, and outperform prompting-based baselines in ambiguity detection. Activation steering on AENs enables causal control over model behavior, shifting responses from direct answers to abstentions. Layerwise analysis shows AENs emerge in early transformer layers.

## Method Summary
The method extracts mean-pooled hidden states at layer 14 during the pre-filling stage, trains logistic regression probes to distinguish ambiguous from clear questions, identifies top neurons by probe weight magnitude, validates sparsity through noise injection, and constructs steering vectors via contrastive PCA on abstention vs. answering examples. The approach achieves strong empirical results with high probe accuracy and efficient abstention control.

## Key Results
- Linear probes achieve 90-97% accuracy in detecting ambiguity across models and datasets
- As few as 1-3 neurons carry the majority of ambiguity signal (AENs)
- AEN-based activation steering achieves 11.6-52.0% abstention rate with 10-100× per-neuron efficiency
- Cross-dataset transfer maintains high accuracy (>85%) when training on AmbigQA and testing on SituatedQA

## Why This Works (Mechanism)

### Mechanism 1: Linear Separability of Ambiguity in Hidden States
Question ambiguity is linearly decodable from mean-pooled hidden states during the pre-filling stage. A logistic regression probe trained on hidden states distinguishes ambiguous from clear questions with 90-97% accuracy. The probe weight magnitudes rank neurons by salience for ambiguity. If ambiguity were encoded non-linearly or distributed across attention patterns rather than MLP activations, linear probes would fail.

### Mechanism 2: Sparse Concentration in Ambiguity-Encoding Neurons (AENs)
The ambiguity signal concentrates in 1-3 neurons, not diffusely across dimensions. Noise injection into top-k neurons (ranked by probe weight) causes sharp accuracy drops. Retraining probes on only these AENs achieves near-full accuracy. If importance were distributed or probe weights captured spurious correlations, AENs would not survive noise perturbation or generalize cross-dataset.

### Mechanism 3: Causal Behavioral Control via Activation Steering
Steering AEN activations shifts output from direct answering to abstention (and reverse). Contrastive PCA extracts a steering direction from abstention vs. answering examples. Adding this direction to AENs (masked) modifies behavior with 10-100× per-neuron efficiency vs. full-vector steering. If AENs encoded correlates rather than causes of ambiguity, steering would produce inconsistent or side-effect-laden outputs.

## Foundational Learning

- **Linear Probing**: Core method to locate where ambiguity resides in activations. Quick check: Can you explain why high probe accuracy implies linear separability but not necessarily causality?
- **Activation Steering**: Intervention technique to test causal influence of AENs on behavior. Quick check: How does activation steering differ from weight editing in terms of reversibility and interpretability?
- **Sparse Feature Encoding**: Explains why few neurons can carry concept-level signals. Quick check: Why might sparsity emerge more in larger models, and how does this relate to AEN identification?

## Architecture Onboarding

- **Component map**: Input prompt → Transformer layers → Mean-pooled hidden states at layer ℓ → Linear probe → AEN identification → Steering vector → Masked activation addition at inference
- **Critical path**: 1. Extract hidden states at target layer (default: layer 14) 2. Train probe on ambiguous vs. clear labels 3. Identify top-k neurons by |wᵢ| 4. Validate via noise injection 5. Construct steering direction from behavioral contrast sets 6. Apply masked steering during inference
- **Design tradeoffs**: Layer choice: Early layers (2-5) show ambiguity emergence; deeper layers may mix with other features. k selection: Smaller k improves specificity but may miss signal; paper finds 1-3 optimal. Steering strength α: Higher α increases effect but risks incoherent outputs
- **Failure signatures**: Probe overfitting: High training accuracy but poor cross-dataset transfer. Steering side effects: Disruption of non-target behaviors (tested via abstention consistency on TriviaQA; >88% accuracy maintained). False positives: Misclassifying clear questions as ambiguous (Table 4 shows <12% error rate)
- **First 3 experiments**: 1. Replicate probe training on AmbigQA: Train logistic regression on layer 14 hidden states, report accuracy and identify top-5 neurons by |wᵢ| 2. Validate AEN sparsity: Inject noise into top-1, -3, -5 neurons and measure accuracy drop; confirm sharp degradation 3. Test cross-dataset transfer: Train AEN probe on AmbigQA, evaluate on SituatedQA; target >85% accuracy to match paper results

## Open Questions the Paper Calls Out

### Open Question 1
How does ambiguity arise at the token level within a question, and how does it influence model uncertainty during generation? The conclusion states: "Looking ahead, an important direction for future work is to extend this analysis to the token level to see how ambiguity arises within a question and how it influences model uncertainty." The current study uses mean-pooled hidden states over the full sequence, masking which specific tokens or spans trigger ambiguity detection.

### Open Question 2
Do Ambiguity-Encoding Neurons emerge during pre-training or instruction tuning, and are they universal across model families? The study only examines instruction-tuned models (LLaMA 3.1 8B Instruct, Mistral 7B Instruct v0.3, Gemma 7B IT), leaving open whether AENs exist in base models and whether they are training-objective dependent.

### Open Question 3
Can AEN-based steering be reliably deployed in real-world systems given sensitivity to prompts and domain shifts? The limitations section states: "its application in real-world systems remains constrained by prompt sensitivity, domain transferability, and the need for reliable neuron identification across models." Experiments use controlled dataset splits; robustness under distribution shift, adversarial prompts, or multi-turn interactions remains untested.

## Limitations

- Sparsity claims depend on linear probe weights as importance proxies, which may capture dataset-specific correlations rather than true causal relevance
- Cross-dataset generalization demonstrated only on two datasets (AmbigQA to SituatedQA)
- Steering intervention may conflate ambiguity with other factors like answer difficulty or model uncertainty calibration

## Confidence

- **High Confidence**: Linear separability of ambiguity (90-97% probe accuracy across models and datasets); sparsity validation via noise injection; abstention steering achieving 11.6-52.0% rate
- **Medium Confidence**: Cross-dataset AEN generalization (tested on two datasets only); causal claims from steering (behavioral confounds possible); layer 14 as optimal for ambiguity emergence
- **Low Confidence**: Claims about "first" neurons encoding ambiguity (no systematic layer-by-layer search beyond layer 14); universality across all LLM architectures (tested on three models only)

## Next Checks

1. **Ablation of Layer Selection**: Systematically test probe accuracy across all transformer layers (1-32) to confirm layer 14 is optimal for ambiguity emergence and identify if earlier layers show stronger sparse encoding
2. **Negative Control Dataset**: Apply the AEN identification pipeline to a dataset with clear questions but other confounding factors (e.g., factual vs. opinion questions) to test whether AENs capture ambiguity specifically versus general uncertainty
3. **Counterfactual Steering Test**: Apply steering to questions known to have single, unambiguous answers to verify that abstention rates remain low (<10%) and that steering doesn't cause excessive false positives