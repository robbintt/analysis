---
ver: rpa2
title: Benchmarking machine learning models for multi-class state recognition in double
  quantum dot data
arxiv_id: '2511.22451'
source_url: https://arxiv.org/abs/2511.22451
tags:
- data
- normalization
- quantum
- training
- experimental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks four modern machine learning architectures
  - convolutional neural networks (CNNs), U-Nets, vision transformers (ViTs), and
  mixture density networks (MDNs) - for multi-class state recognition in double quantum
  dot charge stability diagrams. The study systematically evaluates these models across
  different data budgets (25-100% of training data) and normalization schemes (min-max
  scaling and z-score normalization) using both synthetic and experimental datasets.
---

# Benchmarking machine learning models for multi-class state recognition in double quantum dot data

## Quick Facts
- **arXiv ID:** 2511.22451
- **Source URL:** https://arxiv.org/abs/2511.22451
- **Reference count:** 40
- **Primary result:** CNNs with min-max normalization provide the best practical solution for experimental double quantum dot state recognition

## Executive Summary
This paper presents a systematic benchmark of four modern machine learning architectures - convolutional neural networks, U-Nets, vision transformers, and mixture density networks - for classifying charge states in double quantum dot systems. The study evaluates these models across different training data budgets (25-100%) and normalization schemes (min-max scaling and z-score normalization) using both synthetic and experimental datasets. The authors find that U-Nets achieve the highest accuracy on synthetic data but fail to generalize to experimental measurements, while CNNs provide the most robust performance for real-world quantum dot characterization with significantly fewer parameters than competing approaches.

## Method Summary
The authors created a comprehensive benchmark framework comparing four ML architectures on double quantum dot charge stability diagrams. They generated synthetic datasets with controlled parameters and collected experimental data from actual quantum dot devices. Models were trained and evaluated across multiple data budget scenarios (25%, 50%, 75%, 100%) and with two normalization schemes. Performance was measured using mean squared error and accuracy metrics, with particular attention to computational efficiency and training stability. The experimental design allowed for systematic comparison of how different architectures handle the challenges of real quantum dot data versus idealized synthetic representations.

## Key Results
- U-Nets achieved the highest performance on synthetic data with MSE scores over 0.98 but failed to generalize to experimental data
- CNNs with min-max normalization provided the best practical solution for experimental datasets, using two orders of magnitude fewer parameters than U-Nets and ViTs
- Normalization choice significantly impacted model performance, with min-max scaling generally yielding higher MSE scores but less stable convergence compared to z-score normalization
- MDNs offered the most stable training with the lowest computational costs but achieved lower peak performance than other architectures

## Why This Works (Mechanism)
The superior performance of CNNs on experimental data stems from their ability to capture local spatial features in charge stability diagrams while maintaining computational efficiency. The convolution operations effectively identify charge transition lines and state boundaries without requiring the extensive parameter count of U-Nets or ViTs. Min-max normalization proves particularly effective because it preserves the relative intensity relationships in the experimental data that encode physical information about charge configurations. The stable training dynamics of MDNs arise from their probabilistic framework, which naturally handles the uncertainty inherent in quantum dot measurements.

## Foundational Learning

**Convolutional neural networks** - Why needed: Extract spatial features from 2D charge stability diagrams; Quick check: Verify receptive field captures characteristic length scales of charge transitions

**U-Net architecture** - Why needed: Handle multi-scale features through encoder-decoder structure with skip connections; Quick check: Confirm skip connections preserve spatial resolution information

**Vision transformers** - Why needed: Capture long-range spatial dependencies through self-attention mechanisms; Quick check: Validate attention maps focus on relevant charge state boundaries

**Mixture density networks** - Why needed: Model uncertainty in quantum dot measurements through probabilistic outputs; Quick check: Ensure mixture components align with distinct charge states

## Architecture Onboarding

**Component map:** Raw charge stability diagram → Normalization → ML model (CNN/U-Net/ViT/MDN) → State classification output

**Critical path:** Input normalization → Feature extraction (convolutions/attention) → Classification head → State prediction

**Design tradeoffs:** U-Nets offer highest synthetic accuracy but fail experimentally; CNNs balance accuracy and efficiency; ViTs capture global patterns but require more parameters; MDNs prioritize stability over peak performance

**Failure signatures:** U-Net overfitting to synthetic patterns, ViT sensitivity to normalization, CNN underfitting with insufficient data, MDN poor peak accuracy

**First experiments:** 1) Test CNN-min-max on independent experimental dataset; 2) Compare attention visualization in ViT vs CNN feature maps; 3) Evaluate MDN uncertainty calibration on noisy measurements

## Open Questions the Paper Calls Out

None

## Limitations

- Limited scope to four specific model families without exploring other potentially relevant architectures like graph neural networks
- Hyperparameter tuning performed within constrained ranges, potentially leaving performance gains unrealized
- Computational cost analysis lacks comprehensive benchmarking across different hardware configurations and inference-time efficiency evaluation

## Confidence

**High:** Systematic comparison methodology and relative performance rankings on both synthetic and experimental datasets
**Medium:** Generalizability to other quantum dot systems or related quantum device characterization tasks
**Low:** Absolute performance metrics due to specific synthetic data generation and experimental conditions

## Next Checks

1. Test the proposed CNN-min-max normalization approach on double quantum dot datasets from at least three independent experimental groups to assess cross-laboratory reproducibility and robustness.
2. Implement and benchmark additional architectures including graph neural networks and attention-based spatial models specifically designed for lattice-structured data to determine if performance can be further improved beyond the current best approaches.
3. Conduct comprehensive inference-time benchmarking on edge computing hardware representative of typical laboratory control systems to quantify the practical deployment feasibility of the highest-performing models identified in this study.