---
ver: rpa2
title: Towards Formalizing Reinforcement Learning Theory
arxiv_id: '2511.03618'
source_url: https://arxiv.org/abs/2511.03618
tags:
- learning
- convergence
- theorem
- linear
- almost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the almost sure convergence of Q-learning
  and linear TD learning with Markovian samples using the Lean 4 theorem prover. The
  work addresses the challenge of verifying convergence proofs in reinforcement learning,
  which are often delicate due to their reliance on ODE-based approaches and the need
  for rigorous treatment of Markov Decision Process frameworks.
---

# Towards Formalizing Reinforcement Learning Theory

## Quick Facts
- arXiv ID: 2511.03618
- Source URL: https://arxiv.org/abs/2511.03618
- Authors: Shangtong Zhang
- Reference count: 4
- Formalizes almost sure convergence of Q-learning and linear TD learning with Markovian samples in Lean 4 theorem prover

## Executive Summary
This paper formalizes the almost sure convergence of Q-learning and linear TD learning with Markovian samples using the Lean 4 theorem prover. The work addresses the challenge of verifying convergence proofs in reinforcement learning, which are often delicate due to their reliance on ODE-based approaches and the need for rigorous treatment of Markov Decision Process frameworks. The paper develops a unified framework based on the Robbins-Siegmund theorem to formalize the convergence of these algorithms.

The formalization proves that Q-learning and linear TD learning converge almost surely under specific conditions. For linear TD, it shows convergence with step sizes of the form 1/(t+2)^ν where ν ∈ (2/3, 1), and for Q-learning, it establishes similar convergence guarantees. The work represents the first formalization of these results and provides a foundation for further formalization of convergent RL results.

## Method Summary
The paper formalizes RL convergence proofs in Lean 4 using a Robbins-Siegmund theorem framework combined with skeleton iterates techniques. The approach converts Markovian noise to Martingale difference noise, enabling formalization of almost sure convergence, concentration bounds, Lp convergence, and convergence rates. The work proves convergence for linear TD and Q-learning under specific step size conditions, using Lyapunov functions with appropriate norms to establish contraction toward fixed points.

## Key Results
- Formal verification of almost sure convergence for linear TD learning and Q-learning with Markovian samples
- First formalization of RL convergence results using the Robbins-Siegmund theorem framework
- Demonstration that step sizes αt = 1/(t+2)^ν with ν ∈ (2/3, 1) suffice for convergence
- Development of skeleton iterates technique to handle Markovian noise in formal proofs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Robbins-Siegmund theorem provides a unified framework for proving almost sure convergence of stochastic iterates.
- Mechanism: When iterates satisfy an "almost supermartingale" condition (E[zn+1|Fn] ≤ (1-Tn)zn + CT²n), and step sizes satisfy Robbins-Monro conditions, the theorem guarantees limn→∞zn = 0 almost surely.
- Core assumption: The noise term e1,n+1 must be a Martingale difference sequence (E[e1,n+1|Fn] = 0 a.s.).
- Evidence anchors:
  - [abstract] "This paper formally verifies their almost sure convergence in a unified framework based on the Robbins-Siegmund theorem."
  - [section 4] Theorem 4.6 formalizes the special case of Robbins-Siegmund used.
  - [corpus] "Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning" (FMR=0.615) confirms relevance.
- Break condition: If the Martingale difference property fails (e.g., time-inhomogeneous chains), the theorem cannot be directly applied.

### Mechanism 2
- Claim: Skeleton iterates convert Markovian noise into Martingale difference noise, enabling use of Robbins-Siegmund.
- Mechanism: Define deterministic "anchor" times {tm} with tm → ∞. Between anchors, the cumulative Markovian error splits into: (1) a Martingale difference term (amenable to Robbins-Siegmund), and (2) a smaller bias term (O(α²n)) that doesn't require Martingale properties.
- Core assumption: Anchors must satisfy αtm ≤ Cβ²m where βm = Σ αt. This forces ν ∈ (2/3, 1) for step sizes αt = 1/(t+2)^ν.
- Evidence anchors:
  - [abstract] "The approach uses skeleton iterates techniques to convert Markovian noise to Martingale difference noise."
  - [section 4, p.7-8] Detailed derivation showing how e1,n+1 captures Martingale differences and e2,n+1 captures smaller bias.
  - [corpus] Weak direct evidence on skeleton iterates; this is a specialized technique from Qian et al. (2024).
- Break condition: For ν ≤ 2/3, the anchor condition fails, requiring either extended analysis or ODE-based methods.

### Mechanism 3
- Claim: Lyapunov functions with appropriate norms establish contraction toward fixed points.
- Mechanism: For Linear TD, ϕ(w) = ½||w-w*||²₂ works because the matrix A is negative definite. For Q-learning, ϕ(q) = ½||q-q*||²_p with large p works because the weighted Bellman operator is a pseudo-contraction in ||·||∞.
- Core assumption: The stationary distribution exists and is unique (irreducibility + aperiodicity ensure Doeblin minorization).
- Evidence anchors:
  - [section 4, p.6] Assumption 4.2(iv) requires ⟨∇ϕ(x-x*), f(x)-x⟩ ≤ -ηϕ(x-x*).
  - [section 4, p.6] Explicit computation showing η = (1 - γ'|S||A|^(1/p)) > 0 for Q-learning with large p.
  - [corpus] "Linear Q-Learning Does Not Diverge in L²" (FMR=0.0, weak signal) touches on related Lyapunov analysis.
- Break condition: If the Markov chain is periodic or reducible, stationary distribution uniqueness fails.

## Foundational Learning

- **Concept: Martingales and Filtrations**
  - Why needed here: The core proof requires showing certain error terms are Martingale difference sequences relative to a filtration.
  - Quick check question: Given a stochastic process {Xn} adapted to filtration {Fn}, can you explain why E[Xn+1 - Xn | Fn] = 0 defines a Martingale difference?

- **Concept: Measure-Theoretic Conditional Expectation**
  - Why needed here: Lean defines conditional expectation abstractly via σ-algebras; the paper notes this took ~1,000 lines (~10% of project) to formalize properly.
  - Quick check question: Why is E[X|G] defined as the unique G-measurable function satisfying ∫_B E[X|G] dμ = ∫_B X dμ for all B ∈ G?

- **Concept: Ionescu-Tulcea Theorem**
  - Why needed here: Constructs probability measures on infinite trajectory spaces from transition kernels; essential for formalizing MDP sample paths.
  - Quick check question: How does this theorem extend finite-horizon transition probabilities to infinite-horizon sample path measures?

## Architecture Onboarding

- **Component map:**
  - MarkovChain/ -> SamplePath/ -> Iterates/ -> Convergence/

- **Critical path:**
  1. Define MDP and Markov chain structures → 2. Construct sample path probability space via Ionescu-Tulcea → 3. Define algorithm iterates as measurable functions → 4. Prove Martingale difference property for noise terms → 5. Apply Robbins-Siegmund → 6. Establish Lyapunov contraction

- **Design tradeoffs:**
  - ODE-based methods (traditional) vs. Robbins-Siegmund (chosen): ODE methods have limited Mathlib support; Robbins-Siegmund is more tractable but restricts step sizes to ν ∈ (2/3, 1).
  - Counter-based step sizes vs. global step sizes: Counter-based (historical Q-learning) would allow Dvoretzky's theorem, but practitioners use global; global requires proving ∑αt I(s,a)=(St,At) = ∞ a.s.

- **Failure signatures:**
  - `measurability` tactic fails → Likely missing explicit `Measurable` instances for custom functions
  - `integrability` goals unsolved → May need to prove boundedness of iterates first
  - Conditional expectation computation stuck → Requires unfolding the Ionescu-Tulcea construction explicitly

- **First 3 experiments:**
  1. **Build and browse**: Clone the repo, build with Lake, and locate `ae_tendsto_of_linearTD_markov` in the source. Trace its dependencies to understand the proof skeleton.
  2. **Mini-formalization**: Prove a simplified version of the Robbins-Siegmund theorem for a 1-dimensional case to practice Martingale reasoning in Lean.
  3. **Step size extension**: Attempt to relax ν ∈ (2/3, 1) to ν ∈ (1/2, 1] by adjusting anchor definitions; document where the proof breaks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Robbins-Siegmund framework be extended to formalize almost sure convergence of Q-learning and linear TD for step sizes with ν ∈ (1/2, 2/3] under Markovian samples?
- Basis in paper: [explicit] "For ν∈(1/2, 2/3], we need to either extend the results of Qian et al. (2024) or resort to the canonical ODE based approach"
- Why unresolved: The current formalization restricts ν ∈ (2/3, 1) due to the anchor condition α_tm ≤ Cβ²_m; extending requires new proof techniques or formalizing ODE-based approaches.
- What evidence would resolve it: A formal proof in Lean covering the extended step size range, either via extended skeleton iterates or formalized ODE methods.

### Open Question 2
- Question: Can concentration bounds and Lp convergence for Q-learning and linear TD be formalized using the developed framework?
- Basis in paper: [explicit] "Both should be straightforward if we can formalize Hoeffding's lemma"
- Why unresolved: Formalizing Hoeffding's lemma in Mathlib is a prerequisite that has not yet been completed.
- What evidence would resolve it: A formalization of Hoeffding's lemma in Mathlib, followed by concentration/Lp convergence proofs for the RL algorithms.

### Open Question 3
- Question: What modifications to the framework are needed to formalize convergence of algorithms involving time-inhomogeneous Markov chains (e.g., SARSA, policy gradient)?
- Basis in paper: [explicit] "For those algorithms, the Markov chain is deeply coupled with the iterates and we envision major updates of our framework are necessary"
- Why unresolved: Current framework assumes fixed transition kernels; time-inhomogeneous chains create coupling between iterates and dynamics that breaks existing proof structures.
- What evidence would resolve it: A formalized convergence proof for SARSA or policy gradient with explicit framework modifications documented.

### Open Question 4
- Question: Can LLMs independently complete formalization projects of this complexity without significant human guidance?
- Basis in paper: [explicit] "Can LLM complete this project alone or with little help from humans? Our answer is negative as of Nov 2025."
- Why unresolved: Empirical question about AI capabilities; current evidence shows LLMs struggle with subgoal completion even when problems are decomposed.
- What evidence would resolve it: An LLM or agent system completing a comparable formalization project (e.g., convergence of gradient TD methods) with minimal human intervention.

## Limitations
- The framework restricts step sizes to ν ∈ (2/3, 1), excluding broader regimes achievable with ODE-based methods.
- Skeleton iterates technique, while effective, is specialized with limited external validation beyond cited work.
- Current framework assumes fixed transition kernels and cannot handle time-inhomogeneous Markov chains.

## Confidence
- **High confidence**: The formal proofs of almost sure convergence for both algorithms under the stated conditions, as verified in Lean 4 with ~10,000 lines of code and successful compilation.
- **Medium confidence**: The correctness of the skeleton iterates technique and its application to RL convergence proofs, given the specialized nature of the method and limited external validation.
- **Medium confidence**: The Robbins-Siegmund theorem's applicability to this RL setting, though this is a well-established result in stochastic approximation theory.

## Next Checks
1. **Build verification**: Clone the repository and successfully compile all Lean 4 files to confirm the formal proofs are executable and error-free.
2. **Step size boundary testing**: Attempt to extend the formal proofs to step sizes with ν ∈ (1/2, 2/3] to identify exactly where and why the current proof breaks.
3. **Manual proof audit**: Have an external expert verify the skeleton iterates construction and the conversion from Markovian to Martingale noise, focusing on the critical anchor condition derivation.