---
ver: rpa2
title: How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined
  Concepts
arxiv_id: '2507.18479'
source_url: https://arxiv.org/abs/2507.18479
tags:
- prerequisite
- llms
- skill
- zero-shot
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ESCO-PrereqSkill, a new benchmark dataset
  of 3,196 skills with expert-defined prerequisite links derived from the ESCO taxonomy.
  It investigates whether large language models (LLMs) can predict prerequisite skills
  in a zero-shot setting, using only skill descriptions and without fine-tuning.
---

# How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined Concepts

## Quick Facts
- arXiv ID: 2507.18479
- Source URL: https://arxiv.org/abs/2507.18479
- Reference count: 30
- Primary result: 13 LLMs achieved F1BERT scores above 0.82 on zero-shot prerequisite prediction, matching expert-defined skill dependencies

## Executive Summary
This paper introduces ESCO-PrereqSkill, a benchmark dataset of 3,196 skills with expert-defined prerequisite links, and evaluates whether large language models can predict prerequisite skills in a zero-shot setting. Thirteen state-of-the-art models were assessed using standardized prompts and semantic evaluation metrics. Results show that open models like LLaMA4-Maverick and proprietary models like Claude-3-7-Sonnet achieved comparable performance, with F1BERT scores above 0.82. The findings demonstrate that LLMs can effectively infer prerequisite relationships from natural language alone, supporting scalable AI-driven educational tools.

## Method Summary
The study evaluates 13 LLMs on the ESCO-PrereqSkill benchmark using zero-shot prompting. For each skill, models receive a standardized prompt containing context, instruction, input text, and formatting constraints, then generate comma-separated prerequisite lists. Responses are parsed, normalized, and evaluated using semantic similarity (Sentence-BERT cosine similarity), BERTScore (Precision, Recall, F1), and inference latency. No fine-tuning or few-shot examples are used. The dataset and evaluation code are publicly available.

## Key Results
- LLaMA4-Maverick, Claude-3-7-Sonnet, and Qwen2-72B achieved F1BERT scores above 0.82
- All models showed higher recall than precision, indicating over-generation of prerequisites
- DeepSeek-R1 achieved competitive F1BERT (0.8136) but had high latency variance (0.3s to 135.6s)
- Open models matched or exceeded proprietary models on semantic quality metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs encode latent hierarchical knowledge about skill dependencies through pre-training on educational and technical corpora.
- **Mechanism:** During pre-training on web-scale data—including instructional materials, technical documentation, and educational discourse—models implicitly absorb patterns of how concepts build upon one another. When prompted, this latent structure surfaces as prerequisite predictions without explicit supervision.
- **Core assumption:** The pre-training corpora contain sufficient pedagogical structure for models to internalize prerequisite relationships (not directly verified in the paper).
- **Evidence anchors:**
  - [abstract]: "LLMs can effectively infer prerequisite relationships from natural language alone"
  - [section III-B]: "through exposure to extensive textual data—including instructional materials, technical documentation, and educational discourse—LLMs may implicitly learn hierarchical and pedagogical relationships among concepts"
  - [corpus]: Weak direct evidence; related paper "Predicting Language Models' Success at Zero-Shot Probabilistic Prediction" examines zero-shot confidence but not mechanism specifically.
- **Break condition:** If educational content in pre-training data is sparse, noisy, or contradicts expert taxonomies, implicit learning degrades.

### Mechanism 2
- **Claim:** High F1BERT scores (>0.82) indicate models capture semantic equivalence between predicted and expert-defined prerequisites even with lexical variation.
- **Mechanism:** BERTScore uses contextual embeddings to match predicted skills to ground truth at the token level. High recall (0.85+) means most expert prerequisites find semantically similar predictions; precision reflects how many predictions are grounded.
- **Core assumption:** Semantic similarity measured by BERTScore correlates with pedagogical validity (an assumption, not proven).
- **Evidence anchors:**
  - [abstract]: "models like LLaMA4-Maverick, Claude-3-7-Sonnet, and Qwen2-72B achieved F1BERT scores above 0.82"
  - [section V-A]: Table II shows F1BERT ranging 0.81-0.83 for top models; recall consistently higher than precision
  - [corpus]: No direct corpus evidence for BERTScore validity in educational contexts.
- **Break condition:** If BERTScore rewards surface-level lexical overlap without capturing true pedagogical dependency, performance is overstated.

### Mechanism 3
- **Claim:** Standardized prompt framing (context + instruction + input + format) elicits consistent prerequisite reasoning across models.
- **Mechanism:** The prompt explicitly instructs models to adopt an "education expert" persona and constrains output to comma-separated lists, reducing variance and focusing model attention on the task structure.
- **Core assumption:** Prompt design does not artificially inflate scores by priming models toward ESCO-like terminology.
- **Evidence anchors:**
  - [section IV-C]: Figure 3 shows the standardized prompt template with role assignment and formatting constraints
  - [section III-B]: "This information is embedded into a standardized prompt composed of four components"
  - [corpus]: Related paper "TAXREC" suggests taxonomy-guided prompts improve recommendations, supporting prompt importance.
- **Break condition:** If prompt variations significantly change rankings or scores, results may not generalize beyond this specific framing.

## Foundational Learning

- **Concept: Zero-shot generalization**
  - Why needed here: The entire evaluation assumes models can perform prerequisite prediction without task-specific fine-tuning. Understanding what zero-shot means—and its limitations—is essential for interpreting results.
  - Quick check question: Can you explain why a model might succeed at zero-shot prerequisite prediction but fail at zero-shot medical diagnosis?

- **Concept: Semantic vs. lexical evaluation metrics**
  - Why needed here: The paper relies on BERTScore and cosine similarity rather than exact match. Understanding why semantic metrics matter helps assess whether "close enough" predictions are pedagogically valid.
  - Quick check question: If a model predicts "statistics" when the ground truth is "probability theory," would BERTScore capture this as correct? Should it?

- **Concept: Prerequisite relationship modeling**
  - Why needed here: The task assumes skills have hierarchical dependencies. Understanding how prerequisite graphs work (and where they break down) is critical for applying this work.
  - Quick check question: Why might prerequisite relationships be domain-dependent or even learner-dependent?

## Architecture Onboarding

- **Component map:**
  ESCO-PrereqSkill dataset (3,196 skills with expert prerequisites) -> Prompt constructor (context + instruction + input + format) -> LLM API calls (13 models) -> Response parser (normalization) -> Evaluation layer (Simsem, BERTScore) -> Latency tracking

- **Critical path:**
  1. Dataset curation quality (expert-defined ground truth validity)
  2. Prompt design consistency across all model calls
  3. Metric selection—BERTScore parameters and Sentence-BERT model choice affect rankings

- **Design tradeoffs:**
  - Latency vs. quality: DeepSeek-R1 achieves competitive F1BERT (0.8136) but 16s avg latency; Gemini-2.0-Flash is fast (0.3s min) but lower semantic similarity (0.6049)
  - Open vs. proprietary: LLaMA4-Maverick (open) matches/exceeds GPT-4.5 (proprietary) on F1BERT
  - Precision vs. recall: All models show higher recall than precision, suggesting over-generation of prerequisites

- **Failure signatures:**
  - High F1BERT but low Simsem (e.g., Gemini-2.0-Flash: 0.8238 vs. 0.6049) suggests surface lexical matching without deep conceptual alignment
  - Long-tail latency (DeepSeek-R1 max: 135.6s) indicates instability for real-time systems
  - Prompt sensitivity not tested—results may not generalize to alternative framings

- **First 3 experiments:**
  1. **Prompt ablation:** Remove "education expert" persona or formatting constraints; measure impact on F1BERT and output consistency.
  2. **Domain shift test:** Apply same pipeline to a different taxonomy (e.g., ACM Computing Curricula) to test generalization beyond ESCO.
  3. **Human evaluation sample:** For 50-100 randomly selected skills, have educators rate whether predicted prerequisites are pedagogically valid (even if not in ground truth) to assess false negatives in the benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does few-shot prompting with examples from ESCO-PrereqSkill significantly improve prerequisite prediction accuracy compared to zero-shot performance?
- Basis in paper: [explicit] Authors state in conclusion: "Next steps include exploring few-shot settings with the ESCO-PrereqSkill dataset."
- Why unresolved: The study only evaluated zero-shot performance; no few-shot experiments were conducted.
- What evidence would resolve it: Comparative experiments showing F1BERT and semantic similarity scores for 1-shot, 3-shot, and 5-shot prompting on the same benchmark.

### Open Question 2
- Question: How robust are LLM prerequisite predictions to prompt wording variations and instructional framing changes?
- Basis in paper: [explicit] Conclusion lists "evaluating robustness to prompt variation" as a next step.
- Why unresolved: Only one standardized prompt template was used across all models; sensitivity to prompt design remains unknown.
- What evidence would resolve it: Systematic evaluation using multiple prompt variants (different phrasings, role assignments, format specifications) with statistical analysis of score variance.

### Open Question 3
- Question: Can LLMs maintain prerequisite prediction accuracy when applied to domains outside the ESCO taxonomy, such as specialized technical or non-European occupational frameworks?
- Basis in paper: [explicit] Authors mention "evaluating robustness to... domain shifts" as future work.
- Why unresolved: All evaluation used ESCO-derived skills; generalization to other taxonomies (O*NET, ACM curricula) is untested.
- What evidence would resolve it: Cross-taxonomy evaluation showing performance retention when models are tested on prerequisite links from independent expert-curated frameworks.

### Open Question 4
- Question: Do hybrid approaches combining structured ontologies with LLM reasoning outperform pure zero-shot generation on prerequisite prediction?
- Basis in paper: [explicit] Conclusion states: "Future work may explore hybrid approaches that combine structured ontologies with generative reasoning."
- Why unresolved: The study only evaluated pure generative approaches without ontology-augmented methods.
- What evidence would resolve it: Ablation studies comparing retrieval-augmented or ontology-constrained generation against baseline zero-shot predictions on the same test set.

## Limitations

- BERTScore correlation with pedagogical validity is assumed but unproven; the metric may reward lexical similarity over true prerequisite relationships
- Results may reflect ESCO-specific linguistic patterns rather than general prerequisite reasoning ability
- Significant latency variance across models raises practical deployment concerns

## Confidence

**High confidence**: Models achieve consistent semantic similarity and BERTScore performance across the 13 tested LLMs, with open models like LLaMA4-Maverick matching proprietary alternatives. The dataset construction methodology and zero-shot evaluation framework are clearly specified and reproducible.

**Medium confidence**: The claim that models "effectively infer prerequisite relationships" holds for the ESCO taxonomy but may not generalize to other educational domains or skill structures. BERTScore's correlation with pedagogical validity remains unproven.

**Low confidence**: The mechanism by which pre-training corpora encode prerequisite knowledge is assumed rather than empirically verified. The paper does not demonstrate that models understand pedagogical hierarchy versus pattern matching on ESCO-specific terminology.

## Next Checks

1. **Human expert validation study**: Select 100 random skills from ESCO-PrereqSkill and have domain experts rate whether predicted prerequisites (beyond ground truth) are pedagogically valid. This tests whether high F1BERT scores reflect true understanding versus lexical matching.

2. **Prompt sensitivity analysis**: Systematically vary the prompt components (remove "education expert" persona, change formatting instructions, alter context phrasing) and measure impact on F1BERT scores and output consistency across models. This determines whether results generalize beyond the specific framing used.

3. **Cross-taxonomy generalization test**: Apply the same zero-shot pipeline to a completely different skill taxonomy (e.g., IEEE Computing Curricula or medical competency frameworks). Compare performance drops to assess whether models capture general prerequisite reasoning or ESCO-specific patterns.