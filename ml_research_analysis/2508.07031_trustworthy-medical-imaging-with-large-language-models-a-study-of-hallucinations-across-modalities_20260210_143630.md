---
ver: rpa2
title: 'Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations
  Across Modalities'
arxiv_id: '2508.07031'
source_url: https://arxiv.org/abs/2508.07031
tags:
- image
- medical
- hallucinations
- clinical
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates hallucinations in large
  language models (LLMs) across both image-to-text and text-to-image medical imaging
  tasks. The authors evaluate LLMs on detecting pleural effusion from chest X-rays
  and classifying chest CT scans, as well as generating synthetic medical images from
  text prompts.
---

# Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities

## Quick Facts
- arXiv ID: 2508.07031
- Source URL: https://arxiv.org/abs/2508.07031
- Reference count: 31
- Primary result: Systematic evaluation reveals significant hallucination rates in LLMs across medical image interpretation and generation tasks

## Executive Summary
This study systematically investigates hallucinations in large language models (LLMs) across both image-to-text and text-to-image medical imaging tasks. The authors evaluate LLMs on detecting pleural effusion from chest X-rays and classifying chest CT scans, as well as generating synthetic medical images from text prompts. Their findings reveal significant hallucination rates, with models often producing clinically implausible or anatomically incorrect outputs. For example, in pleural effusion detection, models showed error rates ranging from 45.58% to 80.82%, while in image generation, clinically implausible content was produced in 30-94% of attempts depending on prompt phrasing. The study demonstrates that current LLMs are prone to both hallucination and omission errors in medical imaging contexts, highlighting the need for improved safeguards, medically grounded decoding strategies, and rigorous validation before deployment in clinical settings.

## Method Summary
The paper evaluates hallucinations in LLMs across medical imaging tasks using Indiana Chest X-ray dataset for pleural effusion detection and IQ-OTH/NCCD lung cancer CT dataset for classification. The study employs zero-shot and few-shot (5 examples) evaluation using LLaVA-v1.5-7B, Gemma-3B, and Qwen2.5-VL-7B for interpretation tasks, while GPT-4o and Gemini-2.5 Flash are used for generation tasks. Evaluation metrics include accuracy, F1 scores, confusion matrices for diagnostic questions, and success rates of implausible content generation under different prompt variants.

## Key Results
- Pleural effusion detection models showed error rates ranging from 45.58% to 80.82% with F1 scores between 0.115 and 0.48
- GPT-4o's success rate for generating clinically implausible content increased from 66% to 94% when prompts included "research purposes" justification
- Qwen showed 51.5% accuracy but F1=0.37 in zero-shot classification, indicating systematic omission of positive findings
- Models demonstrated asymmetric laterality bias, defaulting to right-sided or left-sided pathology when laterality was unspecified

## Why This Works (Mechanism)

### Mechanism 1: Pattern Completion Without Visual Grounding
LLMs generate outputs through statistical pattern completion from training data rather than by grounding outputs in the actual visual input. The model's learned linguistic and visual associations dominate over faithful input analysis. When encountering medical images, the model retrieves prototypical responses from training rather than verifying each claim against the specific image. Core assumption: Hallucinations arise from the model optimizing for plausible output rather than factually verified output.

### Mechanism 2: Co-occurrence Bias from Training Data
Models inject unprompted clinical elements because training images frequently contain such features as co-occurring artifacts. Medical imaging datasets used for training often include incidental findings, surgical artifacts, or common pathology pairings. The model learns these statistical associations and reproduces them even when not requested. Core assumption: Training data contains latent correlations that the model memorizes and applies indiscriminately.

### Mechanism 3: Contextual Safeguard Evasion via Prompt Rephrasing
Safety filters that block clinically implausible prompts can be bypassed by adding narrative justification or modifying prompt structure. Safety classifiers respond to surface-level prompt features. Adding phrases like "for research purposes" or embedding the implausible request in a narrative context shifts the prompt distribution, evading detection while still producing hallucinated outputs. Core assumption: Safeguards are brittle and do not verify semantic consistency between prompt intent and output plausibility.

## Foundational Learning

- Concept: Hallucination taxonomy in multimodal models
  - Why needed here: The paper categorizes hallucinations differently for interpretation (false positives, omissions) vs. generation (unprompted elements, implausible content). Understanding this distinction is required to interpret the error metrics.
  - Quick check question: Can you distinguish between a "hallucinated finding" in image interpretation and an "unprompted element" in image generation?

- Concept: Zero-shot vs. few-shot evaluation in medical imaging
  - Why needed here: The paper evaluates models under both settings (Table 3), showing that few-shot examples improve but do not eliminate hallucinations.
  - Quick check question: Why might providing 5 labeled examples still fail to prevent a model from hallucinating cancer in a normal CT scan?

- Concept: Clinical plausibility constraints
  - Why needed here: The paper's implausible content tests (e.g., radioulnar joint in abdominal CT) assume readers understand why certain anatomy-modality combinations are impossible.
  - Quick check question: Why is a "chest X-ray showing toe fractures" fundamentally different from a "chest X-ray showing pneumonia"?

## Architecture Onboarding

- Component map: Vision encoder -> Embeddings -> LLM -> Generated report (image-to-text) or Text prompt -> Safety filter -> Image generator -> Synthetic image (text-to-image)
- Critical path: 1. Input image → Vision encoder → Embeddings 2. Embeddings + prompt → LLM → Generated report (image-to-text) 3. Text prompt → Safety filter → Image generator → Synthetic image (text-to-image) 4. Output → Hallucination evaluation (accuracy, F1, expert assessment)
- Design tradeoffs:
  - Zero-shot vs. few-shot: Few-shot reduces some errors but adds inference cost and requires curated examples
  - Strict vs. permissive safety filters: Stricter filters reduce implausible outputs but may block legitimate research use cases
  - Model size vs. deployment feasibility: Larger models (GPT-4o) show different hallucination profiles than smaller open-source models (LLaVA-7B), but clinical deployment favors local inference
- Failure signatures:
  - High false negative rate with low F1: Qwen shows 51.5% accuracy but F1=0.37 (zero-shot), indicating systematic omission of positive findings
  - Asymmetric laterality bias: Models default to right-sided or left-sided pathology when laterality unspecified
  - Prompt-dependent hallucination rate: 66% → 94% implausible generation when adding "research purposes"
- First 3 experiments:
  1. Baseline hallucination quantification: Run pleural effusion detection task on your target model using Indiana Chest X-ray subset; compute accuracy, F1, and confusion matrices
  2. Safeguard robustness test: Submit P1 and P2 prompt variants to your image generation pipeline; measure success rate of implausible content generation
  3. Few-shot ablation: Compare zero-shot vs. 5-shot classification on chest CT cancer detection; quantify whether hallucinations decrease and identify residual error patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can medically grounded decoding strategies effectively mitigate the generation of anatomically implausible content in text-to-image medical synthesis?
- Basis in paper: [explicit] The conclusion explicitly identifies "medically grounded decoding" and "constraint-based generation" as necessary future avenues to address the critical vulnerabilities and anatomical inaccuracies revealed in the study.
- Why unresolved: The current study only evaluates existing models using standard decoding methods; it does not implement or test constrained generation architectures that could enforce anatomical validity.
- What evidence would resolve it: A comparative experiment showing that a model using anatomy-constrained decoding produces significantly lower rates of clinically implausible images compared to standard baselines when given adversarial prompts.

### Open Question 2
- Question: Does specialized medical fine-tuning significantly reduce the high false negative rates observed in general-purpose LLMs for specific detection tasks like pleural effusion?
- Basis in paper: [explicit] The authors list "specialized fine-tuning" as a key direction for future work to improve reliability, noting that current few-shot approaches still suffer from significant hallucination and omission errors.
- Why unresolved: The numerical experiments were limited to zero-shot and few-shot evaluations of general models; the efficacy of domain-specific training on these particular hallucination patterns remains unmeasured.
- What evidence would resolve it: Performance metrics (F1 score, confusion matrices) from a model fine-tuned specifically on chest X-ray datasets when evaluated on the same pleural effusion detection task used in the paper.

### Open Question 3
- Question: How can safety safeguards be designed to remain robust against semantic rephrasing attacks, such as the "research purpose" justification used to bypass content filters?
- Basis in paper: [explicit] The study shows that rephrasing prompts to include a research justification (P2) increases the success rate of implausible generation from 66% to 94% in GPT-4o, leading the authors to call for "stricter safeguards" in response.
- Why unresolved: Current moderation mechanisms appear to rely on heuristics that are easily bypassed by narrative context, a vulnerability demonstrated but not resolved in the current work.
- What evidence would resolve it: The development of a moderation framework that maintains a consistent refusal rate across semantically distinct but intent-equivalent prompts without generating anatomically impossible imagery.

## Limitations
- Findings are limited to specific model architectures and datasets evaluated, which may not generalize across the broader spectrum of medical imaging modalities
- Image generation experiments rely on proprietary APIs that cannot be fully reproduced
- Clinical expert review process for hallucination assessment is not fully specified, introducing potential subjectivity
- Safety bypass results may not apply to different model versions or safety implementations

## Confidence
- **High confidence**: The core finding that LLMs exhibit significant hallucination rates in medical imaging tasks is well-supported by quantitative metrics and systematic evaluation
- **Medium confidence**: The mechanisms explaining hallucination patterns are logically coherent and partially supported by empirical results, but require additional mechanistic validation
- **Low confidence**: The generalizability of specific hallucination rates and failure patterns to other medical imaging contexts, model architectures, or clinical workflows remains uncertain

## Next Checks
1. **Dataset and Task Generalization Test**: Replicate the hallucination evaluation across at least two additional medical imaging datasets (e.g., MIMIC-CXR for chest X-rays, Brain Tumor Segmentation Challenge for MRI) to verify whether observed hallucination patterns hold across different modalities and clinical tasks.

2. **Mechanism-Specific Intervention Validation**: Implement targeted interventions addressing each proposed mechanism (visual grounding for pattern completion, curated training data for co-occurrence bias, semantic consistency checks for safeguard evasion) and measure their impact on hallucination rates using the same evaluation protocol.

3. **Clinical Impact Assessment**: Conduct a prospective study with clinical end-users using the evaluated models in realistic diagnostic workflows to measure not just hallucination rates but also downstream impact on clinical decision-making, diagnostic accuracy, and patient safety outcomes.