---
ver: rpa2
title: LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning
  and Post-correction
arxiv_id: '2510.03577'
source_url: https://arxiv.org/abs/2510.03577
tags:
- dans
- pour
- donn
- entit
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses biomedical named entity recognition (NER)
  and health event extraction in French using large language models (LLMs) under a
  few-shot learning setting. Three approaches are explored: (1) GPT-4.1 with in-context
  learning and annotation guidelines, (2) GLiNER fine-tuned on synthetic data with
  LLM-based post-correction, and (3) LLaMA-3.1-8B-Instruct fine-tuned on synthetic
  data.'
---

# LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction

## Quick Facts
- arXiv ID: 2510.03577
- Source URL: https://arxiv.org/abs/2510.03577
- Reference count: 0
- Primary result: GPT-4.1 with few-shot prompting achieved 61.53% macro-F1 for NER in low-resource French biomedical text

## Executive Summary
This study explores three approaches for biomedical named entity recognition and health event extraction in French under few-shot learning conditions. The authors compare GPT-4.1 with in-context learning, GLiNER fine-tuned on synthetic data with LLM post-correction, and LLaMA-3.1 fine-tuned on synthetic data. Using synthetic data augmentation to expand limited training data, GPT-4.1 with carefully crafted prompts and few-shot examples achieved the highest performance (61.53% macro-F1 for NER, 15.02% for event extraction). The work demonstrates that well-designed prompting can outperform fine-tuning in low-resource biomedical NLP tasks.

## Method Summary
The study evaluates three approaches on French biomedical NER (21 entity types) and health event extraction. GPT-4.1 uses in-context learning with 10 few-shot examples selected via cosine similarity and annotation guidelines embedded in the system prompt. GLiNER-biomed-large-v1.0 is fine-tuned on synthetic data (1,748 examples generated via GPT-4.1 with 40 variants per original) with a GPT-4.1 post-verification step to improve recall. LLaMA-3.1-8B-Instruct is fine-tuned using LoRA rank-16 on the same synthetic data. All approaches output XML-tagged entities with automatic span alignment to handle LLM text modifications.

## Key Results
- GPT-4.1 achieved 61.53% macro-F1 and 75.79% micro-F1 for NER, outperforming fine-tuned approaches
- GLiNER with verification achieved 51.56% macro-F1, showing the value of hybrid pipelines
- LLaMA fine-tuned on synthetic data achieved 40.91% macro-F1, the lowest performance
- Event extraction remained challenging across all approaches (15.02% macro-F1 max for GPT-4.1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with structured prompting outperforms fine-tuning in low-resource biomedical NER
- Mechanism: GPT-4.1 receives annotation guidelines summarized in system prompt, 10 automatically-selected few-shot examples via cosine similarity, and explicit XML output formatting rules—this injects domain knowledge and task structure without weight updates, leveraging the model's pre-trained reasoning capacity
- Core assumption: The base model has sufficient biomedical knowledge encoded during pre-training; ICL merely activates and guides it
- Evidence anchors:
  - [abstract] "Results show GPT-4.1 leads with a macro-F1 of 61.53% for NER and 15.02% for event extraction, highlighting the importance of well-crafted prompting"
  - [section 2.2.3] "Les exemples few-shot sont sélectionnés automatiquement par similarité cosinus avec le texte à traiter... Le guide d'annotation fourni par les organisateurs est intégré sous forme de résumé structuré dans le prompt système"
  - [corpus] Neighbor paper on medical symptom coding (arXiv:2504.03051) similarly finds task-specific prompting improves accuracy, supporting the pattern but not proving generalizability
- Break condition: When annotation guidelines are too complex to summarize effectively, or when entity types are novel and absent from pre-training data

### Mechanism 2
- Claim: Synthetic data augmentation via LLM generation compensates for limited annotated examples
- Mechanism: GPT-4.1 generates 40 variants per original example (40 → 1,748 total) with temperature modulation for diversity; post-processing corrects span misalignments and removes malformed outputs, creating a larger fine-tuning corpus
- Core assumption: LLM-generated variants preserve annotation quality and task-relevant patterns despite distribution shift
- Evidence anchors:
  - [section 2.2.1] "À partir de chaque exemple du jeu d'entraînement, nous avons généré 40 variantes annotées... Un post-traitement automatique a été appliqué pour corriger les décalages dans les positions des spans"
  - [section 4] GLiNER and LLaMA fine-tuned on synthetic data achieved intermediate performance (51.56% and 40.91% macro-F1 respectively)
  - [corpus] arXiv:2502.16022 uses data augmentation for medical jargon extraction but doesn't isolate its causal contribution; limited external validation
- Break condition: When synthetic examples lack linguistic diversity (despite temperature modulation), causing overfitting to synthetic patterns that don't generalize

### Mechanism 3
- Claim: Hybrid pipelines combining specialized models with LLM verification improve coverage
- Mechanism: GLiNER provides high-precision entity detection; GPT-4.1 post-hoc verifies predictions and identifies missed entities, trading some precision for higher recall
- Core assumption: Specialized models and LLMs make complementary errors that can be reconciled
- Evidence anchors:
  - [section 2.2.4] "GLiNER obtenait une précision élevée mais souffrait d'un rappel insuffisant... le LLM reçoit les entités extraites par un modèle ainsi que le texte source, et est chargé de valider les prédictions initiales tout en identifiant les entités potentiellement manquantes"
  - [section 4] Run 2 (GLiNER + verification) achieved 51.56% macro-F1 vs. 40.91% for LLaMA alone, but still underperformed GPT-4.1 ICL
  - [corpus] No direct corpus validation for this specific hybrid pattern
- Break condition: When LLM verification introduces hallucinated entities or contradicts valid specialized model predictions

## Foundational Learning

- Concept: **Macro-F1 vs. Micro-F1**
  - Why needed here: Paper reports both; macro-F1 (61.53% for GPT-4.1) heavily penalizes errors on rare entity types (21 categories with highly skewed distributions), while micro-F1 (75.79%) reflects overall token accuracy
  - Quick check question: If a model perfectly identifies all LOCATION entities (324 occurrences) but misses all BIO_TOXIN entities (2 occurrences), which metric would remain high and which would drop?

- Concept: **In-Context Learning (ICL)**
  - Why needed here: Core mechanism for GPT-4.1 approach; examples are provided in the prompt without gradient updates, relying on the model's ability to pattern-match and generalize
  - Quick check question: What happens to ICL performance if few-shot examples are randomly selected instead of retrieved via similarity?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: LLaMA-3.1-8B fine-tuning uses LoRA with rank 16, enabling parameter-efficient adaptation while preserving general capabilities
  - Quick check question: Why might LoRA underperform full fine-tuning when the target domain differs significantly from pre-training data?

## Architecture Onboarding

- Component map: Example Selector -> Model Inference -> Post-Processing
- Critical path: Prompt construction (guideline summary + example selection) -> Model inference -> XML parsing -> Span alignment -> Output format
- Design tradeoffs:
  - **GPT-4.1 ICL**: Highest performance (61.53% macro-F1) but requires API costs (€3.76/€4.77 for NER/events) and proprietary dependency
  - **GLiNER + verification**: Lowest inference CO₂ (0.2g) and cost (€0.006) but adds verification latency and complexity
  - **LLaMA fine-tuned**: Open-source and adaptable but highest CO₂ (91.23g training, 19.7g inference) and lowest performance (40.91%)
- Failure signatures:
  - **Low recall on rare entities**: Check synthetic data distribution (e.g., ABS_DATE had 2 original → 51 synthetic examples; still underperformed)
  - **XML malformation**: LLMs insert extra whitespace or alter punctuation; alignment module may silently drop entities
  - **Event extraction collapse**: NER errors propagate; if central element is missed, entire event fails (event macro-F1 dropped to 9.93–15.02%)
- First 3 experiments:
  1. **Ablate example selection**: Compare cosine-similarity retrieval vs. random selection vs. no examples on GPT-4.1 NER performance to quantify ICL contribution
  2. **Vary synthetic data ratio**: Train GLiNER with 25%/50%/100% synthetic data to find minimum augmentation threshold for rare entity types
  3. **Isolate post-verification impact**: Run GLiNER without LLM verification to measure recall gain from hybrid pipeline; compare precision/recall tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does adapting tokenizers for the specific lexicon of French biomedical text improve NER performance compared to standard tokenization?
  - Basis in paper: [explicit] The authors explicitly cite "limitations of current tokenizers" when applied to the biomedical lexicon as a difficulty in the discussion (Section 6).
  - Why unresolved: The study utilized off-the-shelf models (GPT-4.1, LLaMA, GLiNER) without implementing or evaluating custom tokenizer adaptations for the medical domain.
  - What evidence would resolve it: A comparative study on the EvalLLM dataset using models with domain-adapted tokenizers versus standard tokenizers, measuring the reduction in out-of-vocabulary rates and F1-score improvements.

- **Open Question 2**: Does enhancing the diversity of synthetic data generation (beyond simple temperature modulation) improve the robustness of fine-tuned models against "non-canonical" cases?
  - Basis in paper: [inferred] In Section 4, the authors attribute the lower performance of EvalLLM-GLiNER to the "synthetic and little varied" nature of the fine-tuning data, which limits robustness.
  - Why unresolved: The paper only employed a basic data augmentation strategy using temperature variation, leaving the impact of more sophisticated generation techniques unexplored.
  - What evidence would resolve it: An ablation study where the fine-tuned models are trained on synthetic datasets generated using diverse prompting strategies (e.g., style transfer, entity substitution) rather than just temperature-based variations.

- **Open Question 3**: Can a joint extraction architecture (simultaneous entity and event detection) outperform the pipeline approach in low-resource French health surveillance?
  - Basis in paper: [inferred] Section 4 highlights the "cumulative and interdependent" nature of the tasks, noting that errors in the NER stage directly caused failures in the event extraction stage.
  - Why unresolved: The implemented solution relied entirely on a sequential pipeline where event extraction depended on pre-extracted entities, amplifying early errors.
  - What evidence would resolve it: A comparison of the current pipeline performance against a unified model (or prompt) that extracts entities and event structures in a single pass to determine if error propagation is reduced.

## Limitations

- Performance claims rely on proprietary GPT-4.1 API access and lack cross-dataset validation
- Synthetic data generation quality is unverified, potentially propagating systematic errors
- Hybrid pipeline benefits are poorly quantified without baseline GLiNER performance comparison
- Limited external validation with only one evaluation dataset (EvalLLM challenge)

## Confidence

- **High confidence**: GPT-4.1 ICL outperforms fine-tuned approaches by 20+ percentage points in macro-F1
- **Medium confidence**: Synthetic data augmentation effectiveness is plausible but not definitively proven
- **Low confidence**: Specific contribution of hybrid pipeline verification step remains unclear

## Next Checks

1. **Ablate the few-shot selection mechanism**: Run GPT-4.1 ICL with randomly selected examples versus cosine-similarity retrieved examples on the same validation set to quantify how much performance depends on example quality versus ICL mechanism itself.

2. **Cross-dataset validation**: Test the best-performing GPT-4.1 ICL approach on an independent French biomedical NER dataset (if available) or English biomedical NER data to assess whether the prompting strategy generalizes beyond the EvalLLM challenge domain.

3. **Quantify verification impact**: Run the GLiNER model without LLM post-verification and measure the recall/precision tradeoff directly, then compare against the hybrid approach to determine if verification justifies its computational cost and complexity.