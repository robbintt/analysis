---
ver: rpa2
title: Benchmarking In-context Experiential Learning Through Repeated Product Recommendations
arxiv_id: '2511.22130'
source_url: https://arxiv.org/abs/2511.22130
tags:
- product
- products
- agent
- persona
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a benchmark to evaluate in-context experiential
  learning in recommendation tasks, where agents must adapt their strategies over
  multiple interactions with customers. The BELA dataset combines real Amazon products,
  diverse user personas, and an LLM simulator to create multi-turn, interactive recommendation
  environments.
---

# Benchmarking In-context Experiential Learning Through Repeated Product Recommendations

## Quick Facts
- **arXiv ID:** 2511.22130
- **Source URL:** https://arxiv.org/abs/2511.22130
- **Reference count:** 40
- **Key outcome:** State-of-the-art models fail to improve recommendations across episodes and struggle to leverage past experiences in a multi-turn, interactive recommendation environment.

## Executive Summary
This paper introduces BELA, a benchmark designed to evaluate in-context experiential learning in recommendation tasks. BELA creates a multi-turn, interactive environment where agents must adapt their strategies over multiple interactions with customers to uncover latent user preferences. Using a dataset of real Amazon products, diverse user personas, and an LLM simulator, the benchmark tests whether agents can improve recommendations across episodes by learning from past experiences. Experiments with leading models like GPT-4o and Claude reveal significant failures in cross-episodic learning, highlighting a critical gap in current agentic systems' ability to reason through uncertainty and adapt from interactions.

## Method Summary
BELA evaluates in-context experiential learning through repeated product recommendation dialogues where agents must discover latent user preferences through strategic questioning across multiple episodes. The benchmark uses 71K products from Amazon Reviews, 2K choice sets from ASINSpotlight categories, and 1M personas from Li et al. [2025]. Product preference scores (0-100) are generated by averaging GPT-4o and Gemini-1.5-Pro outputs. A customer simulator (GPT-4o) interacts with recommender agents using persona prompts. Models are tested over 10 episodes × 40 seeds with free-form text feedback, measuring regret (best score minus recommended product score), question counts, and calibration metrics. Baselines include Random, Popularity, and Oracle (Claude-Sonnet-4 with full persona access).

## Key Results
- GPT-4o and Claude models fail to improve recommendations across episodes, showing no meaningful learning from past experiences
- Agents ask fewer questions in later episodes without corresponding performance gains, indicating broken exploration-exploitation trade-offs
- Manual questioning demonstrates that better trajectories are achievable, highlighting current models' inability to leverage accrued experiences effectively
- Models struggle with uncertainty calibration and often ignore explicitly stated preferences in final recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark creates a valid testbed for experiential learning by enforcing information asymmetry, where agent performance is causally linked to the extraction of latent factors from unobserved persona descriptions.
- Mechanism: The system hides the "ground truth" (persona description and product scores) from the recommender agent. The agent must engage in active exploration (questioning) to reduce uncertainty about the latent factor $\theta_e$. Successful recommendations require the agent to construct an internal belief state that approximates the hidden scoring function $h(p_a, c)$.
- Core assumption: The latent preferences are recoverable through natural language dialogue, and the user simulator provides logically consistent answers based on the persona.
- Evidence anchors:
  - Page 5-6: Defines the environment where the agent must discover preferences via "multiple turns of queries" rather than direct observation.
  - Page 9: Describes the "Regret" metric derived from hidden scores, validating the signal is present but obscured.
  - No direct corpus precedent for this specific asymmetry mechanism in standard RLHF benchmarks (which are single-step).
- Break condition: If the user simulator hallucinates preferences inconsistent with the ground-truth persona (low consistency), the information signal becomes noise, and the learning task becomes impossible.

### Mechanism 2
- Claim: Cross-episodic improvement is driven by the persistence of latent factors (fixed user or fixed product set) across shifting observation windows.
- Mechanism: In settings (a) and (b) (Page 10), the environment constrains the distribution of $\Theta$ (latent factors). The agent receives feedback $f_e$ (reward) in text form. Theoretically, an agent should compress history $H_{e,t}$ into a policy $\pi$ that generalizes (e.g., "This user prioritized sustainability in Ep 1, likely prioritizes it in Ep 2").
- Core assumption: Current Transformer architectures can maintain and utilize long-context history to perform implicit Bayesian updates over user preferences.
- Evidence anchors:
  - Abstract: "evaluates agents' ability to learn and adapt across episodes by uncovering latent user preferences."
  - Page 12: Shows an "Ideal Trajectory" where manually curated questions lead to regret $45.0 \to 0.0$, proving the causal link between information gain and performance.
  - EchoTrail-GUI (Corpus) similarly emphasizes "actionable memory" for GUI agents, supporting the need for history utilization.
- Break condition: If the model's context window is saturated or it fails to attend to historical tokens (amnesia), the mechanism breaks, and the agent reverts to zero-shot guessing.

### Mechanism 3
- Claim: The use of free-form text as the primary reward signal necessitates semantic reasoning capabilities rather than scalar optimization heuristics.
- Mechanism: Unlike traditional POMDPs with explicit scalar rewards, BELA encodes the reward in the user's textual response (e.g., "I prefer sustainable goods"). The agent must semantically parse this feedback to update its strategy.
- Core assumption: LLMs can translate semantic satisfaction/dissatisfaction expressed in text into actionable policy updates without a finetuning gradient.
- Evidence anchors:
  - Page 2: Explicitly contrasts with POMDPs, noting "rewards primarily encoded in free-form natural language responses."
  - Page 4: Validates that models struggle here, often ignoring stated preferences.
  - Fine-grained auxiliary learning (Corpus) suggests auxiliary signals are often needed for recommendation, implying raw text signals may be insufficient for current models.
- Break condition: If the model treats the feedback text as non-instructive context (e.g., just conversation filler) and fails to map sentiment to utility, the feedback loop is broken.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper frames the recommendation task as a POMDP where the user's preference is the unobserved state. Understanding "belief states" helps explain why the models fail—they cannot maintain a probability distribution over user needs effectively.
  - Quick check question: Can you explain why a standard MDP fails to model a user with hidden preferences?

- Concept: **Exploration-Exploitation Trade-off**
  - Why needed here: The benchmark specifically tests if agents know *when* to stop asking questions (exploration) and recommend (exploitation). The failure of models to ask fewer questions *with* better performance indicates a broken trade-off mechanism.
  - Quick check question: If an agent asks fewer questions but performance stays random, is it exploring or exploiting correctly?

- Concept: **User Modeling / Persona Simulation**
  - Why needed here: The "user" is an LLM simulating a specific persona. Understanding how prompt-based simulation generates ground truth is critical to validating the benchmark's reliability.
  - Quick check question: How does the paper ensure the simulated user doesn't contradict their own persona description?

## Architecture Onboarding

- Component map: Recommender Agent -> User Simulator -> Environment -> Evaluator
- Critical path:
  1. Setup: Select a Persona and a Choice Set
  2. Scoring (Offline): Use a Judge LLM to score all products in the set for that persona ($0-100$). Store $y^*$
  3. Interaction Loop: Recommender asks Question → Simulator answers (based on persona)
  4. Termination: Recommender selects Product → Simulator generates Text Feedback
  5. Metric: Compare selected product score to $y^*$

- Design tradeoffs:
  - Simulator Consistency vs. Diversity: The system relies on the simulator maintaining internal logic. The paper notes a 6.1% "conflict" rate in persona consistency, which introduces noise.
  - Feedback Richness: Free-form text is realistic but hard for models to parse for optimization. Explicit "Regret" feedback is easier but unrealistic.

- Failure signatures:
  - Passive Disengagement: Agents ask fewer questions in later episodes without performance gains (Page 10, Figure 5)
  - Repetitive Loops: GPT-4o frequently asks the same query until hitting the turn limit (Page 33)
  - Amnesia: Agents ignore explicitly stated preferences in the current context (Page 33)
  - Rigid Scripting: Gemini models follow pre-determined scripts ("Are you looking for a free option?") regardless of context (Page 33)

- First 3 experiments:
  1. Oracle Baseline: Run the recommender with full access to the persona description to determine the lower bound of Regret (is the task solvable?)
  2. Consistency Check: Replicate the user simulator's answers to the same question 10 times to verify the environment is stable (Appendix E)
  3. Episodic Drift: Run the agent for 10 episodes with the same persona but different product categories to test if it learns the *user's* general style (Setting A, Page 10)

## Open Questions the Paper Calls Out

- Question: What specific architectural or training modifications would enable LLMs to successfully learn from multi-episode experiences?
  - Basis in paper: The authors conclude that "SOTA models still fail to learn effectively from experiences" and call for "future research on experiential learning, which is essential for building agents capable of reliable decision-making under real-world uncertainties."
  - Why unresolved: Current models perform worse than oracle baselines and show no episode-over-episode improvement, but the paper only demonstrates the failure without identifying the root cause (architecture, pre-training data, objective functions).
  - What evidence would resolve it: Targeted experiments fine-tuning models on experiential learning tasks, or architectural modifications (e.g., external memory modules, belief-state tracking) showing systematic improvement across episodes on BELA.

- Question: Does the use of LLM-based customer simulators (GPT-4o) and LLM-based scoring introduce circularity or bias into benchmark evaluations?
  - Basis in paper: The customer simulator is powered by GPT-4o, and preference scoring uses GPT-4o and Gemini-1.5-Pro. Evaluating frontier models against LLM-generated personas and LLM-scored preferences may conflate model-specific patterns with genuine experiential learning capability.
  - Why unresolved: The paper does not analyze whether different simulator/scoring models systematically advantage or disadvantage particular evaluated models, nor compare against human-generated personas or human preference judgments.
  - What evidence would resolve it: Cross-validation using different LLMs as simulators/scorers, or comparison with human-annotated preference scores and human-simulated customer responses.

- Question: How can agents be trained to strategically maintain or increase exploratory questioning when uncertainty remains high?
  - Basis in paper: "We observe that the number of questions asked generally declines over episodes" despite stagnant performance, which the authors highlight as "a major deficiency in the behavior patterns of the SOTA models."
  - Why unresolved: Models prematurely reduce questioning even when they should recognize insufficient information—suggesting a failure in uncertainty-aware exploration strategies that the paper does not diagnose or remedy.
  - What evidence would resolve it: Demonstrating training methods (e.g., reinforcement learning with uncertainty-aware rewards) that produce agents whose question count adapts to actual information gaps rather than declining monotonically.

## Limitations

- The benchmark relies on LLM-generated personas and scoring, which may introduce circularity or model-specific biases in evaluation
- The 6.1% persona consistency conflicts in the user simulator introduce noise that could affect learning signal reliability
- Results may not generalize beyond recommendation tasks to other domains requiring experiential learning

## Confidence

- **High confidence:** Models fail to improve recommendations across episodes, with systematic degradation in question efficiency and substantial gaps versus scripted optimal trajectories
- **Medium confidence:** Failure stems from reasoning through uncertainty rather than fundamental architectural limitations (not systematically isolated)
- **Low confidence:** Benchmark's external validity—whether success in BELA translates to real-world deployment with noisier feedback

## Next Checks

1. **Cross-domain generalization test:** Apply BELA's methodology to a non-recommendation task (e.g., technical support) to verify if experiential learning failures persist across domains
2. **Ablation study on simulator prompts:** Systematically vary the user simulator's instructions to determine which components most affect learning outcomes
3. **Long-context capability analysis:** Test whether extending model context windows improves cross-episodic learning by providing more historical information