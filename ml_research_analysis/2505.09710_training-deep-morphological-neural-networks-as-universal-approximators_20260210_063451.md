---
ver: rpa2
title: Training Deep Morphological Neural Networks as Universal Approximators
arxiv_id: '2505.09710'
source_url: https://arxiv.org/abs/2505.09710
tags:
- networks
- morphological
- have
- network
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates deep morphological neural networks (DMNNs),
  which are based on mathematical morphology operations like dilation and erosion.
  The authors show that despite their inherent non-linearity, "linear" activations
  are essential for DMNNs to be universal approximators.
---

# Training Deep Morphological Neural Networks as Universal Approximators

## Quick Facts
- arXiv ID: 2505.09710
- Source URL: https://arxiv.org/abs/2505.09710
- Reference count: 40
- Primary result: DMNNs with linear activations achieve 97.49% test accuracy on MNIST

## Executive Summary
This paper investigates deep morphological neural networks (DMNNs) based on mathematical morphology operations like dilation and erosion. The authors demonstrate that while pure morphological networks cannot approximate simple linear functions, introducing linear activations between morphological layers restores universal approximation capability. They propose three architectures with different parameter constraints to preserve sparsity, improve generalization with residual connections and weight dropout, and show these networks can be successfully trained on image classification tasks while being more prunable than linear networks.

## Method Summary
The paper proposes Max-Plus-Min (MPM) networks that combine dilation (max) and erosion (min) operations using shared weights but separate biases, then sum the results. The MPM layer computes: output = (bias_max + max(inputs + weights)) + (bias_min + min(inputs + weights)). This output is then scaled by learnable linear activations. The authors prove that pure morphological stacks fail to approximate simple functions, while MPM networks with linear activations are universal approximators. They implement three architectures: MPM (high sparsity, O(N) parameters), MPM-SVD (intermediate), and Hybrid-MLP (standard parameter count but faster convergence with large batches).

## Key Results
- MPM network with weight dropout achieved 97.49% test accuracy on MNIST, only 0.52% lower than standard MLP
- MPM networks are more prunable than linear networks while maintaining accuracy
- Hybrid-MLP networks significantly accelerate convergence of gradient descent with large batches (6400)
- MPM-ResNet-20 achieved 62.14% accuracy on CIFAR-10 compared to 81.74% for linear ResNet-20

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pure morphological networks are not universal approximators; introducing "linear" activations between morphological layers restores this capability.
- **Mechanism:** Pure max-plus or min-plus networks suffer from extreme gradient sparsity and cannot represent simple linear functions. By interleaving linear transformations between morphological layers, the network diffuses gradient signals across parameters, allowing the composition of morphological operations to approximate continuous functions on compact domains.
- **Core assumption:** The input domain is compact, and standard gradient-based optimization (Adam) is used.
- **Evidence anchors:**
  - [abstract] "linear activations are essential for DMNNs to be universal approximators."
  - [section 3] Theorem 1 and Theorem 2 demonstrate the representational and optimization limitations of pure morphological stacks.
  - [corpus] Weak/No direct evidence in the provided corpus regarding the failure of pure morphological networks.

### Mechanism 2
- **Claim:** The MPM (Max-Plus-Min) layer enables stable training by naturally centering output distributions, unlike standard Morphological Perceptrons.
- **Mechanism:** Standard dilation layers shift output means positively, while erosion shifts them negatively. The MPM layer fixes λ=1/2 and sums the output of a dilation and erosion using shared weights, balancing the positive and negative shifts to produce a zero-mean output that stabilizes optimization.
- **Core assumption:** Input features are roughly centered (e.g., normalized); otherwise, the first layer requires specific zero-initialization.
- **Evidence anchors:**
  - [section 3] "both introducing the learnable scaling and taking λ^(n)_i = 0.5 are essential for the network to be trainable."
  - [section 4] MPM achieves 94.66% accuracy vs Act-DEP (learnable λ) at 84.51%.

### Mechanism 3
- **Claim:** Hybrid linear-morphological networks converge faster than standard networks when trained with large batch sizes.
- **Mechanism:** While morphological layers introduce noise in gradient estimation that hinders small-batch training, they accelerate descent dynamics when gradients are estimated reliably with large batches, likely due to distinct loss landscapes created by max/min operations.
- **Core assumption:** Batch size must be sufficiently large (e.g., 6400) to overcome noise in morphological gradient estimation.
- **Evidence anchors:**
  - [abstract] "inclusion of morphological layers significantly accelerates the convergence of gradient descent with large batches."
  - [section D] Figure 5 shows Hybrid-MLP converging faster than MLP and Maxout on MNIST/F-MNIST with batch size 6400.

## Foundational Learning

- **Concept:** Tropical Algebra (Max-Plus/Min-Plus Semirings)
  - **Why needed here:** DMNNs are defined over semirings, not standard fields. Understanding associativity and distributivity in this context explains why stacks of pure morphological layers collapse into single layers.
  - **Quick check question:** How does the distributivity of + over max differ from standard algebra, and how does this cause layer collapse in pure DMNNs?

- **Concept:** Mathematical Morphology (Dilation & Erosion)
  - **Why needed here:** These are the fundamental atomic operations of the network. Dilation (max) acts as a local maximum filter expanding bright regions, while erosion (min) shrinks them.
  - **Quick check question:** If you have a 1D signal [1, 5, 2] and apply a dilation with weights [0, 0], what is the output?

- **Concept:** Universal Approximation Theory
  - **Why needed here:** To understand why "non-linearity" is not enough. The paper proves that while pure morphological nets are non-linear, they cannot approximate simple linear functions, thus failing the universal approximation criteria.
  - **Quick check question:** Why does Theorem 1 imply that a pure max-plus network cannot approximate the function f(x) = 2x₁?

## Architecture Onboarding

- **Component map:** Input -> MPM Core (Dilation + Erosion with shared weights) -> Linear Activation (diag scaling or full linear) -> Output

- **Critical path:** Implementing the MPM layer correctly is the highest risk. Implementation: output = (bias_max + max(inputs + weights)) + (bias_min + min(inputs + weights)), then apply activation: output = output * alpha (Setting 1) or Linear(output) (Setting 3).

- **Design tradeoffs:**
  - **Setting 1 (MPM):** High sparsity, O(N) learnable parameters per layer. Use case: Edge devices, extreme pruning requirements. Cost: Slightly lower accuracy.
  - **Setting 3 (Hybrid-MLP):** Standard parameter count. Use case: Standard cloud training where speed is priority. Cost: Loses inherent sparsity benefits; requires large batch sizes.

- **Failure signatures:**
  - **Dead Gradients:** Using pure Max-Plus layers without MPM fusion or linear activations results in gradients being 0 almost everywhere.
  - **Mean Drift:** Incorrect initialization with non-zero mean weights on non-centered data causes slow/no training. Fix: Zero-initialize first layer or normalize data.
  - **Small Batch Stalls:** Hybrid-MLP fails to learn with small batches. Fix: Increase batch size to >1000 or switch to pure MPM.

- **First 3 experiments:**
  1. **Sanity Check (Regression):** Train a 2-layer MPM to fit f(x) = 2x on synthetic data. Verify it succeeds where a pure Max-Plus network fails.
  2. **Convergence Test (Hybrid):** Compare Hybrid-MLP vs Standard MLP on MNIST using batch size 64 vs 6400. Observe the performance gap.
  3. **Pruning Stress Test:** Train MPM (Setting 1) on Fashion-MNIST and apply L₁ pruning at ratios [0.8, 0.9, 0.95]. Compare accuracy drop-off against standard MLP.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specific gradient estimation techniques be developed to stabilize the training of Hybrid-MLPs with small batch sizes?
- **Basis in paper:** [explicit] The Conclusion states future research could explore "improved gradient estimation techniques," while Section 4.3 and Appendix D demonstrate Hybrid-MLPs fail to train with small batches (e.g., 64) but converge rapidly with large batches (e.g., 6400).
- **Why unresolved:** The authors hypothesize morphological layers introduce significant noise in gradient estimation for stochastic methods like Adam but do not propose a solution.
- **What evidence would resolve it:** A modified optimizer or variance-reduction technique allowing Hybrid-MLP to achieve high accuracy using batch size 64 or 128.

### Open Question 2
- **Question:** What is the theoretical mechanism causing the accelerated convergence of gradient descent in hybrid morphological-linear networks when using large batches?
- **Basis in paper:** [inferred] The abstract and introduction claim morphological layers "significantly accelerate the convergence of gradient descent with large batches," supported empirically in Figure 5, but provide no theoretical explanation.
- **Why unresolved:** Empirical results demonstrate the phenomenon, but the paper does not analyze the loss landscape geometry or gradient properties that drive this speedup.
- **What evidence would resolve it:** A theoretical analysis of the Hybrid-MLP loss landscape showing specific properties that explain faster convergence relative to standard MLPs.

### Open Question 3
- **Question:** Can the proposed DMNN architectures be modified to close the generalization gap with standard linear networks on complex, natural image datasets like CIFAR-10?
- **Basis in paper:** [inferred] Table 7 shows a significant performance gap on CIFAR-10, where MPM-ResNet-20 achieves 62.14% accuracy compared to linear ResNet-20's 81.74%.
- **Why unresolved:** While authors prove networks are universal approximators, empirical results suggest current architectural inductive biases or training dynamics limit performance on complex datasets.
- **What evidence would resolve it:** An architectural refinement or training methodology enabling a morphological network to achieve accuracy within 1-2% of a linear ResNet-20 on CIFAR-10.

### Open Question 4
- **Question:** How does the initialization of the mean of weights in morphological layers interact with the learnable "linear" activations during training?
- **Basis in paper:** [inferred] Appendix C highlights that "morphological networks struggle with the wrong initialization of the mean of their weights," but the paper relies on heuristics to ensure zero-mean output.
- **Why unresolved:** The interaction between morphological weights' mean and learnable scaling parameters is described as crucial but not formally analyzed.
- **What evidence would resolve it:** A sensitivity analysis or theoretical derivation showing the range of valid initial means for weights that allows learnable activations to successfully normalize layer outputs.

## Limitations
- Hybrid-MLP networks require large batch sizes (6400) for effective training, limiting applicability to small datasets or online learning scenarios
- Significant generalization gap remains on complex datasets like CIFAR-10 (62.14% vs 81.74% for linear ResNet-20)
- Implementation details of weight dropout are unclear, as standard PyTorch dropout masks activations rather than weights

## Confidence
- **High Confidence:** The universal approximation theorem for MPM networks is well-supported by theoretical framework and empirical results showing competitive accuracy (97.49% on MNIST) with significant pruning capability
- **Medium Confidence:** The MPM layer mechanism for balancing output distributions is plausible but relies heavily on specific initialization schemes and input normalization without rigorous proof
- **Low Confidence:** The exact implementation and impact of weight dropout is unclear, and computational efficiency gains from sparsity are not quantified in the main text

## Next Checks
1. **Initialization Sensitivity Test:** Train MPM networks on MNIST using different weight initialization schemes (He, Xavier) to determine sensitivity of the "mean shift" problem
2. **Small Batch Hybrid-MLP:** Systematically vary batch sizes for Hybrid-MLP on Fashion-MNIST to precisely quantify performance degradation and identify minimum batch size for effective training
3. **Weight Dropout Implementation:** Implement and test a custom weight dropout layer for MPM networks to verify its contribution to reported 97.49% accuracy and compare against standard dropout