---
ver: rpa2
title: A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic
  Transformers
arxiv_id: '2502.04535'
source_url: https://arxiv.org/abs/2502.04535
tags:
- seqmap
- length
- pages
- reranker
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SeqMAP, a new decoding algorithm for length-control
  summarization using Directed Acyclic Transformers (DAT). Unlike previous methods
  that use Connectionist Temporal Classification (CTC) models, SeqMAP aligns better
  with DAT's training objective by marginalizing possible paths and finding the most
  probable sequence satisfying length constraints.
---

# A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers

## Quick Facts
- arXiv ID: 2502.04535
- Source URL: https://arxiv.org/abs/2502.04535
- Reference count: 36
- Primary result: SeqMAP achieves R-1 of 36.74 on Gigaword (25% length) and 30.11 on DUC2004 (20% length), outperforming CTC-based methods

## Executive Summary
This paper introduces SeqMAP, a new decoding algorithm for length-control summarization using Directed Acyclic Transformers (DAT). Unlike previous methods that use Connectionist Temporal Classification (CTC) models, SeqMAP aligns better with DAT's training objective by marginalizing possible paths and finding the most probable sequence satisfying length constraints. The approach uses beam search with dynamic programming to approximate the SeqMAP objective and includes a reranker for performance improvement. Experiments on Gigaword and DUC2004 datasets show that SeqMAP outperforms both CTC-based methods and existing DAT approaches, achieving state-of-the-art results in length-control summarization while maintaining faster inference speed than autoregressive models.

## Method Summary
SeqMAP is a decoding algorithm designed specifically for DAT-based length-control summarization. It uses beam search with dynamic programming to find the most probable sequence that satisfies exact length constraints. The algorithm initializes a DP table with ⟨bos⟩, then iteratively expands and merges paths while marginalizing over possible prediction steps. A reranker trained on RoBERTa-base embeddings with rank embeddings and a 1-layer Transformer is applied to the final beam candidates. The method is trained on Gigaword (3M train, 189K val, 1951 test pairs) and evaluated on both Gigaword and DUC2004 (500 samples).

## Key Results
- Achieves R-1 of 36.74 on Gigaword at 25% length ratio
- Achieves R-1 of 30.11 on DUC2004 at 20% length ratio
- Outperforms CTC-based methods and existing DAT approaches
- Maintains faster inference speed than autoregressive models

## Why This Works (Mechanism)
SeqMAP works by aligning the decoding objective with DAT's training objective. While CTC-based methods were used in previous DAT work, they don't match the underlying training distribution. SeqMAP marginalizes over all possible paths that could generate a given sequence, then finds the most probable sequence satisfying length constraints. This approach better captures the uncertainty in non-autoregressive generation and provides more accurate probability estimates for sequence selection.

## Foundational Learning
**Directed Acyclic Transformer (DAT)**: A non-autoregressive model where predictions at each step depend only on previous steps, creating a directed acyclic graph. Why needed: Enables parallel generation while maintaining some sequential dependencies. Quick check: Verify model architecture allows multiple prediction paths for the same output sequence.

**Connectionist Temporal Classification (CTC)**: A loss function that marginalizes over all possible alignments between input and output sequences. Why needed: Previous method for training DAT models but not optimal for length-control tasks. Quick check: Compare CTC vs SeqMAP objective formulations.

**Beam Search with Dynamic Programming**: An approximate search algorithm that maintains K candidate sequences while exploring the search space efficiently. Why needed: Exact SeqMAP is computationally intractable, requiring approximation. Quick check: Verify beam size K=20 provides sufficient search coverage.

## Architecture Onboarding

**Component Map**: Input -> DAT Encoder -> DP Table (SeqMAP) -> Beam Candidates -> Reranker -> Output Summary

**Critical Path**: The most important computational path is the DP table initialization and iterative expansion/merging. This involves computing marginalization scores (sum_{s'=t}^s u_{s'}(b)) and maintaining the top-K candidates at each step. The reranker adds a final refinement but doesn't affect the core length control mechanism.

**Design Tradeoffs**: The beam search with K=20 and vocabulary expansion V=5 balances computational efficiency with search quality. Larger beams improve results but increase computation linearly. The marginalization approach trades exactness for tractability, similar to other approximate inference methods.

**Failure Signatures**: 
- Length constraint not met exactly: Check DP table initialization and marginalization calculations
- Low ROUGE scores: Verify reranker training and beam candidate diversity
- Slow inference: Beam search may need optimization for parallel hardware

**3 First Experiments**:
1. Verify length constraint adherence across different input lengths and beam sizes
2. Compare ROUGE scores with and without reranking to measure reranker impact
3. Test different beam sizes (K=10, 20, 30) to find optimal tradeoff between quality and speed

## Open Questions the Paper Calls Out

**Open Question 1**: What are the precise causes of performance degradation in SeqMAP when the beam search scope becomes very large, and how can this be mitigated? The authors observe that performance decreases with large beam sizes and hypothesize label bias issues but don't confirm this or propose solutions.

**Open Question 2**: Can the SeqMAP decoding algorithm be optimized for parallel hardware to achieve inference speeds competitive with PathMAP? The current implementation hasn't been optimized for parallelism, leaving room for efficiency improvements.

**Open Question 3**: Does an efficient exact algorithm exist for the SeqMAP objective, or is finding the most probable sequence in a DAT computationally intractable? The authors suggest the problem might be NP-hard but don't provide a formal complexity proof.

## Limitations
- Training hyperparameters (learning rate, optimizer, prediction steps S) are unspecified
- Reranker training details lack information on epochs and validation strategy
- Performance degradation with large beam sizes requires further investigation
- No formal complexity analysis for exact SeqMAP solution

## Confidence
**High Confidence**: Theoretical foundation of SeqMAP and experimental methodology
**Medium Confidence**: Implementation details of beam search and reranker architecture
**Low Confidence**: Exact reproduction of results due to unspecified training hyperparameters

## Next Checks
1. Conduct hyperparameter sensitivity analysis varying learning rate, batch size, and prediction steps S
2. Implement systematic reranker evaluation with different configurations and training procedures
3. Analyze length constraint adherence quantitatively across different beam sizes and vocabulary expansion factors