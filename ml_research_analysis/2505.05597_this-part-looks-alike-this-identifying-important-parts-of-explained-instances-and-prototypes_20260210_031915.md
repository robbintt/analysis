---
ver: rpa2
title: 'This part looks alike this: identifying important parts of explained instances
  and prototypes'
arxiv_id: '2505.05597'
source_url: https://arxiv.org/abs/2505.05597
tags:
- prototype
- feature
- importance
- prototypes
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to identify important overlapping features
  between an instance and its nearest prototype, termed alike parts, using feature
  importance scores derived from SHAP. It incorporates these scores into prototype
  selection algorithms to enhance both interpretability and diversity.
---

# This part looks alike this: identifying important parts of explained instances and prototypes

## Quick Facts
- arXiv ID: 2505.05597
- Source URL: https://arxiv.org/abs/2505.05597
- Reference count: 13
- This work proposes a method to identify important overlapping features between an instance and its nearest prototype, termed alike parts, using feature importance scores derived from SHAP.

## Executive Summary
This paper introduces a method to improve prototype-based explanations by identifying "alike parts" - the most informative overlapping features between an instance and its nearest prototype. The approach computes SHAP feature importance for both the instance and prototype, normalizes these scores, and multiplies them element-wise to identify features that are highly important for both. These alike parts are then used to enhance both the interpretability of prototypes and the diversity of selected prototypes through modified objective functions in k-medoids algorithms. The method was evaluated on six benchmark datasets showing improved user comprehension and in some cases higher predictive accuracy.

## Method Summary
The method identifies alike parts by computing SHAP feature importance for both an instance and its nearest prototype, normalizing these scores by squaring and dividing by sum-of-squares, then multiplying element-wise to get alignment weights. Features with weights above the mean are retained as a binary mask. This approach is incorporated into prototype selection algorithms by modifying their objective functions to include a feature importance term controlled by parameter β. The method was tested on six tabular datasets using Random Forest models and modified versions of A-PETE, G-KM, and SM-A algorithms.

## Key Results
- Improved user comprehension of prototypes through highlighting of alike parts
- Higher predictive accuracy in some cases (e.g., G-KM accuracy improved from 0.785 to 0.861 on Apple Quality dataset)
- Optimal β values balancing distance minimization and feature importance found between 0 and 2.0
- Method supports missing values and can potentially extend to non-tabular data modalities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Identifying overlapping high-importance features between an instance and its nearest prototype directs user attention to a meaningful subset of features, improving comprehension.
- **Mechanism:** Compute SHAP feature importance for both the instance and prototype, normalize scores by squaring (to handle positive/negative equally) and dividing by sum-of-squares, then multiply element-wise to get alignment weights. Features with weights above the mean are retained as the binary mask.
- **Core assumption:** Features with high importance in BOTH the instance and its prototype are the most relevant for explaining why this prototype matches this instance.
- **Evidence anchors:** [abstract]: "Using feature importance scores derived from an agnostic explanation method, it emphasizes the most relevant overlapping features between an instance and its nearest prototype." [section 3.1, Formula 1-3]: Provides the complete normalization, weight calculation, and binary mask formulation.
- **Break condition:** If SHAP values are unreliable for the data type; if the mean threshold selects too many or too few features; if importance is distributed uniformly across features (no discriminative signal).

### Mechanism 2
- **Claim:** Incorporating feature importance into the prototype selection objective function promotes global diversity in which features are highlighted across the prototype set.
- **Mechanism:** Extend the k-medoids objective by adding a feature importance term: f(P) = Σ min(d(xi, pj) + β·fi(xi, pj)), where fi captures normalized importance alignment between instance and candidate prototype.
- **Core assumption:** Prototypes should be spatially representative AND diverse in their feature importance patterns—different prototypes should highlight different subsets of important features.
- **Evidence anchors:** [abstract]: "Furthermore, the feature importance score is incorporated into the objective function of the prototype selection algorithms to promote global prototypes diversity." [section 3.2, Formula 5-6]: Defines fi computation and revised objective function.
- **Break condition:** If β is set too high (sacrifices spatial coverage for importance diversity); if importance patterns don't cluster meaningfully; if pre-computed fi scores become stale.

### Mechanism 3
- **Claim:** Squaring SHAP values before normalization preserves relative importance magnitude while treating positive and negative contributions equally.
- **Mechanism:** Raw SHAP values can be positive or negative; squaring converts both to positive values proportional to absolute importance, enabling direct comparison without cancellation effects.
- **Core assumption:** The direction of SHAP contribution (pushing toward/away from a class) is less relevant than the magnitude for identifying "alike" features.
- **Evidence anchors:** [section 3.1]: "We treat both positive and negative scores equally by squaring them, which avoids cancellations and enables the identification of similarities and differences." [Table 1]: Demonstrates the full pipeline on Apple Quality data.
- **Break condition:** If sign information is semantically meaningful for the domain (e.g., knowing which features push toward vs. away from a diagnosis); if feature interactions dominate (this approach captures only marginal importance).

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - Why needed here: The entire method builds on SHAP-derived feature importance; understanding what SHAP values represent (contribution of each feature to pushing prediction away from baseline) is essential.
  - Quick check question: For a binary classifier, if a feature has SHAP value +0.3 for an instance, what does that mean relative to the base rate?

- **Concept: Prototype-based explanation and k-medoids**
  - Why needed here: The method modifies existing prototype selection algorithms (A-PETE, SM-A, G-KM) that solve k-medoids problems; understanding medoids vs. centroids clarifies why actual data points are selected.
  - Quick check question: Why might a medoid (actual data point) be preferred over a centroid (computed average) for human-interpretable explanations?

- **Concept: Surrogate model evaluation**
  - Why needed here: The paper evaluates prototype quality using a 1-nearest-neighbor classifier on the prototype set; this measures how well prototypes represent the full dataset.
  - Quick check question: If a 1-NN surrogate on prototypes achieves 85% accuracy while the original model achieves 92%, what might this indicate about the prototype coverage?

## Architecture Onboarding

- **Component map:** Black-box classifier (Random Forest) -> SHAP explainer -> Normalization module -> Weight calculator -> Binary mask generator -> Pre-computation cache -> Modified prototype selector -> Surrogate 1-NN evaluator
- **Critical path:** 1) Train black-box model on S; 2) Pre-compute and cache fi scores (Formula 5) for optimization efficiency; 3) Run prototype selection with tuned β parameter; 4) At inference: find nearest prototype for new instance, compute alike parts mask; 5) Return instance + prototype with highlighted overlapping important features
- **Design tradeoffs:** β parameter: Controls distance vs. importance diversity balance; paper suggests β ≤ 2.0 often works but requires tuning; Mean threshold: Simple but may over/under-select; could be replaced with top-k or percentile; Squaring vs. absolute value: Squaring emphasizes large values more aggressively; SHAP vs. other explainers: Paper claims agnostic approach, but only SHAP is tested
- **Failure signatures:** Same features highlighted for all prototypes → β too low or feature importance concentrated on few features; Mask selects nearly all or nearly no features → mean threshold inappropriate for importance distribution; Surrogate accuracy drops significantly → β too high, prototypes no longer cover data space; Alike parts don't match domain knowledge (e.g., missing known risk factors) → SHAP values may not align with causal factors
- **First 3 experiments:** 1) Replicate Table 1 on a held-out dataset instance: compute SHAP for instance and prototype, verify normalization/multiplication/mask steps produce expected output; 2) Ablation on β (following Figure 4): Sweep β ∈ {0, 0.5, 1.0, 1.5, 2.0} and plot mean feature importance of alike parts vs. number of selected features to identify sweet spot; 3) Compare raw vs. FI-informed prototype selection: For each algorithm (A-PETE, G-KM, SM-A), measure surrogate accuracy and count unique features highlighted across all prototypes (diversity metric)

## Open Questions the Paper Calls Out

- **Question:** Does the "alike parts" identification method significantly improve human comprehension of model decisions compared to standard prototype visualization?
  - **Basis in paper:** [explicit] The Discussion states, "Future research should explore its effectiveness from the user perspective, assessing whether these explanations enhance human understanding of model decisions."
  - **Why unresolved:** The current experiments rely on surrogate model accuracy and feature alignment metrics rather than direct human-subject studies to validate interpretability claims.
  - **What evidence would resolve it:** Results from controlled user studies measuring explanation utility, trust, or decision-making speed.

- **Question:** Can the proposed optimization function be effectively adapted for non-tabular data modalities such as images and text?
  - **Basis in paper:** [explicit] The Discussion notes that "evaluating the approach on non-tabular modalities, such as images and text, is necessary to assess its broader applicability."
  - **Why unresolved:** The method and experiments are designed specifically for tabular data (feature-value vectors), and it is unclear how "alike parts" would be extracted from unstructured data like pixels or tokens.
  - **What evidence would resolve it:** Successful application of the method to image or text datasets, demonstrating meaningful overlapping features between instances and prototypes.

- **Question:** How sensitive are the identified "alike parts" and prototype selection to the choice of the underlying feature importance estimator (e.g., SHAP vs. LIME)?
  - **Basis in paper:** [inferred] The method relies on SHAP values, but Section 3 states "any feature importance method can be applied," implying the results may vary depending on the explainer used.
  - **Why unresolved:** The paper does not analyze how the variance or bias of different explanation methods impacts the stability of the binary mask or the resulting prototype diversity.
  - **What evidence would resolve it:** An ablation study comparing the consistency of "alike parts" when different feature attribution methods are substituted into the objective function.

## Limitations
- Method relies on SHAP values being reliable indicators of feature importance without validation or alternative explainer comparisons
- Mean threshold for selecting alike parts is arbitrary and may not generalize across datasets with different feature importance distributions
- Limited to tabular data and Random Forest models; extension to other modalities or model types remains theoretical
- No user study to validate that alike parts actually improve human comprehension as claimed

## Confidence
- High confidence: SHAP-based normalization and weight calculation (well-defined mathematical operations)
- Medium confidence: Prototype selection objective modification (formulaic but β sensitivity not fully characterized)
- Low confidence: User comprehension claims (no empirical validation with actual users)

## Next Checks
1. Conduct ablation study varying the alike parts threshold (mean, median, top-k) to assess robustness of feature selection
2. Compare alike parts against alternative methods (LIME, integrated gradients) on the same datasets to evaluate relative performance
3. Design and execute a controlled user study measuring comprehension time and accuracy when explanations include alike parts vs. full feature sets