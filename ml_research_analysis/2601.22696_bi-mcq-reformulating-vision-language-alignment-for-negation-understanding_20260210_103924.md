---
ver: rpa2
title: 'Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding'
arxiv_id: '2601.22696'
source_url: https://arxiv.org/abs/2601.22696
tags:
- image
- alignment
- negation
- text
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of vision-language models (VLMs)
  in understanding negation in medical imaging, particularly for disease absence detection.
  The core method, Bi-MCQ, reformulates vision-language alignment as a conditional
  semantic comparison problem using bi-directional multiple-choice learning (I2T and
  T2I MCQ tasks) with affirmative, negative, and mixed prompts.
---

# Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding

## Quick Facts
- arXiv ID: 2601.22696
- Source URL: https://arxiv.org/abs/2601.22696
- Reference count: 26
- This paper addresses the limitation of vision-language models (VLMs) in understanding negation in medical imaging, particularly for disease absence detection.

## Executive Summary
This paper addresses the fundamental challenge of negation understanding in medical vision-language models (VLMs). Standard contrastive learning approaches struggle to distinguish between affirmative and negated clinical statements because they optimize for global similarity rather than semantic distinction. The authors propose Bi-MCQ, which reformulates vision-language alignment as a conditional semantic comparison problem using bi-directional multiple-choice learning (I2T and T2I tasks) with affirmative, negative, and mixed prompts. This approach forces the model to explicitly reason about negation as a semantic dimension rather than treating it as a minor modifier. Experiments on ChestXray14, Open-I, CheXpert, and PadChest datasets demonstrate significant improvements in negation understanding, with up to 0.47 AUC improvement over zero-shot performance and reduced affirmative-negative AUC gaps.

## Method Summary
Bi-MCQ reformulates vision-language alignment as bi-directional multiple-choice learning (I2T and T2I tasks) to explicitly handle negation semantics. For each batch, the method constructs MCQ candidates using affirmative ("There is [disease]"), negative ("There is no [disease]"), and mixed prompts (hybrid statements). Direction-specific cross-attention fusion modules are used to prevent alignment interference between I2T and T2I reasoning. The image and text encoders are frozen during training, with global and local embeddings processed through separate cross-attention modules. The model is trained with Adam optimizer (lr=1e-5) on ChestXray14 dataset, with evaluations on Open-I, CheXpert, and PadChest.

## Key Results
- Bi-MCQ improves negation understanding by up to 0.47 AUC over zero-shot performance of CARZero
- Affirmative-negative AUC gap is reduced by an average of 0.12 compared to InfoNCE-based fine-tuning
- Mixed prompts improve out-of-distribution generalization on Open-I but slightly decrease in-domain negation performance on ChestXray14

## Why This Works (Mechanism)

### Mechanism 1
Reformulating alignment as conditional semantic comparison (via MCQ) enables negation discrimination that contrastive learning structurally cannot achieve. InfoNCE maximizes global image-text similarity, treating negation as a minor modifier since affirmative and negated prompts share disease terms. MCQ forces direct competition among affirmative, negative, and mixed candidates within the same forward passâ€”negation becomes a decision-critical semantic factor rather than a perturbation.

### Mechanism 2
Direction-specific cross-attention modules prevent alignment interference between Image-to-Text and Text-to-Image reasoning. I2T reasoning (image conditioning on text candidates) and T2I reasoning (text conditioning on image candidates) require asymmetric cues. Shared modules dilute subtle negation signals; separated modules preserve direction-specific semantics.

### Mechanism 3
Mixed prompts (hybrid affirmative-negated statements) improve out-of-distribution generalization by enforcing finer-grained semantic parsing. Mixed prompts like "There is Fibrosis but no Atelectasis" prevent shortcut learning where models attend only to negation cues; they must parse the full compositional semantics.

## Foundational Learning

- **Concept: InfoNCE contrastive loss**
  - Why needed here: Understanding why standard fine-tuning fails for negation requires recognizing that InfoNCE optimizes for global similarity, not semantic distinction.
  - Quick check question: Given an image of healthy lungs, would InfoNCE distinguish between "There is pneumonia" and "There is no pneumonia" if both share the word "pneumonia"?

- **Concept: Cross-attention fusion**
  - Why needed here: The architecture replaces cosine similarity with cross-attention for modality alignment; practitioners must understand query/key/value roles in conditional fusion.
  - Quick check question: In I2T cross-attention, which modality provides the query and which provides keys/values?

- **Concept: Multi-label class imbalance**
  - Why needed here: Medical datasets are dominated by negative findings; this creates "easy positive" alignment problems that Bi-MCQ explicitly addresses.
  - Quick check question: Why does high prevalence of "no finding" samples create low-information training signals in contrastive learning?

## Architecture Onboarding

- **Component map:**
  Image Encoder -> Global + Local embeddings (v_g, V_l)
  Text Encoder -> Global + Token embeddings (t_g, T_l)
  I2T Cross-Attention: Q=image global, K/V=text global+tokens -> MLP -> S_I2T logits
  T2I Cross-Attention: Q=text global, K/V=image global+patches -> MLP -> S_T2I logits
  Bi-MCQ Data Constructor: Batch annotations -> {POS, NEG, HYB} prompts for I2T/T2I

- **Critical path:**
  1. Batch construction must generate semantically consistent MCQ options (correct answer exists for each query)
  2. Cross-attention modules must remain separate through backpropagation
  3. Inference uses PNC (Positive-Negative Combined) via softmax over POS/NEG logits

- **Design tradeoffs:**
  - Mixed prompts: Better OOD generalization vs. slight in-domain NEG degradation
  - Encoder freezing: Faster training vs. reduced fine-grained semantic adaptation
  - Batch size: Larger batches provide more MCQ candidates but increase memory

- **Failure signatures:**
  - Near-chance NEG AUC with strong POS AUC -> Model learned disease-name matching without negation semantics
  - Large POS-NEG AUC gap -> InfoNCE-style shortcut learning persisted
  - T2I significantly underperforming I2T -> Cross-attention modules may be incorrectly shared

- **First 3 experiments:**
  1. Baseline verification: Run zero-shot CARZero/MedKLIP on ChestXray14 with POS/NEG prompts; confirm near-chance NEG performance before fine-tuning.
  2. Ablation on CA separation: Compare shared vs. separated cross-attention with identical MCQ training; expect separated CA to show higher PNC AUC.
  3. Prompt composition study: Train with {POS, NEG} only vs. {POS, NEG, HYB}; evaluate on held-out dataset to quantify mixed-prompt regularization effect.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would closing the formal discrepancy between the multi-choice training objective and the inference protocol yield further performance improvements? The authors acknowledge the discrepancy but do not validate unified training vs. inference objectives.

- **Open Question 2:** Can the Bi-MCQ framework effectively transfer to non-medical vision-language domains where negation understanding is critical? All experiments are confined to chest X-ray datasets; no validation exists on general-domain VLMs or non-medical tasks.

- **Open Question 3:** What mechanisms could improve cross-dataset generalization when domain mismatch is severe, as observed with PadChest? The paper demonstrates Bi-MCQ improves in-domain and moderately out-of-distribution transfer but does not investigate techniques to address severe domain shift scenarios.

## Limitations
- Exact computational and training hyperparameters (batch size, epochs, cross-attention architecture details) are unspecified
- Evaluation is limited to AUC metrics without examining failure cases or model uncertainty calibration on negation predictions
- External validation is sparse, with only a subset of CheXpert and PadChest evaluated

## Confidence
- **High confidence:** Core claim that contrastive learning struggles with negation understanding; improvement in NEG and PNC metrics over zero-shot baselines
- **Medium confidence:** Direction-specific cross-attention modules are essential for preventing alignment interference; exact contribution of separated CA versus MCQ reform alone is not fully isolated
- **Medium confidence:** Benefit of mixed prompts for out-of-distribution generalization; slight decrease in ChestXray14 NEG performance suggests regularization trade-off

## Next Checks
1. **Ablation Study on Cross-Attention Architecture:** Train Bi-MCQ with shared cross-attention modules versus direction-specific ones while keeping all other components identical; compare I2T, T2I, POS, NEG, and PNC AUC metrics to quantify the precise contribution of the direction-specific design.

2. **Error Analysis and Calibration Study:** Perform detailed error analysis on NEG predictions, categorizing failures and plotting calibration curves for POS and NEG predictions to assess if the model has truly learned negation semantics or is exploiting dataset artifacts.

3. **Generalization Robustness Test:** Evaluate the fine-tuned model on a held-out subset of training data containing rare compositional negations (e.g., "There is Cardiomegaly but no Effusion"); compare performance on these compositional cases versus simple single-disease negations to test generalization to complex negation scenarios.