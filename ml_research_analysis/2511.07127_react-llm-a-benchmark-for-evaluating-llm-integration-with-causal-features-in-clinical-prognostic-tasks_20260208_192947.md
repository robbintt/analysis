---
ver: rpa2
title: 'REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features
  in Clinical Prognostic Tasks'
arxiv_id: '2511.07127'
source_url: https://arxiv.org/abs/2511.07127
tags:
- causal
- clinical
- llms
- features
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REACT-LLM, a benchmark designed to evaluate
  the integration of Large Language Models (LLMs) with causal features for clinical
  prognostic tasks. The benchmark assesses 15 LLMs, 6 traditional ML models, and 3
  causal discovery (CD) algorithms across 7 clinical outcomes using real-world datasets.
---

# REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks

## Quick Facts
- **arXiv ID:** 2511.07127
- **Source URL:** https://arxiv.org/abs/2511.07127
- **Reference count:** 40
- **Primary result:** LLMs lag traditional ML models by ~10-20% in clinical prognostic tasks; integrating causal features provides limited gains due to strict CD assumptions

## Executive Summary
This paper introduces REACT-LLM, a benchmark designed to evaluate how Large Language Models (LLMs) integrate with causal features for clinical prognostic tasks. The study assesses 15 LLMs, 6 traditional ML models, and 3 causal discovery algorithms across 7 clinical outcomes using real-world MIMIC-III/IV datasets. While LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. The research reveals that LLMs can serve effectively as knowledge-rich collaborators for identifying and optimizing causal features, and that different LLMs show varying sensitivity to structured data encoding formats.

## Method Summary
The benchmark evaluates models on 7 clinical outcomes using MIMIC-III and MIMIC-IV datasets (16,000 patients total). Six traditional ML models (LR, RF, XGBoost, SVM, DT, AdaBoost) serve as baselines with default hyperparameters. Three causal discovery algorithms (DirectLiNGAM, GES, CORL) identify causal feature sets. Fifteen LLMs are tested with 5 prompting strategies (Direct, CoT, Self-Reflection, Role-Playing, ICL) and 5 serialization formats (JSON, RCF, NS, TBNL, LaTeX). The key innovation is using LLMs to refine causal features derived from CD algorithms, addressing their tendency to produce spurious or incomplete feature sets.

## Key Results
- LLMs lag traditional ML baselines (like XGBoost) by approximately 10-20% in predictive performance
- Different LLMs show varying sensitivity to structured data encoding formats, with JSON benefiting open-source models and Narrative Serialization helping smaller models
- Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to strict assumptions of many CD methods being violated in complex clinical data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs compensate for the brittleness of algorithmic Causal Discovery (CD) by injecting domain knowledge to refine feature sets
- **Mechanism:** CD algorithms rely on strict statistical assumptions that are often violated in noisy, high-dimensional clinical EHR data, leading to incomplete or incorrect feature sets. LLMs map these statistically derived features to clinical pathophysiology, adding relevant features the algorithm missed and removing spurious correlations
- **Core assumption:** The LLM has internalized accurate clinical knowledge that maps correctly to the specific variable names used in the dataset
- **Evidence anchors:**
  - [abstract]: "LLMs serve effectively as knowledge-rich collaborators for identifying and optimizing causal features"
  - [section]: "LLM optimization is key for DirectLiNGAM feature sets... optimized DirectLiNGAM features consistently outperform their unoptimized counterparts"
  - [corpus]: Paper 35880 demonstrates feature importance from ML does not equate to causal relationships

### Mechanism 2
- **Claim:** Matching input serialization formats to a model's pretraining distribution significantly impacts predictive performance
- **Mechanism:** LLMs predict tokens based on patterns learned during pretraining. Open-source models (trained on code-heavy datasets) process hierarchical data structures (JSON) more efficiently, while proprietary models handle tabular formats (RCF) better. Smaller models lack capacity to parse rigid syntax and perform better with Narrative Serialization
- **Core assumption:** The performance delta is driven by the "familiarity" of the token sequence rather than semantic content
- **Evidence anchors:**
  - [abstract]: "Different LLMs show varying sensitivity to structured data encoding formats"
  - [section]: "RCF benefits proprietary large models, JSON consistently enhances performance in open-source models, while NS proves effective for smaller models"
  - [corpus]: Corpus signals emphasize that how information is presented is as critical as the information itself

### Mechanism 3
- **Claim:** In-Context Learning (ICL) stabilizes predictions in highly imbalanced clinical tasks by calibrating the model's priors
- **Mechanism:** Clinical datasets often have severe class imbalances. ICL provides demonstrations that implicitly define the decision boundary and output distribution, transiently aligning the model's internal state to task statistics without weight updates
- **Core assumption:** The examples provided in the context window are representative of the test set distribution and are labeled correctly
- **Evidence anchors:**
  - [abstract]: "In-context learning improves LLM predictions when prompts are tailored to the task and model"
  - [section]: "ICL outperforms direct prompting in 18 cases... ICL enhances performance: Gemini-2-Pro's AUROC increases by 0.077"
  - [corpus]: Weak/missing explicit corpus support for ICL specifically in this context

## Foundational Learning

- **Concept:** Causal Faithfulness & Acyclicity Assumptions
  - **Why needed here:** The paper shows CD algorithms often fail because real-world clinical data violates these assumptions, explaining why raw CD features didn't work and why LLM semantic filtering was necessary
  - **Quick check question:** Why would a CD algorithm identify "Wheelchair usage" as a cause of "Mobility issues" rather than a consequence, and how does this violate strict causal modeling?

- **Concept:** Data Serialization strategies (JSON vs. RCF vs. Narrative)
  - **Why needed here:** The architecture requires converting structured EHR rows into text prompts. The choice of serialization is not neutral; it is a hyperparameter that interacts with the model's architecture
  - **Quick check question:** If you are deploying a 7B parameter open-source model, which serialization format should you likely avoid, and which should you prefer?

- **Concept:** Prompt Engineering vs. Fine-Tuning
  - **Why needed here:** The benchmark tests strategies like Chain-of-Thought and Role-Playing. Understanding the difference between intrinsic model capability and transient context behavior is crucial for interpreting the "limited gains" reported
  - **Quick check question:** Why might "Role-Playing" prompts fail to improve performance if the model lacks the underlying clinical reasoning capacity?

## Architecture Onboarding

- **Component map:** MIMIC-III/IV -> Cohort selection (16k patients) -> Feature Extraction (Demographics, Vitals, Labs) -> Three CD algorithms (DirectLiNGAM, GES, CORL) -> Raw Causal Feature Sets -> Serialization (JSON/RCF/NS) -> Prompt Construction (Direct/CoT/ICL) -> LLM Inference -> LLM-assisted Causal Feature Editing

- **Critical path:** The most valuable path identified is not direct prediction, but the LLM-assisted Causal Feature Editing. Start by generating features via GES/DirectLiNGAM, then passing that list to an LLM (e.g., GPT-4o) for "optimization" (adding/removing features based on clinical logic) before running the final prediction

- **Design tradeoffs:**
  - **RCF vs. JSON:** RCF is token-efficient (cheap) but hurts performance for open-source models. JSON is verbose (expensive) but robust for open-source
  - **Recall vs. Precision:** LLMs lag behind ML baselines by ~10-20%. Using LLMs sacrifices raw predictive power for potentially better semantic alignment/interpretability
  - **CD Strictness:** DirectLiNGAM produces sparse/specific features (strict), while GES is more permissive. DirectLiNGAM features require more LLM optimization to be useful

- **Failure signatures:**
  - **DirectLiNGAM Degradation:** Using features from DirectLiNGAM directly usually degrades LLM performance compared to using all features
  - **Small Model Parsing Failure:** Small models often fail to extract correct values from RCF or LaTeX formats, requiring Narrative Serialization
  - **ML Dominance:** If an LLM unexpectedly beats XGBoost, verify for data leakage; the paper establishes that LLMs generally do not beat ML baselines

- **First 3 experiments:**
  1. **Format Sensitivity Check:** Run `Qwen3-8b` on a subset of data using both JSON and Narrative Serialization. Verify the paper's claim that JSON/Narrative outperforms RCF for this model class
  2. **Causal Refinement Loop:** Run `GES` to get a feature set. Prompt `GPT-4o` to "optimize" this list. Run predictions using (a) the raw GES set and (b) the optimized set. Compare AUROC deltas
  3. **The "ML Gap" Baseline:** Train a simple `Logistic Regression` or `XGBoost` model on the same features the LLM sees. Confirm the ML baseline is indeed higher to calibrate expectations for the LLM's performance ceiling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM-derived domain knowledge be formally integrated into the causal discovery process during graph construction, rather than strictly as post-hoc feature selection?
- **Basis in paper:** [explicit] The conclusion states that "incorporating LLM-guided priors into the CD process... offers a promising path," while the current study only evaluated post-hoc optimization
- **Why unresolved:** Current experiments limited integration to LLMs editing feature sets after algorithms like GES or DirectLiNGAM ran, rather than guiding the discovery logic itself
- **What evidence would resolve it:** A modified causal discovery algorithm that accepts LLM priors as constraints and outperforms standard CD methods on clinical benchmarks

### Open Question 2
- **Question:** Can causal discovery algorithms be modified to relax strict assumptions—such as causal faithfulness or the absence of hidden confounders—specifically for complex clinical environments?
- **Basis in paper:** [inferred] The authors attribute the limited gains of CD features to strict assumptions often violated in high-dimensional clinical data
- **Why unresolved:** Standard CD algorithms used in the study failed to consistently improve predictions, suggesting a mismatch with data properties
- **What evidence would resolve it:** Novel CD algorithms that allow for violated assumptions while maintaining predictive power in clinical settings

### Open Question 3
- **Question:** To what extent do specific pretraining corpora determine the optimal input encoding format for LLMs in clinical tasks?
- **Basis in paper:** [inferred] The paper finds varying sensitivities to formats based on model type and hypothesizes this links to pretraining data
- **Why unresolved:** The study identifies the correlation but does not causally link specific pretraining datasets to the preference for formats like JSON or Row-Column Format
- **What evidence would resolve it:** Ablation studies controlling for pretraining data content to isolate its effect on format sensitivity

## Limitations

- **Strict CD Assumptions:** Causal discovery algorithms rely on strict statistical assumptions (faithfulness, acyclicity) that are frequently violated in real-world clinical data
- **Modest Performance Gains:** The performance gains from LLM integration are modest, suggesting current approaches have not yet surpassed traditional ML baselines
- **Format Generalization Uncertainty:** Serialization format findings may not generalize across different model architectures or training corpora beyond those tested

## Confidence

**High Confidence:** The benchmark framework itself is sound, with robust methodology for comparing 15 LLMs, 6 ML models, and 3 CD algorithms across standardized clinical tasks. The finding that LLMs lag behind traditional ML models by ~10-20% is well-supported.

**Medium Confidence:** The mechanism by which LLMs serve as "knowledge-rich collaborators" for causal feature optimization is plausible but depends heavily on the specific clinical knowledge encoded in each model's pretraining data.

**Low Confidence:** The specific impact of In-Context Learning in highly imbalanced clinical tasks, while observed, lacks strong external validation.

## Next Checks

1. **Format Generalization Test:** Evaluate whether the JSON/Narrative serialization recommendations hold when testing with additional open-source models (e.g., Llama-3, Mistral) not included in the original benchmark.

2. **CD Assumption Violation Analysis:** Systematically test how different CD algorithms perform when their core assumptions are explicitly violated (e.g., introducing hidden confounders or cyclic relationships) to better understand the "spurious features" problem.

3. **LLM Knowledge Grounding Verification:** Conduct a qualitative analysis of LLM-generated feature optimizations to verify whether additions/removals are based on actual clinical knowledge versus statistical patterns learned from pretraining data.