---
ver: rpa2
title: 'Spatial Speech Translation: Translating Across Space With Binaural Hearables'
arxiv_id: '2504.18715'
source_url: https://arxiv.org/abs/2504.18715
tags:
- translation
- speech
- binaural
- expressive
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces spatial speech translation, the first real-time\
  \ binaural hearable system that translates multiple speakers in the wearer\u2019\
  s environment while preserving spatial cues and voice characteristics. The system\
  \ combines blind source separation, localization, simultaneous expressive translation,\
  \ and binaural rendering to handle noisy, reverberant, real-world conditions."
---

# Spatial Speech Translation: Translating Across Space With Binaural Hearables

## Quick Facts
- arXiv ID: 2504.18715
- Source URL: https://arxiv.org/abs/2504.18715
- Authors: Tuochao Chen; Qirui Wang; Runlin He; Shyam Gollakota
- Reference count: 40
- Primary result: Real-time binaural hearable system translating multiple speakers while preserving spatial cues and voice characteristics, achieving BLEU scores up to 22.01

## Executive Summary
This paper introduces the first real-time binaural hearable system that translates multiple concurrent speakers while preserving their spatial locations and voice characteristics. The system combines blind source separation with localization, simultaneous expressive translation, and binaural rendering to handle noisy, reverberant real-world conditions. By dividing the auditory space into angular regions and extracting clean binaural speech from each speaker, the system achieves robust separation performance. The integration of spatial perception into speech translation for wearable devices represents a significant advancement in assistive technology for hearing-impaired users and multilingual communication.

## Method Summary
The system consists of three main components: (1) a search-based joint localization and separation module using streaming TF-GridNet that divides 360° space into 36 angular regions and extracts clean binaural speech from each speaker, (2) a simultaneous S2T translation pipeline with expressive T2S vocoder that preserves prosody and speaker identity, and (3) binaural rendering that transfers spatial cues from input to translated output. The system processes binaural audio at 16kHz, trains on synthetic mixtures from CoVoST2 combined with WHAM! noise and 4 BRIR datasets, and achieves real-time performance with 3-4 second latency through chunk-based processing.

## Key Results
- Achieved BLEU scores up to 22.01 on real-world test data with background noise
- Preserved spatial cues with median angular error of 6.8° and ΔITD of 72.3 µs
- Improved speaker similarity with Vocal Style Similarity (VSim) of 0.25 using expressive vocoder
- Maintained separation performance with SI-SDRi improvements of 10.79-14.52 dB across test conditions

## Why This Works (Mechanism)

### Mechanism 1: Search-Based Angular Separation
The system divides the 360-degree auditory space into discrete angular regions, scanning each region to extract unknown speakers in real-time without prior speaker count knowledge. At each angle, it shifts the binaural input to align Time Difference of Arrival (TDoA) for that direction, and a streaming TF-GridNet processes this aligned input. If a source exists, aligned signals reinforce and are extracted; if no source exists, the network outputs silence. Clustering removes duplicates from multipath reflections.

### Mechanism 2: Robustness Fine-Tuning on Imperfect Inputs
Pre-training a translation model on clean data causes failure when fed distorted separation outputs containing residual interference and artifacts. The system mixes this imperfect separation output 50/50 with clean data and fine-tunes the S2T model to map these noisy embeddings to clean text, restoring performance.

### Mechanism 3: Dynamic Spatial Cue Transfer
Translation models introduce latency ($D$ chunks). Rather than applying spatial cues from the input as it was spoken $D$ chunks ago, the system applies spatial cues (ILD/ITD) from the current real-time input chunk to the delayed translated output as it is played back, ensuring the translated voice comes from the speaker's current location.

## Foundational Learning

- **Concept: Binaural Cue Extraction (ITD/ILD)**
  - Why needed here: ITD (Interaural Time Difference) and ILD (Interaural Level Difference) are how the human ear localizes sound; the system must extract these from raw mic input to know "where" the speaker is, and re-apply them to translated audio to make it sound spatial
  - Quick check question: Why does the paper use a generic HRTF for ITD estimation but measured ILD from the separated signal? (Answer: Generic HRTF is sufficient for time delays [ITD], but level differences [ILD] are heavily dependent on specific ear shape and frequency, which are better captured from the actual recording)

- **Concept: Streaming vs. Non-Streaming Inference**
  - Why needed here: Standard translation often processes a full sentence; this system requires "streaming" (chunk-based) inference, meaning the model must make decisions with strictly limited future context to minimize latency
  - Quick check question: What is the trade-off of increasing the chunk size in the Conformer encoder? (Answer: It improves accuracy by providing more context, but directly increases the "Average Lagging" [AL] latency experienced by the user)

- **Concept: Expressive Translation vs. Simple TTS**
  - Why needed here: Preserving "voice characteristics" isn't just about pitch; it's about prosody (rhythm, stress). A simple text-to-speech engine would lose the emotion of the original speaker, making it harder for the wearer to identify *who* said what in a crowded room
  - Quick check question: How does the system separate the content (what was said) from the style (how it was said)? (Answer: It uses a Text-to-Unit [T2U] model for content/linguistics and a frozen Expressive Encoder to extract a style embedding from the source audio to condition the vocoder)

## Architecture Onboarding

- **Component map:** Binaural Mics -> TF-GridNet Separation (36 angular regions) -> Streaming S2T Translation -> T2U + Expressive Vocoder -> HRTF + ILD Compensation -> Stereo Output
- **Critical path:** The Separation Layer must process audio at < 40ms RTF to feed the Translation Layer without buffering; the Translation Layer is the bottleneck, running on 960ms chunks, dictating overall latency (2-4 seconds)
- **Design tradeoffs:** Expressive vocoder improves speaker similarity (VSim 0.013 -> 0.250) but slightly lowers translation accuracy (ASR-BLEU 22.07 -> 18.54); 1-2s latency results in broken translations while 3-4s latency is preferred for semantic accuracy
- **Failure signatures:** "Phantom Speakers" from clustering failure identifying reverberation as unique speakers; "Garbled Translation" from upstream separation failure; "Static/Incorrect Spatialization" from ILD compensation or HRTF mismatch
- **First 3 experiments:** (1) Feed synthetic binaural mixture of 2 speakers at -30° and +45°; verify 2 distinct streams with median error ~6.8°; (2) Compare "Pre-trained only" vs. "Fine-tuned for separation" translation models; measure BLEU score degradation; (3) Have users listen to conversation with translation latencies at 1s, 3s, and non-simultaneous; survey for "breakdown of conversational flow"

## Open Questions the Paper Calls Out

- Can fine-tuning or knowledge distillation techniques successfully decouple noise preservation from voice characteristic preservation in the translation vocoder? (Basis: current expressive model achieves lower ASR-BLEU scores because frozen encoder/vocoder preserve input noise and distortion)
- To what extent can larger translation models (e.g., 2 billion parameters) be deployed on wearable devices to improve translation quality without violating real-time latency constraints? (Basis: current 175M parameter model is limited compared to state-of-the-art 2B parameter models)
- How can the system be adapted to dynamically balance the trade-off between translation latency and accuracy based on specific social or professional context? (Basis: participants requested domain-specific calibration, noting casual interactions require low latency while high-stakes applications require maximum accuracy)

## Limitations

- Requires controlled microphone placement and synchronized binaural recording hardware, limiting real-world deployment
- Search-based separation approach (36 angular regions) may struggle with speakers closer than resolution threshold or in highly reverberant environments
- 3-4 second latency, while acceptable for semantic accuracy, breaks conversational flow in interactive settings

## Confidence

- **High confidence:** Spatial cue preservation mechanism (ITD/ILD transfer) is well-validated through ΔITD (72.3 µs) and ΔILD (0.16 dB) measurements
- **Medium confidence:** Joint localization and separation works robustly in diverse acoustic conditions, though real-world validation beyond synthetic mixtures is limited
- **Medium confidence:** Expressive translation preserves voice characteristics (VSim 0.25), but user preference data is sparse

## Next Checks

1. Test system robustness with moving speakers at different speeds to validate the dynamic spatial cue transfer mechanism under motion
2. Evaluate performance degradation when reducing angular resolution from 36 to 18 regions to quantify the tradeoff between computational cost and separation quality
3. Conduct user studies comparing 1s, 3s, and 5s latency conditions in interactive conversation scenarios to better understand the real-world latency tradeoff