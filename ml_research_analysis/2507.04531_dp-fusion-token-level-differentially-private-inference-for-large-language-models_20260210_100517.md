---
ver: rpa2
title: 'DP-Fusion: Token-Level Differentially Private Inference for Large Language
  Models'
arxiv_id: '2507.04531'
source_url: https://arxiv.org/abs/2507.04531
tags:
- privacy
- private
- dp-fusion
- baseline
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of privacy leakage at inference
  time in large language models, where generated outputs can inadvertently reveal
  sensitive information from the context, such as personally identifiable information.
  The authors propose DP-Fusion, a token-level Differentially Private Inference (DPI)
  mechanism that provably bounds the influence of sensitive tokens on the LLM's output.
---

# DP-Fusion: Token-Level Differentially Private Inference for Large Language Models

## Quick Facts
- arXiv ID: 2507.04531
- Source URL: https://arxiv.org/abs/2507.04531
- Authors: Rushil Thareja; Preslav Nakov; Praneeth Vepakomma; Nils Lukas
- Reference count: 40
- Primary result: 6× lower perplexity than related DPI methods while providing formal privacy guarantees

## Executive Summary
This paper addresses privacy leakage at inference time in large language models, where generated outputs can inadvertently reveal sensitive information from the context. The authors propose DP-Fusion, a token-level Differentially Private Inference (DPI) mechanism that provably bounds the influence of sensitive tokens on the LLM's output. The method works by inferring the model with and without sensitive tokens, then blending the output distributions to ensure the final output remains within a bounded distance of the baseline.

## Method Summary
DP-Fusion introduces a token-level differential privacy mechanism for LLM inference that works by generating outputs both with and without sensitive tokens, then blending the resulting distributions. The approach ensures the final output remains within a bounded distance of the baseline inference, providing formal privacy guarantees. The method achieves this by computing the KL-divergence between distributions with and without sensitive tokens, then adjusting the output probabilities accordingly to satisfy differential privacy constraints.

## Key Results
- Achieves 6× lower perplexity compared to related DPI methods
- Provides formal privacy guarantees with average perplexity between 1.42–1.46
- Effective at privacy levels ε = 16–66 for document privatization tasks

## Why This Works (Mechanism)
The mechanism works by leveraging the probabilistic nature of LLMs and the mathematical properties of differential privacy. By computing the model's output distributions with and without sensitive tokens, DP-Fusion can bound the maximum influence any single token can have on the final output. The blending process ensures that sensitive information cannot disproportionately affect the generated text, while still maintaining reasonable perplexity and coherence.

## Foundational Learning
- Differential Privacy: Mathematical framework for quantifying privacy guarantees
  - Why needed: Provides formal guarantees about how much individual data points can influence outputs
  - Quick check: Verify ε values are within acceptable privacy bounds for the use case

- Perplexity: Measure of how well a probability model predicts a sample
  - Why needed: Standard metric for evaluating language model quality and coherence
  - Quick check: Compare perplexity values against baseline models to ensure acceptable performance

- KL-divergence: Measure of difference between two probability distributions
  - Why needed: Used to quantify the difference between outputs with and without sensitive tokens
  - Quick check: Ensure KL-divergence calculations are correctly implemented and bounded

## Architecture Onboarding

**Component Map:**
LLM Model -> Token-level Analysis -> Distribution Blending -> Privacy-bounded Output

**Critical Path:**
1. Identify sensitive tokens in input context
2. Generate output distributions with and without sensitive tokens
3. Compute KL-divergence between distributions
4. Blend distributions to achieve privacy bounds
5. Generate final output

**Design Tradeoffs:**
- Privacy vs. Perplexity: Stronger privacy guarantees typically increase perplexity
- Computational overhead: Requires dual inference passes for each generation step
- Token granularity: Tradeoff between fine-grained privacy control and computational efficiency

**Failure Signatures:**
- High perplexity indicating poor blending of distributions
- Inconsistent privacy guarantees across different token types
- Computational bottlenecks from dual inference requirements

**First Experiments to Run:**
1. Baseline comparison: Run standard LLM inference vs. DP-Fusion on same inputs
2. Privacy parameter sweep: Test different ε values to understand privacy-perplexity tradeoff
3. Token sensitivity analysis: Identify which token types most affect privacy guarantees

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses specifically on text-based sensitive content rather than arbitrary sensitive information types
- Experimental evaluation constrained to document privatization tasks
- Privacy guarantees are theoretical without empirical validation against real privacy attacks

## Confidence
- Privacy guarantees: High - The differential privacy mechanism is formally specified with clear bounds
- Perplexity improvements: Medium - Results are demonstrated but limited to specific experimental conditions
- Practical privacy protection: Low - No empirical validation against real privacy attacks

## Next Checks
1. Test DP-Fusion against adversarial extraction attacks to verify the practical privacy protection matches theoretical bounds
2. Evaluate performance on diverse real-world datasets beyond curated document privatization tasks
3. Assess computational overhead and latency impact in production LLM serving environments