---
ver: rpa2
title: 'N-EIoU-YOLOv9: A Signal-Aware Bounding Box Regression Loss for Lightweight
  Mobile Detection of Rice Leaf Diseases'
arxiv_id: '2601.09170'
source_url: https://arxiv.org/abs/2601.09170
tags:
- leaf
- gradient
- loss
- rice
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: N-EIoU-YOLOv9 is a lightweight mobile rice leaf disease detection
  system that introduces a novel signal-aware bounding box regression loss called
  N-EIoU. The method combines non-monotonic gradient focusing (from N-IoU) with geometric
  decoupling (from EIoU) to enhance localization accuracy for small, low-contrast
  disease lesions.
---

# N-EIoU-YOLOv9: A Signal-Aware Bounding Box Regression Loss for Lightweight Mobile Detection of Rice Leaf Diseases

## Quick Facts
- arXiv ID: 2601.09170
- Source URL: https://arxiv.org/abs/2601.09170
- Reference count: 40
- Primary result: 90.3% mAP@50 on rice leaf disease detection with 4.3% improvement over CIoU baseline

## Executive Summary
N-EIoU-YOLOv9 introduces a novel signal-aware bounding box regression loss that combines non-monotonic gradient focusing with geometric decoupling to enhance small object detection in mobile scenarios. The method addresses the challenge of detecting tiny, low-contrast disease lesions on rice leaves by reshaping regression gradients to amplify signals for hard samples while preventing gradient interference between width and height optimization. Deployed on Android devices using TensorFlow Lite with Float16 quantization, the system achieves 90.3% mAP@50 with 156ms inference time per frame.

## Method Summary
The approach modifies YOLOv9t's regression loss by replacing CIoU with N-EIoU, which combines N-IoU's non-monotonic gradient amplification (n=9) with EIoU's decoupled width/height penalties. The loss function enhances gradient signals for low-IoU samples (0.1-0.4 range) while maintaining stable optimization. The model is trained for 200 epochs using SGD with momentum 0.937 on a self-collected dataset of 5,908 rice leaf images, then exported to TensorFlow Lite with Float16 quantization for mobile deployment.

## Key Results
- Achieves 90.3% mAP@50 on rice leaf disease detection, 4.3% improvement over CIoU baseline
- Maintains 156ms inference time per frame on Android devices with Float16 quantization
- Shows highest performance on the Rice Hispa class (95.4% AP) while maintaining reasonable performance on smaller lesions (81.1% AP for Leaf Blast, 75.4% AP for Brown Spot)

## Why This Works (Mechanism)
The N-EIoU loss reshapes gradient distributions to amplify signals for hard samples with low overlap while preventing gradient interference between width and height optimization. By combining non-monotonic gradient focusing (N-IoU) with geometric decoupling (EIoU), the loss creates stronger optimization signals for small, difficult-to-localize objects. The n=9 hyperparameter in N-IoU creates a Dice-like amplification effect in the low-IoU regime, while the decoupled penalties ensure width and height gradients don't interfere with each other during optimization.

## Foundational Learning
- **IoU-based Loss Functions**: Understanding baseline IoU, CIoU, and EIoU is essential to grasp what N-EIoU changes. Quick check: Can you explain why standard IoU suffers from gradient vanishing when boxes don't overlap?
- **Gradient Analysis for Loss Design**: The core innovation of N-EIoU is reshaping gradient distributions; understanding how to analyze gradient magnitude vs. IoU curves is critical. Quick check: What does a non-monotonic gradient curve mean, and how does it differ from monotonic attenuation?
- **Model Quantization (Float16)**: The deployment uses Float16 quantization; understanding precision-compression tradeoffs is necessary for edge deployment. Quick check: What accuracy loss is acceptable when quantizing from Float32 to Float16 for this application?

## Architecture Onboarding
- **Component map**: YOLOv9t (GELAN blocks) -> N-EIoU loss (N-IoU + EIoU) -> TFLite Float16 quantization -> Android deployment
- **Critical path**: Replace CIoU with N-EIoU in regression head -> Train 200 epochs (SGD, momentum 0.937) -> Export to TFLite Float16 -> Deploy on Android
- **Design tradeoffs**: YOLOv9t chosen over larger variants for mobile feasibility (2M parameters); Float16 quantization stable with 0.1% mAP drop; 6 FPS sufficient for static photos but not real-time video
- **Failure signatures**: False positives on Brown Spot from dirt/mud confusion; lower AP on small lesions (75-81%); Int8 quantization instability
- **First 3 experiments**: 1) Gradient simulation: Plot gradient magnitude vs. IoU for N-EIoU vs. CIoU; 2) Ablation: Compare N-IoU only, EIoU only, and N-EIoU; 3) Quantization sweep: Compare Float32, Float16, and Int8 on mobile

## Open Questions the Paper Calls Out
- **Open Question 1**: Can stabilization techniques enable Int8 quantization to double inference speed? The authors identified Int8 instability but didn't investigate causes or mitigation strategies.
- **Open Question 2**: Does expanding the dataset with extreme lighting and diverse backgrounds reduce Brown Spot false positives? Current dataset lacks sufficient background noise and illumination variability.
- **Open Question 3**: Can model pruning integrate with N-EIoU to achieve >15 FPS on mobile without accuracy loss? Current 6 FPS is insufficient for video scanning.

## Limitations
- Proprietary dataset (DUNG_BK65) not publicly available, preventing independent validation
- No comparative analysis against other mobile detectors like YOLO-NAS or NanoDet
- No cross-dataset validation to demonstrate generalization beyond rice leaf diseases

## Confidence
- **High confidence**: Mathematical formulation of N-EIoU combining N-IoU and EIoU components; reported mAP@50 improvement on self-collected dataset
- **Medium confidence**: Generalization of results due to proprietary dataset; mobile deployment claims without hardware specifications
- **Low confidence**: Deployment details (Android hardware, quantization process, post-processing overhead)

## Next Checks
1. Implement gradient analysis to verify N-EIoU produces non-monotonic amplification in 0.1-0.4 IoU range using synthetic bounding boxes
2. Conduct ablation studies isolating N-IoU and EIoU components to quantify individual contributions to 4.3% mAP@50 improvement
3. Test quantization stability across Float16 and Int8 on different mobile hardware to verify 0.1% mAP drop and identify Int8 instability conditions