---
ver: rpa2
title: 'INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability
  and Knowledge Profiling'
arxiv_id: '2505.16303'
source_url: https://arxiv.org/abs/2505.16303
tags:
- routing
- knowledge
- score
- llms
- capability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InferenceDynamics, a multi-dimensional routing
  framework that models the capability and knowledge profiles of Large Language Models
  (LLMs) to intelligently match queries with the most suitable models. The framework
  addresses the challenge of efficiently navigating a diverse landscape of specialized
  LLMs by extracting required capabilities and domain-specific knowledge from queries,
  quantifying LLM proficiencies, and routing queries based on weighted knowledge and
  capability scores.
---

# INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling

## Quick Facts
- **arXiv ID:** 2505.16303
- **Source URL:** https://arxiv.org/abs/2505.16303
- **Reference count:** 31
- **Primary result:** Multi-dimensional LLM routing framework achieving 74.55 average score on 4 benchmarks, outperforming best single model by 1.28 points

## Executive Summary
This paper introduces InferenceDynamics, a multi-dimensional routing framework that models the capability and knowledge profiles of Large Language Models (LLMs) to intelligently match queries with the most suitable models. The framework addresses the challenge of efficiently navigating a diverse landscape of specialized LLMs by extracting required capabilities and domain-specific knowledge from queries, quantifying LLM proficiencies, and routing queries based on weighted knowledge and capability scores. Evaluated on a comprehensive dataset RouteMix, which aggregates 24 diverse benchmarks, InferenceDynamics achieved the highest average score of 74.55 across four challenging benchmarks (MMLU-Pro, GPQA, BigGenBench, LiveBench), outperforming the top single LLM (Gemini-1.5-Pro) by 1.28 points. Under cost constraints, the framework maintained competitive performance at approximately half the budget of the best single model, demonstrating its ability to effectively identify and leverage top-performing models while optimizing resource utilization.

## Method Summary
InferenceDynamics operates through a two-phase profiling and routing pipeline. First, it constructs an Index Set by sampling from 20 diverse benchmarks to profile 8 candidate LLMs across 7 capability categories (reasoning, comprehension, instruction following, agentic, knowledge retrieval, coding, multilingual) and 14+ knowledge domains. For each query, GPT-4o-mini extracts ranked capabilities and knowledge domains, which are then clustered using MiniLM-L6 embeddings (cosine similarity >0.6) with low-frequency domains (<10) aggregated into "Other". Models are scored on the Index Set using a weighted formula S^α_β that combines per-domain scores with optional cost penalties. During routing, incoming queries are analyzed for required capabilities (Cx) and knowledge domains (Kx), and each model receives a knowledge score (KS) and capability score (CS). The final routing decision selects the model maximizing γ·KS + δ·CS, with γ=δ=1.0 by default.

## Key Results
- Achieved highest average score of 74.55 across four challenging benchmarks (MMLU-Pro, GPQA, BigGenBench, LiveBench)
- Outperformed best single LLM (Gemini-1.5-Pro) by 1.28 points
- Maintained competitive performance at approximately half the budget of the best single model under cost constraints
- Effectively identified top-performing models for different task types, with Qwen-Max selected for 91% of coding tasks

## Why This Works (Mechanism)
The framework works by creating structured, quantifiable profiles of both queries and models across multiple dimensions. By extracting specific capability requirements and knowledge domains from each query, and maintaining detailed proficiency scores for each model across these same dimensions, the routing system can make informed decisions rather than relying on single-score rankings. The knowledge clustering approach handles the inherent ambiguity in domain extraction by grouping similar concepts while preserving specificity for common domains. The weighted scoring mechanism allows for flexible trade-offs between knowledge expertise and general capability, while the cost-aware formulation enables budget optimization without sacrificing too much accuracy.

## Foundational Learning
- **Knowledge Domain Extraction:** Using GPT-4o-mini to identify required knowledge domains from queries. Why needed: Queries often span multiple domains and require precise identification for accurate routing. Quick check: Verify extracted domains cover all major concepts in sample queries.
- **Capability Profiling:** Systematically categorizing queries into 7 capability types. Why needed: Different models excel at different capabilities, requiring multi-dimensional assessment. Quick check: Confirm capability distribution matches expected patterns across benchmarks.
- **Knowledge Clustering:** Using MiniLM-L6 embeddings with cosine similarity >0.6 to group similar domains. Why needed: Prevents sparse routing categories while maintaining meaningful distinctions. Quick check: Validate that semantically similar domains cluster together.
- **Cost-Aware Scoring:** Incorporating cost penalties into model scoring with parameter β. Why needed: Enables budget optimization without complete accuracy sacrifice. Quick check: Verify cost reduction correlates with acceptable performance drops.
- **Benchmark Sampling:** Selecting diverse queries from 20 benchmarks for profiling. Why needed: Ensures comprehensive coverage of knowledge domains and capabilities. Quick check: Confirm Index Set covers domains appearing in evaluation benchmarks.

## Architecture Onboarding
- **Component Map:** Query -> GPT-4o-mini Extractor -> Knowledge Clusterer -> Model Profiler -> Router -> Selected Model
- **Critical Path:** The bottleneck is the GPT-4o-mini extraction step, which must process every incoming query before routing decisions can be made.
- **Design Tradeoffs:** The framework trades some accuracy for efficiency by using MiniLM-L6 for knowledge clustering rather than more expensive methods, and by relying on a single judge model for evaluation.
- **Failure Signatures:** Poor knowledge clustering (too many "Other" categories), routing collapse to single model (cost penalty too low), or performance below single-model baseline (incomplete Index Set coverage).
- **First Experiments:** 1) Verify Index Set sampling covers all evaluation domains, 2) Test knowledge clustering with varied similarity thresholds, 3) Benchmark routing decisions against random and cost-blind baselines.

## Open Questions the Paper Calls Out
None

## Limitations
- Key design choices (number of trials K, normalization method, prompt templates) are unspecified, preventing exact replication
- Results are sensitive to the choice of GPT-4o-mini for both knowledge extraction and evaluation judging
- Framework's generalization to new, unseen domains is untested beyond the four evaluation benchmarks
- Cost values and measurement methodology are not specified, making budget claims difficult to verify

## Confidence
- **Routing performance claim (74.55 avg score, +1.28 over best single model):** Medium confidence
- **Cost-efficiency claim (competitive at half budget):** Medium confidence
- **Knowledge clustering and extraction robustness:** Low confidence

## Next Checks
1. Clarify and fix K, normalization, and prompt templates: Reconstruct the Index Set evaluation pipeline with explicit values for the number of trials, normalization method, and prompt templates per benchmark. Run a small-scale pilot to confirm that routing performance matches the paper's claims within 2 points on MMLU-Pro.
2. Benchmark sensitivity and generalization: Test the routing framework on two additional out-of-distribution benchmarks not used in training (e.g., HumanEval for coding, HellaSwag for reasoning). Compare against a random router and a cost-blind router to isolate the contribution of structured capability profiling.
3. Knowledge extraction ablation: Vary the cosine similarity threshold (0.5–0.8) and minimum frequency (5–15) in knowledge clustering, and compare routing accuracy. Also test alternative knowledge extractors (e.g., GPT-4o) to assess brittleness to the choice of extractor model.