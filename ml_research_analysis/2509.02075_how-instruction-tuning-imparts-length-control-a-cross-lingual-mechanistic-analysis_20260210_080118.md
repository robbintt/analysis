---
ver: rpa2
title: 'How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic
  Analysis'
arxiv_id: '2509.02075'
source_url: https://arxiv.org/abs/2509.02075
tags:
- words
- length
- control
- generation
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed instruction-tuned and base Llama 3.1 8B models'
  performance on word count-constrained text generation in English and Italian. The
  instruction-tuned model outperformed its base counterpart, achieving near-zero mean
  errors, while the base model frequently over-generated text.
---

# How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis

## Quick Facts
- **arXiv ID**: 2509.02075
- **Source URL**: https://arxiv.org/abs/2509.02075
- **Authors**: Elisabetta Rocchetti; Alfio Ferrara
- **Reference count**: 6
- **Primary Result**: Instruction-tuned Llama 3.1 8B outperforms base model on word-count constrained generation, with attention heads in later layers crucial for English and final-layer MLPs compensating in Italian.

## Executive Summary
This study analyzes how instruction-tuning enables large language models to adhere to word count constraints in text generation. Using Llama 3.1 8B models in both base and instruction-tuned configurations, the researchers found that instruction-tuned models significantly outperform their base counterparts on length-constrained tasks in both English and Italian. The analysis reveals that later-layer attention heads drive success in English, while final-layer MLPs provide compensatory mechanisms in Italian. These findings suggest instruction-tuning reconfigures deeper model layers to prioritize task adherence over general language modeling priors.

## Method Summary
The researchers compared Llama 3.1 8B base and instruction-tuned models on word count-constrained generation tasks in English and Italian. They used Cumulative Weighted Attribution (CWA), derived from Direct Logit Attribution, to analyze component-level contributions to constraint adherence. The study examined attention heads and MLPs across all 32 layers, focusing on how different components contribute to successful length control. Experiments tested generation with specific word counts (N=3 to 9), analyzing both accuracy and the underlying mechanisms driving performance differences between base and instruction-tuned models.

## Key Results
- Instruction-tuned models achieved near-zero mean errors on length-constrained generation, while base models consistently over-generated text
- Later-layer attention heads (layers 24-31) showed increasingly positive contributions to constraint adherence in English
- Final-layer MLPs exhibited stronger positive contributions in Italian, suggesting compensatory mechanisms for language-specific challenges
- Base models showed markedly negative CWA in later layers, indicating active opposition to constraint satisfaction

## Why This Works (Mechanism)

### Mechanism 1: Deep-Layer Attention Specialization (Primary Driver)
- **Claim**: Instruction-tuning reconfigures attention heads in layers 24-31 to actively promote constraint satisfaction, particularly in English
- **Mechanism**: Attention heads learn to attend to the numeric constraint and generated context, contributing positively to logits that signal correct termination
- **Core assumption**: The model can bind the abstract concept of "count" to specific token positions, and DLA captures true functional contribution
- **Evidence anchors**: Positive CWA trends begin around layer 24, particularly pronounced for English; BASE models show negative CWA in later layers
- **Break condition**: This mechanism attenuates in lower-resource languages where instruction-following circuits are weaker

### Mechanism 2: Final-Layer MLP Compensation (Secondary/Backup)
- **Claim**: When attention mechanisms are attenuated (Italian), final-layer MLPs play stronger compensatory roles in enforcing constraints
- **Mechanism**: MLPs intervene to correct residual stream trajectories, increasing probability of adherence at final processing stage
- **Core assumption**: MLPs can perform "cleanup" logic distinct from knowledge retrieval
- **Evidence anchors**: Italian experiments show notable CWA increase in final layer's MLP; English shows near-zero MLP contribution
- **Break condition**: If language is unseen or constraint is structurally foreign to MLP patterns, compensation may fail

### Mechanism 3: Inhibition of General Language Modeling Priors
- **Claim**: Base models fail at length control because deep-layer components actively oppose termination to continue natural text flow
- **Mechanism**: Base models optimize for general continuation; deep-layer components contribute negatively to constraint satisfaction
- **Core assumption**: Default pre-trained behavior maximizes plausible continuation, conflicting with artificial constraints
- **Evidence anchors**: BASE model consistently failed by over-generating; later-stage components show markedly negative CWA
- **Break condition**: Overridden in IT models where instruction-tuning penalizes "continue generating" prior

## Foundational Learning

**Concept: Residual Stream Decomposition**
- **Why needed here**: DLA relies on output logits being a linear function of the residual stream; understanding that final output sums embeddings plus all component outputs is essential
- **Quick check question**: If I remove the output of Layer 10's MLP from the residual stream, how does that change the final logits?

**Concept: Attribution vs. Causation**
- **Why needed here**: CWA is diagnostic, not proven causal; understanding this distinction is critical for interpreting results
- **Quick check question**: If a component has high positive CWA score, does that prove it is necessary for the task? (Answer: No, could be correlational)

**Concept: Tokenization vs. Word Count**
- **Why needed here**: LLMs operate on tokens, not words; single words can be multiple tokens, making counting non-trivial
- **Quick check question**: Why might a model generate 4 words when asked for 3, even if it is "trying" to stop?

## Architecture Onboarding

**Component map**: Prompt -> Embedding -> Residual Stream -> [32 Layers: Attention + MLP] -> Final Layer MLP -> Unembedding -> Logits

**Critical path**:
1. Prompt enters -> Embedded into Residual Stream
2. Layers 0-23: General language processing (minor difference between Base/IT)
3. **Layers 24-31 (The "Decision Circuit")**: In IT models, Attention Heads read constraint and write "stop" signals
4. **Final Layer MLP**: If Attention signals weak (e.g., Italian), MLP steps in to boost adherence
5. Unembedding: Residual stream projects to vocabulary logits to select next token (or EOS)

**Design tradeoffs**:
- English vs. Multilingual: Attention head optimization works well for English but brittle for other languages, forcing MLP compensation
- Base vs. IT: Base models prioritize fluency/completion; IT models sacrifice generative freedom for constraint enforcement

**Failure signatures**:
- Late termination (Over-generation): Deep layers output negative contributions to constraint
- Early termination: Observed in IT models (negative mean error), suggesting over-cautious stopping
- N=3 Dip: Performance and positive attribution drop specifically at N=3, suggesting difficulty with single-digit constraints

**First 3 experiments**:
1. Reproduce CWA Plot: Run Llama 3.1 IT model on "Generate exactly N words" for N=3 to 9. Compute CWA for final layer's MLP. Verify near-zero for English but positive for Italian.
2. Ablation Study: Zero-out outputs of high-CWA attention heads (Layer 30) during generation. Observe if model loses English counting ability.
3. Cross-lingual Transfer Check: Test English-specialized attention heads on low-resource language (e.g., Luxembourgish). Check if MLP compensation activates or model fails entirely.

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies on DLA/CWA metrics that show correlation but don't establish causation; ablation studies needed for causal proof
- Findings limited to English and Italian; generalization to truly low-resource or unseen languages unproven
- Distinction between genuine constraint-processing limitations and tokenization-induced counting errors not fully explored
- Results specific to Llama 3.1 8B decoder-only architecture; may not transfer to other architectures or parameter regimes

## Confidence

**High Confidence**:
- Instruction-tuned models outperform base models on length-constrained generation in both English and Italian
- Later-layer attention heads in IT models show positive CWA contributions in English
- Final-layer MLPs exhibit compensatory positive contributions in Italian

**Medium Confidence**:
- The attentional "decision circuit" (layers 24-31) is specialized for constraint adherence in English
- MLP compensation represents a distinct language-specific strategy rather than random variation

**Low Confidence**:
- Exact causal mechanism by which attention heads "recognize" word count constraints
- Whether N=3 performance dip reflects fundamental limitation or dataset artifact
- Generalizability to languages beyond studied pair or to non-synthetic instructions

## Next Checks
1. **Causal Ablation Study**: Implement targeted ablation of high-CWA attention heads (layers 24-31 for English) and final-layer MLPs (for Italian). Measure degradation in length control accuracy to establish necessity and convert correlational evidence into causal proof.

2. **Cross-Lingual Transfer Experiment**: Test English-specialized attention heads on genuinely low-resource or unseen languages (e.g., Luxembourgish). Measure whether MLP compensation activates or if model fails entirely, revealing mechanism's robustness boundaries.

3. **Tokenization Control Study**: Create controlled dataset where word-token alignment is perfect (using languages with consistent tokenization or synthetic vocabularies). Compare length control performance against original study to isolate whether errors stem from constraint processing or tokenization artifacts.