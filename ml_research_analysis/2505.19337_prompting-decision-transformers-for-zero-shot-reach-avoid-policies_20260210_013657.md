---
ver: rpa2
title: Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies
arxiv_id: '2505.19337'
source_url: https://arxiv.org/abs/2505.19337
tags:
- avoid
- state
- radt
- learning
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RADT introduces a decision transformer that learns reach-avoid
  policies from random offline data without rewards or cost functions. It encodes
  goals and avoid regions as prompt tokens, enabling zero-shot generalization to new
  avoid-region sizes and counts.
---

# Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies

## Quick Facts
- **arXiv ID:** 2505.19337
- **Source URL:** https://arxiv.org/abs/2505.19337
- **Authors:** Kevin Li; Marinka Zitnik
- **Reference count:** 40
- **Primary result:** Zero-shot reach-avoid generalization to new avoid-region configurations without retraining

## Executive Summary
RADT introduces a decision transformer that learns reach-avoid policies from random offline data without rewards or cost functions. It encodes goals and avoid regions as prompt tokens, enabling zero-shot generalization to new avoid-region sizes and counts. Using goal and avoid-region hindsight relabeling, RADT trains entirely from suboptimal trajectories. Across 11 tasks, RADT matches or exceeds baselines retrained on new avoid-region configurations, achieving up to 35.7% improvement in normalized cost in zero-shot settings while maintaining high goal success rates. In cell reprogramming, RADT reduces visits to unsafe gene expression states, adapting flexibly even under stochastic transitions.

## Method Summary
RADT builds on GPT-2-style causal transformers, using prompt tokens to specify goals and avoid regions rather than augmenting state vectors. The model learns from random offline trajectories via two-pass hindsight relabeling that generates paired demonstrations with opposite avoid-success labels. Training minimizes action prediction loss plus an auxiliary box-awareness loss predicting state violations. Attention boosting biases transformer focus toward prompt tokens. At evaluation, users specify arbitrary avoid regions via prompt tokens, enabling zero-shot adaptation without retraining.

## Key Results
- RADT achieves zero-shot generalization to new avoid-region sizes and counts without retraining
- Matches or exceeds baselines (AM-Lag, RbSL) retrained on new avoid-region configurations
- Reduces visits to unsafe gene expression states in cell reprogramming by up to 3.8×
- Maintains high goal success rates (SR) while improving normalized cost (MNC) by up to 35.7%

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Goal and Avoid Region Decoupling
Encoding goals and avoid regions as prompt tokens rather than augmented state vectors enables zero-shot generalization to novel configurations without retraining. The prompt structure `p = (z, ib, b1, ..., bnavoid, ig, g, e)` separates task specification from state representation. Unlike prior methods (RbSL, AM-Lag) that fix avoid region count in state space dimensionality (requiring retraining when n_avoid changes), RADT's prompt tokens can vary arbitrarily at evaluation time.

### Mechanism 2: Paired Hindsight Avoid-Region Relabeling
Generating paired trajectories with opposite avoid-success labels allows the model to isolate the concept of avoidance from trajectory-specific features. Two-pass relabeling creates (τ_orig, τ_copy) pairs where one has z=1 (successful avoid) and the other z=0 (unsuccessful), using the same underlying trajectory with different hindsight-sampled avoid regions.

### Mechanism 3: Box Awareness Auxiliary Loss
An auxiliary binary classification loss predicting whether current state violates any avoid box improves the model's spatial awareness of constraints. During training, RADT predicts both actions and a binary indicator k_t (whether state violates avoid boxes). The combined loss `L = L_action + α * L_avoid_awareness` trains explicit constraint monitoring.

## Foundational Learning

- **Concept: Decision Transformer Sequence Modeling**
  - **Why needed here:** RADT builds on causal transformers that predict actions autoregressively from trajectory sequences. Understanding that policy learning is framed as next-token prediction (not value optimization) is essential.
  - **Quick check question:** Can you explain how a decision transformer maps (state, action, reward) sequences to action predictions without explicit value functions?

- **Concept: Hindsight Experience Replay (HER)**
  - **Why needed here:** RADT extends HER from goals to avoid regions. Standard HER relabels terminal states as goals; RADT additionally samples avoid regions in hindsight and labels trajectories as successful/unsuccessful avoids.
  - **Quick check question:** Given a failed trajectory that didn't reach goal g, how would standard HER relabel it for training?

- **Concept: Offline RL Constraints**
  - **Why needed here:** RADT operates purely offline with no environment interaction. Understanding distribution shift, the impossibility of exploration, and why suboptimal data is the only available signal is critical for diagnosing failure modes.
  - **Quick check question:** Why can't offline RL methods safely query actions that weren't well-represented in the training distribution?

## Architecture Onboarding

- **Component map:**
  Input: Prompt p = (z, ib, b1...bn, ig, g, e) → Trajectory τ = (s_1, a_1, s_2, ...) → Embedding layers → GPT-2 causal transformer → Outputs: â_t, k̂_t

- **Critical path:**
  1. Prompt construction with avoid-success token z=1 at evaluation
  2. Embedding projection to shared hidden dimension
  3. Attention with bias boost to prompt tokens (a_delta hyperparameter)
  4. Autoregressive action prediction

- **Design tradeoffs:**
  - Box representation: Avoid regions as axis-aligned bounding boxes enables variable sizes but cannot represent complex shapes
  - Attention boosting (a_delta): Higher values improve instruction-following but may over-constrain; paper uses 1-2
  - Contour-based vs. uniform avoid sampling: Contour sampling improves FetchReach but requires convex/concave hull computation

- **Failure signatures:**
  - Low SR, low MNC: Model learned to avoid but not reach (over-conservative)
  - High SR, high MNC: Model learned to reach but ignores avoid constraints (prompt not attended)
  - Both degrade at large avoid-region counts: Capacity or attention limit reached
  - Long training time with no convergence: Check relabeling produces meaningful z=0/z=1 balance

- **First 3 experiments:**
  1. Validate relabeling quality: On small dataset, manually inspect that two-pass relabeling produces meaningful (z=0, z=1) pairs for same trajectory. Confirm ~50/50 split.
  2. Ablate attention boosting: Train with a_delta ∈ {0, 1, 2, 3} on FetchReachObstacle; plot SR and MNC vs. training steps. Expect: higher a_delta improves SR without degrading MNC.
  3. Test zero-shot generalization: Train with max box width 0.16, evaluate zero-shot on widths 0.18–0.24. Compare to retraining baseline. Expected outcome: RADT matches or exceeds retrained AM-Lag on MNC while maintaining SR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RADT's avoid region representation be extended to support complex non-rectangular shapes (e.g., polygons, ellipses, or arbitrary boundaries)?
- Basis in paper: [explicit] "the current iteration of RADT can only handle avoid boxes, and while we have been using conservative box sizes for avoid regions that are not inherently rectangular, applications that require tight boundaries for avoid regions of more complex shapes will be difficult for our model."
- Why unresolved: The current architecture encodes avoid regions as box coordinates (lower and upper bounds per dimension), which cannot represent curved or irregular boundaries.
- What evidence would resolve it: A modified architecture that encodes avoid regions using alternative representations (e.g., signed distance functions, mesh coordinates, or learned embeddings) and demonstrates comparable performance on tasks requiring precise non-rectangular avoidance.

### Open Question 2
- Question: Can RADT's training efficiency be improved to match or exceed baseline methods while maintaining its zero-shot generalization capabilities?
- Basis in paper: [explicit] "it takes a lot longer and a lot more computational resources to train compared to the baselines models, having many more parameters (as it is based on a GPT-2 architecture)."
- Why unresolved: The transformer-based architecture inherently requires more parameters and computation than the baseline neural network models.
- What evidence would resolve it: Demonstrating reduced training time through architectural modifications (e.g., smaller transformer variants, distillation, or efficient attention mechanisms) while preserving zero-shot generalization performance.

### Open Question 3
- Question: How does RADT scale to environments requiring long-range planning, hierarchical skill learning, or significantly higher-dimensional state spaces?
- Basis in paper: [explicit] "we do not evaluate on those, since the focus of those environments is testing for proficiency in long-range planning, skills learning, and hierarchical learning, which are not the focus of the current iteration of RADT."
- Why unresolved: The paper only evaluates on relatively simple environments (FetchReach, PointMaze, 15-gene Boolean network); complex multi-step planning remains untested.
- What evidence would resolve it: Evaluation on benchmark environments requiring multi-step reasoning (e.g., FetchPickAndPlace, AntMaze, or high-dimensional robotic manipulation) showing maintained zero-shot avoid generalization.

### Open Question 4
- Question: Can RADT's performance be further improved by incorporating mixed-quality offline data, including expert demonstrations alongside random-policy trajectories?
- Basis in paper: [inferred] The paper emphasizes learning from purely suboptimal random-policy data (Property 2), but does not investigate whether expert demonstrations could enhance learning efficiency or final policy quality.
- Why unresolved: The experimental design exclusively uses random-policy trajectories; the impact of data quality mix remains unexplored.
- What evidence would resolve it: Ablation studies comparing