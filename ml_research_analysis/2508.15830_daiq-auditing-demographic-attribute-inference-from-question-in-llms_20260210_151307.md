---
ver: rpa2
title: 'DAIQ: Auditing Demographic Attribute Inference from Question in LLMs'
arxiv_id: '2508.15830'
source_url: https://arxiv.org/abs/2508.15830
tags:
- demographic
- inference
- neutral
- female
- male
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAIQ, a diagnostic framework for auditing
  whether language models infer sensitive demographic attributes (gender, race, socioeconomic
  status, geographic location, educational background) from demographically neutral
  questions. The authors evaluate 18 open- and closed-source LLMs across six real-world
  domains using a curated dataset of neutral queries.
---

# DAIQ: Auditing Demographic Attribute Inference from Question in LLMs

## Quick Facts
- arXiv ID: 2508.15830
- Source URL: https://arxiv.org/abs/2508.15830
- Authors: Srikant Panda; Hitesh Laxmichand Patel; Shahad Al-Khalifa; Amit Agarwal; Hend Al-Khalifa; Sharefah Al-Ghamdi
- Reference count: 40
- Most LLMs infer demographics from neutral questions at high rates, defaulting to socially dominant categories

## Executive Summary
This paper introduces DAIQ, a diagnostic framework for auditing whether language models infer sensitive demographic attributes from demographically neutral questions. The authors evaluate 18 open- and closed-source LLMs across six real-world domains using a curated dataset of neutral queries. They find that most models exhibit high response rates for unintended demographic inference, often defaulting to socially dominant categories with stereotype-aligned rationales. Statistical analysis reveals systematic directional biases, such as male and white defaults across attributes. The authors also demonstrate that inferred demographics condition downstream responses, creating silent personalization effects. They develop an abstention-oriented prompting strategy that substantially reduces unintended inference without requiring model fine-tuning.

## Method Summary
The DAIQ framework evaluates LLMs by presenting demographically neutral questions and measuring whether models infer demographic attributes (gender, race, socioeconomic status, geographic location, educational background) with specific values rather than abstaining. Using the AccessEval benchmark, researchers filter for neutral queries and apply structured inference prompts requiring JSON-formatted outputs with demographic attributions and reasoning. Response rates (RR) measure speculation frequency, while value-specific analysis tracks asymmetric defaults. An abstention guardrail prompt requiring "Unknown" output when evidence is insufficient substantially reduces inference rates. Evaluation spans 18 models including GPT-4o, Claude-3.5, Llama-3.1, and Qwen-2.5 across temperature 0.5 settings.

## Key Results
- Most models exhibit response rates >50% for demographic inference from neutral questions
- Systematic directional biases: male and white defaults across attributes (Cohen's d: 0.23-0.49, p<0.05)
- Inferred demographics condition downstream responses, creating silent personalization effects
- Abstention guardrail reduces response rates substantially, nearly eliminating gender/race inference for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs infer demographics from neutral inputs by applying learned statistical associations from pretraining data rather than abstaining under uncertainty.
- Mechanism: Models encode topic-demographic correlations from training corpus. When prompted to infer demographics, they retrieve these population priors and generate stereotype-aligned justifications post-hoc.
- Core assumption: Pretraining data contains systematic associations between linguistic patterns/topics and demographic attributes that persist through instruction tuning.
- Evidence anchors:
  - [abstract]: "defaulting to socially dominant categories with stereotype-aligned rationales...indicating reliance on learned population priors"
  - [section 4.1]: "This pattern suggests a strong majority default bias rather than random guessing, with abstention frequently replaced by fabrication of the dominant racial category"
  - [corpus]: No direct corpus validation of this specific prior-activation mechanism
- Break condition: If models were trained with explicit abstention conditioning for epistemically uncertain inferences, or if instruction tuning successfully overrode statistical priors.

### Mechanism 2
- Claim: Inferred demographics function as latent conditioning variables that systematically shape downstream response content and tone.
- Mechanism: Once a demographic inference is generated internally, it becomes implicit context that influences subsequent token generation, creating "silent personalization" without user consent.
- Core assumption: Generation is conditioned on all prior context, including the model's own speculative demographic attributions.
- Evidence anchors:
  - [abstract]: "inferred demographics condition downstream responses, creating silent personalization effects"
  - [section 4.4/Table 1]: "neutral responses are systematically closer to gender aligned conditioned responses than to misaligned ones...consistent directional alignment across all three evaluated models" (Cohen's d: 0.23-0.49, p<0.05)
  - [corpus]: Corpus does not address latent conditioning mechanisms
- Break condition: If inference and response generation were structurally decoupled, or if models treated speculative attributions as invalid context.

### Mechanism 3
- Claim: Explicit abstention instructions substantially reduce unintended demographic inference without fine-tuning.
- Mechanism: Guardrail prompt adds constraint requiring "Unknown" output when evidence is insufficient, leveraging instruction-following capability to override default speculation behavior.
- Core assumption: Instruction-tuned models can prioritize explicit abstention directives over implicit prior-driven generation.
- Evidence anchors:
  - [abstract]: "abstention-oriented prompting strategy that substantially reduces unintended inference without requiring model fine-tuning"
  - [section 5/Figure 3]: "sharp and systematic reduction in responses at the value level...gender and race inferences almost entirely eliminated"
  - [corpus]: No corpus validation of this specific prompting technique
- Break condition: If models lack sufficient instruction-following strength, or if priors are encoded at a level inaccessible to prompt-based intervention.

## Foundational Learning

- Concept: Epistemic uncertainty vs. speculation
  - Why needed here: DAIQ diagnoses when models fail to recognize insufficient evidence and speculate anyway.
  - Quick check question: Given "How do I improve my credit score?" with no demographic cues, should a model infer the asker's income level? Why or why not?

- Concept: Statistical priors as implicit bias
  - Why needed here: The paper shows models default to dominant categories (male, white, high-SES, urban, high-education) via learned correlations, not random guessing.
  - Quick check question: If a model infers "White" from a neutral cruise question, what type of training data artifact might cause this?

- Concept: Silent personalization harms
  - Why needed here: Inferred demographics shape responses users receive without transparency, affecting healthcare, finance, and education outcomes.
  - Quick check question: How might a finance chatbot's investment advice differ if it infers "high income" vs. "low income" from the same neutral question?

## Architecture Onboarding

- Component map:
  - Probe Dataset -> Inference Elicitation Prompt -> Response Rate Metric -> Value-Specific Analysis -> Directional Alignment Test -> Abstention Guardrail

- Critical path:
  1. Filter queries for demographic neutrality -> 2. Apply inference prompt -> 3. Parse JSON output -> 4. Count inferences vs. abstentions -> 5. Calculate value-specific rates -> 6. Test guardrail effectiveness

- Design tradeoffs:
  - Binary categories (Male/Female, White/Black) enable controlled measurement but obscure intersectional complexity (acknowledged in Limitations)
  - Zero-shot evaluation avoids fine-tuning artifacts but may under- or over-estimate production behavior
  - Temperature 0.5 balances reproducibility with natural variation; paper shows results stable across 0.0-1.0 (Appendix A.6)

- Failure signatures:
  - Response rate >50% on any attribute -> epistemic overreach
  - Value-specific asymmetry (e.g., 85% male vs. 15% female) -> stereotyped defaults
  - Neutral responses consistently closer to one conditioned demographic -> silent personalization active
  - Guardrail fails to reduce response rates -> instruction-following insufficient for this model

- First 3 experiments:
  1. Run DAIQ inference prompt on 30 neutral queries from your domain; calculate response rates across 5 attributes to establish baseline speculation propensity.
  2. Apply abstention guardrail (Appendix A.4) to same queries; measure reduction in response rates. Paper shows near-elimination of gender/race inference for most models.
  3. For 5 queries where model inferred demographics, generate three responses: (a) neutral, (b) conditioned on inferred demographic, (c) conditioned on opposite demographic. Compute semantic similarity; alignment with (b) confirms silent personalization.

## Open Questions the Paper Calls Out

- Does the abstention-oriented prompting strategy generalize across languages and cultural contexts where demographic associations may differ from English?
- Can architectural interventions or fine-tuning provide more robust mitigation of unintended demographic inference than prompt-based guardrails alone?
- Do inferred demographic attributes compound or persist across multi-turn conversations, leading to accumulated personalization effects?
- How does DAIQ behavior manifest for intersectional identities that the current binary operationalization cannot capture?

## Limitations
- Dataset Scope: Evaluation relies on filtered subset of 212 neutral queries; full corpus validation across cultural contexts not provided
- Binary Classification: Uses binary gender and race categories, oversimplifying intersectional identities
- Model Coverage: Excludes models like Gemini due to lack of access; "future" model versions introduce uncertainty

## Confidence
- **High Confidence**: Core finding of systematic demographic inference from neutral prompts, with statistical analysis across multiple models and domains
- **Medium Confidence**: Silent personalization mechanism demonstrated through semantic similarity analysis, but acknowledges this only scratches surface of downstream effects
- **Low Confidence**: Mechanism claims rely on correlation rather than direct validation of training data artifacts; guardrail effectiveness across all neutral queries cannot be guaranteed

## Next Checks
1. Test DAIQ framework with multi-dimensional demographic categories beyond binary splits to assess intersectional inference patterns
2. Conduct controlled analysis of model pretraining data to identify whether observed demographic-topic correlations directly map to training corpus patterns
3. Evaluate whether abstention guardrail maintains effectiveness when deployed in real-world applications with diverse query distributions and user contexts