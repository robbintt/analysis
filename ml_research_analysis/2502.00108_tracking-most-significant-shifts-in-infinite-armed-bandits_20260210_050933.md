---
ver: rpa2
title: Tracking Most Significant Shifts in Infinite-Armed Bandits
arxiv_id: '2502.00108'
source_url: https://arxiv.org/abs/2502.00108
tags:
- regret
- log3
- algorithm
- bound
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies infinite-armed bandits with non-stationary rewards
  where actions' mean rewards evolve over time. Prior approaches relied on parameter
  knowledge of non-stationarity and restrictive distributional assumptions on the
  reservoir distribution.
---

# Tracking Most Significant Shifts in Infinite-Armed Bandits

## Quick Facts
- arXiv ID: 2502.00108
- Source URL: https://arxiv.org/abs/2502.00108
- Authors: Joe Suk; Jung-hun Kim
- Reference count: 40
- This work introduces parameter-free algorithms for infinite-armed bandits with rotting rewards, using significant shifts to track non-stationarity

## Executive Summary
This paper addresses infinite-armed bandits with non-stationary (rotting) rewards where mean rewards evolve over time. Unlike prior approaches that required knowledge of non-stationarity parameters and restrictive distributional assumptions, the authors propose a blackbox scheme that converts finite-armed MAB algorithms into parameter-free algorithms for infinite-armed non-stationary problems with optimal regret guarantees. They introduce the novel concept of "significant shifts" to track the most severe rotting changes in rewards, enabling tighter regret bounds. Their elimination algorithm achieves optimal rates without requiring upper bounds on reservoir masses, providing a simpler analysis compared to previous methods. Experiments validate their theoretical findings, showing superior performance compared to state-of-the-art algorithms.

## Method Summary
The authors develop two algorithms for infinite-armed bandits with rotting rewards. The blackbox approach uses a base algorithm (like UCB) with empirical regret tracking for restarts, subsampling arms at rate S_m ∝ 2^(m·β/(β+1)) per block. The elimination algorithm performs uniform sampling from a candidate set and eliminates arms when cumulative importance-weighted regret exceeds a threshold. Both algorithms employ a doubling block structure. The key innovation is tracking "significant shifts" - changes in mean rewards exceeding a threshold - rather than total variation. This allows for regret bounds that depend on the number of significant shifts L rather than total rotting variation, achieving optimal rates of (L+1)^(1/(β+1)) T^(β/(β+1)) ∧ (V_R^(1/(β+2)) T^((β+1)/(β+2)) + T^(β/(β+1))).

## Key Results
- Blackbox scheme converts finite-armed algorithms to infinite-armed non-stationary problems with optimal regret
- Significant shifts tracking provides tighter regret bounds than total variation methods
- Elimination algorithm achieves optimal rates without requiring upper bounds on reservoir masses
- Experimental results validate theoretical findings with superior performance to state-of-the-art

## Why This Works (Mechanism)
The paper's mechanism exploits the structure of rotting rewards in infinite-armed bandits. By tracking only "significant shifts" - changes above a threshold - rather than all changes, the algorithms can ignore minor variations that don't impact optimal arm selection. The subsampling strategy ensures that among the sampled arms, the best one has a gap that's large enough to be statistically distinguishable. The blackbox approach leverages existing finite-armed algorithms while the elimination method directly handles the infinite arm space through importance weighting.

## Foundational Learning
- **β-regular reservoir distributions**: Initial mean rewards follow P(μ₀(a) > 1-x) ∝ x^β for β > 0. This controls the concentration of good arms and affects the optimal regret rate.
- **Rotting variation**: The cumulative decrease in mean rewards, V_R = Σ_t |μ_t(a_t) - μ_{t-1}(a_t)|, measures non-stationarity magnitude.
- **Significant shifts**: Changes exceeding a threshold that matter for optimal arm selection, providing a more nuanced measure than total variation.
- **Subsampling in infinite-armed bandits**: Selecting a subset of arms from the infinite pool to make the problem tractable while preserving the ability to find near-optimal arms.
- **Importance-weighted gap estimation**: Technique for estimating arm gaps in infinite-armed settings by weighting observations based on sampling probabilities.
- **Doubling block structure**: Algorithm framework that restarts periodically, with block size doubling each iteration to balance exploration and adaptation.

## Architecture Onboarding
- **Component map**: β-regular reservoir → Algorithm 1 (Blackbox with UCB) / Algorithm 2 (Elimination) → Subsampling → Change detection → Regret minimization
- **Critical path**: Subsampling → Arm selection → Reward observation → Gap estimation → Changepoint detection → Restart/elimination → Regret update
- **Design tradeoffs**: Subsampling rate vs. computational efficiency vs. ability to find good arms; sensitivity to constant C₁/C₂ vs. change detection accuracy
- **Failure signatures**: Excessive restarts indicate constants too small; slow adaptation indicates constants too large; poor arm diversity suggests aggressive subsampling
- **First experiments**: 1) Verify β-regular sampling produces expected arm quality distribution; 2) Test blackbox performance across different β values; 3) Compare elimination algorithm with and without importance weighting

## Open Questions the Paper Calls Out
- Can significant shift-based regret bounds be developed for structured infinite-armed bandit settings (e.g., linear, kernel, or convex bandits)?
- Can the blackbox scheme be modified to achieve optimal regret rates for β < 1?
- Can the phenomenon that "rising non-stationarity is benign" be characterized more generally beyond the infinite-armed rotting bandit setting?

## Limitations
- The algorithms still require empirical tuning of constants C₁ and C₂, despite being called "parameter-free"
- Theoretical guarantees rely on specific distributional assumptions about the reservoir that may not hold in practice
- The analysis for β < 1 incurs an extra term that prevents achieving optimal rates with the current approach

## Confidence
- **High confidence** in the theoretical framework and regret bounds derivation
- **Medium confidence** in practical performance claims due to unspecified implementation details
- **Low confidence** in the parameter-free characterization given required knowledge of β for subsampling

## Next Checks
1. Perform sensitivity analysis of C₁ and C₂ values to identify ranges that achieve theoretical guarantees across different β values
2. Test algorithm performance with alternative noise distributions (Gaussian vs. Bernoulli) to verify robustness of theoretical bounds
3. Compare the actual number of restarts and elimination events against the theoretical prediction of O(L) for the given rotting parameters