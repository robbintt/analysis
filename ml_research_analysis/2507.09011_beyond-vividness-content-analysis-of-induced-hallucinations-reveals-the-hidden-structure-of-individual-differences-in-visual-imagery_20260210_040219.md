---
ver: rpa2
title: 'Beyond vividness: Content analysis of induced hallucinations reveals the hidden
  structure of individual differences in visual imagery'
arxiv_id: '2507.09011'
source_url: https://arxiv.org/abs/2507.09011
tags:
- imagery
- visual
- content
- vividness
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed over 4,000 free-text descriptions of hallucinations
  induced by a flickering visual stimulus (Ganzflicker) to investigate how individual
  differences in visual imagery ability relate to the content of internally generated
  visual experiences. Using computational methods including topic modeling, language
  model embeddings, and sensorimotor content analysis, the researchers found that
  individuals with stronger visual imagery reported more complex, naturalistic hallucinations
  (e.g., faces, scenes, structured imagery), while those with weaker imagery predominantly
  described simple geometric patterns and visual distortions.
---

# Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery

## Quick Facts
- arXiv ID: 2507.09011
- Source URL: https://arxiv.org/abs/2507.09011
- Reference count: 40
- Individuals with stronger visual imagery report more complex, naturalistic hallucinations compared to those with weaker imagery

## Executive Summary
This study investigated how individual differences in visual imagery ability relate to the content of internally generated visual experiences by analyzing over 4,000 free-text descriptions of hallucinations induced by a flickering visual stimulus (Ganzflicker). Using computational methods including topic modeling, language model embeddings, and sensorimotor content analysis, researchers found systematic differences between individuals with strong versus weak visual imagery. Those with stronger imagery reported more complex, naturalistic hallucinations such as faces and scenes, while those with weaker imagery predominantly described simple geometric patterns and visual distortions. Language models trained on both text and images outperformed text-only models in capturing these differences, and participants with stronger imagery used language with richer sensorimotor associations.

## Method Summary
The study analyzed free-text descriptions of hallucinations from over 4,000 participants who experienced a flickering visual stimulus (Ganzflicker). Researchers employed computational methods including topic modeling to identify thematic patterns, language model embeddings to capture semantic content, and sensorimotor content analysis to examine embodied language use. Participants were assessed for visual imagery ability, and their hallucination descriptions were systematically compared across this dimension. The analysis included both text-only and multimodal language models (trained on text and images) to evaluate which best captured individual differences in hallucination content.

## Key Results
- Individuals with stronger visual imagery reported more complex, naturalistic hallucinations (faces, scenes, structured imagery) compared to those with weaker imagery who described simple geometric patterns and distortions
- Multimodal language models (trained on both text and images) outperformed text-only models in capturing individual differences in hallucination content
- Participants with stronger visual imagery used language with richer sensorimotor associations in their descriptions

## Why This Works (Mechanism)
The study demonstrates that language patterns in self-reported hallucination experiences contain measurable signatures of underlying visual imagery ability. Computational analysis of free-text descriptions can reveal systematic differences in mental imagery strength that are not captured by traditional vividness measures alone. Multimodal language models appear better equipped to capture these differences because they can integrate visual-semantic relationships that text-only models miss.

## Foundational Learning
**Ganzflicker hallucination induction** - A flickering visual stimulus that reliably induces visual hallucinations; needed to create controlled hallucination experiences across participants; quick check: consistent induction protocol applied to all participants
**Topic modeling** - Computational method for identifying thematic patterns in text data; needed to discover common hallucination content themes; quick check: coherence scores and topic interpretability
**Language model embeddings** - Vector representations capturing semantic meaning in text; needed to quantify semantic differences between strong and weak imagery groups; quick check: embedding quality and stability across models
**Sensorimotor content analysis** - Examination of language related to bodily sensations and physical experiences; needed to identify embodied language differences; quick check: reliability of sensorimotor coding schemes

## Architecture Onboarding
**Component map**: Participants -> Ganzflicker stimulus -> Hallucination experience -> Text description -> Computational analysis -> Individual differences mapping
**Critical path**: Ganzflicker presentation → hallucination induction → text description → computational processing → imagery ability correlation
**Design tradeoffs**: Natural language descriptions provide rich data but introduce subjectivity; computational methods offer scalability but may miss nuanced meaning; multimodal models improve accuracy but require more complex training
**Failure signatures**: Inconsistent hallucination reporting across participants; language models failing to capture meaningful differences; inability to distinguish between imagery groups
**First experiments**: 1) Validate computational findings with direct neuroimaging measures during hallucination experiences; 2) Test model performance on held-out hallucination descriptions; 3) Compare computational results with expert human coding of hallucination content

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on self-reported hallucination descriptions introduces subjectivity and reporting bias
- Cross-sectional design prevents establishing causal relationships between imagery ability and hallucination content
- Sample consisted primarily of individuals with typical visual perception, limiting generalizability to clinical populations

## Confidence
- **High**: Finding that stronger visual imagery correlates with more complex, naturalistic hallucinations
- **Medium**: Superiority of multimodal language models over text-only models in capturing individual differences
- **Medium**: Language patterns reveal hidden structure of individual differences in mental imagery

## Next Checks
1. Replicate findings using controlled, standardized hallucination induction protocols across diverse participant populations
2. Validate computational model results through comparison with direct neuroimaging measures of visual cortex activity during hallucination experiences
3. Conduct longitudinal studies to track changes in hallucination content and language use as visual imagery abilities develop or change over time