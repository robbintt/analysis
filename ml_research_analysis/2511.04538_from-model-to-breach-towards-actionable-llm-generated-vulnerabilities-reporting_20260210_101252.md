---
ver: rpa2
title: 'From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting'
arxiv_id: '2511.04538'
source_url: https://arxiv.org/abs/2511.04538
tags:
- code
- instruct
- security
- score
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates the security of code generated by popular open-weight
  large language models (LLMs), finding that even the latest models produce vulnerable
  code in well-documented scenarios, with vulnerability rates ranging from 10% to
  40%. This suggests a lack of effective vulnerability reporting and patching pipelines
  in LLM-generated code.
---

# From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting

## Quick Facts
- arXiv ID: 2511.04538
- Source URL: https://arxiv.org/abs/2511.04538
- Reference count: 40
- Key outcome: Even latest open-weight LLMs produce vulnerable code in documented scenarios (10-40% vulnerability rate), highlighting absent vulnerability reporting and patching pipelines.

## Executive Summary
This study evaluates the security of code generated by popular open-weight large language models (LLMs), finding that even the latest models produce vulnerable code in well-documented scenarios, with vulnerability rates ranging from 10% to 40%. To address this, the authors introduce two new severity metrics: Prompt Exposure (PE), which measures the risk posed by a specific prompt accounting for vulnerability severity, generation likelihood, and prompt usage likelihood; and Model Exposure (ME), which aggregates PE scores to assess overall model security. The results emphasize the need for systematic security evaluation and reporting in LLM-generated code.

## Method Summary
The authors benchmark LLM-generated code security using a modified Asleep at the Keyboard (AATK) dataset, computing Prompt Exposure (PE) and Model Exposure (ME) scores that weight vulnerability severity by generation probability and prompt likelihood. They manually rewrite 17 Python scenarios from AATK, generate 10 semantically similar prompt reformulations per scenario, and sample 25 completions per reformulation using temperature 0.2 and top-p 0.95. Code validity is checked via py_compile, and CodeQL detects target CWE vulnerabilities. PE and ME scores aggregate vulnerability severity (via exponential-logarithmic transformation of CVSS scores) with generation and prompt usage probabilities.

## Key Results
- Prompt reformulation dramatically changes vulnerability generation probability (P_y ranges from 0 to 1 for semantically similar prompts)
- Open-weight coding LLMs show no measurable security improvement over three years on documented vulnerability scenarios
- Model Exposure (ME) ranking differs from raw vulnerability proportion, revealing severity clustering in specific CWEs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantically equivalent prompt reformulations can produce dramatically different vulnerability generation probabilities (P_y ranging from 0 to 1), making single-prompt evaluation insufficient for security assessment.
- **Mechanism:** The paper demonstrates that prompt formulation affects how the model retrieves patterns from training data. Certain phrasings may align more closely with vulnerable code patterns in the training corpus, triggering insecure completions.
- **Core assumption:** The relationship between prompt phrasing and vulnerability generation reflects training data artifacts rather than fundamental security reasoning capabilities.
- **Evidence anchors:**
  - [abstract] "Results show that prompt reformulation can dramatically change vulnerability generation probability (P_y ranges from 0 to 1 for semantically similar prompts)"
  - [section 5.3, Table 4] Concrete example where two nearly identical prompts for CWE-79 lead to P_y = 1 vs P_y = 0 for CodeLlama 34B-Instruct
- **Break condition:** If models were to develop robust security reasoning, prompt phrasing variance would not affect vulnerability rates; P_y would be consistently low or zero across reformulations.

### Mechanism 2
- **Claim:** Aggregating vulnerability severity via exponential-logarithmic transformation of CVSS scores provides a more actionable model security metric than raw vulnerability counts.
- **Mechanism:** The authors compute a "representative CVSS-B score" for each CWE class by exponentiating individual CVE scores, averaging in transformed space, then taking the logarithm. This is applied to both Prompt Exposure (PE) and Model Exposure (ME).
- **Core assumption:** CVSS scores approximate a logarithmic scale of real-world impact.
- **Evidence anchors:**
  - [section 5.1] "This ensures that high-severity vulnerabilities (e.g., CVSS 9-10) have a disproportionately larger influence"
  - [section 5.3, Table 2 vs Table 3] ME ranking differs from naive vulnerability proportion: CodeLlama 70B-Instruct ranks worst by raw proportion (32%) but best by ME score (3.9), because its vulnerabilities cluster in lower-severity CWE-79
- **Break condition:** If CVSS scores do not correlate with exploitability or impact in LLM-generated code contexts, the exponential weighting would distort rather than clarify risk assessment.

### Mechanism 3
- **Claim:** Open-weight coding LLMs exhibit no measurable security improvement over three years when evaluated on well-documented vulnerability scenarios, suggesting an absent or ineffective vulnerability patching pipeline.
- **Mechanism:** The authors benchmark models released years after the original AATK dataset (2021) using identical CWE scenarios. Vulnerability rates remain at 10-40%. They hypothesize a safety-functionality trade-off prevents effective patching.
- **Core assumption:** The AATK scenarios adequately represent real-world vulnerability patterns.
- **Evidence anchors:**
  - [abstract] "even the latest open-weight models are vulnerable in the earliest reported vulnerability scenarios"
  - [section 6] "coding LLMs still cannot be trusted to write secure code, even for the best documented test cases"
- **Break condition:** If future models begin consistently reporting and improving on security benchmarks, or if AATK scenarios are shown to be unrepresentative of deployment risks, the claimed pipeline absence would be mitigated.

## Foundational Learning

- **Concept:** Common Vulnerability Scoring System (CVSS)
  - **Why needed here:** The PE and ME metrics extend CVSS to LLM-generated code. Understanding base scores, temporal metrics, and the 0-10 severity scale is essential to interpret the paper's exponential-log aggregation approach.
  - **Quick check question:** Given two vulnerabilities with CVSS scores 5.0 and 9.0, why does the paper argue their average risk is closer to 9.0 than 7.0?

- **Concept:** Common Weakness Enumeration (CWE)
  - **Why needed here:** The AATK benchmark maps scenarios to CWE classes (e.g., CWE-79 for XSS, CWE-502 for deserialization). The paper's severity proxy aggregates CVEs by CWE category.
  - **Quick check question:** Why might multiple CVEs within the same CWE class have different CVSS scores, and how does the paper handle this variation?

- **Concept:** Static Analysis (CodeQL)
  - **Why needed here:** The methodology relies on CodeQL to detect vulnerabilities in generated code. Understanding taint analysis, pattern-based detection, and false positive/negative trade-offs is critical for evaluating benchmark validity.
  - **Quick check question:** The paper notes CodeQL can only test for specific patterns. How might this limitation affect PE/ME score reliability when prompts are reformulated?

## Architecture Onboarding

- **Component map:** Manual prompt rewriting -> Prompt augmentation module (N=10 reformulations) -> Code generation layer (M=25 completions) -> Static analysis filter (CodeQL + py_compile) -> Severity engine (CVSS proxy) -> PE/ME aggregation

- **Critical path:**
  1. Start with 17 manually-rewritten AATK prompts (Python subset)
  2. Generate 10 reformulations per prompt (N=10)
  3. Generate 25 completions per reformulation (M=25)
  4. Filter valid code -> run CodeQL for target CWE
  5. Compute P_y per prompt, weight by CVSS and perplexity
  6. Aggregate to ME score for model comparison

- **Design tradeoffs:**
  - Base b selection: b=2 vs b=10 changes PE/ME sensitivity to high-severity outliers
  - N and M sampling: Higher values improve probability estimates but increase compute cost
  - CWE-specific CodeQL queries: Reduces false positives but may miss cross-category vulnerabilities
  - Manual prompt rewriting: Introduces human bias; automated paraphrasing could standardize but may lose semantic precision

- **Failure signatures:**
  - High P_y variance across reformulations (Figure 2): Indicates prompt fragility, not robust security behavior
  - ME ranking inversion vs raw vulnerability rate: Signals severity clustering in specific CWEs
  - Zero valid completions for a prompt: Triggers P_y=0 assignment, potentially underestimating risk
  - CodeQL query mismatch: Reformulation may generate code outside target CWE pattern, leading to false negatives

- **First 3 experiments:**
  1. **Baseline replication:** Run the 17-prompt pipeline on a new model not in Table 2. Compare ME score and vulnerability proportion to the reported range (4.0-4.9 ME, 14-32% vulnerable). Verify P_y variance patterns.
  2. **Sensitivity analysis:** Vary base b (2, 5, 10) and observe ME ranking stability. Identify which CWEs drive ranking changes and whether conclusions are robust to aggregation choice.
  3. **Prompt augmentation stress test:** Increase N from 10 to 50 for a subset of prompts showing high P_y variance (e.g., CWE-20, CWE-79). Determine whether additional reformulations converge or reveal extreme outliers not captured in the original sample.

## Open Questions the Paper Calls Out

- **Question:** Why do semantically equivalent prompt reformulations produce dramatically different vulnerability generation probabilities (P_y ranging from 0 to 1) in some cases?
  - **Basis in paper:** [explicit] The authors state: "We hypothesize that it is either a property of the prompts themselves, or an artifact of common training data/procedure for the different models" but do not resolve this.
  - **Why unresolved:** The paper demonstrates the phenomenon empirically but does not investigate whether the cause lies in training data contamination, attention patterns, or prompt linguistic features.
  - **What evidence would resolve it:** Controlled ablation studies mapping specific prompt features to vulnerability rates, combined with training data provenance analysis to identify memorized vulnerable patterns.

- **Question:** How would vulnerability detection rates change if dynamic analysis or manual review were used instead of static CodeQL analysis?
  - **Basis in paper:** [explicit] "We rely on CodeQL to detect vulnerabilities... However, this tool only scans specific patterns, which may not be present in the code generated in response to a given prompt reformulation if the reformulation itself is not precise enough."
  - **Why unresolved:** The methodology only uses static analysis; the authors explicitly acknowledge this limitation but do not compare detection methods.
  - **What evidence would resolve it:** A comparative study applying static, dynamic (runtime testing, fuzzing), and human expert review to the same generated code samples.

- **Question:** How do human developers compare to LLMs on these same vulnerability benchmarks?
  - **Basis in paper:** [explicit] The authors note "it is not clear if humans would perform better, it definitely means that coding LLMs still cannot be trusted."
  - **Why unresolved:** No human baseline was collected; the 10-40% vulnerability rate lacks context for whether this represents improvement or regression.
  - **What evidence would resolve it:** Human developer study using identical AATK scenarios with code samples analyzed via the same CodeQL methodology.

## Limitations

- Manual prompt reformulation introduces human bias and prevents exact replication
- CVSS scores as severity proxy may not accurately reflect unique threat models of AI-generated code
- CodeQL-based detection is limited to pattern-matching and may miss context-dependent vulnerabilities

## Confidence

- **High confidence:** Prompt reformulation significantly affects vulnerability generation probability; no security improvement over three years on documented scenarios
- **Medium confidence:** PE and ME metrics provide more actionable security assessment than raw vulnerability counts
- **Low confidence:** Safety-functionality trade-off hypothesis preventing vulnerability patching remains speculative

## Next Checks

1. **Replicate the prompt reformulation sensitivity test** by generating 50 reformulations (instead of 10) for the highest-variance prompts (CWE-20, CWE-79) to determine if P_y variance converges or reveals additional outliers.

2. **Cross-validate the CVSS-based severity weighting** by comparing PE/ME rankings using alternative severity proxies (e.g., exploitability scores, monetary impact estimates, or practitioner surveys) to assess whether the exponential-logarithmic transformation meaningfully improves security assessment.

3. **Benchmark against a broader vulnerability dataset** by applying the 17-prompt pipeline to a new model not included in the original study, then comparing ME scores and vulnerability proportions to the reported range (4.0-4.9 ME, 14-32% vulnerable).