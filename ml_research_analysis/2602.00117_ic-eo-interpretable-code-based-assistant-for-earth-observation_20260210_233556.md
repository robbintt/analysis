---
ver: rpa2
title: 'IC-EO: Interpretable Code-based assistant for Earth Observation'
arxiv_id: '2602.00117'
source_url: https://arxiv.org/abs/2602.00117
tags:
- code
- tools
- earth
- data
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IC-EO, a conversational, code-generating agent
  that transforms natural-language queries into executable, auditable Python workflows
  for Earth Observation (EO) analysis. By leveraging a large language model to generate
  verifiable code using a unified EO API, IC-EO overcomes the limitations of black-box
  EO systems and fragmented toolchains.
---

# IC-EO: Interpretable Code-based assistant for Earth Observation

## Quick Facts
- **arXiv ID:** 2602.00117
- **Source URL:** https://arxiv.org/abs/2602.00117
- **Reference count:** 40
- **Primary result:** Code-generating EO agent achieves 64.2% vs 51.7% accuracy on land-cover and 50% vs 0% on post-wildfire analysis compared to VLM baselines

## Executive Summary
IC-EO is a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows for Earth Observation (EO) analysis. By leveraging a large language model to generate verifiable code using a unified EO API, IC-EO overcomes the limitations of black-box EO systems and fragmented toolchains. The approach is evaluated at three levels: tool performance on public EO benchmarks, agent-level code generation accuracy, and task-level performance on land-cover and post-wildfire use cases. IC-EO achieves 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis compared to VLM baselines, while producing interpretable, reproducible results through executable Python code.

## Method Summary
IC-EO integrates a structured API of EO tools (data retrieval, preprocessing, classification, segmentation, detection, spectral indices) with a GPT-4o controller that generates executable Python code from natural-language queries. The tool registry provides systematic descriptions with metadata for each function, enabling the LLM to plan and synthesize correct code. Generated scripts run in sandboxed containers with resource limits and logging. The system uses DOFA as a frozen backbone for classification/segmentation, trained with task-specific heads on public benchmarks. FastAPI orchestrates tool retrieval, LLM invocation, and sandbox execution, returning results and exposing code for reproducibility.

## Key Results
- IC-EO achieves 64.2% accuracy on land-composition tasks vs 51.7% for VLM baselines
- IC-EO achieves 50% accuracy on post-wildfire analysis vs 0% for VLM baselines
- Code validity rate is 87%, but 13% of executions fail (mostly due to GPU memory constraints)

## Why This Works (Mechanism)

### Mechanism 1
Program synthesis via executable code yields more reliable EO analysis than direct VLM inference. An LLM translates natural-language queries into Python code that calls curated EO tools. The code is explicitly inspectable and reproducible, avoiding black-box predictions. The paper reports 64.2% vs 51.7% accuracy on land-composition and 50% vs 0% on post-wildfire analysis vs VLM baselines. Core assumption: The LLM can reliably map queries to correct tool sequences given structured tool descriptions; code correctness implies task correctness. Evidence anchors: [abstract] "By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process." [section 3.2] "The LLM generates executable code invoking IC-EO functions, based on the descriptions... The script executes in a sandbox and returns figures, masks, or statistics." Break condition: If tool descriptions are ambiguous, or if tasks require reasoning not expressible in the current tool palette, code generation may be syntactically valid but semantically wrong.

### Mechanism 2
Structured, standardized tool descriptions reduce LLM hallucinations and improve tool selection. Each tool exposes a description with sections: general description, technical description (inputs/outputs), supported sensors with normalization, usage examples, and training datasets. This schema narrows the generation space and guides the controller. Core assumption: The LLM reads and adheres to these specifications during planning and code synthesis. Evidence anchors: [section 3.1] "This description is systematically structured around pre-defined sections: A general description of the tool; A technical description... For model tools: A list of supported sensors... An example of usage... A list of datasets used for training." [section 4.5] "These results shows that the adopted structure for the API... is well understood by the LLM... 87.0% valid code." Break condition: If tools have overlapping functionality or inconsistent naming, the LLM may select suboptimal tools; new tools without complete metadata may degrade performance.

### Mechanism 3
A sandboxed execution environment with resource controls enables safe, auditable workflows and exposes runtime failures distinct from planning errors. Generated code runs in containerized sandboxes with restricted permissions, pinned dependencies, and logging. The paper notes many of the 13% execution failures are due to GPU memory constraints, not incorrect code. Core assumption: Failures are primarily resource or data-related, not due to fundamental flaws in the controller's plan. Evidence anchors: [section 4.3] "Generated code is executed in a sandboxed runtime with restricted permissions and resource limits. Each run is fully logged..." [section 4.5 / Fig.2] Shows a "CUDA out of memory" runtime failure despite correct code; reports 87% code validity but lower final accuracy. Break condition: If tasks require resources beyond sandbox limits (large-area mosaics, high-res time series), runtime failures will obscure agent capability.

## Foundational Learning

- **LLM Tool-Using Agents (e.g., ReAct, Toolformer)**: Why needed here: IC-EO's controller is not a standard chatbot; it must plan tool calls, validate outputs, and generate runnable code. Understanding the "plan → act → observe" loop clarifies why structured prompts and tool schemas matter. Quick check: Can you explain the difference between an LLM generating free-text reasoning vs. generating executable code that calls external functions?

- **Earth Observation Preprocessing Chain**: Why needed here: EO workflows require cloud masking, reprojection, tiling, band normalization, and coordinate handling before inference. The API abstracts this, but knowing what's hidden helps debug failures. Quick check: What preprocessing steps are typically required before running a segmentation model on Sentinel-2 imagery?

- **EO Foundation Models (e.g., DOFA, Prithvi)**: Why needed here: IC-EO uses DOFA for classification/segmentation and Prithvi for burn-scar mapping. These models provide multi-sensor, multi-task backbones; understanding their input requirements and limitations informs tool selection. Quick check: Why might a single EO foundation model (like DOFA) be preferred over task-specific models in a modular tool system?

## Architecture Onboarding

- **Component map:** Frontend (chat interface) -> Backend (FastAPI) -> Tool Registry (structured API) -> Controller (GPT-4o) -> Execution Sandbox (containerized runtime) -> Results

- **Critical path:** 1. User submits query (and optionally data). 2. Backend retrieves tool registry; formats tool descriptions into prompt. 3. GPT-4o generates code-only response (no prose). 4. Code executes in sandbox; outputs (figures, masks, stats) returned to frontend. 5. Code is exposed to user for inspection and reproducibility.

- **Design tradeoffs:** GPT-4o as controller: High coding accuracy but cost and latency; open LLMs cheaper but may need fine-tuning (paper cites 33% task accuracy for open LLMs in prior work). Code-only output: Enforces structure but limits interactive clarification; errors surface only at execution. Unified DOFA backbone: Simplifies maintenance but may underperform vs. specialized models on some tasks.

- **Failure signatures:** Valid code but wrong answer: Tool selection or parameter error; check if LLM misinterpreted tool scope. CUDA OOM / resource limit: Reduce input size, tile large images, or dispatch to higher-capacity instances. Tool not found / hallucinated function: Gap between tool descriptions and LLM knowledge; verify registry completeness.

- **First 3 experiments:** 1. Smoke test: Submit a simple land-cover query (e.g., "Is there snow in this image?") and verify the generated code uses the correct classification/segmentation tool and returns interpretable output. 2. Failure mode probe: Intentionally query a large-area task to trigger resource limits; confirm the failure is logged as a runtime error, not a code-generation error. 3. Tool extension test: Add a new tool (e.g., a different spectral index) with complete metadata; verify the LLM correctly selects and uses it in a new query.

## Open Questions the Paper Calls Out

- **What specific mechanisms can bridge the performance gap between the LLM's high code validity rate (87%) and the lower final task accuracy (64.2%)?** The authors explicitly note, "This experiment shows that there is a large gap between the percentage of code that is well generated by the LLM, and the final accuracy of the IC–EO framework." The paper identifies the discrepancy but primarily attributes it to "execution-time constraints" and hardware limitations rather than fully solving the alignment between correct syntax and semantic task success. Evidence would require ablation studies isolating the root causes of the 22.8% drop and the integration of error-correction loops that allow the LLM to refine code based on runtime exceptions.

- **How can the framework be enhanced to robustly handle comparative and temporal prompts where current performance is lacking?** The conclusion states, "IC-EO performs most reliably on concrete, computation-defined tasks... while comparative or temporal prompts reveal gaps related to default change analytics and summarization." The current toolset and prompting strategy are optimized for static, single-scene analysis, failing to capture complex temporal dynamics required for change detection without manual intervention. Evidence would require an evaluation on temporal benchmarks showing improved accuracy when specific temporal reasoning tools or chain-of-thought prompting for time-series data are added.

- **Can automated resource management and dynamic tiling strategies mitigate the runtime failures (e.g., CUDA OOM) currently limiting execution success?** The authors report that "a majority of failures is due to execution-time constraints (e.g., GPU memory), rather than incorrect code," and cite "memory limits on high-resolution imagery" as a key failure mode. The current execution environment relies on standard sandboxing which throws fatal errors when processing large EO images, halting the workflow despite correct code generation. Evidence would require implementation of a dynamic memory manager or automatic tiling mechanism within the API that allows high-resolution images to be processed without exceeding GPU memory limits.

## Limitations

- Performance gains rely on a single LLM controller (GPT-4o) and fixed tool set; may degrade with different LLMs or expanded task domains
- 87% code validity rate masks that 13% of executions fail, many due to GPU memory constraints rather than planning errors
- Tool descriptions are assumed to be complete and unambiguous, but no ablation study shows how missing or vague metadata affects tool selection or code correctness

## Confidence

- **High** for the core claim that code-synthesis enables auditable EO workflows (supported by explicit reproducibility via exposed Python scripts)
- **Medium** for the superiority over VLM baselines (task accuracy gains are clear, but only two use cases tested and VLM methods are not fully described)
- **Low** for generalizability beyond the curated tool set and current LLM controller (no experiments with open-source LLMs or novel tools)

## Next Checks

1. **Prompt Robustness:** Vary the structured tool descriptions (add/remove details, alter format) and measure changes in code validity and task accuracy to test the LLM's reliance on schema completeness
2. **Resource Scaling:** Test IC-EO on progressively larger AOIs and higher-resolution imagery to quantify the breakpoint where sandbox resource limits dominate failure modes
3. **Controller Substitution:** Replace GPT-4o with an open-source code-generation model (e.g., CodeLlama) and evaluate if task-level accuracy and code validity remain comparable