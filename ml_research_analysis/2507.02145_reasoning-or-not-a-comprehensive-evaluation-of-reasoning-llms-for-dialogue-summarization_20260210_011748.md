---
ver: rpa2
title: Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue
  Summarization
arxiv_id: '2507.02145'
source_url: https://arxiv.org/abs/2507.02145
tags:
- summarization
- reasoning
- dialogue
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents the first systematic comparison of reasoning
  and non-reasoning large language models (LLMs) for dialogue summarization across
  three paradigms: generic, role-oriented, and query-oriented summarization. The evaluation
  spans multiple datasets, languages, and domains, using both automatic metrics and
  LLM-based judges.'
---

# Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization

## Quick Facts
- arXiv ID: 2507.02145
- Source URL: https://arxiv.org/abs/2507.02145
- Authors: Keyan Jin; Yapeng Wang; Leonel Santos; Tao Fang; Xu Yang; Sio Kei Im; Hugo GonÃ§alo Oliveira
- Reference count: 9
- Primary result: Reasoning LLMs consistently underperform non-reasoning models for dialogue summarization

## Executive Summary
This study presents the first systematic comparison of reasoning and non-reasoning large language models for dialogue summarization across three paradigms: generic, role-oriented, and query-oriented. The evaluation spans multiple datasets, languages, and domains, using both automatic metrics and LLM-based judges. Contrary to expectations, reasoning models (OpenAI-o1, DeepSeek-R1, QwQ-32B) consistently underperform their non-reasoning counterparts (GPT-4o, DeepSeek-V3, Qwen2.5-32B) in summary quality. They produce longer, less concise summaries with higher novelty but lower coverage and factual consistency. Detailed analysis of reasoning traces reveals limited depth and integration, with excessive paraphrasing and redundancy. LLM-based evaluation aligns with automatic metrics, confirming that explicit reasoning does not improve dialogue summarization. These findings highlight the need for targeted modeling and evaluation strategies for real-world dialogue summarization.

## Method Summary
The study evaluates six LLMs (three reasoning, three non-reasoning) across four benchmark datasets: SAMSum (819 English messenger dialogues), DialogSum (1500 English daily-life dialogues), CSDS (800 Chinese customer service dialogues), and QMSum (279 English long meeting transcripts). Models are compared using few-shot prompting without fine-tuning, with optimal in-context learning counts determined per dataset. Evaluation uses 14 automatic metrics (ROUGE variants, BERTScore, MoverScore, etc.) plus LLM-as-judge rankings across five dimensions (relevance, consistency, fluency, coherence, overall) with Bradley-Terry scoring. The study analyzes reasoning traces for quality and correlates them with summary metrics to understand performance gaps.

## Key Results
- Reasoning LLMs consistently produce lower-quality summaries than non-reasoning models across all datasets and paradigms
- Reasoning models generate significantly longer, less concise summaries with higher novelty scores but lower coverage and factual consistency
- Analysis of reasoning traces reveals limited depth and integration, with excessive paraphrasing and redundancy
- LLM-based evaluation aligns with automatic metrics, confirming that explicit reasoning does not improve dialogue summarization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit stepwise reasoning may induce verbosity in dialogue summarization, leading to lower conciseness without improved quality.
- **Mechanism:** Reasoning-oriented training encourages models to generate long intermediate traces before final output. In summarization tasks requiring abstraction, this behavior may persist, producing over-detailed summaries instead of concise abstractions.
- **Core assumption:** The model's tendency to elaborate during reasoning transfers inappropriately to the final summary generation step.
- **Evidence anchors:**
  - [abstract]: "reasoning LLMs are often prone to verbosity... and less concise summaries"
  - [Section 5.1, Table 5]: Reasoning LLMs show higher mean summary length and lower compression rates across datasets
  - [corpus]: Related work on reasoning LLMs notes overthinking and redundant reasoning (e.g., DeepSeek-R1 vs. o3-mini), but does not directly test summarization

### Mechanism 2
- **Claim:** Long reasoning chains may increase the risk of factual inconsistencies in abstractive dialogue summarization.
- **Mechanism:** Each step in a reasoning chain is an opportunity to introduce hallucination or unsupported inference. For tasks where grounding in dialogue is critical, more steps can amplify error propagation.
- **Core assumption:** Stepwise reasoning does not inherently improve factual grounding for tasks requiring direct extraction and abstraction.
- **Evidence anchors:**
  - [abstract]: "reasoning LLMs are often prone to... factual inconsistencies"
  - [Section 5.4, Table 6]: Case study shows DeepSeek-R1's reasoning includes hallucinated details that propagate to the summary
  - [corpus]: Weak direct evidence; related papers discuss reasoning model biases but not specifically hallucination in summarization

### Mechanism 3
- **Claim:** There is a disconnect between assessed reasoning process quality and final summarization effectiveness.
- **Mechanism:** Models may generate logically valid but shallow reasoning (e.g., paraphrasing dialogue steps) without achieving the abstraction needed for high-quality summaries. Evaluation of reasoning traces does not guarantee task performance.
- **Core assumption:** High scores on reasoning process dimensions (validity, coherence) do not necessarily correlate with summarization metrics.
- **Evidence anchors:**
  - [Section 5.3]: "predominantly weak and statistically insignificant correlations" between reasoning quality scores and summarization metrics
  - [Section 5.3, Figure 7]: DeepSeek-R1 shows high relevance/validity but lower depth in reasoning
  - [corpus]: No direct corpus support; this is an internal finding

## Foundational Learning

- **Concept: Dialogue Summarization Paradigms**
  - **Why needed here:** The paper evaluates three distinct tasks (generic, role-oriented, query-oriented); understanding their differences is essential for interpreting model performance.
  - **Quick check question:** In role-oriented summarization, what must a model adjust compared to generic summarization?

- **Concept: Reasoning vs. Non-Reasoning LLM Architectures**
  - **Why needed here:** The core comparison hinges on how models like OpenAI-o1 (with explicit CoT/RL reasoning) differ from GPT-4o in summarization contexts.
  - **Quick check question:** What training approach is commonly used to create reasoning LLMs like DeepSeek-R1?

- **Concept: LLM-as-Judge Evaluation Protocols**
  - **Why needed here:** The paper uses LLM evaluators for ranking summaries; understanding this method helps interpret Bradley-Terry scores and inter-annotator agreement.
  - **Quick check question:** What is one advantage and one limitation of using LLMs as evaluators for summarization?

## Architecture Onboarding

- **Component map:**
  - Inputs -> Prompting module -> Model inference -> Evaluation layer

- **Critical path:**
  1. Select dialogue dataset and task paradigm
  2. Apply standardized prompts with determined optimal few-shot count
  3. Run inference on both model families via APIs
  4. Compute automatic metrics; sample for LLM-judge evaluation
  5. Analyze correlations, case studies, and reasoning process quality

- **Design tradeoffs:**
  - Reasoning models provide interpretability (explicit traces) but at the cost of inference speed and summary conciseness
  - LLM-judge evaluation aligns better with human judgment than traditional metrics but introduces evaluator model biases
  - Few-shot prompting improves performance but increases token usage; zero-shot is necessary for very long dialogues

- **Failure signatures:**
  - Verbosity: Summary length significantly exceeds reference; low compression rate
  - Factual inconsistency: Hallucinated entities or events not supported by dialogue
  - Reasoning-quality disconnect: High reasoning trace scores but low summarization metric performance

- **First 3 experiments:**
  1. **Baseline comparison:** Run all six models on SAMSum (generic) with 10-shot prompting; report ROUGE, BERTScore, and time
  2. **Role-oriented test:** Evaluate DeepSeek-R1 vs. DeepSeek-V3 on CSDS user/agent/final summaries; analyze coverage and novelty
  3. **Reasoning process analysis:** Sample 50 dialogues from DialogSum; have LLM judges score DeepSeek-R1's reasoning traces for depth and utility; correlate with summary quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the disconnect between high reasoning process quality and final summarization performance be bridged?
- Basis in paper: [explicit] Section 5.3 notes "predominantly weak and statistically insignificant correlations" between reasoning process scores (Coherence, Depth) and summarization metrics, indicating a disconnect that contradicts the hypothesis that better reasoning improves output.
- Why unresolved: The study evaluated existing models but did not propose mechanisms to align the reasoning trace with the summary generation objectives.
- What evidence would resolve it: A method that modifies reasoning generation to maximize summary utility, resulting in a statistically significant positive correlation between reasoning scores and ROUGE/BERTScore.

### Open Question 2
- Question: Would supervised fine-tuning (SFT) mitigate the verbosity and factual inconsistency issues observed in reasoning models?
- Basis in paper: [inferred] The methodology (Section 3.2.1) explicitly states the evaluation "exclusively utilizes prompting, without any task-specific fine-tuning," leaving the impact of training strategies unexplored.
- Why unresolved: The paper isolates the "reasoning capability" via prompting, but it is unknown if training on dialogue data could teach reasoning models when to stop reasoning and start summarizing.
- What evidence would resolve it: An experiment where reasoning models (e.g., DeepSeek-R1) are fine-tuned on SAMSum/CSDS, showing improved conciseness and factual consistency compared to their zero-shot counterparts.

### Open Question 3
- Question: What evaluation frameworks are necessary to robustly capture factuality and conciseness given the failure of current metrics?
- Basis in paper: [explicit] Section 5.2 and the Conclusion state that "it remains difficult to robustly assess factuality, conciseness, and pragmatic adequacy" due to low correlations between automatic metrics and LLM-based judgments.
- Why unresolved: Current automatic metrics fail to penalize the specific failure modes of reasoning models (like "hallucinated or factually inconsistent content" noted in Section 5.4).
- What evidence would resolve it: The development of a new benchmark or metric that correlates strongly with human judgment specifically on the dimensions of factual consistency and verbosity for reasoning-heavy outputs.

## Limitations
- Evaluation focuses on English and Chinese datasets, limiting conclusions about multilingual performance
- All experiments use API-based inference without fine-tuning, which may not reflect potential improvements from model adaptation
- The zero-shot setting for QMSum long meetings represents a significant constraint that may disadvantage all models

## Confidence

**High confidence:** The consistent underperformance of reasoning models across multiple datasets, metrics, and languages
**Medium confidence:** The mechanism explanations linking verbosity to reasoning traces, as the study observes correlations but does not establish causal mechanisms
**Medium confidence:** The LLM-as-judge alignment with automatic metrics, given potential evaluator model biases

## Next Checks

1. Test whether fine-tuning reasoning models on summarization-specific data improves their performance relative to non-reasoning models
2. Conduct ablation studies on reasoning trace generation to determine if verbosity directly causes summarization quality degradation
3. Evaluate model performance on additional languages and domains to assess generalization beyond the current dataset scope