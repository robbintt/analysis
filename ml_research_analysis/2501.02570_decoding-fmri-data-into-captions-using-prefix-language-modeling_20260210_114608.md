---
ver: rpa2
title: Decoding fMRI Data into Captions using Prefix Language Modeling
arxiv_id: '2501.02570'
source_url: https://arxiv.org/abs/2501.02570
tags:
- fmri
- brain
- image
- language
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for decoding fMRI data into image
  captions by predicting DINOv2 embeddings from brain activity and using GPT-2 for
  text generation. The approach addresses computational efficiency and potential data
  contamination issues in brain captioning by using a single DINOv2 [CLS] vector instead
  of high-dimensional GIT embeddings.
---

# Decoding fMRI Data into Captions using Prefix Language Modeling

## Quick Facts
- **arXiv ID**: 2501.02570
- **Source URL**: https://arxiv.org/abs/2501.02570
- **Reference count**: 4
- **Primary result**: 3D CNN + prefix-tuning achieves METEOR 0.457 on NSD, using 1/171th the parameters of GIT-based approaches

## Executive Summary
This paper presents a method for decoding fMRI brain activity into image captions by predicting DINOv2 embeddings from brain signals and using GPT-2 for text generation. The approach addresses computational efficiency and potential data contamination issues in brain captioning by using a single DINOv2 [CLS] vector instead of high-dimensional GIT embeddings. Experiments on the NSD dataset show that the proposed method achieves superior METEOR scores (0.457) and competitive ROUGE scores compared to existing approaches while requiring only 1/171th of the parameter space.

## Method Summary
The method uses a two-stage approach: first, a 3D convolutional neural network maps fMRI signals to DINOv2 [CLS] embeddings, then a lightweight transformer projects these embeddings into prefix tokens for GPT-2 language model. The brain module captures spatial voxel relationships that linear methods miss, while the captioning module leverages pre-trained language knowledge. Training is done separately for each module, with the brain module predicting visual embeddings via MSE loss and the captioning module generating captions via cross-entropy loss conditioned on the predicted prefix.

## Key Results
- Achieves METEOR score of 0.457, outperforming prior approaches
- Uses only 1536-dimensional DINOv2 embeddings versus 262,144-dimensional GIT embeddings
- 3D CNN captures positional information and out-of-ROI voxel contributions missed by Ridge Regression
- Eliminates COCO data contamination by using DINOv2 instead of GIT embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single DINOv2 [CLS] vector (1536 dims) can serve as an effective prefix conditioning signal for GPT-2 to generate image captions from brain activity.
- Mechanism: The captioning module uses a lightweight transformer to project the DINOv2 embedding into prefix tokens matching GPT-2's word embedding dimensions. These prefix tokens are prepended to the language model input, conditioning autoregressive text generation on the predicted visual embedding. GPT-2's pre-trained language knowledge then completes the caption.
- Core assumption: DINOv2's self-supervised visual features contain sufficient semantic information to guide language generation without task-specific vision-language alignment training.
- Evidence anchors: [abstract] "providing its [CLS] token as the prefix to the GPT-2 language model which decreases computational requirements considerably"
- Break condition: If DINOv2 embeddings lack semantic richness comparable to CLIP or GIT for captioning, the prefix will undercondition GPT-2, producing generic or uninformative captions.

### Mechanism 2
- Claim: 3D CNNs improve fMRI-to-embedding mapping over Ridge Regression by preserving positional relationships between voxels and incorporating signals outside predefined ROI masks.
- Mechanism: Unlike linear regression on flattened ROI-masked voxels, 3D convolutions operate on the full volumetric fMRI array, applying learned kernels that aggregate information across spatial neighborhoods. This allows the network to discover informative voxel patterns regardless of ROI boundaries and capture spatial structure that linear methods ignore.
- Core assumption: Spatial relationships between voxels and contributions from voxels outside standard ROIs contain task-relevant information for predicting visual embeddings.
- Evidence anchors: [abstract] "3D Convolutional Neural Network mapping of fMRI signals to image embedding space for better accounting positional information of voxels"
- Break condition: If informative signals are predominantly localized within the ROI mask and spatially invariant, 3D CNNs add computational overhead without performance gains.

### Mechanism 3
- Claim: Using DINOv2 instead of GIT for visual embedding eliminates potential data contamination in brain captioning evaluation on NSD.
- Mechanism: NSD test images come from COCO. Since GIT was trained on COCO, its embeddings may already encode information about test stimuli, inflating performance metrics. DINOv2's training did not include COCO, so its embeddings represent zero-shot visual features uncontaminated by test-set exposure.
- Core assumption: Data contamination materially affects evaluation validity and DINOv2's different training distribution does not introduce other evaluation artifacts.
- Evidence anchors: [abstract] "the current captioning approach introduces the challenge of potential data contamination given that the GIT model was trained on the COCO dataset"
- Break condition: If DINOv2's training data has significant COCO overlap through data laundering or if the contamination effect is negligible in practice, this advantage is overstated.

## Foundational Learning

- Concept: **Prefix-tuning for conditioned language generation**
  - Why needed here: The core technique bridges visual embeddings to text generation. You must understand how prepending learned prefix tokens shifts GPT-2's output distribution without modifying its weights.
  - Quick check question: Can you explain why prefix tokens are projected to match word embedding dimensions rather than using the raw visual vector directly?

- Concept: **3D convolutions on volumetric neuroimaging data**
  - Why needed here: The brain module replaces standard linear regression with 3D CNNs. Understanding kernel operations in 3D space (depth × height × width) is essential for debugging spatial feature extraction.
  - Quick check question: How does a 3×3×3 kernel aggregate information differently from applying 1D regression on flattened voxels?

- Concept: **fMRI preprocessing (GLM betas, z-normalization, ROI masks)**
  - Why needed here: The paper uses specific NSD preprocessing (betas_fithrf_GLMdenoise_RR, NSDGeneral ROI). Without this context, you cannot replicate the input format or understand what the network receives.
  - Quick check question: What does a "beta" value represent in GLM-based fMRI analysis, and why is z-normalization applied before linear but not CNN mapping?

## Architecture Onboarding

- Component map:
  - **Brain module**: 3D ResNet-18 (Shallow or Wide variants) → DINOv2 [CLS] embedding (1536-dim vector)
  - **Captioning module**: Lightweight transformer (projects embedding to prefix tokens) → GPT-2 base (autoregressive text generation)
  - **Training flow**: Modules trained separately. Brain module uses MSE loss against ground-truth DINOv2 embeddings. Captioning module uses cross-entropy loss for next-token prediction.
  - **Inference flow**: fMRI volume → 3D CNN → predicted embedding → transformer prefix projection → GPT-2 beam search → caption text

- Critical path:
  1. Preprocess fMRI to 3D volume (scale to [-1, 1])
  2. Forward pass through Wide CNN to get 1536-dim embedding
  3. Project embedding through transformer to prefix tokens
  4. Feed prefix to GPT-2 with beam search decoding
  5. Return generated caption string

- Design tradeoffs:
  - **Wide CNN vs Shallow CNN**: Wide has more feature planes, better performance but more parameters. Paper shows Wide outperforms Shallow on all metrics.
  - **Ridge Regression vs 3D CNN**: Ridge is simpler and faster but ignores positional information. CNN captures spatial structure at computational cost.
  - **DINOv2 vs GIT embedding**: DINOv2 avoids contamination but provides 1/171th the embedding dimensionality (1536 vs 262,144). Smaller capacity may limit fine-grained visual detail.
  - **Separate training vs end-to-end**: Decoupled modules are easier to debug and iterate, but may miss joint optimization opportunities.

- Failure signatures:
  - **Generic captions**: If brain module fails to predict meaningful embeddings, GPT-2 produces bland outputs ("an image", "a scene"). Check embedding MSE on validation set.
  - **Nonsensical text**: If prefix projection is misaligned with GPT-2's embedding space, output degrades. Verify transformer output dimensions match GPT-2 word embedding size.
  - **Subject-specific collapse**: If CNN overfits to one subject's brain geometry, cross-subject transfer fails. Monitor per-subject metrics.
  - **ROI mismatch**: If using different preprocessing or ROI mask, input shape and normalization will differ from expected. Validate against paper's NSDGeneral format.

- First 3 experiments:
  1. **Reproduce Ridge Regression baseline**: Start with the simpler mapping approach using z-normalized linearized voxels. Verify you can match reported METEOR/ROUGE scores before attempting CNN implementation.
  2. **Ablate CNN depth/width**: Train Shallow CNN and Wide CNN variants. Confirm Wide outperforms Shallow by the margin reported in Table 2 before investing in further architecture search.
  3. **Test captioning module in isolation**: Feed ground-truth DINOv2 embeddings (from actual images, not fMRI predictions) to the captioning module. This upper-bounds performance and isolates brain-module errors from captioning-module errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can providing fMRI data directly as the prefix for language models enable complex tasks beyond captioning, such as visual-question answering?
- Basis in paper: [explicit] "providing fMRI data as the prefix for the language models (Ye et al., 2023) can be promising in creating brain decoding frameworks designed for complex tasks like visual-question answering."
- Why unresolved: The current work only demonstrates caption generation; the prefix approach has not been tested on tasks requiring reasoning about visual content from brain activity.
- What evidence would resolve it: Experiments applying the prefix LM approach to VQA datasets with fMRI data, measuring task accuracy on question-answering performance.

### Open Question 2
- Question: What specific spatial patterns or inter-regional interactions does the 3D CNN capture that Ridge Regression misses, and which brain regions contribute most to improved decoding?
- Basis in paper: [inferred] The ablation shows CNN outperforms Ridge Regression, and the paper claims this "suggests the importance of capturing both out-of-ROI voxel contributions and positional information," but provides no analysis of what the CNN actually learns.
- Why unresolved: The paper demonstrates performance gains but offers no interpretability analysis or saliency mapping of the 3D CNN.
- What evidence would resolve it: Activation mapping, attention visualization, or ablation over spatial regions to identify which voxel patterns and positions drive performance improvements.

### Open Question 3
- Question: Does using a larger modern language model (e.g., LLaMA, GPT-4) instead of GPT-2 improve caption quality while maintaining the efficiency benefits of the prefix approach?
- Basis in paper: [inferred] The paper uses GPT-2 base model without justification or comparison to other LMs; the prefix tuning approach should be model-agnostic.
- Why unresolved: No experiments compare different language model backbones to determine if model scale affects caption quality in this framework.
- What evidence would resolve it: Controlled experiments swapping GPT-2 for larger models while keeping the brain module fixed, measuring METEOR/ROUGE scores and computational costs.

### Open Question 4
- Question: Does the claimed elimination of COCO data contamination actually improve generalization to novel images not seen during LM pretraining?
- Basis in paper: [inferred] The paper motivates DINOv2 by claiming GIT's COCO training causes "potential data contamination," but provides no experiment testing whether this contamination affects generalization in practice.
- Why unresolved: All test images in NSD still come from COCO, so the contamination concern cannot be evaluated within the current experimental design.
- What evidence would resolve it: Testing on fMRI data from images outside the COCO dataset distribution (e.g., ImageNet, domain-specific images) to compare DINOv2-GPT2 vs GIT-based approaches.

## Limitations

- The 1/171 parameter reduction from GIT (1536 vs 262,144 dims) may compromise fine-grained visual detail necessary for accurate captioning
- Critical implementation details remain unspecified, including exact CNN architectures, prefix transformer architecture, and beam search parameters
- NSD dataset contains only 4 subjects with limited stimulus diversity, potentially limiting generalizability

## Confidence

- **High confidence**: The two-stage prefix-tuning approach with separate brain and captioning modules is technically sound and reproducible given access to the GitHub repository
- **Medium confidence**: The claim that 3D CNNs significantly outperform Ridge Regression due to positional information capture is supported by ablation results, but the magnitude of improvement may depend on implementation details not fully specified
- **Low confidence**: Performance superiority over HAVIR and NeuroSwift methods is claimed but not directly validated through head-to-head comparison on identical preprocessing and evaluation protocols

## Next Checks

1. **Ablation of contamination effects**: Train an identical pipeline using GIT embeddings on a non-COCO dataset or on NSD's test split to empirically measure contamination impact on METEOR scores
2. **Head-to-head NSD evaluation**: Implement HAVIR and NeuroSwift methods using identical NSD preprocessing (GLM betas, NSDGeneral ROI) and evaluate all three approaches on the same test subjects
3. **Cross-subject generalization study**: Evaluate the trained model on subject 1's test set and report per-subject performance across all four subjects to quantify robustness across different brain anatomies