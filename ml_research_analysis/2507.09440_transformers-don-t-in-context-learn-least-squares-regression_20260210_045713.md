---
ver: rpa2
title: Transformers Don't In-Context Learn Least Squares Regression
arxiv_id: '2507.09440'
source_url: https://arxiv.org/abs/2507.09440
tags:
- prompts
- regression
- subspace
- training
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformers perform in-context learning
  (ICL) for linear regression tasks, challenging the prevailing hypothesis that transformers
  implement algorithms like Ordinary Least Squares (OLS) regression during inference.
  Through a series of out-of-distribution generalization experiments, the authors
  demonstrate that transformers fail to generalize when input or weight vectors fall
  outside the subspace seen during pre-training, a behavior inconsistent with classical
  regression rules that should generalize regardless of input subspace.
---

# Transformers Don't In-Context Learn Least Squares Regression

## Quick Facts
- arXiv ID: 2507.09440
- Source URL: https://arxiv.org/abs/2507.09440
- Authors: Joshua Hill; Benjamin Eyre; Elliot Creager
- Reference count: 17
- Key outcome: Transformers fail to generalize least squares regression to out-of-distribution subspaces, exhibiting a stable low-dimensional spectral signature in-distribution that breaks OOD.

## Executive Summary
This paper investigates how transformers perform in-context learning (ICL) for linear regression tasks, challenging the prevailing hypothesis that transformers implement algorithms like Ordinary Least Squares (OLS) regression during inference. Through a series of out-of-distribution generalization experiments, the authors demonstrate that transformers fail to generalize when input or weight vectors fall outside the subspace seen during pre-training, a behavior inconsistent with classical regression rules that should generalize regardless of input subspace. Notably, even in-distribution, transformers achieve error rates orders of magnitude worse than OLS. The authors identify a "spectral signature" in the transformer's residual stream representations: in-distribution prompts exhibit a stable low-dimensional structure with rapidly decaying singular values and consistent principal directions, while out-of-distribution prompts show flatter spectra and unstable directions. This signature correlates strongly with performance and can reliably detect distribution shifts.

## Method Summary
The authors train GPT-2 style transformers on synthetic linear regression prompts with controlled distribution shifts. They generate prompts with d=20 dimensions and k=40 in-context examples per prompt, where weight vectors w = P_w·w_g and inputs x = P_x·x_g are projected onto specific subspaces. The training curriculum starts with restricted dimensionality and gradually increases complexity. They evaluate transformers on three distributions: training subspace (D∥), orthogonal subspace (D⊥), and full space (D□). Performance is measured against OLS and ridge regression baselines, and residual stream representations are analyzed via SVD to identify spectral signatures.

## Key Results
- Transformers trained on subspace-restricted data achieve orders of magnitude higher error than OLS, even in-distribution.
- Performance collapses when inputs or weight vectors fall outside the training subspace, unlike OLS which generalizes.
- In-distribution prompts show stable low-dimensional spectral signatures with consistent top singular vectors, while OOD prompts exhibit flattened spectra and unstable directions.
- The spectral alignment metric (||C_p,0:2||_2) correlates strongly with loss (r = -0.557) and can detect distribution shifts with 94.45% accuracy in-distribution vs 19.39% OOD.

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Geometry Coupling in Residual Representations
- Claim: Transformer ICL performance depends on the geometric alignment between prompt representations and a low-dimensional subspace established during pre-training.
- Mechanism: During pre-training on subspace-restricted data, the transformer learns to map in-distribution prompts to residual-stream representations with a stable low-rank structure. The first two singular vectors become canonical directions shared across prompts. Predictions are then generated primarily from projections onto these fixed directions.
- Core assumption: The readout head has learned to extract predictions predominantly from the span of the top-2 singular vectors rather than from the full representation space.
- Evidence anchors:
  - [abstract] "Inputs from the same distribution as the training data produce representations with a unique spectral signature: inputs from this distribution tend to have the same top two singular vectors."
  - [section 5.2] "For D∥'s representations from T∥, the restricted transformer, the first two singular vectors are almost identical across prompts."
  - [corpus] Related work on low-rank task structure (arXiv:2510.04548) suggests transformers exploit shared low-dimensional structure, but does not directly confirm the spectral signature mechanism.

### Mechanism 2: Implicit Task-Vector Estimation Within Training Subspace
- Claim: Transformers trained on subspace-restricted regression tasks implicitly estimate weight vectors that have significant norm only within the training subspace, and near-zero norm in orthogonal directions.
- Mechanism: Given in-context examples, the transformer constructs an implicit weight estimate β̂ via its forward pass. This β̂ is biased toward the training subspace because the model has never observed task components in the orthogonal subspace during pre-training. The implicit β̂ thus behaves like a projection of the true weight vector onto the training subspace, plus noise.
- Core assumption: The transformer's prediction function can be locally approximated as a linear readout of an implicitly constructed β̂ that is subspace-aligned.
- Evidence anchors:
  - [section 4.1] "The transformer performs well when w lies in the training subspace, similar to OLS. However, its performance degrades heavily when w falls in the orthogonal subspace."
  - [appendix E] Table 4 shows the implicitly learned weight vector β̂ has much greater average norm (0.35 vs 0.04) when projected into the training subspace than the orthogonal subspace.
  - [corpus] Corpus does not provide direct evidence for or against this implicit weight-vector mechanism.

### Mechanism 3: Spectral Signature as OOD Detection Signal
- Claim: The spectral signature—the consistency of top singular vectors and the rapid decay of singular values—can serve as an internal diagnostic for whether a prompt is in-distribution, and predicts loss.
- Mechanism: For each prompt, compute the cosine similarity vector C_p between the prompt's residual representation singular vectors and a canonical set derived from training data. The norm ||C_p,0:2||_2 is correlated with low loss; prompts with low alignment exhibit higher MSE.
- Core assumption: The relationship between spectral alignment and loss is monotonic and sufficiently strong to serve as a practical OOD detector.
- Evidence anchors:
  - [section 5.3] Figure 6 shows correlation r = -0.557 (p = 1.92E-41) between ||C_p,0:2||_2 and average MSE across prompts from D∥ and D⊥.
  - [section 5.2] Table 1 shows 94.45% of training-subspace prompts fall within the 95% confidence region of the Gaussian fit to C_p,0:2, vs 19.39% for orthogonal-subspace prompts.
  - [corpus] Preliminary evidence in Qwen3-4B (appendix I) suggests similar spectral differences between in-distribution and OOD prompts on a Caesar cipher task, but corpus provides no independent validation.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) and Low-Rank Structure**
  - Why needed here: The paper's core analysis relies on decomposing residual-stream representations into singular values and vectors to identify spectral signatures. Understanding how SVD reveals dominant directions and variance decay is essential for interpreting Figures 4, 5, and 6.
  - Quick check question: Given a 50×20 representation matrix Z_p, what would a "rapidly decaying" singular value spectrum look like, and what does it imply about the effective dimensionality of the representation?

- Concept: **Ordinary Least Squares (OLS) and Its Invariance Properties**
  - Why needed here: The paper argues transformers do not implement OLS because OLS generalizes across subspaces while transformers do not. Understanding the closed-form OLS solution and its independence from the input subspace (provided the system is not underdetermined) is needed to follow the generalization experiments.
  - Quick check question: If you train a model on inputs constrained to a 10-dimensional subspace of ℝ^20, should an OLS regressor generalize to inputs in the orthogonal 10-dimensional subspace? Why or why not?

- Concept: **Distribution Shift and Out-of-Distribution (OOD) Generalization**
  - Why needed here: The experimental design hinges on controlled distribution shifts (input subspace restriction, weight subspace restriction, scale shifts) to probe ICL mechanisms. The distinction between in-distribution and OOD prompts is central to interpreting the results and the spectral signature as an OOD detector.
  - Quick check question: If a transformer is pre-trained on prompts with inputs x ∼ N(0, I_d) and tested on prompts with inputs x ∼ N(0, 4I_d), is this a distribution shift? Would OLS performance change under this shift?

## Architecture Onboarding

- Component map:
  Input tokenization -> Transformer backbone (12 layers, 8 heads, 256 hidden) -> Residual stream extraction -> Readout head -> Prediction

- Critical path:
  1. Pre-training data generation → 2. Prompt construction and tokenization → 3. Transformer forward pass → 4. Residual-stream representation extraction → 5. Readout prediction → 6. Loss computation and backprop

- Design tradeoffs:
  - **Subspace restriction vs. full-space training**: Restricting inputs or weights to a subspace enables controlled OOD experiments but reduces training diversity. Full-space training (T□) yields better generalization but makes it harder to isolate distribution-specific mechanisms.
  - **Spectral signature threshold selection**: Using a 95% Gaussian confidence region on C_p,0:2 provides a binary OOD detector but may be sensitive to batch size and the choice of canonical singular vectors. Alternative thresholds or multivariate models could improve detection rates.
  - **Assumption**: The paper assumes 500k training steps is sufficient for convergence; extending to 1M or 1.5M steps produced no appreciable change, suggesting the findings are not training-length artifacts.

- Failure signatures:
  - **OOD performance collapse**: Error on orthogonal-subspace prompts (D⊥) is orders of magnitude higher than on training-subspace prompts (D∥), and does not improve with more in-context examples.
  - **Flattened singular spectra**: OOD prompts produce residual representations with slowly decaying singular values and unstable principal directions (Figures 4, 12, 15).
  - **Low spectral alignment**: Low ||C_p,0:2||_2 correlates with high loss (Figure 6); prompts with spectral alignment below a threshold are likely OOD.
  - **Scale sensitivity**: Transformers trained on a single input scale (s = 1.0) show rapidly growing error as test-time scale deviates (Figure 9).

- First 3 experiments:
  1. **Replicate the input subspace restriction experiment**: Train T∥ on D(P = P_A) and evaluate on D∥, D⊥, D□. Compute MSE vs. number of in-context examples for T∥, OLS, and ridge. Verify the generalization gap and compare spectral signatures of residual representations across the three test distributions.
  2. **Extract and analyze implicit weight vectors**: Following Appendix E, use a trained T∥ to generate predictions on a set of query points X_q given a fixed context prompt, then solve for β̂ = X_q†ŷ. Project β̂ into the training and orthogonal subspaces and compare norms and variances (Table 4).
  3. **Implement the spectral signature OOD detector**: Fit a bivariate Gaussian to C_p,0:2 values from a held-out subset of D∥ prompts. For new prompts from D∥ and D⊥, compute the percentage falling within the 95% confidence region (Table 1). Evaluate the correlation between ||C_p,0:2||_2 and loss on a mixed batch of prompts (Figure 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do spectral signatures predict ICL generalization failures in real-world tasks like language translation and code generation?
- Basis in paper: [explicit] The Limitations section states: "the extent to which these findings generalize to richer, real-world ICL tasks (e.g. language-translation, code generation) remains an open question."
- Why unresolved: Experiments only cover synthetic linear regression; real-world tasks involve more complex distributions and objectives.
- What evidence would resolve it: Spectral signature analysis on pre-trained LLMs performing diverse real-world ICL tasks, showing similar correlation between spectral patterns and OOD degradation.

### Open Question 2
- Question: How does the spectral signature emerge during pretraining, and what training dynamics drive its formation?
- Basis in paper: [explicit] The Limitations section states the authors have "not yet fully characterized how it emerges during pretraining."
- Why unresolved: The paper only analyzes final trained models, not training trajectories.
- What evidence would resolve it: Longitudinal analysis tracking singular value spectra and principal directions across training steps, correlating changes with gradient statistics for in-distribution vs. OOD subspaces.

### Open Question 3
- Question: Which architectural components produce the spectral signature, and how do layer normalization, attention heads, and MLP layers each contribute?
- Basis in paper: [explicit] The Limitations section states the authors have "not yet fully characterized... how it interacts with specific architectural components (e.g. layer normalization, attention heads, MLP layers)."
- Why unresolved: No ablation studies were conducted on individual components.
- What evidence would resolve it: Ablation experiments measuring changes to spectral signature structure and OOD detection accuracy when removing or modifying specific components.

## Limitations

- **Generalization to other architectures and tasks**: All experiments are conducted on GPT-2 style transformers with a fixed 20-dimensional regression task. The spectral signature mechanism may not generalize to larger models, different attention mechanisms, or non-linear tasks.
- **Reliance on synthetic data distributions**: The paper uses carefully controlled synthetic distributions with orthogonal subspaces. Real-world data may not exhibit such clean geometric structure, potentially weakening the spectral signature as an OOD detector in practice.
- **Implicit weight vector extraction methodology**: The approach of solving for β̂ using X_q†ŷ assumes the transformer's prediction is approximately linear in the implicit weight estimate. This approximation may break down for more complex tasks or when the number of query points is insufficient relative to dimensionality.

## Confidence

- **High confidence**: The experimental demonstration that transformers fail to generalize to out-of-distribution subspaces (Sections 4.1, 4.2) is well-supported by clear MSE comparisons and controlled synthetic data generation. The spectral signature differences between in-distribution and OOD prompts (Section 5.2) are also robustly demonstrated.
- **Medium confidence**: The mechanism interpretation that transformers rely on training distribution geometry rather than implementing optimal regression algorithms (Sections 4.3, 5.3) is compelling but requires further validation on diverse architectures and tasks. The implicit weight vector analysis (Appendix E) provides supporting evidence but is based on a simplifying approximation.
- **Low confidence**: The practical utility of the spectral signature as an OOD detector in real-world applications is not established. The paper demonstrates correlation with loss on synthetic data but does not validate on noisy, high-dimensional, or non-linear real-world datasets.

## Next Checks

1. **Cross-architecture validation**: Replicate the core experiments (input subspace restriction, spectral signature analysis) on a different transformer architecture (e.g., GPT-Neo, Llama) and on a non-linear regression task (e.g., quadratic regression). Verify whether the spectral signature mechanism and OOD generalization failure persist across architectures and task types.

2. **Real-world data evaluation**: Apply the spectral signature OOD detection method to a real-world regression dataset (e.g., housing prices, energy consumption) where controlled distribution shifts can be simulated through feature transformations or temporal splits. Measure false positive/negative rates and compare against established OOD detection baselines.

3. **Fine-tuning sensitivity analysis**: Investigate whether fine-tuning the transformer on a small amount of OOD data (D⊥ or D□) can recover the generalization performance, and whether this fine-tuning modifies the spectral signature. This would help determine if the OOD failure is due to lack of exposure or an inherent architectural limitation.