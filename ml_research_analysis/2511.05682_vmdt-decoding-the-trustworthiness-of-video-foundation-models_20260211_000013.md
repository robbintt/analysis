---
ver: rpa2
title: 'VMDT: Decoding the Trustworthiness of Video Foundation Models'
arxiv_id: '2511.05682'
source_url: https://arxiv.org/abs/2511.05682
tags:
- video
- each
- prompt
- evaluation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'VMDT introduces the first comprehensive trustworthiness evaluation
  framework for video foundation models, addressing a critical gap as these models
  become more sophisticated but lack standardized trustworthiness benchmarks. The
  framework evaluates text-to-video and video-to-text models across five key dimensions:
  safety, hallucination, fairness, privacy, and adversarial robustness.'
---

# VMDT: Decoding the Trustworthiness of Video Foundation Models

## Quick Facts
- **arXiv ID:** 2511.05682
- **Source URL:** https://arxiv.org/abs/2511.05682
- **Reference count:** 40
- **Primary result:** Introduces first comprehensive trustworthiness evaluation framework for video foundation models across 5 dimensions

## Executive Summary
VMDT addresses the critical gap in trustworthiness evaluation for video foundation models (VFMs), which are becoming increasingly sophisticated but lack standardized benchmarks for safety, hallucination, fairness, privacy, and adversarial robustness. The framework evaluates both text-to-video (T2V) and video-to-text (V2T) models across 26 models total, revealing significant trustworthiness gaps. Open-source T2V models universally fail to recognize harmful queries, while V2T models show concerning trends where fairness and privacy risks increase with model size despite improvements in hallucination detection. The highest-performing model achieves only around 70-73 points, indicating substantial room for improvement in trustworthy video generation.

## Method Summary
The VMDT framework evaluates video foundation models using curated benchmark datasets across five trustworthiness dimensions. For safety, it uses 780 T2V prompts and 990 V2T prompts with GPT-4o as the primary evaluator (86-88% human agreement). Hallucination evaluation employs 1,650 T2V prompts and 1,218 V2T prompts using GPT-4o and Qwen2.5-VL-72B-Instruct. Fairness assessment uses 1,086 T2V prompts and 5,008 V2T prompts with FairFace classifier. Privacy evaluation measures location inference using 1,000 T2V prompts from WebVid-10M and 200 V2T videos from BDD100K. Adversarial robustness testing uses FMM-Attack and gradient-based attacks on 329 T2V prompts and 1,523 V2V videos. Code is available at https://sunblaze-ucb.github.io/VMDT-page/

## Key Results
- All open-source T2V models fail to recognize harmful queries and generate harmful content
- V2T model fairness demonstrates significant negative correlation with model size
- Privacy risks increase as V2T model size increases
- Safety shows no correlation with model size
- Highest-performing model achieves only around 70-73 points on the trustworthiness benchmark

## Why This Works (Mechanism)

### Mechanism 1
In Video-to-Text (V2T) models, increasing parameter scale appears to decouple helpful capabilities from privacy/fairness safety; larger models hallucinate less but are significantly better at inferring private data (location) and exhibit stronger social stereotypes. As model capacity increases, the ability to memorize fine-grained training data (privacy risk) and infer latent patterns from visual scenes (location inference) improves. Simultaneously, the model may internalize societal biases present in the pre-training corpus more deeply, leading to increased unfairness, while enhanced reasoning capabilities reduce hallucination.

### Mechanism 2
Open-source Text-to-Video (T2V) models generate harmful content primarily due to a lack of "refusal" mechanisms, whereas closed-source models fail on "video-specific" temporal and physical risks despite having guardrails. Open-source models typically lack the safety fine-tuning or RLHF layers present in LLMs, resulting in a Bypass Rate (BR) of 1.0 (no refusal). Closed-source models employ blacklists/moderation that detect static harmful concepts but fail to reason about harms that emerge dynamically across frames (temporal risks) or physical interactions.

### Mechanism 3
Adversarial attacks optimized on one VFM (the surrogate) transfer effectively to other VFMs, particularly those within the same model family or architecture, exploiting shared feature extractors. By optimizing perturbations to maximize feature discrepancy or output divergence in a surrogate model, the resulting adversarial noise exploits structural commonalities (e.g., visual encoders or diffusion U-nets) shared across models, degrading their ability to perform object/spatial recognition.

## Foundational Learning

- **Concept:** Video-Specific Risk Taxonomy (Temporal & Physical Harm)
  - **Why needed here:** Standard image safety benchmarks miss risks unique to video. A static frame of a strobe light is harmless; the video can cause seizures. A single frame of an action is ambiguous; the sequence reveals the harm.
  - **Quick check question:** Can a frame-by-frame classifier detect the "temporal risk" of a harmful sequence? (Answer: No).

- **Concept:** "Overkill Fairness"
  - **Why needed here:** Generative models often attempt to correct societal bias by erasing historical fact (e.g., generating diverse Founding Fathers). This paper quantifies this trade-off.
  - **Quick check question:** If a model generates a historically inaccurate image to ensure racial diversity, which specific metric in this paper captures that error?

- **Concept:** LLM-as-a-Judge for Video
  - **Why needed here:** Evaluating video generation (especially open-ended prompts) is difficult. The paper uses GPT-4o and Qwen2.5-VL to judge safety and hallucination, requiring a validation step against human agreement.
  - **Quick check question:** What is the reported correlation score between the LLM judge and human annotators for the Hallucination task? (Hint: Check Appendix D.1).

## Architecture Onboarding

- **Component map:** Prompt/Dataset Curator -> Model Zoo (7 T2V + 19 V2T) -> Frame Sampler -> Multi-modal LLM Evaluator OR External Classifier -> Trustworthiness Profile

- **Critical path:**
  1. Data Generation: Construct adversarial prompts or curated video datasets
  2. Inference: Run VFM to generate video (T2V) or text (V2T)
  3. Sampling: For T2V, sample frames (temporal reasoning relies on frame selection)
  4. Judging: Pass frames + criteria to the evaluator model (e.g., "Does this video contain harmful content?")
  5. Analysis: Calculate Bypass Rate (BR), Harmful Generation Rate (HGR), or Accuracy drop

- **Design tradeoffs:**
  - Frame Count vs. Cost: The paper samples 5-10 frames. Higher counts improve accuracy on temporal tasks but linearly increase token costs for LLM judges
  - Surrogate Selection: Using a strong surrogate (InternVL) for adversarial attacks ensures high attack success rates but might overfit to that specific architecture, reducing transferability to others
  - Metric Granularity: Simple "harmful/not harmful" vs. detailed taxonomies. The paper uses 13 risk categories for T2V to provide granular failure signatures

- **Failure signatures:**
  - Safety Blindness: BR = 1.0 (Open source T2V)
  - Privacy Leakage: High Location Inference Score (>50%) indicates the model is "too good" at reasoning about background metadata
  - Overkill Fairness: Score > 0.5 indicates the model prioritizes diversity over factual accuracy in historical contexts

- **First 3 experiments:**
  1. Baseline Safety Check: Run the 780 vanilla safety prompts against a local T2V model (e.g., CogVideoX). Verify that the Bypass Rate is near 1.0 (confirming the open-source gap)
  2. Scaling Law Replication: Take a V2T family with multiple sizes (e.g., InternVL 2B, 8B, 26B). Run the Privacy location inference task. Plot the inference accuracy against log(model size) to confirm the positive correlation
  3. Adversarial Transfer Test: Generate adversarial perturbations on the InternVideo2Chat surrogate. Apply these to the InternVL-2B model. Measure the performance drop to verify transferability within the same family

## Open Questions the Paper Calls Out

- **Open Question 1:** What factors other than scale govern the safety levels of video foundation models?
  - Basis in paper: The abstract states that safety "shows no correlation with model size, implying that factors other than scale govern current safety levels."
  - Why unresolved: The paper identifies the anomaly but does not isolate specific architectural or data-driven causes responsible for safety performance.
  - What evidence would resolve it: Controlled experiments that vary training data composition and alignment techniques while holding model size constant.

- **Open Question 2:** Why do fairness and privacy risks increase with model size in Video-to-Text (V2T) models?
  - Basis in paper: The evaluation reveals that "unfairness and privacy risks rise with scale" in V2T models.
  - Why unresolved: The paper highlights this negative trend but does not explain why larger models perform worse on these specific trustworthiness dimensions.
  - What evidence would resolve it: Studies analyzing whether larger V2T models memorize more private or biased data from their training sets.

- **Open Question 3:** How can robust safety refusal mechanisms be implemented for open-source Text-to-Video (T2V) models?
  - Basis in paper: The authors find that "all open-source T2V models evaluated fail to recognize harmful queries" and lack refusal capabilities.
  - Why unresolved: Current alignment techniques appear absent or ineffective for T2V, leaving a critical gap compared to closed-source counterparts.
  - What evidence would resolve it: The development of T2V-specific alignment protocols (e.g., RLHF) that successfully reject harmful prompts without degrading video quality.

## Limitations

- Reliance on GPT-4o as the primary evaluator, despite reported 86-88% human agreement, introduces potential systematic bias in safety and hallucination judgments
- Safety evaluation focuses on English prompts and may not generalize to multilingual contexts or culturally-specific harm definitions
- Adversarial robustness evaluation uses transferability-based attacks which may not capture worst-case scenarios where attackers can directly optimize against target models

## Confidence

- **High Confidence (8-10/10):** The observed safety gaps between open-source and closed-source T2V models are robust, as evidenced by universal bypass rates for open-source models and concrete failure modes for closed-source models on temporal/physical risks. The scaling correlation showing V2T models' privacy risks increase with size is also well-supported with statistical significance.
- **Medium Confidence (5-7/10):** The safety-vs-size correlation showing no relationship between model size and safety performance has moderate confidence due to the limited sample size of closed-source models and potential content moderation API changes over time.
- **Low Confidence (2-4/10):** The generalizability of fairness metrics across different cultural contexts and the assumption that LLM-as-a-judge provides consistent evaluation across diverse video content types have lower confidence due to limited cross-cultural validation.

## Next Checks

1. **Cross-Cultural Safety Validation:** Run the safety benchmark using GPT-4o and a culturally-diverse human annotator pool (including non-Western perspectives) on a subset of 100 prompts to verify the safety judgments generalize beyond the reported 86-88% agreement rate.

2. **Temporal Risk Benchmark Extension:** Develop a controlled test set with identical frames but different temporal sequences (one harmless, one containing strobe/light-induced risks) to empirically validate whether frame-by-frame classifiers indeed fail at detecting temporal risks as hypothesized.

3. **Transfer Attack Robustness:** Generate adversarial examples using a different surrogate architecture (e.g., VideoCrafter2) and test transferability to InternVL-2B and other models to confirm that the observed transferability patterns aren't specific to the InternVL surrogate model.