---
ver: rpa2
title: Decoding Neighborhood Environments with Large Language Models
arxiv_id: '2505.08163'
source_url: https://arxiv.org/abs/2505.08163
tags:
- llms
- image
- accuracy
- prompt
- indicators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of large language models (LLMs) to
  decode neighborhood environments using Google Street View images, addressing the
  challenge of scaling environmental assessments without extensive labeled training
  data. A YOLOv11 model was trained as a baseline, achieving 99.13% mAP50 and 96.3%
  F1 score for detecting six environmental indicators.
---

# Decoding Neighborhood Environments with Large Language Models

## Quick Facts
- **arXiv ID:** 2505.08163
- **Source URL:** https://arxiv.org/abs/2505.08163
- **Reference count:** 39
- **Primary result:** LLMs achieve 88% accuracy for neighborhood environmental assessment without training data, compared to 99% for supervised YOLOv11

## Executive Summary
This study demonstrates that large language models with vision capabilities can effectively decode neighborhood environments from Google Street View images without requiring labeled training data. Using a YOLOv11 model as a baseline, the research evaluates four LLMs (ChatGPT 4o mini, Gemini 1.5 Pro, Claude 3.7, Grok-2) for detecting six environmental indicators including streetlights, sidewalks, and powerlines. The ensemble approach with majority voting across top three LLMs achieves over 88% accuracy, showing that LLMs can serve as a viable alternative to traditional computer vision models when training data is scarce or unavailable.

## Method Summary
The study employs a two-pronged approach: first, training a YOLOv11 Nano model on 1,200 Google Street View images from North Carolina counties as a supervised baseline, achieving 99.13% mAP50 and 96.3% F1 score; second, using four vision-enabled LLMs to perform zero-shot detection of six environmental indicators. The LLMs are prompted with parallel yes/no questions about each indicator in a single prompt, and majority voting is applied across the top three models to improve accuracy. The approach leverages the pre-trained visual-semantic representations of LLMs, eliminating the need for labeled training data while maintaining competitive performance.

## Key Results
- YOLOv11 baseline achieved 99.13% mAP50 and 96.3% F1 score
- Individual LLMs achieved 84-88% accuracy for environmental indicator detection
- Majority voting across top three LLMs improved accuracy to over 88%
- English prompts outperformed non-English prompts by 15-20% in recall accuracy
- Parallel prompts outperformed sequential prompts by 8-12% in recall

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Visual Recognition via Pre-Trained Vision-Language Representations
LLMs with vision capabilities detect environmental indicators without task-specific training by leveraging pre-trained visual-semantic representations from massive paired image-text corpora. When prompted with natural language queries about specific objects, the model aligns the prompt to learned visual concepts without requiring labeled training data for this specific task.

### Mechanism 2: Ensemble Consensus via Majority Voting Reduces Individual Model Biases
Aggregating predictions across multiple LLMs through majority voting improves overall accuracy beyond any single model. Different LLMs have different training distributions and failure modes, so when at least 2 of 3 top-performing models agree on a prediction, uncorrelated errors are filtered out while systematic biases common across models persist.

### Mechanism 3: Prompt Structure Constrains Model Attention and Output Format
Parallel prompts (single compound query for all indicators) outperform sequential prompts (multiple individual queries) because simpler sentence structures reduce cognitive load on the model's attention mechanism. English prompts benefit from dominant representation in pre-training data, yielding better visual-semantic grounding.

## Foundational Learning

- **Concept: Zero-Shot vs. Few-Shot Learning**
  - Why needed: The paper's core contribution is demonstrating zero-shot capability—using LLMs without any labeled training data. Understanding this distinction clarifies why the 84-88% LLM accuracy (vs. 99% for supervised YOLOv11) is meaningful.
  - Quick check: If you had 100 labeled GSV images available, would zero-shot or few-shot prompting likely yield better accuracy, and why?

- **Concept: Object Detection Metrics (Precision, Recall, F1, mAP)**
  - Why needed: The paper reports per-class performance variations (e.g., single-lane road F1=0.66 for ChatGPT vs. F1=0.93 for multilane road). Understanding these metrics is essential for interpreting where LLMs fail.
  - Quick check: If a model has 100% recall but 30% precision for "apartment" detection, what does this mean practically for neighborhood assessment?

- **Concept: Vision-Language Model Architecture**
  - Why needed: The paper assumes readers understand how LLMs process images. Knowing that models like GPT-4o use an image encoder + projection layer to align visual features with language embeddings clarifies why prompt language affects visual recognition.
  - Quick check: Why might an English prompt for "sidewalk" yield different results than a Chinese prompt for the same concept, even if the image is identical?

## Architecture Onboarding

- **Component map:** Google Street View API → 640×640 pixel images from 4 cardinal directions → YOLOv11 Nano training (70/20/10 split) or LLM inference layer → Parallel prompts to ChatGPT, Gemini, Claude, Grok → Majority voting across top 3 LLMs → Binary presence/absence output

- **Critical path:** 1) Define target environmental indicators and create parallel prompt template; 2) Fetch GSV images via API (coordinate-based, 4 directions per location); 3) Send images + parallel prompts to at least 3 LLMs; 4) Aggregate via majority voting; 5) Validate against held-out labeled data

- **Design tradeoffs:** Speed vs. accuracy (single LLM faster, 84-88% accuracy vs. ensemble slower, 88.5% accuracy); Prompt simplicity vs. specificity (simple yes/no parallel prompts better per paper); Language selection (English prompts highest accuracy vs. local language for non-English stakeholders 15-20% accuracy drop); Model selection (Gemini 1.5 Pro highest individual accuracy, but ensemble provides robustness)

- **Failure signatures:** Single-lane road detection lowest ensemble accuracy (68.19%); Non-English prompts Chinese sidewalk detection at 1% recall; Complex sequential prompts 8-12% recall drop vs. parallel prompts; Data augmentation risks (flipping/rotating streetlight images degraded YOLOv11 performance due to "inherent directionality")

- **First 3 experiments:** 1) Baseline validation: Replicate parallel prompt format on held-out 50-100 GSV images from target geography; measure per-indicator precision/recall against manual labels; 2) Ablation on prompt format: Compare parallel vs. sequential prompting on same image set using single LLM (recommend Gemini 1.5 Pro); quantify accuracy delta; 3) Ensemble threshold test: Test majority voting vs. unanimous-only voting (require all 3 LLMs to agree); trade off coverage vs. precision

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating multiple consecutive images from different directions improve detection accuracy of partially occluded environmental indicators compared to single-frame analysis?
  - Basis: The authors state they will incorporate multiple consecutive images in different directions to improve performance, especially for partially occluded indicators.

- **Open Question 2:** Can prompt engineering or few-shot learning techniques effectively mitigate the 15-20% accuracy drop observed when using non-English prompts for neighborhood assessment?
  - Basis: The authors note optimizing prompts for cross-lingual robustness and fine-tuning LLMs on a region remains an open challenge after observing significant performance degradation with Spanish and Chinese prompts.

- **Open Question 3:** To what extent does fine-tuning LLMs on region-specific environmental data narrow the performance gap between zero-shot LLMs and supervised models like YOLOv11?
  - Basis: The paper concludes LLMs achieve over 88% accuracy without training but still lag behind the 99.13% accuracy of the supervised YOLOv11 baseline, suggesting fine-tuning as a potential avenue for improvement.

- **Open Question 4:** How do computational costs and API latency of majority voting schemes impact practical scalability of LLM-based environmental audits compared to single-model inference?
  - Basis: The paper notes that while majority voting improves accuracy, it "might introduce practical barriers such as computational costs and API latency."

## Limitations

- **Geographic Generalization Gap:** Performance demonstrated only on two North Carolina counties; single-lane road detection accuracy (68.19%) suggests significant geographic bias that may not hold for urban areas with different architectural styles.
- **Language Representation Imbalance:** English prompts achieve 89.7% recall versus 69% for simplified Chinese, revealing severe visual-semantic grounding disparities that limit utility for multilingual applications.
- **Prompt Structure Sensitivity:** While parallel prompts outperform sequential by 8-12% recall, the paper doesn't explore why this occurs or whether alternative prompt engineering strategies could further improve performance.

## Confidence

**High Confidence Claims:**
- YOLOv11 baseline performance (99.13% mAP50, 96.3% F1) with specific training details
- Ensemble majority voting improves accuracy over individual LLMs (88.5% vs 84-88%)
- Prompt language significantly impacts performance (English vs non-English 15-20% gap)

**Medium Confidence Claims:**
- Zero-shot LLM capability for environmental indicator detection without training data
- Parallel prompt superiority over sequential format
- Model-specific performance rankings (Gemini 1.5 Pro > others)

**Low Confidence Claims:**
- Geographic generalization beyond Robeson and Durham counties
- Robustness to architectural and urban design variations
- Cross-lingual performance parity

## Next Checks

1. **Geographic Transfer Test:** Apply validated prompt and majority voting ensemble to GSV images from at least two geographically distinct urban areas (e.g., Midwest US and Southeast Asia); measure per-indicator accuracy drop and identify which indicators show greatest degradation, particularly single-lane road detection.

2. **Language-Agnostic Prompting Experiment:** Test whether visual descriptors combined with universal symbols or icons in prompts can achieve parity with English performance for non-English speakers; compare accuracy of English text prompts versus symbol-based prompts in Chinese and Spanish.

3. **Prompt Engineering Ablation Study:** Systematically test alternative prompt structures including detailed descriptions with examples, multiple-choice formats, and confidence-weighted responses; measure whether parallel prompt advantage holds or if other structures can achieve 90%+ accuracy without ensemble voting.