---
ver: rpa2
title: AI-generated data contamination erodes pathological variability and diagnostic
  reliability
arxiv_id: '2601.12946'
source_url: https://arxiv.org/abs/2601.12946
tags:
- data
- synthetic
- clinical
- generation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-referential training on synthetic medical data causes rapid
  erosion of diagnostic accuracy and pathological variability across clinical text,
  vision-language reports, and medical images. As models train on their own outputs,
  vocabulary shrinks (98.9% reduction in radiology reports), rare findings disappear,
  and false reassurance rates triple to 40%.
---

# AI-generated data contamination erodes pathological variability and diagnostic reliability

## Quick Facts
- arXiv ID: 2601.12946
- Source URL: https://arxiv.org/abs/2601.12946
- Reference count: 40
- Self-referential training on synthetic medical data causes rapid vocabulary collapse, diagnostic accuracy erosion, and pathological variability loss

## Executive Summary
This study demonstrates that iteratively training medical AI models on their own synthetic outputs causes rapid degradation of diagnostic accuracy and clinical reliability. Using MIMIC-CXR, i2b2, and private glaucoma datasets, the research shows vocabulary shrinking by 98.9%, rare findings disappearing, and false reassurance rates tripling to 40%. Models become overconfident on degraded synthetic content while failing to detect critical conditions like pneumothorax. Physician evaluation reveals near-complete rewrite requirements after just two generations, necessitating mandatory human oversight and provenance tagging to preserve diagnostic integrity.

## Method Summary
The study employed a self-referential training loop across text, vision-language, and image generation tasks. Models were reset to original pre-trained weights at each generation to isolate data degradation effects. For text generation, GPT-2 (124M) and Qwen3-8B were fine-tuned on MIMIC-CXR and i2b2 datasets, then used to generate synthetic outputs that became the training data for the next generation. Vision-language tasks used R2GenGPT (Llama-2 7B + Swin) on MIMIC-CXR image-report pairs. Image generation used DDPM (U-Net) architecture. The loop ran for 4 generations with quality-aware filtering selecting bottom 75% synthetic and top 50% real samples based on k-NN distance vectors.

## Key Results
- Vocabulary contraction: 98.9% reduction in unique tokens in radiology reports
- Clinical safety degradation: False reassurance rates increased to 40% across generations
- Critical condition detection failure: Models missed life-threatening findings including pneumothorax and effusions
- Physician assessment: Near-complete rewrite requirements after 2 generations of self-training

## Why This Works (Mechanism)
The mechanism of model collapse occurs because synthetic data lacks the statistical diversity and pathological variability present in real clinical data. As models train exclusively on their own outputs, they progressively reinforce common patterns while eliminating rare but clinically important findings. The confidence gap emerges because PPL on synthetic data artificially improves while real data performance deteriorates, creating false assurance. Without human oversight, this self-reinforcing loop leads to diagnostic systems that appear confident but are fundamentally unreliable.

## Foundational Learning
- **Self-referential training**: Models iteratively trained on their own synthetic outputs - needed to simulate data contamination scenarios, check by tracking vocabulary size across generations
- **Model collapse**: Progressive degradation of model performance when trained on synthetic data - needed to understand clinical safety implications, check by measuring false reassurance rates
- **Confidence gap**: Divergence between PPL on synthetic vs real data - needed to detect degradation early, check by plotting PPL curves across generations
- **Vocabulary contraction**: Rapid reduction in unique tokens and medical terminology - needed to quantify information loss, check by counting unique medical terms per generation
- **Quality-aware filtering**: Selection of synthetic samples based on distance from real data distribution - needed to simulate realistic contamination scenarios, check by measuring k-NN distances

## Architecture Onboarding
**Component map**: Real Data -> Generation 0 (Fine-tune) -> Synthetic Data -> Reset Weights -> Generation 1 (Fine-tune on synthetic) -> Repeat for 4 generations

**Critical path**: Fine-tuning on authentic data → synthetic generation → quality filtering → weight reset → fine-tuning on synthetic → evaluation on real test set

**Design tradeoffs**: Resetting weights isolates data degradation from model drift but may not reflect continuous deployment scenarios; quality filtering attempts to preserve realism but may still propagate biases; small model sizes enable rapid iteration but may not represent production systems

**Failure signatures**: Vocabulary reduction >90% within 2 generations; false reassurance rates >30%; physician edit distance approaching 100%; pneumothorax detection dropping below 50%

**First experiments**: 1) Track unigram repetition scores daily to detect early collapse; 2) Plot PPL synthetic vs PPL real to identify confidence gap; 3) Calculate medical term density per generation to quantify clinical vocabulary loss

## Open Questions the Paper Calls Out
None

## Limitations
- Study relies entirely on publicly available datasets rather than prospectively collected clinical data
- Pre-trained GPT-2 models may not reflect current state-of-the-art medical AI systems
- Controlled experimental conditions may not fully capture real-world clinical environment complexity
- Effectiveness of proposed mitigation strategies not validated in practical implementation

## Confidence
- **High confidence**: Documented vocabulary contraction (98.9% reduction), systematic increase in false reassurance rates (reaching 40%), physician evaluation showing near-complete rewrite requirements
- **Medium confidence**: Broader clinical implications regarding diagnostic reliability and patient safety
- **Low confidence**: Effectiveness of proposed mitigation strategies in practice

## Next Checks
1. Replicate self-referential training loop using contemporary large language models (GPT-4 or Claude) to assess model collapse persistence
2. Conduct prospective clinical trials comparing diagnostic accuracy between AI systems trained on mixed real-synthetic data versus purely real data across multiple healthcare institutions
3. Implement and test proposed provenance tagging system in simulated clinical workflow to evaluate practical feasibility and impact on diagnostic decision-making