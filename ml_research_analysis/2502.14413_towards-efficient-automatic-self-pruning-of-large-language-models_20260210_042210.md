---
ver: rpa2
title: Towards Efficient Automatic Self-Pruning of Large Language Models
arxiv_id: '2502.14413'
source_url: https://arxiv.org/abs/2502.14413
tags:
- pruning
- llms
- self-pruner
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) due to their enormous size. The proposed method, Self-Pruner, is an
  end-to-end automatic self-pruning framework that leverages LLMs to autonomously
  execute the entire evolutionary search process to search for layer-wise pruning
  rates.
---

# Towards Efficient Automatic Self-Pruning of Large Language Models

## Quick Facts
- arXiv ID: 2502.14413
- Source URL: https://arxiv.org/abs/2502.14413
- Reference count: 23
- Primary result: End-to-end automatic self-pruning framework using LLMs to autonomously execute evolutionary search for layer-wise pruning rates, achieving 1.39× speedup on LLaMA-2-70B pruned to 49B with only 0.80% accuracy drop.

## Executive Summary
This paper addresses the challenge of deploying large language models (LLMs) by introducing Self-Pruner, an end-to-end automatic self-pruning framework. The method leverages LLMs to autonomously execute the entire evolutionary search process, generating populations, selecting parent solutions, and performing crossover and mutation operations to find optimal layer-wise pruning rates. Extensive experiments demonstrate that Self-Pruner achieves superior performance compared to existing state-of-the-art methods while maintaining significant speedup and accuracy.

## Method Summary
Self-Pruner is an end-to-end automatic self-pruning framework that uses LLMs (GPT-4o) to autonomously execute evolutionary search for layer-wise pruning rates. The method prompts an Oracle LLM with the current population of pruning configurations and their fitness scores to generate new offspring configurations while maintaining constraints. The evolutionary search optimizes a vector of layer-wise pruning rates, identifying layers with higher redundancy versus critical layers. Perplexity on WikiText-2 serves as a proxy fitness signal during search, which correlates with downstream task performance.

## Key Results
- Pruned LLaMA-2-70B to 49B level with only 0.80% drop in accuracy across seven commonsense reasoning tasks
- Achieved 1.39× speedup on NVIDIA A100 80GB GPU
- Further pruning to 35B level resulted in only 3.80% decrease in accuracy while obtaining 1.70× speedup

## Why This Works (Mechanism)

### Mechanism 1
Using a capable Large Language Model (Oracle LLM) to execute evolutionary search operators accelerates convergence to optimal pruning rates compared to random or manual heuristics. The system prompts GPT-4o with current population and fitness scores, using its reasoning capabilities to generate new offspring configurations that maintain constraints while exploring the search space.

### Mechanism 2
Assigning non-uniform, layer-wise pruning rates based on specific layer sensitivity preserves model accuracy better than uniform pruning. The evolutionary search optimizes a vector where each element represents pruning rate for a specific transformer layer, identifying layers that can tolerate higher rates versus critical layers requiring lower rates.

### Mechanism 3
Perplexity on a small calibration dataset (WikiText-2) acts as a reliable proxy fitness signal for downstream task performance during the search. Instead of expensive downstream evaluations, the method uses WikiText-2 perplexity as the fitness score, assuming lower perplexity correlates with better preservation of language modeling capabilities.

## Foundational Learning

- **Structured Pruning vs. Unstructured Pruning**: Needed because the paper specifically optimizes for structured pruning (removing entire neurons/channels) which differs from unstructured pruning and leads to actual hardware speedups. Quick check: Does removing 50% of individual weights provide the same inference speedup as removing 50% of channels on a standard GPU? (Answer: No)

- **Evolutionary Algorithms (EA)**: The core logic relies on generating populations, evaluating them, and mixing/mutating the best ones. Quick check: What represents the "DNA" of an individual in the population? (Answer: A list of floating-point numbers representing layer-wise pruning rates)

- **Perplexity**: This is the objective function (fitness). Needed to understand that perplexity measures how "surprised" a model is by test data; lower is better. Quick check: If a pruned model has perplexity of 5.0 compared to baseline of 3.0, has the model improved or degraded? (Answer: Degraded)

## Architecture Onboarding

- **Component map**: Oracle LLM (GPT-4o) -> Prompt Manager -> Pruning Engine (Wanda-sp) -> Fitness Evaluator
- **Critical path**: The interaction between the Oracle LLM and the Fitness Evaluator, bottlenecked by inference time to evaluate perplexity for every individual in every generation
- **Design tradeoffs**: API Cost vs. Search Iterations (using GPT-4o is faster but incurs high costs), Population Size (K) vs. Stability (small population is fast but risks premature convergence)
- **Failure signatures**: Constraint Drift (Oracle LLM outputs rates that don't average to target), Format Errors (LLM outputs conversational text instead of strict format), Gradient Collapse (high pruning rates lead to perplexity explosion)
- **First 3 experiments**: Sanity Check (Random vs. LLM population generation), Ablation (Operators - remove Crossover then Mutation), Correlation Check (WikiText-2 perplexity vs. specific downstream task)

## Open Questions the Paper Calls Out
- How can Self-Pruner be extended to achieve full end-to-end automation beyond just determining layer-wise pruning rates?
- What specific efficient fine-tuning strategies are most effective for recovering accuracy in post-training structured pruning at high pruning rates (e.g., 50%)?
- Is the reliance on large, proprietary models (e.g., GPT-4o) as the evolutionary search engine computationally efficient compared to traditional heuristic search methods?

## Limitations
- Performance gains rely heavily on Oracle LLM executing evolutionary operators correctly - a hard dependency with unclear failure rate
- Correlation between WikiText-2 perplexity and zero-shot commonsense task accuracy is assumed but not rigorously validated
- Implementation details for the pruning backend (Wanda-sp) are not provided and wall-clock search time/API costs are not reported

## Confidence
- **High**: Clear experimental setup, consistent results across multiple model sizes (7B-70B), technically sound framework
- **Medium**: Claim that GPT-4o provides superior search guidance supported by ablation but depends on unknown prompt parameters
- **Low**: Fitness proxy assumption (WikiText-2 perplexity) not experimentally validated beyond observed correlation

## Next Checks
1. Measure and report Pearson correlation between WikiText-2 perplexity and accuracy on each individual commonsense task
2. Run the search with only mutation, only crossover, and with random search to isolate contribution of each evolutionary component
3. Intentionally provide Oracle LLM with infeasible average pruning rate and verify constraint violation detection without crashing