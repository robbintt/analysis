---
ver: rpa2
title: 'CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction
  with Domain-Aware Transformers'
arxiv_id: '2506.18185'
source_url: https://arxiv.org/abs/2506.18185
tags:
- task
- subtask
- extraction
- recall
- insomnia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a system for SMM4H-HeaRD 2025 shared tasks
  focusing on insomnia detection from clinical notes and food safety event extraction
  from news articles. For Task 5 (food safety event detection), the authors achieved
  first place with an F1 score of 0.958 on sentence-level classification by using
  RoBERTa with GPT-4 data augmentation and class weighting.
---

# CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers

## Quick Facts
- arXiv ID: 2506.18185
- Source URL: https://arxiv.org/abs/2506.18185
- Reference count: 4
- First place in Task 5 (food safety event detection) with F1 score of 0.958

## Executive Summary
This paper presents CareLab's systems for two shared tasks in the SMM4H-HeaRD 2025 challenge. For Task 4 (insomnia detection from clinical notes), ClinicalBERT achieved the best multi-label classification performance (F1=0.769) but struggled with evidence extraction using rule-based approaches (F1=0.135). For Task 5 (food safety event detection), the team achieved first place using RoBERTa with GPT-4 data augmentation and class weighting, reaching F1=0.958 on sentence-level classification. The authors demonstrate that domain-specific pretraining and strategic data augmentation significantly improve classification performance, while highlighting the need for span-based neural models for robust evidence extraction in clinical text.

## Method Summary
The system combines domain-specific transformer models with strategic data augmentation. For Task 4, ClinicalBERT was fine-tuned for multi-label classification using binary cross-entropy loss, achieving F1=0.769. For Task 5, RoBERTa-large was employed with GPT-4-generated synthetic examples for class balancing, class-weighted loss, and ensemble inference across five independently trained models. Rule-based approaches using regex patterns and spaCy dependency parsing were used for entity extraction tasks, though these showed limited effectiveness with F1 scores of 0.119-0.135.

## Key Results
- Task 4 Subtask 1: ClinicalBERT achieved F1=0.769 for multi-label insomnia classification
- Task 4 Subtask 2B: Rule-based evidence extraction achieved F1=0.135
- Task 5 Subtask 1: First place with F1=0.958 using RoBERTa + GPT-4 augmentation + ensemble
- Task 5 Subtask 2: Rule-based entity extraction achieved F1=0.119-0.135

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Pretraining Transfer
ClinicalBERT's pretraining on clinical corpora improves classification of medical terminology compared to general-purpose transformers. Domain-specific vocabulary and contextual patterns learned during pretraining reduce the gap between pretrained representations and clinical text distributions, enabling faster convergence and better generalization on limited clinical data. Core assumption: The pretraining corpus (MIMIC notes) sufficiently covers the insomnia-related terminology and clinical narrative structures in the target task. Evidence: ClinicalBERT performed best in multi-label classification (F1=0.769) and consistently outperformed other models due to contextual embeddings and domain-specific pretraining. Break condition: When target clinical subdomain uses terminology or documentation styles poorly represented in pretraining data.

### Mechanism 2: LLM-Augmented Class Balancing
GPT-4 generated synthetic negative examples combined with class-weighted loss improves recall on underrepresented classes while maintaining precision. Synthetic augmentation expands the decision boundary for the "Neither" class by providing diverse neutral examples; class weighting amplifies gradient signals from minority classes during training. Core assumption: GPT-4 can generate realistic FDA-style press releases that match the linguistic distribution of real documents without introducing artifacts. Evidence: Employed RoBERTa with GPT-4 data augmentation and augmented training dataset with GPT-4-generated examples that resembled neutral content from FDA documents. Break condition: When synthetic examples diverge from test distribution, creating spurious patterns the model overfits.

### Mechanism 3: Ensemble Inference for Robustness
Averaging predictions across multiple independently seeded models reduces variance and improves generalization. Different random initializations lead to different local optima; averaging predictions smooths idiosyncratic errors from individual models. Core assumption: Individual model errors are at least partially uncorrelated. Evidence: Used ensemble inference by averaging predictions across five independently trained models using different random seeds. Break condition: When all models make systematically correlated errors due to shared training data biases.

## Foundational Learning

- **[CLS] Token Classification Head**: Why needed here: Both ClinicalBERT and RoBERTa use [CLS] token representations for document/sentence-level classification. Quick check question: Explain why the [CLS] token is trained to aggregate sequence-level information during pretraining.
- **Multi-Label Binary Cross-Entropy**: Why needed here: Task 4 Subtask 2A requires predicting multiple diagnostic rule labels simultaneously (not mutually exclusive). Quick check question: How does sigmoid activation per label differ from softmax for multi-class classification?
- **Span Extraction vs. Sequence Labeling**: Why needed here: The paper shows rule-based span extraction (F1=0.135) vs. neural approaches needed for evidence extraction. Quick check question: Why would BIO tagging with CRF outperform regex for discontinuous or nested spans?

## Architecture Onboarding

- Component map: Raw text → Preprocessing → [GPT-4 Augmentation pipeline] → Training data + Synthetic examples → RoBERTa/ClinicalBERT encoder → [CLS] token → Classification head → Class-weighted loss (wc ∝ 1/frequency) → 5× models (different seeds) → Prediction averaging → Final output
- Critical path: 1. Compute class frequencies and generate GPT-4 synthetic examples for minority classes 2. Fine-tune transformer with class-weighted BCE loss 3. Run ensemble inference at test time
- Design tradeoffs: Rule-based extractors: Fast, interpretable, but F1=0.119–0.135 on extraction tasks due to paraphrase/implicit mention failures; Neural span models: Higher annotation cost but required for robust evidence extraction; Class weighting vs. SMOTE: Paper uses both—SMOTE for traditional baselines, weighting for neural models
- Failure signatures: High recall (1.000) + low precision (0.648): Over-prediction from class imbalance and ambiguous negative examples; Near-zero recall on Disease/Cause entities: Regex patterns fail on semantic variations ("improper handling" vs. "bacterial contamination"); Confusion between Outbreak and Recall classes: Lexical overlap in news text requires contextual disambiguation
- First 3 experiments: 1. Ablation: Train RoBERTa without GPT-4 augmentation to quantify augmentation contribution (expect F1 drop on minority classes) 2. Baseline comparison: Replace ClinicalBERT with standard BERT on Task 4 to measure domain pretraining benefit 3. Architecture swap: Implement CRF-augmented transformer for Task 5 Subtask 2 entity extraction vs. rule-based baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can contrastive pretraining or hard negative mining effectively mitigate the high false positive rate observed in ClinicalBERT-based insomnia classification?
- Basis in paper: The authors state in the Error Analysis that "future work could explore contrastive pretraining or hard negative mining to reduce false positives" caused by ambiguous medication mentions or co-occurring conditions.
- Why unresolved: The current ClinicalBERT model achieved perfect recall (1.0) but low precision (0.648), indicating a tendency to over-predict positive insomnia cases due to subtle linguistic cues and a lack of explicit negative examples.
- What evidence would resolve it: Experiments applying these techniques to Task 4 Subtask 1 data, showing a significant increase in precision without a substantial loss of recall.

### Open Question 2
- Question: How do CRF-augmented transformers or joint span-sequence frameworks perform on the FORCE dataset compared to the rule-based baseline for extracting implicit food safety entities?
- Basis in paper: The Conclusion and Error Analysis state that "future work will investigate context-aware neural sequence labeling models, such as CRF-augmented transformers" to address the failure of regex models on implicit expressions.
- Why unresolved: The current rule-based system failed to generalize (F1=0.119) on Task 5 Subtask 2, specifically missing implicit or semantically diverse expressions like generic references ("the item") or coreference resolution.
- What evidence would resolve it: Quantitative results from training these proposed architectures on the FORCE dataset, specifically measuring recall improvements for "Cause" and "Disease" entity types.

### Open Question 3
- Question: To what extent does Span Selection Pre-training improve the extraction of discontinuous or indirect insomnia evidence spans in clinical notes?
- Basis in paper: The authors note in the Methodology and Conclusion that "Recent advancements in span-based pre-training techniques—such as Span Selection Pre-training—offer promising improvements" over rule-based approaches.
- Why unresolved: The regex-based approach used for Task 4 Subtask 2B yielded an F1 of only 0.135 because it could not handle idiomatic expressions, embedded temporal cues, or non-contiguous span annotations.
- What evidence would resolve it: A comparative study evaluating span-aware models (e.g., QA-style fine-tuning) against the regex baseline on the MIMIC-III evidence extraction task.

## Limitations

- Rule-based entity extraction achieved only F1=0.119-0.135, demonstrating fundamental inadequacy for handling paraphrased or implicit mentions in clinical and news text
- Critical implementation details missing: exact number of GPT-4 synthetic examples, training hyperparameters (learning rate, batch size, epochs), specific class weight values, and random seeds for ensemble models
- Limited error analysis scope: No ablation studies quantifying individual contribution of data augmentation, class weighting, or ensemble averaging components

## Confidence

- **High confidence**: Domain-specific pretraining benefits (ClinicalBERT vs. general transformers) and ensemble averaging mechanisms are well-established in literature and directly supported by reported performance improvements
- **Medium confidence**: GPT-4 data augmentation contribution is inferred from task ranking improvements but lacks ablation studies or synthetic example quality analysis
- **Low confidence**: Rule-based evidence extraction viability claims are contradicted by poor performance metrics, suggesting the approach is fundamentally inadequate for clinical text

## Next Checks

1. Conduct ablation study comparing RoBERTa performance with and without GPT-4 augmentation across minority classes to quantify synthetic data contribution
2. Implement CRF-enhanced transformer architecture for Task 5 entity extraction and compare against rule-based baseline to establish neural approach necessity
3. Perform error analysis on false positive predictions from Task 4 insomnia classification to identify medication mention ambiguity patterns and test context-aware filtering strategies