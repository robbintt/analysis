---
ver: rpa2
title: Modality-Composable Diffusion Policy via Inference-Time Distribution-level
  Composition
arxiv_id: '2503.12466'
source_url: https://arxiv.org/abs/2503.12466
tags:
- diffusion
- policy
- mcdp
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Modality-Composable Diffusion Policy (MCDP),
  a method to enhance diffusion-based robotic policies by combining multiple pre-trained
  single-modality diffusion policies at inference time without additional training.
  MCDP leverages the compositional properties of diffusion models to combine distributional
  scores from policies trained on different visual modalities (e.g., RGB images and
  point clouds), creating a more expressive and adaptable policy.
---

# Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition

## Quick Facts
- **arXiv ID**: 2503.12466
- **Source URL**: https://arxiv.org/abs/2503.12466
- **Reference count**: 9
- **Primary result**: Achieves up to 0.86 success rate on RoboTwin tasks by composing RGB and point cloud diffusion policies at inference time, outperforming individual policies (0.42 and 0.62).

## Executive Summary
This paper proposes Modality-Composable Diffusion Policy (MCDP), a method to enhance diffusion-based robotic policies by combining multiple pre-trained single-modality diffusion policies at inference time without additional training. MCDP leverages the compositional properties of diffusion models to combine distributional scores from policies trained on different visual modalities (e.g., RGB images and point clouds), creating a more expressive and adaptable policy. The authors demonstrate that MCDP achieves superior performance compared to unimodal diffusion policies on the RoboTwin dataset, with success rates reaching up to 0.86 in certain tasks, surpassing individual policies that achieve 0.42 and 0.62. The paper also provides empirical guidance on optimal weight configurations for combining policies based on their relative performance, showing that MCDP is most effective when the better-performing unimodal policy is given higher weight in the composition. The approach offers a practical and efficient way to improve policy generalization and adaptability across different modalities and domains.

## Method Summary
MCDP composes pre-trained diffusion policies from different visual modalities (RGB and point cloud) at inference time by summing their noise estimates (scores) with weighted coefficients. The method takes separate RGB and point cloud observations, processes them through their respective frozen diffusion policy networks (DP_img and DP_pcd), and combines their output noise predictions using a weighted sum: $\hat{\epsilon} = w_1\epsilon_1 + w_2\epsilon_2$. This composed noise is then used in the standard DDPM denoising loop to generate actions. The approach requires no training, only manual tuning of composition weights based on unimodal policy performance. Both policies are aligned to use the same scheduler (DDPM with 100 steps) for coherent trajectory generation.

## Key Results
- Achieved 0.86 success rate on "Empty Cup Place" task, surpassing individual RGB (0.42) and point cloud (0.62) policies
- Demonstrated consistent improvement across 7 manipulation tasks in RoboTwin benchmark
- Showed that optimal performance requires assigning higher weights to the better-performing unimodal policy (weight tuning critical for success)
- Validated that composition works best when modalities provide complementary information rather than redundant signals

## Why This Works (Mechanism)

### Mechanism 1: Distribution Intersection via Score Summation
- **Claim:** Summing the noise estimates (scores) of independent diffusion policies effectively multiplies their probability distributions, narrowing the sampling space to regions where both modalities agree.
- **Mechanism:** The method leverages Energy-Based Model (EBM) theory, where the product of probabilities $p_1 \times p_2$ equates to the sum of energy gradients (scores). By calculating $\hat{\epsilon}_{M^*} = w_1 \epsilon_1 + w_2 \epsilon_2$, the denoising step moves toward a trajectory that satisfies the constraints of both the RGB and point-cloud policies simultaneously.
- **Core assumption:** The action trajectories from different modalities share a sufficiently overlapping high-probability region in the state space; if distributions are disjoint, the composed gradient may point toward a low-density void.
- **Evidence anchors:**
  - [section 3.1]: Derives the composed score function $\nabla_\tau \log p(\tau | c_1, \dots)$ as a sum of individual scores.
  - [section 3.2]: Implements this via a weighted sum of noise estimates (Eq. 10).
  - [corpus]: High relevance neighbor "Compose Your Policies!" validates the general paradigm of test-time distribution-level composition for robot policies.
- **Break condition:** Fails if unimodal policies produce conflicting action predictions (e.g., one predicts "left" with high confidence while the other predicts "right"), resulting in gradient cancellation or invalid actions.

### Mechanism 2: Complementary Error Filtering
- **Claim:** Composition mitigates modality-specific failures (e.g., RGB lighting issues vs. depth sparsity) by averaging out noise, provided one policy maintains high accuracy in the failure region of the other.
- **Mechanism:** If Policy A fails due to a visual artifact but Policy B (different modality) succeeds, Policy B's score dominates the update direction in that region of the trajectory. The composed distribution effectively "filters" trajectories that fall into the failure basin of either individual policy.
- **Core assumption:** The errors in one modality are not systematic or identically present in the other modality (error independence).
- **Evidence anchors:**
  - [section 4.1, Finding 2]: Notes that performance gains require the stronger policy to counterbalance the weaker one.
  - [Figure 4(b)]: Visualizes how MCDP corrects "Directional Deviation" from one policy using the other.
  - [corpus]: "Fidelity-Aware Data Composition" suggests robust generalization relies on managing data/modality fidelity, implicitly supporting error filtering via diverse sources.
- **Break condition:** If one modality consistently underperforms (low accuracy), it acts as noise that degrades the superior policy, as seen in the "Pick Apple Messy" task where MCDP failed to surpass the better unimodal baseline.

### Mechanism 3: Inference-Time Weight Optimization
- **Claim:** Performance relies heavily on manually tuning composition weights ($w$) to prioritize the more reliable modality for a specific task.
- **Mechanism:** The weight $w$ scales the gradient contribution. Assigning a higher weight to the stronger policy ensures its score signal dominates the denoising process, preventing the weaker policy from derailing the trajectory.
- **Core assumption:** A static weight configuration is sufficient for the duration of the inference task, or that relative modality reliability does not shift drastically during execution.
- **Evidence anchors:**
  - [Table 1]: Shows peak performance shifts based on weight tuning (e.g., 0.86 success at $w=0.4$ vs 0.61 at $w=0.9$).
  - [section 4.1, Finding 3]: Explicitly states improvement is maximized when the better-performing DP holds a larger weight.
- **Break condition:** Fails in dynamic environments where modality reliability changes mid-trajectory (e.g., lighting changes suddenly), as fixed weights cannot adapt in real-time.

## Foundational Learning

- **Concept: Score-Based Generative Modeling (Diffusion)**
  - **Why needed here:** The core operation of MCDP is manipulating the "score" (gradient of log-probability) during the reverse diffusion process.
  - **Quick check question:** Can you explain why adding two noise predictions ($\epsilon_\theta$) corresponds to multiplying two probability distributions in the diffusion context?

- **Concept: Denoising Probabilistic Models (DDPM)**
  - **Why needed here:** Understanding the iterative denoising loop (Eq. 1) is essential to inject the composed score at the correct timestep.
  - **Quick check question:** In a standard DDPM denoising step, what is the role of the random noise term $\xi$, and does MCDP modify this term?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** The paper contrasts its CFG-free approach with theoretical composition methods that rely on unconditional scores.
  - **Quick check question:** Why does the author argue that avoiding CFG (calculating unconditional scores) improves "Policy flexibility" and "Sampling efficiency"?

## Architecture Onboarding

- **Component map:**
  - RGB Observation -> DP_img Encoder -> Noise Estimator 1 -> Weight w₁
  - Point Cloud Observation -> DP_pcd Encoder -> Noise Estimator 2 -> Weight w₂
  - Noise Estimators -> Weighted Sum -> DDPM Scheduler -> Action Output

- **Critical path:**
  1. **Scheduler Alignment (Crucial):** You must align the inference steps of both policies. The paper specifically notes converting the default DDIM scheduler (10 steps) of $DP_{pcd}$ to DDPM (100 steps) to match $DP_{img}$.
  2. **Score Composition:** At every timestep $t$, compute $\epsilon$ from both branches *before* the scheduler update.

- **Design tradeoffs:**
  - **CFG-free vs. CFG-based:** The paper chooses CFG-free (weighted sum of conditionals) for efficiency and compatibility with off-the-shelf models (like Octo/DP3) that may lack unconditional training. This sacrifices theoretical rigor for practical speed.
  - **Training-free:** Zero training cost vs. the inability to learn complex correlations between modalities (only linear combinations of scores).

- **Failure signatures:**
  - **Performance Collapse:** If $w$ is set near 0.5 for a task where one policy is incompetent (e.g., <10% SR), the composed policy often fails both modalities' tests.
  - **Scheduler Mismatch:** If one policy uses DDIM and the other DDPM without alignment, the noise levels ($\sigma_t$) will mismatch, causing incoherent trajectory generation.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Replicate $DP_{img}$ and $DP_{pcd}$ on a single task to establish unimodal success rates.
  2. **Scheduler Ablation:** Verify that changing $DP_{pcd}$ from DDIM to DDPM (100 steps) does not degrade its unimodal performance significantly before composition.
  3. **Weight Sweep:** Run MCDP on the "Empty Cup Place" task with weights $w \in \{0.1, 0.5, 0.9\}$ to confirm the non-linear performance peak observed in Table 1.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal composition weights ($w_i$) be determined dynamically or automatically during inference rather than requiring manual tuning?
- **Basis in paper:** [explicit] Section 3.2 states that the weight $w_i$ "needs to be manually tuned," and Section 4.1 concludes that "assigning higher weights to the better-performing unimodal distribution" is necessary, implying a lack of an adaptive mechanism.
- **Why unresolved:** The current work relies on grid search (e.g., 0.1 to 0.9) to identify the best static weights offline, which is inefficient and may not be optimal for varying real-time states.
- **What evidence would resolve it:** A method that adjusts weights based on the confidence or gradient magnitude of the individual policies at each timestep, achieving comparable or superior success rates without manual weight selection.

### Open Question 2
- **Question:** How can the composition framework be made robust to "failure modes" where one modality provides low-accuracy scores that degrade the joint distribution?
- **Basis in paper:** [explicit] Section 4.1, Finding 2 notes that when one policy has significantly lower accuracy, "MCDP struggles to surpass the highest accuracy of the better-performing unimodal DP" because low-accuracy scores impact the joint distribution.
- **Why unresolved:** The simple linear composition of scores averages the errors of the weaker policy into the final action, causing negative transfer.
- **What evidence would resolve it:** A modified composition rule (e.g., non-linear filtering or outlier rejection) that allows the composed policy to ignore a failing modality and match the performance of the stronger unimodal expert.

### Open Question 3
- **Question:** Can this inference-time composition approach successfully generalize to cross-domain or cross-embodiment policy transfer?
- **Basis in paper:** [explicit] The Abstract and Conclusion explicitly list "facilitating the development of generalizable cross-modality, cross-domain, and even cross-embodiment policies" as a future direction enabled by this exploration.
- **Why unresolved:** The current experiments are limited to combining different visual modalities (RGB and Point Cloud) on the same embodiment and domain; it is unverified if the score composition remains semantically valid when combining policies trained on different robot morphologies.
- **What evidence would resolve it:** Successful experiments combining diffusion policies trained on different robot arms or simulation-to-real domains without retraining, showing successful transfer of skills.

## Limitations

- Performance highly sensitive to manual weight tuning, requiring offline grid search for each task rather than adaptive online adjustment
- Cannot overcome systematic failures when one modality consistently underperforms, as poor scores negatively impact the composed distribution
- Limited to combining policies with compatible action spaces and compatible noise schedules, restricting cross-embodiment or cross-domain applications

## Confidence

- **High confidence**: The compositional mechanism via score summation is theoretically sound and experimentally validated on the RoboTwin benchmark. The core finding that better-performing policies should receive higher weights is consistently supported.
- **Medium confidence**: The practical guidance for weight selection (based on unimodal performance) works for the tested tasks but may not generalize to more complex, multi-stage manipulation scenarios.
- **Low confidence**: The paper doesn't address computational overhead implications or benchmark inference speed compared to unimodal baselines, leaving uncertainty about real-world deployment feasibility.

## Next Checks

1. **Error Correlation Analysis**: Systematically analyze whether failures in one modality correlate with failures in the other across different lighting conditions and object configurations to test the independence assumption.

2. **Dynamic Weight Adaptation**: Implement and evaluate online weight adjustment based on confidence scores or recent success rates to handle environments where modality reliability changes during task execution.

3. **Computational Overhead Benchmark**: Measure and compare inference time and memory usage between MCDP and unimodal baselines across different hardware configurations to assess practical deployment constraints.