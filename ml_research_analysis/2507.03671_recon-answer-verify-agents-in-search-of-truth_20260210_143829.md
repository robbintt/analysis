---
ver: rpa2
title: 'Recon, Answer, Verify: Agents in Search of Truth'
arxiv_id: '2507.03671'
source_url: https://arxiv.org/abs/2507.03671
tags:
- claim
- evidence
- question
- label
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating automated fact-checking
  systems under realistic conditions, where post-claim analysis and annotator cues
  are unavailable. The authors introduce Politi-Fact-Only (PFO), a manually curated
  dataset of 2,982 political claims with such cues removed, resulting in an average
  performance drop of 22% in macro-F1 for LLMs compared to unfiltered versions.
---

# Recon, Answer, Verify: Agents in Search of Truth

## Quick Facts
- arXiv ID: 2507.03671
- Source URL: https://arxiv.org/abs/2507.03671
- Authors: Satyam Shukla; Himanshu Dutta; Pushpak Bhattacharyya
- Reference count: 11
- Primary result: RA V outperforms state-of-the-art baselines on RAWFC (25.28% improvement) and HOVER (1.54–4.94% improvement)

## Executive Summary
This paper addresses the challenge of evaluating automated fact-checking systems under realistic conditions by introducing Politi-Fact-Only (PFO), a manually curated dataset of 2,982 political claims with post-claim analysis and annotator cues removed. The authors propose RA V (Recon-Answer-Verify), an agentic framework using three specialized agents to iteratively generate and answer sub-questions, improving generalization across domains and label granularities. RA V demonstrates significant performance improvements over state-of-the-art baselines while showing the least performance drop when evaluated on the challenging PFO benchmark.

## Method Summary
RA V employs a three-agent pipeline for automated fact-checking: a Question Generation agent (QGagent) that iteratively generates verification or inquiry sub-questions conditioned on previous answers, an Answer Generation agent (AGagent) that responds using provided evidence, and a Label Generation agent (LGagent) that synthesizes the complete Q&A history into a final veracity label. The framework operates iteratively with a maximum of 10 question-answer cycles, using in-context examples for prompting. The approach was evaluated on multiple benchmarks including RAWFC, HOVER, and the newly introduced PFO dataset.

## Key Results
- RA V achieves 25.28% improvement on RAWFC (3-class) compared to state-of-the-art baselines
- RA V shows 1.54–4.94% improvements on HOVER (2-class) across 2-hop, 3-hop, and 4-hop claims
- RA V demonstrates the least performance drop (16.3% macro-F1) when evaluated on PFO versus its unfiltered version
- Removing post-claim analysis causes average 22% macro-F1 performance drop across LLMs

## Why This Works (Mechanism)

### Mechanism 1: Iterative Question-Answer Conditioning
Generating questions iteratively—conditioning each on prior Q&A pairs—improves claim decomposition compared to generating all questions at once. The QGagent generates one question at a time, receives an answer from AGagent using evidence, then uses the accumulated history to identify unverified aspects. This adaptive loop allows later questions to target gaps revealed by earlier answers, rather than predicting all needed questions upfront.

### Mechanism 2: Mixed Question Type Coverage
Combining True/False verification questions with inquiry questions yields better coverage than either alone. True/False questions validate complete triples (entity₁, relation, entity₂), while inquiry questions extract missing entities or relationships. Together they enable both confirmation and discovery within the same pipeline.

### Mechanism 3: Leakage Removal Exposes Model Fragility
Removing post-claim analysis and annotator cues reveals realistic model limitations that are masked in unfiltered benchmarks. Post-claim analysis often contains verdict-signaling language ("but this photo doesn't show it") that leaks label information. Models exploit these cues rather than performing genuine verification reasoning.

## Foundational Learning

- **Multi-hop Reasoning**: Required for connecting chains of evidence across multiple statements. Quick check: Can you trace how the agent connects "author of The Shame of the Cities" → "Lincoln Steffens" → "death date 1936" → "comparison to Monique Wittig"?

- **Question-Answer Generation as Intermediate Supervision**: Decomposes fact-checking into QG→AG→LG stages, where intermediate Q&A pairs provide explicit reasoning traces. Quick check: Why does the QGagent not have access to evidence during question generation? (Answer: To ensure domain-agnostic generalization without overfitting to specific evidence patterns.)

- **Label Granularity (Fine-grained Classification)**: Required for nuanced judgment beyond binary support/refute decisions. Quick check: What distinguishes "mostly-true" from "half-true" in the PFO annotation guidelines? (Answer: Mostly-true needs clarification; half-true leaves out important details or takes things out of context.)

## Architecture Onboarding

- **Component map**: QGagent -> AGagent -> LGagent
- **Critical path**: 1) Initialize empty history H₀, 2) Loop: QGagent generates qₜ → AGagent answers aₜ using evidence → append to history → check stop signal, 3) Maximum 10 iterations, 4) LGagent synthesizes history into final label with reasoning
- **Design tradeoffs**: Iterative vs. all-at-once yields +5.43% F1 but requires k× more LLM calls; Mixed vs. single question type yields +11.82% but requires more complex prompt engineering; Evidence access in QGagent is excluded to ensure domain agnosticism
- **Failure signatures**: 59.05% of errors from insufficient evidence (AGagent outputs "Can't answer"); 23.81% from answer generation faults; 2.86% from label generation
- **First 3 experiments**: 1) Reproduce the k=10 saturation result on HOVER, 2) Ablate question types on your domain, 3) Test evidence retrieval integration with a retriever

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the RAV framework's performance shift when relying on dynamically retrieved external context instead of gold evidence? (Explicit in Conclusion: "for future work, we aim to utilise the external context as evidence")

- **Open Question 2**: How can information retrieval systems be specifically optimized to handle the iterative, sub-question-based querying of the RAV pipeline? (Explicit in Conclusion: "adapting retrievers for the question answering process in our pipeline")

- **Open Question 3**: Can the RAV pipeline be enhanced to explicitly identify and handle "insufficient evidence" rather than forcing a veracity prediction? (Inferred from human evaluation showing 59.05% of errors stem from insufficient evidence)

## Limitations

- The framework relies on gold evidence rather than live retrieval, which is a significant limitation for real-world deployment
- Iterative conditioning mechanism's effectiveness may be overstated without direct ablation comparing it to parallel decomposition methods
- Performance degrades significantly on PFO benchmark (22% macro-F1 drop), indicating models heavily rely on post-claim analysis cues

## Confidence

- High confidence: Performance drops on PFO and RA V's superior performance on RAWFC/HOVER benchmarks
- Medium confidence: Iterative conditioning mechanism providing +5.43% improvement
- Low confidence: Mixed question types providing +11.82% improvement

## Next Checks

1. Run RAV(P2,T1) vs RAV(P2,T1&2) on your domain to determine if inquiry questions provide value for your claim distribution
2. Replace gold evidence with a retriever (as noted in Limitations) and measure AGagent answer quality degradation
3. Test RAV on claims requiring simultaneous multi-constraint verification to identify if sequential question generation becomes a bottleneck