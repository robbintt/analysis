---
ver: rpa2
title: Diffusion models for inverse problems
arxiv_id: '2508.01975'
source_url: https://arxiv.org/abs/2508.01975
tags:
- diffusion
- inverse
- problems
- where
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter surveys diffusion models for solving inverse problems
  in imaging, categorizing approaches into explicit approximations and other methods
  like variational inference, sequential Monte Carlo, and decoupled data consistency.
  The core idea is to leverage generative priors from diffusion models to sample from
  posterior distributions given corrupted measurements.
---

# Diffusion models for inverse problems

## Quick Facts
- arXiv ID: 2508.01975
- Source URL: https://arxiv.org/abs/2508.01975
- Reference count: 11
- Primary result: Survey of diffusion models for solving inverse problems in imaging, categorizing explicit approximations, variational inference, and decoupled data consistency methods.

## Executive Summary
This chapter surveys diffusion models for solving inverse problems in imaging, categorizing approaches into explicit approximations and other methods like variational inference, sequential Monte Carlo, and decoupled data consistency. The core idea is to leverage generative priors from diffusion models to sample from posterior distributions given corrupted measurements. Explicit methods, such as DPS, approximate the likelihood term using posterior mean estimates, while others employ variational inference or particle filters for more accurate posterior sampling. Extensions address challenges like blind inverse problems, high-dimensional data, and limited training data through techniques like test-time adaptation and ambient diffusion. Text-driven solutions further enhance reconstruction by incorporating auxiliary information. Overall, diffusion-based solvers offer a flexible, unsupervised toolkit for inverse problems, with trade-offs between computational efficiency and reconstruction accuracy.

## Method Summary
Diffusion-based solvers leverage pre-trained generative models to sample from posterior distributions in inverse problems. The primary approach approximates the intractable likelihood term using posterior mean estimates via Tweedie's theorem, converting the problem into optimizing measurement consistency through backpropagation. Alternative methods employ variational inference to balance data fidelity and generative priors, or use decoupled data consistency where denoising and measurement enforcement are separated for improved stability. These methods can handle various inverse problems including super-resolution, inpainting, and MRI reconstruction without task-specific training, with extensions for blind problems, high-dimensional data, and text-driven reconstruction.

## Key Results
- Diffusion models can solve general inverse problems by sampling from posterior distributions using generative priors
- Explicit methods like DPS approximate likelihood terms using posterior mean estimates for computational efficiency
- Variational and sequential Monte Carlo approaches provide more accurate posterior sampling at higher computational cost
- Extensions address blind inverse problems, high-dimensional data, and limited training scenarios through specialized techniques

## Why This Works (Mechanism)

### Mechanism 1: Likelihood Approximation via Posterior Mean
- **Claim:** Diffusion models can sample from the posterior $p(x|y)$ for general inverse problems by approximating the intractable likelihood term $p(y|x_t)$ using the estimated clean signal $\hat{x}_{0|t}$.
- **Mechanism:** The reverse diffusion process requires the gradient of the log-posterior (Eq. 3). While the prior score $\nabla \log p(x_t)$ is learned, the likelihood score $\nabla \log p(y|x_t)$ is intractable. Explicit methods (e.g., DPS) approximate this by pushing the expectation inside: $p(y|x_t) \approx p(y|\hat{x}_{0|t})$. This converts the problem into an optimization of the measurement consistency $\|y - A(\hat{x}_{0|t})\|^2$ via backpropagation through the denoiser.
- **Core assumption:** The approximation $p(x_0|x_t) \approx \delta(x_0 - \hat{x}_{0|t})$ (a point estimate or "Jensen's approximation") sufficiently captures the likelihood landscape without tracking the full covariance of the posterior.
- **Evidence anchors:**
  - [section 3.2, Eq. 31-33]: The paper explicitly details the approximation $p(y|x_t) \approx p(y|\hat{x}_{0|t})$ used in DPS.
  - [abstract]: Mentions "explicit methods, such as DPS, approximate the likelihood term using posterior mean estimates."
  - [corpus]: Related work (Benchmarking Diffusion Annealing-Based Bayesian Inverse Problem Solvers) highlights the difficulty of integrating likelihoods, validating the need for this approximation.
- **Break condition:** If the posterior is highly multi-modal, a point estimate $\hat{x}_{0|t}$ may fail to capture uncertainty, causing mode collapse or divergence in heavily ill-posed tasks.

### Mechanism 2: Variational Regularization via Score Matching
- **Claim:** Inverse problems can be solved by optimizing a variational distribution $q_\phi(x|y)$ to balance data fidelity and adherence to the generative prior.
- **Mechanism:** Instead of guiding every reverse diffusion step, variational methods (e.g., RED-Diff) minimize the KL divergence to the posterior. This decomposes into a "data consistency" term (matching measurements) and a "regularizer" (matching the score of the variational distribution to the pre-trained diffusion prior score).
- **Core assumption:** The variational family (often Gaussian) can capture the posterior structure, and the score distillation loss acts as a sufficient prior regularizer without needing to backpropagate through the full sampling trajectory.
- **Evidence anchors:**
  - [section 4.1, Eq. 43-45]: Derives the objective as minimizing $D_{KL}$ which splits into data consistency and a score matching term.
  - [abstract]: Categorizes "variational inference" as a distinct class of solvers.
  - [corpus]: Weak signal; corpus focuses mostly on operator learning and plug-and-play, suggesting this specific variational mechanism is a distinct sub-field within the survey.
- **Break condition:** If the variational distribution is too simple (e.g., unimodal Gaussian), it fails on multi-modal posteriors (see Feng et al. in section 4.1), leading to averaged/blurry reconstructions.

### Mechanism 3: Decoupled Data Consistency
- **Claim:** Stability improves when the generative sampling process is decoupled from the aggressive enforcement of data consistency.
- **Mechanism:** Methods like DAPS separate the denoising step from the consistency step. Instead of applying a small gradient at every step, they run the reverse ODE to get a candidate $x_0$, solve a dedicated optimization sub-problem (e.g., Langevin dynamics or proximal steps) to enforce $y = A(x)$ on the clean estimate, and then re-noise to continue the trajectory.
- **Core assumption:** The intermediate estimates $\hat{x}_{0|t}$ lie on a clean manifold where standard optimization (like conjugate gradient) is more effective than manifold-constrained gradients in noisy space.
- **Evidence anchors:**
  - [section 4.2, Eq. 51-52]: Describes DAPS and DCDP where data consistency is enforced by separate Langevin or proximal steps.
  - [section 3.2]: Contextualizes this as a fix for the "diverging samples" seen in coupled methods for challenging problems like phase retrieval.
- **Break condition:** If the re-noising process destroys the hard-fought data consistency of the clean estimate, the algorithm may oscillate or fail to converge, requiring careful tuning of the consistency step magnitude.

## Foundational Learning

- **Concept: Tweedie's Theorem (Bayes Least Squares)**
  - **Why needed here:** This theorem connects the noisy latent variable $x_t$ to the expected clean image $\hat{x}_{0|t}$. It is the mathematical engine for Mechanism 1 (DPS) and Mechanism 3 (DAPS), allowing the solver to evaluate the likelihood of a clean image from a noisy state.
  - **Quick check question:** Can you explain why the posterior mean $\mathbb{E}[x_0|x_t]$ is proportional to the score function $\nabla \log p(x_t)$?

- **Concept: Forward and Reverse SDEs**
  - **Why needed here:** The entire framework relies on the equivalence between sampling (generating images) and solving a differential equation. Understanding the drift ($f$) and diffusion ($g$) coefficients is necessary to modify the trajectory for inverse problems.
  - **Quick check question:** How does the probability-flow ODE (Eq. 6) differ from the reverse SDE (Eq. 5) in terms of stochasticity and sampling speed?

- **Concept: Ill-Posedness and Bayes' Rule**
  - **Why needed here:** The paper defines inverse problems as ill-posed (Eq. 1). Understanding that the goal is not a single "correct" answer but a posterior distribution $p(x|y)$ is critical for choosing between MMSE (average) and posterior sampling (diverse) methods.
  - **Quick check question:** Why does adding the prior $p(x)$ to the likelihood $p(y|x)$ (Eq. 2) constrain the infinite set of feasible solutions?

## Architecture Onboarding

- **Component map:**
  - Prior Network ($s_\theta$ or $D_\theta$) -> Measurement Model ($A$) -> Guidance Controller -> Reverse SDE/ODE

- **Critical path:**
  1. Start with pure noise $x_T$.
  2. **Estimate Clean Signal:** At step $t$, apply Tweedie's formula via $D_\theta(x_t, t)$ to get $\hat{x}_{0|t}$.
  3. **Check Physics:** Compute measurement error $e = y - A(\hat{x}_{0|t})$.
  4. **Guide/Correct:** Compute gradient $\nabla_{x_t}$ or solve sub-problem to push $x_t$ toward a state that satisfies $A(x)=y$.
  5. **Step Reverse SDE/ODE:** Update $x_t$ to $x_{t-1}$.

- **Design tradeoffs:**
  - **DPS Family (Explicit):** Fast and flexible (handles nonlinear $A$ easily). *Tradeoff:* Can be unstable; requires tuning step size $\rho$ (Eq. 33).
  - **DDRM Family:** Very stable for linear $A$ (uses SVD). *Tradeoff:* Cannot easily handle nonlinear forward models.
  - **SMC (Particle Filters):** High fidelity and uncertainty quantification. *Tradeoff:* Computationally expensive (requires running $N$ parallel trajectories).

- **Failure signatures:**
  - **"Oversmoothing/Color Shifting":** Often seen in blind deconvolution or when the variational prior over-regularizes.
  - **"Divergence/Artifacts":** Occurs in DPS when the guidance gradient is too large, pushing $x_t$ off the image manifold.
  - **"Hallucination":** In text-driven or severely ill-posed problems, the prior may dominate, generating content consistent with the prior but not strictly with the measurement.

- **First 3 experiments:**
  1. **Linear Super-Resolution (SR):** Implement DPS on a pre-trained CelebA model. Verify that $\hat{x}_{0|t}$ improves over 1000 steps. This tests basic gradient guidance.
  2. **Non-linear Phase Retrieval:** Test DAPS (Decoupled). Compare the stability of decoupled Langevin steps vs. DPS gradients. This tests the robustness of the "decoupled" mechanism.
  3. **Blind Deblurring:** Implement BlindDPS (Section 5.1). Train a second diffusion model for the kernel $k$ and run the coupled ODEs (Eq. 66-67). This tests the ability to jointly optimize two latent variables.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inherent trade-off between computational efficiency and reconstruction fidelity be systematically reconciled in diffusion-based inverse problem solvers?
- Basis in paper: [explicit] Section 7 states, "Future work will likely focus on reconciling the trade-offs between speed and accuracy, pushing the boundaries of what is achievable in unsupervised inverse problem-solving."
- Why unresolved: Current methods force a choice between fast, deterministic ODE solvers (lower fidelity) and slow, stochastic sampling or particle filters (higher fidelity).
- What evidence would resolve it: A unified solver that achieves state-of-the-art reconstruction metrics (PSNR/LPIPS) using significantly fewer Neural Function Evaluations (NFEs) than standard DPS or SMC methods.

### Open Question 2
- Question: Can computationally efficient guidance gradients be computed for high-dimensional inverse problems without relying on restrictive data manifold assumptions?
- Basis in paper: [inferred] Section 3.2 notes that while DDS circumvents the expensive Jacobian computation of moment matching, it relies on "certain conditions on the data manifold" (Proposition 1), limiting its general applicability.
- Why unresolved: There is currently a dichotomy between methods requiring expensive Jacobian calculations (accurate but slow) and those using manifold approximations (fast but theoretically limited).
- What evidence would resolve it: An algorithm that computes the exact posterior gradient (or a provably tight bound) with the same time complexity as a backpropagation pass, independent of manifold geometry.

### Open Question 3
- Question: How can diffusion models be effectively trained to solve inverse problems when the training data consists solely of corrupted measurements or out-of-distribution samples?
- Basis in paper: [inferred] Section 5.3 highlights that in domains like black-hole imaging, "one only has access to the partial measurements," and Section 5.4 discusses the complexity of "Ambient Diffusion" for handling arbitrary "bad-quality data."
- Why unresolved: Standard diffusion training requires clean data; current alternatives like GSURE or Ambient Diffusion rely on specific noise models or correlation structures that may not hold in all data-scarce contexts.
- What evidence would resolve it: A training framework that achieves equivalent downstream reconstruction performance to a clean-data model while training exclusively on measurements with unknown, mixed, or non-Gaussian degradations.

## Limitations
- The Tweedie approximation in DPS may fail for highly ill-posed or multi-modal posterior distributions, leading to mode collapse
- Variational methods with simple distributions can produce blurry reconstructions when posteriors are multi-modal
- Computational overhead of sequential Monte Carlo methods limits their practical applicability despite high fidelity

## Confidence
- **Medium**: Core mechanisms (DPS, variational, decoupled) are well-established with reasonable empirical support
- **Low**: Blind and high-dimensional extensions rely on additional assumptions with limited empirical validation
- **Medium**: Text-driven solutions show promise but effectiveness depends heavily on quality of auxiliary information

## Next Checks
1. **Stability Test:** Reproduce DPS on phase retrieval or compressed sensing with increasing undersampling ratios to identify the breaking point of the Tweedie approximation.
2. **Efficiency Benchmark:** Compare the wall-clock time and PSNR of variational (DDRM) vs. explicit (DPS) methods on a linear inverse problem like MRI reconstruction.
3. **Uncertainty Quantification:** Implement sequential Monte Carlo (SMC) on a multi-modal inverse problem (e.g., inpainting with multiple valid completions) to assess whether particle diversity matches posterior uncertainty.