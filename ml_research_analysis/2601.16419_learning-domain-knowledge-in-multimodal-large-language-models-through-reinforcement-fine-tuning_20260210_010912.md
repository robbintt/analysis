---
ver: rpa2
title: Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement
  Fine-Tuning
arxiv_id: '2601.16419'
source_url: https://arxiv.org/abs/2601.16419
tags:
- domain
- knowledge
- learning
- shot
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reinforcement fine-tuning framework for multimodal
  large language models (MLLMs) to internalize domain knowledge at the optimization
  level rather than through prompts or captions. The method introduces domain-aware
  constraints and reward shaping to guide the model toward domain-consistent behaviors
  like rotation invariance and symmetry consistency.
---

# Learning Domain Knowledge in Multimodal Large Language Models through Reinforcement Fine-Tuning

## Quick Facts
- arXiv ID: 2601.16419
- Source URL: https://arxiv.org/abs/2601.16419
- Reference count: 15
- This paper proposes reinforcement fine-tuning to internalize domain knowledge at the optimization level rather than through prompts or captions.

## Executive Summary
This paper addresses the challenge of adapting multimodal large language models (MLLMs) to specialized domains like remote sensing and medical imaging. The authors demonstrate that traditional input-level domain knowledge injection (prompts, captions) fails to provide meaningful improvements, even when domain knowledge is explicitly provided. Instead, they propose a reinforcement fine-tuning framework that incorporates domain knowledge as distributional constraints and reward shaping during optimization. The method achieves consistent improvements across few-shot settings, with up to 12.8% gains over supervised fine-tuning in remote sensing tasks.

## Method Summary
The method builds on a base MLLM (Qwen2.5-VL) and uses Group Relative Policy Optimization (GRPO) as the underlying RL framework. Domain knowledge is encoded through two components: (1) Domain-aware constraints that minimize KL divergence between the policy distribution and a domain-support distribution created by applying domain-specific transformations to inputs, and (2) Domain-aware reward shaping that reweights advantages using JS divergence to emphasize domain-consistent samples. The approach is applied to remote sensing (rotation invariance) and medical imaging (symmetry consistency) domains across few-shot settings (1-8 shots).

## Key Results
- RFT achieves up to 12.8% improvement over supervised fine-tuning in remote sensing tasks
- Input-level domain knowledge injection (prompts, captions) yields negligible improvements despite explicit knowledge provision
- Distribution-level constraints outperform output-level constraints by substantial margins
- Both domain-aware constraints and reward shaping components contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimization-level domain knowledge integration outperforms input-level injection for specialized scientific domains
- Mechanism: Domain knowledge is encoded as distributional constraints and reward signals during policy optimization, not as textual conditioning
- Core assumption: MLLMs cannot internalize abstract domain principles from language descriptions alone
- Evidence anchors:
  - [abstract] Input-level domain knowledge injection yields little to no improvement
  - [section 1, Table 1] Domain prompts and caption-based augmentation show negligible gains
  - [corpus] Related work confirms general MLLMs lack specialized domain knowledge

### Mechanism 2
- Claim: Domain-aware constraints enforce invariance by minimizing KL divergence between policy and domain-support distributions
- Mechanism: Apply domain-specific transformations to create πDθ, minimize DKL(πDθ || πθ)
- Core assumption: Domain transformation correctly captures the invariance property
- Evidence anchors:
  - [section 3.2] KL divergence minimization between original and domain-support distributions
  - [section 4.4, Table 7] Distribution-level constraints substantially outperform output-level constraints

### Mechanism 3
- Claim: Domain-aware reward shaping reweights advantages using JS divergence to emphasize domain-relevant samples
- Mechanism: Compute Di = DJS(πDθ || πθ), reweight advantages: Adi = (1 - Di) · Ai
- Core assumption: JS divergence boundedness provides stable reward optimization
- Evidence anchors:
  - [section 3.3] Smaller divergence samples receive larger advantages
  - [section 4.4, Table 5] Both DC and DR individually improve performance
  - [section 4.4, Table 8] KL for DC + JS for DR outperforms same divergence for both

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Base RL framework used. Computes relative advantages within sampled groups without value function estimation
  - Quick check question: Can you explain how GRPO normalizes rewards within a group to compute advantages?

- Concept: **KL vs JS Divergence Properties**
  - Why needed here: KL is asymmetric and unbounded (used for constraints); JS is symmetric and bounded [0,1] (used for advantage reweighting)
  - Quick check question: Why would boundedness matter more for reward shaping than for constraint enforcement?

- Concept: **Domain-Specific Invariances**
  - Why needed here: Framework requires explicit domain transformations. Remote sensing uses rotation invariance; medical imaging uses symmetry consistency
  - Quick check question: For your target domain, what geometric or semantic invariances should hold across inputs?

## Architecture Onboarding

- Component map: Input (image + prompt) → Base MLLM → Policy distribution πθ → Domain transform → Transformed input → Domain-support distribution πDθ → Constraint Loss and Reward Shaping → Final Objective

- Critical path:
  1. Implement domain-specific transformations correctly (rotation for RS, symmetry for medical)
  2. Compute both πθ and πDθ from same batch
  3. Calculate KL and JS divergences
  4. Integrate into modified GRPO objective

- Design tradeoffs:
  - KL vs JS for different components is empirically validated—do not swap without re-evaluation
  - More epochs (4) for higher-shot settings vs fewer (2) for low-shot—balances overfitting risk
  - DC more effective in extremely low-shot; DR more effective in higher-shot regimes

- Failure signatures:
  - Performance degrades vs baseline: Check if domain transformations are appropriate for task
  - Caption-augmentation helps but RFT doesn't: Domain knowledge may be better expressed textually
  - Training instability: Verify JS divergence computation produces bounded values [0,1]

- First 3 experiments:
  1. Replicate Table 1 comparison: prompt-based, caption-based, vs your implementation on one dataset
  2. Ablate DC and DR individually to confirm both contribute
  3. Test divergence choice if adapting to new domain with different invariance properties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework incorporate domain knowledge that cannot be expressed as input-space transformations (e.g., physics-based constraints, causal relationships, or abstract procedural knowledge)?
- Basis in paper: [explicit] The authors state the framework is "general and can incorporate arbitrary domain priors that can be expressed via transformations or distributional constraints," but only instantiate it with rotation invariance and symmetry consistency
- Why unresolved: The paper does not demonstrate whether more abstract domain knowledge can be effectively encoded through the proposed distribution-level constraints
- What evidence would resolve it: Experiments applying the framework to domains requiring non-geometric priors, showing whether transformation-based distribution constraints generalize

### Open Question 2
- Question: Why do current MLLMs fail to internalize domain knowledge through textual conditioning, and can this limitation be addressed through architectural or training modifications?
- Basis in paper: [explicit] The authors observe that "such input-level domain knowledge injection yields little to no improvement" and state this "reveal[s] a fundamental limitation of textual domain conditioning in current MLLMs"
- Why unresolved: The paper identifies the failure mode but does not analyze whether the issue stems from attention mechanisms, pre-training objectives, or capacity limitations
- What evidence would resolve it: Probing studies analyzing attention patterns during textual domain conditioning, or architectural interventions that successfully enable textual domain knowledge transfer

### Open Question 3
- Question: Can appropriate domain-specific transformations be automatically discovered or learned from data rather than manually specified?
- Basis in paper: [inferred] The method requires practitioners to manually identify and encode domain knowledge as transformations, which assumes prior domain expertise
- Why unresolved: The paper does not explore whether the constraint transformations themselves can be derived from data analysis or meta-learning
- What evidence would resolve it: A method that automatically identifies domain-relevant invariances from unlabeled domain data, validated by comparing auto-discovered constraints against manually specified ones

## Limitations
- Heavy reliance on domain-specific invariances without empirical validation that these transformations are optimal for all sub-tasks
- Visual-RFT reward functions are referenced but not specified in the paper, creating reproducibility dependencies
- Assumes geometric transformations capture relevant domain knowledge without testing alternative invariance specifications

## Confidence
- **High confidence**: The core claim that optimization-level domain knowledge injection outperforms prompt-based approaches - strongly supported by direct experimental comparisons
- **Medium confidence**: The superiority of distribution-level constraints over output-level constraints - well-supported within this paper but lacks external validation
- **Medium confidence**: The choice of KL divergence for constraints and JS divergence for reward shaping - validated within the ablation study but without broader literature support

## Next Checks
1. **Domain transformation appropriateness test**: Systematically vary the geometric transformations (e.g., test different rotation angles, add scaling/shearing transforms) to verify that the chosen invariances are optimal rather than arbitrary choices
2. **Cross-domain applicability validation**: Apply the same RFT framework to a third domain (e.g., satellite imagery for disaster response or histopathology for cancer detection) with different expected invariances to test whether the method generalizes beyond the two domains studied
3. **Baselines comparison completeness**: Replicate the prompt-based and caption-based experiments with more sophisticated prompting strategies (chain-of-thought, few-shot prompting) to ensure the baseline comparison is exhaustive before concluding that input-level injection is ineffective