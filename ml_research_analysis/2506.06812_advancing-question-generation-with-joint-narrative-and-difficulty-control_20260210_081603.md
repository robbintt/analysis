---
ver: rpa2
title: Advancing Question Generation with Joint Narrative and Difficulty Control
arxiv_id: '2506.06812'
source_url: https://arxiv.org/abs/2506.06812
tags:
- difficulty
- narrative
- control
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating questions that
  are both narrative-aware and difficulty-controllable, which is crucial for personalized
  education. The authors propose a joint control strategy that combines narrative
  attributes (like character, action, causal relationships) with difficulty levels
  estimated using Item Response Theory.
---

# Advancing Question Generation with Joint Narrative and Difficulty Control

## Quick Facts
- arXiv ID: 2506.06812
- Source URL: https://arxiv.org/abs/2506.06812
- Reference count: 18
- Proposes joint control of narrative attributes and difficulty for personalized question generation

## Executive Summary
This paper tackles the challenge of generating questions that are both narrative-aware and difficulty-controllable, addressing a critical need in personalized education. The authors propose a novel joint control strategy that combines narrative attributes (character, action, causal relationships) with difficulty levels estimated using Item Response Theory. By fine-tuning a Flan-T5 model on augmented FairyTaleQA data, they enable generation of question-answer pairs conditioned on both narrative and difficulty attributes, achieving improvements in both narrative similarity and difficulty-appropriate performance.

## Method Summary
The authors develop a joint control strategy for question generation by combining narrative attributes with difficulty estimation. They augment FairyTaleQA data with IRT-based difficulty labels and fine-tune a Flan-T5 model to generate question-answer pairs conditioned on both narrative and difficulty attributes. The narrative control incorporates elements like character, action, and causal relationships, while difficulty is estimated using Item Response Theory. The model generates questions by integrating both controls simultaneously, allowing for personalized question generation that maintains narrative coherence while adjusting difficulty levels.

## Key Results
- Incorporating narrative control improves similarity between generated and human-authored questions
- Difficulty control produces clear performance gradients: simulated learners perform better on easier questions and worse on harder ones
- Joint control shows less consistent performance at intermediate difficulty levels, with some narrative elements (prediction, feeling) being harder to control than others (causal, outcome)

## Why This Works (Mechanism)
The approach works by integrating narrative awareness directly into the question generation process through fine-tuning on data augmented with both narrative attributes and difficulty labels. The Flan-T5 model learns to generate questions that not only reflect the story's content but also match the specified difficulty level through the conditioning mechanism. The IRT-based difficulty estimation provides a theoretically grounded method for quantifying question difficulty, while the joint control architecture allows the model to balance narrative fidelity with difficulty calibration simultaneously.

## Foundational Learning

**Item Response Theory (IRT)**: A psychometric framework for estimating item difficulty and learner ability. Needed to provide theoretically sound difficulty labels for training data. Quick check: Verify IRT parameters (difficulty, discrimination) align with human difficulty judgments.

**Narrative Attributes**: Story elements like character, action, causal relationships, and outcomes that structure narrative understanding. Needed to ensure generated questions maintain story coherence and relevance. Quick check: Confirm attribute extraction captures key narrative elements without oversimplification.

**Fine-tuning with Conditional Generation**: Training approach where model learns to generate outputs conditioned on specific attributes. Needed to enable simultaneous control over multiple dimensions (narrative + difficulty). Quick check: Validate that conditioning tokens effectively steer generation toward target attributes.

## Architecture Onboarding

**Component Map**: FairyTaleQA Data -> IRT Difficulty Estimation -> Flan-T5 Fine-tuning -> Conditional Question Generation

**Critical Path**: The model receives narrative context and difficulty instruction as inputs, processes through the fine-tuned Flan-T5 architecture, and generates narrative-aware, difficulty-appropriate question-answer pairs. The critical path involves encoding both the narrative content and control instructions, then decoding the generated output.

**Design Tradeoffs**: The approach trades model complexity for controllability, using fine-tuning rather than prompting to achieve better control. This increases training time and data requirements but provides more reliable control over both narrative and difficulty dimensions compared to prompting-based approaches.

**Failure Signatures**: Hallucinations where generated questions don't match story content, nonsensical QA pairs, and inconsistent difficulty control at intermediate levels. Some narrative elements like prediction and feeling are harder to control than others like causal relationships and outcomes.

**First Experiments**:
1. Generate questions with only narrative control enabled to verify narrative coherence
2. Generate questions with only difficulty control to validate difficulty calibration
3. Compare joint control outputs against single-attribute controls to measure synergistic effects

## Open Questions the Paper Calls Out
The study identifies uncertainties around the generalizability of the joint control approach across different narrative domains and difficulty estimation methods. It remains unclear whether similar performance would be achieved with other content types (scientific texts, historical narratives) or alternative difficulty estimation approaches beyond IRT.

## Limitations
- Observed inconsistencies in intermediate difficulty levels suggest potential instability in joint control mechanism
- Certain narrative elements (prediction, feeling) are harder to control than others (causal, outcome)
- Generalizability across different narrative domains and content types remains unproven

## Confidence

**High**: Narrative control effectiveness (improved similarity metrics), difficulty control performance gradient (easy vs hard questions)
**Medium**: Joint control effectiveness at intermediate difficulty levels (inconsistent results, challenges with specific narrative elements)

## Next Checks

1. Test the joint control approach on diverse narrative domains beyond fairy tales to assess generalizability and identify domain-specific challenges
2. Compare IRT-based difficulty estimation with alternative methods (expert ratings, learner performance data) to evaluate robustness of difficulty control mechanism
3. Conduct ablation studies isolating narrative elements to better understand their individual and combined effects on difficulty control performance, particularly at intermediate levels