---
ver: rpa2
title: 'VIDEOP2R: Video Understanding from Perception to Reasoning'
arxiv_id: '2511.11113'
source_url: https://arxiv.org/abs/2511.11113
tags:
- reasoning
- video
- answer
- perception
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VIDEOP2R, a process-aware video reasoning
  framework that models perception and reasoning as distinct processes to improve
  video understanding. The key innovation is a two-stage reinforcement fine-tuning
  approach: an SFT stage that generates a process-aware CoT dataset (VIDEOP2R-CoT-162K)
  with separate perception and reasoning annotations, followed by an RL stage using
  a novel process-aware GRPO (PA-GRPO) that assigns separate rewards for perception
  and reasoning.'
---

# VIDEOP2R: Video Understanding from Perception to Reasoning

## Quick Facts
- arXiv ID: 2511.11113
- Source URL: https://arxiv.org/abs/2511.11113
- Reference count: 40
- Outperforms previous RFT methods by 1.3% on average accuracy across 7 video reasoning benchmarks

## Executive Summary
This paper introduces VIDEOP2R, a process-aware video reasoning framework that models perception and reasoning as distinct processes to improve video understanding. The key innovation is a two-stage reinforcement fine-tuning approach: an SFT stage that generates a process-aware CoT dataset (VIDEOP2R-CoT-162K) with separate perception and reasoning annotations, followed by an RL stage using a novel process-aware GRPO (PA-GRPO) that assigns separate rewards for perception and reasoning. This design addresses the challenge of credit assignment in video reasoning where perception errors can propagate to reasoning errors. VIDEOP2R achieves state-of-the-art performance on six out of seven video reasoning benchmarks, outperforming previous RFT methods by 1.3% on average accuracy.

## Method Summary
VIDEOP2R employs a two-stage reinforcement fine-tuning pipeline. First, it generates VIDEOP2R-CoT-162K through a three-step process: CoT generation, verification, and observation sufficiency filtering using Claude 3.7. The model is then fine-tuned for one epoch on this dataset using SFT. In the second stage, PA-GRPO updates the model on 260K QA samples with separate perception and reasoning rewards. Perception rewards are assigned by a Claude 3.7 LLM judge evaluating observation sufficiency, while reasoning rewards use task-specific accuracy metrics. The PA-GRPO algorithm normalizes each process's advantages separately to prevent scale mismatches between perception and reasoning rewards.

## Key Results
- Achieves state-of-the-art performance on six out of seven video reasoning benchmarks
- Outperforms previous RFT methods by 1.3% on average accuracy
- Demonstrates consistent gains of 1.9%-9.1% over the base model across benchmarks
- Shows think-answer mismatch rates below 10%, significantly lower than Video-R1 and VideoRFT
- Perception outputs are information-sufficient for downstream reasoning, even surpassing raw video input performance when augmenting text-only questions

## Why This Works (Mechanism)

### Mechanism 1
Separating perception and reasoning into distinct token segments with independent rewards improves credit assignment during RL. PA-GRPO assigns perception rewards only to `<observation>` tokens and reasoning rewards only to `<think>+<answer>` tokens, then normalizes each process's advantages separately. This prevents a single scalar reward from penalizing correct perception when reasoning fails (or vice versa). The core assumption is that perception and reasoning errors are partially decouplable—correct perception can occur even when downstream reasoning is wrong.

### Mechanism 2
A process-aware CoT dataset with verified observation sufficiency provides stronger SFT initialization for RL. The three-step pipeline generates perception traces, verifies answer correctness, then uses a text-only LLM (Claude 3.7) to confirm that `<observation>` segments contain sufficient evidence to answer—filtering out incomplete perceptions before training. The core assumption is that text-only verification of observation sufficiency is a reliable proxy for whether perception captures task-relevant visual evidence.

### Mechanism 3
Process-aware rewards reduce "advantage collapse" and mitigate think-answer mismatch. By maintaining two reward streams, PA-GRPO preserves non-zero gradients even when one process saturates. Separate perception supervision also penalizes reasoning traces that diverge from stated answers (mismatch), which single-reward GRPO cannot detect. The core assumption is that advantage collapse is a material bottleneck in video RFT, and mismatched reasoning traces are reinforced by single-reward setups.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: PA-GRPO extends GRPO; understanding baseline advantage normalization within sampled groups is prerequisite
  - Quick check question: Given 4 sampled responses with rewards [1.0, 0.8, 0.6, 0.4], what is the advantage of the first response after normalization?

- **Concept: Credit assignment in RL**
  - Why needed here: The core thesis is that single-reward assignment conflates perception and reasoning errors; understanding how rewards propagate to token-level updates clarifies why separation helps
  - Quick check question: In standard GRPO, if a model produces correct perception but wrong reasoning, how does the single scalar reward affect the perception tokens?

- **Concept: Chain-of-Thought (CoT) supervision**
  - Why needed here: VIDEOP2R-CoT-162K provides structured perception-reasoning traces; understanding CoT warmup explains why SFT stage is necessary before RL
  - Quick check question: What is the risk of starting RL without a CoT-warmed model that follows the process-aware template?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-7B-Instruct -> VIDEOP2R-CoT-162K (SFT stage) -> VIDEOP2R-SFT -> PA-GRPO updates -> VIDEOP2R final

- **Critical path**: 
  1. Generate or obtain VIDEOP2R-CoT-162K via three-step pipeline (generation → verification → sufficiency filtering)
  2. Run SFT (1 epoch) to enforce template adherence
  3. Run PA-GRPO (1K updates) with separate reward normalization and assignment

- **Design tradeoffs**:
  - LLM judge choice: Claude 3.7 achieves 95% agreement with human labels on perception correctness; smaller judges (Llama3.1-8B, 82%) still work but yield smaller gains
  - Length rewards: Fixed length targets ([128,320] for perception, [320,512] for reasoning) help conciseness but hurt tasks requiring long descriptions (e.g., VSI-Bench spatial reasoning)
  - Frame sampling: Uniform 16 frames during training; 32 frames at inference. Key-frame missing is a documented failure mode

- **Failure signatures**:
  - Key-frame missing: Model produces correct reasoning from incomplete observations; uniform sampling omits critical frames
  - Overly detailed configurations: Spatial reasoning tasks exceed trained length regimes, causing perception compression and dropped details
  - Domain knowledge gaps: Correct perception but wrong reasoning when external knowledge (e.g., molar volume of gas) is missing

- **First 3 experiments**:
  1. Validate template adherence: After SFT, sample 100 outputs and verify all contain properly formatted `<observation>`, `<think>`, `<answer>` segments with regex matching
  2. Ablate perception reward: Run PA-GRPO with perception reward set to zero; compare against full PA-GRPO to isolate contribution of perception supervision (expected: ~2.3% drop)
  3. Check advantage collapse: Log the fraction of batches where all reasoning rewards are identical; compare GRPO vs PA-GRPO over training to verify reduced collapse

## Open Questions the Paper Calls Out

### Open Question 1
Can a dynamic length reward mechanism improve performance on tasks requiring fine-grained spatial descriptions (e.g., VSI-Bench)? The fixed length constraints ([128,320] for perception, [320,512] for reasoning) penalize necessary detailed descriptions in spatial reasoning tasks, creating a trade-off between conciseness and completeness. A dynamic length reward that adapts to task complexity could improve spatial reasoning accuracy without sacrificing performance on other benchmarks.

### Open Question 2
Would adaptive frame sampling strategies effectively mitigate key-frame missing failures without excessive computational overhead? Uniform frame sampling (16 frames during training) may omit critical visual evidence, but simply increasing frames is computationally expensive. Adaptive sampling could reduce perception errors on temporal reasoning benchmarks with comparable inference costs.

### Open Question 3
How can domain-specific knowledge be effectively integrated into the VIDEOP2R framework without compromising its process-aware architecture? The current training data lacks chemistry and other specialized domains, causing reasoning failures even when perception is correct. Integration of domain-specific knowledge bases or retrieval mechanisms could improve performance on MMVU and chemistry-related VQA tasks.

## Limitations

- Fixed length rewards can be counterproductive for tasks requiring long, fine-grained descriptions (e.g., VSI-Bench spatial reasoning)
- Key-frame missing failures occur when uniform sampling omits critical visual evidence
- Domain knowledge gaps limit performance on specialized reasoning tasks (e.g., chemistry questions about molar volume)

## Confidence

- **High**: Performance improvements on the seven benchmarks, particularly the consistent gains across diverse task types
- **Medium**: The mechanism of PA-GRPO advantage normalization and its role in reducing credit assignment ambiguity
- **Low**: The robustness of observation sufficiency filtering to variations in judge model and prompt variations

## Next Checks

1. Run VIDEOP2R on an additional video reasoning dataset not used in the paper (e.g., Video-MME variants) to test generalization beyond the seven reported tasks
2. Replace Claude 3.7 with Llama3.1-8B for the full training pipeline and measure performance delta; randomly sample 100 sufficiency judgments for human verification
3. Log and compare the variance of perception and reasoning advantages per batch during PA-GRPO training; verify that normalization yields higher minimum advantage values compared to standard GRPO