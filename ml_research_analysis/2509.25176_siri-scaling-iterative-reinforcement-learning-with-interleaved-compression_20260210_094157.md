---
ver: rpa2
title: 'SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression'
arxiv_id: '2509.25176'
source_url: https://arxiv.org/abs/2509.25176
tags:
- length
- training
- arxiv
- compression
- siri
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of repetitive thinking patterns
  in Large Reasoning Models (LRMs), which leads to increased token usage without proportional
  gains in performance. The proposed method, SIRI, introduces an iterative reinforcement
  learning framework that alternates between compressing and expanding the reasoning
  budget by dynamically adjusting the maximum rollout length during training.
---

# SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression

## Quick Facts
- arXiv ID: 2509.25176
- Source URL: https://arxiv.org/abs/2509.25176
- Reference count: 26
- One-line primary result: SIRI achieves 43.2% accuracy improvement on AIME24 while reducing token usage by 46.9% through interleaved compression-expansion training cycles

## Executive Summary
This paper addresses the problem of repetitive thinking patterns in Large Reasoning Models (LRMs), which leads to increased token usage without proportional gains in performance. The proposed method, SIRI, introduces an iterative reinforcement learning framework that alternates between compressing and expanding the reasoning budget by dynamically adjusting the maximum rollout length during training. The compression phase forces the model to produce more concise reasoning, while the expansion phase allows for exploration in long-horizon settings. The key insight is that this cyclical approach improves performance even as output length decreases, pushing the model toward a better efficiency-performance trade-off.

Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that SIRI-low improves AIME24 performance by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods. The method demonstrates robustness across different model sizes and consistently pushes the Pareto frontier forward. The compression-expansion cycle particularly influences the model's backtracking and verification behavior, with "wait" tokens being suppressed during compression and encouraged during expansion.

## Method Summary
SIRI implements an iterative reinforcement learning framework with interleaved compression and expansion phases using a cosine length scheduler that cycles between maximum (L_max=16384) and minimum (L_min=8192) rollout lengths over 640-step cycles. The method uses GRPO with DAPO modifications for policy updates, applying length-capping rewards where responses are truncated to the current maximum length and correct answers are extracted from the clipped output. The training alternates between three full compression-expansion cycles, with the compression phase forcing the model to produce more concise reasoning by penalizing longer outputs, while the expansion phase allows exploration of longer reasoning traces. This cyclical approach aims to push the model toward the Pareto frontier by improving accuracy while reducing token usage.

## Key Results
- SIRI-low achieves 43.2% accuracy improvement on AIME24 while reducing token usage by 46.9% after three iterations on DeepSeek-R1-Distill-Qwen-1.5B
- SIRI-high achieves the highest accuracy among all methods tested, demonstrating the effectiveness of the compression-expansion cycle
- The method consistently pushes the Pareto frontier forward, achieving better efficiency-performance trade-offs compared to baseline approaches
- Token efficiency gains generalize across different model sizes, with both 1.5B and 7B models showing similar improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Alternating compression and expansion phases drives models toward the Pareto frontier by enabling accuracy gains while reducing output length.
- **Mechanism:** Compression phases force the model to condense reasoning into fewer tokens, pruning redundant patterns. Expansion phases then provide capacity for exploration and planning based on the condensed traces. The cycle repeats, with each iteration improving both accuracy and efficiency.
- **Core assumption:** The model can preserve or recover performance after compression if given sufficient steps before the next expansion.
- **Evidence anchors:**
  - [abstract] "after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier"
  - [Section 4.1] "we hypothesize that interleaving compression with expansion can yield performance gains while maintaining comparable response lengths across expansion stages"
  - [corpus] Related work on iterative length capping (Hou et al., 2025 in references) aligns with staged compression preserving performance better than direct capping.
- **Break condition:** If compression phase is too aggressive (short cycle length), performance drops sharply and does not recover (Figure 7a shows 320-cycle scheduler degrades accuracy).

### Mechanism 2
- **Claim:** Cosine scheduling of maximum rollout length produces smoother behavioral adaptation than abrupt stair-step changes.
- **Mechanism:** The cosine scheduler gradually reduces and increases the token budget, giving the model time to adapt its generation patterns. This prevents the sharp performance drops seen with sudden transitions.
- **Core assumption:** Smoother transitions allow the model to redistribute reasoning effort without catastrophic forgetting of useful patterns.
- **Evidence anchors:**
  - [Section 4.3] Cosine scheduler formula provided; "the smoothness of the cosine scheduler" allows model output length to lag behind scheduler by 100-200 steps
  - [Section 5.3] "cosine scheduler mitigates performance loss during compression, while the stair scheduler maximizes performance gain during expansion"
  - [corpus] No direct external validation of cosine vs. stair schedules for LRM training found in neighbor papers.
- **Break condition:** Very short cycles (e.g., 320 steps) do not provide enough adaptation time, causing unstable training.

### Mechanism 3
- **Claim:** Compression primarily suppresses backtracking and verification behaviors while preserving deductive reasoning.
- **Mechanism:** Under length constraints, the model selectively reduces tokens associated with self-correction ("wait", "hold on") while maintaining core deduction tokens ("so", "compute"). This increases reasoning density without sacrificing correctness.
- **Core assumption:** Backtracking tokens contribute less to final accuracy than deductive tokens, so pruning them is efficiency-positive.
- **Evidence anchors:**
  - [Section 5.2] "the frequency of 'wait' tokens that stand for backtracking and verification changes significantly during training, while others remain stable"
  - [Section 5.2] Analysis shows compression suppresses "wait" tokens; expansion re-enables them
  - [corpus] REA-RL paper addresses reflection-aware training, suggesting reflection tokens are a known target for efficiency optimization.
- **Break condition:** If tasks require extensive self-correction (e.g., complex multi-step code generation), aggressive compression may harm accuracy.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** SIRI builds on GRPO for policy updates; understanding advantage estimation from group-level rewards is essential.
  - **Quick check question:** Can you explain how GRPO computes advantages differently from standard PPO?

- **Concept: Test-Time Scaling / Chain-of-Thought Budgeting**
  - **Why needed here:** SIRI manipulates the output length budget during training to control exploration vs. efficiency.
  - **Quick check question:** What happens to accuracy if you cap output length at inference time without training under that constraint?

- **Concept: Pareto Frontier in Performance-Efficiency Trade-offs**
  - **Why needed here:** SIRI's goal is to push the model toward the Pareto frontier (higher accuracy, lower tokens).
  - **Quick check question:** If a model achieves higher accuracy but uses more tokens, is it closer to or further from the Pareto frontier compared to a lower-accuracy, lower-token baseline?

## Architecture Onboarding

- **Component map:** Length scheduler (cosine/stair/stair-cosine) -> GRPO/DAPO policy update -> reward function with length capping -> iterative training loop with multiple compression-expansion cycles

- **Critical path:** Scheduler sets max length -> model generates rollouts -> responses truncated per current L -> reward computed -> GRPO update -> repeat

- **Design tradeoffs:**
  - Longer cycle (640 steps) = smoother adaptation, better final performance, but longer training
  - Cosine vs. stair: cosine preserves performance during compression; stair gives stronger exploration during expansion
  - L_min set too low -> performance cliff; set too high -> insufficient compression

- **Failure signatures:**
  - Sharp accuracy drop mid-training -> cycle length too short or L_min too aggressive
  - No length reduction after compression phase -> reward signal not differentiating short vs. long correct responses
  - Entropy collapse -> expansion phase insufficient or clip thresholds too tight

- **First 3 experiments:**
  1. Replicate 1.5B model training with 640-cycle cosine scheduler on provided dataset; verify accuracy and length match reported figures (40.4% / 7093 tokens for SIRI-low).
  2. Ablate scheduler type (cosine vs. stair vs. stair-cosine) with 480-cycle length; compare compression stability and expansion gains.
  3. Test generalization: train on math dataset, evaluate on held-out AIME25 and AMC; check if token efficiency gains transfer to harder problems.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the SIRI framework be effectively adapted for reasoning-intensive tasks beyond mathematics, such as code generation?
- **Basis in paper:** [explicit] The conclusion explicitly states, "how SIRI can be applied in other tasks that require intensive reasoning, such as code generation, is also a promising direction."
- **Why unresolved:** The current experiments are restricted to mathematical benchmarks (AIME, MATH500, AMC). Code generation involves different constraints (e.g., syntax correctness, executability) where excessive compression might break logic more severely than in natural language reasoning.
- **What evidence would resolve it:** Successful application of the interleaved compression-expansion schedule to code generation benchmarks (e.g., HumanEval, MBPP) showing improved pass@1 rates with reduced token usage.

### Open Question 2
- **Question:** What are the theoretical or empirical limits defining the upper performance threshold of SIRI?
- **Basis in paper:** [explicit] The conclusion notes, "the upper performance threshold remains to be discovered and understood (e.g., limited by dataset size, algorithm efficiency, etc)."
- **Why unresolved:** While the paper demonstrates iterative improvement over three cycles, it does not determine if the "Pareto frontier" eventually saturates or if performance gains would continue with further iterations.
- **What evidence would resolve it:** A scaling study varying the number of iterations beyond three and analyzing the asymptotic behavior of accuracy and token efficiency.

### Open Question 3
- **Question:** Does the optimal cycle length for the scheduler scale with model size or dataset complexity?
- **Basis in paper:** [inferred] The ablation study tests cycle lengths (320, 480, 640) on the 1.5B model, finding 640 optimal, and applies this to the 7B model. It leaves unresolved whether larger models require longer cycles to fully utilize the expansion phase for exploration.
- **Why unresolved:** The paper uses a fixed cycle length (640 steps) for both 1.5B and 7B models without investigating if the scheduler dynamics should change relative to model capacity.
- **What evidence would resolve it:** Experiments on larger models (e.g., 32B, 70B) comparing fixed cycle lengths versus cycle lengths scaled proportionally to parameter count or training compute.

### Open Question 4
- **Question:** Can an explicit entropy bonus during the expansion phase further enhance SIRI's exploration and final performance?
- **Basis in paper:** [inferred] In Appendix A.2, the authors observe a correlation between rising entropy and performance gains, suggesting "the possibility of using entropy bonus or clipping even higher during the expansion stage."
- **Why unresolved:** The current implementation relies on the scheduler to implicitly manage exploration. An explicit entropy mechanism is proposed but not tested.
- **What evidence would resolve it:** Ablation studies comparing standard SIRI against variants adding an entropy bonus term to the loss function during the expansion stages.

## Limitations

- The effectiveness of SIRI depends heavily on precise tuning of length scheduler parameters (cycle length, L_min threshold), with unclear generalization to different model scales or task domains
- The claim that compression-expansion cycles consistently push models toward the Pareto frontier could be dataset-specific, as evaluation is limited to mathematical reasoning tasks from curated datasets
- Token frequency analysis used to support mechanism claims may not fully capture whether compressed reasoning traces are genuinely equivalent in logical completeness to longer traces

## Confidence

- **High Confidence**: The experimental results showing SIRI-low and SIRI-high achieving better accuracy-token efficiency trade-offs compared to baselines on the specified mathematical reasoning datasets
- **Medium Confidence**: The claim that cosine scheduling produces smoother behavioral adaptation than stair-step scheduling, as this is supported by internal ablation but lacks external validation
- **Medium Confidence**: The mechanism that compression primarily suppresses backtracking and verification behaviors while preserving deductive reasoning, based on token frequency analysis that may not capture logical equivalence
- **Low Confidence**: The generalizability claim that SIRI will push models toward the Pareto frontier across different model scales and task domains, given the limited evaluation scope

## Next Checks

1. **Cross-Domain Generalization**: Train SIRI on non-mathematical reasoning tasks (e.g., code generation or multi-step decision making) and evaluate whether the accuracy-token efficiency improvements transfer to these domains, particularly testing if aggressive compression harms accuracy in tasks requiring extensive self-correction.

2. **Scheduler Parameter Sensitivity**: Systematically vary cycle length (T) and L_min values across a wider range (e.g., T ∈ {320, 480, 640, 960}, L_min ∈ {4096, 8192, 12288}) to identify the precise boundaries where performance degrades, confirming whether the 640-step recommendation is optimal or merely sufficient.

3. **Logical Equivalence Validation**: Beyond token frequency analysis, implement a formal verification check comparing the compressed reasoning traces against their longer counterparts to ensure logical completeness is preserved, particularly for critical deduction steps, rather than just counting token types.