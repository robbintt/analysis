---
ver: rpa2
title: Node-level Contrastive Unlearning on Graph Neural Networks
arxiv_id: '2503.02959'
source_url: https://arxiv.org/abs/2503.02959
tags:
- unlearning
- nodes
- graph
- embeddings
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Node-CUL, a novel node-level contrastive
  graph unlearning framework for GNNs. It addresses the challenge of removing target
  nodes' influence while maintaining model utility by directly optimizing the embedding
  space.
---

# Node-level Contrastive Unlearning on Graph Neural Networks

## Quick Facts
- **arXiv ID:** 2503.02959
- **Source URL:** https://arxiv.org/abs/2503.02959
- **Reference count:** 40
- **Primary result:** Node-CUL achieves 87.65% test accuracy on Cora-ML vs 85.92% for GNNDelete with lowest unlearn scores (2.62 vs 51.75)

## Executive Summary
This paper introduces Node-CUL, a novel node-level contrastive graph unlearning framework for GNNs that addresses the challenge of removing target nodes' influence while maintaining model utility by directly optimizing the embedding space. It employs two complementary components: node representation unlearning, which contrasts embeddings of unlearning nodes with remaining nodes and neighbors to push them toward decision boundaries, and neighborhood reconstruction, which modifies neighbor embeddings to remove unlearning nodes' influence and preserve utility. Experiments on multiple datasets (Cora-ML, PubMed, Citeseer, CS) with GCN, GAT, and GIN models show Node-CUL achieves superior test accuracy and lowest unlearn scores, indicating effective unlearning. LiRA membership inference attack results confirm strong privacy preservation, and Node-CUL demonstrates comparable efficiency to state-of-the-art methods.

## Method Summary
Node-CUL is an iterative node-level graph unlearning framework that optimizes the embedding space rather than retraining from scratch. It consists of two complementary components: (1) Node Representation Unlearning using contrastive loss to push unlearning node embeddings away from same-class neighbors and toward different-class remaining nodes, and (2) Neighborhood Reconstruction that recursively modifies k-hop neighbor embeddings to remove influence of unlearning nodes while preserving utility. The method terminates when accuracy on unlearning nodes drops to the level of unseen nodes, balancing forgetting and utility preservation.

## Key Results
- Achieves 87.65% test accuracy on Cora-ML (vs 85.92% for GNNDelete) while maintaining effective unlearning
- Lowest unlearn scores across all datasets (e.g., 2.62 vs 51.75 for GNNDelete on Cora-ML)
- Strong privacy preservation with LiRA AUC scores approaching 0.5 (random guessing)
- Comparable efficiency to state-of-the-art methods despite iterative optimization
- Validated across GCN, GAT, and GIN models with consistent improvements

## Why This Works (Mechanism)

### Mechanism 1: Representation Space Isolation via Contrastive Push-Pull
The framework constructs a contrastive loss using positive sets (same-class neighbors) and negative sets (different-class remaining nodes). By maximizing distance from positive pairs and minimizing distance to negative pairs, node embeddings lose their class-specific structure and migrate toward decision boundaries, behaving like unseen data. The core assumption is that unseen/test nodes naturally cluster near decision boundaries and that embeddings of intra-class samples are initially similar.

### Mechanism 2: Utility Preservation via Recursive Neighborhood Reconstruction
A reconstruction loss optimizes the embeddings of k-hop neighbors by pulling them toward their remaining k-hop neighbors. This is done recursively from the outermost hop inward to ensure stable positioning before updating inner neighbors. The mechanism assumes that a neighbor's representation can be effectively reconstructed using only the remaining subgraph, effectively "healing" the hole left by the unlearning node.

### Mechanism 3: Explicit Termination based on "Unseen" Performance
The algorithm monitors a validation set of unseen nodes and terminates when accuracy on unlearning nodes drops to the level of this validation set. This explicit accuracy-based criteria aims to prevent over-unlearning (utility loss) and under-unlearning (privacy leaks). The assumption is that accuracy on the held-out validation set is a stable proxy for the "maximum allowable" information the model should have about the unlearning nodes.

## Foundational Learning

- **Concept: Message Passing / Neighborhood Aggregation**
  - **Why needed here:** Standard GNNs learn node representations by aggregating features from neighbors, creating mutual dependencies where unlearning a node affects its neighbors. Understanding this is required to grasp why "neighborhood reconstruction" is necessary.
  - **Quick check question:** If you delete Node A, why does Node B's embedding (a neighbor of A) change in a standard GCN?

- **Concept: Contrastive Learning (InfoNCE style)**
  - **Why needed here:** Node-CUL frames unlearning as a representation learning problem, using contrastive loss to manipulate relative distances in the embedding space rather than just masking weights.
  - **Quick check question:** In this context, does the "positive" sample represent what the node *was* (to be pushed away from) or what it *should become*?

- **Concept: Membership Inference Attacks (MIA)**
  - **Why needed here:** The primary evaluation of unlearning efficacy is defending against MIA (specifically LiRA). A successful unlearning method ensures an attacker cannot distinguish unlearning nodes from test nodes (AUC ≈ 0.5).
  - **Quick check question:** If an unlearning method achieves 0% accuracy on unlearning nodes (random guessing), is it necessarily successful? (Hint: Check Section 5.3/GNNDelete behavior).

## Architecture Onboarding

- **Component map:** Input GNN -> Node Representation Unlearning (L_U + L_C) -> Neighborhood Reconstruction (L_N + L_C) -> Termination Monitor (Acc check) -> Output unlearned GNN
- **Critical path:** 1) Sample unlearning nodes and construct positive/negative sets, 2) Optimize model using contrastive loss to push unlearning nodes toward remaining nodes, 3) Recursively update k-hop neighbors to reconstruct their embeddings, 4) Check accuracy on unlearning vs validation nodes, terminate if Acc_u ≤ Acc_eval
- **Design tradeoffs:** Iterative (multi-step updates) vs. one-shot approaches, utility vs. forgetting balance through hyperparameters β and γ, embedding space optimization vs. weight masking
- **Failure signatures:** Low test accuracy suggests reconstruction failure or late termination; high unlearn score with low MIA AUC indicates under-unlearning; dense graph slowdowns due to subgraph computation requirements
- **First 3 experiments:** 1) Baseline efficacy check on Cora-ML with 10% random unlearning nodes comparing test accuracy and unlearn score against Retrain and GIF, 2) Ablation study disabling neighborhood reconstruction to quantify utility cost, 3) LiRA attack verification checking if AUC ≈ 0.5

## Open Questions the Paper Calls Out
None

## Limitations
- Key hyperparameters (GNN architecture details, temperature parameter, k-hop value, sample sizes) are not specified, making faithful reproduction difficult
- Termination condition relies on unvalidated assumption that validation set represents "unseen" distribution
- Paper doesn't report variance across runs, making result stability assessment difficult
- Doesn't explore limits of neighborhood reconstruction for high-degree hub nodes

## Confidence

### Major Uncertainties
- **GNN architecture sensitivity (Medium):** Efficacy across different layer counts and hidden dimensions is not explored
- **Termination condition validity (Low-Medium):** The assumption that V_eval represents "unseen" distribution is not validated
- **Hub node failure (Medium):** Limits of neighborhood reconstruction for high-degree nodes are not thoroughly explored

### Confidence Assessment
- **Node-CUL framework efficacy (High):** Multiple datasets and model types show consistent improvements over baselines
- **Contrastive unlearning mechanism (Medium):** Theoretical justification is sound but unexplored edge cases exist
- **Neighborhood reconstruction effectiveness (Medium):** Ablation studies show utility preservation but limits are not explored
- **Termination condition validity (Low-Medium):** Explicit accuracy-based criteria is paper-specific and unvalidated

## Next Checks
1. **Architecture sensitivity:** Reproduce results across different GNN depths (1-3 layers) and hidden dimensions to test if efficacy holds beyond the reported configuration
2. **Distribution shift impact:** Test termination condition with validation sets drawn from different data distributions to assess robustness when V_eval doesn't represent "unseen" data
3. **Hub node failure:** Systematically test unlearning on high-degree nodes (>20 neighbors) to quantify where neighborhood reconstruction fails and measure the resulting utility/privacy tradeoff