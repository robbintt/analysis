---
ver: rpa2
title: Training neural networks faster with minimal tuning using pre-computed lists
  of hyperparameters for NAdamW
arxiv_id: '2503.03986'
source_url: https://arxiv.org/abs/2503.03986
tags:
- hyperparameter
- workloads
- list
- workload
- lists
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using pre-computed lists of hyperparameters
  for NAdamW optimization to enable faster neural network training with minimal tuning.
  The method involves sampling hyperparameter points from a broad search space, running
  extensive experiments on the AlgoPerf benchmark workloads, and using a greedy procedure
  to construct ordered lists based on a cost function that prioritizes training time
  to reach validation targets.
---

# Training neural networks faster with minimal tuning using pre-computed lists of hyperparameters for NAdamW

## Quick Facts
- **arXiv ID:** 2503.03986
- **Source URL:** https://arxiv.org/abs/2503.03986
- **Reference count:** 9
- **Primary result:** 5-point hyperparameter list achieves at least 3x tuning efficiency gain on most AlgoPerf workloads compared to random search.

## Executive Summary
This paper addresses the challenge of efficiently tuning hyperparameters for deep learning training by proposing a method to generate pre-computed lists of hyperparameter points for the NAdamW optimizer. The approach involves extensive experimentation across a broad search space on the AlgoPerf benchmark workloads, followed by a greedy selection procedure to construct ordered lists that minimize time-to-target. The resulting 5-point list demonstrates significant tuning efficiency improvements and generalizes well across most AlgoPerf workloads, outperforming basic sweeps and Bayesian optimization tools under the same computational budget.

## Method Summary
The method involves sampling 200 candidate hyperparameter configurations from a defined search space using quasirandom search, then running full training trials on all AlgoPerf workloads. A cost function is defined to measure the geometric mean of normalized time-to-target across workloads, with a penalty for missing targets. A greedy procedure iteratively selects the optimal next point to add to the list by minimizing this cost function. The final output is an ordered list of 5 hyperparameter points that collectively cover the search space efficiently.

## Key Results
- The 5-point hyperparameter list achieves at least 3x tuning efficiency gain on most AlgoPerf workloads compared to random search.
- The list successfully trains on 7 of 8 AlgoPerf base workloads in leave-one-out cross-validation.
- The approach outperforms basic learning rate/weight decay sweeps and Bayesian optimization tools when restricted to the same budget of 5 parallel trials.

## Why This Works (Mechanism)
The method exploits the correlation in hyperparameter sensitivity across different deep learning workloads. By running extensive experiments across a diverse set of benchmarks, the greedy selection procedure can identify points that are broadly effective rather than optimized for a single task. The cost function formulation with a penalty factor ensures the list balances performance across easy and hard-to-train workloads, preventing overfitting to the former. The ordered nature of the list allows practitioners to start with the most broadly effective configuration and only move to subsequent points if needed.

## Foundational Learning
- **NAdamW optimizer**: A variant of AdamW with Nesterov momentum, combining adaptive learning rates with momentum. Understanding its dynamics is crucial as the hyperparameter list is specific to this optimizer.
- **Quasirandom search**: A sampling technique (using Sobol or Halton sequences) that provides more uniform coverage of the search space than purely random sampling. This ensures the 200 candidate points represent the space well.
- **Cost function with penalty**: The geometric mean of normalized time-to-target with a penalty factor τ=2.0 for missed targets. This formulation balances finding points that work across all workloads rather than just easy ones.
- **Leave-one-out cross-validation**: A validation technique where each workload is held out in turn, and the list's performance is tested on the held-out workload. This verifies generalization beyond the training set.
- **Linear warmup + cosine decay schedule**: The specific learning rate schedule used, where learning rate starts at zero, linearly increases to the base learning rate, then decays following a cosine curve to zero. The schedule's parameters are tied to total training steps.
- **AlgoPerf benchmark suite**: A collection of 8 diverse deep learning workloads (Criteo 1TB, fastMRI, ImageNet ResNet/ViT, LibriSpeech Conformer/DeepSpeech, OGBG, WMT) used to evaluate and construct the hyperparameter lists.

## Architecture Onboarding
- **Component map**: AlgoPerf workloads -> NAdamW optimizer with specific LR schedule -> 200 candidate hyperparameter points sampled via quasirandom search -> training trials on all workloads -> cost function evaluation -> greedy selection loop -> ordered 5-point list.
- **Critical path**: Sampling -> training trials -> cost function evaluation -> greedy selection. Each stage must complete successfully for the final list to be generated.
- **Design tradeoffs**: Broad search space vs. computational cost (200 points × 8 workloads is expensive), greedy selection vs. global optimization (greedy is faster but may miss better combinations), fixed 5-point list vs. adaptive length (fixed is simpler but may not be optimal for all scenarios).
- **Failure signatures**: Poor generalization to new workloads (overfitting to AlgoPerf), high variance in list performance across seeds (unstable sampling), inability to meet targets within reasonable steps (schedule or search space mismatch).
- **First experiments**: 1) Verify NAdamW implementation matches Optax behavior, 2) Run a single training trial with one hyperparameter configuration on one workload to validate data loading and metric tracking, 3) Implement and test the cost function on a small synthetic dataset to ensure the penalty and geometric mean are computed correctly.

## Open Questions the Paper Calls Out
- Can the proposed hyperparameter lists generalize to large-scale models like LLMs without extensive modification? The paper notes it has not tested the lists on billion-parameter language model workloads.
- Does the greedy construction methodology yield effective hyperparameter lists for training algorithms other than NAdamW? The approach was only validated for NAdamW optimization.
- Is it possible to derive effective scaling curves for hyperparameters starting from the suggested fixed values to accommodate increasing model sizes? The current work provides static values optimal for specific model sizes and step budgets.

## Limitations
- The approach is tightly coupled to the specific AlgoPerf workload set and NAdamW optimizer configuration, with untested effectiveness on workloads outside this domain.
- The quasirandom sampling introduces stochasticity, though the procedure appears stable; different seeds may produce slightly different lists.
- The reliance on a specific cost function formulation with penalty factor τ=2.0 is critical, and deviating from this value risks overfitting to easy workloads.

## Confidence
- **3x tuning efficiency gain is robust within AlgoPerf domain**: High confidence, demonstrated by leave-one-out cross-validation across 7 of 8 base workloads.
- **Effectiveness on non-AlgoPerf workloads**: Low confidence, as transferability to industrial-scale models or different optimization landscapes remains unverified.
- **Stability of quasirandom sampling procedure**: Medium confidence, the 200-point pool and 5-point selection process appears stable enough that different seeds would likely produce similar lists.
- **Criticality of cost function formulation**: High confidence based on ablation evidence showing that deviating from τ=2.0 risks overfitting.

## Next Checks
1. **Robustness to dataset drift**: Re-run the candidate generation phase using the current public Criteo 1TB dataset version and measure performance degradation in the resulting hyperparameter list.
2. **Generalization beyond AlgoPerf**: Apply the 5-point list to at least two distinct deep learning workloads (e.g., BERT pretraining, GAN training) not represented in the training set and report time-to-target improvements versus random search.
3. **Cost function sensitivity**: Generate hyperparameter lists using τ values of 1.33, 2.0, and 3.0, then evaluate each list's leave-one-out cross-validation performance to confirm the claimed optimal range.