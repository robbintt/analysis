---
ver: rpa2
title: Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks
arxiv_id: '2503.02656'
source_url: https://arxiv.org/abs/2503.02656
tags:
- gemma
- attention
- encoder
- pooling
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting decoder-only language
  models, such as Gemma, for encoder-only tasks like classification, regression, and
  ranking. The authors propose a systematic approach to convert decoder-based models
  into encoder architectures by exploring various pooling strategies, attention mechanisms,
  and hyperparameters like dropout.
---

# Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks

## Quick Facts
- arXiv ID: 2503.02656
- Source URL: https://arxiv.org/abs/2503.02656
- Reference count: 15
- This paper proposes a systematic approach to adapt decoder-only models like Gemma for encoder tasks, achieving strong performance on GLUE, SuperGLUE, and MS MARCO benchmarks.

## Executive Summary
This paper addresses the challenge of adapting decoder-only language models (like Gemma) for encoder-only tasks such as classification, regression, and ranking. The authors propose a systematic approach that involves switching from causal to bidirectional attention during fine-tuning, adding dropout regularization, and using simple pooling strategies. Their Gemma Encoder models achieve strong performance on GLUE, SuperGLUE, and MS MARCO benchmarks, demonstrating that decoder models can be effectively repurposed for encoder tasks without encoder-decoder pretraining.

## Method Summary
The authors adapt Gemma decoder models (2B and 9B parameters) for encoder tasks by: 1) replacing causal attention masks with bidirectional attention during fine-tuning, 2) adding dropout regularization (0.1 rate) to attention and FFN outputs, and 3) using simple pooling strategies (Mean or Last-Token) instead of attention pooling. The adapted models are fine-tuned on GLUE/SuperGLUE for classification/regression and MS MARCO for ranking tasks.

## Key Results
- Gemma Encoder outperforms competitive baselines on GLUE and SuperGLUE benchmarks
- Bidirectional attention during fine-tuning dramatically improves performance compared to causal attention
- Simple pooling strategies (Mean/Last-Token) outperform attention pooling on limited GLUE datasets
- Dropout rate of 0.1 provides optimal regularization for encoder task fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Switching from causal to bidirectional attention during fine-tuning significantly improves performance on encoder tasks. Bidirectional attention allows the model to consume the entire input context simultaneously, enhancing its ability to aggregate global representations for classification or regression, overcoming the restrictive "no look-ahead" property of decoder pre-training.

### Mechanism 2
Re-introducing dropout regularization during fine-tuning mitigates overfitting specific to encoder tasks. Modern LLMs often omit dropout during pre-training due to massive data scale, but encoder tasks use smaller datasets where dropout prevents the model from memorizing spurious features in limited supervision signals.

### Mechanism 3
Simple pooling strategies (Mean or Last-Token) outperform complex Attention Pooling in low-data fine-tuning regimes. While attention pooling introduces more learnable parameters to weight important tokens, this added complexity appears to overfit or under-train on the relatively small GLUE datasets, whereas simple pooling acts as a strong regularizer.

## Foundational Learning

- **Attention Masking (Causal vs. Bidirectional)**
  - Why needed here: This is the primary architectural shift. You must understand how masks control information flow to diagnose why a decoder struggles with classification.
  - Quick check question: Does a causal mask allow token 5 to attend to token 6?

- **Pooling Strategies (Aggregating Sequence Representations)**
  - Why needed here: Encoder tasks require a fixed vector output, but Transformers produce variable-length sequences. Understanding how to collapse L×D into 1×D is critical for the model's final performance.
  - Quick check question: Why might averaging all tokens (Mean Pooling) dilute the signal from a keyword in a long sequence?

- **Regularization in Transfer Learning**
  - Why needed here: The paper highlights a "distribution shift" in regularization needs—from no dropout in pre-training to high dropout in fine-tuning.
  - Quick check question: Why does a model pre-trained on 10T tokens not need dropout, but a model fine-tuned on 5k tokens might?

## Architecture Onboarding

- **Component map:** Gemma Decoder -> Bidirectional Attention Mask -> Pooling Layer (Mean/Last-Token) -> Task-specific MLP Head
- **Critical path:**
  1. Load pre-trained Gemma decoder weights
  2. Replace Causal Mask with Bidirectional Mask
  3. Append Pooler and MLP Head (random init)
  4. Enable Dropout (rate 0.1)
  5. Fine-tune on target task (GLUE/MARCO)
- **Design tradeoffs:**
  - Attention: Bidirectional is faster and more accurate but breaks auto-regressive compatibility
  - Pooling: Mean pooling is robust but may dilute features; Attention pooling is expressive but data-hungry (risky for small datasets)
  - Padding: Left vs. Right padding shows no significant difference after fine-tuning with bidirectional attention, but right padding is standard for encoders
- **Failure signatures:**
  - Causal Lock-in: Performance lags significantly if attention masking is not switched to bidirectional
  - Overfitting: Validation loss diverges quickly while training loss drops if dropout is disabled on small datasets
  - Complex Pooling Failure: Attention Pooling underperforms simple Mean pooling on datasets with <10k examples
- **First 3 experiments:**
  1. Attention Ablation: Run identical fine-tuning runs using Causal vs. Bidirectional masks to verify performance delta on a single GLUE task
  2. Dropout Sweep: Test dropout rates [0.0, 0.05, 0.1, 0.15] on the 2B model to confirm the 0.1 optimum locally
  3. Pooling Baseline: Compare Last-Token vs. Mean vs. Attention Pooling to validate that simple pooling is sufficient for the target dataset size

## Open Questions the Paper Calls Out

### Open Question 1
Does the relative efficacy of simple pooling strategies (Mean, Last-K) over attention pooling persist when fine-tuning decoder-based encoders on large-scale datasets or using pre-finetuning for retrieval tasks? The paper's experiments were restricted to benchmarks with limited training sets (fewer than 1 million examples), which may be insufficient to effectively train the randomly initialized parameters of complex attention poolers.

### Open Question 2
Do the optimal adaptation configurations identified for Gemma—specifically bidirectional attention and 10% dropout—generalize effectively to other decoder-only architectures? While the title and introduction frame the approach for "Decoder-Based Language Models" broadly, the empirical evaluation is restricted solely to the Gemma family (2B and 9B).

### Open Question 3
How does the performance of the adaptation method scale with model size beyond 9B parameters compared to encoder-decoder baselines? The study evaluates 2B and 9B variants against T5 baselines (up to 11B), leaving the efficiency of this adaptation technique for frontier-scale decoders (e.g., 70B+) undemonstrated.

## Limitations
- Results are based on adaptations of a single model family (Gemma) and limited encoder tasks (GLUE/SuperGLUE, MS MARCO)
- Optimal dropout rate of 0.1 is asserted but not rigorously justified across different dataset sizes and model scales
- Superiority of simple pooling strategies is demonstrated only on small datasets without specifying the threshold dataset size
- Ablation studies are incomplete—individual contribution of each mechanism to final performance is not quantified

## Confidence

- **High Confidence:** The core experimental results demonstrating that bidirectional attention and dropout regularization improve performance on encoder tasks are well-supported by the presented tables and figures.
- **Medium Confidence:** The claims regarding the superiority of simple pooling strategies over attention pooling on limited data are plausible but lack comprehensive ablation or sensitivity analysis across diverse dataset sizes.
- **Low Confidence:** The broader generalization claims are based on a specific benchmark suite and single model family, making assertions about this being a "systematic solution" for all decoder-to-encoder adaptations unsubstantiated.

## Next Checks

1. **Ablation Study on Individual Mechanisms:** Conduct controlled experiments where the Gemma Encoder model is fine-tuned on GLUE using different combinations of attention masks, dropout, and pooling strategies to isolate each mechanism's contribution.

2. **Dropout Rate Sensitivity Analysis:** Perform a hyperparameter sweep for the dropout rate on the 9B Gemma model using a wider range [0.0, 0.05, 0.1, 0.15, 0.2] on a representative GLUE task like RTE or CoLA to confirm if 0.1 is a global optimum.

3. **Pooling Strategy on Larger Datasets:** Replicate the pooling experiments on a larger, more complex encoder task (e.g., SQuAD v2 or custom dataset with >100k examples) to test whether attention pooling becomes competitive or superior when sufficient training data is available.