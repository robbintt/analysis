---
ver: rpa2
title: Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations
arxiv_id: '2505.23846'
source_url: https://arxiv.org/abs/2505.23846
tags:
- agent
- agents
- simulation
- language
- non-ai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a parallel discrete event simulation (PDES)
  based methodology to couple multiple AI and non-AI agents for collaborative problem-solving.
  The approach breaks complex problems into structured sub-tasks, where AI agents
  (using small language models) handle reasoning and decision-making while non-AI
  agents perform verification and mathematical calculations.
---

# Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations

## Quick Facts
- arXiv ID: 2505.23846
- Source URL: https://arxiv.org/abs/2505.23846
- Reference count: 40
- This paper presents a PDES-based methodology coupling AI and non-AI agents to solve complex problems, achieving 68% accuracy versus <23% for standalone AI models.

## Executive Summary
This paper introduces a parallel discrete event simulation (PDES) framework for coupling small language models (SLMs) with non-AI verifier agents to solve complex algorithmic problems. The approach decomposes problems into structured sub-tasks where SLMs handle reasoning within constrained choice spaces, while non-AI agents verify outputs and perform deterministic calculations. Using the open-source Simian PDES engine and MPI parallelization, the system achieves scalable deployment across multiple compute nodes while maintaining causal ordering and addressing memory bottlenecks for large AI models.

## Method Summary
The methodology employs Simian PDES engine to coordinate AI and non-AI agents as entities across MPI ranks. SLMs (Qwen2.5-7B, Llama3-1-8B, Mistral-Nemo-12B) receive prompts for sub-tasks and select from predefined choices rather than generating free-form responses. Non-AI agents audit SLM outputs using rule-based verification, triggering correction or re-prompting when needed. The system uses event scheduling via `reqService` calls, preserving causal ordering while enabling parallel execution. Experiments tested four problem domains (geometry, combinatorics, arithmetic, graph theory) with 10 random instances each, varying temperature settings and MPI configurations.

## Key Results
- PDES-coupled approach achieved 68% accuracy compared to <23% for standalone AI models
- Strong scaling demonstrated: simulation time decreased as MPI ranks increased (Figure 9-10)
- System throughput reached 40 tokens/minute with optimal MPI configuration
- Constrained choice mechanism reduced hallucination and improved task success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining AI agent solution space via predefined choices reduces hallucination and improves task success.
- Mechanism: SLM agents select from finite set of validated options at each sub-task step, bounding search space and limiting drift.
- Core assumption: Correct answer exists within provided choices; problem decomposition preserves solution validity.
- Evidence anchors: [abstract] "providing a set of multiple choices to the AI agents"; [section 4.2] constrained solution space reduces hallucination.
- Break condition: If decomposition cannot enumerate valid choices exhaustively or correct solution lies outside options.

### Mechanism 2
- Claim: Non-AI agent auditing provides deterministic verification that compensates for SLM reasoning errors.
- Mechanism: After each SLM action, non-AI agents validate outputs against rules (e.g., speed constraints, array bounds).
- Core assumption: Verification rules are expressible algorithmically; non-AI agents can parse SLM outputs reliably.
- Evidence anchors: [abstract] "non-AI agents act as unbiased auditors, verifying each action"; [section 4.1, Figure 2] Non-AI checks constraints and corrects invalid positions.
- Break condition: If SLM output format becomes unparseable or verification requires semantic judgment beyond rule logic.

### Mechanism 3
- Claim: PDES event ordering with MPI parallelization enables scalable multi-agent coordination without causal errors.
- Mechanism: Simian PDES assigns agents as entities to MPI ranks; events carry virtual timestamps preserving causal ordering while enabling parallel execution.
- Core assumption: Sub-tasks can be partially ordered; communication latency between ranks is acceptable for problem granularity.
- Evidence anchors: [abstract] "tightly integrates passage of time, with each agent considered as an entity"; [section 3.1] explains causality error avoidance.
- Break condition: If inter-agent dependencies create dense event cross-scheduling limiting parallelism or MPI overhead exceeds compute savings.

## Foundational Learning

- **Parallel Discrete Event Simulation (PDES)**
  - Why needed: Core execution model; understanding virtual time, event lists, and causality is essential for debugging agent coordination.
  - Quick check: If Agent A sends event to Agent B with timestamp 10, and Agent B's current time is 15, what happens?

- **Constrained decoding / multiple-choice prompting**
  - Why needed: Primary technique for improving SLM reliability; you'll design choice sets for sub-tasks.
  - Quick check: Why might providing 5 choices be better than free-form generation for a sorting sub-task?

- **MPI rank assignment and entity distribution**
  - Why needed: Simian distributes entities across ranks automatically; understanding this helps diagnose load imbalance.
  - Quick check: If 10 SLM agents run on 4 ranks, how might entity placement affect token throughput?

## Architecture Onboarding

- **Component map**: Simian PDES Engine -> reqService calls -> SLMAgent Entity -> NonAIAgent Entity -> reqService calls
- **Critical path**: 
  1. `simianEngine.schedService()` fires initial event
  2. Non-AI agent computes and calls `reqService` on SLM agents
  3. SLM agents run inference, parse output, call `reqService` back to non-AI for verification
  4. Non-AI validates; if invalid, re-schedules SLM event with corrective feedback
  5. Loop until terminal condition

- **Design tradeoffs**:
  - Temperature setting: Lower (0.1) = more deterministic but may loop; higher (0.4+) = diverse but more verification load
  - Parallelism vs. dependencies: More ranks help independent tasks; tightly coupled problems see diminishing returns
  - Choice set size: Larger sets give flexibility but increase SLM error probability; smaller sets constrain but may exclude correct answers

- **Failure signatures**:
  - Parse failures: SLM outputs don't match expected format
  - Verification loops: Non-AI repeatedly rejects SLM outputs
  - Causality stalls: Simulation hangs from circular `reqService` calls
  - Memory overflow: Multiple SLM instances on single rank exceed GPU/CPU RAM

- **First 3 experiments**:
  1. Baseline zero-shot: Run SLM on target problem without PDES coupling; measure accuracy to establish gap
  2. Single-choice constrained: Implement one sub-task with predefined choices; verify non-AI audit catches errors
  3. Scale test: Run geometry problem with 6 agents across 1, 3, 6 MPI ranks; plot simulation time and tokens/minute

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the rule-based decomposition strategy generalize to complex, unstructured real-world problems where sub-tasks are not easily formalized as algorithmic steps?
- Basis: [explicit] Authors acknowledge four test problems are "relatively simple" and serve as "representative tough-case scenarios."
- Why unresolved: Methodology relies on non-AI agents verifying specific rules requiring known solution structure that may not exist in open-ended domains.
- What evidence would resolve it: Successful application to non-algorithmic tasks (e.g., strategic planning) where verification is subjective.

### Open Question 2
- Question: Can the framework maintain strong scaling efficiency when number of AI agents significantly exceeds available MPI ranks?
- Basis: [inferred] Results show scaling up to agent count (6), but introduction proposes deploying "hundreds" of agents.
- Why unresolved: Paper demonstrates scaling only when parallelism matches agent count; doesn't test resource contention when many agents share single rank.
- What evidence would resolve it: Performance benchmarks showing throughput and latency stability with 100+ agents mapped to fixed smaller set of compute nodes.

### Open Question 3
- Question: How does total inference latency of PDES-coupled SLM system compare to single LLM using chain-of-thought prompting?
- Basis: [inferred] Paper argues SLMs circumvent LLM memory/cost issues and focuses on accuracy (68% vs 23%), but doesn't compare wall-clock speed.
- Why unresolved: Parallel approach may have higher total latency than single monolithic inference despite scalability benefits.
- What evidence would resolve it: Comparative timing analysis measuring total time-to-solution for coupled system versus baseline LLM.

## Limitations

- Prompt templates and verification logic are only partially specified, requiring significant engineering effort to reproduce exact 68% accuracy
- Methodology demonstrated only on structured algorithmic problems with clear verification criteria, limiting generalizability to open-ended reasoning tasks
- Scalability claims depend on problem characteristics; heavily interdependent tasks may see diminishing returns not fully explored in paper

## Confidence

**High Confidence**: PDES framework integration and MPI parallelization mechanics are well-established and clearly described
**Medium Confidence**: Core claim that constraining AI choices improves accuracy (68% vs <23%) is supported but exact implementation details needed for reproduction are incomplete
**Low Confidence**: Claims about addressing "local memory bottlenecks for large AI models" are not thoroughly validated with limited evidence beyond token throughput metrics

## Next Checks

1. Reconstruct and test exact prompt templates and choice sets for sorting sub-task, then verify non-AI verification catches at least 80% of SLM errors
2. Apply methodology to new problem domain (e.g., planning/scheduling) with different verification complexity to test whether 68% accuracy improvement holds outside tested domains
3. Systematically measure GPU/CPU memory usage across different SLM sizes and MPI configurations to validate claimed memory efficiency benefits and identify actual scalability ceiling