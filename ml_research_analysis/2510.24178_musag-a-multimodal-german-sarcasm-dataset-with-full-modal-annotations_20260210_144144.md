---
ver: rpa2
title: 'MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations'
arxiv_id: '2510.24178'
source_url: https://arxiv.org/abs/2510.24178
tags:
- sarcasm
- audio
- video
- multimodal
- sarc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MuSaG, the first German multimodal sarcasm
  detection dataset with modality-specific annotations. It contains 214 manually selected
  and human-annotated statements from German TV shows, each with aligned text, audio,
  and video modalities.
---

# MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations

## Quick Facts
- **arXiv ID**: 2510.24178
- **Source URL**: https://arxiv.org/abs/2510.24178
- **Reference count**: 0
- **Primary result**: First German multimodal sarcasm dataset with modality-specific annotations; reveals human-model modality preference inversion

## Executive Summary
This paper introduces MuSaG, the first German multimodal sarcasm detection dataset with modality-specific annotations. It contains 214 manually selected and human-annotated statements from German TV shows, each with aligned text, audio, and video modalities. The dataset supports evaluation in both unimodal and multimodal settings. Benchmarking nine models, including text, audio, vision, and multimodal architectures, shows that humans rely heavily on audio cues, while models perform best on text alone. This reveals a gap between human and model performance, highlighting the dataset's value for developing more effective multimodal sarcasm detection systems.

## Method Summary
MuSaG contains 214 statements from 4 German TV shows, with aligned text, audio, and video modalities. Audio is extracted at 44.1kHz/320kbps, video at 426×240/15fps, and transcripts are generated via Whisper 3 with human correction. Twelve annotators label each statement with majority vote, including modality-isolated annotations for ablation. Nine models are evaluated zero-shot across 7 modality configurations using modality-specific and generic prompts. Performance is measured by macro-averaged precision, recall, and F1-score.

## Key Results
- Humans achieve 87.93 F1 on audio-only classification, while the best model (Gemini-2.5-flash) achieves only 66.95 F1
- Text-based models (Qwen3-8B: 83.28 F1) outperform multimodal models on non-textual inputs
- Extending context by 15 seconds causes all models to drop to near-chance performance
- Open-source multimodal models struggle on video-only (Phi-4: 30.52 F1, worse than chance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio provides the strongest unimodal sarcasm cues for humans, but models fail to leverage these effectively.
- Mechanism: Prosodic features—tone, pitch, pacing, intonation—carry primary sarcastic intent signals in spoken language. Humans achieve 87.93 F1 on audio-only classification versus models' best 66.95 F1 (Gemini-2.5-flash), indicating a ~21-point gap in prosodic understanding.
- Core assumption: Human annotation accuracy reflects genuine signal availability rather than annotation artifact.
- Evidence anchors:
  - [section 4.4] "humans derive the most reliable cues from audio (87.93 F1), suggesting that prosodic features such as tone and intonation provide strong indicators of sarcasm"
  - [section 4.4] "in the audio and video domains, humans outperform models by substantial margins, nearly 21 F1 points for audio"
  - [corpus] Related work on spoken sarcasm (arXiv:2509.15476) confirms multimodal models struggle with audio-visual sarcasm understanding, supporting the generalization of this gap.
- Break condition: If audio features were noise rather than signal, humans would not consistently outperform models on audio-only classification.

### Mechanism 2
- Claim: Text provides the strongest unimodal cues for current models, creating a modality preference inversion between humans and machines.
- Mechanism: Text-based LLMs (Qwen3-8B: 83.28 F1) outperform their multimodal counterparts on non-textual inputs. Models appear to rely primarily on lexical incongruity and linguistic patterns rather than integrating cross-modal cues.
- Core assumption: Model performance differences reflect architectural limitations rather than insufficient training data for non-textual modalities.
- Evidence anchors:
  - [abstract] "models perform best on text"
  - [section 4.3.1] "Qwen3-8B achieves the best results with an 83.28 F1, outperforming smaller and older versions"
  - [section 4.3.2] "the transcript remains the most informative modality... highlighting that in conditions resembling real human communication, where information is conveyed through speech and visual cues, commercial models still outperform open-source alternatives"
  - [corpus] arXiv:2510.11852 finds open-source VLMs struggle with multimodal sarcasm, consistent with this pattern.
- Break condition: If models improved audio/video performance to match text with more multimodal training, the mechanism would shift from architectural to data-limitation explanation.

### Mechanism 3
- Claim: Adding extended conversational context degrades sarcasm detection performance across all modalities.
- Mechanism: Providing 15 seconds of preceding content caused all models to drop to near-chance performance. The paper hypothesizes models struggle with attribution—identifying which segment within extended input to classify.
- Core assumption: Performance degradation stems from attention/attribution failure rather than genuinely confusing contextual signals.
- Evidence anchors:
  - [section 4.5] "this makes the task significantly harder for all models, and performance drops to chance"
  - [section 4.5] "The added temporal context likely introduces distracting or conflicting cues, making it harder for the model to focus on the relevant part of the provided signal"
  - [corpus] arXiv:2510.23727 (MUStReason) finds pragmatic reasoning deficits in video-LMs for sarcasm, suggesting context integration is a broader architectural challenge. No direct corpus evidence validates the attribution hypothesis.
- Break condition: If explicit target-marking in prompts failed to isolate the correct segment, attribution failure is confirmed; if models improve with better prompting, context may be genuinely helpful.

## Foundational Learning

- Concept: **Multimodal sarcasm cues** (prosodic, visual, linguistic)
  - Why needed here: Understanding which modality carries signal informs model architecture choices. The paper shows audio carries strongest human-detectable signal, text carries strongest model-detectable signal.
  - Quick check question: Given an utterance "Na das läuft ja mal wieder super!" with flat prosody and neutral expression, which modality most strongly indicates sarcasm?

- Concept: **Inter-annotator agreement (Fleiss' Kappa)**
  - Why needed here: Dataset quality depends on annotation reliability. MuSaG achieves 0.623 (substantial agreement) for multimodal, 0.594 for single-modality annotations.
  - Quick check question: What does a Fleiss' Kappa of 0.623 indicate about annotation reliability, and why might single-modality agreement be lower?

- Concept: **Modality-specific evaluation protocol**
  - Why needed here: Enables diagnosing where multimodal models fail. The paper evaluates text-only, audio-only, video-only, and all combinations against human baselines.
  - Quick check question: Why is it important to have separate human annotations for each modality rather than a single multimodal label?

## Architecture Onboarding

- Component map:
  - Data collection -> Manual selection of candidate segments
  - Processing -> Audio/video extraction, Whisper transcription, human correction
  - Annotation -> Multimodal labeling, then isolated modality labeling
  - Evaluation -> Unimodal, bimodal, trimodal configurations with two prompting strategies

- Critical path:
  1. Data collection → Manual selection of candidate segments
  2. Processing → Audio/video extraction, Whisper transcription, human correction
  3. Annotation → Multimodal labeling, then isolated modality labeling
  4. Evaluation → Unimodal, bimodal, trimodal configurations with two prompting strategies

- Design tradeoffs:
  - Manual selection vs. automatic tagging: Higher quality but smaller scale (214 statements vs. thousands in social media datasets)
  - Full-modal annotations vs. single label: Enables fine-grained analysis but increases annotation cost
  - TV shows vs. social media: Natural conversational sarcasm but limited domain diversity

- Failure signatures:
  - Models performing better on text than audio-video (indicates failure to integrate non-textual cues)
  - Performance dropping with extended context (indicates attribution/attention failure)
  - Open-source multimodal models underperforming on video-only (Phi-4 achieves 30.52 F1, worse than chance)

- First 3 experiments:
  1. **Baseline replication**: Run text-only, audio-only, video-only evaluations on MuSaG-FullAgree to confirm the human-model gap on your infrastructure.
  2. **Audio feature ablation**: Extract prosodic features (pitch variance, speech rate, emphasis patterns) and test whether explicit feature engineering improves audio-only model performance beyond raw waveform input.
  3. **Context window reduction**: Test whether shorter context windows (5s, 10s) mitigate the attribution failure observed at 15s, or if the problem is context presence rather than length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal architectures be improved to effectively leverage extended conversational context without degrading performance due to attention dispersion?
- Basis in paper: [inferred] The authors extended input context by 15 seconds and found that performance dropped to chance level for all models (Section 4.5), hypothesizing that models struggle to isolate the target utterance.
- Why unresolved: Current models seemingly cannot distinguish relevant cues from distracting information in longer temporal sequences.
- What evidence would resolve it: Demonstrating a model architecture with a specific attention mechanism that successfully utilizes conversational history to increase F1 scores on MuSaG.

### Open Question 2
- Question: What specific pre-training objectives or architectural constraints are necessary to force models to rely on prosodic (audio) features rather than defaulting to textual patterns?
- Basis in paper: [explicit] The paper states that current models "fail to fully exploit audio cues" and rely primarily on text, whereas humans rely heavily on audio (Section 5).
- Why unresolved: Current benchmarks and training methods appear to bias models toward textual shortcuts, leaving the human-preferred modality (audio) underutilized.
- What evidence would resolve it: Developing a model that achieves comparable or superior performance on the audio-only subset compared to the text-only subset.

### Open Question 3
- Question: Why does the inclusion of visual data sometimes degrade performance in state-of-the-art models, and how can fusion mechanisms be designed to filter visual noise?
- Basis in paper: [inferred] The authors observed that the text-audio-video condition did not yield the best performance for the top commercial model, hypothesizing that video "can sometimes introduce noise" (Section 4.3.2).
- Why unresolved: It is unclear if the failure stems from low-level visual processing (resolution/frame rate) or high-level integration issues where visual data conflicts with audio/text.
- What evidence would resolve it: An ablation study showing consistent F1 improvements when adding processed video to text-audio inputs, or identifying specific visual features that correlate with the observed performance drop.

## Limitations
- Dataset size (214 statements) is small relative to social media sarcasm datasets, limiting statistical power and generalizability
- Human annotation agreement, while substantial (0.623 Fleiss' Kappa), shows disagreement exists on what constitutes sarcasm in multimodal contexts
- Model evaluation uses zero-shot inference without fine-tuning, making it unclear whether multimodal models could improve with domain adaptation

## Confidence
- **High**: Text modality superiority for models; audio modality superiority for humans; existence of human-model modality preference inversion
- **Medium**: Audio signal strength hypothesis; context degradation mechanism; dataset quality and annotation reliability
- **Low**: Attribution failure as sole cause of context degradation; generalizability to other domains or languages

## Next Checks
1. **Dataset scale validation**: Create a MuSaG-FullAgree subset (155 statements with unanimous annotation) and verify reported performance drops from full to full-agree subset are statistically significant across all models
2. **Audio signal isolation**: Extract prosodic features (pitch variance, speech rate, emphasis patterns) from MuSaG audio and test whether explicit feature engineering improves audio-only model performance beyond raw waveform input
3. **Context attribution test**: Test shorter context windows (5s, 10s) to determine if the 15s attribution failure is length-dependent or context-dependent, or explicitly mark target segments in prompts to test the attribution hypothesis directly