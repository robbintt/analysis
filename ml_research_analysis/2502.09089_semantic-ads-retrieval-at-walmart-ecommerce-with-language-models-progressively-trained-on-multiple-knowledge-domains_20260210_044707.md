---
ver: rpa2
title: Semantic Ads Retrieval at Walmart eCommerce with Language Models Progressively
  Trained on Multiple Knowledge Domains
arxiv_id: '2502.09089'
source_url: https://arxiv.org/abs/2502.09089
tags:
- search
- walmart
- retrieval
- embedding
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving semantic ad retrieval
  in e-commerce search, where matching user queries with relevant sponsored products
  is difficult due to language ambiguity, sparse data, and large volumes. To tackle
  this, the authors propose a two-stage progressive training framework using a DistilBERT-based
  two-tower Siamese network.
---

# Semantic Ads Retrieval at Walmart eCommerce with Language Models Progressively Trained on Multiple Knowledge Domains

## Quick Facts
- arXiv ID: 2502.09089
- Source URL: https://arxiv.org/abs/2502.09089
- Reference count: 22
- Primary result: Progressive pretraining on multiple knowledge domains improves semantic ad retrieval, achieving up to 16% NDCG gain and 4% reduction in irrelevant ads rate

## Executive Summary
This paper tackles the challenge of semantic ad retrieval in e-commerce by addressing the gap between user queries and relevant sponsored products. The authors propose a two-stage progressive training framework that leverages a DistilBERT-based two-tower Siamese network, pretrained on product categories and then fine-tuned on diverse datasets including natural language inference data, search logs, and ad interactions. The approach is designed to handle language ambiguity, sparse data, and large volumes typical of e-commerce search. A human-in-the-loop mechanism dynamically adjusts training weights to further improve relevance. The model demonstrates significant offline and online performance gains, including up to 16% improvement in NDCG over a DSSM baseline and measurable increases in click-through rates and ad revenue in A/B testing.

## Method Summary
The method employs a two-stage progressive training framework using a DistilBERT-based two-tower Siamese network for semantic ad retrieval. First, the model is pretrained on product category labels to capture semantic relationships. Next, it is fine-tuned using a diverse set of datasets: natural language inference data, search logs, and ad interaction data. A human-in-the-loop mechanism dynamically adjusts training weights to optimize relevance. The architecture enables efficient encoding of queries and ads into a shared semantic space, facilitating improved matching. The progressive training approach allows the model to leverage multiple knowledge domains, enhancing its ability to handle the complexities of e-commerce search.

## Key Results
- Up to 16% improvement in NDCG over a baseline DSSM model
- 4% reduction in irrelevant ads rate
- Significant increases in click-through rates, cost efficiency, and ad revenue in online A/B testing across multiple placements

## Why This Works (Mechanism)
The progressive training approach works by first establishing a strong semantic foundation through pretraining on product categories, which provides a broad understanding of product relationships. Subsequent fine-tuning on diverse datasets (NLI, search logs, ad interactions) allows the model to adapt to the specific nuances of ad retrieval, such as user intent and commercial relevance. The two-tower Siamese architecture enables efficient encoding and matching of queries and ads in a shared semantic space. The human-in-the-loop mechanism introduces dynamic, relevance-driven adjustments to training, further refining the model's performance.

## Foundational Learning
- **DistilBERT**: A distilled version of BERT, used here to reduce model size and computational cost while maintaining strong performance. Needed to balance efficiency and accuracy in large-scale production. Quick check: Ensure model retains sufficient representational power for semantic matching.
- **Two-tower Siamese Network**: Encodes queries and ads separately into a shared semantic space for efficient similarity scoring. Needed to scale to large product catalogs. Quick check: Verify encoding quality and retrieval accuracy at scale.
- **Progressive Training**: Sequentially pretraining and fine-tuning on different knowledge domains to leverage diverse signals. Needed to handle sparse and heterogeneous e-commerce data. Quick check: Confirm each stage meaningfully improves performance.
- **Human-in-the-loop**: Dynamic adjustment of training weights based on human feedback to optimize relevance. Needed to capture subtle relevance signals not present in logged data. Quick check: Ensure human adjustments improve, not degrade, model stability.

## Architecture Onboarding
- **Component Map**: Query Encoder -> Ad Encoder -> Similarity Scorer -> Ranking Module
- **Critical Path**: User query → Query encoder → Semantic vector → Similarity computation with ad vectors → Ranked ad list → Display
- **Design Tradeoffs**: Smaller DistilBERT model reduces latency and resource usage but may sacrifice some semantic nuance compared to full BERT; two-stage training improves relevance but increases complexity and data requirements; human-in-the-loop improves adaptability but introduces potential bias and operational overhead.
- **Failure Signatures**: Overfitting to training data (poor generalization), bias introduced by human annotators, instability in dynamic training weight adjustments, degradation in retrieval quality for rare or long-tail queries.
- **First 3 Experiments**: (1) Compare NDCG on a held-out validation set with and without progressive pretraining; (2) Measure impact of each fine-tuning dataset (NLI, search logs, ad interactions) via ablation; (3) Evaluate online A/B test results for click-through rate and irrelevant ad rate.

## Open Questions the Paper Calls Out
None

## Limitations
- Improvements are measured primarily against a DSSM baseline, not state-of-the-art transformer models, limiting the significance of reported gains.
- The two-stage progressive training approach introduces substantial engineering complexity and dependency on diverse datasets, raising generalizability concerns.
- The human-in-the-loop mechanism lacks detail on implementation, frequency, and potential bias, making its effectiveness difficult to assess.

## Confidence
- **High Confidence**: Technical feasibility and successful production deployment of the two-stage progressive training framework are well-supported.
- **Medium Confidence**: Offline improvements (up to 16% NDCG) are plausible but not compared to more recent baselines.
- **Low Confidence**: Online A/B testing results and human-in-the-loop impact lack sufficient detail for independent verification.

## Next Checks
1. Conduct head-to-head comparisons against state-of-the-art transformer-based semantic search models (e.g., Sentence-BERT, ColBERT).
2. Perform ablation studies to isolate the contribution of each training stage and the human-in-the-loop mechanism.
3. Execute a comprehensive fairness and bias audit across multiple user demographics and product categories.