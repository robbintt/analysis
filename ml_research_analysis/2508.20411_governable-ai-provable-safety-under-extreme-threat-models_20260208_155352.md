---
ver: rpa2
title: 'Governable AI: Provable Safety Under Extreme Threat Models'
arxiv_id: '2508.20411'
source_url: https://arxiv.org/abs/2508.20411
tags:
- rules
- security
- rule
- signature
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Governable AI (GAI), a novel framework for
  ensuring AI safety under extreme threat models. The core idea is to shift from internal
  AI constraints to externally enforced structural compliance using cryptographic
  mechanisms.
---

# Governable AI: Provable Safety Under Extreme Threat Models

## Quick Facts
- arXiv ID: 2508.20411
- Source URL: https://arxiv.org/abs/2508.20411
- Reference count: 40
- This paper proposes a framework for AI safety using cryptographic enforcement that guarantees compliance even under extreme threat models.

## Executive Summary
This paper introduces Governable AI (GAI), a novel framework that shifts AI safety from internal constraints to externally enforced structural compliance. The system uses cryptographic mechanisms to create a "safety floor" where even a superintelligent AI cannot bypass human-defined governance rules. GAI consists of a Rule Enforcement Module (REM) that verifies AI commands against signed rules, and a Governable Secure Super-Platform (GSSP) that ensures non-bypassability, tamper-resistance, and unforgeability through hardware-based security measures.

## Method Summary
GAI implements a cryptographically-enforced safety layer between AI decision-making and physical actuators. The architecture physically separates AI and Controller nodes, with the Controller containing a TPM-based root of trust, Linux OS with eBPF LSM for mandatory access control, and a REM built on Drools rule engine with OpenSSL signature verification. Commands flow from AI through the REM, which validates them against signed governance rules before allowing actuator access. The framework uses USB hardware keys for offline rule signing and restricts rules to deterministic logic to enable formal proofs.

## Key Results
- 100% compliance rate with safety rules across all tested map configurations in autonomous driving scenarios
- Successfully intercepted non-compliant commands from AI, even when underlying AI exhibited high error rates
- Formal proofs demonstrate GAI's security properties under extreme threat models (misjudgment, manipulation, active attack)

## Why This Works (Mechanism)

### Mechanism 1: Cryptographic Binding of Governance
The system uses asymmetric cryptography to bind governance rules to hardware, preventing the AI from forging or tampering with rules. Private keys remain isolated in USB hardware modules, and rule updates require offline signing. Security relies on cryptographic hardness assumptions and key isolation.

### Mechanism 2: Structural Isolation via Mandatory Access Control
Hardware separation and OS-level MAC ensure the AI cannot bypass the REM to control actuators directly. The Linux LSM with eBPF hooks restricts actuator access to only the REM process, blocking all other attempts with EPERM errors.

### Mechanism 3: Deterministic Rectification (The "Safety Floor")
The REM acts as a pure function that transforms unsafe commands into safe ones through clamping or NOP operations. This creates a deterministic safety floor that guarantees compliance regardless of AI intent, relying on complete rule definitions and bug-free logic.

## Foundational Learning

- **Threat Models (T1, T2, T3)**: Understanding the difference between Misjudgment (T1), Manipulation (T2), and Active Attack (T3) is necessary to see why cryptographic external enforcement is proposed. Quick check: Why does the paper claim model alignment fails to solve the T3 (Active Attack) threat?

- **Trusted Platform Module (TPM) & Chain of Trust**: The GSSP relies on hardware root of trust. Without understanding how TPM measures the boot process, the "non-bypassability" guarantee cannot be verified. Quick check: How does the TPM ensure that the REM hasn't been swapped for a malicious version before the OS even starts?

- **Formal Verification vs. Probabilistic Safety**: The paper distinguishes between probabilistic AI (which can fail) and deterministic REM (which must not). The safety claim rests on mathematically proving REM behavior rather than testing. Quick check: Why is the REM restricted to "pure functions without side effects" when formalizing its logic?

## Architecture Onboarding

- **Component map**: AI System -> Controller (TPM + OS + REM) -> Actuators; Signature Terminals (offline) -> USB Keys (PKCS#11) -> Rule Updates

- **Critical path**: 1. Boot: TPM verifies OS integrity -> OS loads REM; 2. Input: Sensors read state -> Data sent to AI and REM; 3. Decision: AI generates command c; 4. Intercept: Command c hits Controller OS; 5. Enforcement: REM applies rectify(c, r, s); 6. Output: Resulting safe command sent to actuator

- **Design tradeoffs**: Latency vs. Safety (REM introduces processing delays), Decoupling (separating rule definition from implementation allows non-technical updates but requires robust interpreter layer), Expressiveness (restricting rules to deterministic logic enables formal proofs but may limit complex governance scenarios)

- **Failure signatures**: IMA Appraisal Failure (EACCES or integrity errors if REM binary or rule files are tampered with), LSM Bypass Attempt (EPERM errors for non-REM processes trying to access actuator ports), Re-planning Loop (AI repeatedly issuing commands that result in NOP)

- **First 3 experiments**: 1. Tamper Validation: Modify REM binary and verify Linux IMA/EVM refuses to load it; 2. Isolation Test: Run script with root privileges to write directly to actuator port and confirm eBPF hook blocks it; 3. Adversarial Command Injection: Feed "illegal" path plan (e.g., driving through wall) and observe if REM converts to stop or valid alternative

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the GAI framework be extended to distributed architectures while maintaining trust propagation and system integrity? The current prototype relies on single-node controller architecture; distributed systems introduce synchronization and network partitioning vulnerabilities.

- **Open Question 2**: How can the security of the entire dependency chain (OS, firmware, libraries) be formally verified to support the REM? The current analysis assumes TCP and OS security but lacks formal verification for the complete software stack.

- **Open Question 3**: Can GAI maintain provable safety if the threat model includes physical hardware attacks by the AI? The paper's guarantees rely on software/network attack restrictions and exclude physical attacks that could bypass structural compliance boundaries.

## Limitations

- Security guarantees depend on strong cryptographic and hardware assumptions that are difficult to validate in practice
- Prototype experiments limited to grid-world autonomous driving with non-adversarial AI, not tested under sophisticated adversarial conditions
- Restriction to deterministic, non-recursive rule languages may severely limit expressiveness needed for complex real-world governance scenarios

## Confidence

- **High Confidence**: Cryptographic mechanism for binding governance rules is well-established with strong theoretical foundations; hardware separation and OS-level MAC is sound architectural principle
- **Medium Confidence**: Prototype's 100% compliance rate is compelling for tested scenarios but sample size and threat model are narrow; formal proofs valid within stated assumptions but real-world deployment requires extensive rule completeness testing
- **Low Confidence**: System's resilience against truly superintelligent adversary discovering novel hardware or cryptographic exploits is unverifiable; paper does not address side-channel attacks or compromised supply chain backdoors

## Next Checks

1. **Hardware Security Audit**: Commission penetration test of USB key hardware modules and TPM/Trusted Boot implementation to identify potential key extraction or boot modification vulnerabilities

2. **Adversarial AI Stress Test**: Replace GPT-4o-mini with AI specifically trained to find edge cases in governance rules (using reinforcement learning to maximize task completion while minimizing rule violations) and measure system's failure rate

3. **Formal Verification of Enforcer Code**: Apply formal verification tool (e.g., Coq, Isabelle/HOL) to eBPF LSM program and signature verification logic to mathematically prove absence of memory safety violations or logic flaws that could be exploited to bypass enforcement