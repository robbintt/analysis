---
ver: rpa2
title: 'MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for
  Edge AI'
arxiv_id: '2508.09500'
source_url: https://arxiv.org/abs/2508.09500
tags:
- accuracy
- search
- bops
- hardware
- schemes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MiCo is an end-to-end framework for efficient mixed-precision quantization
  of neural networks for edge AI deployment. It introduces a novel exploration algorithm
  that combines ensemble learning models (random forests) with specialized sampling
  strategies to efficiently search for optimal quantization schemes under hardware
  constraints.
---

# MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI

## Quick Facts
- arXiv ID: 2508.09500
- Source URL: https://arxiv.org/abs/2508.09500
- Authors: Zijun Jiang; Yangdi Lyu
- Reference count: 33
- Key outcome: MiCo achieves significant speedup with minimal accuracy loss through efficient mixed-precision quantization exploration under hardware constraints

## Executive Summary
MiCo is an end-to-end framework for efficient mixed-precision quantization of neural networks for edge AI deployment. It introduces a novel exploration algorithm that combines ensemble learning models (random forests) with specialized sampling strategies to efficiently search for optimal quantization schemes under hardware constraints. The framework uses hardware-aware latency proxy models (CBOPs) that adapt to different target platforms (accelerators and SIMD-extended CPUs), providing more accurate performance predictions than traditional metrics. MiCo enables direct deployment from PyTorch models to bare-metal C code, achieving significant speedup with minimal accuracy loss.

## Method Summary
MiCo employs a Random Forest-based predictor combined with orthogonal initial sampling and near-constraint optimization to efficiently explore the mixed-precision quantization space. The framework evaluates quantization schemes using hardware-aware latency proxies (CBOPs) that model latency as a function of compute and memory traffic variables specific to the target hardware. The search process iterates between predictor training and evaluation of promising candidates, using PTQ for bitwidths ≥4 and short QAT (pretrain_time/20) for sub-4-bit schemes. The framework supports direct deployment to various hardware targets through graph conversion and C code generation.

## Key Results
- MiCo achieves 0.77× latency vs. 0.87× for BOPs-guided search on BitFusion hardware
- SqueezeNet accuracy improves by +0.56% and TinyLLaMa by +4.01% with near-constraint sampling
- The framework enables direct deployment from PyTorch to bare-metal C code with minimal accuracy loss
- CBOPs shows R²=0.55 correlation with actual latency on BitFusion compared to BOPs R²=0.35

## Why This Works (Mechanism)

### Mechanism 1: Random Forest-based MPQ Accuracy Prediction
Random forest ensemble models can efficiently predict neural network accuracy from quantization schemes with limited samples by constructing multiple decision trees on random data subsets, averaging their predictions to reduce variance. The mapping from quantization scheme to accuracy is learnable from relatively few evaluated samples and generalizes to unseen schemes.

### Mechanism 2: Near-Constraint Sampling Optimization
Concentrating search near the BOPs/latency constraint boundary yields better solutions faster than uniform exploration because accuracy positively correlates with BOPs. Optimal schemes under constraint likely lie near the constraint boundary, allowing pruning of unpromising regions.

### Mechanism 3: Hardware-aware CBOPs Latency Proxy
Hardware-specific composite bit operations (CBOPs) better predict actual inference latency than generic BOPs metric by modeling latency as a function of BMACs (compute), ALoads (activation memory traffic), and WLoads (weight memory traffic) with coefficients fitted to profiling data.

## Foundational Learning

- **Concept: Mixed-Precision Quantization (MPQ)**
  - Why needed here: Core problem MiCo solves; understanding that layers have different quantization sensitivity is essential
  - Quick check question: Why might a convolution layer need W8A8 while a fully-connected layer works well with W2A4?

- **Concept: PTQ vs. QAT Trade-offs**
  - Why needed here: MiCo handles them differently; sub-4-bit quantization requires QAT to recover accuracy
  - Quick check question: At what bitwidth does the paper show QAT accuracy significantly exceeds PTQ accuracy?

- **Concept: BOPs Metric and Limitations**
  - Why needed here: Understanding why traditional BOPs fails motivates the CBOPs approach
  - Quick check question: Why doesn't reducing BOPs by 50% guarantee 2× speedup on real hardware?

## Architecture Onboarding

- **Component map:** MPQ Search Engine (RF predictor, orthogonal sampler, near-constraint optimizer) -> Hardware Profiler (kernel benchmarks → CBOPs coefficient fitting) -> Graph Converter (PyTorch FX → DnnWeaver graph for BitFusion) -> MP Kernel Library (MatMul, Conv2D for all 8/4/2/1-bit combinations) -> C Code Generator (FX graph → bare-metal C with buffer allocation)

- **Critical path:**
  1. Pre-trained model + bitwidth options + hardware profile → Initial orthogonal sampling (evaluates Ninit schemes)
  2. Search loop: RF trains on samples → predicts best near-constraint schemes → evaluates PTQ/short-QAT → adds to training set
  3. Best scheme selection → full QAT if needed → graph conversion → C code generation → compilation for target

- **Design tradeoffs:**
  - Sample budget (16–48 samples in experiments) vs. search quality—more samples improve accuracy but increase exploration time
  - PTQ speed vs. QAT accuracy—PTQ fast but fails below 4-bit; QAT recovers accuracy but costs ~pretrain_time/10
  - CBOPs model complexity (linear vs. regression trees)—linear faster to fit but may miss CPU non-linearities

- **Failure signatures:**
  - Predictor overfitting: high accuracy on evaluated schemes but poor recommendations
  - Constraint violation: BOPs-compliant scheme exceeds actual latency due to poor proxy correlation
  - Accuracy collapse: QAT short-training approximation fails for complex models
  - Deployment errors: unsupported bitwidth combination or memory allocation failure

- **First 3 experiments:**
  1. Reproduce PTQ search on SqueezeNet with 0.6 BOPs constraint (Tab. IV baseline) to validate predictor quality
  2. Compare BOPs vs. CBOPs exploration on VGG7/BitFusion (Tab. VII) to measure latency improvement from hardware-aware proxy
  3. End-to-end deploy LeNet5 on VexiiMiCo CPU (Tab. X) to validate complete flow from PyTorch to bare-metal C

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does excluding input and weight tensor shapes from the CBOPs latency model significantly degrade prediction accuracy on hardware with shape-dependent caching or tiling strategies?
- Basis in paper: [explicit] Section IV-A states, "other factors like input shapes and weight shapes can also be used... but in this work, we focus solely on the three variables mentioned."
- Why unresolved: The authors explicitly simplified the proxy model; shape-dependent behaviors in complex memory hierarchies might not be fully captured by load/compute counts alone.
- What evidence would resolve it: Ablation studies comparing CBOPs prediction error with and without shape features on diverse hardware targets.

### Open Question 2
- Question: Can the short-retraining QAT evaluation proxy reliably predict the accuracy recovery potential for extremely low bitwidths (e.g., 1-2 bits)?
- Basis in paper: [inferred] Section III-D notes QAT accuracy is validated after a "very short re-training" to approximate recoverable accuracy.
- Why unresolved: Extremely low bitwidths often require extensive training to converge; a short proxy might mislead the search algorithm regarding the final achievable accuracy.
- What evidence would resolve it: Comparing the ranking of MPQ schemes from the short proxy versus full QAT convergence to check for ordering inconsistencies.

### Open Question 3
- Question: Does the Random Forest-based exploration algorithm maintain sample efficiency and search quality when scaling to modern Large Language Models (LLMs) with significantly deeper architectures?
- Basis in paper: [inferred] The paper validates TinyLLaMa (1M parameters), explicitly calling it a "small-scale" model, while general LLMs are much larger.
- Why unresolved: The exponential growth of search space ($B^L$) and the high cost of QAT evaluations for LLMs may exceed the practical sample budgets where Random Forests excel.
- What evidence would resolve it: Extending the experiments to standard LLM sizes (e.g., 7B parameters) and analyzing search convergence rates.

## Limitations

- RF predictor generalization across diverse models and constraints remains unclear with limited evaluation on very deep architectures
- Near-constraint sampling assumes smooth accuracy-BOPs correlation which may not hold for complex models or tight constraints
- CBOPs proxy accuracy depends heavily on hardware profiling quality and may miss architectural nuances like caching or pipeline effects

## Confidence

- **High**: Hardware-aware CBOPs proxy provides more accurate latency predictions than BOPs metric (validated on BitFusion and CPU)
- **Medium**: Random Forest ensemble can efficiently predict accuracy from quantization schemes with limited samples
- **Medium**: Near-constraint sampling yields better solutions faster than uniform exploration
- **Low**: Short QAT reliably recovers accuracy for all sub-4-bit quantization scenarios across diverse models

## Next Checks

1. **Predictor generalization test**: Evaluate RF accuracy prediction on a held-out set of quantization schemes (not used in training) to measure overfitting risk
2. **Constraint boundary analysis**: Systematically vary constraint tightness to verify accuracy-BOPs correlation remains smooth and monotonic
3. **Hardware proxy validation**: Measure actual latency vs. CBOPs prediction across diverse kernel types to quantify model accuracy and identify outliers