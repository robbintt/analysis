---
ver: rpa2
title: 'DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor
  Diffusion Policy Learning'
arxiv_id: '2509.17684'
source_url: https://arxiv.org/abs/2509.17684
tags:
- policy
- arxiv
- diffusion
- learning
- dinov3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates DINOv3, a large-scale self-supervised
  vision backbone, for visuomotor diffusion policy learning in robotic manipulation.
  The study compares DINOv3 against conventional supervised ImageNet-pretrained backbones
  (e.g., ResNet-18) under three training regimes: training from scratch, frozen, and
  fine-tuned.'
---

# DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning

## Quick Facts
- arXiv ID: 2509.17684
- Source URL: https://arxiv.org/abs/2509.17684
- Reference count: 24
- Primary result: Fine-tuned DINOv3 matches or exceeds ResNet-18 on visuomotor diffusion tasks; frozen DINOv3 remains competitive

## Executive Summary
This paper investigates the use of DINOv3, a large-scale self-supervised vision backbone, for visuomotor diffusion policy learning in robotic manipulation. The study compares DINOv3 against conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under three training regimes: training from scratch, frozen, and fine-tuned. Across four benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned diffusion policy, the results show that fine-tuned DINOv3 matches or exceeds ResNet-18 on several tasks, frozen DINOv3 remains competitive, and self-supervised features improve sample efficiency and robustness. Notably, DINOv3 achieves up to a 10% absolute increase in test-time success rates on challenging tasks such as Can compared to ResNet18, while performing on par with ResNet18 on tasks like Lift, PushT, and Square.

## Method Summary
The authors evaluate DINOv3 as a perceptual front-end for diffusion-based visuomotor policies. They train a FiLM-conditioned diffusion policy on four MetaWorld manipulation tasks, comparing DINOv3 with ResNet-18 backbones under three conditions: training from scratch, frozen, and fine-tuned. The diffusion policy maps image observations to continuous action distributions, with the visual encoder either kept fixed or adapted during policy training. Evaluation measures success rates and sample efficiency across training runs.

## Key Results
- Fine-tuned DINOv3 matches or exceeds ResNet-18 on several tasks, with up to 10% higher success on the Can task
- Frozen DINOv3 remains competitive with supervised backbones across all tasks
- Self-supervised features improve sample efficiency and robustness in policy learning
- DINOv3 performs on par with ResNet18 on Lift, PushT, and Square tasks

## Why This Works (Mechanism)
DINOv3's self-supervised pretraining on large-scale unlabeled data yields rich, generalizable visual features that transfer well to downstream robotic tasks. These features capture diverse visual patterns without task-specific supervision, making them robust to domain shifts and enabling efficient policy learning when fine-tuned. The diffusion policy architecture benefits from these features by receiving high-quality perceptual inputs that support stable action distribution learning.

## Foundational Learning
- **Self-supervised learning**: Trains models without manual labels using pretext tasks like contrastive prediction; needed to avoid expensive labeling and improve generalization. Quick check: Are the pretraining objectives preserving task-relevant invariances?
- **Diffusion models**: Generative models that iteratively denoise data; needed to produce smooth, probabilistic action outputs for continuous control. Quick check: Does the noise schedule match the action space scale?
- **FiLM conditioning**: Feature-wise linear modulation that injects task-specific parameters; needed to adapt shared backbones to different manipulation behaviors. Quick check: Are FiLM parameters properly regularized to prevent overfitting?
- **Visual representation transfer**: Using pretrained vision models in downstream tasks; needed to bootstrap policy learning from rich perceptual priors. Quick check: Does fine-tuning preserve pretrained invariances while adapting to task-specific features?
- **Sample efficiency**: Learning effective policies with minimal interaction data; needed for real-world deployment where data collection is costly. Quick check: Are learning curves steeper for self-supervised encoders compared to training from scratch?

## Architecture Onboarding
- **Component map**: RGB Image → DINOv3 Backbone → Feature Map → FiLM Layers → Diffusion Policy Network → Action Distribution
- **Critical path**: Image → Backbone → FiLM → Policy → Action
- **Design tradeoffs**: Large self-supervised models offer better generalization but require more compute; fine-tuning improves performance but risks overfitting; frozen backbones ensure stability but may limit task adaptation
- **Failure signatures**: Poor performance when visual features don't align with task semantics; overfitting when fine-tuning on small datasets; instability if diffusion noise schedule mismatches action scale
- **First experiments**:
  1. Compare frozen DINOv3 vs. frozen ResNet-18 on a simple task (e.g., Lift) to establish baseline generalization
  2. Test fine-tuning impact by training from scratch vs. fine-tuning on a mid-difficulty task (e.g., PushT)
  3. Evaluate sample efficiency by measuring learning curves with limited training samples on Can task

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four manipulation tasks in MetaWorld simulation, constraining generalization claims
- Performance gains over supervised backbones are inconsistent and task-dependent rather than universal
- No comparison with other state-of-the-art self-supervised backbones beyond DINO-family models

## Confidence
- DINOv3 performance relative to supervised backbones: Medium
- Self-supervised features improve sample efficiency and robustness: Medium
- DINOv3 is an effective generalizable perceptual front-end: Low-Medium

## Next Checks
1. Evaluate DINOv3 across a broader suite of manipulation and navigation tasks in both simulation and real-world settings to assess true generalization.
2. Compare against other state-of-the-art self-supervised backbones (e.g., MAE, SimCLR, SwAV) under identical training and evaluation protocols.
3. Conduct long-horizon or multi-task experiments to test whether self-supervised features scale in more complex, temporally extended scenarios.