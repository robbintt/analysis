---
ver: rpa2
title: Zero-shot data citation function classification using transformer-based large
  language models (LLMs)
arxiv_id: '2511.02936'
source_url: https://arxiv.org/abs/2511.02936
tags:
- https
- data
- were
- evaluation
- citation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies an open-source large language model (Llama 3.1-405B)
  to classify data citation functions in genomic datasets, achieving an F1 score of
  0.674 on a zero-shot task with no predefined categories. Using a decision-tree prompting
  structure and Retrieval-Augmented Generation, the model identifies whether datasets
  are accessed, their use cases, and associated tools/software.
---

# Zero-shot data citation function classification using transformer-based large language models (LLMs)

## Quick Facts
- arXiv ID: 2511.02936
- Source URL: https://arxiv.org/abs/2511.02936
- Reference count: 22
- Primary result: Zero-shot LLM classification of genomic data citation functions achieves F1 score of 0.674

## Executive Summary
This study demonstrates that large language models can classify data citation functions in genomic publications without training data or predefined categories. Using Llama 3.1-405B with a decision-tree prompting structure and Retrieval-Augmented Generation, the model identifies whether datasets are accessed, their use cases, and associated tools/software. Manual annotation of 20 publications linked to 34 identifiers served as the gold standard. While promising, performance was lower on unseen examples than tuned ones, indicating prompt brittleness. Resource demands and evaluation complexity remain significant barriers to broader adoption.

## Method Summary
The method applies a zero-shot approach using Llama 3.1-405B (4-bit quantized) to classify data citation functions through a decision-tree prompting structure. For each publication-identifier pair, the model sequentially answers questions about whether the article includes genomics, whether authors access the dataset, what use cases apply, and what tools/software are used. Retrieval-Augmented Generation provides metadata context about identifiers from NCBI and JGI systems. The SARGO framework evaluates performance by aggregating redundant or granular machine outputs and comparing against manually annotated gold standards.

## Key Results
- Achieved F1 score of 0.674 on zero-shot classification of data citation functions
- Performance dropped from 0.805 to 0.674 between tuned and evaluation sets, indicating prompt brittleness
- Required 200+ GB GPU RAM with 4-bit quantization; full-precision model could not be instantiated
- Approximately 19.5% of outputs were false positives and 40-50% of relevant items were missed

## Why This Works (Mechanism)

### Mechanism 1: Decision-tree prompting structure
Sequential, conditional prompts improve classification accuracy on multi-label tasks with undefined label sets by chaining multiple prompts where earlier answers constrain downstream queries, reducing ambiguity at each step.

### Mechanism 2: Retrieval-Augmented Generation (RAG) for identifier context
Providing structured metadata about dataset identifiers improves the model's ability to recognize and reason about how those datasets are used by injecting domain-specific statements into prompts, grounding reasoning in accurate metadata rather than parametric knowledge.

### Mechanism 3: Large context window for whole-document reasoning
Full-publication context enables identification of distributed clues about data usage that would be missed by sentence-level approaches by ingesting entire publications and aggregating signals scattered across methods, results, and discussion sections.

## Foundational Learning

- Concept: Zero-shot classification
  - Why needed here: The paper explicitly avoids training data, relying on the model's pre-trained knowledge to classify without predefined categories or examples.
  - Quick check question: Can you explain why zero-shot performance is typically lower than fine-tuned performance, and what "prompt brittleness" means in this context?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG provides external metadata about dataset identifiers to ground model reasoning, compensating for gaps in parametric knowledge.
  - Quick check question: What are the tradeoffs between injecting metadata directly into prompts versus fine-tuning on domain-specific corpora?

- Concept: F1 score, precision, and recall for open-label tasks
  - Why needed here: The evaluation framework (SARGO) adapts these metrics for tasks without predefined label sets, handling granularity and redundancy issues.
  - Quick check question: How does the SARGO framework aggregate redundant or overly granular outputs to compute meaningful precision and recall?

## Architecture Onboarding

- Component map:
  Data Citation Explorer (DCE) -> Text preprocessing pipeline -> RAG context builder -> Decision-tree prompt controller -> LLM inference engine -> Evaluation framework (SARGO)

- Critical path:
  1. Retrieve publication-identifier pairs from DCE
  2. Preprocess full-text XML to plain text
  3. Construct RAG metadata statement for the identifier
  4. Execute decision-tree prompts sequentially for each pair
  5. Aggregate outputs and apply SARGO for evaluation against gold standard

- Design tradeoffs:
  - Chunking vs. whole-document ingestion: Chunking reduces per-pass context but loses cross-chunk dependencies; whole-document preserves context but risks attention dilution.
  - Local vs. commercial LLM: Local ensures reproducibility and avoids per-token costs but requires significant GPU infrastructure; commercial models may offer better recall but at high cost.
  - SARGO aggregation: Manual aggregation ensures quality but does not scale; full automation risks misalignment between machine and human granularity judgments.

- Failure signatures:
  - Prompt overfitting: F1 drops from 0.805 (initial set) to 0.674 (evaluation set) indicate prompts tuned to seen examples do not generalize.
  - Spitballing: Model generates redundant outputs that SARGO must aggregate.
  - Hallucination rate: ~19.5% (1 - precision) of outputs are false positives; ~40-50% of relevant items are missed (low recall).
  - Infrastructure bottleneck: 200+ GB GPU RAM required; full-precision model could not be instantiated even with 8×80GB H100s.

- First 3 experiments:
  1. Replicate the zero-shot pipeline on a small sample (5-10 publication-identifier pairs) using Llama 3.1-405B with the provided decision-tree prompts and RAG context; compute precision, recall, F1 using SARGO-style manual aggregation.
  2. Ablate RAG: Run the same sample without metadata statements and compare performance to quantify RAG's contribution to classification accuracy.
  3. Compare models: Substitute a smaller open-weight model (e.g., Llama 3.1-70B) or a commercial API (e.g., Claude Opus 4.1) on the evaluation set to measure performance vs. cost/infrastructure tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can automated prompt optimization frameworks (e.g., GEPA, MIPRO) mitigate the prompt brittleness and overfitting observed in manually tuned decision-tree structures?
- **Basis in paper:** [explicit] Page 13 states that recent innovations in automated prompt development "could help mitigate such difficulties, and the application of these methods to our workflow are grounds for future experiments."
- **Why unresolved:** The manual, iterative nature of the current prompting strategy resulted in a significant performance drop (F1 difference of 0.131) between the initial (tuned) set and the evaluation (unseen) set.
- **What evidence would resolve it:** A comparative study benchmarking the generalization performance (F1 on unseen data) of automatically optimized prompts versus the manually engineered decision tree.

### Open Question 2
- **Question:** Can an automated method be developed to selectively chunk publications to retain context while improving the retrieval of scattered data usage clues?
- **Basis in paper:** [explicit] Page 13 notes that the authors lack an automated means to determine which chunks are necessary, but "establishing such a method would constitute grounds for future exploration."
- **Why unresolved:** While chunking reduces context size (simplifying the "Needle-in-a-Haystack" problem), current stateless LLMs lose critical information when usage clues are scattered across multiple chunks.
- **What evidence would resolve it:** An algorithm capable of dynamically selecting relevant text segments that maintains recall parity with full-text processing while reducing computational token load.

### Open Question 3
- **Question:** Does the SARGO evaluation framework effectively generalize to other domains or open-vocabulary classification tasks beyond genomic data citation?
- **Basis in paper:** [inferred] Page 15 discusses evaluation complexity, noting that "other LLM-based classifiers in other contexts will likely require different or additional mechanisms," suggesting the current framework's applicability is uncertain.
- **Why unresolved:** SARGO was specifically designed to handle the "spitballing" and granularity issues of this specific task; it is unclear if the aggregation logic holds for different types of unstructured scientific text.
- **What evidence would resolve it:** Successful application of the SARGO methodology to a different domain (e.g., social sciences) demonstrating reliable alignment between aggregated machine outputs and human gold standards.

## Limitations
- Significant performance drop (F1 0.805 → 0.674) between tuned and evaluation sets indicates prompt brittleness
- Requires 200+ GB GPU RAM with 4-bit quantization; full-precision model could not be instantiated
- Manual SARGO aggregation step cannot scale to larger corpora and introduces subjective bias

## Confidence
- **High confidence**: The core finding that zero-shot LLM classification is feasible for data citation functions, achieving F1 ~0.67, is well-supported by manual annotation comparison and clearly documented.
- **Medium confidence**: The specific contribution of RAG metadata to performance improvement is supported by qualitative observations but lacks direct ablation studies.
- **Low confidence**: The scalability claims are based on a single small evaluation set (20 publications, 34 identifiers) and do not address performance degradation at corpus scale.

## Next Checks
1. **Ablation study on RAG contribution**: Run the complete pipeline on the evaluation set with identical prompts but without RAG metadata injection, then compare precision/recall/F1 scores to quantify the exact performance gain from context augmentation.

2. **Generalization test on unseen data**: Apply the same decision-tree prompts to a new, independent sample of publication-identifier pairs (not used in prompt tuning) and measure F1 score degradation to assess prompt brittleness across different genomic domains.

3. **Cost-effectiveness analysis**: Compare performance and infrastructure costs between the open-weight Llama 3.1-405B setup and commercial APIs (e.g., Claude Opus 4.1) on the same evaluation data, including both GPU rental costs and potential API token expenses for full corpus scaling.