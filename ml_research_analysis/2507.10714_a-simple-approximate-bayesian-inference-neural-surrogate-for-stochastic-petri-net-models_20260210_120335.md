---
ver: rpa2
title: A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri
  Net Models
arxiv_id: '2507.10714'
source_url: https://arxiv.org/abs/2507.10714
tags:
- petri
- parameters
- each
- parameter
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neural-surrogate framework for parameter
  estimation in Stochastic Petri Nets (SPNs) with covariate-dependent transition rates.
  The method learns to predict coefficients of functional rate models directly from
  noisy, partially observed token trajectories using a 1D Convolutional Residual Network
  trained on Gillespie-simulated SPN realizations.
---

# A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models

## Quick Facts
- **arXiv ID**: 2507.10714
- **Source URL**: https://arxiv.org/abs/2507.10714
- **Reference count**: 29
- **Primary result**: Neural surrogate recovers SPN rate coefficients from noisy trajectories with 0.043 RMSE in 26 minutes training time

## Executive Summary
This paper presents a neural-surrogate framework for parameter estimation in Stochastic Petri Nets (SPNs) with covariate-dependent transition rates. The method learns to predict coefficients of functional rate models directly from noisy, partially observed token trajectories using a 1D Convolutional Residual Network trained on Gillespie-simulated SPN realizations. Monte Carlo dropout provides uncertainty quantification. Experiments on a two-patch mosquito-borne disease model with 10% missing events achieve an RMSE of 0.043 for coefficient recovery, with full model training in 26 minutes and inference in 54.7 seconds. Uncertainty calibration via STD-scaling improves reliability, making the approach suitable for real-time inference in complex discrete-event systems.

## Method Summary
The framework trains a 1D Convolutional Residual Network to invert SPN dynamics by learning to predict coefficients of known functional rate models from token trajectories. The network takes simulated trajectories as input and outputs predicted coefficients, with Monte Carlo dropout providing approximate Bayesian uncertainty quantification. The method uses Gillespie simulation to generate training data with uniformly sampled coefficients, trains on 5000 trajectories with 10% missing events, and applies STD-scaling recalibration to improve uncertainty calibration. The approach enables fast inference (54.7 seconds) compared to traditional simulation-based methods while maintaining accuracy (0.043 RMSE).

## Key Results
- 1D Convolutional Residual Network achieves 0.043 RMSE for coefficient recovery from partially observed token trajectories
- Monte Carlo dropout provides calibrated uncertainty with coverage improving from 32.7% to 68% at 68% nominal level after STD-scaling
- Full model training completes in 26 minutes with inference time of 54.7 seconds
- Uncertainty calibration via STD-scaling reduces ENCE from 123.2% to 17.0% for parameter ι₁

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A 1D Convolutional Residual Network can learn to invert stochastic discrete-event dynamics and recover covariate-dependent rate coefficients from partially observed token trajectories.
- Mechanism: The network takes simulated token trajectories $X \in \mathbb{R}^{T \times d_{in}}$ as input and learns a mapping $F_w(x) \approx p(\theta|x)$, where $\theta$ represents the coefficients of known functional rate models. The 1D convolutions extract temporal features across timesteps, residual connections stabilize gradient flow, and global average pooling collapses temporal dimension before the final prediction head.
- Core assumption: The functional form of rate dependencies on covariates is known and fixed; only the coefficients need recovery. Trajectory dynamics encode sufficient information about these coefficients despite stochasticity and missing events.
- Evidence anchors: [abstract] "learns to invert system dynamics under realistic conditions of event dropout"; [section 3.3] Describes 1D-ResNet architecture with 128 filters, kernel size 5, 3 residual blocks

### Mechanism 2
- Claim: Monte Carlo dropout at inference time provides approximate Bayesian uncertainty quantification for recovered parameters.
- Mechanism: Dropout is kept active during inference. Each forward pass samples from an approximate variational posterior $q(w|D)$ over network weights. With $M=50$ stochastic passes, the predictive distribution $\hat{p}(\theta|x,D) \approx \frac{1}{M}\sum_{m=1}^{M} p(\theta|x,w^{(m)})$ is approximated, yielding predictive mean and variance.
- Core assumption: Dropout approximates a deep Gaussian process posterior; the variational approximation is sufficiently expressive to capture true posterior uncertainty.
- Evidence anchors: [abstract] "Monte Carlo dropout provides calibrated uncertainty bounds together with point estimates"; [section 3.4] Formal derivation of MC dropout as variational inference

### Mechanism 3
- Claim: STD-scaling recalibration corrects systematic miscalibration in uncertainty estimates without affecting point predictions.
- Mechanism: After observing that raw MC dropout uncertainty is miscalibrated (e.g., only 32.7% coverage at 68% nominal level for some parameters), a per-parameter scaling factor $s_i$ is learned on validation data by minimizing Gaussian NLL. The calibrated standard deviation $\bar{\hat{\sigma}}_i^{cal} = s_i \cdot \bar{\hat{\sigma}}_i$ adjusts intervals toward nominal coverage.
- Core assumption: Miscalibration is systematic and can be corrected by isotropic scaling; the relationship between predicted and empirical uncertainty is approximately linear.
- Evidence anchors: [abstract] "Uncertainty calibration via STD-scaling improves reliability"; [section 3.6] Describes ENCE metric and shows post-recalibration ENCE drops from 123.2% to 17.0% for $\iota_1$

## Foundational Learning

- Concept: **Stochastic Petri Nets (SPNs)**
  - Why needed here: SPNs are the forward model generating trajectories. Understanding places, transitions, tokens, and firing rules is essential to interpret what the neural network is inverting.
  - Quick check question: Can you explain why computing likelihoods in SPNs involves summing over exponentially many firing sequences?

- Concept: **Gillespie Algorithm / Stochastic Simulation**
  - Why needed here: Training data is generated via Gillespie simulation. Understanding how stochastic trajectories arise from transition rates clarifies what information is (and isn't) encoded in observations.
  - Quick check question: Why does the Gillespie algorithm produce exact samples from the continuous-time Markov chain defined by an SPN?

- Concept: **Bayesian Uncertainty Quantification (Epistemic vs. Aleatoric)**
  - Why needed here: MC dropout estimates epistemic (model) uncertainty. Distinguishing this from aleatoric (data) noise is critical for interpreting calibration results.
  - Quick check question: In this paper, what represents aleatoric uncertainty vs. epistemic uncertainty in the problem formulation?

## Architecture Onboarding

- Component map:
  - Forward simulator (Spike tool with Gillespie algorithm) -> Data pipeline (5000 trajectories, 80/10/10 split) -> 1D-ResNet (1×1 conv, 3 residual blocks, global average pooling) -> MC dropout inference (50 passes) -> STD-scaling recalibration

- Critical path:
  1. Define SPN structure and basis functions for covariate-dependent rates
  2. Generate training data via Gillespie simulation with sampled coefficients
  3. Train 1D-ResNet end-to-end with dropout regularization
  4. Run MC dropout inference for point estimates and uncertainty
  5. Recalibrate uncertainties via STD-scaling

- Design tradeoffs:
  - Dropout rate: Lower (0.1) gives lower RMSE but requires more recalibration; higher (0.2) increases uncertainty spread but may underfit. Tuned to 0.08 via Optuna.
  - Network depth: 3 residual blocks chosen for lightweight inference (54.7s); deeper networks may improve accuracy but increase training time.
  - Simulation budget: 5000 samples took ~9 hours; more samples may improve generalization but scale simulation cost linearly.

- Failure signatures:
  - Miscalibrated uncertainty: Pre-recalibration coverage far from nominal (e.g., 32.7% at 68% level). Diagnosed via coverage plots and ENCE metric.
  - High RMSE on specific parameters: γ₂ (0.067) and η₂ (0.061) show highest error, suggesting weaker identifiability from trajectory data.
  - Training instability: If dropout too high or learning rate too large, validation loss may not converge; monitor early stopping.

- First 3 experiments:
  1. Baseline replication: Train 1D-ResNet with p=0.1 dropout on two-patch SPN data. Measure RMSE and coverage before recalibration. Compare to paper's 0.043 RMSE.
  2. Ablation on missing events: Train and test with 0%, 5%, 10%, 20% missing events to characterize robustness. Expect degradation beyond 10%.
  3. Recalibration validation: Implement STD-scaling on held-out validation set. Verify ENCE drops post-recalibration and coverage approaches nominal levels across confidence thresholds (50%, 68%, 90%, 95%).

## Open Questions the Paper Calls Out

- **Question**: How does the neural surrogate's performance compare to traditional Approximate Bayesian Computation (ABC) or ODE-based inference in terms of accuracy and computational cost?
  - Basis in paper: [explicit] The authors state that future work will involve "formal benchmarking against existing simulation-based methods such as Approximate Bayesian Computation (ABC) or ODE-based inference approaches where feasible."
  - Why unresolved: The current study demonstrates feasibility on synthetic data but does not quantify the trade-offs (speed vs. precision) against established likelihood-free baselines.
  - What evidence would resolve it: A comparative study reporting RMSE and wall-clock time for the neural surrogate versus ABC on identical SPN inference tasks.

- **Question**: Can the framework generalize to significantly more complex SPN topologies and real-world epidemiological datasets?
  - Basis in paper: [explicit] The conclusion identifies "testing the model's generalization to more complex SPN topologies and real-world datasets" as a necessary next step.
  - Why unresolved: Experiments were restricted to a synthetic two-patch model; it is uncertain if the 1D-ResNet captures dynamics in larger, more interconnected networks or noisy empirical data.
  - What evidence would resolve it: Successful recovery of parameters in SPNs with >2 patches or application to a real-world disease outbreak dataset with known ground-truth parameters.

- **Question**: How robust is the inference accuracy when the fraction of missing events increases substantially beyond the 10% tested?
  - Basis in paper: [inferred] The method is tested on a specific condition of 10% missing events, but real-world systems often suffer from sparser observation.
  - Why unresolved: It is unclear if the "lightweight" architecture maintains low RMSE and calibration when the token trajectories are severely incomplete.
  - What evidence would resolve it: A sensitivity analysis measuring parameter recovery error (RMSE) and calibration (ENCE) across a gradient of missing event rates (e.g., 10% to 50%).

## Limitations

- The method is validated only on 10% missing events; performance may degrade significantly for higher dropout rates (>20%).
- The approach assumes known, fixed functional dependencies on covariates; misspecification of these forms would break the inversion mechanism.
- Training requires 5000 Gillespie simulations (~9 hours); for larger SPNs or longer trajectories, this cost may become prohibitive without model reduction techniques.

## Confidence

- **High confidence**: 
  - 1D-ResNet can learn to recover coefficients from token trajectories with 10% missing events (validated by 0.043 RMSE)
  - MC dropout provides approximate Bayesian uncertainty (formal derivation and coverage improvement documented)
  - STD-scaling improves calibration (post-recalibration ENCE drops from 123.2% to 17.0%)

- **Medium confidence**:
  - Method is "suitable for real-time inference" (inference time is fast at 54.7s, but training time is 26 minutes)
  - Method generalizes to other SPN structures (only tested on two-patch mosquito model)

- **Low confidence**:
  - Claims about robustness to higher missing rates (>10%) are speculative (not experimentally validated)
  - Claims about scalability to larger SPNs are speculative (only tested on 2-patch model)

## Next Checks

1. **Robustness to missing events**: Replicate experiments with 0%, 5%, 10%, 15%, 20% missing events to quantify performance degradation beyond the validated 10% regime.

2. **Functional form sensitivity**: Systematically test whether incorrect specification of rate functional forms (e.g., linear vs. exponential) breaks coefficient recovery, confirming the mechanism's dependency on correct model structure.

3. **Scaling validation**: Test the method on a larger SPN (e.g., 4-patch model or 3-species interaction) to verify whether the 1D-ResNet architecture and training approach scale without architectural modifications.