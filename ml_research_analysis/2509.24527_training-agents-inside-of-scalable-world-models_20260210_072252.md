---
ver: rpa2
title: Training Agents Inside of Scalable World Models
arxiv_id: '2509.24527'
source_url: https://arxiv.org/abs/2509.24527
tags:
- world
- arxiv
- dreamer
- actions
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dreamer 4 introduces a scalable agent that learns to solve complex
  control tasks by reinforcement learning inside a fast and accurate world model.
  The key innovation is a shortcut forcing objective combined with an efficient transformer
  architecture, enabling real-time interactive inference on a single GPU while accurately
  predicting complex object interactions in Minecraft.
---

# Training Agents Inside of Scalable World Models

## Quick Facts
- arXiv ID: 2509.24527
- Source URL: https://arxiv.org/abs/2509.24527
- Authors: Danijar Hafner; Wilson Yan; Timothy Lillicrap
- Reference count: 40
- Primary result: Achieves 29% success rate for crafting iron pickaxes and 0.7% for obtaining diamonds in Minecraft using only offline data

## Executive Summary
Dreamer 4 introduces a scalable agent architecture that learns to solve complex control tasks through reinforcement learning inside a fast and accurate world model. The key innovation combines a shortcut forcing objective with an efficient transformer architecture, enabling real-time interactive inference on a single GPU while accurately predicting complex object interactions in Minecraft. This approach allows the agent to train purely in imagination without online environment interaction, achieving strong performance on challenging tasks using 100× less data than previous methods.

## Method Summary
The method trains an agent within a learned world model using a combination of model-based reinforcement learning and transformer architectures. The world model predicts future observations, rewards, and values based on past observations and actions. A shortcut forcing objective accelerates training by providing additional supervision signals. The transformer architecture enables efficient processing of variable-length sequences while maintaining real-time inference capabilities. The agent learns purely from offline data, imagining trajectories within the world model rather than interacting with the actual environment during training.

## Key Results
- Achieves 29% success rate for crafting iron pickaxes in Minecraft using only offline data
- Obtains diamonds with 0.7% success rate, outperforming previous offline agents
- Demonstrates accurate simulation of diverse game mechanics including block placement, tool use, and monster combat
- Shows action conditioning can be learned from as little as 100 hours of action-labeled data out of 2500 total training hours

## Why This Works (Mechanism)
The approach works by creating a compressed yet accurate representation of the environment that enables fast simulation. The shortcut forcing objective provides stronger learning signals by directly optimizing for task-relevant outcomes rather than just prediction accuracy. The transformer architecture efficiently processes sequential data and captures long-range dependencies crucial for planning complex actions. By training entirely in imagination, the agent avoids the sample inefficiency and potential safety concerns of real-world exploration while still learning effective policies.

## Foundational Learning

**World Models** - Why needed: Enable fast simulation of environment dynamics without real interaction. Quick check: Compare prediction accuracy against ground truth across various time horizons.

**Transformer Architectures** - Why needed: Handle variable-length sequences and capture long-range dependencies essential for planning. Quick check: Measure inference latency and memory usage for different sequence lengths.

**Offline Reinforcement Learning** - Why needed: Learn from existing datasets without requiring new environment interactions. Quick check: Compare performance against online RL baselines with similar data budgets.

**Shortcut Forcing Objectives** - Why needed: Accelerate learning by providing direct task-relevant supervision. Quick check: Ablation study showing impact on convergence speed and final performance.

## Architecture Onboarding

**Component Map:**
World Model (Observation Encoder -> Transformer -> Prediction Heads) -> Agent (Policy Network -> Value Network) -> Imagination Rollouts -> Update World Model

**Critical Path:**
Observation sequence → World model prediction → Agent action selection → World model rollforward → Reward/value prediction → Policy update

**Design Tradeoffs:**
- Accuracy vs. inference speed in world model predictions
- Model complexity vs. generalization to unseen scenarios
- Offline data quality vs. diversity requirements
- Computational cost vs. planning horizon length

**Failure Signatures:**
- Degraded performance on tasks requiring precise timing or spatial reasoning
- Overfitting to training distribution when world model lacks sufficient diversity
- Planning failures when predictions become unreliable beyond short horizons
- Suboptimal policies when shortcut forcing objective dominates reward signal

**First Experiments:**
1. Test world model prediction accuracy on held-out trajectories from the training distribution
2. Evaluate agent performance on simplified versions of target tasks
3. Compare inference latency with different transformer configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused primarily on Minecraft with discrete interactions, raising questions about generalization to continuous environments
- Offline training may limit discovery of novel strategies beyond training corpus
- Reported success rates indicate substantial room for improvement, especially for complex diamond-obtaining tasks

## Confidence
- World model accurately simulates diverse game mechanics: High confidence
- Transformer architecture enables real-time interactive inference: High confidence
- Achieves results using only offline data with 100× less data: Medium confidence

## Next Checks
1. Test generalization to procedurally generated Minecraft worlds with unseen biome distributions and resource layouts
2. Conduct ablation studies isolating the impact of shortcut forcing objective versus transformer architecture
3. Evaluate world model prediction accuracy on long-horizon trajectories (beyond 100 steps) to assess temporal consistency