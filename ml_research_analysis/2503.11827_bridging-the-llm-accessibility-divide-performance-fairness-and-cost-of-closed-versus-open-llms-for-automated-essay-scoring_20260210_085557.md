---
ver: rpa2
title: Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed
  versus Open LLMs for Automated Essay Scoring
arxiv_id: '2503.11827'
source_url: https://arxiv.org/abs/2503.11827
tags:
- llms
- open
- closed
- gpt-4
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared nine large language models (LLMs) across automated
  essay scoring tasks to assess performance, fairness, and cost differences between
  closed, open, and open-source models. Researchers evaluated human-written essays
  using zero-shot and few-shot learning approaches, measuring mean squared error,
  correlation metrics, and demographic bias.
---

# Bridging the LLM Accessibility Divide? Performance, Fairness, and Cost of Closed versus Open LLMs for Automated Essay Scoring

## Quick Facts
- arXiv ID: 2503.11827
- Source URL: https://arxiv.org/abs/2503.11827
- Reference count: 10
- Open LLMs like Qwen2.5 and Llama 3 achieve comparable AES performance to GPT-4 with up to 37x lower costs

## Executive Summary
This study evaluates nine large language models (GPT-3.5/4/4o, Llama 2/3/3.1, Qwen2.5, DeepSeek-R1, OLMo 2) on automated essay scoring tasks using zero-shot and few-shot learning. The researchers found that open LLMs (Llama 3, Qwen2.5) perform comparably to closed models like GPT-4 in predictive accuracy (MSE, QWK) while offering significantly lower costs (15-37x). No significant fairness differences were found across race or age demographics. Open models also generated essays that received higher scores than human-written ones, challenging the dominance of closed LLMs while offering greater accessibility and transparency.

## Method Summary
The study used two essay corpora: ASAP (12,979 essays, 8 prompts) and FCE (2,466 essays, 5 genres). LLMs were evaluated using few-shot prompts (task instructions, rubric, 3 scored examples) and zero-shot generation. Performance was measured using MSE, MAE, QWK, and correlation metrics. Fairness was analyzed via ANOVA on scoring errors across demographic groups. Generated essays were analyzed using t-SNE on BERT embeddings. All models except OLMo 2 were accessed via APIs, with costs calculated based on token usage.

## Key Results
- Open LLMs (Qwen2.5, Llama 3) achieve comparable AES performance to GPT-4 with significantly lower costs
- No significant fairness differences found across race or age demographics
- Generated essays scored higher than human-written essays, suggesting potential bias in generation quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context learning with few-shot examples enables open LLMs to achieve AES performance comparable to GPT-4
- **Mechanism:** Few-shot conditioning with task instructions, rubric, and scored examples allows models to align scoring logic with human judgment
- **Core assumption:** Few-shot examples are representative of the essay distribution
- **Evidence anchors:** Qwen2.5 and Llama 3 outperformed GPT-4 on MSE and QWK respectively
- **Break condition:** Few-shot examples are unrepresentative or rubric is ambiguous

### Mechanism 2
- **Claim:** Open and closed LLMs show comparable fairness profiles due to error patterns not systematically correlated with demographics
- **Mechanism:** Clear rubrics and examples reduce spurious demographic correlations
- **Core assumption:** Demographic data is sufficient and statistical tests have adequate power
- **Evidence anchors:** ANOVA shows no significant interaction between model performance and race/age
- **Break condition:** Rubrics implicitly encode demographic biases or test set lacks diversity

### Mechanism 3
- **Claim:** Open LLMs provide cost-performance advantage through lower API pricing
- **Mechanism:** Market competition among API providers reduces per-token costs
- **Core assumption:** Third-party provider availability and latency are acceptable
- **Evidence anchors:** Llama 3 offers 37x cost efficiency over GPT-4
- **Break condition:** API pricing changes or local deployment required

## Foundational Learning

- **Concept:** In-Context Learning (Zero-shot & Few-shot)
  - **Why needed here:** Understanding prompt engineering is critical to interpreting results
  - **Quick check question:** Can you explain the difference between zero-shot and few-shot prompts for essay scoring?

- **Concept:** Automated Essay Scoring (AES) & Evaluation Metrics
  - **Why needed here:** Familiarity with MSE, QWK, and correlation metrics is necessary to interpret performance claims
  - **Quick check question:** If a model has lower MSE but lower QWK than another, what might this imply about error types?

- **Concept:** Statistical Fairness (ANOVA & Disparate Impact)
  - **Why needed here:** Understanding ANOVA results is key to interpreting fairness claims
  - **Quick check question:** What would a significant interaction effect between "assessment model" and "race" imply?

## Architecture Onboarding

- **Component map:** ASAP/FCE corpora -> LLM Assessors (APIs/local) -> Prompting Module -> Analysis Layer (Metrics + Fairness + Cost)
- **Critical path:** Ingest human essays -> Construct few-shot prompts -> Call LLM APIs -> Collect scores -> Compute metrics -> Run fairness ANOVA -> Calculate costs
- **Design tradeoffs:**
  - API vs. Local: APIs ensure fair cost comparison but introduce dependency; local offers control but hardware costs
  - Few-shot Example Selection: Random selection from tertiles is heuristic; sophisticated methods might yield different results
  - Model Selection: Current snapshot; newer models may obsolete findings
- **Failure signatures:**
  - Scores outside valid range: Refine prompt constraints
  - API failures: Implement retries and logging
  - Cost overruns: Optimize prompt verbosity
  - Metric instability: Run multiple trials with different seeds
- **First 3 experiments:**
  1. Reproduce performance parity: Run Llama 3 and GPT-4 few-shot assessment on 2 ASAP prompts, verify MSE/QWK
  2. Test prompt sensitivity: Vary prompt structure for one model/prompt pair, measure impact on consistency
  3. Audit generated essays: Generate essays for new prompt, have human raters score alongside human essays

## Open Questions the Paper Calls Out
- Generalizability to text generation tasks beyond essay scoring
- Comparison of security vulnerabilities and adversarial attack susceptibility
- Whether open model weights enable practical improvements in explainability and bias mitigation

## Limitations
- Findings may not generalize beyond specific essay corpora and prompts tested
- Fairness analysis constrained by limited demographic metadata in FCE dataset
- Cost comparisons assume stable API pricing and acceptable latency

## Confidence
- **High Confidence:** Cost-performance comparison showing 15-37x cost advantage for open LLMs
- **Medium Confidence:** Fairness analysis showing no detectable demographic bias (limited by data)
- **Medium Confidence:** Generative quality findings showing higher LLM scores than human essays

## Next Checks
1. Replicate performance parity using GitHub repository for Llama 3 and GPT-4 on 2 ASAP prompts
2. Test prompt sensitivity by systematically varying prompt structure and measuring scoring consistency
3. Audit generated essays by having human raters score new LLM-generated essays alongside human-written ones on the same topic