---
ver: rpa2
title: 'The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language
  Models'
arxiv_id: '2510.02230'
source_url: https://arxiv.org/abs/2510.02230
tags:
- training
- learning
- rlvr
- problems
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the "reasoning boundary paradox" in Reinforcement
  Learning with Verifiable Rewards (RLVR), where RLVR training can actually shrink
  the set of problems a language model can solve rather than expand it. The authors
  identify two key phenomena: negative interference, where learning to solve certain
  problems reduces the likelihood of correct solutions for others, and a winner-take-all
  effect, where RLVR disproportionately reinforces problems with high initial success
  rates while suppressing low-likelihood ones.'
---

# The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models

## Quick Facts
- arXiv ID: 2510.02230
- Source URL: https://arxiv.org/abs/2510.02230
- Reference count: 40
- Key outcome: RLVR training can shrink solution coverage, with Pass@k declining despite Pass@1 improvements

## Executive Summary
This paper investigates the "reasoning boundary paradox" in Reinforcement Learning with Verifiable Rewards (RLVR), where RLVR training can actually shrink the set of problems a language model can solve rather than expand it. The authors identify two key phenomena: negative interference, where learning to solve certain problems reduces the likelihood of correct solutions for others, and a winner-take-all effect, where RLVR disproportionately reinforces problems with high initial success rates while suppressing low-likelihood ones. Through theoretical and empirical analysis across multiple mathematical reasoning benchmarks, they show these effects arise from on-policy sampling in standard RL objectives, causing models to converge toward narrow solution strategies.

## Method Summary
The authors propose SELF (Selective Examples with Low-likelihood and Forward-KL), a data curation algorithm that focuses RLVR learning on low-likelihood problems. SELF works by filtering out training problems where the greedy response from the base model is correct, then applying Forward KL regularization instead of the standard Reverse KL. This approach maintains computational efficiency while mitigating coverage shrinkage. The method is evaluated on Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and Llama-3.2-3B-Instruct across multiple mathematical reasoning benchmarks including DeepScaleR, Math, and MinTest.

## Key Results
- RLVR training causes Pass@k to decline below base model performance while Pass@1 improves
- SELF significantly improves Pass@k performance compared to standard GRPO, particularly for larger sampling budgets
- Forward KL regularization preserves solution diversity better than Reverse KL
- Negative interference increases over training steps, measured by decreasing ∆+ values

## Why This Works (Mechanism)

### Mechanism 1: Negative Interference from Shared Parameters
- Claim: Learning to solve one training problem reduces the likelihood of correct solutions for others, causing Pass@k decline.
- Mechanism: Gradient updates from one problem-solution pair affect others through the kernel K_t(x, x', y, y') = ∇θ log π(y|x)⊤ ∇θ log π(y'|x'). When gradients correlate across examples, updates intended for one problem inadvertently suppress correct solutions elsewhere.
- Core assumption: Parameter sharing in LLMs creates non-diagonal kernel matrices where off-diagonal influences are non-negligible.
- Evidence anchors:
  - [abstract] "negative interference, where learning to solve certain problems reduces the likelihood of correct solutions for others"
  - [Section 4.2] "∆+(πθt, μ) measures the direction of the relative influence... a negative ∆logπθt(y+|x) corresponds to negative interference"
  - [corpus] Limited direct corroboration; related work "The Invisible Leash" discusses RLVR boundary constraints but doesn't confirm interference specifically
- Break condition: If gradient orthogonality across problem types increases (e.g., through architectural separation), interference diminishes.

### Mechanism 2: Winner-Take-All from On-Policy Sampling
- Claim: On-policy RL objectives disproportionately reinforce high-likelihood responses regardless of correctness, suppressing exploration of low-likelihood correct solutions.
- Mechanism: The REINFORCE gradient ∇θL = A(y)πθ(y)∇θ log πθ(y) includes an explicit πθ(y) scaling factor. High-likelihood tokens dominate updates; when πθ(y) ≈ 0, gradients vanish, creating saddle points where zero-probability regions cannot be updated.
- Core assumption: The policy distribution is non-uniform (peaky) at initialization, typical for pretrained LLMs.
- Evidence anchors:
  - [abstract] "winner-take-all phenomenon: RLVR disproportionately reinforces problems with