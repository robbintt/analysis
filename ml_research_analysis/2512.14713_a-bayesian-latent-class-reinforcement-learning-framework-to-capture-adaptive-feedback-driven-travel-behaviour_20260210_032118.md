---
ver: rpa2
title: A Bayesian latent class reinforcement learning framework to capture adaptive,
  feedback-driven travel behaviour
arxiv_id: '2512.14713'
source_url: https://arxiv.org/abs/2512.14713
tags:
- learning
- route
- class
- choice
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of capturing dynamic travel
  behavior by introducing a Latent Class Reinforcement Learning (LCRL) model that
  integrates econometric latent class structures with reinforcement learning dynamics.
  The LCRL model allows for discrete representation of unobserved heterogeneity in
  learning types (e.g., exploratory vs.
---

# A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour

## Quick Facts
- arXiv ID: 2512.14713
- Source URL: https://arxiv.org/abs/2512.14713
- Reference count: 10
- Three distinct behavioral classes identified: context-dependent preferences with exploitative tendencies (22%), persistent exploitative tendencies regardless of context (49%), and exploratory strategy with context-specific preferences (29%)

## Executive Summary
This study introduces a Latent Class Reinforcement Learning (LCRL) model that integrates econometric latent class structures with reinforcement learning dynamics to capture how travel preferences evolve through experience. The model identifies discrete behavioral types while simultaneously modeling preference evolution, providing a more nuanced understanding of adaptive travel decision-making than traditional approaches. Using a driving simulator dataset with 83 participants making 1,660 choices, the three-class LCRL significantly outperforms benchmark models in capturing feedback-driven travel behavior.

## Method Summary
The LCRL model combines latent class analysis with reinforcement learning to represent unobserved heterogeneity in learning types while modeling preference evolution through experience-based feedback. Variational Bayes estimation is employed to handle the computational complexity of the model. The framework allows for discrete representation of behavioral classes (e.g., exploratory vs. exploitative) while capturing how individuals update their expectations and preferences based on travel outcomes.

## Key Results
- Three-class LCRL model achieves log-likelihood of -803.51 versus -962.91 for benchmark model
- AIC of 1,667.02 versus 1,937.82, and BIC of 1,829.46 versus 1,970.31 demonstrate superior model fit
- Three distinct behavioral classes identified: context-dependent preferences with exploitative tendencies (22%), persistent exploitative tendencies regardless of context (49%), and exploratory strategy with context-specific preferences (29%)

## Why This Works (Mechanism)
The LCRL framework succeeds by integrating two complementary approaches: latent class analysis captures discrete heterogeneity in behavioral types while reinforcement learning models continuous preference evolution through feedback. This dual structure allows the model to represent both the categorical differences in how people learn (exploratory vs. exploitative) and the continuous updating of preferences based on experience. The Bayesian estimation approach enables proper uncertainty quantification while handling the computational complexity of the combined model structure.

## Foundational Learning
- **Latent Class Analysis**: Used to identify unobserved subgroups in the population; needed to capture discrete heterogeneity in learning behaviors that standard continuous models miss
- **Reinforcement Learning**: Models how preferences evolve through feedback; needed to capture dynamic adaptation that static choice models cannot represent
- **Variational Bayes Estimation**: Computational approximation method for complex Bayesian models; needed to make inference tractable for the high-dimensional LCRL model
- **Log-likelihood, AIC, BIC**: Model fit and comparison metrics; needed to demonstrate superiority of the LCRL approach over benchmarks
- **Exploratory vs. Exploitative Learning**: Behavioral distinction in decision-making strategies; needed to characterize the fundamental types of adaptive behavior in travel choices

## Architecture Onboarding
- **Component Map**: Data Collection -> Choice Modeling -> Latent Class Identification -> Preference Evolution -> Model Validation
- **Critical Path**: The integration of latent class structure with reinforcement learning dynamics is the core innovation, allowing simultaneous representation of discrete behavioral types and continuous preference updating
- **Design Tradeoffs**: Discrete class representation provides interpretability but may oversimplify behavioral heterogeneity; reinforcement learning captures dynamics but adds computational complexity
- **Failure Signatures**: Poor class separation would manifest as overlapping preference parameters across classes; inadequate model fit would appear as minimal improvement in likelihood metrics over benchmarks
- **First Experiments**: 1) Test different numbers of latent classes to assess stability of three-class solution, 2) Compare model performance across different travel scenarios, 3) Validate class identification using holdout samples

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 83 participants may limit generalizability of latent class identification
- Laboratory driving simulator context may not fully capture real-world travel behavior complexity
- Three-class solution may oversimplify the spectrum of adaptive behaviors
- Focus on single travel scenario type (highway route choice) limits applicability to other contexts

## Confidence
- Model performance superiority (High): Substantial and statistically meaningful improvements in likelihood, AIC, and BIC
- Class identification (Medium): Well-supported by model fit but may not fully capture behavioral heterogeneity
- Behavioral interpretation (Medium): Plausible characterization but requires external validation
- Generalization to real-world settings (Low): Simulator data may not fully represent natural travel behavior

## Next Checks
1. Replicate the analysis with field data from real travel choices to test external validity
2. Conduct sensitivity analysis varying the number of latent classes to assess stability of the three-class solution
3. Validate the model with cross-validation or holdout samples to ensure robustness of class identification and parameter estimates