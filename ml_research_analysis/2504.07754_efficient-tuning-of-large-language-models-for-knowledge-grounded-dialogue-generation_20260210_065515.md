---
ver: rpa2
title: Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation
arxiv_id: '2504.07754'
source_url: https://arxiv.org/abs/2504.07754
tags:
- knowledge
- kedit
- dialogue
- evaluation
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KEDiT, a method for efficiently tuning large
  language models (LLMs) for knowledge-grounded dialogue generation. KEDiT compresses
  retrieved knowledge into learnable vectors using an information bottleneck and integrates
  them via a lightweight knowledge-aware adapter that fine-tunes less than 2% of LLM
  parameters.
---

# Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation

## Quick Facts
- **arXiv ID:** 2504.07754
- **Source URL:** https://arxiv.org/abs/2504.07754
- **Reference count:** 40
- **Primary result:** Introduces KEDiT, compressing retrieved knowledge into learnable vectors for efficient LLM tuning, achieving strong performance on Wizard of Wikipedia (F1: 22.45 seen, 21.05 unseen) and PubMed-Dialog (F1: 38.63).

## Executive Summary
This paper presents KEDiT, a method for efficiently tuning large language models for knowledge-grounded dialogue generation. KEDiT compresses retrieved knowledge into learnable vectors using an information bottleneck and integrates them via a lightweight knowledge-aware adapter that fine-tunes less than 2% of LLM parameters. Experiments on Wizard of Wikipedia and a newly constructed PubMed-Dialog dataset show that KEDiT outperforms competitive baselines across automatic, LLM-based, and human evaluations, achieving F1 scores of 22.45 (seen) and 21.05 (unseen) on Wizard of Wikipedia, and 38.63 on PubMed-Dialog. The method demonstrates superior performance in generating contextually relevant and informative responses while maintaining computational efficiency, and shows strong generalization across different LLM architectures.

## Method Summary
KEDiT uses a two-phase training approach to efficiently tune LLMs for knowledge-grounded dialogue. Phase 1 trains a knowledge compression module using an information bottleneck: BERT and a Q-Former encode knowledge passages into $m=16$ compressed vectors $Z$, with an auxiliary reconstruction task (LLM reconstructs original text from $Z$) plus alignment loss to optimize information content. Phase 2 freezes the compression module and trains a lightweight knowledge-aware adapter (KA-Adapter) that gates both attention and feed-forward networks, with less than 2% of LLM parameters tuned. The compressed vectors are prepended to dialogue context, allowing the adapter to integrate external knowledge while preserving the LLM's generation capabilities. The method uses TF-IDF retrieval for knowledge selection and demonstrates strong performance on both general and domain-specific dialogue datasets.

## Key Results
- KEDiT achieves F1 scores of 22.45 (seen) and 21.05 (unseen) on Wizard of Wikipedia, outperforming competitive baselines including full fine-tuning and adapter-based methods.
- On the newly constructed PubMed-Dialog dataset, KEDiT achieves an F1 score of 38.63, demonstrating strong performance on domain-specific medical dialogues.
- Human evaluation shows KEDiT generates more informative and contextually relevant responses compared to baselines, with strong generalization across different LLM architectures.

## Why This Works (Mechanism)
KEDiT's effectiveness stems from its efficient knowledge integration strategy. The information bottleneck compresses retrieved knowledge into compact, learnable vectors that capture essential information while filtering noise. By using an auxiliary reconstruction task during compression training, the method ensures the compressed representation retains sufficient information content. The lightweight KA-Adapter architecture allows the model to learn how to effectively integrate external knowledge without modifying the vast majority of LLM parameters, preserving the model's pre-trained capabilities while adding knowledge-grounded generation ability. The gating mechanism in the adapter enables selective knowledge incorporation based on dialogue context, preventing knowledge overload while maintaining relevance.

## Foundational Learning

**Information Bottleneck Principle**: Compressing input while preserving task-relevant information to improve generalization. Why needed: Enables efficient knowledge representation without overwhelming the LLM. Quick check: Verify compressed vectors retain reconstruction capability while reducing dimensionality.

**Adapter Architecture**: Lightweight neural modules inserted into pre-trained models to add task-specific capabilities with minimal parameter updates. Why needed: Allows knowledge integration without expensive full fine-tuning. Quick check: Confirm adapter parameters are << 2% of total LLM parameters.

**Knowledge-Grounded Dialogue**: Dialogue systems that incorporate external knowledge sources to generate informative, context-aware responses. Why needed: Enables generation of factually accurate and relevant dialogue responses. Quick check: Ensure retrieved knowledge is relevant to dialogue context before compression.

## Architecture Onboarding

**Component Map**: Wikipedia chunks → TF-IDF Retriever → BERT + Q-Former → Compressed vectors $Z$ (m=16) → KA-Adapter → Llama-3-8B → Generated response

**Critical Path**: Knowledge retrieval → Compression (Phase 1) → Adapter training (Phase 2) → Response generation. The compression module must be trained first to create effective knowledge representations, then the adapter learns to integrate these representations.

**Design Tradeoffs**: The method trades some knowledge fidelity (compression to 16 vectors) for computational efficiency and parameter efficiency. Using frozen components (BERT, Q-Former, LLM) reduces training cost but limits end-to-end optimization. The TF-IDF retriever is computationally cheap but may miss semantic relevance compared to neural retrievers.

**Failure Signatures**: 
- GPU OOM during Phase 1: Reconstruction task requires full LLM forward passes
- Degenerate responses: Gating scalars too large/small or alignment loss insufficient
- Poor knowledge integration: Compressed vectors don't align with LLM hidden states

**First Experiments**:
1. Train knowledge compression module on Wikipedia chunks and verify reconstruction quality
2. Test TF-IDF retrieval on WoW dialogue contexts to ensure relevant knowledge selection
3. Train KA-Adapter on WoW dataset and evaluate response quality with different knowledge relevance levels

## Open Questions the Paper Calls Out

**Multimodal Knowledge Integration**: How can KEDiT be extended to effectively compress and integrate multimodal knowledge (e.g., images, tables) into dialogue generation? The current architecture relies solely on textual knowledge compression, but future work will explore multimodal encoders like CLIP to handle diverse knowledge sources and improve adaptability.

**Dynamic Knowledge Updates**: Can the knowledge bottleneck be adapted to support real-time knowledge updates without expensive re-training? The current two-stage training requires optimizing the compression module on specific knowledge datasets, which is static and computationally intensive. A mechanism for incremental updates to compressed vectors as new documents are added would enhance practical utility.

**Unseen Domain Generalization**: What architectural or training modifications are required to close the performance gap between seen and unseen domains? While KEDiT generalizes better than some baselines, performance drops on unfamiliar topics suggest the adapter may overfit to training topics. Enhanced techniques for better domain generalization are needed.

**Retrieval-Mechanism Joint Training**: Would end-to-end joint training of retrieval and compression modules yield better performance than the current frozen-retriever setup? The paper uses a frozen TF-IDF/Contriever retriever, but gradient-based feedback from the KEDiT adapter to the retriever could improve the relevance of compressed vectors and final response quality.

## Limitations
- The method requires pre-training the knowledge compression module on large-scale Wikipedia data, adding computational overhead before fine-tuning.
- TF-IDF retrieval may not capture semantic relevance as effectively as neural retrievers, potentially limiting performance in complex dialogue scenarios.
- While the adapter is lightweight, the full pipeline including the knowledge compression module and multiple forward passes during training may still be computationally intensive for resource-constrained deployment.

## Confidence
- **High confidence**: Experimental results on Wizard of Wikipedia and PubMed-Dialog showing KEDiT's superior performance across multiple evaluation metrics
- **Medium confidence**: Claims about computational efficiency and parameter efficiency based on relative comparisons but limited absolute cost analysis
- **Medium confidence**: Generalization claims across different LLM architectures based on experiments primarily with Llama-3-8B

## Next Checks
1. **Verify the knowledge compression module training**: Implement and validate Phase 1 training on Wikipedia chunks, ensuring the information bottleneck properly compresses knowledge into the 16-vector representation while maintaining reconstruction capability.
2. **Test retrieval quality impact**: Compare KEDiT's performance using different retrieval methods (TF-IDF vs. semantic retrieval) to quantify the impact of retrieval quality on final dialogue generation performance.
3. **Evaluate adapter parameter efficiency**: Measure the actual memory and computational overhead during inference, comparing KEDiT's runtime efficiency against full fine-tuning and other adapter-based methods on identical hardware.