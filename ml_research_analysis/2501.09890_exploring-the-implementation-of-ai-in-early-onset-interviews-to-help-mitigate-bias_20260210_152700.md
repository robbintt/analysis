---
ver: rpa2
title: Exploring the Implementation of AI in Early Onset Interviews to Help Mitigate
  Bias
arxiv_id: '2501.09890'
source_url: https://arxiv.org/abs/2501.09890
tags:
- page
- https
- negative
- bias
- however
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel AI-driven interview system designed
  to reduce sentiment bias in early-stage recruitment by focusing on candidate knowledge
  and skills rather than emotional tone. The study combines a custom-built conversational
  AI bot with human evaluation by experienced HR managers to assess both technical
  competence and bias in hiring.
---

# Exploring the Implementation of AI in Early Onset Interviews to Help Mitigate Bias

## Quick Facts
- arXiv ID: 2501.09890
- Source URL: https://arxiv.org/abs/2501.09890
- Authors: Nishka Lal; Omar Benkraouda
- Reference count: 0
- One-line result: AI-driven interview system reduces sentiment bias by 41.2% compared to human raters in early recruitment.

## Executive Summary
This paper introduces an AI-driven conversational interview system designed to reduce sentiment bias in early-stage recruitment by focusing on candidate knowledge and skills rather than emotional tone. The system uses speech-to-text transcription, an LLM for evaluation against a structured rubric, and sentiment analysis to isolate bias. Results show human raters favored candidates with positive sentiment by an average of 0.62 points and penalized those with negative sentiment by 1.28 points, revealing a significant 2.06-point sentiment bias gap. In contrast, the AI bot exhibited far less sensitivity to sentiment, maintaining consistent ratings based on skill and knowledge alone. This demonstrates that AI can effectively mitigate sentiment-driven biases, offering a fairer and more objective evaluation process for early recruitment interviews, particularly in technical roles.

## Method Summary
The system uses a FastAPI backend with three endpoints: `/talk` for audio upload and conversation, `/analyze` for sentiment scoring, and `/clear` for conversation reset. Audio is transcribed via OpenAI's Whisper API, then evaluated by ChatGPT using a 5-point knowledge rubric embedded in the prompt. Eleven Labs converts responses to audio. Sentiment is analyzed separately using TextBlob. The study tested 10 synthetic interviews with varied knowledge levels and sentiment, rated by two HR managers with 10+ years experience from TechMahindra.

## Key Results
- Human raters showed a 2.06-point sentiment bias gap between positive and negative sentiment candidates at the same knowledge level
- AI system maintained consistent ratings regardless of sentiment, reducing bias by 41.2%
- AI ratings correlated with knowledge level while human ratings showed systematic sentiment bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio-to-text transcription removes non-verbal bias carriers before evaluation
- Mechanism: The system uses OpenAI's Whisper API to transcribe candidate audio into text, stripping away vocal tone, pace, and affect that humans unconsciously weight
- Core assumption: Text-based evaluation is less susceptible to sentiment bias than multimodal evaluation
- Evidence anchors: [abstract] emphasizes skill and knowledge over emotional sentiments; [section 4.2] describes audio-to-text conversion; [corpus] "Addressing Bias in LLMs" discusses text-based interventions

### Mechanism 2
- Claim: Explicit rating rubrics constrain evaluator drift toward sentiment-based judgments
- Mechanism: The system embeds a 5-point knowledge scale with concrete behavioral descriptors directly into the LLM prompt context
- Core assumption: LLMs follow rubric constraints when explicitly specified in context
- Evidence anchors: [section 4.5] provides detailed rubric definitions; [section 4.5] explains parameter input; [corpus] lacks direct validation of rubric-based LLM evaluation

### Mechanism 3
- Claim: Separating sentiment measurement from competence scoring enables bias isolation
- Mechanism: The `/analyze` endpoint uses TextBlob for polarity scoring independently from the rating process
- Core assumption: Sentiment polarity scores from TextBlob are sufficiently accurate for bias detection
- Evidence anchors: [section 4.4] describes TextBlob utilization; [section 5.1] shows AI ratings correlated with knowledge level; [corpus] "Modeling Fairness in Recruitment AI" addresses fairness but doesn't validate sentiment-analysis-as-debiasing

## Foundational Learning

- **Concept**: Sentiment Analysis Polarity Scoring
  - Why needed here: TextBlob returns polarity scores (-1 to +1); understanding this metric is essential for interpreting the bias gap calculations
  - Quick check question: If a candidate says "I unfortunately failed but learned a lot," would TextBlob likely score this as positive, negative, or near-zero polarity?

- **Concept**: Prompt Engineering for Constrained Evaluation
  - Why needed here: The system relies on embedding rubric definitions in the LLM context to control rating behavior
  - Quick check question: What happens if you provide a rubric but don't explicitly instruct the LLM to "rate using only these criteria"?

- **Concept**: Human-AI Rating Comparison Methodology
  - Why needed here: The paper's core claim rests on comparing AI vs human ratings across sentiment conditions to isolate bias
  - Quick check question: Why is it necessary to control knowledge level while varying sentiment when testing for bias?

## Architecture Onboarding

- **Component map**: Wix website (Frontend) -> FastAPI (API Layer) -> Whisper API (Speech-to-Text) -> ChatGPT (Reasoning/Response) -> Eleven Labs API (Text-to-Speech) -> TextBlob (Sentiment Module)
- **Critical path**: User uploads audio → `/talk` endpoint receives and saves file → Whisper transcribes audio → text → Text + conversation history sent to ChatGPT → generates response + tracks rating → Response converted to audio via Eleven Labs → streamed back → At interview end, `/analyze` scores sentiment; rating extracted from LLM
- **Design tradeoffs**: TextBlob vs. transformer-based sentiment (simpler but less nuanced); Single LLM for both conversation and evaluation (simplifies architecture but risks influence); Small sample size (n=10) limits statistical power
- **Failure signatures**: LLM returns ratings outside 1-5 range → rubric not being followed; AI ratings correlate strongly with sentiment polarity → sentiment isolation failed; Transcription errors from Whisper → downstream evaluation on wrong content; Conversation history not clearing between candidates → cross-contamination
- **First 3 experiments**: 1) Run 20 synthetic interviews with identical content but varying sentiment; verify AI ratings remain stable (std dev < 0.3); 2) Compare AI ratings when sentiment is explicitly mentioned vs. absent; confirm no systematic difference; 3) Expand human rater pool to 10+ raters; compute inter-rater reliability to establish whether the 2.06-point gap is consistent

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the AI model be expanded to assess emotional intelligence and soft skills without reintroducing the biases the current system avoids? Basis: [explicit] Abstract notes future work could expand to assess emotional skills; Section 5.4 suggests minimizing bias in emotional aspects. Why unresolved: Current system was designed to ignore emotional sentiment in favor of technical knowledge. What evidence would resolve it: Study testing updated model on roles requiring high emotional intelligence, measuring correlation between sentiment and ratings.

- **Open Question 2**: Can this AI framework effectively minimize demographic biases, such as gender or race bias, in addition to sentiment bias? Basis: [explicit] Section 5.4 explicitly calls for researchers to minimize gender or race bias using the system's functionalities. Why unresolved: Study focused strictly on sentiment and used synthetic voice to control for auditory bias. What evidence would resolve it: Trials using diverse human candidates to see if AI's ratings diverge from human raters based on demographic markers.

- **Open Question 3**: Do the AI's bias mitigation capabilities remain consistent when evaluated by a larger, more diverse pool of human raters? Basis: [explicit] Section 5.3 states using more human interviewers might strengthen results and notes limitation to only two raters from one company. Why unresolved: "Significant" 2.06-point gap was derived from very small sample of evaluators. What evidence would resolve it: Replication with statistically significant number of HR managers from various industries to confirm human bias baseline.

## Limitations
- Small sample size (n=10 interviews, 2 human raters) limits statistical power and generalizability
- Tested only on technical knowledge questions, unclear if bias mitigation generalizes to non-technical roles
- Single sentiment detection method (TextBlob) may miss nuanced sentiment expression
- Human rater calibration unmeasured with only two evaluators

## Confidence
- **High Confidence**: Architectural claim that separating transcription from evaluation can reduce multimodal bias is well-supported
- **Medium Confidence**: Quantitative claim of "41.2% reduction in sentiment bias" is plausible but requires larger sample validation
- **Low Confidence**: Claim that approach generalizes to non-technical roles or different types of bias is speculative without additional testing

## Next Checks
1. Scale replication test: Re-run with 50+ real candidates across technical and non-technical roles; verify sentiment bias gap persists and measure inter-rater reliability among 5+ human evaluators
2. Sentiment detection robustness check: Replace TextBlob with transformer-based analyzer (e.g., VADER or BERT-based); confirm AI's relative insensitivity to sentiment remains consistent
3. Generalization to emotional skills: Adapt system to evaluate communication and interpersonal skills in customer service or management roles; test whether bias mitigation holds when sentiment is part of evaluation criteria