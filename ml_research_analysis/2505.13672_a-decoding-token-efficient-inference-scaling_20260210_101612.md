---
ver: rpa2
title: 'A*-Decoding: Token-Efficient Inference Scaling'
arxiv_id: '2505.13672'
source_url: https://arxiv.org/abs/2505.13672
tags:
- search
- decoding
- wang
- inference
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes A-Decoding, a search-based inference-time strategy
  that builds on the A search algorithm to optimally utilize a fixed compute budget
  by prioritizing high-quality reasoning paths during language model generation. The
  method frames language model decoding as a structured search in a state space of
  partial solutions, applying the A transition model to identify promising continuations
  guided by an external process supervision signal.
---

# A*-Decoding: Token-Efficient Inference Scaling

## Quick Facts
- arXiv ID: 2505.13672
- Source URL: https://arxiv.org/abs/2505.13672
- Reference count: 40
- Primary result: A*-Decoding achieves competitive accuracy on MATH500 and AIME 2024 benchmarks while generating up to 3x fewer tokens and 30% fewer inference passes compared to baselines like best-of-N, self-consistency, and particle filtering

## Executive Summary
This paper proposes A*-Decoding, a search-based inference-time strategy that builds on the A* search algorithm to optimally utilize a fixed compute budget by prioritizing high-quality reasoning paths during language model generation. The method frames language model decoding as a structured search in a state space of partial solutions, applying the A* transition model to identify promising continuations guided by an external process supervision signal. Experiments on the MATH500 and AIME 2024 benchmarks show that A*-decoding achieves competitive exact-match accuracy compared to strong inference-time baselines while generating significantly fewer tokens and inference passes under equivalent compute budgets.

The results demonstrate how structured search in decoding can enhance reasoning efficiency in small language models, offering an alternative to brute-force sampling or scale-driven gains. On these benchmarks, A*-decoding enables a 1B-parameter Llama model to match the performance of a 70B-parameter model and allows a 1.7B-parameter Qwen model to reach OpenAI o1-like reasoning accuracy, suggesting that search-based inference strategies can effectively bridge the gap between model scale and reasoning performance.

## Method Summary
A*-Decoding treats language model decoding as a structured search problem, where each state represents a partial solution to a reasoning task. The method employs the A* algorithm to explore this state space, using a heuristic that combines the model's likelihood predictions with an external process supervision signal. This signal provides feedback on the quality of intermediate reasoning steps, allowing the search to prioritize promising paths while pruning unpromising ones early. By maintaining a priority queue of partial solutions ordered by their estimated total cost (likelihood plus heuristic), A*-Decoding can efficiently allocate a fixed compute budget to explore the most promising reasoning trajectories. The approach is designed to generate fewer tokens and inference passes while maintaining or improving accuracy compared to sampling-based baselines like best-of-N and self-consistency.

## Key Results
- A*-Decoding achieves competitive exact-match accuracy on MATH500 and AIME 2024 benchmarks compared to strong inference-time baselines
- The method generates up to 3x fewer tokens and 30% fewer inference passes under equivalent compute budgets
- A 1B-parameter Llama model matches the performance of a 70B-parameter model, and a 1.7B-parameter Qwen model reaches OpenAI o1-like reasoning accuracy on the evaluated benchmarks

## Why This Works (Mechanism)
A*-Decoding works by reframing language model decoding as a structured search problem over the space of partial solutions. The A* algorithm is well-suited for this task because it efficiently explores large state spaces by maintaining a priority queue of candidate solutions ordered by their estimated total cost. The key insight is that reasoning tasks can be decomposed into a sequence of intermediate steps, each of which can be evaluated for quality using an external process supervision signal. This signal provides a more informative guide than the model's likelihood alone, allowing the search to focus on reasoning paths that are both likely and logically sound. By pruning unpromising paths early and allocating compute budget to the most promising trajectories, A*-Decoding achieves better token efficiency than sampling-based methods that generate many complete solutions before selecting the best one.

## Foundational Learning
- A* Search Algorithm: A best-first search algorithm that finds the shortest path in a weighted graph by maintaining a priority queue of candidate paths. Why needed: Provides the theoretical foundation for efficiently exploring the space of partial solutions. Quick check: Can be verified by ensuring the heuristic is admissible (never overestimates true cost) and consistent.
- Process Supervision: External feedback on the quality of intermediate reasoning steps, as opposed to outcome supervision which only evaluates final answers. Why needed: Enables the search to distinguish between logically sound and unsound reasoning paths early in generation. Quick check: Can be implemented via reward models or human annotations of intermediate steps.
- Heuristic Function Design: The combination of likelihood predictions and process supervision into a single cost estimate for each partial solution. Why needed: Must balance exploration of likely continuations with exploitation of high-quality reasoning. Quick check: Can be tuned by evaluating search performance on validation tasks.
- Fixed Compute Budget Framing: The constraint that the total number of inference passes is limited, requiring efficient allocation across promising paths. Why needed: Reflects real-world deployment scenarios where latency and cost are constrained. Quick check: Can be simulated by limiting the size of the priority queue or the number of expansions.

## Architecture Onboarding

### Component Map
Language Model -> A* Search Engine -> External Process Supervisor -> Priority Queue of Partial Solutions

### Critical Path
The critical path is: Language Model generates next-token probabilities for each partial solution in the priority queue → External Process Supervisor evaluates the quality of each new partial solution → A* Search Engine updates the priority queue based on estimated total costs → Process repeats until solution is found or budget is exhausted.

### Design Tradeoffs
The main tradeoff is between search depth and breadth: deeper search (longer reasoning chains) may find better solutions but consumes more budget, while broader search (more parallel partial solutions) increases the chance of finding a good path but requires more parallel computation. The choice of heuristic function is also critical: a more accurate heuristic leads to better pruning and efficiency but may require more expensive supervision signals. The method trades increased per-step computation (running the process supervisor) for reduced total token generation.

### Failure Signatures
A*-Decoding may fail when: the external supervision signal is unreliable or unavailable for certain reasoning steps; the heuristic is poorly designed and leads to premature pruning of correct paths; the compute budget is too small to explore sufficiently diverse reasoning trajectories; or the task structure is not amenable to decomposition into intermediate steps that can be meaningfully supervised.

### 3 First Experiments
1. Ablation study: Run A*-Decoding with and without the external process supervision signal on MATH500 to quantify its contribution to performance and efficiency gains.
2. Budget scaling: Evaluate performance across different fixed compute budgets (e.g., 10, 50, 100 inference passes) to understand how efficiency scales with resource constraints.
3. Model scale comparison: Test A*-Decoding on models of different sizes (e.g., 7B, 33B, 70B parameters) to determine whether efficiency gains are consistent across scales.

## Open Questions the Paper Calls Out
None

## Limitations
- Efficiency gains are benchmark-specific and may not generalize to other reasoning tasks or model architectures
- The comparison to OpenAI o1-like reasoning accuracy is based on aggregate benchmark performance without accounting for qualitative differences in reasoning chains
- The external process supervision signal is crucial but not thoroughly characterized in terms of availability, cost, or domain specificity

## Confidence

**High confidence in:**
- The technical framing of A*-decoding as A* search over partial solution states
- The reported benchmark performance improvements on MATH500 and AIME 2024
- The qualitative demonstration that structured search can improve token efficiency

**Medium confidence in:**
- The generalizability of efficiency gains across different reasoning tasks and model scales
- The practical accessibility of the external supervision signal in real-world applications
- The robustness of performance claims under varying compute budgets

**Low confidence in:**
- The claim of matching "OpenAI o1-like reasoning accuracy" as this appears to be a benchmark-level comparison without analysis of reasoning quality
- The scalability of the approach to non-mathematical reasoning domains
- The absolute efficiency improvements relative to other inference-time scaling methods not included in the comparison

## Next Checks
1. Evaluate A*-decoding on non-mathematical reasoning benchmarks (e.g., coding, commonsense reasoning, or scientific reasoning) to assess generalizability of the efficiency gains and performance improvements.

2. Conduct ablation studies on the external process supervision signal to quantify its contribution to performance and determine whether alternative, more accessible supervision sources could maintain similar gains.

3. Test the approach with varying compute budgets and real-time constraints to understand how performance scales when the fixed-budget assumption is relaxed or when latency requirements dominate.