---
ver: rpa2
title: 'Small steps no more: Global convergence of stochastic gradient bandits for
  arbitrary learning rates'
arxiv_id: '2502.07141'
source_url: https://arxiv.org/abs/2502.07141
tags:
- have
- lemma
- gradient
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes global convergence for stochastic gradient
  bandit algorithms using any constant learning rate. The key insight is that the
  softmax parameterization naturally ensures sufficient exploration without explicit
  exploration bonuses.
---

# Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates

## Quick Facts
- arXiv ID: 2502.07141
- Source URL: https://arxiv.org/abs/2502.07141
- Reference count: 40
- Primary result: Global convergence for stochastic gradient bandit algorithms with any constant learning rate

## Executive Summary
This paper establishes global convergence for stochastic gradient bandit algorithms using any constant learning rate, resolving the open question of whether aggressive updates would cause premature convergence to suboptimal actions. The key insight is that the softmax parameterization naturally ensures sufficient exploration without explicit exploration bonuses. The main result proves that the algorithm converges to the globally optimal policy almost surely for any constant learning rate, even when learning rates are orders of magnitude larger than previously thought necessary. Experiments with 4-action bandits validate the theoretical findings, showing convergence despite long "stuck" phases for large learning rates.

## Method Summary
The paper studies stochastic gradient bandit algorithms using softmax parameterization for multi-armed bandits. The method updates policy parameters using stochastic gradients without baselines or reward normalization. The core algorithm uses a constant learning rate η and updates all K actions each iteration based on the sampled action and observed reward. The update rule leverages the softmax Jacobian structure, where actions not sampled still receive gradient updates. The approach is evaluated on a synthetic 4-action bandit with Gaussian rewards, comparing convergence across different learning rates (η ∈ {1, 10, 100, 1000}) over 10^6 iterations.

## Key Results
- Proves global convergence to optimal policy almost surely for any constant learning rate
- Shows at least two distinct actions are sampled infinitely often regardless of learning rate
- Establishes that cumulative progress dominates cumulative noise for actions sampled infinitely often
- Demonstrates recursive elimination of suboptimal arms drives convergence to optimal action

## Why This Works (Mechanism)

### Mechanism 1: Automatic Exploration from Softmax Jacobian
The stochastic gradient bandit algorithm guarantees that at least two distinct actions are sampled infinitely often almost surely, for any constant learning rate. The update uses the softmax Jacobian (diag(πθ) − πθπ⊤θ). When πθt(a) → 1, the (1−πθt(a)) term in its own update shrinks toward 0, slowing commitment. The proof shows that if only one action were sampled infinitely often, all θt(a) would remain bounded, implying all πθt(a) are bounded away from 0, contradicting the assumption via the extended Borel-Cantelli lemma.

### Mechanism 2: Progress-Dominates-Noise for Sampled Actions
For any action sampled infinitely often, θt(a) eventually moves in the expected gradient direction, with cumulative progress overwhelming cumulative noise almost surely. The update decomposes as θt(a) = E[θ1(a)] + Σ Ws(a) + Σ Ps(a), where Ws(a) is martingale noise and Ps(a) = η·πθs(a)·(r(a) − π⊤θs r) is "progress". Freedman's inequality bounds cumulative noise at O(√(Vt log Vt)). Progress terms grow as O(t) while noise grows as O(√t log t), ensuring asymptotic dominance.

### Mechanism 3: Recursive Elimination of Suboptimal Arms
Once the optimal arm a* ∈ A∞ (sampled infinitely often), suboptimal arms are recursively forced to have θt → −∞, yielding πθt(a*) → 1 almost surely. Sort A∞ by reward. Let i1 be lowest-reward action in A∞. Since r(i1) < π⊤θt r < r(a*) for large t, Pt(i1) < 0, driving θt(i1) → −∞. This makes πθt(a*)/πθt(i1) → ∞. For the next-lowest action i2, vanishing probabilities of lower actions eventually force π⊤θt r > r(i2), making Pt(i2) < 0. Recursion eliminates all suboptimal arms.

## Foundational Learning

- Concept: Martingale Convergence and Freedman's Inequality
  - Why needed here: The proof bounds cumulative noise Ws(a) using Freedman's inequality. Without this, one cannot establish progress dominance.
  - Quick check question: Given a martingale difference sequence {Xt} with |Xt| ≤ 1/2 and conditional variance Vn, what does Freedman's inequality guarantee about P(|Σ Xt| ≥ ε)?

- Concept: Softmax Policy Parameterization and Its Jacobian
  - Why needed here: The update rule uses the Jacobian diag(πθ) − πθπ⊤θ, which shapes exploration. Understanding its effect on all actions is crucial.
  - Quick check question: If πθ = [0.7, 0.3], what is the 2×2 Jacobian? How does it multiply r = [1, 0]?

- Concept: Borel-Cantelli Lemma and Infinite Sampling
  - Why needed here: Lemmas 1-2 use the extended Borel-Cantelli lemma to connect Σt πθt(a) to whether action a is sampled infinitely often.
  - Quick check question: If Σt πθt(a) = ∞, what can you conclude about N∞(a) almost surely? Conversely, if N∞(a) < ∞, what must be true about the sum?

## Architecture Onboarding

- Component map:
  - Policy Module: Softmax(θt) → πθt ∈ ΔK
  - Sampling Module: Draws at ∼ πθt(·)
  - Reward Oracle: Returns Rt(at) from distribution Pa_t
  - Update Module: θt+1 = θt + η · (diag(πθt) − πθtπ⊤θt) · ˆrt (IS estimator)
  - Logging: Tracks Nt(a), πθt(a), θt(a) for all actions

- Critical path:
  1. Initialize θ1 ∈ RK (often θ1 = 0)
  2. Per iteration: sample at, observe reward, update θt+1 via Algorithm 1
  3. Monitor πθt(a*) for convergence to 1; track sub-optimality gap r(a*) − π⊤θt r
  4. Early stop when sub-optimality gap falls below threshold

- Design tradeoffs:
  - Learning rate η: Larger η accelerates final convergence but prolongs initial "stuck" phases near suboptimal corners (observed for η = 100, 1000). Small η yields more monotonic improvement but slower final convergence.
  - Baseline usage: Paper studies baseline-free version; adding baselines (common in practice) may alter variance and convergence.
  - Reward scaling: Bounded rewards in [−Rmax, Rmax] assumed; rescaling affects practical η choices.

- Failure signatures:
  - Stuck near suboptimal corner: πθt approaches one-hot on suboptimal action for many iterations before escaping. Observed for large η. Expect eventual escape but long plateau.
  - Non-monotonic objective: Expected reward π⊤θt r may oscillate, especially for large η. This is expected, not failure.
  - Numerical underflow: For very large η and long runs, πθt(a*) may become numerically 1.0; use log-space computations for θt.

- First 3 experiments:
  1. Replicate 4-action bandit (r = [0.2, 0.05, -0.1, -0.4]) with η ∈ {1, 10, 100, 1000}. Plot log-suboptimality gap vs log(t) for 10 seeds. Verify all seeds converge but larger η shows longer stuck phases.
  2. Run 2-action case (K=2) with η=100. Plot πθt(a*) and r(a*) − π⊤θt r. Confirm monotonic expected drift of θt per Theorem 1.
  3. Stress-test with reward scale factors s ∈ {0.1, 1, 10}. Observe whether convergence scales with s (probing practical limits of "any constant η" claim).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the stochastic gradient bandit algorithm converge to a globally optimal policy even when the true mean rewards contain ties (relaxing Assumption 1)?
- Basis in paper: [explicit] Page 3 states, "Removing Assumption 1 remains an open question for future work, while we believe that Algorithm 1 works without Assumption 1."
- Why unresolved: The current proof relies on strict inequalities between action rewards (Eq. 17) to recursively eliminate sub-optimal actions.
- Evidence would resolve it: A theoretical proof showing convergence to the set of optimal policies when r(i) = r(j) for some i ≠ j, or demonstrating that non-convergence occurs only on a set of measure zero.

### Open Question 2
- Question: Is the O(ln(T)/T) asymptotic convergence rate provided in Theorem 3 improvable to O(1/T)?
- Basis in paper: [explicit] Page 9 notes, "More work is needed to verify if the asymptotic convergence rate in Theorem 3 is improvable or not," citing empirical evidence in Figure 1 suggesting a faster rate.
- Why unresolved: The theoretical bound is derived for averaged iterates and includes logarithmic factors that may be artifacts of the specific martingale concentration bounds used.
- Evidence would resolve it: A tighter theoretical analysis of the cumulative noise and progress terms that eliminates the ln(T) factor, matching the empirical slopes observed in the log-log plots.

### Open Question 3
- Question: Why do large constant learning rates cause the algorithm to spend longer periods stuck on plateaus near sub-optimal corners before converging?
- Basis in paper: [inferred] Page 9 discusses the trade-off where large η leads to faster final convergence but "longer plateaus," noting that "asymptotic convergence results are insufficient for explaining these subtleties."
- Why unresolved: The paper establishes almost sure convergence but lacks a finite-time analysis characterizing the duration of transient phases or the probability of "stuck" runs within a finite horizon.
- Evidence would resolve it: A finite-time analysis characterizing the "stages" of convergence, specifically quantifying the probability of escaping sub-optimal plateaus as a function of the learning rate η.

### Open Question 4
- Question: Can the global convergence guarantees for arbitrary constant learning rates be extended to general reinforcement learning settings and function approximation?
- Basis in paper: [explicit] Page 10 lists extending results to "the more general RL setting" and "handle function approximation" as future directions.
- Why unresolved: The current proof relies heavily on the tabular softmax parameterization and the specific structure of the multi-armed bandit update; function approximation introduces approximation errors and non-tabular dynamics.
- Evidence would resolve it: A proof extending the "cumulative progress versus noise" technique to policy gradient methods in Markov Decision Processes (MDPs) or with neural network parameterizations.

## Limitations
- The "any constant learning rate" claim is mathematically true but practically problematic: large η causes extended periods of apparent stagnation near suboptimal policies
- Assumption 1 (strict reward differences) is critical to the proof but may not hold in real-world applications with reward ties
- The analysis is limited to tabular softmax parameterization and doesn't extend to function approximation settings
- No finite-time analysis characterizes the duration of convergence plateaus or escape probabilities

## Confidence
- Theoretical claims: High - The convergence proofs appear rigorous and directly address the open question
- Practical implications: Medium - While convergence is guaranteed, large learning rates cause extended "stuck" phases that may be problematic in practice
- Scalability: Medium - The recursive elimination mechanism may not scale effectively to large action spaces
- Generalization: Low - Extension to function approximation and general RL settings remains open

## Next Checks
1. Test the algorithm on larger action spaces (K > 4) to verify whether the recursive elimination mechanism scales effectively.
2. Evaluate performance under non-Gaussian reward noise (e.g., heavy-tailed distributions) to stress-test the Freedman inequality bounds.
3. Compare against baseline methods that use learning rate decay or explicit exploration bonuses to quantify the practical cost of using constant learning rates.