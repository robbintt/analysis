---
ver: rpa2
title: Knowledge distillation through geometry-aware representational alignment
arxiv_id: '2509.25253'
source_url: https://arxiv.org/abs/2509.25253
tags:
- distillation
- feature
- student
- matrix
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper theoretically and empirically evaluates feature distillation
  methods for transferring representational structure from teacher to student models
  in knowledge distillation. It shows that common methods like CKA and learned projections
  fail to preserve feature geometry, even when their losses are minimized.
---

# Knowledge distillation through geometry-aware representational alignment

## Quick Facts
- **arXiv ID**: 2509.25253
- **Source URL**: https://arxiv.org/abs/2509.25253
- **Reference count**: 40
- **Primary result**: Procrustes distance and Feature Gram Matrix Frobenius norm better preserve feature geometry than CKA/learned projections in knowledge distillation, improving task performance by up to 2 percentage points

## Executive Summary
This paper identifies a critical limitation in existing feature distillation methods: common approaches like CKA and learned projections fail to preserve the geometric structure of teacher representations even when their losses converge to zero. The authors theoretically prove this failure and propose two geometry-aware alternatives—Procrustes distance and Feature Gram Matrix Frobenius norm—that provably preserve the complete inner product structure. Experiments on BERT for GLUE tasks and OPT for instruction-following show statistically significant improvements over CKA baselines, validating that preserving feature geometry is crucial for effective knowledge transfer.

## Method Summary
The method replaces common feature distillation losses (CKA, learned projections) with geometry-preserving alternatives. Procrustes distance finds the optimal orthogonal transformation minimizing ||RsQ - Rt||_F, reformulated efficiently via nuclear norm of Rs^T Rt. Feature Gram Matrix distance directly minimizes ||K_t - K_s||_F where K = RR^T encodes all pairwise inner products. The combined loss is L = γL_CE + αL_sim + (1-α)L_KD where L_sim ∈ {D_P, D_FG}. Implementation requires computing Gram matrices, applying the nuclear norm formulation for Procrustes, and backpropagating to the student model while keeping the teacher frozen.

## Key Results
- Procrustes distance and Feature Gram Matrix distance significantly outperform CKA and learned projections in synthetic experiments measuring orthogonal vector preservation
- BERT distillation on GLUE tasks shows up to 2 percentage point improvements over CKA baselines
- Geometry-aware distillation achieves statistically significant improvements on 4/5 GLUE tasks (CoLA, MRPC, RTE, QQP)
- Feature distillation alone without KL divergence or supervised loss causes catastrophic performance drops

## Why This Works (Mechanism)

### Mechanism 1: Procrustes Distance Guarantees Geometric Equivalence
- Claim: Optimizing Procrustes distance is necessary and sufficient for preserving complete inner product structure of teacher representations in the student
- Mechanism: Procrustes finds optimal orthogonal transformation Q minimizing ||RsQ - Rt||_F. The kernel reformulation computes this via nuclear norm: D²_P = tr(Kt) + tr(Ks) - 2||Rs^T Rt||_*. Theorem 3 proves D_P = 0 iff all pairwise inner products are identical (K_t = K_s)
- Core assumption: Task-relevant knowledge is encoded in relative geometry (angles/inner products) between feature vectors on a unit sphere, not absolute coordinates
- Evidence anchors: [abstract], Theorem 3 (page 5), synthetic experiment (page 6-7)
- Break condition: If student dimension d_s < effective rank of teacher features, D_FG = 0 is impossible

### Mechanism 2: CKA Normalization Discards Absolute Geometry Information
- Claim: CKA's invariance properties make it unsuitable as a distillation objective because near-zero CKA loss can coexist with severely degraded feature geometry
- Mechanism: CKA normalizes by tr(K_t K_t) and tr(K_s K_s), making it invariant to isotropic scaling. Theorem 1 constructs counterexample where D_CKA ≤ ε but D_FG = √ε ||K_t - J_n||_F, which can be O(n) for over-parameterized models
- Core assumption: Models are over-parameterized (n >> d), standard for language models
- Evidence anchors: Theorem 1 construction (page 5), Figure 1 illustration (page 3), synthetic experiment (page 6-7)
- Break condition: N/A

### Mechanism 3: Learned Projections Require Orthogonal Constraints
- Claim: Learned linear projections preserve geometry only when constrained to be right-orthogonal matrices; unconstrained projections can achieve zero loss without preserving inner product structure
- Mechanism: Theorem 2 proves D_LinProj = 0 implies D_FG = 0 only if optimal projector P is in S(d_s, d_t) (right-orthogonal). Unconstrained learned projections can "cheat" by distorting geometry while minimizing reconstruction error
- Core assumption: Projection matrix is learned freely without spectral constraints (common practice)
- Evidence anchors: Theorem 2 (page 5), Lemma 3 (page 17), synthetic experiment Figure 2a (page 6)
- Break condition: If student representations span subspace contained in P's eigenspace with eigenvalue 1, geometry may be preserved accidentally (rare)

## Foundational Learning

- **Gram Matrices and Inner Product Structure**
  - Why needed here: The paper's framework rests on K = RR^T encoding all pairwise inner products; geometry preservation is defined as K_t = K_s
  - Quick check question: Given two representation matrices R_t (n×d_t) and R_s (n×d_s), why does ||K_t - K_s||_F = 0 imply identical geometry even when d_t ≠ d_s?

- **Nuclear Norm and SVD Decomposition**
  - Why needed here: The kernel-based Procrustes distance uses nuclear norm ||Rs^T Rt||_* = sum of singular values; understanding this is essential for implementation
  - Quick check question: If A has singular values [3, 2, 1], what is ||A||_* and how does it differ from ||A||_F?

- **Orthogonal Transformations and Shape Analysis**
  - Why needed here: Procrustes distance originated in statistical shape analysis; understanding why Q must be orthogonal clarifies why it preserves geometry
  - Quick check question: Why does rotating or reflecting a point cloud (Q^T Q = I) preserve all pairwise distances, but an arbitrary linear transformation does not?

## Architecture Onboarding

- **Component map:**
  Teacher (frozen) → hidden layers → Gram matrix K_t or representation R_t
                                            ↓
  Student (trainable) → hidden layers → Gram matrix K_s or representation R_s
                                            ↓
                                D_P or D_FG computed, backprop to student only

- **Critical path:**
  1. Extract hidden representations from matched layers (e.g., layer n of student ↔ layer 2n of teacher for BERT)
  2. Center representations (subtract mean across batch dimension)
  3. Compute Procrustes loss: `D_P = tr(K_t) + tr(K_s) - 2 * torch.norm(R_s.T @ R_t, p='nuc')`
  4. Combine with task loss: L = γ·L_CE + α·L_sim + (1-α)·L_KD

- **Design tradeoffs:**
  - Procrustes vs. Gram Matrix: Procrustes is more stable during optimization; Gram matrix shows fluctuations from batch noise
  - Single vs. multi-layer distillation: Single layer simpler but less effective; multi-layer better but requires careful layer matching
  - α hyperparameter: Controls feature vs. logit balance; paper uses sweep over [0, 0.2, 0.4, 0.6, 0.8, 1]

- **Failure signatures:**
  - Feature distillation alone (no KL divergence or supervised loss) is "disastrous"
  - CKA loss converging to near-zero but orthogonal structure degrading
  - Learned projection loss noisy and uncorrelated with geometry preservation

- **First 3 experiments:**
  1. Replicate synthetic experiment: Generate ε-orthogonal teacher vectors, optimize student with each loss, track orthogonal vector count to validate Procrustes superiority
  2. Single-layer distillation on GLUE comparing D_P vs D_CKA with identical hyperparameters to isolate loss function effect
  3. Ablation on α: Test α ∈ {0.2, 0.5, 0.8} to find optimal feature-to-logit balance for your specific teacher/student pair

## Open Questions the Paper Calls Out

- **Does multi-layer geometry-aware distillation yield significant improvements over last-layer-only alignment in large decoder-only models, and is the computational overhead tractable?**
  - Basis: The OPT experiments only align the last layers due to increased computational complexity
  - Why unresolved: Benefits observed in encoders for full-layer alignment haven't been confirmed for decoders
  - What evidence would resolve it: Experiments on OPT measuring performance and duration for full-layer vs final-layer Procrustes alignment

- **Does geometry-aware distillation using Procrustes distance provide performance gains in computer vision tasks, or is effectiveness specific to language model representations?**
  - Basis: The introduction claims results in "vision and language tasks" but validation is restricted to NLP
  - Why unresolved: Efficacy of preserving "spherical geometry" hasn't been confirmed for spatial data like images
  - What evidence would resolve it: Experiments distilling Vision Transformer or ResNet using Procrustes objective

## Limitations

- The synthetic experiment's ε-orthogonal vectors may not reflect real-world feature distributions in language models, and the orthogonal structure metric may not capture actual geometry relevant for downstream tasks
- The layer matching strategy (n of student ↔ 2n of teacher for BERT) uses an exponential relationship without strong theoretical justification, which may not generalize to other teacher-student pairs
- The theoretical results rely on over-parameterization (n >> d) assumptions that may not hold for smaller models or computer vision tasks

## Confidence

- **High Confidence**: Theoretical results (Theorems 1-3) showing limitations of CKA and learned projections, and superiority of Procrustes/FG distances. Mathematically proven and validated by synthetic experiment
- **Medium Confidence**: Empirical results on GLUE and instruction-following tasks. Statistically significant improvements shown but gains are modest (2 percentage points) and task-dependent
- **Low Confidence**: The claim that CKA is "unsuitable as a distillation objective" is overly strong. CKA remains valuable for similarity measurement and may work better in untested contexts

## Next Checks

1. **Feature Distribution Analysis**: Extract and visualize feature distributions from real language model layers to verify whether synthetic ε-orthogonal structure is representative of actual teacher representations

2. **Alternative Layer Matching**: Test linear layer matching (n→n) and other heuristics on BERT to determine if exponential matching (n→2n) is optimal or simply works well for the specific teacher-student pairs used

3. **Cross-Domain Validation**: Apply Procrustes/FG distillation to computer vision tasks with smaller models to test whether theoretical advantages hold outside the over-parameterized regime where they were proven