---
ver: rpa2
title: 'Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark
  from Psychological Support Hotlines'
arxiv_id: '2506.01329'
source_url: https://arxiv.org/abs/2506.01329
tags:
- performance
- suicidal
- risk
- were
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PsyCrisisBench benchmarks 64 large language models on 540 real-world
  hotline transcripts across four crisis assessment tasks. LLMs achieved strong performance
  on suicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),
  and risk assessment (F1=0.907), with fine-tuning and few-shot prompting providing
  notable improvements.
---

# Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines

## Quick Facts
- **arXiv ID:** 2506.01329
- **Source URL:** https://arxiv.org/abs/2506.01329
- **Reference count:** 40
- **Primary result:** Fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks

## Executive Summary
This paper presents PsyCrisisBench, a benchmark evaluating 64 large language models on 540 real-world hotline transcripts across four crisis assessment tasks: mood status recognition, suicidal ideation detection, suicide plan identification, and risk assessment. The benchmark demonstrates that LLMs achieve strong performance on explicit crisis detection tasks, with fine-tuning and few-shot prompting providing notable improvements. Notably, a fine-tuned 1.5B-parameter model outperformed larger models on mood and suicidal ideation tasks. The benchmark provides a robust, real-world evaluation framework to guide future model development and ethical deployment in clinical mental health settings.

## Method Summary
The benchmark evaluates 64 LLMs (15 families) on 540 hotline call transcripts from Hangzhou Psychological Assistance Hotline (2023), with 520 auxiliary calls from 2022 for few-shot examples and fine-tuning. Tasks include four binary classification problems requiring structured JSON output. Models are evaluated under zero-shot, static 4-shot, dynamic few-shot (0/2/4/6/8 shots), and fine-tuned conditions. Fine-tuning uses Qwen2.5-1.5B-Instruct with specified hyperparameters. Performance is measured via F1-score with 95% bootstrap confidence intervals, compared against human baseline operators working from text-only transcripts.

## Key Results
- LLMs achieved strong performance on suicidal ideation detection (F1=0.880) and risk assessment (F1=0.907)
- Fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks
- Performance was broadly comparable to trained human operators, with complementary strengths across tasks
- Mood status recognition remained challenging (max F1=0.709), likely due to missing vocal cues and semantic ambiguity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs achieve stronger performance on tasks with explicit linguistic markers than on affective inference requiring prosodic integration.
- **Mechanism:** Structured crisis tasks contain specific factual details scattered across transcripts that attention mechanisms can localize; mood status requires reconstructing emotional state from text where vocal cues are lost during transcription, creating a signal-to-noise ceiling.
- **Core assumption:** The performance gap reflects missing modality information rather than fundamental reasoning limitations.
- **Evidence anchors:**
  - [abstract] "Mood status recognition remained challenging (max F1=0.709), likely due to missing vocal cues and semantic ambiguity"
  - [section IV-A] "Human experts achieved F1-scores of 0.729 [0.688, 0.767] for mood status recognition... outperforming the best LLMs on these two tasks"
  - [corpus] "Can Large Language Models Identify Implicit Suicidal Ideation?" addresses implicit signal detection challenges

### Mechanism 2
- **Claim:** Domain-specific fine-tuning outperforms scale increases for clinically grounded classification tasks.
- **Mechanism:** Task-specific training aligns model representations with clinical construct definitions (e.g., psychiatric depression criteria vs. colloquial sadness), enabling smaller parameter counts to capture domain-relevant features more efficiently than general-purpose scale alone.
- **Core assumption:** The clinical label definitions differ meaningfully from general language usage patterns in pre-training data.
- **Evidence anchors:**
  - [abstract] "a fine-tuned 1.5B-parameter model (Qwen2.5-1.5B) outperformed larger models on mood and suicidal ideation tasks"
  - [section IV-F] "Fine-tuning Qwen2.5-1.5B using the 2022 auxiliary dataset yielded substantial improvements: mood status recognition from 0.607... to 0.805"
  - [corpus] Evidence primarily from primary text

### Mechanism 3
- **Claim:** Few-shot prompting effectiveness is task-dependent and saturates when examples exceed context relevance thresholds.
- **Mechanism:** In-context examples help calibrate label interpretations for ambiguous tasks, but for structurally explicit tasks, random examples may introduce noise conflicting with task-specific patterns, causing performance decline beyond optimal shot counts.
- **Core assumption:** Example quality and task-relevance matter more than quantity; randomly sampled examples contain heterogeneous signal quality.
- **Evidence anchors:**
  - [section IV-F] "suicidal plan identification showed progressive decline: from 0.751 at zero-shot to 0.721 at 8-shot, suggesting high sensitivity to example quality"
  - [section V] "For certain tasks, such as suicidal planning, we observed a counterintuitive performance decline as the number of examples increased beyond four"
  - [corpus] "Rethinking Suicidal Ideation Detection" discusses annotation reliability challenges

## Foundational Learning

- **Concept:** Binary classification with clinical label definitions
  - **Why needed here:** Tasks use psychiatric criteria (e.g., depression requiring specific symptom counts and duration), not colloquial interpretations; misunderstanding this leads to label mismatch.
  - **Quick check question:** Can you explain why a caller saying "I feel down" might not meet the "Depression" label threshold?

- **Concept:** In-context learning saturation and noise sensitivity
  - **Why needed here:** Adding examples doesn't always help; understanding when more shots degrade performance prevents wasted prompt engineering.
  - **Quick check question:** For a task with explicit factual markers, would you expect 8-shot to outperform 4-shot with random examples?

- **Concept:** Modality gap in text-only affective inference
  - **Why needed here:** Text transcripts lose prosodic cues; this explains the mood recognition ceiling and sets realistic expectations for text-based systems.
  - **Quick check question:** What audio features would help distinguish clinical depression from transient sadness that text alone cannot capture?

## Architecture Onboarding

- **Component map:** Transcription pipeline (Whisper-large-v3-turbo) → PII removal → Turn-by-turn normalization → LLM inference (zero-shot/few-shot/fine-tuned) → JSON parsing → F1 evaluation with bootstrap CI. Auxiliary dataset (2022 calls) provides few-shot examples and fine-tuning data, isolated from 2023 evaluation set.

- **Critical path:**
  1. Ensure strict JSON output format compliance—models failing this are scored as incorrect regardless of substantive accuracy.
  2. Use the 2022 auxiliary dataset (520 calls) for any few-shot or fine-tuning experiments; never sample from the 2023 evaluation set.
  3. For fine-tuning: 3 epochs, lr=1e-5, batch size 16, bfloat16, select checkpoint with lowest validation loss.

- **Design tradeoffs:**
  - Closed-source models: Better mood recognition (p=0.007 vs. open-source) but require API dependency and data egress.
  - Reasoning mode: Improves mood tasks but produces 4.5× longer outputs, increasing latency and operator burden.
  - AWQ quantization: 60-70% memory reduction with minimal F1 degradation (~0.025 drop worst case), enabling resource-constrained deployment.

- **Failure signatures:**
  - Models scoring near random chance (F1~0.5) often fail JSON formatting, not clinical reasoning—check raw outputs first.
  - Mood recognition consistently underperforms across all models (max 0.709)—this is architectural, not fixable via prompting alone.
  - Few-shot performance declining with more examples signals example quality issues, not model capability limits.

- **First 3 experiments:**
  1. **Baseline replication:** Run zero-shot evaluation on 3 model families (Qwen, Claude, DeepSeek) across all 4 tasks to validate benchmark reproduction.
  2. **Fine-tuning ablation:** Fine-tune Qwen2.5-1.5B on 100, 250, and 520 examples from auxiliary dataset to establish data efficiency curve.
  3. **Curated vs. random few-shot:** Compare 8-shot performance using randomly sampled vs. manually curated examples for suicide plan identification to test the example-quality hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integration of prosodic and paralinguistic features (audio) overcome the performance ceiling observed in text-based mood status recognition?
- **Basis in paper:** [Explicit] The authors state that mood status recognition remains challenging (max F1=0.709) likely due to "missing vocal cues" and explicitly call for "future research [to] prioritize multimodal integration incorporating prosodic and paralinguistic features."
- **Why unresolved:** The current study relied solely on ASR transcripts, which strip away non-verbal emotional indicators essential for detecting depression, creating a potential performance ceiling that text-only models cannot breach.
- **What evidence would resolve it:** A comparative benchmark evaluating models on raw audio versus text-only transcripts for the same callers, specifically measuring the delta in F1-scores for the mood status recognition task.

### Open Question 2
- **Question:** How does static, single-turn assessment compare to longitudinal evaluation in capturing the dynamic nature of psychological crises?
- **Basis in paper:** [Explicit] The authors note in the Limitations section that the "current evaluation is static and single-turn, and therefore does not capture longitudinal changes or multi-turn conversational strategies that are central to real-world crisis intervention."
- **Why unresolved:** Crisis states are fluid, but the benchmark treats each call as an isolated event, failing to model how risk escalates or de-escalates over time or across multiple interactions.
- **What evidence would resolve it:** Developing a temporal extension of the dataset that tracks repeat callers over time to evaluate if LLMs can accurately predict risk trajectory rather than just a snapshot status.

### Open Question 3
- **Question:** Do guided reasoning approaches that explicitly integrate clinical protocols improve the safety and transparency of crisis assessments over standard prompting?
- **Basis in paper:** [Explicit] The authors suggest "exploring guided reasoning approaches that integrate model inference with clinical protocols could enhance both transparency and practical utility."
- **Why unresolved:** While reasoning models (like GPT-o1) showed strong results, it remains unclear if forcing models to follow structured clinical workflows (e.g., specific risk assessment questionnaires) reduces false negatives or hallucinations compared to unconstrained reasoning.
- **What evidence would resolve it:** An ablation study comparing "free-form" reasoning models against models constrained by step-by-step clinical guidelines (e.g., the C-SSRS protocol) to measure accuracy differences and error types.

### Open Question 4
- **Question:** Does the benchmark framework generalize to authentic multilingual and cross-cultural clinical contexts without reliance on translation?
- **Basis in paper:** [Explicit] The authors acknowledge that the cross-lingual evaluation was only a "simplified approximation" and explicitly state that "future work should evaluate the framework on authentic multilingual datasets."
- **Why unresolved:** The current cross-lingual results rely on translating Chinese transcripts to English, which may preserve semantic content but lose cultural nuances in emotional expression and distress.
- **What evidence would resolve it:** Evaluating the benchmark's tasks on native, non-translated transcripts from diverse linguistic hotlines (e.g., English or Spanish services) to determine if the models' performance hierarchy remains consistent.

## Limitations

- The 540-call evaluation set may not capture the full spectrum of crisis presentation patterns across different cultural contexts or linguistic variations.
- Text-only transcription inherently loses prosodic and paralinguistic cues critical for mood assessment, creating an artificial ceiling for LLM performance on affective tasks.
- The closed-loop evaluation (human annotators and LLMs both working from the same transcripts) may overestimate real-world utility since operators have access to live audio and visual cues unavailable to models.

## Confidence

- **High confidence:** Claims about LLM performance on explicit crisis detection tasks (suicidal ideation, suicide plans, risk assessment) are well-supported by the data, with clear numerical results and statistical comparisons to human baselines.
- **Medium confidence:** The performance gap between LLMs and humans on mood recognition is convincingly demonstrated, but the mechanistic explanation (missing vocal cues) remains inferential rather than experimentally validated.
- **Low confidence:** The finding that a 1.5B fine-tuned model outperforms larger models requires replication across different model architectures and training regimes to rule out data-specific effects or hyperparameter optimization artifacts.

## Next Checks

1. **Multimodal replication:** Test the same benchmark with a multimodal LLM system (audio + transcript) to isolate whether the mood recognition ceiling is truly due to lost prosodic information versus annotation schema limitations.

2. **Cross-cultural validation:** Evaluate the benchmark across hotline transcripts from different geographic regions or cultural contexts to assess generalizability beyond the Hangzhou dataset.

3. **Human-AI collaboration assessment:** Conduct a controlled study where human operators use LLM suggestions during live calls versus working independently, measuring whether complementary strengths translate to improved real-world outcomes.