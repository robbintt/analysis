---
ver: rpa2
title: Autoregressive Speech Enhancement via Acoustic Tokens
arxiv_id: '2507.12825'
source_url: https://arxiv.org/abs/2507.12825
tags:
- speech
- tokens
- enhancement
- audio
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel autoregressive transducer-based architecture
  for speech enhancement using discrete acoustic tokens. The approach addresses limitations
  of prior work that used non-autoregressive models and semantic tokens, which lose
  speaker identity information.
---

# Autoregressive Speech Enhancement via Acoustic Tokens

## Quick Facts
- arXiv ID: 2507.12825
- Source URL: https://arxiv.org/abs/2507.12825
- Authors: Luca Della Libera; Cem Subakan; Mirco Ravanelli
- Reference count: 40
- Primary result: Acoustic tokens from neural codecs (EnCodec, DAC) preserve speaker identity better than semantic tokens, and autoregressive SET architecture improves intelligibility.

## Executive Summary
This paper introduces a novel autoregressive transducer-based architecture for speech enhancement using discrete acoustic tokens. The approach addresses limitations of prior work that used non-autoregressive models and semantic tokens, which lose speaker identity information. The proposed Speech Enhancement Transducer (SET) employs an encoder-decoder framework with autoregressive prediction, outperforming semantic token methods in preserving speaker identity and improving intelligibility. Experiments on VoiceBank and Libri1Mix datasets show that acoustic tokens from EnCodec and DAC outperform semantic tokens from WavLM, and the SET architecture further enhances performance. However, discrete representations still lag behind continuous ones, particularly in intelligibility metrics, highlighting the need for further research in this area.

## Method Summary
The paper proposes using discrete acoustic tokens from neural codecs (EnCodec, DAC) instead of semantic tokens for speech enhancement. These tokens are processed through a Speech Enhancement Transducer (SET) architecture that combines an encoder, a causal predictor, and a joiner. The encoder processes noisy tokens with bidirectional attention, the predictor generates autoregressive predictions conditioned on previous enhanced tokens, and the joiner fuses their features. Training uses cross-entropy loss with a critical detail: teacher forcing is disabled for the final 5 epochs to mitigate exposure bias. The model is evaluated on VoiceBank and Libri1Mix datasets using DNSMOS for quality, cosine similarity for speaker fidelity, and dWER for intelligibility.

## Key Results
- Acoustic tokens (EnCodec, DAC) preserve speaker identity significantly better than semantic tokens (WavLM discrete), with CosSim scores 0.872-0.927 vs 0.833-0.837
- Autoregressive SET architecture improves intelligibility over non-autoregressive models, reducing dWER from 30.21% to 23.97% on VoiceBank
- Discrete representations still lag behind continuous baselines in intelligibility, with gap attributed to data requirements for high-bitrate token language models

## Why This Works (Mechanism)

### Mechanism 1: Acoustic tokens preserve speaker identity better than semantic tokens
Acoustic tokens from neural codecs retain fuller spectral information compared to semantic tokens from self-supervised models. While semantic tokens prioritize linguistic content through k-means clustering on self-supervised representations, they discard prosody, intonation, and speaker-specific acoustic details. Acoustic tokens from residual vector quantization (RVQ) codecs maintain these fine-grained features, enabling better reconstruction of speaker characteristics during enhancement.

### Mechanism 2: Autoregressive modeling improves intelligibility
The Speech Enhancement Transducer (SET) captures temporal dependencies between output tokens through autoregressive prediction. Unlike non-autoregressive models that predict each token independently, SET conditions each prediction on previously generated enhanced tokens via a predictor network. This allows the model to maintain coherence across the enhanced sequence by leveraging past clean predictions as disambiguating signal.

### Mechanism 3: Transducer architecture is more parameter-efficient
The SET architecture avoids the cross-attention overhead of full encoder-decoder transformers because speech enhancement is a synchronous many-to-many mapping where each input token corresponds to an output token. The joiner combines encoder and predictor features via element-wise sum, eliminating the need to learn alignment between input and output sequences.

## Foundational Learning

**Residual Vector Quantization (RVQ)**: Acoustic tokens from EnCodec and DAC use RVQ to compress audio into multiple parallel codebook indices. This is essential for understanding bitrate experiments and multi-codebook prediction heads. *Quick check*: Can you explain why RVQ uses multiple codebooks in sequence rather than a single large codebook, and how this affects reconstruction quality vs bitrate?

**Exposure Bias in Autoregressive Models**: The paper identifies exposure bias as a critical failure mode where training uses teacher forcing but inference must use model predictions, causing error accumulation. *Quick check*: Why does disabling teacher forcing for the final 5 epochs help mitigate exposure bias, and what trade-off does this introduce?

**Transducer Loss (RNN-T)**: SET borrows from speech recognition transducers. The joiner combines encoder and predictor outputs, and training uses forward-backward alignment-free computation. *Quick check*: How does the transducer framework handle variable-length input-output alignment without explicit cross-attention, and what role does the blank token play in traditional RNN-T that may be adapted here?

## Architecture Onboarding

**Component map**: Noisy Waveform → Tokenizer → Noisy Tokens → SET Encoder → Encoder Features → Predictor → Predictor Features → Joiner → Enhanced Token Predictions → Detokenizer → Enhanced Waveform

**Critical path**:
1. Tokenizer selection determines token vocabulary size, bitrate, and whether multi-codebook prediction is needed
2. Encoder processes full noisy sequence (bidirectional attention allowed)
3. Predictor is strictly causal—can only attend to positions < current
4. Joiner fuses encoder and predictor features; design choice (sum vs concat) affects gradient flow
5. Inference requires beam search; greedy decoding underperforms significantly

**Design tradeoffs**:
| Decision | Option A | Option B | Paper finding |
|----------|----------|----------|---------------|
| Codec choice | EnCodec | DAC | DAC better quality/fidelity; EnCodec+Vocos competitive with specialized vocoder |
| Architecture | NAR | AR (SET) | AR improves intelligibility (dWER) but exposes to bias; requires refinement |
| Bitrate | 1.5 kbps | 12 kbps | DAC improves with bitrate up to 6 kbps then saturates; EnCodec saturates at 3 kbps |
| Vocoder | Codec default | External (Vocos) | External vocoder can boost quality without retraining quantizer |

**Failure signatures**:
- High dWER with AR at inference but not with teacher forcing: Exposure bias. Check scheduled sampling ratio; ensure final epochs train without teacher forcing
- Speaker identity drift: Using semantic tokens (WavLM discrete) instead of acoustic. Switch to EnCodec/DAC
- Quality degradation at high bitrates: Language model capacity insufficient for longer token sequences. Increase model size or reduce bitrate
- Catastrophic failure at negative SNR: DAC more sensitive to extreme noise; consider EnCodec or noise-aware training

**First 3 experiments**:
1. Codec ablation on held-out speakers: Train SET on VoiceBank with EnCodec, DAC, and WavLM discrete. Measure CosSim on test speakers. Validates acoustic > semantic for speaker preservation claim
2. Teacher forcing schedule sweep: Train AR models with teacher forcing disabled for [0, 1, 3, 5, 10] final epochs. Measure TF vs BS vs BSR dWER gap. Identifies optimal exposure bias mitigation
3. Bitrate scaling study: Evaluate EnCodec and DAC at [1.5, 3.0, 6.0, 12.0] kbps on both datasets. Plot DNSMOS and dWER curves. Confirms codec-specific saturation points and AR robustness at high bitrates

## Open Questions the Paper Calls Out

**Improving discrete representations**: How can discrete acoustic representations be improved to close the performance gap with continuous representations, specifically regarding speech intelligibility? The study demonstrates that while acoustic tokens preserve speaker identity better than semantic tokens, they still significantly lag behind continuous baselines in intelligibility.

**Streaming and rescoring adaptation**: Can the proposed SET architecture be effectively adapted for streaming inference and hypothesis rescoring using external speech language models? The paper notes the architecture is "well-suited for streaming inference" but this capability was not implemented or tested.

**Dataset scaling effects**: To what extent does increasing the training dataset size mitigate the intelligibility degradation observed in autoregressive models using high-bitrate acoustic tokens? The authors hypothesize that the performance gap is due to data requirements, noting they used only ~100 hours compared to thousands in other works.

## Limitations

**Exposure bias mitigation**: The solution (disabling teacher forcing for final 5 epochs) is minimally justified with arbitrary choice of 5 epochs and no ablation studies of alternative strategies like scheduled sampling or refinement.

**Speaker identity measurement**: Cosine similarity on WavLM x-vectors may not perfectly capture perceptual speaker identity and conflates speaker distinctiveness with general acoustic similarity; no perceptual listening tests validate the quantitative findings.

**Transducer efficiency claim**: The assertion that transducer architecture is more parameter-efficient than encoder-decoder lacks direct empirical validation through head-to-head comparisons on identical tasks.

## Confidence

**High confidence**: The comparative performance of acoustic vs semantic tokens is well-supported by consistent experimental evidence across multiple metrics and datasets, with unambiguous numerical superiority in CosSim and DNSMOS.

**Medium confidence**: The autoregressive intelligibility improvement is demonstrated but complicated by exposure bias; while AR consistently outperforms NAR in controlled settings, real-world performance depends heavily on final training phase configuration.

**Low confidence**: The transducer efficiency argument lacks direct empirical support within the paper; the architectural advantages are logically argued but not validated through head-to-head comparisons with alternative architectures.

## Next Checks

1. **Exposure bias ablation study**: Systematically vary the number of epochs without teacher forcing (0, 1, 3, 5, 10) and measure the gap between teacher-forced and free-running inference performance to quantify the optimal trade-off.

2. **Direct transducer vs encoder-decoder comparison**: Implement a full encoder-decoder transformer baseline with cross-attention and compare parameter efficiency, convergence speed, and final performance against SET on identical tasks.

3. **Perceptual validation of speaker identity**: Conduct controlled listening tests where human raters evaluate speaker similarity between enhanced and clean speech across acoustic and semantic token approaches to assess the validity of x-vector cosine similarity as a proxy.