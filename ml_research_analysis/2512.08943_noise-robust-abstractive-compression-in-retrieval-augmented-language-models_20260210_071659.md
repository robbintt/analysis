---
ver: rpa2
title: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models
arxiv_id: '2512.08943'
source_url: https://arxiv.org/abs/2512.08943
tags:
- documents
- noise
- evidential
- information
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of information loss in abstractive
  compression for retrieval-augmented language models, particularly when retrieved
  documents contain noise such as irrelevant information or factual errors. The proposed
  method, ACoRN (Abstractive Compression Robust against Noise), introduces two key
  innovations: (1) offline data augmentation to train the compressor against two types
  of retrieval noise (irrelevant documents and factual error documents), and (2) fine-tuning
  to generate summaries focused on evidential documents that directly support correct
  answers, thereby reducing positional bias.'
---

# Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models
## Quick Facts
- **arXiv ID**: 2512.08943
- **Source URL**: https://arxiv.org/abs/2512.08943
- **Reference count**: 0
- **Primary result**: T5-large trained with ACoRN improves Exact Match (EM) and F1 scores on three ODQA benchmarks compared to baseline compression methods

## Executive Summary
This paper addresses the challenge of information loss in abstractive compression for retrieval-augmented language models, particularly when retrieved documents contain noise such as irrelevant information or factual errors. The proposed method, ACoRN (Abstractive Compression Robust against Noise), introduces offline data augmentation to train compressors against retrieval noise and fine-tuning to generate summaries focused on evidential documents. The approach uses a teacher model to generate pseudo labels from only evidential documents, then distills this capability into a smaller compressor model. Experimental results demonstrate improvements in EM and F1 scores across NQ, TriviaQA, and PopQA benchmarks, with particular effectiveness in high-noise settings.

## Method Summary
The ACoRN method addresses information loss in abstractive compression through two key innovations: offline data augmentation and evidential document identification. The approach generates synthetic noisy retrieval examples by corrupting relevant documents with irrelevant or factually erroneous content. A teacher model creates pseudo labels by generating summaries from only evidential documents, which are then used to fine-tune a smaller compressor model. The method employs a two-stage training process: first, training with augmented noisy data to improve robustness, and second, fine-tuning with evidential document labels to reduce positional bias. The compressor learns to prioritize information from documents that directly support correct answers while filtering out noise.

## Key Results
- T5-large trained with ACoRN shows improved Exact Match (EM) and F1 scores on NQ, TriviaQA, and PopQA benchmarks
- Method demonstrates particular effectiveness on datasets with high noise-document ratios
- Preserves answer strings while reducing inference time compared to baseline compression methods

## Why This Works (Mechanism)
The method works by addressing two fundamental challenges in retrieval-augmented QA: noise robustness and positional bias. By training on synthetic noisy examples, the compressor learns to identify and filter irrelevant or erroneous information. The evidential document identification component ensures the model focuses on documents that directly support correct answers, mitigating the tendency of standard transformers to overweight information from higher-ranked documents regardless of relevance. This dual approach enables the compressor to maintain accuracy while processing shorter, more focused document sets.

## Foundational Learning
- **Abstractive Compression**: Text summarization that generates novel phrases rather than copying verbatim - needed to reduce retrieved documents to concise representations while preserving key information
- **Evidential Document Identification**: Process of determining which retrieved documents contain evidence supporting the correct answer - needed to focus compression on relevant content and reduce noise impact
- **Knowledge Distillation**: Technique where a smaller student model learns from a larger teacher model - needed to transfer noise-robust capabilities to more efficient compressor models
- **Positional Bias**: Tendency of transformers to overweight information from higher-ranked documents - needed to understand why standard compression fails with noisy retrievals
- **Data Augmentation for Noise Robustness**: Synthetic generation of noisy examples for training - needed to expose the model to various noise patterns during training

## Architecture Onboarding
- **Component Map**: Retrieval System -> ACoRN Compressor -> QA Model (T5-large -> Teacher Model -> Student Compressor)
- **Critical Path**: Input documents → Evidential document identification → Noise-robust compression → Answer generation
- **Design Tradeoffs**: Accuracy vs. inference speed, model size vs. capability, synthetic noise diversity vs. computational cost
- **Failure Signatures**: Overfitting to synthetic noise patterns, loss of answer precision, inability to handle novel noise types
- **First Experiments**:
  1. Baseline comparison with standard abstractive compression on clean retrieval sets
  2. Performance evaluation on varying noise-document ratios
  3. Ablation study removing evidential document identification component

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on pseudo-label generation from teacher models may propagate biases and errors
- Effectiveness primarily demonstrated on English-language benchmarks, limiting cross-lingual generalization
- Offline data augmentation requires significant computational resources for synthetic example generation

## Confidence
- **High**: Core technical contribution of using evidential document identification to reduce positional bias
- **Medium**: Claimed improvements in EM and F1 scores, dependent on teacher model quality
- **Low**: Generalizability across diverse retrieval scenarios and noise types beyond irrelevant and factually erroneous documents

## Next Checks
1. Conduct ablation studies to isolate contributions of offline data augmentation versus evidential document identification across different noise ratios
2. Evaluate robustness to additional noise types including contradictory evidence and ambiguous queries
3. Test approach on non-English datasets and specialized domains to assess cross-domain generalizability