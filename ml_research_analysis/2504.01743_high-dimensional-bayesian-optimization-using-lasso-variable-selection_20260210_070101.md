---
ver: rpa2
title: High Dimensional Bayesian Optimization using Lasso Variable Selection
arxiv_id: '2504.01743'
source_url: https://arxiv.org/abs/2504.01743
tags:
- optimization
- variables
- function
- bayesian
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LassoBO, a Bayesian optimization algorithm
  designed for high-dimensional problems. The core idea is to iteratively identify
  important variables using a Lasso-based method on Gaussian process inverse length
  scales, and then restrict the acquisition function optimization to a subspace containing
  only these important variables, while imputing the rest via a mix of exploitation
  (using the best observed point) and exploration (random sampling).
---

# High Dimensional Bayesian Optimization using Lasso Variable Selection

## Quick Facts
- arXiv ID: 2504.01743
- Source URL: https://arxiv.org/abs/2504.01743
- Reference count: 40
- High-dimensional BO algorithm achieving sublinear cumulative regret with novel Lasso-based variable selection

## Executive Summary
This paper proposes LassoBO, a Bayesian optimization algorithm designed for high-dimensional problems with low effective dimensionality. The core innovation is using Lasso regularization on Gaussian process inverse length scales to iteratively identify important variables, then restricting acquisition function optimization to this lower-dimensional subspace. The method achieves sublinear cumulative regret bounds and outperforms state-of-the-art methods on both synthetic benchmarks and real-world tasks including Rover trajectory optimization and DNA classification.

## Method Summary
LassoBO fits a GP surrogate on all D dimensions, then applies L1-regularized optimization to the negative log marginal likelihood to estimate sparse inverse length scales. Variables with length scales above the mean are classified as important, creating a subset I_t of dimension d_t. The acquisition function (GP-UCB) is optimized over this d_t-dimensional subspace while unimportant variables are imputed using a mixture of the best observed point and random sampling. This reduces computational complexity from O(D) to O(d_t) and enables theoretical regret bounds scaling with the lower-dimensional subspace.

## Key Results
- Proposes LassoBO achieving sublinear cumulative regret bound, novel for variable-selection-based high-dimensional BO
- Outperforms state-of-the-art methods on synthetic benchmarks (Hartmann, Ackley, Levy) and real-world tasks (Rover, MuJoCo, DNA)
- Demonstrates strong performance in both high-dimensional (D=300) and low-dimensional regimes
- Successfully identifies true important dimensions in synthetic tests, converging to ground truth

## Why This Works (Mechanism)

### Mechanism 1
Lasso regularization on GP inverse length scales identifies important variables by forcing coefficients of unimportant parameters to zero. The inverse length scale correlates with function partial derivatives, enabling sparse selection of active dimensions. This assumes the objective has intrinsic low-dimensional structure where only a subset of variables significantly affects the outcome.

### Mechanism 2
Restricting acquisition optimization to the selected important subspace reduces effective dimensionality from D to d_t, lowering computational cost and tightening regret bounds. This works under the assumption that variable selection correctly identifies the true active subspace with high probability, preventing the cubic scaling and large confidence intervals of full-dimensional BO.

### Mechanism 3
Hybrid imputation strategy for unimportant variables balances exploitation and exploration by combining values from the best observed point with uniform random sampling. This prevents the algorithm from getting stuck if variable classification was premature or if unimportant variables have weak but non-zero effects, assuming these variables have low sensitivity.

## Foundational Learning

- **Gaussian Process Kernels & Length Scales**: Needed to understand how inverse length scales (ρ_i) serve as proxies for variable sensitivity. Quick check: If length scale ℓ_i is very large, is ρ_i large or small, and does this imply the variable is important or unimportant? (Answer: Small ρ_i, unimportant).

- **Regularization (L1 vs L2)**: Needed to understand why Lasso (L1) is preferred over Ridge (L2) for subset selection. Quick check: Why is L1 regularization preferred over L2 for selecting a subset of variables? (Answer: L1 forces coefficients to exactly zero, enabling hard variable selection).

- **Regret Bounds (Sublinear Growth)**: Needed to understand the significance of the sublinear cumulative regret bound. Quick check: What does a sublinear cumulative regret bound imply about the algorithm's convergence over time? (Answer: The average regret per iteration approaches zero as T → ∞).

## Architecture Onboarding

- **Component map**: GP Surrogate -> Lasso Estimator -> Variable Selector -> Subspace Generator -> Acquisition Optimizer
- **Critical path**: The Lasso Estimator is the linchpin. If ρ estimates are noisy or λ is poorly tuned, the Variable Selector fails, and the acquisition optimizer searches in the wrong subspace.
- **Design tradeoffs**:
  - λ (Regularization Rate): High λ increases sparsity but risks dropping relevant variables. Default: 10^-3.
  - M_t (Number of Subspaces): High M_t increases exploration but raises optimization cost. Default: ⌈³√t⌉.
- **Failure signatures**:
  - Stagnation: Algorithm stops improving early; likely indicates Lasso erroneously dropped a critical variable and M_t is too low.
  - High Variance: Erratic optimization path; may indicate λ is too low or function has no low-dimensional structure.
- **First 3 experiments**:
  1. Sanity Check: Run on Levy function (D=300, d_e=15), monitor Selected Dimensions plot to verify convergence to true 15 dimensions.
  2. Ablation on λ: Test λ ∈ {10^-1, 10^-3, 10^-5} to observe sensitivity of variable selection.
  3. Baseline Comparison: Compare against Dropout to confirm learned selection outperforms random selection, tracking Log Distance to Optimum.

## Open Questions the Paper Calls Out

### Open Question 1
How does LassoBO performance degrade as the ratio of effective dimensions to total dimensions increases? The method relies on low effective dimensionality and might not work well if the percentage of valid variables is high. Empirical evaluation focused on problems where effective dimensionality (6-15) was small compared to total dimensionality (300).

### Open Question 2
Is LassoBO effective when the low-dimensional effective subspace is not axis-aligned? The method selects specific variables (axes) based on length scales, contrasting with embedding-based methods using random projections. Synthetic benchmarks preserve axis-aligned importance by adding variables, but performance on rotated functions is not demonstrated.

### Open Question 3
How does violating the GP sample path assumption impact the sublinear cumulative regret bound? Theoretical guarantees rely on the assumption that the objective function is a GP sample, which may not be true in some problems. The regret bound is derived strictly under this regularity assumption, leaving behavior on non-GP-like functions undefined.

## Limitations
- Performance degrades when the percentage of important variables is high relative to total dimensions
- Requires the low-dimensional effective subspace to be axis-aligned (not rotated)
- Theoretical regret bounds rely on the assumption that the objective function is a GP sample, which may not hold for all problems

## Confidence
- Claims about achieving sublinear regret bounds: High
- Claims about outperforming baselines on tested benchmarks: High  
- Claims about theoretical novelty of Lasso-based variable selection: High
- Claims about effectiveness on axis-aligned vs rotated subspaces: Medium (based on synthetic test design)
- Claims about performance when important variable ratio is high: Medium (acknowledged limitation but not extensively tested)

## Next Checks
1. Reproduce the Levy function experiment (D=300, d_e=15) to verify variable selection converges to the true 15 dimensions over iterations
2. Test LassoBO on a synthetic function where the important subspace is rotated (not axis-aligned) to assess this limitation
3. Conduct ablation study varying the ratio of effective to total dimensions (e.g., d_e = 50, 100, 150 with D = 300) to quantify performance degradation