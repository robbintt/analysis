---
ver: rpa2
title: 'Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their
  Labels'
arxiv_id: '2511.21038'
source_url: https://arxiv.org/abs/2511.21038
tags:
- label
- natural
- inverted
- semantic
- demonstrations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Small language models (1-12B parameters) cannot override pre-trained
  label semantics through in-context learning. Across eight classification tasks and
  eight open-source models, systematic label inversion fails to produce coherent anti-semantic
  classifiers: semantic override rates remain exactly zero even with 8 demonstrations.'
---

# Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels

## Quick Facts
- **arXiv ID:** 2511.21038
- **Source URL:** https://arxiv.org/abs/2511.21038
- **Reference count:** 16
- **One-line primary result:** Small language models (1-12B parameters) cannot override pre-trained label semantics through in-context learning; semantic override rates remain exactly zero even with 8 demonstrations.

## Executive Summary
This study investigates whether small language models can override their pre-trained label semantics through in-context learning by systematically inverting demonstration labels. Across eight classification tasks and eight open-source models (1-12B parameters), inverted demonstrations consistently fail to produce coherent anti-semantic classifiers. The semantic override rate remains exactly zero, while natural ICL improves accuracy by refining pre-trained priors rather than learning new mappings. These findings demonstrate that ICL operates within stable semantic spaces established during pre-training, unable to redefine label meanings.

## Method Summary
The paper evaluates 8 classification tasks (SST-2, IMDB, SNLI, MNLI, MRPC, QQP, ETHOS, AG News) using 8 open-source models (1-12B parameters) from LLaMA, Mistral, Qwen, and Gemma families. For each task-model combination, the study compares zero-shot performance, natural k-shot ICL (k∈{1,2,4,8}), and inverted k-shot ICL where demonstration labels are systematically permuted. Four metrics are computed: accuracy, prior alignment (agreement with zero-shot), prompt alignment (agreement with demonstrated mapping), and semantic override rate (joint correctness under inverted labels). Greedy decoding with temperature=0 is used, and outputs are parsed via task-specific heuristics.

## Key Results
- Semantic override rates remain exactly 0% across all 320 experimental conditions (8 tasks × 8 models × 4 k values)
- Natural ICL improves accuracy while maintaining strong coupling to zero-shot predictions (prior alignment typically >50%)
- Inverted ICL degrades performance monotonically as k increases, with accuracy dropping from 25-40% (k=1) to near-chance levels (k=8)
- High prompt-following (~50% on sentiment tasks) with zero joint correctness under inverted demonstrations

## Why This Works (Mechanism)

### Mechanism 1: Prior Refinement, Not Flexible Remapping
ICL refines pre-trained classifiers rather than learning de novo input-label mappings. Demonstrations adjust how inputs project onto stable semantic directions learned during pre-training, sharpening correct predictions while suppressing errors—but cannot redefine label meanings. Evidence: semantic override rates = 0% and natural ICL maintains tight zero-shot coupling even with weak priors.

### Mechanism 2: Semantic Anchoring via Token-Level Priors
Label tokens carry rigid semantic commitments that resist contextual override in small models. Tokens like "POS" and "ENTAILMENT" occupy stable representation regions locked by millions of pre-training observations; ICL operates within this geometry rather than reshaping it. Evidence: exactly zero override rates across all 1-12B models tested.

### Mechanism 3: Conflict-Induced Incoherence
When prompt demonstrations contradict pre-trained semantics, models produce outputs satisfying neither constraint. Inverted demonstrations create competing optimization pressures—prompt-following increases only by sacrificing accuracy and prior alignment, producing incoherent predictions. Evidence: accuracy and prior alignment decay while prompt-following increases under inversion, but joint alignment stays at zero.

## Foundational Learning

- **Zero-shot vs. In-Context Classification:** Understanding baseline zero-shot performance is essential for decomposing ICL behavior into prior refinement vs. new learning. Quick check: Can you explain why accuracy improves under natural demonstrations but degrades under inverted ones?

- **Alignment Metrics (Truth/Prior/Prompt):** These three metrics form the core analytical framework for diagnosing ICL behavior. Quick check: Given accuracy=79%, prior alignment=65%, prompt alignment=79%—what can you infer about the relationship between ICL and zero-shot behavior?

- **Label Semantics as Constraints:** The central thesis depends on understanding why tokens like "POS" and "NEG" are not interchangeable symbols. Quick check: Why would switching "positive" to "great" as a label affect model accuracy?

## Architecture Onboarding

- **Component map:** Zero-shot prior f₀(x) -> In-context classifier f_icl(x; S) -> Alignment metrics (Truth/Prior/Prompt) -> Semantic override rate computation

- **Critical path:** 1) Run zero-shot baseline → establish prior 2) Apply natural k-shot demonstrations → measure accuracy + prior alignment 3) Apply inverted demonstrations → measure semantic override rate 4) Compare across k∈{1,2,4,8} to identify monotonic patterns

- **Design tradeoffs:** Model scale (1-12B) vs. semantic flexibility: larger models within this range show no advantage; Demonstration count vs. conflict intensity: more inverted examples increase degradation; Base vs. instruction-tuned: both included to isolate fine-tuning effects

- **Failure signatures:** Semantic override rate = 0.0% (exactly, not approximately); Monotonic accuracy degradation under inverted demonstrations as k increases; High prompt-following (~50%) with zero joint correctness

- **First 3 experiments:** 1) Replicate SST-2 sentiment analysis with natural vs. inverted labels on a single model to verify semantic override = 0%; 2) Test across k∈{1,2,4,8} demonstrations to confirm monotonic degradation pattern under inversion; 3) Measure all three alignment metrics on a task with weak zero-shot prior (e.g., QQP) to observe whether prior refinement still operates when baseline is near-chance

## Open Questions the Paper Calls Out

### Open Question 1
Does the semantic anchoring constraint apply only to labels with pre-existing semantic meaning (e.g., POS/NEG, ENTAILMENT), or does it extend to arbitrary symbol-concept mappings? All tested tasks used labels with strong pre-trained semantic associations. Future work should test with arbitrary label tokens (e.g., "X"/"Y") that have no semantic relationship to the classification task.

### Open Question 2
At what model scale does semantic override become possible, and is the transition discrete or gradual? The study deliberately focused on small open-source models (1–12B), leaving the scale threshold unidentified. Systematic evaluation across a finer-grained model scale spectrum (13B, 30B, 70B, 175B) using identical tasks and metrics would identify where semantic override rate rises above zero.

### Open Question 3
What is the mechanistic basis for semantic anchors in the representation space? The paper proposes a geometric interpretation but provides no empirical validation. Layer-wise analysis of hidden states, probing classifiers for label representations, or attention head ablation studies could identify where and how semantic rigidity manifests.

### Open Question 4
What explains the anomalously weak zero-shot performance on QQP across all models, and why does even weak prior strength not enable semantic override? QQP zero-shot accuracy ranges from 24.7% to 85.0%, often far below other tasks. Analysis of what makes QQP tokens/patterns difficult zero-shot could reveal whether artificially weakened priors on other tasks produce similar patterns.

## Limitations
- The semantic override rate of exactly 0% could reflect architectural limits rather than fundamental impossibility
- Token-level parsing methodology introduces potential brittleness without full specification
- Inverted ICL assumes symmetric label spaces may not generalize to tasks with complex label relationships
- Analysis focuses on accuracy-based metrics without examining alternative evaluation frameworks

## Confidence

**High Confidence**: The monotonic degradation pattern under inverted demonstrations and consistent coupling between ICL and zero-shot predictions across all 320 experimental conditions.

**Medium Confidence**: The claim that ICL operates as "prior refinement" rather than de novo learning, though alternative interpretations involving task-specific attention mechanisms cannot be entirely ruled out.

**Low Confidence**: The assertion that semantic override rates remain "exactly zero" across all conditions, which depends critically on unspecified token-label mapping heuristics.

## Next Checks

1. **Token-Label Mapping Validation**: Implement and test the exact token-label parsing heuristics on a small subset of tasks, manually verifying label assignments and quantifying false negative rates.

2. **Larger Model Scalability Test**: Extend the experimental framework to 30-70B parameter models to test whether semantic override rates remain exactly zero at scale, validating whether the constraint is fundamental or capacity-dependent.

3. **Alternative Semantic Manipulation**: Design demonstrations that manipulate semantic relationships in subtle ways (soft contradictions rather than complete inversion) to test whether semantic anchoring exists on a spectrum rather than being binary.