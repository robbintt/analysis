---
ver: rpa2
title: 'Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL'
arxiv_id: '2506.09359'
source_url: https://arxiv.org/abs/2506.09359
tags:
- equivalence
- query
- evaluation
- queries
- equivalent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating semantic equivalence
  in SQL queries generated by Large Language Models (LLMs), a critical task for refining
  Text-to-SQL systems. The authors propose an LLM-based framework that combines preprocessing,
  efficient string-based matching, and sophisticated LLM reasoning to assess both
  strict and practical ("weak") semantic equivalence.
---

# Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL

## Quick Facts
- **arXiv ID**: 2506.09359
- **Source URL**: https://arxiv.org/abs/2506.09359
- **Reference count**: 40
- **Primary result**: LLM-based framework achieves high accuracy in SQL semantic equivalence evaluation, with Miniature & Mull strategy improving equivalent query identification from 61.25% to 95% on synthetic datasets.

## Executive Summary
This paper addresses the critical challenge of evaluating semantic equivalence in SQL queries generated by Large Language Models (LLMs), a key requirement for refining Text-to-SQL systems. The authors propose a comprehensive LLM-based framework that combines preprocessing, efficient string-based matching, and sophisticated LLM reasoning to assess both strict and practical ("weak") semantic equivalence. The work introduces innovative techniques including characterizing common SQL equivalence patterns, a multi-run strategy to enhance LLM response stability, and advanced prompting strategies like "Miniature & Mull" that simulate query execution. Experiments demonstrate that GPT-4 achieves high accuracy in equivalence assessment, with prompt engineering and advanced techniques significantly improving performance. The methodology provides practical insights for improving the reliability of SQL equivalence evaluation in production Text-to-SQL systems.

## Method Summary
The paper presents a tiered evaluation pipeline that combines string-based matching with LLM reasoning to achieve high precision while reducing computational cost. The approach uses Exact Match (EM) and Exact Set Match (ESM) for deterministic equivalence detection in unambiguous cases, with only remaining queries proceeding to LLM-based evaluation. The LLM evaluation employs a multi-run strategy (3-5 runs with majority voting) and advanced techniques including query rewriting that normalizes subqueries to JOINs, and the "Miniature & Mull" prompting strategy where LLMs simulate query execution on hypothetical databases. The framework processes SQL pairs through preprocessing (standardization), string matching (early exit), optional query rewriting, LLM evaluation with structured prompts, and stability checking before producing final equivalence judgments.

## Key Results
- LLM-based evaluation with Miniature & Mull prompting improved equivalent query identification from 61.25% to 95% on synthetic datasets
- String-based methods (EM/ESM) achieve 100% precision on equivalent cases with 0.5536 recall, cutting approximately 50% of LLM calls
- Query rewriting improves equivalent case accuracy but causes regression on inequivalent pairs (90% to 83.75%)
- Multi-run strategy (3-5 runs) significantly improves stability, with majority voting handling unstable judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A tiered evaluation pipeline combining string-based matching with LLM reasoning achieves high precision while reducing computational cost.
- Mechanism: Exact Match (EM) and Exact Set Match (ESM) handle unambiguous equivalence cases deterministically. Only queries that cannot be resolved through string comparison proceed to LLM-based evaluation, saving approximately 50% of GPT calls on datasets with similar distribution to the tested data.
- Core assumption: A meaningful portion of SQL pairs in production can be resolved through syntactic comparison alone.
- Evidence anchors: [section 8.2.1] Table 3 shows string-based methods achieve 100% precision on equivalent cases with 0.5536 recall; [section 7] describes the preprocessing and string-based methods for efficiency.

### Mechanism 2
- Claim: The "Miniature & Mull" prompting strategy significantly improves semantic equivalence detection by having LLMs simulate query execution on hypothetical databases.
- Mechanism: The LLM is instructed to: (1) create an example database instance, (2) execute both SQL queries on it, (3) modify the database to stress-test edge cases, (4) re-execute and compare outputs. This forces concrete reasoning rather than abstract pattern matching.
- Core assumption: LLMs can reliably simulate SQL execution semantics internally, including case-sensitivity and edge cases.
- Evidence anchors: [section 8.2.3] shows improvement from 61.25% to 95% on equivalent cases; [appendix D.3] provides full prompt template showing explicit instructions for database simulation.

### Mechanism 3
- Claim: Query rewriting that normalizes subqueries to JOINs improves LLM equivalence detection, but introduces regression risk for inequivalent pairs.
- Mechanism: Before LLM evaluation, subqueries are automatically rewritten as LEFT JOINs. This reduces structural variation that confuses LLMs, as the paper observed GPT struggled to compare subquery-based and join-based formulations.
- Core assumption: The rewriting transformation preserves semantic equivalence perfectly and doesn't introduce artifacts.
- Evidence anchors: [section 8.2.3] Table 4 shows equivalent case accuracy improved from 61.25% to 95%, but inequivalent cases dropped from 90% to 83.75%.

## Foundational Learning

- Concept: **Execution Accuracy (EX) Limitations**
  - Why needed here: The entire paper is motivated by EX's false positives (incorrect queries that happen to return correct results on test data) and false negatives (correct queries rejected due to column order, aliases, etc.). Understanding these failure modes is prerequisite to evaluating alternative approaches.
  - Quick check question: Given a sparse test database where all employees earn >$50,000, why would `WHERE department = 'Sales'` incorrectly pass evaluation when the gold query requires `WHERE department = 'Sales' AND salary > 50000`?

- Concept: **Semantic vs. Weak Equivalence**
  - Why needed here: The paper distinguishes strict semantic equivalence (identical results on all possible database states) from practical "weak" equivalence (likely same results on production data). Business applications like Dataverse prioritize the latter.
  - Quick check question: Would `SELECT name FROM restaurants ORDER BY AVG(rating) DESC LIMIT 3` be considered weakly equivalent to `SELECT name, AVG(rating) FROM restaurants ORDER BY 2 DESC LIMIT 3`? Why might strict semantic equivalence reject this?

- Concept: **Multi-Run LLM Stability**
  - Why needed here: LLM outputs are non-deterministic. The paper's pipeline runs evaluation 3 times by default, expanding to 5 runs with majority voting for unstable cases. This pattern is essential for reliable automated evaluation.
  - Quick check question: If an LLM returns "equivalent," "not equivalent," and "equivalent" across three runs, what should the pipeline output per Algorithm 1?

## Architecture Onboarding

- Component map: Preprocessing -> String Matcher (EM/ESM) -> Query Rewriter -> LLM Evaluator -> Stability Handler
- Critical path: Preprocessing → String Matching (early exit if conclusive) → Query Rewrite (improved pipeline) → LLM Evaluation (3-5 runs) → Stability Check → Final Judgment
- Design tradeoffs:
  - String matching first: Cuts ~50% of LLM calls but risks false negatives on syntactically different but semantically equivalent queries
  - Query rewriting: +33.75 percentage points on equivalent pairs but -6.25 on inequivalent pairs—acceptable if catching equivalents is prioritized
  - Multi-run strategy: Higher reliability but 3-5x API costs per query pair
- Failure signatures:
  - **Subquery confusion**: LLMs struggle comparing `WHERE x IN (SELECT...)` to `JOIN` formulations—triggered query rewrite module
  - **Unstable judgments**: High variance across runs indicates prompt needs Chain-of-Thought examples or more explicit criteria
  - **False precision on equivalent, low recall on inequivalent**: Table 2 shows 0.9545 precision/0.8936 recall on equivalent but 0.6429 precision/0.8182 recall on inequivalent—acceptable for debugging but not production quality control
- First 3 experiments:
  1. **Baseline calibration**: Run Algorithm 1 on a held-out set of 20 equivalent and 20 inequivalent pairs to establish precision/recall baseline for your data distribution.
  2. **Ablation on query rewrite**: Compare Algorithm 1 (no rewrite) vs. Algorithm 2 (with rewrite) on synthetic pairs with heavy subquery usage to quantify the regression on inequivalent cases.
  3. **Miniature & Mull vs. standard prompting**: Isolate the prompting strategy by keeping all else constant; measure both accuracy and token cost since M&M requires longer reasoning traces.

## Open Questions the Paper Calls Out

- How can query rewriting techniques be refined to prevent regressions in detecting inequivalent SQL pairs?
- What is the optimal architecture for integrating syntactic, semantic, and execution-based evaluation methods into a unified framework?
- Can advanced reasoning techniques or self-correction mechanisms reliably reduce LLM hallucination in SQL equivalence tasks?

## Limitations
- The approach relies heavily on proprietary GPT-4 models and unspecified prompt templates, limiting reproducibility
- Real-world deployment faces unknown failure modes when query complexity exceeds training distribution or contains edge cases like NULL handling
- Query rewriting introduces a trade-off: significant improvement on equivalent pairs but regression on inequivalent pairs

## Confidence

- **High**: String-based matching efficiency gains (50% reduction in LLM calls), multi-run stability strategy
- **Medium**: LLM equivalence accuracy on synthetic data, Miniature & Mull effectiveness
- **Low**: Query rewriting implementation details, prompt template specifications

## Next Checks

1. **Real-world distribution shift test**: Evaluate the pipeline on production SQL pairs from diverse business domains to measure performance degradation outside the synthetic pattern set.

2. **Edge case stress test**: Construct test cases focusing on NULL handling, correlated subqueries, and complex window functions to identify LLM failure modes not captured in current datasets.

3. **Cost-benefit analysis**: Measure API costs and latency of multi-run strategies across different query complexity tiers to optimize for production deployment scenarios.