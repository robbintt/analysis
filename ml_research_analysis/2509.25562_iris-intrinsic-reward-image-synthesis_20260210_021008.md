---
ver: rpa2
title: 'IRIS: Intrinsic Reward Image Synthesis'
arxiv_id: '2509.25562'
source_url: https://arxiv.org/abs/2509.25562
tags:
- image
- reward
- iris
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IRIS, a reinforcement learning framework that
  improves text-to-image generation using only intrinsic rewards without external
  human-labeled data or domain-specific verifiers. The key insight is that autoregressive
  T2I models with higher self-certainty tend to generate simple, uniform images less
  aligned with human preferences, while lower self-certainty correlates with richer,
  more vivid images.
---

# IRIS: Intrinsic Reward Image Synthesis

## Quick Facts
- **arXiv ID:** 2509.25562
- **Source URL:** https://arxiv.org/abs/2509.25562
- **Reference count:** 40
- **Primary result:** Intrinsic reward framework that improves text-to-image generation without external labeled data or domain-specific verifiers

## Executive Summary
This paper introduces IRIS, a reinforcement learning framework that improves autoregressive text-to-image generation by maximizing negative self-certainty (NSC) as an intrinsic reward signal. The key insight is that autoregressive T2I models with lower self-certainty tend to generate more vivid, detailed images compared to high-certainty outputs which produce simple, uniform visuals. IRIS applies GRPO to optimize this intrinsic reward on both text and image tokens, achieving significant improvements on multiple benchmarks without relying on external human-labeled data or domain-specific verifiers. The framework also incentivizes the emergence of descriptive Chain-of-Thought reasoning that scaffolds higher-quality image synthesis.

## Method Summary
IRIS uses GRPO to optimize Negative Self-Certainty (NSC) as an intrinsic reward for autoregressive T2I generation. The method generates 8 text+image rollouts per prompt, computes per-token NSC using forward KL divergence from uniform distribution, and aggregates to trajectory-level rewards. A semantic CoT generation step produces descriptive text before image generation. The model is trained for 800 steps with LR=1e-6, β=0.01 regularization, and batch size of 8. NSC is applied to both text and image tokens to encourage exploratory behavior in text generation and rich visual details in image generation.

## Key Results
- Improves Janus-Pro-7B by 13.3% on T2I-CompBench, 28.8% on WISE, and 10.7%/4.2% on TIIF-short/long
- Surpasses individual external rewards and matches ensemble external reward performance
- Achieves these gains without external human-labeled data or domain-specific verifiers
- Incentivizes emergence of descriptive Chain-of-Thought reasoning for high-quality image generation

## Why This Works (Mechanism)

### Mechanism 1
Maximizing Negative Self-Certainty (NSC) improves image generation by counteracting the model's tendency to collapse into simple, uniform visual distributions. In T2I generation, high model certainty correlates with "safe," low-detail outputs. By minimizing certainty, the policy is encouraged to explore a broader mode of the distribution, resulting in richer visual features. This contrasts with Math/Code domains where maximizing certainty improves accuracy. If the KL divergence is minimized without an RL policy constraint (e.g., via direct gradient descent), the model generates meaningless noise to maximize entropy.

### Mechanism 2
Applying NSC to text tokens incentivizes the emergence of descriptive Chain-of-Thought (CoT) reasoning, which scaffolds higher-quality image synthesis. Minimizing certainty in the text generation phase encourages the model to "explore" diverse semantic descriptions of the prompt rather than converging on a terse summary. This expanded reasoning provides a richer conditioning signal for the subsequent image generation. If text certainty is maximized (standard RL approach), the model produces brief, uninformative captions, degrading image quality.

### Mechanism 3
Intrinsic rewards enable better generalization than ensembles of external, domain-specific verifiers. External verifiers optimize for narrow metrics, potentially causing "reward hacking" or loss of other capabilities. Intrinsic rewards utilize the model's pre-existing internal priors, allowing balanced improvements across aesthetics, spatial reasoning, and realism simultaneously. The model's internal distribution already contains the necessary latent capabilities for high-quality generation, which need only be "unlocked" rather than "taught" via external supervision.

## Foundational Learning

- **Group-wise Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the specific RL algorithm used to optimize the intrinsic reward. Unlike standard PPO, it relies on group-based advantage estimation rather than a learned value function, which is crucial for stabilizing training on non-differentiable or noisy intrinsic signals.
  - **Quick check question:** How does GRPO estimate the advantage for a specific generation without a value model? (Answer: By comparing the reward of a specific sample against the mean/std of a group of samples generated for the same prompt).

- **Forward vs. Reverse KL Divergence**
  - **Why needed here:** The paper specifically selects Forward KL (uniform vs. policy) to define Self-Certainty. Understanding this distinction is vital; Forward KL is mode-covering (encouraging diversity), while Reverse KL is mode-seeking (encouraging certainty), which explains why Reverse KL (entropy maximization) underperforms in this specific synthesis task.
  - **Quick check question:** Why does Forward KL divergence encourage "mode-covering" behavior compared to Reverse KL? (Answer: Forward KL penalizes the policy if it assigns zero probability to areas where the reference/uniform distribution has mass, forcing it to spread out).

- **Autoregressive Text-to-Image (T2I) Generation**
  - **Why needed here:** Unlike diffusion models, autoregressive models (like Janus-Pro used here) generate images sequentially (often via discrete tokens). This sequential nature allows the application of token-level RL techniques developed for LLMs, which is the foundation of this entire approach.
  - **Quick check question:** In an autoregressive T2I model, how does the generation process differ fundamentally from a diffusion model? (Answer: It predicts the next discrete token in a sequence, rather than denoising a latent representation over continuous steps).

## Architecture Onboarding

- **Component map:** Prompt -> CoT Generation -> Image Generation -> Reward Calculator (NSC) -> Advantage Estimation -> GRPO Update
- **Critical path:**
  1. Input: Prompt (e.g., "A photo of a bench")
  2. CoT Generation: Model generates a descriptive text chain (guided by initial prompt)
  3. Image Generation: Model generates image tokens conditioned on the CoT
  4. Reward Calculation: For every token in the sequence (text+image), calculate -KL(U || π). Sum for total reward
  5. Advantage Estimation: Compare reward of specific generation against the 7 other siblings generated for that prompt
  6. Update: Adjust model weights to maximize probability of high-advantage sequences

- **Design tradeoffs:**
  - RL vs. Gradient Descent: You cannot simply maximize NSC via direct backprop (gradient descent). It leads to noise. The RL framework (specifically GRPO's implicit boundaries) is required to constrain the search to "grounded" high-uncertainty regions
  - Token Selection: Must apply NSC to *both* text and image tokens. Applying only to image tokens shows early success but degrades rapidly

- **Failure signatures:**
  - Grey/Uniform Images: Indicates the model is maximizing Self-Certainty (behaving like a standard Math/Code LLM) instead of Negative Self-Certainty
  - Noisy/Garbage Output: Indicates the model is maximizing NSC via direct optimization or the regularization coefficient (β) is too low, allowing the distribution to flatten into pure noise
  - Stagnant CoT: If text SC is maximized instead of minimized, the reasoning chain will remain short and non-descriptive

- **First 3 experiments:**
  1. Self-Certainty Sign Check: Train two small models—one maximizing SC, one maximizing NSC—on a subset of GenEval. Verify that SC produces simple images while NSC produces detailed ones
  2. RL vs. GD Ablation: Attempt to maximize NSC using direct gradient descent on a small model. Confirm the rapid degradation to noise described in Section 4.3 to ensure the GRPO implementation is necessary
  3. CoT vs. No-CoT: Run IRIS with and without the intermediate text generation step. Quantify the performance gap on T2I-CompBench to justify the computational overhead of generating reasoning chains

## Open Questions the Paper Calls Out

### Open Question 1
Can intrinsic reward signals be effectively adapted for non-autoregressive T2I architectures? The paper identifies adapting IRIS to continuous diffusion, masked-modeling, and MAE-style models as a key future direction, as IRIS relies on GRPO over autoregressive token probabilities which do not directly map to the latent noise prediction steps or parallel decoding of diffusion/masked models.

### Open Question 2
What is the theoretical limit for minimizing self-certainty before the model collapses into incoherence? The ablation study shows that directly maximizing Negative Self-Certainty via gradient descent fails, suggesting that "infinite" uncertainty (randomness) degrades quality, but the paper does not quantify the boundary where low certainty becomes semantic noise.

### Open Question 3
Why does minimizing text-token self-certainty improve T2I performance when it contradicts the trend in pure text reasoning? The authors hypothesize a need for "explorative" text but do not confirm the mechanism, leaving unclear whether the gain comes from the intrinsic reward signal itself or simply from the resulting longer, more descriptive Chain-of-Thought data.

## Limitations

- **Scalability to larger models:** The framework's effectiveness on larger or more diverse autoregressive architectures remains untested
- **Task scope limitations:** The framework focuses on generating visually detailed images but doesn't address semantic accuracy or prompt alignment as primary objectives
- **Hyperparameter sensitivity:** The framework depends on several critical hyperparameters without systematic sensitivity analysis

## Confidence

**High Confidence:** The empirical claim that IRIS improves autoregressive T2I generation performance across multiple benchmarks is well-supported by the reported results showing consistent gains over baselines.

**Medium Confidence:** The claim that IRIS matches or surpasses ensemble external reward performance is supported by the data, though the comparison is limited to specific metrics and model sizes.

**Low Confidence:** The claim that IRIS "incentivizes emergence of descriptive Chain-of-Thought reasoning" is primarily supported by ablation studies showing CoT generation helps performance, but the deeper mechanism needs further validation.

## Next Checks

1. **Domain transfer validation:** Apply IRIS to a completely different autoregressive generation task (e.g., story generation or code synthesis) to test whether the self-certainty reversal principle holds across domains.

2. **Semantic accuracy assessment:** Conduct a human evaluation specifically measuring prompt adherence and semantic accuracy alongside visual detail to ensure IRIS doesn't optimize for detail at the expense of following instructions.

3. **Cross-model scalability test:** Apply the framework to a different autoregressive T2I architecture to verify that the self-certainty relationship to image quality is architecture-agnostic.