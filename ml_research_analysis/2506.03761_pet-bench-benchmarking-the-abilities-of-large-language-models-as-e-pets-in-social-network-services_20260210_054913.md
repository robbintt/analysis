---
ver: rpa2
title: 'Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in
  Social Network Services'
arxiv_id: '2506.03761'
source_url: https://arxiv.org/abs/2506.03761
tags:
- language
- arxiv
- large
- evaluation
- interactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pet-Bench is a new benchmark for evaluating Large Language Models
  as virtual pets on social networks. It measures both human-interaction and self-interaction,
  covering 7,500+ instances across tasks like pet chat, note understanding, memory,
  and daily routines.
---

# Pet-Bench: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services

## Quick Facts
- arXiv ID: 2506.03761
- Source URL: https://arxiv.org/abs/2506.03761
- Reference count: 40
- Models: 28 LLMs evaluated on 7,815 instances across 14 tasks measuring pet companionship capabilities

## Executive Summary
Pet-Bench is a comprehensive benchmark designed to evaluate Large Language Models as virtual pets in social network services. It uniquely assesses both human-interaction (PetChat, PetNote) and self-interaction (PetMemory, PetRoutine) dimensions through 7,815 instances across 14 subtasks. The benchmark reveals that while closed-source models like GPT-4o achieve the highest average scores (41.20), open-source models such as Qwen-2.5-72B-Instruct perform comparably (40.87). The evaluation highlights that larger models generally outperform smaller ones, but complex tasks like memory rewriting remain challenging even for top-performing models.

## Method Summary
The benchmark uses a composite scoring system combining BLEU-n, ROUGE, and semantic similarity metrics, with GPT-4o serving as a supplementary evaluator for routine generation tasks. It evaluates 28 LLMs across 7,815 instances spanning 14 subtasks, with input tokens totaling 4,288,651 and average input length of 548.77 words. The evaluation is inference-only, requiring no training, and was conducted using 128 NVIDIA H800 GPUs. Each model generates responses to benchmark prompts, which are then scored against reference outputs using automated metrics and, for certain tasks, GPT-4o scoring.

## Key Results
- GPT-4o achieves highest average score (41.20), closely followed by Qwen-2.5-72B-Instruct (40.87)
- Larger models consistently outperform smaller models across most tasks
- Memory-intensive tasks (PetMemory-Rewrite) show significant performance gaps, with even top models scoring below 50
- Open-source models demonstrate strong performance, narrowing the gap with closed-source counterparts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Virtual pet companionship requires both human-interaction AND self-interaction capabilities to achieve authentic engagement.
- Mechanism: Human-interaction enables responsive dialogue with users, while self-interaction allows autonomous simulation of pet behaviors, routines, and memory formation. This dual mode mirrors real pet companionship where pets have independent behaviors alongside interactive responses.
- Core assumption: Authentic companionship emerges from the combination of reactive engagement and autonomous internal processes, not from interaction alone.
- Evidence anchors:
  - [abstract] "Pet-Bench evaluates LLMs across both self-interaction and human-interaction dimensions"
  - [section 3.1] "Human-interaction enables meaningful engagement with users through dialogue and responsive behaviors, while self-interaction allows the model to autonomously simulate pet-like behaviors, routines, and memories"
  - [corpus] MoodBench (arxiv:2511.18926) focuses on emotional companionship dialogue but lacks the self-interaction dimension Pet-Bench introduces
- Break condition: If autonomous pet behaviors don't contribute meaningfully to user-perceived companionship quality, the dual-mode advantage collapses.

### Mechanism 2
- Claim: Model scale predicts performance on complex tasks but shows diminishing returns on simpler interactions.
- Mechanism: Larger models (72B+ parameters) demonstrate superior performance on tasks requiring long-context reasoning (PetMemory-Rewrite: Qwen-2.5-72B-Instruct scores 51.31 vs. 1.5B models scoring ~15-20), while simpler tasks show compressed score ranges (PetRoutine Level III: 78-90 across most models).
- Core assumption: The performance gap between model sizes reflects genuine capability differences in handling complexity, not just benchmark artifacts.
- Evidence anchors:
  - [abstract] "Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities"
  - [table 2] Qwen-2.5-72B-Instruct achieves 40.87 average vs. Qwen-2.5-1.5B at 24.44; however, PetChat-General shows narrower gaps (18.43 vs. 16.19)
  - [corpus] No direct corpus evidence on scale-performance relationships in companion AI benchmarks
- Break condition: If instruction-tuning accounts for more variance than raw scale, the size-based optimization guidance would need revision.

### Mechanism 3
- Claim: Memory-intensive tasks expose fundamental LLM limitations that persist even in top-performing models.
- Mechanism: PetMemory-Rewrite requires reconstructing experiences into coherent narratives across time, demanding both comprehension and generative synthesis. Even GPT-4o scores only 48.92 on this task, while achieving 71.97 on structured routine generation—a ~23-point gap suggesting distinct capability demands.
- Core assumption: The performance differential reflects genuine architectural constraints in temporal reasoning and narrative coherence, not insufficient training data.
- Evidence anchors:
  - [section 4.3.1] "Tasks like PetMemory-Rewrite pose significant challenges, with even top models struggling"
  - [table 2] Memory-Rewrite scores cluster in 10-51 range across all models, while Routine tasks show 45-90 range
  - [corpus] MoodBench identifies similar challenges in emotional dialogue systems but doesn't isolate memory-specific limitations
- Break condition: If the task formulation (rather than model capabilities) creates artificial difficulty, optimization should target benchmark design rather than model architecture.

## Foundational Learning

- **Concept: Role-Playing vs. Persona-Driven Agents**
  - Why needed here: Pet-Bench moves beyond simple role-playing (static persona matching) to developmental behaviors where pets evolve through interactions. Understanding this distinction clarifies why existing benchmarks fail to capture companionship quality.
  - Quick check question: Can you explain why a model that perfectly mimics pet speech patterns might still fail at authentic companionship?

- **Concept: Episodic Memory in LLMs**
  - Why needed here: PetMemory tasks require summarization, rewriting, and proactive dialogue based on stored experiences—fundamentally different from context-window memory. The benchmark assumes models can maintain coherent identity across sessions.
  - Quick check question: How would you distinguish between a model using in-context examples versus one demonstrating genuine episodic recall?

- **Concept: Multi-Metric Evaluation for Generative Tasks**
  - Why needed here: Pet-Bench uses a composite score (BLEU-n, ROUGE, semantic similarity) plus GPT-4o scoring for routine generation. Understanding why single metrics fail here prevents misinterpreting results.
  - Quick check question: Why might a model score high on BLEU but low on semantic similarity for pet companion responses?

## Architecture Onboarding

- **Component map:**
  ```
  Pet-Bench
  ├── Human-Interaction
  │   ├── PetChat (General | Psychological | Routine-based)
  │   └── PetNote (Extraction | Conversation | Intent | Greeting)
  └── Self-Interaction
      ├── PetMemory (Summary | Rewrite | Greet)
      └── PetRoutine (Level I | II | III | Feedback | Greet)
  ```

- **Critical path:** Start with PetChat-General (highest instance count: 2,511) to establish baseline conversational ability, then progress to PetMemory-Rewrite (most challenging task) to stress-test temporal reasoning. PetRoutine Level III (143 instances, highest complexity) reveals planning capabilities.

- **Design tradeoffs:**
  - Closed-source models (GPT-4o: 41.20) offer ~1% improvement over top open-source (Qwen-2.5-72B-Instruct: 40.87), but open-source enables fine-tuning for domain-specific pet behaviors
  - Instruction-tuned variants consistently outperform base models (Qwen-2.5-7B vs. Qwen-2.5-7B-Instruct: 29.83 vs. 37.59), suggesting prompt engineering alone insufficient
  - Evaluation combines automated metrics with GPT-4o scoring—faster but may miss nuanced emotional appropriateness

- **Failure signatures:**
  - Low PetMemory-Rewrite scores (below 30) indicate temporal coherence failures
  - High PetRoutine but low PetChat scores suggest rigid planning without adaptive conversation
  - Large gaps between instruct and base variants indicate prompt sensitivity issues

- **First 3 experiments:**
  1. Run Qwen-2.5-7B-Instruct on PetChat-General and PetMemory-Rewrite to establish open-source baseline; expect ~15 vs. ~40 score gap indicating task-complexity sensitivity
  2. Compare PetRoutine Level I vs. III scores on your target model; convergent scores suggest strong planning, divergent scores indicate complexity thresholds
  3. Test instruction-tuned vs. base variant on PetNote Intent Recognition; large gaps (>10 points) suggest retrieval-augmented dialogue needs prompt optimization rather than architectural changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specialized optimization techniques can effectively improve model performance on complex tasks like Memory Rewriting?
- Basis in paper: [explicit] The Conclusion states that the evaluation reveals performance gaps, "underscoring the need for specialized optimization in this domain," specifically noting that complex tasks like memory rewriting remain challenging.
- Why unresolved: The paper benchmarks existing models but does not propose or validate specific training methods to overcome the identified deficiencies in long-term memory reconstruction.
- What evidence would resolve it: Experiments demonstrating that specific fine-tuning strategies or architectural modules significantly improve scores on the PetMemory Rewrite subset compared to baseline models.

### Open Question 2
- Question: Do the automated evaluation metrics (BLEU, ROUGE, Semantic Similarity) correlate strongly with human perceptions of "emotional immersion"?
- Basis in paper: [inferred] The paper prioritizes "emotionally rich experiences" and "psychological conversations," yet relies primarily on n-gram overlap and semantic similarity metrics which may fail to capture nuanced emotional quality.
- Why unresolved: The study validates model outputs using automated metrics and GPT-4 scoring but lacks a human evaluation study to confirm that high-scoring outputs are perceived as emotionally supportive.
- What evidence would resolve it: A human annotation study comparing model rankings from Pet-Bench metrics against human rankings of empathy and companionship quality.

### Open Question 3
- Question: How well does performance on static benchmark instances predict a model's ability to maintain "self-evolution" over long-term, dynamic interactions?
- Basis in paper: [inferred] The Introduction defines "self-evolution" and "developmental behaviors" as core components of the benchmark, but the Methodology relies on static interaction instances rather than continuous, longitudinal simulations.
- Why unresolved: Evaluating memory and routine on static data may not reflect the accumulated errors or persona drift that would occur in a real-time, evolving social network service.
- What evidence would resolve it: Longitudinal deployment results showing that high Pet-Bench scores correlate with stable persona consistency and memory retention over extended multi-session interactions.

## Limitations
- Benchmark relies on GPT-4o for scoring complex tasks without transparent rubrics
- Generation hyperparameters and exact prompt templates for all subtasks are unspecified
- Performance gap between open-source and closed-source models may reflect dataset familiarity rather than genuine capability differences
- Static evaluation instances may not reflect real-world dynamic interaction capabilities

## Confidence

- Mechanism 1 (dual-mode companionship): **Medium** - Strong theoretical foundation but limited empirical validation beyond benchmark performance
- Mechanism 2 (scale-performance relationship): **High** - Consistent patterns across 28 models with clear score differentials
- Mechanism 3 (memory limitations): **Medium** - Large performance gaps observed, but task difficulty characterization needs further validation

## Next Checks
1. Replicate the full evaluation pipeline on a subset of 3-5 representative tasks to verify metric consistency and scoring variance
2. Conduct ablation studies comparing prompt-tuned vs. instruction-tuned models on PetNote Intent Recognition to isolate prompt sensitivity effects
3. Test cross-dataset generalization by evaluating models on MoodBench and Pet-Bench to identify systematic performance gaps in emotional companionship tasks