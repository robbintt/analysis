---
ver: rpa2
title: 'MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models'
arxiv_id: '2507.20395'
source_url: https://arxiv.org/abs/2507.20395
tags:
- spatial
- reasoning
- maze
- navigation
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MazeEval, a benchmark for evaluating spatial
  reasoning in LLMs using coordinate-based maze navigation tasks without visual cues.
  The benchmark tests models' ability to maintain spatial state and make sequential
  decisions using only coordinate feedback and distance-to-wall information.
---

# MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models

## Quick Facts
- **arXiv ID**: 2507.20395
- **Source URL**: https://arxiv.org/abs/2507.20395
- **Reference count**: 0
- **Key outcome**: MazeEval evaluates spatial reasoning in LLMs using coordinate-based mazes; OpenAI's O3 achieved perfect navigation up to 30×30 grids while other models failed catastrophically beyond 9×9 due to excessive looping behavior

## Executive Summary
MazeEval is a novel benchmark designed to evaluate sequential decision-making and spatial reasoning capabilities in large language models using coordinate-based maze navigation tasks. The benchmark tests models' ability to maintain spatial state and make sequential decisions using only position coordinates and distance-to-wall feedback, without visual cues. Eight state-of-the-art LLMs were evaluated on mazes ranging from 5×5 to 15×15 grids in both English and Icelandic, revealing significant performance differences across model sizes and languages.

The benchmark exposes fundamental limitations in how LLMs process spatial information, with most models failing catastrophically at 9×9 mazes due to excessive looping behavior (revisiting cells ≥10 times). Cross-linguistic analysis revealed a significant performance degradation in Icelandic, with models solving 3-4 maze sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These findings have important implications for the deployment of LLM-powered autonomous systems in global contexts.

## Method Summary
The MazeEval benchmark uses DFS-generated mazes with exactly one solution path, testing LLMs on 5×5 to 15×15 grids using a function-calling interface. Models receive current (x,y) position, goal position, distance-to-wall in each direction, and complete travel history as inputs. The task requires navigating to the goal using only coordinate feedback without visual cues, with constraints of max 10 visits per cell and max 3n² moves for n×n mazes. Success rate within move limits and step efficiency (optimal/actual steps) are measured, with failure classification showing 100% of failures attributed to excessive looping behavior (≥10 cell revisits).

## Key Results
- OpenAI's O3 achieved perfect navigation across all maze sizes up to 30×30 grids
- All other tested models failed catastrophically beyond 9×9 mazes with 100% failure rate
- 100% of failures were attributed to excessive looping behavior (revisiting cells ≥10 times)
- Cross-linguistic analysis showed 3-4 maze size performance degradation in Icelandic compared to English

## Why This Works (Mechanism)
The benchmark isolates spatial reasoning from visual processing by using coordinate-only navigation, forcing models to maintain and update internal spatial representations purely through sequential decision-making. This design reveals whether spatial reasoning emerges from linguistic patterns or language-agnostic mechanisms, as evidenced by the significant performance differences between English and Icelandic.

## Foundational Learning
- **DFS maze generation**: Creates single-solution paths essential for controlled evaluation
  - *Why needed*: Ensures consistent difficulty and unambiguous success criteria
  - *Quick check*: Verify each maze has exactly one path from start to goal
- **Coordinate-based spatial reasoning**: Models must track position without visual context
  - *Why needed*: Tests pure spatial working memory and sequential decision-making
  - *Quick check*: Compare performance on coordinate vs. visual navigation tasks
- **Function-calling interface**: Structured interaction for navigation commands
  - *Why needed*: Enables precise control over model inputs and outputs
  - *Quick check*: Validate JSON schema matches expected format
- **Cell visit tracking**: Monitors looping behavior as primary failure mechanism
  - *Why needed*: Identifies spatial memory limitations in LLMs
  - *Quick check*: Plot visit counts per cell to identify looping patterns
- **Cross-linguistic comparison**: Tests language dependence of spatial reasoning
  - *Why needed*: Reveals whether spatial cognition is language-agnostic
  - *Quick check*: Test additional language pairs beyond English/Icelandic
- **Move limit constraints**: Prevents infinite exploration and defines task boundaries
  - *Why needed*: Ensures fair evaluation and identifies efficiency issues
  - *Quick check*: Verify 3n² move limit is sufficient for optimal solutions

## Architecture Onboarding

**Component Map**: Maze Generator -> Function Caller -> LLM -> Evaluation Loop -> Results Analysis

**Critical Path**: Maze generation → API interface setup → Sequential decision loop → Loop detection → Performance metrics

**Design Tradeoffs**: Coordinate-only interface vs. visual cues (isolates spatial reasoning but less realistic); single language pair testing (controlled comparison but limited generalizability); fixed maze seeds (reproducibility vs. generalization)

**Failure Signatures**: Excessive looping (≥10 cell revisits) at 9×9 threshold; sharp performance drop between 9×9 and 10×10; consistent failure pattern across multiple model attempts

**First Experiments**:
1. Verify DFS maze generation produces single-solution paths by tracing all generated mazes
2. Test function-calling interface with a simple navigation task to validate API integration
3. Run O3 on 5×5 mazes to confirm baseline perfect performance before scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Maze generation methodology uses unknown DFS seeds, making exact reproduction difficult
- Cross-linguistic analysis only tests English and Icelandic, limiting generalizability to broader multilingual contexts
- Coordinate-only interface represents an artificial constraint that may not reflect real-world navigation scenarios

## Confidence

**High confidence**: Identification of looping as primary failure mechanism (100% of failures from ≥10 cell revisits); size-dependent performance degradation showing catastrophic failure beyond 9×9 grids

**Medium confidence**: Spatial reasoning emerges from linguistic patterns rather than language-agnostic mechanisms (supported by Icelandic-English comparison but requires additional language pairs)

**Low confidence**: Implications for global deployment of LLM-powered autonomous systems (extends beyond benchmark's scope of abstract coordinate navigation)

## Next Checks
1. Reproduce exact maze generation with documented seeds to verify single-solution property and compare failure patterns across different configurations
2. Test additional language pairs (e.g., Mandarin, Arabic, Swahili) to determine if linguistic interference pattern generalizes beyond English-Icelandic
3. Implement mixed-modality version incorporating partial visual cues to assess integration of spatial information from multiple input channels