---
ver: rpa2
title: 'Failing to Explore: Language Models on Interactive Tasks'
arxiv_id: '2601.22345'
source_url: https://arxiv.org/abs/2601.22345
tags:
- qwen2
- b-instruct
- queries
- reward
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well language models (LMs) explore unknown\
  \ environments under a limited interaction budget. It introduces three parametric\
  \ tasks\u2014HillSearch, TreeSearch, and MaxSatSearch\u2014with controllable difficulty\
  \ and traps to test exploration."
---

# Failing to Explore: Language Models on Interactive Tasks

## Quick Facts
- arXiv ID: 2601.22345
- Source URL: https://arxiv.org/abs/2601.22345
- Reference count: 40
- Language models consistently under-explore under limited interaction budgets, committing early to suboptimal solutions even when better options exist.

## Executive Summary
This paper evaluates how well language models (LMs) explore unknown environments under a limited interaction budget. It introduces three parametric tasks—HillSearch, TreeSearch, and MaxSatSearch—with controllable difficulty and traps to test exploration. Across models including GPT-5 and Qwen, performance was consistently worse than simple explore–exploit baselines, with models committing early to suboptimal solutions and scaling weakly with budget. Two lightweight interventions improved results: parallelizing a fixed budget into independent threads, and periodic summarization of the interaction history. Both interventions consistently improved performance across tasks and difficulty levels, despite theory showing no gain from parallelization in optimal strategies, indicating practical benefit for LM exploration.

## Method Summary
The study introduces three parametric exploration tasks with controllable difficulty and traps: HillSearch (find global max of hidden Gaussian function), TreeSearch (find max-reward node in hidden tree via connected queries), and MaxSatSearch (maximize satisfied clauses with hidden formula). Models are evaluated via API/local inference with temperature T=0.7, top-p=0.95 across N-round episodes. Key interventions tested include parallel threads (p∈{2,3,4}) and periodic summarization (s∈{2,3,4,6}). Performance is measured by normalized reward (achieved/max_possible) and scaling with budget N∈{36,48,60}, compared against explore-exploit baselines.

## Key Results
- Models consistently under-explore and commit early to trap solutions across all three tasks
- Performance scales weakly with interaction budget N, failing to match explore-exploit baselines
- Both parallelization and periodic summarization interventions consistently improve exploration performance
- Theoretical benefit of parallelization is zero for optimal strategies, yet practical gains are observed

## Why This Works (Mechanism)
The paper identifies systematic under-exploration as the core failure mode, where models commit early to local optima or trap solutions without fully utilizing available interaction budget. The proposed interventions work by addressing different aspects: parallelization creates independent exploration paths that avoid early commitment, while periodic summarization manages context length issues that prevent effective long-horizon exploration.

## Foundational Learning

**Parametric task design** - Creating controllable environments with tunable difficulty and known optimal strategies. *Why needed:* To isolate exploration failures from other confounding factors in real-world benchmarks. *Quick check:* Can generate instances where optimal solution requires specific number of queries.

**Explore-exploit tradeoff** - The fundamental tension between gathering information and exploiting known good options. *Why needed:* Provides theoretical baseline for optimal performance to compare against model behavior. *Quick check:* Calculate expected reward for given α parameters on each task.

**Context window management** - Handling the accumulation of interaction history in language model inference. *Why needed:* Long histories degrade model performance, making exploration harder as N increases. *Quick check:* Track performance degradation as context length grows.

## Architecture Onboarding

**Component map:** Task Environment -> Oracle -> Model API -> Performance Evaluation -> Intervention Application

**Critical path:** Task instance generation → Oracle feedback provision → Model query generation → Reward calculation → Performance comparison to baseline

**Design tradeoffs:** Simple parametric tasks vs. ecological validity, synthetic exploration vs. real-world complexity, intervention simplicity vs. theoretical optimality

**Failure signatures:** Early commitment to local optima (HillSearch), trap gateway selection (TreeSearch), premature convergence to suboptimal clause sets (MaxSatSearch)

**First experiments:** 1) Implement baseline explore-exploit algorithms for each task, 2) Run Qwen2.5-7B on HillSearch with N=48 to verify scaling patterns, 3) Test parallel intervention on TreeSearch to confirm performance improvement

## Open Questions the Paper Calls Out

**Generalization to real-world domains** - Whether exploration failures in parametric toy tasks extend to complex domains like software engineering or web navigation remains unknown. The study uses controlled synthetic environments specifically to isolate exploration from other skills, so real-world applicability is uncertain.

**Native explore-exploit strategies** - Can models be trained or architected to implement effective explore-exploit strategies without external interventions? The paper demonstrates interventions work but doesn't address whether underlying model weights or architectures can be modified to resolve systematic under-exploration.

**Intervention combination effects** - The interaction between parallel budget allocation and periodic summarization is unexplored. While both interventions improve performance independently, it's unclear if they address orthogonal failure modes or if combining them would compound benefits or create redundancy.

**Scaling beyond tested budgets** - Performance asymptotes or degradation at significantly higher interaction budgets (N>60) is unknown. The paper notes weak scaling with N, but whether longer horizons exacerbate context-related failures or eventually allow recovery through exhaustive local exploration remains untested.

## Limitations

- Study uses synthetic parametric tasks rather than real-world interactive environments
- GPT-5 models mentioned are fictional/future models, requiring substitution with current frontier models
- Theoretical analysis of parallelization benefits conflicts with empirical observations, leaving mechanism unclear

## Confidence

- Core finding (LMs struggle with exploration): High
- Specific quantitative results and scaling trends: Medium
- Theoretical analysis of parallelization benefits: Low

## Next Checks

1. Implement the three task environments and oracle with controlled difficulty parameters, then validate against the provided baseline algorithms at N=48 to establish expected performance ranges.

2. Test Qwen2.5-7B-Instruct on HillSearch with both standard and parallel-thread interventions to verify the reported performance improvements and scaling patterns.

3. Compare TreeSearch performance across multiple current frontier models (GPT-4o, Claude) with the parallel intervention to assess whether the improvement generalizes beyond the Qwen model used in the original study.