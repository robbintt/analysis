---
ver: rpa2
title: Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement Learning
arxiv_id: '2503.19007'
source_url: https://arxiv.org/abs/2503.19007
tags:
- subgoal
- option
- robot
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient exploration and
  generalization in reinforcement learning (RL) for complex robotic tasks. It proposes
  LDSC, a hierarchical RL framework that integrates Large Language Models (LLMs) to
  guide subgoal generation and option selection.
---

# Option Discovery Using LLM-guided Semantic Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.19007
- Source URL: https://arxiv.org/abs/2503.19007
- Authors: Chak Lam Shek; Pratap Tokekar
- Reference count: 40
- LDSC outperforms baselines by 55.9% in average reward, 53.1% reduction in completion time, and 72.7% improvement in success rates

## Executive Summary
This paper addresses the challenge of efficient exploration and generalization in reinforcement learning for complex robotic tasks by proposing LDSC, a hierarchical RL framework that integrates Large Language Models (LLMs) to guide subgoal generation and option selection. LDSC uses a three-level hierarchy: a subgoal policy that selects subgoals based on LLM reasoning, an option policy that chooses reusable options, and an action policy that executes low-level actions. The method constructs a subgoal relation tree and an option tree to systematically explore and refine task decompositions. Experimental results show that LDSC significantly outperforms baseline methods (DSC, DDPG, Option-Critic) across four Mujoco maze environments while maintaining similar training time to DSC.

## Method Summary
LDSC integrates LLM-guided semantic subgoal generation with hierarchical reinforcement learning. The LLM receives natural language task descriptions and environment constraints, then generates k possible subgoal sequences that form a subgoal relation tree. A three-level hierarchy is trained: the subgoal policy uses DQN to select from available subgoals, the option policy uses DQN to choose from executable options (either global exploration options or learned goal options), and the action policy executes low-level control via DDPG. Goal-oriented options are learned from successful trajectories and can be reused across different tasks. The framework uses SMDP Q-learning for efficient long-horizon credit assignment, and initiation classifiers determine when options can be executed.

## Key Results
- LDSC achieves 55.9% higher average reward compared to baseline methods
- Task completion time reduced by 53.1% compared to baselines
- Success rate improved by 72.7% over competing approaches
- Maintains similar training time to DSC while outperforming it across all metrics

## Why This Works (Mechanism)

### Mechanism 1: LLM-Guided Semantic Subgoal Generation
LLMs provide structured task decomposition into semantically meaningful subgoals that reduce exploration complexity. The LLM receives natural language task descriptions, environment constraints, and initial state, then generates k possible subgoal sequences using reasoning. These sequences form a subgoal relation tree enabling systematic exploration of decomposition paths.

### Mechanism 2: Goal-Oriented Option Learning and Reuse
Structuring options around semantic subgoals rather than state-transitions enables cross-task transfer and reduces retraining. Options are initialized as "global options" with initiation sets covering the full state space. After successful subgoal achievement, "goal options" are learned from trajectories, and the initiation classifier is trained to identify states from which the option can successfully execute.

### Mechanism 3: Hierarchical Credit Assignment via SMDP Q-Learning
Three-level hierarchy with SMDP Q-learning enables efficient long-horizon credit assignment. The subgoal policy uses DQN to learn Q-values for subgoal selection, the option policy learns Q-values for option selection, and the action policy executes low-level control. The SMDP formulation handles variable-duration options through temporal aggregation in the target computation.

## Foundational Learning

- **Concept: Options Framework (Sutton et al.)**
  - Why needed here: LDSC's core contribution is option discovery—you must understand the (I, π, β) tuple structure to implement initiation classifiers, intra-option policies, and termination conditions correctly.
  - Quick check question: Can you explain why an option's initiation set restricts where it can be executed, and how this enables skill chaining?

- **Concept: Semi-Markov Decision Processes**
  - Why needed here: The Q-learning updates use SMDP formulation to handle temporally extended actions. You need to understand why standard MDP Q-learning fails for variable-duration options.
  - Quick check question: Why does the SMDP Q-target include γτ rather than γ, and what does τ represent in the option execution context?

- **Concept: Skill Chaining**
  - Why needed here: LDSC extends Deep Skill Chaining—you need to understand how skills are incrementally discovered by learning to reach initiation sets of existing skills, building backward from goals.
  - Quick check question: In skill chaining, why are new skills learned to reach the initiation set of existing skills rather than the skills themselves?

## Architecture Onboarding

- **Component map:**
  - LLM Reasoning Module (pre-training only): Receives (task_description, environment_description, initial_state) → outputs k subgoal sequences
  - Subgoal Relation Tree: DAG structure with shared nodes for overlapping subgoals; built once before training
  - Subgoal Policy (Qϕ): DQN selecting from available subgoals given current state and task
  - Option Tree: Hierarchical structure connecting subgoals via options; grows during training
  - Option Policy (Qψ): DQN selecting from executable options (O′(st) = {oi | Ioi(st) = 1 ∩ βoi(st) = 0})
  - Action Policy: Low-level control (DDPG actor for continuous action spaces)
  - Initiation Classifier: Binary classifier trained per option to determine valid execution states

- **Critical path:**
  1. LLM generates subgoal sequences → Subgoal Relation Tree constructed (Equation 3-4)
  2. Global options initialized for each subgoal
  3. During training: Subgoal Policy selects gi → Option Policy selects oi → Action Policy executes
  4. Upon subgoal achievement: Goal option learned, initiation classifier trained, Option Tree expanded (Equation 11)

- **Design tradeoffs:**
  - LLM query cost vs. exploration efficiency: LLM called only once pre-training (minimal runtime overhead) but requires well-structured prompts and environment descriptions
  - Tree breadth (k sequences) vs. planning complexity: More sequences increase robustness to LLM errors but expand search space
  - Option generality vs. task-specificity: Goal-oriented options transfer better but may require more training data than state-specific options

- **Failure signatures:**
  - Subgoal infeasibility: Agent oscillates near subgoal without achieving it → LLM generated physically impossible or poorly defined subgoal
  - Option starvation: Option Policy always selects global option → Initiation classifiers too restrictive or insufficient exploration
  - Tree explosion: Memory/performance degradation → Too many subgoal sequences or insufficient node merging
  - Credit assignment failure: Q-values unstable or converging to poor policies → Subgoal horizons still too long; consider additional decomposition

- **First 3 experiments:**
  1. Simple maze validation: Implement LDSC on a 2-room navigation task with 1 subgoal; verify LLM produces correct subgoal sequence and option learning converges. Compare success rate and sample efficiency against flat DDPG baseline.
  2. Ablation on k (number of subgoal sequences): Test k=1, 2, 3 on Point Maze environment. Measure impact on success rate and robustness to LLM reasoning errors. Identify minimum k for reliable performance.
  3. Option transfer test: Train options on Task 1 (e.g., navigate to key then goal), freeze option policies, test zero-shot transfer on Task 2 with different goal position but same subgoal structure. Validate the generalization claim empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LDSC performance scale when applied to high-dimensional robotic tasks?
- **Basis in paper:** The conclusion explicitly lists "scaling LDSC to high-dimensional robotic tasks" as a future research direction.
- **Why unresolved:** The current experiments are restricted to simulated point navigation tasks (Mujoco) with relatively low-dimensional state spaces.
- **What evidence would resolve it:** Demonstrating that LDSC maintains its efficiency and success rate in high-DoF manipulation tasks or complex locomotion scenarios.

### Open Question 2
- **Question:** How can the framework be extended to support multi-agent collaboration and lifelong learning?
- **Basis in paper:** The authors state that "investigating multi-agent collaboration and lifelong learning extensions for LDSC could open new directions."
- **Why unresolved:** The current implementation focuses on single-agent decision-making without mechanisms for continual knowledge retention or coordinated multi-agent behavior.
- **What evidence would resolve it:** Successful application of LDSC in cooperative multi-agent benchmarks or sequential learning scenarios without catastrophic forgetting.

### Open Question 3
- **Question:** How can semantic reasoning mechanisms be refined to better handle LLM inaccuracies?
- **Basis in paper:** The paper acknowledges that LLM suggestions "may not always be accurate" and explicitly calls for "further refining semantic reasoning mechanisms."
- **Why unresolved:** While the Subgoal Relation Tree explores alternatives, the system relies heavily on the initial reasoning quality of the LLM (ChatGPT-o1).
- **What evidence would resolve it:** Improved success rates in environments where the LLM provides initially flawed subgoal sequences, or robust performance with smaller, less capable language models.

## Limitations
- LLM integration reliability: Performance depends heavily on LLM reasoning quality, which may vary across domains and task complexities
- Environment specificity: All experiments use controlled Mujoco maze environments; semantic decomposition may not generalize to ambiguous environments
- Option transferability quantification: Limited evidence for cross-task transfer performance across substantially different task families

## Confidence
- **High confidence**: Three-level hierarchical architecture and SMDP Q-learning formulation are well-established techniques with sound experimental methodology
- **Medium confidence**: LLM-guided subgoal generation shows promise but depends on natural language task description quality and LLM reasoning consistency
- **Low confidence**: Goal-oriented options enabling significant cross-task transfer is supported by limited evidence without systematic transfer performance quantification

## Next Checks
1. **LLM robustness testing**: Systematically vary LLM reasoning quality by using different LLM models (GPT-3.5, Claude, open-source alternatives) and analyze impact on LDSC performance across all four environments
2. **Transfer learning quantification**: Design experiments that explicitly measure zero-shot transfer performance of learned options across tasks with different goal positions but identical subgoal structures, and compare against options learned from scratch
3. **Real-world applicability assessment**: Test LDSC on a more complex, less structured robotic task (e.g., household manipulation with multiple objects) to evaluate whether semantic decomposition remains effective outside controlled maze environments