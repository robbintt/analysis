---
ver: rpa2
title: 'SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional
  Machine Translation System'
arxiv_id: '2508.02268'
source_url: https://arxiv.org/abs/2508.02268
tags:
- arabic
- translation
- syrian
- dialect
- dialectal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SHAMI-MT, a bidirectional machine translation\
  \ system for Modern Standard Arabic (MSA) and Syrian dialect, addressing the linguistic\
  \ gap between formal and colloquial Arabic. The system uses AraT5v2, a state-of-the-art\
  \ Arabic transformer model, fine-tuned on the N\xE2bra dataset\u2014a rich, real-world\
  \ corpus of Syrian Arabic from diverse sources."
---

# SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System

## Quick Facts
- arXiv ID: 2508.02268
- Source URL: https://arxiv.org/abs/2508.02268
- Reference count: 14
- Primary result: Achieved 4.01/5.0 average quality score on MADAR test set using GPT-4.1 automated judging

## Executive Summary
This paper introduces SHAMI-MT, a bidirectional machine translation system for Modern Standard Arabic (MSA) and Syrian dialect. The system uses AraT5v2, a state-of-the-art Arabic transformer model, fine-tuned on the Nâbra dataset—a rich, real-world corpus of Syrian Arabic from diverse sources. The models were evaluated on the MADAR corpus using GPT-4.1 as an automated judge, achieving an average quality score of 4.01 out of 5.0, demonstrating strong semantic accuracy, dialectal authenticity, and fluency. Qualitative analysis revealed high performance in idiomatic and contextual translation, with occasional challenges in ultra-short, highly informal expressions. SHAMI-MT fills a critical gap in Arabic NLP, enabling accurate and natural dialectal translation for applications like content localization, cultural preservation, and intercultural communication. The work emphasizes the value of specialized models over general-purpose multilingual systems for dialectal tasks and sets a new benchmark for Arabic dialect translation.

## Method Summary
SHAMI-MT builds on pre-trained AraT5v2-base-1024, fine-tuning it on the Nâbra dataset for bidirectional translation between MSA and Syrian Arabic. The training used 22 epochs with batch size 256, cosine learning rate schedule starting at 5e-5, and produced two separate models (MSA→Shami and Shami→MSA). Evaluation was conducted on the MADAR corpus using GPT-4.1 as an automated judge, scoring translations on semantic accuracy, dialectal authenticity, and fluency. The system is released on Hugging Face under `Omartificial-Intelligence-Space/Shami-MT` (MSA→Shami) and `SHAMI-MT-2MSA` (Shami→MSA).

## Key Results
- Achieved 4.01/5.0 average quality score on MADAR test set using GPT-4.1 automated judging
- Demonstrated strong performance in idiomatic and contextual translation while maintaining dialectal authenticity
- Revealed occasional challenges with ultra-short, highly informal expressions and flattened formality in some outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language-specific pre-training provides deeper morphological and syntactic representations than multilingual alternatives for dialectal Arabic translation.
- **Mechanism:** AraT5v2's exclusive pre-training on Arabic text concentrates representational capacity on Arabic's complex morphology (root-pattern system, templatic morphology, agglutination). This dense exposure builds representations of shared morphological roots between MSA and dialects that differ in surface realization (e.g., MSA /kataba/ → dialectal /katab/), enabling the model to recognize correspondences that multilingual models—with capacity diluted across hundreds of languages—may miss.
- **Core assumption:** Morphological patterns learned from MSA-heavy pre-training transfer to dialectal Arabic despite surface differences. Assumes underlying derivational morphology remains consistent across varieties.
- **Evidence anchors:**
  - [abstract] "both built upon the state-of-the-art AraT5v2-base-1024 architecture"
  - [section 3] "AraT5v2 is pre-trained exclusively on a large and diverse corpus of high-quality Arabic text, granting it a deep understanding of the language's rich morphology, syntax, and semantics"
  - [corpus] Related paper "Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation" (arxiv:2507.20301) supports specialized approaches; corpus shows active research in dialectal MT but limited prior work on Syrian specifically
- **Break condition:** When dialectal morphology deviates fundamentally from MSA patterns (e.g., heavy loanword integration in Maghrebi), or when pre-training corpus lacks morphological diversity for rare constructions.

### Mechanism 2
- **Claim:** Fine-tuning on diverse, authentic, multi-register dialectal data produces translations with higher dialectal authenticity than synthetic or single-register datasets.
- **Mechanism:** Multi-register exposure (social media, TV/scripts, lyrics, proverbs) during fine-tuning builds a representation of the dialect as a variable system rather than a fixed code. Social media captures contemporary slang; scripts capture naturalistic dialogue; proverbs capture fossilized idioms. This heterogeneity teaches the model that dialectal correctness is context-dependent—Damascus and Aleppo speakers may use different vocabulary for the same concept, both valid—enabling production that satisfies multiple authenticity criteria simultaneously.
- **Core assumption:** Variation in training data is representative of the dialect broadly, and the test distribution falls within the convex hull of training variation.
- **Evidence anchors:**
  - [abstract] "fine-tuned on the comprehensive Nabra dataset"
  - [section 3] "Nâbra dataset... is not a sterile, academic dataset; rather, it is compiled from a wide array of real-world sources, including social media posts, film and television scripts, song lyrics, and traditional proverbs" covering "Damascus, Aleppo, Homs, Latakia"
  - [corpus] "Alexandria" dataset paper (arxiv:2601.13099) confirms multi-domain dialectal data improves MT; Nabra is Syrian-specific but corpus shows this pattern generalizes
- **Break condition:** When test inputs require vocabulary/idioms absent from all training registers; when demographic/ regional bias in training creates blind spots (e.g., urban youth data won't generalize to rural elderly speech).

### Mechanism 3
- **Claim:** Extended context window (1024 tokens) in encoder-decoder architecture preserves discourse coherence across long, syntactically complex Arabic sentences.
- **Mechanism:** Arabic's pro-drop nature, flexible word order (VSO/SVO), and extensive conjunction chains create dependencies spanning many tokens. The 1024-token window allows the encoder to maintain representations of discourse referents established earlier, enabling the decoder to resolve pronouns, maintain topic continuity, and produce grammatically coherent outputs. This is particularly valuable for dialectal translation where discourse strategies differ from MSA (different pronominal clitics, discourse markers).
- **Core assumption:** The model effectively uses information from early tokens when generating late tokens; long context window translates to long-dependency-learning. Assumption: translation quality degrades gracefully within window.
- **Evidence anchors:**
  - [section 3] "support for extended sequence lengths, up to 1024 tokens, is particularly valuable for machine translation, as it enables the model to maintain coherence and context across long, syntactically complex sentences"
  - [section 3] "encoder-decoder architecture is purpose-built for text generation, making it especially well-suited for translation tasks"
  - [corpus] Corpus evidence for context-window effects on Arabic MT is weak; no direct neighbor papers test this mechanism
- **Break condition:** When documents exceed the context window requiring cross-sentence reference; when discourse dependencies span multiple dialogue turns (not addressed by sentence-level architecture).

## Foundational Learning

- **Concept: Diglossia in Arabic NLP**
  - **Why needed here:** The entire paper is motivated by the gap between MSA (formal, written) and regional dialects (spoken, informal). Understanding this linguistic reality explains why MSA-trained models fail on dialectal input and why specialized systems are necessary.
  - **Quick check question:** Can you explain why a model trained only on Arabic Wikipedia would struggle to translate a casual conversation between two people in Damascus?

- **Concept: Fine-tuning vs. Pre-training**
  - **Why needed here:** SHAMI-MT builds on pre-trained AraT5v2 and specializes it via fine-tuning on Nabra. Distinguishing these stages is essential for understanding what knowledge comes from where—and where failures might originate.
  - **Quick check question:** If the model produces incorrect dialectal vocabulary but correct MSA grammar, would you investigate the pre-training corpus or the fine-tuning data first?

- **Concept: LLM-as-judge evaluation**
  - **Why needed here:** The paper uses GPT-4.1 as an automated evaluator rather than traditional metrics like BLEU. Understanding the tradeoffs (semantic/dialectal nuance capture vs. potential judge bias) is critical for interpreting the 4.01/5.0 score.
  - **Quick check question:** What are two potential failure modes when using an LLM to evaluate dialectal authenticity?

## Architecture Onboarding

- **Component map:**
  - AraT5v2-base-1024 (encoder-decoder transformer): Pre-trained exclusively on Arabic; 1024-token context window
  - Nabra dataset: Fine-tuning corpus with diverse registers (social media, scripts, lyrics, proverbs) from Syrian sub-dialects
  - MADAR corpus: Blind test set (1,500 Damascus sentence pairs) for evaluation
  - GPT-4.1 evaluator: Automated judge scoring on semantic accuracy, dialectal authenticity, fluency (0-5 scale)
  - Two separate models: One for MSA→Shami, one for Shami→MSA (not a single bidirectional model)

- **Critical path:**
  1. Load pre-trained AraT5v2-base-1024 from HuggingFace (`UBC-NLP/AraT5v2-base-1024`)
  2. Prepare Nabra dataset as parallel sentence pairs (MSA↔Shami); tokenize with AraT5 tokenizer
  3. Fine-tune with cosine LR schedule (initial 5e-5), batch size 256, 22 epochs (~10K steps)
  4. Evaluate on held-out MADAR Damascus subset using GPT-4.1 scoring prompt
  5. Export to HuggingFace Hub

- **Design tradeoffs:**
  - **Separate models vs. single bidirectional model:** Separate models allow each direction to specialize (reported eval focuses on MSA→Shami at 4.01/5; Shami→MSA performance less emphasized), but doubles inference and maintenance cost
  - **AraT5v2 vs. multilingual alternatives (mT5, mBERT):** Monolingual focus yields deeper Arabic understanding but no zero-shot cross-lingual transfer from other languages
  - **GPT-4.1 judge vs. BLEU/metric-based eval:** Captures dialectal nuance but introduces evaluator subjectivity and cost; results not directly comparable to BLEU-reported systems

- **Failure signatures:**
  - **Overly literal translations:** Model produces faithful but non-idiomatic output (Table 2 examples)—suggests insufficient idiomatic training coverage or exposure to colloquial compression strategies
  - **Temporal/spatial semantic shifts:** Model preserves source aspect while reference shifts focus (Table 2, score 2)—reflects ambiguity in short, context-poor inputs
  - **Flattened formality:** Output neither fully MSA nor authentically dialectal—may indicate training data mixing issues or model uncertainty at dialect boundaries
  - **Short-phrase failures:** Ultra-short, highly informal inputs score poorly (Table 2, score 1)—model lacks sufficient context for disambiguation

- **First 3 experiments:**
  1. **Baseline replication:** Load released models from HuggingFace (`Omartificial-Intelligence-Space/Shami-MT` and `SHAMI-MT-2MSA`), run inference on 50 MADAR samples, manually inspect quality against reported 4.01 average. Document any systematic error patterns.
  2. **Directional asymmetry test:** Compare MSA→Shami vs. Shami→MSA performance on the same sentence pairs (bidirectional test). Quantify if one direction is significantly stronger and hypothesize why (training data composition, dialectal variation coverage).
  3. **Data ablation:** Fine-tune AraT5v2 on subsets of Nabra (e.g., social-media-only vs. scripts-only) and evaluate on MADAR. Determine which register contributes most to dialectal authenticity scores; identify coverage gaps for future data collection.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can the MSA-Syrian translation capabilities be transferred to other Levantine dialects via cross-dialectal transfer learning?
- **Basis in paper:** [explicit] The authors state, "The immediate next step is to expand the system's capabilities to other major Levantine dialects... to explore the potential for cross-dialectal transfer learning."
- **Why unresolved:** The current study is restricted to the Syrian dialect, and the architectural adaptability to related but distinct dialects (e.g., Lebanese, Jordanian) has not been tested.
- **What evidence would resolve it:** Performance benchmarks of a single adapted model across multiple Levantine dialect test sets compared to specialized single-dialect models.

### Open Question 2
- **Question:** How closely does the GPT-4.1 automated evaluation protocol align with human expert judgments regarding semantic accuracy and dialectal authenticity?
- **Basis in paper:** [inferred] The paper relies exclusively on GPT-4.1 for quantitative scoring (e.g., 4.01/5.0) to capture nuance, without providing correlation data against human annotators.
- **Why unresolved:** LLM-based evaluators can exhibit biases or "flattened" perceptions of dialectal nuance, making the relationship between the reported scores and actual human perception uncertain.
- **What evidence would resolve it:** A correlation study comparing GPT-4.1 scores with ratings from native Syrian Arabic speakers on a shared test set.

### Open Question 3
- **Question:** Can the computational cost of dialectal adaptation be significantly reduced using parameter-efficient techniques like LoRA without compromising translation fidelity?
- **Basis in paper:** [explicit] The authors list as a future direction the intent to "investigate more efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), to reduce the computational cost."
- **Why unresolved:** The current implementation relies on full fine-tuning of the AraT5v2 model, and it is unknown if low-rank adaptations suffice for complex morphological and dialectal transformations.
- **What evidence would resolve it:** A comparison of quality scores (GPT-4.1 or human) and training resource consumption between the current fully fine-tuned model and a LoRA-adapted variant.

## Limitations

- The Nâbra dataset is not publicly available in the form used for training, preventing direct replication of results.
- The exact GPT-4.1 evaluation prompt and scoring rubric are not fully specified, introducing uncertainty about whether the reported 4.01/5.0 score can be exactly reproduced.
- The paper focuses primarily on MSA→Shami performance (4.01/5.0), with Shami→MSA results mentioned but not emphasized, creating an asymmetry in reported evaluation.

## Confidence

- **High Confidence:** The mechanism that specialized Arabic pre-training (AraT5v2) provides deeper morphological understanding than multilingual models for dialectal tasks. This is supported by the model architecture choice and the specific focus on Arabic-only pre-training.
- **Medium Confidence:** The claim that multi-register, authentic dialectal data (Nâbra) produces more authentic translations than synthetic or single-register datasets. While the dataset description is thorough, the evaluation relies on automated judging rather than human assessment.
- **Low Confidence:** The assertion that the 1024-token context window significantly improves discourse coherence for Arabic dialect translation. This mechanism lacks direct empirical support in the paper or related literature.

## Next Checks

1. **Data Access Verification:** Obtain the Nâbra dataset and verify its composition matches the paper's description. Test whether the reported training statistics (10,384 steps, loss curves) align with the actual data size and distribution.
2. **GPT-4.1 Prompt Replication:** Reconstruct the exact evaluation prompt used for GPT-4.1 judging. Run the prompt on 100 MADAR samples and compare scores to the reported 4.01 average to assess consistency.
3. **Bidirectional Performance Comparison:** Evaluate both MSA→Shami and Shami→MSA models on the same MADAR sentence pairs. Quantify performance differences and analyze whether the directional asymmetry reflects training data composition or inherent task difficulty.