---
ver: rpa2
title: Evaluating LLMs on Chinese Idiom Translation
arxiv_id: '2508.10421'
source_url: https://arxiv.org/abs/2508.10421
tags:
- translation
- idiom
- chinese
- translations
- idioms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IDIOM EVAL, a new evaluation framework for
  Chinese idiom translation, including a taxonomy of 7 error types (e.g., Mistranslation,
  Literal Translation, Partial Translation). The authors collected 900 annotated translations
  from 9 modern systems (including GPT-4o, Google Translate) across four domains.
---

# Evaluating LLMs on Chinese Idiom Translation

## Quick Facts
- arXiv ID: 2508.10421
- Source URL: https://arxiv.org/abs/2508.10421
- Reference count: 40
- 900 annotated translations from 9 systems show even GPT-4 makes idiom errors in 28% of cases

## Executive Summary
This paper introduces IDIOM EVAL, a comprehensive framework for evaluating Chinese idiom translation quality. The authors collected 900 annotated translations from 9 modern systems across four domains (News, Web, Wikipedia, Social Media) and developed a taxonomy of 7 error types. Their analysis reveals that even state-of-the-art systems like GPT-4 make idiom translation errors in 28% of cases, and existing automatic metrics correlate poorly with human judgments. To address this gap, they fine-tuned Qwen2.5 models for error detection, achieving F1 scores of 0.68, significantly outperforming existing metrics and prompting approaches.

## Method Summary
The study evaluates Chinese idiom translation quality across 9 systems (GPT-4, GPT-4o, Qwen variants, Alma, mT0, Google Translate) using a new IDIOM EVAL taxonomy with 7 error categories. The authors collected 900 annotated translations from a post-2023 corpus, sampling 25 sentences per domain (5 per frequency range) from an idiom vocabulary of 30,999 items. They fine-tuned Qwen2.5 Base models using LoRA with 4 output strategies, finding that short thought process plus final answer format achieved the highest Macro F1 of 0.68 for detecting idiom translation errors.

## Key Results
- GPT-4 makes idiom translation errors in 28% of cases despite being the best-performing system
- Existing automatic metrics (BLEU, BERTScore, COMET, MetricX) correlate poorly with human judgments (Pearson correlation < 0.48)
- Qwen2.5-32B fine-tuned with short thought + answer format achieves Macro F1 of 0.68 for error detection
- Performance varies significantly across domains, with Web and Social Media contexts showing higher error rates than News and Wikipedia

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilingual pre-training data improves idiom translation quality compared to English-centric training.
- Mechanism: Models exposed to both Chinese and English during pre-training (e.g., Qwen) develop better cross-lingual representations of culturally-embedded idioms than models primarily trained on English (e.g., Alma/LLaMA-2, mT0), as bilingual exposure captures semantic and pragmatic aspects specific to Chinese idioms.
- Core assumption: Idiom understanding requires exposure to both source-language cultural context and target-language expression patterns.
- Evidence anchors:
  - [abstract]: "best-performing system (GPT-4) still making errors in 28% of cases" while Alma and mT0 show <25% good translations
  - [section]: "Alma's backbone, LLaMA-2, is primarily pre-trained in English, limiting its performance. In contrast, Qwen, which is pre-trained on both Chinese and English bilingual data, achieves better translation quality."
  - [corpus]: Related work (Chengyu-Bench, arXiv:2506.18105) confirms LLMs exhibit "fundamental misunderstanding of idiom meanings," suggesting training data composition is critical but not sufficient alone.

### Mechanism 2
- Claim: Domain-specific idiom usage patterns cause performance variance across contexts.
- Mechanism: Idioms in News/Wikipedia follow more standardized, historical usage patterns likely present in training corpora, while Web/Social Media feature "newly emerging contexts" where idioms appear in novel compositional arrangements, degrading model performance.
- Core assumption: Models rely partially on memorized idiom-context co-occurrences rather than compositional understanding.
- Evidence anchors:
  - [abstract]: Systems evaluated across four domains show variable performance
  - [section]: "For GPT-4 and GPT-4o, more than 80% of its translations are rated as good on News, but this drops to 50% on Web and 60% on Social Media."
  - [corpus]: Tug-of-war paper (arXiv:2506.01723) finds LLMs struggle with figurative vs. literal disambiguation, suggesting context-dependency is not fully learned.

### Mechanism 3
- Claim: Instruction-tuning with short reasoning traces improves idiom error detection over zero-shot prompting.
- Mechanism: Fine-tuning on structured output formats (short thought + answer) teaches models to explicitly map idiom meaning to translation quality, whereas zero-shot prompting lacks the supervised signal for this specific judgment task. Smaller models (1.5B-32B) benefit from reasoning scaffolding, while 72B exhibits format collapse.
- Core assumption: Explicit reasoning training creates specialized circuitry for idiom evaluation that generalizes to unseen idioms.
- Evidence anchors:
  - [abstract]: "fine-tuned Qwen2.5 models, achieving a Macro F1 score of 0.68 for detecting idiom translation errors, outperforming existing metrics and prompting-based approaches"
  - [section]: "Fine-tuning Qwen2.5-32B with a short thought process and final answer achieves the highest Macro F1 of 0.68" vs "GPT-4o reaches a macro F1 of 0.63"

## Foundational Learning

- **Chinese Idiom (Chengyu) Properties**
  - Why needed: Idioms are non-compositional, often contain historical references, and differ from English idioms in their semantic/pragmatic emphasis. Without this, you'll misinterpret why literal translations are classified as errors.
  - Quick check: Can you explain why "身无长处" translated as "I have no merits in this life" is a Literal Translation error, not a correct translation?

- **Translation Evaluation Metrics**
  - Why needed: The paper shows BLEU, BERTScore, COMET, and MetricX all correlate poorly with human judgment (Pearson < 0.48). Understanding what each metric captures helps diagnose why they fail on idioms.
  - Quick check: Why would a reference-based metric like BLEU give a high score to a literal idiom translation that humans rate as incorrect?

- **Instruction Tuning vs. Prompting**
  - Why needed: The paper's main contribution uses instruction-tuning to improve error detection. Understanding the difference between training-time supervision and inference-time prompting is critical for reproducing results.
  - Quick check: Why would chain-of-thought prompting lower performance (as observed with GPT-4o and Qwen2.5-72B) while short reasoning traces during fine-tuning improve it?

## Architecture Onboarding

- **Component map:**
  Data Collection Pipeline -> IDIOM EVAL Taxonomy -> Annotation Interface (Thresh) -> Translation System Layer -> Metric Evaluation Layer -> Fine-tuning Module

- **Critical path:**
  1. Collect new corpus (post-2023) -> 2. Extract idiom-containing sentences -> 3. Sample 25 per domain (5 per frequency range) -> 4. Generate translations from 9 systems -> 5. Annotate with IDIOM EVAL -> 6. Train/test split by model (older models in train, newer in test) -> 7. Fine-tune Qwen2.5-32B with short thought + answer format

- **Design tradeoffs:**
  - **Temporal split vs. random split**: Chose model-based split (older models in train, newer in test) to simulate real-world deployment where metrics evaluate new systems. Tradeoff: reduced training data diversity.
  - **Short thought vs. long thought output**: Short thought + answer works best for 1.5B-32B models; long thought provides no additional benefit. 72B exhibits format collapse.
  - **Include vs. omit idiom in input**: For MetricX fine-tuning, omitting the idiom (using original format) works better at smaller scales, suggesting format consistency matters with limited data.

- **Failure signatures:**
  - **72B training instability**: Loss plateaus at 0.2-0.3 (vs. 0.02 for smaller models) when trained on thought-based outputs; model ignores training format and always outputs short answers
  - **CoT degradation**: Chain-of-thought prompting reduces accuracy (models become biased toward "correct" judgments)
  - **Reference-free metric failure**: COMETKIWI and MetricX-QE show negative correlations with human ratings on idiom-level evaluation

- **First 3 experiments:**
  1. **Baseline reproduction**: Run all 9 translation systems on the 100 test sentences (25 per domain), collect outputs, and verify your annotation pipeline achieves the reported 0.73 Cohen's κ on category agreement.
  2. **Metric correlation check**: Compute Pearson correlation between your human annotations and all 8 metrics on the idiom span only; confirm correlations are < 0.48 before proceeding to fine-tuning.
  3. **Fine-tuning ablation**: Train Qwen2.5-7B with all 4 output strategies (short answer, long answer, short thought + answer, long thought + answer) and verify short thought + answer achieves highest Macro F1 before scaling to 32B.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation constrained to Chinese-to-English translation with 9 specific systems, limiting generalizability to other language pairs or model families
- Manual annotation process remains subject to individual annotator interpretation despite 0.73 Cohen's κ agreement
- Idiom vocabulary of 30,999 items may not capture all contemporary usage patterns, especially for rapidly evolving web and social media contexts

## Confidence
- **High Confidence**: Poor correlation between existing metrics and human judgments (Pearson < 0.48) is well-supported by systematic evaluation across domains and systems
- **Medium Confidence**: Superiority of Qwen2.5 models over English-centric models is supported experimentally but mechanism remains correlational
- **Low Confidence**: Effectiveness of instruction-tuning with reasoning traces for error detection is demonstrated for Qwen2.5 but may not generalize to other architectures

## Next Checks
1. **Cross-linguistic validation**: Evaluate IDIOM EVAL framework and error detection models on English-to-Chinese idiom translation to test generalizability across translation directions
2. **Temporal robustness test**: Re-evaluate model performance on idioms that emerged after the training corpus cutoff date to verify domain-specific performance claims for Web and Social Media contexts
3. **Metric ablation study**: Conduct systematic ablation of metric components (reference-based vs. reference-free, semantic vs. syntactic features) to identify why specific aspects fail for idiom translation