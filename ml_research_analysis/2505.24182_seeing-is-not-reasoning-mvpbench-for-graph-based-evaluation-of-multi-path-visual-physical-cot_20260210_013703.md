---
ver: rpa2
title: 'Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path
  Visual Physical CoT'
arxiv_id: '2505.24182'
source_url: https://arxiv.org/abs/2505.24182
tags:
- reasoning
- step
- object
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVPBench, a benchmark designed to evaluate
  visual physical reasoning in multimodal large language models (MLLMs). It addresses
  the gap in current benchmarks by focusing on real-world visual physics problems
  that require grounded, multi-step reasoning over visual evidence, going beyond surface-level
  image description.
---

# Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT

## Quick Facts
- arXiv ID: 2505.24182
- Source URL: https://arxiv.org/abs/2505.24182
- Reference count: 40
- Key outcome: Introduces MVPBench, a benchmark showing current MLLMs struggle with visual physical reasoning despite multi-image inputs and CoT prompting

## Executive Summary
This paper introduces MVPBench, a benchmark designed to evaluate visual physical reasoning in multimodal large language models (MLLMs). It addresses the gap in current benchmarks by focusing on real-world visual physics problems that require grounded, multi-step reasoning over visual evidence, going beyond surface-level image description. MVPBench includes 1,211 carefully curated examples across four domains: physics experiments, exam-style physics problems, spatial relations, and dynamic prediction. Each example features interleaved multi-image inputs and demands a coherent, step-by-step reasoning path grounded in evolving visual cues.

The paper proposes a graph-based CoT consistency metric suite to evaluate reasoning fidelity, visual grounding, and path diversity. This metric represents each reasoning chain as a directed acyclic graph of atomic facts and assesses step-wise fidelity through exact or fuzzy graph matching. Experimental results reveal that even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. Surprisingly, reinforcement learning-based post-training alignment, commonly believed to improve visual reasoning performance, often harms spatial reasoning, suggesting a need to rethink current fine-tuning practices. Providing models with the full image sequence boosts performance by up to 21% points, indicating that temporal context matters.

## Method Summary
MVPBench evaluates MLLMs on visual physical reasoning through a three-dimensional framework: Quality (Step Accuracy Score, Key Step Coverage, CoT Reasoning Score), Diversity (Path Validity Rate, Path Coverage Score, CoT Match Score), and Efficiency (Step Relevance Score, Reflection Validity Rate). The benchmark contains 1,211 examples across four categories with multi-image inputs and multi-path CoT annotations. Evaluation uses zero-shot CoT prompting, with GPT-4o decomposing model outputs into structured steps and judging reasoning quality. Graph-based matching converts annotated reasoning chains into directed acyclic graphs to assess validity and coverage of model-generated paths.

## Key Results
- Current MLLMs show poor visual reasoning accuracy and weak image-text alignment in physical domains
- Providing multi-image inputs boosts performance by up to 21% points, highlighting the importance of temporal context
- RL-based post-training alignment often harms spatial reasoning performance, contrary to common belief
- Spatial-Relation tasks represent a major source of perception failures (~33% of errors)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based CoT consistency metrics can evaluate whether reasoning paths adhere to valid physical logic.
- Mechanism: Convert annotated reasoning chains into directed acyclic graphs where key steps become nodes and logical flows become edges. Model-generated reasoning is mapped onto this reference graph, then matched via Path Validity Rate (PVR) and Path Coverage Score (PCS) to quantify validity and coverage.
- Core assumption: Valid physical reasoning can be decomposed into discrete, ordered atomic facts that admit multiple valid orderings.
- Evidence anchors: [abstract] "graph-based CoT consistency metric that verifies whether the reasoning path of model adheres to valid physical logic"; [section 4.2] "Each annotated instance is converted into directed graphs, with key steps as nodes and logical flows as edges"; [corpus] Limited direct corpus support; related work InPhyRe notes LMMs encode physical laws as parametric knowledge but may not apply them correctly.
- Break condition: If reasoning steps cannot be cleanly decomposed or ordered (e.g., truly parallel inferences), graph-based matching may penalize valid paths.

### Mechanism 2
- Claim: Multi-image inputs significantly improve visual physical reasoning compared to single-image inputs.
- Mechanism: Providing temporal context via multiple key frames captures evolving physical states (initial setup, intermediate steps, final results), enabling models to reason over causally-linked visual evidence rather than inferring dynamics from a single snapshot.
- Core assumption: Models can integrate information across multiple images and use temporal cues for inference.
- Evidence anchors: [abstract] "Each example features interleaved multi-image inputs"; [section 6] "Providing models with the full image sequence boosts performance by up to 21% points-evidence that temporal context matters"; [corpus] DeepPHY (arXiv:2508.05405) similarly notes VLMs struggle with dynamic environments requiring spatial reasoning.
- Break condition: Models without effective multi-image integration (e.g., QVQ-72B showed performance drops) may not benefit or could degrade.

### Mechanism 3
- Claim: RL-based post-training alignment commonly harms spatial and visual-centric reasoning.
- Mechanism: RL alignment optimizes for conversational fluency and preference alignment, which may reward plausible-sounding explanations over grounded visual reasoning, thereby introducing distributional biases that degrade perceptual grounding.
- Core assumption: Reward signals in current RL post-training do not adequately incentivize visual grounding.
- Evidence anchors: [abstract] "RL-based post-training alignment—commonly believed to improve visual reasoning performance—often harms spatial reasoning"; [section 5] "InternVL2.5-78B-MPO underperforms its base counterpart... MPO may introduce distributional biases"; [corpus] No direct corpus evidence; this appears to be a novel finding in this paper.
- Break condition: If reward designs are modified to explicitly incorporate visual grounding metrics, this degradation may be avoided.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: MVPBench evaluates multi-step reasoning chains; understanding CoT decomposition is essential to interpret SAS, KSC, and CRS metrics.
  - Quick check question: Given a physics problem, can you decompose the solution into sequential atomic inference steps?

- Concept: **Directed Acyclic Graphs (DAGs) for Reasoning**
  - Why needed here: The diversity evaluation converts reasoning paths into DAGs;