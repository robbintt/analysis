---
ver: rpa2
title: 'ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback'
arxiv_id: '2509.05091'
source_url: https://arxiv.org/abs/2509.05091
tags:
- feedback
- agents
- agent
- protom
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProToM, a Theory of Mind-informed facilitator
  that promotes prosocial behaviour in multi-agent systems by providing targeted,
  context-sensitive feedback to individual agents. ProToM infers agents' goals using
  Bayesian inverse planning and selects feedback by maximizing expected utility conditioned
  on the inferred goal distribution.
---

# ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback

## Quick Facts
- arXiv ID: 2509.05091
- Source URL: https://arxiv.org/abs/2509.05091
- Reference count: 17
- One-line primary result: ProToM achieved perfect success rates and significantly higher task speedups compared to baselines while requiring minimal communication.

## Executive Summary
This paper introduces ProToM, a Theory of Mind-informed facilitator that promotes prosocial behavior in multi-agent systems by providing targeted, context-sensitive feedback to individual agents. ProToM infers agents' goals using Bayesian inverse planning and selects feedback by maximizing expected utility conditioned on the inferred goal distribution. Evaluated in two multi-agent environments (Doors, Keys, and Gems, and Overcooked), ProToM achieved perfect success rates and significantly higher task speedups compared to baselines. It required minimal communication, averaging 0.70-0.81 messages per episode versus 1.00-1.50 for baseline models. Human participants consistently preferred ProToM's feedback as more helpful, appropriate, and better explained.

## Method Summary
ProToM is a facilitator that promotes prosocial behavior in multi-agent systems through Theory of Mind reasoning. It operates as a two-level POMDP, maintaining beliefs about each agent's internal state and goals. The method uses particle filtering to maintain belief distributions, Bayesian inverse planning to infer goals from observed actions, and expected utility maximization to select prosocial feedback. Communication is gated by both utility thresholds and divergence metrics to ensure messages are only sent when they will change agent behavior. The system generates natural language explanations using template-based methods grounded in inferred goals.

## Key Results
- Achieved perfect success rates in both mDKG and Overcooked environments
- Required significantly fewer feedback messages (0.70-0.81 per episode) compared to baselines (1.00-1.50)
- Human participants rated ProToM's feedback as more helpful (4.75/5 vs 4.00/5), more appropriate (4.64/5 vs 3.79/5), and better explained (4.64/5 vs 3.79/5) than VLMs/RMs

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Goal Inference via Particle Filtering
- Claim: ProToM infers agents' goals from observed actions under partial observability, enabling context-sensitive feedback selection.
- Mechanism: Particle filter maintains N belief particles per agent, each containing a sampled belief state b^S,k and goal distribution b^G,k. Goal distributions are updated via Bayesian inverse planning: b^G,k(g_i) ∝ b^G,k(g_i) · P(a^t_i | g_i, b^S,k). The aggregated goal estimate b̂^G_i averages across particles.
- Core assumption: Agents act approximately rationally given their beliefs and goals; action likelihood can be computed under goal hypotheses.

### Mechanism 2: Expected Utility Maximization for Feedback Selection
- Claim: Selecting feedback that maximizes expected utility (reduction in task steps) promotes prosocial actions that improve global efficiency.
- Mechanism: For each candidate feedback f ∈ F_t, compute U(f) = E_g∼b̂_G[ΔC(f,g)] where ΔC(f,g) = C(∅,g) − C(f,g). C(f,g) measures expected steps to completion under feedback f.
- Core assumption: The utility function ℓ(·) (steps to completion) adequately captures prosocial value; agents will execute received feedback.

### Mechanism 3: Divergence-Gated Communication Timing
- Claim: Communicating only when feedback would change agent behavior reduces overhead and improves reception.
- Mechanism: Feedback is issued only if div(f) > ε, where div(f) measures divergence between agent's predicted behavior with vs. without feedback.
- Core assumption: The divergence metric accurately predicts behavioral change; agents respond to feedback only when it provides new actionable information.

## Foundational Learning

- **Concept**: Bayesian Inverse Planning
  - Why needed here: Core mechanism for inferring goals from observed actions; requires understanding of P(observation | goal, belief) and belief updates.
  - Quick check question: Given an agent moving toward door A instead of door B, how would you update P(goal=door_A) vs P(goal=door_B)?

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The problem is formulated as a two-level POMDP; facilitator must reason about agents' hidden internal states (beliefs, goals).
  - Quick check question: In Overcooked, if agent A can only see room 1 and agent B is in room 2, what beliefs might agent A hold about agent B's location?

- **Concept**: Theory of Mind (ToM) in Multi-Agent Systems
  - Why needed here: ProToM models others' mental states to predict behavior and select interventions; related work shows ToM enables coordination.
  - Quick check question: If agent A believes agent B knows where the red key is, but agent B actually doesn't, how might agent A's actions differ from optimal?

## Architecture Onboarding

- **Component map**: Belief Maintainer -> Feedback Constructor -> Utility Computer -> Communication Gate -> Explanation Generator
- **Critical path**:
  1. Observe (s_t, a_t) → Extract agent observations o^t_i
  2. For each particle: BeliefUpdate → GoalUpdate (Eq. 8)
  3. Aggregate goal distributions: b̂^G_i = (1/N) Σ_k b^G,k (Eq. 9)
  4. Construct F_t from feasible actions
  5. Compute U(f) via simulation (Algorithm 2)
  6. Filter by thresholds, select f̂ uniformly from valid candidates
  7. Generate explanation via template T(ĝ_{-i}, f̂, s_t)

- **Design tradeoffs**:
  - **Particle count N**: Higher N improves inference under partial observability but increases compute (paper uses N=1 for fully observable mDKG, N=5 for Overcooked)
  - **Thresholds (φ, ε)**: Control feedback frequency vs. coverage; determined via held-out search
  - **Template vs. learned explanations**: Templates are interpretable but less adaptive to novel contexts
  - **Centralized planner assumption**: div(f) computation assumes access to optimal policy π*

- **Failure signatures**:
  - **Goal misidentification**: If P(a^t_i | g_i, b^S,k) is misspecified, inferred goals diverge from truth → feedback becomes irrelevant
  - **Particle collapse**: Under narrow observations, particles may converge to incorrect beliefs
  - **Over-communication**: Low ε threshold → excessive messages (VLM baseline problem)
  - **Under-communication**: High φ or ε → missed intervention opportunities (ProToM vs. ProToM-Oracle gap in Overcooked)
  - **Spatial reasoning errors**: VLMs/RMs struggled with environment geometry (Figure 5)

- **First 3 experiments**:
  1. **Ablation on particle count**: Run ProToM with N ∈ {1, 3, 5, 10} on Overcooked scenarios; measure success rate, speedup, and inference time to validate N=5 tradeoff.
  2. **Threshold sensitivity analysis**: Systematic grid search on (φ, ε) pairs; plot Pareto frontier of communication frequency vs. task speedup to confirm robustness.
  3. **Goal inference validation**: Compare ProToM's inferred goals b̂^G against ground truth (ProToM-Oracle condition) across scenarios; quantify inference error vs. performance gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ProToM's feedback facilitation approach generalize to real-world settings with physical human agents and noisy observations?
- Basis in paper: [explicit] "We have not tested ProToM in real-world settings, which we aim to do in future work."
- Why unresolved: Current evaluation is limited to simulated grid-world environments (mDKG, Overcooked) and a controlled human study with 18 participants using simplified 2D interfaces.
- What evidence would resolve it: Demonstration of ProToM operating effectively in physical environments (e.g., robot-assisted collaboration, smart home settings) with real-time sensor input and human participants.

### Open Question 2
- Question: Can incorporating pragmatic communication, where the facilitator adapts feedback language based on inferred beliefs and goals, improve the effectiveness of prosocial facilitation?
- Basis in paper: [explicit] "Another direction worth exploring further is pragmatic communication, where the facilitator adapts its language based on agents' inferred beliefs and goals."
- Why unresolved: ProToM currently uses fixed explanation templates populated with inferred goals, rather than generating context-adapted natural language.
- What evidence would resolve it: Comparison experiments showing that pragmatic, belief-adapted explanations improve subjective ratings (helpfulness, appropriateness) and objective metrics (speedup, compliance rate) over template-based explanations.

### Open Question 3
- Question: How does ProToM's performance scale with the number of agents, and can recursive reasoning about agents modeling each other improve facilitation in complex multi-agent settings?
- Basis in paper: [explicit] "Finally, future work could consider more complex settings where the [facilitator] agents may recursively reason about each other."
- Why unresolved: All experiments used exactly two agents; the computational and modeling complexity of Bayesian inverse planning and expected utility computation for larger groups remains unexplored.
- What evidence would resolve it: Experiments in environments with 3+ agents showing maintained success rates and acceptable communication overhead, or improved performance from recursive social reasoning.

### Open Question 4
- Question: How robust is ProToM when agents do not comply with feedback, or when agents have conflicting or adversarial goals?
- Basis in paper: [inferred] ProToM assumes "agent i will follow the feedback upon receiving it" and focuses on cooperative prosocial scenarios, not adversarial ones.
- Why unresolved: Real-world deployment may involve agents who ignore, misinterpret, or actively resist facilitation; the utility-based selection mechanism has not been tested under non-compliance.
- What evidence would resolve it: Experiments with stochastic compliance rates or adversarial agents, measuring ProToM's ability to detect non-compliance and adapt feedback strategy accordingly.

## Limitations

- The exact implementations of Bayesian inverse planning likelihood functions and scenario configurations are not fully specified, making exact reproduction challenging.
- The performance gap between ProToM and ProToM-Oracle in Overcooked (18.7% speedup vs. 23.4%) suggests ProToM's goal inference is imperfect under partial observability.
- The human study, while indicating ProToM's feedback is more helpful and appropriate, involved a relatively small sample (18 participants in 9 pairs) and may not generalize to broader populations or more complex tasks.

## Confidence

- **High Confidence**: ProToM achieves perfect success rates in both environments and significantly reduces communication overhead compared to baselines.
- **Medium Confidence**: The mechanism of Bayesian goal inference enabling context-sensitive feedback is well-supported by the results, though the exact quality of inference is not fully quantified.
- **Medium Confidence**: Human participants' preference for ProToM's feedback as more helpful, appropriate, and better explained is supported, but sample size and task complexity limit generalizability.
- **Low Confidence**: The claim that ProToM "outperforms current large language and reasoning models" is based on specific VLMs/RMs and may not extend to all LLMs without further testing.

## Next Checks

1. **Ablation on particle count**: Run ProToM with N ∈ {1, 3, 5, 10} on Overcooked scenarios; measure success rate, speedup, and inference time to validate N=5 tradeoff.
2. **Threshold sensitivity analysis**: Systematic grid search on (φ, ε) pairs; plot Pareto frontier of communication frequency vs. task speedup to confirm robustness.
3. **Goal inference validation**: Compare ProToM's inferred goals b̂^G against ground truth (ProToM-Oracle condition) across scenarios; quantify inference error vs. performance gap.