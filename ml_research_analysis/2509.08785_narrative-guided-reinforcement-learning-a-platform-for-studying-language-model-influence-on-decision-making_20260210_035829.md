---
ver: rpa2
title: 'Narrative-Guided Reinforcement Learning: A Platform for Studying Language
  Model Influence on Decision Making'
arxiv_id: '2509.08785'
source_url: https://arxiv.org/abs/2509.08785
tags:
- narrative
- learning
- language
- frameworks
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a preliminary experimental platform that combines
  reinforcement learning (RL) with language model reasoning to study how narrative
  frameworks might influence AI decision-making. The system uses a dual architecture
  where an RL policy suggests actions based on experience, and a language model processes
  these suggestions through different narrative contexts to guide decisions.
---

# Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making

## Quick Facts
- arXiv ID: 2509.08785
- Source URL: https://arxiv.org/abs/2509.08785
- Authors: Anup Tuladhar; Araz Minhas; Adam Kirton; Eli Kinney-Lang
- Reference count: 0
- One-line primary result: Narrative-guided agents achieved comparable performance to RL-only agents after 10 episodes versus 100 episodes of training

## Executive Summary
This paper introduces a novel experimental platform that combines reinforcement learning with language model reasoning to study how narrative frameworks influence AI decision-making. The system employs a dual architecture where an RL policy suggests actions based on learned experience, while a language model processes these suggestions through configurable narrative contexts to guide final decisions. Implemented in a gridworld environment with configurable sizes and obstacle densities, the platform enables controlled testing of narrative influence on decision-making while maintaining consistent environmental conditions. Initial experiments demonstrated that narrative-guided agents achieved performance comparable to RL-only agents after 100 episodes within just 10 episodes, despite significant computational overhead.

## Method Summary
The platform implements a dual-system architecture combining Q-learning reinforcement learning with language model processing in a configurable gridworld environment. The RL component uses tabular Q-learning with randomly initialized parameters, trained for short episodes (10) to maintain suboptimal policies that highlight LLM contributions. The language model (GPT-4o-mini) receives RL-suggested actions, Q-values, and local environmental observations, processing them through configurable narrative prompts (Theseus navigator, Sherlock Holmes detective, Westworld AI) to produce final action selections. The system logs success rates, step counts, RL Q-values, LLM reasoning traces, and policy adherence rates across grid sizes from 5x5 to 11x11 with 30-40% obstacle density.

## Key Results
- Narrative-guided agents achieved performance comparable to RL-only agents after 10 episodes versus 100 episodes of training
- Different narrative frameworks showed systematic differences in success rates and path efficiency, with Westworld framework showing most consistent performance
- Computational overhead was substantial: 5-30 minutes for 10 episodes (LLM+RL) versus <1 second for RL-only agents
- Narrative frameworks influenced decision-making patterns beyond raw performance metrics

## Why This Works (Mechanism)

### Mechanism 1: Pre-trained Knowledge Substitution for Exploration
- Claim: LLM processing may accelerate learning by providing structured reasoning that substitutes for experience-based exploration
- Mechanism: The LLM contributes planning heuristics through narrative reasoning, allowing the system to make informed decisions before the RL policy has converged—potentially compensating for sparse experience
- Core assumption: The LLM possesses transferable spatial or planning knowledge from pre-training that applies to gridworld navigation
- Evidence anchors: [abstract]: "narrative-guided agents achieved performance comparable to RL-only agents after 100 episodes within just 10 episodes"; [section]: Page 3 results state "LLM+RL agents achieved in 10 episodes performance comparable to what RL-only agents reached after 100 episodes of training"
- Break condition: If the target environment requires domain-specific knowledge absent from LLM pre-training data, this acceleration effect would likely diminish or disappear

### Mechanism 2: Narrative Context as Action Selection Bias
- Claim: Different narrative frameworks appear to systematically influence how the LLM weighs environmental observations against RL suggestions
- Mechanism: Persona-based prompts (e.g., detective vs. labyrinth navigator) may activate different reasoning patterns in the LLM, causing it to prioritize certain action features—such as caution, exploration, or efficiency—when overriding or reinforcing RL outputs
- Core assumption: LLMs respond differentially to character-driven framing in ways that affect discrete action choices
- Evidence anchors: [abstract]: "Different narrative frameworks...demonstrated systematic differences in success rates and path efficiency, with the Westworld framework showing the most consistent performance"; [section]: Page 3 notes that "narrative frameworks influenced not just performance metrics but also decision-making patterns" and "the Westworld AI framework demonstrated more consistent integration of RL suggestions with environmental observations"
- Break condition: If narrative prompts become misaligned with task structure—for example, a cautious persona in a time-critical environment—performance may degrade relative to baseline

### Mechanism 3: Dual-System Arbitration Through Contextual Override
- Claim: The architecture enables the LLM to productively override RL policy suggestions when narrative context and environmental observations suggest better alternatives
- Mechanism: RL provides experience-based priors (Q-values); the LLM evaluates these against current local observations and narrative goals, selecting either the suggested action or an alternative
- Core assumption: The LLM can meaningfully interpret RL confidence signals and environmental state descriptions to make informed arbitration decisions
- Evidence anchors: [abstract]: "language model processes these suggestions through different narrative contexts to guide decisions"; [section]: Page 2 describes "the language model processes this information through a configurable narrative framework, which can either reinforce or override the RL agent's suggestion"
- Break condition: If the LLM consistently overrides optimal RL suggestions due to narrative misalignment, cumulative performance would fall below RL-only baseline

## Foundational Learning

- Concept: Q-learning value functions
  - Why needed here: The RL component uses Q-learning with randomly initialized parameters to estimate action values; understanding Q-values is essential for interpreting what the policy suggests and why overrides might occur
  - Quick check question: Given a state with Q-values [up: 0.3, down: 0.1, left: 0.8, right: 0.2], which action would a greedy policy select and what would an epsilon-greedy policy with ε=0.2 potentially do instead?

- Concept: Language model prompt engineering for persona adoption
  - Why needed here: The system relies on narrative prompts to induce different reasoning styles; prompt design directly shapes action selection
  - Quick check question: How might framing the same navigation task as "Sherlock Holmes analyzing clues" versus "Theseus navigating a dangerous labyrinth" differently influence an LLM's attention to obstacles versus goal proximity?

- Concept: Dual-process theory foundations
  - Why needed here: The architecture explicitly references dual-system models (cited as [8,9] on page 2); understanding the complementary roles of fast/experiential versus deliberate/reasoning systems helps explain why RL and LLM might be combined
  - Quick check question: In human cognition, what types of decisions are typically associated with "System 1" versus "System 2," and how might this map onto RL policy lookup versus LLM deliberation?

## Architecture Onboarding

- Component map: Environment state -> RL Policy (computes Q-values, suggests best action) -> LLM (receives suggestion + local observations + narrative prompt -> deliberates -> outputs final action) -> Environment step -> Loop until goal or timeout
- Critical path: Environment state → RL Policy (computes Q-values, suggests best action) → LLM (receives suggestion + local observations + narrative prompt → deliberates → outputs final action) → Environment step → Loop until goal or timeout
- Design tradeoffs:
  - Computational cost: 5-30 minutes for 10 episodes (LLM+RL) versus <1 second (RL-only); consider batch processing or caching for scalability
  - Training episode count: Short RL training (10 episodes) intentionally limits policy convergence to highlight LLM contribution; increase for production use
  - Narrative complexity: Rich personas may improve reasoning but increase prompt length and token costs
  - Assumption: GPT-4o-mini was selected for cost-efficiency; larger models may yield different results
- Failure signatures:
  - Success rate below RL-only baseline: Check narrative prompt alignment with task goals
  - LLM ignoring RL suggestions entirely: Review prompt formatting for Q-value presentation clarity
  - Timeout on larger grids (>11x11): LLM inference latency scales with observation complexity
  - Inconsistent results across identical runs: LLM stochasticity requires setting temperature/seed controls for reproducibility
  - High override rate with poor outcomes: Narrative may be misinforming environmental interpretation
- First 3 experiments:
  1. Replicate baseline comparison: Run pure RL for 100 episodes versus LLM+RL for 10 episodes on identical 7x7 grids with 30% obstacles to confirm efficiency gains reported in the paper
  2. Narrative framework A/B test: Compare at least two frameworks (e.g., Westworld AI vs. direct instructions) while holding environment and RL policy fixed to isolate narrative influence on success rate and path efficiency
  3. Grid complexity scaling test: Run LLM+RL across 5x5, 7x7, 9x9, and 11x11 grids to characterize computational overhead growth and identify where narrative benefits diminish

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does language model scale affect the performance and reliability of narrative-guided reinforcement learning?
- Basis in paper: [explicit] The discussion explicitly lists "how different model scales affect performance" as a key remaining question
- Why unresolved: The experiments exclusively utilized GPT-4o-mini, leaving the impact of varying model capacities unexplored
- What evidence would resolve it: Comparative benchmarking of the dual-system architecture using a range of model sizes (e.g., smaller specialized models vs. larger general-purpose models) on identical navigation tasks

### Open Question 2
- Question: Do advanced reasoning schemas, such as chain-of-thought prompting, alter the decision-making dynamics between the RL policy and the narrative framework?
- Basis in paper: [explicit] The authors ask "whether more sophisticated reasoning approaches might yield different results" and suggest exploring chain-of-thought and multi-agent architectures
- Why unresolved: The current implementation relied on direct narrative processing without testing complex reasoning layers
- What evidence would resolve it: Experiments comparing standard narrative prompting against chain-of-thought prompting to measure changes in success rates and RL policy adherence

### Open Question 3
- Question: Does the observed influence of specific narrative frameworks (e.g., Westworld) generalize to non-gridworld domains or more complex environments?
- Basis in paper: [explicit] The paper notes that "extensions [to complex domains] would require careful validation" and asks how consistently frameworks influence behavior "across different environmental conditions"
- Why unresolved: The study was constrained to simple gridworlds (5x5 to 11x11), limiting the generalizability of the narrative effects
- What evidence would resolve it: Testing the same narrative-guided architecture in continuous control tasks or 3D environments to see if the Westworld framework retains its performance advantage

## Limitations
- The platform operates within a simplified gridworld environment that may not generalize to complex real-world decision-making scenarios
- Computational overhead (5-30 minutes vs <1 second for baseline) represents a significant scalability concern that wasn't addressed in this preliminary work
- The specific narrative prompt formulations and LLM override mechanisms remain underspecified, creating uncertainty about reproducibility

## Confidence
- High confidence: The platform architecture is technically sound and the computational overhead measurements are clearly documented
- Medium confidence: The comparative performance results between LLM+RL and RL-only agents are reproducible within the described experimental setup
- Low confidence: Generalizability of narrative framework effects to more complex decision domains beyond gridworld navigation

## Next Checks
1. **Reproducibility validation**: Implement the system with multiple random seeds and grid configurations to verify that narrative framework effects persist across different environment instantiations
2. **Computational efficiency analysis**: Profile the system to identify LLM inference bottlenecks and test whether prompt optimization or model quantization can reduce the 5-30 minute runtime while maintaining performance gains
3. **Domain transfer experiment**: Test the narrative framework approach on a different decision-making domain (such as text-based adventure games or resource allocation tasks) to assess whether the observed narrative influence generalizes beyond gridworld navigation