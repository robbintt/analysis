---
ver: rpa2
title: Learning non-Markovian Dynamical Systems with Signature-based Encoders
arxiv_id: '2509.12022'
source_url: https://arxiv.org/abs/2509.12022
tags:
- neural
- learning
- signature
- encoder
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of signature transforms as an alternative
  to recurrent neural networks (RNNs) for encoding non-Markovian dynamical systems
  within encoder-decoder frameworks. While RNNs are standard for capturing historical
  dependencies, they are inherently discrete and suffer from issues like vanishing
  gradients, making them ill-suited for continuous-time modeling.
---

# Learning non-Markovian Dynamical Systems with Signature-based Encoders

## Quick Facts
- arXiv ID: 2509.12022
- Source URL: https://arxiv.org/abs/2509.12022
- Reference count: 35
- Primary result: Signature-based encoders significantly outperform RNNs in modeling non-Markovian dynamics with better accuracy and training stability

## Executive Summary
This paper proposes signature-based encoders as a continuous-time alternative to recurrent neural networks for learning non-Markovian dynamical systems. By leveraging the mathematical properties of signature transforms from rough path theory, the authors demonstrate superior performance on synthetic benchmarks involving delayed differential equations. The approach integrates signature encoders into two state-of-the-art decoder architectures, achieving lower test RMSE and faster, more stable training compared to RNN baselines.

## Method Summary
The method employs an encoder-decoder framework where the encoder uses signature transforms to compress historical trajectories into latent initial conditions. A learnable preprocessing map Φ_θ transforms the input stream using a sliding window MLP, which is then passed through a truncated signature layer (depth N=3) to capture nonlinear temporal interactions. The resulting features are linearly projected to the latent space and fed into either a Neural Laplace or Neural Flow ResNet decoder. The approach is evaluated on four synthetic DDE benchmarks with fixed delays.

## Key Results
- Signature-based models achieve significantly lower RMSE than RNN baselines (e.g., 0.0540 vs 0.1063 for Neural Laplace on delayed Lotka-Volterra)
- Training is faster and more stable with signature encoders compared to RNNs
- The learnable preprocessing map Φ_θ is critical for performance, capturing complex dependencies at lower signature depths
- Signature models show greater robustness to coupling strength and noise, and are more computationally efficient

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Truncated signature transforms effectively summarize non-Markovian history for downstream decoders.
- **Mechanism:** The signature represents a path as a graded sequence of iterated integrals, hierarchically compressing historical path x(t) into a feature vector that acts as a "fingerprint" of the system's history.
- **Core assumption:** The dynamical system can be approximated by a path of bounded variation, and relevant history dependencies are captured within the truncated depth N.
- **Evidence anchors:** [abstract] "The signature transform... offers a continuous-time alternative... proven efficiency in summarizing multidimensional information in time." [section 2] "Intuitively, one can think of the signature as an infinite dictionary of path features... higher-order terms capture more nuanced interactions."
- **Break condition:** Performance collapses if history depends on interactions of order significantly higher than chosen truncation depth N, and Φ_θ fails to lift these features to lower orders.

### Mechanism 2
- **Claim:** Replacing RNNs with signature encoders mitigates vanishing gradient issues and improves training stability.
- **Mechanism:** Unlike RNNs that unroll sequentially and propagate gradients through n time steps, signature computation is a direct transformation of the input stream. Gradients flow directly from loss through signature coefficients to parameters of Φ_θ, avoiding long temporal credit assignment chain.
- **Core assumption:** The primary bottleneck in learning these dynamics is optimization difficulty (vanishing gradients) rather than raw representational capacity of hidden state.
- **Evidence anchors:** [abstract] "...RNNs... suffer from issues like vanishing gradients... signature transform... facilitates more effective training." [section 5.2] "...signature-based models achieve faster training, suggesting a more favorable loss landscape or informative gradient updates."
- **Break condition:** If system dynamics require strict statefulness that cannot be approximated by summary of fixed-length history window, model may fail to converge regardless of gradient stability.

### Mechanism 3
- **Claim:** The learnable preprocessing map Φ_θ allows the model to capture complex dependencies at lower signature depths.
- **Mechanism:** Signature is calculated on transformed path y(t) = Φ_θ(x(t)). By lifting input to higher dimension (d → e) via sliding window MLP, model can linearize complex interactions. Lower-order term in signature of y can represent what would require higher-order term in signature of x.
- **Core assumption:** Path information depends non-trivially on higher-order terms that are difficult to capture with standard truncation alone.
- **Evidence anchors:** [section 4] "...since the signature must be truncated... this approach may alleviate issues that arise when path information depends nontrivially on higher-order signature terms." [section 5.2] Ablation study shows removing Φ_θ increases error significantly.
- **Break condition:** If Φ_θ is insufficiently expressive or window size m is too small to capture local temporal features, signature encoder will underperform.

## Foundational Learning

- **Concept: Rough Path Theory & The Signature Transform**
  - **Why needed here:** This is the core alternative to RNNs proposed. You must understand that a signature is not just an embedding but a structured set of iterated integrals that uniquely characterizes a stream.
  - **Quick check question:** Can you explain why the signature is described as a "universal nonlinearity" for functions on paths?

- **Concept: Non-Markovian Dynamics (Delay Differential Equations)**
  - **Why needed here:** The paper specifically targets systems where ẋ(t) depends on x(t-τ). Understanding that standard Neural ODEs fail here (because they assume ẋ(t) = f(x(t))) is crucial for problem motivation.
  - **Quick check question:** Why does a standard Neural ODE fail to model a system with a fixed time delay τ?

- **Concept: Encoder-Decoder Architectures for Dynamics**
  - **Why needed here:** The paper unifies Neural Laplace and Neural Flow under this framework. You need to grasp how the "Encoder" compresses history into a latent initial condition z₀, and the "Decoder" unrolls the future trajectory.
  - **Quick check question:** In this framework, what is the specific output of the encoder u_α that is passed to the decoder?

## Architecture Onboarding

- **Component map:** Input Stream -> Lifting Layer (Φ_θ) -> Signature Core -> Projection (g_ξ) -> Decoder
- **Critical path:** The selection of Signature Depth (N) and Lifting Dimension (e). The dimension of signature grows exponentially with N. The paper validates N=3 as sweet spot; going higher risks dimension explosion and overfitting, while lower (N=1) loses interaction data.
- **Design tradeoffs:**
  - **RNN vs. Signature:** RNNs are flexible but suffer from gradient issues and discretization error. Signatures are robust and continuous but require fixed history window and exponential dimension management.
  - **Depth vs. Dimension:** Deeper signature captures more complex history but creates massive feature vector that can slow down projection layer and overfit on small datasets.
- **Failure signatures:**
  - **Gradient Instability:** If lifting dimension e is too high combined with high depth N, output dimension becomes unmanageable, potentially leading to memory errors or noisy gradients.
  - **Underfitting History:** If window size m in Φ_θ is smaller than delay τ of underlying DDE, model may fail to capture causal link.
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run Sig-Neural Laplace model on dataset with and without Φ_θ lifting layer to quantify performance gain from preprocessing map.
  2. **Depth Sensitivity:** Train models with N ∈ {1, 2, 3} on Delayed Lotka-Volterra system. Verify N=3 offers best RMSE/parameter balance as per Table 2.
  3. **Robustness to Irregular Sampling:** Sub-sample training trajectories (simulating irregular time steps) and compare degradation of RNN-baseline vs. Signature model to validate continuous-time advantage.

## Open Questions the Paper Calls Out

- **Question:** How do other signature-based architectures, such as Neural CDEs or Logsig-RNN, compare to the proposed Sig Neural Laplace and Sig Neural Flow ResNet as encoders for non-Markovian systems?
- **Question:** Do signature-based encoders retain their superiority over RNNs when applied to complex, real-world physical systems with unknown or stochastic delay structures?
- **Question:** Is there a theoretical or empirical relationship linking optimal signature truncation depth (N) to magnitude of system's time delay (τ)?

## Limitations
- Performance gains are demonstrated only on synthetic benchmark datasets with fixed delays and known governing equations
- The exponential growth of signature features with depth (dimension ~e^(N+1)) creates scalability concerns not empirically explored beyond N=3
- No validation on real-world systems where measurement noise, irregular sampling, and model mismatch are common

## Confidence

- **High Confidence:** The signature transform provides mathematically sound, continuous-time alternative to RNNs for encoding temporal dependencies. Mechanism for gradient stability (avoiding sequential unrolling) is well-founded.
- **Medium Confidence:** Specific performance improvements (RMSE reductions) on four synthetic DDEs are reproducible, but magnitude may not translate to real-world scenarios with noise and irregular sampling.
- **Medium Confidence:** Claim that learnable lifting layer Φ_θ is essential for capturing complex dependencies is supported by ablation, but exact sensitivity to architectural choices (window size, MLP depth) remains unclear.

## Next Checks

1. **Noise Robustness Test:** Evaluate both RNN and signature-based models on same DDE benchmarks with added Gaussian noise (SNR 10-30 dB) to assess real-world applicability.
2. **Irregular Sampling Analysis:** Sub-sample training trajectories at non-uniform intervals and measure degradation in both models to validate continuous-time advantage of signatures.
3. **Scalability Study:** Systematically increase signature depth N and measure trade-off between performance gains and computational cost (memory, training time) to identify practical limits.