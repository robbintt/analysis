---
ver: rpa2
title: $O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation
arxiv_id: '2504.19375'
source_url: https://arxiv.org/abs/2504.19375
tags:
- bound
- lemma
- assumption
- two-time-scale
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper obtains an improved O(1/k) finite-time bound for non-linear\
  \ two-time-scale stochastic approximation, significantly improving upon the previous\
  \ O(1/k^{2/3}) bound. The key innovation is introducing an averaged noise sequence\
  \ Uk+1 = \u03B2kM\u2032k+1 + (1-\u03B2k)Uk (with U0=0) and defining zk = yk - Uk,\
  \ which allows the original iteration to be rewritten in terms of zk."
---

# $O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation

## Quick Facts
- arXiv ID: 2504.19375
- Source URL: https://arxiv.org/abs/2504.19375
- Reference count: 28
- Primary result: O(1/k) mean square error bound for non-linear two-time-scale stochastic approximation

## Executive Summary
This paper establishes an improved O(1/k) finite-time bound for non-linear two-time-scale stochastic approximation, improving upon the previous O(1/k^{2/3}) bound. The key innovation involves rewriting the iteration using an averaged noise sequence and transformed iterates, allowing tighter control of the slower time-scale dynamics. The analysis proves iterate boundedness through induction and provides two theorems: one requiring knowledge of system parameters for optimal rate, and another achieving a more robust O(1/k^a) bound without parameter dependence.

## Method Summary
The method analyzes coupled iterations where a faster time-scale variable x_k tracks a conditional fixed point x^*(y_k) while a slower time-scale variable y_k converges to the global fixed point y*. The core innovation is introducing an averaged noise sequence U_{k+1} = β_k M'_{k+1} + (1-β_k)U_k with transformed iterates z_k = y_k - U_k, which enables tighter finite-time bounds. Stepsizes α_k = α/(k+K_1) and β_k = β/(k+K_1) are used with constraints β ≥ 2/(1-μ) and β/α ≤ C_1. The proof employs induction-based boundedness arguments and recursive Lyapunov analysis.

## Key Results
- Achieves O(1/k) mean square error bound for non-linear two-time-scale stochastic approximation
- Introduces averaged noise technique that enables tight finite-time bounds
- Provides two theorems: one with optimal rate requiring parameter knowledge, another with robust O(1/k^a) rate without parameter dependence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing an auxiliary averaged noise sequence enables tight finite-time bounds in the slower time-scale iteration.
- Mechanism: Define $U_{k+1} = \beta_k M'_{k+1} + (1-\beta_k)U_k$ with $U_0 = 0$ and analyze transformed iterates $z_k = y_k - U_k$. The averaged noise $\mathbb{E}[\|U_k\|^2]$ decays at rate $O(\beta_k)$ under iterate boundedness, which is fast enough to close the recursion without degrading the overall rate.
- Core assumption: Martingale difference noise with second moment scaling affinely with iterates: $\mathbb{E}[\|M'_{k+1}\|^2 | \mathcal{F}_k] \leq c_1(1 + \|x_k\|^2 + \|y_k\|^2)$ (Assumption 4).
- Evidence anchors:
  - [abstract] "The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence which decays sufficiently fast."
  - [Section 1.1] "We identify that the reason for a bound of O(1/k^{2/3}) in [15] was the manner in which M'_{k+1}, the martingale noise in the slower time-scale iteration, was handled."
  - [corpus] Related work [2503.18391] achieves O(1/k^{2/3}) for arbitrary norm contractions with Markovian noise but O(1/k) only when the slower time-scale is noiseless—confirming noise handling is the bottleneck.

### Mechanism 2
- Claim: Strong induction coupled with a recursive Lyapunov function proves iterate boundedness in expectation for all $k$.
- Mechanism: Define $\Gamma_2 = 2 + 4(4L_0^2 + 2)S_0 + 8\|x^*\|^2 + 4\|y^*\|^2$. Show: if $\mathbb{E}[1 + \|x_i\|^2 + \|y_i\|^2] \leq \Gamma_2$ for all $i \leq k-1$, then the same holds at $k$. Base case $k=0$ is verified directly; induction extends to all $k$.
- Core assumption: Lipschitz and contractive structure (Assumptions 1-3) ensures the Lyapunov recursion closes with a negative drift term proportional to stepsize.
- Evidence anchors:
  - [Section 4.1] "The above lemma shows that if E[1 + \|x_i\|^2 + \|y_i\|^2] is bounded by \Gamma_2 for i \leq k-1, then E[1 + \|x_k\|^2 + \|y_k\|^2] is also bounded by \Gamma_2. By strong induction, this implies that E[1 + \|x_k\|^2 + \|y_k\|^2] is bounded by \Gamma_2 for all k."
  - [Lemma 5 proof] Explicit bound construction showing the inductive step holds when $K_1$ is sufficiently large.

### Mechanism 3
- Claim: Time-scale separation enforced through stepsize ratio constraints yields optimal O(1/k) convergence.
- Mechanism: Set $\alpha_k = \alpha/(k+K_1)$, $\beta_k = \beta/(k+K_1)$ with constraint $\beta/\alpha \leq C_1$ where $C_1 = \frac{\lambda'\mu'}{8LL_0 + 64L_0^2 + 4 + 14L^2}$. This ensures the cross-terms from coupled dynamics remain dominated by the negative drift. For robust tuning, use $\alpha_k = O(1/k^a)$ with $a \in (0.5,1)$ and $\beta_k = O(1/k)$, achieving $O(1/k^a)$ without knowing system parameters.
- Core assumption: The ratio $\beta_k/\alpha_k$ must be sufficiently small (Theorem 1) or decay to zero (Theorem 2) to decouple time-scales asymptotically.
- Evidence anchors:
  - [Section 3.1] "The assumptions that \beta \geq 2/(1-\mu) and \beta/\alpha \leq C_1 are necessary for our analysis."
  - [Section 3.2] "We do not need the assumption \beta/\alpha \leq C_1... Instead, the time-scale separation is specified by \lim_{k \uparrow \infty} \beta_k/\alpha_k \to 0."

## Foundational Learning

- Concept: Banach Fixed-Point Theorem
  - Why needed here: Establishes existence and uniqueness of $x^*(y)$ for each $y$ given $\lambda$-contractivity of $f(\cdot, y)$, and of the global fixed point $(x^*, y^*)$ given $\mu$-contractivity of $g(x^*(\cdot), \cdot)$.
  - Quick check question: If $\|f(x_1, y) - f(x_2, y)\| \leq 0.9\|x_1 - x_2\|$, does $x^*(y)$ exist uniquely for each $y$?

- Concept: Martingale Difference Sequences
  - Why needed here: The noise terms $M_{k+1}$ and $M'_{k+1}$ are assumed zero-mean conditioned on history ($\mathcal{F}_k$), which ensures cross-terms vanish in expectation when expanding Lyapunov recursions.
  - Quick check question: Given $\mathbb{E}[M_{k+1}|\mathcal{F}_k] = 0$, what is $\mathbb{E}[\langle x_k - x^*, M_{k+1}\rangle]$?

- Concept: Polyak-Ruppert Averaging
  - Why needed here: The averaged noise sequence $U_k$ is structurally similar to Polyak averaging but applied to noise rather than iterates. Understanding averaging helps see why $U_k$ inherits the $O(\beta_k)$ decay rate.
  - Quick check question: If $U_{k+1} = (1-\beta_k)U_k + \beta_k \xi_k$ with $\beta_k = \beta/(k+K)$ and $\mathbb{E}[\xi_k^2] \leq \sigma^2$, what is the order of $\mathbb{E}[U_k^2]$?

## Architecture Onboarding

- Component map:
  - Faster time-scale iteration: $x_{k+1} = x_k + \alpha_k(f(x_k, y_k) - x_k + M_{k+1})$ — tracks the conditional fixed point $x^*(y_k)$
  - Slower time-scale iteration: $y_{k+1} = y_k + \beta_k(g(x_k, y_k) - y_k + M'_{k+1})$ — converges to the equilibrium $y^*$
  - Auxiliary averaged noise (analysis only): $U_{k+1} = \beta_k M'_{k+1} + (1-\beta_k)U_k$, $z_k = y_k - U_k$ — not implemented; used to decouple noise from slow dynamics

- Critical path: Verify contractivity assumptions → Choose stepsizes satisfying $\beta \geq 2/(1-\mu)$ and $\beta/\alpha \leq C_1$ → Run coupled iterations → Track $\mathbb{E}[\|x_k - x^*\|^2 + \|y_k - y^*\|^2]$

- Design tradeoffs:
  - Optimal rate vs. robustness: Theorem 1 gives $O(1/k)$ but requires $K_1 \geq C_2$ (dependent on system parameters). Theorem 2 gives $O(1/k^a)$ with $a \approx 1$ but requires only $K_2 \geq D_1$, which avoids knowing $\lambda, \mu, L$ for tuning.
  - Noise assumption: State-dependent noise variance permits broader applicability but requires the inductive boundedness proof. Uniformly bounded noise would simplify analysis.
  - Contraction strength: Smaller $\lambda, \mu$ (stronger contraction) tightens constants $C_1, C_2, D_1$; as $\lambda \to 1$, the bounds become vacuous.

- Failure signatures:
  - Iterates diverge: Likely $\beta_k$ too large relative to $\alpha_k$ (violates time-scale separation) or noise variance grows faster than assumed.
  - Convergence stalls at high error: Check if $K_1$ or $K_2$ is too small, making stepsizes too aggressive early.
  - Oscillatory behavior: May indicate weak contractivity ($\lambda, \mu$ close to 1) or correlated noise violating martingale assumption.

- First 3 experiments:
  1. **Linear warm-up**: Implement $x_{k+1} = x_k + \alpha_k(b_1 - A_{11}x_k - A_{12}y_k + M_{k+1})$, $y_{k+1} = y_k + \beta_k(b_2 - A_{21}x_k - A_{22}y_k + M'_{k+1})$ with known $-A_{11}$ and $-\Delta$ Hurwitz. Verify $O(1/k)$ MSE decay against Theorem 1 prediction.
  2. **Nonlinear test — gradient descent-ascent**: Apply to $\min_y \max_x F(x,y)$ where $F$ is strongly convex-concave. Track $\mathbb{E}[\|x_k - x^*\|^2 + \|y_k - y^*\|^2]$ and compare rate under $\alpha_k = c/k$ for various $c$ and $\beta/\alpha$ ratios.
  3. **Stress test — weak contraction**: Vary $\lambda \to 1$ (e.g., via temperature in softmax operators) and observe when bounds become loose. Compare induction-based boundedness predictions against empirical second moments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can O(1/k) finite-time bounds be obtained for non-linear two-time-scale SA under Markovian noise instead of martingale difference noise?
- Basis in paper: [explicit] "Future directions include obtaining similar O(1/k) bounds for the non-linear setting under Markovian noise"
- Why unresolved: The current analysis relies on martingale difference properties (orthogonality of terms) that do not hold under Markovian state-dependent noise, requiring new proof techniques.
- What evidence would resolve it: A proof extending Theorem 1 or 2 to settings where noise depends on the current iterate via a Markov chain, with explicit finite-time bounds.

### Open Question 2
- Question: Can O(1/k) bounds be achieved when the contraction property holds under arbitrary norms rather than just the Euclidean norm?
- Basis in paper: [explicit] "Future directions include obtaining similar O(1/k) bounds... for arbitrary norm contractions"
- Why unresolved: The current proof uses Euclidean-specific properties in the recursive bounds (e.g., cross-term handling in Lemmas 3-4), which may not generalize directly.
- What evidence would resolve it: A proof showing O(1/k) convergence when ∥·∥ is replaced by a general norm with the same contraction factor.

### Open Question 3
- Question: Can high probability concentration bounds be derived for non-linear two-time-scale SA with the same O(1/k) rate?
- Basis in paper: [explicit] "Another extension could be to obtain high probability bounds for these two-time-scale iterations"
- Why unresolved: The current analysis provides only mean square error bounds; concentration requires different techniques (e.g., martingale concentration inequalities) to control tail probabilities.
- What evidence would resolve it: A theorem providing bounds of the form P(∥xk−x∗∥² + ∥yk−y∗∥² ≥ ε) ≤ δ with explicit k, ε, δ relationships.

### Open Question 4
- Question: Can O(1/k) bounds be obtained under non-expansive or weaker assumptions instead of strict contractivity?
- Basis in paper: [explicit] "Replacing our assumption that the mappings are contractive with weaker assumptions such as non-expansive mappings could be another direction"
- Why unresolved: The current proof critically uses λ < 1 and µ < 1 to obtain contraction factors (1−λ′αm) and (1−µ′βm) in recursions; λ=1 would break this structure.
- What evidence would resolve it: Finite-time bounds for the case where contraction factors equal 1, possibly with slower rates or additional averaging assumptions.

## Limitations

- The analysis critically depends on the induction-based boundedness proof, which has limited support in the corpus
- The state-dependent noise variance assumption requires careful verification for specific problems
- The constants C₁, C₂, D₁ involve system parameters that must be estimated or bounded for any concrete implementation

## Confidence

- **High Confidence**: The O(1/k) convergence rate claim under Theorem 1 conditions, supported by the averaging technique and induction argument
- **Medium Confidence**: The extension to O(1/k^a) rates under Theorem 2, as it requires additional stepsize tuning without parameter knowledge
- **Medium Confidence**: The boundedness induction, novel to this paper with limited external validation

## Next Checks

1. **Test boundedness induction**: Implement the proposed algorithm on a simple non-linear example (e.g., gradient descent-ascent on a strongly convex-concave quadratic) and empirically verify that E[1 + ‖x_k‖² + ‖y_k‖²] remains bounded over all k, matching the induction claim.

2. **Validate stepsize constraints**: For the same example, systematically vary α and β to verify the empirical impact of violating β/α ≤ C₁ and β ≥ 2/(1-μ), comparing against predicted performance degradation.

3. **Stress test noise assumptions**: Modify the noise variance scaling (uniformly bounded vs. state-dependent) and observe how it affects both empirical convergence rates and the necessity of the averaging technique.