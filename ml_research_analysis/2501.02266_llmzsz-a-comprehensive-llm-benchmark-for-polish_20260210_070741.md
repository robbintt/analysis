---
ver: rpa2
title: "LLMzSz\u0141: a comprehensive LLM benchmark for Polish"
arxiv_id: '2501.02266'
source_url: https://arxiv.org/abs/2501.02266
tags:
- exams
- language
- school
- polish
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLMzSz\u0141 is the first large-scale Polish LLM benchmark, built\
  \ from national school and vocational exams (nearly 19k multiple-choice questions\
  \ across 154 domains). Using CKE exam archives, the dataset covers four exam types\
  \ and is stratified by difficulty."
---

# LLMzSzŁ: a comprehensive LLM benchmark for Polish

## Quick Facts
- arXiv ID: 2501.02266
- Source URL: https://arxiv.org/abs/2501.02266
- Authors: Krzysztof Jassem; Michał Ciesiółka; Filip Graliński; Piotr Jabłoński; Jakub Pokrywka; Marek Kubis; Monika Jabłońska; Ryszard Staruch
- Reference count: 22
- Primary result: First large-scale Polish LLM benchmark built from national exams with 19k questions across 154 domains; multilingual models outperform monolingual ones, with best model (Mistral-Large-Instruct-2407) achieving 67.17% accuracy

## Executive Summary
LLMzSzŁ is the first comprehensive benchmark for evaluating large language models on Polish language exam questions, constructed from national school and vocational exams with nearly 19,000 multiple-choice questions across 154 domains. The benchmark reveals distinct performance tiers based on model size, with models under 3B parameters performing near random guessing, while larger models show significant improvements. The dataset enables evaluation across four exam types and provides insights into how instruction tuning and cross-lingual knowledge transfer affect model performance on structured Polish language tasks.

## Method Summary
The benchmark uses Polish national exam questions from the Central Examination Board (CKE) archives, extracted from PDF documents and manually cleaned to create a dataset of ~19k closed-ended questions across 4 exam types and 154 domains. Evaluation follows the LM Evaluation Harness framework with MMLU-style likelihood comparison: questions are formatted with a Polish prompt template, the model computes likelihood for each answer option (A-D), and the highest-probability answer is compared to the gold label. Models are evaluated based on size, language (monolingual vs. multilingual), release date, and fine-tuning status, with recommendations to filter by temporal contamination.

## Key Results
- Model scale creates distinct performance tiers: models <3B parameters perform at ~25% (random baseline), while models >15B parameters achieve 67.17% accuracy
- Multilingual models generally outperform monolingual Polish models, though monolingual models may be beneficial when model size matters
- Instruction-tuned models generally perform better than base models, though Mistral-7B-v0.1 showed a -1.77% degradation with instruction tuning
- The best model (Mistral-Large-Instruct-2407) achieved 67.17% accuracy, while smaller models (under 15B parameters) lagged significantly

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Knowledge Transfer in Multilingual Models
- Claim: Multilingual models may outperform monolingual Polish models by leveraging knowledge encoded during training on higher-resource languages.
- Mechanism: Large multilingual models potentially share representational space across languages, allowing Polish queries to activate knowledge primarily encoded through English or other high-resource training data.
- Core assumption: Knowledge representations are at least partially language-agnostic within multilingual models.
- Evidence anchors:
  - [abstract] "We show that multilingual LLMs can obtain superior results over monolingual ones; however, monolingual models may be beneficial when model size matters."
  - [section 4.2] "We hypothesize that multilingual models, especially large ones, are able to transfer knowledge from other languages to Polish."
  - [corpus] Limited direct support; neighbor papers focus on exam benchmarks but don't address cross-lingual transfer mechanisms.
- Break condition: Assumption may fail for models under ~8B parameters or for culture-specific knowledge not well-represented in dominant training languages.

### Mechanism 2: Instruction Tuning Alignment with Structured Evaluation
- Claim: Instruction-tuned models generally achieve higher accuracy on closed-ended exam tasks than their base counterparts.
- Mechanism: Instruction tuning potentially shapes model behavior toward structured response formats, improving the selection of correct answers through learned patterns of following task specifications.
- Core assumption: The instruction tuning process exposes models to formats or reasoning patterns similar to multiple-choice examination.
- Evidence anchors:
  - [abstract] "instruction-tuned models generally perform better, except for Mistral-7B-v0.1"
  - [section 4.4] Table 2 shows 8 of 9 tested models improved with instruction tuning (range +0.33 to +3.45 percentage points).
  - [corpus] Weak corpus support; neighbor papers don't isolate instruction tuning effects.
- Break condition: Not universal—Mistral-7B-v0.1 showed a -1.77% degradation, suggesting instruction tuning quality or data composition matters significantly.

### Mechanism 3: Scale-Dependent Polish Language Competence
- Claim: Model scale creates distinct performance tiers, with models below ~3B parameters performing near random baseline for Polish exam tasks.
- Mechanism: Larger models may better encode Polish morphological complexity (rich case system, inflection) and broader domain knowledge required across 154 exam domains.
- Core assumption: Polish language processing requires sufficient parameter capacity to capture its grammatical complexity.
- Evidence anchors:
  - [section 4.1] "Models smaller than 3B parameters generally perform at the level of random guessing, with an accuracy of approximately 25%."
  - [section 4.1] Performance scales with size: 123B achieves 67.17%, 70B achieves 66.59%, 11B achieves 57.52%, 7B achieves 46.84%.
  - [corpus] No direct corpus evidence on Polish-specific scale thresholds.
- Break condition: Monolingual Polish models in the 8-15B range (e.g., Bielik-11B) may achieve competitive results, suggesting language-specific training can partially offset size disadvantages.

## Foundational Learning

- Concept: **Likelihood-based multiple-choice evaluation**
  - Why needed here: The benchmark computes probability for each answer option rather than generating free-text responses, enabling objective automated scoring.
  - Quick check question: Can you explain why comparing token likelihoods across answer options is more reliable for multiple-choice evaluation than prompting for text generation?

- Concept: **Cross-lingual transfer and training data distribution**
  - Why needed here: Understanding why multilingual models succeed on Polish exams requires distinguishing between genuine transfer and potential memorization from training data.
  - Quick check question: What experimental design would help disentangle cross-lingual transfer from contamination by Polish training data?

- Concept: **Temporal data contamination mitigation**
  - Why needed here: The benchmark includes timestamps to enable evaluation on exams published after model training cutoffs.
  - Quick check question: Why is temporal separation critical for benchmark validity, and what limitations remain even with timestamp filtering?

## Architecture Onboarding

- Component map: Polish Central Examination Board (CKE) PDFs → PyPDF + OCR extraction → manual question-answer matching → LLMzSzŁ dataset → LM-Evaluation-Harness → accuracy scores per model, exam type, year, and domain

- Critical path:
  1. Format question using Polish prompt template with 4 answer options (A-D)
  2. Compute likelihood for each answer option using the target model
  3. Select highest-probability answer and compare against gold label
  4. Aggregate accuracy across stratifications (exam type, domain, year)

- Design tradeoffs:
  - Closed-ended only: Enables automated evaluation but excludes open-response questions that comprise significant portions of actual exams
  - Single authoritative source (CKE): High credibility and no duplicates, but limited to Polish national exam context
  - PDF extraction: Some questions with images/charts excluded; manual cleaning required

- Failure signatures:
  - Calculation-heavy questions underperform (Table 3: numeric answers and terms like "wynosi," "oblicz" correlate with lower scores)
  - Domain-specific gaps: Professional exams in forestry (R.13) and mechanics (M.39, M.11) show elevated difficulty
  - Answer position bias: Model showed tendency toward wrong answers for option B in some cases

- First 3 experiments:
  1. Run a baseline comparison of 3 multilingual vs. 3 monolingual models on a single exam category (e.g., high school biology) to validate cross-lingual transfer hypothesis locally.
  2. Apply temporal filtering to evaluate only on exams published after each model's training cutoff, comparing to full-dataset scores to estimate contamination effects.
  3. Test error-detection capability: Present models with known-erroneous exam questions (as identified in Section 5) and measure low-probability assignment to stated correct answers as a validation signal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs serve as reliable tools for pre-publication verification of exam difficulty and error detection, given the observed correlations between model and human performance?
- Basis in paper: [explicit] Section 6.4 states: "If this phenomenon is confirmed with more data (possibly including open questions), it will advocate for a possible use of LLMs as a primary tool for the verification of exam questions prior to their publication."
- Why unresolved: Current correlation data is limited to closed-ended questions and shows inconsistent patterns across exam types (positive for Junior High/MMM, negative for High School). The sample size per category is small, and the mechanism behind these correlations remains unexplained.
- What evidence would resolve it: Longitudinal studies tracking model performance on newly released exams against subsequent human results, including open-ended questions, with statistical validation of predictive power.

### Open Question 2
- Question: What is the extent of data contamination in the benchmark, and how does it affect the validity of model comparisons?
- Basis in paper: [explicit] Section 8 acknowledges: "Taking into consideration that the exams are published online, there is also a risk of data contamination. Although the exam questions are stored separately from the answers... this issue cannot be neglected."
- Why unresolved: The authors recommend tracking performance on questions published before/after model release dates but do not quantify actual contamination or analyze differential impact across models.
- What evidence would resolve it: Controlled experiments comparing model performance on held-out exam questions versus likely-seen questions, plus analysis of memorization patterns through membership inference tests.

### Open Question 3
- Question: To what extent does proficiency on exam-based benchmarks translate to real-world professional competence for vocational domains?
- Basis in paper: [explicit] Section 8 states: "The extent to which an examinee's proficiency in solving tests generalizes to performance in real-life situations is an open question. Measuring real-world competence of an LLM in skills covered by vocational exams would require the construction of an embodied agent which was not feasible at the time of writing."
- Why unresolved: The benchmark measures declarative knowledge and reasoning on closed questions but cannot assess practical skills (e.g., mechanical operations, safety procedures) that define professional competence.
- What evidence would resolve it: Comparative studies between LLM exam performance and outcomes on practical skill assessments or simulated task environments for vocational domains.

### Open Question 4
- Question: Why does instruction tuning degrade performance for some models (e.g., Mistral-7B-v0.1), and what determines whether fine-tuning helps or harms?
- Basis in paper: [explicit] Table 2 shows instruction tuning improves performance for 8/9 models, but Mistral-7B-v0.1 shows a -1.77% gain, leading the authors to conclude "fine-tuning generally helps, although it is not always guaranteed."
- Why unresolved: The paper documents this anomaly but offers no hypothesis or investigation into the underlying causes, leaving the conditions for successful instruction tuning unclear.
- What evidence would resolve it: Systematic analysis of instruction-tuning datasets and procedures across models showing negative gains, examining factors such as data quality, language coverage, and training hyperparameters.

## Limitations
- Data contamination risk: Exams published online may have been encountered during model training, potentially inflating performance scores
- Limited generalizability: Single national examination board source restricts context to Polish educational system
- Format bias: Exclusive focus on closed-ended questions excludes significant portions of actual exam content

## Confidence
- High Confidence: Model scale-performance relationship, instruction tuning effects on structured tasks, methodology soundness
- Medium Confidence: Cross-lingual transfer mechanism, correlation between LLM and human performance
- Low Confidence: Practical utility of LLM-based quality control, exact reasons for domain performance gaps

## Next Checks
1. **Cross-lingual Transfer Isolation Test**: Create a controlled experiment comparing multilingual and monolingual models on exams published exclusively after the training cutoffs of both model types.
2. **Human-LLM Performance Correlation Validation**: Conduct systematic study where human experts grade same exam questions that LLMs evaluate, measuring correlation coefficients across different question types.
3. **Open-Response Question Extension**: Extend benchmark to include open-response questions from same exam archives, either through human evaluation or automated scoring mechanisms.