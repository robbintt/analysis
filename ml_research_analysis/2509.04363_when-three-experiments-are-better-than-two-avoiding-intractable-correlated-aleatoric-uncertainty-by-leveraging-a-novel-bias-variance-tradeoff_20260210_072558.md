---
ver: rpa2
title: 'When three experiments are better than two: Avoiding intractable correlated
  aleatoric uncertainty by leveraging a novel bias--variance tradeoff'
arxiv_id: '2509.04363'
source_url: https://arxiv.org/abs/2509.04363
tags:
- bias
- type
- points
- uncertainty
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes novel active learning strategies that leverage
  a bias-variance tradeoff to efficiently reduce the mean squared error between a
  model distribution and a ground-truth random variable in experimental scenarios
  with heteroskedastic aleatoric uncertainty. The core method calculates the expected
  mean squared error as the sum of epistemic uncertainty, bias squared, and aleatoric
  uncertainty terms, then uses the negative gradient of this error between experimental
  rounds to select informative points for labeling.
---

# When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff

## Quick Facts
- arXiv ID: 2509.04363
- Source URL: https://arxiv.org/abs/2509.04363
- Reference count: 34
- Primary result: Novel active learning strategies leveraging cobias-covariance relationships and difference-based gradients outperform canonical methods like BALD and Least Confidence in batched settings with heteroskedastic aleatoric uncertainty.

## Executive Summary
This paper addresses the challenge of active learning under heteroskedastic aleatoric uncertainty by introducing a novel acquisition function that decomposes mean squared error into epistemic, bias, and aleatoric components. The key innovation is the "difference-PEMSE" method, which uses the negative gradient between experimental rounds to select informative points while filtering out intractable noise. The authors also introduce a cobias-covariance matrix completion framework that enables quadratic estimation of bias and principled batching through eigendecomposition. Experimental results on noiseless, uncorrelated noise, and correlated noise scenarios demonstrate superior performance compared to standard active learning methods, particularly in low-data regimes and batched settings.

## Method Summary
The method decomposes the expected mean squared error into three terms: epistemic uncertainty, bias squared, and aleatoric uncertainty. It uses the difference between consecutive rounds' PEMSE to cancel out the intractable aleatoric noise, leaving only the reducible components. For batching, the approach estimates the cobias-covariance matrix (representing bias as pairwise relationships between points) using a symmetric neural network, then selects batches via eigendecomposition of this matrix to capture diverse error directions. The quadratic estimation approach leverages historical data more effectively than direct scalar estimation, particularly for correlated noise scenarios.

## Key Results
- Difference-PEMSE acquisition function outperforms canonical methods (BALD, Least Confidence) in batched settings with correlated noise
- Quadratic cobias-covariance estimation provides better stability and performance than direct bias estimation for batch selection
- Method shows particular strength in low-data regimes and when aleatoric uncertainty is correlated across points
- Eigendecomposition-based batching captures diverse points across error directions more effectively than greedy top-K selection

## Why This Works (Mechanism)

### Mechanism 1: Decomposition of Error into Reducible and Irreducible Components
The method applies standard bias-variance decomposition to active learning, separating the expected mean squared error into epistemic uncertainty (reducible), bias squared (reducible), and aleatoric uncertainty (irreducible). By targeting only the reducible terms in the acquisition function, it avoids wasting queries on regions dominated by noise.

### Mechanism 2: Difference-Based Gradient Estimation (The "Three Experiments" Logic)
The difference operator κ[g_k](x) = g_{k-1}(x) - g_k(x) is applied to PEMSE, causing the aleatoric term σ²_Y(x) to cancel out. This creates a derivative approximation that identifies points where the reducible error is collapsing fastest, effectively filtering out high-noise regions.

### Mechanism 3: Cobias-Covariance Matrix Completion for Batch Diversity
Bias is recast as a covariance relationship between points, allowing it to be estimated as a symmetric matrix completion task. The low-rank structure of the cobias matrix enables stable quadratic estimation using a symmetric neural network. Batch selection via eigendecomposition ensures diversity by picking points corresponding to principal error directions.

## Foundational Learning

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed: The entire method relies on distinguishing irreducible noise (Aleatoric) from model ignorance (Epistemic/Bias) to implement the difference mechanism
  - Quick check: If I double the training data, does the uncertainty term in question decrease? (Yes = Epistemic; No = Aleatoric)

- **Concept: Matrix Completion (Symmetric)**
  - Why needed: The method estimates the bias covariance matrix where many entries are missing, inferring them based on low-rank structure
  - Quick check: Given observed biases for x₁, x₂, how might we infer the relationship between unobserved x₃, x₄? (Answer: By learning an embedding ψ(x) that reconstructs observed interactions)

- **Concept: Eigendecomposition for Diversity**
  - Why needed: The batching mechanism relies on eigenvalues of the error matrix to ensure batch diversity
  - Quick check: Why pick points from different eigenvectors rather than top m coordinates of single largest eigenvector? (Answer: To ensure diversity; top coordinates of one vector are likely correlated/similar)

## Architecture Onboarding

- **Component map:** Base Model F_k -> Quadratic Estimator ψ -> Acquisition Engine -> Batch Selector
- **Critical path:**
  1. Train Base Model F_k on labeled data L_k
  2. Compute observed bias/cobias for labeled pairs to construct training set for ψ
  3. Train ψ to approximate full Cobias-Covariance matrix Ω_k for all x ∈ X
  4. Calculate Difference-PEMSE (requires data from round k-1)
  5. Run Eigendecomposition on predicted error reduction matrix
  6. Select top m points corresponding to largest eigenvalues

- **Design tradeoffs:**
  - Direct vs. Quadratic Estimation: Direct is simpler but error-prone when constructing batch matrix; Quadratic is more stable for batching but requires complex symmetric network
  - Batch Size vs. Stability: Larger batches allow better estimation of correlated noise but risk model collapse if eigendecomposition picks redundant modes

- **Failure signatures:**
  - Cold Start: With very few initial points (<10), quadratic estimator has insufficient data, leading to worse performance than random
  - Stochastic Drift: If Base Model F_k changes drastically between rounds due to SGD randomness, the "Difference" signal may be dominated by training noise

- **First 3 experiments:**
  1. Sanity Check (Type I): Run on noiseless function; verify Difference-PEMSE offers no benefit over PEMSE (no aleatoric noise to cancel)
  2. Batch Ablation (Type III): Compare Top-K vs Eigendecomposition selection on correlated noise problem; verify Eigen method captures diverse points
  3. Low-Data Stress Test: Initialize with 10 vs 100 points; confirm quadratic estimator struggles at 10 points but succeeds at 100

## Open Questions the Paper Calls Out

### Open Question 1
Can the cobias-covariance decomposition be generalized to classification tasks or losses based on non-symmetric Bregman divergences? The current method relies on symmetry of squared L2 norm to formulate matrix completion and eigendecomposition; non-symmetric divergences likely break the rank-1 cobias matrix structure.

### Open Question 2
Does incorporating sampling frequency weights into the bias estimator improve performance in the low-data regime? The current quadratic estimator treats all historical data points equally, potentially introducing noise from over-sampled regions.

### Open Question 3
How should the acquisition strategy be modified when the model class is intentionally simpler than the ground truth to prevent resolving indescribable behaviors? Standard active learning assumes the model class can approximate the ground truth; if constrained, reducing bias in complex regions might be counter-productive.

### Open Question 4
Is the difference-based acquisition method suboptimal in sequential (non-batched) settings due to the stochasticity of gradient descent? The method estimates gradient of error between rounds; in sequential settings, noise from SGD may obscure the true signal of bias reduction that batching helps to average out.

## Limitations
- Core assumption of zero covariance between model predictions and true labels is strong and may not hold in real-world scenarios
- Quadratic estimator performance heavily depends on having sufficient initial labeled data (fails with only 10 points)
- Method requires storing and processing full cobias matrices, which scales quadratically with labeled points

## Confidence

- **High Confidence:** Mathematical decomposition of MSE into epistemic, bias, and aleatoric components is well-established theory
- **Medium Confidence:** Cobias matrix completion approach shows strong empirical results but lacks direct theoretical guarantees
- **Low Confidence:** Generalization to high-dimensional problems beyond the 2D toy example remains untested

## Next Checks

1. **Noise-Covariance Validation:** Design experiment where aleatoric noise is intentionally correlated with model predictions to test whether difference-PEMSE still outperforms baselines

2. **Dimensionality Stress Test:** Apply method to 5-10 dimensional regression problem to assess scalability and whether cobias matrix completion still provides advantages over direct estimation

3. **Cold-Start Benchmarking:** Compare performance when initialized with 5, 10, 50, and 100 points to precisely characterize minimum viable dataset size and identify transition point where quadratic estimation becomes beneficial