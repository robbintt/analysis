---
ver: rpa2
title: RAG-based Architectures for Drug Side Effect Retrieval in LLMs
arxiv_id: '2507.13822'
source_url: https://arxiv.org/abs/2507.13822
tags:
- effect
- side
- drug
- data
- graphrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes Retrieval-Augmented Generation (RAG) and GraphRAG
  architectures to enhance drug side effect retrieval in large language models (LLMs).
  The GraphRAG approach integrates structured drug-side effect knowledge into a Llama
  3-8B model using a Neo4j graph database, while RAG leverages vector embeddings from
  the SIDER 4.1 database.
---

# RAG-based Architectures for Drug Side Effect Retrieval in LLMs

## Quick Facts
- arXiv ID: 2507.13822
- Source URL: https://arxiv.org/abs/2507.13822
- Reference count: 29
- Near-perfect accuracy (0.9999) achieved with GraphRAG for drug-side effect retrieval

## Executive Summary
This study proposes two architectures to enhance drug side effect retrieval in large language models: GraphRAG and traditional RAG. The GraphRAG approach integrates structured drug-side effect knowledge into a Llama 3-8B model using a Neo4j graph database, achieving near-perfect accuracy (0.9999) on 19,520 drug-side effect associations. Traditional RAG, leveraging vector embeddings from the SIDER 4.1 database, performed significantly better with granular data formatting (accuracy 0.998) compared to aggregated lists (accuracy 0.886). This framework offers a scalable, highly accurate solution for drug safety monitoring and pharmacovigilance applications.

## Method Summary
The study evaluated two retrieval-augmented generation architectures using the SIDER 4.1 database. GraphRAG stored drug-side effect relationships as nodes and edges in Neo4j, enabling exact matching through Cypher queries. Traditional RAG embedded individual drug-side effect pairs as text chunks using OpenAI's ada002 model, stored in Pinecone, with top-5 retrieval. Both architectures used entity recognition to extract drug and side effect terms from queries, constructed grounded prompts with retrieval results, and employed Llama 3-8B to generate binary YES/NO outputs. The evaluation set comprised 19,520 balanced pairs (10 positive, 10 negative associations per drug) covering 976 drugs and 3,851 side effects.

## Key Results
- GraphRAG achieved near-perfect accuracy of 0.9999 on binary drug-side effect association classification
- Granular data formatting (individual pairs) significantly outperformed aggregated lists in RAG (0.998 vs 0.886 accuracy)
- Standalone LLMs performed poorly at ~0.53 accuracy, highlighting the necessity of retrieval augmentation
- GraphRAG achieved F1 score of 0.9999, precision of 0.9998, sensitivity of 0.9999, and specificity of 0.9998

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based exact matching outperforms vector similarity retrieval for binary drug-side effect association queries.
- Mechanism: Drug-side effect pairs are stored as nodes connected by directed edges labeled "may_cause_side_effect" in Neo4j. Queries execute deterministic Cypher pattern matching that returns either a match or empty result—eliminating approximation inherent in vector similarity thresholds.
- Core assumption: Drug and side effect names in user queries can be reliably extracted and match canonical node identifiers in the graph.
- Evidence anchors:
  - GraphRAG achieved near-perfect accuracy (0.9999), significantly outperforming standalone LLMs and traditional RAG approaches
  - GraphRAG achieved an accuracy of 0.9999, F1 score of 0.9999, precision of 0.9998, sensitivity of 0.9999, and specificity of 0.9998
- Break condition: If entity extraction fails on synonyms, misspellings, or brand-vs-generic name variations, graph traversal returns false negatives regardless of graph completeness.

### Mechanism 2
- Claim: Granular data formatting (individual drug-side effect pairs) improves RAG retrieval precision over aggregated list formats.
- Mechanism: Data Format B encodes each association as a discrete text chunk, enabling embedding vectors to capture specific semantic relationships. Data Format A aggregates all side effects per drug into single chunks, diluting embedding specificity and increasing retrieval noise.
- Core assumption: The embedding model produces distinguishable vectors for individual drug-side effect pairs that survive top-k retrieval cutoffs.
- Evidence anchors:
  - RAG with Data Format A achieved an accuracy of 0.886 and sensitivity of 0.776, while RAG with Data Format B performed significantly better, with an accuracy of 0.998 and sensitivity of 0.999
  - Data Format B, which structures drug-side effect pairs individually, enables more precise retrieval compared to Data Format A, where side effects are aggregated into a single list per drug
- Break condition: If chunk count explodes, retrieval latency or embedding costs may become prohibitive, forcing coarser chunking.

### Mechanism 3
- Claim: Prompt engineering with retrieval-grounded context constrains LLM output to binary decisions, reducing hallucination.
- Mechanism: After retrieval, the system constructs a modified prompt stating explicitly whether the association was found and instructs the LLM to answer YES/NO based strictly on provided results. This external grounding bypasses the LLM's parametric knowledge.
- Core assumption: The LLM will follow instructions to ignore its pre-training and rely only on the provided context.
- Evidence anchors:
  - Integrating comprehensive drug side effect knowledge into a Llama 3-8B Language Model
  - Base your answer strictly on the RAG Results provided below... Do not infer or speculate beyond the information provided
- Break condition: If the retrieval result itself is incorrect, the LLM will faithfully propagate the error.

## Foundational Learning

- Concept: **Graph Databases (Neo4j/Cypher)**
  - Why needed here: GraphRAG stores drug-side effect relationships as traversable edges, requiring understanding of node-edge models and query syntax.
  - Quick check question: Can you write a Cypher query that returns all side effects connected to a drug node named "aspirin"?

- Concept: **Vector Embeddings and Similarity Search**
  - Why needed here: RAG relies on embedding queries and comparing them against stored vectors using cosine or dot-product similarity.
  - Quick check question: Explain why semantically similar queries produce vectors with smaller angular distance in embedding space.

- Concept: **Entity Recognition for Query Parsing**
  - Why needed here: Both architectures extract drug and side effect terms from natural language queries before retrieval.
  - Quick check question: Given "Does lisinopril cause dry cough?", what entities must be extracted for the retrieval step?

## Architecture Onboarding

- Component map:
  Neo4j -> Cypher query execution -> Prompt construction -> Llama 3-8B
  Pinecone -> Vector search -> Entity extraction -> Prompt construction -> Llama 3-8B

- Critical path: Query → Entity Extraction → (GraphRAG: Cypher query to Neo4j) OR (RAG: Vector search to Pinecone) → Prompt Construction → Llama 3-8B → Binary Response

- Design tradeoffs:
  - GraphRAG: Highest accuracy (0.9999) but requires exact entity matching; brittle to name variations
  - RAG Format B: High accuracy (0.998) with more tolerance for semantic matching; higher storage cost
  - RAG Format A: Lower accuracy (0.886) but simpler indexing; unsuitable for production pharmacovigilance

- Failure signatures:
  - Standalone LLM accuracy ~0.53 indicates parametric knowledge is insufficient
  - False negatives spike when entity recognition fails on synonyms/misspellings
  - False positives in RAG when top-k retrieval includes semantically similar but incorrect associations

- First 3 experiments:
  1. Replicate GraphRAG on a 100-drug subset: Load SIDER pairs into Neo4j, implement Cypher matching, measure accuracy against held-out associations.
  2. Ablate entity recognition: Replace with ground-truth entity labels to isolate retrieval vs. extraction error rates.
  3. Stress-test name variations: Query using brand names (e.g., "Tylenol" instead of "acetaminophen") and measure accuracy degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GraphRAG architecture maintain near-perfect accuracy when supporting reverse queries (identifying drugs from side effects) or class-based queries?
- Basis in paper: The authors state the framework currently "does not yet accommodate reverse queries (e.g., 'Which drugs cause hand-foot rash?') or class-based queries" which are frequently required in clinical practice.
- Why unresolved: The evaluation was restricted to predicting the presence or absence of associations for single-drug queries, leaving the system's capability to traverse the graph in reverse or by category untested.
- What evidence would resolve it: Performance metrics (Accuracy, F1) derived from a dataset specifically designed for reverse and drug class-based query evaluation.

### Open Question 2
- Question: How does the integration of noisy, unstructured real-world data (e.g., social media) impact the precision of the GraphRAG system?
- Basis in paper: The authors propose integrating "real-world, self-reported data from... Reddit, X (Twitter)" to capture emerging side effects, noting that current data relies on reported associations.
- Why unresolved: The near-perfect accuracy (0.9999) was achieved using the curated SIDER database; it is unknown if the model can maintain this performance when augmented with the linguistic ambiguity and noise of social media.
- What evidence would resolve it: A comparative study measuring accuracy and hallucination rates when the knowledge graph is augmented with social media data versus the curated baseline.

### Open Question 3
- Question: To what extent does the entity recognition module handle semantic variations, such as brand name synonyms or misspelled drug names, in user queries?
- Basis in paper: The paper notes the "potential for errors in drug name queries due to complex nomenclatures" and lists semantic search for mapping brand names and typos as a goal for future iterations.
- Why unresolved: The evaluation likely utilized standardized names from the SIDER database rather than the variable input typical of real patient queries (e.g., using "Tylenol" instead of "paracetamol").
- What evidence would resolve it: Robustness testing using a query dataset containing intentional misspellings, synonyms, and brand-name variations.

## Limitations
- The architecture assumes exact string matching between query entities and Neo4j node labels, with no handling of synonyms or misspellings
- Entity recognition modules and prompt templates remain unspecified, preventing faithful reproduction
- Evaluation dataset construction details are incomplete, raising questions about potential bias in the balanced sampling strategy

## Confidence
- **High confidence**: GraphRAG accuracy superiority over standalone LLMs (0.9999 vs ~0.53) - directly measured and reported with multiple metrics
- **Medium confidence**: Data Format B superiority over Format A in RAG (0.998 vs 0.886) - clear metrics but limited to single embedding model and chunk count
- **Low confidence**: Generalizability of near-perfect accuracy to real-world queries - no stress testing on name variations, misspellings, or edge cases reported

## Next Checks
1. Ablate entity recognition: Replace the entity extraction module with ground-truth labels and measure the isolated accuracy of GraphRAG vs RAG retrieval, revealing the contribution of extraction errors
2. Stress test name variations: Systematically query using brand names, common misspellings, and generic alternatives to measure accuracy degradation and identify brittleness thresholds
3. Measure retrieval completeness: For RAG Format B, log whether the correct drug appears in top-5 results before LLM filtering, determining if false positives stem from retrieval noise or LLM hallucinations