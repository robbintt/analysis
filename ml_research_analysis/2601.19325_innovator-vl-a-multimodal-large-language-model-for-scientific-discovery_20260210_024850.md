---
ver: rpa2
title: 'Innovator-VL: A Multimodal Large Language Model for Scientific Discovery'
arxiv_id: '2601.19325'
source_url: https://arxiv.org/abs/2601.19325
tags:
- reasoning
- question
- multimodal
- scientific
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Innovator-VL is a scientific multimodal large language model that
  achieves strong performance across diverse scientific domains while maintaining
  excellent general vision capabilities through a fully transparent and reproducible
  training pipeline. It uses principled data selection rather than massive pretraining,
  achieving competitive results with fewer than five million curated samples.
---

# Innovator-VL: A Multimodal Large Language Model for Scientific Discovery

## Quick Facts
- arXiv ID: 2601.19325
- Source URL: https://arxiv.org/abs/2601.19325
- Authors: Zichen Wen, Boxue Yang, Shuang Chen, Yaojie Zhang, Yuhang Han, Junlong Ke, Cong Wang, Yicheng Fu, Jiawang Zhao, Jiangchao Yao, Xi Fang, Zhen Wang, Henxing Cai, Lin Yao, Zhifeng Gao, Yanhui Hong, Nang Yuan, Yixuan Li, Guojiang Zhao, Haoyi Tao, Nan Wang, Han Lyu, Guolin Ke, Ning Liao, Xiaoxing Wang, Kai Chen, Zhiyu Li, Feiyu Xiong, Sihan Hu, Kun Chen, Yanfeng Wang, Weinan E, Linfeng Zhang, Linfeng Zhang
- Reference count: 40
- Key outcome: Achieves strong performance across diverse scientific domains while maintaining excellent general vision capabilities through a fully transparent and reproducible training pipeline using fewer than five million curated samples

## Executive Summary
Innovator-VL is a scientific multimodal large language model that achieves strong performance across diverse scientific domains while maintaining excellent general vision capabilities. The model uses principled data selection rather than massive pretraining, achieving competitive results with fewer than five million curated samples. Through a four-stage training pipeline (projector alignment, mid-training, SFT, and RL), Innovator-VL demonstrates strong generalization across general vision, multimodal reasoning, and scientific benchmarks, with superior token efficiency in reasoning tasks compared to peer models.

## Method Summary
Innovator-VL uses a four-stage training pipeline starting from Qwen3-8B-Base LLM and RICE-ViT vision encoder. The approach employs principled data selection with fewer than five million curated scientific samples rather than massive domain-specific pretraining. Key innovations include region-aware visual encoding (RICE-ViT) for fine-grained scientific imagery, sequence-level reinforcement learning with GSPO algorithm, and hierarchical rewards (format + accuracy with α=0.1 weighting). The training data includes general vision (22M), Chain-of-Thought reasoning (15M), and scientific domains (OCSR, reactions, EM characterization), with discrepancy-driven RL sample selection targeting samples where Pass@N significantly exceeds Pass@1.

## Key Results
- Achieves competitive performance on various scientific tasks using fewer than five million carefully curated scientific training samples
- Demonstrates strong generalization across general vision, multimodal reasoning, and scientific benchmarks
- Shows 1.4× to 2× higher accuracy-to-token ratio than peer models in reasoning tasks
- Maintains excellent general vision capabilities while adding scientific domain expertise

## Why This Works (Mechanism)

### Mechanism 1
Principled data selection with fewer than 5M curated scientific samples achieves competitive scientific reasoning performance without massive domain-specific pretraining. The approach uses human-in-the-loop synthetic pipelines and discrepancy-driven RL sample selection targeting samples where Pass@N significantly exceeds Pass@1, concentrating training on samples where the model has latent capability but unreliable policy. The base LLM (Qwen3-8B-Base) is assumed to have already absorbed substantial scientific knowledge during general pretraining, so multimodal training need only align visual representations and elicit reasoning rather than inject domain knowledge.

### Mechanism 2
Region-aware visual encoding (RICE-ViT) improves scientific multimodal understanding by capturing fine-grained, localized visual structures common in scientific imagery. RICE-ViT integrates region-level semantics through specialized Region Transformer layers, jointly optimizing object and OCR region representations. This decomposes scientific images into semantically coherent visual units, reducing the reasoning burden on the language model.

### Mechanism 3
Sequence-level reinforcement learning (GSPO) with hierarchical rewards improves reasoning accuracy while reducing token consumption. GSPO aligns optimization units with reward units by performing importance sampling and clipping at the sequence level using length-normalized sequence likelihood. The hierarchical reward system (format + accuracy with α=0.1) enforces structural discipline while prioritizing correctness through cascaded verification.

## Foundational Learning

- **Vision-Language Projector Design**
  - Why needed here: PatchMerger compresses visual tokens to balance computational efficiency with representational fidelity, critical when training on limited scientific data
  - Quick check question: Can you explain why token compression matters more for scientific MLLMs than general-purpose MLLMs?

- **Discrepancy-Driven Data Selection (Pass@N vs Pass@1)**
  - Why needed here: The RL data curation strategy explicitly targets samples where the model has capability but unreliable execution, maximizing training efficiency
  - Quick check question: Given a sample with Pass@8=0.9 and Pass@1=0.2, should it be included in RL training? What if Pass@8=0.3?

- **Length-Normalized Sequence Likelihood**
  - Why needed here: GSPO uses this to ensure fair importance ratios across responses of varying lengths, preventing bias toward shorter or longer outputs
  - Quick check question: Why does token-level importance weighting introduce instability for sequence-level rewards?

## Architecture Onboarding

- Component map: Input Images → RICE-ViT (variable-length visual tokens) → PatchMerger (compression) → Text Tokens + Compressed Visual Tokens → Qwen3-8B-Base → Response

- Critical path:
  1. Pre-training: LLaVA-1.5 558K (projector only) → Innovator-VL-Mid-Training-85M (full parameters)
  2. SFT: Innovator-VL-Instruct-46M (general + CoT + scientific data, full parameters)
  3. RL: Innovator-VL-RL-172K with GSPO (full parameters)

- Design tradeoffs:
  - No scientific text pretraining on LLM: Trades potential domain knowledge injection for avoiding bias/overfitting with low-quality data
  - RICE-ViT over CLIP/SigLIP: Trades global representation strength for region-level granularity
  - GSPO over GRPO: Trades implementation complexity for training stability on long-context reasoning

- Failure signatures:
  - If scientific benchmarks show >20% gap vs. general models, check: (1) data quality in scientific SFT subsets, (2) whether base LLM has sufficient domain knowledge
  - If reasoning chains are verbose without accuracy gains, check: (1) format reward weight α may be too low, (2) reward verification pipeline accuracy
  - If training diverges during RL, check: (1) sequence-level clipping threshold ε, (2) reward normalization

- First 3 experiments:
  1. Ablate vision encoder: Swap RICE-ViT for SigLIP on a subset of scientific benchmarks (MolParse, EMVista) to quantify region-aware encoding contribution
  2. Data efficiency sweep: Train with 1M, 2.5M, 5M scientific samples to validate the scaling curve and identify minimum viable data threshold
  3. Token efficiency analysis: Compare reasoning trace lengths and accuracy between GSPO and GRPO on MathVision-mini to isolate the algorithm's contribution from data effects

## Open Questions the Paper Calls Out

### Open Question 1
Would continued scientific-domain pretraining of the language backbone (Qwen3-8B-Base) improve Innovator-VL's scientific performance beyond the current data-efficient SFT+RL approach, or does it risk degrading general capabilities? The paper deliberately avoided scientific pretraining to preserve general capabilities, but an internal variant based on scientifically-pretrained Qwen3-8B-Base exists, suggesting this remains an open empirical question.

### Open Question 2
What is the minimum scale of curated scientific training data required to achieve competitive scientific reasoning performance, and how does performance scale below the 5 million sample threshold? The paper demonstrates feasibility at ~5M samples but does not systematically probe the lower bound of data requirements or the shape of the scaling curve.

### Open Question 3
Does the superior token efficiency observed in Innovator-VL-8B-Thinking stem primarily from the GSPO optimization algorithm, the hierarchical reward system design, or the discrepancy-driven data selection strategy? The paper attributes efficiency gains to "reinforcement learning stage that explicitly optimizes for both correctness and conciseness," but three distinct components are introduced simultaneously without ablation.

### Open Question 4
Can Innovator-VL's reasoning capabilities effectively integrate with external scientific tools (computational chemistry packages, simulation engines, lab automation systems) while maintaining its token-efficient reasoning patterns? Current evaluations focus on static benchmark performance; tool integration requires different capabilities not assessed in the paper.

## Limitations

- The core data efficiency claim rests on the assumption that Qwen3-8B-Base has absorbed substantial scientific knowledge during pretraining, which is not directly validated
- RICE-ViT's contribution to scientific understanding is claimed but not directly compared against standard vision encoders on scientific benchmarks
- GSPO's sequence-level advantages over GRPO are not benchmarked head-to-head on the same tasks

## Confidence

- High confidence: Data efficiency claims (5M samples vs massive pretraining), strong general vision performance, reproducible training pipeline
- Medium confidence: RICE-ViT contribution to scientific understanding, GSPO algorithm advantages over alternatives
- Low confidence: Base LLM pretraining assumption, cross-domain generalization beyond reported benchmarks

## Next Checks

1. **Base Knowledge Validation**: Train a baseline model with 5M random samples (no scientific pretraining assumption) and compare scientific benchmark performance to quantify the pretraining knowledge contribution.

2. **Vision Encoder Ablation**: Systematically replace RICE-ViT with CLIP/SigLIP on scientific benchmarks (MolParse, EMVista) while controlling for all other variables to isolate encoder contribution.

3. **RL Algorithm Head-to-Head**: Run GSPO and GRPO on identical scientific reasoning tasks with same data to quantify the sequence-level advantage claim.