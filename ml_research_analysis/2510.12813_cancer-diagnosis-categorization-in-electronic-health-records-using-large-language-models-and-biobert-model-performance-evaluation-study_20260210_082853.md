---
ver: rpa2
title: 'Cancer Diagnosis Categorization in Electronic Health Records Using Large Language
  Models and BioBERT: Model Performance Evaluation Study'
arxiv_id: '2510.12813'
source_url: https://arxiv.org/abs/2510.12813
tags:
- cancer
- data
- clinical
- biobert
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated four large language models (GPT-3.5, GPT-4o,
  Llama 3.2, Gemini 1.5) and BioBERT for classifying cancer diagnoses from EHR data.
  Using 762 diagnoses from 3456 patients, models categorized structured ICD codes
  and unstructured free-text entries into 14 cancer types.
---

# Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study

## Quick Facts
- **arXiv ID:** 2510.12813
- **Source URL:** https://arxiv.org/abs/2510.12813
- **Reference count:** 18
- **Primary result:** BioBERT achieved 90.8% accuracy for structured ICD codes while GPT-4o matched this for ICDs and reached 81.9% accuracy for free-text cancer diagnosis classification

## Executive Summary
This study evaluates four large language models (GPT-3.5, GPT-4o, Llama 3.2, Gemini 1.5) alongside BioBERT for classifying cancer diagnoses from EHR data. Using 762 diagnoses from 3456 patients, models categorized structured ICD codes and unstructured free-text entries into 14 cancer types. BioBERT demonstrated superior performance on structured data with 90.8% accuracy and 84.2% weighted F1-score, while GPT-4o matched BioBERT's ICD accuracy and showed strong performance on free-text classification. The research identifies persistent challenges with classifying benign tumors, metastasis, and ambiguous clinical descriptions, suggesting the need for human oversight in clinical applications.

## Method Summary
The study employed a retrospective cohort design using the MIMIC-IV database to extract cancer diagnoses from 3456 patients. Four LLMs (GPT-3.5, GPT-4o, Llama 3.2, Gemini 1.5) and BioBERT were evaluated for categorizing diagnoses into 14 cancer types. The dataset included both structured ICD-10 codes and unstructured free-text entries. Models were prompted with patient diagnosis information and asked to classify into predefined cancer categories or mark as "Unknown." Performance was assessed using accuracy, precision, recall, and F1-scores for both structured and unstructured data types. Error analysis was conducted to identify systematic misclassification patterns, particularly for challenging cases like metastasis and benign tumors.

## Key Results
- BioBERT achieved highest accuracy (90.8%) and weighted F1-score (84.2%) for structured ICD code classification
- GPT-4o matched BioBERT's ICD accuracy (90.8%) while achieving 81.9% accuracy and 71.8% F1-score for free-text classification
- All models struggled with classifying metastasis and benign tumors, often misclassifying metastasis as primary cancers
- Models frequently force-classified vague free-text entries rather than correctly assigning "Unknown" category

## Why This Works (Mechanism)
Large language models demonstrate strong performance in cancer diagnosis categorization due to their ability to capture semantic relationships in clinical text and understand hierarchical coding structures. BioBERT's architecture, pre-trained on biomedical literature, provides superior understanding of medical terminology and coding conventions. The transformer architecture enables these models to handle both structured codes and unstructured text by learning contextual relationships between diagnosis descriptions and cancer categories. Performance differences between models reflect variations in training data, fine-tuning approaches, and architectural design choices specific to medical domain understanding.

## Foundational Learning
- **ICD-10 coding system** - why needed: Standardized classification system for diseases and health conditions; quick check: Verify models correctly map ICD codes to 14 cancer categories
- **Transformer architecture** - why needed: Enables contextual understanding of medical text; quick check: Confirm models process both structured and unstructured input effectively
- **F1-score metric** - why needed: Balances precision and recall for imbalanced classification tasks; quick check: Validate weighted F1-scores reflect true model performance
- **Multi-class classification** - why needed: Required for categorizing into 14 distinct cancer types; quick check: Ensure models maintain performance across all cancer categories
- **Error analysis methodology** - why needed: Identifies systematic misclassification patterns; quick check: Review error patterns for clinical significance
- **EHR data extraction** - why needed: Provides real-world clinical data for model evaluation; quick check: Confirm data extraction captures complete diagnostic information

## Architecture Onboarding

**Component Map:** EHR Data -> Preprocessing -> Model Input -> Classification Model -> Output Category -> Performance Evaluation

**Critical Path:** Data extraction and preprocessing form the foundation, with model selection and prompt engineering as critical decision points affecting downstream classification accuracy and clinical applicability.

**Design Tradeoffs:** BioBERT prioritizes structured data accuracy over free-text flexibility, while GPT-4o balances both but requires more computational resources. Single-institution data limits generalizability but ensures consistency in documentation standards.

**Failure Signatures:** Common failures include misclassification of metastasis as primary tumors, force-classification of vague entries, and systematic confusion between benign and malignant conditions. Performance degradation occurs with ambiguous clinical descriptions and incomplete documentation.

**First 3 Experiments:**
1. Stratified performance evaluation by cancer type to identify category-specific accuracy patterns
2. Confidence threshold optimization to minimize force-classification of ambiguous cases
3. Comparative analysis of structured vs unstructured data performance to determine optimal use cases

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** To what extent do performance metrics for GPT-4o and BioBERT degrade or improve when applied to multi-institutional datasets with varying EHR platforms, coding practices, and clinical documentation styles?
- **Basis in paper:** The authors state that because the dataset was drawn from a single institution, "model performance may vary across health care systems... Future work should validate these models using data from multiple institutions."
- **Why unresolved:** The current study is limited to a single radiation oncology department, making it unclear if the high performance on structured data translates to settings with different documentation norms.
- **What evidence would resolve it:** A comparative study replicating the methodology across at least three diverse healthcare systems to benchmark accuracy variance.

### Open Question 2
- **Question:** Can specific fine-tuning strategies or enhanced prompt engineering effectively resolve the persistent misclassification patterns observed between metastasis and central nervous system (CNS) tumors?
- **Basis in paper:** The error analysis revealed that "metastasis presents an even greater challenge," often being miscategorized as CNS or lung cancers, a difficulty arising from the clinical decision to treat metastasis as a distinct category regardless of body location.
- **Why unresolved:** The paper identifies this as a common error pattern but does not test interventions (like few-shot prompting with metastasis examples) to correct this specific semantic confusion.
- **What evidence would resolve it:** An ablation study testing domain-specific fine-tuning or modified prompts specifically designed to disentangle primary site descriptions from metastatic indicators.

### Open Question 3
- **Question:** What specific confidence thresholds or preprocessing heuristics are required to prevent models from force-classifying vague free-text entries into specific cancer categories rather than "Unknown"?
- **Basis in paper:** The authors note that models often "force-classify vague or incomplete clinical descriptions... rather than correctly assign them as Unknown," suggesting a need for the "confidence-based filtering" mentioned in the conclusion.
- **Why unresolved:** The study evaluates raw model performance but does not define or test the specific statistical thresholds needed to filter out these ambiguous inputs safely.
- **What evidence would resolve it:** ROC analysis determining the optimal probability cutoff for the "Unknown" category that minimizes false positives without sacrificing recall.

### Open Question 4
- **Question:** How does diagnostic classification performance vary across different demographic groups given the potential for LLMs to reflect biases present in their training data?
- **Basis in paper:** The discussion notes that "LLMs can reflect biases in their training data, leading to uneven performance across demographic groups," and explicitly states these models "should be validated on diverse populations."
- **Why unresolved:** The current analysis aggregates performance across the dataset without stratifying results by race, age, or socioeconomic status, leaving equity concerns unaddressed.
- **What evidence would resolve it:** Stratified performance metrics (Accuracy, F1-score) across key demographic subgroups within the existing dataset to detect performance disparities.

## Limitations
- Small sample size of 762 diagnoses may limit generalizability to broader clinical populations
- Single-institution dataset (MIMIC-IV) restricts external validity across diverse healthcare settings
- Focus on classification accuracy without evaluation of downstream clinical impact or implementation feasibility
- Persistent challenges with clinically critical cases including metastasis detection and benign/malignant differentiation

## Confidence
- BioBERT ICD code classification results: **High** - supported by robust evaluation metrics and consistent with existing literature on transformer-based models for structured medical data
- GPT-4o free-text classification performance: **Medium** - while metrics are strong, limited comparison with other LLM approaches and potential variability in free-text quality across healthcare settings
- Clinical applicability conclusions: **Low** - current evidence focuses on classification accuracy without addressing implementation barriers, workflow integration, or real-world validation

## Next Checks
1. External validation on multi-institutional datasets with diverse patient populations and varying documentation practices to assess model generalizability
2. Prospective clinical study measuring diagnostic accuracy impact on actual clinical decision-making and patient outcomes
3. Error analysis focused on clinically critical misclassifications (malignant vs benign, metastasis detection) to quantify potential patient safety implications