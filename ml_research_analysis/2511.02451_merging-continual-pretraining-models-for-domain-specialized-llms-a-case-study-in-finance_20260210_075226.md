---
ver: rpa2
title: 'Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study
  in Finance'
arxiv_id: '2511.02451'
source_url: https://arxiv.org/abs/2511.02451
tags:
- merging
- million
- knowledge
- financial
- other
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building specialized large
  language models for finance by proposing to merge existing continual pre-training
  (CPT) models specialized in finance, math, and Japanese. Instead of costly and unstable
  multi-skill training, the authors apply model merging techniques (Task Arithmetic,
  TIES, DARE-TIES) to combine CPT experts with a base model and with each other.
---

# Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance

## Quick Facts
- **arXiv ID:** 2511.02451
- **Source URL:** https://arxiv.org/abs/2511.02451
- **Reference count:** 0
- **Primary result:** Model merging can recover general knowledge lost during continual pretraining and create emergent cross-domain capabilities in finance LLMs.

## Executive Summary
This paper proposes a three-stage model merging framework to build specialized large language models for finance by combining existing continual pre-training (CPT) models. Rather than costly retraining, the authors apply Task Arithmetic, TIES, and DARE-TIES to merge CPT experts (finance, math, Japanese) with a base model and with each other. They introduce a comprehensive evaluation framework across 18 financial NLP tasks from 8 datasets, measuring knowledge recovery, complementarity, and emergence. Results show that merging CPT models with their base recovers lost general capabilities, while merging different CPT models can produce emergent reasoning skills that exceed individual constituent performance.

## Method Summary
The approach uses three CPT experts (finance, math, Japanese) and a base Llama-3-8B model, merging them through hierarchical 3-stage framework. Stage 1 merges each CPT with base to recover general knowledge using Task Arithmetic (λ sweep) and TIES (density d sweep). Stage 2 merges the best Stage 1 models to test complementarity. Stage 3 attempts tri-CPT merging to test integration limits. Evaluation uses 18 tasks from 8 datasets measuring Macro-Gain (improvement over constituent average) and Outperform Gap (improvement over best constituent).

## Key Results
- Merging CPT models with their base recovers general capabilities lost during continual pretraining
- Merging complementary domain experts (finance + math) produces emergent reasoning capabilities exceeding both constituents
- Task Arithmetic achieves highest gains but requires careful hyperparameter tuning; TIES offers more robust performance
- Tri-CPT merging introduces destructive interference and degrades performance relative to dual-CPT configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Merging a CPT model with its base model recovers general capabilities lost during continual pretraining (catastrophic forgetting).
- **Mechanism:** Task Arithmetic computes task vector τ = θ_CPT − θ_base and adds scaled version back: θ_merged = θ_base + λ·τ. This linear recombination reintroduces weights associated with general knowledge that shifted during CPT.
- **Core assumption:** Knowledge lost during CPT is displaced in weight space and can be linearly restored.
- **Evidence anchors:**
  - [abstract]: "merging an expert with its base model recovers general knowledge lost during CPT"
  - [section 4.1.1]: "all merged models exhibit positive Macro-Gain... BFTA and BMTA... achieves a positive Macro-OG"
  - [corpus]: "Improper corpus-mixture ratios limit effectiveness" in CPT approaches (arXiv:2505.12043) supports that CPT degrades general skills
- **Break condition:** When λ > 0.8, domain knowledge dominates and general recovery plateaus; when λ < 0.2, insufficient recovery occurs.

### Mechanism 2
- **Claim:** Merging complementary domain experts (e.g., finance + math) can produce emergent capabilities exceeding both constituents on complex reasoning tasks.
- **Mechanism:** TIES-Merging retains only top-d% magnitude weights per task vector and resolves sign conflicts via majority voting, combining only mutually consistent parameter updates.
- **Core assumption:** Complementary skills reside in partially overlapping parameter regions; sign-consistent regions encode compatible knowledge.
- **Evidence anchors:**
  - [abstract]: "merging experts improves performance and can yield emergent cross-domain skills"
  - [section 4.1.2]: "FMTA and FMTI achieved a Macro-OG exceeding 2... combination of financial knowledge and mathematical reasoning may have interacted complementarily"
  - [section 4.3.1]: Merged model FM solved CS-CoT task that BF and BM both failed individually
  - [corpus]: Related work on model merging (arXiv:2408.07666) documents emergent behaviors from weight composition
- **Break condition:** When models have low cosine similarity (<0.978) or high L2 distance (>120), merged performance collapses; when domains are too similar, gains diminish.

### Mechanism 3
- **Claim:** Tri-CPT merging introduces more interference than benefit, with performance declining relative to dual-CPT configurations.
- **Mechanism:** Each additional CPT model increases parameter conflict probability. DARE-TIES applies stochastic dropout to task vectors before TIES merging, but cannot fully resolve accumulated interference from three divergent CPT trajectories.
- **Core assumption:** Interference grows non-linearly with number of merged experts; parameter space has limited capacity for incompatible updates.
- **Evidence anchors:**
  - [abstract]: "merging experts improves performance" but scaling limits not claimed
  - [section 4.1.3]: "all tri-merged models show a decline in Overall performance, and Oracle Retention falls below 1.0 in most cases"
  - [section 5]: "adding a third may lead to diminishing returns"
  - [corpus]: ADEPT (arXiv:2510.10071) addresses similar capacity limits via explicit parameter expansion
- **Break condition:** When three or more CPT models with divergent training objectives are merged without architectural modifications, interference dominates.

## Foundational Learning

- **Concept: Task Vectors**
  - **Why needed here:** All three merging methods operate on task vectors (θ_CPT − θ_base), not raw weights. Understanding this abstraction is essential for interpreting merge equations.
  - **Quick check question:** Given base weights [1.0, 2.0, 3.0] and CPT weights [1.1, 1.8, 3.3], what is the task vector?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper frames base+CPT merging as a forgetting recovery technique. You must understand that CPT trades general capability for domain specialization.
  - **Quick check question:** Why would a model trained on financial documents perform worse on general NLP benchmarks?

- **Concept: Sign Conflict Resolution**
  - **Why needed here:** TIES-Merging explicitly filters parameters where task vectors have opposing signs. This is the core mechanism distinguishing TIES from simple averaging.
  - **Quick check question:** If τ_finance[p] = +0.5 and τ_math[p] = −0.3 for parameter p, what does TIES do with this parameter?

## Architecture Onboarding

- **Component map:**
  Base Model (Llama-3-8B) → CPT-Finance → Task Vector τ_f
  ├──> CPT-Math → Task Vector τ_m
  └──> CPT-Japanese → Task Vector τ_j

- **Critical path:**
  1. Compute L2 distance and cosine similarity between all CPT models and base (Table 2)
  2. Run Stage 1 sweeps for each method (λ ∈ {0.1–0.9} for TA, d ∈ {0.1–0.9} for TIES/DARE)
  3. Select best hyperparameters per domain via Macro-OG maximization (Eq. 2)
  4. Merge top-2 Stage 1 models for Stage 2
  5. Evaluate on 18-task financial benchmark (Table 3)

- **Design tradeoffs:**
  - **Task Arithmetic:** Highest peak performance, lowest robustness. Use when you can afford extensive hyperparameter tuning.
  - **TIES:** Lower peak but stable across settings. Use as default for production.
  - **DARE-TIES:** Underperformed in this study (Figure 7). Not recommended for CPT merging despite SFT success.
  - **Model selection:** Finance+Math showed synergy; Finance+Japanese showed cross-lingual spillover. Math+Japanese showed destructive interference on linguistic tasks (Region B, Figure 2).

- **Failure signatures:**
  - Macro-Gain positive but Macro-OG negative → recovery without emergence (knowledge restored but not enhanced)
  - Oracle Retention < 1.0 → merged model underperforms best constituent on aggregate
  - Large L2 distance (>120) or low cosine (<0.978) between models → high interference risk
  - Tri-merge with Macro-OG < −5 → capacity exceeded, rollback to dual-merge

- **First 3 experiments:**
  1. **Reproduce Stage 1 with your own CPT model:** Merge your domain-specific CPT checkpoint with base using TA (λ=0.5) and TIES (d=0.3). Compare Macro-Gain on general benchmarks vs. domain benchmarks.
  2. **Ablate task vector components:** Randomly zero out 50% of τ values before merging to verify that gains come from structured knowledge, not noise.
  3. **Test cross-domain pair:** If you have two CPT models (e.g., legal + medical), compute L2/cosine similarity first. If cosine > 0.98, attempt Stage 2 merge with TIES (d=0.2). Report Macro-OG to detect emergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors beyond parameter-space similarity determine whether merged CPT models will exhibit emergent capabilities (positive Outperform Gap)?
- Basis in paper: [explicit] The authors state in Section 4.5 and the Conclusion that while model similarity correlates with general gains (Macro-Gain), the emergence of new skills depends on "more complex factors" and cannot be inferred from distance metrics alone.
- Why unresolved: The paper’s analysis showed no significant monotonic relationship between L2 distance/cosine similarity and Macro-OG (emergence), leaving the causal mechanisms for emergent reasoning unidentified.
- What evidence would resolve it: Identification of functional metrics (e.g., gradient alignment, representation geometry, or data distribution overlap) that statistically correlate with or predict positive Outperform Gap results in merged models.

### Open Question 2
- Question: Can alternative merging architectures (e.g., MoE or non-linear routing) overcome the destructive interference observed when integrating three or more CPT experts?
- Basis in paper: [explicit] Section 4.1.3 notes that tri-domain merging resulted in performance decline and Oracle Retention below 1.0, leading the authors to conclude that "integrating three specialized CPTs introduces significant interference" and highlighting "integration limits."
- Why unresolved: The study tested linear/sparsification methods (TA, TIES, DARE), which failed to scale to three domains, suggesting that simple weight composition is insufficient for high-capacity integration.
- What evidence would resolve it: Demonstration of a merging strategy that successfully integrates three or more distinct CPT models while maintaining or exceeding the Oracle Retention of the best dual-merged model.

### Open Question 3
- Question: To what extent does the specialization gained via CPT merging come at the cost of general-purpose language capabilities?
- Basis in paper: [explicit] The Limitations section states that the evaluation was "confined to finance-specific tasks, leaving the impact of merging on general-purpose language abilities unevaluated."
- Why unresolved: While the paper demonstrates that merging with the base model recovers general knowledge (Stage 1), it does not measure if the resulting specialized models lose proficiency in unrelated domains (e.g., world knowledge, generic reasoning).
- What evidence would resolve it: Evaluation of the merged financial experts on standard general-purpose benchmarks (e.g., MMLU, HellaSwag) to quantify any regression in generic capability compared to the base model.

### Open Question 4
- Question: Do the trade-offs between Task Arithmetic (high gain, high sensitivity) and TIES (robustness) persist across different model scales and architectures?
- Basis in paper: [explicit] The Limitations section notes the analysis was restricted to Llama-3-8B and asks whether the findings "extend to larger-scale models from diverse architectures" given that CPT models exhibit greater parameter divergence.
- Why unresolved: It is unclear if the hyperparameter sensitivity of Task Arithmetic or the robustness of TIES is an artifact of the 8B parameter scale or the specific Llama 3 architecture.
- What evidence would resolve it: Replication of the three-stage framework using larger models (e.g., 70B+) or different families (e.g., Mistral, Gemma) to compare the stability and performance distributions of the merging methods.

## Limitations

- Study evaluates only three CPT models (Finance, Math, Japanese) with single base architecture (Llama-3-8B), limiting generalization to other domain combinations
- Does not explore whether observed emergence is due to genuine cross-domain synergy or variance in task difficulty and evaluation sensitivity
- Hyperparameter sensitivity of Task Arithmetic creates practical deployment barriers without providing automated tuning guidance

## Confidence

- **High Confidence:** Claims about catastrophic forgetting during CPT and recovery via base model merging. Well-supported by direct empirical evidence across all methods and confirmed by Macro-Gain metric improvements in Stage 1.
- **Medium Confidence:** Claims about complementary skill emergence from merging distinct domain experts (e.g., Finance+Math). Supported by Macro-OG improvements and specific task successes like CS-CoT, but limited to three domain pairs and one base architecture.
- **Low Confidence:** Claims about tri-CPT merging consistently degrading performance and recommendations to stop at dual-CPT. Based on single experimental setup without exploring alternative architectural modifications or more sophisticated interference mitigation techniques.

## Next Checks

1. **Cross-Domain Generalization Test:** Apply the three-stage merging framework to a new domain pair (e.g., Legal+Medical) with a different base model (e.g., Qwen-7B). Compute L2/cosine similarity first, then evaluate Macro-OG on domain-specific benchmarks to verify whether emergence patterns replicate beyond the original Finance/Math/Japanese setup.

2. **Hyperparameter Robustness Sweep:** For each merging method, conduct a wider λ/d sweep (0.05 increments) across all six CPT pairs. Measure variance in Macro-OG and identify whether stable performance regions exist that contradict the paper's claim that TA requires precise tuning.

3. **Interference Analysis:** For each merged model, compute parameter-wise variance across task vectors and correlate with performance drops. Test whether masking high-variance parameters before merging improves Oracle Retention, particularly for tri-CPT configurations where interference was reported as problematic.