---
ver: rpa2
title: 'AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian
  Tribal Languages'
arxiv_id: '2512.04765'
source_url: https://arxiv.org/abs/2512.04765
tags:
- languages
- tribal
- translation
- language
- adibhashaa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AdiBhashaa, a community-driven initiative
  to create the first open parallel corpora and baseline MT systems for four major
  Indian tribal languages: Bhili, Mundari, Gondi, and Santali. The authors combine
  participatory data creation with native speakers, human-in-the-loop validation,
  and systematic evaluation of both encoder-decoder MT models and large language models.'
---

# AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages

## Quick Facts
- **arXiv ID:** 2512.04765
- **Source URL:** https://arxiv.org/abs/2512.04765
- **Reference count:** 2
- **Primary result:** First open parallel corpora and baseline MT systems for Bhili, Mundari, Gondi, and Santali

## Executive Summary
This paper introduces AdiBhashaa, a community-driven initiative to create the first open parallel corpora and baseline machine translation systems for four major Indian tribal languages: Bhili, Mundari, Gondi, and Santali. The authors combine participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. Their participatory workflow involves native-speaking translators and independent validators to ensure linguistic quality and cultural fidelity. They establish baseline systems using fine-tuned multilingual encoder-decoder architectures like NLLB-200, mT5, and IndicTrans2, and evaluate them against LLMs in zero-shot and few-shot settings. Fine-tuned multilingual MT models substantially outperform zero-shot performance for all four languages. Translation into high-resource languages (tribal→Hindi/English) systematically outperforms translation into the tribal languages, reflecting difficulties in generating morphologically rich, low-frequency vocabulary. Few-shot LLMs are competitive but generally underperform dedicated fine-tuned MT models, especially for Santali in Ol Chiki script. AdiBhashaa serves as both a practical resource for MT research and a template for similar initiatives targeting other under-represented languages.

## Method Summary
The paper creates a 80,000-sentence parallel corpus (20,000 per language) through a three-stage participatory workflow: source curation, community translation, and independent validation. Native-speaking translators and validators ensure linguistic quality and cultural fidelity. The authors fine-tune multilingual encoder-decoder models (NLLB-200, mT5, IndicTrans2) on 95% of the parallel data and evaluate them using chrF++ and sentence-level BLEU metrics, supplemented by human evaluation. They also evaluate LLMs (Bloom, Gemini 2.5 Flash, GPT-4o-mini) in zero-shot and few-shot settings for comparison.

## Key Results
- Fine-tuned multilingual MT models outperform zero-shot baselines with substantial gains across all four tribal languages
- Translation into high-resource languages (tribal→Hindi/English) is more accurate than reverse direction
- Few-shot LLMs are competitive but generally underperform fine-tuned MT models, especially for Santali in Ol Chiki script

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual models on community-curated parallel data yields substantial gains over zero-shot baselines for unseen tribal languages.
- Mechanism: Multilingual encoder-decoder models (NLLB-200, mT5, IndicTrans2) transfer cross-lingual representations from high-resource languages present during pretraining. Fine-tuning on even ~19,000 sentence pairs per language provides sufficient gradient signal to adapt these representations to new language-specific token distributions, likely leveraging shared script properties (Devanagari) or phonological patterns from related language families.
- Core assumption: The pretrained multilingual representations contain transferable sublexical or structural features that can be repurposed for unseen languages with similar orthographic or typological characteristics.
- Evidence anchors:
  - [abstract] "Results show that fine-tuned multilingual MT models outperform zero-shot baselines, with notable gains across all four languages."
  - [section: Quantitative Results] "Fine-tuning multilingual encoder–decoder models on the AdiBhashaa corpus yields consistent, substantial gains over zero-shot performance for all four languages, showing that a carefully curated, community-produced dataset of modest size provides sufficient signal for models to learn previously unseen languages."
  - [corpus] Neighbor paper on Bhili-Hindi-English Parallel Corpus (arxiv:2511.00486) corroborates that low-resource tribal languages benefit from dedicated parallel corpora construction.
- Break condition: If target language has no script overlap or typological similarity with pretraining languages, or if data quality is compromised (e.g., noisy alignments, translation drift), gains may diminish substantially.

### Mechanism 2
- Claim: Translation into high-resource languages (tribal→Hindi/English) systematically outperforms translation into tribal languages.
- Mechanism: High-resource target languages have richer vocabulary coverage and more stable token embeddings in the model's output layer. Generating morphologically complex, low-frequency tribal-language vocabulary requires the model to produce tokens poorly represented in pretraining, increasing exposure bias and decoding uncertainty.
- Core assumption: The asymmetry is primarily a target-side generation problem, not source-side comprehension.
- Evidence anchors:
  - [abstract] "Translation into high-resource languages (tribal→Hindi/English) is more accurate than the reverse direction."
  - [section: Quantitative Results] "This reflects known difficulties in generating morphologically rich, low-frequency vocabulary on the target side and underscores the need to expand both the volume and domain coverage of tribal-language data."
  - [corpus] No direct corpus evidence for this specific tribal-language asymmetry; this remains an inferred pattern from general MT literature.
- Break condition: If tribal-language vocabulary were added to the model vocabulary and sufficiently pretrained, or if larger monolingual tribal corpora were used for target-side language modeling, the asymmetry may reduce.

### Mechanism 3
- Claim: Few-shot LLMs are competitive but underperform fine-tuned MT models, especially for scripts with minimal pretraining representation (Ol Chiki).
- Mechanism: LLMs can leverage in-context learning to approximate translation from prompt exemplars, but without gradient updates, they cannot adapt token embeddings or output distributions for unseen scripts. Ol Chiki, being both low-resource and script-distinct from Devanagari/Latin, lacks sufficient character-level or subword-level support in tokenizer vocabularies.
- Core assumption: The performance gap is driven by tokenizer coverage and lack of gradient-based adaptation, not model capacity.
- Evidence anchors:
  - [abstract] "Few-shot LLMs are competitive but generally lag behind fine-tuned MT models, especially for Santali in Ol Chiki script."
  - [section: Quantitative Results] "LLMs with few-shot prompting provide a strong baseline when fine-tuning is infeasible, but generally underperform dedicated fine-tuned MT models, especially for directions involving Santali in Ol Chiki."
  - [corpus] No neighbor papers specifically address Ol Chiki script challenges; this is a novel contribution of AdiBhashaa.
- Break condition: If LLM tokenizer is extended with Ol Chiki characters and the model undergoes vocabulary expansion with continued pretraining, few-shot performance may approach fine-tuned MT.

## Foundational Learning

- **Concept: Encoder-Decoder Multilingual MT Architectures**
  - Why needed here: The paper evaluates NLLB-200, mT5, and IndicTrans2—all encoder-decoder models designed for sequence-to-sequence tasks across many languages. Understanding how these models handle multilingual input/output is essential for interpreting fine-tuning results.
  - Quick check question: Can you explain why encoder-decoder models are preferred for translation over decoder-only LLMs when fine-tuning is possible?

- **Concept: Participatory Data Creation and Human-in-the-Loop Validation**
  - Why needed here: The paper's core contribution is a community-driven workflow with three stages (source curation, community translation, independent validation). This methodology is presented as essential for data quality and cultural fidelity.
  - Quick check question: What are the three stages of the participatory translation workflow, and why is independent validation necessary?

- **Concept: chrF++ and BLEU Metrics for Low-Resource MT Evaluation**
  - Why needed here: The paper reports chrF++ and sentence-level BLEU, supplemented by human evaluation. Understanding these metrics—and their limitations for morphologically rich languages—is critical for interpreting reported results.
  - Quick check question: Why might chrF++ be preferred over BLEU for evaluating translation into morphologically complex tribal languages?

## Architecture Onboarding

- **Component map:**
  - Data layer: 80,000-sentence parallel corpus (Hindi↔tribal), 20K per language; domains include education, civic info, healthcare, media; scripts: Devanagari (Bhili, Mundari, Gondi), Ol Chiki (Santali)
  - Annotation layer: 3-stage participatory workflow (source curation → community translation → independent validation); validators resolve disagreements through structured discussion
  - Model layer: Fine-tuned multilingual MT (NLLB-200, mT5, IndicTrans2); evaluated LLMs (Bloom, Gemini 2.5 Flash, GPT-4o-mini) in zero-shot/few-shot settings
  - Evaluation layer: chrF++, sentence-level BLEU, human evaluation by native speakers on curated test subset

- **Critical path:**
  1. Recruit native-speaking translators and validators from target communities
  2. Curate Hindi source sentences from education, civic, and healthcare domains
  3. Execute 3-stage translation and validation workflow
  4. Fine-tune multilingual encoder-decoder models on 95% of parallel data
  5. Evaluate on 5% held-out test set using automatic metrics + human judgment

- **Design tradeoffs:**
  - Modest corpus size (20K/language) vs. scalability: Enables high-quality, community-validated data but may limit domain coverage and model robustness
  - Fine-tuned MT vs. few-shot LLM: Fine-tuning yields better performance but requires infrastructure and data; few-shot LLMs offer accessibility with performance penalty
  - Devanagari vs. Ol Chiki script: Three languages share Devanagari with Hindi, enabling script transfer; Santali's Ol Chiki requires separate handling and shows lower performance

- **Failure signatures:**
  - Low chrF++/BLEU for high-resource→tribal directions (expected due to target-side vocabulary sparsity)
  - Large performance drop for Santali/Ol Chiki compared to Devanagari-script languages (tokenizer coverage issue)
  - Validator disagreement on culturally specific idioms or domain terminology (may indicate need for expanded annotation guidelines)

- **First 3 experiments:**
  1. Baseline replication: Fine-tune NLLB-200 on AdiBhashaa for Hindi↔Bhili (Devanagari script); compare zero-shot vs. fine-tuned chrF++/BLEU to validate paper's reported gains
  2. Script ablation: Train separate models for Santali using (a) native Ol Chiki script, (b) transliterated Devanagari; measure performance gap to quantify script-specific tokenization impact
  3. Data scaling test: Fine-tune with 5K, 10K, 15K, and 20K sentence pairs per language; plot learning curves to identify diminishing returns and minimum viable corpus size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed human-in-the-loop paradigm, where models propose translations for community review, scale corpus creation while maintaining linguistic quality and community governance?
- Basis in paper: [explicit] "In the next phase, models will propose translations for additional monolingual text, and community reviewers will accept, correct, or reject them. This human-in-the-loop paradigm scales corpus creation while maintaining quality."
- Why unresolved: The paper describes this as a planned "next phase" rather than an evaluated approach; no empirical results on scalability or quality retention are reported.
- What evidence would resolve it: Experiments measuring annotation speed, error rates, and inter-annotator agreement when using model-assisted versus fully manual translation workflows.

### Open Question 2
- Question: What techniques can effectively close the performance gap between tribal→high-resource and high-resource→tribal translation directions?
- Basis in paper: [explicit] "Translation into high-resource languages (tribal→Hindi/English) systematically outperforms translation into the tribal languages... underscores the need to expand both the volume and domain coverage of tribal-language data."
- Why unresolved: The paper identifies the asymmetry and hypothesizes that more tribal-language data may help, but does not test specific interventions (e.g., back-translation, morphological augmentation, target-side pretraining).
- What evidence would resolve it: Controlled experiments comparing data augmentation strategies or architecture modifications specifically designed to improve generation into low-resource target languages.

### Open Question 3
- Question: What are the specific barriers to MT performance for low-resource scripts like Ol Chiki, and can script-specific interventions (e.g., dedicated tokenization, pretraining on script-native text) improve results?
- Basis in paper: [inferred] The paper notes that Santali in Ol Chiki script shows the largest performance gap for LLMs, and that Ol Chiki has "limited digital infrastructure and minimal representation in existing large-scale corpora."
- Why unresolved: While the paper identifies Ol Chiki as particularly challenging, it does not isolate whether the bottleneck is tokenization, pretraining coverage, or data scarcity, nor does it test script-targeted solutions.
- What evidence would resolve it: Ablation studies with alternative tokenizers or synthetic script-pretraining data, measuring impact on Santali–Ol Chiki translation quality.

### Open Question 4
- Question: Does the participatory methodology transfer effectively to other under-resourced language communities outside the Indian tribal context?
- Basis in paper: [explicit] "We hope that AdiBhashaa will serve both as a practical resource for MT research and as a template for similar initiatives targeting other under-represented languages."
- Why unresolved: The paper demonstrates the approach for four specific Indian tribal languages but does not test its applicability in different cultural, linguistic, or infrastructural contexts.
- What evidence would resolve it: Case studies applying the same workflow to geographically and linguistically distinct low-resource languages, reporting on community engagement outcomes and corpus quality.

## Limitations
- Core claims rest on a novel community-curated dataset of limited size (20K sentence pairs per language), constraining generalizability to broader domains and real-world deployment
- Performance gaps between high-resource→tribal and tribal→high-resource directions lack direct empirical validation through ablation studies on vocabulary coverage or morphological complexity
- The Ol Chiki script challenge for Santali is highlighted but not experimentally isolated from other confounding factors like typological distance or data quality differences

## Confidence
- **High confidence:** Fine-tuning multilingual encoder-decoder models yields substantial gains over zero-shot baselines for unseen tribal languages, supported by consistent results across three model families and four languages
- **Medium confidence:** Translation into high-resource languages systematically outperforms the reverse direction, based on theoretical explanation and observed patterns but lacking direct corpus-level validation
- **Medium confidence:** Few-shot LLMs underperform fine-tuned MT models, especially for Ol Chiki script, with reasonable mechanistic explanation but limited ablation on script-specific factors

## Next Checks
1. **Script-specific tokenization impact:** Create a controlled experiment training separate models for Santali using (a) native Ol Chiki script and (b) transliterated Devanagari; measure performance gap to isolate script-specific tokenization effects from other factors

2. **Human evaluation robustness:** Replicate the human evaluation component with inter-annotator agreement calculation, detailed rubric documentation, and statistical significance testing between model outputs to validate automatic metric findings

3. **Data scaling sensitivity:** Conduct fine-tuning experiments with incremental corpus sizes (5K, 10K, 15K, and 20K sentence pairs per language) and plot learning curves to identify diminishing returns and establish minimum viable dataset size for each language