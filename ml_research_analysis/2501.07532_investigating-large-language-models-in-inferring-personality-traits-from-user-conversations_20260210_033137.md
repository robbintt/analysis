---
ver: rpa2
title: Investigating Large Language Models in Inferring Personality Traits from User
  Conversations
arxiv_id: '2501.07532'
source_url: https://arxiv.org/abs/2501.07532
tags:
- gpt-4o
- personality
- traits
- five
- mini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates GPT-4o and GPT-4o mini for inferring Big Five
  personality traits from user conversations. It compares direct trait inference with
  a structured approach using BFI-10 item scores.
---

# Investigating Large Language Models in Inferring Personality Traits from User Conversations

## Quick Facts
- arXiv ID: 2501.07532
- Source URL: https://arxiv.org/abs/2501.07532
- Reference count: 0
- Large Language Models can infer personality traits from conversations with improved accuracy using structured psychometric approaches

## Executive Summary
This study evaluates GPT-4o and GPT-4o mini for inferring Big Five personality traits from user conversations. The research compares direct trait inference with a structured approach using BFI-10 item scores, finding that GPT-4o mini generally outperformed GPT-4o, especially in structured psychometric tasks and with participants experiencing depressive symptoms. Using BFI-10 item scores improved prediction accuracy over direct trait inference, with mean differences ranging from -0.985 to 0.382 for direct predictions and improving to -0.852 to 0.088 with BFI-10-based calculations. These findings highlight the value of structured psychological frameworks and suggest potential for LLMs in scalable mental health assessment.

## Method Summary
The study employed a mixed-methods approach comparing two LLMs (GPT-4o and GPT-4o mini) in inferring Big Five personality traits from user conversations. Researchers collected conversation data and self-reported personality trait assessments from Korean participants. Two inference methods were tested: direct trait prediction and a structured approach using BFI-10 item scores. The BFI-10 items were used to calculate personality trait scores through established psychometric formulas. Performance was evaluated by comparing LLM predictions against self-reported ground truth, with particular attention to differences between direct inference and structured psychometric approaches.

## Key Results
- GPT-4o mini outperformed GPT-4o in both direct trait inference and structured BFI-10 approaches
- BFI-10 item score methodology improved prediction accuracy compared to direct trait inference
- Mean differences in predictions improved from -0.985 to 0.382 (direct) to -0.852 to 0.088 (BFI-10-based)

## Why This Works (Mechanism)
The study demonstrates that structured psychometric frameworks provide better grounding for LLM inference tasks compared to direct trait prediction. By breaking down personality assessment into specific BFI-10 items, the models can leverage their reasoning capabilities more effectively rather than attempting holistic trait classification. This structured approach appears to reduce cognitive load on the models and provides more concrete reference points for inference. The improved performance with BFI-10 items suggests that LLMs benefit from explicit, item-level prompts that align with established psychological assessment methodologies.

## Foundational Learning
- Big Five personality traits (OCEAN): Five-factor model of personality (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) - why needed: Provides standardized framework for personality assessment; quick check: Verify each trait is measured with appropriate BFI-10 items
- BFI-10 psychometric items: Short-form personality assessment tool with 10 items measuring the Big Five traits - why needed: Enables structured, validated personality measurement; quick check: Confirm item-to-trait mappings are correct
- Psychometric formula application: Mathematical computation of personality scores from item responses - why needed: Converts raw responses to standardized trait scores; quick check: Validate formula implementation against established standards
- Self-report validation: Using participant self-assessments as ground truth - why needed: Provides benchmark for LLM performance; quick check: Assess potential biases in self-reporting
- Depression screening integration: Considering mental health status in personality assessment - why needed: Accounts for how depressive symptoms affect personality expression; quick check: Verify depression screening methodology

## Architecture Onboarding

**Component Map:**
Conversation Data -> Preprocessing -> LLM Inference (Direct/Structured) -> Score Calculation -> Ground Truth Comparison

**Critical Path:**
Conversation Data → Preprocessing → LLM Inference → Score Calculation → Ground Truth Comparison

**Design Tradeoffs:**
- Direct inference vs. structured approach: Simpler but less accurate vs. more complex but higher accuracy
- Model size vs. performance: GPT-4o mini outperformed larger GPT-4o in structured tasks
- Cultural specificity vs. generalizability: Korean dataset limits broader application

**Failure Signatures:**
- High variance in predictions when dealing with depressive symptoms
- Reduced accuracy with ambiguous conversation content
- Systematic biases in cross-cultural personality assessment

**3 First Experiments:**
1. Test structured BFI-10 approach on non-depressed participants to isolate effect of depression
2. Compare LLM performance with human expert personality assessments
3. Evaluate performance on multi-lingual conversation datasets

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Results rely on self-reported personality traits as ground truth, introducing potential measurement error and biases
- Study focused exclusively on Korean participants, limiting generalizability to other cultural contexts and languages
- Comparison only between GPT-4o and GPT-4o mini may not reflect capabilities of other LLM architectures

## Confidence
- High confidence in finding that structured psychometric approaches outperform direct trait inference
- Medium confidence in cross-cultural generalizability due to single cultural focus
- Medium confidence in measurement accuracy due to reliance on self-reported ground truth

## Next Checks
1. External validation with a diverse, multi-cultural dataset to assess generalizability across languages and cultural contexts
2. Comparison with alternative psychometric frameworks beyond the Big Five to determine if structured approaches consistently outperform direct inference across different personality models
3. Longitudinal study to evaluate the stability and consistency of LLM-based personality inferences over time and across different conversation contexts