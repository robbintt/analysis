---
ver: rpa2
title: HY-MT1.5 Technical Report
arxiv_id: '2512.24092'
source_url: https://arxiv.org/abs/2512.24092
tags:
- translation
- hy-mt1
- wang
- language
- xcomet-xxl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents HY-MT1.5-1.8B and HY-MT1.5-7B, two high-performance
  machine translation models developed using a holistic training framework. The models
  are designed to address the challenge of balancing translation quality and efficiency,
  particularly for resource-constrained deployment scenarios.
---

# HY-MT1.5 Technical Report

## Quick Facts
- arXiv ID: 2512.24092
- Source URL: https://arxiv.org/abs/2512.24092
- Reference count: 10
- HY-MT1.5-1.8B achieves approximately 90% of ultra-large proprietary model performance while significantly outperforming larger open-source baselines

## Executive Summary
This paper presents HY-MT1.5-1.8B and HY-MT1.5-7B, two high-performance machine translation models developed using a holistic training framework. The models are designed to address the challenge of balancing translation quality and efficiency, particularly for resource-constrained deployment scenarios. The training pipeline integrates MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning with a rubrics-based evaluation system. Both models support advanced features such as terminology intervention, context-aware translation, and format preservation.

## Method Summary
The HY-MT1.5 models employ a comprehensive training pipeline that combines multiple techniques to achieve high translation quality while maintaining efficiency. The framework begins with MT-oriented pre-training using a carefully curated corpus, followed by supervised fine-tuning on parallel translation data. On-policy distillation is then applied to transfer knowledge from larger models, and finally reinforcement learning with a rubrics-based evaluation system refines the model's translation quality. The holistic approach aims to create compact models that can rival much larger proprietary systems while being deployable in resource-constrained environments.

## Key Results
- HY-MT1.5-1.8B achieves approximately 90% of Gemini-3.0-Pro's performance on Flores-200 with an XCOMET-XXL score of 0.8361
- HY-MT1.5-7B reaches 95% of Gemini-3.0-Pro's performance on Flores-200 and surpasses it on WMT25 and Mandarin-minority language benchmarks
- HY-MT1.5-1.8B maintains high translation quality with an average response time of 0.18 seconds, demonstrating efficient deployment capabilities

## Why This Works (Mechanism)
The success of HY-MT1.5 models stems from their holistic training framework that addresses multiple aspects of translation quality simultaneously. By combining MT-oriented pre-training with supervised fine-tuning, the models develop strong foundational language understanding and translation capabilities. On-policy distillation allows the models to learn from larger, more capable systems without requiring excessive computational resources. The reinforcement learning phase with rubrics-based evaluation ensures that the models optimize for human-aligned translation quality metrics rather than just likelihood-based objectives.

## Foundational Learning
- **Machine Translation Fundamentals**: Understanding sequence-to-sequence modeling, attention mechanisms, and transformer architectures is essential for grasping how translation models process and generate text across languages. Quick check: Verify understanding of encoder-decoder architecture and attention mechanisms.
- **Pre-training vs Fine-tuning**: Recognizing the difference between general language understanding (pre-training) and task-specific adaptation (fine-tuning) is crucial for understanding the model's development pipeline. Quick check: Explain why pre-training on MT-specific data differs from general language modeling.
- **Reinforcement Learning for NLP**: Knowledge of policy gradient methods and reward-based optimization is necessary to understand how the models improve through interaction with evaluation rubrics. Quick check: Describe how reward signals guide model improvements in translation tasks.
- **Knowledge Distillation**: Understanding how smaller models can learn from larger ones through teacher-student training is key to grasping the efficiency gains achieved by HY-MT1.5. Quick check: Explain the difference between offline and on-policy distillation approaches.
- **Evaluation Metrics for Translation**: Familiarity with metrics like XCOMET-XXL, BLEU, and human evaluation rubrics is important for interpreting the reported performance results. Quick check: Compare different translation evaluation metrics and their strengths/weaknesses.

## Architecture Onboarding
- **Component Map**: MT-oriented Pre-training -> Supervised Fine-tuning -> On-policy Distillation -> Reinforcement Learning
- **Critical Path**: The most critical components are the MT-oriented pre-training phase and the reinforcement learning phase. Pre-training establishes the foundational language understanding specific to translation tasks, while reinforcement learning with rubrics-based evaluation ensures the final model optimizes for human-aligned quality metrics rather than just likelihood.
- **Design Tradeoffs**: The primary tradeoff involves model size versus performance. The 1.8B parameter model achieves 90% of Gemini-3.0-Pro's performance, while the 7B parameter model reaches 95%. This represents a significant efficiency gain but with diminishing returns for larger models. The choice between them depends on deployment constraints and required performance levels.
- **Failure Signatures**: Potential failure modes include degradation in low-resource language pairs, loss of translation quality in domain-specific contexts not represented in training data, and reduced performance under computational constraints that prevent full model utilization. The models may also struggle with extremely long-form translation tasks or highly specialized terminology not covered in the training corpus.
- **First Experiments**:
  1. Test translation quality on a held-out validation set from the same domain as training data to establish baseline performance.
  2. Evaluate cross-domain generalization by translating text from legal, medical, and technical domains not seen during training.
  3. Measure inference latency and throughput under different hardware configurations to validate deployment efficiency claims.

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Benchmark scope is limited to specific test sets (Flores-200, WMT25, Mandarin-minority languages), with unclear real-world generalization across diverse domains and language pairs
- Training data transparency is lacking, with no detailed information about composition, quality filtering, or potential biases
- Deployment context performance may vary from controlled conditions due to hardware differences, network latency, and diverse input patterns

## Confidence
- **High Confidence**: The architectural framework (MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning) is well-established in the literature and the reported benchmark scores are internally consistent
- **Medium Confidence**: The comparison with proprietary models like Gemini-3.0-Pro is methodologically sound but depends on the availability and quality of evaluation protocols; claimed 90-95% performance relative to Gemini-3.0-Pro requires independent verification
- **Low Confidence**: The model's robustness to adversarial inputs, out-of-domain translations, and long-form translation tasks has not been thoroughly evaluated

## Next Checks
1. Conduct zero-shot and few-shot translation tests across diverse domains (legal, medical, technical) to assess real-world generalization
2. Perform ablation studies to quantify the individual contributions of each training component (pre-training, fine-tuning, distillation, RL) to the final performance
3. Evaluate the models' performance under varying computational constraints and hardware configurations to validate deployment efficiency claims