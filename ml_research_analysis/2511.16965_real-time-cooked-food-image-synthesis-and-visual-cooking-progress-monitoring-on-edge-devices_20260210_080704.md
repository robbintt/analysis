---
ver: rpa2
title: Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring
  on Edge Devices
arxiv_id: '2511.16965'
source_url: https://arxiv.org/abs/2511.16965
tags:
- cooking
- image
- images
- similarity
- food
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating realistic cooked
  food images on edge devices, enabling real-time cooking progress monitoring. The
  authors introduce a novel dataset of oven-based cooking progressions with chef-annotated
  doneness levels and propose an edge-efficient U-Net generator conditioned on raw
  images, recipe names, and cooking states.
---

# Real-Time Cooked Food Image Synthesis and Visual Cooking Progress Monitoring on Edge Devices

## Quick Facts
- arXiv ID: 2511.16965
- Source URL: https://arxiv.org/abs/2511.16965
- Reference count: 40
- This paper introduces a novel dataset and edge-efficient method for real-time cooked food image synthesis and cooking progress monitoring.

## Executive Summary
This paper addresses the challenge of generating realistic cooked food images on edge devices while enabling real-time cooking progress monitoring. The authors propose a U-Net generator conditioned on raw images, recipe names, and cooking states, enhanced with Feature-wise Linear Modulation (FiLM) for recipe-specific transformations. A domain-specific Culinary Image Similarity (CIS) metric is introduced to ensure temporal consistency and culinary plausibility during both generation and monitoring. The system achieves significant improvements over baselines (30% better FID on their dataset, 60% on public datasets) while maintaining real-time performance on edge hardware.

## Method Summary
The method employs a U-Net generator (8.68M parameters) with FiLM-based conditioning using sinusoidal embeddings to handle diverse recipe transformations efficiently. A Siamese network trained as the CIS metric provides temporal consistency for cooking progress monitoring. The system generates target cooked images from raw inputs and monitors live cooking by comparing camera frames to these targets using the CIS similarity score. The approach enables user-preferred visual cooking targets and offers practical solutions for smart kitchen appliances.

## Key Results
- Achieved 52.18 FID score and 0.2145 LPIPS score on proprietary dataset
- Demonstrated 30% improvement in FID scores compared to existing methods on their dataset
- Maintained real-time performance: 1.2s/image generation and 0.3s similarity computation on 5 TOPS NPU
- Showed 60% improvement on public datasets compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1: FiLM-based Conditional Modulation
The model uses a lightweight U-Net architecture where intermediate feature maps are dynamically modulated using Feature-wise Linear Modulation (FiLM). Sinusoidal positional embeddings map recipe name and cooking state to scale (γ) and shift (β) parameters, allowing the network to apply recipe-specific transformations using shared weights. This enables efficient handling of diverse recipes while maintaining low computational overhead.

### Mechanism 2: Temporal Embedding Space via Siamese Network
The authors train a Siamese network to function as the Culinary Image Similarity (CIS) metric, which learns a temporal embedding space for cooking progression. Unlike generic perceptual metrics, CIS is trained on sequential cooking frames where ground truth similarity is defined by temporal distance, forcing the model to map cooking progression into a smooth trajectory sensitive to domain-specific changes like browning or bubbling.

### Mechanism 3: Closed-Loop Feedback Control
The system generates a target "cooked" image based on user preference before cooking begins. During cooking, live camera frames are compared to this target using the pre-computed CIS embedding. The system monitors for a peak in similarity within a sliding window, triggering a stop command when the live view aligns with the generated target, achieving closed-loop feedback control.

## Foundational Learning

- **Feature-wise Linear Modulation (FiLM)**
  - Why needed here: Core technique enabling the "Edge-Efficient" design by conditioning feature maps with recipe-specific parameters
  - Quick check question: If you remove the FiLM layers, what is the theoretical impact on the generator's ability to distinguish "rare" vs. "well-done" outputs?

- **Siamese Networks & Contrastive Learning**
  - Why needed here: The CIS metric relies on a Siamese architecture trained with temporal proximity as supervisory signal
  - Quick check question: Why is MSE used for the CIS loss instead of Binary Cross Entropy (BCE)?

- **Fréchet Inception Distance (FID) & LPIPS**
  - Why needed here: Baseline metrics that fail to capture cooking-specific temporal dynamics
  - Quick check question: Why would a standard GAN loss alone fail to preserve the specific structural changes of cooking (e.g., rising dough)?

## Architecture Onboarding

- **Component map:** Raw Image (224×224) + Text (Recipe/State) → Sinusoidal Embedding → FiLM-modulated U-Net → Generated Target → CIS Network → Cosine Similarity Score → Threshold Check

- **Critical path:** 
  1. Inference: Text + Raw Image → FiLM-modulated U-Net → Generated Target
  2. Monitoring: Camera Frame + Generated Target → CIS Network → Cosine Similarity Score → Threshold Check

- **Design tradeoffs:**
  - Params vs. Control: Uses only 8.68M params (vs. Pix2Pix-Turbo's ~1290M) potentially reducing high-frequency texture detail
  - Metric Specificity: CIS trained on cooking data outperforms generic metrics but lacks transferability to other domains
  - Fixed Trajectory: Assumes stationary food item (no flipping/moving) as noted in the Conclusion

- **Failure signatures:**
  - Static Output: Generated images look identical across cooking states (Diagnosis: Embedding collapse or FiLM layers failing to modulate)
  - Early Stopping: Oven stops too early (Diagnosis: CIS score peaking prematurely; dynamic range needs recalibration)
  - Lighting Drift: Monitoring fails when oven door opens (Diagnosis: Model overfitted to internal oven lighting conditions)

- **First 3 experiments:**
  1. Baseline Sanity Check: Train generator without CIS loss (λ_3=0) and verify drop in temporal consistency
  2. Metric Range Analysis: Plot CIS vs LPIPS scores for session video to verify CIS shows wider dynamic range
  3. Latency Profiling: Deploy ONNX model to target 5 TOPS NPU to confirm 1.2s generation and 0.3s similarity computation

## Open Questions the Paper Calls Out

- **Lighting changes and door openings:** The system currently uses data collected with the oven door closed, and real-world scenarios involving door openings changing light conditions were not explicitly modeled.
- **In-session food manipulation:** The authors state that food flipping and in-session food movement were not explicitly modeled, leaving the model's robustness to these scenarios untested.
- **Cross-appliance generalization:** The dataset focuses exclusively on oven-based cooking, and the authors note that generalization to other appliance types like stovetops or air fryers remains untested.

## Limitations
- Reliance on proprietary dataset limits reproducibility and generalization to other cooking contexts
- Assumes stationary food items during cooking, precluding applications involving stirring, flipping, or multiple food components
- FiLM-based modulation assumes recipe-specific transformations can be linearly separated, which may fail for complex recipes

## Confidence
- **High Confidence:** Edge-efficiency claims supported by architectural design and reported latency measurements
- **Medium Confidence:** Performance improvements specific to proprietary dataset may not generalize to other cooking contexts
- **Low Confidence:** Assumption of monotonic cooking progression in embedding space is theoretical and may break down for complex scenarios

## Next Checks
1. Evaluate the model on a public cooking dataset (e.g., VideoBake) to verify performance claims hold beyond proprietary data
2. Test the system with cooking scenarios involving food movement (stirring, flipping) to identify failure modes and assess need for motion compensation
3. Conduct systematic ablation study removing FiLM layers to quantify exact contribution to both performance and edge-efficiency