---
ver: rpa2
title: Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders
arxiv_id: '2506.10094'
source_url: https://arxiv.org/abs/2506.10094
tags:
- clustering
- data
- latent
- learning
- triplet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research proposes a two-phase unsupervised clustering framework
  for MNIST digits using a convolutional autoencoder enhanced with triplet loss. In
  Phase 1, the autoencoder learns compact latent embeddings through reconstruction.
---

# Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders

## Quick Facts
- arXiv ID: 2506.10094
- Source URL: https://arxiv.org/abs/2506.10094
- Reference count: 7
- Key outcome: Proposed framework achieves Silhouette Score of 0.2061, NMI of 0.4960, and ARI of 0.3923 on MNIST

## Executive Summary
This research introduces a two-phase unsupervised clustering framework for MNIST digits using a convolutional autoencoder enhanced with triplet loss. The approach first learns compact latent embeddings through reconstruction, then refines these embeddings with triplet loss to improve cluster separability. The model incorporates batch normalization, dropout, and weight decay to enhance stability and generalization. Results demonstrate superior clustering performance compared to baseline methods on raw pixels and PCA-reduced features, with clear cluster separation visible in t-SNE visualizations for most digits.

## Method Summary
The proposed framework consists of two distinct phases for unsupervised clustering of MNIST digits. In Phase 1, a convolutional autoencoder learns to compress input images into compact latent representations through reconstruction loss. Phase 2 introduces triplet loss to refine these embeddings, pulling similar samples closer while pushing dissimilar ones apart in the latent space. The architecture employs batch normalization and dropout for stability, with weight decay to prevent overfitting. The model is trained entirely without labels, using only the inherent structure of the data to form clusters.

## Key Results
- Achieves Silhouette Score of 0.2061, NMI of 0.4960, and ARI of 0.3923 on MNIST
- Outperforms baseline KMeans on raw pixels and PCA-reduced features
- t-SNE visualizations show distinct clusters for most digits, though some overlap remains
- Most digits form clear clusters, with 5, 6, and 7 showing some overlap

## Why This Works (Mechanism)
The two-phase approach leverages the complementary strengths of autoencoders and metric learning. The autoencoder first learns a compressed representation that preserves essential information about the input images. The subsequent triplet loss phase then refines this representation by explicitly enforcing separation between different digit classes in the latent space. This combination allows the model to learn both general feature extraction and class-specific discriminative features without any labeled data.

## Foundational Learning
- Convolutional Autoencoders: Learn compressed representations through reconstruction - needed for initial feature learning, check by verifying reconstruction quality
- Triplet Loss: Enforces relative distance constraints between samples - needed for cluster refinement, check by examining embedding distributions
- Batch Normalization: Stabilizes training by normalizing layer inputs - needed for consistent convergence, check by monitoring training stability
- Dropout Regularization: Prevents overfitting during training - needed for generalization, check by comparing train/test performance
- Weight Decay: Adds L2 regularization to prevent overfitting - needed for stable training, check by monitoring weight magnitudes

## Architecture Onboarding
Component Map: Input Images -> Convolutional Autoencoder -> Latent Embeddings -> Triplet Loss Refinement -> Cluster Assignments

Critical Path: The reconstruction phase establishes initial embeddings, followed by triplet loss refinement that creates well-separated clusters in the latent space.

Design Tradeoffs: Fixed cluster count assumes prior knowledge vs. flexibility of adaptive methods; batch distance-based triplet mining vs. computational cost of hard mining strategies.

Failure Signatures: Overlapping clusters in t-SNE indicate insufficient separation; poor reconstruction quality suggests autoencoder issues; unstable training indicates normalization or regularization problems.

First Experiments:
1. Visualize reconstruction quality to verify autoencoder learning
2. Examine embedding distributions before and after triplet refinement
3. Test different triplet mining strategies to assess impact on clustering quality

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Fixed cluster count of 10 assumes prior knowledge of digit classes
- Batch distance-based triplet mining may not capture hardest samples
- Performance on MNIST may not generalize to complex real-world datasets

## Confidence
High: Methodology is well-defined and results are clearly presented
Medium: Moderate clustering performance and dataset limitations reduce generalizability
Medium: Limited comparison with state-of-the-art methods

## Next Checks
1. Test model on complex datasets (CIFAR-10, Fashion-MNIST) for scalability
2. Implement hard triplet mining strategies to improve cluster refinement
3. Explore adaptive clustering methods for unknown cluster counts