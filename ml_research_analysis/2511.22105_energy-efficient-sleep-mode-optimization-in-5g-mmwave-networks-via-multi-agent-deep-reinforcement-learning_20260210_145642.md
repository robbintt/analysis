---
ver: rpa2
title: Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent
  Deep Reinforcement Learning
arxiv_id: '2511.22105'
source_url: https://arxiv.org/abs/2511.22105
tags:
- marl-ddqn
- throughput
- each
- while
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-agent deep reinforcement learning (MARL)
  framework using a Double Deep Q-Network (DDQN) for sleep mode optimization (SMO)
  in millimeter-wave (mmWave) networks. The method addresses scalability limitations
  of single-agent RL by enabling distributed decision-making across multiple base
  stations (BSs) while maintaining minimal signaling overhead.
---

# Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.22105
- Source URL: https://arxiv.org/abs/2511.22105
- Reference count: 36
- Primary result: MARL-DDQN achieves up to 0.60 Mbit/Joule EE and 8.5 Mbps 10th-percentile throughput while satisfying QoS constraints 95% of the time in dynamic mmWave networks

## Executive Summary
This paper proposes a multi-agent deep reinforcement learning framework using Double Deep Q-Networks (DDQN) for sleep mode optimization in millimeter-wave (mmWave) cellular networks. The approach addresses scalability limitations of single-agent reinforcement learning by enabling distributed decision-making across multiple base stations while maintaining minimal signaling overhead. A realistic power consumption model incorporating beamforming and load-dependent behavior is used, with QoS defined in terms of throughput. The method adapts sleep mode policies to maximize energy efficiency while mitigating inter-cell interference and ensuring throughput fairness. Extensive simulations demonstrate significant improvements over state-of-the-art strategies including All On, iterative QoS-aware load-based, MARL-DDPG, and MARL-PPO approaches.

## Method Summary
The proposed framework formulates sleep mode optimization as a multi-agent Markov decision process where each base station operates as an independent DDQN agent with binary active/sleep actions. Agents learn decentralized policies using local state observations while receiving a shared global reward, promoting cooperation without explicit inter-agent communication. The state representation uses K-means clustering to compress UE positions into cluster centroids and occupation ratios, reducing dimensionality while preserving spatial information. A piecewise reward function balances energy efficiency gains against QoS penalties, with hard failure penalties preventing all-BS-sleep deadlocks. Training uses experience replay with batch updates every few steps, and target networks are updated periodically to stabilize learning. The framework is evaluated in a 3D urban mmWave environment with realistic channel models and community-based UE mobility patterns.

## Key Results
- MARL-DDQN achieves 0.60 Mbit/Joule energy efficiency, outperforming All On, IT-QoS-LB, MARL-DDPG, and MARL-PPO baselines
- 10th-percentile throughput reaches 8.5 Mbps while maintaining QoS satisfaction above 95% threshold
- Scalability demonstrated across 7-15 base stations with 28-112 UEs, showing consistent performance gains
- Clustering-based state compression with K=10 tracking areas provides optimal balance between information retention and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the sleep mode optimization problem across multiple agents enables scalable learning in dense mmWave deployments. Each base station operates as an independent DDQN agent with binary action space (active/sleep), reducing the joint action space complexity from 2^N to N independent decisions. Agents learn decentralized policies using only local state observations while receiving a shared global reward, promoting cooperation without explicit inter-agent communication. The global reward signal provides sufficient gradient information for individual agents to learn coordinated behaviors despite non-stationary environments caused by mutual policy updates.

### Mechanism 2
Double Deep Q-Network (DDQN) mitigates value overestimation and stabilizes learning in the discrete sleep-mode action space. DDQN decouples action selection (using online network parameters) from action evaluation (using target network parameters), reducing optimistic bias. The target value uses the online network to select the best action and the target network to evaluate it, with target networks updated periodically rather than continuously. This provides stable bootstrapped targets in the sparse reward environment with threshold-based QoS penalties.

### Mechanism 3
Clustering-based state compression and piecewise reward shaping enable effective credit assignment across high-dimensional network states. UEs are clustered into K tracking areas using K-means, reducing state dimensionality from raw UE coordinates to cluster centroids and occupation ratios. The state vector includes historical steps of cluster information, BS load, QoS satisfaction, and past actions. The reward function is piecewise: high reward for EE when QoS met with BSs in sleep, reduced reward proportional to sleep BSs when QoS marginally met, negative penalty when QoS violated, and hard failure penalty when all BSs asleep and QoS failed.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Q-Learning
  - Why needed here: The paper formulates sleep mode optimization as an MDP and builds on Q-learning foundations. Understanding Bellman equations, discount factors, and exploration-exploitation tradeoffs is essential before tackling DDQN.
  - Quick check question: Can you explain why the Q-learning update rule Y_t = r_t + γ max_{a'} Q(s_{t+1}, a') bootstraps value estimates and why this creates overestimation bias in standard DQN?

- Concept: Deep Q-Networks and Experience Replay
  - Why needed here: The DDQN agent uses neural network function approximation with replay buffers. Understanding why replay breaks temporal correlations and why target networks stabilize training is prerequisite knowledge.
  - Quick check question: Why does sampling random mini-batches from a replay buffer improve learning stability compared to using consecutive online samples?

- Concept: mmWave Propagation and Beamforming
  - Why needed here: The system model incorporates 3D UMa path loss models, beamforming gains, and load-dependent power consumption. Without this context, the state representation and reward engineering will appear arbitrary.
  - Quick check question: How does beamforming gain affect the tradeoff between energy efficiency and coverage in mmWave networks when BSs are selectively deactivated?

## Architecture Onboarding

- Component map: Environment Emulator -> State Generator -> RL Agents -> Reward Computer -> Replay Buffer -> Network Updates
- Critical path: UE mobility generates time-varying spatial distribution → State generator clusters UEs and assembles historical vectors → Each agent selects sleep/active action via ε-greedy DDQN → Environment emulator computes throughput, power, EE, QoS → Reward computer broadcasts piecewise reward → Transitions stored in buffer → Every τ_update steps, sample batch and update online networks via gradient descent → Every τ steps, sync target networks
- Design tradeoffs:
  - Clustering granularity K: Higher K captures finer spatial variation but increases state dimensionality (3K features). Paper experiments show K=10 better at high UE density.
  - State history window t_ℓ: Longer windows capture more temporal context but increase compute and may include stale information. Paper uses t_ℓ=4.
  - Exploration decay rate ε_th: Faster decay accelerates exploitation but risks premature convergence to suboptimal policies. Paper uses 0.9.
  - Target network update frequency τ: More frequent updates reduce staleness but increase instability. Paper uses τ_update=4 for online weights.
- Failure signatures:
  1. Q-value divergence: Oscillating or exploding Q-values indicate learning instability; check reward scaling and target network sync.
  2. QoS violation cascade: If >5% of episodes fail QoS constraints after convergence, reward penalties may be insufficient.
  3. All-BS-sleep deadlock: If agents converge to all-sleep state, λ_fail penalty may be too low relative to EE reward.
- First 3 experiments:
  1. Baseline sanity check: Run All On, IT-QoS-LB, and random sleep policies for N=9 BSs, U=70 UEs over 100 episodes to validate environment emulator and metric computation.
  2. Single-agent DDQN ablation: Train a single centralized DDQN agent controlling all N=5 BSs to verify DDQN implementation and validate multi-agent decomposition claim.
  3. Cluster granularity sweep: Train MARL-DDQN with K ∈ {3, 5, 7, 10, 15} clusters for fixed N=9, U=70 to validate state representation design and identify sensitivity to this hyperparameter.

## Open Questions the Paper Calls Out

### Open Question 1
How can sleep mode optimization (SMO) and beamforming be jointly optimized to enhance link budgets and energy efficiency? The current framework assumes codebook-based analog beamforming to calculate gains, treating beamforming as an input parameter rather than a decision variable within the RL agent's action space. A modified MARL framework where the action space includes beam selection indices alongside sleep mode states would demonstrate improved Energy Efficiency without increased signaling overhead.

### Open Question 2
Can the proposed MARL-DDQN framework be effectively validated in a real-world setting using the O-RAN XApp framework? The conclusion explicitly identifies real-world validation of the proposed EE enhancement approach using the O-RAN XApp framework as a future direction. The current results are based on simulations that assume perfect channel state information and idealized community-based mobility, which may not hold in physical hardware deployments with latency and measurement noise. Implementation on an O-RAN compliant testbed with live traffic conditions would report Energy Efficiency and QoS satisfaction metrics.

### Open Question 3
How can transfer learning be utilized to initialize new Base Station (BS) agents to avoid full retraining in dynamic deployment scenarios? Section VI-C notes that new BSs can be fine-tuned via transfer learning rather than retraining, stating this will be explored in future work. While the paper demonstrates scalability by varying BS density, the agents are trained from scratch for each configuration. Experiments showing the convergence time and stability of a newly added BS agent initialized with weights from a pre-trained neighbor compared to a randomly initialized agent would resolve this.

### Open Question 4
How robust is the proposed MARL-DDQN policy when exposed to user mobility traces that deviate significantly from the community-based model used during training? The authors critique prior works for relying on static or aggregated traffic and validate their approach using a specific time-varying community-based model. The RL policy may overfit to the specific transition probabilities and epoch durations of the community model. If deployed in environments with high-speed linear mobility rather than localized community roaming, the QoS constraints might be violated more frequently. Evaluation of the trained policy's performance using disparate mobility datasets, such as Random Waypoint or Gauss-Markov models, without retraining would resolve this.

## Limitations
- Critical hyperparameters (learning rate, discount factor, target network sync frequency, hardware power constants, mobility transition probabilities, training duration) are unspecified, affecting reproducibility
- Centralized reward design assumes perfect communication of system-wide metrics, which may not reflect real deployment constraints
- Fixed K=10 clustering assumes stable UE distributions, potentially limiting generalization to highly dynamic or sparse scenarios

## Confidence

- **High Confidence**: The multi-agent DDQN architecture, piecewise reward shaping, and state compression via clustering are well-justified and technically sound. The claimed EE gains (0.60 Mbit/Joule) and QoS satisfaction (>95%) are internally consistent with the model and methodology.
- **Medium Confidence**: The scalability advantage over single-agent RL is plausible given the exponential action space reduction, but requires validation at larger N (beyond N=15) and different UE densities. The DDQN stability claim is reasonable but not empirically verified against standard DQN within the paper.
- **Low Confidence**: The sensitivity to hyperparameters (K, t_ℓ, ε_th) and the robustness to different mobility patterns or network topologies are not extensively explored. The claim of outperforming MARL-PPO/DRPG is based on limited experimental scenarios.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary K (3-15), t_ℓ (2-6), and ε_th (0.8-0.99) to identify optimal ranges and quantify performance degradation outside these ranges.

2. **Scalability Stress Test**: Evaluate MARL-DDQN at N=20-30 BSs with U=150-200 UEs to verify the claimed scalability advantage over single-agent RL (2^N action space growth).

3. **Reward Function Ablation**: Remove the hard failure penalty (λ_fail) or reduce λ_QoS' to quantify their necessity for preventing QoS collapse and all-BS-sleep deadlocks.