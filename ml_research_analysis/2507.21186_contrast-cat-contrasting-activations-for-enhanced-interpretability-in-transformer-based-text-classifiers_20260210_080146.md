---
ver: rpa2
title: 'Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based
  Text Classifiers'
arxiv_id: '2507.21186'
source_url: https://arxiv.org/abs/2507.21186
tags:
- attribution
- contrast-cat
- aopc
- lodds
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpreting transformer-based
  text classification models, where existing activation-based attribution methods
  can be undermined by class-irrelevant features within activations, leading to less
  reliable interpretations. The authors propose Contrast-CAT, a novel activation contrast-based
  attribution method that refines token-level attributions by filtering out class-irrelevant
  features through contrastive reference activations.
---

# Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers

## Quick Facts
- arXiv ID: 2507.21186
- Source URL: https://arxiv.org/abs/2507.21186
- Reference count: 13
- This paper proposes Contrast-CAT, an activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features through contrastive reference activations, achieving average improvements of ×1.30 in AOPC and ×2.25 in LOdds under the MoRF setting compared to best competitors.

## Executive Summary
This paper addresses the challenge of interpreting transformer-based text classification models, where existing activation-based attribution methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. The authors propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features through contrastive reference activations. Contrast-CAT generates clearer and more faithful attribution maps by contrasting target activations with multiple reference activations from different classes, weighting by gradients and attention scores, and refining via a deletion test. Experimental results across various datasets and models (BERTbase, DistilBERT, RoBERTa, GPT-2, Llama-2) confirm that Contrast-CAT consistently outperforms state-of-the-art methods.

## Method Summary
Contrast-CAT generates token-level attributions for transformer-based text classifiers by contrasting target activations with reference activations from low-scoring sequences. The method computes the difference between target and reference activations at each layer, weights this contrast by gradients and attention scores, and aggregates across layers. A deletion test refinement step filters out unreliable attribution maps by measuring their impact on model probability when top-attributed tokens are removed. The final attribution map averages the remaining high-quality maps. The approach requires pre-computing a reference library of 30 low-scoring sequences per class (with fc(r) < 0.001) and their activations across all transformer layers.

## Key Results
- Contrast-CAT achieves ×1.30 average improvement in AOPC and ×2.25 improvement in LOdds under MoRF setting compared to best competitors
- Using all transformer layers yields ×1.52 AOPC and ×3.05 LOdds improvement over using only the penultimate layer
- Optimal reference count is 25-30, with diminishing returns beyond this point
- Deletion test refinement with mean + std threshold improves AOPC from 0.557 to 0.703 on Amazon dataset

## Why This Works (Mechanism)

### Mechanism 1: Activation Contrasting Filters Class-Irrelevant Features
Subtracting reference activations from target activations removes features that do not contribute to the target class prediction. For each token activation Aℓ_i, compute (Aℓ_i - Rℓ_i) where Rℓ_i is drawn from a reference sequence r satisfying fc(r) < γ (γ = 10^-3). The gradient ∂fc/∂Aℓ_i then weights only the contrast residual, isolating class-relevant signals. Core assumption: Activations contain both class-relevant and class-irrelevant features that are separable via contrast with low-activation references from non-target classes.

### Mechanism 2: Multi-Layer Aggregation Captures Hierarchical Semantics
Integrating activations across all transformer layers produces more faithful attributions than single-layer approaches. Sum contributions from layers ℓ = 1 to L, weighted by attention scores ˆαℓ_i. This captures both low-level phrase patterns (early layers) and abstract semantic features (later layers). Core assumption: Important tokens influence the prediction at multiple abstraction levels, not just the final layer.

### Mechanism 3: Deletion-Test Refinement Discards Unreliable Maps
Averaging only high-quality attribution maps (as measured by deletion test drop scores) improves final attribution faithfulness. Generate multiple maps using different references, compute deletion score S by removing top-k tokens and measuring probability drop, retain only maps with S ≥ ρ (mean + std), then average. Core assumption: Maps that cause larger probability drops when top tokens are removed are more faithful to true model reasoning.

## Foundational Learning

- Concept: **Gradient-weighted attribution (Grad-CAM family)**
  - Why needed here: Contrast-CAT builds directly on this paradigm; the core innovation is what gets weighted (contrast residual vs. raw activation).
  - Quick check question: Given activation A and gradient g, what does A ⊙ g represent vs. (A - R) ⊙ g?

- Concept: **Transformer attention and layer structure**
  - Why needed here: Method aggregates multi-head attention scores (ˆαℓ_i) and uses layer-wise activations; understanding skip connections and attention flow is essential.
  - Quick check question: Why might the penultimate layer alone fail to capture token importance for sentiment classification?

- Concept: **Faithfulness metrics (AOPC, LOdds, MoRF/LeRF)**
  - Why needed here: Paper's claims rest on these evaluation protocols; understanding what they measure is critical for interpreting results.
  - Quick check question: Under MoRF, if removing top-attributed tokens causes small probability change, what does that imply about the attribution map?

## Architecture Onboarding

- Component map: Input sequence x -> Reference library (30 low-scoring sequences per class) -> Contrast engine (Aℓ_i - Rℓ_i) ⊙ ∂fc/∂Aℓ_i -> Deletion test refinement -> Aggregated attribution map I(x)

- Critical path: 1) Load reference library (one-time; must match model architecture) 2) Forward pass to collect Aℓ and ˆαℓ across all L layers 3) Backward pass to compute ∂fc/∂Aℓ_i 4) Contrast with 30 references → 30 candidate maps 5) Deletion test each map → filter → average survivors

- Design tradeoffs:
  - Reference count: 30 used; diminishing returns after 25 but storage scales linearly with class count × reference count × activation size
  - γ threshold: Smaller γ (0.001) improves quality but reduces eligible references; may fail for rare classes
  - Deletion threshold: mean + std is conservative; lower thresholds retain more maps but risk noise
  - Layer scope: All layers optimal but increases compute; 3+ layers gives most gains

- Failure signatures:
  - Same-class references: Using references from target class destroys contrast effect (AOPC drops from 0.703 to 0.144 on Amazon)
  - Missing reference library: Must fall back to on-the-fly sampling; ×2.24 slower, slight quality variance
  - Incompatible model architecture: Reference activations must match layer dimensions; swapping models requires rebuilding library

- First 3 experiments:
  1. Validate contrast effect: Run Contrast-CAT with 'Same', 'Random', and 'Contrasting' reference modes on a held-out set; confirm 'Contrasting' > 'Random' > 'Same' per Table 2
  2. Layer ablation: Compare attribution quality (AOPC/LOdds) using penultimate layer only vs. all layers; expect ~1.5× improvement (Figure 5A)
  3. Reference count sweep: Test k ∈ {1, 5, 10, 20, 30} references; verify saturation around 25-30 (Figure 5B) and measure storage/compute tradeoff for your deployment context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What lower-cost alternative representations can replace the pre-built reference library while maintaining Contrast-CAT's attribution quality?
- Basis in paper: The authors state in the conclusion: "Future work will explore lower-cost alternative tensors," acknowledging that storage requirements grow with the number of classes and activation size.
- Why unresolved: The current approach requires caching reference activations for all classes, which becomes costly for large-scale applications with many classes or high-dimensional activations.
- What evidence would resolve it: A systematic comparison of alternative representations (e.g., learned prototypes, compressed embeddings, synthetic references) against the current reference library approach, measuring both attribution quality (AOPC, LOdds) and storage/memory efficiency.

### Open Question 2
- Question: How does Contrast-CAT perform on multi-class classification tasks with more than two classes?
- Basis in paper: All experiments use binary sentiment classification datasets. The method's reliance on contrasting with "non-target class" references raises questions about scaling to settings where multiple plausible alternative classes exist.
- Why unresolved: The reference selection criterion (fc(r) < γ) becomes more complex when multiple non-target classes could serve as contrast sources, and it's unclear whether references should be drawn from all non-target classes or selectively chosen.
- What evidence would resolve it: Experiments on multi-class text classification benchmarks (e.g., AG News with 4 classes, topic classification datasets) comparing different reference pooling strategies across multiple non-target classes.

### Open Question 3
- Question: Can Contrast-CAT generalize to transformer-based tasks beyond text classification, such as question answering or sequence labeling?
- Basis in paper: Appendix J discusses: "Contrast-CAT has the potential to generalize beyond this setting... applying Contrast-CAT to new tasks or modalities will likely require task-specific adaptations."
- Why unresolved: The method operates on token-level attributions for classification outputs. Tasks with different output structures (e.g., span extraction for QA, token-level predictions for NER) may require modified formulations for both the attribution target and reference selection.
- What evidence would resolve it: Adaptation of Contrast-CAT to tasks like extractive question answering (SQuAD) or named entity recognition, with appropriate modifications to handle structured outputs, evaluated against task-specific interpretability metrics.

## Limitations
- Reference library dependency: Method requires pre-computed reference activations, creating storage overhead that scales with class count and model size
- Binary classification focus: All experiments use binary sentiment tasks; performance on multi-class problems remains untested
- Model architecture specificity: Reference library must match model architecture exactly; swapping models requires rebuilding library

## Confidence

- **High confidence**: The core technical contribution (contrast-based activation refinement) is clearly specified and mathematically sound. The ablation studies demonstrating the superiority of "Contrasting" over "Same" and "Random" references are robust and well-documented.

- **Medium confidence**: The multi-layer aggregation benefits and the deletion test refinement mechanism show strong empirical support, but the theoretical justification for why these specifically work (rather than being artifacts of the particular datasets or models tested) is limited.

- **Low confidence**: The claim that Contrast-CAT works across "various datasets and models" is overstated given the narrow experimental scope (4 datasets, 4 model variants). The storage and computational costs for maintaining reference libraries across many classes or model architectures are not fully explored.

## Next Checks

1. **Reference library sensitivity analysis**: Systematically vary γ (0.001 → 0.01 → 0.1) and reference count (1 → 5 → 10 → 20 → 30) to map the exact tradeoff between reference quality, storage requirements, and attribution quality. Test edge cases where few references meet the γ threshold.

2. **Deletion test validation**: Compare deletion scores against an independent ground-truth attribution dataset (e.g., human-annotated token importance for sentiment) to verify that higher deletion scores actually correlate with human judgment of importance rather than just model behavior.

3. **Cross-model transferability**: Test whether reference libraries trained on one model variant (e.g., BERT-base) can be effectively used with similar architectures (e.g., BERT-large, RoBERTa-base) without significant quality degradation, or if libraries must be rebuilt for each model variant.