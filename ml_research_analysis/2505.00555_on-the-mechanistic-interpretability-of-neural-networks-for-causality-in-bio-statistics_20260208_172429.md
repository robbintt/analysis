---
ver: rpa2
title: On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics
arxiv_id: '2505.00555'
source_url: https://arxiv.org/abs/2505.00555
tags:
- causal
- data
- input
- learning
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mechanistic Interpretability (MI) was applied to neural networks
  within Targeted Learning (TL) for causal inference in bio-statistics. The study
  aimed to validate and understand NN-based nuisance function estimators (e.g., propensity
  scores and outcome models) in TMLE, and to visualize computational pathways for
  treatment and confounder variables.
---

# On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics

## Quick Facts
- **arXiv ID:** 2505.00555
- **Source URL:** https://arxiv.org/abs/2505.00555
- **Reference count:** 0
- **One-line primary result:** MI techniques successfully validated and explained neural network nuisance function estimators in TMLE for causal inference, identifying confounders and confirming their causal impact on ATE estimates.

## Executive Summary
This study demonstrates the application of Mechanistic Interpretability (MI) to neural networks used within Targeted Learning (TL) for causal inference in bio-statistics. The research validates and explains NN-based nuisance function estimators (propensity scores and outcome models) in TMLE, aiming to improve trust in these models for high-stakes bio-statistical analysis. MI techniques—probing, ablation, and causal tracing—successfully identified internal representations of confounders and confirmed their causal impact on model predictions and final ATE estimates. Pathway analysis revealed distinct, sometimes overlapping, processing routes for different inputs, enhancing model transparency and demonstrating MI's feasibility in this domain.

## Method Summary
The study applied MI techniques to analyze neural networks estimating nuisance functions (propensity scores and outcome models) within the TMLE framework for causal inference. Experiments used synthetic datasets with known causal structures, where a multi-task feed-forward neural network learned both outcome and propensity functions. The MI analysis involved training linear probes on hidden layer activations to detect confounder representations, ablating important neurons to verify causal reliance on these representations, and using causal tracing to map computational pathways for different input variables. The methodology aimed to validate that the network correctly captured causal information and to understand how this information flows through the model to affect final ATE estimates.

## Key Results
- Linear probes achieved high $R^2$ scores (> 0.8) for detecting confounder $W_1$ in early hidden layers, indicating strong linear encoding of causal variables.
- Ablation of neurons identified as important for confounder representation significantly impacted ATE estimates, confirming causal reliance on these representations.
- Pathway analysis revealed distinct, sometimes overlapping, computational routes for treatment and confounder variables through the network architecture.
- The study successfully demonstrated MI's feasibility in validating and explaining NN-based nuisance estimators in TMLE for causal inference.

## Why This Works (Mechanism)

### Mechanism 1: Linear Encoding of Nuisance Parameters
Neural networks estimating nuisance functions appear to encode causal variables (confounders) in linearly accessible subspaces within hidden layers. A linear probe trained on hidden layer activations to predict a confounder achieves high $R^2$, suggesting the information is sufficiently disentangled for linear recovery. This allows researchers to verify if the "right" variables are being captured by the model. The core assumption is that the confounder is represented in a sufficiently disentangled manner that a simple regression can detect it. Evidence shows high probe accuracy indicating strong and linear representation of confounders. Break condition occurs if the network uses highly non-linear, distributed representations where confounder information is spread across neurons in non-linear combinations.

### Mechanism 2: Causal Reliance Verification via Ablation
Identifying a representation via probing is necessary but insufficient; interventional ablation confirms that the model causally relies on this representation for the final ATE. Neurons identified as important for representing a confounder are zeroed out, and if the final ATE estimate shifts significantly, it confirms those specific neurons were mechanistically involved in the causal adjustment calculation. The core assumption is that the drop in performance or shift in estimate is directly attributable to the loss of specific causal information encoded in those neurons. Evidence shows ablating important neurons had a noticeable impact on model performance while ablating least important neurons had considerably less effect. Break condition occurs if the model is robust or highly redundant, where ablating specific neurons doesn't change the output because information is encoded elsewhere.

### Mechanism 3: Input-Specific Computational Pathways
Distinct input variables (e.g., Treatment vs. Confounder) are processed via distinct, partially non-overlapping "circuits" or pathways in the network architecture. Causal tracing from specific input neurons propagates forward, and comparing activated downstream neurons for different inputs using Jaccard Index reveals if the network separates processing of treatment effects from confounding adjustments. The core assumption is that the network learns modular features rather than a fully monolithic transformation of all inputs. Evidence shows pathway analysis revealed distinct processing routes for different inputs. Break condition occurs if the model is a "black box" with dense layers where every input influences every neuron, causing pathways to appear identical or fully overlapping.

## Foundational Learning

- **Concept: Targeted Learning / TMLE**
  - Why needed here: The ultimate goal is not just prediction but causal inference (ATE estimation). The NN is merely a component (nuisance estimator) within the larger TMLE framework. You must understand what the NN is supposed to estimate ($g$ and $Q$) to interpret MI results correctly.
  - Quick check question: In the context of this paper, does the Neural Network output the final causal effect directly? (Answer: No, it estimates nuisance functions used to calculate the ATE).

- **Concept: Probing vs. Ablation (Observational vs. Interventional)**
  - Why needed here: The paper explicitly distinguishes between "what is encoded" (Probing) and "what is used" (Ablation). Confusing these leads to incorrect conclusions about model behavior.
  - Quick check question: If a probe detects "Age" in a hidden layer, does that prove the model uses "Age" to determine the outcome? (Answer: No, you need an interventional technique like ablation to confirm causal reliance).

- **Concept: Nuisance Parameters ($g$ and $Q$)**
  - Why needed here: The experiments analyze a multi-task network with two heads. Understanding that $Q$ represents the Outcome Model and $g$ represents the Propensity Score is required to interpret the "Pathway" visualizations.
  - Quick check question: Which "head" of the multi-task network is responsible for modeling the probability of receiving treatment? (Answer: The $g$-head/Propensity Score head).

## Architecture Onboarding

- **Component map:** Input Covariates $W$ and Treatment $A$ → Shared dense layers (5 layers, 100 units) extracting features $h_{shared}$ → Bifurcation to Task Heads: $f_Q$ Head (Outcome, MSE Loss) and $f_g$ Head (Propensity, BCE Loss)
- **Critical path:** Raw Input → Shared Encoder → (Bifurcation) → Task Heads. The MI analysis happens primarily at the Shared Encoder level to see if confounders are preserved and how they diverge to the heads.
- **Design tradeoffs:**
  - Shared vs. Separate Representations: A shared encoder allows for efficient learning of confounders but risks "leaking" information between treatment and outcome tasks in ways that might obscure individual causal mechanisms. The paper analyzes this via "pathway overlap."
  - Synthetic Data: The paper relies on synthetic datasets (DS1, DS2) with known ground truths (e.g., "Strong Confounder"). This is a tradeoff between tractability/verifiability and real-world applicability.
- **Failure signatures:**
  - High Probe / Low Ablation: The model "knows" the confounder (high probe $R^2$) but ignores it (ablation does nothing). This indicates the model is using spurious correlations.
  - Uniform Pathway Overlap: The heatmap shows high Jaccard indices everywhere. The network has failed to modularize; it is treating treatment and confounders as a single entangled blob.
- **First 3 experiments:**
  1. Reproduce Probe Accuracy: Train the $f_{Q,g}$ network on the "Strong Confounder" dataset and fit linear probes on the hidden layers. Verify that $R^2$ for the confounder $W_1$ is high (e.g., > 0.8) in early layers.
  2. Run Ablation Sensitivity: Ablate the top 10% of neurons identified by the probe. Compare the shift in the final ATE estimate against a baseline where random neurons are ablated. Confirm that the "important" neurons cause a significant deviation.
  3. Visualize Pathway Overlap: Perform causal tracing from two different input neurons (one confounder, one noise). Plot the Jaccard index of their activated paths. Check if they are distinct (validating the "distinct processing routes" claim).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can mechanistic interpretability findings be formally integrated into statistical inference procedures to refine causal estimates like TMLE?
- Basis in paper: [Explicit] The conclusion states that "Formalizing the integration of MI findings... directly into statistical inference procedures like refining TMLE estimates... presents a crucial next step."
- Why unresolved: The study validates the feasibility of MI for validating nuisance estimators but stops short of deriving quantitative adjustments for bias or confidence intervals.
- What evidence would resolve it: A theoretical framework where identified causal pathways or circuits directly inform the targeting step or variance estimation within the TMLE algorithm.

### Open Question 2
- Question: What metrics can rigorously evaluate the quality and faithfulness of mechanistic interpretability explanations in bio-statistics?
- Basis in paper: [Explicit] Section 4.5 identifies "defining clear metrics for the quality and faithfulness of MI explanations in this specific domain" as an ongoing task.
- Why unresolved: Complex MI outputs (e.g., high-dimensional SAE features) risk being misinterpreted or remaining as opaque as the original model.
- What evidence would resolve it: The development of quantitative metrics that correlate explanation fidelity with statistical robustness and domain expert validation.

### Open Question 3
- Question: Can mechanistic interpretability techniques robustly disentangle representations in neural networks trained on real-world biological systems?
- Basis in paper: [Inferred] Section 4.5 discusses "complexity mismatch" and entangled representations, while Chapter 5 relies exclusively on synthetic datasets with known ground truths.
- Why unresolved: Real biological systems exhibit intricate feedback loops and dependencies that may not align with the modular structures assumed by current MI analysis tools.
- What evidence would resolve it: Successful application of MI (e.g., causal tracing) to high-dimensional clinical data to recover known biological mechanisms without relying on synthetic validation.

## Limitations

- The study relies exclusively on synthetic datasets with known ground truths, limiting generalization to real-world bio-statistical data with complex, entangled causal structures.
- The exact synthetic data generation process (coefficients, noise parameters) is not fully specified, which could affect the strength of confounding signals that MI techniques detect.
- Pathway analysis methodology lacks precise thresholds for determining "significant" activation propagation in causal tracing, potentially affecting reproducibility.

## Confidence

- **High Confidence:** The feasibility of using MI techniques (probing, ablation, causal tracing) to analyze neural networks in causal inference frameworks is well-supported by the experimental results on synthetic data.
- **Medium Confidence:** The interpretation that distinct input-specific computational pathways exist is supported by pathway overlap analysis, but the practical significance and generalizability of these modular circuits require further validation.
- **Medium Confidence:** The conclusion that MI can improve trust in NNs for high-stakes causal bio-statistical analysis is plausible but requires demonstration beyond synthetic datasets.

## Next Checks

1. **Real-World Dataset Validation:** Apply the same MI pipeline to a real bio-statistical dataset with known causal structure (e.g., from clinical trials) to verify if confounding variables are detected and processed similarly.
2. **Robustness to Model Architecture:** Test if the MI findings (linear encoding, distinct pathways) hold when using different neural network architectures (e.g., residual connections, attention mechanisms) or when the shared encoder is replaced with separate task-specific encoders.
3. **Ablation Impact on Downstream Decisions:** Beyond measuring ATE deviation, analyze if ablating key neurons changes the clinical decision-making (e.g., treatment recommendation) derived from the causal estimate, to assess practical impact on model trustworthiness.