---
ver: rpa2
title: 'The KoLMogorov Test: Compression by Code Generation'
arxiv_id: '2503.13992'
source_url: https://arxiv.org/abs/2503.13992
tags:
- sequence
- data
- sequences
- programs
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Kolmogorov Test (KT) is a new benchmark for evaluating the
  reasoning capabilities of code-generating language models through data compression.
  KT challenges models to generate the shortest program that outputs a given data
  sequence, measuring their ability to identify patterns and compress information.
---

# The KoLMogorov Test: Compression by Code Generation

## Quick Facts
- arXiv ID: 2503.13992
- Source URL: https://arxiv.org/abs/2503.13992
- Authors: Ori Yoran; Kunhao Zheng; Fabian Gloeckle; Jonas Gehring; Gabriel Synnaeve; Taco Cohen
- Reference count: 40
- Key outcome: Current flagship models like GPT-4 and Llama-3.1-405B perform poorly on KT, struggling to generate correct programs even for synthetic sequences designed to follow simple compositional patterns.

## Executive Summary
The KoLMogorov Test (KT) introduces a new benchmark for evaluating code-generating language models through data compression. The task requires models to generate the shortest program that outputs a given data sequence, measuring their ability to identify patterns and compress information. Experiments show that while smaller specialized models trained on synthetic data can outperform classical compression algorithms like GZIP on structured sequences, they fail to generalize to real data modalities. Current models, including both prompted flagship LLMs and fine-tuned specialized models, struggle to generate correct programs even for sequences designed to follow simple compositional patterns, suggesting that KT represents an extremely challenging benchmark that current approaches cannot effectively solve.

## Method Summary
The KoLMogorov Test challenges models to generate executable programs that output given sequences, with compression efficiency measured by program bit-length versus original sequence length. The approach uses a compositional DSL to generate synthetic training data (1M program-sequence pairs) and evaluates models on both synthetic and natural sequences (audio, text, DNA). Programs are encoded using uniform priors over the DSL or GZIP for prompted models. The evaluation measures Accuracy (exact output match), CompressionRate (relative size), and Precision (program efficiency). Models tested include zero-shot prompted LLaMA-3.1/GPT-4o and fine-tuned SeqCoder-1.5B/8B transformers trained on synthetic pairs.

## Key Results
- Flagship models (GPT-4, Llama-3.1-405B) fail to generate correct programs even for synthetic sequences with simple compositional patterns
- SeqCoder models trained on 1M synthetic pairs outperform classical compression algorithms on synthetic data but fail completely on natural sequences
- Program accuracy decreases significantly with sequence length, with models making errors within the first 16 characters on natural data
- Models often resort to repetition fallback, hardcoding sequences rather than identifying compositional patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating short programs that reproduce sequences can outperform classical compression algorithms when appropriate priors and training data are available.
- Mechanism: The model learns to identify patterns (repetitions, ranges, compositions) in input sequences and generates executable programs that reconstruct those sequences more compactly than frequency-based compression methods.
- Core assumption: Programs capturing algorithmic structure compress better than statistical methods for structured sequences.
- Evidence anchors: [abstract] "models trained on synthetic program-sequence pairs outperform previous compression algorithms like GZIP"; [section 5.2] "models can be trained to outperform previous compression algorithms, reaching a CompressionRate of 0.38 for SEQCODER-8B"; [corpus] Related work on "Single-pass Adaptive Image Tokenization for Minimum Program Search" similarly connects Kolmogorov complexity to program-based compression.

### Mechanism 2
- Claim: The choice of prior over programs significantly affects compression performance; uniform priors over structured DSLs can outperform general-purpose compression priors.
- Mechanism: Encoding cost = cost of function selection + cost of parameters. A well-designed DSL with uniform prior reduces bits needed to encode common operations (e.g., "range" costs ~log(|functions|) + log parameters vs. GZIP treating Python syntax as arbitrary text).
- Core assumption: The DSL's function vocabulary aligns with patterns actually present in target sequences.
- Evidence anchors: [section 3.3] "A sub-class of our modifiers includes mathematical operations, such as scan-adding the elements in the sequence or applying a modulo operation"; [section 5.3/appendix B.2] "The same programs achieve a compression rate of 5.26 and 0.59 with raw and GZIP encoding, showing that strong priors are needed for good CompressionRate"; [corpus] Weak corpus evidence—no direct comparisons of prior design in neighbor papers.

### Mechanism 3
- Claim: Synthetic training on program-sequence pairs improves compression but generalizes poorly to natural data.
- Mechanism: Training on DSL-generated pairs teaches model to recognize DSL-specific patterns. Natural data (audio, DNA, text) exhibits different statistical and structural properties not captured by synthetic distribution.
- Core assumption: Compositional structure in synthetic data overlaps with natural structure enough for transfer.
- Evidence anchors: [section 5.1] "SEQCODER models significantly outperform all CODELMS on synthetic data... it reaches near-zero performance on natural sequences"; [section 5.2] "models trained on more examples are significantly better on short sequences, but all models struggle on longer ones"; [corpus] "Structured Program Synthesis using LLMs" notes similar synthetic-to-real transfer challenges in IPARC tasks.

## Foundational Learning

- Concept: **Kolmogorov Complexity**
  - Why needed here: The theoretical foundation defining optimal compression as the shortest program producing sequence x; uncomputable but approximable.
  - Quick check question: Why can't we directly compute Kolmogorov complexity for arbitrary sequences?

- Concept: **Arithmetic Coding with Priors**
  - Why needed here: Used to convert model-assigned probabilities into bit-lengths; compression cost ≈ -log₂ p(sequence|program) - log₂ p(program|prior).
  - Quick check question: How does the choice of prior p(program) affect achievable compression rate?

- Concept: **Domain-Specific Languages (DSLs) for Program Synthesis**
  - Why needed here: The paper constructs a compositional DSL to generate synthetic training data with known program-sequence mappings.
  - Quick check question: What properties should a DSL have to support both sampling and compression?

## Architecture Onboarding

- Component map: DSL -> Synthetic Data Generator -> Model (SeqCoder) -> Program Execution -> Metrics (Accuracy, CompressionRate, Precision) -> Evaluation
- Critical path: DSL design → synthetic data sampling → model fine-tuning → evaluation on synthetic + natural sequences
- Design tradeoffs:
  - DSL expressiveness vs. encoding efficiency: More functions = more patterns but higher function-selection cost
  - Training data scale vs. generalization: 1M pairs improve synthetic performance but don't fix natural-data transfer
  - Model size vs. inference cost: 8B outperforms 1.5B but requires more compute
- Failure signatures:
  - Repetition fallback: Model outputs input as hard-coded list (high Precision >1.0)
  - Early errors: 88%+ of programs err within first 16 characters on natural sequences
  - Pattern hallucination: Model identifies patterns that don't match input (execution mismatch)
- First 3 experiments:
  1. Establish baseline: Run GZIP and LMIC on your target data modality (128-byte sequences) to get CompressionRate baselines.
  2. Zero-shot prompting: Test existing code LLMs (e.g., Llama-3.1-8B) with the paper's prompt (Figure 9) to measure Accuracy and Precision gaps.
  3. DSL validation: Implement the DSL from Appendix A.2, sample 10K program-sequence pairs, verify uniform prior encoding matches claimed bit-lengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can methods that tilt the synthetic training distribution toward real data distributions (e.g., via domain adaptation or filtering easy-to-detect synthetic sequences) significantly improve generalization to natural data?
- Basis in paper: [explicit] "Future work can experiment with methods that tilt the synthetic distribution towards the real one, e.g., by filtering easy-to-detect synthetic sequences."
- Why unresolved: Models trained on synthetic data show near-zero performance on natural sequences despite outperforming baselines on synthetic data.
- What evidence would resolve it: Demonstration of improved accuracy on natural data (audio, text, DNA) from models trained on modified synthetic distributions, with statistically significant gains over current SEQCODER models.

### Open Question 2
- Question: Can reinforcement learning approaches that learn from reward signals when correct solutions are generated, with a bias toward shorter programs, outperform supervised learning on the KoLMogorov Test?
- Basis in paper: [explicit] "An exciting direction for future research is to model KT as a reinforcement learning task and learn from a reward when correct solutions are generated, with a bias towards shorter solutions."
- Why unresolved: Current supervised training on synthetic program-sequence pairs shows limited generalization; RL might enable discovery of novel compression strategies beyond the training distribution.
- What evidence would resolve it: Comparison of RL-trained models versus supervised SEQCODER on both synthetic and natural data, measuring compression rate, accuracy, and program novelty.

### Open Question 3
- Question: How does the choice of programming language or DSL affect model performance, training progress rate, and generalization to out-of-distribution sequences?
- Basis in paper: [explicit] "Future work could explore how the choice of language affects the performance of models, the rate of training progress, and the degree of generalization to OOD sequences."
- Why unresolved: Only Python and a custom DSL were tested; lower-level languages might enable stronger priors and different generalization properties.
- What evidence would resolve it: Systematic comparison across multiple languages (e.g., Python, DSL variants, lower-level representations) with controlled experiments measuring learning curves and OOD generalization.

### Open Question 4
- Question: Can curriculum learning approaches that progressively increase problem complexity enable better scaling to longer sequences and improved performance on the KoLMogorov Test?
- Basis in paper: [explicit] "Additionally, future methods can benefit from curriculum-learning where models learn simpler examples before more complex ones."
- Why unresolved: All current models perform poorly on longer sequences (64+ bytes); accuracy decreases with program and sequence length even for trained models.
- What evidence would resolve it: Training curves and final performance metrics for curriculum-learned models versus standard training, specifically evaluating on progressively longer sequences up to 1024 bytes.

## Limitations

- Synthetic-to-natural transfer gap: Models trained on synthetic data outperform classical compression on synthetic sequences but fail completely on natural data modalities
- Evaluation metric sensitivity: CompressionRate varies dramatically based on encoding choices, raising questions about metric validity
- Resource requirements: Best-performing models require substantial computational resources for training and inference

## Confidence

- High Confidence: The core claim that current LLMs struggle with the KoLMogorov Test is well-supported by experimental evidence across multiple model sizes and architectures.
- Medium Confidence: The claim that synthetic training improves compression on synthetic data but not natural data is supported by experiments, but the analysis of why this transfer fails is limited.
- Low Confidence: The assertion that this benchmark represents a "grand challenge" requiring "new innovations" is somewhat speculative.

## Next Checks

1. Cross-DSL Generalization Test: Implement an alternative DSL with different compositional primitives and evaluate whether models trained on the original DSL can transfer knowledge to programs generated by the new DSL.

2. Encoding Scheme Ablation: Systematically vary the uniform prior encoding (function selection probability, parameter encoding) and measure impact on CompressionRate across all baselines.

3. Natural Data Structure Analysis: Perform detailed analysis of natural sequences (LibriSpeech, enwik9, GRCh38) to identify whether they contain any compositional patterns that current DSLs could capture.