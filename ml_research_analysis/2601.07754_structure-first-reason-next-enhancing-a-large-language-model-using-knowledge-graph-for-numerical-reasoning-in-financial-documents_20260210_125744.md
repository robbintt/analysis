---
ver: rpa2
title: 'Structure First, Reason Next: Enhancing a Large Language Model using Knowledge
  Graph for Numerical Reasoning in Financial Documents'
arxiv_id: '2601.07754'
source_url: https://arxiv.org/abs/2601.07754
tags:
- financial
- reasoning
- numerical
- llms
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework that enhances numerical reasoning
  in financial documents by incorporating knowledge graphs (KGs) alongside large language
  models (LLMs). The framework addresses the challenge of LLMs struggling with numerical
  data in financial texts by extracting structured information from documents into
  KGs using a predefined schema, then using this structured information to improve
  reasoning performance.
---

# Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents

## Quick Facts
- **arXiv ID:** 2601.07754
- **Source URL:** https://arxiv.org/abs/2601.07754
- **Reference count:** 3
- **Key result:** 12.3% relative improvement in execution accuracy (from 51.93% to 58.34%) on FinQA benchmark using Llama 3.1 8B Instruct

## Executive Summary
This paper proposes a framework that enhances numerical reasoning in financial documents by incorporating knowledge graphs (KGs) alongside large language models (LLMs). The framework addresses the challenge of LLMs struggling with numerical data in financial texts by extracting structured information from documents into KGs using a predefined schema, then using this structured information to improve reasoning performance. Evaluated on the FinQA benchmark, the approach achieves a 12.3% relative improvement in execution accuracy compared to the vanilla LLM, demonstrating the effectiveness of combining structured knowledge representation with LLM reasoning capabilities.

## Method Summary
The framework processes financial documents through three main stages: document preprocessing, KG extraction, and retrieval-enhanced reasoning. First, documents are preprocessed by linearizing tables into templated sentences and normalizing text format. Then, a predefined schema-based KG is extracted from the document using few-shot prompting with Llama 3.1 8B Instruct, capturing triplets with financial metrics, entities, periods, values, and units. For reasoning, a 2-layer MLP classifier filters relevant triplets using a combination of semantic embeddings and structural features (subject type, relation type, temporal distance), before passing the filtered KG to the LLM for final numerical reasoning. The approach addresses the limitations of vanilla LLMs in handling numerical data and temporal disambiguation in financial documents.

## Key Results
- 12.3% relative improvement in execution accuracy (from 51.93% to 58.34%) on FinQA benchmark
- Framework successfully addresses LLM limitations in numerical reasoning over financial documents
- KG-based approach effectively handles temporal disambiguation compared to pure semantic retrieval

## Why This Works (Mechanism)

### Mechanism 1
Structured knowledge graph extraction preserves temporal and entity relationships that text linearization destroys. The framework extracts triplets with explicit period attributes (e.g., "HAS_VALUE_IN_2015") and entity types, enabling precise filtering when queries specify time periods or metric categories. This bypasses the ambiguity of semantic similarity matching on linearized text. Core assumption: Financial queries frequently require disambiguating between similar metrics across different time periods or companies. Break condition: If documents contain primarily single-period data with unique metric names, temporal disambiguation provides diminishing returns.

### Mechanism 2
Lightweight MLP classification of triplet relevance outperforms pure semantic retrieval for numerical reasoning. The retriever combines semantic embeddings (question + triplet + cosine similarity) with structural features (subject type, relation type, temporal distance) in a 2-layer MLP classifier. This hybrid approach filters irrelevant triplets before LLM reasoning, reducing context noise. Core assumption: Engineered structural features capture domain-specific relevance signals that semantic embeddings alone miss. Break condition: If the training data for the MLP classifier is insufficient or biased toward certain query types, filtering may systematically exclude relevant triplets for underrepresented patterns.

### Mechanism 3
Few-shot prompting with a constrained schema reduces extraction errors compared to open-ended KG construction. The predefined schema restricts triplet format to (subject, relation, object, {attributes}) with specific entity types, enforcing exact text extraction, standardized period formats, and liberal coverage. Core assumption: Financial documents share sufficient structural regularity that a single schema can capture most numerical facts. Break condition: If financial documents use heterogeneous terminology or non-standard period formats not covered by the schema, extraction will fail or produce incomplete KGs.

## Foundational Learning

- **Concept: Knowledge Graph Triplets**
  - Why needed here: The entire framework depends on understanding how (subject, relation, object) structures encode facts and how attributes extend this for numerical data.
  - Quick check question: Given "Revenue was $5.8B in 2023," what subject, relation, object, and attributes would this schema produce?

- **Concept: Few-Shot Prompting**
  - Why needed here: KG extraction uses few-shot prompts to guide LLM extraction behavior without fine-tuning. Understanding prompt engineering is critical for schema compliance.
  - Quick check question: How would adding an example triplet for a ratio metric (e.g., "debt-to-equity ratio") change extraction behavior on new documents?

- **Concept: Hybrid Retrieval (Semantic + Structural Features)**
  - Why needed here: The MLP classifier combines embeddings with engineered features. Understanding why pure semantic similarity fails for numerical disambiguation is essential.
  - Quick check question: Why might cosine similarity between "2020 revenue" and "$100M in 2020 revenue" be high, yet still retrieve the wrong triplet for "2020 expenses"?

## Architecture Onboarding

- **Component map:** Preprocessor → linearized text → KG Extractor (Llama 3.1 8B) → triplets following financial schema → Retriever (MLP classifier) → filtered triplets → Reasoner (Llama 3.1 8B) → predicted numerical answer

- **Critical path:** KG extraction quality → retrieval precision → reasoning accuracy. If extraction misses key facts or introduces errors, downstream components cannot recover.

- **Design tradeoffs:**
  - Schema specificity vs. coverage: Tight constraints reduce errors but may miss edge cases; liberal extraction increases coverage but introduces noise.
  - MLP simplicity vs. expressiveness: 2-layer MLP is fast and interpretable but may underfit complex relevance patterns.
  - Single-LLM dependency: Using Llama for both extraction and reasoning simplifies deployment but creates error propagation risk.

- **Failure signatures:**
  - Low execution accuracy on queries requiring metrics not in schema → schema gap
  - High variance across similar queries → retrieval inconsistency
  - Systematic errors on specific period formats (e.g., fiscal years vs. calendar years) → extraction rule gaps

- **First 3 experiments:**
  1. Schema coverage audit: Run KG extraction on 50 diverse FinQA documents; manually verify what percentage of ground-truth relevant facts appear in extracted triplets.
  2. Retrieval ablation: Compare MLP classifier vs. pure cosine similarity retrieval on held-out queries; quantify precision@k for each.
  3. Error taxonomy: Analyze 100 failed predictions; categorize by failure mode (extraction miss, retrieval error, reasoning error) to identify bottleneck.

## Open Questions the Paper Calls Out
1. Can the performance gains observed on FinQA be replicated across diverse financial reasoning benchmarks that feature different document structures and complexity?
2. How does the framework's performance scale when applied to larger open-source LLMs or different model architectures?
3. To what extent does the predefined KG schema limit the framework's applicability to datasets with high variance in entity and relationship types?

## Limitations
- Evaluation restricted to FinQA benchmark; generalizability to other financial QA datasets unproven
- 8B parameter LLM constraint; performance with larger models or different architectures untested
- Predefined schema may not capture all entity and relationship types across diverse financial datasets

## Confidence
- **High confidence** in the overall framework design pattern as a plausible approach for numerical reasoning
- **Medium confidence** in the 12.3% improvement claim, as execution accuracy improvements are reported but not accompanied by detailed ablation studies
- **Low confidence** in the specific architectural choices without implementation details and hyperparameter specifications

## Next Checks
1. Schema coverage audit: Process 50 diverse FinQA documents through the KG extraction pipeline and manually verify what percentage of ground-truth relevant facts appear in extracted triplets.
2. Retrieval ablation study: Compare the proposed MLP classifier against pure cosine similarity retrieval on held-out validation queries. Measure precision@k for each method to quantify the hybrid approach's contribution.
3. Error mode classification: Analyze 100 failed predictions from the complete pipeline. Categorize failures into extraction misses, retrieval errors, and reasoning errors to identify the primary bottleneck.