---
ver: rpa2
title: Assured Autonomy with Neuro-Symbolic Perception
arxiv_id: '2505.21322'
source_url: https://arxiv.org/abs/2505.21322
tags:
- scene
- graph
- perception
- data
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neuro-symbolic perception framework to address
  security vulnerabilities in deep learning models used in autonomous systems. By
  integrating scene graph generation (SGG) with symbolic reasoning, the approach aims
  to detect adversarial attacks that manipulate sensor data while remaining stealthy
  to traditional detection methods.
---

# Assured Autonomy with Neuro-Symbolic Perception

## Quick Facts
- arXiv ID: 2505.21322
- Source URL: https://arxiv.org/abs/2505.21322
- Reference count: 16
- Primary result: Neuro-symbolic framework detects adversarial attacks missed by traditional methods

## Executive Summary
This paper addresses security vulnerabilities in deep learning models used in autonomous systems by proposing a neuro-symbolic perception framework. The approach integrates scene graph generation with symbolic reasoning to detect adversarial attacks that manipulate sensor data while remaining stealthy to conventional detection methods. By combining foundation models for graph generation with rule-based approaches for LiDAR data, followed by integrity evaluation against physics-based knowledge bases, the framework identifies inconsistencies in semantic relationships across sensors.

## Method Summary
The framework combines scene graph generation (SGG) with symbolic reasoning to create a robust perception system for autonomous vehicles. It uses foundation models to generate scene graphs from camera images and rule-based approaches for LiDAR data processing. These components feed into a symbolic reasoning engine that evaluates the integrity of the perception data against a physics-based knowledge base. The system is designed to detect adversarial attacks that would otherwise remain undetected by traditional methods, particularly those that manipulate sensor data in subtle ways that preserve apparent functionality while introducing dangerous inconsistencies.

## Key Results
- SGG successfully detects previously undetectable attacks, including frustum attacks
- Framework demonstrates effectiveness on both CARLA and nuScenes datasets
- System identifies inconsistencies in semantic relationships across multiple sensor types
- Approach provides improved robustness against adversarial manipulation compared to traditional methods

## Why This Works (Mechanism)
The neuro-symbolic approach works by creating a multi-layered validation system where perception data is cross-checked against both learned representations (scene graphs) and explicit symbolic knowledge (physics rules). This dual validation catches attacks that would slip past single-modality detection because adversaries must now fool both the neural network's learned patterns and the logical consistency checks. The symbolic reasoning layer acts as a fail-safe, catching semantic inconsistencies that deep learning models might otherwise miss.

## Foundational Learning
- Scene Graph Generation (SGG): Converts raw sensor data into structured semantic representations - why needed: provides interpretable intermediate representation for analysis - quick check: verify graph quality on benchmark datasets
- Symbolic Reasoning: Applies logical rules to validate semantic consistency - why needed: catches attacks that exploit statistical learning blind spots - quick check: test rule coverage against attack scenarios
- Physics-based Knowledge Bases: Encodes domain-specific constraints and relationships - why needed: grounds perception in physical reality - quick check: validate completeness of physical constraints
- Multi-sensor Fusion: Combines information from camera and LiDAR sensors - why needed: provides redundancy and cross-validation - quick check: measure consistency across sensor modalities
- Adversarial Attack Detection: Identifies malicious manipulations of sensor data - why needed: ensures system security in hostile environments - quick check: test against known attack patterns
- Integrity Evaluation: Assesses the logical consistency of perception outputs - why needed: validates that detected objects make sense together - quick check: measure false positive/negative rates

## Architecture Onboarding
Component map: Camera Images -> SGG Foundation Model -> Scene Graph A
LiDAR Data -> Rule-based Processor -> Scene Graph B
Scene Graph A + Scene Graph B -> Symbolic Reasoning Engine -> Integrity Check -> Perception Output

Critical path: Sensor Data → Scene Graph Generation → Symbolic Reasoning → Integrity Evaluation → Final Perception

Design tradeoffs: The framework balances computational complexity against detection accuracy. Using both camera and LiDAR data increases detection capability but also increases computational overhead. The choice between more complex scene graph models versus faster rule-based approaches represents another key tradeoff.

Failure signatures: System may fail when environmental conditions severely degrade sensor quality, when attack vectors exploit gaps in the physics knowledge base, or when computational constraints prevent real-time processing of complex scenes.

First experiments:
1. Test detection capability against simple object deletion attacks
2. Evaluate performance under varying weather and lighting conditions
3. Measure computational overhead compared to baseline perception systems

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Performance heavily dependent on quality of scene graph generation, which can be affected by environmental conditions
- Physics-based knowledge base may not capture all possible attack vectors or environmental interactions
- Limited scope of tested attacks and specific environmental conditions warrant cautious interpretation

## Confidence
The core claims about neuro-symbolic detection effectiveness are Medium confidence. The methodology is sound and experimental results are promising, but the limited attack scope and specific environmental conditions require broader validation before real-world deployment.

## Next Checks
1. Test the framework against a wider range of attack types, including those that manipulate both semantic and geometric features simultaneously
2. Evaluate performance across diverse environmental conditions (weather, lighting, sensor degradation)
3. Assess the computational overhead and real-time feasibility for deployment in actual autonomous systems