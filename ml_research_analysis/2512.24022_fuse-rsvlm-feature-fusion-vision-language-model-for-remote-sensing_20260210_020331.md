---
ver: rpa2
title: 'FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing'
arxiv_id: '2512.24022'
source_url: https://arxiv.org/abs/2512.24022
tags:
- image
- remote
- sensing
- visual
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MF-RSVLM, a remote sensing vision-language
  model that extracts and fuses multi-scale visual features to address the limitations
  of existing models, which often lose fine-grained details and suffer from visual
  forgetting. The proposed method employs a multi-scale feature extraction scheme
  to capture both global context and local details, and a recurrent visual feature
  injection mechanism to maintain visual grounding during deep language processing.
---

# FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing

## Quick Facts
- **arXiv ID**: 2512.24022
- **Source URL**: https://arxiv.org/abs/2512.24022
- **Reference count**: 40
- **Primary result**: MF-RSVLM achieves state-of-the-art performance across remote sensing VQA, captioning, and classification tasks

## Executive Summary
This paper introduces MF-RSVLM, a remote sensing vision-language model that extracts and fuses multi-scale visual features to address the limitations of existing models, which often lose fine-grained details and suffer from visual forgetting. The proposed method employs a multi-scale feature extraction scheme to capture both global context and local details, and a recurrent visual feature injection mechanism to maintain visual grounding during deep language processing. Extensive experiments on diverse remote sensing benchmarks demonstrate that MF-RSVLM achieves state-of-the-art or highly competitive performance across scene classification, image captioning, and visual question answering tasks, with notable improvements in tasks requiring fine-grained visual understanding.

## Method Summary
MF-RSVLM uses a two-stage training approach with a CLIP ViT-L/14@336 encoder and Vicuna-v1.5-7B LLM. The model extracts multi-scale features using sliding windows (336×336 and 168×168) with Hann-weighted overlap-add stitching, capturing details from ViT layers 8/16/24. A recurrent gated injection mechanism injects visual features at LLM layers 2/4/6/8. The model is pretrained on VersaD (1.4M QA pairs) and fine-tuned on a 293K instruction corpus from multiple remote sensing datasets. Training uses 8×A6000 GPUs for ~80 total hours across both stages.

## Key Results
- VQA accuracy of 65.76% across benchmarks
- Captioning METEOR score up to 89.47%
- Classification top-1 accuracy of 74.51%
- State-of-the-art performance on multiple remote sensing vision-language tasks

## Why This Works (Mechanism)
The multi-scale feature extraction captures both global context and local details through sliding windows with overlapping regions, while the recurrent visual injection maintains visual grounding throughout the LLM's deep layers. This addresses the visual forgetting problem where early visual information gets diluted during long language processing sequences.

## Foundational Learning
- **Multi-scale feature extraction**: Why needed - Remote sensing images contain both large-scale structures and fine-grained details; quick check - Verify overlapping windows properly stitch without artifacts
- **Recurrent visual injection**: Why needed - Prevents visual information loss during deep language processing; quick check - Confirm gated injection at layers 2/4/6/8 maintains feature consistency
- **Phase subsampling (f=2)**: Why needed - Reduces token count while preserving spatial information; quick check - Validate checkerboard pattern doesn't miss critical features
- **Hann-weighted overlap-add**: Why needed - Smooths transitions between adjacent sliding windows; quick check - Inspect stitched regions for visible seams
- **Attention-based router**: Why needed - Fuses global and detail features effectively; quick check - Verify attention weights properly balance global vs local features
- **Two-stage training**: Why needed - Pretraining establishes foundation before task-specific fine-tuning; quick check - Confirm performance gain from stage 1 to stage 2

## Architecture Onboarding

**Component Map**
CLIP ViT-L/14@336 encoder -> Multi-scale extractor (sliding windows + ViT layers 8/16/24) -> Attention router -> Gated injection -> Vicuna-v1.5-7B LLM

**Critical Path**
Image -> Multi-scale sliding windows -> ViT feature extraction (layers 8/16/24) -> Detail stack formation (phase subsampled) -> Attention-based fusion with global tokens -> Gated injection at LLM layers 2/4/6/8 -> Language generation

**Design Tradeoffs**
- Multi-scale vs single-scale: Multi-scale captures both global and local features but increases computational cost
- Recurrent vs one-time injection: Recurrent maintains visual grounding but adds complexity
- Phase subsampling (f=2) vs dense sampling: Reduces tokens but may miss contiguous information

**Failure Signatures**
- Out-of-memory during multi-scale extraction indicates excessive token count
- Degraded VQA accuracy suggests insufficient SFT epochs or poor visual grounding
- Poor localization performance indicates multi-scale fusion impairs spatial coordinate mapping

**Three First Experiments**
1. Test multi-scale extraction with single window size to isolate impact on performance
2. Run one-epoch vs two-epoch SFT comparison to verify 8% accuracy difference
3. Visualize detail stacks to confirm local features are properly preserved

## Open Questions the Paper Calls Out
- **Task-aware tiling strategy**: How can the model dynamically determine optimal tiling strategy per task to reduce computational overhead?
- **Localization task performance**: How can the multi-scale fusion be adapted to maintain spatial alignment for bounding box prediction?
- **Token-length discrepancy**: How does phase subsampling affect the model's ability to reason about global scene relationships?

## Limitations
- Underperforms on localization tasks due to impaired coordinate mapping from multi-scale fusion
- Fixed dual-window approach may be computationally redundant for simple classification tasks
- Limited ablation studies make it difficult to isolate individual component contributions

## Confidence
- **High Confidence**: Two-stage training methodology and overall architectural framework are well-documented
- **Medium Confidence**: Multi-scale feature extraction and sliding window stitching methodology are sufficiently detailed
- **Low Confidence**: Ablation studies are limited and lack detailed failure mode analysis

## Next Checks
1. **Component Ablation Study**: Reproduce model with and without recurrent visual injection to quantify specific contribution to VQA accuracy improvements
2. **Multi-scale Feature Quality Assessment**: Visualize and quantify detail stacks to verify local detail preservation without excessive token redundancy
3. **Training Stability Verification**: Conduct controlled experiments varying SFT epochs (1 vs 2) to confirm 8% VQA accuracy improvement is consistently reproducible