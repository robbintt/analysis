---
ver: rpa2
title: 'CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement
  Learning'
arxiv_id: '2502.11896'
source_url: https://arxiv.org/abs/2502.11896
tags:
- action
- masking
- policy
- camel
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAMEL introduces a framework that leverages LLM-generated suboptimal
  policies to guide reinforcement learning in continuous action spaces. By dynamically
  constraining the action space through action masking and gradually reducing reliance
  on LLM guidance via epsilon-masking, CAMEL improves exploration efficiency and mitigates
  convergence to suboptimal solutions.
---

# CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.11896
- Source URL: https://arxiv.org/abs/2502.11896
- Reference count: 1
- LLMs generate suboptimal policies to guide RL exploration in continuous action spaces, improving sample efficiency in Hopper-v4 and Ant-v4 environments

## Executive Summary
CAMEL introduces a framework that leverages LLM-generated suboptimal policies to guide reinforcement learning in continuous action spaces. By dynamically constraining the action space through action masking and gradually reducing reliance on LLM guidance via epsilon-masking, CAMEL improves exploration efficiency and mitigates convergence to suboptimal solutions. In Hopper-v4 and Ant-v4 environments, LLM-generated policies significantly enhanced sample efficiency, achieving performance comparable to or exceeding expert masking baselines. In Walker2d-v4, where LLMs struggled to model bipedal gait dynamics, CAMEL maintained robust RL performance without degradation, demonstrating adaptability across diverse tasks. The framework shows promise in addressing exploration inefficiencies and convergence challenges in RL, though further research is needed to generalize CAMEL to multimodal LLMs and automate policy evaluation.

## Method Summary
CAMEL integrates LLM-generated suboptimal policies with Twin Delayed DDPG (TD3) through a masking-aware actor architecture. The LLM produces Python-executable policies from environment documentation, which are used to constrain the continuous action space via dynamic bounds. The actor network outputs normalized values mapped to constrained action ranges around the LLM policy, with epsilon-masking gradually reducing this guidance over training. This approach combines structured prior knowledge with autonomous policy refinement, enabling efficient exploration while avoiding over-reliance on potentially suboptimal LLM guidance.

## Key Results
- CAMEL achieves improved sample efficiency in Hopper-v4 and Ant-v4 compared to vanilla TD3
- LLM-generated policies enhance exploration efficiency when task dynamics are simple enough to model (proportional-derivative control)
- Framework maintains baseline performance in Walker2d-v4 where LLM policies fail to capture complex bipedal dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated hard-coded policies provide useful inductive bias for continuous control tasks when domain dynamics are sufficiently simple to express in PD control logic
- Mechanism: The LLM receives environment documentation (state/action dimensions, MuJoCo XML, task objectives) and generates Python functions implementing proportional-derivative control. These policies map observations to actions through fixed gain parameters, creating a structured prior around which the RL agent explores
- Core assumption: The optimal policy lies within a learnable neighborhood of the LLM-generated policy (π* ≈ π_LLM ± bias)
- Evidence anchors: [abstract] "At the core of CAMEL lies the integration of Python-executable suboptimal policies generated by LLMs based on environment descriptions and task objectives." [section 3.1] "Figure 3 shows an example of a Python policy generated by the LLM, which uses hard-coded proportional-derivative control logic to compute torque actions for basic stability."

### Mechanism 2
- Claim: Dynamic action space constriction improves sample efficiency by focusing exploration around plausible actions rather than the full continuous space
- Mechanism: The actor network outputs normalized values x ∈ [0,1] which are mapped to the constrained action range [a_lb, a_ub] via ACTION_MAPPING. Bounds are computed as π_LLM(s) ± bias, creating a search corridor around the LLM's recommendation
- Core assumption: The bias parameter sufficiently captures uncertainty in LLM policy quality while remaining narrow enough to exclude clearly suboptimal regions
- Evidence anchors: [section 3.2] "By combining guidance from the LLM with stochastic masking, Masking-Aware TD3 enables efficient exploration of the action space while avoiding over-reliance on suboptimal prior guidance."

### Mechanism 3
- Claim: Probabilistic epsilon-masking with linear decay enables gradual transition from guided exploration to autonomous policy refinement without catastrophic forgetting of useful priors
- Mechanism: With probability ε_t, full action masking applies (constrained exploration); with probability 1-ε_t, the agent explores the full original action space. Epsilon decays linearly: ε_t = max(1 - t/(f_m·T), 0.0) where f_m=0.2 means masking fully disables at 20% of training
- Core assumption: The RL agent learns robust representations during the masked phase that transfer to unmasked exploration
- Evidence anchors: [abstract] "epsilon-masking gradually reduces reliance on LLM-generated guidance, enabling agents to transition from constrained exploration to autonomous policy refinement."

## Foundational Learning

- Concept: **Twin Delayed DDPG (TD3)**
  - Why needed here: CAMEL builds directly on TD3; understanding actor-critic architecture, delayed policy updates, and twin critics is prerequisite for modifying the forward pass
  - Quick check question: Can you explain why TD3 uses two critic networks and why policy updates are delayed relative to critic updates?

- Concept: **Action Masking in Continuous Spaces**
  - Why needed here: Standard action masking applies to discrete spaces (set invalid actions to zero probability). Continuous masking requires differentiable mapping functions
  - Quick check question: How would you constrain a continuous action output to lie within [a_lb, a_ub] while maintaining gradient flow through the constraint?

- Concept: **Proportional-Derivative (PD) Control**
  - Why needed here: LLM-generated policies use PD control structure; understanding error signals, gain tuning, and stability helps evaluate generated policy quality
  - Quick check question: Given a desired joint angle θ_d and current angle θ, what is the basic PD control formula for computing torque?

## Architecture Onboarding

- Component map:
  LLM Policy Generator -> Masking Module -> Epsilon Scheduler -> Masking-Aware Actor -> Action Mapper -> Environment

- Critical path:
  1. Generate LLM policy candidates → human evaluation → select π_LLM
  2. For each environment step: compute bounds → apply epsilon masking → actor forward → action mapping → environment step
  3. Store (s, a_lb, a_ub, a, r, s') tuples in replay buffer (bounds stored for target computation)
  4. Critic update uses actions mapped from stored bounds; actor gradient flows through mapping function

- Design tradeoffs:
  - **bias=0.3**: Narrower bounds improve early sample efficiency but risk convergence to suboptimal LLM policy; wider bounds are more robust but slower
  - **f_m=0.2**: Faster decay (smaller f_m) enables earlier autonomous exploration but may lose prior benefits; slower decay preserves guidance longer
  - **Human policy selection**: Required for quality control (episode return alone is misleading) but adds manual overhead and scalability concerns

- Failure signatures:
  - Agent achieves high reward quickly then plateaus below baseline → bias too narrow, over-reliance on LLM prior
  - Agent shows no improvement over baseline → LLM policy too poor (Walker2d pattern) or bias too wide
  - Training becomes unstable after epsilon decays → actor never learned valid representations during masked phase
  - Generated policy causes immediate episode termination → LLM policy invalid, need better candidate screening

- First 3 experiments:
  1. **Baseline comparison**: Run CAMEL vs. vanilla TD3 on Hopper-v4 with identical hyperparameters and seeds; plot episodic return curves to confirm sample efficiency gains
  2. **Ablation on masking components**: Test (a) masking without epsilon decay, (b) epsilon decay without masking, (c) full CAMEL to isolate contribution of each mechanism
  3. **Robustness to LLM policy quality**: Generate 5 LLM policy candidates per environment, test CAMEL with each individually to characterize sensitivity to prior quality; compare selected vs. highest-return vs. random candidate

## Open Questions the Paper Calls Out

- Can the selection of LLM-generated policies be automated to eliminate the reliance on human expert review?
  - Basis in paper: [explicit] The authors state that "reliance on expert screening for policy evaluation introduces a manual overhead" and list "automate policy evaluation, reducing human intervention" as a specific avenue for future work
  - Why unresolved: Currently, the framework requires human experts to review rendered videos because simple metrics like episode return are unreliable proxies for policy quality (e.g., a policy that stands still in Hopper-v4 yields high returns but is suboptimal)
  - What evidence would resolve it: A demonstration of a metrics-based or learning-based heuristic that can reliably select optimal policy candidates without human visual inspection, achieving performance parity with the manual selection process

- Can CAMEL be effectively generalized to multimodal LLMs to handle environments with non-vectorized observation spaces?
  - Basis in paper: [explicit] The paper explicitly notes that "applicability is limited to vectorized observation spaces" and identifies "generaliz[ing] CAMEL to multimodal LLMs" as a goal to handle broader observation-action spaces
  - Why unresolved: The current framework relies on text-based descriptions of the state space (Gymnasium documentation/MuJoCo XML), which restricts its use to environments where observations can be easily serialized into text prompts
  - What evidence would resolve it: Successful application of the CAMEL framework using vision-capable models (e.g., processing pixel data directly) in environments where text descriptions are insufficient or unavailable

- How can the LLM prompting or policy generation be improved to successfully model complex periodic dynamics, such as bipedal gait alternation?
  - Basis in paper: [explicit] The results show that CAMEL failed to improve performance in Walker2d-v4, a failure attributed to the LLMs struggling "to accurately model bipedal gait dynamics" compared to the simpler mechanics of Hopper-v4 or Ant-v4
  - Why unresolved: The "hard-coded" nature of the Python policies generated via Chain of Thought appears insufficient for capturing the phase-dependent coordination required for bipedal walking, limiting the framework's universality
  - What evidence would resolve it: A modified prompting strategy or architecture that yields a functional suboptimal policy for Walker2d-v4 (or similar complex locomotion tasks), resulting in improved sample efficiency comparable to the Hopper-v4 results

## Limitations

- Framework requires human expert review to select among LLM-generated policies, creating scalability concerns
- Performance highly dependent on bias and epsilon decay hyperparameters that likely require task-specific tuning
- Demonstrated only on MuJoCo locomotion tasks; generalization to more complex or diverse continuous control domains remains unknown

## Confidence

- **High confidence**: The core mechanism of using LLM-generated policies for action masking is sound and well-documented; the empirical improvement in sample efficiency on Hopper-v4 and Ant-v4 is demonstrated with statistical rigor
- **Medium confidence**: The epsilon-masking curriculum approach is theoretically justified but lacks ablation studies to quantify its independent contribution to performance gains
- **Low confidence**: Claims about CAMEL maintaining performance without degradation in Walker2d-v4 are based on comparison to "expert masking" rather than comprehensive baseline analysis; the framework's behavior with multimodal LLMs is purely speculative

## Next Checks

1. **Hyperparameter ablation study**: Systematically vary bias (0.1, 0.3, 0.5) and epsilon decay rate (f_m=0.1, 0.2, 0.3) across all three environments to identify robustness boundaries and optimal configurations

2. **Policy selection automation**: Develop and evaluate automated metrics for selecting LLM policies without human intervention, comparing human-selected vs. algorithm-selected policies across multiple LLM candidates

3. **Cross-domain generalization**: Test CAMEL on non-Mujoco continuous control tasks (e.g., OpenAI Gym's LunarLanderContinuous, robotic manipulation tasks) to assess framework transferability beyond locomotion domains