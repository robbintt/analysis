---
ver: rpa2
title: An approach of deep reinforcement learning for maximizing the net present value
  of stochastic projects
arxiv_id: '2511.12865'
source_url: https://arxiv.org/abs/2511.12865
tags:
- project
- policy
- activity
- scheduling
- ddqn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses maximizing expected net present value (NPV)
  for stochastic project scheduling with uncertain activity durations and cash flows.
  The problem is formulated as a discrete-time Markov Decision Process (MDP), where
  the goal is to accelerate cash inflows and defer outflows.
---

# An approach of deep reinforcement learning for maximizing the net present value of stochastic projects

## Quick Facts
- arXiv ID: 2511.12865
- Source URL: https://arxiv.org/abs/2511.12865
- Reference count: 10
- Primary result: DDQN outperforms traditional rigid and dynamic strategies in maximizing expected NPV for stochastic project scheduling

## Executive Summary
This paper addresses the challenge of maximizing expected Net Present Value (NPV) for stochastic project scheduling with uncertain activity durations and cash flows. The authors formulate the problem as a discrete-time Markov Decision Process (MDP) where the objective is to accelerate cash inflows and defer outflows. They propose a Double Deep Q-Network (DDQN) approach that demonstrates superior performance compared to traditional scheduling strategies, particularly in large-scale or highly uncertain environments.

## Method Summary
The method formulates stochastic project scheduling as an MDP where states encode current time, activity status (active/unstarted/finished), durations, start times, and project progress. Actions represent scheduling decisions subject to precedence constraints. The DDQN architecture uses separate online and target networks to mitigate Q-value overestimation bias, with experience replay for training stability. The reward function is designed to maximize discounted cash flows, encouraging acceleration of positive cash flows and deferral of negative ones.

## Key Results
- DDQN achieves 9-19% better NPV performance compared to dynamic scheduling baselines
- Dual-network architecture and target network significantly improve training convergence and robustness
- DDQN demonstrates superior computational capability and adaptability in high-dimensional state spaces
- Ablation studies confirm that removing the target network leads to highly fluctuating training curves and unstable convergence

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Action Selection and Value Evaluation
DDQN separates action selection (online network) from value estimation (target network), reducing Q-value overestimation bias in stochastic environments. This breaks the maximization bias where the same network both selects and evaluates actions.

### Mechanism 2: Target Network for Training Stability
The periodically frozen target network parameters provide stable bootstrap targets by remaining fixed between updates, reducing TD target volatility during gradient updates and improving convergence.

### Mechanism 3: MDP State Encoding Captures Project Execution Dynamics
The state representation (time, active/completed/pending activities, durations, start times, status) enables Markovian decision-making under uncertainty while satisfying non-anticipativity constraints by including only information available at current time.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - Why needed: The entire scheduling problem is cast as an MDP; understanding state/action/reward/transition structure is prerequisite to implementing DDQN.
  - Quick check: Can you explain why the non-anticipativity constraint is automatically satisfied by defining state s_k to include only information available at time k?

- **Concept: Temporal Difference (TD) Learning and Q-Functions**
  - Why needed: DDQN updates Q-values via TD error δ_k = r_k + βQ̂(s_{k+1}, a*) - Q(s_k, a_k); understanding this bootstrap mechanism is essential for debugging training.
  - Quick check: What happens to Q-value updates if the discount factor β is set too close to 1.0 in a long-horizon project?

- **Concept: Experience Replay Buffer**
  - Why needed: Breaking temporal correlation in training samples is critical for stable deep RL; the buffer stores (s, a, r, s') tuples for random minibatch sampling.
  - Quick check: Why might sampling from a buffer that prioritizes recent high-reward experiences improve or harm convergence in stochastic project scheduling?

## Architecture Onboarding

- **Component map**: Environment -> State Encoder -> DDQN (Online + Target networks) -> Action Selector -> Reward Calculator -> Experience Replay Buffer -> Loss Calculator

- **Critical path**: Initialize both networks with same random weights → For each episode: sample scenario, reset state → At each decision time: select feasible action via ε-greedy, observe reward and next state → Store transition in buffer; if buffer ≥ batch size, sample minibatch and compute TD targets using target network → Update online network via gradient descent → Every C steps: copy online weights to target network → Log episode reward

- **Design tradeoffs**: Network depth vs. overfitting (3 hidden layers provide sufficient capacity for 5-30 activities but may need widening for larger projects); Target update frequency C (too frequent destabilizes, too infrequent slows learning); Replay buffer size (50k sufficient for tested scales, larger spaces may need prioritized replay)

- **Failure signatures**: Q-values diverge to infinity (learning rate too high or reward scaling issue); Training curve oscillates wildly (target network not providing stable targets); Policy ignores precedence constraints (action masking not properly enforced); Convergence to poor local optimum (insufficient exploration)

- **First 3 experiments**: 1) Reproduce Example 1 (5 activities, 2 scenarios) to verify DDQN achieves expected NPV ≈ 129.77; 2) Ablation: Remove target network to confirm training instability and fluctuating curves; 3) Scale test on dataset Ω₁ with (n-2)=15, |Σ|=10 to compare DDQN gap vs. dynamic policy

## Open Questions the Paper Calls Out

- **Open Question 1**: Can DDQN be extended to solve resource-constrained project scheduling while maintaining stable convergence? The current formulation ignores resource constraints, which would significantly expand the state-action space.

- **Open Question 2**: How would incorporating continuous-action reinforcement learning and explicit uncertainty modeling impact stability and sample efficiency? The current discrete-action DDQN may suffer from overestimation or inefficiency in more complex environments.

- **Open Question 3**: Can transfer learning or meta-learning mechanisms be integrated to mitigate high training costs and hyperparameter sensitivity? Training from scratch for every new project is computationally burdensome.

## Limitations
- Synthetic data generation may not capture real-world project complexity with correlations, resource constraints, and systematic dependencies
- Markovian assumption relies on state encoding being sufficiently rich, but unmodeled factors could violate this assumption
- Computational advantage claims assume DDQN scales better than exact methods, but no complexity analysis is provided for large-scale projects

## Confidence

- **High Confidence**: DDQN outperforms rigid and dynamic baselines in tested synthetic environments with statistically significant NPV improvements (9-19% relative gaps)
- **Medium Confidence**: MDP formulation correctly captures project scheduling under uncertainty for tested problem scales (5-30 activities)
- **Low Confidence**: Computational advantage claim for large-scale environments is based on synthetic data only without real project testing or runtime complexity analysis

## Next Checks
1. **Real-World Dataset Validation**: Test DDQN on actual project datasets with recorded precedence relationships and cash flow patterns, comparing against exact methods for small instances and traditional heuristics for larger ones.

2. **Robustness to MDP Violation**: Design experiments with hidden state variables affecting outcomes and measure DDQN performance degradation compared to ideal Markovian case, testing whether recurrent architectures improve robustness.

3. **Computational Scalability Analysis**: Systematically vary project size, precedence density, and scenario count to measure training time, sample efficiency, and policy quality as functions of these parameters, comparing against both exact dynamic programming and traditional heuristics.