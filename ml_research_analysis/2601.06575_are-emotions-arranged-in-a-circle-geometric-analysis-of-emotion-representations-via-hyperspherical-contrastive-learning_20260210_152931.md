---
ver: rpa2
title: Are Emotions Arranged in a Circle? Geometric Analysis of Emotion Representations
  via Hyperspherical Contrastive Learning
arxiv_id: '2601.06575'
source_url: https://arxiv.org/abs/2601.06575
tags:
- emotion
- learning
- labels
- ngpt
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines whether circular (circumplex) representations\
  \ of emotion, as used in psychology, can be effectively learned within language\
  \ model embeddings via hyperspherical contrastive learning. It proposes a method\
  \ using nGPT (normalized Transformer) heads and three contrastive loss functions\u2014\
  SINCERE, SoftCSE, and a novel CircularCSE\u2014to induce circular emotion structures\
  \ in the embedding space."
---

# Are Emotions Arranged in a Circle? Geometric Analysis of Emotion Representations via Hyperspherical Contrastive Learning

## Quick Facts
- arXiv ID: 2601.06575
- Source URL: https://arxiv.org/abs/2601.06575
- Authors: Yusuke Yamauchi; Akiko Aizawa
- Reference count: 40
- This paper shows that while CircularCSE yields high interpretability and alignment with psychological emotion models, it sacrifices discriminative performance in high-dimensional or fine-grained label settings.

## Executive Summary
This paper examines whether circular (circumplex) representations of emotion, as used in psychology, can be effectively learned within language model embeddings via hyperspherical contrastive learning. It proposes a method using nGPT (normalized Transformer) heads and three contrastive loss functions—SINCERE, SoftCSE, and a novel CircularCSE—to induce circular emotion structures in the embedding space. Evaluations across four datasets and six backbone models show that while CircularCSE yields high interpretability and alignment with psychological emotion models (higher CD-r), it sacrifices discriminative performance (lower V-Measure) in high-dimensional or fine-grained label settings. SINCERE and SoftCSE perform better for clustering accuracy but less so for psychological alignment. The results reveal a fundamental trade-off between human interpretability and discriminative power in emotion representation learning.

## Method Summary
The method involves attaching a single-layer GPT or nGPT head to pre-trained backbone models and training with three contrastive loss functions. SINCERE uses InfoNCE-style contrastive learning, SoftCSE incorporates weighted negative pairs based on emotional distance, and CircularCSE directly enforces hard geometric constraints to align embeddings with Russell's circumplex model. The approach operates on normalized embeddings where distances are measured via cosine similarity, with evaluation using both clustering metrics (V-Measure) and geometric alignment metrics (CD-r) to assess the trade-off between discriminative power and interpretability.

## Key Results
- CircularCSE achieves superior psychological alignment (higher CD-r) but lower clustering performance (lower V-Measure) compared to SINCERE
- SINCERE and SoftCSE show higher discriminative power (higher V-Measure) but poor alignment with circumplex geometry
- The trade-off between interpretability and discriminability becomes more pronounced as dimensionality increases or label count grows
- nGPT heads preserve spherical geometry better than standard GPT heads but may lose norm-based information important for some models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CircularCSE enforces hard geometric constraints that align embeddings with psychological circumplex structures.
- **Mechanism:** The loss function directly penalizes deviations from target pairwise cosine similarities derived from angular distances on the circumplex (cos(Δθ_ij)), treating same-label pairs with a margin and different-label pairs with exact distance matching. This forces the learned manifold to converge to a 2D ring geometry.
- **Core assumption:** Assumption: Emotion labels in the training data can be meaningfully mapped to positions on Russell's circumplex model; the predefined angular distances accurately reflect psychological relationships.
- **Evidence anchors:**
  - [abstract] "We show that while this circular alignment offers superior interpretability... it underperforms compared to conventional designs in high-dimensional settings and fine-grained classification."
  - [Section 3.4] "CircularCSE, which directly learns distances on the circle... ℓ_ij = (e^T_i e_j − cos(∆θ_ij))²"
  - [corpus] Related work "Coordinate Heart System" similarly positions emotions on a unit circle but uses coordinate mixing rather than contrastive learning; limited external validation of the psychological validity claim.
- **Break condition:** If emotion categories in downstream tasks do not correspond to positions on the predefined circumplex, or if multi-label/mixed emotions dominate, the hard constraint will misrepresent relationships.

### Mechanism 2
- **Claim:** SINCERE achieves higher discriminative power by arranging class representations as an approximately orthogonal simplex in high-dimensional space.
- **Mechanism:** The InfoNCE-style loss minimizes positive-negative similarity; in high dimensions, the optimal configuration approaches a regular simplex where class prototypes achieve pairwise inner products near 0 (orthogonality), yielding ~90° decision margins that maximize cluster separability.
- **Core assumption:** Assumption: High-dimensional embedding spaces permit near-orthogonal configurations; the curse of dimensionality allows local optima at 0 rather than the theoretical bound of -1/(E-1).
- **Evidence anchors:**
  - [Section 5.1] "SINCERE... attains its theoretical lower bound when the cosine similarity between all positive-negative pairs equals −1/(E−1). In practice... often settles at a local optimum of 0, resulting in an average boundary margin of 90° (orthogonality)."
  - [Section 4.3/Table 1] SINCERE consistently achieves highest V-Measure (e.g., 0.760 for mE5) but lowest CD-r (e.g., 0.317).
  - [corpus] No direct corpus validation; related work on contrastive learning focuses on discriminative performance rather than geometric interpretability.
- **Break condition:** In low-dimensional spaces (< ~3D), the simplex cannot achieve large margins; discriminability will degrade as dimensions decrease.

### Mechanism 3
- **Claim:** The fundamental trade-off between interpretability and discriminability arises from incompatible optimal margin requirements in 2D vs. high-D geometries.
- **Mechanism:** CircularCSE's 2D ring constrains maximum inter-class margins to π/E (30° for 12 classes), while SINCERE achieves ~90° margins in high dimensions. As model capacity (dimensions) or label count increases, the margin gap widens, making circular structures inherently less discriminable.
- **Core assumption:** Assumption: Human interpretability requires low-dimensional manifold structures; discriminative power benefits from exploiting high-dimensional capacity.
- **Evidence anchors:**
  - [Section 5.1/Figure 6] CircularCSE maintains stable accuracy in low dimensions but degrades in high dimensions or with many labels; SINCERE shows the opposite pattern.
  - [Section 5.1] "The optimal boundary margins of SINCERE and CircularCSE coincide only in 2 dimensions; otherwise, the gap in discriminative power widens."
  - [corpus] Weak external validation; corpus papers do not address this specific trade-off.
- **Break condition:** If downstream tasks require only coarse emotion distinctions (few labels) or benefit from visualization/debugging, CircularCSE's interpretability may outweigh discriminability loss.

## Foundational Learning

- **Concept: Contrastive Learning on Hyperspheres**
  - Why needed here: The paper operates entirely on normalized embedding spaces where distances are angular (cosine similarity), requiring understanding of how contrastive losses behave on S^(d-1) rather than Euclidean space.
  - Quick check question: Given two unit vectors with cosine similarity 0.5, what is their angular distance in radians? (Answer: π/3 ≈ 1.047)

- **Concept: Circumplex Models of Emotion (Russell's Model)**
  - Why needed here: The entire experimental design presupposes that emotions can be arranged on a circle defined by valence (positive-negative) and arousal (activated-deactivated) axes.
  - Quick check question: On the circumplex, which emotion would be positioned opposite to "excitement" (high arousal, positive valence)? (Answer: Something like "boredom" or "depression"—low arousal, negative valence)

- **Concept: Regular Simplex Geometry**
  - Why needed here: The paper proves SINCERE's optimal solution is a regular simplex; understanding this configuration is essential for interpreting why SINCERE achieves high discriminability.
  - Quick check question: For E classes arranged as a regular simplex on a unit hypersphere, what is the pairwise inner product between any two class prototypes? (Answer: -1/(E-1))

## Architecture Onboarding

- **Component map:** Backbone (BERT-like, LLM-encoder, or decoder-only LLM) -> Projection Head (GPT or nGPT) -> Pooling (CLS/last/mean) -> ℓ2 normalization -> Contrastive loss computation

- **Critical path:**
  1. Backbone outputs hidden states → 2. Head processes via attention + MLP (with or without spherical constraints) → 3. Pooling extracts sentence representation → 4. ℓ2 normalization → 5. Contrastive loss computation with pairwise target distances

- **Design tradeoffs:**
  - **nGPT vs. GPT head:** nGPT preserves spherical geometry (better for CircularCSE) but may lose information encoded in vector norms; decoder-only models show larger V-Measure drops with nGPT (Table 1: Llama-3.2-3B drops from 0.725 to 0.577).
  - **Loss function selection:** CircularCSE for interpretability/robustness to dimensionality reduction; SINCERE for maximum discriminability; SoftCSE as a middle ground.
  - **Fine-tuning vs. frozen backbone:** Full fine-tuning (BERT-like) may yield different geometry than head-only training (LLMs).

- **Failure signatures:**
  - **Low V-Measure with CircularCSE in high dimensions:** Expected; indicates margin constraint is too tight for the label count.
  - **Near-zero CD-r with SINCERE:** Expected; orthogonal arrangement ignores circumplex structure.
  - **Decoder-only + nGPT showing severe V-Measure drop:** Norm information loss; decoder models may rely more on magnitude for contextual encoding.

- **First 3 experiments:**
  1. **Baseline sanity check:** Train SINCERE-GPT and CircularCSE-nGPT on PersonaGen (synthetic, clean) with mE5 backbone; expect CircularCSE to show high CD-r (>0.9) and SINCERE to show high V-Measure (>0.9). If not, check label mapping and loss implementation.
  2. **Dimensional robustness test:** Apply PCA to reduce embeddings to 2D, 4D, 8D, 16D, 32D, 64D; run Spherical k-means and plot V-Measure. Confirm CircularCSE degrades less at low dimensions.
  3. **Label scaling test:** Train on subsets of 4, 6, 8, 10, 12 emotion labels; plot V-Measure vs. label count. Confirm CircularCSE approaches SINCERE performance at low label counts but diverges as labels increase.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the geometric interpretability of circular emotion representations be effectively utilized to control or steer text generation in Large Language Models?
- **Basis in paper:** [explicit] The authors state: "As interpretability and visualization facilitate model controllability, extending this approach to generation tasks is a key direction for future research."
- **Why unresolved:** The current study evaluates discriminative performance (clustering) and interpretability (alignment) using embeddings, but does not test whether these structures can be manipulated during inference to generate text with specific emotional arcs.
- **What evidence would resolve it:** Experiments demonstrating that intervening on the circular manifold (e.g., rotating an embedding along the valence axis) results in generated text that coherently shifts emotional tone according to the model's parameters.

### Open Question 2
- **Question:** How can the "neutral" emotional state and simultaneous conflicting emotions be effectively represented within a hyperspherical or circumplex geometry?
- **Basis in paper:** [explicit] The authors note that "Neutral" is treated as a distinct class rather than the origin, and "Real-world emotions... often involve simultaneous conflicting states... developing a more natural representation for such states remains a challenge for future work."
- **Why unresolved:** The current CircumplexCSE method arranges emotions as a ring, which does not account for the "center" (neutral) or the simultaneous occupancy of multiple positions (e.g., bittersweet), forcing a single-label assumption.
- **What evidence would resolve it:** A modified loss function or architecture that maps "neutral" to the vector origin or models blended emotions as composite vectors, validated against datasets annotated with mixed or ambiguous emotions.

### Open Question 3
- **Question:** Is there an optimal manifold geometry that mitigates the trade-off between the high discriminative power of orthogonal simplex structures and the interpretability of low-dimensional circular structures?
- **Basis in paper:** [explicit] The paper concludes: "Determining the optimal manifold structure for specific tasks is an open question for future work," following the discovery of a "structural dilemma" where circular geometry degrades clustering performance in high dimensions.
- **Why unresolved:** The results show CircularCSE is robust in low dimensions but fails in fine-grained settings, while SINCERE succeeds via orthogonal margins; a geometry that balances these constraints is not identified.
- **What evidence would resolve it:** A theoretical or empirical analysis identifying a geometry (e.g., a hierarchical sphere or toroidal structure) that maintains high V-Measure scores on fine-grained tasks while preserving high CD-r (psychological alignment).

### Open Question 4
- **Question:** Can hyperspherical contrastive learning effectively capture non-verbal emotional cues when applied to multimodal data (audio or vision)?
- **Basis in paper:** [inferred] Under limitations, the authors state: "Capturing these nuances necessitates a multimodal architectural design," noting that text-only analysis misses physical cues like gestures or drowsiness.
- **Why unresolved:** The methodology is strictly constrained to text-based Transformer architectures (GPT/nGPT), leaving the geometric validity of circumplex models in non-textual embedding spaces unexplored.
- **What evidence would resolve it:** Application of the CircularCSE/nGPT framework to audio-visual datasets to determine if the induced circular structure aligns with psychological models better than unimodal text baselines.

## Limitations

- **Psychological validity concerns:** The circumplex model's assumption that emotions can be arranged on a 2D circle may not reflect true psychological relationships, particularly for mixed or ambiguous emotions.
- **Cultural specificity:** All datasets are English-language and based on Western psychological theories, limiting generalizability across different cultural contexts.
- **Synthetic data limitations:** The PersonaGen dataset is generated via LLM synthesis, which may not capture the full complexity of genuine human emotional expression.

## Confidence

**High Confidence:**
- The technical implementation of the three contrastive loss functions and their mathematical properties
- The observed trade-off between V-Measure and CD-r across different experimental conditions
- The mechanism by which CircularCSE enforces circular geometry through hard distance constraints

**Medium Confidence:**
- The interpretation that the trade-off represents a fundamental conflict between interpretability and discriminability in high-dimensional spaces
- The claim that CircularCSE provides "superior interpretability" based on geometric alignment metrics
- The conclusion that CircularCSE is "more robust to dimensionality reduction"

**Low Confidence:**
- The assertion that CircularCSE better "captures the nature of human emotions"
- The broader implications for emotion representation learning beyond the specific experimental setup
- The claim that nGPT heads are universally better for preserving spherical geometry across all model architectures

## Next Checks

**Validation Check 1:** Cross-cultural replication with non-English datasets
- **Purpose:** Test whether the interpretability-discriminability trade-off holds across different cultural contexts
- **Method:** Apply the same experimental pipeline to emotion-labeled datasets in Chinese, Japanese, and Arabic
- **Success criterion:** If the same pattern emerges across cultures, confidence in the fundamental trade-off increases

**Validation Check 2:** Human evaluation of interpretability
- **Purpose:** Ground the claim of "superior interpretability" in human perception rather than geometric metrics
- **Method:** Conduct a user study where participants cluster emotion embeddings from different methods
- **Success criterion:** If humans find CircularCSE embeddings more intuitively organized, this validates the interpretability claims

**Validation Check 3:** Downstream task performance evaluation
- **Purpose:** Assess practical utility beyond clustering metrics
- **Method:** Fine-tune emotion embeddings for downstream tasks like emotion classification and sentiment analysis
- **Success criterion:** If CircularCSE embeddings show competitive performance in tasks requiring emotional understanding, this suggests practical benefits despite lower clustering performance