---
ver: rpa2
title: 'Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics'
arxiv_id: '2507.00380'
source_url: https://arxiv.org/abs/2507.00380
tags:
- chant
- segmentation
- mode
- melodies
- melody
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applies Bayesian nonparametric methods to segment Gregorian\
  \ chant melodies, aiming to uncover their structural basis and explore the relationship\
  \ between modality and memorisation. Using a Nested Hierarchical Pitman-Yor Language\
  \ Model (NHPYLM), the authors automatically infer segmentations of chant melodies\
  \ that are then evaluated for their ability to classify modes\u2014a proxy for structural\
  \ coherence."
---

# Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics

## Quick Facts
- arXiv ID: 2507.00380
- Source URL: https://arxiv.org/abs/2507.00380
- Reference count: 0
- This study applies Bayesian nonparametric methods to segment Gregorian chant melodies, aiming to uncover their structural basis and explore the relationship between modality and memorisation.

## Executive Summary
This paper introduces a Bayesian nonparametric approach to automatically segment Gregorian chant melodies, aiming to uncover their structural basis and explore the relationship between modality and memorisation. Using a Nested Hierarchical Pitman-Yor Language Model (NHPYLM), the authors automatically infer segmentations of chant melodies that are then evaluated for their ability to classify modes—a proxy for structural coherence. The method outperforms existing baselines and achieves state-of-the-art mode classification accuracy, especially when conditioning on mode. Applied to a medieval manuscript dataset, the segmentation reveals that formulaicity (i.e., re-use of melodic segments) is greater at the beginnings and ends of melodies, corresponding to liturgical performance contexts. Memory efficiency, measured by perplexity, correlates with mode classification performance, supporting the link between modality and ease of memorisation. However, the resulting segmentations do not resemble the traditional "centonisation" theory, suggesting that institutional practices may have shaped chant differently from other oral traditions.

## Method Summary
The method uses a Nested Hierarchical Pitman-Yor Language Model (NHPYLM) to perform unsupervised segmentation of chant melodies by minimizing description length (equivalent to maximizing posterior probability). The model segments melodies at tone boundaries and uses Gibbs sampling for training, followed by Viterbi decoding for inference. A variant called NHPYLMClasses trains separate segmentation models for each of the eight modes, allowing mode-specific vocabularies. The quality of the segmentations is evaluated through mode classification using bag-of-segments features with TF-IDF weighting and linear SVM classifiers. The approach is tested on the CantusCorpus dataset with pitch and interval representations, and validated on a single medieval manuscript to avoid data leakage.

## Key Results
- NHPYLM achieves state-of-the-art mode classification accuracy (91.7 F1 for antiphons) compared to baselines like syllable/word counts, 4-grams, and BERT embeddings
- Mode-conditional segmentation (NHPYLMClasses) further improves accuracy to 92.7 F1 for antiphons
- Segmentation reveals higher formulaicity at melody beginnings and ends, corresponding to liturgical performance contexts
- Memory efficiency (low perplexity) correlates with mode classification performance, supporting the link between modality and ease of memorisation
- The resulting segmentations do not resemble traditional "centonisation" theory, suggesting institutional practices shaped chant differently from other oral traditions

## Why This Works (Mechanism)

### Mechanism 1: MDL-optimal segmentation via NHPYLM
Unsupervised segmentation discovers memory-efficient melody boundaries without labeled data by exploiting the MDL-Bayes equivalence. The NHPYLM learns segment boundaries through Gibbs sampling, minimizing perplexity (compression cost) while maintaining sequential predictability. The Pitman-Yor prior induces "rich-get-richer" dynamics via the Chinese Restaurant Process, favoring reuse of existing segments over creating new ones. The core assumption is that Gregorian chants evolved under memory pressure to have compressible structure.

### Mechanism 2: Mode-conditional vocabulary induction
Conditioning segmentation on mode class improves both memory efficiency and downstream mode classification. NHPYLMClasses trains 8 parallel vocabularies (one per mode). At inference, Bayes' theorem combines mode-specific likelihoods to predict the most likely mode. This enforces mode-specific "dialects" of melodic segments, reducing within-mode perplexity. The core assumption is that modes have characteristic segment distributions that differ systematically.

### Mechanism 3: Boundary formulaicity reflects liturgical constraints
Formulaic (reusable) segments cluster at melody boundaries where liturgical transitions require compatibility with psalm tones. Segments at psalm-tone interfaces face selection pressure for conventionalization—singers must transition smoothly between antiphon and psalm tone. Middle sections face weaker constraints, allowing more melodic freedom. The core assumption is that liturgical performance practices shaped melodic structure over transmission history.

## Foundational Learning

- **Concept: Minimum Description Length (MDL) principle**
  - Why needed here: Core theoretical justification for treating perplexity minimization as memory optimization
  - Quick check question: Given L(D|H) + L(H), what happens to the optimal codebook if a subsequence appears frequently but a rare subsequence gets a short code?

- **Concept: Pitman-Yor process / Chinese Restaurant Process**
  - Why needed here: Understanding how discount parameter d creates "rich-get-richer" dynamics that prefer fewer vocabulary elements
  - Quick check question: In a CRP with α=1, what is the probability that the 10th customer sits at a new table?

- **Concept: Perplexity as compression metric**
  - Why needed here: The paper correlates perplexity with mode classification to link memory efficiency and modal structure
  - Quick check question: If perplexity drops from 20 to 15, what does this imply about average uncertainty per segment?

## Architecture Onboarding

- **Component map:** Preprocessing (CantusCorpus → pitch/interval sequences) → NHPYLM core (Tone-level HPYLM + Segment-level HPYLM) → Mode-conditional wrapper (8 parallel NHPYLM instances) → Inference (Blocked Gibbs sampling → Viterbi decode) → Evaluation (Bag-of-segments → TF-IDF → Linear SVM → mode F1)

- **Critical path:** Data prep → NHPYLM training (monitor convergence on 10% validation) → Viterbi segmentation → SVM training → Classification + perplexity evaluation

- **Design tradeoffs:**
  - Single manuscript vs. full corpus: Cleaner evaluation (no Cantus ID leakage) but ~7% lower F1 due to data scarcity
  - Pitch vs. interval encoding: Intervals are transposition-invariant but yield higher perplexity (Table 3: 20.0 vs 15.4 for antiphons)
  - Max segment length 7: Prevents overlong segments but may truncate longer formulas

- **Failure signatures:**
  - Non-convergence: Perplexity oscillates without decreasing over iterations
  - Trivial segmentation: Dominance of length-1 segments (Poisson correction λ misconfigured)
  - Mode collapse: NHPYLMClasses performs identically to unconditional (mode not encoded in segments)

- **First 3 experiments:**
  1. Replicate Table 1: Compare NHPYLM vs. syllable/word/4-gram baselines on CantusCorpus antiphons with pitch encoding
  2. Single-manuscript validation: Train on D-KA Aug. LX, verify perplexity-F1 correlation (paper reports r≈−0.77)
  3. Ablate Poisson correction: Set λ prior to uniform, measure impact on short-segment frequency and perplexity

## Open Questions the Paper Calls Out

### Open Question 1
To what extent did institutional rules and practices counteract the structural tendencies inherent in oral transmission? The authors suggest that the lack of centonisation "opens the intriguing possibility that institutional rules and practices were able to significantly counteract processes inherent in oral transmission," but this remains untested against other traditions.

### Open Question 2
What is the constructive theory for Gregorian melody if not centonisation? The introduction states "No good explanation exists... for why Gregorian melodies should be the way they are," and the conclusion reiterates that "The mystery of constructing Gregorian melodies remains unresolved."

### Open Question 3
Is the correlation between memory efficiency (perplexity) and mode classification performance robust across different encoding schemes and datasets? The authors note that "Pilot experiments with other melody encodings suggest this holds across conditions, warranting further study."

## Limitations

- Liturgical practice reconstruction remains speculative, as the paper provides only correlational evidence between boundary formulaicity and liturgical position
- Performance on a single manuscript dataset shows ~7% lower F1 than corpus-wide training, raising questions about generalizability
- The positive correlation between memory efficiency and mode classification could result from mode-specific harmonic/timbral cues not captured in the segmentation

## Confidence

- **High confidence**: NHPYLM's technical implementation and performance metrics on CantusCorpus; the superiority of mode-conditional segmentation for classification; the observed boundary formulaicity patterns
- **Medium confidence**: The link between modality and memorizability; the failure to find evidence for centonisation as a structural principle
- **Low confidence**: The specific claim that institutional liturgical practices (rather than transmission dynamics) explain boundary formulaicity

## Next Checks

1. **Cross-manuscript validation**: Apply the method to 3-5 additional medieval chant manuscripts and test whether boundary formulaicity patterns remain consistent across different institutional sources
2. **Control for non-structural features**: Test whether mode classification performance drops significantly when training on randomized pitch sequences with preserved melodic contour but scrambled intervallic content
3. **Liturgical position ablation**: Train NHPYLM on only middle segments (excluding boundaries) and test whether mode classification F1 scores drop proportionally more than when ablating other segment types