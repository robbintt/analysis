---
ver: rpa2
title: Identifying Causal Direction via Dense Functional Classes
arxiv_id: '2509.00538'
source_url: https://arxiv.org/abs/2509.00538
tags:
- causal
- function
- noise
- cubic
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of determining causal direction
  between two continuous variables under the assumption of no hidden confounders.
  The authors propose a method called LCUBE, which uses the Minimum Description Length
  (MDL) principle with dense functional classes, specifically cubic regression splines,
  to infer the correct causal direction.
---

# Identifying Causal Direction via Dense Functional Classes

## Quick Facts
- arXiv ID: 2509.00538
- Source URL: https://arxiv.org/abs/2509.00538
- Authors: Katerina Hlavackova-Schindler; Suzana Marsela
- Reference count: 40
- One-line primary result: LCUBE achieves 87 AUDRC on Tuebingen cause-effect pairs, outperforming state-of-the-art methods

## Executive Summary
This paper addresses bivariate causal discovery by proposing LCUBE, a method that uses Minimum Description Length (MDL) with dense functional classes to identify causal direction. The approach assumes no hidden confounders and models cause-effect pairs as Y = f(X) + noise, using cubic splines to approximate f and comparing MDL description lengths for both causal directions. The authors prove theoretical identifiability under certain conditions and demonstrate superior performance on benchmark datasets, achieving 87 AUDRC on the Tuebingen dataset while being computationally efficient.

## Method Summary
LCUBE is a bivariate causal discovery method that identifies causal direction between two continuous variables by comparing MDL-based description lengths of competing regression models. The method normalizes input pairs to [0,1], fits cubic regression splines for both directions (Y|X and X|Y) with varying numbers of knots, and calculates two-part code lengths that balance model complexity and fit quality. The direction with the shorter total description length is inferred as causal. LCUBE leverages the density property of cubic splines to approximate any continuous function with arbitrary precision, theoretically guaranteeing identifiability under low-noise conditions.

## Key Results
- LCUBE achieves 87 AUDRC on the real-world Tuebingen cause-effect pairs dataset, the highest among all methods tested
- Shows superior average precision across 10 common benchmark datasets compared to state-of-the-art approaches
- Achieves above-average precision on 13 different datasets while maintaining computational efficiency (runs in minutes on large datasets)
- Performance degrades on high-noise datasets and confounded relationships, as expected from theoretical assumptions

## Why This Works (Mechanism)

### Mechanism 1: MDL-based Asymmetry Detection
- **Claim:** Infer causal direction by comparing total description length (complexity + fit) of two regression models
- **Mechanism:** LCUBE computes two-part code length $L(Y|X, \theta) = L(\theta) + L(Y|\theta)$, where $L(\theta)$ penalizes model complexity and $L(Y|\theta)$ penalizes lack of fit. The direction with shorter total description length is inferred as causal
- **Core assumption:** Causal mechanism allows more compact description of joint distribution than anti-causal mechanism
- **Evidence anchors:** [abstract] "propose a bivariate causal score based on MDL principle" [section III.B] "causal direction rule from Eq. (5) becomes... min L(Y|X, θX) ≤ min L(X|Y, θY)"

### Mechanism 2: Dense Functional Approximation
- **Claim:** Using cubic splines ensures method can approximate true causal function with arbitrary precision, guaranteeing identifiability
- **Mechanism:** Cubic regression splines with equidistant knots leverage density property - any continuous function on compact interval can be approximated
- **Core assumption:** Underlying causal mechanism is continuous function sufficiently smooth for spline approximation
- **Evidence anchors:** [abstract] "using functions that possess the density property on compact real interval" [section V.A] "class of cubic splines with variable number of knots is dense... approximated by cubic spline within arbitrary precision"

### Mechanism 3: Low Noise Identifiability
- **Claim:** Causal direction distinguishable primarily when noise variance is low relative to signal
- **Mechanism:** Theoretical proof relies on limit where noise parameter $\alpha \to 0$. Anti-causal direction requires significantly higher complexity to model "noise" (includes inverse function structure)
- **Core assumption:** Data follows Additive Noise Model with low noise
- **Evidence anchors:** [abstract] "Gaussianity of noise... is not assumed, only that noise is low" [section IV] "lim n→∞ ... ≤ 1, with equality if and only if function f... is linear"

## Foundational Learning

- **Concept: Minimum Description Length (MDL)**
  - **Why needed here:** Core scoring logic - cannot interpret LCUBE score without understanding trade-off between model complexity and goodness-of-fit
  - **Quick check question:** If I increase number of knots indefinitely, why does MDL score eventually stop decreasing (or start increasing)?

- **Concept: Additive Noise Models (ANM)**
  - **Why needed here:** Method assumes Y = f(X) + N. If true relationship is confounded or has non-additive noise, identifiability guarantees break
  - **Quick check question:** Does LCUBE method require noise N to be Gaussian?

- **Concept: Spline Regression**
  - **Why needed here:** LCUBE uses cubic splines as function approximator. Understanding how knot placement affects flexibility is key to debugging overfitting
  - **Quick check question:** Why does paper insist on "dense" functional classes rather than simple linear or polynomial regression?

## Architecture Onboarding

- **Component map:** Normalizer -> Regression Engine -> Encoder -> Scorer
- **Critical path:** Calculation of RSS (Eq 12) and parameter penalty (Eq 21) for optimal number of knots m, followed by comparison $\hat{\Delta}(X \to Y)$ vs $\hat{\Delta}(Y \to X)$
- **Design tradeoffs:**
  - **Knot count ($m_{max}$):** Paper sets $m_{max} \approx 10$. Increasing improves approximation but increases computational cost ($O(nm + m^3)$) and risks overfitting
  - **Equidistant vs. Variable Knots:** LCUBE uses equidistant knots for speed/density proofs, while alternatives like Slope optimize knot placement (slower)
- **Failure signatures:**
  - **Linearity:** Returns "equality" (undecided) if relationship is perfectly linear
  - **High Noise:** Performance degrades on high-noise synthetic sets where low-noise assumption is violated
  - **Confounders:** Assumes no hidden confounders; performance on "SIM-c" (confounded) is weaker than unconfounded sets
- **First 3 experiments:**
  1. **Tuebingen Pairs Validation:** Run LCUBE on Tuebingen dataset to verify AUDRC score approaches 87
  2. **Linearity Check:** Test on synthetic linear dataset ($Y = 2X + \epsilon$) to verify algorithm correctly returns "undecided"
  3. **Noise Sensitivity:** Generate data $Y = f(X) + \alpha N$ and sweep $\alpha$ to find noise threshold where accuracy drops significantly

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MDL-based causal score framework be successfully instantiated with other dense functional classes, such as harmonic functions, while preserving identifiability?
- **Basis in paper:** [explicit] Authors state in conclusion "aim to explore causal scores via other dense functional classes"
- **Why unresolved:** Work proves identifiability and provides algorithm only for cubic regression splines instantiation
- **What evidence would resolve it:** Theoretical proof of identifiability for LCUBE variant using different dense basis class

### Open Question 2
- **Question:** Can MDL encoding for shallow feed-forward neural networks (FNNs) be constructed such that it satisfies conditions of Identifiable Regression-based Scoring Function (IRSF)?
- **Basis in paper:** [inferred] Section VI.B discusses density property of FNNs but notes difficulty finding encodings characterizing structure and learning
- **Why unresolved:** While FNNs are dense (universal approximators), their parameterization and training complexity make defining necessary code length $L(\theta)$ non-trivial
- **What evidence would resolve it:** Derivation of valid MDL code for FNNs adhering to strictly increasing function requirements

### Open Question 3
- **Question:** Can identifiability guarantees of LCUBE be extended to scenarios involving hidden confounders?
- **Basis in paper:** [inferred] Abstract and introduction explicitly restrict scope to "assumption of no hidden confounders"
- **Why unresolved:** Proposed scoring function compares only two directions ($X \rightarrow Y$ vs $Y \rightarrow X$), theoretical proofs rely on independence of noise term
- **What evidence would resolve it:** Modified score or three-variable test remaining valid when independence assumption violated by latent common cause

## Limitations

- **Assumption dependency:** Method requires no hidden confounders, low additive noise, and continuous functional relationships - violations significantly degrade performance
- **Noise sensitivity:** Theoretical guarantees rely on asymptotic limits and low-noise regimes that may not hold in real-world data
- **Implementation ambiguity:** Exact knot placement methodology (equidistant vs data-adaptive) remains unclear, potentially affecting reproducibility

## Confidence

- **High confidence:** Core MDL scoring mechanism and basic algorithm implementation (Algorithm 1, 2) are clearly specified and reproducible
- **Medium confidence:** Theoretical identifiability claims hold under stated assumptions, but practical limitations reduce real-world reliability
- **Low confidence:** Exact implementation details for knot placement and basis functions remain unclear, potentially affecting numerical stability and performance

## Next Checks

1. **Implementation verification:** Replicate LCUBE results on Tuebingen benchmark to confirm AUDRC ≈ 87 and validate exact knot placement methodology
2. **Assumption stress test:** Systematically vary noise levels and test on confounded datasets to quantify performance degradation and identify practical limitations
3. **Method comparison under stress:** Compare LCUBE against state-of-the-art methods (Slope, RECI) on high-noise and confounded synthetic datasets to establish relative robustness