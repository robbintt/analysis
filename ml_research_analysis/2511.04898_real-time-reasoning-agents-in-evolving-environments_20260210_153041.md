---
ver: rpa2
title: Real-Time Reasoning Agents in Evolving Environments
arxiv_id: '2511.04898'
source_url: https://arxiv.org/abs/2511.04898
tags:
- reasoning
- agent
- reactive
- planning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces real-time reasoning as a critical challenge
  for AI agents operating in dynamic environments where the world evolves independently
  of the agent's computation. The authors develop Real-Time Reasoning Gym, the first
  benchmark suite featuring three games (Freeway, Snake, Overcooked) where environment
  states update at fixed intervals regardless of agent reasoning progress.
---

# Real-Time Reasoning Agents in Evolving Environments

## Quick Facts
- arXiv ID: 2511.04898
- Source URL: https://arxiv.org/abs/2511.04898
- Reference count: 40
- Primary result: Introduced Real-Time Reasoning Gym benchmark and AgileThinker dual-thread architecture achieving consistent performance gains across Freeway (0.88), Snake (0.45), and Overcooked (0.89)

## Executive Summary
This paper addresses the critical challenge of real-time reasoning in dynamic environments where the world evolves independently of agent computation. The authors identify a fundamental limitation: neither reactive agents (fast but shallow) nor planning agents (deep but slow) can effectively operate in environments with fixed update intervals. To solve this, they propose AgileThinker, which employs parallel planning and reactive threads with partial trace sharing, enabling informed real-time decisions without waiting for complete analysis. Experiments demonstrate consistent performance improvements across varying cognitive loads and time constraints, with advantages becoming more pronounced as tasks grow more difficult.

## Method Summary
The paper introduces Real-Time Reasoning Gym, featuring three games (Freeway, Snake, Overcooked) where environment states update at fixed intervals regardless of agent reasoning progress. AgileThinker employs two parallel threads: a planning thread for extended reasoning and a reactive thread for timely actions, with the reactive thread accessing partial reasoning traces from the ongoing planning process. The method uses token-based time proxies (T = NT × TPOT) for reproducible evaluation, with hyperparameter TR controlling the resource trade-off between planning depth and reactive responsiveness. Implementation uses DeepSeek R1 for planning and DeepSeek V3 for reactive decisions, with optimal TR values varying by game.

## Key Results
- AgileThinker consistently outperforms single-paradigm methods across varying cognitive loads and time pressures
- Performance advantages become more pronounced as tasks grow more difficult and time constraints tighten
- Wall-clock time experiments confirm benefits translate to real-world deployments (scores of 0.88 Freeway, 0.45 Snake, 0.89 Overcooked under 6-minute time budgets)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel execution of reactive and planning threads enables timely responses while maintaining strategic depth.
- Mechanism: Planning thread streams extended reasoning continuously while reactive thread makes time-constrained decisions using partial traces, avoiding complete analysis wait times.
- Core assumption: Planning insights remain useful across short time horizons even as environment changes, and reactive decisions improve with strategic context.
- Evidence anchors: Abstract confirms reactive thread can access partial reasoning traces; section 3 describes parallel thread architecture.

### Mechanism 2
- Claim: Time-sharing coordination between threads optimizes resource allocation between planning depth and reactive responsiveness.
- Mechanism: Reactive thread activates only in final TR tokens of each environment step, with TR controlling trade-off between adaptive responses and planning guidance.
- Core assumption: Optimal TR approximates natural token upper bound of reactive thread's unconstrained generation.
- Evidence anchors: Section 5 shows performance peaks when NTR approximates natural token upper bound; figure 7 displays performance curves.

### Mechanism 3
- Claim: Strategic guidance from partial planning traces prevents reactive myopia while maintaining reactivity to environmental changes.
- Mechanism: Reactive thread receives strategic insights (collision risks, opportunity costs) from planning traces combined with latest observation.
- Core assumption: Partial traces contain useful strategic insights valid across short time horizons despite observation staleness.
- Evidence anchors: Figure 6 case study shows reactive agent colliding while AgileThinker avoids trap using planning guidance.

## Foundational Learning

- Concept: **Dual-process theory (System 1/System 2)**
  - Why needed here: AgileThinker explicitly draws from dual-process cognition understanding fast intuitive vs slow deliberate reasoning.
  - Quick check question: Can you explain why a single reasoning mode cannot simultaneously optimize for both speed and depth in dynamic environments?

- Concept: **Token-based temporal abstraction**
  - Why needed here: Paper uses generated token count as hardware-agnostic time proxy (T = NT × TPOT).
  - Quick check question: Why is token count preferred over wall-clock time for reproducible evaluation across different hardware configurations?

- Concept: **Budget forcing and test-time compute scaling**
  - Why needed here: Reactive agents use budget forcing to truncate reasoning at token limits.
  - Quick check question: What happens to reasoning quality when a thinking model's generation is forcibly truncated mid-reasoning?

## Architecture Onboarding

- Component map: Environment -> Planning Thread (continuous) + Reactive Thread (TR tokens) -> Action Buffer -> Environment
- Critical path:
  1. Environment provides observation at step start
  2. Planning thread begins extended reasoning immediately
  3. At (TE - TR) tokens remaining, reactive thread activates
  4. Reactive thread reads: latest observation + current partial planning trace
  5. Reactive thread generates action within TR budget
  6. Action submitted to environment; unused planning reasoning continues

- Design tradeoffs:
  - TR tuning: Larger TR → more adaptive reactive responses, less planning guidance; smaller TR → more planning context, risk of truncation
  - Parallel vs concurrent execution: Parallel offers modest gains but concurrent remains effective under resource constraints
  - Thinking vs non-thinking models: Thinking models provide richer traces but longer latency

- Failure signatures:
  - Reactive thread truncation: When TR < natural token requirements, forced truncation yields no-op actions
  - Planning thread obliviousness: When environment changes invalidate planning assumptions mid-reasoning, causing collisions
  - Stale guidance: When planning predictions reference invalid states, reactive decisions become misaligned

- First 3 experiments:
  1. Baseline calibration: Run reactive-only and planning-only agents across all three games at medium difficulty
  2. TR sweep: Implement AgileThinker with TR ∈ {0.5k, 2k, 4k, 5k, 8k} on Freeway; plot score vs TR alongside CDF of reactive token usage
  3. Wall-clock validation: Deploy with real API calls at TE = 6 minutes; confirm simulated performance gains translate to physical time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can improved coordination mechanisms between reactive and planning threads yield further performance gains beyond the simple time-sharing protocol?
- Basis in paper: Future work can improve coordination mechanisms between two threads
- Why unresolved: Current design uses only partial reasoning traces via time-sharing; richer information exchange protocols unexplored
- What evidence would resolve it: Ablation studies comparing alternative coordination protocols against baseline time-sharing

### Open Question 2
- Question: Can the optimal reactive thread budget be determined adaptively without per-environment empirical tuning?
- Basis in paper: Optimal time budget varies across environments and requires empirical tuning
- Why unresolved: Current approach requires manual calibration; no adaptive mechanism proposed
- What evidence would resolve it: Meta-learning or online adaptation algorithm dynamically adjusting reactive budget

### Open Question 3
- Question: Can training LLMs specifically on Real-Time Reasoning Gym produce urgency-aware agents that outperform post-hoc dual-thread architectures?
- Basis in paper: Future work can leverage our gym to train urgency-aware LLM agents
- Why unresolved: AgileThinker uses frozen models; no fine-tuning or training within benchmark attempted
- What evidence would resolve it: Models trained with timely decision rewards showing superior performance

### Open Question 4
- Question: How can real-time reasoning agents operate effectively when reasoning traces are unavailable (e.g., with proprietary models)?
- Basis in paper: Experiments restricted to DeepSeek because other providers don't provide reasoning traces crucial for evaluation
- Why unresolved: AgileThinker's reactive thread depends on accessing partial traces; alternative designs for non-transparent models unexplored
- What evidence would resolve it: AgileThinker variant using only final outputs or external memory performing competitively with closed-source models

## Limitations
- Token-based time proxies rather than direct wall-clock measurements for most results may not capture real-world computational constraints
- Benchmark suite covers only three game domains, raising questions about generalizability to other real-time reasoning scenarios
- Trace-sharing mechanism between planning and reactive threads not fully specified, particularly how partial reasoning outputs are formatted and parsed

## Confidence

**High confidence**: Core observation that neither pure reactive nor pure planning approaches suffice for real-time reasoning agents is well-supported by experimental results across all three games.

**Medium confidence**: Effectiveness of dual-thread architecture with partial trace sharing is demonstrated but relies on simulation experiments for most results, with wall-clock validation at only one configuration point.

**Low confidence**: Generalizability of TR hyperparameter tuning approach across different games and environments, as methodology may not transfer to other domains.

## Next Checks
1. Implement AgileThinker on a new real-time reasoning task (e.g., autonomous vehicle navigation) with different environmental dynamics and document whether partial-trace-sharing mechanism provides similar advantages.

2. Systematically vary token-per-second generation rate (TPOT) across multiple orders of magnitude while holding all other parameters constant and measure performance degradation.

3. Introduce controlled noise and truncation in partial planning traces shared with reactive thread and measure performance degradation as function of trace incompleteness.