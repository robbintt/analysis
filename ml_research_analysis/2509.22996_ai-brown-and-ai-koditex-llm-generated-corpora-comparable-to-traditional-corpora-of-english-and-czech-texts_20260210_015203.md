---
ver: rpa2
title: 'AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora
  of English and Czech Texts'
arxiv_id: '2509.22996'
source_url: https://arxiv.org/abs/2509.22996
tags:
- corpus
- openai
- https
- chat
- brown
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two corpora of English and Czech texts generated
  using large language models (LLMs) - AI Brown and AI Koditex. These corpora were
  created to facilitate linguistic comparisons between human-written and LLM-generated
  texts, addressing the need for directly comparable datasets in corpus linguistics
  research.
---

# AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts

## Quick Facts
- arXiv ID: 2509.22996
- Source URL: https://arxiv.org/abs/2509.22996
- Authors: Jiří Milička; Anna Marklová; Václav Cvrček
- Reference count: 20
- Primary result: Introduces two comparable corpora (English and Czech) of LLM-generated texts for linguistic analysis

## Executive Summary
This paper presents AI Brown and AI Koditex, two corpora of English and Czech texts generated using 13 frontier LLMs from OpenAI, Anthropic, Meta, Alphabet, and DeepSeek. These corpora enable direct comparison between human-written and AI-generated texts by providing stylistically matched datasets. The corpora were generated using prompts extracted from the BE21 and Koditex reference corpora, with each model producing texts at two different temperatures (T=0 and T=1). The resulting datasets are annotated according to Universal Dependencies standards and are accessible through the Czech National Corpus interface, facilitating analyses of formulaicity, stylistic variability, and lexical diversity across different models and languages.

## Method Summary
The researchers created AI Brown and AI Koditex by continuing 500-word prompts extracted from the BE21 English corpus and Koditex Czech corpus using 13 different LLMs. Each model generated text at both deterministic (T=0) and sampling (T=1) temperatures, producing on average 864k tokens per model for English and 768k tokens per model for Czech. The generated texts were cleaned of meta-preambles and refusals, then annotated with UDPipe for tokenization, POS tagging, and syntactic parsing. The final corpora are available in CoNLL-U format and searchable through the KonText interface, with the English part totaling 27M tokens and the Czech part totaling 21.5M tokens.

## Key Results
- AI Brown contains 27M English tokens across 13 models; AI Koditex contains 21.5M Czech tokens
- Formulaic phrases like "a testament to the" propagate across instruction-tuned models but not base models
- Temperature T=1 increases lexical diversity (MATTR) but risks semantic drift; T=0 causes looping but maintains coherence
- LLMs perform better at approximating human writing styles in English than in morphologically rich Czech

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Seeded Stylistic Induction
- **Claim:** Providing a 500-word context window effectively conditions LLMs to mimic specific human registers, enabling direct stylistic comparison.
- **Mechanism:** The model attends to statistical regularities in prompt tokens, shifting probability distribution to align with the prompted genre.
- **Core assumption:** The model's pre-training data contains sufficient representation of the specific register to allow retrieval and continuation.
- **Evidence anchors:** [Section 3.1] Uses 500-word prompts as generation seeds; [Section 4.3.2] notes narrativity is easier to approximate; [Corpus] provides benchmark of stylistic variation.

### Mechanism 2: Temperature-Controlled Mode Collapse vs. Drift
- **Claim:** Deterministic sampling (T=0) minimizes semantic drift at the cost of repetitive "looping," while stochastic sampling (T=1) increases lexical diversity but risks thematic divergence.
- **Mechanism:** T=0 selects highest probability token, trapping in local minima; T=1 samples from full distribution, breaking loops but introducing noise.
- **Core assumption:** Quality is a trade-off between coherence and diversity.
- **Evidence anchors:** [Section 4.1] Explicitly states T=1 models drift, T=0 shows mode collapse; [Section 4.3.3] shows T=1 has greater lexical diversity.

### Mechanism 3: Alignment-Induced Formulaicity
- **Claim:** Instruction-tuning (RLHF/RLAIF) propagates specific formulaic phrases across models, distinct from base models or human writing.
- **Mechanism:** Alignment processes optimize for "safe" personas; if synthetic data is used in training, specific phrases propagate like a "virus."
- **Core assumption:** Formulaicity prevalence is due to alignment data, not base architecture.
- **Evidence anchors:** [Section 4.3.1] Finds "testament to" frequent in AI-Brown but absent in human reference; [Section 4.3.1] suggests synthetic data allows formula propagation.

## Foundational Learning

- **Concept: Universal Dependencies (UD)**
  - **Why needed here:** The corpus utility relies on UD annotations (lemmas, UPOS, syntactic dependencies) to perform linguistic queries.
  - **Quick check question:** Can you query for a specific syntactic relation (e.g., determiner-noun) in a CoNLL-U formatted file?

- **Concept: Corpus Comparability (Brown Tradition)**
  - **Why needed here:** The project's validity rests on comparing AI-generated text against a reference human corpus (BE21/Koditex).
  - **Quick check question:** Why is comparing AI text to a random web scrape less scientifically rigorous than comparing it to the BE21 corpus?

- **Concept: Sampling Temperature**
  - **Why needed here:** It is the primary independent variable in this study, affecting drift vs. looping trade-offs.
  - **Quick check question:** If a model loops at T=0, what is the likely effect on its Moving-Average Type-Token Ratio (MATTR)?

## Architecture Onboarding

- **Component map:** BE21/Koditex -> Splitter (500-word prompt vs. reference) -> Generators (13 LLMs) -> Processor (Cleaner -> UDPipe -> Verticalizer) -> Interface (KonText)
- **Critical path:** Prompting (500-word seed with system instruction) -> Archiving (raw JSON with logprobs) -> Cleaning (stripping preambles/refusals) -> Indexing (vertical format for Czech National Corpus)
- **Design tradeoffs:** Clean vs. Raw data (use clean for stylistic analysis, raw for studying alignment/refusals); Temp 0 vs. 1 (use T=0 for deterministic benchmarking, T=1 for studying natural variance)
- **Failure signatures:** The Loop (repetition at T=0), The Drift (semantic shift at T=1), The Refusal (policy blocks in outputs)
- **First 3 experiments:** 1) Query "a testament to the" across subcorpora to quantify alignment formulaicity; 2) Calculate MATTR for GPT-4o at T=0 vs. T=1; 3) Use CQL to find `[upos="NOUN" & p_lemma="delve"]` to inspect syntactic patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does synthetic data distillation in instruction tuning cause cross-model homogenization of formulaic expressions?
- **Basis in paper:** The authors observe identical formulaic phrases across models from different companies and state synthetic data may be the explanation.
- **Why unresolved:** The paper identifies correlation but lacks access to proprietary training data to confirm causality.
- **What evidence would resolve it:** Comparative analysis of training datasets or controlled experiments with human vs. synthetic instruction data.

### Open Question 2
- **Question:** Can LLMs produce stylistically diverse texts in morphologically rich languages as effectively as in English?
- **Basis in paper:** The authors explicitly ask whether LLMs can produce stylistically diverse texts and conclude performance is better in English than Czech.
- **Why unresolved:** While a performance gap is established, the specific linguistic features causing degradation in Czech remain under-analyzed.
- **What evidence would resolve it:** Fine-grained error analysis of syntactic and morphological register markers in Czech vs. English corpora.

### Open Question 3
- **Question:** What specific factors cause "mode collapse" (repetitive looping) in non-English generation?
- **Basis in paper:** The paper notes Gemini models frequently fell into repetitive loops in Czech while functioning correctly in English.
- **Why unresolved:** The paper documents the failure mode but doesn't determine if the cause is tokenization, attention mechanisms, or data scarcity.
- **What evidence would resolve it:** Ablation studies on specific models focusing on token probability distributions during looping onset.

## Limitations

- Reproducibility challenges across different LLM providers due to unspecified generation dates and potential API version changes
- Incomplete cleaning pipeline with heuristic-based refusal detection that may leave fragments in the corpus
- Assumption that reference corpora (BE21/Koditex) remain stable for longitudinal comparisons without validation

## Confidence

**High Confidence:**
- Corpus contains specified number of tokens (27M English, 21.5M Czech)
- UD annotation quality and distribution
- Temperature effects on lexical diversity (MATTR measurements)

**Medium Confidence:**
- Formulaicity propagation through instruction-tuning mechanisms
- Stylistic approximation quality compared to human baselines
- Drift vs. looping trade-offs being primarily temperature-driven

**Low Confidence:**
- Exact reproduction of generation outputs across all providers
- Completeness of refusal removal in cleaning process
- Long-term stability of reference corpora for future comparisons

## Next Checks

1. **Formulaic Phrase Propagation Analysis:** Query "a testament to the" across all subcorpora using KonText to verify instruction-tuning artifact and compare frequencies across base vs. instruction-tuned models.

2. **MATTR Temperature Calibration:** Calculate Moving-Average Type-Token Ratio for GPT-4o at both T=0 and T=1 using provided vertical files to confirm T=1 consistently shows higher diversity than T=0.

3. **Syntactic Dependency Consistency:** Use CQL to find `[upos="NOUN" & p_lemma="delve"]` in both human and AI-generated subcorpora to compare syntactic patterns and validate UD annotation quality.