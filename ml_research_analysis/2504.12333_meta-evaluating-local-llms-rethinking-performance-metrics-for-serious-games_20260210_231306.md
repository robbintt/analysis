---
ver: rpa2
title: 'Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games'
arxiv_id: '2504.12333'
source_url: https://arxiv.org/abs/2504.12333
tags:
- evaluation
- energy
- llms
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the reliability of five small-scale LLMs (Llama
  3.2, Mistral, Phi-4, Qwen 2.5, DeepSeek R1) as evaluators in a serious game simulating
  energy community decision-making. Using binary classification metrics (accuracy,
  TPR, TNR, F1 scores), we systematically compare model performance across three game
  levels with 18 hand-designed answers (9 true, 9 false).
---

# Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games

## Quick Facts
- arXiv ID: 2504.12333
- Source URL: https://arxiv.org/abs/2504.12333
- Authors: Andrés Isaza-Giraldo; Paulo Bala; Lucas Pereira
- Reference count: 24
- This study evaluates the reliability of five small-scale LLMs as evaluators in a serious game simulating energy community decision-making, revealing significant variation in model reliability and the need for context-aware evaluation frameworks.

## Executive Summary
This study systematically evaluates five small-scale LLMs (Llama 3.2, Mistral, Phi-4, Qwen 2.5, DeepSeek R1) as evaluators in a serious game simulating energy community decision-making. Using binary classification metrics (accuracy, TPR, TNR, F1 scores), we compare model performance across three game levels with 18 hand-designed answers (9 true, 9 false). Results show significant variation in model reliability: Phi-4 achieved the highest overall accuracy (79%) with balanced TPR and TNR, Qwen 2.5 showed strong specificity (98% TNR), while Mistral excelled at detecting positives but struggled with false positives (16% TNR). No single model maintained consistent performance across all levels, highlighting the need for context-aware evaluation frameworks and careful model selection when deploying LLMs for subjective assessment tasks.

## Method Summary
The study evaluated five small LLMs (3B-14B parameters) as binary evaluators for player responses in a serious game about energy community decision-making. Each model assessed 18 hand-designed answers (9 true, 9 false) across three game levels, with each answer evaluated 20 times per model. Models were run locally via Ollama at temperature 0.8 using zero-shot prompting with instructions to prefix responses with "Success!" or "Fail." Performance metrics (accuracy, TPR, TNR, F1 scores) were computed from confusion matrices, and heatmaps were generated to visualize level-specific performance variations.

## Key Results
- Phi-4 achieved highest overall accuracy (79%) with balanced TPR (71%) and TNR (88%) across all levels
- Mistral showed high sensitivity (90% TPR) but poor specificity (16% TNR), accepting almost any answer
- Qwen 2.5 demonstrated strong specificity with 98% TNR but lower sensitivity at 53% TPR
- No single model maintained consistent performance across all three complexity levels
- Performance degraded significantly at Level 3 (city-wide grid management) for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary classification metrics (TPR, TNR, F1) can surface systematic evaluation biases in LLM judges that raw accuracy obscures.
- Mechanism: By treating each LLM evaluation as a classification decision against ground-truth labels, confusion matrices reveal whether a model is permissive (high TPR, low TNR) or conservative (low TPR, high TNR), enabling targeted model selection for task-specific error tolerances.
- Core assumption: Ground-truth labels from domain experts reliably represent correct evaluation outcomes.
- Evidence anchors:
  - [abstract] "Using binary classification metrics (accuracy, TPR, TNR, F1 scores), we systematically compare model performance across three game levels"
  - [section 4.4] "The performance metrics assist in identifying 'problem areas' AND 'success areas'"
  - [corpus] Limited direct corpus support; neighboring papers focus on LLM-as-judge bias but not this specific metric decomposition approach.
- Break condition: If ground-truth labels are contested or inherently subjective without expert consensus, TPR/TNR comparisons become unreliable.

### Mechanism 2
- Claim: Model architecture and training methodology create predictable trade-offs between sensitivity (catching positives) and specificity (rejecting negatives).
- Mechanism: Smaller models (3B-7B parameters) exhibited higher permissiveness (Mistral: 90% TPR, 16% TNR), while 14B models with curated training data achieved better balance (Phi-4: 71% TPR, 88% TNR). Chain-of-thought reasoning (DeepSeek R1) introduced non-determinism that reduced consistency without improving accuracy.
- Core assumption: Parameter count and training data quality are the primary drivers of evaluation behavior, not prompt design or temperature settings.
- Evidence anchors:
  - [section 5] "Mistral - 7B was the most forgiving, accepting almost any answer... it also showed the worst results at evaluating negatives at 16%"
  - [section 5] "Phi-4 had the best overall accuracy... having an almost perfect score on L2"
  - [corpus] "Do LLM Evaluators Prefer Themselves for a Reason?" suggests model-specific biases exist but doesn't address size-scaling relationships.
- Break condition: If temperature or prompt engineering significantly alters TPR/TNR ratios, architectural inferences become confounded.

### Mechanism 3
- Claim: Task complexity modulates LLM evaluator reliability, with no single model maintaining consistent performance across difficulty levels.
- Mechanism: Performance degraded differentially across levels—for example, Phi-4 dropped to 20% TPR on Level 3 (city-wide grid management) despite 100% TPR on Level 1 (single household). This suggests domain complexity interacts with model-specific knowledge boundaries.
- Core assumption: Level progression in the game corresponds to genuine cognitive/evaluation complexity rather than confounding factors like prompt length.
- Evidence anchors:
  - [abstract] "No single model maintained consistent performance across all levels"
  - [section 5] "In L3, the model showed extremely negative evaluations of most answers, except for A18, for a TPR on L3 of 20%"
  - [corpus] Weak corpus support; no neighboring papers examine complexity-scaling in LLM evaluation.
- Break condition: If level differences are driven by answer ambiguity rather than task complexity, the mechanism would reflect annotation quality rather than model capability.

## Foundational Learning

- Concept: **Confusion Matrix Decomposition (TPR vs TNR)**
  - Why needed here: Raw accuracy masks asymmetric failure modes; a 53% accurate model (Mistral) might be worse for your use case than a 63% accurate model depending on whether false positives or false negatives are costlier.
  - Quick check question: If your game penalizes players for incorrect accepted answers more than for rejected correct answers, which metric should you optimize?

- Concept: **Zero-Shot Evaluation**
  - Why needed here: The study relies on LLMs evaluating responses without task-specific fine-tuning, using only prompt instructions ("evaluate if pro-social and effective").
  - Quick check question: What happens to evaluation consistency when you provide few-shot examples vs. zero-shot instructions?

- Concept: **Temperature and Determinism**
  - Why needed here: All models ran at temperature 0.8; the paper notes Llama 3.2 and DeepSeek R1 showed "less deterministic behavior," which may confound architectural comparisons.
  - Quick check question: How would you isolate the effect of temperature from the effect of model architecture on evaluation consistency?

## Architecture Onboarding

- Component map:
  - Game Engine -> LLM Evaluator Agent -> Metrics Layer -> Ground Truth

- Critical path:
  1. Define evaluation criteria in prompt ("pro-socially and effectively")
  2. Run each answer N times (paper used 20) per level per model
  3. Parse outputs for "Success!"/"Fail" prefixes
  4. Compute TPR, TNR, F1 per level and aggregated
  5. Select model based on task-specific error cost profile

- Design tradeoffs:
  - **Permissiveness vs Specificity**: Mistral accepts most answers (good player experience, poor assessment validity); Qwen rejects aggressively (high validity, potential frustration)
  - **Consistency vs Reasoning**: DeepSeek R1's chain-of-thought adds transparency but introduces variance
  - **Model Size vs Latency**: 3B-7B models run faster but exhibit worse TNR; 14B models balance better but require more VRAM

- Failure signatures:
  - **Permissive evaluator**: TPR > 80% but TNR < 30% → model is rubber-stamping answers
  - **Level collapse**: TPR drops sharply on one level (e.g., Phi-4 on L3) → domain knowledge gap
  - **Non-deterministic outputs**: Same answer receives different evaluations across runs → temperature too high or model instability

- First 3 experiments:
  1. **Baseline metric profiling**: Run your candidate models on a held-out validation set with known ground truth; compute full confusion matrix metrics before deployment.
  2. **Temperature sweep**: Test at least 3 temperature settings (e.g., 0.2, 0.5, 0.8) to quantify consistency-accuracy tradeoffs for your specific prompts.
  3. **Level-stratified analysis**: If your game has difficulty progression, evaluate whether model performance degrades at higher levels and consider level-specific model selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ensemble methods combining high-specificity (e.g., Qwen 2.5) and high-sensitivity (e.g., Phi-4) models outperform single-model evaluators in serious games?
- Basis in paper: [explicit] The authors identify "leveraging ensemble methods to combine the strengths of Qwen and Phi-4" as a promising avenue for future work to balance TPR and PPV.
- Why unresolved: The current study only evaluated models individually, finding distinct trade-offs (e.g., Qwen's high specificity vs. Mistral's high sensitivity) but no single reliable model.
- What evidence would resolve it: Comparative experiments using voting or stacking strategies that demonstrate superior F1 scores and balanced confusion matrices compared to individual models.

### Open Question 2
- Question: Does chain-of-thought (CoT) reasoning degrade the consistency of LLM evaluations in zero-shot game contexts?
- Basis in paper: [inferred] The authors observed that DeepSeek R1 (a CoT model) exhibited "less deterministic behavior" and would sometimes "deceive itself," suggesting CoT might be a liability.
- Why unresolved: DeepSeek R1 was the only CoT architecture tested, making it unclear if the poor performance was due to the reasoning process or the underlying model architecture.
- What evidence would resolve it: Ablation studies on a single model architecture evaluating the same tasks with CoT prompting enabled versus disabled.

### Open Question 3
- Question: To what extent does model size (parameter count) versus training data quality determine evaluation accuracy for local LLMs?
- Basis in paper: [explicit] The authors conclude it is "essential to assess the relevance of training configurations and model sizes" to determine if performance gains stem from parameters or efficient architectures.
- Why unresolved: The study compared models of varying sizes (3B to 14B) and architectures simultaneously, making it impossible to isolate the effect of model scale on evaluation reliability.
- What evidence would resolve it: A controlled comparison evaluating different parameter sizes (e.g., 7B vs. 14B vs. 32B) within the same model family (e.g., Qwen 2.5) on the evaluation task.

### Open Question 4
- Question: Can level-specific prompt engineering mitigate the performance degradation observed in complex game scenarios (Level 3)?
- Basis in paper: [explicit] The authors suggest "fine-tuning of models for specific levels" or adjusting prompts to improve performance where models like Phi-4 and Llama showed significantly low TPR.
- Why unresolved: High failure rates were observed in Level 3 (city-wide complexity) across most models, suggesting the generic system prompt failed to guide the models through higher complexity.
- What evidence would resolve it: Testing adapted system prompts that explicitly define evaluation criteria for complex multi-agent scenarios, resulting in stabilized TPR/TNR metrics across all levels.

## Limitations
- Small answer set (18 total) with hand-designed responses may not capture natural player variability
- Ground-truth labels represent expert consensus but may contain subjective judgments
- Temperature setting of 0.8 introduces non-determinism that confounds architectural comparisons

## Confidence
- **High confidence**: Binary classification metrics (TPR/TNR) can reveal asymmetric evaluator biases that raw accuracy masks
- **Medium confidence**: Model architecture (parameter count) correlates with permissiveness vs specificity trade-offs, though temperature effects remain uncontrolled
- **Low confidence**: Claims about task complexity affecting model reliability across levels require more extensive testing with naturally-generated responses

## Next Checks
1. Test the same five models on a larger, naturally-occurring dataset of player responses (minimum 100 unique answers) to verify if architectural patterns hold with authentic content
2. Conduct a controlled temperature sweep (0.2, 0.5, 0.8) for each model to isolate the effect of stochasticity on TPR/TNR ratios
3. Implement an ablation study where ground-truth labels are generated by multiple independent experts to quantify inter-rater reliability and its impact on metric validity