---
ver: rpa2
title: 'Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants'
arxiv_id: '2510.24328'
source_url: https://arxiv.org/abs/2510.24328
tags:
- arabic
- language
- llms
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating large language
  models on culturally grounded and dialect-specific knowledge in Arabic. The authors
  extend an existing Modern Standard Arabic multiple-choice dataset into open-ended
  questions and dialectal variants (Egyptian, Levantine, Gulf, Maghrebi, and English).
---

# Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants

## Quick Facts
- arXiv ID: 2510.24328
- Source URL: https://arxiv.org/abs/2510.24328
- Reference count: 0
- Arabic dialect QA benchmark reveals format and dialect performance gaps

## Executive Summary
This work addresses the challenge of evaluating large language models on culturally grounded and dialect-specific knowledge in Arabic. The authors extend an existing Modern Standard Arabic multiple-choice dataset into open-ended questions and dialectal variants (Egyptian, Levantine, Gulf, Maghrebi, and English). They benchmark multiple models in both MCQ and OEQ formats, and generate chain-of-thought annotations for fine-tuning. Results show that MCQ performance is consistently higher than OEQ, especially for dialects. Arabic-centric models perform well on MCQs but lag in OEQs. CoT fine-tuning improves judged correctness but reduces lexical overlap. The dataset is the first to provide parallel QAs across multiple Arabic dialects and English, enabling cross-variety and cross-format evaluation.

## Method Summary
The authors extend a Modern Standard Arabic multiple-choice dataset (PalmX-GC) by translating it into five Arabic dialects and English, then converting MCQs to open-ended questions. They generate chain-of-thought annotations using a four-stage pipeline (generate, rationalize with gold, verify, match) and apply LoRA fine-tuning. The evaluation uses three metrics: MCQ accuracy, n-gram overlap (BERTScore, ROUGE-L), and LLM-as-judge scoring. Models are tested in zero-shot and fine-tuned settings across all language variants.

## Key Results
- MCQ performance consistently exceeds OEQ performance across all models and dialects
- Arabic dialects show significant performance gaps compared to MSA, with Maghrebi performing worst
- CoT fine-tuning improves LLM-judge scores but reduces n-gram metrics like BERTScore F1
- English prompts outperform native-language prompts for Arabic reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting MCQs to open-ended questions (OEQs) reveals actual model understanding by eliminating selection-based shortcuts.
- Mechanism: MCQ formats allow models to exploit positional bias (e.g., favoring option "A") or elimination strategies without true comprehension. OEQs force models to generate answers from internal knowledge, exposing gaps that MCQ accuracy masks.
- Core assumption: Assumption: Performance differences between MCQ and OEQ primarily reflect format exploitation rather than evaluation noise.
- Evidence anchors:
  - [abstract]: "benchmark a range of zero-shot and fine-tuned LLMs under both MCQ and OEQ settings"
  - [section 1]: "models can sometimes exploit the test format rather than genuinely understanding the question, leading to a form of selection bias, for instance, consistently favoring certain options"
  - [corpus]: Myrzakhan et al. (2024, cited in paper) show MCQ→OEQ conversion exposes deeper understanding gaps
- Break condition: If OEQ performance consistently matches MCQ performance across models, format artifacts may not be the primary driver of observed differences.

### Mechanism 2
- Claim: Dialectal underperformance stems from training data distribution skewed toward MSA and high-resource languages.
- Mechanism: Arabic dialects differ lexically, morphologically, and pragmatically from MSA. Models trained predominantly on MSA/English lack exposure to dialectal patterns, causing degraded performance even when cultural content is semantically equivalent.
- Core assumption: Assumption: The gap is primarily a data coverage problem, not an architectural limitation in representing dialectal variation.
- Evidence anchors:
  - [abstract]: "models underperform on Arabic dialects, revealing persistent gaps in culturally grounded and dialect-specific knowledge"
  - [section 5, Table 2]: MSA accuracy averages ~61% across base models vs. ~57% for Maghrebi; fine-tuning narrows but doesn't eliminate gaps
  - [corpus]: NileChat (arXiv:2505.18383) and related work address similar low-resource language enhancement challenges
- Break condition: If Arabic-centric models with explicit dialect training data show no improvement over general multilingual models, architectural constraints may be implicated.

### Mechanism 3
- Claim: Chain-of-thought (CoT) fine-tuning improves semantic correctness as judged by LLMs while reducing n-gram overlap with gold references.
- Mechanism: CoT encourages reasoning traces that yield briefer, semantically valid answers. These differ lexically from longer gold references, lowering token-based metrics (BERTScore F1, ROUGE-L) while improving judged helpfulness/accuracy.
- Core assumption: Assumption: LLM-as-judge scoring reflects human preferences for semantic correctness better than n-gram metrics for this task domain.
- Evidence anchors:
  - [abstract]: "CoT improves judged correctness while yielding mixed n-gram-based metrics"
  - [section 5, Table 4]: CoT model achieves highest LLM-judge score (6.01) but lower F1 (0.67) vs. non-CoT FT (0.72)
  - [corpus]: Zhu et al. (2025) and Li et al. (2025, both cited) show CoT rationales can hurt performance depending on task/model size
- Break condition: If human evaluation contradicts LLM-as-judge scores, the improvement may be an artifact of judge model preferences rather than genuine semantic gains.

## Foundational Learning

- Concept: **MCQ vs. OEQ evaluation paradigms**
  - Why needed here: Understanding why format choice affects measured performance and what each format actually tests.
  - Quick check question: If a model achieves 75% MCQ accuracy but 45% OEQ F1, what two factors could explain this gap?

- Concept: **Arabic dialect continuum and diglossia**
  - Why needed here: MSA and regional dialects differ systematically; performance on one doesn't predict performance on others.
  - Quick check question: Name three linguistic dimensions where Arabic dialects diverge from Modern Standard Arabic (hint: see Section 3.1).

- Concept: **CoT fine-tuning vs. inference-time CoT prompting**
  - Why needed here: This paper uses fine-tuning on generated CoT traces, which has different failure modes than prompting alone.
  - Quick check question: What is the difference between zero-shot CoT ("Let's think step by step") and the 4-stage CoT pipeline described in Section 3.4?

## Architecture Onboarding

- Component map:
Source Dataset (PalmX-GC: 2K train, 500 dev, 2K test, MSA MCQ) → [Translation Module] → [MCQ→OEQ Converter] → [CoT Generation Pipeline] → [Training] → [Evaluation Stack]

- Critical path:
1. **Dataset construction**: Translate MSA MCQ → dialects/English → convert to OEQ → generate/verify CoT chains
2. **Fine-tuning**: Apply LoRA to target models (ALLaM-7B for OEQ; all models for MCQ)
3. **Evaluation**: Run zero-shot and FT models across 6 language variants, 2 formats, 3 metric types

- Design tradeoffs:
  - LLM-assisted translation/conversion vs. full human verification (speed/cost vs. quality; paper uses spot-checks only)
  - CoT fine-tuning: semantic gains vs. n-gram metric losses (depends on deployment context)
  - English prompts vs. native-language prompts (prior work shows English prompts often outperform for reasoning tasks)

- Failure signatures:
  - High MCQ / low OEQ gap → format exploitation, not genuine knowledge
  - MSA >> dialect performance → insufficient dialect coverage in training
  - CoT improves judge score but drops F1/ROUGE → metric misalignment; check answer brevity

- First 3 experiments:
1. **Baseline profiling**: Run all base models (Falcon, NileChat, Fanar, Qwen, ALLaM, GPT-4.1/5) on MCQ across 6 language variants to establish dialect gaps.
2. **Fine-tuning ablation**: Compare base vs. LoRA-FT models on MCQ; isolate gains from task-specific training vs. pre-existing knowledge.
3. **CoT impact analysis**: For OEQ, compare base / FT / FT+CoT using all three metric types; manually inspect low-F1/high-judge cases to validate semantic correctness.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies heavily on GPT-4.1 with only spot-checks, lacking human-verified ground truth
- Evaluation uses LLM-as-judge without human validation, potentially reflecting model-specific preferences
- Dataset covers only 6 Arabic dialects, representing a subset of linguistic diversity

## Confidence
- **High confidence**: MCQ performance consistently exceeds OEQ performance with clear statistical significance
- **Medium confidence**: Gap reflects format exploitation rather than evaluation noise, though controlled experiments lacking
- **Medium confidence**: Dialectal gaps are primarily data coverage issues, not empirically tested
- **Low confidence**: Practical significance of CoT improvements without human validation

## Next Checks
1. Conduct human evaluation of OEQ responses to validate LLM-as-judge scores, particularly for cases where CoT improves judge scores but reduces n-gram overlap
2. Test targeted dialect data augmentation to determine if dialectal performance gaps are truly data-driven or reflect deeper architectural limitations
3. Implement controlled experiment comparing MCQ vs. OEQ performance on a subset with human-verified gold answers to isolate format effects