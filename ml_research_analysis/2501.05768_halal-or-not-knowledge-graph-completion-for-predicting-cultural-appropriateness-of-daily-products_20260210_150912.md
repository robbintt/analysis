---
ver: rpa2
title: 'Halal or Not: Knowledge Graph Completion for Predicting Cultural Appropriateness
  of Daily Products'
arxiv_id: '2501.05768'
source_url: https://arxiv.org/abs/2501.05768
tags:
- graph
- cosmetic
- knowledge
- halal
- ingredients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately predicting the
  halal status of cosmetic products, a task complicated by complex ingredient relationships
  and varied certification standards. The authors propose HaCKG, a knowledge graph-based
  framework that models the relationships between cosmetics, ingredients, and their
  properties.
---

# Halal or Not: Knowledge Graph Completion for Predicting Cultural Appropriateness of Daily Products

## Quick Facts
- arXiv ID: 2501.05768
- Source URL: https://arxiv.org/abs/2501.05768
- Authors: Van Thuy Hoang; Tien-Bach-Thanh Do; Jinho Seo; Seung Charlie Kim; Luong Vuong Nguyen; Duong Nguyen Minh Huy; Hyeon-Ju Jeon; O-Joun Lee
- Reference count: 9
- Primary result: HaCKG achieves 96.57% accuracy, 95.73% recall, 97.94% precision, and 96.82% F1-score in halal prediction tasks using a knowledge graph approach

## Executive Summary
This paper addresses the challenge of accurately predicting the halal status of cosmetic products, a task complicated by complex ingredient relationships and varied certification standards. The authors propose HaCKG, a knowledge graph-based framework that models the relationships between cosmetics, ingredients, and their properties. By leveraging a relational graph attention network with pre-training, HaCKG captures high-order and complex entity relationships. Experiments show that HaCKG significantly outperforms state-of-the-art baselines, achieving 96.57% accuracy, 95.73% recall, 97.94% precision, and 96.82% F1-score in halal prediction tasks.

## Method Summary
HaCKG is a knowledge graph completion framework that predicts the halal status of cosmetic products by modeling entities (cosmetics, ingredients, properties) and their relationships within a structured graph. The method uses a two-stage approach: pre-training on triplet structural similarity using margin ranking loss, followed by fine-tuning on the classification task using binary cross-entropy loss. The core architecture is a relational graph attention network (r-GAT) that incorporates a fusion layer to merge entity embeddings with numerical ingredient properties, with initial residual connections to prevent over-smoothing. The model is trained on a dataset containing 101,186 entities across 11 entity types and 5 relation types, using a 60/20/20 train/val/test split.

## Key Results
- HaCKG achieves 96.57% accuracy, 95.73% recall, 97.94% precision, and 96.82% F1-score on halal prediction
- Pre-training improves accuracy by 5.86 percentage points compared to training from scratch
- Model performs best with 2-3 GNN layers; accuracy drops with deeper layers due to over-smoothing
- Missing numerical attributes reduces accuracy to 94.71%, highlighting their importance

## Why This Works (Mechanism)

### Mechanism 1
Representing cosmetics, ingredients, and properties as entities in a knowledge graph enables the model to capture high-order, multi-hop relationships that isolated ingredient analysis cannot. The Cosmetic Knowledge Graph (CKG) links products → ingredients → properties through explicit relation types. GNN message propagation allows information to flow across these paths (e.g., Product A shares Ingredient X with Product B, which has known halal status), enabling the model to infer status from structural proximity rather than just direct features. This works because halal status depends on relational patterns (shared ingredients, ingredient property chains) rather than only direct ingredient-to-status mappings.

### Mechanism 2
Relational attention weights neighbors by relation type, allowing the model to prioritize more predictive connections. The attention mechanism computes coefficients conditioned on both entity embeddings and relation embeddings. This enables learning that "contains ingredient" edges may be more informative than "belongs to brand" edges for halal prediction, and adjusts aggregation accordingly. This works because relation types contribute unequally to predicting halal status; uniform aggregation would dilute signal.

### Mechanism 3
Self-supervised pre-training on triplet scoring forces embeddings to encode structural proximity, which transfers to the halal classification task. Pre-training optimizes a scoring function to distinguish real KG triplets from corrupted negatives via margin ranking loss. This forces embeddings of structurally related entities (e.g., products sharing ingredients) to be similar, creating a useful initialization before task-specific fine-tuning. This works because structural similarity in the KG correlates with halal status similarity, even without labels during pre-training.

## Foundational Learning

- **Knowledge Graphs & Triplets**:
  - Why needed: The entire framework represents domain knowledge as ⟨head, relation, tail⟩ triples. Understanding entity-relation graphs is prerequisite.
  - Quick check: Given ⟨ProductA, contains, IngredientX⟩ and ⟨IngredientX, has_toxicity, High⟩, what 2-hop path connects ProductA to toxicity?

- **Graph Neural Networks & Message Passing**:
  - Why needed: r-GAT updates node embeddings by aggregating neighbor information. Understanding how attention weights control this aggregation is essential.
  - Quick check: If a cosmetic node has 3 ingredient neighbors with attention weights [0.5, 0.3, 0.2], how is the aggregated embedding computed?

- **Self-Supervised Pre-training**:
  - Why needed: The model learns structural patterns before seeing labels. Understanding why this helps—and when it fails—is critical for reproduction.
  - Quick check: In pre-training, what makes a triplet "positive" vs "negative"?

## Architecture Onboarding

- **Component map**: Entity embeddings + numerical attributes → Fusion layer → r-GAT layers (with residual) → Concatenated output → Fine-tuning MLP → Halal probability
- **Critical path**: Fusion → r-GAT layers (with residual) → concatenated output → fine-tuning MLP → halal probability
- **Design tradeoffs**: GNN depth (l) optimal at 2-3 per Figure 4; deeper causes over-smoothing. Residual strength (α) balances local vs. global structure; removal drops accuracy 5.5 points. Pre-training adds computation but gains ~5.9 accuracy points.
- **Failure signatures**: Over-smoothing: Accuracy declines at l > 3. Missing numerical attributes: Accuracy drops to 94.71%. No pre-training: Accuracy drops to 91.71%.
- **First 3 experiments**:
  1. Reproduce pre-training ablation: Train with/without pre-training. Expect ~5 point gap. If smaller, verify pre-training hyperparameters.
  2. Vary GNN depth: Test l=1,2,3,4,5. Expect peak at l=2-3. If no peak, dataset may lack multi-hop structure.
  3. Inspect attention weights: Visualize learned attention per relation type on sample products. Expect higher weights for ingredient/property relations vs. brand relations. If uniform, verify relation embeddings are being trained.

## Open Questions the Paper Calls Out
- Can integrating contrastive learning into the pre-training phase further enhance the model's representation capability compared to the current structural similarity approach?
- Does the HaCKG framework effectively generalize to other "daily products" like food or pharmaceuticals, which possess distinct regulatory complexities and chemical interactions?
- Can the knowledge graph schema distinguish between different sources of the same chemical ingredient to resolve status conflicts (e.g., synthetic ethanol vs. fermentation ethanol)?

## Limitations
- Evaluation limited to one cultural domain (halal status) and one product category (cosmetics), limiting generalizability
- KG construction relies on a single corporate dataset from Shukran Korea, raising external validity concerns
- Exact negative sampling strategy for pre-training remains unspecified, affecting reproducibility

## Confidence
- High confidence: KG-based representation learning for cultural prediction, r-GAT architecture implementation, pre-training + fine-tuning pipeline
- Medium confidence: Performance superiority over baselines, ablation study interpretations
- Low confidence: Generalizability across domains, negative sampling methodology

## Next Checks
1. Reproduce the pre-training ablation with explicit negative sampling strategy to verify the ~5.9 point accuracy drop
2. Test model performance on an independent halal certification dataset from a different source to assess external validity
3. Conduct ablation on relation types to confirm that attention weights meaningfully prioritize ingredient/property relations over other connections