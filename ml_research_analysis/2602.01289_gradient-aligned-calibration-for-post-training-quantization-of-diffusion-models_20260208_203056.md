---
ver: rpa2
title: Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models
arxiv_id: '2602.01289'
source_url: https://arxiv.org/abs/2602.01289
tags:
- quantization
- timesteps
- diffusion
- gradient
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of gradient conflicts in post-training
  quantization of diffusion models. Existing PTQ methods for diffusion models assign
  uniform weights to calibration samples across timesteps, which is sub-optimal due
  to varying activation distributions and gradients.
---

# Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models

## Quick Facts
- arXiv ID: 2602.01289
- Source URL: https://arxiv.org/abs/2602.01289
- Authors: Dung Anh Hoang; Cuong Pham anh Trung Le; Jianfei Cai; Toan Do
- Reference count: 38
- Key outcome: Achieves 0.45-0.46 FID improvement on CIFAR-10 and LSUN-Bedrooms under W4A32 and W4A8 settings

## Executive Summary
This paper addresses the problem of gradient conflicts in post-training quantization of diffusion models. Existing PTQ methods assign uniform weights to calibration samples across timesteps, which is sub-optimal due to varying activation distributions and gradients. The authors propose a novel meta-learning-based approach that dynamically learns sample-wise importance weights to align gradients across timesteps during quantization. Extensive experiments demonstrate consistent improvements over state-of-the-art PTQ methods, with FID score reductions of 0.45-0.46 on CIFAR-10 and LSUN-Bedrooms under W4A32 and W4A8 settings.

## Method Summary
The method formulates post-training quantization as a bi-level optimization problem where sample weights are learned to minimize a validation loss that includes both MSE and gradient alignment terms. The outer loop optimizes sample weights ω to minimize a validation loss, while the inner loop updates the quantized model using weighted calibration samples. The validation loss combines MSE reconstruction loss with a gradient matching term that encourages aligned gradients across timestep groups. The approach uses a second-order gradient matching surrogate (L^(2)_GM) that implicitly minimizes the true gradient alignment objective (L_GM) while remaining computationally tractable. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate consistent improvements over state-of-the-art PTQ methods.

## Key Results
- FID improvements of 0.45-0.46 on CIFAR-10 and LSUN-Bedrooms under W4A32 and W4A8 settings
- Particularly effective in low-bit regimes (W4A8) where improvements are most pronounced
- Shows consistent performance across different architectures (DDPM, LDM-4) and datasets
- Gradient alignment validation loss correlates with generation quality improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration samples from different diffusion timesteps produce conflicting gradient signals during quantization, which degrades quantized model performance when treated uniformly.
- Mechanism: The paper demonstrates that gradients computed on calibration samples vary significantly across timesteps (Figure 1a shows cosine distance variation). When these divergent gradients are averaged with uniform weights, the quantization optimization receives conflicting directional signals, causing the model to perform well on some timesteps while underperforming on others.
- Core assumption: Gradient direction consistency across timesteps correlates with better quantization outcomes.
- Evidence anchors:
  - [abstract] "treating them equally can lead to conflicting gradients that degrade performance"
  - [section 3] Figure 1a shows pairwise cosine distance between gradient vectors varies from 0.2 to 1.0 across timesteps
  - [section 3] "gradients from earlier timesteps exhibit higher consistency, those from later stages tend to diverge more noticeably"
  - [corpus] Limited corpus support for diffusion-specific gradient conflict; related work on PTQ calibration exists but doesn't address timestep dynamics
- Break condition: If gradient alignment doesn't correlate with FID improvement, or if the computational overhead of meta-optimization exceeds benefits.

### Mechanism 2
- Claim: A bi-level meta-learning framework can learn sample-wise importance weights that improve validation performance while promoting gradient alignment.
- Mechanism: The outer loop optimizes sample weights ω to minimize a validation loss (Eq. 1), while the inner loop updates the quantized model using weighted calibration samples. The validation loss combines MSE reconstruction loss with a gradient matching term that encourages aligned gradients across timestep groups.
- Core assumption: Sample weights that optimize the surrogate validation objective will transfer to improved generation quality on held-out data.
- Evidence anchors:
  - [section 4.1] Eq. 1 defines the bi-level optimization: ω* = argmin L_VAL(θ*_Q(ω), θ_FP, X^(V))
  - [section 4.2] L_VAL = L_GM + L_MSE where L_GM promotes gradient alignment across timestep groups
  - [section 5.2] FID improvements of 0.45-0.46 on CIFAR-10 and LSUN-Bedrooms under W4A32 and W4A8
  - [corpus] Related PTQ methods (TFMQ-DM, Q-Diffusion) use uniform weighting but don't employ meta-learning for calibration
- Break condition: If the validation set is too small or unrepresentative, meta-overfitting may occur; temperature parameter τ too small causes weight collapse.

### Mechanism 3
- Claim: Optimizing a second-order gradient matching surrogate (L^(2)_GM) implicitly minimizes the true gradient alignment objective (L_GM) while remaining computationally tractable.
- Mechanism: Direct optimization of L_GM requires third-order derivatives. The paper proves (Theorem 4.1, Lemma 4.2, 4.3) that minimizing L^(2)_GM—which measures gradient alignment w.r.t. sample weights rather than model parameters—serves as a faithful surrogate that Algorithm 2 can optimize efficiently.
- Core assumption: The first-order Taylor approximation used in the proof holds sufficiently well during optimization.
- Evidence anchors:
  - [section 4.2] Lemma 4.2 states: "minimization of L^(2)_GM will implicitly lead to minimization of L_GM"
  - [appendix A.2] Full proof shows that cos(G_ω,i, G_ω,j) = 1 implies cos(G_θ*Q,i, G_θ*Q,j) = 1 for small learning rate
  - [section 5] Empirical validation via FID improvements across multiple datasets and bit-width settings
  - [corpus] No direct corpus validation of this surrogate approach; appears novel to this work
- Break condition: If learning rate η is too large, Taylor approximation breaks down; if Hessian spectral norm varies dramatically across timesteps.

## Foundational Learning

- Concept: **Post-Training Quantization (PTQ)**
  - Why needed here: The entire method operates within the PTQ paradigm—compressing a pretrained diffusion model without retraining. Understanding calibration data's role is essential.
  - Quick check question: Can you explain why PTQ requires calibration data and how it differs from quantization-aware training?

- Concept: **Diffusion Model Denoising Timesteps**
  - Why needed here: The core insight is that different timesteps have different activation distributions and gradient dynamics. You must understand how diffusion models progressively denoise across timesteps.
  - Quick check question: Why would the gradient of a loss function differ when computed at early (noisy) vs. late (clean) timesteps?

- Concept: **Bi-level Optimization / Meta-Learning**
  - Why needed here: The method learns sample weights by optimizing a validation objective where the quantized model itself is updated in an inner loop. This is a meta-learning structure.
  - Quick check question: In Eq. 1, what is optimized in the outer loop vs. the inner loop, and why can't this be solved with standard gradient descent?

## Architecture Onboarding

- Component map:
  1. **Calibration Data Generator**: Samples from full-precision model at fixed intervals (following Q-Diffusion)
  2. **Training/Validation Split**: Timestep-balanced partitioning; 5% typically used for validation
  3. **Sample Weight Optimizer** (Algorithm 2): Meta-optimization loop using `higher` library for gradient-based weight learning
  4. **Block-wise Quantizer**: AdaRound + block reconstruction for weights; EMA-based activation quantization (from TFMQ-DM)
  5. **Gradient Alignment Loss**: L_GM computed across T timestep groups (default: 5 groups)

- Critical path:
  1. Generate calibration data via full-precision inference
  2. Split into training/validation sets (timestep-balanced)
  3. For each layer block:
     - Run Algorithm 2 to learn sample weights (1500 iterations, Adam, lr=5e-6 for weights)
     - Apply AdaRound quantization with weighted calibration samples (20000 iterations)
  4. Evaluate using FID/sFID on 50K generated samples

- Design tradeoffs:
  - **Validation set size**: 5% works best; larger doesn't consistently improve (Table 3a)
  - **Number of timestep groups**: 5 groups is default; more groups add compute without clear gain (Table 5)
  - **Temperature τ**: Default 1.0; smaller values degrade performance (Table 3b)
  - **Compute vs. quality**: ~3.5 GPU hours on LSUN-Bedrooms vs. 2.32 for TFMQ-DM baseline—50% more compute for ~0.4 FID improvement

- Failure signatures:
  - **Weight collapse**: If τ too small, weights concentrate on few samples (ablation shows τ=0.2 → FID 4.85 vs. 4.28)
  - **Meta-overfitting**: Very small validation sets (<2%) show degraded sFID
  - **Gradient explosion**: If learning rate for sample weights too high, optimization becomes unstable
  - **No improvement over baseline**: Check that timestep grouping is implemented correctly; groups should span consecutive timesteps

- First 3 experiments:
  1. **Reproduce CIFAR-10 DDPM W4A32 baseline**: Start with TFMQ-DM numbers (FID 4.73), then add sample weighting. Should achieve ~4.28 FID. Validates entire pipeline.
  2. **Ablate gradient matching term**: Remove L_GM from validation loss, keeping only L_MSE. Expect degradation, confirming gradient alignment's contribution.
  3. **Visualize learned weights vs. gradient alignment**: Reproduce Figure 2—plot sample weights against average gradient similarity. Should show positive correlation, confirming mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the gradient-aligned calibration method generalize effectively to large-scale text-to-image diffusion models with complex textual conditioning?
- Basis in paper: [inferred] The paper evaluates the method on DDPM and LDM for class-conditional and unconditional generation (CIFAR-10, LSUN, ImageNet), but does not test on text-conditioned architectures like Stable Diffusion.
- Why unresolved: Text embeddings introduce high-dimensional, variable semantic inputs that may alter the gradient dynamics and conflict patterns across timesteps differently than class labels.
- What evidence would resolve it: Quantitative results (FID/CLIP score) from applying the method to standard text-to-image benchmarks (e.g., MS-COCO) using latent diffusion models.

### Open Question 2
- Question: Can an adaptive timestep grouping strategy improve performance over the uniform partitioning currently used?
- Basis in paper: [inferred] The authors state, "In practice, we uniformly divide timesteps into 5 groups for simplicity," and acknowledge that "adjacent timesteps often exhibit similar gradient behavior," suggesting the current fixed grouping is a heuristic.
- Why unresolved: Figure 1a shows that gradient dissimilarity varies non-uniformly across the denoising process; fixed groups may mix divergent gradient directions or separate similar ones.
- What evidence would resolve it: Ablation studies comparing uniform grouping against grouping strategies based on clustering gradient similarity metrics.

### Open Question 3
- Question: Can the computational overhead of the meta-learning optimization be reduced to facilitate faster deployment?
- Basis in paper: [inferred] The paper notes that while efficient, the method adds computational cost (approx. 3.5 GPU hours vs. 2.32 for baselines) due to the iterative weight updates.
- Why unresolved: The bi-level optimization requires distinct inner and outer loops (Algorithm 2), which inherently demands more compute than single-pass calibration.
- What evidence would resolve it: A modified optimization scheme that reduces convergence iterations or utilizes a one-shot weight estimation while maintaining FID scores.

## Limitations

- The method's effectiveness may not generalize to larger-scale diffusion models beyond LDM-4, as experiments are limited to relatively small architectures
- The computational overhead of the bi-level optimization (approximately 50% more compute than TFMQ-DM) may become prohibitive for larger models
- The validation of the gradient alignment surrogate (L^(2)_GM) relies on theoretical proofs that assume small learning rates, but the actual optimization uses larger learning rates that may violate these assumptions

## Confidence

- **High confidence**: The core observation that gradients vary across timesteps (supported by Figure 1a showing cosine distance variation from 0.2 to 1.0) and that the bi-level optimization framework is mathematically sound
- **Medium confidence**: The effectiveness of the second-order gradient matching surrogate (L^(2)_GM) as a faithful approximation, given the reliance on first-order Taylor approximations in the proof
- **Medium confidence**: The transferability of sample weights learned via validation loss to actual generation quality improvements, as this assumes the validation set is representative

## Next Checks

1. **Test gradient alignment correlation**: Systematically vary the learning rate η in the inner loop and measure whether the theoretical guarantee (cos(G_ω,i, G_ω,j) = 1 implies cos(G_θ*Q,i, G_θ*Q,j) = 1) still holds, establishing the robustness of the surrogate approach
2. **Evaluate validation set sensitivity**: Run ablation studies with validation set sizes ranging from 1% to 20% to determine the minimum required size for stable meta-optimization and identify potential meta-overfitting
3. **Benchmark against specialized methods**: Compare FID improvements specifically on W4A8 quantization (where the paper claims largest gains) against other diffusion-specific PTQ methods to isolate the contribution of gradient alignment versus other design choices