---
ver: rpa2
title: Improving LLM-based Ontology Matching with fine-tuning on synthetic data
arxiv_id: '2511.22612'
source_url: https://arxiv.org/abs/2511.22612
tags:
- complex
- matching
- ontology
- alignments
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of using large language models
  (LLMs) for complex ontology matching, specifically generating alignments between
  ontology modules. The core method involves combining search space reduction to select
  relevant ontology subsets with synthetic data generation for fine-tuning LLMs.
---

# Improving LLM-based Ontology Matching with fine-tuning on synthetic data

## Quick Facts
- arXiv ID: 2511.22612
- Source URL: https://arxiv.org/abs/2511.22612
- Reference count: 21
- Primary result: Fine-tuning LLMs on synthetic data significantly improves performance on ontology matching tasks, with Qwen3-14B achieving highest average F-measure for both simple and complex alignments.

## Executive Summary
This paper addresses the challenge of using large language models (LLMs) for complex ontology matching by combining search space reduction with synthetic data generation for fine-tuning. The method partitions large ontologies into smaller, focused subontology pairs using PageRank centrality and semantic similarity, then generates synthetic training data by having LLMs create ontologies from alignment templates. Experiments on five OAEI Complex track datasets demonstrate that the fine-tuned LLM outperforms the base model, particularly for simple alignments, while highlighting limitations in handling complex n:m correspondences.

## Method Summary
The approach involves three key components: (1) search space reduction that uses PageRank centrality and embedding similarity to partition ontologies into relevant subontology pairs, (2) synthetic data generation where an LLM fills masked placeholders in EDOAL templates and generates corresponding ontologies, and (3) format-constrained fine-tuning on this synthetic data to improve both task understanding and output format compliance. The method is evaluated across three variants - baseline zero-shot, cross-validation fine-tuning, and synthetic data fine-tuning - using Llama-3.2-3B-Instruct as the fine-tuning model.

## Key Results
- Fine-tuned LLM models (b1, b2) show improved precision and F1 scores compared to the baseline across all datasets
- Qwen3-14B (zero-shot) achieves the highest average F-measure for both simple and complex alignments
- Synthetic data fine-tuning (b2) significantly improves simple alignments but struggles with complex n:m correspondences
- b1 variant increases precision on complex alignments at the cost of recall
- Automated repair pipeline successfully handles syntactic errors, achieving 95% validation success rate

## Why This Works (Mechanism)

### Mechanism 1: Search Space Reduction via Structural-Semantic Partitioning
Partitioning large ontologies into smaller subontology pairs makes the matching task tractable within LLM context windows while preserving alignment-relevant content. PageRank centrality identifies structurally significant entities, while embedding similarity retrieves semantically related target entities. Local modules are extracted around both sets, forming paired subontologies for individual LLM calls. Core assumption: important correspondences involve structurally central entities and their semantic neighbors. Break condition: If key correspondences involve entities with low PageRank or outside semantic similarity thresholds, they will be excluded, causing recall loss.

### Mechanism 2: Inverted Synthetic Data Generation
Generating ontologies from alignment templates is cognitively easier for an LLM than generating alignments from ontologies, enabling scalable training data creation. EDOAL grammar templates with masked entity placeholders are filled by an LLM, then source and target ontologies are generated consistent with the alignment. Negative examples teach the model when not to align. Core assumption: models learn transferable alignment patterns even from synthetic data that may lack semantic coherence. Break condition: If synthetic ontology pairs lack semantic coherence or equivalence relationships do not hold, models may learn spurious syntactic patterns.

### Mechanism 3: Format-Constrained Fine-tuning for Structured Output
Fine-tuning on synthetic EDOAL-formatted data improves both task understanding (when to align) and output format compliance (how to express alignments). Synthetic data exposes the model to EDOAL syntax patterns, reducing the need to infer format from zero-shot prompts. Negative examples teach restraint, reducing false positives. Core assumption: structured syntax patterns transfer to real matching tasks. Break condition: If EDOAL templates do not cover the diversity of real alignment patterns, transfer will be limited.

## Foundational Learning

- **Concept: EDOAL (Expressive and Declarative Ontology Alignment Language)**
  - Why needed here: EDOAL is the target output format. Understanding its grammar is essential for constructing templates, validating outputs, and diagnosing syntactic errors.
  - Quick check question: Can you write a simple EDOAL correspondence expressing that `ex:FullName` is equivalent to the concatenation of `ex:FirstName` and `ex:LastName`?

- **Concept: PageRank and Graph Centrality for Ontology Entities**
  - Why needed here: PageRank identifies structurally significant entities to seed subontology extraction. Understanding why centrality correlates with alignment importance is key to debugging reduction quality.
  - Quick check question: Given an ontology where `Conference` has many incoming references and `AuxiliaryProperty` has none, which entity will PageRank prioritize, and what alignment scenarios might this miss?

- **Concept: Precision-Recall Tradeoffs in Fine-tuning Strategies**
  - Why needed here: The paper shows b1 increases precision at the cost of recall, while b2 improves simple F1 but struggles with complex alignments. Understanding this helps select the right strategy for deployment.
  - Quick check question: If your application requires minimizing false positives (high precision) for complex alignments, which fine-tuning variant should you prioritize, and what is the expected recall tradeoff?

## Architecture Onboarding

- **Component map:**
  Space Reduction Module -> Prompt Constructor -> LLM Matcher -> Aggregation Module (Inference path)
  EDOAL Grammar -> Template Generator -> Phi-4 (fill placeholders) -> Phi-4 (generate ontologies) -> Validation/Repair -> Fine-tuning Pipeline (Training path)

- **Critical path:**
  - **Inference**: Ontology pair → Space Reduction (PageRank + embeddings) → Prompt Construction → LLM calls (per subontology pair) → Aggregation (merge, dedupe, repair) → Final EDOAL alignment
  - **Training**: EDOAL grammar → Template generator → Phi-4 fills placeholders → Phi-4 generates ontologies → Validation/repair (95% pass rate) → Fine-tuning data → Instruction fine-tuning (100 epochs for b1, variable for b2)

- **Design tradeoffs:**
  - b1 (cross-validation): Higher precision on complex alignments, lower recall, requires real labeled data, "leave-one-out" evaluation limits training size
  - b2 (synthetic): Improves simple alignments significantly, no labeled data required, but limited to 1:1 correspondences - poor on complex alignments
  - Model size vs. fine-tuning: Qwen3-14B (zero-shot) achieves best average F1, but fine-tuned Llama-3.2-3B can outperform larger base models on specific tasks

- **Failure signatures:**
  - Empty alignment output: Check prompt template, system role, and user format alignment with training data
  - Low recall: Space reduction may be too aggressive (PageRank threshold too high, embedding similarity threshold too strict)
  - Syntactic errors: Missing prefix declarations, missing ontology tags, entities without prefix, invalid literals, premature EOS tokens
  - Complex alignment failure: Synthetic training (b2) was limited to 1:1 - use b1 or extend synthetic data generation
  - Zero scores on specific datasets: Some matchers fail silently on datasets without instance data

- **First 3 experiments:**
  1. Baseline reproduction: Run b0 (zero-shot) with Llama-3.1-8B on Conference dataset. Validate prompt construction matches reported F1=0.28 (simple) and F1=0.05 (complex).
  2. Module isolation test: Use manually created modules to isolate LLM performance from space reduction effects. Compare against automatic reduction to quantify reduction-induced error.
  3. Synthetic data validation: Generate 100 synthetic alignment pairs using template generator and Phi-4. Apply repair script and measure validity rate (target: 95%+). Manually inspect 5% failure cases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating an automated verification step for semantic equivalence in synthetic data generation improve LLM performance on complex alignment tasks?
- Basis in paper: The authors state future work will "expand the synthetic data generation process to include the verification of the integrity and semantic equivalence of correspondences."
- Why unresolved: The current method relies on the hypothesis that useful patterns are learned despite noise, but there is no guarantee that generated entities are semantically coherent or that equivalence holds.
- What evidence would resolve it: A comparison of models fine-tuned on verified synthetic datasets against those trained on unverified data, specifically measuring F-measure on complex OAEI tracks.

### Open Question 2
- Question: To what extent does increasing the diversity of logical constructors in synthetic training data enhance the discovery of n:m correspondences?
- Basis in paper: The authors identify a need to "extend its coverage to a broader variety of logical constructors" to address the current limitation where performance on complex n:m correspondences remains low.
- Why unresolved: The current fine-tuned model (b2) was limited in its ability to generate complex alignments, suggesting the synthetic data did not sufficiently cover the necessary structural complexity.
- What evidence would resolve it: An ablation study varying the complexity of EDOAL grammar rules in the training data and measuring the resulting recall on complex alignment benchmarks.

### Open Question 3
- Question: Can integrating explicit symbolic reasoning mechanisms into the LLM pipeline resolve the structural heterogeneity issues that pure LLMs currently fail to address?
- Basis in paper: The authors suggest that "more improvements can come from integrating explicit reasoning mechanisms into the LLM pipeline" in the conclusion.
- Why unresolved: Pure LLMs often struggle to infer and formalize structural transformations (e.g., composition rules like `FullName` ↔ `FirstName` + `LastName`) solely from prompts.
- What evidence would resolve it: A comparative evaluation of a neuro-symbolic approach versus the fine-tuned LLM baseline on datasets requiring complex subgraph isomorphisms.

## Limitations

- Synthetic data generation is limited to 1:1 correspondences, resulting in poor performance on complex n:m alignments
- Search space reduction strategy may exclude important correspondences involving entities with low PageRank centrality or outside semantic similarity thresholds
- Experimental validation lacks comparison with state-of-the-art ontology matching systems beyond CANARD, which fails on datasets without instance data
- Fine-tuning process relies on a relatively small dataset (343 pairs) for b1 and synthetic data that may contain semantic noise despite automated repair

## Confidence

- **High confidence**: Search space reduction technique effectively reduces computational complexity while maintaining alignment quality; synthetic data fine-tuning improves LLM performance on ontology matching tasks
- **Medium confidence**: Inverted synthetic data generation (ontologies from alignments) is cognitively easier than alignment generation from ontologies; b1 variant (cross-validation) achieves higher precision on complex alignments
- **Low confidence**: Synthetic data generation produces semantically coherent ontology pairs; automated repair pipeline adequately handles all syntactic error types; the approach generalizes to all ontology matching scenarios

## Next Checks

1. **Module Isolation Validation**: Create manually curated ontology modules (following Section 5 procedure) and evaluate LLM performance in isolation from space reduction effects. Compare against automatic reduction to quantify how much performance loss is attributable to aggressive entity filtering.

2. **Complex Alignment Synthetic Data Generation**: Extend the synthetic data generation pipeline to include n:m correspondence templates beyond 1:1 relationships. Fine-tune a new model variant (b3) on this extended synthetic data and evaluate performance on complex alignments across all five OAEI datasets.

3. **State-of-the-Art Comparison**: Implement and evaluate at least two recent ontology matching systems (e.g., AML, LogMap) on the same OAEI 2020 Complex track datasets. Compare their performance against all three LLM variants (b0, b1, b2) to establish relative effectiveness in both simple and complex alignment scenarios.