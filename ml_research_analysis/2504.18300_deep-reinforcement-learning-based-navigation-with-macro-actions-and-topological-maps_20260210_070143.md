---
ver: rpa2
title: Deep Reinforcement Learning Based Navigation with Macro Actions and Topological
  Maps
arxiv_id: '2504.18300'
source_url: https://arxiv.org/abs/2504.18300
tags:
- agent
- navigation
- actions
- learning
- topological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the problem of efficient navigation in large,
  visually complex environments with sparse rewards, where traditional reinforcement
  learning methods struggle due to high-dimensional state spaces and inefficient exploration.
  The authors propose a method that leverages topological maps and object-oriented
  macro actions, allowing a simple Deep Q-Network to learn effective navigation policies.
---

# Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps

## Quick Facts
- arXiv ID: 2504.18300
- Source URL: https://arxiv.org/abs/2504.18300
- Reference count: 25
- Primary result: DRL-based navigation using topological maps and macro actions achieves sample-efficient learning in large environments with sparse rewards, outperforming random baselines.

## Executive Summary
This paper addresses the challenge of navigation in large, visually complex environments with sparse rewards, where traditional reinforcement learning struggles. The authors propose a method that combines topological mapping with object-oriented macro actions, enabling a simple Deep Q-Network to learn effective navigation policies. By abstracting over long action sequences through discrete macro actions corresponding to navigating to detected objects, the approach drastically reduces the complexity of the underlying reinforcement learning problem. Evaluation in a photorealistic 3D simulation demonstrates significant performance improvements over random baselines under both immediate and terminal reward conditions.

## Method Summary
The approach uses object detection and localization from RGBD input to build a topological map as a graph of navigable targets. The agent selects discrete macro actions to navigate to objects rather than primitive actions, with a low-level controller executing A* path planning between connected nodes. A modified DQN with a single output neuron takes both the action representation (multiple image patches of an object) and a progress vector as inputs, enabling variable-sized action spaces. The network processes multiple image patches per object node through shared-weight CNN branches, improving visual feature learning and value function stability under occlusion and viewpoint variation.

## Key Results
- The proposed method significantly outperforms a random baseline in steps per episode for both immediate and terminal reward settings
- Multi-view aggregation per object node improves training stability and convergence speed
- The approach enables sample-efficient learning from pixel data even in environments with sparse rewards
- Navigation success is demonstrated in photorealistic 3D simulation environments with complex layouts

## Why This Works (Mechanism)

### Mechanism 1
Object-oriented macro actions grounded in a topological map reduce RL problem complexity by abstracting over long action sequences. Instead of selecting primitive actions (step, rotate), the agent selects a target node (object or waypoint). A low-level controller executes the sequence of elementary actions via A* path planning. This transforms a sparse-reward, long-horizon problem into a shorter-horizon decision problem over discrete navigable targets. Core assumption: A low-level controller can reliably navigate between connected nodes given accurate localization and object detection.

### Mechanism 2
A single-output DQN that accepts both state and action as inputs can handle dynamically growing action spaces. Standard DQNs fix output neurons to action count, failing when actions grow. This architecture computes Q(s, a) pairwise: the action representation (multiple image patches of an object) and progress vector x_t are processed jointly, outputting one scalar. Each candidate node is evaluated separately, enabling unbounded action spaces. Core assumption: The outer-product fusion of CNN features and progress vector preserves enough information for meaningful value comparison across nodes.

### Mechanism 3
Storing multiple image patches per object node improves visual feature learning and value function stability under occlusion and viewpoint variation. Each node stores multiple 16×16 RGB patches observed from different angles/positions. During Q-value computation, Ni images pass through shared-weight CNN branches. This yields richer gradients per backprop step and provides redundancy against occlusion or misleading single views. Core assumption: Multiple views provide complementary evidence that resolves visual ambiguity better than any single view.

## Foundational Learning

- **Concept:** Deep Q-Networks (DQN) and action-value functions
  - Why needed here: The policy is learned via a modified DQN; you must understand Q(s,a), replay buffers, and ε-greedy/Boltzmann exploration
  - Quick check question: Can you explain why a single-output network paired with action-as-input enables variable-sized action spaces?

- **Concept:** Topological maps and graph-based representations
  - Why needed here: The agent builds and reasons over a graph G_t=(V_t, E_t); understanding nodes, edges, and path planning (A*) is essential
  - Quick check question: How does a topological map differ from a metric map, and what advantage does it offer for growing environments?

- **Concept:** Temporal abstraction / options framework
  - Why needed here: Macro actions are temporally extended behaviors (like options) that abstract over primitive actions
  - Quick check question: What is the key difference between learned options and the handcrafted macro actions used here?

## Architecture Onboarding

- **Component map:** RGBD input -> Ground truth object detection -> 3D position estimation -> Node creation with multiple 16×16 patches -> Graph G_t=(V_t, E_t) -> Modified DQN (shared-weight CNN branches + outer product with progress vector + FC layers) -> Single Q-output per (state, action) pair -> Policy selection -> A* path planning -> Low-level controller executes turn-and-move to waypoints

- **Critical path:** Object detection and localization accuracy directly affect map quality; multi-view aggregation per node determines visual feature robustness; the pairwise Q(s,a) computation loop must scale as nodes grow

- **Design tradeoffs:** Simplicity vs. realism (uses ground-truth object detection and idealized low-level navigation); sample efficiency vs. wall time (rapid policy convergence in few episodes, but episodes are long); explicit progress vector x_t simplifies credit assignment but partially sidesteps the full challenge of terminal-reward settings

- **Failure signatures:** Map fails to grow (object detection misses targets or localization is inaccurate); policy collapses to random (exploration bonus q not effective, or CNN training unstable due to sparse rewards); computation bottleneck (large number of nodes causes slow per-step Q evaluation); false associations (occluded or ambiguous views cause incorrect target identification)

- **First 3 experiments:**
  1. **Sanity check with colored cylinders:** Run the 1-target, immediate-reward condition; verify the agent reaches targets faster than random baseline
  2. **Ablate multi-view aggregation:** Reduce Ni from 10 to 1 patch per node; compare training stability and steps-per-episode
  3. **Stress-test terminal reward:** Run 3-target terminal-reward condition; analyze whether the explicit progress vector x_t is necessary for success, or if the agent can infer progress from action/observation history

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the navigation policy maintain efficiency if ground truth object detection is replaced with learned visual perception and noisy SLAM?
  - Basis: The conclusion states the method "assumes perfect object detection and localization" and suggests future work must "explore more realistic perception and mapping components (e.g., SLAM)"
  - Why unresolved: The current experiments rely on idealized inputs (ground truth bounding boxes and positions), which masks the difficulty of visual feature extraction and localization errors inherent in real-world navigation
  - What evidence would resolve it: Evaluation results showing success rates and sample efficiency when the agent uses raw sensor data with a learned SLAM module instead of oracle data

- **Open Question 2:** Can the agent infer task progress without the explicit one-hot encoded progress vector $x_t$?
  - Basis: The discussion notes that the handcrafted progress vector "undermines the challenge" of the terminal reward setting, and authors propose to "remove the hand-crafted progress tracking vector" in future work
  - Why unresolved: It is unknown if a simple DQN can learn to track sequential subgoals (e.g., "I have found object A, now look for B") solely from visual history or recurrent states
  - What evidence would resolve it: Successful learning curves in the terminal reward setting using a recurrent policy that receives only visual observations and no explicit progress ID

- **Open Question 3:** Does incorporating explicit reasoning about spatial relations and graph connectivity improve navigation efficiency?
  - Basis: The conclusion states the agent currently treats "all nodes independently" and does not reason about "spatial relations between objects, or the overall connectivity"
  - Why unresolved: The current architecture processes nodes individually via a CNN and outer product; the marginal benefit of exploiting the graph's edge structure (topology) for decision-making remains unquantified
  - What evidence would resolve it: A comparison of convergence speeds between the current method and a Graph Neural Network (GNN) based policy that utilizes edge information

## Limitations

- The approach assumes perfect object detection and localization, relying on ground truth bounding boxes and positions rather than learned perception
- Computational scaling of pairwise Q-value computation becomes a bottleneck as the number of nodes grows large
- The explicit progress vector partially sidesteps the challenge of learning from terminal rewards alone
- Limited evaluation in photorealistic simulation environments without testing in real-world conditions

## Confidence

- **High confidence** in the mechanism descriptions based on the paper's explicit algorithmic details and pseudocode
- **Medium confidence** in architectural specifics due to missing hyperparameter values and CNN layer specifications
- **Low confidence** in real-world applicability given reliance on ground-truth object detection and idealized low-level navigation

## Next Checks

1. Implement the exact method using provided details and test on the colored cylinder scenario with immediate rewards; compare steps-to-target against the random baseline

2. Conduct an ablation study removing multi-view aggregation (using only 1 patch per node) to measure impact on training stability and convergence speed

3. Evaluate the terminal-reward condition (3 targets, only final reward) to determine whether the explicit progress vector x_t is essential or if the agent can infer progress from observation history alone