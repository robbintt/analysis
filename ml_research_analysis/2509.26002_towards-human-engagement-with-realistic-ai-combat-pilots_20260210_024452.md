---
ver: rpa2
title: Towards Human Engagement with Realistic AI Combat Pilots
arxiv_id: '2509.26002'
source_url: https://arxiv.org/abs/2509.26002
tags:
- agents
- combat
- learning
- human
- vr-f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework enabling real-time human
  interaction with AI-controlled fighter jets in realistic 3D air combat scenarios.
  The approach combines Multi-Agent Reinforcement Learning (MARL) trained in a custom
  JSBSim-based environment with deployment into VR-Forces via the IEEE DIS protocol.
---

# Towards Human Engagement with Realistic AI Combat Pilots

## Quick Facts
- arXiv ID: 2509.26002
- Source URL: https://arxiv.org/abs/2509.26002
- Reference count: 16
- 80% win rate in 10-vs-10 air combat scenarios

## Executive Summary
This paper presents a framework for real-time human interaction with AI-controlled fighter jets in 3D air combat scenarios. The system combines Multi-Agent Reinforcement Learning (MARL) trained in a custom JSBSim-based environment with deployment into VR-Forces via IEEE DIS protocol. The approach supports mixed human-AI teams and creates new opportunities for tactical training while maintaining human oversight in safety-critical domains.

## Method Summary
The framework uses a hierarchical policy selection approach where a commander policy (πc) dynamically switches between three control policies (Attack πa, Engage πe, Defend πd) based on tactical state. Agents are trained using MA-PPO with Actor-Critic networks under the CTDE paradigm in a JSBSim-based environment running at 100Hz integration frequency. The trained policies are deployed to VR-Forces via IEEE DIS protocol, enabling real-time state synchronization through UDP/IP communication using EntityState PDUs.

## Key Results
- 80% win rate in 10-vs-10 combat scenarios against mixed-strategy opponents
- Hierarchical policy selection (Attack, Engage, Defend) controlled by commander policy
- Real-time human-AI interaction via IEEE DIS protocol integration with VR-Forces
- High-fidelity physics transfer from JSBSim training environment to deployment platform

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Policy Selection for Combat Strategy
A commander policy (πc) dynamically switches between three control policies (Attack πa, Engage πe, Defend πd) based on tactical state. Combined with CTDE, agents receive global information during training but execute independently. This achieves 80% win rate in 10-vs-10 air combats against mixed-strategy enemies (40% πa, 40% πe, 20% πd).

### Mechanism 2: High-Fidelity Physics Transfer via JSBSim
Training agents in a JSBSim-based environment at 100Hz integration frequency enables learned behaviors to transfer effectively to VR-Forces deployment. JSBSim provides physics-grounded flight dynamics modeling using F-16 aerodynamic models, constraining the action space to realistic maneuvers.

### Mechanism 3: DIS Protocol for Real-Time State Synchronization
IEEE DIS protocol via UDP/IP enables real-time bidirectional communication between MARL agents and VR-Forces with sufficient latency for tactical interactions. EntityState PDUs serialize and transmit entity position, orientation, and velocity at simulation frequency.

## Foundational Learning

- Concept: Markov Games (Multi-Agent MDPs)
  - Why needed here: The paper frames MARL as a Markov Game with tuple (N, S, {A_i}, P, ρ, {R_i}, γ). Understanding joint action spaces, shared state transitions, and independent reward functions is prerequisite to comprehending CTDE and policy learning.
  - Quick check question: Can you explain why centralized training with decentralized execution addresses non-stationarity in multi-agent learning?

- Concept: Curriculum Learning with Self-Play
  - Why needed here: Agents are trained using curriculum learning (gradually increasing difficulty) and self-play (competing against themselves). These techniques drive strategic diversity and robustness.
  - Quick check question: What failure mode might occur if curriculum difficulty increases too rapidly or self-play opponents become too homogeneous?

- Concept: Actor-Critic Methods (MA-PPO)
  - Why needed here: The system uses MA-PPO (Multi-Agent PPO) with Actor-Critic networks. Understanding the actor (policy) and critic (value function) separation is essential for debugging training dynamics.
  - Quick check question: Why does PPO use a clipped objective, and how does this affect policy stability during self-play?

## Architecture Onboarding

- Component map:
  - Training Environment: Custom JSBSim-based simulator (100Hz) → generates states, executes actions, computes rewards
  - MARL Policy Network: Actor-Critic with MA-PPO → outputs actions given states; includes πa, πe, πd, and commander πc
  - DIS Bridge: OpenDIS-based UDP socket handler → parses EntityStatePdus, triggers callbacks, encodes commands
  - Deployment Target: VR-Forces simulation → renders scenarios, accepts human input, hosts mixed human-AI teams
  - Human Interface: Cockpit with controllers/joysticks → allows human operators to control entities in VR-Forces

- Critical path:
  1. Define Markov Game (state/action spaces, reward functions) → 2. Train MA-PPO agents in JSBSim with curriculum + self-play → 3. Export trained policy weights → 4. Initialize DIS socket connection to VR-Forces network → 5. Run inference loop: receive PDUs → parse state → compute actions via πc → transmit commands/States → repeat

- Design tradeoffs:
  - Local JSBSim simulation vs. VR-Forces simulation: Local simulation (dashed line in Fig. 2) maintains physics consistency but duplicates computation; VR-Forces simulation reduces compute but may introduce physics discrepancies.
  - UDP vs. TCP: UDP chosen for low-latency real-time communication but sacrifices delivery guarantees; TCP would ensure reliability but add latency.
  - CTDE vs. fully decentralized: CTDE enables stronger coordination during training but requires global state access; fully decentralized would scale better but likely reduce coordination quality.

- Failure signatures:
  - Agents circle indefinitely without engaging → likely reward shaping issue or πc failing to trigger πa/πe appropriately
  - Agents crash or exhibit aerodynamically impossible maneuvers → physics mismatch between JSBSim and VR-Forces or action space definition error
  - Discontinuous entity movement in VR-Forces → DIS PDU timing issues, packet loss, or frequency mismatch between inference loop and simulation tick rate
  - Human-AI teams fail to coordinate → no explicit coordination mechanism in current architecture; agents treat humans as independent entities

- First 3 experiments:
  1. **Latency baseline**: Measure end-to-end latency from PDU receipt to command transmission under load (10v10 scenario). Establish upper bound before control degradation.
  2. **Physics transfer validation**: Run identical initial conditions in both JSBSim-only and JSBSim→VR-Forces modes. Compare trajectory divergence over 60-second episodes.
  3. **Commander policy ablation**: Compare win rates of πc vs. fixed πa, πe, πd against the same mixed-strategy opponent to quantify hierarchical selection benefit.

## Open Questions the Paper Calls Out

- How do air combat strategies evolve when purely driven by AI compared to human-AI collaboration, and what novel maneuvers emerge that deviate from human expectations? (Basis: explicit statement about exploring AI-driven strategy evolution and imaginative trajectories)
- Can hybrid algorithms combining MARL with imitation learning from real pilot behavioral data outperform pure self-play training? (Basis: stated goal to improve model realism through imitation learning and hybrid algorithms)
- What performance degradation occurs when deploying agents trained at 100Hz (JSBSim) into VR-F running at approximately 10Hz? (Basis: frequency mismatch between training environment and deployment platform)
- How effectively does the commander policy (πc) select among control policies compared to human tactical oversight? (Basis: 80% win rate claim without comparison to human-commanded policy switching)

## Limitations

- State and action space definitions are not specified, preventing faithful reproduction
- Reward functions for each control policy are not described, leaving critical behavioral shaping unclear
- Training hyperparameters (network architecture, learning rates, training duration) are unspecified
- No quantitative validation of physics transfer between JSBSim and VR-Forces environments
- Network latency and state synchronization accuracy are not measured or reported

## Confidence

- High Confidence: Hierarchical policy architecture and MA-PPO with CTDE are well-established approaches
- Medium Confidence: 80% win rate based on self-play provides reasonable baseline but lacks robustness validation
- Low Confidence: Without specific state/action space definitions, reward functions, and training hyperparameters, faithful reproduction is not possible

## Next Checks

1. **Transfer Validation Experiment**: Run identical initial conditions in both JSBSim-only and JSBSim→VR-Forces deployment modes. Measure trajectory divergence over 60-second episodes and quantify state synchronization accuracy.

2. **Latency Baseline Measurement**: Under 10-vs-10 combat load, measure end-to-end latency from DIS PDU receipt to command transmission. Establish the upper bound before control degradation occurs, and test with simulated network jitter and packet loss.

3. **Commander Policy Ablation Study**: Compare win rates of the commander policy (πc) against fixed strategies (πa, πe, πd) using the same mixed-strategy opponent distribution. Quantify the tactical benefit of hierarchical selection versus individual control policies.