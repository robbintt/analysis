---
ver: rpa2
title: 'Beyond the Safety Tax: Mitigating Unsafe Text-to-Image Generation via External
  Safety Rectification'
arxiv_id: '2508.21099'
source_url: https://arxiv.org/abs/2508.21099
tags:
- safety
- unsafe
- generation
- prompt
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafePatch, a method to reduce unsafe content
  generation in text-to-image models without degrading benign output quality. Instead
  of modifying the base model, SafePatch adds a separate safety module that learns
  to correct unsafe features using a carefully constructed counterfactual dataset.
---

# Beyond the Safety Tax: Mitigating Unsafe Text-to-Image Generation via External Safety Rectification

## Quick Facts
- arXiv ID: 2508.21099
- Source URL: https://arxiv.org/abs/2508.21099
- Reference count: 40
- SafePatch reduces unsafe text-to-image generation while preserving benign output quality

## Executive Summary
This paper introduces SafePatch, a novel approach to mitigate unsafe content generation in text-to-image models without the typical quality degradation associated with safety alignment. Unlike traditional methods that modify the base model, SafePatch employs a separate safety rectification module that learns to correct unsafe features using counterfactual data. The method demonstrates strong safety performance on benchmarks like I2P (7% unsafe probability) while maintaining image fidelity and prompt alignment, and shows robustness to adversarial attacks across different model architectures.

## Method Summary
SafePatch operates as an external safety rectification module rather than modifying the base text-to-image model. It uses counterfactual data generation to create paired examples of safe and unsafe image outputs, training the rectification module to transform unsafe features into safe ones. The approach decouples safety mechanisms from the primary generation model, allowing it to preserve the base model's creative capabilities while adding a safety layer. The rectification module learns patterns of unsafe content through this counterfactual training data and applies corrections during inference without requiring changes to the underlying diffusion or generation architecture.

## Key Results
- Achieves 7% unsafe probability on I2P benchmark, demonstrating strong safety improvements
- Maintains high image fidelity and prompt alignment, avoiding the quality degradation typical of aligned models
- Shows robustness to adversarial attacks and generalizes across different model architectures
- Outperforms existing safety baselines in both safety performance and benign output quality preservation

## Why This Works (Mechanism)
SafePatch works by decoupling safety rectification from the core text-to-image generation process. Instead of training the base model to avoid unsafe outputs (which typically degrades overall quality), it adds a separate module that learns to identify and correct unsafe features post-generation. This approach preserves the base model's original capabilities while adding safety as an overlay. The counterfactual training data provides concrete examples of unsafe-to-safe transformations, enabling the rectification module to learn specific patterns and corrections rather than simply suppressing content. This architectural separation prevents the safety mechanisms from interfering with the model's creative and generative capabilities.

## Foundational Learning
- **Counterfactual data generation**: Creating paired examples of safe and unsafe outputs is essential for training the rectification module to learn specific transformations rather than just content suppression
  - Why needed: Provides concrete examples of unsafe-to-safe transformations for supervised learning
  - Quick check: Verify that generated counterfactual pairs maintain semantic consistency while altering only the unsafe elements

- **External safety modules**: Adding safety as a separate component rather than modifying the base model preserves original capabilities while adding protective layers
  - Why needed: Prevents degradation of benign output quality that occurs when safety is baked into the core model
  - Quick check: Compare base model performance on creative prompts before and after adding SafePatch

- **Feature-level rectification**: Learning to transform unsafe features rather than simply rejecting unsafe prompts allows for nuanced safety corrections
  - Why needed: Enables preservation of legitimate content while removing only the problematic elements
  - Quick check: Analyze examples where only specific unsafe elements are removed while context is preserved

## Architecture Onboarding

**Component map:** Text prompt -> Base T2I model -> Generated image -> SafePatch rectifier -> Final safe image

**Critical path:** The most critical path is the rectification pipeline where the generated image is analyzed and modified. This occurs after initial generation but before output delivery, making it a post-hoc safety mechanism.

**Design tradeoffs:** The main tradeoff is between safety coverage and computational overhead. Adding an external rectification step increases inference time but preserves base model performance. The counterfactual data generation pipeline requires human raters for safety judgments, creating potential scalability and consistency challenges.

**Failure signatures:** Potential failures include incomplete rectification (unsafe elements remain), over-correction (benign content is removed), and latency issues from the additional processing step. The system may also struggle with novel unsafe patterns not represented in the training data.

**3 first experiments:**
1. Test rectification module on controlled unsafe-safe image pairs to verify learning of transformation patterns
2. Benchmark inference latency with and without SafePatch to quantify computational overhead
3. Evaluate robustness against simple adversarial prompt modifications to establish baseline attack resistance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited qualitative assessment of creative expression impact and subjective safety judgments
- Adversarial robustness demonstrated only against straightforward input perturbations, not sophisticated attack vectors
- Human-dependent counterfactual data generation raises scalability and consistency concerns for real-world deployment

## Confidence

| Claim | Confidence |
|-------|------------|
| Safety performance improvement | High - supported by multiple benchmark evaluations with clear metrics |
| Preservation of benign image quality | Medium - demonstrated through FID and prompt alignment scores, but limited qualitative analysis |
- Safety performance improvement: **High** - supported by multiple benchmark evaluations with clear metrics
- Preservation of benign image quality: **Medium** - demonstrated through FID and prompt alignment scores, but limited qualitative analysis
- Robustness to adversarial attacks: **Medium** - shown against specific attack types, but not comprehensively tested across attack methodologies

## Next Checks
1. Conduct qualitative user studies with diverse prompt types to assess creative expression impact and subjective safety judgments
2. Test against established adversarial attack benchmarks specific to image generation safety (e.g., gradient-based attacks, prompt injection variants)
3. Evaluate scalability and consistency of the counterfactual data generation pipeline across different safety domains and cultural contexts