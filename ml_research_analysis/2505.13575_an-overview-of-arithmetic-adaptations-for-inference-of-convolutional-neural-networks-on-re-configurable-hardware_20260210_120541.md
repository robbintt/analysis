---
ver: rpa2
title: An Overview of Arithmetic Adaptations for Inference of Convolutional Neural
  Networks on Re-configurable Hardware
arxiv_id: '2505.13575'
source_url: https://arxiv.org/abs/2505.13575
tags:
- layer
- conv
- network
- pruning
- batchnorm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents methods for deploying convolutional neural
  networks on FPGA hardware by addressing challenges related to computational intensity,
  memory requirements, and arithmetic limitations. The authors propose a three-step
  adaptation workflow: (1) batch normalization fusing, which eliminates normalization
  layers by integrating their parameters into convolution weights, achieving a 23.8
  MFLOPS reduction; (2) filter pruning using Frobenius norm and sparsity metrics,
  removing redundant filters while maintaining mean average precision, resulting in
  15.7-27.7% parameter reductions; and (3) quantization, converting floating-point
  operations to integer arithmetic with scaling factors, achieving negligible deviation
  (MSE < 0.001) in detection performance.'
---

# An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware

## Quick Facts
- arXiv ID: 2505.13575
- Source URL: https://arxiv.org/abs/2505.13575
- Reference count: 22
- Authors: Ilkay Wunderlich; Benjamin Koch; Sven Schönfeld
- Primary result: Three-step adaptation workflow (batchnorm fusing, filter pruning, quantization) reduces TinyYOLOv3 computational load while maintaining detection accuracy on FPGA hardware

## Executive Summary
This paper presents a three-step workflow for deploying convolutional neural networks on FPGA hardware, addressing computational intensity, memory requirements, and arithmetic limitations. The authors demonstrate that batch normalization layers can be algebraically integrated into convolution weights, filters can be pruned based on norm and sparsity metrics, and floating-point operations can be converted to efficient integer arithmetic. Applied to TinyYOLOv3 on XILINX Artix-7 FPGA, these techniques achieve significant computational reductions while preserving detection accuracy with negligible performance deviation.

## Method Summary
The authors propose a three-step adaptation workflow for CNN deployment on FPGA hardware. First, batch normalization fusing eliminates normalization layers by algebraically integrating their parameters into preceding convolution weights, reducing FLOPS. Second, filter pruning systematically removes low-impact filters using Frobenius norm or sparsity metrics while maintaining mean average precision within a user-defined tolerance. Third, post-training integer quantization converts floating-point operations to 16-bit integer arithmetic with power-of-two scaling factors, enabling efficient fixed-point computation on FPGAs. The complete workflow transforms a floating-point CNN into a quantized integer model optimized for hardware deployment.

## Key Results
- Batch normalization fusing reduces computational load by 23.8 MFLOPS on TinyYOLOv3
- Filter pruning achieves 15.7-27.7% parameter reductions while maintaining mean average precision
- Post-training quantization with S=256 yields negligible deviation (MSE < 0.001) in detection performance
- Final model achieves bounding box deviation ≤ 2 pixels with detection score deviation ≈ 0.0019

## Why This Works (Mechanism)

### Mechanism 1: Batch Normalization Fusing
Eliminating batch normalization sub-layers by algebraically integrating their parameters into preceding convolution weights reduces FLOPS and hardware complexity without altering network output. The method exploits the linearity property k·Conv(A;W,b) + h = Conv(A; k·W, k·b + h), folding batchnorm transformation (scale γ, shift β, mean μ, variance σ²) into convolution weights via W_bn = γ·W/√(σ²+ε) and b_bn = γ·(b-μ)/√(σ²+ε) + β. This makes the batchnorm layer computationally redundant at inference time. Core assumption: The network is in inference mode with frozen batchnorm statistics and follows conv→batchnorm→activation pattern. Evidence: Reports "23.8 MFLOPS reduction" from batchnorm fusing on TinyYOLOv3.

### Mechanism 2: Filter Pruning via Norm and Sparsity Metrics
Systematically removing low-impact filters based on Frobenius norm or sparsity metrics can reduce parameter count by 15.7-27.7% while constraining MAP degradation to a user-defined tolerance (e.g., 1%). The iterative thresholding routine evaluates each filter f_n using ||f_n||_F or Sp_ε(f_n), removes filters below threshold T, and incrementally raises T by δT until MAP deviation exceeds ΔMAP. Optional fine-tuning with low learning rate can recover performance. Core assumption: Filters with low weight magnitudes or high near-zero coefficient density contribute minimally to feature extraction. Evidence: States "15.7-27.7% parameter reductions" while "maintaining mean average precision."

### Mechanism 3: Post-Training Integer Quantization with Power-of-Two Scaling
Converting 32-bit floating-point weights and activations to 16-bit integers using scaling factor S = 2^P yields negligible detection deviation (MSE < 0.001, bounding box error ≤ 2 pixels). Parameters and inputs are scaled by S and rounded to int16. Convolution is computed in int32 (to prevent overflow), then right-shifted by P bits (replacing division with shift). ReLU_α with α = 2^{-Pα} also uses shift for the negative slope. Core assumption: Integer precision with 16-bit representation is sufficient for inference. Evidence: "achieving negligible deviation (MSE < 0.001) in detection performance."

## Foundational Learning

- **Fixed-point vs. floating-point arithmetic on FPGAs**: FPGAs like Artix-7 implement fixed-point arithmetic efficiently; floating-point requires significantly more logic resources and memory bandwidth. Quick check: Can you explain why multiplying two int16 values requires int32 accumulator to prevent overflow?

- **Batch normalization's role in training vs. inference**: Understanding why batchnorm can be "fused away" requires knowing that μ and σ² are computed from mini-batches during training but become fixed constants at inference. Quick check: What happens to batchnorm behavior if you accidentally leave it in training mode during inference?

- **Pruning metrics and network redundancy**: The Frobenius norm and sparsity metrics assume low-magnitude weights indicate redundancy—this is not universally true for all architectures. Quick check: Why might pruning based solely on weight magnitude fail for networks with untrained or poorly initialized layers?

## Architecture Onboarding

- **Component map**: Trained CNN (float32) → [1] Batchnorm Fusing → [2] Filter Pruning → [3] Quantization → Deployable CNN (int16) → FPGA synthesis

- **Critical path**: Batchnorm fusing must precede pruning (pruning operates on fused weights). Quantization is mandatory for FPGA inference but can theoretically be applied independently—though fusing first simplifies the integer model.

- **Design tradeoffs**:
  - Higher ΔMAP tolerance → more aggressive pruning but lower accuracy
  - Larger scaling factor S → better precision but higher bit-width requirements
  - Fine-tuning after pruning → potential accuracy recovery but adds training time
  - Power-of-2 α for LeakyReLU → hardware-efficient but limits activation function flexibility

- **Failure signatures**:
  - Sudden MAP collapse during pruning: threshold too aggressive; reduce δT or lower ΔMAP
  - MSE growing unbounded through layers: quantization scale insufficient; increase S
  - Very high prunable filter count (>80%): network likely oversized or undertrained; investigate training quality
  - Bounding box errors exceeding tolerance: consider per-layer adaptive quantization scales

- **First 3 experiments**:
  1. Apply batchnorm fusing to your trained model, compare outputs element-wise against original (should be identical within floating-point tolerance).
  2. Run pruning routine with ΔMAP = 0.5%, 1%, 2% on a representative validation set; plot parameter reduction vs. accuracy to find the knee point.
  3. Quantize with S = 128, 256, 512; measure layer-wise MSE and final detection metrics to determine minimum acceptable scaling factor for your target FPGA's DSP resources.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific diagnostic criteria distinguish between an oversized network architecture and improper training when observing pruning rates exceeding 80%? The paper establishes a routine for pruning but lacks a methodology to interpret the underlying causes of extreme sparsity in the resulting metrics.

- **Open Question 2**: How do the theoretical reductions in FLOPS and parameters translate into actual on-chip resource utilization and power consumption for the target hardware? While the paper targets the XILINX Artix-7 FPGA, the evaluation relies exclusively on software-side metrics without providing post-synthesis metrics like LUT usage, DSP slice count, or power draw.

- **Open Question 3**: Can advanced non-uniform quantization or binary arithmetic methods be integrated into the proposed workflow while maintaining the detection accuracy achieved by the scaling-factor approach? The authors note that "appropriate concepts of the presented advanced approaches will be considered," specifically referencing XNOR-Net and distribution-based quantization as future improvements to speed up inference.

## Limitations
- The quantization methodology only validates up to 15 layers - deeper architectures may exhibit error accumulation not captured
- The pruning routine's effectiveness depends heavily on the assumption that low-magnitude filters are redundant, which may not hold for all architectures
- The FPGA implementation aspects (memory hierarchy, timing closure, resource utilization) are not specified, limiting assessment of real-world deployment feasibility

## Confidence
- **High confidence**: Batch normalization fusing mechanism and mathematical formulation are clearly derived and experimentally validated with specific FLOPS reduction (23.8 MFLOPS)
- **Medium confidence**: Filter pruning effectiveness (15.7-27.7% reduction) is demonstrated on TinyYOLOv3, but the generalizability to other architectures and the assumption that Frobenius norm/sparsity reliably identify redundant filters lacks broader validation
- **Low confidence**: Integer quantization's negligible error claim (MSE < 0.001) is only validated on TinyYOLOv3 with 15 layers; extension to deeper networks without error accumulation remains unverified

## Next Checks
1. Apply the complete three-step workflow to VGG-16 or ResNet-18 on ImageNet classification task to verify that quantization error remains bounded across deeper networks and that pruning preserves accuracy across different architecture families

2. Implement the quantized int16 CNN on actual XILINX Artix-7 FPGA hardware, measuring DSP utilization, memory bandwidth requirements, and inference latency to confirm the theoretical efficiency gains translate to real resource savings

3. Systematically compare Frobenius norm and sparsity-based pruning against alternative metrics (e.g., Taylor expansion-based importance, geometric median) on the same TinyYOLOv3 network to validate that the proposed metrics consistently identify the most effective filters for removal