---
ver: rpa2
title: Resolution scaling governs DINOv3 transfer performance in chest radiograph
  classification
arxiv_id: '2510.07191'
source_url: https://arxiv.org/abs/2510.07191
tags:
- dinov3
- training
- test
- datasets
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically benchmarks Meta\u2019s DINOv3 against\
  \ DINOv2 and ImageNet initialization for chest radiograph classification. Across\
  \ seven datasets (n814,000), two backbones (ViT-B/16, ConvNeXt-B), and input resolutions\
  \ (224\xD7224, 512\xD7512, 1024\xD71024), DINOv3 significantly outperformed ImageNet\
  \ and DINOv2 at 512\xD7512, especially with ConvNeXt-B, while gains plateaued beyond\
  \ 512\xD7512."
---

# Resolution scaling governs DINOv3 transfer performance in chest radiograph classification

## Quick Facts
- arXiv ID: 2510.07191
- Source URL: https://arxiv.org/abs/2510.07191
- Reference count: 40
- Primary result: DINOv3 achieves peak transfer performance at 512×512 resolution, with diminishing returns beyond this scale

## Executive Summary
This study systematically benchmarks Meta's DINOv3 against DINOv2 and ImageNet initialization for chest radiograph classification. Across seven datasets (n>814,000), two backbones (ViT-B/16, ConvNeXt-B), and input resolutions (224×224, 512×512, 1024×1024), DINOv3 significantly outperformed ImageNet and DINOv2 at 512×512, especially with ConvNeXt-B, while gains plateaued beyond 512×512. Frozen features from the 7B-parameter DINOv3 teacher underperformed relative to finetuned smaller models, highlighting the need for domain-specific adaptation. Resolution scaling was most beneficial for boundary-dependent and small focal abnormalities. The findings establish 512×512 as the practical upper limit for DINOv3 transfer in chest radiography and provide guidance for clinical AI deployment.

## Method Summary
The study evaluated three initialization strategies (ImageNet, DINOv2, DINOv3) across two backbones (ViT-B/16, ConvNeXt-B) at three resolutions (224×224, 512×512, 1024×1024) on seven chest radiograph datasets totaling 814,859 images. Models were trained with AdamW optimizer (lr=1e-5 for full finetuning, 1e-4 for frozen features), binary weighted cross-entropy loss, and standard augmentations. Evaluation used bootstrap AUROC with 1,000 resamples and FDR-corrected paired significance testing. The frozen DINOv3-7B comparison used a DinoNet head (2.1M parameters) trained on frozen features.

## Key Results
- DINOv3 achieved 6/6 dataset wins over ImageNet and 5/6 wins over DINOv2 at 512×512 (p ≤ 0.0060 and p ≤ 0.019 respectively)
- ConvNeXt-B consistently outperformed ViT-B/16 across all settings, with advantage widening at 512×512
- Frozen 7B-parameter DINOv3 features underperformed finetuned 86-89M parameter backbones by 3-8 AUROC points
- Resolution scaling benefits plateaued at 512×512, with 1024×1024 providing no measurable improvement
- Benefits were most pronounced for boundary-dependent and small focal abnormalities

## Why This Works (Mechanism)

### Mechanism 1: Resolution Scaling Activates Gram-Anchored Feature Preservation
DINOv3's Gram-anchored self-distillation explicitly preserves fine-grained patch-level similarity structures during extended training. When resolution increases from 224×224 to 512×512, chest radiographs retain clinically critical spatial details that are lost at lower resolutions. DINOv3's Gram loss term stabilizes these dense features at higher input sizes, allowing the model to leverage its pretraining investments. Beyond 512×512, marginal benefits diminish because the pretrained positional embeddings were optimized for mixed-resolution training that includes but doesn't exclusively target >512px inputs.

### Mechanism 2: Domain-Specific Fine-Tuning Outweighs Parameter Scale
Billion-parameter encoders trained on natural images develop feature spaces optimized for natural-image statistics absent in grayscale medical radiographs. When frozen, these features cannot adapt to the fundamentally different diagnostic signal structure in chest X-rays: subtle contrast variations, boundary-dependent pathology signatures, and projection-specific artifacts. Fine-tuning even a much smaller model allows the network to reorganize its intermediate representations around domain-relevant features, yielding superior AUROC despite ~80× fewer parameters.

### Mechanism 3: ConvNeXt-B Capitalizes on SSL Initialization More Effectively Than ViT-B
ConvNeXt's modern convolutional architecture—hierarchical feature pyramids, locality-biased inductive priors, and patch-agnostic spatial processing—aligns better with the spatial structure of chest radiographs than ViT's patch-based attention. At higher resolutions, ViT's fixed patch size means 512×512 inputs produce 32×32 patch grids versus 14×14 at 224×224, quadrupling self-attention computational costs while potentially diluting global context. ConvNeXt's convolutional hierarchy naturally scales to higher resolutions without this quadratic penalty.

## Foundational Learning

- **Concept:** Self-supervised distillation (DINO family)
  - **Why needed here:** Understanding why DINOv3's Gram-anchored self-distillation enables high-resolution transfer requires grasping how student-teacher consistency objectives preserve patch-level features without labels.
  - **Quick check question:** Can you explain why adding a Gram-matrix anchoring term would help preserve fine-grained spatial structure during long pretraining schedules?

- **Concept:** Transfer learning domain shift
  - **Why needed here:** The frozen-vs-fine-tuned comparison hinges on understanding why natural-image pretraining produces features poorly aligned with medical imaging.
  - **Quick check question:** Why would a 7B-parameter model trained on natural images underperform an 86M-parameter model fine-tuned on chest X-rays, even when the smaller model is initialized from the same pretraining family?

- **Concept:** Resolution-dependent feature learning
  - **Why needed here:** The paper's central claim—that 512×512 is a "practical upper limit"—requires understanding how input resolution affects patch-based architectures versus fully convolutional ones.
  - **Quick check question:** At 224×224 with ViT-B/16 patch size 16, how many patches does the model process? How does this change at 512×512, and what are the computational implications?

## Architecture Onboarding

- **Component map:**
  Input (CXR 224/512/1024) → Preprocessing (histogram equalization) → Backbone (ViT-B/16 or ConvNeXt-B, ~86-89M params) → Classification head (linear to label space) → Multi-label output (3-21 findings depending on dataset)

- **Critical path:**
  1. Select backbone (ConvNeXt-B recommended per paper findings)
  2. Select resolution (512×512 optimal; 224×224 for compute-limited, 1024×1024 not recommended)
  3. Load DINOv3 pretrained weights from Hugging Face
  4. Fine-tune with AdamW, lr=1e-5, no weight decay, binary weighted cross-entropy
  5. Augment with random horizontal flip + rotation (≤7°)
  6. Evaluate via bootstrap AUROC (1000 resamples) with paired significance testing

- **Design tradeoffs:**
  - **Resolution vs. compute:** 512×512 provides optimal DINOv3 performance; 1024×1024 yields no measurable gain despite ~4× compute cost
  - **Frozen vs. fine-tuned:** Frozen 7B underperforms fine-tuned 86M; only use frozen if compute constraints preclude fine-tuning entirely
  - **ViT vs. ConvNeXt:** ConvNeXt-B consistently superior; ViT may be preferred only if architectural consistency with other pipeline components is required
  - **DINOv2 vs. DINOv3:** At 224×224, DINOv2 may match or slightly exceed DINOv3; DINOv3 preferred only if 512×512 is viable

- **Failure signatures:**
  - DINOv3 performs worse than ImageNet at 224×224 on some datasets → suggests resolution mismatch; increase to 512×512
  - Frozen DINOv3-7B AUROC < ImageNet baseline → expected; must fine-tune smaller model instead
  - No improvement from 224→512 on pediatric (Pedi-CXR) dataset → expected; limited sample size and narrow label space constrain initialization benefits
  - ConvNeXt and ViT show similar performance → check for data leakage or insufficient training epochs

- **First 3 experiments:**
  1. **Resolution ablation on single dataset:** Train DINOv3-ConvNeXt-B on CheXpert at 224×224, 512×512, 1024×1024. Verify 512×512 peak and 1024×1024 plateau before expanding to other datasets. Expected: 512×512 yields ~1.5 AUROC point gain over 224×224; 1024×1024 shows <0.5 point change.
  2. **Frozen vs. fine-tuned comparison:** On MIMIC-CXR at 512×512, compare frozen DINOv3-7B with DinoNet head against fine-tuned DINOv3-ConvNeXt-B. Expected: fine-tuned smaller model outperforms frozen larger model by >3 AUROC points.
  3. **Backbone head-to-head:** Train ViT-B/16 and ConvNeXt-B with DINOv3 initialization at 512×512 on PadChest. Expected: ConvNeXt-B wins by ~0.5-1.0 AUROC points; gap widens for boundary-dependent findings (pneumothorax, nodules).

## Open Questions the Paper Calls Out
None

## Limitations
- Resolution scaling benefits assume consistent image quality and standardization across institutions, which may not hold in real-world clinical deployment
- Frozen-vs-fine-tuned comparison uses a single frozen 7B model without exploring intermediate adaptation strategies like feature-selective fine-tuning
- Backbone comparisons focus on ViT and ConvNeXt architectures without evaluating hierarchical transformers that might combine attention benefits with spatial locality

## Confidence
- **High Confidence:** Resolution scaling benefits at 512×512 (consistent across all 6 adult datasets with statistical significance), ConvNeXt-B outperforming ViT-B (consistent across all settings), frozen 7B features underperforming fine-tuned 86M models (statistically significant across all adult datasets)
- **Medium Confidence:** Practical upper limit at 512×512 (based on plateau at 1024×512, but 1024×1024 computational costs not fully characterized), Gram-anchored mechanism driving resolution benefits (inferred from architecture but not directly validated), ConvNeXt advantage stems from spatial locality (plausible but not explicitly tested)

## Next Checks
1. **Resolution-dependent feature analysis:** Train DINOv3-ConvNeXt-B at 224×224 and 512×512 on CheXpert, then extract and visualize intermediate layer activations for boundary-dependent vs texture-dependent findings to empirically confirm which features benefit most from resolution scaling.

2. **Domain-adaptive pretraining validation:** Compare three strategies on MIMIC-CXR: (a) frozen 7B DINOv3, (b) fine-tuned 86M ConvNeXt-B, and (c) domain-adaptive pretraining where the 7B model is partially unfrozen (e.g., last 2-3 layers) with reduced learning rates to test if partial adaptation bridges the performance gap.

3. **Hierarchical transformer comparison:** Train Swin-B with DINOv3 initialization at 512×512 on PadChest to test whether the ConvNeXt advantage stems from spatial locality versus architectural factors, comparing against the reported ViT-B/16 and ConvNeXt-B baselines.