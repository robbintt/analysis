---
ver: rpa2
title: Multimodal Large Language Models with Adaptive Preference Optimization for
  Sequential Recommendation
arxiv_id: '2511.18740'
source_url: https://arxiv.org/abs/2511.18740
tags:
- recommendation
- optimization
- language
- user
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes HaNoRec, a multimodal large language model
  framework that addresses two key challenges in sequential recommendation: imbalanced
  sample hardness and cross-modal semantic bias. HaNoRec dynamically adjusts optimization
  weights based on sample hardness and model responsiveness to prioritize harder examples,
  while introducing Gaussian noise perturbation and KL divergence-based constraints
  to enhance cross-modal semantic consistency.'
---

# Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation

## Quick Facts
- arXiv ID: 2511.18740
- Source URL: https://arxiv.org/abs/2511.18740
- Reference count: 40
- Key outcome: HaNoRec achieves AUC gains of 4.12% over baselines by dynamically adjusting optimization weights and enhancing cross-modal alignment

## Executive Summary
HaNoRec addresses two critical challenges in sequential recommendation: imbalanced sample hardness and cross-modal semantic bias in multimodal LLMs. The framework introduces Hardness-aware Reweighting Scaling (HaRS) to dynamically adjust optimization weights based on sample difficulty and model responsiveness, and Noise-regularized Distribution Optimization (NoDO) to inject Gaussian noise for improved cross-modal alignment. Experimental results on Microlens, Netflix, and Movielens-1M datasets demonstrate significant performance improvements over standard SFT and DPO approaches.

## Method Summary
HaNoRec modifies standard DPO by introducing two key components: HaRS calculates dynamic weights using data hardness (semantic similarity between chosen/rejected items) and model responsiveness (reward gap signals), while NoDO injects Gaussian noise into LoRA parameters to smooth distributions and improve cross-modal alignment. The method trains on Qwen-2.5-VL-3B with LoRA, using a two-stage process: 5 epochs of SFT followed by DPO with adaptive weighting. Key hyperparameters include Top-K=10 for similarity estimation, noise strength σ_n=0.05-0.1, and base weight β₀=0.1.

## Key Results
- Achieves 4.12% AUC improvement over standard DPO baselines
- Demonstrates superior performance across three benchmark datasets (Microlens, Netflix, Movielens-1M)
- Shows effective handling of imbalanced sample hardness through dynamic weighting
- Validates cross-modal semantic consistency improvements through noise regularization

## Why This Works (Mechanism)

### Mechanism 1: Hardness-aware Weight Scaling
The system computes a "hardness" score (λ) for each preference pair by calculating semantic similarity overlap between chosen and rejected items. High overlap indicates hard samples requiring lower optimization weights (β) to relax KL-divergence constraints. This prevents overfitting to easy examples while allowing convergence on difficult pairs.

### Mechanism 2: Noise-Regularized Distribution Smoothing
Gaussian noise is injected into LoRA weights during forward pass, smoothing output logits and preventing the policy model from becoming overly confident in potentially biased reference distributions. This perturbation allows exploration of semantic space regions with improved title-image alignment.

### Mechanism 3: Model Responsiveness Gating
The system calculates an implicit reward gap (R_i) using log-likelihood ratios between current policy and reference models. This gap, normalized across batches and combined with data hardness, creates a final dynamic weight (β') that acts as a gating mechanism, adjusting optimization focus based on the model's learning readiness for specific samples.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: HaNoRec framework is built upon modifying standard DPO loss function; understanding parameter β control is critical
  - Quick check: How does parameter β in standard DPO control the trade-off between maximizing reward and staying close to reference model?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed: NoDO specifically targets LoRA weights (A and B matrices) for noise injection; understanding weight decomposition is critical
  - Quick check: Why is noise applied to A and B matrices in LoRA rather than frozen pre-trained weights W₀?

- **Concept: Cross-Modal Semantic Alignment**
  - Why needed: Recommendation task requires aligning title (text) and poster (image) in shared semantic space; understanding cosine similarity is essential
  - Quick check: If movie title is ambiguous but poster is distinct, how would standard DPO likely fail compared to noise-regularized approach?

## Architecture Onboarding

- **Component map:** Input Layer (User history text + Item Images) -> Backbone (Qwen-2.5-VL) -> HaRS Module (Hardness λ + Responsiveness η) -> NoDO Module (Gaussian noise ε) -> Loss Layer (Modified DPO Loss)

- **Critical path:** Calculation of dynamic weight β' (Equation 11), which relies on successful Top-K neighbor extraction and current log-probabilities; incorrect β' causes entire adaptive weighting logic to fail

- **Design tradeoffs:**
  - Noise Strength (σ_n): Higher noise improves alignment for weak samples but risks destabilizing generative coherence
  - Top-K Size: Small K misses multi-dimensional preferences; large K introduces noise blurring positive/negative boundaries

- **Failure signatures:**
  - Modality Collapse: Model ignores images and relies only on text titles
  - Stagnation on Hard Samples: Validation loss decreases on easy samples but remains flat or increases on hard subsets

- **First 3 experiments:**
  1. Sanity Check (w/o All): Run standard SFT + DPO to establish baseline and confirm "weak get weaker" trend
  2. Component Isolation (w/o NoDO): Enable only HaRS to verify dynamic β improves metrics before introducing noise complexity
  3. Hyperparameter Sweep: Grid search for Top-K (K ∈ {10, 20, 50}) and Noise Strength (σ_n ∈ {0.05, 0.1}) on validation slice

## Open Questions the Paper Calls Out
None

## Limitations
- Semantic similarity may not be reliable proxy for sample hardness when modalities provide complementary rather than overlapping information
- Noise injection lacks extensive ablation studies showing optimal σ_n range
- Experimental setup assumes crawled images are available and correctly matched, introducing data quality dependencies

## Confidence
- **High confidence:** General architecture combining HaRS and NoDO modules is technically sound and well-motivated
- **Medium confidence:** Specific hyperparameter choices (Top-K=10, σ_n=0.05-0.1, β₀=0.1) are likely reasonable but not rigorously justified
- **Low confidence:** Assumption that semantic similarity directly correlates with ranking difficulty across different recommendation domains

## Next Checks
1. Ablation on Top-K retrieval: Systematically vary K from 5 to 50 and measure stability of hardness estimates and downstream performance to identify optimal trade-off

2. Noise sensitivity analysis: Sweep σ_n across [0.01, 0.05, 0.1, 0.2] and measure title-image similarity preservation, model calibration, and ranking accuracy to identify plateau or degradation points

3. Modality importance test: Randomly shuffle or remove images from 20% of test samples and measure performance drop against text-only baseline to quantify cross-modal alignment contribution