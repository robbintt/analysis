---
ver: rpa2
title: Modeling Image-Caption Rating from Comparative Judgments
arxiv_id: '2602.00381'
source_url: https://arxiv.org/abs/2602.00381
tags:
- comparative
- task
- learning
- caption
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a comparative learning framework to model image-caption
  rating, addressing the challenges of subjective and noisy direct human ratings.
  Instead of predicting absolute scores, the model learns from pairwise preferences,
  determining which image-caption pair better matches each other.
---

# Modeling Image-Caption Rating from Comparative Judgments

## Quick Facts
- arXiv ID: 2602.00381
- Source URL: https://arxiv.org/abs/2602.00381
- Reference count: 3
- Primary result: Proposed comparative learning framework for image-caption rating achieves Pearson's ρ: 0.7609 and Spearman's rs: 0.7089, with accuracy approaching 0.85 for identifying better captions

## Executive Summary
This paper addresses the challenges of subjective and noisy direct human ratings in image-caption evaluation by proposing a comparative learning framework. Instead of predicting absolute scores, the model learns from pairwise preferences to determine which image-caption pair better matches each other. The approach uses ResNet-50 for visual features and MiniLM for text features, and is evaluated on the VICR dataset using both regression and comparative learning models.

## Method Summary
The framework employs a comparative learning approach where models learn from pairwise preferences rather than absolute scores. Visual features are extracted using ResNet-50 while text features use MiniLM. The model is trained on the VICR dataset and evaluates both regression-based absolute scoring and comparative learning approaches. The comparative method focuses on determining which of two image-caption pairs is more aligned, rather than predicting a numerical rating.

## Key Results
- Regression model achieves Pearson's ρ: 0.7609 and Spearman's rs: 0.7089 on VICR dataset
- Comparative model approaches regression performance with more data and reaches approximately 0.85 accuracy in identifying better captions
- Human evaluation shows comparative judgments are faster and have higher inter-rater agreement than direct ratings

## Why This Works (Mechanism)
The comparative learning framework works by shifting from absolute rating prediction to pairwise preference learning, which reduces the impact of subjective bias and noise in human ratings. By learning to distinguish which image-caption pair is better rather than predicting exact scores, the model can leverage more reliable comparative judgments that humans can make more consistently than absolute ratings.

## Foundational Learning
- **Image-text alignment**: Understanding how visual and textual features correspond is crucial for evaluating image-caption pairs. Quick check: Verify feature extraction produces semantically meaningful representations.
- **Comparative vs absolute rating**: Learning from relative preferences can be more robust than predicting absolute scores. Quick check: Compare model performance on comparative vs absolute tasks.
- **Pairwise ranking**: Models learn to rank pairs rather than predict scores, which can be more data-efficient. Quick check: Evaluate ranking accuracy across different training set sizes.

## Architecture Onboarding

**Component Map**: Image -> ResNet-50 -> Visual features -> Comparator -> Text -> MiniLM -> Text features -> Comparator

**Critical Path**: Image and caption inputs → Feature extraction (ResNet-50/MiniLM) → Feature comparison → Output (score or preference)

**Design Tradeoffs**: 
- ResNet-50 vs newer vision models: Balance between proven performance and computational efficiency
- MiniLM vs larger text models: Tradeoff between speed and representational capacity
- Regression vs comparative learning: Absolute scores provide interpretability while comparative methods may be more robust

**Failure Signatures**:
- Poor performance on diverse image types suggests feature extraction limitations
- Inconsistent comparative judgments indicate insufficient training data or model capacity
- Low human agreement on comparative tasks suggests task design issues

**First Experiments**:
1. Train regression model on VICR dataset and evaluate Pearson/Spearman correlations
2. Train comparative model on pairwise preferences and measure accuracy
3. Conduct human evaluation comparing direct ratings vs comparative judgments

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Results only validated on single VICR dataset, limiting generalizability claims
- Human evaluation study conducted at small scale, reducing confidence in findings
- Exact sample efficiency gains of comparative model versus regression not quantified

## Confidence
- High confidence: Technical implementation using ResNet-50 and MiniLM, basic experimental setup on VICR dataset
- Medium confidence: Comparative learning framework's effectiveness relative to direct scoring methods
- Medium confidence: Human evaluation results regarding judgment speed and agreement

## Next Checks
1. Test the comparative learning framework on multiple image-caption datasets to verify generalizability
2. Conduct a larger-scale human evaluation study comparing direct ratings vs comparative judgments across different task complexities
3. Measure and report the exact sample efficiency gains of the comparative model versus the regression model across different training set sizes