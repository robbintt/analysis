---
ver: rpa2
title: 'Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding
  to Enhance Safety in Vision-Language Models'
arxiv_id: '2507.21637'
source_url: https://arxiv.org/abs/2507.21637
tags:
- safety
- layer
- layers
- harmful
- sasa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the safety vulnerability of large vision-language
  models (LVLMs) to harmful inputs, which is more pronounced than in their language-only
  counterparts. The authors analyze the internal information flow in LVLMs and identify
  a critical mismatch: safety mechanisms are concentrated in early layers, while robust
  semantic understanding emerges later in intermediate layers.'
---

# Self-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models

## Quick Facts
- **arXiv ID**: 2507.21637
- **Source URL**: https://arxiv.org/abs/2507.21637
- **Reference count**: 40
- **Primary result**: SASA reduces attack success rates by up to 97% across multiple LVLMs while maintaining utility

## Executive Summary
This paper addresses a critical safety vulnerability in large vision-language models (LVLMs), where harmful inputs can bypass safety mechanisms more easily than in text-only models. The authors identify that safety mechanisms concentrate in early transformer layers while robust semantic understanding emerges later in intermediate layers, creating a temporal mismatch where safety decisions are made before comprehensive understanding develops. To address this, they propose Self-Aware Safety Augmentation (SASA), a tuning-free method that projects rich semantic representations from intermediate "fused layers" onto earlier "safety layers" to enhance safety perception. The approach also employs a lightweight linear probe to leverage the model's internal safety awareness for risk detection. Experiments demonstrate significant ASR reduction (up to 97%) across multiple LVLMs and datasets while maintaining model utility.

## Method Summary
SASA addresses the structural mismatch between early-layer safety mechanisms and late-layer semantic understanding in LVLMs through two key components. First, it computes orthogonal projections of rich semantic representations from intermediate "fused layers" onto earlier "safety layers" using inner product projection, enhancing early safety decisions with later-developed understanding. Second, it employs a lightweight logistic regression probe trained on first-token logits from augmented representations to externalize latent safety awareness that the model otherwise fails to express. The method is tuning-free and requires only 5% of typical fine-tuning data while achieving strong zero-shot generalization across different attack types.

## Key Results
- SASA reduces attack success rates by up to 97% across multiple LVLMs while maintaining model utility
- The method achieves strong performance with only 5% of typical training data requirements
- Linear probe detects harmful inputs with 95-100% accuracy using first-token logits
- Demonstrates strong zero-shot generalization across different attack types and datasets

## Why This Works (Mechanism)

### Mechanism 1: Temporal Asynchrony Between Safety Evaluation and Semantic Comprehension
- Claim: LVLMs suffer from a structural mismatch where safety mechanisms execute in early layers before comprehensive semantic understanding develops in intermediate layers.
- Mechanism: Safety-critical attention heads cluster in early layers (layers 0-10), performing rejection decisions on incomplete representations. Meanwhile, rich multimodal semantic fusion occurs later (layers 15-18 in 7B models), where the model can distinguish harmful from benign inputs—but this understanding cannot retroactively influence earlier safety decisions.
- Core assumption: The sequential nature of transformer forward passes creates irreversible decision points where early layers cannot access later-developed semantic information.
- Evidence anchors:
  - [section 3.3]: Ablating top-5 safety heads increased ASR by 47-67%, confirming safety decisions concentrate in early layers
  - [section 3.4]: t-SNE visualizations show harmful/benign inputs maximally separated at intermediate layers (layer 15), but intertwined at both early and final layers
  - [corpus]: Related work (Spot Risks Before Speaking!) corroborates safety head concentration in early transformer layers
- Break condition: If semantic understanding developed uniformly across layers, or if early layers had access to later representations through skip connections spanning the safety-fusion gap, this mechanism would not operate.

### Mechanism 2: Representation Projection as Cross-Layer Knowledge Transfer
- Claim: Projecting semantic-rich representations from fused layers onto earlier safety layers enhances discrimination capability without parameter updates.
- Mechanism: SASA computes orthogonal projections of fused-layer hidden states onto safety-layer hidden states using the formula: Proj_Hf(Hs)[i] = ⟨Hs[i], Hf[i]⟩ / ⟨Hf[i], Hf[i]⟩ · Hf[i]. This transfers discriminative semantic information (which is well-developed at intermediate layers) into early safety layers, augmenting their representations before they make safety decisions.
- Core assumption: The projection operation preserves sufficient semantic structure to improve discrimination without requiring gradient-based learning.
- Evidence anchors:
  - [section 4]: Post-projection t-SNE visualizations show clearer harmful/benign separation persisting from safety layer through output layer
  - [abstract]: "This approach leverages the model's inherent semantic understanding to enhance safety recognition without fine-tuning"
  - [corpus]: No direct corpus evidence for this specific projection mechanism; related work uses different intervention strategies
- Break condition: If the inner product projection operator destroys critical semantic features, or if layer representations are incompatible across the layer gap, augmentation would fail.

### Mechanism 3: Latent Safety Awareness Articulation Through Linear Decoding
- Claim: LVLMs possess internal risk awareness that remains unexpressed in outputs; linear probing can extract this latent knowledge from first-token logits.
- Mechanism: A logistic regression probe trained on output logits (from the vocabulary head applied to augmented representations) learns to map logit distributions to harmful/benign labels. This forces externalization of internal safety signals that the model otherwise fails to translate into refusal responses.
- Core assumption: The model's internal state encodes safety-relevant information that is suppressed during generation, but detectable via linear readout from logit space.
- Evidence anchors:
  - [section 3.4]: High raw ASR (93-99%) despite internal discrimination at fused layers indicates awareness-expression gap
  - [section 4]: Linear probe on SASA-augmented representations achieves 95-100% detection accuracy
  - [corpus]: HiddenDetect similarly monitors hidden states for attack detection, supporting internal-state-based defense approaches
- Break condition: If the model genuinely lacked internal safety awareness (i.e., if the threat representation was never encoded), no linear probe could extract meaningful signal.

## Foundational Learning

### Concept: Attention Head Importance Scoring via Representation Subspace Analysis
- Why needed here: The paper extends SHIPS (Safety Head Important Score) from LLMs to LVLMs to identify which heads critically influence safety decisions. This requires understanding how ablating specific heads shifts the representation subspace.
- Quick check question: If ablating attention head h in layer l causes the top-1 left singular vector of final-layer activations to rotate significantly, what does this tell you about h's role in the model's computation?

### Concept: Layer-wise Semantic Representation Evolution
- Why needed here: The paper maps three processing phases (safety perception → semantic understanding → linguistic alignment) to specific layer ranges. Understanding how representations transform across the forward pass is essential for selecting appropriate source and target layers for projection.
- Quick check question: At which layer range would you expect t-SNE visualizations to show maximal separation between harmful and benign inputs, and why would this separation diminish in later layers?

### Concept: Linear Probing as a Tool for Mechanistic Interpretability
- Why needed here: Linear probes serve dual purposes here—detecting harmful inputs at inference time, and validating that the model encodes latent safety awareness. Understanding when linear classifiers can extract meaningful signals from neural representations is critical for assessing this approach's reliability.
- Quick check question: If a linear probe trained on layer-L activations achieves 95% accuracy but the model still generates harmful outputs, what does this reveal about the relationship between representation and behavior?

## Architecture Onboarding

### Component map:
Input (image + text)
    ↓
Vision Encoder + LLM Embedding Layer
    ↓
Layers 0-12 (SAFETY REGION): Contains safety-critical attention heads
    ├─ Safety Layer (target for projection, e.g., layer 13)
    ↓
Layers 13-18 (FUSION REGION): Rich semantic understanding develops
    ├─ Fused Layer (source for projection, e.g., layer 15)
    ↓
Layers 19-31 (ALIGNMENT REGION): Linguistic expression preparation
    ↓
Vocabulary Head → Logits
    ↓
Linear Probe (trained on first-token logits for harmful/benign classification)

### Critical path:
1. **Identify safety layers**: Compute importance scores for all attention heads on harmful vs. benign data (Eq. 4-7), locate layers containing top-k safety-specific heads
2. **Identify fused layers**: Analyze layer-wise token readability rates (Fig. 5) and semantic similarity curves (Fig. 6) to find layers where semantic understanding peaks before linguistic alignment dominates
3. **Select projection pair**: Choose fused layer (source) and nearest preceding safety layer (target) to minimize representational drift
4. **Apply projection**: During forward pass, replace safety-layer hidden states with projections of fused-layer hidden states (Eq. 8)
5. **Train probe**: Train logistic regression on first-token logits from augmented forward passes using small labeled dataset (5-10 samples per scenario)
6. **Inference-time defense**: Project representations, compute first-token logits, apply probe—if harmful (>0.5 threshold), refuse response

### Design tradeoffs:
- **Projection distance vs. alignment**: Targeting distant safety layers risks incompatibility with fused representations; nearby layers share more structure but may provide less early intervention
- **Probe complexity vs. calibration**: Logistic regression is fast and data-efficient but may miscalibrate on out-of-distribution attacks; deeper probes require more training data
- **Utility preservation vs. safety maximization**: Aggressive projection may interfere with benign-task representations, degrading helpfulness accuracy
- **Intervention timing**: Pre-generation probing enables early rejection but adds latency; post-generation filtering preserves speed but allows harmful content in memory

### Failure signatures:
- **Probe overfitting**: High training accuracy but poor zero-shot generalization (check cross-dataset transfer in Fig. 9)
- **Projection incompatibility**: Post-projection representations show reduced separation in t-SNE space (validate with visualizations like Fig. 7)
- **Utility degradation**: Helpfulness accuracy drops >5% on benign benchmarks (monitor MM-Vet, COCO-VQA, ScienceQA)
- **Layer mismatch**: Different model architectures may have different fusion layer locations—hardcoding layer 15/18 will fail (must re-run layer-wise analysis)

### First 3 experiments:
1. **Layer localization validation**: For a new LVLM (e.g., Qwen2.5-VL), compute head importance scores and readability rates across all layers; verify safety heads cluster early and fused layers appear intermediate before implementing projection
2. **Projection ablation study**: Test projection from fused layer to multiple candidate safety layers (as in Table 3); measure both safety improvement and utility retention to find optimal target
3. **Data efficiency curve**: Train probes with varying sample sizes (5, 10, 20, 50 samples per scenario as in Table 7); establish minimum viable training set for target deployment context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the effectiveness of SASA be improved by replacing the geometric vector projection (Eq. 8) with a learnable transformation matrix, or does the noise introduced by training parameters negate the benefits of semantic transfer?
- Basis in paper: [inferred] The paper utilizes a fixed geometric projection formula to transfer representations from fused layers to safety layers (Eq. 8) to remain tuning-free, but does not evaluate if a learned projection could preserve utility better while maintaining safety.
- Why unresolved: It is unclear if the rigid geometric projection sub-optimally aligns the feature spaces compared to a trained adapter, which could potentially lower the attack success rate (ASR) further without the data overhead of full fine-tuning.
- What evidence would resolve it: A comparative study evaluating SASA’s performance when using a small, trained linear adapter for projection versus the current geometric method.

### Open Question 2
- Question: Does the localization of "safety layers" in early modules and "fused layers" in intermediate modules generalize to encoder-decoder architectures or diffusion-based Vision-Language Models?
- Basis in paper: [inferred] The internal dynamics analysis is conducted exclusively on decoder-only LVLMs (LLaVA, MiniGPT, Qwen), leaving the structural mismatch hypothesis untested in architectures with fundamentally different information flows.
- Why unresolved: The concentration of safety mechanisms in early layers might be a specific artifact of the decoder-only architecture; encoder-decoder models might distribute safety and semantic understanding differently.
- What evidence would resolve it: Replicating the safety head importance analysis and t-SNE visualization of activation distributions on encoder-decoder or diffusion-based multimodal models.

### Open Question 3
- Question: How robust is the linear probe against adaptive attacks specifically designed to shift the intermediate layer representations while maintaining the visual appearance of benign inputs?
- Basis in paper: [inferred] The method relies on a lightweight linear probe trained on output logits to detect risk, demonstrating high accuracy against existing benchmarks like MM-SafetyBench, but does not test against adversarial examples tailored to evade this specific probe.
- Why unresolved: While the method shows strong zero-shot generalization to new datasets, it is unclear if an attacker could generate inputs that activate the "fused layer" semantics but fail to trigger the linear probe's threshold.
- What evidence would resolve it: Evaluation of SASA against white-box attacks where the adversary has knowledge of the probe and aims to maximize the probe's loss function while maintaining image perturbations.

## Limitations
- The paper assumes fixed layer ranges for safety perception, semantic understanding, and linguistic alignment across all LVLMs, which may not generalize to different architectures
- The orthogonal projection mechanism's effectiveness depends on representation geometry that could vary substantially across models trained on different data distributions
- Linear probe calibration across diverse attack types and benign distributions is not thoroughly validated despite achieving high detection accuracy
- Utility-safety tradeoff analysis is limited, with insufficient examination of how SASA affects model performance across diverse benign tasks

## Confidence
**High Confidence (4/4 evidence anchors met)**: The identification of safety head concentration in early layers is strongly supported by ablation studies showing 47-67% ASR increases when removing top safety heads. The demonstration that harmful/benign inputs maximally separate at intermediate layers via t-SNE visualization is well-validated.

**Medium Confidence (2-3/4 evidence anchors met)**: The effectiveness of projection-based augmentation is demonstrated through improved t-SNE separation and reduced ASR, but lacks ablation studies testing alternative projection methods or examining why this specific projection preserves semantic structure. The linear probe's detection capability is validated but its calibration robustness across attack types remains less certain.

**Low Confidence (0-1/4 evidence anchors met)**: Claims about the universality of the three-phase processing model across different LVLM architectures are not adequately validated. The assumption that projection distance can be optimized without extensive per-model analysis is insufficiently tested.

## Next Checks
1. **Architecture Transferability Study**: Apply the layer-wise importance analysis (safety head identification, readability rate measurement, semantic similarity tracking) to three diverse LVLM architectures (different model families, vision encoders, and sizes). Document how the safety-fusion gap varies across architectures and whether the projection mechanism remains effective when source and target layers are optimally selected for each architecture.

2. **Projection Mechanism Ablation**: Systematically compare SASA's projection-based augmentation against alternative cross-layer transfer methods: (a) direct hidden state copying, (b) linear transformation of fused to safety representations, (c) attention-based feature fusion, and (d) no augmentation (baseline). Measure both safety improvement (ASR reduction) and representation compatibility (t-SNE separation maintenance) to isolate the specific contribution of orthogonal projection.

3. **Probe Calibration and Robustness Analysis**: Evaluate the linear probe's performance across a spectrum of attack types including: (a) distribution-shifted attacks not seen in training, (b) adversarial perturbations designed to fool the probe, and (c) benign inputs with borderline semantic content. Measure false positive/negative rates and compare against a deeper probe (e.g., small MLP) trained on 10x the data to establish calibration baselines and identify probe failure modes.