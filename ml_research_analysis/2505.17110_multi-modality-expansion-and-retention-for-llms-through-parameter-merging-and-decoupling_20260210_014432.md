---
ver: rpa2
title: Multi-Modality Expansion and Retention for LLMs through Parameter Merging and
  Decoupling
arxiv_id: '2505.17110'
source_url: https://arxiv.org/abs/2505.17110
tags:
- mmer
- mllms
- parameters
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a training-free method, MMER, to expand multimodal
  capabilities of large language models by merging and decoupling parameters across
  multiple existing multimodal models. MMER reuses modality-specific encoders, merges
  task vectors, and constructs binary masks to decouple modality-specific parameters,
  reducing conflicts and retaining original performance.
---

# Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling

## Quick Facts
- **arXiv ID**: 2505.17110
- **Source URL**: https://arxiv.org/abs/2505.17110
- **Reference count**: 37
- **Key outcome**: MMER achieves 99% retention of original performance across fourteen dual-modal tasks while expanding multimodal capabilities through parameter merging and decoupling

## Executive Summary
This paper introduces MMER, a training-free method for expanding multimodal capabilities in large language models by merging and decoupling parameters across multiple existing multimodal models. The approach reuses modality-specific encoders, merges task vectors, and constructs binary masks to decouple modality-specific parameters, reducing conflicts and retaining original performance. Experiments demonstrate that MMER achieves 99% retention of original performance across fourteen dual-modal tasks, improves multimodal expansion results significantly over baselines, and mitigates catastrophic forgetting when adapting to new tasks without losing performance on previous ones.

## Method Summary
MMER employs a parameter merging and decoupling strategy to expand multimodal capabilities in LLMs without additional training. The method reuses modality-specific encoders from existing models, merges their task vectors, and constructs binary masks to isolate and decouple modality-specific parameters. This approach reduces parameter conflicts that typically arise when combining multimodal models and helps retain the original performance of each component model. The method operates entirely without training, making it computationally efficient compared to fine-tuning approaches.

## Key Results
- Achieves 99% retention of original performance across fourteen dual-modal tasks
- Storage cost is approximately twice that of standard model merging while delivering notable performance gains
- Significantly improves multimodal expansion results over baseline methods
- Effectively mitigates catastrophic forgetting when adapting to new tasks

## Why This Works (Mechanism)
The method works by addressing parameter conflicts that occur when merging multimodal models. By reusing existing modality-specific encoders and applying binary masks to decouple their parameters, MMER prevents interference between different modality representations. The task vector merging allows the combined model to handle multiple modalities while maintaining the specialized knowledge encoded in each original model.

## Foundational Learning
- **Parameter merging**: Combining weights from multiple models into a single model - needed to consolidate multimodal capabilities, quick check: verify merged parameters preserve individual model behaviors
- **Catastrophic forgetting**: Loss of previously learned capabilities when adapting to new tasks - needed context for evaluating retention performance, quick check: measure performance drop on old tasks after new task addition
- **Binary masking**: Using binary vectors to select or isolate specific parameters - needed to decouple modality-specific components, quick check: verify masks correctly isolate intended parameter subsets
- **Task vectors**: Parameter vectors that encode task-specific knowledge - needed for representing different modalities, quick check: ensure merged task vectors maintain modality distinctions
- **Modality-specific encoders**: Model components that process specific input types (text, image, etc.) - needed to handle different input modalities, quick check: verify each encoder maintains its original functionality
- **Training-free adaptation**: Methods that modify model behavior without gradient updates - needed for computational efficiency, quick check: confirm no training loops are used in the implementation

## Architecture Onboarding

### Component Map
Modality-specific encoders -> Binary masks -> Merged parameter space -> Combined task vectors -> Output generation

### Critical Path
Input processing by modality-specific encoders → Binary mask application → Parameter merging → Task vector combination → Output generation

### Design Tradeoffs
- Storage cost vs. performance retention (2x storage for 99% retention)
- Training-free operation vs. potential gains from fine-tuning
- Modality isolation vs. cross-modal interaction capability

### Failure Signatures
- Performance degradation when mask boundaries are incorrectly specified
- Catastrophic forgetting if parameter conflicts are not properly resolved
- Modality interference if task vectors are not properly merged

### First Experiments
1. Merge two text-image models and verify individual modality performance retention
2. Test binary mask effectiveness in preventing parameter conflicts
3. Evaluate catastrophic forgetting by sequentially adding tasks and measuring performance on previous tasks

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Scalability to more than two modalities remains untested
- Performance on more diverse and complex multimodal tasks is unknown
- Long-term stability under continuous adaptation is not evaluated
- Storage cost of approximately twice standard merging may be prohibitive for some applications

## Confidence

**High confidence**: The core methodology of parameter merging and decoupling is technically sound and the storage cost comparison appears reasonable

**Medium confidence**: The retention performance across the tested tasks, as the evaluation scope is limited and may not generalize to all multimodal scenarios

**Medium confidence**: The catastrophic forgetting mitigation claims, as the evaluation period and task sequences tested are not fully specified

## Next Checks
1. Test the method on triple or higher-order modality combinations to evaluate scalability limits
2. Evaluate performance on more complex multimodal reasoning tasks beyond the current task set
3. Conduct long-term stability tests with sequential task addition to verify catastrophic forgetting claims over extended adaptation periods