---
ver: rpa2
title: Discovering Knowledge Deficiencies of Language Models on Massive Knowledge
  Base
arxiv_id: '2503.23361'
source_url: https://arxiv.org/abs/2503.23361
tags:
- error
- zhang
- knowledge
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEA, a scalable framework for uncovering
  knowledge deficiencies in large language models (LLMs) under strict query budgets.
  By framing error discovery as a stochastic optimization process, SEA iteratively
  retrieves high-error candidates through hierarchical semantic similarity to prior
  failures and employs a relation-directed acyclic graph to model error propagation.
---

# Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base

## Quick Facts
- arXiv ID: 2503.23361
- Source URL: https://arxiv.org/abs/2503.23361
- Reference count: 30
- Key outcome: SEA discovers 40.7× more errors than ACD and 26.7% more than AutoBencher, reducing cost-per-error by 599× and 9× respectively.

## Executive Summary
SEA is a scalable framework for uncovering knowledge deficiencies in large language models under strict query budgets. It frames error discovery as stochastic optimization, iteratively retrieving high-error candidates through hierarchical semantic similarity to prior failures. By constructing a relation-directed acyclic graph to model error propagation and pruning low-impact nodes, SEA efficiently identifies systematic failure modes. The framework achieves superior performance compared to Automated Capability Discovery and AutoBencher while maintaining 100% human-verified reliability of generated questions.

## Method Summary
SEA discovers LLM knowledge deficiencies by iteratively retrieving error-related candidates through hierarchical semantic similarity to prior failures, constructing a relation-directed acyclic graph to model error propagation, and pruning low-impact nodes. The framework uses a Wikipedia knowledge base (7.1M documents, 28.8M paragraphs) pre-embedded with mGPT and indexed via FAISS. GPT-4o generates multiple question variants per paragraph, which are evaluated on target LLMs. Source errors are identified where model accuracy falls below a threshold, and the process repeats until the query budget is exhausted.

## Key Results
- SEA uncovers 40.7× more errors than Automated Capability Discovery and 26.7% more than AutoBencher
- Cost-per-error reduced by 599× compared to ACD and 9× compared to AutoBencher
- Human evaluation confirms 100% reliability of generated questions
- Consistent error discovery across steps reveals correlated failure patterns across LLM families

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If model failures cluster semantically, searching the vector space around prior errors yields a higher density of new errors than random sampling.
- **Mechanism:** The framework uses a Sentence Transformer to embed paragraphs and retrieves new candidates based on cosine similarity to a set of "source errors."
- **Core assumption:** LLM knowledge deficiencies are localized in semantic space.
- **Break condition:** If errors are distributed randomly across the knowledge base rather than clustered, semantic retrieval will perform no better than random sampling.

### Mechanism 2
- **Claim:** If error propagation follows relational structures, constructing a Directed Acyclic Graph (DAG) allows for the pruning of spurious errors and identification of systematic failure modes.
- **Mechanism:** SEA links source errors to subsequent errors they induce and calculates a cumulative error score for nodes.
- **Core assumption:** Knowledge errors have a causal or relational dependency that can be mapped.
- **Break condition:** If errors are independent events without relational correlation, the DAG structure will fail to identify meaningful propagation patterns.

### Mechanism 3
- **Claim:** If an LLM fails on multiple rephrased variants of the same question, the failure is likely a knowledge gap rather than prompt sensitivity or format issues.
- **Mechanism:** The generator creates multiple semantically equivalent variants of a question and marks a paragraph as a "source error" only if the model fails consistently across rephrasings.
- **Core assumption:** Random errors or "jailbreak" failures are prompt-specific, whereas true knowledge deficiencies persist across linguistic variations.
- **Break condition:** If the model's failure mode is semantic confusion that persists across rephrasing but is not a factual gap, this mechanism may conflate reasoning failure with knowledge absence.

## Foundational Learning

- **Concept: Stochastic Optimization / Hill Climbing**
  - **Why needed here:** SEA frames error discovery not as exhaustive testing, but as an optimization problem (maximizing errors found under a budget).
  - **Quick check question:** How does SEA decide which direction to "step" into next within the knowledge base?

- **Concept: Dense Retrieval (Vector Search)**
  - **Why needed here:** The mechanism relies on finding "similar" paragraphs to past errors using embeddings, rather than keyword matching.
  - **Quick check question:** What metric is used to determine if a new paragraph is "close" to a previously discovered error?

- **Concept: Directed Acyclic Graphs (DAGs)**
  - **Why needed here:** Understanding how the framework models "error propagation" requires knowing how DAGs represent directed dependencies without cycles.
  - **Quick check question:** In the Relation DAG, what does an edge from Node A to Node B represent in terms of model failure?

## Architecture Onboarding

- **Component map:**
  - Knowledge Base (KB) -> Generator (g) -> Testee (f_close) -> SEA Core -> P_source -> Relation DAG

- **Critical path:**
  1. Retrieve initial batch -> Generate Questions -> Test f_close
  2. Identify errors -> Add to P_source
  3. Embed P_source -> Retrieve similar docs from KB (Hierarchical Retrieval)
  4. Update DAG -> Prune low-impact nodes -> Repeat until budget exhausted

- **Design tradeoffs:**
  - **Precision vs. Cost:** Hierarchical retrieval (doc → paragraph) reduces computation but might miss fine-grained context compared to full-paragraph indexing.
  - **Exploration vs. Exploitation:** SEA is heavily exploitation-focused (climbing the error gradient). This finds clusters of errors efficiently but may miss isolated, distant errors (mode collapse).

- **Failure signatures:**
  - **Convergence:** Cumulative error plateaus early, indicating the model has exhausted findable errors in that semantic region or the retriever is stuck in a loop.
  - **Generator Hallucination:** If human eval rate drops, the Generator is creating questions not grounded in the paragraph.

- **First 3 experiments:**
  1. **Ablation on Retrieval:** Run SEA vs. Random Sampling on the same budget to confirm the "semantic similarity" hypothesis.
  2. **Cross-Model Transfer:** Test if errors found for Model A also trigger errors in Model B to validate if deficiencies are universal or model-specific.
  3. **Budget Scaling:** Plot "Errors Found" vs. "API Cost" to determine the marginal utility of additional queries.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can high-quality question-answer pairs be systematically synthesized from multimodal content (images, videos) to enable SEA-style deficiency discovery in vision-language models?
- **Basis in paper:** Section 8 explicitly identifies generalization to multimodal domains as a limitation, noting that the difficulty of high-quality question-answer pair synthesis from the multimodal domain limits the extension.
- **Why unresolved:** Current automated annotation methods produce low-quality outputs that propagate noise into error discovery, unlike the 100% human-verified quality achieved for text-based questions.
- **What evidence would resolve it:** Demonstration of a multimodal question generation pipeline achieving >95% factual accuracy (verified against ground truth) while maintaining semantic diversity comparable to text-based SEA.

### Open Question 2
- **Question:** Can a lightweight surrogate model be trained to accurately predict LLM failure patterns from paragraph embeddings, thereby enabling search scope enlargement beyond initial seed sets?
- **Basis in paper:** Section 8 and Appendix A describe attempts to fit BERT models to predict error-inducing paragraphs, achieving only 66-68% accuracy.
- **Why unresolved:** The noisy, indirect relationship between surface-level paragraph content and downstream model behavior makes this a challenging classification task; simple fine-tuning approaches fail to capture the semantic cues reliably.
- **What evidence would resolve it:** A surrogate model achieving >85% accuracy on held-out paragraphs for predicting LLM errors, enabling budget-free candidate pre-screening before actual LLM queries.

### Open Question 3
- **Question:** What mechanisms can effectively overcome the memory-context conflict where LLMs ignore retrieved factual corrections in favor of pre-trained parametric knowledge?
- **Basis in paper:** Query 5 results show that providing retrieved factual context alongside error-inducing questions improved gpt-4o accuracy to only 28.6%, indicating the model predominantly relies on its pre-trained internal knowledge despite external augmentation.
- **Why unresolved:** The paper identifies this phenomenon but does not investigate interventions; it remains unclear whether prompt engineering, retrieval-augmented generation architectures, or fine-tuning approaches could shift model behavior toward context reliance.
- **What evidence would resolve it:** Systematic comparison of intervention strategies (e.g., instruction-based prompting, retrieval architecture modifications, targeted fine-tuning) showing statistically significant improvements in context-adoption rates on SEA-discovered deficiency questions.

## Limitations
- The framework assumes semantic clustering of errors is a general phenomenon across knowledge domains, but validation is limited to Wikipedia-style encyclopedic content.
- The Relation DAG pruning mechanism may incorrectly filter legitimate edge-case errors that don't propagate but are still valid knowledge gaps.
- The 100% human evaluation reliability claim for generated questions doesn't account for domain-expert evaluation.

## Confidence
- **High:** SEA's superior cost-per-error performance (599× improvement) - directly measurable from experiments.
- **Medium:** The stochastic optimization mechanism's effectiveness - supported by results but limited to one knowledge base type.
- **Low:** The claim about uncovering "correlated failure patterns across LLM families" - based on pairwise comparisons without systematic analysis of failure correlation strength.

## Next Checks
1. **Domain Transfer Test:** Apply SEA to a specialized knowledge base (e.g., legal documents or biomedical literature) and compare error discovery rate to baseline methods to test semantic clustering assumption across domains.
2. **Pruning Ablation Study:** Run SEA with DAG pruning disabled (retain all source errors regardless of cumulative score) and measure the impact on error discovery efficiency and final error set quality.
3. **Isolated Error Detection:** Modify SEA to include periodic random sampling (exploration phase) and quantify how many isolated, non-clustered errors are discovered that the purely exploitation-focused approach misses.