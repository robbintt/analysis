---
ver: rpa2
title: 'Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for
  Recommendation'
arxiv_id: '2510.13229'
source_url: https://arxiv.org/abs/2510.13229
tags:
- learning
- user
- recommendation
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of integrating large language
  models (LLMs) into recommender systems, addressing issues of high latency and reliability
  from frequent API calls. The proposed IL-Rec framework leverages imitation learning
  from LLM-generated trajectories using inverse reinforcement learning to extract
  reward models, eliminating the need for continuous LLM invocation.
---

# Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation

## Quick Facts
- arXiv ID: 2510.13229
- Source URL: https://arxiv.org/abs/2510.13229
- Authors: Yi Zhang; Lili Xie; Ruihong Qiu; Jiajun Liu; Sen Wang
- Reference count: 40
- One-line primary result: IL-Rec eliminates continuous LLM API calls while transferring semantic knowledge via imitation learning from LLM-generated demonstrations, achieving 13.4-13.6% higher cumulative rewards than SOTA baselines.

## Executive Summary
This paper introduces IL-Rec, a framework that addresses the high latency and reliability issues of using large language models (LLMs) in recommender systems by combining imitation learning with reinforcement learning. Instead of calling LLMs repeatedly during RL policy training, IL-Rec generates demonstrations offline using an LLM Actor-Critic pipeline, then trains a policy using inverse reinforcement learning (IRL) to extract a reward model from these demonstrations. This allows the final RL policy to operate without LLM dependency while preserving semantic knowledge. Experiments on Steam and Amazon-Book datasets show IL-Rec outperforms state-of-the-art RL and LLM baselines in cumulative reward and interaction length.

## Method Summary
IL-Rec first trains a world model (DeepFM for rewards, sequential model for transitions) on offline user-item interaction logs. It then generates expert demonstrations by running an LLM Actor-Critic pipeline that uses a Reflector to analyze user feedback, a Planner to retrieve similar items via FAISS, and an LLM to select actions based on state and retrieved items. A discriminator is trained via adversarial inverse reinforcement learning to distinguish policy-generated trajectories from expert ones, producing IRL-based weights. The final policy is trained using a weighted combination of environment-based and IRL-based weights, blending imitation loss with actor-critic RL loss and entropy regularization.

## Key Results
- IL-Rec achieves 13.4-13.6% higher cumulative rewards (R_traj) than SOTA RL and LLM baselines on Steam and Amazon datasets
- Interaction length (Len) increases by 12.8-16.8% compared to baselines
- Performance scales with stronger LLMs (GPT-4 vs GPT-3.5) and remains robust to hyperparameter variations

## Why This Works (Mechanism)
IL-Rec works by decoupling the knowledge extraction phase (LLM-generated demonstrations) from the deployment phase (policy inference), eliminating runtime LLM API calls. The imitation-enhanced reinforcement learning framework uses IRL to extract a dense reward model from demonstrations, which provides more informative gradients than sparse environmental rewards. By combining environment-based and IRL-based weights through a fusion parameter α, the method balances exploration (via environment rewards) with exploitation of demonstrated preferences. The entropy regularization encourages diverse recommendations, while the weighted loss function allows the policy to focus on high-value state-action pairs identified by both the world model and the discriminator.

## Foundational Learning
- **Inverse Reinforcement Learning (IRL)**: Learning a reward function from expert demonstrations rather than from environmental rewards; needed to extract semantic preferences from LLM-generated trajectories; quick check: can discriminator distinguish policy from expert trajectories?
- **Actor-Critic RL**: Policy gradient method using value function critics to reduce variance; needed for stable policy updates in sequential recommendation; quick check: are advantage estimates well-behaved (not exploding)?
- **World Model Prediction**: Learning environment dynamics (transition and reward functions) from offline data; needed to simulate rollouts for advantage computation without online interaction; quick check: does the model generalize to unseen state-action pairs?
- **FAISS Retrieval**: Efficient similarity search in high-dimensional embedding space; needed to find items similar to user feedback for the Planner module; quick check: are retrieved items semantically relevant to user preferences?
- **Adversarial Training**: Training a discriminator to distinguish between policy and expert behaviors; needed to estimate the reward function that explains expert behavior; quick check: is the discriminator well-calibrated (outputs between 0 and 1)?
- **Weighted Imitation Learning**: Combining multiple reward signals through weighted loss functions; needed to balance environmental rewards with learned reward from demonstrations; quick check: do the weights appropriately emphasize high-value demonstrations?

## Architecture Onboarding

**Component Map**: Offline logs -> World Model (DeepFM + Sequential) -> LLM Demonstration Generator (Reflector->Planner->Actor->Critic) -> IRL Discriminator -> Policy Trainer -> RL Policy

**Critical Path**: LLM demonstrations → IRL discriminator → fused weights → policy updates → improved recommendations

**Design Tradeoffs**: Using LLM demonstrations eliminates runtime API calls but introduces bias from the demonstrator; IRL-based rewards provide richer supervision than environmental rewards but require careful discriminator training to avoid collapse; the fusion parameter α trades off between exploration and exploitation.

**Failure Signatures**: Discriminator collapse (D_ψ→1 or →0 everywhere), weight explosion (w(s,a)→∞), policy overfitting to suboptimal demonstrations, poor world model generalization leading to incorrect advantage estimates.

**First Experiments**:
1. Train the world model and verify it can predict rewards and transitions accurately on held-out data
2. Generate demonstrations with a small LLM and verify the discriminator can distinguish them from random policies
3. Train a policy using only the environment reward (w_IRL=0) and compare to full IL-Rec to measure IRL contribution

## Open Questions the Paper Calls Out
- How do specific biases and coverage limitations in LLM-generated demonstrations quantitatively impact the convergence and optimality of the learned RL policy? (Section VI explicitly states intent to investigate fine-grained aspects of demonstrations including biases and coverage.)
- Can the IL-Rec framework maintain performance and low latency in large-scale, real-world recommendation environments with millions of items? (Section VI lists extending to real-world scenarios as a primary direction.)
- To what extent does the accuracy of the pre-trained world model dictate the success of the advantage-weighting mechanism? (The paper computes advantages via rollouts but does not analyze error propagation from world model inaccuracies.)

## Limitations
- Exact neural architectures for discriminator, policy, and critics are not specified, limiting reproducibility
- No ablation studies isolate the contribution of each component to the reported performance gains
- Experiments limited to two benchmark datasets; scalability to industrial-scale recommendation problems untested
- No statistical significance testing or variance reporting across runs

## Confidence
- Method description: Medium (key details missing)
- Reproducibility: Low (multiple critical hyperparameters and architectures unspecified)
- Performance claims: Medium (results show improvement but lack statistical validation)
- Scalability claims: Low (only tested on small datasets)

## Next Checks
1. Re-implement the discriminator and policy networks with exact hyperparameters from the paper; compare convergence curves and final R_traj values on the Steam dataset
2. Perform an ablation study removing the IRL component (i.e., w_IRL=1) to quantify its contribution to the 13.4–13.6% gain
3. Test the framework with a smaller, open-source LLM (e.g., Llama2-7b) to verify robustness claims under weaker model capacity