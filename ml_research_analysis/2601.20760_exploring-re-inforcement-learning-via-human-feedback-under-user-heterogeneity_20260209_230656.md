---
ver: rpa2
title: Exploring Re-inforcement Learning via Human Feedback under User Heterogeneity
arxiv_id: '2601.20760'
source_url: https://arxiv.org/abs/2601.20760
tags:
- preferences
- reward
- users
- workers
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of heterogeneity in human preferences
  in Reinforcement Learning from Human Feedback (RLHF) by proposing a clustering-based
  approach to create personalized reward models. The method learns worker embeddings
  and clusters workers with similar preferences, then creates separate reward models
  for each cluster.
---

# Exploring Re-inforcement Learning via Human Feedback under User Heterogeneity
## Quick Facts
- arXiv ID: 2601.20760
- Source URL: https://arxiv.org/abs/2601.20760
- Reference count: 18
- Primary result: Clustering-based approach to handle preference heterogeneity in RLHF shows improved win-rates (53.221% vs 52.133%) on Reddit TL;DR dataset

## Executive Summary
This paper addresses a fundamental challenge in Reinforcement Learning from Human Feedback (RLHF): how to handle heterogeneity in human preferences when multiple workers provide feedback. The authors propose a novel clustering-based approach that learns worker embeddings and groups workers with similar preferences, then creates separate reward models for each cluster. This personalized approach aims to better capture diverse user preferences rather than forcing a single homogeneous reward model.

The method is empirically tested on the Reddit TL;DR summarization dataset, demonstrating that personalized reward models aligned to worker clusters outperform naive RLHF approaches. The work represents an important step toward making RLHF systems more adaptable to real-world scenarios where user preferences naturally vary, though validation remains limited to a single dataset.

## Method Summary
The proposed method tackles preference heterogeneity in RLHF through a clustering-based framework that creates personalized reward models. The approach begins by collecting preference data from multiple human workers and learning worker embeddings that capture individual preference patterns. These embeddings are then clustered using algorithms like K-means or hierarchical clustering to identify groups of workers with similar preferences.

Once clusters are formed, separate reward models are trained for each cluster, allowing the system to generate outputs tailored to different preference profiles. During inference, the system identifies which cluster a new user's preferences align with and uses the corresponding reward model for fine-tuning. This contrasts with traditional RLHF approaches that train a single reward model on aggregated preference data, potentially losing nuanced preference information.

## Key Results
- Clustering approach achieves 53.221% win-rate for Group 1 versus 52.133% for naive RLHF
- Clustering approach achieves 52.702% win-rate for Group 2 versus 52.133% for naive RLHF
- Personalized reward models aligned to worker clusters outperform single homogeneous models

## Why This Works (Mechanism)
The approach works by recognizing that human preferences are inherently heterogeneous rather than uniform. By clustering workers based on their preference patterns and creating separate reward models for each cluster, the system can better capture and represent the diversity of human feedback. This allows for more personalized and aligned outputs that better match individual user preferences, rather than forcing all users into a single preference model that may not adequately represent any particular group.

## Foundational Learning
- Reinforcement Learning from Human Feedback (RLHF): Why needed - foundation for aligning AI systems with human preferences; Quick check - understand preference collection and reward modeling basics
- Worker embedding learning: Why needed - captures individual preference patterns for clustering; Quick check - grasp how embeddings represent preference similarities
- Preference clustering algorithms: Why needed - groups similar preferences for personalized modeling; Quick check - familiarity with K-means or hierarchical clustering concepts

## Architecture Onboarding
**Component Map:** Worker feedback -> Embedding learning -> Clustering -> Cluster-specific reward models -> Personalized RLHF

**Critical Path:** Worker preference data → Worker embedding model → Clustering algorithm → Cluster-specific reward model training → Personalized fine-tuning

**Design Tradeoffs:** Multiple reward models increase complexity and computational cost but provide better preference alignment versus single model simplicity but potential misalignment

**Failure Signatures:** Poor clustering quality leads to misaligned reward models; insufficient worker data prevents meaningful clustering; over-clustering creates sparse data issues per cluster

**3 First Experiments:** 1) Validate clustering quality on preference data, 2) Test reward model performance per cluster independently, 3) Compare win-rates across different cluster counts

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to single Reddit TL;DR dataset, limiting generalizability
- Modest win-rate improvements (53.221% vs 52.133%) may not translate to all applications
- Requires sufficient workers for meaningful clustering, may not work in low-data scenarios

## Confidence
- **High confidence**: Methodological framework for personalized reward models through worker clustering is sound
- **Medium confidence**: Empirical results showing improved win-rates over naive RLHF on single dataset
- **Low confidence**: Scalability to diverse domains and performance in low-resource settings

## Next Checks
1. Replicate experiments across multiple diverse datasets to assess generalizability
2. Conduct ablation studies on minimum worker requirements and optimal cluster counts
3. Test approach in low-resource settings with constrained preference data availability