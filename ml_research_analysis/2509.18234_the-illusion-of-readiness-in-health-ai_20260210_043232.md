---
ver: rpa2
title: The Illusion of Readiness in Health AI
arxiv_id: '2509.18234'
source_url: https://arxiv.org/abs/2509.18234
tags:
- image
- answer
- reasoning
- visual
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial stress tests reveal that large multimodal models often
  fail under simple input perturbations, despite strong leaderboard scores. These
  models can guess correct answers without essential visual inputs and fabricate flawed
  reasoning, indicating reliance on superficial patterns rather than true multimodal
  understanding.
---

# The Illusion of Readiness in Health AI

## Quick Facts
- arXiv ID: 2509.18234
- Source URL: https://arxiv.org/abs/2509.18234
- Reference count: 40
- Leading AI models often fail under simple input perturbations despite strong leaderboard scores, revealing reliance on superficial patterns rather than true multimodal understanding.

## Executive Summary
Large multimodal models achieve high scores on medical diagnostic benchmarks but fail under simple adversarial stress tests, exposing fundamental gaps in their readiness for clinical use. Through systematic perturbation of inputs—removing images, shuffling answer choices, substituting visual content—the study reveals that models often rely on superficial shortcuts rather than genuine multimodal reasoning. Clinician-guided rubrics further demonstrate that widely-used benchmarks measure diverse and often mismatched capabilities, undermining their validity for assessing real-world readiness. The findings call for stress testing as a standard evaluation practice to ensure trustworthy, clinically reliable AI systems.

## Method Summary
The study evaluates eight leading multimodal models on six medical diagnostic benchmarks using a suite of six stress tests (T1-T6) that systematically perturb inputs to expose shortcut learning and reasoning failures. Models are tested under baseline conditions and across perturbations including modality removal, answer option shuffling, visual substitution, and Chain-of-Thought analysis. A clinician-designed rubric profiles benchmark complexity across ten axes including reasoning demands and visual requirements. Robustness scores aggregate fragility across all tests, while a three-stage failure taxonomy classifies response types. The approach combines quantitative perturbation analysis with qualitative clinician assessment to comprehensively evaluate model capabilities.

## Key Results
- GPT-5 achieved 80.9% accuracy on NEJM Image Challenge but dropped 5.7 percentage points when answer options were reordered without images
- Visual substitution caused 30-35 percentage point accuracy drops across models, indicating reliance on memorized image-answer associations rather than dynamic reasoning
- Benchmark profiling revealed wide heterogeneity in reasoning and visual demands, with NEJM ranking high on both dimensions while JAMA was largely text-solvable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stress tests expose shortcut learning by disrupting shallow cues that models rely on instead of genuine multimodal reasoning.
- Mechanism: Removing images or shuffling answer choices degrades text-only performance (e.g., GPT-5 dropped -5.7pp, o3 dropped -6.3pp when options reordered without images), revealing positional and semantic shortcut dependencies. Visual grounding compensates: same perturbations with images yielded +2–5pp gains.
- Core assumption: Models exploit dataset artifacts (option position, familiar distractor patterns, disease prevalence priors) rather than integrating visual evidence.
- Evidence anchors:
  - [abstract]: "leading systems can guess the right answer even with key inputs removed, yet may get confused by the slightest prompt alterations"
  - [section]: "On the NEJM Visual-Required Subset (NEJM-VS), accuracy under text-only input declined across all models (e.g., GPT-5: –5.7 pp, Gemini: –4.0 pp, o3: –6.3 pp), indicating sensitivity to positional biases"
  - [corpus]: Related work on adversarial robustness (SafeMed-R1, CARES) confirms vulnerability patterns but lacks comparative cross-model robustness scores.
- Break condition: If models maintain performance across all perturbations, shortcut hypothesis is falsified.

### Mechanism 2
- Claim: Clinician-designed rubrics decompose benchmark demands into measurable reasoning and visual complexity dimensions, exposing heterogeneity masked by aggregate scores.
- Mechanism: Three board-certified clinicians independently rated 9 benchmarks on 10 axes (e.g., text-only solvability, visual detail required). Fleiss' κ ranged 0.67–0.90, revealing NEJM ranks high on both dimensions while JAMA is largely text-solvable.
- Core assumption: Clinician judgment captures task-relevant complexity that model accuracy alone obscures.
- Evidence anchors:
  - [abstract]: "Clinician-guided rubrics showed that medical benchmarks vary widely in reasoning and visual demands, often measuring different competencies"
  - [section]: "Each benchmark was independently annotated by three board-certified clinicians per axis, using a 3-point ordinal scale. We report the median score... Agreement was moderate to strong across axes (range: 0.67-0.90)"
  - [corpus]: Related benchmarking papers (Neural-MedBench, HealthQA-BR) propose deeper reasoning evaluation but without the systematic rubric-based profiling method.
- Break condition: If inter-annotator agreement were near chance (κ ≈ 0), rubric validity would be compromised.

### Mechanism 3
- Claim: Visual substitution isolates grounding failures—models persist with prior predictions despite contradictory visual evidence.
- Mechanism: Replacing images with distractor-matched alternatives caused 30–35pp accuracy drops (GPT-5: 83.3%→51.7%, Gemini: 80.8%→47.5%), indicating memorized image–answer associations rather than dynamic reinterpretation.
- Core assumption: Robust multimodal reasoning requires conditional prediction updates when visual evidence changes.
- Evidence anchors:
  - [abstract]: "failed to adjust predictions when visual input was changed"
  - [section]: "Despite minimal textual change, performance declined markedly across most models. GPT-5 accuracy dropped from 83.3% to 51.7% (–31.6 pp), Gemini-2.5 Pro from 80.8% to 47.5% (–33.3 pp)"
  - [corpus]: Adversarial attack frameworks (CARES, Practical Framework for Medical AI Security) document vulnerability but don't isolate visual grounding specifically.
- Break condition: If models correctly switch to the new ground truth after substitution, grounding mechanism is intact.

## Foundational Learning

- Concept: **Shortcut learning in medical AI**
  - Why needed here: The paper's central finding is that high benchmark scores conceal brittle dependencies on superficial cues; understanding shortcut mechanisms is prerequisite to interpreting stress test results.
  - Quick check question: If a model achieves 85% accuracy on a dermatology benchmark but drops to 40% when answer options are shuffled, what does this suggest about its learning strategy?

- Concept: **Multimodal grounding vs. modality neglect**
  - Why needed here: Stress tests T1, T2, and T5 diagnose whether models genuinely integrate image and text or ignore one modality; this distinction determines clinical reliability.
  - Quick check question: A model answers correctly with image+text, and also answers correctly with text-only on "visual-required" questions. Is this model robustly grounded?

- Concept: **Reasoning fidelity (post-hoc rationalization)**
  - Why needed here: T6 shows models fabricate convincing justifications; evaluating explanation quality requires distinguishing valid inference from fluent confabulation.
  - Quick check question: A model outputs: "The image shows a heliotrope rash (purple eyelid discoloration), therefore dermatomyositis"—but no image was provided. What failure mode does this illustrate?

## Architecture Onboarding

- Component map:
  - Stress test suite (T1–T6) -> Robustness score R(m) -> Benchmark profiler -> Failure taxonomy

- Critical path:
  1. Run baseline accuracy on target benchmark (e.g., NEJM, JAMA)
  2. Apply each stress test condition; record accuracy deltas and response types (correct/incorrect/refused)
  3. Compute robustness score; profile benchmark complexity
  4. Map failures to taxonomy; identify dominant failure modes per model

- Design tradeoffs:
  - **Abstention scoring**: Counting abstentions as incorrect penalizes conservative models (GPT-4o scored 3.4% vs. 37% for others on NEJM-VS text-only) but reflects real-world cost of non-response
  - **Rubric granularity**: 3-point scale improves annotator agreement vs. 5-point, but reduces sensitivity
  - **Visual substitution validity**: Replaced images manually verified but cannot exclude pretraining data overlap

- Failure signatures:
  - **Shortcut reliance**: Text-only accuracy >> chance on visual-required subset (T2)
  - **Format sensitivity**: Performance drops when options reordered without images (T3)
  - **Hallucinated perception**: Model describes image features that don't exist (T6 manual audit pattern)
  - **Grounding failure**: Minimal accuracy change after image substitution (T5)—GPT-4o was only model showing +5pp, suggesting it may ignore images more

- First 3 experiments:
  1. **Replicate T2 on your domain**: Curate 50–100 questions labeled by clinicians as "visual-required"; run text-only vs. image+text comparison. If text-only accuracy >30%, shortcut behavior is likely present.
  2. **Implement T5 visual substitution pipeline**: For 20–40 diagnostic questions, replace images with clinically-validated alternatives matching different answer options. Measure prediction switching rate.
  3. **Benchmark profiling audit**: Have 2–3 domain experts rate your evaluation set on the 10-axis rubric; compute κ and visualize reasoning/visual complexity distribution to ensure test coverage aligns with deployment requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can evaluation benchmarks be redesigned to differentiate between superficial pattern matching and genuine clinical reasoning?
- **Basis in paper:** [explicit] The authors explicitly ask: "If models succeed on benchmarks but fail under stress, what do these benchmarks actually measure-and how can we improve them?"
- **Why unresolved:** Current benchmarks aggregate scores that conflate correct guesses via shortcuts (e.g., text-only heuristics) with actual multimodal understanding.
- **What evidence would resolve it:** The creation of a benchmark suite where leaderboard performance correlates strongly with robustness across the paper’s proposed stress tests (T1–T5).

### Open Question 2
- **Question:** Do the failure modes identified in diagnostic VQA, such as modality neglect and visual hallucination, generalize to generative tasks like radiology report generation?
- **Basis in paper:** [inferred] The methodology focuses on multiple-choice benchmarks, but the discussion notes that general VQA improvements do not translate to report generation performance, leaving the robustness of generative tasks uncertain.
- **Why unresolved:** The paper identifies "fluent factual errors" in reasoning traces, but it is unclear if these manifest similarly in open-ended text generation where no ground-truth options exist.
- **What evidence would resolve it:** Applying the visual substitution and modality removal stress tests to generative models (e.g., on MIMIC-CXR) to measure semantic drift and factual consistency.

### Open Question 3
- **Question:** How can reinforcement learning algorithms be adapted to optimize for faithful reasoning rather than token-level predictive accuracy?
- **Basis in paper:** [inferred] The discussion posits that current RL approaches may optimize for "token-level reward signals rather than faithful reasoning," resulting in convincing but fabricated rationales.
- **Why unresolved:** The study shows that Chain-of-Thought prompting fails to ensure factual grounding, suggesting current training incentives are misaligned with medical validity.
- **What evidence would resolve it:** A training regime that utilizes the proposed "reasoning signal fidelity" metric as a reward signal, successfully reducing the rate of hallucinated visual findings.

## Limitations
- Dataset construction details are partially undisclosed (NEJM-VS subset definition, specific visual substitution images)
- Access to latest frontier models (GPT-5, o3) may be restricted or require different evaluation protocols
- Visual substitution perturbation depends on finding clinically valid distractor images, introducing search-dependent variability
- Performance gaps may partially reflect model-specific safety filters rather than pure capability differences

## Confidence
- **High confidence**: Shortcut learning mechanism and benchmark heterogeneity findings are well-supported by multiple stress tests and inter-annotator agreement
- **Medium confidence**: Generalizability to all medical domains (focused on dermatology/radiology cases)
- **Medium confidence**: That robustness scores meaningfully predict real-world clinical reliability (ecological validity remains to be proven)

## Next Checks
1. Replicate T2 (visual-necessity subset) on 50-100 questions from a different medical specialty (e.g., pathology) to test domain transfer
2. Conduct ablation study: run T5 visual substitution with random vs. clinically-matched distractors to quantify impact of perturbation quality
3. Cross-validate robustness scores by correlating them with known model architectural differences (e.g., ViT vs. ConvNeXt backbones) to ensure interpretability