---
ver: rpa2
title: 'LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text
  Clustering'
arxiv_id: '2511.15424'
source_url: https://arxiv.org/abs/2511.15424
tags:
- clustering
- text
- memory
- framework
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of using large language models
  (LLMs) for text clustering by introducing LLM-MemCluster, a framework that overcomes
  two key limitations: LLMs'' lack of stateful memory for iterative refinement and
  the difficulty of managing cluster granularity. The authors propose a Dynamic Memory
  mechanism to maintain evolving cluster labels and a Dual-Prompt Strategy to control
  clustering granularity.'
---

# LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering

## Quick Facts
- arXiv ID: 2511.15424
- Source URL: https://arxiv.org/abs/2511.15424
- Reference count: 18
- Primary result: Achieves 54.5% average Adjusted Rand Index (ARI) across six benchmark datasets

## Executive Summary
This paper introduces LLM-MemCluster, a framework that enables large language models to perform text clustering through a single-pass sequential process. The key innovation addresses two fundamental LLM limitations: their lack of stateful memory for iterative refinement and difficulty managing cluster granularity. LLM-MemCluster introduces a Dynamic Memory mechanism that maintains evolving cluster labels and a Dual-Prompt Strategy that controls clustering granularity through strict and relaxed prompting modes. The framework achieves significant performance improvements over existing methods, with 54.5% average ARI compared to 33.7% for the best baseline.

## Method Summary
LLM-MemCluster processes text instances sequentially, using a Dynamic Memory module (M_mem) to maintain evolving cluster labels. At each step, the LLM receives current labels in-context, assigns the new document to an existing label or creates a new one, and optionally proposes merges. The Dual-Prompt Strategy switches between strict mode (restricting new label creation when memory size reaches K_max) and relaxed mode (allowing exploration). When merges are suggested, the framework retroactively updates all prior assignments. This single-pass approach requires no model fine-tuning and achieves end-to-end clustering through prompt engineering alone.

## Key Results
- Achieves 54.5% average ARI across six benchmark datasets, significantly outperforming the best baseline at 33.7%
- Robust to hyperparameter variations, maintaining performance when K_max is offset by ±200 from ground truth
- Generalizes well across different LLM models, validating the framework's practical applicability
- Ablation studies confirm critical importance of Dynamic Memory (ARI drops from 54.5% to 7.4% when removed) and few-shot examples (ARI drops from 54.5% to 38.5% when removed)

## Why This Works (Mechanism)

### Mechanism 1
External dynamic memory compensates for LLM statelessness, enabling coherent cluster assignments across sequential document processing. The memory module M_mem maintains an evolving set of descriptive cluster labels, and the LLM receives current labels in-context for each document assignment. This creates a stateful trajectory across N documents.

### Mechanism 2
Dual-prompt mode-switching stabilizes cluster count by dynamically adjusting label creation constraints. Two prompt templates modulate LLM behavior: relaxed mode encourages discovery while strict mode aggressively restricts new label creation and forces broad interpretation of existing labels. The switch triggers when |M_mem| ≥ K_max.

### Mechanism 3
Retroactive label merging preserves global consistency when semantic overlaps are detected mid-process. When the LLM proposes a merge, the framework updates M_mem and re-maps all prior assignments containing deprecated labels to the new consolidated label, allowing real-time optimization of the label space.

## Foundational Learning

- **Concept: In-context learning with structured prompts** - The framework relies entirely on prompting without fine-tuning. Understanding how to structure system prompts, user constraints, and output formats is essential.
- **Concept: Unsupervised clustering evaluation metrics (ARI, NMI, ACC)** - Performance claims rest on these metrics. ARI is chance-adjusted, NMI measures mutual information, and ACC requires label matching.
- **Concept: Single-pass vs. iterative algorithms** - LLM-MemCluster processes documents once (O(N)), unlike K-Means which iterates until convergence.

## Architecture Onboarding

- **Component map:** Input corpus D → Memory module M_mem (evolving labels) → Prompt constructor (injects M_mem, mode) → LLM call → Update logic (reuse/create → merge → retroactive update) → Final partition C
- **Critical path:** Initialize empty memory and assignments → For each document: select mode, construct prompt, call LLM → Update memory and assignments → If merge suggested, retroactively update all prior assignments → After N steps, group assignments by label to produce clusters
- **Design tradeoffs:** Single-pass efficiency vs. no opportunity to revisit early assignments (except via merges); prompt-based control vs. implicit LLM behavior variability; label interpretability vs. potential inconsistency in naming
- **Failure signatures:** Cluster count diverges (too many or too few) → check K_max threshold appropriateness; inconsistent label semantics → merge suggestions not triggering; context overflow on large label sets → no mitigation described
- **First 3 experiments:** 1) Replicate on Massive-D (K=18) with default settings; verify ARI ~53.8. 2) Ablate memory: run with M_mem disabled; confirm catastrophic drop per Table 4. 3) Stress-test K_max sensitivity: try offsets -10, +50, +200 on FewNerd; confirm ARI stability within ~49-53 range per Table 6.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent is the clustering performance sensitive to the sequential order of input documents? The framework operates in a single-pass sequential manner where the assignment of instance x_i depends on the memory state M_{i-1} derived from previous instances, yet the paper does not report variance scores across different input shufflings.

- **Open Question 2:** How does the framework perform on high-cardinality datasets where the ground-truth cluster count approaches the LLM's context window limit? The Dynamic Memory mechanism injects the growing list of "Known labels" into the prompt, and the experiments limit K to a maximum of 102.

- **Open Question 3:** Can the framework be adapted for real-time or interactive clustering where documents arrive in a continuous stream rather than a static corpus? The Dual-Prompt Strategy relies on a target range [K_min, K_max] that requires prior knowledge of the dataset's scale.

## Limitations
- Performance claims rely heavily on benchmark datasets with relatively clean, domain-specific text, limiting generalizability
- No analysis of context window limitations when cluster label sets grow large during processing
- Merge suggestion accuracy and potential for error propagation through retroactive updates remains unassessed

## Confidence
- **High Confidence**: Dynamic Memory mechanism's effectiveness, Dual-Prompt Strategy's granularity control, and overall superior performance vs baselines
- **Medium Confidence**: Single-pass efficiency claims and generalization across different LLM models
- **Low Confidence**: Claims about handling datasets with many more clusters than K_max

## Next Checks
1. Reconstruct and test the exact prompt templates using provided figures and appendix, measuring sensitivity to minor variations in phrasing
2. Systematically evaluate performance as |M_mem| grows, documenting the point at which context window constraints begin affecting results
3. Measure the accuracy of LLM-generated merge suggestions and quantify the impact of incorrect merges on final clustering quality