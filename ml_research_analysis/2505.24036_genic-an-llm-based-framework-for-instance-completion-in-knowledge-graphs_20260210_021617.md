---
ver: rpa2
title: 'GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs'
arxiv_id: '2505.24036'
source_url: https://arxiv.org/abs/2505.24036
tags:
- knowledge
- prediction
- completion
- entity
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of instance completion in knowledge
  graphs, which involves predicting the relation-tail pair for a given head entity.
  The authors propose GenIC, a two-step LLM-based framework that first predicts relevant
  properties as a multi-label classification task and then performs link prediction
  as a sequence-to-sequence task.
---

# GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs

## Quick Facts
- arXiv ID: 2505.24036
- Source URL: https://arxiv.org/abs/2505.24036
- Reference count: 0
- One-line primary result: GenIC achieves 17.7% increase in Hits@1 on CoDEx by leveraging entity descriptions and types in a two-step LLM pipeline

## Executive Summary
This paper addresses instance completion in knowledge graphs—predicting the missing relation-tail pair (r, t) given only head entity h. The authors propose GenIC, a two-step LLM-based framework that first predicts relevant properties as multi-label classification, then performs link prediction as sequence-to-sequence generation. By leveraging entity descriptions and types, the approach transforms the task into a natural language problem. Experiments on three datasets show GenIC outperforms baselines, achieving significant improvements in property prediction F1 scores (7-11% gain) and link prediction accuracy (e.g., 17.7% increase in Hits@1 on CoDEx).

## Method Summary
GenIC employs a two-step pipeline for instance completion. Step 1 uses Mistral-7B with LoRA fine-tuning (0.19% trainable parameters) to predict relevant relations as a multi-label classification task, taking head entity, type, and description as input. Step 2 employs T5-large with LoRA fine-tuning (0.64% trainable parameters) to generate tail entities for each predicted (head, relation) pair via beam search decoding. The approach leverages textual descriptions and entity types to provide semantic context for LLM reasoning, reducing the complex combinatorial search space of KG completion into tractable natural language tasks.

## Key Results
- GenIC achieves 17.7% increase in Hits@1 on CoDEx compared to Recoin baseline
- Property prediction F1 scores improve by 7-11% across datasets versus Recoin
- The framework demonstrates strong precision in property prediction (0.925 on CoDEx) while maintaining competitive link prediction performance

## Why This Works (Mechanism)

### Mechanism 1: Textual Context Integration for Relation Inference
Entity descriptions and types provide semantic signals that improve property prediction accuracy by enabling LLMs to extract explicit facts and infer implicit relations. The model encodes head entity, type, and description as a unified token sequence. Pre-trained LLM knowledge allows it to recognize patterns (e.g., "born in Paris" → `birthplace` relation) even when schema information is sparse. If entity descriptions are absent or uninformative, performance degrades to semantic similarity baseline; if LLM pre-training lacks domain knowledge, inference quality drops.

### Mechanism 2: Type-Constrained Search Space Reduction
Entity types act as schema-level filters that narrow the candidate relation space, reducing false positives in property prediction. By conditioning on entity type (e.g., "human," "artist"), the model focuses on class-typical relations. If entity types are missing, overly generic, or incorrectly assigned, the filtering benefit is lost; if KG schema allows broad cross-type relations, type constraints may over-prune valid candidates.

### Mechanism 3: Two-Step Pipeline Decomposition for Computational Tractability
Decomposing instance completion (h, ?, ?) into property prediction followed by link prediction reduces combinatorial search complexity while maintaining end-to-end coherence. Step 1 predicts a binary vector over |R| relations (multi-label classification). Step 2 uses each predicted (h, r) pair as input to a seq2seq model generating tail entities. If property prediction precision is low, downstream link prediction receives noisy (h, r) pairs, degrading final Hits@k; pipeline architecture assumes independent optimization of steps—joint training could expose different dynamics.

## Foundational Learning

- **Concept: Multi-label Classification**
  - **Why needed here:** Property prediction requires assigning multiple valid relations per entity (e.g., a person has `birthplace`, `nationality`, `occupation`). Standard single-label classification is insufficient.
  - **Quick check question:** Can you explain why binary cross-entropy (not softmax cross-entropy) is appropriate for multi-label outputs?

- **Concept: Sequence-to-Sequence (Seq2Seq) Generation with Beam Search**
  - **Why needed here:** Link prediction generates tail entity tokens autoregressively. Beam search produces top-k candidates efficiently without full entity ranking.
  - **Quick check question:** How does beam search differ from greedy decoding, and why does it matter for Hits@k evaluation?

- **Concept: Knowledge Graph Schema and RDF Triples**
  - **Why needed here:** Understanding (h, r, t) structure, entity types, and relation semantics is essential for designing input prompts and interpreting model outputs.
  - **Quick check question:** Given a triple `(Berlin, capital_of, Germany)`, identify head, relation, and tail; what additional context might help predict this from `Berlin` alone?

## Architecture Onboarding

- **Component map:** Entity descriptions/types → Tokenizer → Mistral-7B (LoRA) → Predicted relations → T5-large (LoRA) → Beam search → Tail entities

- **Critical path:**
  1. Load and preprocess KG dataset (ensure stratified splits by head type)
  2. Fine-tune Mistral on property prediction (binary cross-entropy loss)
  3. Generate (h, r) pairs from step 1 predictions
  4. Fine-tune T5 on link prediction using these pairs (standard cross-entropy)
  5. Evaluate: property prediction (F1, Precision, Recall); link prediction (Hits@1/5/10); instance completion (combined metrics)

- **Design tradeoffs:**
  - Separate vs. joint training: Paper trains steps independently; joint training could improve consistency but increases complexity
  - LoRA efficiency: Reduces trainable parameters (~0.2–0.6%) at cost of potential underfitting on complex schemas
  - Precision vs. recall in step 1: High precision (fewer false (h, r) pairs) benefits downstream; low recall misses valid relations entirely

- **Failure signatures:**
  - Low property prediction F1: Check if descriptions/types are missing or generic; verify stratified splits preserve type distribution
  - Link prediction Hits@1 low but Hits@10 high: Beam size may be too small; tail entity vocabulary may be under-represented in training
  - Large performance gap between datasets: WN18RR's relational structure may not align with textual priors; CoDEx benefits from LLM's encyclopedic pre-training

- **First 3 experiments:**
  1. Baseline replication: Implement Recoin + RotatE pipeline on FB15k-237; verify reported Hits@1 (~0.14) before comparing to GenIC
  2. Ablation sanity check: Run GenIC on CoDEx with descriptions ablated; expect ~0.426 Hits@1 (vs. 0.434 with full context) per Table 5
  3. Cross-dataset generalization: Train GenIC on FB15k-237, evaluate zero-shot on WN18RR; assess whether textual priors transfer across schemas

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models be utilized as an external verification mechanism to validate the accuracy of predicted facts?
- Basis: [explicit] The authors state in the Limitations section that "evaluating entirely new facts... would necessitate external validation" and propose this as future work.
- Why unresolved: Current evaluation methods assume the benchmark datasets are complete, causing the model to misclassify valid new predictions as false positives.
- Evidence: Development of a verification module that successfully filters hallucinated triples while retaining novel, valid facts better than the current closed-world assumption.

### Open Question 2
- Question: Does a joint approach integrating language models with structural graph features outperform the purely textual GenIC framework?
- Basis: [explicit] The authors note that relying uniquely on language models may overlook "rich structural and relational information" and suggest exploring a joint approach.
- Why unresolved: GenIC currently relies exclusively on textual descriptions and types, potentially missing topological patterns inherent in the knowledge graph structure.
- Evidence: A comparative study showing that a hybrid model combining GenIC with graph embedding techniques improves performance on datasets with sparse textual descriptions.

### Open Question 3
- Question: To what degree does error propagation between the property prediction and link prediction steps limit the pipeline's final accuracy?
- Basis: [inferred] The method separates relation prediction (Step 1) and tail prediction (Step 2); however, errors in Step 1 inevitably constrain the input quality for Step 2.
- Why unresolved: The paper evaluates the steps individually and combined but does not isolate the specific performance loss caused by feeding incorrect relations into the link predictor.
- Evidence: An ablation study measuring final Hits@k when using gold-standard relations versus predicted relations as input for the link prediction component.

## Limitations

- The evaluation framework does not address data leakage risks between the two-step pipeline, particularly if the same (head, relation) pairs appear in both training phases.
- Performance gains on CoDEx may reflect dataset-specific characteristics rather than general LLM capabilities for KG completion.
- Missing descriptions or types significantly degrade performance, suggesting limited robustness to real-world KG sparsity.

## Confidence

- **High confidence**: The two-step pipeline architecture and LoRA efficiency claims are well-supported by the reported parameter counts and computational complexity analysis.
- **Medium confidence**: The mechanism linking textual descriptions to relation prediction accuracy is plausible but relies on untested assumptions about description informativeness and LLM pre-training coverage.
- **Low confidence**: Claims about type-constrained search space reduction lack direct corpus validation and depend heavily on dataset-specific type distributions.

## Next Checks

1. **Data split audit**: Verify that (head, relation) pairs in the property prediction training set do not appear in the link prediction test set to prevent leakage.
2. **Ablation on missing data**: Systematically remove entity descriptions and types to quantify performance degradation and test the claimed 18.6-point F1 drop.
3. **Cross-dataset generalization**: Train GenIC on one dataset (e.g., FB15k-237) and evaluate zero-shot on another (e.g., WN18RR) to assess whether textual priors transfer across schemas.