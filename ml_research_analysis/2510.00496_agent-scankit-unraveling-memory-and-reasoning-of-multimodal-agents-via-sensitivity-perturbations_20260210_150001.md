---
ver: rpa2
title: 'Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity
  Perturbations'
arxiv_id: '2510.00496'
source_url: https://arxiv.org/abs/2510.00496
tags:
- agents
- arxiv
- reasoning
- multimodal
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents Agent-ScanKit, a systematic probing framework\
  \ to analyze the memory and reasoning capabilities of multimodal agents in GUI tasks.\
  \ The core method introduces three orthogonal probing paradigms\u2014visual-guided,\
  \ text-guided, and structure-guided\u2014that quantify memorization versus reasoning\
  \ through controlled perturbations without requiring model internals."
---

# Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations

## Quick Facts
- arXiv ID: 2510.00496
- Source URL: https://arxiv.org/abs/2510.00496
- Reference count: 32
- Key outcome: Agent-ScanKit systematically probes multimodal GUI agents using three perturbation paradigms, revealing heavy reliance on memorization over reasoning across 18 agents tested on five benchmarks.

## Executive Summary
Agent-ScanKit introduces a systematic probing framework to analyze whether multimodal GUI agents rely on memorization or reasoning. Through three orthogonal perturbation paradigms—visual-guided, text-guided, and structure-guided—the method quantifies memory versus reasoning contributions without requiring model internals. Experiments across 18 agents and five benchmarks demonstrate that existing agents function primarily as retrievers of training-aligned knowledge rather than systematic reasoners, with substantial room for improvement in generalization and robustness.

## Method Summary
Agent-ScanKit measures perturbation sensitivity ΔP to distinguish memorization from reasoning in multimodal GUI agents. The framework applies three perturbation types: visual-guided (object masking/editing/zoom-in), text-guided (token/sentence substitution), and structure-guided (modality ablation). Agents are evaluated on five benchmarks using standardized action spaces. The method computes metrics including type accuracy, grounding accuracy, step-wise success rate, task success rate, visual memory consistency (VMC), reflection score (RS), and ΔP for each perturbation class. Low ΔP indicates memory-driven behavior while high ΔP suggests reasoning-driven adaptation.

## Key Results
- Existing multimodal agents show heavy reliance on memorization, functioning as retrievers of training-aligned knowledge rather than systematic reasoners
- Visual-guided probing reveals spatial memory dominance, with agents maintaining coordinate predictions under visual corruption
- Structure-guided probing isolates modality shortcuts, showing certain actions (WAIT, COMPLETE) rely on instruction-only or visual-only decisions
- While RL and chain-of-thought strategies improve performance, substantial room remains for building more reliable multimodal agents

## Why This Works (Mechanism)

### Mechanism 1: Perturbation Sensitivity as Memory-Reasoning Discriminator
Controlled input perturbations reveal whether agent behavior stems from memorized input-output mappings versus generalizable reasoning. The perturbation operator P:S→S modifies observed state s_t, and ΔP = E[Acc(π(s_t,g)) - Acc(π(P(s_t),g))] quantifies performance gaps. Low ΔP indicates memory-driven behavior (output remains stable despite corrupted input), while high ΔP suggests reasoning-driven behavior (output correctly adapts to input changes).

### Mechanism 2: Visual-Guided Probing Exposes Spatial Memory Bias
Object masking/editing reveals reliance on positional priors, while zoom-in tests contextual reasoning when global layout is removed. Masking/editing removes target element visual evidence—if agent still predicts original coordinates, it uses memorized spatial priors. Zoom-in crops to quadrant containing target, removing global context—if agent localizes successfully, it demonstrates contextual reasoning over local features rather than position-memory retrieval.

### Mechanism 3: Structure-Guided Probing Isolates Action and Visual Shortcuts
Reflection and state actions often rely on shortcuts—either visual-only (ignoring history) or action-only (ignoring screen content). Visual shortcuts tested by corrupting action instructions while preserving screen—if agent succeeds, it uses visual-only decision. Action shortcuts tested by corrupting visual input while preserving instructions—if agent succeeds, it uses instruction-only decision. High success under corruption reveals shortcut reliance.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP) for GUI Tasks**
  - Why needed here: The paper formalizes GUI tasks as POMDP tuples M=(G,S,A,T,R,H), where the agent must predict actions a_t from perturbed observations s'_t. Understanding this formulation is prerequisite to interpreting how perturbations fit into the agent's decision loop.
  - Quick check question: Given a perturbation operator P:S→S, does the agent select actions from π(a_t|s'_t,g) or π(a_t|s_t,g)?

- **Concept: Sensitivity Analysis via Input Perturbation**
  - Why needed here: The core methodology measures ΔP as the performance gap between perturbed and unperturbed conditions. Interpreting low ΔP as memory and high ΔP as reasoning requires understanding how sensitivity relates to generalization.
  - Quick check question: If ΔP is near zero for object-masking but high for zoom-in, what does this imply about the agent's spatial memory versus reasoning?

- **Concept: Visual Memory Consistency (VMC) and Reflection Score (RS)**
  - Why needed here: VMC measures whether predicted coordinates remain within threshold γ of original predictions under perturbation (indicating memory). RS captures whether agents trigger reflective actions when encountering visual anomalies. These metrics operationalize memory detection.
  - Quick check question: An agent with VMC=98.7% and RS=0.14 under editing exhibits what behavioral pattern?

## Architecture Onboarding

- **Component map:** Perturbation Engine -> Evaluation Harness -> Metric Suite -> Analysis Layer
- **Critical path:**
  1. Load preprocessed benchmark samples with ground-truth action annotations
  2. For each sample, generate perturbed variants (masked screen, edited text, ablated modalities)
  3. Run agent inference on original and perturbed inputs under identical prompts
  4. Compute ΔP_Type and ΔP_SR per perturbation class
  5. Derive VMC (coordinate stability) and RS (reflective action triggering) for visual probing
  6. Classify each action type as memory-dominated or reasoning-dominated based on shortcut scores
- **Design tradeoffs:**
  - Perturbation intensity: 50-pixel masking/editing balances detectability with realism; stronger perturbations may induce out-of-distribution failure unrelated to memory
  - Low-level vs high-level evaluation: Low-level (atomic instructions) provides stronger textual guidance, revealing text-guided memory; high-level stresses reasoning but lowers baseline performance
  - Threshold selection: γ=50 pixels for VMC assumes approximate coordinate matching indicates memory; tighter thresholds may misclassify genuine reasoning as memory
- **Failure signatures:**
  - High VMC + Low RS under masking/editing: Agent ignores visual anomalies, outputs memorized coordinates → spatial memory dominance
  - High action-shortcut scores on WAIT/COMPLETE: Agent defaults to instruction-following without environmental grounding → action shortcut
  - Sharp SR degradation under zoom-in with preserved local features: Agent lacks contextual reasoning beyond global layout cues
- **First 3 experiments:**
  1. **Baseline Probe on OS-Genesis-7B**: Apply object masking to 100 grounded samples; compute ΔP_SR and VMC. Expected: ΔP_SR>80%, VMC>80% (confirming spatial memory dominance per Table 1)
  2. **Text-Guided Token Ablation on UI-TARS-7B-SFT**: Remove action-start words from low-level instructions for TYPE/OPENAPP actions; measure ΔP_SR. Expected: Moderate increase per Table 2 (partial text-guided reasoning)
  3. **Structure Shortcuts on AgentCPM-GUI-8B**: Ablate visual modality for COMPLETE action; measure action-shortcut score. Expected: High shortcut (>90%) per Table 3, indicating instruction-only decision

## Open Questions the Paper Calls Out
None

## Limitations
- Perturbation interpretability ambiguity: The ΔP metric assumes that performance stability under perturbation indicates memorization, but this conflates robustness to input noise with genuine memory
- Benchmark diversity and ecological validity: Experiments focus on synthetic or curated GUI benchmarks with controlled perturbations, which may not generalize to real-world deployment
- Action type generalization: The study classifies certain actions as shortcut-dominated based on ablation tests, but some actions may legitimately require only one modality, leading to false positives in memory attribution

## Confidence
- Multimodal GUI agents rely heavily on memorization rather than systematic reasoning: Medium
- Visual-guided probing effectively distinguishes spatial memory from contextual reasoning: High
- Structure-guided probing isolates modality shortcuts: Medium

## Next Checks
1. **Perturbation robustness validation**: Test agents on naturally noisy inputs (e.g., OCR errors, partial occlusion) to verify whether low ΔP reflects memorization or general robustness
2. **Cross-benchmark generalization**: Apply the same perturbation framework to a real-world GUI benchmark (e.g., Mind2Web) to assess whether memorization patterns hold outside curated datasets
3. **Semantic action analysis**: Conduct ablation studies on actions like WAIT/COMPLETE to determine whether instruction-only success reflects legitimate semantics or memorization shortcuts