---
ver: rpa2
title: 'Generative Artificial Intelligence, Musical Heritage and the Construction
  of Peace Narratives: A Case Study in Mali'
arxiv_id: '2601.14931'
source_url: https://arxiv.org/abs/2601.14931
tags:
- mali
- peace
- musical
- cultural
- national
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that generative AI, when integrated into
  a culturally informed participatory framework, can serve as a tool for constructing
  peace narratives and revitalizing musical heritage in Mali. Through a co-creation
  workshop with 30 participants from diverse backgrounds, the research produced 12
  AI-assisted compositions that blend traditional Malian instruments (kora, balafon,
  djembe) with contemporary arrangements in national languages.
---

# Generative Artificial Intelligence, Musical Heritage and the Construction of Peace Narratives: A Case Study in Mali

## Quick Facts
- arXiv ID: 2601.14931
- Source URL: https://arxiv.org/abs/2601.14931
- Reference count: 14
- This study demonstrates that generative AI, when integrated into a culturally informed participatory framework, can serve as a tool for constructing peace narratives and revitalizing musical heritage in Mali.

## Executive Summary
This research explores the use of generative AI to create culturally-authentic Malian music that supports peace narratives through a participatory workshop with 30 stakeholders. The workshop produced 12 AI-assisted compositions blending traditional instruments (kora, balafon, djembe) with contemporary arrangements in national languages. Results show high participant satisfaction (4.15/5) and perceived authenticity (85%), with 80% feeling capable of independent creation afterward. The resulting hybrid musical architectures feature multilingual lyrics, reconciliation lexicon, and symbolic instrumentation. Institutional recognition at SENARE 2025 confirms social relevance. Challenges include linguistic corpus limitations, algorithmic censorship, and authenticity-innovation tensions, highlighting the need for ethical frameworks supporting cultural sovereignty in AI applications.

## Method Summary
The study employed a 3-day participatory workshop using Suno AI for music generation and ChatGPT/GEMINI for lyric generation/translation. Thirty participants from diverse backgrounds (ministerial, youth, women's orgs, music students) created 12 compositions through iterative prompt engineering using a 5-element framework: [Genre/Style] + [Instrumentation] + [Tempo/Mood] + [Cultural References] + [Thematic Content]. Each composition underwent 8-12 generation cycles with quality filtering based on instrumentation match, tempo, vocal quality, and stylistic coherence. Thematic analysis of outputs used Braun & Clarke framework with Cohen's kappa 0.82 inter-rater reliability. Post-workshop survey (n=13) measured participant satisfaction and perceived cultural authenticity.

## Key Results
- Participant satisfaction averaged 4.15/5 with 85% perceiving authentic cultural representation and 80% feeling capable of independent creation
- 12 AI-assisted compositions successfully blended traditional Malian instruments with contemporary arrangements across 5 national languages
- Hybrid musical architectures included Afrobeat-Mandingo patterns, Reggae-sabar, and meditative Tuareg models featuring reconciliation lexicon and symbolic instrumentation

## Why This Works (Mechanism)

### Mechanism 1
Structured prompt engineering with cultural specificity improves perceived authenticity of AI-generated music. Participants using the framework [Genre/Style] + [Instrumentation] + [Tempo/Mood] + [Cultural References] + [Thematic Content] progressed from generic outputs to culturally grounded compositions through 8-12 iteration cycles per piece. Core assumption: Users can articulate cultural knowledge in structured natural language after brief training. Evidence: Successful Prompt Examples shows explicit prompt structure and iteration statistics; growing sophistication in prompts documented over workshop duration. Break condition: If target culture's instruments lack representation in model training data, no prompt refinement compensates.

### Mechanism 2
Participatory co-creation with diverse stakeholders may foster intergroup dialogue independent of AI's specific contribution. Stratified sampling in a 3-day structured workshop created cross-sector interaction around shared creative goals. Core assumption: The workshop format itself—not AI outputs—drives cohesion effects. Evidence: Authors explicitly acknowledge alternative explanations attributing positive outcomes to factors beyond AI tools themselves. Break condition: If participants self-select based on prior AI enthusiasm, cohesion effects may not replicate with skeptical populations.

### Mechanism 3
Multilingual lyrical strategies with reconciliation lexicon create perceived cultural legitimacy. Compositions systematically used 5 national languages with recurring peace vocabulary and unifying geography, enabling diverse linguistic groups to hear their identity represented. Core assumption: Audiences interpret multilingual code-switching as inclusive rather than fragmented. Evidence: Details four discursive strategies with specific lexical examples; 85% perceived authentic cultural representation. Break condition: If AI mispronounces tonal languages without phonetic guidance, legitimacy degrades.

## Foundational Learning

- **Prompt engineering as cultural encoding**: Why needed: Generic prompts yield stereotypical outputs; cultural specificity requires translating tacit musical knowledge into explicit structured instructions. Quick check: Can you write a prompt specifying both an instrument (e.g., kora) and its traditional musical role (melodic carrier vs. rhythmic support)?

- **Linguistic corpus gaps and tonal languages**: Why needed: Bambara, Fulfulde, Tamasheq are underrepresented in AI training data; tonal mispronunciations break authenticity. Quick check: What phonetic annotation strategy would you use to guide AI pronunciation of a word with high-low tone contrast?

- **Participatory design vs. novelty effects**: Why needed: Positive outcomes may stem from collaborative workshop format rather than AI itself; disentangling mechanisms requires control thinking. Quick check: How would you design a minimal condition to isolate AI's contribution from group collaboration effects?

## Architecture Onboarding

- **Component map**: Suno AI (music) -> ChatGPT/GEMINI (lyrics, translation) -> 5-element prompt framework -> Iterative generation cycles -> Quality filters -> Cultural validation

- **Critical path**: 1. Define cultural reference (instrument + scale + region) -> 2. Draft prompt using framework -> 3. Generate, evaluate against rejection criteria -> 4. Refine with phonetic/structural adjustments -> 5. Validate with community representatives

- **Design tradeoffs**: Specificity vs. generation diversity (over-constrained prompts produce faithful but generic outputs); Linguistic authenticity vs. model capability (phonetic annotations improve pronunciation but increase complexity); Tradition vs. innovation (hybrid forms may alienate purists while engaging youth)

- **Failure signatures**: Generic instrument substitution (imzad → violin); Algorithmic censorship of flagged terms; Incoherent outputs from >3 tradition combinations; Tonal distortion when national languages lack corpus support

- **First 3 experiments**:
  1. Baseline authenticity test: Generate 5 compositions with generic prompts vs. culturally-structured prompts; blind-rate for perceived authenticity with target-community listeners (n≥20)
  2. Linguistic corpus mapping: Audit Suno AI and GEMINI for coverage of Bambara, Fulfulde, Tamasheq phonemes; document systematic substitution patterns
  3. Control condition pilot: Run 2 parallel workshops—AI-assisted vs. traditional co-creation—with matched participant demographics; compare dialogue quality metrics and output reception

## Open Questions the Paper Calls Out

- Does AI-assisted music creation foster social cohesion more effectively than traditional collaborative music-making? The study design lacked a control group, making it difficult to distinguish the specific impact of Gen AI from the inherent benefits of the workshop's collaborative environment. A randomized controlled trial comparing AI-assisted workshops with traditional music collaboration workshops would resolve this.

- How can Gen AI platforms overcome algorithmic censorship to accommodate cultural context in low-resource languages? Current safety filters in commercial models are trained primarily on dominant languages and contexts, lacking the nuance to distinguish between harmful content and culturally significant local vocabulary. Development and testing of culturally-adapted moderation filters would resolve this.

- Are the perceived increases in cultural pride and capacity for peacebuilding sustained long-term? While the authors explicitly call for longitudinal studies assessing sustainable impact, the current data relies on a post-workshop survey administered immediately after the intervention. It is unclear if the high participant satisfaction and reported empowerment represent a "novelty effect" or a durable shift in behavior and attitude. Follow-up interviews and behavioral tracking 6 to 12 months post-intervention would resolve this.

## Limitations
- Generalizability beyond Mali is limited by the specific cultural knowledge embedded in the 30-person workshop cohort
- Causal attribution of peace narrative effects remains unclear—whether AI tools uniquely contributed or structured co-creation alone drove cohesion
- Linguistic corpus gaps in tonal languages create a usability trade-off between phonetic annotation and prompt complexity

## Confidence
- **High confidence**: Prompt engineering improves cultural specificity (supported by iteration statistics and participant progression)
- **Medium confidence**: Multilingual strategies enhance perceived legitimacy (85% satisfaction, but lacks comparative baseline)
- **Low confidence**: AI uniquely fosters intergroup dialogue independent of workshop format (no control condition; alternative explanations acknowledged)

## Next Checks
1. Blind authenticity test: Generate 5 compositions each with generic vs. culturally-structured prompts; blind-rate for perceived authenticity with target-community listeners (n≥20) to isolate prompt effects
2. Linguistic corpus audit: Systematically test Suno AI and GEMINI coverage of Bambara, Fulfulde, Tamasheq phonemes; document substitution patterns and phonetic annotation efficacy
3. Control condition pilot: Run parallel workshops—AI-assisted vs. traditional co-creation—with matched demographics; compare dialogue quality metrics and output reception to isolate AI's contribution