---
ver: rpa2
title: Improving In-Context Learning with Reasoning Distillation
arxiv_id: '2504.10647'
source_url: https://arxiv.org/abs/2504.10647
tags:
- rule
- reasoning
- hypothesis
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving in-context learning
  (ICL) for language models on tasks requiring inductive reasoning. It proposes ReDis,
  a reasoning distillation method that uses a teacher model to generate and evaluate
  candidate hypotheses explaining input-output relationships in few-shot demonstrations.
---

# Improving In-Context Learning with Reasoning Distillation
## Quick Facts
- arXiv ID: 2504.10647
- Source URL: https://arxiv.org/abs/2504.10647
- Reference count: 40
- Primary result: ReDis achieves 23.2%, 2.8%, and 66.6% relative improvements over few-shot prompting on 1D-ARC, ACRE, and MiniSCAN tasks respectively

## Executive Summary
This paper addresses the challenge of improving in-context learning (ICL) for language models on tasks requiring inductive reasoning. The authors propose ReDis (Reasoning Distillation), a method that uses a teacher model to generate and evaluate candidate hypotheses explaining input-output relationships in few-shot demonstrations. The approach combines data augmentation, supervised fine-tuning, and preference alignment to teach the student model both rule generation and rule-following. Experiments show ReDis outperforms few-shot prompting baselines and even surpasses the teacher model GPT-4o on three out of four tasks while providing significant inference-time efficiency gains.

## Method Summary
ReDis works by first using a powerful teacher model (GPT-4o) to generate multiple hypotheses explaining the rules in few-shot demonstrations. These hypotheses are then evaluated and ranked based on their ability to correctly predict outputs. The method employs a three-stage training process: first, data augmentation through hypothesis generation; second, supervised fine-tuning where the student learns to both generate rules and follow them; and third, preference alignment to improve the quality of generated rules. This approach enables the student model to learn reasoning patterns that generalize beyond the specific examples provided in demonstrations.

## Key Results
- ReDis achieves 23.2%, 2.8%, and 66.6% relative improvements over few-shot prompting on 1D-ARC, ACRE, and MiniSCAN tasks respectively
- ReDis surpasses the teacher model GPT-4o on three out of four tasks tested
- ReDis demonstrates significant inference efficiency gains, being 87%, 53%, and 25% more token-efficient than GPT-4o on MiniSCAN, 1D-ARC, and ACRE respectively

## Why This Works (Mechanism)
The paper doesn't provide a detailed mechanism analysis. The approach works by leveraging a teacher model to generate and evaluate multiple hypotheses about the underlying rules in demonstrations, then distilling this reasoning capability into the student model through a combination of data augmentation and preference learning.

## Foundational Learning
1. In-context learning (ICL): Why needed - ICL allows models to perform tasks without parameter updates using only demonstrations; Quick check - Verify model can follow patterns from examples
2. Reasoning distillation: Why needed - Transfer reasoning capabilities from stronger to weaker models; Quick check - Compare performance before/after distillation
3. Rule induction: Why needed - Essential for tasks requiring generalization beyond specific examples; Quick check - Test on novel inputs following same rules

## Architecture Onboarding
**Component map**: Teacher model -> Hypothesis generator -> Hypothesis evaluator -> Student model -> Fine-tuning pipeline
**Critical path**: GPT-4o hypothesis generation → Hypothesis evaluation → Student model training → Inference optimization
**Design tradeoffs**: 
- Powerful teacher model provides quality hypotheses but increases computational cost
- Multi-stage training improves performance but adds complexity
- Token efficiency gains offset some computational overhead

**Failure signatures**: 
- Poor hypothesis generation leads to degraded student performance
- Overfitting to specific rule patterns reduces generalization
- Computational bottlenecks in hypothesis generation stage

**First experiments**:
1. Baseline comparison: ReDis vs few-shot prompting on 1D-ARC
2. Teacher model ablation: Test with different teacher model capabilities
3. Inference efficiency measurement: Compare token usage against GPT-4o

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on powerful teacher model (GPT-4o) may limit applicability in resource-constrained scenarios
- Effectiveness primarily demonstrated on synthetic tasks with clear logical rules
- Unclear how much reasoning distillation specifically improves generalization versus simply increasing training data diversity

## Confidence
- High confidence: The core methodology of reasoning distillation is sound and the experimental results are reproducible
- Medium confidence: The performance improvements are significant but the exact contribution of each component needs further validation
- Medium confidence: The inference efficiency gains are well-documented but may vary across different hardware configurations

## Next Checks
1. Conduct experiments on more diverse real-world reasoning tasks to assess generalization beyond synthetic datasets
2. Perform a detailed ablation study isolating the effects of rule generation versus rule-following on final performance
3. Test the approach with different teacher model capabilities to understand the dependency on model strength and evaluate cost-effectiveness trade-offs