---
ver: rpa2
title: Hierarchical Interaction Summarization and Contrastive Prompting for Explainable
  Recommendations
arxiv_id: '2507.06044'
source_url: https://arxiv.org/abs/2507.06044
tags:
- user
- item
- explanations
- explanation
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to improve explainable recommendation
  by addressing the limitations of embedding-based methods and poor-quality ground
  truth explanations. The core idea is to use hierarchical interaction summarization
  (PGHIS) to generate structured textual profiles of users and items, replacing embeddings.
---

# Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations

## Quick Facts
- **arXiv ID:** 2507.06044
- **Source URL:** https://arxiv.org/abs/2507.06044
- **Reference count:** 40
- **Primary result:** Proposes PGHIS and CPEG to replace embeddings with textual profiles and generate high-quality explanations, improving GPTScore by 5% and BLEU/ROUGE by 20.6% and 19.6% on three datasets.

## Executive Summary
This paper introduces a novel approach to explainable recommendation that addresses the limitations of embedding-based methods and poor-quality ground truth explanations. The method replaces dense embeddings with structured textual profiles of users and items, generated via hierarchical interaction summarization (PGHIS) using a pretrained LLM. It also introduces contrastive prompting for explanation generation (CPEG) to produce high-quality ground truth explanations by guiding a reasoning language model with contrastive prompts. The generated profiles and explanations are used to fine-tune a lightweight LLM via supervised fine-tuning. Experiments on Amazon-Book, Yelp, and Steam datasets show significant improvements over state-of-the-art baselines.

## Method Summary
The approach consists of a three-stage pipeline: (1) PGHIS uses GPT-4o-mini to hierarchically aggregate user-item profiles by simulating graph neural network message passing in text space, (2) CPEG employs DeepSeek-R1 with contrastive prompts and hard negative sampling to generate high-quality ground truth explanations through a retry loop, and (3) a lightweight LLM (Qwen2.5-7B-Instruct) is fine-tuned on the generated profile-explanation pairs using supervised fine-tuning with cross-entropy loss. The method aims to improve explainability by preserving semantic information in textual profiles and generating informative explanations that explicitly reference discriminative features.

## Key Results
- GPTScore improves by 5% compared to state-of-the-art baselines
- BLEU-1, BLEU-4, and ROUGE variants improve by 20.6% and 19.6% respectively
- CPEG-generated explanations demonstrate superior quality compared to user-written reviews and other methods
- The approach outperforms embedding-based methods across all three datasets (Amazon-Book, Yelp, Steam)

## Why This Works (Mechanism)

### Mechanism 1: Textual Profile Generation via Hierarchical Aggregation (PGHIS)
PGHIS replaces dense embeddings with structured textual profiles that preserve more semantic and interaction information. A pretrained LLM iteratively aggregates neighbor profiles, summarizing shared attributes and updating the target profile over multiple layers. This simulates GNN message passing in text space, extracting collaborative signals from interacted item-user pairs. The core assumption is that LLMs can effectively extract and synthesize common features across multiple text documents without gradient-based training.

### Mechanism 2: Contrastive Prompting for Ground Truth Explanation Generation (CPEG)
CPEG generates high-quality ground truth explanations by guiding a reasoning language model with contrastive prompts. The RLM receives a user profile, positive item profile, and negative item profiles, and must identify the correct item and explain the preference. Incorrect predictions are fed back as negative examples in a retry loop, pushing the model toward better reasoning. The core assumption is that RLMs can perform explicit reasoning over structured text inputs when pushed to distinguish positive from negative examples.

### Mechanism 3: Supervised Fine-Tuning on Generated Profiles and Explanations
The final model is trained via standard next-token prediction on pairs of (user profile, item profile) → explanation. The profiles encode collaborative and content signals textually, while the explanations are high-quality synthetic ground truths from CPEG. The core assumption is that these synthetic explanations are sufficiently aligned with real user preferences to serve as effective training targets.

## Foundational Learning

- **Graph Collaborative Filtering (GCF) / Message Passing**
  - Why needed: PGHIS is conceptually a text-space analog of GNN aggregation, understanding how embeddings propagate across user-item interaction graphs helps grasp why hierarchical textual summarization captures similar high-order dependencies.
  - Quick check: Can you explain, in one sentence, how a node's embedding is updated in a GNN-based recommender like LightGCN?

- **Contrastive Learning (Negative Sampling)**
  - Why needed: CPEG's effectiveness depends on selecting informative negatives (hard vs. random), understanding contrastive learning clarifies why pushing the model to distinguish similar but incorrect items improves explanation specificity.
  - Quick check: What is the difference between a "hard negative" and a "random negative" in contrastive learning, and why might hard negatives be more informative?

- **Reasoning Language Models (RLMs)**
  - Why needed: CPEG relies on RLMs (e.g., DeepSeek-R1) to perform explicit reasoning steps before generating explanations, understanding RLM behavior helps anticipate where the retry loop may fail or succeed.
  - Quick check: What distinguishes a reasoning language model from a standard instruction-tuned LLM in terms of output structure?

## Architecture Onboarding

- **Component map:** Initial profiles (RLMRec) → PGHIS aggregation (GPT-4o-mini) → CPEG explanation generation (DeepSeek-R1) → SFT fine-tuning (Qwen2.5-7B-Instruct)

- **Critical path:**
  1. Build initial user/item profiles from raw attributes (titles, descriptions, reviews)
  2. Run hierarchical aggregation (PGHIS) for L layers to enrich profiles with collaborative signals
  3. For each positive user-item pair, sample negatives, run CPEG to generate/refine explanations
  4. Construct SFT dataset from profiles (input) and CPEG explanations (output)
  5. Fine-tune final LLM and evaluate on held-out pairs

- **Design tradeoffs:**
  - Profile depth vs. compute: More PGHIS layers improve profile richness but increase LLM calls quadratically with neighborhood size
  - Hard negative count (k) vs. retry cost: Higher k improves explanation discrimination but increases RLM retries and latency (paper sets k=2, m=5 as balance)
  - Model size for SFT: Larger backbone improves fluency but raises inference cost; paper uses 7B model for efficiency

- **Failure signatures:**
  - Generic or repetitive explanations → likely insufficient neighbor diversity or overly broad aggregation prompts
  - Low GPTScore despite high BLEU/ROUGE → explanations may be fluent but not actually informative or preference-specific
  - High retry rates in CPEG → negative samples may be too similar or user-item signals too weak

- **First 3 experiments:**
  1. **Profile Ablation:** Train with initial profiles only (no PGHIS) vs. 1-layer vs. 2-layer aggregation. Measure GPTScore and BERTScore drop.
  2. **Negative Sampling Ablation:** Compare CPEG with (a) random negatives only, (b) hard negatives only, (c) mixed (k=2, m=5). Track win rate and retry count.
  3. **Ground Truth Quality Check:** Use GPT-as-judge to compare CPEG explanations against original user reviews and XRec-generated ground truths on a held-out subset. Confirm win rate >80% before full SFT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the textual profiles generated by PGHIS be integrated directly into LLM-based recommender systems to improve recommendation accuracy alongside explainability?
- **Basis in paper:** The Conclusion states: "In future work, we aim to apply our profile aggregation approach to LLM-based recommender systems, improving recommendation explainability and performance."
- **Why unresolved:** The current study only validates the profiles for generating explanations via SFT, not for enhancing the core recommendation (ranking/prediction) task.
- **What evidence would resolve it:** Experiments evaluating ranking metrics (e.g., NDCG, Hit Rate) when PGHIS profiles are used as the primary input for an LLM-based recommender model.

### Open Question 2
- **Question:** How can the trade-off between the number of hard negative samples and computational efficiency be optimized without saturating the Reasoning Language Model (RLM)?
- **Basis in paper:** Section 4.4 notes that increasing hard negatives ($k$) improves quality but leads to "diminishing improvements" and "higher computational costs" due to RLM retries.
- **Why unresolved:** The authors selected a fixed configuration ($k=2, m=5$) empirically but did not propose a method to dynamically optimize this balance.
- **What evidence would resolve it:** An adaptive sampling strategy that adjusts the number of hard negatives based on the model's confidence or reasoning complexity.

### Open Question 3
- **Question:** Is the dependency on large, proprietary Reasoning Language Models (RLMs) like DeepSeek-R1 a strict requirement for CPEG, or can smaller open-source models achieve similar ground truth quality?
- **Basis in paper:** Section 3.2 and 4.1 specify the use of DeepSeek-R1 for the complex discrimination and refinement tasks.
- **Why unresolved:** The paper does not ablate the specific contribution of the RLM's size or "reasoning" capability versus standard LLMs in the contrastive prompting loop.
- **What evidence would resolve it:** A comparative analysis of ground truth quality when replacing DeepSeek-R1 with smaller, non-reasoning models in the CPEG framework.

## Limitations

- Profile quality degrades significantly in cold-start scenarios with sparse interactions
- Heavy reliance on large proprietary LLMs (GPT-4o-mini, DeepSeek-R1) creates computational and cost barriers
- All experimental results are reported on datasets with substantial interaction history; generalizability to sparse datasets is untested
- The claim that synthetic explanations are uniformly superior to user-written reviews across all preference dimensions lacks strong empirical backing

## Confidence

- **High confidence:** The hierarchical profile generation framework (PGHIS) is well-specified and grounded in known LLM capabilities for text summarization
- **Medium confidence:** The contrastive prompting mechanism (CPEG) for generating high-quality explanations is plausible but depends critically on RLM performance and negative sampling strategy
- **Low confidence:** The claim that synthetic explanations from CPEG are uniformly superior to user-written reviews across all preference dimensions lacks strong empirical backing in the paper

## Next Checks

1. **Profile Information Gain:** Measure KL divergence between initial and PGHIS-augmented profiles to quantify collaborative signal injection
2. **Negative Sampling Sensitivity:** Systematically vary k (hard negatives) and m (random negatives) in CPEG and measure changes in explanation quality and win rate
3. **Cold-Start Robustness:** Evaluate the pipeline on a held-out subset of users/items with fewer than 5 interactions to assess performance drop