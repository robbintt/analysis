---
ver: rpa2
title: 'MMOne: Representing Multiple Modalities in One Scene'
arxiv_id: '2507.11129'
source_url: https://arxiv.org/abs/2507.11129
tags:
- modality
- modalities
- gaussians
- scene
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MMOne, a framework for representing multiple
  modalities (RGB, thermal, language) in one scene using 3D Gaussian Splatting. The
  key challenges addressed are property disparity (different characteristics across
  modalities) and granularity disparity (different levels of detail required).
---

# MMOne: Representing Multiple Modalities in One Scene

## Quick Facts
- arXiv ID: 2507.11129
- Source URL: https://arxiv.org/abs/2507.11129
- Authors: Zhifeng Gu; Bing Wang
- Reference count: 40
- Primary result: 0.5dB PSNR gain for RGB and 0.4dB for thermal using one-third the Gaussians compared to baselines

## Executive Summary
MMOne proposes a framework for representing multiple modalities (RGB, thermal, language) in a single 3D scene using Gaussian Splatting. The key innovation addresses property disparity (different physical characteristics across modalities) and granularity disparity (different levels of detail required) through modality-specific indicators and gradient-based decomposition. The method achieves consistent improvements across all modalities while significantly reducing the number of Gaussians needed compared to baseline approaches.

## Method Summary
MMOne extends 3D Gaussian Splatting by introducing modality indicators (α_m) for each modality, allowing different opacities for the same Gaussian across modalities. The framework employs a gradient-based multimodal decomposition mechanism that splits Gaussians when modality-specific gradient updates conflict, and uses soft pruning to remove modalities rather than entire Gaussians. The system processes aligned RGB, thermal, and language embeddings through a modified rasterizer that handles modality-specific blending during training.

## Key Results
- 0.5dB PSNR improvement for RGB and 0.4dB for thermal compared to baselines
- Uses one-third of the Gaussians compared to standard 3DGS approaches
- 9% improvement in mIoU for open-vocabulary segmentation tasks
- Scalable to additional modalities like monocular depth without performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Indicators for Property Disparity
- **Claim:** Decoupling opacity for each modality resolves conflicts where physical properties differ, such as thermal permeability versus visual occlusion.
- **Mechanism:** Replaces single shared opacity with learnable modality indicators (α_rgb, α_thermal) that act as per-modality switches.
- **Core assumption:** Distinct modalities require distinct visibility functions to accurately model the same physical scene.
- **Evidence anchors:** Abstract mentions "modality modeling module with a novel modality indicator," section 4.2 describes it as a "switch" to deactivate modalities.
- **Break condition:** If scene contains perfectly aligned multimodal data with zero physical conflicts, this introduces unnecessary parameter overhead.

### Mechanism 2: Gradient-Based Multimodal Decomposition
- **Claim:** Splitting Gaussians based on conflicting optimization gradients resolves granularity disparity.
- **Mechanism:** During densification, splits single Gaussian into single-modal ones when gradient difference exceeds threshold θ.
- **Core assumption:** Gradient direction and magnitude serve as reliable proxies for "granularity needs" of specific modalities.
- **Evidence anchors:** Abstract mentions "multimodal decomposition mechanism that separates multi-modal Gaussians into single-modal ones based on gradient differences."
- **Break condition:** If threshold is too low, scene suffers from Gaussian explosion; if too high, "ghosting" artifacts persist.

### Mechanism 3: Soft Pruning for Compactness
- **Claim:** Pruning at modality level rather than Gaussian level enables more compact scene representation.
- **Mechanism:** Sets specific modality's indicator to "off" instead of deleting entire Gaussian.
- **Core assumption:** A Gaussian redundant for one modality is not necessarily redundant for entire scene representation.
- **Evidence anchors:** Abstract mentions "resulting in a more compact and efficient multimodal scene representation... using one-third of the Gaussians."
- **Break condition:** If computational cost of tracking inactive modality flags exceeds cost of storing denser geometry, efficiency gain is negated.

## Foundational Learning

- **Concept: 3D Gaussian Splatting (3DGS) Primitives**
  - **Why needed here:** MMOne relies on explicit 3D Gaussians defined by position, covariance, and opacity.
  - **Quick check question:** Can you explain how a 3D Gaussian is projected onto 2D plane and how alpha-blending determines final pixel color?

- **Concept: Densification and Adaptive Density Control**
  - **Why needed here:** Core contribution lies in modifying standard 3DGS "clone/split" logic.
  - **Quick check question:** Under standard 3DGS, when does algorithm decide to clone Gaussian versus split it?

- **Concept: Modality Disparity (Property vs. Granularity)**
  - **Why needed here:** Paper explicitly frames solution around these two types of conflict.
  - **Quick check question:** Why would "hot" coffee cup require different geometric structure for Thermal imaging (coarse gradients) than for RGB imaging (fine texture edges)?

## Architecture Onboarding

- **Component map:** Input (multi-view RGB, Thermal, Language) -> Scene Representation (3D Gaussians with modality indicators/features) -> Modality Modeling Module (render pass logic) -> Decomposition Engine (gradient difference calculations) -> Rasterizer (modified differentiable rasterizer)

- **Critical path:** The Multimodal Decomposition logic in Section 4.3 is where standard 3DGS diverges. If implemented incorrectly, model will fail to disentangle modalities and revert to baseline's blurry results.

- **Design tradeoffs:**
  - Memory vs. Disentanglement: Trades higher per-Gaussian attribute storage for lower total Gaussian count
  - Training Stability: Gradient threshold (0.0002) is sensitive hyperparameter; too low causes over-segmentation, too high causes modality bleeding

- **Failure signatures:**
  - Modality Bleeding: Thermal textures appearing in RGB renderings or vice versa → Decomposition threshold likely too high
  - Geometry Loss: Scene structure collapses → Likely error in "Soft Prune" logic where shared structural Gaussians removed too aggressively
  - Semantic Drift: Language queries return incorrect masks → Check alignment between CLIP features and Gaussian centers

- **First 3 experiments:**
  1. Sanity Check (RGB-Only): Run framework with only RGB inputs to ensure it collapses back to standard 3DGS performance
  2. Dual-Modality Stress Test: Train on RGB and Thermal data for scene with significant occlusion differences to verify Modality Indicator learns distinct opacity values
  3. Ablation on Thresholds: Systematically vary gradient decomposition threshold (0.0001 to 0.0004) on single scene to visualize trade-off between Gaussian count and PSNR

## Open Questions the Paper Calls Out

- **Question:** How can MMOne framework be extended to model dynamic scenes where multiple modalities change over time?
  - **Basis in paper:** [explicit] Conclusion states "Future work will focus on... extending multimodal scene representation to dynamic scenes."
  - **Why unresolved:** Current framework operates on static assumptions with fixed Gaussian positions and attributes, lacking temporal component.
  - **What evidence would resolve it:** Modified MMOne model that processes temporal sequences while maintaining consistent object segmentation across frames.

- **Question:** Can camera pose optimization be integrated into learning process to handle unaligned or noisy multimodal inputs?
  - **Basis in paper:** [explicit] Authors identify "incorporating camera poses into the learning process" as specific direction for future work.
  - **Why unresolved:** Current method relies on pre-computed camera poses from RGB Structure-from-Motion; cannot correct registration errors if other modalities misaligned.
  - **What evidence would resolve it:** Successful joint optimization of poses and Gaussian attributes using multimodal losses.

- **Question:** Does computational efficiency of multimodal decomposition mechanism degrade when scaling to larger number of modalities?
  - **Basis in paper:** [inferred] Decomposition mechanism relies on pairwise gradient differences; paper validates scalability only up to 4 modalities.
  - **Why unresolved:** As number of modalities increases, pairwise comparisons scale quadratically; unclear if leads to fragmentation or prohibitive costs.
  - **What evidence would resolve it:** Complexity analysis and performance benchmarks showing stable training times with 5+ modalities.

## Limitations
- Significant computational overhead through gradient-based decomposition mechanism requiring tracking of modality-specific gradients
- 0.0002 decomposition threshold is empirically chosen but may not generalize across datasets with different characteristics
- Scalability to more than three modalities remains untested
- Reliance on specific feature extractors (SAM ViT-H, OpenCLIP ViT-B/16) creates dependency issues for different application domains

## Confidence
- **High Confidence (90%+):** Basic framework architecture and implementation of modality indicators for handling property disparity is technically sound and well-validated through reported PSNR/SSIM improvements
- **Medium Confidence (70-89%):** Gradient-based decomposition mechanism for resolving granularity disparity is theoretically justified but has limited empirical validation; 0.0002 threshold appears arbitrary
- **Low Confidence (Below 70%):** Scalability claims to additional modalities and efficiency gains from soft pruning lack comprehensive validation; doesn't address performance with modalities having different sampling rates

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary gradient decomposition threshold (0.00005 to 0.001) across 5-10 diverse scenes to quantify trade-off between Gaussian count, rendering quality, and training stability

2. **Cross-Dataset Generalization:** Test framework on datasets outside RGBT-Scenes and LERF domains, particularly scenes with challenging lighting conditions, extreme temperature ranges, or complex linguistic queries to validate robustness

3. **Memory Efficiency Benchmarking:** Measure actual memory consumption and training time per iteration compared to standard 3DGS, accounting for overhead of tracking modality-specific gradients and indicators, to verify claimed efficiency gains are practically meaningful