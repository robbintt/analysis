---
ver: rpa2
title: 'Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration
  System Taking a Neuro-Symbolic Approach'
arxiv_id: '2512.16425'
source_url: https://arxiv.org/abs/2512.16425
tags:
- system
- search
- literature
- scholarly
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ORKG ASK is an AI-driven scholarly search system using a neuro-symbolic
  approach that combines vector search, LLMs, and knowledge graphs to help researchers
  find relevant literature. The system processes research questions in natural language,
  retrieves relevant articles using vector embeddings, and generates answers via Retrieval-Augmented
  Generation (RAG).
---

# Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach

## Quick Facts
- arXiv ID: 2512.16425
- Source URL: https://arxiv.org/abs/2512.16425
- Reference count: 0
- Primary result: AI-driven scholarly search system combining vector search, LLMs, and knowledge graphs to help researchers find relevant literature

## Executive Summary
ORKG ASK is an AI-driven scholarly literature search system that takes a neuro-symbolic approach to help researchers find relevant literature. The system processes research questions in natural language, retrieves relevant articles using vector embeddings, and generates answers via Retrieval-Augmented Generation (RAG). ASK combines vector search, large language models, and knowledge graphs to create a comprehensive search and exploration platform for academic literature.

## Method Summary
ORKG ASK processes research questions in natural language and retrieves relevant articles using vector embeddings. The system employs Retrieval-Augmented Generation (RAG) to generate answers based on retrieved literature. It processes 76.4 million articles from the CORE dataset and evaluates through user feedback (1,212 responses) and a controlled experiment with 9 participants comparing it to Google Scholar.

## Key Results
- ASK achieves good usability with a UMUX score of 65.7/100
- Users report lower perceived task load (26.76%) compared to Google Scholar (61.3%)
- The system maintains a low 3% bounce rate, indicating active user engagement

## Why This Works (Mechanism)
ORKG ASK leverages a neuro-symbolic approach that combines vector search for semantic similarity, large language models for natural language understanding and answer generation, and knowledge graphs for structured information representation. This multi-modal approach allows the system to handle complex research queries while maintaining accuracy and relevance in search results.

## Foundational Learning
1. **Retrieval-Augmented Generation (RAG)**: Combines information retrieval with text generation to provide accurate, context-aware responses. Why needed: Enables the system to ground answers in actual literature rather than generating unsupported claims. Quick check: Verify that generated answers cite specific sources from the retrieved articles.

2. **Vector Embeddings for Semantic Search**: Converts text into numerical representations that capture semantic meaning. Why needed: Allows finding relevant articles even when exact keyword matches don't exist. Quick check: Test that semantically similar but lexically different queries return comparable results.

3. **Knowledge Graph Integration**: Provides structured representation of research entities and relationships. Why needed: Enables precise querying of specific research concepts and their interconnections. Quick check: Verify that entity-based queries return accurate and relevant results.

## Architecture Onboarding

**Component Map**: User Query -> Natural Language Processing -> Vector Search -> Knowledge Graph Query -> LLM Processing -> RAG Generation -> Answer Display

**Critical Path**: User input flows through NLP processing for query understanding, vector search for article retrieval, knowledge graph for structured information access, and LLM for answer generation using RAG.

**Design Tradeoffs**: The system prioritizes comprehensive coverage (76.4M articles) over perfect precision, using the neuro-symbolic approach to balance recall and relevance. The choice of CORE dataset provides breadth but may sacrifice depth in specialized domains.

**Failure Signatures**: 
- Poor query understanding leads to irrelevant results
- Vector search limitations cause missing relevant articles
- Knowledge graph gaps result in incomplete structured information
- LLM generation errors produce inaccurate or unsupported claims

**First Experiments**:
1. Test query understanding by inputting semantically equivalent but lexically different research questions
2. Evaluate retrieval precision by checking if top results actually address the research question
3. Verify answer quality by cross-checking generated responses against original article content

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies heavily on user feedback and a small controlled experiment with only 9 participants
- Limited details provided on how neuro-symbolic components interact and balance each other
- Quality and comprehensiveness of the CORE dataset may vary across research domains

## Confidence
- Usability findings: Medium confidence due to small sample size in controlled experiment
- System architecture details: Low confidence due to limited technical specifications provided
- Evaluation methodology: Medium confidence but lacks objective performance metrics
- Long-term user engagement insights: Low confidence due to limited longitudinal data

## Next Checks
1. Conduct a larger-scale user study with diverse research domains and participant pools to validate the usability and task load findings
2. Perform systematic evaluation of retrieval accuracy and answer quality across multiple research domains and query types
3. Implement and test a version tracking system to evaluate the impact of continuous learning and knowledge graph updates on search performance over time