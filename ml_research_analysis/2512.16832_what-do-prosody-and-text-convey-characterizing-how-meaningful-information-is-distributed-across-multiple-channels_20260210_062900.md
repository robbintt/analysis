---
ver: rpa2
title: What Do Prosody and Text Convey? Characterizing How Meaningful Information
  is Distributed Across Multiple Channels
arxiv_id: '2512.16832'
source_url: https://arxiv.org/abs/2512.16832
tags:
- information
- prosody
- text
- sarcasm
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework to quantify
  how much information about a specific linguistic feature (like sarcasm or emotion)
  is conveyed through prosody versus text. The approach estimates mutual information
  between a target feature and different communication channels (audio, text, prosody)
  using fine-tuned speech and language models.
---

# What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels

## Quick Facts
- **arXiv ID**: 2512.16832
- **Source URL**: https://arxiv.org/abs/2512.16832
- **Reference count**: 40
- **Key outcome**: Audio channels convey over an order of magnitude more information about sarcasm and emotion compared to text alone, while for questionhood the advantage is smaller (2.4x).

## Executive Summary
This paper introduces an information-theoretic framework to quantify how much information about a specific linguistic feature (like sarcasm or emotion) is conveyed through prosody versus text. The approach estimates mutual information between a target feature and different communication channels (audio, text, prosody) using fine-tuned speech and language models. Experiments on sarcasm detection, affect classification, and questionhood classification show that audio channels convey substantially more information about sarcasm and emotion compared to text alone, while for questionhood the advantage is smaller. The method reveals that local prosodic information often suffices to determine sarcasm and affect, even when textual context is limited to a single sentence.

## Method Summary
The framework estimates mutual information (MI) between linguistic features (sarcasm, affect, questionhood) and communication channels (audio, text) by fine-tuning neural network classifiers and using cross-entropy as an upper bound on conditional entropy. For text-only analysis, GPT-2 variants are fine-tuned on transcripts; for audio-only analysis, Whisper encoder variants are fine-tuned on audio. MI is computed as H(Y) - H(Y|X), where H(Y) is estimated non-parametrically from label frequencies and H(Y|X) is estimated via classifier cross-entropy. The study uses MUStARD dataset for sarcasm (690 utterances, balanced classes), MSP-Podcast for affect and questionhood (affect: 10 emotion categories; questionhood: binary classification with downsampled non-questions), and systematically varies model size and hyperparameters through 20 sweep runs per configuration.

## Key Results
- Audio channel conveys over an order of magnitude more information about sarcasm and emotion compared to text alone
- For questionhood, audio advantage is smaller (2.4x), reflecting syntactic indicators in text
- Local prosodic information often suffices to determine sarcasm and affect, even with single-sentence context
- Whisper encoder models outperform text-only models for sarcasm and affect detection

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Decomposition of Channel Contributions
The framework decomposes total feature entropy H(Y) into component contributions from different channels using MI identities: MI(Y;A|T) = MI(Y;A) - MI(Y;T). This quantifies audio's unique contribution beyond text. The core assumption is that audio (A) fully determines text (T), meaning A contains both prosodic and textual information. The residual MI(Y;A|T,P)—information in audio not captured by text or prosody—is attributed mainly to noise/speaker identity. This approximation becomes unreliable if audio models fail to encode textual content accurately or if non-prosodic audio features substantially contribute to Y.

### Mechanism 2: Cross-Entropy Upper Bound for Conditional Entropy Estimation
Neural network classifiers trained to predict feature Y from channel X provide tractable upper bounds on conditional entropy H(Y|X). Following Pimentel et al. (2020), H(Y|X) ≤ H_θ(Y|X) = -1/|D| Σ log p_θ(y_i|x_i). Minimizing cross-entropy loss tightens this bound, yielding better MI estimates via MI(X;Y) = H(Y) - H(Y|X). The core assumption is that pretrained models (Whisper for audio, GPT-2 for text) can be fine-tuned to approximate the true conditional distribution p(Y|X) sufficiently well. If model class is misspecified, training is insufficient, or pretraining doesn't transfer, cross-entropy remains a loose upper bound, leading to MI underestimation.

### Mechanism 3: Channel-Specific Feature Sensitivity
Different linguistic features have qualitatively different dependencies on prosodic vs. textual channels. Features with strong textual cues (questionhood has syntactic markers) show smaller audio-text MI ratios (~2.4x). Features with ambiguous text but clear prosodic markers (sarcasm, emotion) show larger ratios (>10x), reflecting information distribution in natural communication. The core assumption is that the datasets used provide representative samples of how information is distributed in natural speech. With longer discourse context, text-based sarcasm/affect detection may improve substantially, reducing the audio advantage.

## Foundational Learning

- **Concept: Mutual Information (MI)**
  - **Why needed here**: MI quantifies shared information between variables; the entire framework depends on understanding MI as entropy reduction when observing another variable.
  - **Quick check question**: If MI(Y;A) = 0.52 bits and H(Y) = 2.58 bits, what fraction of uncertainty about Y does audio resolve?

- **Concept: Conditional Entropy and the Chain Rule**
  - **Why needed here**: The decomposition MI(Y;A|T) = MI(Y;A) - MI(Y;T) relies on conditional entropy identities; misunderstanding this leads to incorrect channel attribution.
  - **Quick check question**: Why does H(Y|X) ≤ H_θ(Y|X) hold, and what does equality require?

- **Concept: Cross-Entropy as Surrogate Loss**
  - **Why needed here**: Practical MI estimation uses classifier cross-entropy; understanding this connection is essential for implementation and debugging.
  - **Quick check question**: If your classifier achieves 95% accuracy but cross-entropy remains high, what might this indicate about the MI estimate?

## Architecture Onboarding

- **Component map**: Data pipeline (aligned audio-transcript pairs with Y labels) -> Text encoder (GPT-2 variants) + classification head -> Audio encoder (Whisper variants) + classification head -> MI estimator (H(Y) from label frequencies, H(Y|X) from cross-entropy)

- **Critical path**: 1) Prepare dataset with aligned (audio, text, Y) tuples; ensure label quality 2) Fine-tune text-only model → record test cross-entropy H_θ(Y|T) 3) Fine-tune audio-only model → record test cross-entropy H_θ(Y|A) 4) Compute H(Y) from label distribution 5) Calculate MI(Y;T) = H(Y) - H_θ(Y|T), MI(Y;A) = H(Y) - H_θ(Y|A), MI(Y;A|T) = MI(Y;A) - MI(Y;T)

- **Design tradeoffs**: Model size vs. estimation quality (larger models tighten entropy bounds but increase compute); LoRA vs. full fine-tuning (LoRA reduces training time but may yield looser bounds); Audio+text fusion (concatenating embeddings theoretically improves predictions but empirically underperforms audio-only Whisper, suggesting redundant/interfering representations)

- **Failure signatures**: Negative MI estimates (indicates H_θ(Y|X) > H(Y), meaning model predictions are worse than marginal distribution—check training convergence or data quality); Audio-only underperforms text-only (check audio preprocessing, model loading, or whether task is text-deterministic); Large variance across runs (hyperparameter sensitivity; increase sweep runs or stabilize with learning rate warmup)

- **First 3 experiments**: 1) Baseline replication: Reproduce MI estimates for sarcasm on MUStARD using GPT-2 small and Whisper small; verify ~10x audio advantage 2) Ablation on context: Truncate transcripts to N words or mask punctuation to test textual cue contribution; expect questionhood MI(Y;T) to drop sharply 3) Model comparison: Swap Whisper for wav2vec 2.0; compare entropy bounds to assess whether speech recognition pretraining helps vs. audio-only pretraining for paralinguistic features

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would direct estimation of MI(Y; P|T) compare to the current approximation using the full audio signal MI(Y; A|T)?
- **Basis in paper**: The authors state: "current spoken language models do not accept only prosodic features as input, making this estimation impossible. We leave it to future work to tackle the substantial problem of designing a new architecture that can take representations of isolated prosodic features as input."
- **Why unresolved**: Existing spoken language models cannot accept isolated prosodic features as input, forcing the authors to use full audio as a proxy.
- **What evidence would resolve it**: Development of a model architecture that accepts prosodic feature representations (pitch, tempo, loudness) as input, enabling direct comparison of the three quantities: MI(Y; T|P), MI(Y; P|T), and MI(Y; T; P).

### Open Question 2
- **Question**: How does information distribution across channels change when long-term discourse context (T', P') is available?
- **Basis in paper**: The authors note: "MI(Y; T,T') would be significantly larger than MI(Y; T) for both sarcasm and affect, i.e., the larger context adds significantly more information about the feature of interest... A fine-grained comparison of the information contributions of text and prosody in short and long contexts is a promising direction for future work."
- **Why unresolved**: The current study deliberately restricts context to single sentences to isolate local prosodic contributions.
- **What evidence would resolve it**: Experiments varying context window size (sentence vs. paragraph vs. full conversation) while measuring MI for sarcasm, affect, and questionhood.

### Open Question 3
- **Question**: How do prosodic information distributions for sarcasm, affect, and questionhood differ across typologically diverse languages?
- **Basis in paper**: The authors state: "whereas we limit the present study to English, future work should compare a more diverse set of languages. Recent work (Wilcox et al., 2025) has suggested that prosodic typologies can be investigated from an information-theoretic lens."
- **Why unresolved**: The study is limited to English; findings may not generalize to tone languages or pitch-accent languages where prosody functions differently.
- **What evidence would resolve it**: Applying the same MI estimation framework to datasets in typologically distinct languages (e.g., Mandarin, Japanese, Yoruba) and comparing the MI(Y; A)/MI(Y; T) ratios.

## Limitations
- The framework assumes audio fully determines text (A → T), but non-prosodic audio features like laughing or filler words may substantially contribute to Y
- Cross-entropy upper bound on conditional entropy depends critically on pretrained models' ability to capture relevant channel-feature relationships
- Single-sentence context limits textual disambiguation for sarcasm/affect detection

## Confidence
- **High confidence**: The methodology for estimating MI via cross-entropy bounds and the observation that audio models outperform text models for sarcasm/affect
- **Medium confidence**: The specific MI ratios (10x for sarcasm/affect, 2.4x for questionhood) and qualitative interpretation of channel dependencies
- **Low confidence**: The assumption that residual MI(Y;A|T,P) ≈ 0 and the claim that local prosodic information often suffices for sarcasm/affect detection

## Next Checks
1. **Context length ablation**: Systematically truncate transcripts to varying word counts (1, 5, 10, full sentence) and measure how MI(Y;T) changes for sarcasm and affect to test whether audio advantage diminishes with more context

2. **Residual channel information validation**: Train a classifier on the residual audio signal (original audio minus reconstructed audio from text via TTS) and measure MI(Y; residual) to validate whether A → T completely

3. **Cross-dataset replication**: Apply the MI estimation framework to a different sarcasm/affect dataset (e.g., Sarcasm Corpus V2, IEMOCAP) to verify whether observed audio-text MI ratios generalize beyond MUStARD and MSP-Podcast