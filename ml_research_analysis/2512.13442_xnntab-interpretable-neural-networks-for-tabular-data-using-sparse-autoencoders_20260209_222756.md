---
ver: rpa2
title: XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders
arxiv_id: '2512.13442'
source_url: https://arxiv.org/abs/2512.13442
tags:
- features
- data
- tabular
- learning
- interpretable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'XNNTab addresses the challenge of combining high predictive performance
  with interpretability in neural networks for tabular data. The method uses a two-stage
  approach: first, a standard neural network learns non-linear feature representations,
  then a sparse autoencoder (SAE) decomposes these into interpretable, monosemantic
  features.'
---

# XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders

## Quick Facts
- arXiv ID: 2512.13442
- Source URL: https://arxiv.org/abs/2512.13442
- Reference count: 0
- Combines high predictive performance with intrinsic interpretability through sparse autoencoder decomposition

## Executive Summary
XNNTab addresses the challenge of combining high predictive performance with interpretability in neural networks for tabular data. The method uses a two-stage approach: first, a standard neural network learns non-linear feature representations, then a sparse autoencoder (SAE) decomposes these into interpretable, monosemantic features. Human-understandable semantics are assigned to each feature through rule-based classifiers that identify subsets of training instances that activate each feature.

Experiments on seven datasets show XNNTab outperforms interpretable models (Logistic Regression, Decision Trees) and achieves comparable performance to black-box models (XGBoost, Random Forests) and state-of-the-art neural architectures (TabNet, NODE, T2G-Former). On Adult dataset, XNNTab achieves macro F1 of 0.795, matching MLP and outperforming interpretable baselines. On Spambase, it reaches 0.948 F1. The learned features are interpretable through simple decision rules with average complexity of 2.4 terms per rule, making the model's predictions transparent through linear combinations of these features.

## Method Summary
XNNTab uses a four-step training procedure: (1) Train a standard MLP with cross-entropy loss and L1 regularization on tabular data, (2) Train a sparse autoencoder to reconstruct the penultimate layer activations, (3) Freeze the MLP and SAE, then fine-tune the decision layer weights, and (4) Combine the linear layers into a single interpretable layer. Interpretability is achieved by extracting Skope-rules from highly activating instances for each SAE latent dimension. The method works on binary and multi-class classification tasks, with preprocessing including normalization for numerical features and one-hot encoding for nominal features.

## Key Results
- Outperforms interpretable baselines (LR, DT) and matches state-of-the-art neural architectures (TabNet, NODE, T2G-Former) on most datasets
- Achieves macro F1 of 0.795 on Adult dataset, comparable to MLP baseline and superior to interpretable models
- Reaches 0.948 F1 on Spambase dataset with interpretable features via 2.4-term decision rules
- Maintains interpretability through linear combination of monosemantic features learned via sparse autoencoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoders decompose polysemantic neural representations into interpretable monosemantic features.
- Mechanism: The SAE encoder maps MLP hidden activations to a higher-dimensional sparse latent space via `h_SAE = ReLU(M·h_l + b)`. Sparsity constraints force each latent dimension to activate only for specific input patterns, encouraging feature specialization rather than distributed representation.
- Core assumption: Polysemantic neurons can be linearly decomposed into monosemantic components without losing predictive information.
- Evidence anchors:
  - [abstract] "XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE)"
  - [Section 3.1] "By design, ĥ_l is a sparse linear combination of (latent) monosemantic dictionary features"
  - [corpus] Neighbor paper "Transcoders Beat Sparse Autoencoders" suggests SAEs are actively debated for interpretability—mechanism not settled science
- Break condition: If MLP hidden representations are already near-linear or highly sparse, SAE provides marginal benefit. If reconstruction loss is high, monosemantic decomposition fails.

### Mechanism 2
- Claim: Linear combination of sparse features preserves interpretability while maintaining predictive performance.
- Mechanism: After training, the decoder weights (M^T) and final classification layer (W) are multiplied: `W' = W·M^T`. This creates a single linear layer from sparse features directly to predictions, where each weight indicates feature importance and direction (positive/negative class contribution).
- Core assumption: The composition of two linear transformations (decoder + classifier) can be collapsed without approximation error.
- Evidence anchors:
  - [abstract] "making the overall model prediction intrinsically interpretable"
  - [Section 3.2] "Therefore, we directly connect the hidden layer of the SAE to the output and set the weights of this layer to W' = W·M^T"
  - [corpus] Weak direct evidence; neighbor papers don't discuss this specific linear collapse technique
- Break condition: If fine-tuning significantly changes feature semantics, pre-collapse weights no longer reflect learned representations. If SAE reconstruction is poor, linear approximation introduces error.

### Mechanism 3
- Claim: Rule-based classifiers on highly-activating instances assign human-readable semantics to latent features.
- Mechanism: For each SAE latent dimension j, identify instances with activations above threshold p (forming subset T_j). Train Skope-rules to describe T_j with conjunctive rules (e.g., "age > 44.5 AND IsActiveMember ≤ 0.5"). Rules capture the input conditions that trigger each monosemantic feature.
- Core assumption: High activations correspond to coherent, rule-describable input patterns rather than noisy or idiosyncratic combinations.
- Evidence anchors:
  - [Section 3.3] "For each dimension in the latent representation of the SAE j ∈ {1, ...d_hid}, we identify the subset of training samples T_j which highly activate this feature"
  - [Table 4] Shows extracted rules like "marital_status_Married is False and age ≤ 37.5 and educational_num < 12"
  - [corpus] Neighbor paper "Binary Sparse Coding for Interpretability" notes SAE features may only be interpretable at high activation—supports thresholding approach
- Break condition: If no rules achieve high precision/recall for a feature, that feature remains uninterpretable. If rules are overly complex (>5 terms), interpretability degrades.

## Foundational Learning

- Concept: **Polysemantic vs. Monosemantic Representations**
  - Why needed here: The core premise is that standard neural neurons respond to multiple unrelated concepts (polysemantic), while SAE enforces single-concept neurons (monosemantic).
  - Quick check question: Can you explain why a single neuron activating for both "capital gains" and "education level" would hurt interpretability?

- Concept: **Sparsity-Interpretability Tradeoff**
  - Why needed here: L1 regularization forces few features to activate per sample, enabling simpler rule extraction but potentially losing information.
  - Quick check question: If 50 features activate for every input, can you write simple rules describing each feature?

- Concept: **Linear Layer Composition**
  - Why needed here: Understanding that `y = W·(M^T·h_SAE)` equals `y = (W·M^T)·h_SAE` is essential for grasping the final interpretable model.
  - Quick check question: Given W is [3×10] and M^T is [10×50], what is the shape of the combined weight matrix?

## Architecture Onboarding

- Component map: Input → MLP (θ_g) → h_l [penultimate layer] → SAE Encoder (M, b) → h_SAE [sparse monosemantic features] → SAE Decoder (M^T) → ĥ_l [reconstruction] → Combined Linear (W' = W·M^T) → ŷ [prediction] → Rule Extraction (Skope-rules on T_j) → Feature semantics

- Critical path:
  1. Train MLP to convergence (cross-entropy + L1)
  2. Extract h_l activations for all training samples
  3. Train SAE on h_l (reconstruction + sparsity loss)
  4. Freeze MLP and SAE; fine-tune W using ĥ_l
  5. Compute W' = W·M^T
  6. For each SAE neuron, find high-activation instances, extract rules

- Design tradeoffs:
  - **Expansion factor R** (SAE hidden size / input size): Higher R = more features, finer granularity, but more rules to interpret. Paper uses R∈{1,2,3}.
  - **Activation threshold p**: Higher p = stricter rule precision but fewer features get rules. Paper tests p∈{50%,60%,70%,80%,90%}.
  - **Rule complexity vs. coverage**: Precision=1.0 constraint ensures reliable rules but may leave features unexplained.

- Failure signatures:
  - SAE reconstruction loss > 0.5: Features don't capture MLP representations
  - <50% of features obtain rules: Model remains partially black-box
  - Rule recall < 0.3: Rules don't reliably describe activating instances
  - Performance drops >5% from MLP baseline: SAE bottleneck too severe

- First 3 experiments:
  1. **Baseline validation**: Train MLP alone on Adult dataset; confirm macro F1 ≈ 0.796. This isolates representation quality before SAE.
  2. **SAE reconstruction sanity check**: Train SAE on MLP activations; verify reconstruction loss < 0.1 and sparsity (>90% zeros in h_SAE). If fails, increase d_hid or adjust L1 coefficient.
  3. **Rule extraction coverage test**: For Spambase (smallest dataset), run full pipeline and measure: (a) fraction of features with rules at p=80%, (b) average rule length, (c) average recall. Target: >70% coverage, rule length ≤3, recall ≥0.4.

## Open Questions the Paper Calls Out

- Can an automated method be developed to optimally select activation thresholds that balance rule recall with the fraction of extracted rules?
- What strategies can effectively reduce redundant rules through pruning or combining neurons?
- Can the SAE-based decomposition approach be extended to other specialized tabular neural architectures (e.g., TabR, FT-Transformer) to improve performance while maintaining interpretability?

## Limitations

- The mechanism by which SAEs decompose polysemantic into monosemantic features remains theoretically contested in the literature
- Training procedure details are underspecified, particularly SAE loss formulation and fine-tuning hyperparameters
- The linear collapse assumption (W' = W·M^T) is not empirically validated through ablation studies

## Confidence

- **High confidence**: Predictive performance comparisons (F1 scores, rank preservation vs. baselines)
- **Medium confidence**: Interpretability mechanism (feature decomposition via SAE)
- **Low confidence**: Generalizability to non-tabular domains and robustness to dataset characteristics

## Next Checks

1. **Ablation on SAE necessity**: Train MLP directly to final layer (no SAE bottleneck) and compare performance to confirm SAE doesn't degrade predictive capability beyond acceptable limits.
2. **Rule coverage validation**: Systematically vary p threshold and report fraction of features obtaining rules, average rule complexity, and coverage statistics to establish robustness of interpretability claims.
3. **Cross-dataset generalization test**: Apply XNNTab to a held-out tabular dataset (not in original 7) to verify performance and interpretability claims generalize beyond the paper's experimental scope.