---
ver: rpa2
title: 'ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation'
arxiv_id: '2511.03656'
source_url: https://arxiv.org/abs/2511.03656
tags:
- questions
- document
- chimdqa
- evaluation
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChiMDQA introduces a Chinese multi-document QA dataset covering
  six domains (academic, education, finance, law, medical, news) with 6,068 QA pairs
  across ten fine-grained question types. It addresses the lack of comprehensive Chinese
  long-document benchmarks by providing explicit and implicit fact-based questions,
  including novel categories like filtering, statistical analysis, and computational
  reasoning.
---

# ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation

## Quick Facts
- arXiv ID: 2511.03656
- Source URL: https://arxiv.org/abs/2511.03656
- Authors: Jing Gao; Shutiao Luo; Yumeng Liu; Yuanming Li; Hongji Zeng
- Reference count: 0
- Primary result: ChiMDQA dataset with 6,068 QA pairs across 6 domains and 10 question types

## Executive Summary
ChiMDQA introduces a Chinese multi-document QA dataset designed to address the lack of comprehensive Chinese long-document benchmarks. The dataset spans six domains (academic, education, finance, law, medical, news) and features ten fine-grained question types, including novel categories like filtering, statistical analysis, and computational reasoning. A multi-stage pipeline combining document screening, LLM-based QA generation, and hybrid automated/manual validation ensures high quality and diversity. Evaluation across eight models reveals GPT-4o achieving 76.5 F1-score on factual questions and 81.2 BERTScore-F1 on open-ended questions, while RAG strategies improve F1-scores by ~4.6% and reduce perplexity by 81.2%.

## Method Summary
ChiMDQA employs a multi-stage pipeline for dataset construction, beginning with document screening to select relevant Chinese texts across six domains. LLM-based QA generation produces both explicit and implicit fact-based questions, including novel categories such as filtering, statistical analysis, and computational reasoning. Hybrid automated and manual validation processes ensure high quality, achieving low error rates (~3%). The dataset comprises 6,068 QA pairs and is designed to support comprehensive evaluation of Chinese document QA systems.

## Key Results
- GPT-4o achieves 76.5 F1-score on factual questions and 81.2 BERTScore-F1 on open-ended questions
- RAG strategies improve F1-scores by ~4.6% and reduce perplexity by 81.2%
- Current models struggle with complex reasoning, with F1-scores below 40 and hallucination rates exceeding 20% on L2 questions

## Why This Works (Mechanism)
The dataset's effectiveness stems from its comprehensive coverage of ten fine-grained question types and six diverse domains, enabling nuanced evaluation of Chinese document QA systems. The multi-stage pipeline ensures high-quality data generation, while the inclusion of both explicit and implicit questions challenges models to perform complex reasoning. The use of RAG strategies enhances retrieval and generation, improving both accuracy and fluency. However, current architectures still face significant challenges with hallucination and low F1-scores on complex questions.

## Foundational Learning
- **Document Screening**: Essential for selecting relevant texts across diverse domains; quick check: verify domain coverage and text quality.
- **LLM-based QA Generation**: Enables scalable creation of diverse question types; quick check: assess question-answer alignment and novelty.
- **Hybrid Validation**: Combines automated and manual processes to ensure data quality; quick check: measure error rates and consistency.
- **RAG Strategies**: Improve retrieval and generation for long-document QA; quick check: compare F1-scores and perplexity before and after RAG application.
- **Fine-grained Evaluation**: Supports detailed assessment of model capabilities; quick check: analyze performance across question types and domains.
- **Cross-lingual Generalization**: Dataset focus on Chinese limits applicability to other languages; quick check: compare with English benchmarks.

## Architecture Onboarding

**Component Map:**
Document Screening -> LLM-based QA Generation -> Hybrid Validation -> Dataset Release

**Critical Path:**
The pipeline flows from document screening to LLM-based generation, followed by hybrid validation, ensuring high-quality data at each stage.

**Design Tradeoffs:**
- LLM-based generation offers scalability but risks model-specific biases.
- Hybrid validation improves quality but increases manual effort.
- Focus on Chinese limits cross-lingual applicability.

**Failure Signatures:**
- High error rates in hybrid validation indicate issues with document quality or generation.
- Low F1-scores and high hallucination rates suggest limitations in model architecture or retrieval mechanisms.

**3 First Experiments:**
1. Ablation study: Remove LLM generation and assess impact on dataset diversity and quality.
2. Cross-validation: Test model performance on held-out samples for computational reasoning.
3. Domain expansion: Evaluate new domains (engineering, government) for quality retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dataset's planned expansion into engineering and government domains maintain the high quality (e.g., low error rate) of the initial vertical domains using semi-automated generation pipelines?
- Basis in paper: Section 2.4 states a roadmap to incorporate "engineering, environmental science, and government reports" via a semi-automated pipeline, while Section 2.3 notes the current error rate is approximately 3%.
- Why unresolved: Scaling to new domains with semi-automation risks introducing noise that the current manual-heavy validation process successfully filtered out.
- What evidence would resolve it: Error rate statistics and quality assessment scores for the new domain subsets comparable to the initial 6,068 QA pairs.

### Open Question 2
- Question: How can RAG architectures be optimized to reduce the high hallucination rates (>20%) and low F1-scores observed when handling complex questions in ChiMDQA?
- Basis in paper: Section 4.2 reports that even with RAG, no model surpassed an F1-Score of 40, and all exhibited hallucination rates exceeding 20%, indicating current retrieval-generation mechanisms struggle with the dataset's complexity.
- Why unresolved: The paper demonstrates current RAG implementations are insufficient for complex reasoning, but does not propose specific architectural solutions to bridge this performance gap.
- What evidence would resolve it: A modified RAG framework that achieves >50 F1-Score or <10% hallucination on the dataset's complex reasoning (L2) questions.

### Open Question 3
- Question: To what extent does the inherent ambiguity of open-ended questions (lacking a single correct answer) contribute to the high perplexity observed in state-of-the-art models?
- Basis in paper: Section 4.1 notes high perplexity across models (e.g., 53.1 for Doubao-Pro-128k) and attributes this to the "inherent diversity" of open-ended questions where models must navigate broad semantic spectrums.
- Why unresolved: It is unclear if high perplexity reflects a failure of model capability or a natural response to the "valid diversity" allowed by the open-ended evaluation criteria.
- What evidence would resolve it: A correlation analysis between human-rated answer acceptability and model perplexity scores on the open-ended subset.

## Limitations
- Reliance on LLM-based processing may introduce model-specific biases.
- Dataset size (6,068 QA pairs) is relatively modest, potentially limiting statistical power.
- Focus on Chinese language restricts cross-linguistic generalizability.

## Confidence
- GPT-4o achieving 76.5 F1-score on factual questions: **High confidence**
- RAG strategies improving F1-scores by ~4.6% and reducing perplexity by 81.2%: **Medium confidence**
- Coverage of ten fine-grained question types and six domains: **High confidence**

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of document screening, LLM generation, and hybrid validation stages to final dataset quality.
2. Test model performance on held-out samples specifically designed to evaluate computational reasoning and statistical analysis capabilities.
3. Perform cross-validation across different Chinese language models to assess whether evaluation results are consistent across model families beyond GPT-4o and BERT-based systems.