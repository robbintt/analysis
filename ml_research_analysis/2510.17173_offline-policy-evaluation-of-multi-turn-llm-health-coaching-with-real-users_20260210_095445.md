---
ver: rpa2
title: Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users
arxiv_id: '2510.17173'
source_url: https://arxiv.org/abs/2510.17173
tags:
- tool
- health
- policy
- arxiv
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Offline policy evaluation (OPE) on real-world health coaching\
  \ logs and a lightweight simulator reveal that while uniform heavy-tool usage maximizes\
  \ average reward, it systematically harms specific user subgroups\u2014particularly\
  \ low-literacy/high-efficacy users\u2014demonstrating the need for archetype-aware\
  \ personalization. Adding a small, bounded early information-gain bonus consistently\
  \ improves task success, reliability (pass@3), and shortens trait-identification\
  \ time in simulation."
---

# Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users

## Quick Facts
- arXiv ID: 2510.17173
- Source URL: https://arxiv.org/abs/2510.17173
- Reference count: 25
- Primary result: OPE reveals subgroup harms masked by aggregate metrics; early curiosity bonus improves simulation performance.

## Executive Summary
This work demonstrates that offline policy evaluation can detect harmful personalization failures in LLM health coaching before deployment. By decomposing actions into TOOL and STYLE heads and evaluating per-archetype (literacy × self-efficacy), OPE exposes heterogeneous treatment effects—the same policy can benefit one subgroup while harming another. Adding a small, bounded early information-gain bonus consistently improves task success, reliability, and shortens trait-identification time in simulation. These findings support an evaluation-first framework: freeze the generator, learn subgroup-aware decision heads using typed rewards, and report per-archetype metrics to detect and prevent subgroup harms.

## Method Summary
The authors evaluate factorized decision heads (TOOL ∈ {∅, Search, Code, Email}, STYLE ∈ {concise, detailed}) for a multi-turn LLM health coach using offline policy evaluation on pilot logs from 7 users (280 rated turns, 350 total). The method reconstructs behavior policies via calibrated classifiers per head, then applies SNIPS and AIPW estimators with importance ratio clipping at c=50. They decompose results by user archetype (health literacy × self-efficacy) and validate findings in a lightweight simulator with an early curiosity bonus (λ ∈ {0.1, 0.2}) that reduces posterior entropy over latent archetypes in first K turns.

## Key Results
- Uniform heavy-tool usage maximizes average reward but systematically harms specific subgroups, particularly low-literacy/high-efficacy users.
- Adding a small early information-gain bonus consistently improves task success, reliability (pass@3), and shortens trait-identification time in simulation.
- OPE with calibration diagnostics enables counterfactual policy comparison without new user trials, supporting personalized coaching strategies that balance overall performance with equity across user types.

## Why This Works (Mechanism)

### Mechanism 1
Factorized decision heads with archetype-conditional reward weighting reveal subgroup harms that aggregate metrics obscure. By decomposing actions into TOOL and STYLE heads and evaluating per-archetype, OPE exposes heterogeneous treatment effects—the same policy can benefit one subgroup while harming another. Core assumption: User archetypes derived from baseline surveys predict differential response to tool usage patterns within sessions. Evidence: L_low × E_high shows ∆Objective = -0.315 and ∆Satisfaction = -1.436 under AlwaysTool vs. NoTool. Break condition: If between-user heterogeneity in context-conditioned responses is minimal, per-archetype decomposition adds noise without signal.

### Mechanism 2
A bounded early information-gain bonus accelerates trait identification and improves downstream task success. Curiosity reward reduces posterior entropy over latent archetypes in first K turns, enabling earlier personalized action selection without sustained exploration costs. Core assumption: Dialogue features in early turns carry actionable signal about latent interaction preferences. Evidence: λ=0.20 improves goal success (0.935→0.970), pass@3 (0.95→0.98), reduces trait-ID turns (6.41→5.86). Break condition: If early dialogue features are uninformative about archetypes, curiosity bonus adds variance without reducing identification time.

### Mechanism 3
OPE with SNIPS/AIPW and calibration diagnostics enables counterfactual policy comparison without new user trials. Importance weighting corrects for logging-target policy divergence; doubly-robust AIPW reduces variance via outcome model augmentation; clipping and diagnostics control estimator pathology. Core assumption: Behavior propensity models are well-calibrated and sufficient overlap exists between logged and target policies. Evidence: ECE scores (TOOL=0.157, STYLE=0.050), clipping rate ≤0.29%, rating-propensity AUC=0.712. Break condition: If propensity model miscalibration is severe or overlap fails, OPE estimates become unreliable.

## Foundational Learning

- **POMDPs and Belief States**
  - Why needed here: User archetypes are latent; the system maintains belief state z_t over hidden traits rather than observing them directly.
  - Quick check question: Why must the policy condition on z_t = f_ϕ(h_t, u_i, m_t) rather than raw history alone?

- **Off-Policy Evaluation Estimators (IPS, SNIPS, AIPW)**
  - Why needed here: The paper uses SNIPS for objective rewards and AIPW for satisfaction with rating-missingness correction.
  - Quick check question: What failure mode does clipping (c=50) prevent in importance-weighted estimators?

- **Intrinsic Motivation / Curiosity Rewards**
  - Why needed here: Early information-gain bonus implements bounded exploration to reduce uncertainty.
  - Quick check question: How does r_curiosity = max{0, H(p_{t-1}) - H(p_t)} translate entropy reduction into scalar reward?

## Architecture Onboarding

- Component map:
  Apple Health export -> daily features (sleep/HRV/steps) -> ML predictions (stress/soreness/injury) -> tool outcome labels via rubric -> R_tool, R_eng, R_user rewards -> factorized behavior policy -> propensity models -> SNIPS/AIPW estimators -> per-archetype decomposition -> simulation with curiosity bonus

- Critical path:
  1. Fit calibrated propensity models per head; report ECE
  2. Compute importance ratios with clipping; check overlap diagnostics
  3. Run OPE comparing candidate policies; decompose by archetype
  4. Validate in simulator with curiosity bonus before any policy update

- Design tradeoffs:
  - Heavy tool usage lifts average R_total but harms L_low × E_high (equity-efficiency tension)
  - Early curiosity trades archetype-aligned action rate for faster trait identification
  - Reconstructed (not logged) propensities introduce calibration risk

- Failure signatures:
  - ECE > 0.2 on TOOL head -> propensity model misspecification
  - Clipping rate > 5% -> overlap violation
  - Rating missingness correlated with archetype -> selection bias in R_user estimates
  - Ratings decline over turns (4.36 → 4.12) -> multi-turn degradation not captured by per-turn model

- First 3 experiments:
  1. **Calibration audit**: Fit behavior policy on logged features; report per-head ECE. If TOOL ECE > 0.2, add interaction features or recalibrate.
  2. **Heterogeneity probe**: Run OPE (NoTool vs. AlwaysTool) with per-archetype decomposition. Flag any subgroup with ∆Satisfaction < -0.5.
  3. **Curiosity ablation in simulator**: Compare λ ∈ {0, 0.1, 0.2} on trait-ID turns and goal success. Confirm improvement is consistent across archetypes.

## Open Questions the Paper Calls Out

### Open Question 1
Do the identified harms to low-literacy/high-efficacy subgroups persist in larger, demographically diverse populations? The authors state their "deployment sample is small and demographically narrow, limiting generalizability." This remains unresolved because the pilot study (N=7) lacks the statistical power and variance required to confirm if the observed subgroup damage is a robust phenomenon or an artifact of the specific student cohort.

### Open Question 2
Does explicitly incorporating self-efficacy into the reward weighting and head policies improve outcomes over literacy-only conditioning? The authors note that "current evaluation weights and personalized routing are conditioned on literacy only; incorporating self-efficacy... is a planned extension." This remains unresolved because while the pilot data suggests self-efficacy correlates with conversation length and satisfaction, the current policy framework does not actively use this dimension for adaptation.

### Open Question 3
Can per-turn OPE estimators accurately rank policies when long-horizon dependencies are critical to the coaching outcome? The authors "interpret OPE as a per-turn contextual bandit evaluation" and explicitly "do not claim unbiased long-horizon policy value." This remains unresolved because health coaching often relies on cumulative context and delayed rewards; it is unclear if ignoring long-horizon bias in the estimator invalidates the policy rankings for real-world deployment.

## Limitations
- Small sample size (n=7 users, 280 rated turns) limits statistical power for detecting rare harms and increases variance in per-archetype estimates.
- Reconstructed (not logged) propensities introduce potential calibration errors, though ECE scores suggest acceptable performance.
- Early curiosity mechanism assumes dialogue features are informative about latent archetypes, but this signal strength is not directly validated.
- Rating missingness (20%) could introduce selection bias even with propensity correction.

## Confidence

- **High**: OPE framework with SNIPS/AIPW and calibration diagnostics is well-established; reported ECE and clipping rates suggest implementation quality.
- **Medium**: Mechanism 1 (heterogeneous subgroup effects) is strongly supported by per-archetype decomposition in Table 6, but small sample size limits generalizability.
- **Medium**: Mechanism 2 (early curiosity benefit) shows consistent improvements in simulation, but the underlying assumption about informative early features requires validation.
- **Low**: Extrapolation to broader user populations requires external validation beyond the 7-pilot cohort.

## Next Checks

1. Conduct external calibration audit on the reconstructed behavior policy using held-out features; flag any head with ECE > 0.2 for feature engineering or recalibration.
2. Perform subgroup heterogeneity analysis on new data (if available) to verify that L_low × E_high consistently shows negative treatment effects under heavy-tool policies.
3. Validate early curiosity mechanism by measuring posterior entropy reduction in actual user sessions, not just simulation, to confirm early dialogue features carry actionable archetype signal.