---
ver: rpa2
title: Study of Training Dynamics for Memory-Constrained Fine-Tuning
arxiv_id: '2510.19675'
source_url: https://arxiv.org/abs/2510.19675
tags:
- gradient
- memory
- layers
- random
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TraDy, a memory-efficient transfer learning
  approach for fine-tuning deep neural networks under strict resource constraints.
  The method exploits heavy-tailed stochastic gradient distributions and architecture-dependent
  layer importance to dynamically select channels for update while maintaining memory
  budgets.
---

# Study of Training Dynamics for Memory-Constrained Fine-Tuning

## Quick Facts
- arXiv ID: 2510.19675
- Source URL: https://arxiv.org/abs/2510.19675
- Authors: Aël Quélennec; Nour Hezbri; Pavlo Mozharovskyi; Van-Tam Nguyen; Enzo Tartaglione
- Reference count: 40
- This paper proposes TraDy, a memory-efficient transfer learning approach for fine-tuning deep neural networks under strict resource constraints.

## Executive Summary
TraDy addresses memory-constrained fine-tuning by dynamically selecting channels for update within pre-determined layers. The method exploits heavy-tailed gradient distributions and architecture-dependent layer importance to achieve up to 99% activation sparsity, 95% weight derivative sparsity, and 97% reduction in FLOPs for weight derivative computation. TraDy outperforms existing approaches across various downstream tasks and architectures while maintaining strict memory budgets.

## Method Summary
TraDy operates in two phases: offline layer ranking using Reweighted Gradient Norm (RGN) to identify top-K layers capturing ~97% of gradient information, followed by online dynamic channel selection. Each training epoch randomly samples input channels within selected layers until memory budget is exhausted. This stochastic approach approximates full gradient updates while maintaining memory constraints, with backward passes computing gradients only for selected channels.

## Key Results
- Achieves up to 99% activation sparsity and 95% weight derivative sparsity under strict memory constraints
- Reduces FLOPs for weight derivative computation by up to 97%
- Outperforms state-of-the-art methods including MeDyate and RGN-based approaches across multiple architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
Stochastic gradients during transfer learning follow heavy-tailed distributions (α < 2), causing gradient information to concentrate disproportionately in a small subset of channels. This creates natural sparsity patterns amenable to selective updating.

### Mechanism 2
Layer importance rankings remain invariant across downstream tasks and are determined primarily by network architecture. Architectural features like residual connections create consistent gradient flow patterns.

### Mechanism 3
Dynamic stochastic channel selection approximates full gradient expectation over time while maintaining strict memory constraints. Uniform random sampling from pre-selected high-importance layers, resampled each epoch, ensures expected gradient direction matches full gradient.

## Foundational Learning

- **Heavy-tailed distributions and α-stable distributions**: Understanding why gradient norms concentrate in few channels requires grasping that heavy-tailed distributions (α < 2) have infinite variance and power-law decay.
  - Quick check: Can you explain why a distribution with tail-index α = 1.5 concentrates more mass in extreme values than a Gaussian?

- **Transfer learning vs. training from scratch**: TraDy exploits properties of pre-trained networks (bounded weights/activations, task-agnostic layer importance) that don't apply to random initialization.
  - Quick check: Why would layer gradient rankings be more predictable for fine-tuning a pre-trained model than for training from scratch?

- **Memory-constrained backpropagation**: Standard backpropagation stores all intermediate activations; understanding the memory bottleneck clarifies why input-channel selection provides dual weight+activation sparsity benefits.
  - Quick check: Why does freezing input channels reduce both weight derivative memory and activation storage, while freezing output channels does not?

## Architecture Onboarding

- **Component map**: Pre-trained Network → [Offline: Layer Ranking via RGN analysis] → Top-K Layer Selection → Training Loop: Input → Forward Pass → Loss → [Per-epoch: Random channel sampling within Top-K layers under B_mem] → Backprop on selected channels only → Weight update

- **Critical path**:
  1. Offline phase: Fine-tune on any available task briefly to establish layer RGN rankings
  2. Select Top-K layers: Use 97% cumulative RGN threshold (e.g., ~35 layers for MobileNetV2)
  3. Online phase: Each epoch, uniformly sample channels from Top-K layers until memory budget exhausted
  4. Training: Standard backpropagation on selected channels only

- **Design tradeoffs**:
  - K (number of layers): Higher K = more flexibility but dilutes gradient concentration; paper uses 97% cumulative RGN threshold
  - Memory budget B_mem: Lower budget = more sparsity but potential accuracy loss; paper tests budgets as low as ~1.3% of full network memory
  - Static vs. Dynamic: Static is simpler but underperforms; Dynamic requires resampling overhead but significantly better accuracy

- **Failure signatures**:
  - Accuracy plateaus below baseline: Check if selected layers miss task-critical features; may need higher K or different layer selection
  - High variance across runs: May indicate budget too restrictive; increase B_mem or check gradient heavy-tailedness (α should be < 2)
  - No improvement over static: Ensure resampling happens each epoch (not once at initialization)

- **First 3 experiments**:
  1. Validate heavy-tailed gradients: Run a few epochs of fine-tuning on your target architecture with any dataset; compute α using the Mohammadi estimator. Confirm α < 2 before proceeding.
  2. Establish layer rankings: Fine-tune for 10-20 epochs with full gradients on a proxy task; compute per-layer cumulative RGN and identify the Top-K layers capturing 97% of total RGN.
  3. Compare Static vs. Dynamic: With same B_mem, run (a) static random channel selection (fixed from epoch 1) vs. (b) TraDy (resample each epoch). Expect dynamic to outperform by 0.5-1.5% accuracy on downstream tasks.

## Open Questions the Paper Calls Out

### Open Question 1
Can TraDy's theoretical benefits translate into measurable latency and energy savings when implemented on actual edge hardware, given the dynamic channel reselection between epochs?
- Basis: Authors state they do not present metrics on actual hardware performance (latency, energy consumption, etc.)
- Why unresolved: Current implementation is simulation; specialized implementation required for efficient execution on edge devices

### Open Question 2
Would adaptively determining the number of selected layers K based on available memory budget improve performance over the fixed 97% RGN threshold criterion?
- Basis: Authors state K could be adaptively determined based on available memory budget
- Why unresolved: Current approach uses fixed threshold; relationship between budget proportionality and optimal K unexplored

### Open Question 3
Does TraDy's stochasticity provide implicit regularization that helps escape sharp minima, explaining why it outperforms the deterministic RGN oracle?
- Basis: Authors hypothesize deterministic RGN selection may lead to local minima while TraDy introduces beneficial stochasticity
- Why unresolved: Mechanism conjectured but not empirically isolated or theoretically proven

### Open Question 4
Does the top-K layer selection methodology require architecture-specific calibration for large transformer models to achieve optimal performance?
- Basis: Section notes that top K layer selection may require more sophisticated calibration for transformer models
- Why unresolved: Transformers showed different strategy rankings with smaller sample sizes

## Limitations
- Layer ranking stability across highly dissimilar downstream tasks remains untested beyond evaluated datasets
- Heavy-tailed gradient behavior during fine-tuning is assumed to extend from standard SGD training without extensive empirical validation
- Memory unit conversion and per-channel memory accounting formula require precise implementation details

## Confidence
- **High Confidence**: Memory savings claims (97% FLOPs reduction, 99% activation sparsity) - directly measured from implementation
- **Medium Confidence**: Heavy-tailed gradient assumption - supported by theoretical analysis but limited empirical validation on fine-tuning tasks
- **Medium Confidence**: Architecture-dependent layer importance invariance - shown across three architectures but limited task diversity
- **Low Confidence**: Dynamic sampling advantage over static - only tested on small-scale datasets and three architectures

## Next Checks
1. Validate heavy-tailedness empirically: Run fine-tuning on your target architecture and compute the tail-index α using the Mohammadi estimator. Confirm α < 2 before applying TraDy.
2. Test layer ranking stability: Fine-tune on multiple proxy tasks and measure correlation of layer RGN rankings across seeds and datasets. Ensure correlation > 0.8.
3. Stress-test dynamic vs static: With identical Bmem, run both dynamic resampling (TraDy) and static random selection across multiple seeds. Measure accuracy difference and variance to confirm dynamic advantage.