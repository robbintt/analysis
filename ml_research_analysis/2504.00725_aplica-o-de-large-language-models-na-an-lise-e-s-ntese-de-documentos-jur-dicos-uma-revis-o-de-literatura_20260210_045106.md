---
ver: rpa2
title: "Aplica\xE7\xE3o de Large Language Models na An\xE1lise e S\xEDntese de Documentos\
  \ Jur\xEDdicos: Uma Revis\xE3o de Literatura"
arxiv_id: '2504.00725'
source_url: https://arxiv.org/abs/2504.00725
tags:
- para
- llms
- modelos
- dicos
- como
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review analyzed the application of Large
  Language Models (LLMs) in legal document analysis, focusing on prompt engineering
  techniques. The study reviewed 17 articles published between 2020-2024 from SCOPUS
  and Springer databases.
---

# Aplicação de Large Language Models na Análise e Síntese de Documentos Jurídicos: Uma Revisão de Literatura

## Quick Facts
- arXiv ID: 2504.00725
- Source URL: https://arxiv.org/abs/2504.00725
- Reference count: 2
- Primary result: Systematic review of LLM applications in legal document analysis with 17 articles, showing specialized models outperform general-purpose LLMs but challenges remain with hallucinations and biases

## Executive Summary
This systematic literature review examines the application of Large Language Models (LLMs) in legal document analysis and synthesis. The study analyzed 17 articles published between 2020-2024 from SCOPUS and Springer databases, focusing on prompt engineering techniques and model performance. The review found that specialized legal models like Legal-Pegasus generally outperformed general-purpose LLMs, with techniques such as Few-shot Learning, Zero-shot Learning, and Chain-of-Thought prompting proving effective for legal tasks. However, challenges with model hallucinations and biases remain significant obstacles to widespread adoption in legal practice.

## Method Summary
The study conducted a systematic literature review following PRISMA methodology, searching SCOPUS and Springer databases for articles published between 2020-2024. The researchers identified 17 relevant articles and analyzed their approaches to LLM applications in legal document analysis, focusing on model types, prompt engineering techniques, and evaluation metrics. The review categorized findings based on task types (summarization, classification, information retrieval) and model performance, while also identifying limitations and future research directions.

## Key Results
- GPT-4, BERT, Llama 2, and specialized models like Legal-Pegasus were widely used for legal document analysis tasks
- Prompt engineering techniques including Few-shot Learning, Zero-shot Learning, and Chain-of-Thought prompting showed high effectiveness
- Evaluation metrics included ROUGE, BLEU, F1-score, and human assessment for measuring model performance
- Specialized legal models generally outperformed general-purpose LLMs, but hallucinations and biases remain significant challenges

## Why This Works (Mechanism)
The effectiveness of LLMs in legal document analysis stems from their ability to process and understand complex language patterns, legal terminology, and contextual relationships within documents. The success of prompt engineering techniques like Chain-of-Thought allows models to break down complex legal reasoning into manageable steps, improving accuracy in tasks such as contract analysis and legal research. The combination of pre-training on vast legal corpora and fine-tuning on domain-specific data enables these models to capture nuanced legal concepts and relationships that traditional rule-based systems struggle to represent.

## Foundational Learning
- **Legal Domain Knowledge**: Essential for understanding specialized terminology and context; quick check: test model performance on domain-specific vs general vocabulary
- **Prompt Engineering Techniques**: Critical for optimizing model performance; quick check: compare few-shot vs zero-shot learning results on identical tasks
- **Evaluation Metrics**: Necessary for objective performance assessment; quick check: validate ROUGE/BLEU scores against human judgment
- **Model Fine-tuning**: Important for adapting general models to legal tasks; quick check: measure performance improvement after domain-specific fine-tuning
- **Hallucination Detection**: Crucial for ensuring reliability in legal applications; quick check: implement automated fact-checking mechanisms

## Architecture Onboarding
Component Map: Legal Document -> Preprocessing -> LLM (Specialized/General) -> Prompt Engineering -> Output Generation -> Evaluation Metrics
Critical Path: Document Input → Preprocessing → Prompt Formulation → Model Processing → Result Validation
Design Tradeoffs: Specialized models offer better accuracy but require more resources; general models are more accessible but less precise
Failure Signatures: Hallucinations in legal reasoning, bias in document interpretation, performance degradation on complex multi-step tasks
First Experiments:
1. Compare few-shot learning performance between Legal-Pegasus and GPT-4 on contract summarization
2. Test Chain-of-Thought prompting effectiveness on multi-step legal reasoning tasks
3. Evaluate hallucination rates across different prompt engineering techniques

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to SCOPUS and Springer databases, potentially missing relevant research from other sources
- Small sample size of 17 articles may not capture the full landscape of LLM applications in legal domains
- Focus on technical aspects without discussing real-world implementation challenges or ethical considerations in depth

## Confidence
- High confidence in technical findings regarding model performance and prompt engineering techniques
- Medium confidence in generalizability of results due to limited sample size
- Medium confidence in comparative analysis between specialized and general-purpose LLMs
- Low confidence in conclusions about practical implementation challenges without broader context

## Next Checks
1. Expand literature search to include additional databases (IEEE Xplore, ACM Digital Library, SSRN) and pre-print repositories to verify if the 17 articles represent a comprehensive sample
2. Conduct validation studies comparing specialized legal models against general-purpose LLMs on identical legal document tasks with standardized benchmarks
3. Perform error analysis on identified model hallucinations and biases to determine if they stem from training data limitations or prompt engineering issues