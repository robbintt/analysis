---
ver: rpa2
title: Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units
arxiv_id: '2512.22599'
source_url: https://arxiv.org/abs/2512.22599
tags:
- price
- bitcoin
- network
- prediction
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a Parallel Gated Recurrent Units (PGRU)\
  \ model for cryptocurrency price prediction, leveraging dual-stream recurrent networks\
  \ to process both price and structural features. The architecture employs two independent\
  \ GRU networks\u2014one analyzing historical price data and the other examining\
  \ blockchain structural characteristics\u2014followed by a fusion network to combine\
  \ their outputs into a final prediction."
---

# Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units

## Quick Facts
- arXiv ID: 2512.22599
- Source URL: https://arxiv.org/abs/2512.22599
- Authors: Milad Asadpour; Alireza Rezaee; Farshid Hajati
- Reference count: 40
- Key outcome: Parallel Gated Recurrent Units (PGRU) model achieves MAPE of 3.243% and 2.641% for window lengths of 20 and 15 days respectively on Bitcoin price prediction

## Executive Summary
This study introduces a Parallel Gated Recurrent Units (PGRU) model for cryptocurrency price prediction, leveraging dual-stream recurrent networks to process both price and structural features. The architecture employs two independent GRU networks—one analyzing historical price data and the other examining blockchain structural characteristics—followed by a fusion network to combine their outputs into a final prediction. Experiments using Bitcoin data demonstrate that the PGRU model achieves mean absolute percentage errors (MAPE) of 3.243% and 2.641% for window lengths of 20 and 15 days, respectively, outperforming existing methods while reducing computational cost and input dimensionality. The model's parallel design enables more accurate pattern capture and robust forecasting compared to single-stream approaches.

## Method Summary
The PGRU model processes Bitcoin price data through a dual-stream architecture where two independent GRU networks analyze price features (average, open, low, high) and blockchain structural features (block size, hash rate, difficulty, transactions, miner revenue) separately before a fusion network combines their predictions. The model uses z-score normalization for input features and employs 10-fold cross-validation with 80/20 training/test splits. Key hyperparameters include 2 GRU layers per stream, Adam optimizer for the GRU networks, and Levenberg–Marquardt algorithm for the fusion network. The model was trained on Bitcoin data from January 2016 to November 2021.

## Key Results
- PGRU achieves MAPE of 3.243% for window length 20 days and 2.641% for window length 15 days
- Multi-day forecasting shows error accumulation, reaching 10.24% by day 10
- The model demonstrates lower MAPE than existing methods while reducing computational cost and input dimensionality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel processing of distinct feature streams improves prediction accuracy by enabling specialized temporal learning in each stream before fusion.
- Mechanism: Two independent GRU networks separately learn temporal patterns from price features (average, open, low, high) and blockchain structural features (block size, hash rate, difficulty, transactions, miner revenue). This separation prevents feature interference and allows each GRU to specialize in its domain before the fusion stage combines their predictions.
- Core assumption: Price-related features and blockchain structural features encode complementary predictive information with different temporal dynamics.
- Evidence anchors:
  - [abstract] "The parallel networks utilize different inputs, each representing distinct price-related features."
  - [section 4.2] "Two parallel GRU-based recurrent networks, each responsible for capturing patterns in one input stream (price features or structural features)."
  - [corpus] Adjacent work on hybrid models (LSTM+XGBoost, Transformer+GRU) similarly separates feature processing streams, though not with parallel GRUs specifically.
- Break condition: If price and structural features become highly correlated (redundant information), the parallel architecture provides diminishing returns over single-stream models.

### Mechanism 2
- Claim: A learned fusion network adaptively weights stream predictions based on their reliability, outperforming fixed combination schemes.
- Mechanism: The feedforward fusion network receives scalar predictions from both GRU streams and learns to weight them according to their historical error characteristics. This enables dynamic adjustment—one stream may be more reliable during certain market conditions.
- Core assumption: The two streams exhibit different error profiles that are themselves predictable from the stream outputs.
- Evidence anchors:
  - [abstract] "The outputs of the parallel networks are combined by a neural network to forecast the future price."
  - [section 4.2] "The fusion network implicitly learns how to weight and combine the two GRU outputs, acting as an ensemble aggregator."
  - [corpus] Evidence is limited; corpus papers focus on single-model architectures without explicit learned fusion stages.
- Break condition: If both streams produce systematically similar predictions with correlated errors, the fusion network adds complexity without benefit.

### Mechanism 3
- Claim: Z-score normalization preserves meaningful variation in high-variance financial data better than min-max scaling.
- Mechanism: Bitcoin prices and structural features span orders of magnitude. Min-max normalization compresses disparate values into a narrow interval, obscuring fluctuations. Z-score normalization (centering by mean, scaling by standard deviation) maintains relative variation, enabling the network to detect patterns more effectively.
- Core assumption: Relative deviations from the mean carry more predictive signal than absolute position within a fixed range.
- Evidence anchors:
  - [abstract] Not explicitly mentioned in abstract.
  - [section 3.2] "This transformation centers the data and scales it by its standard deviation, leading to better separation between values."
  - [section 6] "This choice improves the separation of values and, in our experiments, increases predictive accuracy by approximately 5–7 percentage points relative to simple min–max scaling."
  - [corpus] Normalization strategy is not discussed in corpus papers.
- Break condition: If input distributions are highly non-Gaussian or contain extreme outliers, z-score normalization may still compress meaningful variation.

## Foundational Learning

- Concept: Gated Recurrent Units (GRUs)
  - Why needed here: The core prediction engine uses GRU layers to process sequential time-series data. Understanding gating mechanisms (update and reset gates) is essential for debugging temporal learning behavior.
  - Quick check question: Can you explain how the reset gate determines which past information to forget versus retain?

- Concept: Sliding Window Time-Series Construction
  - Why needed here: The model uses a window of past days (w ∈ {5,10,15,20,25}) to predict the next day's price. Understanding how window length affects pattern capture is critical for hyperparameter selection.
  - Quick check question: Given a dataset of 1,859 days and window length w=15, how many training samples can be constructed?

- Concept: Ensemble Fusion and Meta-Learning
  - Why needed here: The fusion network operates as a learned ensemble aggregator. Understanding how to train such a network without overfitting to the GRU outputs is important.
  - Quick check question: Should the fusion network be trained jointly with the GRU streams or separately? What are the tradeoffs?

## Architecture Onboarding

- Component map:
  - Stream 1 (Price GRU): Input [w × 4] → GRU layers → Fully connected → Scalar prediction
  - Stream 2 (Structural GRU): Input [w × 5] → GRU layers → Fully connected → Scalar prediction
  - Fusion Network: [prediction_1, prediction_2] → Feedforward layers → Final price output
  - Optimizers: Adam for GRU streams, Levenberg–Marquardt for fusion network

- Critical path: Raw price/structural data → Z-score normalization → Sliding window construction → Parallel GRU processing → Fusion network → Denormalized prediction

- Design tradeoffs:
  - GRU vs LSTM: GRU chosen for ~15% faster training with comparable accuracy (Table 6)
  - Window length: Shorter windows (w=15) yield lower MAPE (2.64%) but may miss longer-term patterns; longer windows (w=20) increase error (3.24%)
  - Feature separation vs concatenation: Parallel streams reduce input dimensionality per network but require additional fusion training

- Failure signatures:
  - Predictions lag behind sharp price spikes (visible in Figure 4/5 during high-volatility periods)
  - Multi-day forecasting error accumulates rapidly (Day 10 error reaches 10.24% in Table 3)
  - If fusion network overfits to one stream, the other stream's signal is effectively ignored

- First 3 experiments:
  1. **Baseline replication**: Implement PGRU with w=15 on the provided Bitcoin dataset (2016–2021), verify MAPE ≈ 2.6% using 10-fold cross-validation.
  2. **Ablation study**: Train single-stream models (price-only GRU, structural-only GRU) and compare against fused output to quantify fusion benefit.
  3. **Normalization comparison**: Swap z-score for min-max normalization and measure accuracy degradation to validate the claimed 5–7% improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- The model's reliance on Bitcoin-specific data from 2016-2021 constrains generalizability to other cryptocurrencies or market conditions
- The reported improvements over baseline methods lack detailed comparative analysis with specific alternative approaches
- The fusion network's architecture and training procedure remain underspecified, making replication challenging

## Confidence
- **High Confidence**: The parallel GRU architecture design and its basic implementation are well-described and technically sound. The normalization strategy's impact on performance is empirically validated.
- **Medium Confidence**: The claimed MAPE improvements and computational efficiency gains are supported by the reported results, though the lack of detailed baseline comparisons limits certainty. The mechanism by which parallel streams capture complementary patterns is theoretically plausible but not rigorously proven.
- **Low Confidence**: The fusion network's contribution to overall performance is difficult to quantify precisely due to limited ablation study details. The model's behavior in cross-cryptocurrency scenarios remains entirely unknown.

## Next Checks
1. **Statistical Significance Testing**: Conduct paired t-tests comparing PGRU against LSTM, GRU, and hybrid models across multiple random data splits to verify that reported MAPE improvements are statistically significant rather than due to random variation.
2. **Cross-Cryptocurrency Validation**: Apply the trained PGRU model to Ethereum, Litecoin, and other cryptocurrency price datasets without retraining to assess generalization capabilities and identify architecture limitations.
3. **Feature Importance Analysis**: Perform sensitivity analysis by systematically removing individual features from both price and structural streams to quantify each feature's contribution to prediction accuracy and validate the parallel architecture's necessity.