---
ver: rpa2
title: Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time
  Recurrent RL
arxiv_id: '2602.02236'
source_url: https://arxiv.org/abs/2602.02236
tags:
- learning
- online
- fine-tuning
- recurrent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to online fine-tuning of pretrained
  driving policies using Real-Time Recurrent Reinforcement Learning (RTRRL), enabling
  continuous adaptation to changing environments without retraining from scratch.
  The method combines offline behavioral cloning with online RTRRL-based fine-tuning,
  leveraging biologically plausible gradient computation methods (RTRL/RFLO) for recurrent
  neural networks.
---

# Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL

## Quick Facts
- arXiv ID: 2602.02236
- Source URL: https://arxiv.org/abs/2602.02236
- Reference count: 37
- One-line primary result: Online fine-tuning of pretrained driving policies using RTRRL enables continuous adaptation to changing environments without retraining from scratch.

## Executive Summary
This paper presents an approach to online fine-tuning of pretrained driving policies using Real-Time Recurrent Reinforcement Learning (RTRRL), enabling continuous adaptation to changing environments without retraining from scratch. The method combines offline behavioral cloning with online RTRRL-based fine-tuning, leveraging biologically plausible gradient computation methods (RTRL/RFLO) for recurrent neural networks. Experiments in both simulated CarRacing and real-world line-following tasks with event camera input demonstrate significant performance improvements through fine-tuning.

## Method Summary
The approach uses behavioral cloning to pretrain a CNN encoder and RNN policy on human demonstration data, then deploys the policy with online RTRRL updates. RTRRL computes gradients in a single forward pass using RTRL for diagonal architectures or RFLO for non-diagonal ones, combined with TD(λ) eligibility traces for credit assignment. A parameter change penalty prevents overfitting during single-environment fine-tuning. The method was evaluated on CarRacing simulation and a real-world line-following task with event camera input.

## Key Results
- LRC models showed the largest improvement in evaluation reward during CarRacing fine-tuning
- Real-world line-following experiments showed immediate improvements in the first lap of fine-tuning
- The approach effectively addresses distribution shift challenges in autonomous driving applications
- RTRRL's hyperparameter sensitivity requires careful tuning for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Forward-Only Gradient Computation via RTRL/RFLO
RTRRL enables step-wise parameter updates by computing recurrent gradients in a single forward pass, eliminating the memory bottleneck of backpropagation through time (BPTT). RTRL maintains an approximate Jacobian trace $\hat{J}_t$ updated during forward computation, allowing gradient updates at each step. For diagonal architectures (LrcSSM, LRU), this becomes tractable. RFLO approximates RTRL with $O(n^2)$ complexity by using fixed random feedback matrices instead of exact gradients.

### Mechanism 2: Eligibility Traces Bridge Temporal Credit Assignment
TD(λ) with eligibility traces enables online credit assignment for delayed rewards without episode rollouts. Eligibility traces $e_\theta$ accumulate gradient history with decay factor $\gamma\lambda$. When a TD-error $\delta_t$ arrives, it multiplies accumulated traces to update parameters, propagating reward signals backward through time causally.

### Mechanism 3: Parameter Change Penalty Constrains Online Overfitting
L2 regularization toward pretrained parameters prevents catastrophic forgetting during single-environment fine-tuning. Penalty $L_\theta = \beta\|\theta_{pre} - \theta_t\|^2$ anchors updates near the pretrained initialization, balancing plasticity with stability.

## Foundational Learning

- **Temporal-Difference Learning with Eligibility Traces (TD(λ))**
  - Why needed here: RTRRL's core update rule; understanding how $\lambda$ affects credit assignment is critical for tuning.
  - Quick check question: Can you explain why TD(0) struggles with delayed rewards and how eligibility traces address this?

- **Real-Time Recurrent Learning (RTRL)**
  - Why needed here: Distinguishes RTRRL from BPTT-based methods; explains why diagonal architectures are preferred.
  - Quick check question: What is the memory complexity of RTRL for a network with n units, and why does diagonal connectivity help?

- **Behavioral Cloning Limitations**
  - Why needed here: Motivates the hybrid approach; understanding distribution shift helps diagnose when fine-tuning is necessary.
  - Quick check question: Why does a policy trained via behavioral cloning often fail when deployed, even with low training loss?

## Architecture Onboarding

- **Component map**: CNN Encoder -> RNN Policy/CNN Encoder -> RNN Critic -> Action Sampling/Value Estimation
- **Critical path**: Pretrain CNN encoder + RNN policy via behavioral cloning → Freeze encoder → Initialize critic randomly → Deploy with RTRRL updates at every timestep → Monitor TD-error and trace magnitudes
- **Design tradeoffs**: Separate RNNs for policy/critic increases parameters but may stabilize learning; RTRL exact for diagonal models while RFLO required for non-diagonal introduces approximation error; encoder freezing implicit via low learning rates
- **Failure signatures**: Immediate performance collapse (check learning rates), high variance across seeds (check λ and γ alignment), no improvement after several laps (check eligibility trace decay), LRU models underperforming (use CT-RNN or LrcSSM instead)
- **First 3 experiments**: 1) Pretraining validation: train CNN autoencoder + RNN policy on behavioral cloning dataset and confirm convergence; 2) Simulation fine-tuning sweep: in CarRacing, run ablation over actor learning rates and λ with 3 seeds each; 3) Real-world sanity check: deploy best pretrained LrcSSM model on LineTracking without fine-tuning, then enable RTRRL for 5 laps

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameter sensitivity requires extensive tuning for optimal performance
- Computational complexity of RTRL for non-diagonal architectures necessitates RFLO approximations
- Real-world experiment limited to simplified line-following rather than full autonomous driving

## Confidence

**High Confidence**: The mechanism of eligibility traces for temporal credit assignment and the effectiveness of parameter change penalties for preventing overfitting are well-established and directly supported by empirical results.

**Medium Confidence**: The computational efficiency claims for RTRL/RFLO methods are plausible based on cited complexity analyses, but RFLO approximation quality degradation on longer tasks introduces uncertainty.

**Low Confidence**: Generalization from simplified line-following to full autonomous driving remains uncertain without testing on more complex scenarios.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct systematic grid search over actor learning rates (10^-7 to 10^-5) and λ values (0.8 to 0.99) with 5 seeds per configuration to quantify performance variance.

2. **Long-Horizon Task Evaluation**: Test RFLO approximations on tasks requiring >100 timesteps to measure performance degradation compared to RTRL on diagonal architectures.

3. **Real-World Complexity Scaling**: Deploy best-performing model from CarRacing experiments on a more complex real-world driving scenario (e.g., with obstacle avoidance) to assess transfer beyond line-following.