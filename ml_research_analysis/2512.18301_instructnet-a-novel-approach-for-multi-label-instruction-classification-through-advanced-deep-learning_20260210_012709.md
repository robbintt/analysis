---
ver: rpa2
title: 'InstructNet: A Novel Approach for Multi-Label Instruction Classification through
  Advanced Deep Learning'
arxiv_id: '2512.18301'
source_url: https://arxiv.org/abs/2512.18301
tags:
- text
- bert
- accuracy
- xlnet
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose InstructNet, a transformer-based architecture
  leveraging XLNet for multi-label instruction classification using wikiHow articles.
  The dataset consists of 11,121 observations from wikiHow with multiple categories
  per record.
---

# InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning

## Quick Facts
- **arXiv ID**: 2512.18301
- **Source URL**: https://arxiv.org/abs/2512.18301
- **Reference count**: 40
- **Primary result**: XLNet achieves 97.30% accuracy and 93% macro F1 for multi-label instruction classification on wikiHow articles

## Executive Summary
InstructNet presents a transformer-based architecture leveraging XLNet for multi-label instruction classification using wikiHow articles. The approach addresses the challenge of categorizing instructional text into multiple relevant categories simultaneously. The authors develop a comprehensive evaluation framework and demonstrate state-of-the-art performance through careful data preparation and model selection. The work provides insights into handling multi-label classification tasks with imbalanced label distributions and shows that XLNet's permutation language modeling offers advantages for instructional text classification over other transformer models.

## Method Summary
The method involves fine-tuning XLNet with sigmoid activation for multi-label output prediction using binary cross-entropy loss. The HowSumm dataset of 11,121 wikiHow observations is preprocessed through label filtering (threshold=500 observations per label), reducing the label space from 6000+ to 67 categories. Text preprocessing includes removing special characters, stopwords, lemmatization, and lowercasing. The model uses binary label encoding where each observation becomes a 67-element binary vector, with independent binary classification for each label. XLNet is trained with AdamW optimizer (LR=4e-04), batch size 48, max_length=512, and 40 epochs, evaluated using binary accuracy and macro/micro F1 scores.

## Key Results
- XLNet achieves 97.30% accuracy and 93% macro F1 score on the multi-label instruction classification task
- Binary accuracy and macro F1 score are identified as the most effective metrics for evaluating model performance
- XLNet outperforms other transformer models including BERT, ELECTRA, RoBERTa, and DistilBERT on this task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** XLNet's permutation language modeling provides superior bidirectional context capture compared to BERT's masked language modeling for instructional text classification.
- **Mechanism:** XLNet samples all possible factorization orders of a sequence, training the model to predict each token given all possible contexts (both left and right), rather than BERT's approach of masking 15% of tokens and assuming independence. This uses two-stream self-attention with separate query (g) and content (h) representations, allowing the model to attend to full bidirectional context without the pretrain-finetune discrepancy BERT introduces via [MASK] tokens.
- **Core assumption:** Instructional text contains long-range dependencies where understanding task steps requires bidirectional context (e.g., "add middle initial" makes sense only after seeing "type author first initial").
- **Evidence anchors:**
  - [Section 3.1.3] Equations 1-3 describe the permutation objective: max_θ E_z∼z_T [Σ_t log p_θ(x_t|x_<t)] with two-stream attention parameters reformulated as g_z_t^(m) and h_z_t^(m).
  - [Section 5.2] XLNet achieves 97.30% accuracy vs BERT's 97.17%, with macro F1 of 93% vs BERT's lower score.
  - [Corpus] Weak direct corpus support—neighbor papers focus on multi-label classification but not XLNet specifically; no comparative PLM studies found.
- **Break condition:** If instructional texts are short (<50 tokens) with primarily local dependencies, PLM's computational overhead may not justify marginal gains over simpler autoencoding models.

### Mechanism 2
- **Claim:** Frequency-based label filtering with threshold=500 mitigates class imbalance in multi-label classification by ensuring sufficient positive examples per class.
- **Mechanism:** Algorithm 1 computes Selection Score = total_label_observations / 500, retaining only labels scoring ≥1. This reduces the label space from 6000+ to 67 well-represented categories (Table 1), preventing the classifier from being dominated by rare labels with insufficient training signal.
- **Core assumption:** Labels with <500 observations cannot provide enough positive examples for the model to learn meaningful patterns, and their removal does not critically harm task utility.
- **Evidence anchors:**
  - [Section 3.2.1] "Most of these labels don't have enough observations, which tends to imbalance the dataset and affect the classifier's performance."
  - [Table 1] Shows the 67 retained labels including "Education and Communications," "Health," "Home and Garden"—all broad, well-populated categories.
  - [Corpus] Neighbor paper "Improving Accuracy and Efficiency of Legal Document Tagging" explicitly addresses "significant label imbalance" as a core challenge in multi-label classification, supporting this mechanism's relevance.
- **Break condition:** If downstream applications require fine-grained rare category detection (e.g., "Horses" with few examples), this filtering approach would systematically exclude those labels, requiring hierarchical or few-shot alternatives.

### Mechanism 3
- **Claim:** Binary label encoding with sigmoid activation enables independent multi-label prediction by treating each of 67 labels as a separate binary classification problem.
- **Mechanism:** Each observation's labels become a 67-element binary vector; sigmoid activation on the final layer outputs independent probabilities per label; binary cross-entropy loss (Equation 5: L_Binary = -1/N Σ[y_i·log(p(y_i)) + (1-y_i)·log(1-p(y_i))]) trains each label's classifier independently.
- **Core assumption:** Label correlations in instructional text are sufficiently captured by shared hidden representations without explicit label dependency modeling.
- **Evidence anchors:**
  - [Section 3.2.1] "For each text, we have created a binary list with the length of the total selected levels... the array value will be one based on the label's number; otherwise, the value is 0."
  - [Section 3.2.2] "We use the 'Sigmoid' activation function here in both models."
  - [Corpus] "ProtoECGNet" paper uses contrastive learning for multi-label ECG classification, suggesting label correlation modeling may improve over independent prediction—but not tested here.
- **Break condition:** If strong label co-occurrence patterns exist (e.g., "Dog Training" and "Pets and Animals" always co-occur), independent prediction may produce inconsistent label sets; classifier calibration would be needed.

## Foundational Learning

- **Concept: Permutation Language Modeling vs. Masked Language Modeling**
  - **Why needed here:** XLNet's advantage over BERT stems from its PLM pretraining objective; understanding this difference is essential to explain performance gaps and justify model selection.
  - **Quick check question:** Given sequence [A, B, C, D] and factorization order [3, 1, 4, 2], which tokens can attend to which when predicting position 3?

- **Concept: Binary Cross-Entropy for Multi-Label Classification**
  - **Why needed here:** The paper uses BCE loss rather than categorical cross-entropy; understanding why independent binary losses work for multi-label tasks is foundational.
  - **Quick check question:** If ground truth is [1, 0, 1] and predictions are [0.9, 0.3, 0.2], compute the BCE loss for this single sample.

- **Concept: Macro vs. Micro F1 in Multi-Label Settings**
  - **Why needed here:** The paper reports both (macro=93%, micro=89.02%); the gap indicates per-class performance variation and class imbalance effects.
  - **Quick check question:** If Class A has 1000 positives and Class B has 100 positives, and both achieve 90% precision/recall, will macro F1 equal micro F1?

## Architecture Onboarding

- **Component map:** Input Text → Preprocessing (lowercase, remove special chars, stop words, lemmatize) → Tokenizer (XLNet tokenizer with [CLS], [PAD]) → [input_ids, attention_mask, segment_ids] → XLNet Encoder (pretrained, fine-tuned) → Fully Connected Layer (hidden_size → 67) → Sigmoid Activation → Binary Predictions (threshold at 0.5)

- **Critical path:**
  1. Data preparation: Apply Algorithm 1 with threshold=500 to filter labels; binarize remaining 67 labels
  2. Tokenization: Use `XLNetTokenizer` with max_length=512; truncate longer sequences
  3. Training loop: Binary cross-entropy loss, AdamW optimizer (lr=4e-04), batch_size=48, 40 epochs
  4. Evaluation: Binary accuracy, macro/micro F1, precision, recall

- **Design tradeoffs:**
  - **max_length=512 vs. longer:** Paper claims XLNet has no length limit but uses 512 for fair BERT comparison; Assumption: most instructional text fits within 512 tokens
  - **threshold=500:** Higher threshold reduces label space but risks excluding useful categories; lower threshold increases label space but introduces class imbalance
  - **Binary encoding vs. label embeddings:** Binary encoding is simple but scales poorly to >100 labels; label embeddings would capture semantic relationships but add complexity

- **Failure signatures:**
  - Training accuracy >> testing accuracy with diverging loss curves → overfitting; reduce epochs or add dropout
  - Macro F1 << Micro F1 → model biased toward high-frequency labels; increase threshold or use class-weighted loss
  - All predictions near 0.5 → sigmoid outputs not calibrated; check learning rate or increase training data

- **First 3 experiments:**
  1. **Baseline reproduction:** Implement XLNet with paper's hyperparameters (lr=4e-04, max_length=512, batch_size=48) on the 67-label filtered dataset; verify accuracy ~97.3% and macro F1 ~93%
  2. **Ablation on label threshold:** Test threshold values [250, 500, 750, 1000]; measure impact on label count, class balance, and macro F1 to validate the 500 threshold choice
  3. **Sequence length sensitivity:** Compare max_length=[128, 256, 512, 768] on XLNet (no fixed limit) vs. BERT (512 max); quantify performance degradation from truncation to understand if instructional text length distribution justifies XLNet's advantage

## Open Questions the Paper Calls Out
- **Question:** How does InstructNet perform when applied to the "method" sections of the HowSumm dataset, which contain longer text sequences than the "step" data currently utilized?
  - **Basis in paper:** [explicit] The authors explicitly state in the conclusion that they aim to "optimize the method section of the HowSumm data, which contains more extensive sequences than the step data."
  - **Why unresolved:** The current study restricts its methodology to the "step" portion of the dataset, leaving the handling of longer, extended sequences unexplored.
  - **What evidence would resolve it:** Evaluation results (Accuracy, Macro F1) of the current XLNet-based architecture specifically on the "method" subset of HowSumm.

- **Question:** Can alternative label encoding techniques reduce data sparsity more effectively than binary encoding while maintaining the high accuracy achieved in this study?
  - **Basis in paper:** [explicit] The authors note in the conclusion that they "will address these issues by implementing a more efficient label encoding technique and reducing the amount of sparse data."
  - **Why unresolved:** The research relied solely on binary label encoding, which creates sparse 67-length arrays; no alternative encoding strategies were tested.
  - **What evidence would resolve it:** A comparative analysis of label encoding methods (e.g., Label Powerset) demonstrating reduced sparsity without loss in F1 scores.

- **Question:** How does the arbitrary observation threshold (500) used in the data preparation algorithm impact the model's ability to classify rare or fine-grained instructional categories?
  - **Basis in paper:** [inferred] The paper acknowledges removing labels with fewer than 500 observations to mitigate class imbalance, but this creates a trade-off between statistical balance and label coverage for the "long-tail" of instructions.
  - **Why unresolved:** The study does not evaluate the model's performance on the excluded low-frequency labels, leaving their feasibility for classification unknown.
  - **What evidence would resolve it:** An ablation study testing lower thresholds or few-shot learning approaches on the discarded labels to see if performance degrades significantly.

## Limitations
- The aggressive label filtering (threshold=500) significantly reduces the label space from 6000+ to 67 categories, potentially missing fine-grained instructional categories
- The study doesn't empirically validate the assumption that instructional text length is manageable within 512 tokens or test longer sequences
- Binary cross-entropy treats labels independently without modeling label correlations, which could lead to inconsistent multi-label predictions when strong label dependencies exist

## Confidence
- **High confidence**: The implementation of XLNet for multi-label classification using binary cross-entropy loss and sigmoid activation is technically sound and well-established in the literature. The reported accuracy (97.30%) and macro F1 (93%) scores are internally consistent with the described methodology.
- **Medium confidence**: The claim that XLNet outperforms BERT, ELECTRA, RoBERTa, and DistilBERT specifically due to permutation language modeling requires more rigorous ablation studies. While performance differences are reported, the mechanism explaining why PLM is superior for instructional text is largely theoretical without empirical validation.
- **Low confidence**: The generalizability of results to other instructional domains or languages beyond wikiHow articles, given the specific characteristics of the filtered 67-label dataset and English-only corpus.

## Next Checks
1. **Ablation study on label filtering threshold**: Systematically test threshold values [250, 500, 750, 1000] to measure the impact on macro F1 and per-class performance, validating whether the 500 threshold optimally balances label granularity versus class balance.
2. **Permutation vs. masked language modeling comparison**: Implement a controlled experiment where BERT and XLNet use identical architectures except for their pretraining objectives, then evaluate on the same 67-label task to isolate the contribution of PLM to the reported performance gains.
3. **Long sequence handling validation**: Compare performance using max_length=[128, 256, 512, 768] on XLNet versus BERT (512 max) to quantify truncation effects and empirically verify whether instructional text length justifies XLNet's advantage.