---
ver: rpa2
title: Identifying Gender Stereotypes and Biases in Automated Translation from English
  to Italian using Similarity Networks
arxiv_id: '2502.11611'
source_url: https://arxiv.org/abs/2502.11611
tags:
- bias
- gender
- words
- similarity
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Identifying Gender Stereotypes and Biases in Automated Translation from English to Italian using Similarity Networks

## Quick Facts
- **arXiv ID:** 2502.11611
- **Source URL:** https://arxiv.org/abs/2502.11611
- **Reference count:** 40
- **Key outcome:** Gender bias detected in English-to-Italian MT through geometric skew in word embedding similarity networks

## Executive Summary
This paper presents a method to detect and quantify gender bias in machine translation from English to Italian using similarity networks constructed from word embeddings. The approach measures semantic proximity between words and gender-specific target terms (she/he, lei/lui) using cosine similarity, then calculates bias intensity and direction through absolute and signed differences in these similarities. By analyzing the geometric relationships in the embedding space before and after translation, the authors identify systematic masculine defaults in grammatical gender resolution and quantify the strength of stereotypical associations.

## Method Summary
The method extracts 333 unique words from 236 analogy pairs in Bolukbasi et al. [14], computes multilingual FastText embeddings for these words, and calculates cosine similarity with gender target terms in both English and Italian. The words are then translated to Italian using Google Translate API, and similarity calculations are repeated. Two key features are derived: GenderBiasIntensity (absolute difference in similarities to she/he or lei/lui) and GenderBiasDirection (signed difference). The analysis reveals bias patterns through binned classification of intensity scores and examination of translation gender assignments.

## Key Results
- Nurse shows strong association with "she" (0.556) vs "he" (0.241), indicating occupational gender stereotyping
- 91 neutral English words translated to masculine vs 32 to feminine forms in Italian, demonstrating masculine default bias
- GenderBiasIntensity successfully separates explicitly gendered terms (high intensity) from stereotyped terms (medium intensity)

## Why This Works (Mechanism)

### Mechanism 1
If bias exists in training corpora, it manifests as measurable geometric skew in word embedding vector spaces. Word embedding models map semantic relationships to vector distances, allowing quantification of stereotypical associations through cosine similarity. The vector geometry accurately reflects the statistical distribution of the training corpus, with cosine similarity effectively capturing semantic closeness independent of vector magnitude.

### Mechanism 2
Calculating the absolute difference between similarities to opposing gender targets isolates bias intensity from general semantic relatedness. By defining GenderBiasIntensity as |cos(w, she) - cos(w, he)|, the mechanism strips away baseline similarity a word might have with both genders, highlighting the differential skew. The target pair defines a stable "gender direction" axis in the semantic space.

### Mechanism 3
Translating from a notional gender language (English) to a grammatical gender language (Italian) forces a "gender resolution" step that exposes structural model preferences. When English neutral terms enter Italian, the MT system must select a gendered inflection, typically defaulting to the statistically dominant masculine form and shifting the GenderBiasDirection.

## Foundational Learning

- **Concept: Cosine Similarity**
  - **Why needed here:** This is the core metric used to convert semantic meaning into a number. Without understanding vector angles vs. magnitude, you cannot interpret the bias scores.
  - **Quick check question:** Why is cosine similarity preferred over Euclidean distance for measuring semantic likeness in high-dimensional word embeddings?

- **Concept: Grammatical vs. Notional Gender**
  - **Why needed here:** The paper's central RQ relies on the friction between English (notional) and Italian (grammatical) systems. You must understand why "the doctor" forces a choice in Italian.
  - **Quick check question:** In the context of this paper, why does the "inclusive masculine" rule in Italian pose a challenge for gender-neutral translation?

- **Concept: Word Embeddings (FastText)**
  - **Why needed here:** The paper attributes bias partly to the embedding method (FastText) rather than just the translation algorithm. Understanding subword embeddings explains why the authors chose this for a multilingual study.
  - **Quick check question:** How does FastText's use of subword information differ from Word2Vec, and why does the paper claim this makes it suitable for multilingual analysis?

## Architecture Onboarding

- **Component map:** 333-word list (from Bolukbasi analogies) -> FastText Embeddings (Vectorization) -> Google Translate API (Translation) -> Similarity Network Construction -> GenderBiasIntensity/Direction features -> Binned classification and scatter plot

- **Critical path:** The analysis depends on the sequence: Source Embedding -> Translation -> Target Embedding. If the translation API changes or the specific FastText model version is unavailable, results may not be reproducible.

- **Design tradeoffs:** FastText vs. Contextual Models - The paper uses static embeddings, sacrificing context sensitivity for computational efficiency and multilingual alignment capabilities. Single Word Analysis - Analyzing words in isolation isolates lexical bias but misses syntactic bias.

- **Failure signatures:** High Target Similarity - If cos(she, he) is too high (paper notes 0.61), the bias axis is compressed, making GenderBiasIntensity scores noisy. Unidirectional Shift - If 95%+ of neutral words map to masculine, the metric loses granularity.

- **First 3 experiments:**
  1. Reproduce the Scatter Plot - Take the top 10 biased words from Table 1, plot their (sim_he, sim_she) coordinates to visualize the deviation from the diagonal.
  2. Translation Tally - Translate a new set of 50 neutral profession words and calculate the M/F ratio to see if it matches the paper's ~3:1 masculine bias.
  3. Feature Robustness Check - Swap the target terms from "he/she" to "man/woman" to test if the GenderBiasIntensity metric remains stable or shifts significantly.

## Open Questions the Paper Calls Out

### Open Question 1
Do combinations of different word embedding methods (e.g., GloVe, Bag-of-Words) and translation models result in lower gender bias compared to the FastText and Google Translate pairing? The study restricted its methodology to FastText and Google Translate to prove the concept of similarity networks.

### Open Question 2
What specific methods can effectively mitigate gender bias in Machine Translation for grammatical gender languages like Italian? The current work focused on quantifying and identifying the intensity and direction of existing bias rather than developing debiasing algorithms.

### Open Question 3
How can Large Language Models be fine-tuned to automatically detect culturally contextual stereotypes and biases in Italian without relying on a full-fledged ontology? The current methodology relies on word lists and cosine similarity, which lack the cultural nuance required to understand complex discrimination without a pre-existing ontology.

## Limitations
- Vector Quality Dependency - The method assumes FastText embeddings capture semantic relationships accurately, but polysemy and low-frequency terms may introduce noise.
- Translation System Opacity - The Google Translate API choice introduces undocumented model versions and potential output changes over time.
- Sample Representativeness - The 333-word list may not capture the full spectrum of terms where gender bias manifests.

## Confidence
**High Confidence** in: The mathematical framework for quantifying bias intensity (GenderBiasIntensity = |cos(w,she) - cos(w,he)|) is sound and reproducible.

**Medium Confidence** in: The claim that translation from English to Italian "exposes structural model preferences" through forced gender resolution. While the statistical observation is documented, attribution to specific training data patterns versus algorithmic heuristics remains uncertain.

**Low Confidence** in: The assertion that FastText's subword information makes it "suitable for multilingual analysis" without empirical comparison to contextual models.

## Next Checks
1. **Axis Sensitivity Test** - Reproduce the scatter plot using the top 10 biased words from Table 1. Calculate the cosine similarity between "she" and "he" in the FastText model being used. If this value exceeds 0.7, the bias axis may be too compressed for reliable measurement.

2. **Translation Stability Audit** - Translate the same 50 neutral profession words across three different time periods (e.g., weekly intervals). Document whether the masculine/feminine ratio remains stable or fluctuates, indicating system opacity issues.

3. **Methodological Comparison** - Apply the same bias detection pipeline using a contextual embedding model (e.g., multilingual BERT) instead of FastText. Compare whether the top biased words and translation patterns remain consistent, or if static embeddings systematically underestimate or overestimate bias compared to context-aware representations.