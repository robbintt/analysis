---
ver: rpa2
title: 'One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise
  Model Merging'
arxiv_id: '2508.06163'
source_url: https://arxiv.org/abs/2508.06163
tags:
- tadrop
- merging
- task
- tasks
- sparsification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TADrop addresses the problem of parameter heterogeneity in model
  merging by introducing a tensor-wise adaptive sparsification strategy. Instead of
  applying a uniform sparsity ratio, TADrop assigns each parameter tensor a tailored
  drop rate based on its distributional properties, using a quantile ratio metric.
---

# One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging

## Quick Facts
- arXiv ID: 2508.06163
- Source URL: https://arxiv.org/abs/2508.06163
- Reference count: 40
- Primary result: TADrop achieves 2.0% average accuracy improvement over leading merging methods across 8 ViT-B/32 vision tasks

## Executive Summary
TADrop introduces a distribution-aware sparsification strategy for model merging that addresses the fundamental problem of parameter heterogeneity. Rather than applying uniform sparsity across all parameters, TADrop assigns each parameter tensor a tailored drop rate based on its distributional properties using a quantile ratio metric. This approach preserves critical sparse parameters while aggressively pruning redundant dense ones, consistently improving performance across vision, language, and multimodal tasks when integrated with state-of-the-art merging methods.

## Method Summary
TADrop processes task vectors (fine-tuned model weights minus pretrained weights) through a tensor-wise adaptive sparsification pipeline. For each tensor, it calculates a Quantile Ratio metric (Q0.50/Q0.95 of absolute values) to determine a unique sparsity rate, then drops the lowest magnitude parameters accordingly. Crucially, it preserves the L2 norm of each tensor through rescaling to prevent aggregation imbalances. The transformed vectors are then merged using standard methods like Task Arithmetic or EMR-Merging, achieving consistent performance gains across diverse model architectures and task types.

## Key Results
- Achieves 2.0% average accuracy improvement across 8 ViT-B/32 vision tasks when enhancing leading merging methods
- Demonstrates scalability with performance advantages widening from 8 to 30 merged tasks
- Maintains effectiveness across vision, language, and multimodal domains with different architectures (ViT-B/32, ViT-L/14, GPT-2, BEiT3)
- Ablation study confirms norm-preserving scaling is critical, with performance collapsing to 86.8% without it

## Why This Works (Mechanism)

### Mechanism 1: Tensor-wise Adaptive Sparsity via Quantile Ratio
TADrop assigns unique sparsity rates to each parameter tensor based on distributional properties, specifically heavy-tailedness. A lower Quantile Ratio indicates more critical information and less aggressive pruning, while higher ratios allow more aggressive pruning of redundant parameters. This exploits parameter heterogeneity that uniform methods miss.

### Mechanism 2: Norm-Preserving Scaling
Magnitude-based sparsification distorts tensor L2 norms, creating aggregation imbalances. TADrop restores the original L2 norm after sparsification to maintain parameter scale integrity during merging, preventing performance collapse.

### Mechanism 3: Parameter Heterogeneity Exploitation
Different parameter types exhibit distinct statistical distributions across tasks and layers. TADrop's tensor-wise approach leverages this heterogeneity by applying different sparsification patterns to different components (e.g., aggressive pruning of attention biases while preserving FFN weights), resolving suboptimal tradeoffs of uniform pruning.

## Foundational Learning

- **Task Vectors**: Why needed - Understanding that merging combines parameter modifications rather than full weights is fundamental. Quick check - How is a task vector computed from a fine-tuned model and its pretrained base?
- **Parameter Interference & Sparsification**: Why needed - Sparsification mitigates interference when merging multiple task vectors. Quick check - Why is pruning beneficial before merging?
- **Heavy-Tailed Distributions**: Why needed - The Quantile Ratio metric relies on parameter distributions often being heavy-tailed. Quick check - What does heavy-tailed mean and how might it relate to parameter importance?

## Architecture Onboarding

- **Component map:** Input K task vectors → TADrop Module (Quantile Calculator → Ratio Calculator → Sparsifier → Norm Rescaler) → Output K transformed tensor sets → Standard merging function
- **Critical path:** The per-tensor Quantile Ratio calculation is the core logic; incorrect implementation will propagate errors. Norm rescaling is also critical per ablation study.
- **Design tradeoffs:** Fixed vs. learned quantiles (simple plug-and-play vs. complex optimization), tensor vs. other granularity levels (performance vs. complexity).
- **Failure signatures:** Performance collapse (86.8% accuracy) without norm-preserving scaling, marginal gains on baselines with complex sparsification, sensitivity to extreme quantile choices.
- **First 3 experiments:**
  1. Reproduce core ablation by implementing TADrop with Task Arithmetic on 8 ViT-B/32 tasks, comparing full method vs. "W/o Scale" to confirm norm-preserving criticality
  2. Verify adaptive sparsity rates by logging and visualizing computed rates for all tensors, checking non-uniformity across component types
  3. Test on different baseline by integrating TADrop with a new task vector-based merging method to assess plug-and-play generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Core claims rely on distributional properties being predictive of importance, with evidence primarily observational from experiments
- Quantile Ratio metric (0.50/0.95) is heuristic rather than theoretically grounded
- Performance gains on larger-scale models (ViT-L/14, GPT-2, BEiT3) are less thoroughly validated than ViT-B/32 results
- Computational overhead of tensor-wise approach versus global methods is not explored

## Confidence
- **High Confidence**: Norm-preserving scaling mechanism's importance (clear ablation results showing performance collapse without it)
- **Medium Confidence**: Effectiveness of tensor-wise adaptive sparsification (consistent improvements across benchmarks, relies on distributional assumptions)
- **Medium Confidence**: Generalizability claim (success across domains but varying validation depth)

## Next Checks
1. **Distribution Stability Test**: Apply TADrop to same task vectors with different random seeds, analyze variance in computed sparsity rates and final performance
2. **Alternative Metric Comparison**: Replace Quantile Ratio with alternative distributional metrics (kurtosis, entropy) while keeping tensor-wise framework, compare performance
3. **Scaling Efficiency Analysis**: Measure wall-clock time and memory usage of TADrop preprocessing versus global methods across different model sizes, quantify computational overhead