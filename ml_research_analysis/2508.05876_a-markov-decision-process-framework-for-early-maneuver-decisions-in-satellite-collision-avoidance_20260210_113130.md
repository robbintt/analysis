---
ver: rpa2
title: A Markov Decision Process Framework for Early Maneuver Decisions in Satellite
  Collision Avoidance
arxiv_id: '2508.05876'
source_url: https://arxiv.org/abs/2508.05876
tags:
- policy
- collision
- propellant
- satellite
- maneuver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a Markov decision process framework to autonomously
  make satellite collision avoidance maneuver decisions, specifically addressing the
  trade-off between collision risk and propellant consumption by enabling earlier
  maneuver decisions. They modeled the problem as a continuous-state, discrete-action,
  finite-horizon MDP, trained a policy using reinforcement learning with policy gradients
  on historical conjunction data, and evaluated performance against conventional cutoff
  policies.
---

# A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance

## Quick Facts
- arXiv ID: 2508.05876
- Source URL: https://arxiv.org/abs/2508.05876
- Reference count: 10
- Primary result: Developed an RL-PG MDP framework for early satellite collision avoidance maneuver decisions

## Executive Summary
This paper addresses the challenge of autonomous satellite collision avoidance by developing a Markov decision process (MDP) framework that enables earlier maneuver decisions. The authors model the collision avoidance problem as a continuous-state, discrete-action, finite-horizon MDP where the agent must balance collision risk against propellant consumption. Using reinforcement learning with policy gradients trained on historical conjunction data, the framework learns to make maneuver decisions earlier in the conjunction timeline than traditional cutoff policies. The approach is evaluated on both synthetic and historical conjunction data, demonstrating improved propellant efficiency while maintaining safety margins.

## Method Summary
The method formulates satellite collision avoidance as a finite-horizon MDP with states defined by miss distance and along-track deviation standard deviation from conjunction data messages (CDMs). The agent chooses between maneuver or delay actions at each time step, with transitions modeled using fitted probability distributions (Generalized Normal and Non-Central T). The policy is trained using REINFORCE with ε-greedy exploration, optimizing a reward function that balances propellant consumption (via orbital mechanics) against collision risk (via PoC calculations). The trained policy is evaluated against conventional cutoff policies on synthetic CDMs and historical ESA conjunction data, measuring total propellant consumption, average propellant per maneuver, and confusion matrix accuracy.

## Key Results
- Trained policy consumed significantly less propellant overall and per maneuver than cutoff policies on synthetic conjunction events
- On historical ESA events, trained policy consumed less propellant per maneuver but more overall than cutoff policies
- Policy was slightly more conservative in identifying high-risk events warranting maneuvers
- Performance remained stable across parameter variations with a "stable-η region" (η ≤ 0.6)

## Why This Works (Mechanism)
The MDP framework works by formalizing the sequential decision problem under uncertainty inherent in collision avoidance. Rather than making a single binary decision at a fixed threshold, the agent can adapt its strategy as new CDM data arrives, potentially initiating maneuvers earlier when the evolving conjunction parameters suggest higher risk. The RL-PG algorithm learns this adaptive strategy by maximizing a reward that explicitly trades off the immediate cost of propellant consumption against the future risk of collision. This creates a data-driven policy that can optimize the timing of maneuvers rather than just their execution.

## Foundational Learning

**Markov Decision Process (MDP):**
- Why needed here: The collision avoidance decision is a sequential problem under uncertainty (evolving CDM data). The MDP provides the formal structure to model states (conjunction parameters), actions (maneuver/delay), and stochastic transitions over time.
- Quick check question: Can you explain why a fixed "if-then" rule is insufficient for a problem where the underlying collision risk data changes every few hours until a deadline?

**Reinforcement Learning (Policy Gradient / REINFORCE):**
- Why needed here: The optimal policy is not known a priori. RL provides the data-driven method to learn a policy that maximizes a long-term reward (balancing propellant and risk) by interacting with a simulated environment derived from historical data.
- Quick check question: In the REINFORCE algorithm, what does the gradient of the log-probability of an action represent in the context of updating the policy?

**Probability of Collision (PoC) and Covariance:**
- Why needed here: The core input to the decision model is collision risk. Understanding how PoC is calculated from miss distance and position covariance (uncertainty) is essential to interpret the state space and the risk reward signal.
- Quick check question: If the position covariance of the debris increases while the miss distance stays the same, how does the Probability of Collision typically change?

## Architecture Onboarding

**Component Map:**
The system consists of four primary components: (1) A CDM Data Processor that extracts and formats state variables (miss distance ρ, along-track deviation σ) from Conjunction Data Messages. (2) A Stochastic Transition Model which simulates the time-evolution of these state variables using fitted probability distributions (Generalized Normal, Non-Central t-distribution). (3) The MDP Environment, which uses the transition model to step through time, calculates propellant cost (via orbital mechanics lemmas) and PoC risk, and issues rewards. (4) The RL-PG Agent, a neural network policy trained via the REINFORCE algorithm to map states to maneuver/delay actions.

**Critical Path:**
The most critical path for performance is the reward function formulation (Eq. 26) and the training data quality. The reward weight η directly dictates the risk-propellant trade-off. The sparsity of high-risk events in the historical data creates a challenge for learning a well-calibrated policy, leading to the observed conservative bias. The transition model's fidelity to real CDM dynamics also limits the realism of the training environment.

**Design Tradeoffs:**
- **Propellant vs. Risk (η):** The η parameter explicitly controls this. A low η maximizes safety (conservative maneuvers), while a high η minimizes fuel. The paper finds a "stable-η region" (η ≤ 0.6) where performance is consistent.
- **Model Fidelity vs. Complexity:** The state space is reduced to two key variables (ρ, σ) for computational tractability. This simplification ignores other CDM parameters, potentially reducing accuracy.
- **Fixed vs. Variable Maneuver Parameters:** The paper shows that fixing Hard Body Radius (HBR) and required phase shift leads to faster training convergence and better propellant performance than using variable values derived from each CDM.

**Failure Signatures:**
1. **Non-Convergence:** The REINFORCE algorithm fails to converge, often due to an improper reward structure (e.g., η=1 removes the risk incentive) or insufficient exploration.
2. **Excessive False Positives:** The trained policy maneuvers too often on low-risk historical data, consuming more total propellant than the baseline. This indicates the policy has learned to be overly risk-averse due to training data imbalance or reward penalties.
3. **Instability to Parameter Variation:** Policy performance degrades significantly when key orbital or satellite parameters (e.g., specific impulse I_sp, orbit radius R_serv) are changed from training defaults, suggesting poor generalization.

**First 3 Experiments:**
1. **Reproduce Default Policy:** Using the provided default parameters (η=0.25, fixed HBR, fixed Δθ), train the agent on synthetic CDMs and verify convergence around iteration 1500. Confirm the "stable-η region" by testing η ∈ [0, 0.6].
2. **Ablate Reward Structure:** Retrain the policy with η=0.7 (outside the stable region) and η=0.9. Measure the change in confusion matrix elements (specifically the False Positive rate) and total propellant consumption compared to the default policy.
3. **Evaluate on Real vs. Synthetic Data:** Take the default trained policy and evaluate it on both synthetic CDMs and the historical ESA dataset. Quantify the performance gap in "Total Propellant" and "Average Propellant per Maneuver" to measure the sim-to-real transfer penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating low-thrust propulsion dynamics, as opposed to the currently assumed impulsive maneuvers, affect the propellant efficiency and convergence of the RL-PG policy?
- Basis in paper: [explicit] The authors state in Section 3.3 that they model propellant consumption using high-thrust equations but "plan to explore the low-thrust alternative in future works."
- Why unresolved: LEO satellites frequently utilize low-thrust electric propulsion; the current assumption of instantaneous velocity changes simplifies the optimization landscape but may not capture the continuous control requirements or efficiency profiles of operational systems.
- What evidence would resolve it: A comparison of convergence rates and propellant consumption metrics between the current high-thrust MDP and a modified MDP utilizing continuous low-thrust transfer equations (e.g., thrust integration over time).

### Open Question 2
- Question: Does expanding the state space to include the combined-covariance matrix, relative position, and relative velocity significantly improve the collision prediction accuracy without destabilizing RL training?
- Basis in paper: [explicit] The Conclusion explicitly notes that "The accuracy of the model could be improved upon by introducing more state variables, such as the combined-covariance matrix..."
- Why unresolved: The authors restricted the state space to two variables (miss distance and along-track standard deviation) to balance model fidelity with computation efficiency, leaving the performance trade-off of higher-dimensional states untested.
- What evidence would resolve it: Ablation studies showing confusion matrix accuracy and training convergence speeds for policies trained on the expanded state vector compared to the baseline two-variable state.

### Open Question 3
- Question: How do gravitational perturbations (J2-effect) and atmospheric drag influence the stochastic transition dynamics and the resulting optimal maneuver timing?
- Basis in paper: [explicit] The Conclusion suggests the model could be improved by "introducing gravitational perturbation (J2-effect) and drag, which could help simulate a more accurate environment."
- Why unresolved: The current transition dynamics rely on stochastic distributions (Generalized Normal and Non-Central t-distribution) fitted to historical data, without explicitly modeling the underlying deterministic physical forces that govern orbital evolution in LEO.
- What evidence would resolve it: Performance evaluation of a policy trained on an environment with physics-based propagation (including J2 and drag) versus the current stochastic CDM generator, specifically looking for shifts in the optimal maneuver decision time.

### Open Question 4
- Question: What specific characteristics of historical conjunction data cause the trained policy to consume more total propellant than the cut-off policy, despite consuming less on synthetic data?
- Basis in paper: [inferred] Section 5.3 reports that the trained policy consumes significantly less propellant on synthetic data but more on historical data; the authors partially attribute this to the lack of high-risk events in historical data but do not resolve the inverse performance relationship.
- Why unresolved: This discrepancy suggests the synthetic CDM generator may fail to capture the complexity or "edge cases" of real-world conjunction evolution, potentially leading to a policy that is over-conservative or poorly calibrated for real operations.
- What evidence would resolve it: A sensitivity analysis identifying the specific orbital or covariance features present in the historical dataset (but absent in the synthetic data) that trigger the policy to initiate unnecessary maneuvers.

## Limitations
- The transition dynamics parameters (fitted GND and NCT distributions) are not provided, preventing exact reproduction of the simulation environment
- The policy performs worse on historical ESA data than synthetic data, indicating a significant sim-to-real gap
- The reward function depends on unspecified constants (P_C,max, λ) that significantly affect learned behavior
- The state space is limited to two variables, potentially omitting critical conjunction parameters

## Confidence
- **High Confidence**: The MDP formulation correctly models the sequential nature of collision avoidance decisions. The orbital mechanics lemmas for propellant calculation (ΔV) are well-established. The training procedure (REINFORCE with ε-greedy) is standard and reproducible.
- **Medium Confidence**: The observed performance improvements (reduced propellant consumption, stable parameter sensitivity) are demonstrated, but are contingent on the simulation environment's accuracy. The conservative bias in maneuver decisions is a reasonable response to imbalanced training data but may not generalize.
- **Low Confidence**: The absolute performance on real historical CDMs is uncertain due to the unquantified sim-to-real gap and missing transition parameters. The long-term reliability of the learned policy in operational settings is not established.

## Next Checks
1. **Transition Dynamics Validation**: Obtain or regenerate the fitted GND and NCT distribution parameters for the ESA dataset and validate the simulated CDM evolution against a held-out test set of real CDMs.
2. **Reward Function Sensitivity Analysis**: Systematically vary the key reward constants (P_C,max, λ, η) and measure their impact on policy behavior (confusion matrix, propellant consumption) to understand the robustness of the learned trade-off.
3. **Cross-Validation on Independent Data**: Evaluate the trained policy on a completely independent set of historical CDMs from a different time period or source to assess its generalization and quantify the sim-to-real performance penalty.