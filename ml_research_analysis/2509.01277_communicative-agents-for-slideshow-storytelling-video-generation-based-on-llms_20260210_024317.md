---
ver: rpa2
title: Communicative Agents for Slideshow Storytelling Video Generation based on LLMs
arxiv_id: '2509.01277'
source_url: https://arxiv.org/abs/2509.01277
tags:
- video
- generation
- vgteam
- arxiv
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces VGTeam, a multi-agent system for slideshow
  storytelling video generation that leverages large language models and API-based
  workflows to reduce computational costs. The system uses specialized agents (director,
  editor, painter, composer) operating in a structured Chat Tower architecture to
  convert user text prompts into complete videos.
---

# Communicative Agents for Slideshow Storytelling Video Generation based on LLMs

## Quick Facts
- arXiv ID: 2509.01277
- Source URL: https://arxiv.org/abs/2509.01277
- Reference count: 29
- Primary result: 98.4% success rate for slideshow video generation at $0.103 average cost using multi-agent LLM orchestration

## Executive Summary
VGTeam introduces a multi-agent system for slideshow storytelling video generation that leverages large language models and API-based workflows to reduce computational costs. The system uses specialized agents (director, editor, painter, composer) operating in a structured Chat Tower architecture to convert user text prompts into complete videos. Experiments with 300 video generations across varied prompts and language models demonstrate high success rates and low per-video costs, though challenges remain with prompt length sensitivity and occasional content quality issues.

## Method Summary
VGTeam employs a Chat Tower architecture where a director agent orchestrates four specialized agents in sequential waterfall communication. Each agent receives role-defining system prompts that constrain their outputs to specific tasks: the editor generates video captions, the painter creates image prompts, the composer produces music prompts, and the director coordinates the workflow and performs quality approval. The system integrates external APIs for multimedia generation (images, voice, music) and uses MoviePy for final video assembly, achieving efficiency through API composition rather than training end-to-end models.

## Key Results
- 98.4% success rate across 300 video generations with varied prompts and language models
- Average cost of $0.103 per video through API-based component generation
- 22.7% inappropriate content rate (repetitive visuals or incoherent image prompts) that could be reduced through stricter constraints
- Short prompts cause more failures while long prompts increase runtime and variability

## Why This Works (Mechanism)

### Mechanism 1: Role Specialization via Prompt Engineering
General-purpose LLMs are transformed into role-specific agents through structured system prompts, enabling coordinated multi-agent workflows without specialized models. Each agent receives constructed prompts defining task objectives, input/output requirements, and performance standards that constrain the LLM's generative space to role-appropriate outputs while preserving flexibility within that role.

### Mechanism 2: Chat Tower Sequential Orchestration
Sequential, waterfall-style agent communication reduces linguistic ambiguity and hallucinations compared to unconstrained multi-agent interaction. The director coordinates all communication, formulating task-specific objectives for downstream agents and maintaining a memory stream of all interactions for continuity and reviewability.

### Mechanism 3: Iterative Approval Quality Gate
A review-feedback-revision cycle between director and specialist agents improves output quality while maintaining creative fidelity to user intent. The director reviews outputs against predefined criteria, provides specific feedback when unsatisfied, and agents refine their work until approval is granted.

## Foundational Learning

- **Multi-agent orchestration patterns**: Understanding how agents communicate, share context, and coordinate is essential before modifying the system. Can you diagram which agents share information and in what sequence?
- **Prompt engineering for role definition**: Role specialization is achieved entirely through prompts, not fine-tuning. Modifying agent behavior requires understanding how task objectives, I/O specs, and performance standards are encoded. What three components does VGTeam include in every role-defining prompt?
- **API composition vs. end-to-end models**: VGTeam's cost and computational advantages come from composing external APIs rather than training or running video generation models. Trade-offs include quality variability and dependency on third-party availability. What multimedia generation tasks does VGTeam delegate to external APIs?

## Architecture Onboarding

- **Component map**: User input → Director agent → Editor (captions) + Painter (image prompts) + Composer (music prompts) → API calls → MoviePy assembly
- **Critical path**: 1) User prompt received by director 2) Director generates task objectives 3) Editor produces captions (shared to painter/composer) 4) Painter produces image prompts → API call → images 5) Composer produces music prompts → API call → audio 6) Director reviews and iterates if needed 7) MoviePy assembles final slideshow video
- **Design tradeoffs**: Sequential vs. parallel execution (sequential reduces ambiguity but increases latency); API dependency vs. in-house models (APIs eliminate GPU requirements but introduce quality variability); short vs. long prompts (short prompts yield stable runtimes but higher failure rates)
- **Failure signatures**: Network instability (API call failures); character confusion (agent misinterprets role/task scope); infinite loops (director never approves); inappropriate content (22.7% rate - repetitive visuals or semantically incoherent image prompts)
- **First 3 experiments**: 1) Run 10 generations with identical prompts across all three LLMs to observe token length, loop count, and runtime differences 2) Test 5 short prompts vs. 5 long prompts from same topic category to measure failure rate and output coherence 3) Introduce deliberate "bad" prompt to observe director feedback behavior and identify iteration ceiling

## Open Questions the Paper Calls Out

- What prompt engineering strategies or deterministic control mechanisms can effectively reduce LLM-induced output inconsistencies while preserving creative flexibility? The authors note that "LLMs can introduce a degree of unpredictability" and that "similar inputs do not reliably produce similar outputs," suggesting future work could incorporate "more deterministic control mechanisms or fine-tuning prompt engineering strategies."
- How can the 22.7% inappropriate content rate be systematically reduced without degrading the 98.4% successful generation rate? The paper reports that "inappropriate content appeared in 22.7% of the outputs" and notes this "can further be minimized through stricter constraints on the painter module."
- What is the optimal balance between prompt length and generation stability, given that short prompts cause more failures but long prompts introduce greater runtime variability? Experimental results show all failures occurred with short prompts, while long prompts showed higher token length and communication time with greater variability.

## Limitations
- Exact system prompts for role specialization are not provided, making practical replication uncertain
- Iterative approval mechanism's termination criteria lack specificity with observed 22-44 loops in some cases
- API integration details (endpoints, authentication, request/response formats) are entirely absent

## Confidence
- **High Confidence**: Chat Tower sequential orchestration mechanism and waterfall communication pattern (logically sound with direct experimental validation)
- **Medium Confidence**: Role specialization mechanism via prompt engineering (conceptually sound but lacks exact prompt templates)
- **Low Confidence**: Iterative approval quality gate mechanism (described but minimal detail on feedback structure, approval criteria, or iteration limits)

## Next Checks
1. Reproduce success rate by implementing VGTeam architecture with placeholder prompts and APIs, running 50-100 video generations to verify 98.4% success rate and measure actual communication times
2. Test role drift vulnerability by designing prompts that deliberately push agent boundaries to observe if role confusion occurs and test whether strengthened system prompts can mitigate this
3. Validate cost efficiency by calculating actual API costs per video generation using documented APIs and comparing against the reported $0.103 average while measuring GPU/memory usage