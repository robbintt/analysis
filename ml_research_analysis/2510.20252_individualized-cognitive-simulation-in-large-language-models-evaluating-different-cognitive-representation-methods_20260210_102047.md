---
ver: rpa2
title: 'Individualized Cognitive Simulation in Large Language Models: Evaluating Different
  Cognitive Representation Methods'
arxiv_id: '2510.20252'
source_url: https://arxiv.org/abs/2510.20252
tags:
- linguistic
- cognitive
- style
- author
- concept
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task to evaluate different cognitive
  representation methods in individualized cognitive simulation (ICS), aiming to approximate
  the thought processes of specific individuals using large language models (LLMs).
  The authors constructed a dataset from recently published novels and proposed an
  11-condition cognitive evaluation framework to benchmark seven off-the-shelf LLMs
  in the context of authorial style emulation.
---

# Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods

## Quick Facts
- arXiv ID: 2510.20252
- Source URL: https://arxiv.org/abs/2510.20252
- Reference count: 37
- One-line primary result: Combining conceptual and linguistic features outperforms static profile-based cues in individualized cognitive simulation, with LLMs better at mimicking linguistic style than narrative structure.

## Executive Summary
This paper introduces a novel task for evaluating cognitive representation methods in individualized cognitive simulation (ICS), aiming to approximate specific individuals' thought processes using large language models. The authors constructed a dataset from five recently published novels and proposed an 11-condition cognitive evaluation framework to benchmark seven off-the-shelf LLMs in authorial style emulation. The evaluated cognitive representations included linguistic features, concept mappings, and author profile information. Results showed that combining conceptual and linguistic features was particularly effective in ICS, outperforming static profile-based cues in overall evaluation. Notably, LLMs were more effective at mimicking linguistic style than narrative structure, underscoring their limits in deeper cognitive simulation.

## Method Summary
The study evaluated cognitive representation methods for Individualized Cognitive Simulation in narrative continuation tasks using five contemporary novels as source material. Authors extracted cognitive features (linguistic, conceptual, and profile-based) from text and public author information, then generated continuations using seven off-the-shelf LLMs under 11 different conditions. The evaluation combined LLM-based assessment (GPT-4 Turbo) for linguistic style and narrative structure with human evaluation on style, structure, and overall quality. Structural similarity was computed using event extraction, Hungarian alignment, and a weighted formula combining event similarity, coverage, and ordering. The best-performing generations per condition were selected for human evaluation through a BLEU-based filtering process.

## Key Results
- Combining conceptual and linguistic features significantly outperformed static profile-based cues in overall ICS evaluation
- LLMs demonstrated superior performance in mimicking linguistic style (2.5-3.1 on 1-5 scale) compared to narrative structure (0.1-0.17 on 0-1 scale)
- BLEU filtering effectively identified malformed outputs, particularly from smaller models like Gemma-2B

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Cognitive Representation Methods**: Different ways to encode an author's thinking patterns (linguistic features, concept mappings, profiles) are necessary because they capture distinct aspects of cognitive style that affect narrative generation.
- **Narrative Structure Evaluation**: Measuring event similarity, coverage, and ordering is needed to assess whether models capture not just surface style but deeper narrative construction patterns.
- **BLEU as Quality Filter**: Using BLEU to filter malformed outputs works because completely off-topic or empty generations produce near-zero BLEU scores, though low BLEU on creative text doesn't indicate poor quality.

## Architecture Onboarding
**Component Map**: Novels -> Context/Ground Truth Split -> Feature Extraction -> 11 Generation Conditions -> 7 LLMs -> BLEU Filtering -> LLM Evaluation -> Human Evaluation

**Critical Path**: Feature extraction → generation → BLEU filtering → LLM evaluation → human evaluation

**Design Tradeoffs**: Using off-the-shelf LLMs provides accessibility but limits control over model architecture; combining multiple evaluation methods (LLM + human) balances efficiency with accuracy but increases complexity.

**Failure Signatures**: Low BLEU scores flag malformed outputs, especially from smaller models; uniformly low structural scores (~0.1-0.17) are expected and indicate evaluation methodology rather than model failure.

**First 3 Experiments**:
1. Verify all tested models have release dates before novel publication to prevent data leakage
2. Test BLEU filtering on sample outputs to confirm it effectively identifies malformed generations
3. Run LLM evaluation on a small sample to validate scoring consistency across conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Prompt templates and formatting details for all 11 conditions are only partially specified, introducing potential variability
- Personality assessment methodology cites unavailable "ChatGPT-5," requiring methodological substitution
- Small sample size of five novels limits generalizability across diverse authorial styles and genres

## Confidence
**High confidence**: Combining conceptual and linguistic features outperforms static profile-based cues in ICS tasks, supported by robust statistical evaluation across multiple methods.

**Medium confidence**: LLMs are more effective at mimicking linguistic style than narrative structure, though low structural scores may reflect evaluation methodology limitations.

**Medium confidence**: Dataset construction from five contemporary novels is methodologically sound but small sample size limits generalizability.

## Next Checks
1. Contact authors to obtain complete, exact prompt templates for all 11 conditions used across all five novels
2. Implement an alternative personality assessment method using publicly available LLMs to replicate Big Five personality extraction
3. Conduct ablation studies on structural similarity evaluation methodology using alternative event extraction and alignment approaches