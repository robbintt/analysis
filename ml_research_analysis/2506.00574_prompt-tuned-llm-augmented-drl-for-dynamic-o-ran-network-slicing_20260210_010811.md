---
ver: rpa2
title: Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing
arxiv_id: '2506.00574'
source_url: https://arxiv.org/abs/2506.00574
tags:
- network
- o-ran
- learning
- state
- slicing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Prompt-Augmented Multi-Agent Reinforcement
  Learning (PA-MRL) framework for dynamic O-RAN network slicing, integrating domain-specific
  LLM-driven state representations with learnable prompts to enhance decision-making.
  The approach combines informal and learnable prompts with multi-agent SAC to improve
  policy adaptation and convergence in resource allocation.
---

# Prompt-Tuned LLM-Augmented DRL for Dynamic O-RAN Network Slicing

## Quick Facts
- arXiv ID: 2506.00574
- Source URL: https://arxiv.org/abs/2506.00574
- Reference count: 20
- One-line primary result: PA-MRL framework with learnable prompts outperforms baselines in O-RAN slicing, achieving higher rewards and faster convergence.

## Executive Summary
This paper introduces a Prompt-Augmented Multi-Agent Reinforcement Learning (PA-MRL) framework for dynamic O-RAN network slicing. The approach integrates domain-specific LLM-driven state representations with learnable prompts to enhance decision-making in resource allocation. By combining informal and learnable prompts with multi-agent SAC, the method improves policy adaptation and convergence in dynamic network environments. Experimental results demonstrate significant improvements in cumulative rewards, slice-level QoS, and convergence speed compared to baseline methods.

## Method Summary
The framework combines a frozen ORANSight LLM with learnable prompt embeddings and multi-agent SAC. The State Representation Module (SRM) processes raw network states through informal prompts and learnable tokens, generating multimodal embeddings via adapter networks. These embeddings are fused and fed to distributed actor networks at each Distributed Unit (DU), while a centralized critic at the near-RT RIC provides TD-loss updates. The learnable prompts are updated via RL gradients to align state representations with policy objectives, enabling faster convergence without modifying the LLM's core parameters.

## Key Results
- PA-MRL achieves 55.4% eMBB QoS improvement versus 2.6% for non-prompt-aligned MARL
- Optimal learnable token count is approximately 20 (Fig. 4 shows performance degradation beyond this point)
- Convergence occurs after approximately 1000 episodes, faster than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Learnable prompts enable the pre-trained LLM to generate task-aligned state representations that accelerate RL convergence without modifying the LLM's core parameters. Trainable token embeddings are prepended to informal text prompts describing network states and updated via RL gradient signals during policy optimization. The semantic clustering inherent in LLM latent space maps usefully to network state structure for decision-making. Core assumption: learnable prompts remain generalizable across scenarios. Break condition: Overfitting occurs beyond ~20 tokens (Fig. 4).

### Mechanism 2
Multimodal fusion of LLM-generated textual embeddings with raw numerical network metrics yields more expressive state representations than either modality alone. Two pre-trained adapter networks project raw state vectors and LLM embeddings into a shared latent space. The concatenated representation combines semantic context with precise numerical values. Core assumption: adapter networks trained offline generalize to online RL states. Break condition: Distribution shift between offline training and online conditions causes misalignment.

### Mechanism 3
Multi-agent SAC with centralized critic and distributed actors enables scalable, stable learning in continuous action spaces for resource block allocation. Each DU hosts a local actor network, while a global critic at the near-RT RIC aggregates experiences and computes TD-loss updates. Entropy regularization encourages exploration while maintaining stability. Core assumption: CTDE paradigm works for O-RAN's partial observability. Break condition: Increased coordination requirements cause centralized critic to become a bottleneck.

## Foundational Learning

- Concept: **Prompt Tuning vs. Fine-Tuning**
  - Why needed here: The paper explicitly rejects full LLM fine-tuning as too costly for dynamic O-RAN. Understanding parameter-efficient adaptation is essential.
  - Quick check question: Can you explain why learnable prompts require fewer parameters than LoRA or full fine-tuning?

- Concept: **Soft Actor-Critic (SAC)**
  - Why needed here: SAC's entropy regularization is central to the method's stability in continuous action spaces (RB allocation).
  - Quick check question: What does the temperature parameter β control, and what happens if set too high or too low?

- Concept: **Multi-Agent RL (CTDE Paradigm)**
  - Why needed here: The architecture distributes actors across DUs with a centralized critic. Understanding why this matters for partial observability is prerequisite.
  - Quick check question: Why can't each DU actor learn independently without a centralized critic in this setup?

## Architecture Onboarding

- Component map: Environment -> Raw state st -> Informal prompt pt -> pt ∪ T -> Frozen ORANSight LLM -> Hidden state ht -> Fc2(ht) → ŷsr; Fc1(st) → ŷs -> Actor receives (ŷsr, ŷs) -> Action at (RB allocation) -> Critic estimates Q-value -> Reward rt from QoS deviations -> Update θp, θv, T

- Critical path: 1) Environment emits raw state st (QoS levels, user density, prior allocation) 2) Informal prompt pt constructed from st via template 3) pt ∪ T fed to frozen ORANSight LLM → hidden state ht 4) Fc2(ht) → textual embedding ŷsr; Fc1(st) → numerical embedding ŷs 5) Actor receives (ŷsr, ŷs), outputs action at (RB allocation) 6) Critic estimates Q-value; compute reward rt from QoS deviations 7) Update θp (actors), θv (critic), T (learnable prompts) via Algorithm 1

- Design tradeoffs: Number of learnable tokens optimal at ~20 (Fig. 4); fewer = underfitting, more = overfitting. LLM choice: ORANSight outperforms GPT-2 by 44% on eMBB QoS (Table I) but may be less generalizable. Adapter training: Offline pre-training required; online adaptation not explored.

- Failure signatures: Slow convergence indicates prompt embeddings T not receiving gradient updates. Poor slice QoS suggests reward function normalization saturation or incorrect hyperparameters. Overfitting shows validation performance diverging from training.

- First 3 experiments: 1) Ablation of learnable prompts: Run with T frozen vs. trainable (expect 8.97% vs. 1.12% difference per Table I) 2) Token count sweep: Test T ∈ {5, 10, 20, 30, 40} tokens and plot cumulative reward (confirm peak near 20) 3) LLM substitution: Replace ORANSight with GPT-2 (expect ~44% drop in eMBB QoS improvement)

## Open Questions the Paper Calls Out

- Open Question 1: What is the optimal number of learnable context tokens for PA-MRL, and can this be determined automatically rather than treated as a manually tuned hyperparameter? The paper only empirically tests a range of token counts without proposing an adaptive selection mechanism.

- Open Question 2: Can PA-MRL meet the strict timing requirements of near-RT RIC (10ms–1s control loops) given the inference overhead of LLM-based state representation? The paper claims suitability for real-time deployment but provides no latency benchmarks.

- Open Question 3: How does PA-MRL performance scale to larger deployments with more DUs, users, and slices than the tested 6 DUs and 200 UEs? No scalability analysis is provided despite real-world O-RAN deployments potentially involving orders of magnitude more agents.

- Open Question 4: How robust is the trained policy to distribution shifts in user mobility, traffic patterns, or channel conditions not seen during training? The simulation uses specific mobility models and Rayleigh fading without evaluating generalization to different environments.

## Limitations

- The experimental setup relies heavily on simulation rather than real-world deployment, limiting generalizability to actual O-RAN environments
- Performance improvements are measured against baseline MARL approaches but lack comparison with other emerging LLM-augmented methods in the corpus
- The claim that learnable prompts "accelerate convergence without modifying the LLM's core parameters" needs more empirical validation across different traffic patterns

## Confidence

- High confidence: The multi-agent SAC architecture with centralized critic and distributed actors is a well-established approach for O-RAN resource allocation
- Medium confidence: The claim that learnable prompts significantly improve convergence speed is supported by experimental results but could be more rigorously validated
- Low confidence: The assertion that ORANSight LLM's domain-specific pretraining provides substantial advantages over generic models needs more systematic validation

## Next Checks

1. Cross-traffic validation: Test the framework across diverse traffic patterns (e.g., sudden spikes, different mobility patterns) not present in the original training data to assess generalization and identify potential overfitting

2. Ablation study on adapter networks: Remove the adapter networks and directly concatenate LLM embeddings with raw state features to quantify the contribution of the multimodal fusion approach versus simpler feature combination methods

3. Real-world deployment simulation: Implement a more realistic O-RAN deployment scenario with actual O-RAN interface specifications and timing constraints to evaluate whether the performance gains hold under practical deployment conditions