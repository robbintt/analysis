---
ver: rpa2
title: 'VeFA: Vector-Based Feature Space Adaptation for Robust Model Fine-Tuning'
arxiv_id: '2510.19155'
source_url: https://arxiv.org/abs/2510.19155
tags:
- vefa
- fine-tuning
- lora
- adaptation
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in model fine-tuning
  by proposing Vector-based Feature Space Adaptation (VeFA). VeFA operates directly
  in the feature space, avoiding the introduction of intruder dimensions that can
  overwrite pre-trained knowledge.
---

# VeFA: Vector-Based Feature Space Adaptation for Robust Model Fine-Tuning

## Quick Facts
- arXiv ID: 2510.19155
- Source URL: https://arxiv.org/abs/2510.19155
- Authors: Peng Wang; Minghao Gu; Qiang Huang
- Reference count: 40
- Primary result: VeFA achieves comparable fine-tuning performance to LoRA while using substantially fewer trainable parameters (e.g., 0.018M vs 0.3M on GLUE) and consistently demonstrates stronger robustness across tasks, particularly in cross-dataset transfer settings.

## Executive Summary
This paper addresses catastrophic forgetting in model fine-tuning by proposing Vector-based Feature Space Adaptation (VeFA). VeFA operates directly in the feature space, avoiding the introduction of intruder dimensions that can overwrite pre-trained knowledge. By applying element-wise transformations to individual feature channels, VeFA ensures that fine-tuned weights remain within the column space of the pre-trained weight matrix. This feature-space adaptation preserves pre-trained representations and improves model generalization under distribution shift. The authors evaluate VeFA against LoRA on image classification, natural language understanding (GLUE), and natural language generation (E2E) tasks.

## Method Summary
VeFA introduces a diagonal scaling matrix Λ = diag(λ) applied to layer inputs before multiplication by frozen pre-trained weights W₀. The forward pass becomes h^(ℓ) = W₀^(ℓ)(I + Λ^(ℓ))h^(ℓ-1), where W_ft^(ℓ) = W₀^(ℓ) + W₀^(ℓ)Λ^(ℓ) remains within the pre-trained column space. VeFA initializes λ = 0 and trains with task loss, optionally clipping λ_j ∈ [−γ, γ] to control forgetting. The method is evaluated on CLIP ViT-B/16 (7 image datasets), RoBERTa-base/large (GLUE), and GPT-2 Medium/Large (E2E NLG), achieving comparable performance to LoRA while using ~4–16× fewer parameters.

## Key Results
- VeFA achieves comparable fine-tuning performance to LoRA with significantly fewer trainable parameters (e.g., 0.018M vs 0.3M on GLUE)
- VeFA demonstrates stronger robustness across tasks, particularly in cross-dataset transfer settings (robustness score ~40+ vs LoRA's ~6 on merged models)
- Cross-dataset transfer analysis shows VeFA produces near-zero or positive transfers while LoRA exhibits many negative entries indicating harmful forgetting

## Why This Works (Mechanism)

### Mechanism 1
Feature-space diagonal scaling avoids intruder dimensions that cause catastrophic forgetting in weight-space methods. VeFA applies element-wise transformations via Λ = diag(λ) to layer inputs before multiplication by frozen W₀. Since Col(W₀Λ) ⊆ Col(W₀), all effective weight updates remain within the pre-trained column space—no new representational directions are introduced. This contrasts with LoRA where W_ft = W₀ + BA can create directions orthogonal to S₀.

### Mechanism 2
Distribution shift between pre-training and downstream domains can be compensated through equivalent feature transformations rather than weight modifications. Grounded in Effect Equivalence Modeling (EEM) of lurking variables: unobserved factors U affecting the downstream distribution can be modeled as an equivalent input transformation g(X,U) ≈ f(X+Δ). VeFA learns Δ via diagonal scaling, compensating for domain discrepancy without leaving the pre-trained representational subspace.

### Mechanism 3
Forgetting under VeFA has a provable upper bound directly controllable by clipping the scaling vector magnitude. Proposition 4.2 proves in-subspace forgetting ≤ ||Σx||₂ · ||W₀||²_F · (max_j|λ_j|)². Since VeFA guarantees Δ⊥ = 0 (no intruder dimensions), this bound characterizes total forgetting. Explicitly clipping ||λ||_∞ ≤ γ provides direct control.

## Foundational Learning

- **Column Space and Singular Subspaces**: Understanding what it means for a direction u to have ||PS₀u||₂ < ε is essential since VeFA's core contribution is constraining updates to the pre-trained column space. Quick check: Given SVD W₀ = UΣV^T with effective rank r₀, what does it mean for a direction u to have ||PS₀u||₂ < ε?

- **Catastrophic Forgetting in Transfer Learning**: The paper targets the tradeoff between downstream adaptation and preservation of pre-trained capabilities; understanding why this tension exists is crucial. Quick check: Why might a model fine-tuned on domain A perform worse than zero-shot on domain B?

- **Parameter-Efficient Fine-Tuning (PEFT) Taxonomy**: VeFA is positioned against LoRA and adapters; understanding low-rank factorization W₀ + BA helps contrast with diagonal-only scaling. Quick check: For a 768×768 weight matrix, how many parameters does LoRA (r=2) require versus VeFA?

## Architecture Onboarding

- **Component map**: Frozen backbone (W₀) -> VeFA adapter (Λ = diag(λ)) -> Forward pass (h^(ℓ) = W₀^(ℓ)(I + Λ^(ℓ))h^(ℓ-1))
- **Critical path**: 1) Select layers to adapt (self-attention Q/V for NLU, all layers for vision) 2) Initialize λ = 0 3) Train with task loss, optionally clip λ_j ∈ [−γ, γ] 4) Evaluate on out-of-domain tasks vs zero-shot baseline
- **Design tradeoffs**: Expressivity vs preservation (diagonal scaling cannot model cross-channel mixing; LoRA can but introduces intruder dimensions); Parameter efficiency (VeFA uses ~25% of LoRA(r=2) parameters); Clipping strictness (smaller γ reduces forgetting risk but may underfit complex shifts)
- **Failure signatures**: Large performance gap vs LoRA (diagonal constraint too restrictive for task); λ magnitude explosion without clipping (forgetting bound no longer holds); No improvement over zero-shot (either pre-trained model already optimal or adaptation capacity insufficient)
- **First 3 experiments**: 1) RoBERTa-base on MRPC with VeFA vs LoRA(r=2), target within 1-2% accuracy using ~6× fewer parameters 2) Train on RTE, merge adapters via FedAvg, evaluate on STS-B, expect VeFA to show ~40+ robustness score vs LoRA's ~6 3) E2E benchmark with λ ∈ [−2, 2] vs unclipped, verify minimal performance difference

## Open Questions the Paper Calls Out

### Open Question 1
Can sparsity-inducing constraints on the scaling vector Λ improve mechanistic interpretability without degrading downstream task performance? The conclusion states future work should "investigate sparsity-inducing constraints on the scaling vector Λ, connecting VeFA to sparse and mechanistic interpretability." This remains unresolved as the paper demonstrates effectiveness using dense scaling vectors but does not analyze sparsity or explore if forcing sparsity clarifies critical feature dimensions.

### Open Question 2
Can alternative projected-gradient schemes effectively constrain updates to the pre-trained subspace while reducing in-subspace loss more effectively than VeFA? The conclusion suggests exploring "alternative projected-gradient schemes that constrain the update Δ to S₀ while further reducing in-subspace loss." VeFA guarantees updates remain in the subspace via diagonal feature scaling, but this restricts the optimization path; it is unclear if other optimization trajectories could minimize the forgetting loss term more efficiently.

### Open Question 3
Does the restriction to element-wise (diagonal) feature scaling limit adaptation capability for downstream shifts that require significant cross-channel feature mixing? The paper restricts VeFA to "element-wise adaptation on individual features" to avoid intruder dimensions and cites over-parameterization as justification. However, complex distribution shifts might theoretically require linear combinations of features that diagonal scaling cannot capture.

## Limitations

- The effectiveness vs parameter scaling tradeoff varies across model scales and task complexities without clear characterization
- The EEM mechanism grounding for lurking variable compensation appears largely theoretical and specific to this work
- The necessity of clipping for controlling forgetting bounds remains largely theoretical without extensive empirical validation across tasks

## Confidence

- **High confidence**: VeFA successfully constrains updates to the pre-trained column space (Mechanism 1); The diagonal scaling approach achieves parameter efficiency gains over LoRA; The formal proof of in-subspace forgetting bounds (Mechanism 3) is mathematically sound
- **Medium confidence**: VeFA demonstrates improved robustness across tasks, particularly in cross-dataset transfer settings; The empirical results show promising trends but mechanisms lack strong external validation
- **Low confidence**: The claim that diagonal scaling is sufficient for compensating distribution shift via lurking variable effects (Mechanism 2); This theoretical framing is specific to the paper and not well-supported by external literature

## Next Checks

1. **Scale sensitivity experiment**: Evaluate VeFA vs LoRA across model sizes (small, base, large) on identical tasks to verify whether parameter efficiency gains persist at scale and whether effectiveness remains comparable

2. **Cross-dataset transfer matrix validation**: Replicate the transfer matrix analysis (Table 9) on a held-out task pair not used in the original experiments to verify that VeFA consistently shows near-zero or positive transfers while LoRA exhibits negative entries

3. **Clipping threshold ablation**: Systematically vary the clipping bound γ across a range of values (e.g., 0.5, 1.0, 2.0, 4.0) on multiple tasks to quantify the relationship between clipping strictness, forgetting bounds, and task performance