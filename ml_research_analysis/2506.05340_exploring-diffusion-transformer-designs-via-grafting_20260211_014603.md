---
ver: rpa2
title: Exploring Diffusion Transformer Designs via Grafting
arxiv_id: '2506.05340'
source_url: https://arxiv.org/abs/2506.05340
tags:
- grafting
- operators
- data
- layers
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents grafting, a simple two-stage approach for editing
  pretrained diffusion transformers (DiTs) to explore new architectural designs under
  small compute budgets. The method uses activation distillation to initialize new
  operators and lightweight finetuning to mitigate error propagation, enabling operator
  replacement (e.g., softmax attention with gated convolution, local attention, or
  linear attention) and even architecture restructuring (e.g., converting sequential
  transformer blocks into parallel).
---

# Exploring Diffusion Transformer Designs via Grafting
## Quick Facts
- arXiv ID: 2506.05340
- Source URL: https://arxiv.org/abs/2506.05340
- Reference count: 40
- Primary result: Grafting achieves FID 2.38-2.64 on ImageNet-1K with <2% compute, and 1.43× speedup on PixArt-Σ for 2048×2048 text-to-image generation.

## Executive Summary
This paper introduces grafting, a two-stage method for editing pretrained diffusion transformers (DiTs) to explore new architectural designs under small compute budgets. Grafting uses activation distillation to initialize new operators and lightweight finetuning to mitigate error propagation, enabling operator replacement and architecture restructuring. The approach is demonstrated on ImageNet-1K and PixArt-Σ, showing competitive performance with significant compute savings.

## Method Summary
Grafting is a two-stage approach for editing pretrained diffusion transformers (DiTs) to explore new architectural designs under small compute budgets. The method uses activation distillation to initialize new operators and lightweight finetuning to mitigate error propagation, enabling operator replacement (e.g., softmax attention with gated convolution, local attention, or linear attention) and even architecture restructuring (e.g., converting sequential transformer blocks into parallel). On ImageNet-1K, several hybrid designs achieve FIDs between 2.38–2.64 (vs. 2.27 baseline) using <2% of pretraining compute. Applied to PixArt-Σ for 2048×2048 text-to-image generation, grafting yields a 1.43× speedup with <2% drop in GenEval score. A case study restructuring DiT-XL/2 from depth 28 to 14 achieves FID=2.77, outperforming other models of comparable depth.

## Key Results
- Grafting achieves FID 2.38-2.64 on ImageNet-1K with <2% compute, compared to 2.27 baseline
- 1.43× speedup on PixArt-Σ for 2048×2048 text-to-image generation with <2% drop in GenEval score
- Depth reduction from 28 to 14 in DiT-XL/2 achieves FID=2.77, outperforming other models of comparable depth

## Why This Works (Mechanism)
Grafting leverages activation distillation to initialize new operators with pretrained knowledge, then uses lightweight finetuning to adapt these operators to the new architecture. This two-stage approach allows for efficient exploration of new architectural designs without the need for full retraining. The method is particularly effective for operator replacement and architecture restructuring, enabling significant compute savings while maintaining competitive performance.

## Foundational Learning
- Diffusion Transformers (DiTs): Understanding the architecture and training process of DiTs is crucial for implementing grafting. Quick check: Can you describe the key components of a DiT and how they differ from standard transformers?
- Activation Distillation: This technique is used to initialize new operators with pretrained knowledge. Quick check: How does activation distillation differ from standard knowledge distillation, and what are its benefits?
- Operator Replacement: Grafting allows for replacing operators like attention mechanisms with alternatives like gated convolution or linear attention. Quick check: What are the advantages and disadvantages of different attention mechanisms in transformers?

## Architecture Onboarding
- Component Map: Pretrained DiT -> Activation Distillation -> Lightweight Finetuning -> Modified DiT
- Critical Path: The critical path involves distilling activations from the pretrained DiT to initialize new operators, followed by lightweight finetuning to adapt these operators to the new architecture.
- Design Tradeoffs: Grafting offers a tradeoff between computational efficiency and performance. While it enables significant compute savings, there may be a slight drop in performance compared to full retraining.
- Failure Signatures: If grafting fails, it may be due to poor initialization of new operators or insufficient finetuning. Signs of failure include degraded performance or instability during training.
- First Experiments: 1) Operator replacement: Replace softmax attention with gated convolution and evaluate performance. 2) Architecture restructuring: Convert sequential transformer blocks into parallel and assess impact on compute and quality. 3) Depth reduction: Reduce the depth of a pretrained DiT and measure the tradeoff between compute savings and performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation scope is primarily constrained to controlled diffusion transformer variants with similar pretraining objectives.
- The general applicability to other diffusion architectures (e.g., DDPM, score-based models) or multimodal extensions remains untested.
- The 2.38-2.64 FID range on ImageNet-1K, though competitive, shows a consistent gap versus the 2.27 baseline.

## Confidence
- High Confidence: The empirical demonstration of operator replacement and depth restructuring within diffusion transformers. The relative compute efficiency (sub-2% of pretraining cost) and quantitative results on ImageNet-1K and PixArt-Σ are well-supported by reported metrics and available code.
- Medium Confidence: Generalization claims to arbitrary architectures and diffusion paradigms. The case study on DiT-XL/2 depth reduction is compelling but based on a single example, limiting broad architectural claims.
- Low Confidence: Long-term stability and generalization to out-of-distribution tasks or datasets beyond those evaluated. The quality of generated images at extreme resolutions or in non-standard domains is not assessed.

## Next Checks
1. Test grafting on non-DiT diffusion architectures (e.g., DDPM, latent diffusion) to assess cross-architecture applicability.
2. Evaluate on out-of-distribution datasets (e.g., COCO, FFHQ) and downstream tasks (e.g., inpainting, super-resolution) to measure robustness.
3. Perform ablation studies isolating the contributions of activation distillation vs. lightweight finetuning to quantify their relative importance.