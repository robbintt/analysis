---
ver: rpa2
title: 'ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs'
arxiv_id: '2509.04696'
source_url: https://arxiv.org/abs/2509.04696
tags:
- odke
- facts
- knowledge
- extraction
- ontology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ODKE+ is a production-grade system for scalable open-domain knowledge
  extraction using large language models (LLMs). It automatically ingests millions
  of facts from web sources with high precision by combining modular components: an
  Extraction Initiator detects missing or stale facts, an Evidence Retriever collects
  supporting documents, hybrid Knowledge Extractors apply pattern-based rules and
  ontology-guided LLM prompting, a lightweight Grounder validates facts against source
  context, and a Corroborator ranks and normalizes candidates.'
---

# ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs

## Quick Facts
- arXiv ID: 2509.04696
- Source URL: https://arxiv.org/abs/2509.04696
- Reference count: 4
- Ingests 19M+ high-confidence facts from 9M+ Wikipedia pages with 98.8% precision

## Executive Summary
ODKE+ is a production-grade system for scalable open-domain knowledge extraction using large language models (LLMs). It automatically ingests millions of facts from web sources with high precision by combining modular components: an Extraction Initiator detects missing or stale facts, an Evidence Retriever collects supporting documents, hybrid Knowledge Extractors apply pattern-based rules and ontology-guided LLM prompting, a lightweight Grounder validates facts against source context, and a Corroborator ranks and normalizes candidates. ODKE+ dynamically generates ontology snippets tailored to each entity type, enabling scalable, type-consistent extraction across 195 predicates. The system processes over 9 million Wikipedia pages, ingesting 19 million high-confidence facts with 98.8% precision. It achieves up to 48% overlap with third-party knowledge graphs and reduces update lag by 50 days on average, demonstrating that LLM-based extraction grounded in ontological structure can deliver trustworthy, production-scale knowledge ingestion.

## Method Summary
ODKE+ implements a 5-stage pipeline: (1) Extraction Initiator monitors Wikipedia edits to detect stale or missing facts, (2) Evidence Retriever fetches supporting documents from a web crawl index, (3) hybrid Knowledge Extractors use pattern-based regex rules for infoboxes plus LLM-based extraction guided by dynamically generated ontology snippets, (4) Grounder LLM performs binary verification of fact grounding against source context, and (5) Corroborator normalizes values, consolidates duplicates, and scores candidates using H2O AutoML. The system processes 150-250K facts daily with <2hr latency, achieving 98.8% precision through two-stage filtering and ontology-aware prompting.

## Key Results
- Extracts 19 million high-confidence facts from 9 million Wikipedia pages
- Achieves 98.8% precision on ground truth validation
- Reduces knowledge graph update lag by 50 days on average
- Achieves 48% overlap with third-party knowledge graphs

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Ontology-Guided Prompting
ODKE+ constrains LLM extraction using dynamically generated ontology snippets rather than generic prompts. The system pre-computes a "Predicate 360" view containing predicate names, descriptions, domain/range constraints, and expected units. When processing an entity, only the relevant ontology snippet is injected into the prompt, forcing the LLM to map unstructured text directly to valid schema predicates. This reduces schema violations by ensuring the LLM adheres to the provided schema constraints. The core assumption is that the LLM possesses sufficient instruction-following capability to adhere to these constraints.

### Mechanism 2: Two-Stage Filtering (Grounding + Corroboration)
Extracted triples undergo sequential verification: first, a lightweight LLM (Grounder) performs binary validation ("Is this triple explicitly supported by context?"), filtering out hallucinations. Surviving candidates enter the Corroborator, which normalizes values, aggregates duplicates, and scores candidates based on confidence, frequency, and extractor type. This two-stage approach reduces hallucinations by 35% and improves precision from 91% to 98.8%. The core assumption is that the Grounder is more reliable at verification than the extraction LLM is at self-correction.

### Mechanism 3: Change-Driven Extraction Initiation
The Extraction Initiator monitors external change signals (Wikipedia edits) to trigger extraction, rather than exhaustive batch processing. It detects pages where the KG is stale or missing facts compared to the new web state, prioritizing compute for entities where information has likely changed. This approach improves freshness by reducing update lag by 50 days on average. The core assumption is that web page modification timestamps correlate with factual changes.

## Foundational Learning

- **Concept: Subject-Predicate-Object (SPO) Triples & Ontology**
  - Why needed: The system's entire output is structured SPO triples. Understanding "Domain" (valid subjects) and "Range" (valid objects) is required to configure the ontology snippets that guide LLM extraction.
  - Quick check: Can you explain why defining a "Range" constraint for a predicate like `date_of_birth` prevents an LLM from inserting a location string into that field?

- **Concept: Hallucination in Generative Models**
  - Why needed: The "Grounder" is explicitly designed to combat LLM hallucinations. Recognizing why an LLM might invent a fact (statistical likelihood vs. grounded truth) helps in tuning the verification prompt.
  - Quick check: If an LLM extracts "Population: 5 million" but the source text only says "a large population," should the Grounder accept or reject this? (Hint: The paper implies rejection of unverified inference).

- **Concept: Entity Linking / Resolution**
  - Why needed: The "Ingestion" stage must map extracted text (e.g., "London") to a canonical KG ID (e.g., Q84). If this fails, the KG fragments into duplicate entities.
  - Quick check: How does the system handle an extracted entity string that has no existing match in the KG? (Ref: Section 3.5).

## Architecture Onboarding

- **Component map:** Initiator -> Retriever -> Extractor (Pattern-based OR LLM + Ontology Snippet) -> Grounder -> Corroborator -> Ingestion
- **Critical path:** The generation of the Ontology Snippet is critical. If the snippet contains incorrect predicates for an entity type, the LLM will extract garbage or nothing. Ensure the "Predicate Enrichment" pipeline runs correctly before extraction.
- **Design tradeoffs:**
  - Precision vs. Cost: The Grounder adds latency and compute cost (second LLM call) but is credited for the 35% reduction in hallucinations.
  - Freshness vs. Noise: Streaming mode offers 50-day faster updates but risks processing volatile/vandalized pages compared to stable batch processing.
- **Failure signatures:**
  - Low Precision (<95%): Likely a failure in the Grounder prompt or a drift in the LLM's behavior (Model degradation).
  - Schema Violations: The Ontology Snippet is likely outdated or the LLM failed to follow the "normalized answer" instruction.
  - Stale Data: The Initiator is missing edit signals or the streaming pipeline is lagging (>2 hours latency).
- **First 3 experiments:**
  1. Probe the Grounder: Run a set of known "hallucinated" or "implied-but-not-stated" examples through the Grounder module in isolation to verify it rejects them. (Ref: Appendix B prompt).
  2. Validate Ontology Snippet Generation: Pick a diverse entity type (e.g., "Chemical Element") and inspect the generated Ontology Snippet to ensure the predicates (e.g., "atomic number") are present and enriched correctly.
  3. Stress Test the Initiator: Simulate high-velocity edits on a test page to observe if the Initiator correctly deduplicates or queues the extraction jobs without crashing the downstream Retriever.

## Open Questions the Paper Calls Out

The authors explicitly state in the Ethics Statement that while modules exist for factual accuracy, "mitigating underlying model biases remains a challenge and is an area for future work." The current verification steps (Grounder and Corroborator) focus on factual consistency with source text, not on correcting systemic biases inherent in the pre-trained models or source data.

## Limitations
- System relies on proprietary ontologies and KG schemas, limiting reproducibility
- Exact LLM model versions and configurations are not disclosed
- Grounder's binary verification may overly penalize factually correct but implicitly stated information

## Confidence
- High: Precision claims, Corroboration filtering mechanism, Schema alignment via ontology snippets
- Medium: Overlap with third-party KGs, Freshness improvement metric
- Low: Reproducibility of exact KG schema, Long-term LLM reliability

## Next Checks
1. Probe the Grounder: Run a set of known "hallucinated" or "implied-but-not-stated" examples through the Grounder module in isolation to verify it rejects them. (Ref: Appendix B prompt).
2. Validate Ontology Snippet Generation: Pick a diverse entity type (e.g., "Chemical Element") and inspect the generated Ontology Snippet to ensure the predicates (e.g., "atomic number") are present and enriched correctly.
3. Stress Test the Initiator: Simulate high-velocity edits on a test page to observe if the Initiator correctly deduplicates or queues the extraction jobs without crashing the downstream Retriever.