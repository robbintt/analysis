---
ver: rpa2
title: Improving Sequential Recommenders through Counterfactual Augmentation of System
  Exposure
arxiv_id: '2504.13482'
source_url: https://arxiv.org/abs/2504.13482
tags:
- user
- exposure
- items
- recommendation
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CaseRec improves sequential recommendation by modeling both interacted
  and non-interacted items in system exposure sequences, addressing the limitation
  of existing methods that overlook exposed but non-interacted items. The approach
  uses a decision transformer-based sequential model combined with reinforcement learning
  to account for different exposure rewards based on user feedback.
---

# Improving Sequential Recommenders through Counterfactual Augmentation of System Exposure

## Quick Facts
- arXiv ID: 2504.13482
- Source URL: https://arxiv.org/abs/2504.13482
- Reference count: 40
- Primary result: Improves sequential recommendation by modeling both interacted and non-interacted items in system exposure sequences

## Executive Summary
CaseRec addresses the limitation of existing sequential recommendation methods that overlook exposed but non-interacted items by modeling system exposure sequences. The framework uses a decision transformer-based sequential model combined with reinforcement learning to account for different exposure rewards based on user feedback. To uncover unseen user interests, CaseRec introduces counterfactual augmentation through two strategies: Random (uniform sampling of new items) and Self-Improving (perturbing historical items and generating new sequences). Experiments on three real-world datasets demonstrate significant improvements over state-of-the-art methods.

## Method Summary
CaseRec models system exposure sequences by combining interacted and non-interacted items, using a decision transformer-based sequential model with reinforcement learning to account for exposure rewards. The framework introduces counterfactual augmentation to uncover unseen user interests through two strategies: Random, which uniformly samples new items, and Self-Improving, which perturbs historical items to generate new sequences. A transformer-based user simulator predicts feedback for counterfactual items, enabling the construction of augmented exposure sequences. This approach addresses exposure bias and improves recommendation diversity while achieving state-of-the-art performance on three real-world datasets.

## Key Results
- Recall@20 improvements of up to 0.3217 over state-of-the-art methods
- NDCG@20 improvements of up to 0.1579 on three real-world datasets
- Demonstrated reduced exposure bias and improved recommendation diversity

## Why This Works (Mechanism)
CaseRec works by explicitly modeling the system's exposure history rather than just user interactions. By incorporating both interacted and non-interacted items in exposure sequences, the model captures the full context of what users have been shown. The counterfactual augmentation strategies expand the model's understanding of user interests beyond observed interactions by simulating alternative exposure scenarios. The transformer-based user simulator enables the generation of realistic counterfactual feedback, while the decision transformer framework with reinforcement learning optimizes for long-term user satisfaction rather than just immediate clicks.

## Foundational Learning
- **System exposure sequences**: Understanding that recommendation systems actively expose users to items, not just passively record interactions - needed to capture the full context of user behavior and avoid exposure bias
- **Counterfactual augmentation**: Creating synthetic scenarios that didn't occur but could have - needed to uncover hidden user preferences and improve model robustness
- **Decision transformer framework**: Using reinforcement learning for sequential decision making in recommendation - needed to optimize for long-term user satisfaction beyond immediate feedback
- **Transformer-based user simulation**: Predicting user responses to unseen items - needed to generate realistic counterfactual exposure sequences
- **Exposure reward modeling**: Differentiating between various types of user feedback (clicks, skips, purchases) - needed to capture nuanced user preferences and behaviors

## Architecture Onboarding

Component map: User Interaction History -> Counterfactual Augmentation -> Exposure Sequence Generator -> Transformer-based User Simulator -> Decision Transformer Model -> Reinforcement Learning Optimizer

Critical path: The core workflow involves taking historical user interaction data, generating counterfactual exposure sequences through augmentation strategies, using the user simulator to predict feedback for these counterfactual items, and then training the decision transformer model with reinforcement learning to optimize long-term user satisfaction.

Design tradeoffs: The framework trades computational complexity for improved recommendation quality by using counterfactual augmentation and user simulation. The choice between Random and Self-Improving strategies involves balancing exploration (Random) against exploitation of historical patterns (Self-Improving).

Failure signatures: Poor performance may result from inaccurate user simulator predictions, ineffective counterfactual augmentation that doesn't reflect realistic user behavior, or reinforcement learning that overfits to short-term rewards rather than long-term satisfaction.

First experiments to run:
1. Baseline comparison with and without counterfactual augmentation to isolate its impact
2. Ablation study comparing Random vs Self-Improving augmentation strategies
3. Sensitivity analysis of user simulator accuracy on final recommendation performance

## Open Questions the Paper Calls Out
None

## Limitations
- The transformer-based user simulator's accuracy directly impacts the quality of augmented exposure sequences, and any biases could propagate through the entire pipeline
- Counterfactual augmentation strategies may not adequately capture rapidly changing user preferences or long-term dependencies
- The framework's reliance on specific feedback patterns may not generalize well to scenarios with sparse or noisy user feedback

## Confidence
High confidence: Empirical results showing improvements over state-of-the-art methods on three real-world datasets are well-supported by experimental methodology and evaluation metrics.

Medium confidence: Claims about reduced exposure bias and improved recommendation diversity are supported by experimental results but would benefit from additional analysis of underlying mechanisms.

Low confidence: Generalizability of counterfactual augmentation strategies to other recommendation scenarios with different user behavior patterns or item characteristics.

## Next Checks
1. Conduct ablation studies to quantify individual contributions of Random and Self-Improving counterfactual augmentation strategies to overall performance improvements.

2. Perform cross-dataset validation to assess generalizability across different recommendation domains and user behavior patterns.

3. Implement robustness tests by introducing controlled noise in user simulator predictions to evaluate framework sensitivity to simulator accuracy.