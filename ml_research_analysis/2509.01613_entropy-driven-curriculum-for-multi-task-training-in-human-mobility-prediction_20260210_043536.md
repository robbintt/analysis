---
ver: rpa2
title: Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction
arxiv_id: '2509.01613'
source_url: https://arxiv.org/abs/2509.01613
tags:
- mobility
- prediction
- human
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified training framework that integrates
  entropy-driven curriculum learning and multi-task learning for human mobility prediction.
  The entropy-driven curriculum quantifies trajectory predictability using normalized
  Lempel-Ziv compression, organizing training from simple to complex patterns based
  on information-theoretic principles.
---

# Entropy-Driven Curriculum for Multi-Task Training in Human Mobility Prediction

## Quick Facts
- arXiv ID: 2509.01613
- Source URL: https://arxiv.org/abs/2509.01613
- Reference count: 40
- Primary result: Achieves GEO-BLEU 0.354 and DTW 26.15 on YJMob100K, outperforming existing methods

## Executive Summary
This paper presents MoBERT, a unified training framework that integrates entropy-driven curriculum learning with multi-task learning for human mobility prediction. The method quantifies trajectory predictability using normalized Lempel-Ziv entropy, organizing training from simple to complex patterns based on information-theoretic principles. This approach achieves up to 2.92-fold convergence speed compared to random sampling. The multi-task component simultaneously optimizes location prediction alongside auxiliary distance and direction estimation tasks, achieving state-of-the-art performance on the YJMob100K dataset while demonstrating robust cross-city generalization capabilities.

## Method Summary
The method uses a BERT-style encoder (8 layers, 8 heads, 256-dim) with three task heads for location, distance, and direction prediction. Trajectories are discretized into a 200×200 grid and represented with eight features (spatial, temporal, semantic). The entropy-driven curriculum uses normalized Lempel-Ziv entropy to sort trajectories by difficulty across three stages (H_norm-LZ <0.4, <0.65, all), with progressively longer prediction horizons. Multi-task learning balances location prediction with auxiliary distance (4 bins) and direction (9 classes) classification tasks using tuned weights (λ₁=0.5, λ₂=0.8). Feature interaction uses MHSA over the feature dimension with residual summation.

## Key Results
- Achieves GEO-BLEU score of 0.354 and DTW distance of 26.15 on YJMob100K dataset
- Outperforms existing methods including top HuMob Challenge 2023 performers
- Demonstrates 2.92-fold faster convergence compared to random sampling
- Shows robust cross-city generalization, achieving competitive results on unseen urban environments

## Why This Works (Mechanism)

### Mechanism 1
Organizing training by trajectory entropy (low to high) accelerates convergence and improves final performance. The normalized Lempel-Ziv entropy estimator quantifies predictability from sequence compressibility; low-entropy trajectories have fewer unique subsequences, indicating regularity. By Fano's inequality, lower entropy bounds prediction error lower, making these samples intrinsically learnable earlier. The curriculum stages progressively introduce higher-entropy data and longer prediction horizons.

### Mechanism 2
Jointly predicting location, distance, and direction improves location prediction accuracy through complementary supervision. Auxiliary classification heads on distance (4 bins) and direction (9 classes) share the encoder and inject spatial constraints—distance bounds the spatial search space; direction provides orientation priors. The total loss balances tasks using tuned weights (λ₁≈0.5, λ₂≈0.8).

### Mechanism 3
Attention-based feature interaction over embeddings (time, space, semantics) improves representation quality. Eight features are embedded, stacked along a feature dimension, and fused via MHSA with a residual summation path. MHSA learns dynamic feature weights (e.g., POI-time during leisure, space-time during commute).

## Foundational Learning

**Concept: Entropy and Fano's inequality**
- Why needed here: Establishes the theoretical link between sequence entropy and predictability bounds; justifies curriculum ordering.
- Quick check question: Can you explain why lower entropy permits higher prediction accuracy given a fixed alphabet size?

**Concept: Curriculum learning principles**
- Why needed here: Motivates training from simple to complex to improve optimization and regularization.
- Quick check question: Why might random sampling underperform when data complexity is heterogeneous?

**Concept: Multi-task learning with shared encoder**
- Why needed here: Explains how auxiliary tasks regularize and enrich shared representations.
- Quick check question: What risks arise if auxiliary task loss scales are not balanced with the primary task?

## Architecture Onboarding

**Component map:** Feature embedding and MHSA interaction module -> BERT-style encoder (8 layers, 8 heads, 256-dim) -> Three parallel FFN heads (location, distance, direction)

**Critical path:** (1) Preprocess 8 features; (2) embed and stack; (3) MHSA feature fusion + residual sum; (4) encoder output; (5) three parallel FFN heads; (6) aggregate loss

**Design tradeoffs:** Classification vs regression for auxiliary tasks (paper chooses 4/9-class classification); λ tuning critical (overweighting auxiliary tasks harms primary performance); curriculum stages (3 stages with entropy thresholds 0.4, 0.65) require dataset-specific calibration

**Failure signatures:** Validation loss plateau early; DTW degrades more than GEO-BLEU; auxiliary task accuracy high but location accuracy stagnant (indicates task misalignment)

**First 3 experiments:**
1. Baseline MoBERT with only location prediction to isolate multi-task gains
2. Ablate feature interaction (use simple sum fusion) to quantify MHSA contribution
3. Run with vs without curriculum (same total epochs) to measure convergence speedup and final metric deltas

## Open Questions the Paper Calls Out

**Open Question 1**
Can the entropy-driven curriculum be adapted for continuous coordinate spaces or non-grid-based trajectory representations without relying on discretization for Lempel-Ziv symbolization? The current implementation depends on discrete location tokens to compute the Lempel-Ziv complexity, making it incompatible with regression-based prediction tasks or raw GPS trajectories.

**Open Question 2**
How does the normalized Lempel-Ziv entropy compare to other information-theoretic or spatial complexity metrics (e.g., spectral entropy or approximate entropy) in determining optimal curriculum pacing? While the paper proves the proposed estimator works, it leaves open the possibility that other metrics could provide a smoother difficulty gradient or faster convergence rates.

**Open Question 3**
Does the observed cross-city generalization persist when transferring the model to urban environments with significantly distinct structural layouts (e.g., from grid-planned cities to organically developed cities)? The "zero-shot" capability was tested on a specific challenge dataset; it remains unclear if the "universal" mobility patterns learned generalize across different cultural or infrastructural design paradigms.

## Limitations
- The entropy-driven curriculum relies on static entropy estimates computed offline, which may degrade if trajectory patterns shift during training
- Multi-task auxiliary losses use discretized distance and direction bins that may not align with true mobility dynamics
- Cross-city generalization claims are based on single-city training without addressing multi-city joint training scenarios

## Confidence

**High:** The information-theoretic foundation (Fano's inequality) linking entropy to predictability bounds is well-established and directly applicable.

**Medium:** Multi-task learning with auxiliary spatial tasks (distance/direction) shows consistent gains in related work, but discretization choices and λ-weighting require careful tuning.

**Low:** The entropy-driven curriculum for mobility prediction is novel; limited direct evidence exists for its stability across datasets or training dynamics.

## Next Checks

1. **Curriculum Robustness:** Test curriculum sensitivity by randomly shuffling training order within entropy bins and measuring GEO-BLEU/DTW deltas; verify that performance drops without curriculum.

2. **Auxiliary Task Alignment:** Freeze location prediction head and train only auxiliary tasks; evaluate whether distance/direction predictions correlate with location accuracy improvements.

3. **Cross-City Transfer:** Train MoBERT on a single city, then evaluate on multiple unseen cities with varying entropy distributions; measure if curriculum thresholds (0.4, 0.65) require recalibration.