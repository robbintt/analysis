---
ver: rpa2
title: 'AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for
  Autonomous Driving'
arxiv_id: '2412.15206'
source_url: https://arxiv.org/abs/2412.15206
tags:
- performance
- driving
- arxiv
- question
- drivevlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AutoTrust, a comprehensive benchmark for
  evaluating the trustworthiness of large vision-language models (VLMs) in autonomous
  driving (DriveVLMs). The benchmark assesses models across five dimensions: trustfulness,
  safety, robustness, privacy, and fairness, using over 18k questions derived from
  10k driving scenes.'
---

# AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving

## Quick Facts
- arXiv ID: 2412.15206
- Source URL: https://arxiv.org/abs/2412.15206
- Reference count: 40
- Evaluates DriveVLMs across five trustworthiness dimensions, finding generalist models outperform specialist models

## Executive Summary
AutoTrust introduces a comprehensive benchmark for evaluating the trustworthiness of large vision-language models (VLMs) in autonomous driving applications. The benchmark assesses models across five dimensions—trustfulness, safety, robustness, privacy, and fairness—using over 18,000 questions derived from 10,000 driving scenes. Evaluations of six VLMs reveal that generalist models like GPT-4o-mini and LLaVA-v1.6 outperform specialist DriveVLMs in trustworthiness metrics. DriveVLMs show significant vulnerabilities in privacy leakage, robustness to adversarial attacks, and fairness across diverse populations. The findings highlight critical trustworthiness concerns that must be addressed before deploying VLMs in safety-critical autonomous driving systems.

## Method Summary
AutoTrust employs a multi-stage methodology to evaluate DriveVLMs. First, 10,000 driving scenes are curated from 8 sources (e.g., NuScenes, Cityscapes) and filtered to front-camera images. GPT-4o generates and verifies 18,000 Visual Question Answering (VQA) pairs across five trustworthiness dimensions. The benchmark evaluates six VLMs using specific prompt templates, with open-ended answers scored by GPT-4o-based reward scoring (1-10 scale). Robustness and privacy are assessed via abstention rates, while fairness is measured using Demographic Accuracy Difference (DAD) and Worst Accuracy (WA). White-box safety attacks employ PGD and BIM with $\epsilon=16$ and 10 steps.

## Key Results
- Generalist models (GPT-4o-mini, LLaVA-v1.6) outperform specialist DriveVLMs in trustworthiness metrics
- DriveLM-Agent exhibits significant privacy leakage (12.3% abstention rate) and fairness gaps (DAD=0.15)
- Specialist models show overfitting to driving-specific tasks, degrading overall trustworthiness
- PGD and BIM attacks reduce DriveVLM safety performance by up to 35%

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of trustworthiness dimensions and rigorous evaluation methodology. By using GPT-4o for both dataset generation and scoring, AutoTrust ensures high-quality, contextually relevant questions that capture real-world driving scenarios. The five-dimensional framework systematically identifies vulnerabilities that generic LLM benchmarks miss, while the adversarial testing reveals specific failure modes in DriveVLMs. The use of abstention rates and demographic metrics provides quantitative measures of model reliability and fairness that are directly applicable to safety-critical autonomous driving deployment decisions.

## Foundational Learning
- **Vision-Language Model Architecture**: Understanding how VLMs process multimodal inputs is crucial for interpreting benchmark results. Quick check: Verify that your model can handle both image and text inputs simultaneously.
- **Visual Question Answering**: The core task format determines how trustworthiness is evaluated. Quick check: Ensure your VQA pipeline can handle both closed and open-ended questions.
- **Adversarial Attack Methods**: PGD and BIM are used to test robustness. Quick check: Confirm that attack implementations match the specified hyperparameters (ε=16, 10 steps).
- **Fairness Metrics**: DAD and WA quantify demographic performance differences. Quick check: Validate that your fairness calculations handle all 8 demographic groups correctly.
- **Reward-Based Evaluation**: GPT-4o scoring provides subjective quality assessment. Quick check: Test the reward model on a small sample to calibrate scoring consistency.

## Architecture Onboarding

**Component Map**: Data Sources → Curation Pipeline → VQA Generation → Model Inference → Evaluation Metrics

**Critical Path**: The benchmark execution follows: (1) Load driving datasets, (2) Filter and preprocess images, (3) Generate VQA pairs using GPT-4o, (4) Run inference on target models, (5) Score responses and calculate metrics.

**Design Tradeoffs**: The heavy reliance on GPT-4o enables high-quality data but introduces potential biases and high computational costs. Focusing on front-camera imagery simplifies evaluation but misses important perspectives. Using abstention rates captures uncertainty but may not reflect all privacy concerns.

**Failure Signatures**: 
- Models consistently outputting identical responses indicate collapsed behavior
- High abstention rates suggest privacy concerns or uncertainty
- Large DAD values indicate demographic bias
- Low robustness scores under PGD/BIM attacks reveal vulnerability to adversarial inputs

**First 3 Experiments**:
1. Run the data curation pipeline on a small subset (100 scenes) to validate GPT-4o integration and question quality
2. Test a baseline model (LLaVA-v1.6) on the full curated dataset to establish reference performance
3. Apply PGD attacks to a single model to verify robustness evaluation methodology

## Open Questions the Paper Calls Out
- How can AutoTrust be expanded to encompass the full end-to-end autonomous driving pipeline, including prediction and planning modules?
- What training methodologies can mitigate overfitting in specialist DriveVLMs to match generalist trustworthiness?
- How can DriveVLMs be effectively aligned with safety-critical objectives to defend against novel, adaptive adversarial attack strategies?

## Limitations
- Heavy reliance on GPT-4o for both dataset curation and evaluation introduces potential biases
- Focus on front-facing camera imagery excludes critical in-cabin and side/rear camera perspectives
- Adversarial robustness tests may not capture all realistic attack vectors in production environments
- Fairness assessment covers 8 demographic groups but may not be exhaustive for global driving populations

## Confidence

- **High Confidence**: Generalist VLMs outperforming specialist models is well-supported by systematic evaluation
- **Medium Confidence**: Specific numerical results for privacy leakage and fairness require careful interpretation given GPT-4o-dependent evaluation
- **Low Confidence**: Generalizability to all autonomous driving contexts is uncertain due to specific dataset composition

## Next Checks
1. Reproduce benchmark results using alternative reward models (e.g., Claude-3, open-source reward models) to assess sensitivity
2. Test DriveVLMs on additional camera perspectives (side, rear, and in-cabin) to evaluate trustworthiness gaps
3. Conduct real-world adversarial testing with human-in-the-loop scenarios to validate PGD/BIM-based robustness scores