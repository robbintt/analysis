---
ver: rpa2
title: Unified token representations for sequential decision models
arxiv_id: '2510.21448'
source_url: https://arxiv.org/abs/2510.21448
tags:
- decision
- unified
- token
- gated
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Unified Token Representation (UTR) to address
  redundancy and quadratic attention complexity in transformer-based offline reinforcement
  learning models. UTR merges return-to-go, state, and action into a single token,
  reducing sequence length and computational cost.
---

# Unified token representations for sequential decision models

## Quick Facts
- **arXiv ID**: 2510.21448
- **Source URL**: https://arxiv.org/abs/2510.21448
- **Reference count**: 3
- **Primary result**: Introduces UTR to reduce redundancy and quadratic attention complexity in transformer-based offline RL, achieving comparable performance to DT/DC/DMamba with 67-75% fewer FLOPs and 30% faster inference

## Executive Summary
This paper addresses the redundancy and computational inefficiency in transformer-based offline reinforcement learning by introducing Unified Token Representation (UTR). UTR merges return-to-go, state, and action into a single token per timestep, reducing sequence length and computational cost. The authors propose two model variants: UDT (transformer-based) and UDC (gated CNN-based), both leveraging UTR. Theoretical analysis shows UTR yields tighter Rademacher complexity bounds, indicating better generalization. Experiments on D4RL benchmarks demonstrate that UDT and UDC achieve comparable or superior performance to state-of-the-art methods while significantly reducing FLOPs (up to 74.92%) and inference time (up to 30.02%).

## Method Summary
The paper introduces Unified Token Representation (UTR) to merge return-to-go, state, and action into a single token, reducing sequence length from 3L to L. The method uses a gated RTG embedding, shifted action input, and concatenation with state, followed by a projection layer. Two model variants are proposed: UDT uses standard transformer attention on UTR tokens, while UDC replaces attention with Gated Depthwise Convolutional Modules that capture local temporal dependencies efficiently. Both models are trained autoregressively to predict actions using MSE loss on D4RL offline datasets.

## Key Results
- UTR reduces sequence length from 3L to L, achieving 67.34% FLOP reduction for UDT and 74.92% for UDC
- UDT and UDC match or exceed performance of DT, DC, and DMamba on 11/14 MuJoCo and AntMaze tasks
- Theoretical analysis shows UTR yields tighter Rademacher complexity bounds under linear model assumptions
- UDC achieves 30.02% inference time reduction while maintaining competitive performance
- Performance gains are consistent across diverse D4RL datasets with varying difficulty levels

## Why This Works (Mechanism)

### Mechanism 1: Unified Token Representation Reduces Sequence Redundancy
- Claim: Merging return-to-go (RTG), state, and action into a single token reduces sequence length from 3L to L, lowering computational cost and improving generalization bounds.
- Mechanism: The model concatenates a gated RTG embedding, the current state, and a shifted previous action into one vector per timestep. This unified token is projected and layer-normalized. Theoretical analysis shows this representation yields a tighter Rademacher complexity bound, suggesting reduced overfitting risk on limited offline data. Empirically, this reduces FLOPs by 67-75%.
- Core assumption: RTG, state, and action information can be fused into a single vector without losing essential cross-modal interactions.
- Evidence anchors: Abstract states UTR "substantially reduces sequence length and model complexity"; Theorem 2 proves lower Rademacher upper bound for merged vs. separated tokens.

### Mechanism 2: Gated CNN Captures Local Temporal Dependencies Efficiently
- Claim: A Gated Depthwise Convolutional Module can replace global self-attention for RL tasks, capturing dependencies in linear time with competitive performance.
- Mechanism: The module applies causal depthwise separable convolution to capture local patterns, with a parallel gating branch using SiLU non-linearity. The element-wise product of these branches adaptively filters information. Stacking such modules expands the effective receptive field.
- Core assumption: The critical dependencies for decision-making in the target tasks are predominantly local.
- Evidence anchors: Section 3.3 motivated by findings that gated CNNs excel on short causal sequences; Table 1 shows UDC outperforms or matches transformer baselines on 11/14 tasks.

### Mechanism 3: Conditional Alignment via Shifted Action & Gated RTG
- Claim: Shifting the action sequence and applying a sigmoid gate to RTG ensures correct causal prediction and stable gradient flow.
- Mechanism: The action is shifted forward, so input at step t contains $a_{t-1}$, enabling autoregressive prediction of $a_t$. The RTG scalar is passed through a linear layer and a sigmoid gate, normalizing its influence.
- Core assumption: The previous action is a useful feature for predicting the current action, and a gated projection is sufficient to modulate policy behavior based on target return.
- Evidence anchors: Equations 1-2 define the gated RTG embedding and shifted action, described as ensuring "correct causal alignment."

## Foundational Learning

- **Concept**: Rademacher Complexity
  - Why needed here: The paper uses this statistical tool to theoretically justify that unified tokenization should generalize better from limited data.
  - Quick check question: Would a model with lower Rademacher complexity always perform better on a specific test set? (Answer: No, it indicates a tighter generalization bound, not absolute performance).

- **Concept**: Causal & Depthwise Separable Convolution
  - Why needed here: The UDC architecture relies on this for efficient, non-leaky temporal processing.
  - Quick check question: How does a "causal" convolution differ from a standard convolution for time-series? (Answer: It only operates on past and current inputs, not future ones, preserving temporal order).

- **Concept**: Return-to-Go (RTG)
  - Why needed here: A core input to DT and UTR, representing the target cumulative reward, used to condition policy generation.
  - Quick check question: In UTR, how is the scalar RTG value processed before being merged with the state and action? (Answer: It is projected to a latent vector and passed through a sigmoid gate).

## Architecture Onboarding

- **Component map**: Input Processing: $R_t, s_t, a_{t-1}$ -> UTR Fusion -> Gated CNN Blocks -> Action Head
- **Critical path**: Input Processing: $R_t, s_t, a_{t-1}$ -> UTR Fusion -> Gated CNN Blocks -> Action Head. The efficiency gain hinges on the UTR reducing the sequence length before it enters the main model backbone.
- **Design tradeoffs**:
  - UDT vs. UDC: UDT preserves full attention for global reasoning but has $O(L^2)$ complexity. UDC is $O(L)$ and more efficient but trades off global receptive field for local inductive bias.
  - Unified vs. Separate Tokens: Unified tokens maximize efficiency and theoretical generalization but may obscure modality-specific features that explicit attention could capture.
- **Failure signatures**:
  - Long-horizon degradation: UDC performance drops sharply on tasks requiring credit assignment over hundreds of steps
  - RTG insensitivity: Policy performance becomes constant regardless of the target RTG provided at test time
  - Mode collapse: The model predicts the mean action or a constant output
- **First 3 experiments**:
  1. Baseline Efficiency Benchmark: Implement and measure FLOPs & inference time for DT, UDT, and UDC on a fixed sequence length
  2. Ablation: UTR vs. Separate Tokens: Train two models (same backbone) with and without UTR on a single D4RL task to isolate the impact of tokenization
  3. Receptive Field Test: Evaluate UDC on a synthetic task requiring explicit long-range dependency use to identify its modeling limit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Unified Token Representation (UTR) maintain its efficiency and performance advantages when applied to significantly larger decision models and more complex environments?
- Basis in paper: The conclusion states, "In future work, we plan to explore extending UTR to larger models and more complex environments."
- Why unresolved: The current study evaluates UTR on standard D4RL benchmarks using relatively small model configurations.
- What evidence would resolve it: Empirical results from scaling UTR to high-dimensional tasks or large transformer backbones showing sustained performance with lower resource usage.

### Open Question 2
- Question: Can UTR be effectively integrated with advanced sequence modeling architectures, such as state-space models (SSMs) or hybrid attention mechanisms?
- Basis in paper: The authors propose "integrating it with advanced modeling techniques to further improve scalability and generalization in high-dimensional decision tasks."
- Why unresolved: The paper introduces UDT and UDC but does not explore UTR within other dominant architectures like Mamba or selective SSMs.
- What evidence would resolve it: Implementation of UTR in SSM-based architectures demonstrating compatibility and performance metrics.

### Open Question 3
- Question: At what model scale or sequence length does the theoretical FLOP reduction of UTR translate into significant wall-clock latency improvements?
- Basis in paper: The efficiency analysis shows a 67.34% FLOP reduction for UDT but only a 5.56% reduction in wall-clock time, attributed to GPU parallelism and I/O bottlenecks.
- Why unresolved: It is unclear if the theoretical complexity gains overcome hardware overheads in the current small-scale regime.
- What evidence would resolve it: Profiling experiments on larger models or longer sequences identifying the specific threshold where wall-clock speedup matches FLOP reduction.

## Limitations

- The specific architectural hyperparameters (layer counts, hidden dimensions, kernel sizes) for UDT and UDC are referenced to an appendix not provided, preventing exact replication
- The Rademacher complexity analysis, while mathematically correct, provides an upper bound on generalization error that doesn't directly translate to empirical performance on test sets
- No ablation studies isolate the contribution of UTR from the choice of backbone (transformer vs. CNN), making it difficult to attribute performance gains

## Confidence

- **High**: Claims about UTR reducing sequence length and computational complexity (FLOPs/infer time reductions)
- **Medium**: Claims about UDC achieving competitive performance through local dependency modeling (based on strong empirical results but limited ablation)
- **Low**: Claims about theoretical generalization benefits from Rademacher complexity analysis (proof relies on assumptions not met by actual models)

## Next Checks

1. **UTR Ablation**: Train a Decision Transformer with separate tokens for RTG, state, and action (3L tokens) versus UTR (L tokens) on a single D4RL task to measure the isolated impact of unified tokenization on both performance and efficiency
2. **Receptive Field Analysis**: Systematically evaluate UDC performance on tasks requiring increasing levels of long-range dependency modeling (e.g., synthetic tasks with sparse rewards at distant timesteps) to quantify the exact modeling limit of the convolutional stack
3. **Robustness to Hyperparameters**: Reproduce the main results (performance and efficiency metrics) while varying the number of Gated CNN blocks and kernel sizes to determine sensitivity to these architectural choices