---
ver: rpa2
title: Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems
arxiv_id: '2601.22292'
source_url: https://arxiv.org/abs/2601.22292
tags:
- reward
- resilience
- learning
- agents
- cooperative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of designing reward functions for
  multi-agent systems that operate in dynamic, uncertain environments and must sustain
  cooperation under disruptions. The core idea is to use inverse reinforcement learning
  (IRL) to infer reward functions from ranked agent trajectories, where rankings are
  based on a cooperative resilience metric.
---

# Learning Reward Functions for Cooperative Resilience in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2601.22292
- Source URL: https://arxiv.org/abs/2601.22292
- Reference count: 40
- One-line primary result: Reward functions learned via preference-based inverse reinforcement learning and ranked by a cooperative resilience metric significantly improve multi-agent cooperation and system sustainability under disruptions.

## Executive Summary
This paper addresses the challenge of designing reward functions for multi-agent systems operating in dynamic, uncertain environments where cooperation must be sustained under disruptions. The authors propose a framework that uses inverse reinforcement learning to infer reward functions from ranked agent trajectories, where rankings are based on a cooperative resilience metric. The approach is evaluated in a social dilemma environment with two agents and a shared resource, demonstrating that hybrid reward strategies combining resilience-based and individual consumption rewards significantly improve cooperative resilience and system performance under disruptions.

## Method Summary
The method uses preference-based inverse reinforcement learning to infer reward functions from ranked trajectories in a social dilemma environment. The process involves: (1) collecting trajectories using a random policy in a Commons Harvest environment, (2) ranking trajectories using a cooperative resilience metric that captures failure and recovery profiles, (3) learning a reward function via Margin-based or Probabilistic Preference Learning using different parameterizations (handcrafted, linear, or neural networks), and (4) training PPO agents using a hybrid reward strategy that combines the inferred resilience reward with individual consumption rewards.

## Key Results
- Hybrid reward strategies combining resilience-based and individual consumption rewards significantly improve cooperative resilience without sacrificing task performance
- Learned rewards promote differentiated, specialized agent behaviors that enhance system sustainability
- The approach successfully prevents catastrophic outcomes like resource overuse while maintaining cooperation under disruptions

## Why This Works (Mechanism)

### Mechanism 1: Preference-Based Reward Inference via Resilience Rankings
Reward functions aligned with system-level resilience can be derived by optimizing a preference-based objective over trajectory rankings. The framework collects trajectories and ranks them using a cooperative resilience metric, which induces a preference relation that drives an IRL optimizer to learn a reward function that scores resilient trajectories higher. This works because the chosen resilience metric accurately captures desired system behavior, and the reward parameterization is sufficient to represent the underlying value structure.

### Mechanism 2: Hybrid Reward Composition for Mixed-Motive Balance
A hybrid reward strategy combining resilience-inferred rewards with individual consumption incentives is more effective than either alone. Purely resilience-based rewards can cause agents to become overly cautious and avoid resource use. By hybridizing the rewards, agents are incentivized to maintain system health while still achieving individual utility. This balance prevents the "conservative policy" trap while preserving cooperative performance.

### Mechanism 3: Disruption-Conditioned Metric Computation
Resilience is operationalized by contrasting failure and recovery profiles against a baseline, explicitly encoding temporal recovery dynamics into the reward signal. The metric integrates Recovery Profile (integral of performance from worst degradation to recovery) and Failure Profile. By using this specific temporal shape as the target for IRL, the reward function learns to value states that facilitate rapid recovery post-disruption.

## Foundational Learning

- **Concept: Preference-based Inverse Reinforcement Learning (IRL)**
  - Why needed: Standard IRL requires optimal demonstrations which are hard to obtain for complex resilience scenarios. Preference-based IRL only requires relative rankings of trajectories, which is more intuitive to specify via a metric.
  - Quick check: Can you explain why minimizing the negative log-likelihood of trajectory preferences might be more stable than trying to match expert feature counts?

- **Concept: Cooperative Resilience (System-level Property)**
  - Why needed: Unlike "robustness" (resisting change), resilience includes the ability to recover and transform. Understanding this distinction is crucial to comprehending why the metric includes "Recovery Profiles" rather than just minimum performance thresholds.
  - Quick check: How does the "harmonic mean" aggregation of indicators ensure that the system doesn't optimize one metric (e.g., efficiency) at the expense of another (e.g., equality)?

- **Concept: Social Dilemmas (Commons Harvest)**
  - Why needed: The environment used is a mixed-motive setting where individual rationality (consume now) leads to collective ruin (resource collapse). The architecture is designed specifically to solve this tension.
  - Quick check: In the "Commons Harvest" scenario, why does the paper argue that standard PPO or QMIX with individual rewards fails to maintain the apple population?

## Architecture Onboarding

- **Component map:** Trajectory Generator -> Metric Engine -> Ranker -> Reward Learner (IRL) -> MARL Trainer
- **Critical path:** The Metric Engine is the linchpin. If the math for the resilience metric does not accurately reflect "good" recovery behavior, the entire IRL pipeline optimizes for the wrong objective. Specifically, the definition of recovery endpoint determines the horizon of the "recovery profile."
- **Design tradeoffs:** Handcrafted vs. Neural Rewards: Handcrafted features worked best with limited data (500 trajectories) and encoded meaningful prior structure. Neural network rewards offered flexibility but required more data and risked local optima. MPL vs. PPL: Margin-based (MPL) is convex for linear models (stable); Probabilistic (PPL) offers smoother gradients but is computationally heavier.
- **Failure signatures:** Conservative Collapse: Agents refuse to consume resources to minimize risk (over-optimizing resilience). Last-Apple Failure: Agents consume greedily, causing the resource pool to hit zero (under-optimizing resilience).
- **First 3 experiments:**
  1. Sanity Check the Metric: Generate synthetic trajectories with obvious "good" (recover fast) and "bad" (collapse) behaviors. Verify the Metric Engine ranks them correctly.
  2. Overfitting Test: Train the reward function on a small set of 50 trajectories. Deploy in environment. Check if agents exhibit erratic behavior or "reward hacking."
  3. Ablation on Disruption Timing: Vary the disruption time during the Reward Learning phase. Verify if the learned reward function generalizes to disruptions occurring at different times during PPO training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cooperative resilience reward learning framework generalize to partially observable environments where agents lack full state information?
- Basis in paper: [explicit] "First, extending the framework to the full version of the social dilemma environment under partial observability, as commonly encountered by agents, would provide a more realistic testbed for cooperative resilience."
- Why unresolved: All experiments used a deliberately fully observable variant; the original Melting Pot environment is partially observable.

### Open Question 2
- Question: How does the framework scale to larger agent populations (e.g., 10+ agents) and additional Melting Pot scenarios beyond Commons Harvest?
- Basis in paper: [explicit] "We acknowledge that fully scaling to more agents and to additional Melting Pot scenarios remains an important direction for future work."
- Why unresolved: Experiments only tested 2-agent and 4-agent settings; larger populations introduce exponentially larger joint action spaces and coordination complexity.

### Open Question 3
- Question: Can neural network reward parameterizations match or exceed handcrafted feature performance given sufficient training trajectories?
- Basis in paper: [inferred] The paper notes neural models are "likely underpowered rather than fundamentally flawed" with only 500 trajectories, and handcrafted features encode "meaningful prior structure about the domain, making it the least general."
- Why unresolved: Neural models may require substantially more ranked trajectories to learn equivalent representations without domain-specific feature engineering.

## Limitations

- The approach critically depends on the resilience metric's design and assumes preference-based IRL can extract meaningful reward signals from trajectory rankings
- Results are demonstrated only in a simplified social dilemma with two agents, limiting claims about scalability to larger, more complex systems
- The hybrid reward composition requires careful tuning to avoid either collapse into conservatism or reversion to greedy behavior

## Confidence

- **High Confidence:** The core mechanism of using preference-based IRL to learn resilience-oriented rewards is well-supported by the trajectory ranking framework and ablation studies showing superiority over handcrafted or individual-only rewards.
- **Medium Confidence:** The claim that hybrid rewards prevent conservative collapse is supported by comparisons in the expanded disruption protocol, though the exact contribution of the individual reward term requires more systematic ablation.
- **Low Confidence:** Generalization claims to real-world systems (e.g., infrastructure networks) are not empirically tested; the paper only demonstrates in a simulated social dilemma.

## Next Checks

1. **Cross-Environment Transfer:** Train the reward learner in the Commons Harvest environment, then deploy in a different social dilemma (e.g., Stag Hunt) without retraining the reward function. Measure if resilience behaviors transfer.
2. **Robustness to Metric Perturbations:** Systematically corrupt the resilience metric inputs (e.g., add noise to resource availability readings) during the IRL training phase. Evaluate if the learned reward function still promotes cooperative behaviors under measurement error.
3. **Agent Number Scaling:** Replicate the full pipeline with 4 or 6 agents instead of 2. Investigate whether the learned reward functions still differentiate specialized behaviors or if coordination complexity overwhelms the resilience signal.