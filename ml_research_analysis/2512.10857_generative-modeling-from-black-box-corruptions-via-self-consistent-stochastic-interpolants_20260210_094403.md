---
ver: rpa2
title: Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic
  Interpolants
arxiv_id: '2512.10857'
source_url: https://arxiv.org/abs/2512.10857
tags:
- noise
- diffusion
- samples
- gaussian
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles generative modeling from corrupted data without
  clean samples, a common problem in scientific and engineering domains where measurements
  are observed through noisy, ill-conditioned channels. The authors introduce the
  Self-Consistent Stochastic Interpolant (SCSI) framework, which iteratively updates
  a transport map between corrupted and clean data samples using only black-box access
  to the corruption channel.
---

# Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants

## Quick Facts
- arXiv ID: 2512.10857
- Source URL: https://arxiv.org/abs/2512.10857
- Reference count: 40
- Primary result: SCSI achieves FID scores of 5.38 for random masking and 6.74 for JPEG compression, outperforming variational baselines in inverse problems without requiring clean samples

## Executive Summary
This paper addresses the fundamental challenge of generative modeling when only corrupted observations are available, without access to clean samples or explicit knowledge of the corruption process. The Self-Consistent Stochastic Interpolant (SCSI) framework introduces an iterative scheme that learns a transport map between corrupted and clean data using only black-box access to the forward corruption channel. By leveraging stochastic interpolants and a self-consistency principle, SCSI converges to a transport map that effectively inverts the corruption channel, enabling generation of clean data from corrupted observations.

The method demonstrates superior performance across diverse inverse problems including random masking, Gaussian blurring with noise, motion blur, JPEG compression, and quasar spectra recovery. SCSI achieves competitive results even against methods that require additional access to the forward model or clean data, while being computationally efficient and highly flexible for arbitrary nonlinear forward models.

## Method Summary
SCSI tackles generative modeling from corrupted data by iteratively updating a transport map between corrupted observations and clean data. The framework uses stochastic interpolants - mixtures of clean samples, corrupted samples, and noise - to train a velocity field that maps corrupted data back to clean space. The key innovation is the self-consistency condition: at convergence, applying the forward corruption to generated clean samples and then transporting back should recover the original corrupted observations. This is achieved through an outer loop that updates the transport map and an inner loop that trains using the SI loss on interpolants. The method can use either ODEs (faster) or SDEs (more robust) for the transport, with the SDE variant including added noise for stability in high-corruption regimes.

## Key Results
- Achieves FID scores of 5.38 for random masking and 6.74 for JPEG compression, outperforming variational baselines
- Demonstrates competitive performance against methods requiring clean samples or explicit forward model access
- Shows superior restoration quality across multiple metrics (LPIPS, PSNR, SSIM) on diverse corruption types including motion blur and scientific data
- Provides theoretical guarantees including convergence in Wasserstein metric and KL divergence under appropriate conditions

## Why This Works (Mechanism)
SCSI works by exploiting the self-consistency property of the corruption channel. The key insight is that at convergence, the transport map should satisfy Φ(F(Φ(y))) ≈ y for corrupted observations y. This creates a fixed-point equation that can be solved iteratively. The stochastic interpolant framework allows training with only black-box access to F by creating synthetic training pairs through the corruption process itself. The mixture strategy (using generated observations with probability p=0.9) stabilizes early training when the transport map is far from optimal. The method's flexibility in handling arbitrary nonlinear forward models comes from only requiring simulator access rather than analytical knowledge of the corruption process.

## Foundational Learning

**Stochastic Interpolants**: Stochastic interpolants are time-dependent mixtures of different data distributions (clean, corrupted, and noise) used to train generative models. Why needed: They enable training when only black-box access to a corruption channel is available, without requiring paired clean-corrupted samples. Quick check: Verify that interpolants follow the prescribed time schedule α_t=1-t, β_t=t, γ_t=0 for ODE or include noise term for SDE.

**Wasserstein Metric Convergence**: The Wasserstein distance measures the cost of transporting one probability distribution to another. Why needed: SCSI's theoretical guarantees rely on proving contraction in Wasserstein metric, ensuring the iterative scheme converges to a unique fixed point. Quick check: Confirm that the condition number χ < 1 for your specific corruption channel and function class.

**Self-Consistency Principle**: The idea that at convergence, applying the forward corruption to generated clean samples and then transporting back should recover the original corrupted observations. Why needed: This creates a fixed-point equation that the iterative scheme can solve, eliminating the need for clean samples. Quick check: Monitor KΦ_#μ ≈ μ at the fixed point to verify self-consistency.

## Architecture Onboarding

**Component Map**: Corrupted data y -> Transport map Φ_Θ -> Clean data x -> Forward model F -> Corrupted data ŷ -> Interpolant I_t -> Loss functions E_b, E_g -> Updated Θ

**Critical Path**: The core computational path involves: (1) Transporting corrupted observations y to clean space using current Φ_Θ, (2) Applying black-box forward model F to generate corrupted versions, (3) Creating stochastic interpolants between original and generated corrupted data, (4) Computing drift and denoiser losses, (5) Updating transport map parameters via SGD.

**Design Tradeoffs**: ODE vs SDE - ODEs are faster and have stronger theoretical guarantees but can produce collapsed samples in high-corruption regimes. SDEs add noise for robustness but sacrifice some theoretical properties. The mixture strategy (p=0.9) trades off training stability against potential bias from using generated observations.

**Failure Signatures**: Early instability when Θ is far from optimal manifests as exploding gradients or NaN losses. High-corruption regimes with ODE transport show collapsed, thin samples lacking diversity. Slow convergence or stagnation indicates the forward model F may not be sufficiently injective at the distribution level.

**First Experiments**: 1) Verify the mixture scheme with p=0.9 addresses early instability by monitoring training loss curves, 2) Test both ODE and SDE variants to determine if SDE improves sample diversity in challenging corruption regimes, 3) Check the self-consistency condition KΦ_#μ ≈ μ at the fixed point to ensure the corruption channel is properly inverted.

## Open Questions the Paper Calls Out

**Time Discretization Effects**: The theoretical analysis proves contraction for continuous-time SDE/ODE, but practical algorithms must use discretized time steps. How does this discretization affect convergence guarantees and sample quality? A theoretical extension accounting for Euler or Runge-Kutta discretization errors without losing the contraction property would be valuable.

**Quantitative Analysis of Tradeoffs**: While the theory establishes that smaller function classes improve contraction rates but increase approximation error, the specific balance for neural network architectures like U-Nets remains uncharacterized. Deriving explicit bounds on the condition number χ for specific architectures would provide practical guidance.

**Marginal vs Posterior Transport Comparison**: The analysis demonstrates advantages of marginal transport (ODE) for speed in Gaussian settings, but EM-based posterior transport is known to converge under weaker assumptions. A unified theoretical framework comparing convergence rates under varying noise levels and channel structures is needed.

**Verifying Contraction Conditions**: The Lipschitz stability assumption and condition R < 1 are difficult to verify for general non-linear, non-Gaussian corruption channels. Proving these conditions for specific classes of non-linear operators or deriving alternative, easier-to-verify sufficient conditions would strengthen the theoretical foundation.

## Limitations

- The exact training schedule is underspecified, particularly the number of outer iterations K and how the 50K total iterations are allocated between outer and inner loops
- Implementation details like batch size, optimizer parameters beyond learning rate, random seeds, and exact forward model implementations are not provided
- The theoretical guarantees rely on conditions (Lipschitz stability, R < 1) that are difficult to verify for complex, non-linear real-world channels
- Initialization scheme for Θ^(0) is not clearly described, which could significantly impact early training stability

## Confidence

- **High confidence**: Theoretical framework (SCSI construction, self-consistency property, convergence guarantees under injectivity conditions)
- **Medium confidence**: Core algorithmic implementation (stochastic interpolant framework, loss functions, U-Net architecture, ODE vs SDE trade-offs)
- **Low confidence**: Exact experimental configuration (training schedules, initialization, specific hyperparameters, implementation details)

## Next Checks

1. Implement the mixture scheme with p=0.9 to verify it addresses early instability when Θ is far from optimal, monitoring the training loss curve for convergence behavior.

2. Test both ODE and SDE variants with ε=0.1 and γ_t=t(1-t) to determine if SDE improves sample diversity in high-corruption regimes where ODE transport produces collapsed samples.

3. Verify that the forward model F is approximately injective at the distribution level by checking whether KΦ_#μ ≈ μ at the fixed point, ensuring the self-consistency condition can be satisfied.