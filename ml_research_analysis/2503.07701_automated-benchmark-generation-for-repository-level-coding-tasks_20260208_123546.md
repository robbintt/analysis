---
ver: rpa2
title: Automated Benchmark Generation for Repository-Level Coding Tasks
arxiv_id: '2503.07701'
source_url: https://arxiv.org/abs/2503.07701
tags:
- commands
- test
- installation
- setupagent
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating large, diverse,
  and up-to-date repository-level coding benchmarks. Existing benchmarks like SWE-Bench
  are limited in size and diversity due to the manual effort required to set up historically
  accurate execution environments.
---

# Automated Benchmark Generation for Repository-Level Coding Tasks

## Quick Facts
- arXiv ID: 2503.07701
- Source URL: https://arxiv.org/abs/2503.07701
- Reference count: 40
- This paper introduces SETUPAGENT, an automated system that generates large, diverse repository-level coding benchmarks by extracting and iteratively improving installation and testing commands.

## Executive Summary
This paper addresses the challenge of creating large, diverse, and up-to-date repository-level coding benchmarks. Existing benchmarks like SWE-Bench are limited in size and diversity due to the manual effort required to set up historically accurate execution environments. To solve this, the authors introduce SETUPAGENT, a fully automated system that extracts installation and testing commands from repository files, iteratively improves them, and validates their correctness. Using SETUPAGENT, they generate two new benchmarks: SWA-Bench (44 repositories focusing on applications) and SWEE-Bench (366 repositories emphasizing diversity). Evaluations show that these benchmarks have significant distributional differences compared to SWE-Bench, including lower repository popularity, more complex fixes, and up to 40% lower agent success rates, highlighting the importance of diverse, representative benchmarks for reliable progress measurement in code agent development.

## Method Summary
The authors developed SETUPAGENT, a three-phase system for automated benchmark generation. First, the extraction phase uses an LLM to identify Python version, filter relevant files (CI/CD configs, README, documentation), and propose initial installation and test commands. Second, the iterative improvement phase executes these commands in a Docker container, captures errors, and prompts an LLM to diagnose and fix issues, repeating up to 4 times. Third, the validation phase confirms successful setup (with ≥95% tests passing) and parses test results to identify fail-to-pass transitions. The system uses `uv` with `--exclude-newer` flags for historical dependency pinning and maintains a reference database of successful commands to accelerate setup for similar repositories.

## Key Results
- SETUPAGENT successfully generated SWA-Bench (44 repos) and SWEE-Bench (366 repos), significantly larger than SWE-Bench's 278 instances
- SWA-Bench and SWEE-Bench show lower repository popularity (8.04 vs 40.82) and higher fix complexity (11.03 vs 9.58) compared to SWE-Bench
- Agent success rates on SWA-Bench/SWEE-Bench are up to 40% lower than on SWE-Bench, highlighting the importance of diverse benchmarks
- Ablation studies show iterative improvement and multi-source context aggregation are critical for success

## Why This Works (Mechanism)

### Mechanism 1: Iterative Error-Driven Refinement
- Claim: Repeated execution with LLM-guided error diagnosis improves environment setup success rates compared to single-pass extraction
- Mechanism: The system attempts installation/testing commands, captures error messages, prompts an LLM to classify the error source, and proposes command modifications; this cycle repeats until success or iteration limit
- Core assumption: LLMs can reliably diagnose error causes and propose actionable fixes from truncated error logs
- Evidence anchors: [abstract] "SETUPAGENT... iteratively improves [commands] and validates their correctness"; [Section 5.2, Table 4] ablation shows "no Iterative Improvement" reduces successful repos from 44 to 11

### Mechanism 2: Multi-Source Context Aggregation
- Claim: Combining CI/CD configurations, text documentation, and web references yields more complete command extraction than any single source
- Mechanism: An LLM first filters relevant files by name, then extracts installation/testing commands from CI/CD files, README files, and external documentation links
- Core assumption: Relevant context can be identified heuristically before extraction
- Evidence anchors: [Section 3.3] "SETUPAGENT reviews all relevant files... CI/CD configurations, and referenced webpages"; [Section 5.2, Table 4] "only CI/CD Files" yields 33 repos vs. 44 with full approach

### Mechanism 3: Historical Dependency Pinning
- Claim: Constraining dependency versions to those available at issue creation time produces faithful reproduction of original bug conditions
- Mechanism: Uses the `uv` package manager with `--exclude-newer <issue_date>` flag to prevent installation of packages released after the historical state
- Core assumption: Exact historical reproduction is both achievable and desirable for benchmark validity
- Evidence anchors: [Section 3.3] "configuring it to exclude dependency versions released after the issue creation"; [Section 3.2] Lists "Historical Accuracy" as a core requirement

## Foundational Learning

- Concept: **Repository-level coding tasks**
  - Why needed here: The entire benchmark framework assumes tasks involve multi-file changes, test execution, and dependency resolution—not isolated function generation
  - Quick check question: Can you explain why function-level benchmarks like HumanEval fail to capture real-world software engineering complexity?

- Concept: **CI/CD pipelines and configuration**
  - Why needed here: SETUPAGENT relies on parsing CI/CD files (e.g., `.github/workflows/`) to extract canonical installation and test commands
  - Quick check question: Given a GitHub Actions workflow file, can you identify which steps correspond to installation vs. testing?

- Concept: **Test result granularity and fail-to-pass semantics**
  - Why needed here: Valid benchmarks require at least one F→P test; understanding test state transitions is essential for instance validation
  - Quick check question: What does it mean for a test to have behavior `F → P`, and why is this required for a valid benchmark instance?

## Architecture Onboarding

- Component map: Extraction Phase -> Iterative Improvement Phase -> Validation Phase -> Reference Database
- Critical path: Extraction → Install execution → Error diagnosis → Command modification → Test execution → Validation → Database storage
- Design tradeoffs:
  - Iteration limit (4) balances success rate vs. efficiency; increasing would improve coverage but extend runtime
  - 95% test pass threshold tolerates minor flakiness but may accept partially broken states
  - Web browsing disabled for SWEE-Bench to reduce overhead, at cost of missing linked documentation
- Failure signatures:
  - Build-process errors requiring multi-hop web navigation
  - Test execution requiring Docker containers (not supported)
  - Complex Makefiles with implicit substeps
  - Wrong requirement file selection triggering dependency cascades
- First 3 experiments:
  1. Run SETUPAGENT on 10 repositories from SWEE-Bench list; measure success rate and time per repo. Compare with manual setup time.
  2. Ablate CI/CD file usage on a subset; quantify the drop in successful extractions to validate Table 4 findings.
  3. Extend iteration limit to 8 on previously failed instances; assess whether additional iterations recover any cases or merely increase runtime without benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can SetupAgent be extended to autonomously handle test environments that require nested Docker containers?
- **Basis in paper:** [explicit] The authors' failure analysis identifies that for several failed instances, "the only described way to test the application requires running docker containers, which SETUPAGENT does not support," noting this as a key area for future work
- **Why unresolved:** The current architecture executes commands inside a Docker container but lacks the privileges or tooling to manage sibling or nested containers required by complex integration tests
- **What evidence would resolve it:** A modified agent successfully generating valid execution environments for repositories currently excluded due to Docker-in-Docker requirements

### Open Question 2
- **Question:** To what extent does improving web-browsing depth enable the resolution of installation instructions that span multiple external web pages?
- **Basis in paper:** [explicit] The paper observes that in failure cases, "finding the installation instructions requires following two or more links on web pages," explicitly calling for improved web-browsing capabilities to address this
- **Why unresolved:** The current system may struggle with "multi-hop" reasoning required to traverse documentation across different domains to build a complete set of dependencies
- **What evidence would resolve it:** A higher extraction success rate for repositories with decentralized documentation after implementing a recursive link-following mechanism

### Open Question 3
- **Question:** Can the automated benchmark generation methodology be generalized to non-Python software ecosystems?
- **Basis in paper:** [inferred] The methodology is heavily reliant on Python-specific tooling (e.g., `uv`, `pip`, `pyproject.toml`), leaving the applicability of this approach to languages like JavaScript or Rust unstated
- **Why unresolved:** Different ecosystems use distinct package managers and version resolution strategies (e.g., `npm`, `cargo`) that may not map cleanly to the current "historical accuracy" constraints enforced by `uv`
- **What evidence would resolve it:** The successful automated generation of a benchmark dataset for a diverse set of non-Python repositories using the same pipeline architecture

## Limitations

- **Historical Accuracy Constraint**: The approach relies on exact dependency pinning via `--exclude-newer` flag, but complex dependencies with native compilation or non-PyPI sources may not respect version constraints
- **Iterative Improvement Boundaries**: The 4-iteration limit may be insufficient for repositories with complex multi-step builds or system dependencies requiring web navigation
- **Web Browsing Dependency**: SWA-Bench requires web browsing for documentation extraction, creating operational overhead and potential reproducibility issues

## Confidence

- **High Confidence**: The comparative benchmark characteristics (SWA-Bench and SWEE-Bench showing lower popularity and higher fix complexity than SWE-Bench) are well-supported by the empirical evaluation data in Section 5.1
- **Medium Confidence**: The iterative improvement mechanism's effectiveness is supported by the ablation study (Table 4 showing drop from 44 to 11 repos without iteration), but the LLM error diagnosis reliability remains untested with alternative models
- **Low Confidence**: The claim about LLMs reliably diagnosing error causes from truncated logs lacks direct validation—the corpus comparison only shows SWE-Bench++ achieves FMR 0.52 without demonstrating specific error classification accuracy

## Next Checks

1. Run SETUPAGENT on 10 SWEE-Bench repositories with iteration limit extended to 8; measure whether additional iterations recover failed cases or merely increase runtime without benefit
2. Ablate web browsing capability on SWA-Bench subset; quantify the impact on successful repository setup to validate the tradeoff between overhead and completeness
3. Implement alternative error classification using a different LLM model; compare diagnosis accuracy and success rates against the reported GPT-4o-mini results to test model dependency