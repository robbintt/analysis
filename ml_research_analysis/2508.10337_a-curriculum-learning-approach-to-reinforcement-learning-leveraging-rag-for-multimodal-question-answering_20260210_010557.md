---
ver: rpa2
title: 'A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for
  Multimodal Question Answering'
arxiv_id: '2508.10337'
source_url: https://arxiv.org/abs/2508.10337
tags:
- learning
- answer
- task
- question
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors developed a retrieval-augmented vision-language system
  for the MM-RAG QA competition, using curriculum learning within reinforcement learning
  to improve accuracy and reduce hallucinations. Their approach combines supervised
  fine-tuning via knowledge distillation, staged reinforcement learning on increasingly
  difficult question types, and multi-source retrieval augmentation.
---

# A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering

## Quick Facts
- arXiv ID: 2508.10337
- Source URL: https://arxiv.org/abs/2508.10337
- Reference count: 24
- 1st place on Task 1 (52.38% lead), 3rd place on Task 3 in MM-RAG competition

## Executive Summary
The authors developed a retrieval-augmented vision-language system for the MM-RAG QA competition that combines supervised fine-tuning, curriculum learning within reinforcement learning, and multi-source retrieval augmentation. Their approach addresses the hallucination problem in multimodal QA by progressively training from easy to hard samples, allowing the model to balance answer accuracy with reduced hallucination. The system achieved top performance on Task 1 (52.38% lead in truthfulness score) and placed 3rd on Task 3, with ablation studies confirming each component's contribution.

## Method Summary
The system uses a three-stage pipeline: (1) Supervised fine-tuning with knowledge distillation from GPT-4.1, where the larger model generates chain-of-thought reasoning and refusal patterns that are transferred to Llama 3.2-11B-Vision-Instruct via LoRA; (2) Reinforcement learning with curriculum learning using GRPO (Grouped Relative Policy Optimization) across three stages - easy samples only, then 1:1 easy/hard mix, finally 1:2 distribution matching real competition; (3) Retrieval-augmented generation with web search, where the model generates its own search queries after analyzing the question and image, using a multi-stage retrieval pipeline with coarse ranking (bge-large-en-v1.5 + BM25 + TF-IDF) followed by neural re-ranking (bge-reranker-v2-m3). The approach was evaluated on three multimodal QA tasks with truthfulness score as the primary metric.

## Key Results
- Task 1: 1st place with 52.38% lead in truthfulness score
- Task 3: 3rd place performance
- RL with curriculum learning improved accuracy from 0.059 to 0.190 and reduced missing rate from 0.909 to 0.713
- Adding RAG improved accuracy from 0.190 to 0.282 and truthfulness from 0.093 to 0.151
- SFT alone improved accuracy from 0.197 to 0.262 and reduced hallucination from 0.718 to 0.501

## Why This Works (Mechanism)

### Mechanism 1: Curriculum Learning Stabilizes RL by Preventing Reward Collapse
Training progressively from easy to hard samples prevents the model from learning to refuse all questions to avoid hallucination penalties. The three-stage curriculum (easy-only → 1:1 easy/hard → 1:2 easy/hard) allows the model to first acquire answer generation skills before learning refusal behavior. Without this, RL optimization collapses into a "reward black hole" with 90% missing rate. Easy samples (those GPT-4.1 answers correctly under competition evaluation) provide sufficient gradient signal for initial policy learning.

### Mechanism 2: Model-Generated Queries Improve Retrieval Precision Over Raw User Questions
Allowing the model to generate its own search queries yields more semantically complete queries than raw questions. The model analyzes the question and image, identifies missing knowledge, and generates targeted queries (e.g., "Who is the CEO of BMW?" rather than "Who is the CEO of the company that manufactures this?"). Retrieval uses multi-stage ranking: bge-large-en-v1.5 + BM25 + TF-IDF for coarse ranking, bge-reranker-v2-m3 for re-ranking, top-k=5.

### Mechanism 3: Knowledge Distillation from GPT-4.1 Establishes Reasoning Patterns Before RL
SFT with distilled chain-of-thought from GPT-4.1 provides the reasoning scaffolding that RL later refines. GPT-4.1 is given ground-truth answers and prompted to generate CoT reasoning and appropriate refusals. The smaller Llama 3.2-11B-Vision-Instruct is fine-tuned on these outputs using LoRA. This teaches both answer format (między między and <answer></answer>) and when to refuse.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF) / Policy Optimization**
  - Why needed here: The paper uses GRPO to optimize answer quality. You need to understand reward shaping, policy gradients, and the KL-divergence constraint to tune this stage.
  - Quick check question: Can you explain why a reward of -1 for hallucination but 0 for refusal creates a bias toward excessive refusal?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Tasks 2 and 3 require web search integration. Understanding retrieval pipelines, chunking, and re-ranking is essential for reproducing this work.
  - Quick check question: Why would BM25 alone be insufficient for this task, necessitating neural re-ranking?

- **Curriculum Learning**
  - Why needed here: The core contribution. Understanding how to define difficulty and stage training is critical.
  - Quick check question: What would happen if you trained on hard samples first? How would the policy gradient differ?

## Architecture Onboarding

- **Component map:**
  SFT Module (Llama 3.2-11B-Vision-Instruct + LoRA) → RL Module (VisualRFT + GRPO with curriculum) → RAG Module (Query generator → Web search API → Multi-stage retriever → Top-5 context)

- **Critical path:**
  1. Prepare SFT data by prompting GPT-4.1 with ground-truth answers to generate CoT + refusal patterns
  2. Fine-tune base model using LoRA
  3. Label easy/hard samples using GPT-4o mini under competition evaluation protocol
  4. Run 3-stage RL curriculum (easy → 1:1 mix → 1:2 distribution)
  5. For Tasks 2-3: add query generation capability via SFT, integrate retriever at inference

- **Design tradeoffs:**
  - Image search vs. web search: Abandoned image search due to noise and poor object detection in Llama Vision; chose web-only retrieval. Assumption: text queries capture sufficient semantic information.
  - Missing rate vs. hallucination: RL without curriculum achieves low hallucination (0.032) but 90.9% missing rate; curriculum trades some hallucination increase (0.131) for much better accuracy (0.282).
  - Query capability degradation during RL: GRPO phase may reduce query generation quality; accepted as tradeoff for answer accuracy focus.

- **Failure signatures:**
  - Reward black hole: If missing rate exceeds ~80% and accuracy is near 0%, the model has learned to refuse everything. Check reward scale and curriculum stage progression.
  - No retrieval improvement: If RAG version performs same or worse than non-RAG, check query generation quality and whether retriever is returning relevant results.
  - Format violations: If model outputs don't follow ... <answer>...</answer>, increase format reward weight.

- **First 3 experiments:**
  1. SFT-only baseline: Fine-tune Llama 3.2-11B-Vision on distilled data, evaluate accuracy/hallucination/missing on held-out set. Expect ~0.26 accuracy, ~0.50 hallucination per Table 2.
  2. RL without curriculum: Train with GRPO on full dataset. Expect high missing rate (~0.90) to confirm reward black hole problem exists.
  3. Single-stage curriculum comparison: Test easy-only vs. hard-only vs. mixed training to validate staged approach is necessary, not just data selection.

## Open Questions the Paper Calls Out

### Open Question 1
Can image-based retrieval be effectively integrated into multimodal RAG systems, or is textual query generation inherently superior for knowledge retrieval?
The authors state "the Llama Vision model was not well-suited for object detection tasks" and "image search introduced significant noise, even after manual sub-image extraction," leading them to abandon image search entirely. This provides no systematic analysis of why image search fails or whether alternative object detection methods or noise filtering could salvage this retrieval modality.

### Open Question 2
What is the optimal curriculum design for RL-based hallucination control, beyond the empirically-derived three-stage approach?
The curriculum relies on GPT-4o mini's performance to classify samples as "easy" or "hard," and the three-stage progression (easy → 1:1 → 1:2 distribution) is determined empirically without theoretical justification. The paper does not explore whether alternative difficulty metrics or different stage progressions could yield better accuracy-missing rate trade-offs.

### Open Question 3
Can query generation and answer accuracy be jointly optimized in reinforcement learning, or is the observed trade-off fundamental to GRPO-style training?
The authors note "while focusing on enhancing answer accuracy, there is an observed trade-off where the model's ability to generate queries might slightly diminish" but accept this as inevitable. No experiments explore multi-objective reward functions or auxiliary losses that could preserve query generation capability during answer-focused RL.

### Open Question 4
How does the approach generalize to base models with different visual encoding architectures or scale?
All experiments use Llama 3.2-11B-Vision-Instruct as the base model, and the paper does not address whether the curriculum RL benefits transfer to other VLLMs or different scales. The approach may be sensitive to the specific visual encoder or language model capacity, but no cross-model validation is provided.

## Limitations
- Curriculum learning requires a reliable difficulty signal—unclear how well GPT-4o mini approximates competition evaluation for easy/hard labeling
- Web search vs. image search tradeoff may not generalize to domains where visual search is critical
- The authors acknowledge RL may degrade query generation quality, creating tension between answer accuracy and retrieval effectiveness

## Confidence
- **High confidence**: Curriculum learning preventing reward collapse (accuracy 0.190→0.282, missing 0.909→0.713, truthfulness 0.027→0.151)
- **High confidence**: Knowledge distillation scaffolding (accuracy 0.197→0.262, hallucination 0.718→0.501)
- **Medium confidence**: Query generation mechanism (accuracy +0.123 with RAG but no direct comparison to raw user queries)

## Next Checks
1. Test curriculum learning without GPT-4o mini labeling—use alternative difficulty metrics to verify staged training is essential, not just data selection
2. Evaluate RAG performance on image search vs. web search for visual QA tasks requiring object recognition to quantify the tradeoff made
3. Measure query generation quality before/after RL training to quantify the degradation mentioned and determine if this represents a fundamental limitation of the approach