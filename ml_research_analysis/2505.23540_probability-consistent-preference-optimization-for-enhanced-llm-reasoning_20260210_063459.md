---
ver: rpa2
title: Probability-Consistent Preference Optimization for Enhanced LLM Reasoning
arxiv_id: '2505.23540'
source_url: https://arxiv.org/abs/2505.23540
tags:
- preference
- pcpo
- pairs
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving mathematical reasoning
  in large language models by developing a more sophisticated method for selecting
  preference training pairs. While existing approaches focus solely on outcome-based
  criteria like answer correctness, the authors propose Probability-Consistent Preference
  Optimization (PCPO), which incorporates both final answer correctness and internal
  token-level probability consistency across responses.
---

# Probability-Consistent Preference Optimization for Enhanced LLM Reasoning

## Quick Facts
- arXiv ID: 2505.23540
- Source URL: https://arxiv.org/abs/2505.23540
- Reference count: 31
- Primary result: PCPO improves LLM mathematical reasoning by incorporating token-level probability consistency into preference pair selection, achieving 82.8% Pass@1 accuracy on GSM8K with Llama-3-8B-Instruct

## Executive Summary
This paper addresses the challenge of improving mathematical reasoning in large language models by developing a more sophisticated method for selecting preference training pairs. While existing approaches focus solely on outcome-based criteria like answer correctness, the authors propose Probability-Consistent Preference Optimization (PCPO), which incorporates both final answer correctness and internal token-level probability consistency across responses. PCPO calculates a weighted score between preferred and dispreferred answers by evaluating the conditional probability of each token, selecting preference pairs based on the highest weighted scores. Extensive experiments across multiple benchmarks and diverse models demonstrate that PCPO consistently outperforms existing outcome-only criterion approaches.

## Method Summary
PCPO extends Direct Preference Optimization by incorporating token-level probability consistency into preference pair selection. The method generates N=16 responses per prompt, splits them into correct/incorrect based on gold labels, and computes a pair-weighted score for each candidate pair by measuring the consistency of token probabilities between chosen and rejected responses. The preference pairs with highest scores are selected for training using a weighted combination of DPO and NLL losses. The approach is trained for 6 epochs per iteration with batch size 128, learning rate 1e-7 cosine, and AdamW warm-up ratio 0.1, using 8×A800 GPUs.

## Key Results
- PCPO achieves 82.8% Pass@1 accuracy on GSM8K with Llama-3-8B-Instruct, outperforming the 81.6% seed model
- Consistent improvements across all benchmarks (GSM8K, MATH-500, Olympiadbench, AMC23) and models tested
- PCPO Loss outperforms standard DPO Loss on the same curated pairs (e.g., OlympiadBench M2: 9.5 vs 9.3 Pass@1)
- Adds ~15% computational overhead compared to baseline DPO due to token probability calculations

## Why This Works (Mechanism)

### Mechanism 1
Token-level probability consistency identifies preference pairs with stronger internal logical alignment than outcome-only methods. PCPO computes a consistency score for matched tokens between chosen and rejected responses, then aggregates into a pair-weighted score. Higher scores indicate the chosen response shares more reasoning structure with the rejected one, making the preference signal more discriminative.

### Mechanism 2
Selecting preference pairs with highest pair-weighted scores concentrates training on pairs where the model can learn fine-grained reasoning distinctions. For each rejected response, PCPO selects the chosen response with strongest token-level consistency, pairing wrong answers with the most similar correct reasoning path rather than arbitrary correct answers.

### Mechanism 3
Integrating the pair-weighted score as a dynamic weight in both DPO and NLL loss components prioritizes high-consistency pairs during gradient updates. The score amplifies loss for pairs where reasoning differs subtly, applying stronger updates to more informative preference pairs.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: PCPO builds on DPO's framework; understand how DPO replaces reward models with direct policy optimization from preference pairs before grasping PCPO's modifications.
- **Token-level language modeling and conditional probability**: PCPO's core innovation relies on computing P(y_t|x, y_<t>) for each token; understanding autoregressive factorization is essential.
- **Chain-of-Thought (CoT) reasoning in LLMs**: PCPO is evaluated specifically on mathematical reasoning with CoT; the token-level consistency metric is designed for multi-step reasoning chains.

## Architecture Onboarding

- **Component map**:
```
[Seed Model M_0] + [Training Data]
        |
        v
[Response Generation] --N=16 responses per prompt-->
        |
        v
[Correctness Split] --Y_w (correct) vs Y_l (incorrect)-->
        |
        v
[Levenshtein Filtering] --top-k=8 similar pairs per rejected response-->
        |
        v
[Token Probability Scoring] --compute c_t and s_w for each candidate pair-->
        |
        v
[Preference Pair Selection] --argmax s_w per rejected response-->
        |
        v
[PCPO Loss Training] --weighted DPO + weighted NLL, 6 epochs-->
        |
        v
[Next Iteration Model M_{t+1}]
```

- **Critical path**:
  1. Response generation quality determines candidate diversity
  2. Levenshtein threshold gates computational cost vs pair quality tradeoff
  3. Matching function must correctly identify common subsequences
  4. Loss hyperparameters control DPO vs NLL balance

- **Design tradeoffs**:
  - Computational cost: ~15% additional GPU hours
  - Label dependency: Requires gold answers for Y_w/Y_l split
  - Response generation budget: N=16 balances diversity with cost

- **Failure signatures**:
  - Saturated baseline models show minimal gains (<1% on GSM8K)
  - Low response diversity causes uniform pair-weighted scores
  - Over-iteration can cause performance regression

- **First 3 experiments**:
  1. Ablate scoring method: Compare PCPO with random pair selection on GSM8K/Llama-3-8B
  2. Vary N (response count): Test N∈{4, 8, 16, 32} to find minimum viable candidate pool size
  3. Cross-domain transfer: Apply PCPO to code generation with test-case verification

## Open Questions the Paper Calls Out
- How can PCPO adapt to domains where ground-truth answers are unavailable?
- Can PCPO scale effectively to larger models (70B+) that already exhibit high reasoning fidelity?
- How can computational efficiency of candidate selection be optimized without degrading pair quality?

## Limitations
- Requires ground-truth answers for preference pair construction, limiting applicability to unsupervised scenarios
- Computational overhead of ~15% additional GPU hours due to token probability calculations
- Effectiveness depends heavily on generating diverse candidate responses with sufficient reasoning pattern variation

## Confidence

**High confidence**:
- PCPO consistently outperforms outcome-only criterion approaches across all evaluated benchmarks and models
- Pair-weighted scoring mechanism successfully identifies responses with similar reasoning patterns
- PCPO Loss produces faster convergence than standard DPO Loss on the same curated pairs

**Medium confidence**:
- PCPO's improvements are "consistent across training iterations" (though IRPO shows potential instability)
- The method demonstrates "broad applicability" by enhancing various DPO variants (with varying improvement magnitudes)

**Low confidence**:
- PCPO would generalize to non-mathematical reasoning tasks (not tested beyond math benchmarks)
- Computational overhead is acceptable for production deployment (only compared to baseline DPO)

## Next Checks

1. **Ablation study on pair selection method**: Implement a baseline that selects preference pairs using only answer correctness and compare performance to PCPO on GSM8K/Llama-3-8B to isolate the contribution of token-level probability consistency scoring.

2. **Scaling analysis**: Systematically vary N (response count per prompt) from 4 to 32 and measure both performance gains and computational overhead to identify the inflection point where additional candidates no longer improve pair quality.

3. **Cross-domain transfer experiment**: Apply PCPO to a non-mathematical reasoning task such as code generation with test-case verification as correctness criterion to test whether the token-level probability consistency mechanism generalizes beyond structured math problems.