---
ver: rpa2
title: 'PDAC: Efficient Coreset Selection for Continual Learning via Probability Density
  Awareness'
arxiv_id: '2511.09487'
source_url: https://arxiv.org/abs/2511.09487
tags:
- samples
- selection
- each
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of efficient coreset selection
  for continual learning, where models must learn sequentially without forgetting
  past knowledge. Existing methods rely on expensive bilevel optimization, limiting
  their practical use.
---

# PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness

## Quick Facts
- arXiv ID: 2511.09487
- Source URL: https://arxiv.org/abs/2511.09487
- Reference count: 40
- Primary result: PDAC achieves superior accuracy and forgetting metrics while being several times faster than bilevel optimization baselines for coreset selection in continual learning

## Executive Summary
This paper addresses the challenge of efficient coreset selection in continual learning, where models must learn sequentially without forgetting past knowledge. Traditional methods rely on expensive bilevel optimization, limiting their practical use. The authors propose PDAC (Probability Density-Aware Coreset), which leverages probability density estimation in feature space to select representative samples without costly optimization. By analyzing the Mean Squared Error between buffer-trained and Bayes-optimal models, they demonstrate that high-density samples contribute most to error suppression, enabling more effective coreset selection.

## Method Summary
The paper introduces a novel approach to coreset selection based on probability density awareness. The method first estimates sample density in the feature space using a Projected Gaussian Mixture (PGM) model. This enables efficient sampling of high-density regions without expensive bilevel optimization. For streaming scenarios, the authors extend PDAC to SPDAC using streaming Expectation Maximization to update PGM parameters on-the-fly. The approach is theoretically grounded in error decomposition analysis, showing that samples from high-probability density regions contribute most to error suppression.

## Key Results
- PDAC and SPDAC outperform state-of-the-art methods on Split-CIFAR10, Split-CIFAR100, and Split-TinyImageNet
- The method achieves superior accuracy and forgetting metrics compared to bilevel optimization baselines
- PDAC is several times faster than existing coreset selection methods while maintaining better performance
- Streaming extension SPDAC successfully handles online data arrival scenarios

## Why This Works (Mechanism)
The core insight is that samples from high-probability density regions contribute most to error suppression when training with a coreset buffer. By focusing on these regions, the method can select more representative samples that better approximate the true data distribution, leading to improved continual learning performance.

## Foundational Learning
- Continual Learning: Why needed? To understand the context of preventing catastrophic forgetting. Quick check: Can you explain the stability-plasticity dilemma?
- Probability Density Estimation: Why needed? To understand how samples are selected based on their feature space density. Quick check: What's the difference between kernel density estimation and mixture models?
- Gaussian Mixture Models: Why needed? To understand the PGM approach for density estimation. Quick check: What are the advantages of mixture models over single distribution assumptions?
- Streaming Expectation Maximization: Why needed? To understand the online adaptation of the density model. Quick check: How does streaming EM differ from batch EM?
- Coreset Selection: Why needed? To understand the problem of selecting representative samples for buffer-based training. Quick check: What makes a coreset "good" in the context of continual learning?

## Architecture Onboarding

**Component Map:** Data stream -> Feature extractor -> PGM density estimator -> Sample selection -> Buffer update

**Critical Path:** Feature extraction → Density estimation → High-density sampling → Buffer storage → Model training

**Design Tradeoffs:** The method trades some accuracy (compared to optimal bilevel solutions) for significant efficiency gains. Fixed feature representations enable faster density estimation but may miss distribution shifts.

**Failure Signatures:** Poor performance when feature distributions shift significantly between tasks, or when the Gaussian mixture assumption doesn't match the true data distribution.

**First Experiments:**
1. Test coreset selection quality on a simple 2D synthetic dataset with known density structure
2. Evaluate density estimation accuracy on held-out validation sets
3. Compare sampling efficiency (time and memory) against bilevel optimization baselines

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Assumes fixed feature representations during coreset selection, which may not hold for significant distribution shifts
- Theoretical analysis relies on idealized Gaussian mixture assumptions that may not match real-world data
- Empirical evaluation focuses primarily on image classification benchmarks, limiting generalizability claims

## Confidence
- Theoretical contribution linking probability density to error minimization: High
- Practical effectiveness across diverse domains: Medium
- Streaming extension's ability to maintain density estimates in highly dynamic environments: Medium

## Next Checks
1. Test PDAC on non-image domains (e.g., text classification, time series) to verify generalization beyond visual data
2. Evaluate robustness when task distributions exhibit heavy overlap or gradual concept drift beyond discrete task boundaries
3. Benchmark computational overhead of PGM density estimation at scale with millions of samples to validate claimed efficiency advantages