---
ver: rpa2
title: 'TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation
  in Aya-23 8B'
arxiv_id: '2510.06249'
source_url: https://arxiv.org/abs/2510.06249
tags:
- language
- hindi
- treplina
- layer
- repina
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates layer-wise cross-lingual alignment for improving
  low-resource machine translation from underrepresented languages to high-resource
  languages using the Aya-23 8B model. The authors propose TRepLiNa, a method combining
  Centered Kernel Alignment (CKA) for cross-lingual representation similarity with
  REPINA regularization to stabilize high-resource language features during training.
---

# TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B

## Quick Facts
- arXiv ID: 2510.06249
- Source URL: https://arxiv.org/abs/2510.06249
- Authors: Toshiki Nakai; Ravi Kiran Chikkala; Lena Sophie Oberkircher; Nicholas Jennings; Natalia Skachkova; Tatiana Anikina; Jesujoba Oluwadara Alabi
- Reference count: 24
- TRepLiNa improves low-resource translation by aligning mid-level transformer layers (10-15) using CKA and REPINA regularization

## Executive Summary
This work introduces TRepLiNa, a layer-wise alignment method for improving low-resource machine translation from underrepresented languages to high-resource languages using the Aya-23 8B model. The approach combines Centered Kernel Alignment (CKA) for cross-lingual representation similarity with REPINA regularization to stabilize high-resource language features during training. Through experiments on four low-resource Indian language pairs with Hindi/English pivots under zero-shot, few-shot, and fine-tuning settings, the study finds that aligning mid-level transformer layers yields the strongest performance gains, particularly for more distant language pairs like Mundari→Hindi and Santali→English.

## Method Summary
TRepLiNa applies layer-wise cross-lingual alignment to a multilingual decoder-only LLM during fine-tuning. The method computes CKA similarity between hidden states of low-resource and high-resource language pairs at a target layer, then adds a REPINA regularization term that anchors high-resource representations to their pretrained values. This asymmetric approach guides low-resource representations toward a stable high-resource target without allowing drift in the well-trained high-resource features. The method is evaluated across four Indian language pairs (Bhili, Mundari, Odia, Santali) with Hindi and English pivots under zero-shot, few-shot, and full fine-tuning conditions.

## Key Results
- Aligning mid-level transformer layers (approximately layers 10–15) yields the strongest performance gains over baselines
- TRepLiNa consistently favors layer 15 in data-scarce conditions, improving weighted composite scores (0.6×BLEU+0.4×ChrF)
- Performance gains are particularly strong for more distant language pairs like Mundari→Hindi and Santali→English
- Over-alignment can degrade results for closely related pairs like Bhili→Hindi where strong CKA pressure washes out beneficial language-specific features

## Why This Works (Mechanism)

### Mechanism 1: Mid-Layer Language-Agnostic Alignment
Multilingual decoder-only LLMs progressively convert language-specific inputs into language-agnostic representations in middle layers. By applying CKA alignment at this transition point (layers 10-15), LRL representations are guided into the shared subspace where cross-lingual transfer is most effective. This avoids forcing alignment in early layers specialized for language-specific encoding or late layers focused on task-specific decoding.

### Mechanism 2: Asymmetric Stabilization via REPINA
CKA-only alignment encourages both languages to "meet in the middle," potentially corrupting pretrained HRL representations. REPINA anchors HRL hidden states to their pretrained references via stop-gradient identity mapping, allowing CKA to pull LRL representations toward stable HRL targets rather than mutual drift. This preserves the quality of already-learned HRL features while improving LRL alignment.

### Mechanism 3: Typology-Dependent Alignment Sensitivity
Closely related languages (e.g., Bhili–Hindi, both Indo-Aryan) already share representational structure due to token overlap and linguistic similarity. Strong CKA pressure can wash out beneficial language-specific features that aid translation. Distant pairs (e.g., Mundari–Hindi, Austro-asiatic to Indo-Aryan) require explicit alignment to bridge the representational gap. The method recommends adjusting alignment strength based on language proximity.

## Foundational Learning

- **Centered Kernel Alignment (CKA):**
  - Why needed: CKA measures similarity between representations of different dimensionalities and scales, enabling comparison of LRL and HRL hidden states at any layer. Unlike cosine similarity, CKA handles the full activation matrix rather than individual vectors.
  - Quick check: Given two activation matrices of shapes (T₁×d) and (T₂×d) where T₁≠T₂, how would CKA compare them after mean-centering?

- **QLoRA Fine-tuning:**
  - Why needed: The method requires gradient access to intermediate hidden states for alignment losses while keeping compute tractable for an 8B parameter model. QLoRA with 4-bit quantization and gradient checkpointing enables this.
  - Quick check: When applying LoRA adapters to query/key/value projections, how do you ensure gradients flow to hidden states needed for CKA computation?

- **Language-Agnostic Representations in Transformers:**
  - Why needed: The entire approach assumes a "language-agnostic subspace" emerges in middle layers. Understanding this phenomenon explains why early layers (language-specific encoding) and late layers (task-specific decoding) are poor alignment targets.
  - Quick check: In a multilingual decoder, would you expect language-agnostic representations to emerge earlier or later than in a multilingual encoder? Why?

## Architecture Onboarding

- **Component map:**
  Forward pass: Standard causal LM → hidden states collected at target layer ℓ → CKA computed between LRL/HRL parallel pair → REPINA computed by running reference pass with adapters disabled → losses combined

- **Critical path:**
  1. Parallel sentence pairs (LRL source, HRL source) must be batched together or processed in sync
  2. Hidden state extraction requires `output_hidden_states=True` at model load
  3. Layer indexing: hidden state tuple index 0 = embedding output; ℓ = 1-based transformer block output
  4. Loss weights: task loss (cross-entropy) + λ×CKA + μ×REPINA

- **Design tradeoffs:**
  - λ (CKA weight): Higher values (0.05) for small data/short training; lower values (0.01) for larger data to avoid over-regularization
  - μ (REPINA weight): Higher values stabilize HRL but may slow early task learning; reduce to 0.01 for very low-data regimes
  - Layer selection: Sweep layers 10–20 for LRL→HRL; later layers (≈20) may be better for HRL→LRL (preliminary evidence)
  - Sequence truncation: Santali requires 368 tokens vs. 256 for others; truncation reduces token-wise alignment overlap

- **Failure signatures:**
  - Over-alignment: Performance degrades on typologically close pairs → reduce λ or switch to REPINA-only
  - Under-alignment: No improvement over NoAlign baseline → increase λ or verify parallel data quality
  - Early-training instability: REPINA cancels task learning with large μ → reduce μ to 0.01 for small-data sweeps
  - Layer mismatch: Peak performance at unexpected layer → verify hidden state indexing, check model architecture matches expectations

- **First 3 experiments:**
  1. **Layer sweep with 1k samples, 1 epoch:** Test ℓ ∈ {1, 2, 5, 10, 15, 20, 25, 30, 31, 32} with CKA-only and TRepLiNa (λ=0.05, μ=0.05) on one LRL→HRL pair to identify peak layer
  2. **Ablation at best layer:** Compare NoAlign vs. CKA-only vs. REPINA-only vs. TRepLiNa at the peak layer to isolate contribution of each component
  3. **Scale-up validation:** Train 5 epochs on full 20k data at best layer with reduced CKA weight (λ=0.01, μ=0.05), evaluating on development set each epoch to check for over-alignment as training progresses

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed layer asymmetry (mid-layer peaks for LRL→HRL vs. later-layer peaks for HRL→LRL) generalize across diverse language pairs and model architectures? Only a preliminary 1k-pair sweep was conducted for one HRL→LRL direction, insufficient to establish general patterns.

### Open Question 2
How should alignment strength (λ) be adaptively scheduled or tuned based on language pair proximity and dataset size? No systematic approach was developed; coefficients were set without schedulers.

### Open Question 3
Would alternative similarity objectives (cosine, contrastive InfoNCE, or newer methods) outperform CKA for layer-wise alignment in low-resource MT? Only CKA was evaluated; no comparison to other alignment metrics was conducted.

### Open Question 4
Does TRepLiNa transfer effectively to encoder-decoder architectures and other modalities such as speech-to-text? All experiments used only the decoder-only Aya-23 8B model on text translation.

## Limitations

- Model-specific findings may not generalize to other architectures or scales
- REPINA mechanism lacks direct comparison to alternative stabilization approaches
- Typology-based recommendations based on limited set of four language pairs
- No systematic hyperparameter ablation study for λ and μ coefficients

## Confidence

- **High confidence:** Layer 15 consistently emerges as optimal for TRepLiNa in data-scarce conditions, and CKA alignment improves low-resource translation over no-alignment baselines across multiple language pairs
- **Medium confidence:** The asymmetric stabilization mechanism via REPINA is effective, and REPINA-only can outperform TRepLiNa for related language pairs, though evidence is primarily from ablation studies
- **Low confidence:** Specific λ and μ hyperparameter recommendations (0.05 for small data, 0.01 for large data) are highly context-dependent and may require adjustment for different models, tasks, or data characteristics

## Next Checks

1. **Layer Transferability Test:** Apply the same layer-wise sweep methodology to a smaller multilingual model (e.g., 2B-4B parameters) to verify whether the mid-layer alignment phenomenon holds across scales, and identify whether optimal layers shift proportionally with model depth

2. **Script Variation Impact:** Systematically vary script representation (e.g., transliterate Santali to Devanagari, test Bhili in non-Devanagari scripts) to isolate the contribution of script similarity versus linguistic typology to alignment effectiveness, particularly for the observed over-alignment in related pairs

3. **Zero-Shot Transfer Robustness:** Evaluate TRepLiNa's impact on zero-shot translation quality for unseen language pairs sharing the same HRL pivot (e.g., train on Bhili→Hindi, test on Marwari→Hindi) to determine whether layer-wise alignment creates transferable representational bridges beyond the training pairs