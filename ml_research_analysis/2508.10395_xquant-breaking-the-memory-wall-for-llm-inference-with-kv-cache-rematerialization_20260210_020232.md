---
ver: rpa2
title: 'XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization'
arxiv_id: '2508.10395'
source_url: https://arxiv.org/abs/2508.10395
tags:
- memory
- cache
- quantization
- which
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "XQuant addresses the memory bottleneck in large language model\
  \ inference by quantizing and caching layer input activations (X) instead of the\
  \ standard Key-Value (KV) cache. This approach provides a 2\xD7 memory savings compared\
  \ to KV cache quantization, as only one tensor per layer needs to be stored."
---

# XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization

## Quick Facts
- **arXiv ID**: 2508.10395
- **Source URL**: https://arxiv.org/abs/2508.10395
- **Reference count**: 40
- **Primary result**: Achieves up to 10× memory savings with <0.1 perplexity degradation via KV cache rematerialization

## Executive Summary
XQuant addresses the memory bottleneck in large language model inference by quantizing and caching layer input activations instead of the standard Key-Value (KV) cache. This approach provides 2× memory savings compared to KV cache quantization, as only one tensor per layer needs to be stored. By rematerializing the Keys and Values on-the-fly during inference, XQuant achieves up to 7.7× memory savings with less than 0.1 perplexity degradation compared to the FP16 baseline. The method also supports Grouped Query Attention models through singular value decomposition of projection matrices, enabling efficient quantization in a lower-dimensional latent space.

## Method Summary
XQuant introduces a novel approach to LLM inference memory optimization by caching layer input activations (X) rather than the traditional KV cache. The method uses asymmetric uniform quantization with group sizes of 128 bits to compress X values, which are then dequantized and used to rematerialize K and V on-the-fly during inference. XQuant-CL extends this by exploiting cross-layer similarity in X embeddings, compressing deltas between layers using an accumulator. For Grouped Query Attention models, the method applies SVD to projection matrices offline, projecting X into a latent space before caching. This allows efficient quantization while maintaining near-FP16 accuracy across various models and datasets.

## Key Results
- Achieves up to 10× memory savings with only 0.01 perplexity degradation using XQuant-CL
- Provides 2× memory savings compared to KV cache quantization methods
- Maintains <0.1 perplexity degradation relative to FP16 baseline across multiple models and datasets
- Supports both Multi-Head Attention and Grouped Query Attention architectures

## Why This Works (Mechanism)
The method exploits the observation that layer input activations (X) are more compressible than KV values due to their similarity across layers. By caching X instead of K and V, only one tensor per layer needs storage rather than two. The rematerialization process uses projection weights to reconstruct K and V on-demand, trading compute for memory bandwidth. For GQA models, SVD projection reduces dimensionality before quantization, further improving compression ratios. The cross-layer compression in XQuant-CL leverages the fact that X values don't change dramatically between consecutive layers, allowing delta encoding with minimal information loss.

## Foundational Learning

**KV Cache Compression**: Why needed - Reduces memory footprint during autoregressive generation. Quick check - Compare memory usage of cached K/V vs. cached X values.

**Quantization Methods**: Why needed - Enables aggressive compression while maintaining accuracy. Quick check - Verify that 2-4 bit quantization doesn't cause significant perplexity degradation.

**Rematerialization**: Why needed - Allows reconstruction of K and V from cached X values. Quick check - Ensure projection matrices can accurately recover K and V from quantized X.

**SVD Projection**: Why needed - Enables efficient quantization for GQA models by reducing dimensionality. Quick check - Verify that latent space representation preserves attention patterns.

**Delta Encoding**: Why needed - Exploits similarity between consecutive layer inputs for cross-layer compression. Quick check - Measure distribution of X deltas to confirm compressibility.

## Architecture Onboarding

**Component Map**: Input X -> Quantization -> Cache Storage -> Dequantization -> Projection Matrices -> Rematerialized K,V -> Attention

**Critical Path**: The forward pass must intercept X after normalization, quantize and cache it, then during generation rematerialize K and V from cached X using projection weights before computing attention.

**Design Tradeoffs**: Memory vs. Compute - caching X reduces memory but increases compute due to rematerialization. Compression ratio vs. accuracy - higher quantization reduces memory but may increase perplexity.

**Failure Signatures**: Increased latency despite memory savings indicates compute overhead dominates. Significant perplexity degradation suggests quantization or rematerialization introduces too much error.

**First Experiments**:
1. Implement basic XQuant on a small MHA model and verify 2× memory reduction
2. Test XQuant-CL on Llama-2-7B with varying quantization levels
3. Apply SVD projection to a GQA model and measure compression gains

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section and methodology, several important questions remain: whether the method achieves wall-clock latency speedups on current hardware, if the XQuant-CL accumulator suffers from error accumulation over deep networks, and whether the observed outlier structure in GQA latent space is universal across different model families.

## Limitations
- Evaluation limited to models up to 13B parameters and English text datasets
- Lacks comprehensive runtime performance measurements beyond memory savings
- Cross-layer compression assumes similarity between layer inputs which may not hold for all architectures
- Does not thoroughly investigate performance on multilingual datasets or specialized domains

## Confidence

**Memory Compression Claims** (High confidence): The methodology for quantizing layer inputs instead of KV values is clearly defined, and the 2× improvement over KV cache quantization is well-supported by the mathematical framework.

**Perplexity Degradation Results** (Medium confidence): While the paper reports <0.1 perplexity degradation for XQuant and <0.01 for XQuant-CL, these results are primarily demonstrated on standard benchmarks. The robustness of these results across diverse model types and tasks requires further validation.

**Runtime Performance** (Low confidence): The paper focuses primarily on memory savings without providing comprehensive latency measurements or demonstrating the method's effectiveness on actual hardware under realistic serving conditions.

## Next Checks

1. Implement runtime latency measurements on H100 GPU to verify that memory bandwidth savings translate to practical speed improvements, particularly for sequence lengths between 2K-8K tokens where the method should be most effective.

2. Test XQuant-CL on multilingual datasets (e.g., multilingual WikiText or OSCAR) to validate whether the cross-layer similarity assumption holds across different languages and token distributions.

3. Evaluate the method on larger models (70B+ parameters) and alternative architectures (e.g., DeepSeek-V2, LLaMA-3.2) to assess scalability and generalizability beyond the tested 7B-13B parameter range.