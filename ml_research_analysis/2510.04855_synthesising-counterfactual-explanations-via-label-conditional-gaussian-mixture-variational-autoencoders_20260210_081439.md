---
ver: rpa2
title: Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture
  Variational Autoencoders
arxiv_id: '2510.04855'
source_url: https://arxiv.org/abs/2510.04855
tags:
- input
- latent
- l-gmv
- each
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LAPACE, a method for generating counterfactual
  explanations that are robust to input and model perturbations while satisfying plausibility
  and diversity. The core idea is to train a Label-conditional Gaussian Mixture Variational
  Autoencoder (L-GMVAE) that learns a structured latent space where each class is
  represented by multiple Gaussian clusters.
---

# Synthesising Counterfactual Explanations via Label-conditional Gaussian Mixture Variational Autoencoders

## Quick Facts
- arXiv ID: 2510.04855
- Source URL: https://arxiv.org/abs/2510.04855
- Authors: Junqi Jiang; Francesco Leofante; Antonio Rago; Francesca Toni
- Reference count: 17
- Primary result: Introduces LAPACE, a method for generating robust and diverse counterfactual explanations using a Label-conditional Gaussian Mixture Variational Autoencoder.

## Executive Summary
This paper proposes LAPACE, a method for generating counterfactual explanations that are robust to input and model perturbations while satisfying plausibility and diversity. The core idea is to train a Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE) that learns a structured latent space where each class is represented by multiple Gaussian clusters. LAPACE then generates paths of counterfactuals by interpolating from an input's latent representation to the learned cluster centroids of the target class. This design inherently ensures robustness to input perturbations (all paths converge to the same fixed centroids) and provides diverse recourse options.

## Method Summary
LAPACE trains a Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE) to learn a structured latent space where each class is represented by multiple Gaussian clusters. For a given input, it generates counterfactuals by interpolating from the input's latent representation to the learned cluster centroids of the target class. The method is model-agnostic, computationally efficient, and supports actionability constraints via gradient optimization.

## Key Results
- Achieves perfect robustness to input changes and strong performance in model-change robustness
- Outperforms several state-of-the-art baselines across eight datasets
- Demonstrates competitive performance across eight quantitative metrics, including validity, proximity, plausibility, diversity, and robustness

## Why This Works (Mechanism)
The method works by learning a structured latent space through the L-GM-VAE, where each class is represented by multiple Gaussian clusters. This structure allows for generating counterfactuals by interpolating from an input's latent representation to the cluster centroids of the target class. The use of fixed centroids ensures robustness to input perturbations, while the multiple clusters per class provide diversity in the generated counterfactuals.

## Foundational Learning
- Variational Autoencoders (VAEs): A generative model that learns to encode data into a latent space and decode it back, used here to learn a structured latent representation of the data.
- Gaussian Mixture Models (GMMs): A probabilistic model that represents data as a mixture of multiple Gaussian distributions, used here to model each class in the latent space.
- Counterfactual Explanations: Explanations that describe how an input would need to change to achieve a different outcome, used here to provide actionable recourse for users.

## Architecture Onboarding

Component Map:
Input -> L-GM-VAE -> Latent Space -> Interpolation to Centroids -> Counterfactuals

Critical Path:
The critical path is the generation of counterfactuals through interpolation from the input's latent representation to the learned cluster centroids of the target class.

Design Tradeoffs:
- Using a structured latent space with Gaussian clusters provides robustness and diversity but may limit the expressiveness of the model.
- The method is model-agnostic but relies on the quality of the learned latent space.

Failure Signatures:
- Poor performance on datasets with complex decision boundaries or high-dimensional data.
- Limited diversity in the generated counterfactuals if the learned clusters are not well-separated.

First Experiments:
1. Test LAPACE on a simple 2D dataset to visualize the learned latent space and generated counterfactuals.
2. Evaluate the robustness of the generated counterfactuals to input perturbations on a benchmark dataset.
3. Compare the diversity of the generated counterfactuals with other state-of-the-art methods on a real-world dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to high-dimensional data beyond the tested datasets
- Handling of actionability constraints in practice
- Performance under real-world input noise patterns and dynamic model updates

## Confidence
High: Robustness to input perturbations, diverse counterfactual generation
Medium: Model-change robustness, evaluation on pre-trained models
Low: Scalability, practical deployment, handling of real-world constraints

## Next Checks
1. Test LAPACE on high-dimensional image or text datasets to evaluate scalability and latent space structure quality.
2. Conduct user studies to assess whether the generated counterfactuals are semantically meaningful and actionable from a human perspective.
3. Evaluate robustness under real-world input noise distributions and dynamic model updates to validate the claims beyond synthetic settings.