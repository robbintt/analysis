---
ver: rpa2
title: Model Fusion via Neuron Transplantation
arxiv_id: '2502.06849'
source_url: https://arxiv.org/abs/2502.06849
tags:
- ensemble
- fusion
- pruning
- fine-tuning
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Neuron Transplantation (NT), a novel ensemble
  compression technique that fuses multiple trained neural networks into a single
  model by transplanting important neurons from all ensemble members into the space
  obtained by pruning insignificant neurons. The method addresses the memory and inference
  time challenges of deploying deep ensembles by creating a fused model that can recover
  ensemble performance through fine-tuning.
---

# Model Fusion via Neuron Transplantation

## Quick Facts
- arXiv ID: 2502.06849
- Source URL: https://arxiv.org/abs/2502.06849
- Reference count: 40
- Primary result: Neuron Transplantation (NT) fuses ensembles into single models matching or exceeding Optimal-Transport fusion performance while requiring less memory and faster fusion time.

## Executive Summary
This paper introduces Neuron Transplantation (NT), a novel ensemble compression technique that fuses multiple trained neural networks into a single model by transplanting important neurons from all ensemble members into the space obtained by pruning insignificant neurons. The method addresses the memory and inference time challenges of deploying deep ensembles by creating a fused model that can recover ensemble performance through fine-tuning. Experiments on multiple datasets (MNIST, CIFAR10/100, SVHN) and architectures (MLP, LeNet, VGG11, ResNet18) show that NT consistently outperforms individual ensemble members and matches or exceeds the performance of Optimal-Transport fusion while requiring less memory and faster fusion time.

## Method Summary
Neuron Transplantation works by concatenating models layer-wise, pruning low-magnitude neurons to maintain the original architecture, and then fine-tuning the fused model. The method uses L2-norm structured pruning to remove low-contribution neurons from each ensemble member, creating capacity for transplanting high-magnitude neurons from other models. The fused model retains neurons that independently exist in low-loss basins, avoiding the loss barrier problem inherent to weight averaging. Zero-initialized connections between transplanted neuron groups are learned during fine-tuning, enabling knowledge fusion. The approach scales from small MLPs to larger architectures like ResNet18 and demonstrates robustness across different layer widths, depths, and number of models.

## Key Results
- NT consistently outperforms individual ensemble members and matches or exceeds OT-fusion performance
- Hierarchical fusion strategy scales better than iterative approaches for large ensembles
- NT shows diminishing returns beyond ~8 models but maintains superior memory efficiency
- The method fails when fusing highly similar models due to redundancy in transplanted neurons

## Why This Works (Mechanism)

### Mechanism 1: Selective Knowledge Preservation via Magnitude-Based Pruning
High-magnitude neurons capture the most functionally important learned representations, enabling selective retention of ensemble knowledge. L2-norm structured pruning removes low-contribution neurons from each ensemble member, creating capacity for transplanting high-magnitude neurons from other models. The fused model retains neurons that independently exist in low-loss basins.

### Mechanism 2: Loss Barrier Avoidance Through Selection vs. Averaging
NT avoids the loss barrier problem inherent to weight averaging by selecting rather than interpolating between neurons. Weight averaging between models in different loss basins creates interpolated weights in high-loss regions. NT preserves actual learned weights that already exist in low-loss basins, merely reorganizing them.

### Mechanism 3: Cross-Weight Integration via Fine-Tuning
Zero-initialized connections between transplanted neuron groups can learn beneficial interactions during fine-tuning, enabling knowledge fusion. Concatenation creates new inter-layer connections initialized to zero. Fine-tuning optimizes both transplanted weights and cross-weights simultaneously, allowing the model to discover synergies between previously separate networks.

## Foundational Learning

- **Structured Magnitude Pruning**: NT's core operation relies on removing entire neurons (not individual weights) based on L2-norm. Understanding why magnitude indicates importance is essential for debugging fusion failures.
  - Quick check: Given a layer with weights of norms [3.2, 0.8, 2.1, 0.3], which neurons would be pruned first at 50% sparsity?

- **Loss Landscapes and Mode Connectivity**: The paper explicitly contrasts NT with weight averaging by referencing loss barriers. Without this concept, you cannot understand why NT outperforms vanilla averaging.
  - Quick check: If two trained models lie in different loss basins, why does their weight average often perform worse than either individual model?

- **Ensemble Diversity Metrics**: NT's fundamental limitation is fusing "too similar" models. Recognizing diversity through metrics (e.g., prediction disagreement, weight distance) prevents wasted experimentation.
  - Quick check: Two models trained from different random seeds on the same data achieve 99% prediction agreement. Would NT fusion likely succeed or fail?

## Architecture Onboarding

- **Component map**: Ensemble Trainer -> Layer Concatenator -> Pruning Engine -> Fine-Tuning Loop
- **Critical path**: 1. Train ensemble (parallelizable) -> 2. Concatenate all layers -> 3. Prune jointly to original size -> 4. Fine-tune fused model
- **Design tradeoffs**: Merge-then-prune slightly outperforms prune-then-merge, but latter enables parallel local pruning. Hierarchical/recursive fusion scales better than iterative. Pruning to larger-than-original size preserves more ensemble performance but increases inference cost.
- **Failure signatures**: 
  1. Self-fusion collapse: Fusing identical models drops accuracy with minimal recovery—indicates redundancy issue.
  2. Ensemble saturation: Performance gains plateau beyond ~8 models.
  3. Immediate post-fusion crash: Very low accuracy after pruning is expected; recovery requires adequate fine-tuning.
- **First 3 experiments**:
  1. Reproduce the 2-model MLP fusion on SVHN (width=512, 3 hidden layers). Target: ~84.91% after 20 epochs fine-tuning.
  2. Run the "merge-with-itself" failure case: fuse a trained model with its own copy. Confirm the redundancy problem.
  3. Compare merge-prune-fine-tune vs. prune-merge-fine-tune on the same 2-model setup. Verify Table 1 findings.

## Open Questions the Paper Calls Out

- **Can Neuron Transplantation effectively extend to Transformer architectures and their attention mechanisms?** The authors explicitly state this application is left for future work, as the study only validated NT on MLPs and CNNs.
- **Can a reliable heuristic be developed to predict if models are sufficiently diverse for NT to succeed?** The paper suggests it might be possible but does not propose a metric to quantify this "diversity" threshold pre-fusion.
- **Does the joint pruning approach overcompensate for introduced cross-weights when NT is used strictly for ensemble pruning?** The authors leave open whether joint pruning and joint training can overcompensate for the newly introduced cross-weights in the context of pure ensemble pruning.

## Limitations

- The method fails when fusing highly similar models due to redundancy in transplanted neurons, causing net information loss
- Performance gains plateau beyond ~8 models, showing diminishing returns despite larger ensemble size
- Implementation details for complex architectures like ResNet skip connections are underspecified, potentially affecting reproducibility

## Confidence

- **High confidence**: The basic fusion methodology (concatenate → prune → fine-tune) works as described for moderately diverse ensembles
- **Medium confidence**: The loss barrier avoidance mechanism is plausible given contrasting results with vanilla averaging
- **Medium confidence**: The scalability claims (hierarchical vs. iterative fusion) are supported by ablation studies
- **Low confidence**: The assumption that L2-norm pruning optimally identifies functionally important neurons is not rigorously validated

## Next Checks

1. **Self-fusion replication**: Fuse a trained model with an identical copy and verify the predicted accuracy collapse (~67% immediate, minimal recovery) to confirm the redundancy mechanism.
2. **Pruning metric ablation**: Repeat the 2-model fusion experiment using L1-norm pruning instead of L2-norm to test the sensitivity of results to the pruning criterion.
3. **Fine-tuning schedule sensitivity**: Systematically vary fine-tuning learning rates (0.01, 0.001, 0.0001) and epoch counts (10, 30, 50) to establish minimum requirements for performance recovery.