---
ver: rpa2
title: 'Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration
  of knowledge'
arxiv_id: '2502.02173'
source_url: https://arxiv.org/abs/2502.02173
tags:
- memit
- heads
- factual
- knowledge
- catalan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies knowledge editing methods in large language
  models, particularly focusing on cross-lingual performance and the role of attention
  mechanisms. The authors observe that existing methods like MEMIT are sensitive to
  subject tokenization and show limited cross-lingual effectiveness.
---

# Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge

## Quick Facts
- arXiv ID: 2502.02173
- Source URL: https://arxiv.org/abs/2502.02173
- Reference count: 40
- Primary result: MEMAT achieves 10% improvement in magnitude metrics across English and Catalan evaluations

## Executive Summary
This paper addresses cross-lingual knowledge editing in large language models by extending MEMIT with attention head corrections. The authors identify that MEMIT's effectiveness degrades when subject entities are tokenized differently across languages. To address this, they propose MEMAT, which trains binary classifiers on attention head representations to identify language-independent factual signals, then optimizes attention corrections to reinforce edited facts. The method demonstrates improved cross-lingual performance and better generalization than MEMIT alone.

## Method Summary
MEMAT extends MEMIT by incorporating attention head corrections optimized through a classifier-based approach. The method operates in three stages: first applying MEMIT edits to MLP output weights in a source language, then training binary classifiers on attention head representations from a target language to identify fact-relevant heads, and finally optimizing attention correction vectors through a multi-term loss function that balances edit strength against catastrophic forgetting. The approach requires minimal parameter modifications and demonstrates improved cross-lingual transfer compared to MEMIT alone.

## Key Results
- MEMAT achieves a 10% improvement in magnitude metrics (EM, PM) over MEMIT in cross-lingual settings
- Attention head classifiers achieve ~80% accuracy distinguishing true from false facts across languages
- Top-K attention head optimization (K=16) significantly improves cross-lingual knowledge editing effectiveness
- Method demonstrates portability, improving performance on unseen languages

## Why This Works (Mechanism)

### Mechanism 1
MEMIT-style knowledge editing depends on subject token representations, and cross-lingual transfer fails when subject tokenization diverges between languages. MEMIT computes correction matrices using hidden representations at the final token of subject entities. When the same subject is tokenized differently across languages (low Jaccard similarity), the representation used for correction diverges, preventing effective transfer. The MLP layer acts as key-value memory anchored to specific subject token representations. Evidence shows alterations to subject tokens result in pronounced performance decline in cross-lingual settings, particularly when Jaccard similarity ≤ 0.5.

### Mechanism 2
Certain attention heads encode language-independent signals about factual correctness detectable by binary classifiers. After MEMIT edits in language L1, attention head activations at the final token for correct and incorrect completions in language L2 can be used to train classifiers achieving ~78-82% accuracy. This suggests attention heads aggregate cross-lingual semantic information relevant to factual evaluation, independent of surface tokenization. Classification accuracy remains consistent across language pairs, though limited to two language pairs tested.

### Mechanism 3
Optimizing attention head corrections after MEMIT improves model confidence in edited facts without catastrophic forgetting. MEMAT identifies top-K heads with highest classifier accuracy, then optimizes correction vectors via a loss function maximizing target object probability, minimizing KL divergence on unrelated prompts, and regularizing correction magnitude. Head corrections encode complementary information to MLP edits, reinforcing associations rather than reintroducing them. Regularization strength critically balances edit efficacy against specificity degradation.

## Foundational Learning

- **Concept**: Factual knowledge editing (ROME/MEMIT)
  - **Why needed here**: MEMAT builds directly on MEMIT; understanding how MLP weight edits encode facts is prerequisite to grasping why attention supplementation helps
  - **Quick check question**: Explain why MEMIT modifies a subset of MLP layers rather than all layers

- **Concept**: Attention head representations
  - **Why needed here**: MEMAT's core innovation is training classifiers on headℓ,h vectors to locate fact-relevant heads
  - **Quick check question**: What is the dimensionality of a single attention head's output in a model with d=4096 and H=32 heads?

- **Concept**: Cross-lingual tokenization effects
  - **Why needed here**: The paper's central finding is that subject token Jaccard similarity predicts cross-lingual transfer success
  - **Quick check question**: Why would "Google" and "Google" (same string) have J=1 in English/Catalan but potentially J<0.5 in English/Chinese?

## Architecture Onboarding

- **Component map**: Águila-7B model -> MEMIT MLP Wout correction -> Attention head extraction -> Binary classifier training -> Top-K head selection -> ω optimization -> Inference with head corrections

- **Critical path**: 1) Run MEMIT on L1 facts → produces edited model G 2) Extract head representations for L2 prompts → train classifiers → select top-K heads 3) Optimize ω vectors via Adam with gradient accumulation 4) Apply ω corrections at inference

- **Design tradeoffs**: K (number of heads) - higher K improves efficacy but risks over-constraining; K=16 optimal. λω (regularization) - controls edit strength vs. model stability. L1/L2 language choice - same-language vs. cross-lingual affects classifier accuracy but not drastically

- **Failure signatures**: Magnitude metrics (EM, PM) plateau or degrade despite high success metrics → model uncertain, not confident. Neighborhood specificity (NM) drops → edits bleeding to unrelated subjects, regularization too weak. Classifier accuracy ~50% → heads not encoding factual signal; check prompt construction or layer selection

- **First 3 experiments**: 1) Replicate MEMIT baseline on 100 facts in English; record ES, EM to establish reference point 2) Train head classifiers on Catalan prompts after English MEMIT; verify ~80% accuracy on held-out facts 3) Run full MEMAT with K=16, λω from paper; compare EM improvement over MEMIT baseline (>10% expected)

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the cross-lingual effectiveness of MEMAT generalize to languages with distinct alphabets or tokenization schemes (e.g., Chinese, Arabic) where subject token overlap is minimal?
- **Open Question 2**: Can the proposed attention head optimization strategy be effectively transferred to alternative domains beyond factual association, such as logical reasoning or sentiment analysis?
- **Open Question 3**: To what extent do the optimized attention head corrections decouple knowledge editing performance from the specific tokenization of the subject entity?

## Limitations
- Limited linguistic diversity - experiments restricted to English and Catalan, both using Latin alphabets with high subject token similarity
- Classifier-based head selection validity - binary classifiers achieve ~80% accuracy but only tested on two language pairs
- Regularization parameter sensitivity - λω weight critical for balancing edit strength against catastrophic forgetting, but no sensitivity analysis reported

## Confidence

- **High confidence**: MEMIT's sensitivity to subject tokenization (supported by direct experimental evidence showing performance decline with divergent tokenizations)
- **Medium confidence**: MEMAT's 10% magnitude metric improvement over MEMIT (reproducible within tested conditions but not yet generalized)
- **Low confidence**: Cross-lingual applicability beyond English/Catalan (insufficient linguistic diversity in validation)

## Next Checks

1. **Cross-linguistic robustness test**: Evaluate MEMAT on at least three additional language pairs spanning different language families (e.g., English-Japanese, English-Arabic, English-Chinese) to verify that attention head classifiers maintain ~80% accuracy and that magnitude improvements persist across typologically diverse languages.

2. **Tokenization sensitivity analysis**: Systematically vary subject tokenization methods (e.g., different tokenizers, byte-pair encoding variants) within a single language pair to determine whether Jaccard similarity remains the primary predictor of cross-lingual transfer success, or whether other factors (semantic drift, positional encoding differences) contribute.

3. **Long-term stability evaluation**: After applying MEMAT edits, evaluate model performance on the edited facts after 1,000 additional training steps on unrelated tasks to quantify catastrophic forgetting resistance and confirm that attention corrections provide more stable knowledge encoding than MLP-only edits.