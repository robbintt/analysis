---
ver: rpa2
title: Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing
  Low-Level Process Event Streams
arxiv_id: '2505.05880'
source_url: https://arxiv.org/abs/2505.05880
tags:
- event
- process
- trace
- activity
- activities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a neuro-symbolic approach to analyzing low-level
  process event streams by combining a deep learning sequence tagger with an Abstract
  Argumentation Framework (AAF)-based reasoner. The tagger provides context-aware,
  probabilistic event-to-activity mappings, while the AAF-based reasoner refines these
  predictions using domain knowledge (behavioral constraints) to ensure semantic validity
  and interpretability.
---

# Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams

## Quick Facts
- arXiv ID: 2505.05880
- Source URL: https://arxiv.org/abs/2505.05880
- Reference count: 38
- Combines neural sequence tagging with Abstract Argumentation Frameworks to improve event-to-activity mapping accuracy in process mining, achieving up to 20% better performance than pure ML approaches.

## Executive Summary
This paper introduces a neuro-symbolic approach to analyzing low-level process event streams by combining a deep learning sequence tagger with an Abstract Argumentation Framework (AAF)-based reasoner. The tagger provides context-aware, probabilistic event-to-activity mappings, while the AAF-based reasoner refines these predictions using domain knowledge (behavioral constraints) to ensure semantic validity and interpretability. This hybrid method improves prediction accuracy—by up to 20% in some cases—over pure ML approaches, especially when training data is scarce. The framework also supports interactive query answering and explanations, enhancing transparency and usability in process mining applications.

## Method Summary
The method combines a neural sequence tagger (either LSTM-based MA or window-based DNN MB architecture) with an AAF-based reasoner. The tagger predicts a probability distribution over activities for each event, conditioned on the preceding event sequence. The reasoner then filters these predictions, zeroing out those that violate domain constraints encoded in the AAF. The system uses top-k pruning to maintain efficiency by constructing the AAF with only the k most probable activities per event. The approach includes Laplace smoothing and supports interactive explanations by querying the AAF structure for why certain interpretations are rejected.

## Key Results
- The hybrid neuro-symbolic approach achieves up to 20% higher accuracy than pure ML taggers, particularly with limited training data
- The framework provides explainable predictions through AAF-based reasoning, a feature absent in black-box ML models
- Experimental results demonstrate effectiveness on synthetic datasets, though real-world validation remains future work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining a neural sequence tagger with an AAF-based reasoner improves event-to-activity mapping accuracy compared to using either component in isolation.
- Mechanism: The neural tagger (MA or MB architectures) provides a context-aware probabilistic distribution over possible activities for a given event, conditioned on the preceding event sequence. The AAF-based reasoner then filters these probabilistic predictions, zeroing out those that violate domain constraints (e.g., behavioral rules like must-constraints). This acts as a "probabilistic beam search" where the tagger proposes and the reasoner disposes based on symbolic knowledge.
- Core assumption: The domain knowledge encoded in the AAF (mappings and constraints) is correct and sufficient to invalidate incorrect tagger predictions.
- Evidence anchors:
  - [abstract]: "This hybrid method improves prediction accuracy—by up to 20% in some cases—over pure ML approaches, especially when training data is scarce."
  - [section 4.1]: Algorithm 1 details the process: the tagger predicts `pd := M.predict(ecurr)`, and then the reasoner revises this: `if R.answer(⟨ecurr, a, , ⟩) = ∅ then pd[a]:=0`.
- Break condition: This mechanism fails if the tagger consistently assigns near-zero probability to the *correct* activity, or if the AAF reasoner's constraints are so loose they don't prune enough candidates, or so tight (or incorrect) they prune the correct one.

### Mechanism 2
- Claim: The framework enables efficient analysis by focusing the AAF construction on a small set of top-k probable activities per event, rather than all possible candidates.
- Mechanism: The system performs focused search. Instead of building an AAF with arguments for all possible event-activity mappings, it uses the tagger's probability scores to select only the k most probable candidates. The AAF is then updated using only these k candidates, reducing the size and complexity of the argumentation graph that must be solved.
- Core assumption: The correct event-to-activity mapping is, with high probability, within the top-k predictions provided by the neural tagger.
- Evidence anchors:
  - [section 4.1]: "retain only the k most probable activities in pd... Recompute the AAF using only the k most probable... activities... R.updateAAF(A);"
- Break condition: Efficiency gains are lost if k must be set very high to capture the correct mapping, or if the overhead of running the tagger and the reasoner query for every candidate outweighs the savings from a smaller AAF.

### Mechanism 3
- Claim: The framework provides interpretable results and explanations for its predictions, a feature lacking in pure black-box ML models.
- Mechanism: The system is built on an Abstract Argumentation Framework (AAF). The final prediction is an argument accepted under a specific semantics. When an interpretation is rejected, the AAF structure inherently supports querying *why*. The `R.explain(·)` function can provide the reasons (e.g., "activity A cannot follow activity B"), which correspond to attacks in the AAF graph.
- Core assumption: The underlying AAF is constructed such that arguments and attacks faithfully represent the domain's decision logic and constraints.
- Evidence anchors:
  - [abstract]: "The framework also supports interactive query answering and explanations, enhancing transparency and usability..."
  - [section 3.2]: "...explanations for those that conflict with prior process knowledge... explanation requests can be answered."
- Break condition: The explanations become useless or misleading if the AAF is too complex for a user to understand, or if the mapping from symbolic constraints to AAF arguments is not transparent.

## Foundational Learning

- Concept: **Abstract Argumentation Frameworks (AAF)**
  - Why needed here: The core reasoning engine. One must understand AAFs (arguments, attacks, extensions, preferred semantics) to grasp how the system resolves conflicting interpretations and provides explanations.
  - Quick check question: Given a set of arguments A={a, b, c} where 'a' attacks 'b' and 'b' attacks 'c', what are the conflict-free sets?

- Concept: **Sequence Tagging / Labeling with Recurrent Neural Networks (LSTMs)**
  - Why needed here: This is the neural component (the tagger) that provides the initial probabilistic predictions. Understanding how it processes a sequence of events and outputs a distribution over activities is crucial.
  - Quick check question: How does an LSTM handle long-range dependencies in a sequence compared to a simple feed-forward network with a fixed window?

- Concept: **Declarative Process Models and Constraints**
  - Why needed here: This is the symbolic domain knowledge. The reasoner's power comes from these constraints (must, not, precedence). Understanding their semantics is key to understanding what the reasoner contributes.
  - Quick check question: In a process with activities {A, B, C}, what does a "must-constraint" A =>2 B mean for a trace that executes A and then C?

## Architecture Onboarding

- Component map:
  Input Layer: Raw event stream Φ -> Neural Tagger (M): LSTM or Feed-Forward model -> Symbolic Reasoner (R): AAF-based module -> Integration Logic (Algorithm 1) -> Output Layer: Ranked interpretations and explanations

- Critical path:
  The online event processing loop (Algorithm 1). For each new event `ecurr`:
  1. `M.predict(ecurr)` -> get `pd`
  2. Loop over `a` in activities: if `R.answer(<ecurr, a, _, _>)` is empty, set `pd[a]=0`. Apply smoothing
  3. Select top-k activities from `pd` to form set A
  4. `R.updateAAF(A)` to rebuild the argumentation graph for the current trace state
  5. Present results to user and handle queries. This path determines the system's latency and accuracy.

- Design tradeoffs:
  - **Accuracy vs. Efficiency (Parameter k):** A small `k` makes the AAF smaller and faster to solve but increases the risk of discarding the correct interpretation if the tagger ranks it low. A large `k` improves recall but hurts performance.
  - **Flexibility vs. Complexity:** Enforcing constraints at runtime allows changing rules without retraining the neural model. However, it makes the online inference step more complex and slower than a pure neural forward pass.
  - **Model Choice (MA vs. MB):** MA (LSTM) captures long-term dependencies but may be slower to train and infer. MB (Feed-Forward with window) is faster but has limited context.

- Failure signatures:
  - **Zero Valid Interpretations:** If `pd[a]` is zeroed for all `a` by the reasoner and the smoothing factor is insufficient or applied to already-rejected candidates, the system fails to produce a prediction.
  - **Constant "NotInterpreted" Argument:** If the AAF construction or argument selection is flawed, the `NotInterpretedi` argument might remain un-attacked or un-resolved, leading to an inability to commit to any interpretation.
  - **Rejection of Correct Interpretation:** If the tagger assigns a very low probability to the correct activity and it falls outside the top-k, the reasoner will never consider it.

- First 3 experiments:
  1. **Reproduce Baseline Accuracy:** Replicate the main result (Fig 4) comparing AccT, AccT+A, and AccT+R on the SYN dataset with varying trace lengths to verify the ~20% improvement claim of the hybrid model.
  2. **Sensitivity to `k`:** Vary the `k` parameter (e.g., k=1, 3, 5, 10, |A|) and measure both accuracy (AccT+R) and average prediction time (TimeT+R). This will quantify the accuracy-efficiency trade-off.
  3. **Constraint Perturbation:** Introduce noise or errors into the declarative process model (W) used by the reasoner (e.g., flip a "must-constraint" to a "not-constraint") and measure the degradation in AccT+R. This tests the system's robustness to flawed symbolic knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Active Learning strategies effectively minimize the manual annotation effort required to train the trace tagger?
- **Basis in paper:** [explicit] The authors state they plan to investigate Active Learning schemes to assist experts, ranking traces based on prediction entropy or the frequency of reasoner-rejected predictions.
- **Why unresolved:** The current framework trains on a fixed set of annotated traces; the proposed interactive selection mechanisms have not yet been implemented or evaluated.
- **What evidence would resolve it:** Experimental results comparing model accuracy and annotation load between random sampling and the proposed active learning selection criteria.

### Open Question 2
- **Question:** Can semi-supervised learning techniques utilizing unlabeled traces improve the tagger's performance in data-scarce scenarios?
- **Basis in paper:** [explicit] The authors propose extending the training procedure to include unlabeled traces by applying an entropy-regularization loss term.
- **Why unresolved:** The current implementation relies entirely on labeled data, which is acknowledged as costly and difficult to obtain.
- **What evidence would resolve it:** Benchmarks showing accuracy improvements (specifically Acc_T+R) when the tagger is trained with the proposed entropy-regularization on unlabeled data versus purely supervised training.

### Open Question 3
- **Question:** How does the neuro-symbolic framework perform on real-world, noisy event logs compared to the synthetic dataset used?
- **Basis in paper:** [inferred] The experimental evaluation relies entirely on a synthetic dataset (SYN) generated from a known model, while the authors claim applicability to "complex real-life engineering applications."
- **Why unresolved:** Real-world logs typically contain noise, incomplete mappings, and deviations from declarative constraints that may not be present in synthetic data, potentially affecting the reasoner's validity checks.
- **What evidence would resolve it:** Evaluation of the approach on publicly available real-life event logs to validate robustness and efficiency outside of controlled synthetic environments.

## Limitations
- The evaluation relies solely on a synthetic dataset, requiring validation on real-world event logs
- The system's performance is highly sensitive to the correctness and completeness of the declarative constraints
- The hybrid approach is approximately 1000x slower than pure ML taggers, raising practical efficiency concerns

## Confidence
- **High Confidence**: The general neuro-symbolic approach combining neural sequence tagging with symbolic constraint reasoning is valid and addresses a real problem in process mining
- **Medium Confidence**: The specific mechanisms for top-k pruning and AAF-based refinement are sound, but their effectiveness may vary with different constraint sets and datasets
- **Low Confidence**: The exact numerical results (e.g., the precise 20% improvement) and their generalizability to real-world scenarios

## Next Checks
1. **Real-World Dataset Validation**: Apply the framework to a real-world event log (e.g., from the BPI challenge) and compare its performance against the synthetic dataset results to assess generalizability
2. **Constraint Error Robustness Test**: Systematically introduce errors or noise into the declarative process model and measure the degradation in accuracy
3. **Efficiency Benchmarking**: Conduct a detailed runtime analysis of the hybrid approach, varying the top-k parameter and the number of activities, to identify the sweet spot where accuracy gains justify the computational cost