---
ver: rpa2
title: 'Robust Distributed Learning under Resource Constraints: Decentralized Quantile
  Estimation via (Asynchronous) ADMM'
arxiv_id: '2601.20571'
source_url: https://arxiv.org/abs/2601.20571
tags:
- mean
- asyladmm
- trimming
- estimation
- quantile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust decentralized learning
  under resource constraints in edge AI systems, where data is processed locally on
  resource-constrained devices. The main challenge is achieving robustness to data
  corruption while maintaining communication efficiency and low memory usage.
---

# Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM

## Quick Facts
- arXiv ID: 2601.20571
- Source URL: https://arxiv.org/abs/2601.20571
- Reference count: 40
- Primary result: Asynchronous ADMM-based gossip algorithm requiring only 2 variables per node for decentralized median/quantile estimation under resource constraints

## Executive Summary
This paper addresses robust decentralized learning in edge AI systems where data is processed locally on resource-constrained devices. The main challenge is achieving robustness to data corruption while maintaining communication efficiency and low memory usage. The authors propose AsylADMM, an asynchronous and memory-efficient ADMM-based gossip algorithm for decentralized median and quantile estimation that requires only two variables per node instead of O(d) variables. Empirical results show AsylADMM converges faster than competing methods across various network topologies and data distributions, while also demonstrating strong robustness to data contamination and scalability to large networks.

## Method Summary
The method reformulates decentralized quantile estimation as a consensus optimization problem using ADMM. The key innovation is AsylADMM, which uses a z-y-x update ordering and aggregates dual variables per node to reduce memory from O(d) to O(1). The algorithm operates asynchronously by randomly sampling edges for updates, computing consensus using current values from both endpoints, updating aggregated duals, and applying proximal operators for the non-smooth pinball loss. This approach achieves faster convergence and better robustness to data contamination compared to rank-based trimming methods while maintaining low memory footprint suitable for edge devices.

## Key Results
- AsylADMM converges faster than DAPD, AsyncADMM, and subgradient descent across various network topologies and data distributions
- Quantile-based trimming empirically outperforms existing rank-based methods with MAE < 0.5 vs rank-based plateauing > 2.0
- Novel theoretical analysis of rank-based trimming via Markov chain theory provides exponential bounds
- Strong robustness to data contamination and scalability to large networks (n > 1000)

## Why This Works (Mechanism)

### Mechanism 1
Aggregating dual variables per node reduces memory from O(d) to O(1) while preserving convergence information. The reparameterization ˆµk = Σe∈Nk ye,k/dk compresses edge-specific dual variables into a single node-level aggregate that encodes the history of consensus constraints with all neighbors. This aggregate enables the primal update to depend on local information without storing per-edge states. Core assumption: The aggregate sufficiently captures the constraint violation history that would otherwise require per-edge tracking. Evidence: [abstract] "requiring only two variables per node" vs "memory that scales with node degree" and [Section 3.2] "reparameterization ˆµk = Σe∈Nk ye,k/dk".

### Mechanism 2
Reordering ADMM updates to z-y-x sequence eliminates the need to store neighbor values from previous iterations. Standard ADMM (x-z-y) requires the z-update to reference previous x values, necessitating storage. The z-y-x ordering computes consensus first using current x values, then updates duals, then primal—each step depends only on values from the current iteration or locally stored aggregates. Core assumption: The reordered variant maintains the same fixed-point properties as standard ADMM. Evidence: [Section 3.2] "we adopt a z-y-x ordering similar to Bianchi et al. (2015)" and [Appendix B.1] formal derivation.

### Mechanism 3
Using current values from both communication endpoints accelerates convergence compared to methods using stale neighbor information. When edge e=(i,j) activates, AsylADMM computes ze = (xi + xj)/2 using both nodes' current iterates. AsyncADMM uses outdated values from previous iteration. Current information better approximates the true neighborhood average that gossip protocols asymptotically achieve. Core assumption: The benefit of current information outweighs any theoretical guarantees sacrificed. Evidence: [Section 3.3] explicit comparison and [Figure 1a] empirical convergence curves.

## Foundational Learning

- **Proximal operators for non-smooth optimization**: Quantile estimation uses pinball loss (non-differentiable at zero). The algorithm's primal update requires computing prox_fk/(ρdk), which has closed form for pinball loss. Quick check: Given pinball loss Lα(z) = (α - I{z≤0})z, can you derive the three cases of the proximal operator?

- **ADMM for consensus optimization**: The decentralized quantile problem is reformulated as minimizing Σfi(xi) subject to xi = xj for all edges. ADMM splits this into local optimization (proximal step) and consensus enforcement (averaging step). Quick check: Why does the constraint matrix M being full column-rank matter for Problem (2) being well-posed?

- **Gossip protocols and graph connectivity**: Convergence rate depends on spectral gap of the graph Laplacian (c = λ2/|E|). The edge sampling probability pe = (1/n)·(1/di + 1/dj) ensures proper mixing. Quick check: Why does a cycle graph converge slower than Watts-Strogatz in Figure 1c?

## Architecture Onboarding

- Component map: Node k state {x_k, μ̂_k} → Edge selection (randomized gossip) → Consensus computation: z_e = (x_i + x_j)/2 → Dual update: μ̂_k += ρ(z_e - x_k)/d_k → Primal update: x_k = prox_{f_k/(ρd_k)}(z_e + μ̂_k/ρ)

- Critical path: The proximal operator computation requires the pinball loss proximal form. For observation a_k at node k:
  ```python
  def prox_pinball(z, a, gamma, beta):
      if z < a - gamma*beta: return z + gamma*beta
      elif z > a + gamma: return z - gamma
      else: return a
  ```

- Design tradeoffs: Step size ρ (smaller stabilizes but slows, larger accelerates but risks oscillation); synchronous vs asynchronous (async ~5x faster per |E| updates but lacks formal guarantees); quantile-based vs rank-based trimming (quantile-based reaches MAE < 0.5 while rank-based plateaus > 2.0).

- Failure signatures: Divergence with ρ > 2.0 on contaminated data; stagnation on complete graphs for AsyncADMM/DAPD (outdated information accumulates); slow convergence at extreme quantiles (α = 0.1, 0.9) with tail contamination.

- First 3 experiments:
  1. Implement synchronous variant on n=21 cycle graph with clean Gaussian data to verify convergence to true median within 5k iterations.
  2. Benchmark AsylADMM vs AsyncADMM on geometric graph with n=1000, measuring per-node memory and iterations to MAE < 0.1.
  3. Contaminate 30% of data with N(30, 5²) outliers and compare median estimation via AsylADMM against distributed mean.

## Open Questions the Paper Calls Out

### Open Question 1
Can formal convergence guarantees be established for the fully asynchronous AsylADMM algorithm under the standard gossip model? The abstract and introduction state that theoretical analysis is restricted to a synchronous variant, while the asynchronous version is supported only empirically. Appendix G outlines "technical obstacles" to the proof.

### Open Question 2
Can the discrepancy term A(t) identified in Appendix G be bounded such that it vanishes or is dominated by the residual norm during convergence? Appendix G details that the reparameterization breaks the antisymmetry property required for standard ADMM convergence proofs, resulting in a term A(t) that prevents the Lyapunov function from decreasing.

### Open Question 3
How do the tail probability bounds for rank-based trimming degrade when the Markov chain does not start from its stationary distribution? Remark 4.2 notes that the derived exponential bound holds for the stationary distribution, but for a general initial distribution, the bound only holds "up to an additive constant."

## Limitations
- Theoretical gaps: Asynchronous case lacks formal proof due to term A(t) breaking antisymmetry in Lyapunov function
- Extreme quantile sensitivity: Higher variance at extreme quantiles (α=0.1, 0.9) when data contamination is present
- Network topology assumptions: Performance claims assume reliable pairwise communication without characterizing behavior under packet loss or dynamic topologies

## Confidence

**High confidence**: Memory efficiency claim (2 variables vs 2d+1), empirical convergence speed comparisons, quantile-based trimming outperforming rank-based methods.

**Medium confidence**: Theoretical analysis of rank-based trimming via Markov chain theory, convergence rate dependence on spectral gap.

**Low confidence**: Asynchronous convergence proof (formally stated as open problem), robustness to severe communication delays (not explicitly tested).

## Next Checks
1. Implement the synchronous variant (Algorithm 1) on n=21 cycle graph with clean Gaussian data to verify basic convergence properties before attempting the asynchronous case.
2. Benchmark AsylADMM against AsyncADMM on geometric graph with n=1000 nodes, measuring per-node memory usage, iterations to MAE < 0.1, and variance across 100 trials.
3. Test quantile estimation at α=0.1 and α=0.9 with 30% contamination to empirically validate the higher variance warnings and determine practical quantile ranges.