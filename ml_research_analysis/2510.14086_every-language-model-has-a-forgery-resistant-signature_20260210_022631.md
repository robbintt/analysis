---
ver: rpa2
title: Every Language Model Has a Forgery-Resistant Signature
arxiv_id: '2510.14086'
source_url: https://arxiv.org/abs/2510.14086
tags:
- ellipse
- language
- outputs
- output
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel approach to identify the source
  of language model outputs by leveraging a geometric property: language model outputs
  lie on the surface of a high-dimensional ellipse (hyperellipsoid). The authors propose
  using this ellipse constraint as a "signature" for the model, which has unique properties
  compared to existing methods.'
---

# Every Language Model Has a Forgery-Resistant Signature

## Quick Facts
- arXiv ID: 2510.14086
- Source URL: https://arxiv.org/abs/2510.14086
- Authors: Matthew Finlayson; Xiang Ren; Swabha Swayamdipta
- Reference count: 23
- Primary result: Language model outputs lie on a high-dimensional ellipse (hyperellipsoid) that can serve as a forgery-resistant signature for model attribution.

## Executive Summary
This paper introduces a novel approach to identify the source of language model outputs by leveraging a geometric property: language model outputs lie on the surface of a high-dimensional ellipse (hyperellipsoid). The authors propose using this ellipse constraint as a "signature" for the model, which has unique properties compared to existing methods. The signature is naturally occurring (present in all modern language models), self-contained (verifiable without model inputs or full weights), compact and redundant (detectable in each logprob output), and forgery-resistant (hard to replicate without direct access to model parameters). The authors demonstrate the effectiveness of this method by showing that outputs from various models can be correctly identified by checking their distance to the model's ellipse. They also show that while it's possible to extract the ellipse from a model's outputs, it is computationally expensive and practically infeasible for large-scale models, making the signature forgery-resistant. Finally, the authors propose using this ellipse signature as a message authentication code for language model output verification, analogous to cryptographic systems.

## Method Summary
The method works by recognizing that modern language models with final normalization layers produce outputs constrained to lie on the surface of a d-dimensional ellipsoid in vocabulary space. This occurs because normalization maps hidden states to a unit sphere, which is then transformed by the unembedding matrix W into an ellipsoid. Verification involves applying an inverse affine transformation to centered logprob vectors and checking if the result has unit norm (indicating it lies on the model's ellipse). The forgery resistance comes from the computational difficulty of extracting ellipse parameters without direct model access - requiring O(d²) samples and O(d⁶) fitting time for large models.

## Key Results
- Language model outputs from different models can be correctly identified by checking their distance to each model's ellipse, with correct model showing 2-4 orders of magnitude smaller distance
- Extracting ellipse parameters from API-only access requires O(d²) samples and O(d⁶) time, making it practically infeasible for large models
- The ellipse signature serves as a compact, redundant, and forgery-resistant verification mechanism for language model outputs

## Why This Works (Mechanism)

### Mechanism 1: Ellipse Constraint Emergence from Final Layer Geometry
- Claim: Language model logit outputs naturally lie on a d-dimensional hyperellipsoid surface embedded in v-dimensional vocabulary space.
- Mechanism: Final normalization (RMS/Layer norm) maps hidden states onto unit sphere surface (‖x̂‖=1). The unembedding matrix W then applies an affine transformation W(γ⊙x̂+β), which stretches, rotates, and translates this sphere into an ellipsoid. The ellipsoid remains d-dimensional despite inhabiting ℝ^v.
- Core assumption: Models use final normalization followed by linear unembedding (applies to virtually all modern LLMs per authors).
- Evidence anchors:
  - [Section 2.1]: "Normalization has the property of mapping inputs onto the surface of a d-dimensional sphere... the logits lie on the surface of a d-dimensional ellipsoid, because W(γ⊙x̂+β) is an affine transformation."
  - [Abstract]: "language model outputs lie on the surface of a high-dimensional ellipse"
  - [corpus]: No direct corpus support for this specific geometric property; related work focuses on fingerprinting methods, not ellipse constraints.
- Break condition: Models without final normalization, or with ε-smoothing so severe that outputs fall in ellipsoid interior rather than surface.

### Mechanism 2: Signature Verification via Inverse Transform Distance
- Claim: A logprob vector can be attributed to its source model by measuring its distance to the model's ellipse after inverse affine transformation.
- Mechanism: Apply pseudo-inverse transform (W⁺C⁺Cℓ−β)/γ to centered logprobs. If from the model, result has unit norm. Deviation from 1 indicates distance from ellipse. Different models produce distinct ellipses with negligible intersection probability.
- Core assumption: Ellipses of different models rarely intersect in the relevant output regions.
- Evidence anchors:
  - [Section 2.2]: "If it is on the ellipse, it likely came from the model... the likelihood of an output falling on the intersection of two ellipses is extremely low."
  - [Figure 3]: Shows 2-4 orders of magnitude separation between correct model distance vs. other models.
  - [corpus]: Related fingerprinting work (arXiv:2510.06605) addresses black-box identification but uses different mechanisms.
- Break condition: Models with nearly identical final layer parameters (e.g., fine-tuned variants); deliberate parameter copying.

### Mechanism 3: Forgery Resistance via Super-Cubic Extraction Cost
- Claim: Producing novel logprobs on a model's ellipse without parameter access is practically infeasible for production-scale models.
- Mechanism: Ellipse extraction requires O(d²) output samples (quadric surface has d(d+3)/2 parameters), O(d³log d) API queries with vocabulary constraints, and O(d⁶) fitting time for ellipsoid-specific algorithms.
- Core assumption: Attacker has API access returning logprobs but not parameters; uses ellipsoid-specific fitting (SVD-based fails with ε-smoothing).
- Evidence anchors:
  - [Section 3.2]: "Recovering the ellipsoid of an LM requires O(d²) outputs... the cost of discovering the model ellipse grows at a rate of O(vd + d³log d)"
  - [Section 3.3]: "Both the SVD-based and ellipsoid-specific fitting methods typically require O(d⁶) time"
  - [Table 1]: GPT-3.5-turbo extraction cost ~$150K; Llama-3-70B ~$16M and 16,000+ years compute.
  - [corpus]: arXiv:2506.17047 addresses signature extraction attacks on neural networks but focuses on gradient-based methods, not geometric constraints.
- Break condition: Algorithmic breakthroughs reducing fitting complexity below O(d⁴); smaller models where extraction becomes tractable.

## Foundational Learning

- **Affine transformations and quadratic surfaces**
  - Why needed here: Understanding how normalization→linear creates ellipsoid constraint; ellipsoid is quadric with positive-definite quadratic form matrix.
  - Quick check question: Why does multiplying points on a sphere by a matrix produce an ellipsoid, not a sphere?

- **Softmax invariance and logprob centering**
  - Why needed here: APIs return logprobs, not logits; centering operation C log softmax(x) = Cx enables recovery of centered logits needed for ellipse fitting.
  - Quick check question: If softmax is translation-invariant, what information is lost when converting logits to logprobs?

- **Semidefinite programming for ellipsoid fitting**
  - Why needed here: Standard SVD fitting produces general quadrics (may be hyperboloids); ellipsoid-specific SDP constrains Q to be positive-definite.
  - Quick check question: Why would fitting any quadric surface fail when outputs are slightly inside the ellipsoid due to ε-smoothing?

## Architecture Onboarding

- **Component map**: Hidden state (ℝ^d) → [Normalization: sphere surface] → [Affine γ,β: scaled/shifted sphere] → [Linear W: ellipsoid in ℝ^v] → [Softmax: probabilities] → [Centering: recoverable centered logits]
- **Critical path**: Normalization layer (RMS/LayerNorm) is the constraint source. Without it, no ellipse signature exists. The unembedding matrix W determines ellipse shape/orientation—the "secret key."
- **Design tradeoffs**:
  - Larger ε in normalization → more smoothing → outputs inside ellipsoid → fitting degrades (mitigated by oversampling or larger models per Section G)
  - Revealing ellipse parameters (W, γ, β) to verifier enables signature checks but exposes model architecture details
  - Compact signature (single token) vs. robustness: signature is fragile to any output modification
- **Failure signatures**:
  - Extracted singular values systematically underestimated → ε-smoothing not accounted for
  - Fitting produces non-positive-definite E → using non-ellipsoid-specific algorithm on noisy data
  - Small model distances near noise floor → d too small, or model checkpoint very similar
  - Verification false positives → attacker stitched pre-collected logprobs (Section 4 discusses defenses)
- **First 3 experiments**:
  1. **Ellipse parameter extraction on small model**: Take 1M-param model with known parameters, collect ~10k logprob outputs, run Algorithm 1 with Ying et al. ellipsoid-specific fitting. Verify recovered (U, Σ, b) against true parameters. Expected MSE <0.01 on stretch/bias per Figure 5.
  2. **Cross-model verification separation**: Generate 1000 outputs each from 3+ open-weight models (OLMo-2, Llama-3.1, Qwen-3). For each output, compute distance to each model's ellipse. Confirm generating model has smallest distance by ≥2 orders of magnitude (replicate Figure 3 pattern).
  3. **Forgery attempt via partial extraction**: For small model (d≤256), attempt ellipse extraction with limited samples (n=d rather than n=d²). Measure degradation in ability to generate novel on-ellipse outputs. Compare against linear signature extraction feasibility from Finlayson et al. methodology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a computationally efficient algorithm exist for generating valid outputs on an unknown model ellipse using only sample data?
- Basis in paper: [explicit] The authors state on page 8, "We leave it as an open question as to whether there exists a fast algorithm for generating new outputs on an unknown model ellipse based on samples."
- Why unresolved: Current extraction methods face a time complexity of $O(d^6)$, rendering them computationally infeasible for production-scale models.
- What evidence would resolve it: The discovery of an algorithm with significantly lower time complexity that can successfully generate points satisfying the ellipse constraint without access to model weights.

### Open Question 2
- Question: Can other geometric constraints in language models be identified that offer stronger than polynomial security guarantees?
- Basis in paper: [explicit] The discussion notes that "the hardness of forging ellipses is only polynomial, far from a cryptographic security guarantee" and suggests future work could identify constraints with stronger guarantees.
- Why unresolved: The current forgery resistance relies on computational difficulty rather than unconditional impossibility, leaving a gap in theoretical security.
- What evidence would resolve it: The identification of a naturally occurring structural constraint in LLMs that requires exponential time to forge or extract.

### Open Question 3
- Question: Is it possible to construct a signature that is robust against removal via model modification?
- Basis in paper: [explicit] Page 9 states, "the ellipse signature does not have the desirable property of being difficult to remove, since modifying the model outputs or parameters erases the signature."
- Why unresolved: The current signature is intrinsic to the specific output layer weights; any fine-tuning or intentional perturbation breaks the verification capability.
- What evidence would resolve it: A method for embedding a signature that persists through parameter updates or output perturbations while remaining verifiable.

## Limitations

- Ellipse signature is fragile to model modifications - any changes to output layer parameters erase the signature
- Verification requires shared vocabulary tokens across models; alignment procedures for significantly different vocabularies are not fully specified
- Forgery resistance is based on polynomial computational hardness rather than cryptographic security guarantees

## Confidence

- **High Confidence**: The geometric mechanism (normalization → sphere → affine → ellipsoid) is mathematically sound and well-established. The inverse transform verification method is straightforward and correctly implemented.
- **Medium Confidence**: The forgery resistance claims are supported by complexity analysis and small-scale experiments, but large-scale extraction attempts on production models remain untested. The distance separation between models (Figure 3) is compelling but may not generalize across all model families.
- **Low Confidence**: The practical limits for models with d < 512, the exact impact of vocabulary alignment on verification accuracy, and the robustness to deliberate evasion attacks are not thoroughly explored.

## Next Checks

1. **Small Model Boundary Test**: Systematically evaluate ellipse extraction and verification accuracy on models with d = 128, 256, 512, 1024. Measure how ellipse fitting MSE and cross-model distance separation degrade as model dimension decreases, particularly focusing on the threshold where verification becomes unreliable.

2. **Vocabulary Alignment Robustness**: Design experiments where source and target models have intentionally misaligned vocabularies (10-90% overlap). Test whether distance-based verification still works after token projection, and quantify the minimum vocabulary overlap required for reliable attribution.

3. **Real-World Extraction Attempt**: Attempt to extract ellipse parameters from a production API (e.g., GPT-3.5-turbo or Claude) using the proposed O(d²) sampling strategy. Measure actual costs, fitting accuracy, and compare against theoretical estimates. Test whether the extracted ellipse can generate novel outputs that pass verification, establishing practical forgery resistance bounds.