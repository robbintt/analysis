---
ver: rpa2
title: 'APEX: Approximate-but-exhaustive search for ultra-large combinatorial synthesis
  libraries'
arxiv_id: '2510.24380'
source_url: https://arxiv.org/abs/2510.24380
tags:
- apex
- compounds
- library
- constraints
- screening
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces APEX, a neural-network-based method for exhaustive
  search of ultra-large combinatorial synthesis libraries. By factorizing a surrogate
  model, APEX enables full enumeration of compound properties on a GPU in under a
  minute, allowing exact top-k retrieval.
---

# APEX: Approximate-but-exhaustive search for ultra-large combinatorial synthesis libraries

## Quick Facts
- arXiv ID: 2510.24380
- Source URL: https://arxiv.org/abs/2510.24380
- Authors: Aryan Pedawi; Jordi Silvestre-Ryan; Bradley Worley; Darren J Hsu; Kushal S Shah; Elias Stehle; Jingrong Zhang; Izhar Wallach
- Reference count: 9
- One-line primary result: GPU-native exhaustive screening of 10B compounds in ~30 seconds with >90% recall for top-100k

## Executive Summary
APEX introduces a novel approach for exhaustive virtual screening of ultra-large combinatorial synthesis libraries (CSLs) by factorizing a surrogate model. The method enables screening of multi-billion compound libraries on a GPU in under a minute while maintaining high recall rates. By precomputing synthon-specific contributions and using a custom CUDA implementation with the AIR top-k algorithm, APEX achieves orders of magnitude speedup compared to traditional approaches while supporting constraint-aware multi-objective optimization.

## Method Summary
APEX trains a surrogate neural network on a 1M compound dataset to predict molecular properties, then factorizes this model to enable exhaustive enumeration. The factorization uses a hierarchical encoder to reconstruct embeddings from CSL structural components (reactions, R-groups, synthons) via a linear associative map. This allows each compound's predicted score to be computed as a sum of pre-calculated synthon contributions rather than full neural network evaluations. The GPU-native implementation uses a chain-of-batches strategy with the AIR top-k algorithm to efficiently screen multi-billion compound libraries while supporting constraint sets like Lipinski's Rule of Five.

## Key Results
- Achieves >90% recall for top-100k compounds across five drug targets on 12M compound library
- Screens 10B compound library in ~30 seconds on GPU versus 5+ minutes on CPU
- Outperforms Thompson sampling at low budgets in unconstrained case
- Maintains high performance under multiple constraint sets (Lipinski, Astex)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Factorizing the surrogate model enables exhaustive enumeration by decoupling expensive inference from search.
- **Mechanism:** The surrogate is trained with linear property heads, then a factorizer learns to reconstruct its embeddings from CSL structural components using key-value associative memory. This allows compound scores to be computed as simple sums of pre-calculated contributions.
- **Core assumption:** Surrogate embedding space can be approximated by linear associative map of CSL hierarchy.
- **Evidence anchors:** Abstract states factorizing enables enumeration in under a minute; Section 3.3 explains 30,000 neural network evaluations vs full enumeration.

### Mechanism 2
- **Claim:** Training surrogate with noise injection makes APEX predictions robust to factorizer reconstruction errors.
- **Mechanism:** Surrogate trained with noise on embeddings forces linear heads to handle perturbations. Factorizer reconstruction error treated as analogous noise, maintaining prediction stability.
- **Core assumption:** Factorizer reconstruction error distribution matches training noise.
- **Evidence anchors:** Section 3.1 describes noise injection during surrogate training; Section 3.3 explains robustness to errors-in-variables.

### Mechanism 3
- **Claim:** GPU-native chain-of-batches with AIR top-k enables screening 10B compounds in under a minute.
- **Mechanism:** CSL partitioned into batches processed in parallel on GPU. AIR top-k applied within batches, results iteratively merged with global top-k set to avoid full library sorting.
- **Core assumption:** Batching overhead significantly lower than global sort or CPU evaluation.
- **Evidence anchors:** Abstract mentions order-of-magnitude GPU speedup; Section 3.4 describes chain-of-batches strategy.

## Foundational Learning

- **Combinatorial Synthesis Libraries (CSLs):** Hierarchical organization of molecules by reactions, R-groups, and synthons. Essential for understanding how APEX factorizer works. *Quick check:* How is a molecule specified within a CSL hierarchy?
- **Surrogate Models:** Faster approximate models trained on expensive ground-truth data for screening. Core prerequisite for understanding APEX's approach. *Quick check:* What is the primary trade-off when using a surrogate model for screening?
- **Linear Associative Memory:** Key-value mechanism producing additive embeddings. Helps demystify the factorization step. *Quick check:* How does the APEX factorizer construct a molecular embedding from its components?

## Architecture Onboarding

- **Component map:** Surrogate (GNN/Transformer with linear heads) -> Factorizer (hierarchical encoders producing key/value pairs) -> APEX Searcher (CUDA C++ extension with AIR top-k)
- **Critical path:**
  1. Data Prep: Create CSL and generate 1M labeled training data
  2. Train Surrogate: Multi-task model predicting docking scores and properties
  3. Train Factorizer: Freeze surrogate, train to minimize embedding reconstruction loss
  4. Precompute Contributions: Calculate synthon associative contributions for all synthons
  5. Run Search: GPU-native search with user-specified objective and constraints
- **Design tradeoffs:**
  - Approximation vs. Speed: Trades exact evaluation for exhaustive surrogate-based evaluation
  - Precomputation Cost: One-time upfront cost enables subsequent sub-minute searches
  - Model Size: Search runtime independent of surrogate embedding dimension after precomputation
- **Failure signatures:**
  - Low Recall: Poor surrogate or factorizer training. Check regression metrics and reconstruction loss
  - Constraint Mismatch: Verify training of constraint heads and factorizer accuracy for those properties
  - CUDA OOM: Reduce batch size to lower memory footprint
- **First 3 experiments:**
  1. Surrogate Validation: Train on 1M labeled data, report regression metrics ($R^2$, RMSE)
  2. Factorizer Reconstruction: Measure MSE between true and factorized embeddings
  3. APEX Search Baseline: Run search on 12M library, compute recall against ground truth

## Open Questions the Paper Calls Out

- **Open Question 1:** Can APEX methodology adapt for non-linear property prediction heads without compromising factorization efficiency?
  - Basis: Method constrains surrogate to linear readouts for efficient factorization, but complex structure-activity relationships may require non-linear modeling
  - Unresolved: Paper doesn't explore factorization techniques for non-linear embedding transformations
  - Resolution: Modified APEX with kernel approximations or higher-order factorizations maintaining sub-linear enumeration

- **Open Question 2:** How does APEX compare to active search or reinforcement learning algorithms that natively support multi-objective constraints?
  - Basis: Comparison with Thompson Sampling limited to unconstrained case since TS implementation doesn't directly support constraints
  - Unresolved: Lack of comparison against other constraint-aware search algorithms
  - Resolution: Benchmark evaluation against constrained Bayesian optimization or genetic algorithms using same constraint sets

- **Open Question 3:** Does hierarchical factorization approach scale effectively to commercial libraries with more complex reaction topologies?
  - Basis: Evaluation on synthetic CSL constructed using BRICS fragmentation rather than real-world commercial library
  - Unresolved: Real commercial libraries may contain reactions with different topological structures
  - Resolution: Validation on full commercial library (e.g., Enamine REAL) to verify factorization efficiency

## Limitations

- Performance heavily dependent on surrogate model quality; errors propagate directly to final ranking
- Factorization introduces additional approximation error without full exploration of sensitivity to reconstruction quality
- Demonstrated primarily on CSLs derived from ZINC22; unclear performance on libraries with different hierarchical structures
- GPU-native implementation requires significant computational resources, limiting accessibility

## Confidence

- **High Confidence:** Core claim of screening multi-billion compounds in under a minute on GPU well-supported by runtime benchmarks and chain-of-batches strategy description
- **Medium Confidence:** High recall claim (>90% for top-100k) supported by 12M dataset results but extrapolated to 10B library; noise injection assumption reasonable but not directly validated
- **Low Confidence:** No detailed error analysis of factorizer reconstruction or thorough ablation study on noise distribution impact; single Thompson sampling comparison insufficient for broader claims

## Next Checks

1. Conduct detailed analysis of factorizer's embedding reconstruction error on held-out compounds, reporting MSE and cosine similarity, then correlate with final recall performance

2. Perform ablation study where surrogate is trained with different noise distributions (varying variance, different distributions) and measure impact on final recall performance

3. Test APEX on CSL with different hierarchical structure or generated from different reaction scheme (e.g., natural product-like scaffolds) and compare recall performance and runtime