---
ver: rpa2
title: Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization
arxiv_id: '2505.17852'
source_url: https://arxiv.org/abs/2505.17852
tags:
- bptt
- cd-rge
- step
- memory
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that Zero-Order Optimization (ZOO) methods,\
  \ specifically Central-Difference Random-Vector Gradient Estimation (CD-RGE), can\
  \ effectively replace Backpropagation Through Time (BPTT) for training large Recurrent\
  \ Neural Networks (RNNs). By leveraging distributed computing and recent optimizations\
  \ like FlashRNN, the authors show that CD-RGE can train RNNs with up to 1 billion\
  \ parameters while using significantly less memory than BPTT\u2014reducing VRAM\
  \ requirements from linear scaling with context length to a constant."
---

# Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization

## Quick Facts
- arXiv ID: 2505.17852
- Source URL: https://arxiv.org/abs/2505.17852
- Authors: Francois Chaubard; Mykel Kochenderfer
- Reference count: 14
- Primary result: CD-RGE achieves 19× faster convergence and up to 1B parameters while reducing VRAM usage to constant scaling

## Executive Summary
This paper presents a novel approach to training large Recurrent Neural Networks (RNNs) using Zero-Order Optimization (ZOO) methods, specifically Central-Difference Random-Vector Gradient Estimation (CD-RGE), as an alternative to Backpropagation Through Time (BPTT). The authors demonstrate that CD-RGE can effectively train RNNs with up to 1 billion parameters while significantly reducing memory requirements - from linear scaling with context length to constant scaling. By leveraging distributed computing and optimizations like FlashRNN, the method achieves comparable wall-clock training times to BPTT despite requiring more forward passes per step. The approach inherently regularizes training through a smoothed surrogate loss function, improving generalization performance across transduction and language modeling tasks.

## Method Summary
The paper proposes replacing traditional BPTT with Zero-Order Optimization using Central-Difference Random-Vector Gradient Estimation (CD-RGE) for training large RNNs. CD-RGE estimates gradients by perturbing parameters with random vectors and measuring the resulting change in loss, eliminating the need to store intermediate activations for backpropagation. The method is implemented in a distributed fashion using FlashRNN optimizations to handle the increased computational load from multiple forward passes. The key insight is that by optimizing a smoothed version of the original loss function, CD-RGE provides inherent regularization that improves generalization. The approach scales to extremely large models (up to 1 billion parameters) while maintaining constant memory usage regardless of sequence length, making it particularly suitable for long-sequence tasks.

## Key Results
- Achieves up to 19× faster convergence rates than BPTT in overfitting experiments
- Reduces VRAM requirements from linear scaling with context length to constant scaling
- Trains RNNs with up to 1 billion parameters while matching or exceeding BPTT performance
- Inherently regularizes training through smoothed surrogate loss, improving generalization

## Why This Works (Mechanism)
CD-RGE works by estimating gradients through finite differences rather than backpropagation. Instead of computing exact gradients via BPTT, it samples random perturbation vectors, evaluates the loss at perturbed parameter values, and uses central differences to estimate the gradient direction. This approach avoids storing intermediate activations, reducing memory from O(L) to O(1) where L is sequence length. The random perturbations create a smoothed loss landscape that acts as implicit regularization, helping prevent overfitting. Distributed implementation with FlashRNN optimizations compensates for the increased computational cost of multiple forward passes per update step.

## Foundational Learning
**Zero-Order Optimization**: Optimization methods that estimate gradients without explicit derivative computation, useful when gradients are expensive or unavailable
- Why needed: Provides gradient-free alternative to BPTT for large RNNs
- Quick check: Verify gradient estimates converge to true gradients as perturbation magnitude decreases

**Central-Difference Random-Vector Gradient Estimation**: Estimates gradients by perturbing parameters with random vectors and measuring loss changes
- Why needed: Enables memory-efficient gradient estimation for large models
- Quick check: Test sensitivity to finite difference step size and vector sampling strategy

**Backpropagation Through Time (BPTT)**: Standard method for training RNNs by unrolling computation graph through time
- Why needed: Provides baseline for comparing memory and performance characteristics
- Quick check: Verify BPTT implementation matches expected O(L) memory scaling

**FlashRNN**: Optimization technique for RNN training that reduces memory overhead
- Why needed: Enables efficient distributed implementation of CD-RGE
- Quick check: Confirm linear memory scaling with sequence length is maintained

## Architecture Onboarding

**Component Map**: Input Data -> RNN Model -> Loss Function -> CD-RGE Optimizer -> Parameter Updates -> RNN Model

**Critical Path**: The forward pass through the RNN, loss computation, random vector generation and perturbation, loss evaluation at perturbed parameters, gradient estimation, and parameter update

**Design Tradeoffs**: CD-RGE trades increased computational cost (more forward passes) for reduced memory usage and implicit regularization. The method requires careful tuning of perturbation magnitude and sampling strategy to balance gradient estimation accuracy with numerical stability.

**Failure Signatures**: Poor gradient estimates from inappropriate perturbation magnitudes, convergence issues from insufficient sampling, memory bottlenecks if distributed implementation isn't properly optimized, and degraded performance if the smoothing effect becomes too strong.

**First Experiments**:
1. Verify gradient estimation accuracy by comparing CD-RGE gradients to analytical gradients on small RNNs
2. Test memory scaling behavior by training on increasing sequence lengths and measuring VRAM usage
3. Benchmark convergence speed on synthetic overfitting task against BPTT baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Convergence improvements of 19× appear in controlled overfitting experiments rather than standard benchmarks, raising questions about generalization to production-scale training
- Memory savings claims assume FlashRNN's linear scaling holds perfectly, which may not account for practical implementation overhead in heterogeneous computing environments
- Reported wall-clock time parity with BPTT relies on idealized distributed setups that may not translate to commodity hardware clusters

## Confidence

**Memory efficiency claims**: Medium - Theoretical reductions are clear, but real-world overheads need verification
**Convergence rate improvements**: Medium - Significant in synthetic experiments, but benchmark validation needed
**Generalization benefits**: Low - Regularization effects observed but not extensively characterized

## Next Checks
1. Benchmark on standard language modeling datasets (e.g., WikiText-103) comparing both final performance and training dynamics
2. Profile memory usage and communication overhead in multi-node setups with heterogeneous hardware
3. Test sensitivity to hyperparameter choices, particularly finite difference step sizes and vector sampling strategies across different RNN architectures