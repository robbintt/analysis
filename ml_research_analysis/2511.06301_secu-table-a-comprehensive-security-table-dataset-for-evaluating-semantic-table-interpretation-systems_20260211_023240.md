---
ver: rpa2
title: 'Secu-Table: a Comprehensive security table dataset for evaluating semantic
  table interpretation systems'
arxiv_id: '2511.06301'
source_url: https://arxiv.org/abs/2511.06301
tags:
- data
- dataset
- should
- security
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Secu-Table, a security-focused tabular dataset
  for evaluating semantic table interpretation (STI) systems, especially those based
  on LLMs. The dataset contains over 1,500 tables with 150K+ entities extracted from
  CVE and CWE security data sources and annotated using Wikidata and the SEPSES CSKG
  knowledge graphs.
---

# Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems

## Quick Facts
- **arXiv ID:** 2511.06301
- **Source URL:** https://arxiv.org/abs/2511.06301
- **Reference count:** 40
- **Primary result:** Introduces Secu-Table, a security-focused dataset for evaluating semantic table interpretation systems with 1,500+ tables and 150K+ entities from CVE/CWE sources

## Executive Summary
This paper introduces Secu-Table, a security-focused tabular dataset for evaluating semantic table interpretation (STI) systems, especially those based on LLMs. The dataset contains over 1,500 tables with 150K+ entities extracted from CVE and CWE security data sources and annotated using Wikidata and the SEPSES CSKG knowledge graphs. It supports three core annotation tasks: cell entity annotation (CEA), column type annotation (CTA), and column property annotation (CPA). The dataset was manually curated with controlled error types to simulate real-world challenges. Preliminary evaluation using Falcon3-7b-instruct, Mistral-7B-Instruct, and GPT-4o mini achieved precision, recall, and F1-scores as baseline performance metrics, demonstrating the dataset's utility for benchmarking STI systems in the security domain.

## Method Summary
The method involves extracting tabular data from CVE and CWE security databases, then manually annotating entities using SPARQL queries against Wikidata and SEPSES CSKG knowledge graphs. The dataset supports three annotation tasks: CEA (linking cells to entities), CTA (linking columns to types), and CPA (linking columns to properties). To simulate real-world challenges, controlled error types were injected: 26% missing context, 26.6% misspelling errors, and 26.26% annotation errors. The dataset was evaluated using LLMs (Falcon3-7b-instruct, Mistral-7B-Instruct, GPT-4o mini) with 2-shot prompting, achieving baseline precision, recall, and F1-scores. The KG was not provided as context to test parametric knowledge.

## Key Results
- Secu-Table contains 1,554 tables with over 150K entities from CVE/CWE sources
- Dataset includes 76 ground truth tables and 1,478 test tables with controlled error injection
- LLMs achieved baseline performance metrics (precision, recall, F1-scores) across CEA, CTA, and CPA tasks
- Wikidata annotations showed significant sparsity compared to SEPSES CSKG, affecting CTA and CPA task reliability

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Knowledge Graph Alignment
Security concepts in CVE/CWE often have specific relational contexts not captured in general graphs. By mapping table cells to SEPSES CSKG (which integrates CAPEC, SCAP, etc.), the system leverages pre-existing security ontologies to resolve ambiguities that Wikidata might miss. The target KG must contain the specific security entities and relations present in the source tables; otherwise, recall drops due to incompleteness.

### Mechanism 2: Robustness via Controlled Noise Injection
By artificially degrading 80% of the data using Pandas, the dataset forces STI systems to rely on semantic reasoning and fuzzy matching rather than simple keyword lookup. The "real-world challenges" simulated (e.g., 26.6% misspelling rate) accurately approximate the noise distribution found in actual security logs or reports.

### Mechanism 3: Task Decomposition for Semantic Interpretation
Breaking STI into three distinct sub-tasks (CEA, CTA, CPA) allows for granular error analysis of LLM capabilities, assuming the model can distinguish between entity recognition and relationship extraction. CEA tests fine-grained entity resolution, CTA tests generalization/abstraction capabilities, and CPA tests relationship inference between columns.

## Foundational Learning

- **Concept:** **Semantic Table Interpretation (STI)**
  - **Why needed here:** This is the core problem space. You must understand that STI is not just "reading" a table, but mapping unstructured tabular content to a structured ontology (Knowledge Graph).
  - **Quick check question:** Can you explain the difference between linking a cell value to an Entity (CEA) versus linking a column header to a Type (CTA)?

- **Concept:** **Knowledge Graphs (Wikidata vs. SEPSES CSKG)**
  - **Why needed here:** The paper relies on two distinct KGs. Wikidata is general-purpose; SEPSES is domain-specific. Understanding the trade-off between broad coverage (Wikidata) and deep domain relations (SEPSES) is crucial for interpreting results.
  - **Quick check question:** Why might a "Buffer Overflow" concept have a richer representation in SEPSES CSKG than in Wikidata?

- **Concept:** **CVE and CWE Standards**
  - **Why needed here:** These are the data sources. CVEs are specific instances of vulnerabilities; CWEs are the classes of weaknesses. The relationship between them drives the CPA (relation) task.
  - **Quick check question:** If a table lists "CVE-2023-1234" and "CWE-89" in the same row, what semantic relation likely exists between these columns?

## Architecture Onboarding

- **Component map:** Source Data (CVE/CWE) -> Parser -> Tabular Format (CSV) -> Annotation Layer (manual + SPARQL) -> Ground Truth (CEA/CTA/CPA JSON/CSV) -> Noise Injection (Pandas) -> Test Set -> LLM (Falcon/Mistral/GPT) -> Predictions -> Evaluator (Precision/Recall/F1)

- **Critical path:** The most fragile step is the Annotation Layer. The paper notes that SPARQL queries on Wikidata returned too much noise, forcing a manual search (Section 2.4). Any automated pipeline must solve this disambiguation bottleneck.

- **Design tradeoffs:**
  - Manual vs. Auto Annotation: The authors chose manual annotation for high quality/low volume (initially), trading off scalability. Future versions propose "semi-automatic" approaches.
  - KG Completeness: Wikidata yielded many empty annotations (Section 2.4), so the dataset effectively tests an LLM's ability to handle "Unknown" or "Null" results.

- **Failure signatures:**
  - Ambiguity Overload: "Windows Server" matches hundreds of Wikidata entries (Q11246, Q11222, etc.). Without version context (e.g., "2008 R2"), the model will likely fail CEA.
  - Context Loss: If the "missing context" error injection removes the version column, resolving the specific "Windows Server" entity becomes impossible.

- **First 3 experiments:**
  1. Baseline Validation: Run the provided code on the 76 ground-truth tables using the specified hyperparameters (Temp=0.7, Top-p=0.95) to replicate the GPT-4o mini F1-scores.
  2. Error Sensitivity Analysis: Isolate the "misspelling" subset of tables. Compare the performance drop of Falcon3-7b versus GPT-4o mini to quantify tokenizer robustness.
  3. KG Ablation: Run CEA tasks using only Wikidata annotations vs. only SEPSES annotations to determine which KG provides better coverage for the CWE vs. CVE subsets.

## Open Questions the Paper Calls Out

### Open Question 1
How does the integration of heterogeneous security data sources (e.g., ATT&CK, CAPEC) in future dataset releases impact the performance disparity between open-source LLMs (like Falcon/Mistral) and closed-source models (like GPT-4o) on semantic table interpretation tasks? The authors explicitly state that future iterations will expand beyond CVE/CWE to include sources like ATT&CK, CCE, and OWASP to increase coverage.

### Open Question 2
Can LLM-based approaches effectively bridge the semantic gap to generate reliable Column Type Annotation (CTA) and Column Property Annotation (CPA) for security tables using Wikidata, despite the general knowledge graph's sparsity in this domain? The authors note that due to limited relevant annotations in Wikidata, "many empty annotation fields" resulted, making it difficult to generate reliable CTA and CPA.

### Open Question 3
Which specific controlled error types (missing context vs. misspellings vs. annotation errors) pose the greatest robustness challenge for current STI systems when processing security tabular data? The methodology section details the injection of specific error proportions, but the evaluation section provides aggregate baseline scores without analyzing the specific failure modes associated with each error type.

## Limitations
- KG-based annotation quality depends heavily on SPARQL query precision and KG completeness, with Wikidata yielding many empty annotations
- Error injection rates may not fully represent real-world noise distributions in security domains
- The 2-shot prompting approach and exact prompt templates are not fully specified in the paper text
- The dataset's scalability is limited by manual curation requirements

## Confidence

- **High Confidence:** Dataset construction methodology (data sources, annotation tasks definition, controlled noise injection procedure)
- **Medium Confidence:** Baseline LLM performance metrics (replication possible but prompt sensitivity not fully characterized)
- **Low Confidence:** Real-world applicability of noise distribution and error types (based on assumption rather than empirical validation)

## Next Checks

1. Replicate baseline evaluation using the exact prompt templates from the code repository to verify reported F1-scores for each LLM model
2. Conduct error sensitivity analysis by isolating specific error types (misspellings vs. missing context) and measuring performance degradation across models
3. Compare Wikidata vs. SEPSES annotation coverage on CVE/CWE entities to quantify the domain-specific knowledge advantage claimed in Mechanism 1