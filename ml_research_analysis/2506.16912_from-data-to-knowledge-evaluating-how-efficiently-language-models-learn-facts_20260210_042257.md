---
ver: rpa2
title: 'From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts'
arxiv_id: '2506.16912'
source_url: https://arxiv.org/abs/2506.16912
tags:
- frequency
- accuracy
- facts
- fact
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel framework for evaluating language
  model sample efficiency by analyzing how well models recall factual knowledge relative
  to the frequency of that knowledge in training data. The authors match subject-object
  pairs from the BEAR probe to sentences in a Wikipedia corpus to estimate fact frequencies,
  then propose two metrics: a weighted accuracy score that emphasizes low-frequency
  facts, and a power-law model that quantifies the probability of correct recall as
  a function of fact exposure.'
---

# From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts

## Quick Facts
- arXiv ID: 2506.16912
- Source URL: https://arxiv.org/abs/2506.16912
- Reference count: 40
- This paper introduces a framework for evaluating language model sample efficiency by analyzing factual knowledge recall relative to training data frequency.

## Executive Summary
This paper introduces a novel framework for evaluating language model sample efficiency by analyzing how well models recall factual knowledge relative to the frequency of that knowledge in training data. The authors match subject-object pairs from the BEAR probe to sentences in a Wikipedia corpus to estimate fact frequencies, then propose two metrics: a weighted accuracy score that emphasizes low-frequency facts, and a power-law model that quantifies the probability of correct recall as a function of fact exposure. Experiments with multiple architectures (GPT2, LLAMA, XLSTM, MAMBA) and sizes (200M vs 400M parameters) show that larger models and LLAMA in particular achieve significantly better performance on rare facts, with less improvement on high-frequency facts.

## Method Summary
The authors match subject-object pairs from the BEAR probe to sentences in a Wikipedia corpus to estimate fact frequencies, then propose two metrics: a weighted accuracy score that emphasizes low-frequency facts, and a power-law model that quantifies the probability of correct recall as a function of fact exposure. Experiments with multiple architectures (GPT2, LLAMA, XLSTM, MAMBA) and sizes (200M vs 400M parameters) show that larger models and LLAMA in particular achieve significantly better performance on rare facts, with less improvement on high-frequency facts. The alpha parameter from the probability model and the weighted accuracy metric both demonstrate robustness across different frequency distributions, providing strong indicators of sample efficiency.

## Key Results
- Larger models and LLAMA architecture achieve significantly better performance on rare facts, with less improvement on high-frequency facts
- The α parameter from the power-law probability model provides an architecture-specific sample efficiency measure robust to test distribution shifts
- Simple subject-object co-occurrence matching within sentences provides sufficient proxy signal for fact frequency estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model architectures differ primarily in their ability to learn low-frequency facts, while converging on high-frequency facts regardless of scale.
- Mechanism: Long-tailed information distributions create differential learning pressures—frequent facts receive sufficient exposure for all architectures to approach saturation, but rare facts expose architectural capacity for sample-efficient generalization. The weighted accuracy score (WASB) uses exponential decay weighting (λ=0.05) to emphasize low-frequency buckets, making this divergence measurable.
- Core assumption: Fact frequency in pre-training data is the primary determinant of recall probability, independent of fact complexity or relation type.
- Evidence anchors:
  - [abstract]: "most models perform similarly on high-frequency facts but differ notably on low-frequency facts"
  - [section 4.1.2, Table 2]: Small GPT2 achieves 83.4% on high-frequency facts (≥1024) while Medium GPT2 achieves 87.5%—minimal gap compared to low-frequency performance divergence (26.2% vs 28.6%)
  - [corpus]: Related work on knowledge propagation conflicts (arXiv:2601.15495) examines how models handle competing knowledge but does not directly validate frequency-based differentiation
- Break condition: If high-frequency fact accuracy shows high variance across architectures while low-frequency accuracy converges, the mechanism is falsified.

### Mechanism 2
- Claim: The α parameter from the power-law probability model provides an architecture-specific sample efficiency measure robust to test distribution shifts.
- Mechanism: The function F(x) = 1 - (L₀ + x₀)/((1+x)^α) models recall probability as a function of fact frequency x. Higher α indicates steeper probability increase per additional exposure—greater sample efficiency. L₀ and x₀ are dataset-dependent constants (fitted to 0.00 and 0.88), while αₘ varies by model.
- Core assumption: The power-law form captures the true underlying relationship between exposure count and recall probability.
- Evidence anchors:
  - [section 3.3.2]: "αₘ controls the slope of the probability function: Higher values increase the probability per additional occurrence, indicating higher sample efficiency"
  - [section 4.3.2, Figure 17]: α-values show lowest variation between low-frequency and high-frequency splits compared to raw accuracy or WASB
  - [corpus]: Godey et al. (2024) identify power-law relationships for geographic knowledge, providing partial external validation of scaling assumptions
- Break condition: If α values show high variance across differently sampled test distributions, or if the power-law fit is poor (low likelihood), the model is misspecified.

### Mechanism 3
- Claim: Simple subject-object co-occurrence matching within sentences provides sufficient proxy signal for fact frequency estimation.
- Mechanism: Given a fact triple (subject, relation, object), count sentences where both subject and object entities appear (with lemmatization and alias matching). This distant supervision approach assumes co-occurrence implies relation expression.
- Core assumption: Entity co-occurrence within a sentence correlates strongly with the specific relational fact being expressed; false positives from unrelated co-occurrences average out.
- Evidence anchors:
  - [section 3.1]: "For simplicity, we only check if two entities...occur within the same sentence...we assume the relational fact is represented within the sentence"
  - [section 4.1.2]: Optimized x₀ = 0.88 suggests non-zero baseline recall even for zero-frequency facts, indicating heuristic underestimation but not systematic failure
  - [corpus]: Mintz et al. (2009) distant supervision work validates co-occurrence assumptions for relation extraction, but precision tradeoffs are well-documented
- Break condition: If x₀ values approach zero consistently across datasets, the heuristic is severely undercounting; if x₀ varies wildly across domains, systematic bias exists.

## Foundational Learning

- Concept: **Long-tailed distributions and Zipf's law**
  - Why needed here: The entire framework assumes facts follow power-law frequency distributions—understanding why natural language exhibits this property is prerequisite to interpreting the results.
  - Quick check question: Can you explain why rare words/facts vastly outnumber common ones in natural corpora, and how this affects model evaluation?

- Concept: **Maximum likelihood estimation for parametric models**
  - Why needed here: The α parameter is derived by minimizing negative log-likelihood across all facts and models—understanding MLE is necessary to interpret what α actually optimizes.
  - Quick check question: Given observations of correct/incorrect predictions at various frequencies, how would you fit the parameters L₀, x₀, and α?

- Concept: **Knowledge probing via cloze-style evaluation**
  - Why needed here: The BEAR probe converts factual triples to multiple-choice statements and uses log-likelihood ranking—this differs from generation-based evaluation.
  - Quick check question: Why might a model correctly rank "Berlin" higher than "Paris" for "The capital of Germany is ___" while still being unable to generate "Berlin" autoregressively?

## Architecture Onboarding

- Component map:
  ```
  Corpus (Wikipedia) -> FactMatcherSimple -> Frequency Table
                                      ↓
  Pre-training Corpus -> Model Training -> Model Checkpoints
                                      ↓
  BEAR Probe + Frequency Table -> Probability Calculation -> αₘ estimation
                                -> Bucket Accuracy -> WASB calculation
  ```

- Critical path:
  1. Implement FactMatcherSimple with spaCy lemmatization (entity alias matching is the complexity bottleneck)
  2. Pre-train models with identical vocabularies and training hyperparameters (Section 4.1.1: batch_size=32, gradient_accum=8, lr=5e-4, cosine schedule)
  3. Save intermediate checkpoints for learning dynamics analysis
  4. Run BEAR probe on each checkpoint, join with frequency data
  5. Fit α via negative log-likelihood minimization; compute WASB with λ=0.05

- Design tradeoffs:
  - **Bucket granularity**: Coarser buckets smooth noise but lose resolution; paper uses 15 buckets but notes sensitivity when sample counts are low (Section 4.3.2)
  - **Frequency estimation precision vs. cost**: Entity linking would improve over simple matching but adds computational overhead—the paper argues simple matching suffices for relative comparisons
  - **Probe selection**: BEAR-big provides more facts but increases probing time ~24x (1 day per training run vs. substantially less with BEAR alone)

- Failure signatures:
  - **High variance in α across train/test splits**: Indicates distribution shift or probe unreliability—verify bucket sample counts are sufficient (>50 per bucket minimum)
  - **x₀ approaching 0.0 or >1.0**: Suggests frequency matching is severely biased; check entity alias coverage
  - **No differentiation between model sizes**: Verify training converged; check that high-frequency bucket accuracy saturates near 80-90% (Table 2 pattern)

- First 3 experiments:
  1. **Baseline replication**: Train GPT2-209M and GPT2-355M on identical Wikipedia slice, verify α increases with model size (target: 0.084 → 0.098 as in Table 1)
  2. **Frequency matching validation**: Manually annotate 100 random fact-sentence pairs from FactMatcherSimple output; measure precision of relation expression (flag if <70%)
  3. **Robustness stress test**: Create extreme frequency splits (90% low-freq vs. 90% high-freq); verify α correlation >0.9 across splits (Figure 17 benchmark)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the trend of increasing sample efficiency with model size hold for language models exceeding one billion parameters?
- Basis in paper: [explicit] The authors state in the Limitations section: "Whether the observed trend of increasing sample efficiency with model size holds for larger models exceeding one billion parameters remains open."
- Why unresolved: The study was resource-constrained, limiting the evaluation to small (approx. 200M) and medium (approx. 400M) parameter models.
- What evidence would resolve it: Pre-training and evaluating models with >1B parameters using the same framework, corpus, and metrics (WASB and $\alpha$) to see if the power-law scaling persists.

### Open Question 2
- Question: Can more advanced heuristics, such as natural language processing pipelines with entity linking, produce significantly more accurate fact occurrence counts than simple matching?
- Basis in paper: [explicit] The authors note: "more advanced heuristics, e.g., adding natural language processing pipelines such as entity linking, could produce more accurate fact occurrence counts."
- Why unresolved: The current study relied on a simple, flexible matching-based heuristic (`FactMatcherSimple`) that may fail to capture every occurrence or disambiguate entities effectively.
- What evidence would resolve it: Comparing frequency statistics generated by an entity-linking pipeline against the simple heuristic and analyzing the impact on the correlation between frequency and model recall.

### Open Question 3
- Question: How robust is the probability model's $L_0$ parameter and the resulting efficiency metrics to significant noise or errors in the BEAR probe or training corpus?
- Basis in paper: [explicit] The authors admit: "further research could be conducted on the robustness of the metric in those scenarios [where significant errors and noise alter outcomes]."
- Why unresolved: The value for $L_0$ (constant error rate) was validated empirically on the specific BEAR dataset used, but it is unclear if this value remains stable if the probe contains substantial errors.
- What evidence would resolve it: Introducing varying levels of synthetic noise into the probe or corpus and measuring the stability of the $L_0$ and $\alpha_m$ parameters.

## Limitations

- **Frequency estimation reliability**: The FactMatcherSimple approach may systematically underestimate rare fact frequencies when entities co-occur without explicitly expressing the relation.
- **Probe coverage and generalization**: The BEAR probe may skew toward certain relation types or entity patterns, affecting frequency matching and evaluation.
- **Architecture-specific confounds**: The study controls for model size but not all architectural differences, potentially introducing confounds beyond sample efficiency.

## Confidence

- **High confidence**: Claims about relative performance differences between architectures on low-frequency facts
- **Medium confidence**: Claims about absolute α values as sample efficiency measures
- **Low confidence**: Claims about the absolute frequency estimation accuracy

## Next Checks

1. **Manual fact-frequency validation**: Annotate 200 random fact-sentence pairs from FactMatcherSimple output to measure precision of relation expression. Target ≥70% precision to ensure frequency estimates are meaningful.

2. **Cross-probe consistency test**: Evaluate the same models on a second knowledge probe (e.g., LAMA or GoogleRE) and verify α correlation >0.8 between probes, confirming the metric's robustness to probe-specific biases.

3. **Zero-shot transfer validation**: Test models on facts from a held-out domain (e.g., scientific knowledge) to verify that α values generalize beyond Wikipedia-style encyclopedic knowledge and capture architecture-specific learning properties rather than domain-specific memorization.