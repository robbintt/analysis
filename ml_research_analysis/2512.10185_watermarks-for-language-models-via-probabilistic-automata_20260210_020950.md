---
ver: rpa2
title: Watermarks for Language Models via Probabilistic Automata
arxiv_id: '2512.10185'
source_url: https://arxiv.org/abs/2512.10185
tags:
- watermarking
- wepa
- text
- distribution
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new class of watermarking schemes for language
  models constructed through probabilistic automata. The proposed method, WEPA (Watermarking
  schemeE through Probabilistic Automata), addresses limitations in existing watermarking
  approaches, particularly their reduced generation diversity and high detection overhead.
---

# Watermarks for Language Models via Probabilistic Automata

## Quick Facts
- arXiv ID: 2512.10185
- Source URL: https://arxiv.org/abs/2512.10185
- Authors: Yangkun Wang; Jingbo Shang
- Reference count: 40
- Key outcome: Introduces WEPA watermarking scheme using probabilistic automata, improving generation diversity from Θ(λ) to Ω(λᵈ n) and reducing detection time complexity from Θ(λn k²) to Θ(λn)

## Executive Summary
This paper presents WEPA (Watermarking schemeE through Probabilistic Automata), a novel watermarking approach for language models that leverages probabilistic automata to overcome limitations of existing schemes. WEPA addresses the trade-off between generation diversity and detection efficiency by using a d-regular automaton topology that enables exponentially more diverse watermark paths while maintaining efficient detection. The scheme demonstrates superior performance in terms of robustness, efficiency, and generation diversity compared to state-of-the-art watermarking methods, validated through extensive experiments on LLaMA-3B and Mistral-7B models.

## Method Summary
WEPA constructs a watermarking scheme by generating a secret key that defines a d-regular probabilistic automaton with λ virtual states. At each decoding step, the decoder samples noise from the automaton and selects tokens using exponential minimum sampling, which preserves the original language model's token distribution. Detection computes the generalized Levenshtein distance between the generated text and the automaton, comparing against an empirical p-value distribution. The d-regular topology enables O(λn) detection time by constraining the dynamic programming search space, while exponentially increasing generation diversity to Ω(λᵈ n) compared to cyclic schemes.

## Key Results
- Improves generation diversity from Θ(λ) to Ω(λᵈ n) using d-regular automaton topology
- Reduces detection time complexity from Θ(λn k²) to Θ(λn)
- Achieves superior robustness and efficiency compared to existing watermarking methods
- Maintains perfect distortion-freeness by preserving the original language model's token distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing cyclic key sequences with a d-regular probabilistic automaton topology exponentially increases generation diversity while maintaining detectability
- Mechanism: The automaton has λ virtual states where each state q_i transitions to q_{i+1 mod λ}, ..., q_{i+d mod λ} with equal probability, forming a d-regular graph. This allows exponentially many distinct noise paths (Ω(λ^d · n)) compared to the cyclic scheme's linear paths (Θ(λ))
- Core assumption: The language model has sufficient entropy at each decoding step
- Evidence anchors: [abstract] "improves generation diversity from Θ(λ) to Ω(λᵈ n)", [section 4.2] Describes d-regular transition structure
- Break condition: If d=1 and bitwidth is large, the scheme degenerates to the baseline cyclic approach

### Mechanism 2
- Claim: Exponential minimum sampling with pre-sampled noise preserves the original language model's token distribution
- Mechanism: At each step, noise ξ_i = (μ_1, ..., μ_{|V|}) is sampled from the automaton, then the decoder selects y_i = argmin_j (π_j / log(μ_j)) where π_j is the LM's probability
- Core assumption: Noise samples are independent of the model's output distribution
- Evidence anchors: [section 4.2] Equation 8 defines exponential minimum sampling decoder, [section 5] Theorem 1 establishes equivalence
- Break condition: If bitwidth b is too small, discretization artifacts may introduce measurable distortion

### Mechanism 3
- Claim: The d-regular automaton structure enables O(λn) detection time by constraining the dynamic programming search space
- Mechanism: Detection computes generalized Levenshtein distance between text y and the automaton M. The d-regular property bounds each state's reachable predecessors to at most d states
- Core assumption: Insertion/deletion costs (γ_i, γ_d) are properly tuned
- Evidence anchors: [abstract] "reduces detection time complexity from Θ(λnk²) to Θ(λn)", [section 4.2] Definition 8 and Algorithm 2 describe the DP approach
- Break condition: If d approaches λ, the DP degrades toward O(λ²n)

## Foundational Learning

- Concept: Probabilistic Automata (PA) and the distinction between PDFA (deterministic) and PNFA (nondeterministic)
  - Why needed here: The paper frames watermarking as sampling from distributions recognized by PAs; PNFA's non-PAC-learnability enables undetectability
  - Quick check question: Given a 3-state PA with uniform transitions, can you trace 3 sampling paths?

- Concept: Exponential Minimum Sampling (inverse of Gumbel-max trick)
  - Why needed here: This decoder is the core mechanism ensuring distortion-freeness while using noise for watermarking
  - Quick check question: If π = [0.3, 0.7] and μ = [0.2, 0.5], which token is selected?

- Concept: Levenshtein/Edit Distance with asymmetric costs
  - Why needed here: Detection relies on computing edit distance between text and automaton language; asymmetric costs handle insertions/deletions differently
  - Quick check question: What is the Levenshtein distance between "abc" and "adc" with substitution cost 1?

## Architecture Onboarding

- Component map: Gen(1^λ) -> Φ_sk (noise generator) -> Γ (decoder) -> Detect_sk (Levenshtein distance computation)
- Critical path: Initialize PA with λ states in d-regular topology → At each token step: transition state → sample Φ_i → sample ξ_i → decode token → Detection: DP over all automaton states → compute d_L → p-value via N random key samples
- Design tradeoffs: Higher d provides more diversity but slightly weaker detection; higher bitwidth b improves detection but lowers diversity; improper γ_i tuning affects robustness
- Failure signatures: Detection p-value stuck near 0.5 indicates key mismatch; perplexity spike suggests low bitwidth; detection O(λ²n) suggests d parameter set too high
- First 3 experiments: 1) Reproduce Figure 2 (p-value vs. text length) with d=1 and d=2 on 20-token sequences; 2) Run ablation on bitwidth b (2, 4, 6, 8) with fixed text length 8; 3) Stress-test robustness with 20% random substitution attacks on 50-token sequences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tight analytical bounds for the p-value distribution be derived for WEPA to eliminate the reliance on computationally intensive empirical sampling?
- Basis in paper: [explicit] The authors state in Appendix A (Limitations) that "the lack of tight analytical bounds on the p-value remains a challenge"
- Why unresolved: The complexity of analyzing edit distance metrics over the proposed probabilistic automata currently prevents a closed-form solution
- What evidence would resolve it: A theorem providing a non-asymptotic, tight bound for the p-value based on the sequence length and automaton structure

### Open Question 2
- Question: Is it possible to construct an asymmetric decoding and detection system for WEPA that allows for public verification without revealing the secret key?
- Basis in paper: [explicit] Appendix A notes that "the detection algorithm requires knowledge of the private key"
- Why unresolved: The current detection algorithm requires the secret key to compute the Levenshtein distance against the automaton defined by that key
- What evidence would resolve it: A modified scheme utilizing public-key cryptography primitives where a public key suffices to compute the detection statistic

### Open Question 3
- Question: Can the framework be adapted to maintain robustness against semantic or paraphrasing attacks?
- Basis in paper: [inferred] In Section 6.2, the authors state the watermark "does not aim to defend against semantic or paraphrasing attacks"
- Why unresolved: The detection mechanism relies on generalized Levenshtein distance, which operates on token-level substitutions
- What evidence would resolve it: A theoretical extension of the cost function to include semantic similarity, or empirical results under paraphrasing attacks

## Limitations

- Detection mechanism's robustness to adaptive attacks is only partially addressed, focusing on edit-distance attacks rather than semantic attacks
- Experimental validation limited to two relatively small language models (LLaMA-3B and Mistral-7B), lacking scaling analysis to frontier models
- p-value threshold of 0.05 appears somewhat arbitrary and may not generalize across different application contexts

## Confidence

**High Confidence (8/10):**
- The d-regular automaton topology genuinely improves generation diversity from Θ(λ) to Ω(λᵈ n)
- The computational efficiency gains from Θ(λnk²) to Θ(λn) detection time are well-supported
- The distortion-freeness property holds under the stated assumptions

**Medium Confidence (6/10):**
- The undetectability claims under cryptographic assumptions may not translate to real-world adversarial settings
- The robustness claims against specific attack types may not generalize to more sophisticated attacks
- The bitwidth trade-off between detection strength and perplexity impact requires more extensive validation

**Low Confidence (4/10):**
- The practical limits of scaling to larger language models are not well-characterized
- The long-term stability of watermarks under continued fine-tuning is unaddressed
- The impact on downstream task performance beyond perplexity metrics is not explored

## Next Checks

1. **Cryptanalytic Vulnerability Assessment**: Conduct systematic evaluation of WEPA against adaptive watermark removal attacks, including gradient-based optimization and black-box inference attacks

2. **Large-Scale Model Compatibility**: Evaluate WEPA on frontier-scale models (1B+ parameters) to assess whether efficiency and diversity benefits scale linearly, specifically testing detection time and generation diversity

3. **Downstream Task Impact Analysis**: Beyond perplexity, measure WEPA's impact on fine-tuning stability, few-shot learning performance, and generation quality metrics across multiple benchmarks