---
ver: rpa2
title: Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based
  Class-Incremental Learning
arxiv_id: '2508.08165'
source_url: https://arxiv.org/abs/2508.08165
tags:
- uni00000013
- learning
- adapter
- uni00000055
- task-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in class-incremental
  learning (CIL) by proposing a method that leverages pre-trained models with adapter
  modules. Existing approaches suffer from incorrect adapter selection and overlook
  shared knowledge across tasks.
---

# Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning

## Quick Facts
- arXiv ID: 2508.08165
- Source URL: https://arxiv.org/abs/2508.08165
- Authors: Yan Wang; Da-Wei Zhou; Han-Jia Ye
- Reference count: 40
- Primary result: Proposes TUNA method achieving state-of-the-art performance on CIL benchmarks

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by proposing Task-Specific and Universal Adapters (TUNA), a method that leverages pre-trained models with adapter modules. The approach trains orthogonal task-specific adapters for each incremental task and fuses them into a universal adapter that captures shared knowledge across tasks. An entropy-based selection mechanism chooses the most suitable task-specific adapter during inference, with predictions from both task-specific and universal adapters being combined. The method operates exemplar-free, making it memory-efficient compared to traditional CIL methods.

## Method Summary
The TUNA method introduces a novel architecture for class-incremental learning that combines task-specific and universal adapters. For each new task, orthogonal task-specific adapters are trained while simultaneously updating a universal adapter that captures knowledge shared across all tasks. During inference, an entropy-based mechanism selects the most appropriate task-specific adapter, and the final prediction is obtained by combining outputs from both the selected task-specific adapter and the universal adapter. This dual-adapter approach addresses the limitations of existing methods that either suffer from incorrect adapter selection or fail to leverage shared knowledge across tasks.

## Key Results
- Achieves state-of-the-art performance on benchmark datasets
- Average accuracy of 79.42% and 73.78% on ImageNet-R and ImageNet-A respectively
- Operates exemplar-free, providing memory efficiency advantages over traditional CIL methods

## Why This Works (Mechanism)
The method works by maintaining both task-specific and universal adapters, allowing the model to leverage task-specific knowledge when available while still retaining general capabilities through the universal adapter. The entropy-based selection mechanism ensures that the most relevant task-specific adapter is chosen during inference, reducing the risk of incorrect predictions that can occur with fixed adapter selection strategies.

## Foundational Learning
- Class-Incremental Learning: Why needed - to handle new classes without forgetting old ones; Quick check - verify performance across task boundaries
- Catastrophic Forgetting: Why needed - to understand the core problem being solved; Quick check - measure forgetting between sequential tasks
- Adapter Modules: Why needed - to efficiently adapt pre-trained models without full fine-tuning; Quick check - compare adapter vs full fine-tuning performance
- Orthogonal Adapters: Why needed - to prevent interference between task-specific knowledge; Quick check - measure correlation between adapter weights
- Universal Adapters: Why needed - to capture shared knowledge across tasks; Quick check - evaluate performance on unseen tasks
- Entropy-based Selection: Why needed - to dynamically choose the most appropriate adapter; Quick check - validate selection accuracy on validation set

## Architecture Onboarding
- Component Map: Input -> Pre-trained Backbone -> Task-Specific Adapters + Universal Adapter -> Entropy Selection -> Combined Prediction
- Critical Path: Data flows through backbone to both adapter types, selection mechanism chooses which task-specific adapter to use, then combines with universal adapter output
- Design Tradeoffs: Task-specific adapters provide precision but risk forgetting; universal adapter provides stability but may lack specificity; entropy selection adds computational overhead but improves accuracy
- Failure Signatures: Incorrect adapter selection leading to poor performance; universal adapter becoming too generic; task-specific adapters becoming too specialized
- First Experiments: 1) Baseline comparison without universal adapter, 2) Comparison with random adapter selection vs entropy-based selection, 3) Performance analysis with varying numbers of tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns as maintaining orthogonal task-specific adapters becomes increasingly difficult with growing task counts
- Memory efficiency gains relative to other CIL methods are not quantitatively compared
- Performance on standard evaluation protocols is unclear, as significant improvements are reported primarily on adversarial or out-of-distribution datasets

## Confidence
- TUNA achieves state-of-the-art performance: High
- Entropy-based selection effectively chooses suitable adapters: Medium
- Universal adapter captures meaningful shared knowledge: Medium

## Next Checks
1. Evaluate TUNA's performance on standard, non-adversarial CIL benchmarks to establish baseline comparability and confirm robustness across different evaluation protocols
2. Conduct a detailed scalability analysis by incrementally increasing the number of tasks and measuring adapter management overhead, memory usage, and inference time to assess practical deployment limits
3. Perform ablation studies to isolate the contribution of the universal adapter and the entropy-based selection mechanism, quantifying their individual impact on overall performance and understanding their interplay