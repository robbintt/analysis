---
ver: rpa2
title: 'Using AI to replicate human experimental results: a motion study'
arxiv_id: '2507.10342'
source_url: https://arxiv.org/abs/2507.10342
tags:
- were
- verbs
- participants
- system
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) can replicate
  human judgments in psycholinguistic studies of motion verbs used in temporal expressions.
  The authors conducted four human-based experiments on affective meanings, valence
  shifts, verb choice, and emoji associations, then replicated the same tasks using
  ChatGPT o1.
---

# Using AI to replicate human experimental results: a motion study

## Quick Facts
- arXiv ID: 2507.10342
- Source URL: https://arxiv.org/abs/2507.10342
- Authors: Rosa Illan Castillo; Javier Valenzuela
- Reference count: 0
- Strong Spearman correlations (0.69-0.96) show LLMs reliably replicate human judgments on motion verb semantics

## Executive Summary
This study tests whether large language models can replicate human judgments in psycholinguistic experiments about motion verbs used in temporal expressions. The authors conducted four human experiments on affective meanings, valence shifts, verb choice, and emoji associations, then replicated the same tasks using ChatGPT o1. Results show strong convergence between human and AI responses across all tasks, with Spearman's rho correlations ranging from 0.69 to 0.96. The findings suggest LLMs can serve as reliable analytical tools in linguistic research, enabling larger-scale studies without compromising validity.

## Method Summary
The researchers conducted four psycholinguistic experiments with human participants, then replicated the exact same tasks using ChatGPT o1. Human experiments were administered through Qualtrics surveys with controlled stimuli (15-20 sentences each). The LLM was prompted with identical instructions to human participants, and for choice tasks, was asked to provide probability estimates. All stimuli were fully specified in appendices. Results were compared using Spearman's rank correlation, with additional analysis of divergent items and emoji interpretation challenges.

## Key Results
- Spearman's rho correlations ranged from 0.69 to 0.96 across four different psycholinguistic tasks
- LLM replicated human patterns of valence shifts: fast verbs increased positive valence, slow verbs decreased it in temporal contexts
- Task 3 (fill-in-the-blank) showed 100% convergence as it mirrors LLM's next-token prediction objective
- Emoji association task showed lower correlation (0.69) due to LLM misinterpreting two emojis

## Why This Works (Mechanism)

### Mechanism 1: Distributional Context Encoding
LLMs capture affective connotations through statistical co-occurrence patterns in training data. The model's vector representations encode contextual usage patterns that include emotional associations absent from dictionary definitions, enabling subjective judgments about verbs like "drag" evoking boredom. Core assumption: The training corpus contains sufficient examples of manner-of-motion verbs in both literal and temporal contexts to build reliable associations.

### Mechanism 2: Emergent Valence Polarization
LLMs replicate the pattern where verb speed determines valence shift direction in metaphorical contexts. Fast verbs (fly, zoom) increase positive valence in temporal settings; slow verbs (drag, creep) decrease valence—a pattern the model reproduces despite different absolute ratings. Core assumption: Valence shift is a learnable regularity from distributional patterns rather than requiring embodied simulation.

### Mechanism 3: Task-Training Alignment
Convergence quality depends on similarity between experimental task and LLM pretraining objectives. Fill-in-the-blank tasks (Study 3) achieve 100% convergence because they mirror next-token prediction; subjective rating tasks show slightly lower but still strong correlations. Core assumption: The prompt presented to the LLM must match the human task format exactly.

## Foundational Learning

- Concept: Spearman's rank correlation (ρ)
  - Why needed here: Primary metric for comparing human and AI judgment patterns across all four studies
  - Quick check question: If two raters give different absolute scores but rank items identically, what would ρ equal?

- Concept: Affective meaning / valence
  - Why needed here: Core construct being measured—emotional tone associated with motion verbs in temporal contexts
  - Quick check question: Why might "drag" receive a lower valence rating than "fly" in metaphorical but not necessarily literal contexts?

- Concept: Manner-of-motion verbs in temporal metaphor
  - Why needed here: Understanding how physical motion features (speed, difficulty) map to subjective time perception
  - Quick check question: What emotional quality does "the hours crawled by" convey versus "the hours flew by"?

## Architecture Onboarding

- Component map: Human Experiment Pipeline: Stimuli Design → Qualtrics Survey → Rating/Choice Collection → Statistical Analysis → LLM Replication Pipeline: Identical Stimuli → Prompt Engineering → Response Extraction → Probability Calibration → Statistical Comparison

- Critical path:
  1. Design stimuli with controlled literal/metaphorical conditions (matched word counts, similar structures)
  2. Present identical prompts to LLM as human participants
  3. For choice tasks, request probability estimates (not just selections) to capture response distributions
  4. Compute Spearman correlations and identify divergent items

- Design tradeoffs:
  - Exact prompt matching vs. optimized prompting: Authors chose exact matching for comparability
  - Single response vs. probability distribution: Probability elicitation showed humans were more categorical
  - One model vs. multi-model validation: Only ChatGPT o1 tested

- Failure signatures:
  - Verb "race" showed highest divergence across Studies 1 and 3 (noun/verb polysemy suspected)
  - Two emojis were misinterpreted by the model; excluding them improved correlation (ρ: 0.69 → 0.73)
  - LLM probability distributions were flatter than human categorical choices

- First 3 experiments:
  1. Replicate Study 2 (valence shifts) with additional motion verbs to test generalization beyond the 10 tested
  2. Test whether different LLMs (e.g., Claude, Gemini) show similar convergence patterns
  3. Scale stimuli from 15 to 200 sentences to validate whether conclusions remain stable at scale (as authors suggest)

## Open Questions the Paper Calls Out

### Open Question 1
Would expanding the stimulus set from 15 to 200 items reveal new emotional triggering mechanisms or simply confirm existing patterns? The authors ask, "what if instead of using 15 sentences as stimuli, we used 200?" and suggest this could uncover "new emotional triggerings" or increase the empirical base. This remains unresolved due to the current study's limited scale.

### Open Question 2
Does the noun form of polysemous words like "race" interfere with the vector representations of their verb form in LLMs? The authors speculate that the specific divergence found with the verb "race" might be because "information in the noun version of race... is tilting the results." This is unresolved because LLMs are "black boxes" and current "probing methods" to check vector information are immature.

### Open Question 3
Does the convergence of LLM and human judgments arise from statistical patterns in text or from language acting as a "mirror" of embodied reality? The authors describe this as a "classic chicken-and-egg problem," questioning if information comes from "statistical connections" or "interaction with the world." This remains unresolved as a fundamental theoretical debate regarding the nature of meaning in computational versus cognitive systems.

## Limitations
- Only one LLM (ChatGPT o1) was tested, limiting generalizability to other models
- Raw human response data per item was not fully shared, making exact replication difficult
- Two emojis were misinterpreted by the model, showing limitations in multimodal understanding

## Confidence
- Claim: LLM can reliably replicate human judgments on motion verb semantics
  - Evidence: Spearman correlations 0.69-0.96 across four tasks
  - Confidence: High
- Claim: Convergence depends on task similarity to LLM training objectives
  - Evidence: 100% convergence on fill-in-the-blank task, lower on subjective ratings
  - Confidence: Medium
- Claim: Polysemy causes specific divergences (e.g., "race")
  - Evidence: "race" showed highest divergence across studies
  - Confidence: Low (speculative)

## Next Checks
1. Replicate Study 2 with 20 additional motion verbs to test generalizability beyond the original 10
2. Test three different LLM models (ChatGPT o1, Claude, Gemini) on the same tasks to compare convergence patterns
3. Scale stimuli from 15 to 100+ sentences to validate whether convergence patterns remain stable at larger scale