---
ver: rpa2
title: Faster Predictive Coding Networks via Better Initialization
arxiv_id: '2601.20895'
source_url: https://arxiv.org/abs/2601.20895
tags:
- training
- initialization
- learning
- coding
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of predictive coding networks
  (PCNs) during training due to their iterative nature. The core contribution is a
  new initialization method called "stream-aligned average initialization" that preserves
  the iterative progress made on previous training samples.
---

# Faster Predictive Coding Networks via Better Initialization

## Quick Facts
- arXiv ID: 2601.20895
- Source URL: https://arxiv.org/abs/2601.20895
- Reference count: 40
- Primary result: Stream-aligned average initialization achieves up to 10% accuracy improvement and requires an order of magnitude fewer SMMs than I_fw on AlexNet.

## Executive Summary
This paper addresses the inefficiency of predictive coding networks (PCNs) during training due to their iterative nature. The authors propose two initialization methods: stream-aligned average initialization (PC-Iavg) for supervised tasks and memory-based initialization via Hopfield networks (PC-Imem) for unsupervised tasks. These methods preserve iterative progress from previous training samples, significantly improving convergence speed and final test loss compared to existing methods.

## Method Summary
The paper introduces two initialization strategies to accelerate PCN training. PC-Iavg aligns data samples within minibatches by class labels and averages neuron values across same-class samples to initialize subsequent batches, reducing the distance to the new convergence point. PC-Imem uses continuous Hopfield networks to approximate the conditional distribution of neurons given observations in unsupervised settings. A hybrid forward-average initialization combines forward initialization for early layers with average initialization for deeper layers to optimally balance information propagation with state preservation.

## Key Results
- PC-Iavg outperforms I_fw by up to 10% accuracy on complex architectures like AlexNet
- PC-Iavg requires up to an order of magnitude fewer SMMs than I_fw
- PC-Imem produces high-quality unsupervised reconstructions even at low inference step counts
- The hybrid initialization with m ≈ L/2+1 provides optimal results across tested architectures

## Why This Works (Mechanism)

### Mechanism 1: Stream-aligned Average Initialization (PC-Iavg)
This method preserves neuron states from previous batches and averages by class to reduce inference steps needed for convergence. It maintains class-conditional hidden state estimates by averaging converged neuron values from previous batch samples of the same class, approximating p_W(h^(T)|y). This works when consecutive minibatch samples from the same class have correlated convergence states and weight updates between batches are small enough that h^(T,b) remains a good approximation of h^(T,b+1).

### Mechanism 2: Memory-based Initialization via Hopfield Networks (PC-Imem)
This approach uses external associative memories to approximate the conditional distribution p(h^(T)|o) for unsupervised tasks. Each PCN layer is enhanced with a continuous Hopfield network that stores patterns mapping observations to converged hidden states. At initialization, the observation is queried against stored patterns to retrieve an approximate hidden state, avoiding dependency on class labels.

### Mechanism 3: Hybrid Forward-Average Initialization
This combines forward initialization for early layers with average initialization for deeper layers to optimally balance information propagation with state preservation. The first m ≈ L/2 + 1 layers use forward pass initialization to ensure input information reaches deeper layers, while remaining layers use class-averaged states. This addresses the limitation that I_avg alone may not propagate input-label relationships through the full network when T < L.

## Foundational Learning

- **Concept: Predictive Coding Energy Function** - Why needed: The entire approach optimizes initialization to reduce inference steps for energy minimization. Quick check: Can you explain why energy being concentrated in the output layer after forward initialization makes inference faster than random initialization?

- **Concept: Sequential Matrix Multiplications (SMMs) Metric** - Why needed: The paper uses SMMs as the primary efficiency metric, not wall-clock time. Quick check: If T = 5 and L = 7, how many SMMs does standard PC require versus BP?

- **Concept: Expectation-Maximization in PC** - Why needed: PC alternates between inference (E-step: update neurons) and learning (M-step: update weights). The proposed methods reduce E-step iterations by starting closer to the equilibrium. Quick check: Why does preserving h^(T) from batch b help the E-step converge faster in batch b+1?

## Architecture Onboarding

- **Component map**: PCN backbone (L+1 layers with L weight matrices) -> PC-Iavg (class-wise averaging buffers, minibatch sorter) -> PC-Imem (Hopfield layers with Q/K/V projections, stored pattern matrix) -> Inference loop (T iterations) -> Learning step (weight updates + memory updates)

- **Critical path**: 1) Receive minibatch → sort by class (PC-Iavg) or process observations (PC-Imem) 2) Initialize neurons via Eq. 7 or 8 3) Run T inference steps 4) Update weights; update memory parameters if PC-Imem 5) Store converged states for next batch initialization

- **Design tradeoffs**: PC-Iavg vs PC-Imem (labels required vs unsupervised, zero parameters vs Q/K/V parameters, O(n) vs O(attention) retrieval), T selection (lower T increases efficiency but risks under-convergence), memory capacity p_H (more patterns improve recall but increase storage and query cost)

- **Failure signatures**: Accuracy drops at very low T despite PC-Iavg (likely m misconfigured), PC-Imem reconstruction quality degrades over training (memory overwriting patterns), no speedup vs I_fw (batch size too small for meaningful class averages)

- **First 3 experiments**: 1) Replicate Fig. 2: Train 5-layer MLP on FashionMNIST with I_N, I_0, I_∅, I_fw, I_avg across T ∈ {5, 10, 25, 50, 100} 2) Ablate hybrid split point m: On MNIST/CIFAR10, sweep m ∈ {1, L/4, L/2, L/2+1, 3L/4} 3) PC-Imem unsupervised reconstruction: Train 4-layer decoder on FashionMNIST with I_mem vs I_0

## Open Questions the Paper Calls Out

- Can the proposed initialization strategies be generalized to other energy-based learning algorithms, specifically equilibrium propagation?
- How does the stream-aligned average initialization perform when applied to complex architectures such as Graph Neural Networks (GNNs) and Transformers?
- How can the label-dependent stream-aligned initialization be adapted for regression tasks where distinct class labels are unavailable?

## Limitations
- Effectiveness of PC-Iavg critically depends on small inter-batch weight updates and high class balance
- PC-Imem performance is bounded by Hopfield memory capacity (pH=24 patterns)
- The hybrid initialization heuristic (m ≈ L/2+1) is empirically derived but lacks theoretical justification

## Confidence
- Supervised accuracy gains of PC-Iavg over I_fw on AlexNet/CIFAR10: **High**
- Unsupervised reconstruction quality of PC-Imem: **Medium**
- Generalization of m ≈ L/2+1 rule across architectures: **Low-Medium**

## Next Checks
1. Test PC-Iavg under severe class imbalance (e.g., 10:1 ratio) to quantify degradation
2. Scale PC-Imem to CIFAR10 classification and measure memory capacity requirements
3. Perform architectural ablation on m by testing with ResNet blocks and DenseNet layers