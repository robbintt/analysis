---
ver: rpa2
title: 'SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework'
arxiv_id: '2509.05886'
source_url: https://arxiv.org/abs/2509.05886
tags:
- learning
- mape
- transfer
- heat
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a self-supervised physics-informed neural network
  (PINN) framework to predict the convective heat transfer coefficient of liquid sodium
  (Na) flow within rectangular miniature heat sinks. Initially, kernel-based machine
  learning techniques and shallow neural networks are applied to a dataset with 87
  Nusselt numbers for liquid sodium in rectangular miniature heat sinks.
---

# SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework

## Quick Facts
- arXiv ID: 2509.05886
- Source URL: https://arxiv.org/abs/2509.05886
- Reference count: 23
- Primary result: Self-supervised PINN achieves ~8% error margin for predicting liquid sodium heat transfer in miniature heat sinks

## Executive Summary
This paper introduces a self-supervised physics-informed neural network (PINN) framework for predicting convective heat transfer coefficients of liquid sodium flow in rectangular miniature heat sinks. The approach combines empirical data with underlying physics through a novel architecture that learns the optimal balance between data-driven and physics-based loss components. By incorporating a dedicated physics coefficient neuron and transfer learning from water-based models, the framework achieves robust predictions with approximately 8% error margin while ensuring physical consistency. The method demonstrates improved accuracy over traditional physics-based regression and standard machine learning approaches through its hybrid data-physics integration.

## Method Summary
The framework trains a PINN on 87 CFD-generated data points for liquid sodium flow, using 6 input features (Peclet number, hydraulic diameter, aspect ratio, length, width, L/D ratio) to predict average Nusselt number. The self-supervised PINN employs three fully-connected layers with Bayesian-optimized neuron counts (20-20-12), augmented by a parallel physics coefficient neuron with sigmoid activation that dynamically weights the contribution of physics-based loss. The combined loss function balances mean absolute percentage error (MAPE) from data with mean squared error (MSE) from physics equations. Transfer learning adapts a shallow neural network trained on water data to improve sodium predictions. Model validation uses 10-fold cross-validation with Monte Carlo evaluation (500 runs) to assess prediction variance and robustness.

## Key Results
- Self-supervised PINN achieves ~8% error margin for holdout data predictions
- Physics-only regression maintains 5-10% error range
- Monte Carlo validation shows Max(var(predictions)) < 0.12, indicating model stability
- Physics coefficient neuron centers around 0.5, demonstrating balanced data-physics integration
- Transfer learning from water models provides marginal improvement when applied to shallow layers

## Why This Works (Mechanism)
The self-supervised PINN framework succeeds by dynamically balancing empirical data with physical constraints through a learned weighting mechanism. The physics coefficient neuron adapts during training to find the optimal trade-off between data fidelity and physical law adherence, preventing overfitting to either component. This hybrid approach leverages the strengths of both data-driven learning (capturing complex nonlinear relationships) and physics-based modeling (ensuring physical plausibility), resulting in more robust predictions than either approach alone. The transfer learning component further enhances performance by initializing the model with relevant knowledge from similar fluids (water), reducing the learning burden for the target fluid (sodium).

## Foundational Learning

**Physics-informed neural networks** - Neural networks that incorporate physical laws as additional loss terms to ensure predictions satisfy governing equations. Needed because purely data-driven models may violate physical constraints; check by verifying physics residuals decrease during training.

**Transfer learning** - Technique where knowledge gained from solving one problem is applied to a different but related problem. Required to leverage existing models for similar fluids; verify by comparing performance with and without transfer initialization.

**Bayesian optimization** - Sequential optimization method for hyperparameter tuning that builds a probabilistic model of the objective function. Essential for efficient architecture search in small datasets; validate by tracking optimization convergence curves.

**Monte Carlo validation** - Statistical method using repeated random sampling to estimate model uncertainty and robustness. Critical for assessing prediction reliability on small datasets; confirm by checking variance distribution across validation runs.

## Architecture Onboarding

**Component map:** Input(6) -> FC(20,ReLU) -> FC(20,ReLU) -> FC(12,ReLU) -> Output(Nu_ave) + Physics coefficient neuron(sigmoid)

**Critical path:** The physics coefficient neuron is the critical innovation, as it dynamically balances data and physics loss components during training. This self-supervised mechanism determines the optimal weighting between empirical accuracy and physical consistency.

**Design tradeoffs:** The framework balances model complexity against dataset size (87 points), using Bayesian optimization to find optimal architecture while preventing overfitting. The physics coefficient neuron adds computational overhead but enables adaptive loss balancing. Transfer learning provides initialization benefits but requires careful layer selection.

**Failure signatures:** Overfitting manifests as large gaps between training and holdout MAPE; physics-data imbalance appears when the coefficient neuron converges to extreme values (near 0 or 1); transfer learning degradation occurs when deeper layers are transferred, increasing MAPE.

**First experiments:** 1) Train with only data loss to establish baseline performance; 2) Train with only physics loss to verify physical constraint adherence; 3) Compare physics coefficient neuron convergence with fixed weighting schemes.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset accessibility: 87 CFD data points not publicly available for independent validation
- Equation formatting: Equation 8 contains ambiguous γ-α relationship specification
- Transfer learning verification: Water model weights from reference [14] not provided for comparison
- Small dataset risk: 87 data points may lead to overfitting despite regularization techniques

## Confidence
High: Self-supervised PINN architecture and physics integration approach are standard implementations
Medium: Reported performance metrics may be affected by potential overfitting on small dataset
Low: Transfer learning claims cannot be independently verified without source model weights

## Next Checks
1. Request access to the 87 CFD data points or detailed boundary conditions for independent dataset generation
2. Clarify the exact form of Equation 8 and the γ-α relationship specification from authors
3. Verify the Monte Carlo variance calculation methodology by implementing the 500-evaluation validation procedure on a held-out test set