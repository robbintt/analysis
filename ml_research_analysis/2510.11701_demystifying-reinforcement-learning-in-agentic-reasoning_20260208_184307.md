---
ver: rpa2
title: Demystifying Reinforcement Learning in Agentic Reasoning
arxiv_id: '2510.11701'
source_url: https://arxiv.org/abs/2510.11701
tags:
- reasoning
- agentic
- training
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates reinforcement learning
  (RL) for agentic reasoning in LLMs across three dimensions: data, algorithm, and
  reasoning mode. Problem: Agentic RL struggles with suboptimal training dynamics,
  including inefficient exploration, reward/entropy collapse, and unstable behavior
  when integrating tool use into reasoning.'
---

# Demystifying Reinforcement Learning in Agentic Reasoning

## Quick Facts
- arXiv ID: 2510.11701
- Source URL: https://arxiv.org/abs/2510.11701
- Authors: Zhaochen Yu; Ling Yang; Jiaru Zou; Shuicheng Yan; Mengdi Wang
- Reference count: 24
- Key outcome: Real end-to-end trajectories improve AIME2025 performance from <10% to 45.72% for 4B models

## Executive Summary
This paper systematically investigates reinforcement learning (RL) for agentic reasoning in LLMs across three dimensions: data, algorithm, and reasoning mode. The authors conduct comprehensive empirical studies using GRPO-based RL, testing variants with different loss aggregation, clipping strategies, and reward shaping. They find that real end-to-end multi-turn trajectories significantly outperform synthetic stitch-style data, diverse datasets sustain higher policy entropy for faster convergence, and deliberative reasoning mode achieves higher tool-use efficiency than reactive mode. The resulting DemyAgent-4B model achieves state-of-the-art agentic reasoning performance, surpassing 32B models on challenging benchmarks.

## Method Summary
The method uses GRPO-TCR (Token-level loss + Clip higher + Overlong reward shaping) on a 4B model. SFT training uses 3k real end-to-end trajectories (filtered from 15k via ReasonFlux-PRM) for 5 epochs. RL training uses 30k diverse samples (17k DAPO-Math, ~5k Skywork-or1, 3k MegaScience) with model-aware filtering to remove 0%/100% accuracy problems. The RL configuration includes token-level loss, clip high (ε_high=0.315, ε_low=0.20), and overlong reward shaping with linear penalty near Lmax. Training runs for 450 steps on 8×A100-80G GPUs.

## Key Results
- Real end-to-end trajectories improve AIME2025 performance from <10% to 45.72% for 4B models
- Diverse datasets sustain higher policy entropy, accelerating convergence (50% accuracy in 150 steps vs 220 steps baseline)
- Deliberative reasoning mode achieves >70% tool-use success rate vs reactive mode
- DemyAgent-4B achieves SOTA agentic reasoning performance, surpassing 32B models on benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real end-to-end agentic trajectories provide stronger SFT initialization than synthetic stitch-style data, enabling models to learn when, why, and how to invoke tools—not just the syntax of tool calls.
- Mechanism: Synthetic data replaces reasoning segments with tool outputs, severing the natural connectivity between internal reasoning and tool invocation. Real trajectories preserve the full decision chain: pre-call problem localization, guarded execution with intermediate checks, error recovery after failed attempts, and post-call self-reflection. This allows the model to learn decision boundaries rather than surface patterns.
- Core assumption: The improved performance stems from better decision cues in real trajectories rather than distribution matching alone.
- Evidence anchors: [abstract] "Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization"; [section 3.1] Table 1 shows Qwen3-4B trained on real data achieves 45.82% maj@32 vs 0.10% with synthetic data on AIME2025.

### Mechanism 2
- Claim: Higher policy entropy during training correlates with faster convergence and better final performance, but requires controlled exploration bounds to prevent instability.
- Mechanism: Entropy serves as a proxy for exploration breadth. Diverse datasets introduce varying problem structures, forcing the policy to maintain multiple reasoning paths rather than collapsing to a narrow deterministic strategy. The clip upper bound (ε_high) controls the exploration budget: modest increases (0.28→0.315) accelerate learning, but excessive values (0.35) introduce instability and degrade performance.
- Core assumption: Entropy causally drives exploration rather than merely correlating with it.
- Evidence anchors: [section 4.3] Figure 6 shows ε_high=0.315 achieves equivalent performance 40% faster than ε_high=0.28; [section 3.2] Figure 2 shows diverse dataset sustains higher entropy and reaches 50% accuracy in 150 steps vs 220 steps baseline.

### Mechanism 3
- Claim: Deliberative reasoning mode (fewer, more targeted tool calls preceded by deeper internal reasoning) achieves higher tool-use efficiency and final accuracy than reactive mode (frequent short-think rounds with rapid tool invocation).
- Mechanism: Reactive mode produces many low-quality tool calls that often yield erroneous or ineffective results. Deliberative mode invests inference tokens in problem decomposition and strategy planning before invoking tools, resulting in fewer but more successful interactions. The quality-over-quantity principle emerges because the model learns to select tool-executable subproblems rather than reflexively calling tools.
- Core assumption: The correlation between deliberative mode and performance is causal, not confounded by model capability.
- Evidence anchors: [section 5.1] Figure 8 shows deliberative mode agents achieve >70% tool-use success rate vs substantially lower rates for reactive mode.

## Foundational Learning

- Concept: Policy entropy in RL
  - Why needed here: The paper uses entropy as a central signal for exploration and training efficiency.
  - Quick check question: If a policy has entropy approaching zero during training, what does this imply about its exploration behavior, and why might this be problematic for agentic RL?

- Concept: Loss aggregation granularity (token-level vs sequence-level)
  - Why needed here: The paper compares token-level and sequence-level loss, finding token-level superior for models with better initial exploration ability.
  - Quick check question: For a multi-turn agentic trajectory with one incorrect tool call near the end, how would token-level vs sequence-level loss distribute credit/blame across the sequence?

- Concept: Importance sampling ratio in PPO/GRPO
  - Why needed here: The clipping mechanism (ε_low, ε_high) operates on the importance ratio π_θ/π_ref.
  - Quick check question: If the clip upper bound ε_high is set too conservatively (e.g., 0.1), what constraint does this place on policy updates, and how does this affect exploration?

## Architecture Onboarding

- Component map:
  SFT dataset (3k real trajectories) → diverse RL dataset (30k samples) → model-aware filtering → GRPO-TCR training → deliberative inference mode

- Critical path:
  1. Curate or acquire real end-to-end agentic trajectories for SFT (not synthetic stitch-style)
  2. Initialize model with SFT checkpoint
  3. Build diverse RL dataset spanning multiple domains; apply model-aware filtering to remove 0%/100% accuracy problems
  4. Configure GRPO with token-level loss, clip-high, overlong reward shaping
  5. Monitor entropy trajectory: early rise followed by stable plateau indicates healthy exploration; early collapse indicates over-conservative clipping
  6. Track tool-use efficiency: >70% success rate indicates deliberative mode emergence

- Design tradeoffs:
  - Token-level vs sequence-level loss: Token-level provides finer-grained credit assignment but may be noisier for models with weak initial exploration.
  - Higher ε_high accelerates early progress but risks instability if set too aggressively.
  - Long-CoT initialization provides strong initial performance but ultimately underperforms instruction-based models due to conflicting optimization objectives.

- Failure signatures:
  - Entropy collapse early in training: indicates overly conservative clipping; increase ε_high
  - Tool-call avoidance: indicates model avoiding tools; ensure SFT initialization with real tool-use trajectories
  - Average reward stagnating near zero: indicates competence–difficulty mismatch; apply model-aware data filtering

- First 3 experiments:
  1. Ablate SFT data source: Train identical models on real end-to-end vs synthetic stitch-style trajectories; measure maj@k and pass@k on held-out benchmark
  2. Ablate ε_high values: Test [0.2, 0.28, 0.315, 0.35] on same model/dataset; plot entropy trajectories and convergence speed
  3. Measure tool-use efficiency: During training, log tool call success rates and correlate with reasoning mode

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the principled method for determining optimal clip upper bounds (ε_high) across different model scales and initial capabilities?
- Basis in paper: The paper shows weaker models require larger clip bounds while stronger models need tighter bounds, but only tests three specific values on 4B/7B models.
- Why unresolved: The relationship between model capacity, exploration needs, and optimal clipping strategy remains empirical.

### Open Question 2
- Question: Can small, high-quality agentic SFT datasets (e.g., 1k-2k samples) provide comparable initialization to large real-trajectory datasets?
- Basis in paper: Section 8.1: "it is still computationally costly to collect [end-to-end trajectories]... develop a recipe for how to curate small-sized high-quality SFT datasets."
- Why unresolved: The paper demonstrates 3k real trajectories outperform synthetic data dramatically, but resource constraints motivate finding minimal sufficient dataset sizes.

### Open Question 3
- Question: How should agentic reasoning frameworks be designed to prioritize high-level tool planning over verbose internal reasoning?
- Basis in paper: Section 8.2: "exploring agent-specific reasoning frameworks that prioritize high-level strategic planning and efficient tool orchestration."
- Why unresolved: The paper shows Long-CoT models avoid tool calls for reasoning tasks, and instruction-based models outperform them despite less internal reasoning capacity.

### Open Question 4
- Question: Do the insights from single-tool (code interpreter) settings generalize to multi-tool environments with optimizable tool selection?
- Basis in paper: Section 8.3: "we can also generalize our insights from a static and single tool environment to multi-tool and optimizable environment."
- Why unresolved: Multi-tool environments introduce combinatorial action spaces and tool-selection decisions not present in the paper's code-interpreter-only experiments.

## Limitations

- Data curation generalizability: The methodology for obtaining real end-to-end trajectories is not fully specified, and the 3k trajectory requirement may be prohibitive for novel domains.
- Optimal hyperparameter transferability: The recommended settings are presented as effective for 4B models on reasoning tasks but not systematically tested across model scales or task types.
- Causal attribution: The paper demonstrates correlation between deliberative reasoning mode and higher tool-use efficiency but doesn't establish whether this is a causal effect of training dynamics or an emergent property of stronger models.

## Confidence

**High confidence**: The comparative effectiveness of real end-to-end trajectories vs synthetic stitch-style data for SFT initialization.
**Medium confidence**: The relationship between entropy maintenance and training efficiency.
**Low confidence**: The universality of deliberative reasoning mode advantages across different task domains and cost structures.

## Next Checks

1. **Hybrid data ablation study**: Create synthetic trajectories that explicitly model decision points and compare against pure real trajectories and pure stitch-style data.
2. **Model size scaling experiment**: Apply the same GRPO-TCR configuration to 7B, 14B, and 32B models on identical datasets and plot entropy trajectories.
3. **Domain transfer validation**: Apply the full pipeline to a non-reasoning domain like customer service and measure whether deliberative mode provides similar advantages.