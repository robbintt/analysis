---
ver: rpa2
title: Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous
  Object Detection for Autonomous Driving Safety
arxiv_id: '2504.13399'
source_url: https://arxiv.org/abs/2504.13399
tags:
- hazard
- detection
- hazards
- driving
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety

## Quick Facts
- **arXiv ID:** 2504.13399
- **Source URL:** https://arxiv.org/abs/2504.13399
- **Reference count:** 23
- **Primary result:** Zero-shot hazard detection and captioning on COOOL benchmark (BESM=0.63, SAM=0.77).

## Executive Summary
This paper presents a multi-agent vision-language pipeline for zero-shot hazardous object detection in autonomous driving scenes. The system leverages a two-track approach combining Vision-Language Models (VLMs) for object and risk identification with Large Language Models (LLMs) for reasoning and intersection. By fusing ranked hazards with detected objects and using CLIP for localization, the pipeline aims to detect and caption novel out-of-label hazards without requiring retraining.

## Method Summary
The pipeline processes video clips in two parallel tracks. Track 1 uses a VLM to extract ranked hazard descriptions (RHS), while Track 2 uses a VLM queried 20 times to compile a comprehensive object list (AES). An LLM intersects these sets to produce a critical object set (COS), which is then localized using CLIP matching against provided bounding boxes. The system filters small objects (<175px) to maintain CLIP accuracy and outputs hazard captions evaluated via cosine similarity to human annotations.

## Key Results
- BESM score of 0.63 and SAM score of 0.77 on COOOL benchmark.
- Pipeline successfully detects and captions novel hazards without requiring training data.
- Performance is limited by small object detection and occasional "no output" failures.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parallel processing of risk reasoning and object detection improves hazard relevance by filtering false positives through intersection.
- **Mechanism:** The system separates scene analysis into Track 1 (Semantic Risk) and Track 2 (Object Presence). Track 1 uses a VLM to identify and rank potential hazards (RHS). Track 2 uses a VLM to list all visible objects (AES). An LLM intersects these sets to produce a "Critical Object Set" (COS), theoretically ensuring a hazard is only flagged if it is both risky *and* visually present.
- **Core assumption:** VLMs can independently generate comprehensive lists of objects and risks, and an LLM can successfully reconcile these two distinct linguistic representations without losing the hazard in the noise.
- **Evidence anchors:**
  - [abstract]: "Our pipeline consists of a Vision-Language Model (VLM), a Large Language Model (LLM)... We refine object detection by incorporating OpenAI's CLIP model to match predicted hazards..."
  - [section III.C]: "The final stage of our pipeline intersects these two parallel pathways... to refine hazard identification... filtering out hazards that were inferred but not explicitly detected."
  - [corpus]: Related work "Embodied Hazard Mitigation..." supports the general use of VLM/LLM for anomaly detection, but specific evidence for the RHS/AES intersection efficacy is limited to this paper's internal results.
- **Break condition:** If the VLM in Track 2 misses the hazard entirely (e.g., due to occlusion), it will be absent from the AES and removed during the intersection, causing a false negative.

### Mechanism 2
- **Claim:** Zero-shot localization is achieved by aligning LLM-verified hazard text with visual bounding box embeddings.
- **Mechanism:** Instead of training a detector, the system uses CLIP to match text labels from the "Anomalous Object Set" (AOS) against image snippets cropped from provided bounding boxes. This links the semantic reasoning of the LLM to spatial coordinates in the frame.
- **Core assumption:** The CLIP embedding space sufficiently aligns the specific hazard descriptions (e.g., "white van driving at high speed") with the visual features of the cropped image patches.
- **Evidence anchors:**
  - [abstract]: "...incorporating OpenAI's CLIP model to match predicted hazards with bounding box annotations, improving localization accuracy."
  - [section III.C]: "Each valid snippet is then compared against the descriptive labels in the COS... producing a 2D matrix... if an element falls within this top percentile, it is marked as detected."
  - [corpus]: "Language as Cost..." supports the use of VLM cost functions for hazard mapping, aligning with the use of semantic-visual similarity.
- **Break condition:** Fails when bounding boxes are too small (<175px width/height) or resolution is too low, as CLIP features become unreliable.

### Mechanism 3
- **Claim:** Iterative LLM prompting acts as a noise filter to consolidate noisy VLM outputs into a deterministic hazard list.
- **Mechanism:** Track 2 queries the VLM 20 times per video to ensure extraction of all objects. An LLM then consolidates these multiple lists into a single "best list" (AES) using a relevance prompt. This reduces the randomness (hallucination) inherent in single-pass generative extraction.
- **Core assumption:** The "temperature=0.2" setting and multiple iterations capture the ground truth objects, and the LLM is capable of discerning "relevant" traffic objects from irrelevant background noise.
- **Evidence anchors:**
  - [section III.B]: "queries the video 20 times... The low temperature ensures consistent and deterministic responses... These lists are then inputted into ChatGPT 4o-mini... to select the most relevant and comprehensive list."
  - [corpus]: (Weak direct evidence; general LLM aggregation is a known heuristic but not explicitly validated as a mechanism in the provided neighbor abstracts).
- **Break condition:** If the VLM consistently hallucinates an object across all 20 queries, the LLM consolidator will likely include it, creating a persistent false positive.

## Foundational Learning

- **Concept:** **Zero-Shot/Open-Set Detection**
  - **Why needed here:** The paper targets "Out-of-Label" hazards (debris, animals) not found in standard training sets (like COCO). Understanding how models generalize to unseen categories via semantic embedding spaces (CLIP) is essential.
  - **Quick check question:** Can you explain why cosine similarity in a shared embedding space allows a model to "detect" an object it was never explicitly trained to classify?

- **Concept:** **VLM vs. LLM Role Differentiation**
  - **Why needed here:** The architecture strictly separates visual perception (VLM: "What do I see?") from logical reasoning/ranking (LLM: "What is dangerous?"). Confusing these roles leads to poorly designed pipelines.
  - **Quick check question:** In this architecture, which model is responsible for *spatial* awareness (bounding boxes) and which is responsible for *contextual* threat ranking?

- **Concept:** **Noun Chunking & spaCy**
  - **Why needed here:** The pipeline extracts objects from VLM captions using noun chunking. This is the bridge between unstructured text and structured list data.
  - **Quick check question:** If a VLM outputs "A large truck is swerving dangerously," which specific component extracts the entity "truck" for the object set?

## Architecture Onboarding

- **Component map:** Input -> NAFNet (Denoising) -> Track 1: OmniVLM -> GPT-4o-mini (Ranking) -> RHS; Track 2: ViLA (20 queries) -> spaCy (Noun Chunking) -> GPT-4o-mini (Consolidation) -> AES; Fusion: LLM Intersection (RHS + AES) -> COS -> AOS; Localization: CLIP (AOS vs. Image Snippets) -> Final Detection.
- **Critical path:** The **Track 2 extraction -> Fusion** step. If the VLM fails to list the hazard in the "All Elements Set" (AES), the subsequent intersection logic (COS) will discard the hazard from Track 1, resulting in a missed detection (False Negative).
- **Design tradeoffs:**
  - **Recall vs. Precision:** The pipeline prioritizes precision by intersecting two tracks (RHS & AES) and filtering small boxes. This increases the risk of missing small but real hazards (as noted in the paper's results).
  - **Computational Cost:** Querying the VLM 20 times per video (Track 2) increases latency significantly compared to single-pass methods, trading speed for extraction robustness.
- **Failure signatures:**
  - **"Empty Set" Failure:** The pipeline outputs nothing. This occurs if the Intersection step yields an empty list (COS) or if CLIP similarity scores fall below the 10th percentile threshold.
  - **Small Object Drop:** Hazards with bounding boxes < 175px are automatically ignored to prevent noisy CLIP matches.
- **First 3 experiments:**
  1. **Baseline Reproduction:** Run the pipeline on a single video from the COOOLER dataset and manually inspect the RHS and AES lists before intersection to see if the hazard is present in both.
  2. **CLIP Threshold Sensitivity:** Vary the CLIP similarity threshold (currently top 10th percentile) and measure the change in BESM/SAM scores to see if strict filtering is hurting performance.
  3. **Ablation on Track 2:** Reduce the number of ViLA queries from 20 to 1 and observe the drop in the AES completeness to validate the necessity of the repetitive querying mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the pipeline be refined to detect small or occluded hazards that fall below the current pixel-size thresholds (e.g., < 175px width) without introducing excessive false positives?
- **Basis in paper:** [explicit] Section V.B notes "Object Size Limitations" as a cause for low scores, and Section VI.A explicitly calls for "refine bounding box processing... to pass through smaller objects."
- **Why unresolved:** The current implementation explicitly ignores snippets smaller than 175x175 pixels to ensure meaningful CLIP comparisons, causing the system to miss valid but distant or small hazards.
- **What evidence would resolve it:** Improved recall rates on small-object subsets of the COOOLER dataset, achieved by integrating super-resolution or scale-invariant detection methods without degrading precision.

### Open Question 2
- **Question:** Can temporal modeling approaches (such as Transformers or RNNs) be effectively integrated into this vision-language framework to predict driver reactions and track hazard evolution?
- **Basis in paper:** [explicit] Section VI.C states future work should "explore temporal modeling approaches such as transformers or recurrent neural networks" to address the Driver Reaction task and model cause-and-effect relationships.
- **Why unresolved:** The current methodology focuses on hazard captioning using static frame sampling ($N=25$), lacking the sequential reasoning required to determine *when* a driver reacts.
- **What evidence would resolve it:** A pipeline extension that successfully correlates hazard appearance with driver state changes in the COOOL benchmark, outperforming non-temporal baselines.

### Open Question 3
- **Question:** How can the cross-referencing logic between Track 1 (Ranked Hazards) and Track 2 (All Elements) be modified to prevent "no output" failures when a specific VLM or LLM agent fails to identify an object?
- **Basis in paper:** [explicit] Section V.B highlights "Failures in merging... outputs" leading to instances where the pipeline generates no output, and Section VI.A suggests refining "cross-referencing strategies."
- **Why unresolved:** The current intersection-based verification creates a single point of failure; if the object is missed in one track, it is excluded from the final Anomalous Object Set (AOS).
- **What evidence would resolve it:** A revised fusion mechanism (e.g., weighted voting or soft filtering) that maintains non-zero recall even when single-track detection confidence is low.

## Limitations
- **Dataset dependency:** Performance is evaluated only on the COOOL benchmark, which is a relatively small dataset (200 videos). The generalizability to diverse real-world autonomous driving scenarios with novel hazards remains unproven.
- **VLM hallucination risk:** The pipeline relies heavily on VLMs (OmniVLM, ViLA) to enumerate objects and hazards. If these models consistently hallucinate or miss critical hazards, the downstream LLM intersection and CLIP matching cannot recover these errors.
- **Metric alignment:** The use of cosine similarity for hazard description matching is standard but assumes the embedding space perfectly captures semantic similarity for safety-critical hazards. This assumption is not validated.

## Confidence
- **High confidence:** The architectural framework (two-track VLM-LLM pipeline with CLIP-based localization) is technically sound and well-specified.
- **Medium confidence:** The zero-shot mechanism via CLIP embedding alignment is plausible but its efficacy depends on the quality and diversity of the input bounding boxes and the VLM's ability to describe the hazard textually.
- **Low confidence:** The paper's claim of superior performance (BESM=0.63, SAM=0.77) is difficult to fully verify without access to the exact ground-truth descriptions and a direct comparison with established baselines on the same test set.

## Next Checks
1. **Ablation on VLM queries:** Reduce Track 2 from 20 ViLA queries to 5 and measure the degradation in AES completeness and final BESM/SAM scores to quantify the value of the repetitive querying mechanism.
2. **Cross-dataset validation:** Test the trained pipeline on a separate, held-out set of real-world dashcam videos (not from COOOL) with known hazards to assess zero-shot generalization.
3. **CLIP threshold analysis:** Systematically vary the CLIP similarity threshold (e.g., top 5th, 15th, 20th percentile) and plot the precision-recall curve to find the optimal tradeoff between missing hazards and including false positives.