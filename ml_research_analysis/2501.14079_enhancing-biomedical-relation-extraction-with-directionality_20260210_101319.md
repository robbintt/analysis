---
ver: rpa2
title: Enhancing Biomedical Relation Extraction with Directionality
arxiv_id: '2501.14079'
source_url: https://arxiv.org/abs/2501.14079
tags:
- relation
- biored
- entity
- biomedical
- directionality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extracting document-level
  biomedical relations, particularly focusing on directionality (subject/object roles)
  which is essential for understanding biological networks. The authors enriched the
  BioRED dataset with 10,864 directionality annotations and proposed a multi-task
  language model with soft-prompt learning to jointly identify relationships, novelty,
  and entity roles.
---

# Enhancing Biomedical Relation Extraction with Directionality

## Quick Facts
- arXiv ID: 2501.14079
- Source URL: https://arxiv.org/abs/2501.14079
- Reference count: 4
- Primary result: 48.62% F1 for relation type with directionality on BioRED

## Executive Summary
This paper tackles the challenge of extracting document-level biomedical relations with directionality, which is crucial for understanding biological networks. The authors enriched the BioRED dataset with 10,864 directionality annotations and proposed a multi-task language model with soft-prompt learning to jointly identify relationships, novelty, and entity roles. Their approach uses context chunking to handle long documents and soft prompts to improve performance. The model significantly outperforms state-of-the-art methods, including fine-tuned LLMs like GPT-4 and Llama-3.

## Method Summary
The method employs a multi-task learning framework using PubMedBERT or BioREx as backbone. The model processes long documents by splitting them into three overlapping chunks (prefix, infix, suffix) to handle BERT's 512-token limit. Three parallel classification heads predict relation type, novelty, and directionality simultaneously. Soft prompts (learnable embeddings) are prepended to inputs to capture task-specific patterns. The model is trained with weighted binary cross-entropy losses and evaluated on BioRED and BC5CDR datasets using F1-score metrics.

## Key Results
- Achieves 48.62% F1-score for relation type with directionality on BioRED dataset
- Outperforms GPT-4 and Llama-3 on directionality determination task
- Demonstrates 71.4% F1-score on BC5CDR dataset using context chunking strategy

## Why This Works (Mechanism)

### Mechanism 1: Soft Prompts
- Soft prompts (learnable embeddings) improve multi-task relation extraction by adapting task-specific signals during training
- Fixed-length virtual tokens are prepended to input and updated via backpropagation
- Optimal length appears to be 8 tokens, with performance degrading at 0 or longer lengths

### Mechanism 2: Context Chunking
- Long documents are split into three overlapping chunks to handle BERT's 512-token limit
- Prefix, suffix, and infix segments are encoded separately then max-pooled to aggregate signals
- Document-level reasoning is enabled by combining boundary and local context chunks

### Mechanism 3: Multi-task Joint Learning
- Shared PLM encoder feeds three parallel classification heads for relation type, novelty, and directionality
- Tasks share underlying linguistic patterns, reducing overfitting through joint representation learning
- Performance gains may come from both representation sharing and task regularization

## Foundational Learning

- **Soft vs. Hard Prompts**: Why needed - soft prompts are learnable embeddings while hard prompts are manual templates. Quick check - can you explain why a learned embedding might capture task structure better than a manually written template like "The relation between [E1] and [E2] is ___"?

- **BERT Token Limitation (512 tokens)**: Why needed - chunking exists specifically to handle documents longer than BERT's input limit. Quick check - if a document is 800 tokens and contains a relation between entities at positions 100 and 700, what information would a standard BERT model lose?

- **Multi-task Learning with Shared Encoder**: Why needed - the model jointly predicts relation type, novelty, and directionality. Quick check - if the DIR task has noisy labels, how might that affect REL and NOV performance during joint training?

## Architecture Onboarding

- **Component map**: Input Layer -> Chunking Module -> Encoder -> Hidden Token Pooler -> Classification Heads -> Loss Aggregation

- **Critical path**:
  1. Entity pair selection → boundary tag insertion
  2. Document tokenization + chunking (prefix/suffix/infix)
  3. Concatenation: [CLS] + hard prompt + chunk + [SEP] + soft prompt
  4. PLM encoding → hidden states per chunk
  5. Task-specific pooler → classification logits per chunk
  6. Max-pooling across chunks → final predictions

- **Design tradeoffs**:
  - Chunking strategy: Prefix/suffix captures document boundaries; infix captures local context. Tradeoff is computational cost (3 forward passes per pair).
  - Soft prompt length: 8 tokens optimal; longer prompts add parameters but may overfit.
  - Max-pooling vs. mean-pooling: Max-pooling preserves strongest signal per task but may ignore distributed evidence.

- **Failure signatures**:
  - Cotreatment relations: 48% FN rate due to training set imbalance (only 55 samples).
  - Cross-sentence relations: 74% of FN/FP in chemical-disease pairs are multi-sentence.
  - Ambiguous directionality: Gene-gene and chemical-gene pairs drop 11-15% F1 when directionality is required.
  - Infix-only instability: Variable window sizes hurt BC5CDR performance (61.2 F1 vs. 71.4 with all chunks).

- **First 3 experiments**:
  1. Reproduce baseline: Run BioREx or PubMedBERT on BioRED without chunking or soft prompts; verify reported F1 (~55.7 for BioREx RT).
  2. Ablate soft prompts: Compare model with soft prompt length 0 vs. 8 vs. 16; expect 8 to match paper's ~1-point F1 gain.
  3. Stress test chunking: Construct synthetic documents with relation evidence at extreme positions; verify prefix+suffix+infix combination recovers the signal.

## Open Questions the Paper Calls Out

- **How can external biomedical resources and Retrieval-Augmented Generation (RAG) be integrated to overcome current performance bottlenecks?**
  - Basis: Authors state they "look forward to exploring these resources further" and mention recent advancements in RAG research
  - Why unresolved: Current study focuses solely on model architecture without incorporating external knowledge bases or RAG techniques
  - Evidence needed: Study demonstrating improved F1-scores when RAG or external knowledge integration is applied to the proposed model

- **To what extent can targeted data augmentation mitigate high false negative rates in underrepresented relation types such as "Cotreatment"?**
  - Basis: Error analysis reveals limited training samples (55 pairs) for "Cotreatment" led to high false negatives
  - Why unresolved: Current experiments did not employ data augmentation; hypothesis remains untested
  - Evidence needed: Experimental results showing reduction in false negatives for "Cotreatment" following targeted data augmentation

- **How can models better resolve directionality in symmetric or ambiguous relations (e.g., "Bind", "Association") where explicit textual mechanisms are absent?**
  - Basis: Performance for gene-gene and chemical-gene directionality drops significantly due to inherently difficult "Association" and "Bind" types
  - Why unresolved: Paper identifies this ambiguity as primary source of error but offers no methodological solution
  - Evidence needed: New modeling approach or dataset enrichment that improves directionality F1-scores for "Association" types

## Limitations
- Directionality annotations cover only 500 BioRED abstracts, limiting coverage of rare relation types
- Performance drops significantly for gene-gene and chemical-gene pairs due to ambiguous "Association" and "Bind" relations
- Comparisons with GPT-4 and Llama-3 use static prompts without few-shot or chain-of-thought prompting

## Confidence
- Soft prompts improve directionality classification: **High**
- Context chunking enables document-level reasoning: **Medium**
- Multi-task joint learning outperforms single-task baselines: **Low**

## Next Checks
1. **Ablate task combinations**: Train single-task models (REL-only, DIR-only, NOV-only) and pairwise multi-task variants; compare to full joint model to quantify representation sharing vs. regularization effects.

2. **Tune chunking parameters**: Systematically vary prefix/suffix length and infix window size; evaluate impact on multi-sentence relation recall and document-level F1.

3. **Cross-corpus generalization**: Evaluate the full multi-task model on a non-biomedical relation extraction dataset (e.g., DocRED) to test if soft prompts and chunking transfer beyond biomedical domain.