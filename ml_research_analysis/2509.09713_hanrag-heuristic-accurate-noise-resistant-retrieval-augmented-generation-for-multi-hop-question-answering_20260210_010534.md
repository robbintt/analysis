---
ver: rpa2
title: 'HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation
  for Multi-hop Question Answering'
arxiv_id: '2509.09713'
source_url: https://arxiv.org/abs/2509.09713
tags:
- question
- query
- retrieval
- answer
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenges in Retrieval-Augmented Generation
  (RAG) for multi-hop question answering, specifically the over-reliance on iterative
  retrieval for compound queries, ineffective querying for complex queries, and noise
  accumulation during retrieval. The proposed HANRAG framework introduces a "Revelator"
  master agent that routes queries, decomposes compound problems, refines complex
  questions, and filters noise from retrieved documents.
---

# HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering

## Quick Facts
- arXiv ID: 2509.09713
- Source URL: https://arxiv.org/abs/2509.09713
- Authors: Duolin Sun; Dan Yang; Yue Shen; Yihan Jiao; Zhehao Tan; Jie Feng; Lianzhen Zhong; Jian Wang; Peng Wei; Jinjie Gu
- Reference count: 21
- Primary result: Outperforms existing methods on single-hop and multi-hop benchmarks with improvements in EM, F1, and accuracy while reducing retrieval steps

## Executive Summary
HANRAG addresses the challenges in Retrieval-Augmented Generation (RAG) for multi-hop question answering by introducing a "Revelator" master agent that routes queries, decomposes compound problems, refines complex questions, and filters noise from retrieved documents. The framework handles different query types with tailored strategies: straightforward questions are answered directly, single-step queries use efficient retrieval, compound queries leverage parallel retrieval, and complex queries use iterative refinement. Experiments show HANRAG achieves superior adaptability, noise resistance, and efficiency across diverse query types while outperforming existing methods on multiple benchmarks.

## Method Summary
HANRAG employs a fine-tuned LLM (Llama-3.1-8B-instruct) as the Revelator agent to classify incoming queries and route them through one of four processing pipelines. The Revelator is trained on synthetic data generated by Qwen2-72B-instruct for multi-task roles including routing, decomposition, refinement, and noise filtering. For compound queries, the Revelator decomposes them into independent sub-questions processed in parallel. For complex queries requiring sequential reasoning, it uses an iterative refinement loop with a seed question approach. The ANRAG (Accurate Noise-resistant RAG) pipeline combines standard BM25 retrieval with the Revelator's relevance discrimination to filter noisy documents before generation.

## Key Results
- Outperforms existing methods on single-hop benchmarks (SQuAD, NQ, TriviaQA) and multi-hop benchmarks (Musique, HotpotQA, 2WikiMultihopQA)
- Achieves improvements in Exact Match (EM), F1, and accuracy across diverse query types
- Reduces average retrieval steps compared to iterative approaches, particularly for compound queries
- Demonstrates superior noise resistance through effective document filtering
- Shows robust performance across straightforward, single-step, compound, and complex query types

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Query Routing via Heuristic Classification
The Revelator agent uses a trained classifier to route incoming queries to one of four processing pipelines (straightforward, single-step, compound, complex), optimizing for efficiency and accuracy. A fine-tuned LLM (Llama-3.1-8B-instruct) acts as the Revelator, trained on a dataset of queries labeled with their type (<Q, CLS> pairs). At inference, it outputs the query class, triggering a pre-defined execution path. For instance, "straightforward" queries go directly to the LLM; "compound" queries trigger parallel retrieval.

### Mechanism 2: Compound Query Decomposition for Parallel Retrieval
For multi-hop questions with independent sub-questions (compound queries), HANRAG decomposes them and executes retrievals in parallel (asynchronously), reducing total steps. The Revelator breaks a compound query into sub-queries (<Q, q1, q2,...>) and each sub-query is processed by the ANRAG module. The answers are then aggregated by the LLM. This contrasts with methods like IRCoT that use iterative (serial) retrieval for all multi-hop questions, often inefficiently.

### Mechanism 3: Iterative Seed-Question Refinement for Complex Reasoning
For multi-hop questions requiring sequential reasoning (complex queries), HANRAG uses an iterative loop of refining a "seed question," retrieving, and checking for sufficiency, which improves accuracy and controls steps. The Revelator extracts the next logical "seed question" from the original complex query and the history of previous answers. This seed question is answered via ANRAG. An Ending Discriminator evaluates if the accumulated information is sufficient to answer the original query. If not, the loop continues.

## Foundational Learning

- **Concept: Multi-hop Question Answering**
  - Why needed: This is the core problem domain. Understanding the difference between a single-hop fact lookup and a multi-hop query requiring synthesis across documents is essential to grasp HANRAG's purpose.
  - Quick check: Can a question like "Who was the U.S. President during the Louisiana Purchase?" be answered by retrieving a single fact, or does it require linking two pieces of information?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: HANRAG is a specific, advanced RAG architecture. You must understand the basic "retrieve-then-generate" loop before appreciating its adaptive routing and iterative extensions.
  - Quick check: In a basic RAG system, what are the two main components, and what is their sequence for a user query?

- **Concept: Noise in Retrieval Systems**
  - Why needed: A central contribution of HANRAG is its noise resistance via the Revelator's relevance discrimination. Understanding how irrelevant documents degrade LLM performance is key to valuing this feature.
  - Quick check: If a retriever returns 10 documents for a query, but only 2 are relevant, what potential problem does passing all 10 to the LLM cause?

## Architecture Onboarding

- **Component map:**
  - Revelator (Master Agent) -> Router/Decomposer/Refiner/Relevance Discriminator/Ending Discriminator
  - BM25 Retriever -> Document candidates
  - ANRAG pipeline -> Retriever -> Revelator (Relevance Discriminator) -> LLM Generator
  - LLM Generator -> Final answer synthesis

- **Critical path:**
  1. Input Query (Q) enters the system
  2. **Revelator (Router)** classifies Q's type
  3. Based on type, Q is routed to the corresponding pipeline:
     - **Straightforward:** Directly to LLM Generator
     - **Single-step:** To the ANRAG pipeline (retrieve -> filter -> generate)
     - **Compound:** **Revelator (Decomposer)** creates sub-queries; each goes through ANRAG in parallel; results are aggregated by LLM Generator
     - **Complex:** **Revelator (Refiner)** generates a seed question -> ANRAG -> **Revelator (Ending Discriminator)** checks sufficiency. Loop continues if needed. Final answer from LLM Generator

- **Design tradeoffs:**
  - **Efficiency vs. Accuracy:** The parallel retrieval for compound queries trades memory/compute parallelism for reduced latency (fewer sequential steps). The iterative loop for complex queries trades latency for improved accuracy via focused reasoning.
  - **Complexity vs. Generality:** The unified Revelator agent simplifies the architecture but requires sophisticated multi-task training data for its various roles.
  - **Retriever vs. Filter:** Using a standard retriever (BM25) places a heavier burden on the Revelator's relevance filter to clean up noisy results, as opposed to using a more complex, fine-tuned retriever.

- **Failure signatures:**
  - **Misrouting:** A complex query is routed as compound, leading to parallel retrieval of irrelevant context and a fragmented answer
  - **Loop Non-termination:** The Ending Discriminator fails to recognize that sufficient information has been gathered, causing the system to hit a maximum step limit
  - **Noise Accumulation:** The Relevance Discriminator fails to filter out a key piece of misinformation in an early step, which then propagates and invalidates all subsequent reasoning

- **First 3 experiments:**
  1. **Router Evaluation:** Isolate the Revelator's routing function. Test its classification accuracy on a held-out set of labeled queries (target ~84%)
  2. **ANRAG Noise Filter Ablation:** On a single-hop dataset, compare the performance of the full ANRAG pipeline (with relevance filtering) against the same pipeline without the filtering step
  3. **Compound vs. Iterative Retrieval Benchmark:** Construct a controlled set of compound queries and compare HANRAG's parallel retrieval against a baseline that uses standard iterative retrieval (like IRCoT)

## Open Questions the Paper Calls Out
None

## Limitations
- Dependence on Revelator's ability to accurately classify query types and execute corresponding strategies, introducing a single point of failure
- Assumes complex reasoning can be decomposed into a linear chain of sub-questions, which may not hold for all reasoning patterns
- Efficiency gains for compound queries rely on the assumption that sub-questions are independent, which may not always be true in practice

## Confidence
- **High confidence** in the framework's design principles and architectural approach
- **Medium confidence** in the noise-filtering effectiveness
- **Medium confidence** in the adaptive routing mechanism

## Next Checks
1. **Router Classification Analysis**: Conduct a detailed error analysis of the Revelator's query type classification, examining which query types are most frequently misclassified and their impact on downstream performance

2. **Noise Filter Ablation**: Isolate and test the Revelator's document relevance discrimination component on a controlled dataset with varying levels of retrieval noise to quantify its filtering effectiveness

3. **Loop Termination Analysis**: Systematically evaluate the Ending Discriminator's behavior across complex queries, measuring both premature termination rates and unnecessary additional steps to optimize the stopping criteria