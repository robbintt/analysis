---
ver: rpa2
title: Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition
arxiv_id: '2504.00194'
source_url: https://arxiv.org/abs/2504.00194
tags:
- they
- pact
- whale
- beetle
- lily
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Local Loss Landscape Decomposition (L3D),
  a method for identifying sparsely active circuits in neural networks by decomposing
  gradients of the loss between paired outputs. The method learns low-rank parameter
  directions (subnetworks) that reconstruct these gradients using a small subset of
  directions per sample.
---

# Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition

## Quick Facts
- arXiv ID: 2504.00194
- Source URL: https://arxiv.org/abs/2504.00194
- Reference count: 40
- One-line primary result: L3D identifies low-rank parameter directions (subnetworks) that sparsely reconstruct per-sample loss gradients, achieving 6.4% reconstruction loss on toy models and enabling interpretable interventions.

## Executive Summary
This paper introduces Local Loss Landscape Decomposition (L3D), a method for identifying sparsely active circuits in neural networks by decomposing gradients of the loss between paired outputs. The method learns low-rank parameter directions (subnetworks) that reconstruct these gradients using a small subset of directions per sample. On toy models with well-defined subnetworks, L3D achieved reconstruction losses as low as 6.4% and successfully recovered individual circuits. In real-world models, L3D identified interpretable subnetworks in a transformer and CNN, with reconstruction losses of 40% and 23% respectively. The results demonstrate L3D's potential for scaling beyond toy settings while preserving interpretability of sparsely active parameter circuits.

## Method Summary
L3D identifies sparsely active circuits by decomposing gradients of divergence between sample outputs into a sparse linear combination of learned low-rank parameter directions. For each sample paired with a reference output, the method computes the gradient of divergence at frozen parameters, transforms it through learnable low-rank subnetworks (with Tucker factorization), applies a batchTopK mask to enforce sparsity, and minimizes reconstruction loss. The approach assumes per-sample gradients lie in unions of low-dimensional subspaces and that models exhibit subset-specific degeneracies near their optima. After training, subnetworks can be used to intervene on model behavior by perturbing parameters along specific directions.

## Key Results
- Achieved 6.4% reconstruction loss on toy models with well-defined circuits
- Successfully recovered individual feature circuits in toy models, with cosine similarity r²≈0.92 with ground truth
- Identified interpretable subnetworks in real models: 40% reconstruction loss in transformer attention block and 23% in CNN conv block
- Demonstrated selective intervention capability in toy models, with perturbations primarily affecting relevant samples

## Why This Works (Mechanism)

### Mechanism 1: Gradient Decomposition into Sparse Subnetworks
- Claim: Per-sample loss gradients can be reconstructed by a sparse linear combination of learned low-rank parameter directions (subnetworks), enabling identification of sample-specific circuits in parameter space.
- Mechanism: For each sample `xi` paired with a reference output `yr`, L3D computes the gradient of divergence `∇W D`. Learnable transforms `Vin` (parameter→subnetwork) and `Vout` (subnetwork→parameter) convert gradients to/from a lower-dimensional subnetwork space. A batchTopK mask selects only the top-k fraction of subnetwork activations per batch; the reconstruction loss `L = ||∇W D − Vout Λ Vin ∇W D||2 / ||∇W D||2` is minimized, forcing sparse usage.
- Core assumption: Per-sample gradients lie in a union of low-dimensional subspaces; models have subset-specific degeneracies near their optima.
- Evidence anchors:
  - [abstract] "L3D identifies a set of low-rank subnetworks: directions in parameter space of which a subset can reconstruct the gradient of the loss between any sample's output and a reference output vector."
  - [section 2.3] "Each sample's gradient should be able to be expressed as a linear combination of a small set of these components."
  - [corpus] Corpus contains related parameter decomposition work (e.g., Attribution-based Parameter Decomposition) but no direct external validation of L3D's specific gradient-sparsity claims.
- Break condition: If gradients are dense/high-rank or subset-specific degeneracies do not exist, reconstruction loss remains high and identified directions lack interpretability.

### Mechanism 2: Intervention via Parameter Perturbation
- Claim: Perturbing parameters along a learned subnetwork direction selectively affects samples for which that subnetwork is active, enabling targeted behavioral modification.
- Mechanism: After training, subnetworks are columns of `Vout`. Intervention updates parameters as `W ← W + δ v_k`. Impact is measured via `I(xi, vk) = |Vin_k,: ∇w D|` averaged over multiple reference outputs. The sparsity enforced during training implies each direction should influence only a subset of samples.
- Core assumption: Local gradient structure approximates global loss landscape behavior; moving in subnetwork directions does not trigger large nonlinear cross-sample effects.
- Evidence anchors:
  - [abstract] "Additionally, we investigate the extent to which perturbing the model in the direction of a given subnetwork affects only the relevant subset of samples."
  - [section 2.5] "If we wish to 'intervene' on a model using a single subnetwork, we can update the model's parameters by moving them in their unit direction, multiplied by a scalar factor (δ)."
  - [section 3.1.3] "Perturbing in the direction of subnetwork 1 primarily affected samples where feature 1 was active..."
  - [corpus] No direct corpus validation of L3D intervention; related subnetwork/lottery ticket work does not address this specific mechanism.
- Break condition: If the loss landscape is highly non-convex or subnetworks have strong superposition, perturbations may cause unintended effects on irrelevant samples.

### Mechanism 3: Low-Rank Tucker Factorization for Tractability
- Claim: Enforcing a Tucker-based low-rank structure on the learnable transforms reduces memory/compute and reflects the hypothesis that individual circuits are low-rank.
- Mechanism: Instead of full-rank `Vin`/`Vout` (size `nv × nw`), each tensor component corresponding to model parameter groups is factorized via Tucker decomposition into a core tensor and factor matrices. This constrains subnetworks to low-rank parameter perturbations.
- Core assumption: Modular circuits are individually lower-rank than the full model; they can be compressed without losing functional specificity.
- Evidence anchors:
  - [section 2.3.1] "Learning a set of full rank parameter directions would be extremely expensive. We also expect that modular, sparsely active circuits would be lower rank than their full-model counterparts..."
  - [appendix B.1] "The Tucker decomposition decomposes a tensor... into a core tensor G and a set of factor matrices U(n)."
  - [corpus] Related work on circuit compression exists ("Circuits in superposition: Compressing many small neural circuits into one"), but no direct validation of the Tucker choice for L3D.
- Break condition: If true subnetworks are high-rank, the low-rank constraint will limit reconstruction quality and may miss or distort circuits.

## Foundational Learning

- Concept: **Loss Landscape Degeneracy (Singular Learning Theory basics)**
  - Why needed here: L3D relies on the idea that models have many near-equivalent parameter configurations and that subsets of data exhibit additional local degeneracies.
  - Quick check question: Can you explain why a wide loss basin might allow multiple parameter directions to achieve similar loss on a subset of data?

- Concept: **Sparse Dictionary Learning (SDL) analogy**
  - Why needed here: L3D's top-k activation and dictionary-like transforms mirror SDL, but operate on gradients in parameter space rather than activations.
  - Quick check question: How does enforcing sparsity in a learned basis encourage interpretability?

- Concept: **First-order Gradient Attribution**
  - Why needed here: L3D uses gradients of divergence between output pairs as the signal to decompose; understanding local attribution methods (e.g., steering vectors) clarifies why this might work.
  - Quick check question: Why might a local gradient between two outputs be a reasonable proxy for how to steer behavior?

## Architecture Onboarding

- Component map: Pre-trained model `f(X, W0)` → divergence computation → gradient `∇W D` → `Vin` transform → batchTopK mask → `Vout` reconstruction → normalized loss

- Critical path:
  1. Forward pass through `f` to get outputs for batch `X`
  2. For each `xi`, sample a reference `yr` from batch outputs
  3. Compute `∇W D(f(xi, W0), yr)` at fixed `W0`
  4. Apply `Vin` to get subnetwork activations; apply TopK mask
  5. Reconstruct gradient via `Vout`; compute normalized reconstruction loss
  6. Backprop through `Vin`/`Vout` only; normalize `Vout` columns after each step

- Design tradeoffs:
  - **Number of subnetworks `nv`**: More subnetworks can reduce reconstruction loss but increase compute and risk dead subnetworks
  - **TopK fraction**: Lower k enforces sparsity but may increase reconstruction error; should be set with expected circuit overlap in mind
  - **Rank of Tucker factors**: Lower rank reduces parameters but may underrepresent complex circuits
  - **Reference strategy**: Random pairing is less biased but higher variance than using a fixed mean output

- Failure signatures:
  - **High reconstruction loss** (>~50%) on toy models with known subnetworks suggests insufficient `nv`, rank, or suboptimal k
  - **Many dead subnetworks** (`Pact ≈ 0`) may indicate excessive `nv` or need for an auxiliary loss (as in SAEs)
  - **Intervention affects irrelevant samples** suggests subnetwork superposition or non-convexity in loss landscape
  - **Training instability** may result from learning rate too high or lack of `Vout` normalization

- First 3 experiments:
  1. **Toy Model of Superposition (TMS) replication**: Train L3D on the 5-feature, 2-hidden-dimension TMS model with `nv=5`, rank-1 subnetworks, k=0.1. Measure reconstruction loss and verify that subnetworks align with known encoder/decoder directions (compare via cosine similarity).
  2. **Ablation on reference strategy**: On TMCS, compare random-pair reference vs. mean-output reference vs. uniform reference in terms of convergence speed, reconstruction loss, and alignment of derived `Â` with true `A`.
  3. **Intervention scale-up test**: On the X→X² toy model, perturb each subnetwork direction with increasing `δ` and quantify the ratio of output change on relevant vs. irrelevant samples. Plot how selectivity degrades with larger `δ`.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do local gradient approximations fail for specific highly non-convex parameter directions in complex models?
- Basis in paper: [explicit] The authors ask, "do they work for all circuits?" and hypothesize that "parameter directions that are highly non-convex" may break the local attribution method.
- Why unresolved: The method relies on the assumption that local gradients approximate global relationships, which is tested on toy models but not fully verified for all circuits in large-scale models.
- What evidence would resolve it: A quantitative analysis of "dark matter" (unreconstructed error) relative to the non-convexity of specific parameter directions.

### Open Question 2
- Question: Does the sparsity constraint prevent L3D from identifying "scaffolding" circuits that are active for every sample?
- Basis in paper: [explicit] The authors ask, "If there is a circuit that is relevant to every output... should it be included in the decomposition?"
- Why unresolved: L3D is designed to find sparsely active subnetworks; ubiquitous circuits might violate the method's core assumption that circuits are sample-specific.
- What evidence would resolve it: Applying L3D to a synthetic model with a known dense circuit to determine if the method recovers it or incorrectly treats it as noise.

### Open Question 3
- Question: Can finetuning using L3D subnetworks remove undesired behaviors more effectively than activation-based steering vectors?
- Basis in paper: [explicit] The authors suggest "finetuning a model on a specific set of parameter directions... to benchmark the intervention capabilities of L3D versus other mechanistic intervention strategies."
- Why unresolved: The paper demonstrates intervention via small parameter perturbations but does not execute the proposed full finetuning pipeline to modify specific behaviors like sycophancy.
- What evidence would resolve it: A comparative study measuring the efficacy of L3D-based finetuning versus sparse dictionary learning steering vectors on behavioral benchmarks.

## Limitations

- Reconstruction losses on real models (40% and 23%) are significantly higher than toy models (6.4%), suggesting limited scalability
- The method's effectiveness for identifying truly sparse, functionally coherent circuits in complex models remains largely theoretical
- The connection between identified subnetworks and actual functional circuits is less clear in real models compared to toy settings

## Confidence

- **High Confidence**: Toy model experiments demonstrating reconstruction of known circuits are convincing with well-controlled conditions
- **Medium Confidence**: Real model results show lower reconstruction losses and some interpretability, but functional circuit identification is less established
- **Low Confidence**: Claims about method's ability to scale to larger, more complex models and identify truly sparse circuits remain largely theoretical

## Next Checks

1. **Circuit Completeness Test**: On the TMS model, systematically increase `n_v` and measure reconstruction loss until it plateaus. Verify that additional subnetworks capture residual variance rather than redundant information, and that cosine similarity with known feature directions remains high.

2. **Cross-Architecture Transfer**: Apply L3D to a second transformer attention block and a different CNN architecture. Compare reconstruction losses and intervention selectivity across architectures to test generalizability.

3. **Dynamic Circuit Analysis**: Implement a moving-window analysis where L3D is reapplied periodically during fine-tuning of a frozen model. Track how reconstruction loss and subnetwork activity patterns evolve to understand circuit stability and adaptation under task shifts.