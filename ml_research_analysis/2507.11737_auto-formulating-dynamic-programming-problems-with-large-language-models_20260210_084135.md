---
ver: rpa2
title: Auto-Formulating Dynamic Programming Problems with Large Language Models
arxiv_id: '2507.11737'
source_url: https://arxiv.org/abs/2507.11737
tags:
- problems
- problem
- generation
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPLM, a 7B-parameter language model fine-tuned
  for auto-formulating dynamic programming (DP) problems from natural language descriptions.
  To address the challenge of limited training data, the authors propose DualReflect,
  a synthetic data generation framework that combines forward generation (for diversity)
  and backward generation (for reliability).
---

# Auto-Formulating Dynamic Programming Problems with Large Language Models

## Quick Facts
- arXiv ID: 2507.11737
- Source URL: https://arxiv.org/abs/2507.11737
- Reference count: 38
- Key outcome: A 7B-parameter language model achieves 77.7% accuracy on DP problem auto-formulation, outperforming GPT-4o and rivaling much larger models.

## Executive Summary
This paper addresses the challenge of auto-formulating dynamic programming (DP) problems from natural language descriptions using a specialized 7B-parameter language model called DPLM. The authors tackle the data scarcity problem by developing DualReflect, a synthetic data generation framework that combines forward generation (for diversity) and backward generation (for reliability). The resulting model, trained on 113K synthetic examples, achieves 65.6% accuracy on easy DP problems and 38.1% on hard problems, demonstrating that domain-specific small models can effectively tackle specialized tasks in operations research.

## Method Summary
The approach uses a two-stage training pipeline on a Qwen-2.5-7B-Instruct base model: first, supervised fine-tuning (SFT) on 113K synthetic trajectories generated by GPT-4o through the DualReflect framework; second, reinforcement learning alignment via GRPO on a subset of verified hard problems. DualReflect combines forward generation (problem→solution via RAG with 5 role-based agents) and backward generation (perturb code→generate problem→verify via Reflected CoT with iterative correction), with backward generation preferred for reliability and forward generation for diversity at scale.

## Key Results
- DPLM achieves 77.7% accuracy on DP-Bench, outperforming its GPT-4o teacher (56.8%) and rivaling DeepSeek-R1 (70.5%)
- The two-stage SFT+RL pipeline improves accuracy by 20.9% over SFT alone
- Backward generation provides better performance at small data scales, while hybrid approaches become superior as dataset size increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage training pipeline combining SFT and RL is necessary to achieve high performance on DP problem auto-formulation.
- Mechanism: SFT on complex reasoning trajectories initializes the model's policy close to human instruction distribution and teaches specific output format, while RL optimizes a reward signal based on correctness of final numerical answer, allowing exploration beyond SFT demonstrations.
- Core assumption: The base model has sufficient capacity to learn the task, and synthetic data provides reliable reward signal for RL.
- Evidence anchors: Experiments show two-stage pipeline significantly improves performance; SFT is crucial preliminary step for establishing strong baseline prior to RL.

### Mechanism 2
- Claim: DualReflect's combination of forward and backward data generation is essential for balancing problem diversity and solution correctness.
- Mechanism: Backward generation starts from known correct solution to ensure correctness but limits diversity; forward generation generates problem first then attempts solution, allowing novel structures but risking incorrectness. Their complementary strengths become critical at different data scales.
- Core assumption: Seed set covers representative distribution of DP structures; GPT-4o can generate novel valid problem formulations in forward direction.
- Evidence anchors: Backward generation favored in low-data regimes for correctness guarantees, while forward generation becomes increasingly valuable at scale; hybrid approach outperforms both pure methods at larger scales.

### Mechanism 3
- Claim: Reflected CoT process enables recovery of flawed but solvable problems and generates rich reasoning trajectories for training.
- Mechanism: When generated problems yield incorrect solutions, the process uses known good solution as reference, prompts model to compare incorrect attempt with correct reference, identify discrepancies, and revise reasoning through iterative self-correction loop.
- Core assumption: Root cause of initial incorrect solution is flawed reasoning, not unsolvable problem; model can successfully perform self-correction given reference answer.
- Evidence anchors: Reflected CoT enables recovery of 19.1% of samples that would otherwise be discarded; enrichment introduces more challenging problems into dataset.

## Foundational Learning

- Concept: Dynamic Programming (DP) Fundamentals (State, Action, Transition, Reward)
  - Why needed here: To understand structure of problems model must learn to formulate; core task is translating natural language into formal model defined by these components.
  - Quick check question: Given a simple inventory problem, can you manually identify the state variable, action space, and transition probabilities?

- Concept: Supervised Fine-Tuning (SFT) vs. Reinforcement Learning (RL) in LLMs
  - Why needed here: To grasp distinct roles of each training stage; SFT provides foundational knowledge and format adherence while RL optimizes for final reward (correctness).
  - Quick check question: In this framework, what specific signal does RL stage use that SFT stage does not?

- Concept: Synthetic Data Generation (Forward vs. Backward)
  - Why needed here: To understand how training data was created and its properties; paper relies on novel mix of generation techniques to overcome data scarcity.
  - Quick check question: Why might model trained only on backward-generated data struggle to generalize to completely novel problem formulations?

## Architecture Onboarding

- Component map: Natural language problem description (P) -> Model (DPLM) -> Triplet of Chain-of-Thought (CoT), formal Model (M), and executable Code (C)
- Critical path: Success hinges on quality of synthetic data from DualReflect; if this data is flawed, neither SFT nor RL will succeed.
- Design tradeoffs:
  - Data Diversity vs. Correctness: Backward generation ensures correctness but limits diversity; forward generation offers diversity but risks incorrectness; system must balance these in training data mix.
  - RL Algorithm: GRPO chosen over PPO for lower memory usage and over DPO for better accuracy, despite being computationally more expensive (8x wall time).
  - Model Size vs. Compute: 7B parameter model chosen to make system more accessible, trading off potential raw power of larger models for lower computational requirements.
- Failure signatures:
  - Low SFT Accuracy: Likely indicates issues with quality of synthetic data or mismatch between base model's pre-training and DP domain.
  - RL Instability (Loss Spikes): Could be caused by overly high learning rate or noisy reward signal from synthetic dataset.
  - Reward Hacking in RL: Model might learn to generate syntactically valid but semantically incorrect code to achieve partial 0.2 reward for executability.
  - Degraded Performance on Easy Problems: After RL, performance might drop on easy problems if RL stage focuses too heavily on hard examples.
- First 3 experiments:
  1. Reproduce SFT Baseline: Perform SFT on small subset (10K samples) of generated DualReflect data and evaluate on full DP-Bench.
  2. Ablate Data Generation Methods: Train two separate models using only forward-generated data and only backward-generated data; compare performance curves as dataset size scales.
  3. Validate RL Reward Shaping: Implement GRPO training loop but ablate partial reward for executable code; compare training stability and final accuracy to full reward setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between forward and backward generation at different data scales for DP problem synthesis?
- Basis in paper: Authors state backward generation favored in low-data regimes for correctness guarantees while forward generation becomes increasingly valuable at scale, showing hybrid approach outperforms both pure methods at larger scales.
- Why unresolved: Paper tests 50/50 hybrid mixes but does not systematically explore optimal ratios as function of dataset size or problem complexity.
- Evidence to resolve: Grid search over generation ratios (20/80 to 80/20) at multiple data scales (1K, 8K, 32K, 100K+), reporting accuracy on held-out test sets.

### Open Question 2
- Question: Can computational cost of GRPO-based RL alignment be reduced while maintaining accuracy advantage over DPO?
- Basis in paper: GRPO consistently outperforms DPO in accuracy but requires approximately eight times more wall time on identical hardware.
- Why unresolved: Paper identifies trade-off but does not propose or evaluate efficiency improvements for GRPO in this domain.
- Evidence to resolve: Develop and benchmark GRPO variants (reduced rollout groups, adaptive sampling) against DPO on DP-Bench, measuring both accuracy and wall-clock time.

### Open Question 3
- Question: To what extent does expanding seed problem set improve coverage of specialized domain knowledge and performance on easy problems?
- Basis in paper: Shortfall likely comes from limited coverage of specialized domain knowledge within current dataset; expanding initial seed data and scaling synthetic data generation seems promising to overcome this limitation.
- Why unresolved: Current seed set contains only 91 problems; relationship between seed diversity and model generalization on easier, knowledge-heavy problems remains unquantified.
- Evidence to resolve: Train DPLM variants with progressively larger and more diverse seed sets (200, 500, 1000 problems) and evaluate performance changes specifically on easy subset of DP-Bench.

## Limitations

- Reliance on synthetic data generation constrains quality and diversity of training examples to capabilities of GPT-4o
- Evaluation on DP-Bench covers only 132 problems across six categories, limiting generalizability to real-world DP applications
- Reflected CoT mechanism assumes base model can successfully self-correct when given reference solution, which may not generalize to more complex reasoning errors

## Confidence

**High Confidence**: The two-stage training pipeline (SFT followed by RL) significantly improves performance over SFT alone, as evidenced by the 20.9% accuracy jump from 56.8% to 77.7%.

**Medium Confidence**: The complementary roles of forward and backward generation in DualReflect are supported by scaling experiments, but analysis relies on synthetic performance trends rather than direct comparison on human-written problems.

**Medium Confidence**: The claim that DPLM rivals much larger models like DeepSeek-R1 requires careful interpretation, as comparison is on specific benchmark rather than general reasoning capabilities.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate DPLM on held-out set of DP problems from textbooks not used in DualReflect generation, comparing performance degradation against GPT-4o and DeepSeek-R1 to assess true generalization beyond synthetic data.

2. **Human Evaluation of Synthetic Data Quality**: Have domain experts review 100 randomly sampled DualReflect-generated problems and solutions, rating them on correctness, realism, and complexity to validate quality assumptions underlying training data.

3. **Ablation of Reflected CoT Mechanism**: Train two models: one using standard backward generation without reflection step, and one using only forward generation with majority voting; compare their performance curves and sample quality to isolate contribution of iterative refinement process.