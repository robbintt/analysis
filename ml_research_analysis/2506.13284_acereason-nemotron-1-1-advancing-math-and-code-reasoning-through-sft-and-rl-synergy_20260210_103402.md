---
ver: rpa2
title: 'AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and
  RL Synergy'
arxiv_id: '2506.13284'
source_url: https://arxiv.org/abs/2506.13284
tags:
- training
- performance
- math
- code
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically investigates the interplay between supervised
  fine-tuning (SFT) and reinforcement learning (RL) in building advanced reasoning
  models. It explores two SFT scaling strategies: increasing the number of prompts
  and increasing the number of responses per prompt.'
---

# AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy

## Quick Facts
- arXiv ID: 2506.13284
- Source URL: https://arxiv.org/abs/2506.13284
- Authors: Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
- Reference count: 40
- Primary result: State-of-the-art reasoning performance among Qwen2.5-7B-based models (AIME24: 72.6%, AIME25: 64.8%, LiveCodeBench >52%)

## Executive Summary
This paper systematically investigates the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) for building advanced math and code reasoning models. Through extensive scaling experiments, the authors find that increasing the number of unique prompts yields greater performance gains than increasing responses per prompt in SFT. The study reveals that RL training, while initially degrading performance, is crucial for compressing verbose reasoning traces and enabling more efficient problem-solving. The final AceReason-Nemotron-1.1-7B model achieves state-of-the-art results on AIME and LiveCodeBench benchmarks through a carefully designed stage-wise RL curriculum that alternates between math and code domains.

## Method Summary
The methodology combines Qwen2.5-Math-7B base model with a stage-wise SFT+RL pipeline. SFT training uses 383K prompts (247K math + 136K code) with responses generated by DeepSeek-R1, scaled by increasing either prompt count or responses per prompt. RL employs Group Relative Policy Optimization (GRPO) with token-level loss, no KL penalty, and training temperature tuned to maintain entropy around 0.3. The RL curriculum follows: Math Stage-1 (8K token budget with overlong filtering) → Stage-2 (16K) → Stage-3 (24K) → Code Stage-I (24K) → Code Stage-II (32K) → Math Stage-4 (32K). Rule-based verifiers provide rewards through answer-box extraction for math and test-case execution for code.

## Key Results
- Prompt scaling yields 1.8x greater performance gains than response-per-prompt scaling in SFT (coefficients: 4.831 vs 2.635, R²=0.989)
- Stronger SFT models consistently produce better RL performance, though gaps narrow significantly during RL training
- Stage-1 RL with 8K token limit degrades immediate performance but enables better long-context reasoning in later stages
- Math-only RL significantly improves code reasoning performance even when starting from strong SFT models
- Final model achieves 72.6% on AIME24, 64.8% on AIME25, and over 52% on LiveCodeBench benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Prompt Diversity Dominates Response Diversity in SFT
Scaling unique prompts yields ~1.8x greater performance gains than scaling responses per prompt because unique prompts increase problem-type coverage, forcing the model to learn diverse reasoning patterns. Multiple responses per prompt provide solution diversity for known problem types but offer diminishing returns on generalization. The regression model (z = a·log₂x + b·log₂y + c) accurately captures the scaling relationship with coefficients a = 4.831 (prompts) vs b = 2.635 (responses/prompt), R² = 0.989.

### Mechanism 2: Entropy Regularization via Temperature Tuning Controls RL Sample Efficiency
Maintaining temperature-adjusted entropy near 0.3 during RL optimizes the exploration-exploitation balance. Low temperature (0.6) causes early entropy collapse (~0.15), reducing exploration. High temperature (1.0) yields excessive exploration with poor initial rewards, triggering premature exploitation. Temperature 0.85 maintains entropy in the 0.26–0.38 range, sustaining productive exploration throughout training and achieving 67.6% on AIME24 vs. 64.6% (temp 0.6) and 65.3% (temp 1.0).

### Mechanism 3: Short-Budget RL Stage Induces Reasoning Compression for Long-Budget Efficiency
An initial 8K token-budget RL stage (Stage-1) degrades immediate performance but is necessary for optimal long-context performance in later stages. The 8K constraint forces the model to compress verbose SFT-imitated reasoning traces into efficient forms. This compression skill transfers when token budgets expand to 16K–32K, enabling more thorough reasoning within available context. Skipping Stage-1 yields 51.8% on AIME25 at Stage-2; with Stage-1, achieves 56.7%. Response length drops from 5000→4000 tokens during Stage-1.

## Foundational Learning

- **Concept: On-policy RL with GRPO (Group Relative Policy Optimization)**
  - Why needed here: The paper uses strict on-policy GRPO with token-level advantages. Understanding how advantages are computed from group rollouts is essential for reproducing results or debugging reward scaling.
  - Quick check question: Given 8 rollouts for a prompt with scores [1, 0, 1, 1, 0, 0, 1, 0], can you compute the normalized advantage for the first correct response?

- **Concept: Rule-based Verification for Math and Code**
  - Why needed here: RL rewards come from deterministic verifiers (answer-box extraction for math, test-case execution for code), not learned reward models. This eliminates reward hacking but requires structured output formats.
  - Quick check question: Why might a code solution pass unit tests but still receive a poor reward under the token-level GRPO loss?

- **Concept: Exposure Bias in Autoregressive Models**
  - Why needed here: The paper attributes multi-epoch SFT gains to reduced exposure bias—the model learns to recover from its own predictions rather than teacher tokens.
  - Quick check question: How does training beyond one epoch on the same responses mitigate exposure bias, and when would additional epochs cause harm?

## Architecture Onboarding

- **Component map:** Qwen2.5-Math-7B base (rope_theta extended 10K→1M) -> SFT on 383K prompts (247K math + 136K code) -> Stage-1 RL Math (8K) -> Stage-2 RL Math (16K) -> Stage-3 RL Math (24K) -> Code Stage-I RL (24K) -> Code Stage-II RL (32K) -> Stage-4 RL Math (32K) -> Final model

- **Critical path:** 1) Data decontamination (9-gram overlap filtering against benchmarks) 2) Difficulty-balanced prompt sampling (avoid over-representation of simple 2K-token responses) 3) Stage-1 (8K) completion before any 16K+ training—early stopping is acceptable once response length stabilizes 4) Temperature tuning to achieve ~0.3 entropy at RL start

- **Design tradeoffs:** Overlong filtering: apply in Stage-1/2 (8K–16K); remove in Stage-4 (32K) to encourage conciseness. Epochs: Train SFT to 5 epochs (performance plateaus); further epochs show diminishing returns. Rollout count: G=8 or 16 rollouts per prompt; more rollouts stabilize advantage estimates but increase compute.

- **Failure signatures:** Entropy collapse (<0.15) early in RL → temperature too low. Stage-2 performance plateaus below SFT baseline → Stage-1 was skipped or undertrained. Code RL destabilizes training → math-only RL was insufficient before code introduction.

- **First 3 experiments:** 1) SFT scaling ablation: Train separate models varying only prompt count vs. responses/prompt; validate regression coefficients on held-out benchmarks. 2) Temperature sweep: Run Stage-1/2 RL with temperatures [0.6, 0.75, 0.85, 1.0]; plot entropy trajectories and final AIME24 scores to confirm 0.3 entropy target. 3) Stage-1 necessity test: Compare Stage-2 performance with vs. without Stage-1 pretraining; measure response length compression and final accuracy gap.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the finding that stronger SFT models lead to better final RL performance hold across significantly different base model architectures (e.g., Llama-based vs. Qwen-based) and at larger scales (e.g., 70B+)? All experiments use the same model family, leaving architecture and scale generalization untested.

- **Open Question 2:** What is the theoretical or mechanistic explanation for why temperature-adjusted entropy around 0.3 consistently yields optimal RL training outcomes? The 0.3 value is empirically derived; the underlying mechanism connecting entropy, exploration-exploitation balance, and reasoning capability remains unexplained.

- **Open Question 3:** What is the optimal stopping criterion for Stage-1 (8K) RL training, given that longer Stage-1 does not consistently improve Stage-2 performance? The trade-off between Stage-1 duration and downstream benefits is unclear; the paper only tests three fixed step counts.

## Limitations
- Training dynamics beyond Stage-1 are unspecified, making precise reproduction difficult
- Data quality variability across problem types may affect SFT and RL performance
- Benchmark contamination risk cannot be entirely ruled out despite 9-gram filtering
- The mechanism and conditions for math-to-code transfer remain unclear
- Compression benefits assume 7B model cannot mimic verbose teacher responses

## Confidence
- SFT Scaling (Prompts > Responses): High - Strong empirical support with regression analysis (R²=0.989)
- Temperature-Entropy Relationship: Medium - Empirical validation strong but universality unproven
- Stage-1 Compression Necessity: Medium - Substantial performance gap but causal relationship needs validation
- Math-to-Code Transfer: Low - Compelling results but underlying mechanism unclear

## Next Checks
1. **Scaling Regression Validation:** Replicate SFT scaling experiments with a different base model (e.g., Llama-7B) to verify the prompt-vs-response scaling relationship holds across model architectures.

2. **Temperature Generalization Test:** Conduct systematic temperature sweeps across multiple RL stages with models of different sizes (7B, 14B, 34B) to determine if the 0.3 entropy target generalizes or requires recalibration.

3. **Compression Mechanism Isolation:** Design experiment where Stage-1 compression is applied but final stage uses original teacher response format to determine whether compression skill transfer or other Stage-1 effects drive performance gains.