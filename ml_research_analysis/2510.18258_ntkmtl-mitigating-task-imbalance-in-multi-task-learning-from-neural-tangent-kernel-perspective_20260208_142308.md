---
ver: rpa2
title: 'NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent
  Kernel Perspective'
arxiv_id: '2510.18258'
source_url: https://arxiv.org/abs/2510.18258
tags:
- learning
- tasks
- task
- multi-task
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses task imbalance in multi-task learning (MTL)
  by leveraging Neural Tangent Kernel (NTK) theory. The authors introduce an extended
  NTK matrix for MTL and use spectral analysis to balance the convergence speeds of
  multiple tasks.
---

# NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective

## Quick Facts
- **arXiv ID:** 2510.18258
- **Source URL:** https://arxiv.org/abs/2510.18258
- **Reference count:** 40
- **Primary result:** Introduces NTKMTL and NTKMTL-SR to balance task convergence speeds in MTL using NTK spectral analysis, achieving state-of-the-art performance across diverse benchmarks.

## Executive Summary
This paper addresses the fundamental challenge of task imbalance in multi-task learning (MTL) by introducing a novel framework based on Neural Tangent Kernel (NTK) theory. The authors develop NTKMTL, which uses spectral analysis of an extended NTK matrix to assign task-specific weights that balance convergence speeds. They also propose NTKMTL-SR, an efficient approximation that computes NTK with respect to shared representations rather than all parameters. Extensive experiments across supervised learning and reinforcement learning benchmarks demonstrate significant improvements over state-of-the-art methods, with NTKMTL achieving superior performance while NTKMTL-SR maintains competitive accuracy with reduced computational cost.

## Method Summary
The method leverages NTK theory to diagnose and mitigate task imbalance in MTL by analyzing the spectral properties of task-specific kernels. For each task, the authors compute the maximum eigenvalue of its NTK block and assign weights inversely proportional to the square root of these eigenvalues, effectively normalizing the convergence speeds. The exact NTKMTL computes the full NTK with respect to shared parameters, while NTKMTL-SR approximates this by computing NTK with respect to the shared representation z, reducing computational overhead. The weighted loss function L = Σ ω_i ℓ_i ensures balanced gradient contributions across tasks during training.

## Key Results
- NTKMTL achieves a mean rank of 4.33 and -6.99% performance drop on NYU-v2 benchmark compared to single-task learning
- NTKMTL-SR shows 0.23% improvement over single-task learning on CelebA with 40 tasks while maintaining training efficiency
- The method demonstrates state-of-the-art performance across supervised learning (NYUv2, CityScapes, QM9, CelebA) and reinforcement learning (MT10) benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Spectral Bias as a Proxy for Task Convergence
The convergence speed of a task in MTL can be characterized by the eigenvalues of its corresponding Neural Tangent Kernel (NTK) matrix; specifically, larger eigenvalues correlate with faster convergence. The method leverages NTK theory, where training dynamics are modeled as an Ordinary Differential Equation (ODE). By performing spectral decomposition on the extended NTK matrix for MTL, the authors show that error components decay at a rate proportional to ηλ_i. If one task has significantly larger NTK eigenvalues, it dominates the gradient updates (spectral bias), causing task imbalance.

### Mechanism 2: Eigenvalue Normalization via Task Weighting
Assigning weights to tasks inversely proportional to the square root of their max NTK eigenvalue balances the effective convergence speeds. By formulating the loss as L = Σ ω_i ℓ_i, the effective NTK matrix is scaled. The authors derive ω_i ∝ 1/√λ_i to equalize the maximum eigenvalues across tasks, preventing high-eigenvalue tasks from dominating the gradient flow.

### Mechanism 3: Efficient Approximation via Shared Representation (NTKMTL-SR)
Calculating the NTK with respect to the shared representation z (rather than all parameters θ) approximates the full method while allowing a single backward pass. Instead of computing the Jacobian w.r.t. all shared parameters (costly), the method computes it w.r.t. the shared features z. This reduces computational overhead from O(K) backward passes to a single pass, as the gradient w.r.t z acts as a common bottleneck.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here:** This is the theoretical lens used to diagnose and solve task imbalance. Understanding that NTK links network initialization/gradients to kernel regression is essential.
  - **Quick check question:** Does the NTK of a standard network remain exactly constant or only approximately constant during training?

- **Concept: Spectral Bias (Frequency Principle)**
  - **Why needed here:** It explains why tasks converge at different rates—low-frequency components (large eigenvalues) are learned first.
  - **Quick check question:** In a neural network, does a component with a larger NTK eigenvalue converge faster or slower?

- **Concept: Multi-Task Optimization Conflicts**
  - **Why needed here:** To understand the baseline problem (negative transfer/imbalance) that NTKMTL tries to solve.
  - **Quick check question:** If two tasks have gradients pointing in opposite directions, what happens to the shared parameters in standard Linear Scalarization?

## Architecture Onboarding

- **Component map:** Shared Encoder (θ) -> Task Heads (k tasks) -> NTK Estimator (Jacobians J_i) -> Weight Calculator (ω_i)

- **Critical path:**
  1. Forward Pass: Compute outputs for a batch of size n
  2. Spectral Analysis: Compute Jacobians (w.r.t θ for NTKMTL or z for NTKMTL-SR). Calculate top eigenvalue λ_i for each task
  3. Weighting: Compute ω_i = √(λ̄ / λ_i)
  4. Backward Pass: Compute weighted loss L = Σ ω_i ℓ_i

- **Design tradeoffs:**
  - NTKMTL (Exact): High computational cost (k gradient backprops). Best for accuracy-critical small models
  - NTKMTL-SR (Approx): Low cost (1 backprop). Best for large-scale or many-task settings (e.g., CelebA 40 tasks)
  - Batch Size (n): Larger n improves eigenvalue estimation stability but increases memory/time

- **Failure signatures:**
  - Instability: Weights ω_i oscillate wildly → typically means n (mini-batches) is too small for stable eigenvalue estimation
  - Stagnation: Loss plateaus early → NTK assumption may be violated (learning rate too high), or eigenvalue estimates are stale

- **First 3 experiments:**
  1. Sanity Check (NYU-v2): Implement NTKMTL with n=1. Verify if it outperforms Linear Scalarization on the "Surface Normal" task (identified as difficult/imbalanced)
  2. Efficiency Validation (CelebA): Run NTKMTL-SR on a 40-task setup. Measure wall-clock time vs. Figure 1 to confirm "Loss-oriented" speed
  3. Hyperparameter Sensitivity (QM9): Vary n (e.g., 1, 2, 4, 6) to replicate Figure 3 results showing performance saturation and variance reduction

## Open Questions the Paper Calls Out

### Open Question 1
How can the off-diagonal blocks (K_ij) of the extended NTK matrix be leveraged to analyze task interactions or develop optimal task grouping strategies? The authors state that analyzing the off-diagonal K_ij blocks could reveal novel insights into task interactions or guide the development of task grouping strategies. These potential avenues are left for future investigation.

### Open Question 2
Does relying solely on the maximum eigenvalue (λ_max) to determine task weights fail to capture the convergence dynamics of tasks with complex or multi-modal eigenvalue distributions? While the method works empirically, there is no theoretical guarantee that normalizing the fastest-converging mode (largest eigenvalue) effectively balances the slower-converging modes critical for generalization.

### Open Question 3
Under what conditions does the NTKMTL-SR approximation, which relies on shared representation, fail to accurately reflect the training dynamics of the full parameter space? The paper treats the approximation as empirically sufficient but does not theoretically bound the error introduced by ignoring the shared parameter gradients during the spectral analysis phase.

## Limitations
- The NTK framework relies on the "lazy training" regime where the kernel remains approximately constant during training, which may break down for deep networks with high learning rates or narrow widths
- The method's effectiveness depends critically on accurate eigenvalue estimation, requiring careful selection of batch size n
- For NTKMTL-SR, the approximation quality depends on the shared representation z capturing sufficient information about task-specific gradients, which may not hold for highly heterogeneous tasks

## Confidence

| Claim | Confidence |
|-------|------------|
| Spectral analysis mechanism linking NTK eigenvalues to convergence rates is theoretically grounded | High |
| Empirical results showing NTKMTL outperforming baseline methods on multiple benchmarks | High |
| Efficiency claims for NTKMTL-SR are supported by runtime comparisons | Medium |
| Robustness to extreme task imbalance scenarios is extensively tested | Low |

## Next Checks

1. **Theoretical Validation:** Analyze NTK evolution during training on a simple MTL problem to verify the constant-kernel assumption holds and quantify approximation error.

2. **Efficiency Benchmark:** Conduct wall-clock time measurements comparing NTKMTL vs NTKMTL-SR on a large-scale MTL problem (e.g., 50+ tasks) to validate the claimed efficiency gains.

3. **Robustness Test:** Design an MTL experiment with extreme task imbalance (e.g., 100:1 ratio in convergence rates) to stress-test the eigenvalue normalization mechanism and identify potential failure modes.