---
ver: rpa2
title: Retrieval augmented generation based dynamic prompting for few-shot biomedical
  named entity recognition using large language models
arxiv_id: '2508.06504'
source_url: https://arxiv.org/abs/2508.06504
tags:
- shot
- biomedical
- prompt
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of few-shot biomedical named
  entity recognition (NER) by combining static and dynamic prompt engineering with
  retrieval-augmented generation (RAG). The proposed method uses structured static
  prompts and dynamically selects in-context examples via retrieval engines (TF-IDF,
  SBERT, ColBERT, DPR) to improve LLM performance in biomedical NER.
---

# Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models

## Quick Facts
- arXiv ID: 2508.06504
- Source URL: https://arxiv.org/abs/2508.06504
- Authors: Yao Ge; Sudeshna Das; Yuting Guo; Abeed Sarker
- Reference count: 40
- Key outcome: Structured static prompts increased average F1-scores by 12% for GPT-4 and 11% for GPT-3.5 and LLaMA 3; dynamic prompting with TF-IDF and SBERT achieved 7.3% and 5.6% further improvements in 5-shot and 10-shot settings respectively

## Executive Summary
This study addresses the challenge of few-shot biomedical named entity recognition (NER) by combining static and dynamic prompt engineering with retrieval-augmented generation (RAG). The proposed method uses structured static prompts and dynamically selects in-context examples via retrieval engines (TF-IDF, SBERT, ColBERT, DPR) to improve LLM performance in biomedical NER. Evaluation on five biomedical datasets with GPT-4, GPT-3.5, and LLaMA 3 demonstrates significant performance improvements over baseline approaches.

The research demonstrates that structured prompt design combined with intelligent example selection through retrieval engines can substantially enhance few-shot learning capabilities of large language models for biomedical NER tasks. The results show consistent improvements across different model architectures and dataset sizes, validating the effectiveness of the proposed approach for addressing data scarcity in specialized domains.

## Method Summary
The method combines static and dynamic prompt engineering with retrieval-augmented generation for few-shot biomedical NER. Static prompts are structured templates with fixed components including task description, entity types, and formatting guidelines. Dynamic prompting involves retrieving relevant in-context examples from a knowledge base using four different retrieval engines: TF-IDF, SBERT, ColBERT, and DPR. The system constructs prompts by combining static templates with dynamically selected examples, then feeds them to LLMs (GPT-4, GPT-3.5, LLaMA 3) for NER prediction. The approach is evaluated across five biomedical datasets with varying shot settings (1, 5, and 10 examples).

## Key Results
- Structured static prompts increased average F1-scores by 12% for GPT-4 and 11% for GPT-3.5 and LLaMA 3
- Dynamic prompting with TF-IDF achieved 7.3% improvement in 5-shot settings
- Dynamic prompting with SBERT achieved 5.6% improvement in 10-shot settings
- TF-IDF and SBERT emerged as top-performing retrieval engines across all evaluated models

## Why This Works (Mechanism)
The approach leverages the inherent few-shot learning capabilities of large language models by providing structured guidance through static prompts and contextually relevant examples through dynamic retrieval. Static prompts establish clear task boundaries and formatting expectations, reducing ambiguity in model interpretation. Dynamic retrieval ensures that in-context examples are semantically relevant to the query, providing more effective learning signals than random or sequential sampling. The combination addresses both the structural and contextual challenges in few-shot biomedical NER, where domain-specific terminology and entity patterns require precise guidance.

## Foundational Learning

1. **Prompt Engineering Principles** - Why needed: LLMs require explicit instructions for task completion; quick check: test different prompt structures with baseline models
2. **Retrieval-Augmented Generation** - Why needed: Static examples may not cover query-specific contexts; quick check: compare retrieval vs random example selection
3. **Biomedical Named Entity Recognition** - Why needed: Domain-specific entities require specialized handling; quick check: validate entity extraction across different biomedical subdomains
4. **Few-Shot Learning Dynamics** - Why needed: Limited examples require efficient learning strategies; quick check: measure performance across varying shot counts
5. **Semantic Similarity Measures** - Why needed: Effective retrieval depends on accurate similarity scoring; quick check: benchmark retrieval engines on entity type coverage
6. **Large Language Model Fine-tuning** - Why needed: Understanding model capabilities for domain adaptation; quick check: compare performance with and without domain-specific pretraining

## Architecture Onboarding

**Component Map**: Input Query -> Retrieval Engine -> Example Selection -> Static Prompt Template -> Dynamic Prompt Construction -> LLM -> NER Output

**Critical Path**: The retrieval engine selection and example matching process is the performance bottleneck, as retrieval quality directly impacts downstream NER accuracy. Static prompt template design affects model comprehension speed and consistency.

**Design Tradeoffs**: Static prompts offer consistency but lack adaptability; dynamic retrieval provides contextual relevance but introduces computational overhead. The choice of retrieval engine involves balancing accuracy (SBERT, ColBERT) against speed (TF-IDF) and complexity (DPR).

**Failure Signatures**: Poor retrieval quality manifests as irrelevant examples, leading to degraded NER performance. Overly rigid static templates may constrain model flexibility. Model-specific limitations become apparent when applying identical prompts across different LLM architectures.

**First Experiments**:
1. Compare static prompt variations (task description placement, entity formatting) on a single dataset with GPT-4
2. Benchmark all four retrieval engines on entity type coverage and retrieval precision for a subset of biomedical entities
3. Test dynamic prompting performance degradation as retrieval database size increases to assess scalability limits

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to five biomedical datasets, potentially missing domain diversity
- Model-specific performance variations suggest dependencies not fully characterized
- Retrieval engine effectiveness criteria remain unclear for different biomedical subdomains
- Computational costs and latency implications for production deployment not addressed

## Confidence

**High Confidence**: The 12% and 11% F1-score improvements from static prompts are well-supported by consistent results across multiple models and datasets. The methodology is clearly described and reproducible.

**Medium Confidence**: Dynamic prompting results showing TF-IDF and SBERT as top performers are reasonably supported, but optimal retrieval strategy selection requires further validation. Performance may vary with different dataset characteristics.

**Low Confidence**: Generalization to broader biomedical domains, different entity types, or real-world clinical applications remains uncertain. Scalability and production deployment considerations are not addressed.

## Next Checks
1. Cross-domain validation: Test approaches on additional biomedical datasets with different entity types and text sources to assess generalizability
2. Retrieval engine benchmarking: Conduct systematic ablation studies comparing all four retrieval engines across varying dataset sizes and entity complexities
3. Real-world deployment assessment: Evaluate computational efficiency, inference latency, and cost implications in production-like environments, including comparisons with fine-tuned biomedical NER models