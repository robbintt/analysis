---
ver: rpa2
title: 'EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational
  Assistants'
arxiv_id: '2511.21742'
source_url: https://arxiv.org/abs/2511.21742
tags:
- retrieval
- response
- student
- evaluation
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EduMod-LLM, a modular framework for LLM-based
  educational QA systems. It enables fine-grained evaluation of function calling,
  retrieval, and response generation components using real student questions.
---

# EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants

## Quick Facts
- arXiv ID: 2511.21742
- Source URL: https://arxiv.org/abs/2511.21742
- Authors: Meenakshi Mittal; Rishi Khare; Mihran Miroyan; Chancharik Mitra; Narges Norouzi
- Reference count: 29
- Key outcome: GPT-4.1 achieves best response quality (factuality 4.648, relevance 4.955, style 2.999), structure-aware retrieval significantly outperforms vector-based baselines (0.784 Recall@1 vs 0.441-0.460), and multihop function-calling matches or exceeds hand-designed rules without requiring course metadata.

## Executive Summary
This paper introduces EduMod-LLM, a modular framework for LLM-based educational QA systems that enables fine-grained evaluation of function calling, retrieval, and response generation components. The system leverages hierarchical course content organization through structure-aware retrieval and employs an LLM-as-a-Judge module aligned with TA grading standards. Key results demonstrate GPT-4.1's superior response quality, the effectiveness of hierarchical retrieval over semantic-vector similarity, and the flexibility of multihop function-calling approaches that match or exceed rule-based methods without requiring metadata.

## Method Summary
The framework consists of three main components: function calling, structure-aware retrieval, and response generation. The system uses seven function-calling variants including multihop approaches that decompose tool selection and argument generation into sequential LLM calls. Structure-aware retrieval builds a hierarchical k-ary tree from course content using recursive summarization, enabling coarse-to-fine traversal during inference. Response generation employs GPT-4.1, while DeepSeek-V3 serves as the LLM-as-a-Judge with a specific rubric. The evaluation uses 1,000 student questions from EdSTEM (UC Berkeley, Spring 2024), with 180 expert-labeled for evaluation and 111 assignment-based questions for retrieval benchmarking.

## Key Results
- GPT-4.1 achieves best response quality: factuality 4.648, relevance 4.955, style 2.999 on 1-5/1-3 Likert scales
- Structure-aware retrieval significantly outperforms vector-based baselines: 0.784 Recall@1 vs 0.441-0.460
- Multihop function-calling approach matches or exceeds hand-designed rule-based methods without requiring course metadata
- DeepSeek-V3 achieves >70% exact match alignment with expert TA scores in LLM-as-Judge evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If retrieval systems utilize the hierarchical structure of educational content (chapters, sections), recall improves significantly over semantic-vector similarity alone.
- **Mechanism:** The system recursively summarizes content into a tree structure. At inference, it navigates this tree (coarse-to-fine) rather than searching a flat embedding space. This aligns retrieval boundaries with logical document boundaries.
- **Core assumption:** Educational documents possess inherent, detectable hierarchical structures (e.g., textbooks, assignments) that map to student query intent better than fixed-length chunks.
- **Evidence anchors:**
  - [abstract]: "structure-aware retrieval method significantly outperforms vector-based baselines (0.784 Recall@1 vs 0.441-0.460)"
  - [section 3.3]: "Recursively summarize the chunks into a hierarchical (k-ary tree) structure... enabling coarse-to-fine traversal."
  - [corpus]: Weak/General. Neighbors discuss LLM agents in education (ELMES) and diagnosis (DocCHA) but do not validate this specific hierarchical retrieval mechanism.
- **Break condition:** Fails if source documents are unstructured (e.g., messy forum threads without clear headers) or if the summarization LLM hallucinates the hierarchy.

### Mechanism 2
- **Claim:** Decomposing tool selection into a multi-step reasoning process (multihop) allows LLMs to match or exceed the accuracy of deterministic, hand-designed rules without requiring metadata.
- **Mechanism:** Instead of a single API call to select a tool, the pipeline separates the decision of *which* function to call from the generation of *arguments* for that function, simulating a reasoning chain.
- **Core assumption:** The underlying LLM possesses sufficient reasoning capability to decompose the user query into tool requirements without explicit hard-coded routing.
- **Evidence anchors:**
  - [abstract]: "multihop function-calling approach matches or exceeds hand-designed rule-based methods without requiring course metadata."
  - [section 3.2]: Describes "Pipeline 6 (fc multihop)" as decomposing function selection and argument generation into two LLM calls.
  - [corpus]: Neighbors (e.g., "MultiMind") mention AI assistants for tasks, supporting the move to flexible tool use, but lack specific validation of the multihop vs. rule-based comparison.
- **Break condition:** Fails if the query is ambiguous to the point that even multi-step reasoning cannot resolve the correct tool without external context.

### Mechanism 3
- **Claim:** Automated evaluation aligns with human experts (TAs) if the judge model is non-reasoning and constrained by a detailed, domain-specific rubric.
- **Mechanism:** A non-reasoning model (DeepSeek-V3) is prompted with specific axes (Factuality, Relevance, Style) and few-shot examples. This constrains the output space, reducing variance better than unconstrained reasoning models.
- **Core assumption:** Pedagogical quality can be discretized into scalar metrics (1-5, 1-3) that correlate strongly with expert judgment.
- **Evidence anchors:**
  - [abstract]: "LLM-as-a-Judge module aligned with TA grading standards."
  - [section 4.1]: "DeepSeek-v3 achieves the best alignment with expert TAs... over 70% exact match."
  - [corpus]: "LLM-as-a-Grader" (neighbor) supports the feasibility of LLM evaluation in classrooms, corroborating the general mechanism.
- **Break condition:** Fails if the "Style" or pedagogical nuance required is highly subjective or context-dependent beyond the provided few-shot examples.

## Foundational Learning
- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The system relies on injecting external course context (textbooks, Q&A) into the LLM.
  - **Quick check question:** Can you explain the difference between a "dense passage retriever" and a "hierarchical retriever"?
- **Concept: Function Calling / Tool Use**
  - **Why needed here:** The system dynamically selects functions (`qa_retrieval`, `textbook_retrieval`) based on the student's query.
  - **Quick check question:** How does an LLM determine which function to call if the user prompt is ambiguous?
- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The framework automates quality control to scale evaluation beyond manual TA grading.
  - **Quick check question:** Why might a model's "logprobs" or reasoning tokens be less useful for evaluation than a direct constrained generation?

## Architecture Onboarding
- **Component map:** Student Query -> Function-Calling Module (GPT-4o, Multihop strategy) -> Structure-Aware Retriever (Hierarchical Tree of Course Content) -> Response LLM (GPT-4.1) -> LLM-as-a-Judge (DeepSeek-V3 with specific rubric)
- **Critical path:** The **Structure-Aware Retriever**. If the document chunking and summarization tree is built incorrectly, recall drops to baseline levels (~0.44).
- **Design tradeoffs:**
  - **Flexibility vs. Control:** The "fc multihop" is flexible but probabilistic; "Edison" (rule-based) is deterministic but brittle.
  - **Cost vs. Quality:** GPT-4.1 provides best generation, but smaller models (DeepSeek) are better/cheaper for evaluation.
- **Failure signatures:**
  - Low Recall@1 (<0.5): Indicates the retrieval tree is not aligned with query types or chunking failed.
  - High MAE in Evaluation: Judge rubric is too vague or model is over-reasoning.
  - Function-calling over/under-selection. fc_multihop should achieve F1 ~0.89 with GPT-4o. If lower, check prompt clarity and schema definitions, or if course metadata (required for Edison baseline) is missing.
- **First 3 experiments:**
  1. Validate Retrieval: Run the Hierarchical Retriever vs. Ada-002 on a subset of assignment questions to verify the ~0.78 Recall@1 claim.
  2. Stress Test Function Calling: Compare "fc multihop" vs. "fc forced" on ambiguous queries to see if accuracy degrades.
  3. Calibrate Judge: Correlate DeepSeek-V3 scores against human TA scores on 50 random samples to establish your local baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- The hierarchical retrieval mechanism's effectiveness depends heavily on document structure quality and may fail with unstructured educational content
- The multihop function-calling approach shows only marginal improvements over rule-based methods in some cases
- The LLM-as-a-Judge evaluation relies on a constrained rubric that may not capture all pedagogical nuances
- The study's focus on a single UC Berkeley course limits generalizability across different educational contexts and disciplines

## Confidence
- **High Confidence**: Structure-aware retrieval performance (0.784 Recall@1) - directly measured against baselines with clear methodology
- **Medium Confidence**: Multi-hop function-calling efficacy - shown to match rule-based methods but with limited comparison across diverse query types
- **Medium Confidence**: LLM-as-a-Judge alignment - demonstrates statistical correlation with TAs but may not capture full pedagogical quality spectrum
- **Low Confidence**: Generalizability - results based on single course data from one institution

## Next Checks
1. **Cross-Domain Retrieval Validation**: Test the hierarchical retrieval mechanism on courses from different disciplines (STEM vs humanities) to verify if the ~0.78 Recall@1 performance generalizes beyond the original course structure.

2. **Pedagogical Alignment Stress Test**: Conduct blind evaluation where human TAs score responses without knowing whether they came from the EduMod-LLM system or human-generated answers, measuring both agreement rates and identifying edge cases where the rubric fails.

3. **Longitudinal Student Impact Study**: Track actual student learning outcomes over a semester using the EduMod-LLM system versus traditional support methods, measuring not just response quality but knowledge retention and course performance.