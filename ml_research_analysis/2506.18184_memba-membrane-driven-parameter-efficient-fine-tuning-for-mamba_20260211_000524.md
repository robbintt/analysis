---
ver: rpa2
title: 'Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba'
arxiv_id: '2506.18184'
source_url: https://arxiv.org/abs/2506.18184
tags:
- membrane
- memba
- proj
- mamba
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Memba, a membrane-driven parameter-efficient
  fine-tuning method for Mamba models. The core innovation is the Leaky Integrate
  Membrane (LIM) neuron, which introduces bio-inspired gating mechanisms that accumulate
  membrane potentials over time to enhance selective information retention.
---

# Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba

## Quick Facts
- arXiv ID: 2506.18184
- Source URL: https://arxiv.org/abs/2506.18184
- Reference count: 40
- Primary result: Memba achieves 1.3% absolute improvement over MambaPEFT on language tasks while using only 28% of trainable parameters

## Executive Summary
Memba introduces a novel parameter-efficient fine-tuning method for Mamba models that addresses the challenge of adapting selective state-space models without degrading pre-trained performance. The core innovation is the Leaky Integrate Membrane (LIM) neuron, which enhances the gating branch with bio-inspired temporal selectivity through membrane potential accumulation. By combining LIM neurons with strategically placed LoRA adapters and cross-layer membrane transfer, Memba achieves consistent improvements across commonsense reasoning and vision tasks while maintaining parameter efficiency.

## Method Summary
Memba enhances Mamba's gating branch with Leaky Integrate Membrane (LIM) neurons that accumulate membrane potentials over time, creating temporal selectivity. The method applies LoRA adapters to input and output projections while keeping the core state-space model frozen. Cross-layer membrane transfer propagates compressed temporal information between layers. LIM neurons split sequences into chunks and update membrane states through leaky integration with threshold-based resets, providing a bio-inspired gating mechanism that complements Mamba's linear gate.

## Key Results
- Achieves 1.3% absolute improvement over MambaPEFT on 8 commonsense reasoning tasks using Mamba-790M
- Attains 72.33% average accuracy on VTAB-1k vision tasks while using only 28% of trainable parameters compared to hybrid methods
- Outperforms baseline LoRA and hybrid methods across both language and vision domains

## Why This Works (Mechanism)

### Mechanism 1
LIM neurons introduce temporal selectivity in the gating branch through membrane potential accumulation without modifying pre-trained SSM dynamics. The LIM neuron partitions sequences into chunks and applies leaky integration (u[i+1] = τ·u[i] + W·X[i]) with a threshold reset. This creates selective amplification of task-relevant features while allowing gradual decay of older context—a form of learned temporal gating absent from Mamba's original linear gate.

### Mechanism 2
Strategic LoRA placement on input/output projections (not SSM components) preserves pre-trained temporal dynamics while enabling efficient adaptation. LoRA adapters on `in_proj` and `out_proj` act as information bottlenecks that modify how data enters and exits the Mamba block, without altering the selective scan's learned state transitions. The ablation shows excluding `in_proj` causes 1.2% accuracy drop, confirming its critical role.

### Mechanism 3
Cross-layer membrane transfer maintains temporal coherence across network depth by propagating compressed membrane states. Each layer averages its membrane potentials (ū = mean(u[1]...u[T])) and passes this to initialize the next layer's first chunk. This creates hierarchical temporal information flow without adding parameters.

## Foundational Learning

- **State Space Models (SSMs) and Selective Scan**: Understanding how discrete state evolution (h_t = Āh_{t-1} + B̄x_t) differs from attention is essential to grasp why direct SSM fine-tuning is problematic. *Quick check*: Can you explain why SSMs have O(L) complexity versus attention's O(L²)?

- **Leaky Integrate-and-Fire (LIF) Neuron Model**: LIM is derived from spiking neuron dynamics; understanding membrane potential integration and reset thresholds clarifies the bio-inspired mechanism. *Quick check*: What happens to membrane potential when input exceeds threshold in a LIF neuron?

- **Low-Rank Adaptation (LoRA)**: Memba combines LIM with LoRA; understanding weight decomposition (W = W₀ + BA) explains how parameter-efficient adaptation works. *Quick check*: If LoRA rank is 8 and original weight is 1024×1024, how many trainable parameters does LoRA add?

## Architecture Onboarding

- **Component map**: Input → [RMS Norm] → [in_proj + LoRA] → Split → ├─ SSM Path: [Selective Scan] → Y_SSM └─ Gate Path: [W_gate_in] → [LIM] → [W_gate_out] → σ → Y_gate Merge: Y_SSM ⊙ Y_gate → [out_proj + LoRA] → Output

- **Critical path**: The gate path (W_gate_in → LIM → W_gate_out) is where Memba's innovation lives. The SSM path remains frozen.

- **Design tradeoffs**:
  - **Chunk count (T)**: More chunks = finer temporal granularity but more sequential computation. Paper uses T=4 as optimal.
  - **LoRA rank**: Higher rank = more expressivity but more parameters. Gate projections use lower rank (16-32) vs. main projections (32-96).
  - **Leak factor (τ)**: Higher τ = longer memory retention. Paper finds τ=1/2 optimal.

- **Failure signatures**:
  - Accuracy drops vs. baseline LoRA → Check if `in_proj` LoRA was accidentally excluded
  - No improvement over vanilla Mamba → Verify membrane transfer is enabled (not just LIM+LoRA)
  - Training instability → Check Vth setting; too low causes excessive resets

- **First 3 experiments**:
  1. Apply LoRA-only (no LIM) to `in_proj` and `out_proj` on a small commonsense task; verify baseline matches MambaPEFT reported values.
  2. Add LIM alone, then LIM+LoRA, then LIM+LoRA+membrane-transfer; measure incremental gains on ARC-Easy subset.
  3. Vary τ ∈ {0.25, 0.5, 0.75} and Vth ∈ {0.5, 1.0, 2.0} on single VTAB task (e.g., CIFAR-100) to validate reported optima before full runs.

## Open Questions the Paper Calls Out

### Open Question 1
Can the LIM algorithm be efficiently integrated into selective scan kernels to eliminate the 8.8% inference overhead? This integration requires specialized hardware kernel modifications that were not implemented in this work. Kernel-level implementation demonstrating inference latency comparable to baseline Mamba would resolve this.

### Open Question 2
How does Memba's performance scale with model sizes beyond 1.4B parameters? The experiments are limited to Mamba-130M through Mamba-1.4B. Evaluation on larger Mamba models (2.8B, 7B, etc.) showing whether the relative improvement over baselines is maintained would provide answers.

### Open Question 3
What are the optimal hyperparameter configurations (τ, Vth, chunk count) across diverse task domains and model architectures? The ablation covers limited configurations. Comprehensive grid search across hyperparameters on a wider range of tasks, yielding principled selection rules, would resolve this.

## Limitations
- Evaluation covers limited downstream task diversity (8 commonsense reasoning tasks, 19 VTAB-1k datasets)
- Mechanism benefits may be specific to Mamba's gating branch rather than general temporal modeling improvements
- Cross-layer membrane transfer adds training-time complexity and requires careful implementation

## Confidence
- **High Confidence**: Claims about Memba outperforming LoRA and hybrid methods on Mamba-790M for commonsense reasoning and VTAB-1k tasks
- **Medium Confidence**: Claims about bio-inspired LIM neurons improving temporal modeling
- **Low Confidence**: Claims about Memba being broadly applicable to "any Mamba-based model" without modification

## Next Checks
1. Apply Memba to a transformer-based language model (e.g., Llama) using the same commonsense reasoning tasks to test generalizability
2. Systematically vary chunk count (T ∈ {2, 4, 8, 16}) and LoRA rank (32, 64, 128) on a single VTAB task while keeping membrane parameters fixed
3. Measure wall-clock training time and memory usage for Memba vs. baseline LoRA on the same hardware