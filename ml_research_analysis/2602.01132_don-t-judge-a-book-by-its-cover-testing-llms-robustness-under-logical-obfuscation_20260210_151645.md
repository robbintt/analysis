---
ver: rpa2
title: 'Don''t Judge a Book by its Cover: Testing LLMs'' Robustness Under Logical
  Obfuscation'
arxiv_id: '2602.01132'
source_url: https://arxiv.org/abs/2602.01132
tags:
- obfus
- reasoning
- logical
- obfuscation
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models handle reasoning tasks well in standard form
  but fail when questions are logically equivalent yet obfuscated. To evaluate this
  brittleness, the authors introduce Logifus, a structure-preserving logical obfuscation
  algorithm, and LogiQAte, a first-of-its-kind benchmark of 1,108 questions across
  four reasoning tasks.
---

# Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation

## Quick Facts
- arXiv ID: 2602.01132
- Source URL: https://arxiv.org/abs/2602.01132
- Reference count: 40
- Large language models handle reasoning tasks well in standard form but fail when questions are logically equivalent yet obfuscated.

## Executive Summary
This paper investigates the robustness of large language models to logical obfuscation—situations where questions are rewritten in logically equivalent but syntactically different forms. The authors introduce Logifus, a structure-preserving obfuscation algorithm, and LogiQAte, a benchmark of 1,108 questions across four reasoning tasks. Across six state-of-the-art models, performance drops by 28.8% on average under logical obfuscation. Mechanistic analysis reveals that obfuscated inputs trigger higher reliance on training patterns and reduce late-layer reasoning confidence by 50-80%. The results demonstrate that current LLMs parse rather than understand logical structure, highlighting the need for models that genuinely comprehend meaning beyond surface form.

## Method Summary
The study evaluates LLM robustness using Logifus, a structure-preserving logical obfuscation algorithm, and LogiQAte, a benchmark of 1,108 questions across four reasoning tasks (First-Order Logic, Blood Relations, Number Series, Direction Sense). The evaluation measures behavioral performance (Exact Match accuracy) and mechanistic signatures (memorization sensitivity via Min-K%++ scores and layer-wise next-token log probabilities). Models are tested in zero-shot, few-shot, and chain-of-thought settings with temperature 0.0. The obfuscation includes transformations like De Morgan's laws, relational obfuscation chains, pattern-preserving substitutions, and spatial obfuscation. Performance degradation and mechanistic analysis reveal that obfuscated inputs trigger higher reliance on training patterns and reduce late-layer reasoning confidence.

## Key Results
- Performance drops by 28.8% on average across six state-of-the-art models under logical obfuscation
- Memorization sensitivity increases, with detection rates rising from 50% on original to 82% on obfuscated questions
- Late-layer reasoning confidence (layers 28-31 in LLaMA 3.1 8B) drops by 50-80% for obfuscated inputs
- GPT-4o shows 47% average performance degradation on LogiQAte benchmark across zero-shot, few-shot, and CoT settings

## Why This Works (Mechanism)

### Mechanism 1: Surface-Form Pattern Matching Over Reasoning
- **Claim:** LLMs' reasoning performance is heavily dependent on recognizing familiar surface patterns from training data, and this reliance increases when logical structure is obscured.
- **Mechanism:** Obfuscated inputs fail to trigger well-rehearsed reasoning templates. The model, unable to match a familiar pattern, falls back on shallower memorized associations and heuristics. This shift is detectable as an increase in memorization sensitivity scores.
- **Core assumption:** The Min-K%++ metric accurately reflects a shift from novel reasoning to reliance on memorized training data.
- **Evidence anchors:**
  - [abstract] "Memorization sensitivity analysis... reveal that obfuscated inputs trigger higher reliance on training patterns..."
  - [Section 5.1] "Memorization tests show detection rates rising from 50% on original to 82% on obfuscated questions..."
  - [corpus] Evidence is internal to the paper's experiments; corpus does not discuss this specific mechanism.

### Mechanism 2: Disrupted Semantic Consolidation in Late Layers
- **Claim:** Logical obfuscation specifically impairs the function of late transformer layers, which are critical for integrating semantic information and producing a confident prediction.
- **Mechanism:** The novel syntactic and semantic structure of obfuscated questions disrupts the typical flow of information processing. This disproportionately affects late layers (e.g., 28-31 in LLaMA 3.1 8B), where final meaning is consolidated, resulting in a sharp drop in prediction confidence.
- **Core assumption:** Layer-wise next-token log probability is a reliable proxy for a model's internal reasoning confidence and semantic integration.
- **Evidence anchors:**
  - [abstract] "...reduce late-layer reasoning confidence by 50–80%."
  - [Section 5.1] "In Obfus FOL, the obfuscated questions show over 80% suppression of the confidence rise observed in the base question."
  - [corpus] Evidence is internal; corpus does not provide external validation for this mechanism.

### Mechanism 3: Cognitive Load from De-obfuscation
- **Claim:** The process of untangling obfuscated syntax consumes model capacity, leaving fewer resources for the core logical inference.
- **Mechanism:** Problems with indirect relational chains or symbolic substitutions add a "parsing" or "translation" load. The model must first resolve this surface complexity to access the underlying simple logic. This added task competes for computational resources, leading to failures even on simple problems.
- **Core assumption:** LLMs have a finite "cognitive budget" per forward pass, and parsing load directly reduces available reasoning capacity.
- **Evidence anchors:**
  - [abstract] "...current LLMs parse questions without deep understanding..."
  - [Section 5, Results] Performance drops on Obfus FOL (GPT-5: 98% to 56%) and Obfus Number Series (GPT-4o: 87.3% drop).
  - [corpus] Assumption-based; no direct corpus evidence for a "cognitive budget" mechanism.

## Foundational Learning

- **Concept: Logical Equivalence and Obfuscation**
  - **Why needed here:** This is the principle behind Logifus. Understanding that `P → Q` is equivalent to `¬Q → ¬P` or `¬(P ∧ ¬Q)` is essential for interpreting benchmark validity.
  - **Quick check question:** Is "The ground does not stay dry whenever it rains" logically equivalent to "If it rains, the ground gets wet"?

- **Concept: Layer-wise Analysis in Transformers**
  - **Why needed here:** This is the key interpretability tool. It involves tracing a signal (e.g., log probability of the correct answer token) through the network's layers to identify where reasoning falters.
  - **Quick check question:** If a model's confidence for a correct token is high in early layers but drops sharply in the final layers, what might this indicate about its internal process?

- **Concept: Memorization Sensitivity (e.g., Min-K%++)**
  - **Why needed here:** This metric is used to quantify the claim that models switch to "memorization mode" under stress. It measures how much a model's predictions depend on data seen during training.
  - **Quick check question:** A high Min-K%++ score suggests a model is relying more on which of the two: A) Generalizable reasoning rules or B) Memorized patterns from its training set?

## Architecture Onboarding

- **Component map:** Logifus -> LogiQAte Benchmark -> Evaluation Suite (Behavioral + Mechanistic)
- **Critical path:**
  1. Take a standard reasoning question (e.g., from FOLIO)
  2. Apply a Logifus obfuscation (e.g., De Morgan's laws) to create a syntactically different but logically identical variant
  3. Verify the variant's equivalence with Prover9 and human review
  4. Evaluate an LLM on both the base and obfuscated versions
  5. Analyze: Compare accuracy drop. Run mechanistic analysis by computing Min-K%++ scores and layer-wise log probabilities for the correct answer token
- **Design tradeoffs:**
  - **Obfuscation Strength vs. Interpretability:** Strong obfuscation (e.g., MD5) more clearly isolates reasoning but makes qualitative error analysis difficult
  - **Automation vs. Human Verification:** Relying on Prover9 is scalable but limited to formal logic; human annotators are costlier but more flexible
- **Failure signatures:**
  - **Behavioral:** A performance drop of >20% on obfuscated questions
  - **Mechanistic (Memorization):** Min-K%++ AUROC score rising from ~50% to >80%
  - **Mechanistic (Confidence):** A 50-80% suppression of late-layer (e.g., layers 28-31) next-token log probability
- **First 3 experiments:**
  1. Reproduce Main Result: Run GPT-4o on the full LogiQAte benchmark under zero-shot, few-shot, and CoT settings to confirm the ~47% average performance degradation
  2. Ablation by Task: Test a smaller open-weights model (e.g., Llama-4-Maverick-17B) on each of the four tasks separately to identify which reasoning types are most vulnerable to obfuscation
  3. Mechanistic Probe: Using LLaMA 3.1 8B, run 20 questions from the "Obfus Blood Relation" task and plot the layer-wise log probability for the correct relationship token to visualize the late-layer confidence collapse described in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-tuning or training on logically obfuscated examples improve LLM robustness to logical reformulation without sacrificing standard task performance?
- **Basis in paper:** [inferred] The paper demonstrates brittleness under obfuscation but does not investigate remediation strategies. The authors state current models "parse questions without deep understanding" but do not test whether exposure to obfuscated variants during training could build genuine logical comprehension.
- **Why unresolved:** No experiments were conducted to test whether training interventions could close the 20-50% performance gap between base and obfuscated questions.
- **What evidence would resolve it:** A study comparing models fine-tuned on Logifus-obfuscated data versus standard data, measuring both base and obfuscated task performance, ideally showing transfer to held-out obfuscation types.

### Open Question 2
- **Question:** Do the memorization sensitivity and layer-wise confidence degradation patterns observed in LLaMA 3.1 8B generalize across different model architectures and scales?
- **Basis in paper:** [explicit] The authors note in Section 5.1 that mechanistic analysis "experiments are conducted on LLaMA 3.1 8B (due to budget constraints)." The 50-80% late-layer confidence suppression and increased AUROC for memorization remain untested in other architectures.
- **Why unresolved:** Resource constraints limited mechanistic probing to a single open-weights model; closed-source reasoning models (GPT-5, o4-mini) could not be analyzed at the layer level.
- **What evidence would resolve it:** Replication of the Min-K%++ memorization analysis and layer-wise log-probability tracking across diverse architectures (e.g., Mistral, Qwen, Gemini) and parameter scales.

### Open Question 3
- **Question:** Does logical obfuscation robustness transfer across languages, particularly to low-resource languages with different syntactic structures for expressing logical relations?
- **Basis in paper:** [explicit] The Limitations section states: "it is currently limited to tasks in English. In future work, we plan to expand the dataset to include multilingual data, with a particular emphasis on low-resource languages."
- **Why unresolved:** All 1,108 LogiQAte questions are English-only; languages with different kinship terminology (e.g., distinct words for maternal vs. paternal uncle) or logical connectives may exhibit different failure modes.
- **What evidence would resolve it:** A multilingual LogiQAte extension showing whether performance degradation under obfuscation correlates with language resource level or syntactic distance from English.

### Open Question 4
- **Question:** What architectural or training modifications could enable LLMs to maintain invariance to logically equivalent reformulations?
- **Basis in paper:** [explicit] The conclusion highlights "the urgency of building models that genuinely comprehend and preserve meaning beyond surface form." The memorization analysis (AUROC rising from 50% to 82%) suggests current training objectives reward pattern matching over logical equivalence.
- **Why unresolved:** The paper diagnoses the problem (surface-level parsing, pattern reliance) but does not propose or evaluate interventions at the architecture or objective-function level.
- **What evidence would resolve it:** Experiments with contrastive objectives that explicitly penalize different outputs for logically equivalent inputs, or architectural modifications (e.g., explicit logical representation layers) evaluated on LogiQAte.

## Limitations
- The evaluation's internal validity rests heavily on the Logifus obfuscation algorithms correctly preserving logical equivalence across all transformations, which is difficult to fully verify automatically for complex transformations
- The benchmark's diversity is constrained by the four selected reasoning domains, which may not fully represent the breadth of logical reasoning patterns in real-world use
- The mechanistic analysis depends on layer-wise confidence as a proxy for reasoning quality, which assumes that log probability patterns directly reflect semantic understanding rather than superficial artifacts of the architecture

## Confidence
- **High Confidence:** The core behavioral finding that LLM performance degrades under logical obfuscation (28.8% average drop) is well-supported by the experimental results across six different models and multiple reasoning tasks. The memorization sensitivity analysis showing increased reliance on training patterns is also robust.
- **Medium Confidence:** The specific mechanism that late-layer transformer blocks are uniquely disrupted by obfuscation is supported by layer-wise analysis but could potentially be explained by other architectural factors. The "cognitive load" hypothesis for why obfuscation fails is plausible but not directly tested.
- **Low Confidence:** The exact relationship between Min-K%++ scores and memorization versus reasoning remains somewhat unclear, as the metric's interpretation in this context is novel and not extensively validated in prior work.

## Next Checks
1. Test the LogiQAte benchmark on a non-transformer architecture (e.g., Mamba or RWKV) to determine whether the late-layer confidence collapse is specific to transformers or a more general phenomenon of obfuscated reasoning
2. Systematically disable each of the four Logifus obfuscation algorithms to identify which transformations cause the most severe performance degradation, providing clearer insight into which aspects of logical structure are most challenging for LLMs
3. Have humans solve the obfuscated questions to establish a ceiling for what performance degradation is due to the obfuscation itself versus fundamental limitations in LLM reasoning capabilities