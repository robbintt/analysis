---
ver: rpa2
title: Enhancing Interpretability for Vision Models via Shapley Value Optimization
arxiv_id: '2512.14354'
source_url: https://arxiv.org/abs/2512.14354
tags:
- shapley
- value
- vision
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of enhancing interpretability in
  deep neural networks while maintaining performance. It introduces a novel self-explaining
  framework that integrates Shapley value estimation as an auxiliary task during training,
  allowing for fair allocation of prediction scores to image patches.
---

# Enhancing Interpretability for Vision Models via Shapley Value Optimization

## Quick Facts
- arXiv ID: 2512.14354
- Source URL: https://arxiv.org/abs/2512.14354
- Authors: Kanglong Fan; Yunqiao Yang; Chen Ma
- Reference count: 10
- Primary result: Novel self-explaining framework using Shapley value optimization achieves state-of-the-art interpretability without performance loss across multiple vision tasks

## Executive Summary
This paper addresses the challenge of enhancing interpretability in deep neural networks while maintaining performance. The authors propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, enabling fair allocation of prediction scores to image patches. By optimizing the model to produce Shapley values alongside predictions, the approach achieves state-of-the-art interpretability metrics without compromising performance. The method demonstrates superior explanation faithfulness and segmentation performance compared to existing methods across image classification, object localization, and image-text matching tasks.

## Method Summary
The paper introduces a self-explaining framework that incorporates Shapley value estimation as an auxiliary task during training. The approach divides input images into patches and optimizes the model to predict both the final classification and the Shapley values for each patch, which represent their contribution to the prediction. This is achieved by sampling perturbations of the input image and calculating marginal contributions, then using these to train the model to produce interpretable explanations alongside its predictions. The method employs a trade-off parameter to balance performance and interpretability, allowing for flexible application across different vision tasks while maintaining state-of-the-art results.

## Key Results
- Achieves state-of-the-art interpretability metrics without compromising performance on image classification, object localization, and image-text matching tasks
- Superior explanation faithfulness and segmentation performance compared to existing interpretability methods
- Successfully demonstrates the trade-off between performance and interpretability through adjustable parameters

## Why This Works (Mechanism)
The framework works by leveraging cooperative game theory principles through Shapley values to provide fair attribution of prediction scores to image patches. By treating patches as players in a cooperative game where the prediction is the payoff, the Shapley value calculation ensures that each patch's contribution is fairly distributed based on its marginal contribution across all possible coalitions. The self-explaining nature of the framework, where the model is trained to produce both predictions and their corresponding Shapley values, creates an intrinsic alignment between the model's decision-making process and its explanations. This integrated approach eliminates the need for post-hoc explanation methods and ensures that the explanations are faithful to the model's actual reasoning process.

## Foundational Learning

**Shapley Values** - A solution concept in cooperative game theory that fairly distributes gains or costs among players. Needed because it provides a mathematically principled way to attribute model predictions to input features. Quick check: Verify that the Shapley value calculation satisfies the four axioms (efficiency, symmetry, dummy player, and additivity).

**Cooperative Game Theory** - Mathematical framework for analyzing situations where groups of players can form coalitions to achieve mutual benefits. Needed because it provides the theoretical foundation for attributing model predictions to input features. Quick check: Ensure understanding of characteristic functions and coalition formation in the context of image patches.

**Attention Mechanisms** - Neural network components that allow models to focus on specific parts of the input when making predictions. Needed because the proposed method leverages attention-like mechanisms to weigh patch contributions. Quick check: Review how attention weights are computed and normalized in transformer-based architectures.

## Architecture Onboarding

**Component Map:**
Input Image -> Patch Division -> Shapley Value Estimator -> Prediction Head -> Final Output
                        â†“
                    Performance-Interpretability Trade-off Parameter

**Critical Path:**
The critical path involves patch division, Shapley value estimation through sampling perturbations, and the joint optimization of both prediction and explanation tasks. The Shapley value estimator samples different patch combinations to calculate marginal contributions, which are then used to train the model to produce both accurate predictions and faithful explanations.

**Design Tradeoffs:**
The main tradeoff is between computational complexity and explanation quality, as Shapley value estimation requires sampling multiple perturbations. The choice of patch size affects both the granularity of explanations and computational efficiency. The performance-interpretability trade-off parameter allows users to prioritize either accurate predictions or detailed explanations based on application needs.

**Failure Signatures:**
The method may struggle with very small or large patch sizes - small patches may lead to noisy explanations while large patches may miss important details. Computational complexity increases significantly with image size and number of patches. The method's performance may degrade if the sampling strategy for Shapley value estimation is insufficient to capture all relevant patch interactions.

**First Experiments to Run:**
1. Ablation study varying patch sizes to find the optimal balance between explanation granularity and computational efficiency
2. Comparison of different sampling strategies for Shapley value estimation to minimize computational cost while maintaining explanation quality
3. Testing the performance-interpretability trade-off parameter across different datasets to establish general guidelines for parameter selection

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity due to Shapley value estimation requiring sampling perturbations, which can be expensive for large images or models
- Performance depends on patch size and aggregation strategy, requiring task-specific tuning
- Evaluation metrics for interpretability remain subjective and may not fully capture real-world explanation quality

## Confidence

**Claims Assessment:**
- Claims about maintaining performance while adding interpretability: High confidence, supported by quantitative results across multiple tasks
- Claims about superior explanation faithfulness: Medium confidence, based on standard metrics but limited to specific datasets
- Claims about general applicability to different vision tasks: Medium confidence, demonstrated on three tasks but not exhaustively validated

## Next Checks

1. Test the method on out-of-distribution datasets to verify robustness of both predictions and explanations
2. Conduct ablation studies on patch size, sampling strategy, and trade-off parameters to understand their impact on interpretability-performance balance
3. Compare explanation quality using human evaluation studies to complement automated metrics and assess real-world usefulness