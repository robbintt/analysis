---
ver: rpa2
title: A Graphical Approach to State Variable Selection in Off-policy Learning
arxiv_id: '2501.00854'
source_url: https://arxiv.org/abs/2501.00854
tags:
- assumption
- state
- policy
- walk
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unifying graphical framework for causal identification
  in sequential decision-making problems, encompassing both Dynamic Treatment Regimes
  (DTRs) and Markov Decision Processes (MDPs). The key contribution is a set of graphical
  identification criteria based on acyclic directed mixed graphs (ADMGs) that extend
  the backdoor criterion to dynamic settings and relax the memorylessness assumption
  common in MDPs.
---

# A Graphical Approach to State Variable Selection in Off-policy Learning

## Quick Facts
- arXiv ID: 2501.00854
- Source URL: https://arxiv.org/abs/2501.00854
- Reference count: 6
- This paper provides a unifying graphical framework for causal identification in sequential decision-making problems, encompassing both Dynamic Treatment Regimes (DTRs) and Markov Decision Processes (MDPs).

## Executive Summary
This paper introduces a unifying graphical framework for causal identification in off-policy learning using acyclic directed mixed graphs (ADMGs). The authors establish graphical criteria—nestedness, memorylessness, and dynamic back-door—that extend Pearl's back-door criterion to dynamic settings and relax the memorylessness assumption common in MDPs. The framework bridges DTR and RL literatures by showing how their differing assumptions are special cases of a general graphical condition. A simulation study of dynamic pricing in container logistics demonstrates that violations of these criteria can lead to suboptimal policies, with policy iteration using incorrect state selections performing worse than the null policy.

## Method Summary
The paper develops a causal graphical framework for off-policy learning where the state selection problem is formalized through three key assumptions on an ADMG: nestedness, memorylessness, and a dynamic back-door criterion. The authors prove that under these conditions, the joint distribution of rewards, states, and actions under an adaptive policy can be identified from observational data, allowing for policy evaluation and optimization. The method is evaluated through a simulation study of dynamic pricing in container logistics, where Policy Iteration is applied to data generated from different structural equation models representing various scenarios of state selection violations.

## Key Results
- Under three assumptions (nested states, memorylessness, and dynamic back-door), the joint distribution of rewards, states, and actions under an adaptive policy can be identified from observational data.
- Violations of these criteria can lead to suboptimal policies, with policy iteration using incorrect state selections performing worse than the null policy.
- The framework unifies DTR and MDP approaches by showing their differing assumptions are special cases of a general graphical condition.
- Partially observed MDPs (POMDPs) are generally not identified due to violations of memorylessness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-policy policy value identification is possible when the chosen state variables satisfy nestedness, memorylessness, and a dynamic back-door criterion.
- Mechanism: An adaptive policy modifies the natural data-generating process. Under the three core graphical assumptions, the joint density of rewards, states, and actions under the policy can be re-expressed solely using the observed distribution and the known policy density, allowing recursive computation of the expected reward (g-formula).
- Core assumption: The causal system is correctly modeled by an ADMG, and dynamic consistency plus positivity hold.
- Evidence anchors:
  - [abstract] Mentions "graphical identification criteria... using acyclic directed mixed graphs (ADMGs)" and three key assumptions.
  - [section] Theorem 1 formally states the identification formula under Assumptions 1, 2, 3, 4, 5.
  - [corpus] The corpus lacks direct validation of this specific ADMG framework.
- Break condition: If the true causal graph contains unblocked confounding paths between actions and future rewards that cannot be blocked by any feasible state set, identification fails.

### Mechanism 2
- Claim: Incorrect state variable selection can cause standard policy iteration to converge to policies with negative regret relative to the null policy.
- Mechanism: Policy iteration relies on the Markov property and unconfoundedness to correctly estimate state-action value functions. If the state omits a relevant historical variable or includes a collider, the estimated transition dynamics and reward expectations are biased, leading the algorithm to optimize based on a distorted model of the world.
- Core assumption: The policy iteration algorithm is applied to data generated from a fixed behavior policy on a system that violates the paper's identifiability assumptions.
- Evidence anchors:
  - [abstract] "violations of these criteria can lead to suboptimal policies... showing that policy iteration using incorrect state selections can perform worse than the null policy."
  - [section] Figure 3 visually demonstrates negative regret in scenarios, and Table 1 reports negative percentage regret.
  - [corpus] Corpus evidence on this specific negative regret phenomenon for state selection is weak/absent.
- Break condition: If the violated assumption has negligible impact on the specific dynamics, the performance degradation may be minor.

### Mechanism 3
- Claim: The framework unifies DTR and MDP approaches by showing their differing assumptions are special cases of a general graphical condition.
- Mechanism: The DTR literature typically assumes "sequential ignorability" with a state containing the full history, while RL assumes a pre-specified, memoryless state. This paper generalizes sequential ignorability into the "dynamic back-door" criterion and relaxes the full-history state requirement using the "memorylessness" criterion, provided they hold for a reduced state set.
- Core assumption: Both DTR and MDP literatures are valid but partial perspectives on a general causal off-policy learning problem.
- Evidence anchors:
  - [abstract] "encompassing both Dynamic Treatment Regimes (DTRs) and Markov Decision Processes (MDPs)... extends Pearl's back-door criterion to dynamic settings."
  - [section] Section 4 and Section 5 explicitly map DTR's sequential ignorability and MDP's implicit assumptions to the graphical framework.
  - [corpus] Corpus lacks comparative unification frameworks.
- Break condition: The unification holds for standard DTR/MDP settings but may require extension for non-standard problems like POMDPs with infinite memory.

## Foundational Learning

- Concept: **m-separation in Acyclic Directed Mixed Graphs (ADMGs)**
  - Why needed here: This is the formal tool used to define "memorylessness" and "unconfoundedness" in the presence of unobserved confounders. It extends the d-separation concept from DAGs.
  - Quick check question: Given an ADMG with vertices $A, B, C$ and edges $A \rightarrow B \leftarrow C$ and $A \leftrightarrow C$, are $A$ and $C$ m-separated given $B$? (Answer: No, because $B$ is a collider and the bidirected edge is an unblocked confounding path.)

- Concept: **g-formula (Robins' g-formula)**
  - Why needed here: This is the standard identification formula for dynamic treatment regimes. The paper's main theorem provides a generalized g-formula that accounts for state abridgment.
  - Quick check question: In a simple two-step treatment setting with observed confounder $L$, action $A$, and outcome $Y$, what does the g-formula for $E[Y(g)]$ look like under sequential ignorability?

- Concept: **Policy Iteration**
  - Why needed here: This is the primary algorithm evaluated in the paper's simulation study. It works by alternating between policy evaluation and policy improvement.
  - Quick check question: In a simple gridworld MDP, describe one iteration of the policy improvement step assuming a current policy $\pi$ and value function $V^\pi$.

## Architecture Onboarding

- Component map:
  1. Causal ADMG Specification -> 2. State Candidate Set -> 3. Graphical Validator -> 4. Policy Learning Algorithm -> 5. Simulation Environment

- Critical path: Causal ADMG Specification -> State Candidate Selection -> Graphical Validator (PASS/FAIL) -> Policy Learning Algorithm (if PASS) -> Evaluation.

- Design tradeoffs:
  - State Completeness vs. Efficiency: A state containing the full history always satisfies Assumptions 1 and 2 but may be statistically inefficient and computationally expensive. The paper provides criteria for a smaller, valid state.
  - Model Fidelity vs. Identifiability: A more detailed ADMG might reveal confounding that makes identification impossible, whereas a simplified graph might permit identification but yield biased results if the simplification is wrong.

- Failure signatures:
  - Negative Regret: The learned policy performs worse than the baseline null policy.
  - Violation of Memorylessness: The learned policy ignores a variable that influences future rewards.
  - Violation of Back-door: The learned policy fails to account for confounding, leading to biased value estimates.

- First 3 experiments:
  1. **State Selection Audit:** Implement the "Graphical Validator" logic for the dynamic pricing ADMG. Manually propose different state sets and verify which sets pass all three assumptions for the basic graph and which fail for scenarios $\vec{G}_1$, $\vec{G}_2$, $\vec{G}_3$.
  2. **Baseline Policy Iteration:** Replicate the simulation for the basic scenario using the valid state set. Confirm that the learned policy achieves positive regret over the null policy, validating the pipeline.
  3. **Ablation on Assumption Violations:** Run the simulation for scenario $\vec{G}_2$ using two different state sets: (a) the invalid set and (b) the corrected set that includes the omitted variable. Compare the resulting regrets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can algorithms be developed to automate state variable selection in off-policy learning, particularly in an online setting?
- Basis in paper: [explicit] The authors state in Section 7 that it is a "possible avenue of future work" to develop algorithms for state variable selection, specifically highlighting the interest in online RL algorithms that can "select state variables and improve the policy at the same time."
- Why unresolved: The paper establishes the theoretical criteria for a state to be valid but provides no algorithmic method for searching the variable space to find such a set during policy learning.
- What evidence would resolve it: An algorithm that converges on a valid state set and optimal policy simultaneously, accompanied by theoretical guarantees of convergence and identification.

### Open Question 2
- Question: How do the graphical identifiability criteria change when natural counterfactuals are accessible?
- Basis in paper: [explicit] Section 7 lists "extend[ing] the graphical identifiability criteria when we can also access the natural counterfactuals" as an "interesting problem," citing recent attention to this setting.
- Why unresolved: The current framework assumes standard observational data; the graphical implications of utilizing natural counterfactuals within this dynamic ADMG setting are unexplored.
- What evidence would resolve it: A modified set of graphical conditions or an extension of Theorem 1 that incorporates natural counterfactuals to relax current assumptions like the dynamic back-door criterion.

### Open Question 3
- Question: Can valid state variables be identified when the underlying causal ADMG is unknown or must be learned from data?
- Basis in paper: [inferred] The framework relies on verifying assumptions against a known ADMG, but in real-world applications, the "true" graph structure is often uncertain.
- Why unresolved: The paper proves that incorrect state selection leads to suboptimal policies but does not address the sensitivity of the identification to graph misspecification or the feasibility of structure learning in this context.
- What evidence would resolve it: Theoretical bounds on policy error relative to graph estimation error, or a unified method that learns the graph structure and identifies the state variables concurrently.

## Limitations
- The framework's practical utility is limited by the difficulty of specifying the true causal ADMG in real-world domains where the structure is unknown.
- The simulation study uses a relatively simple synthetic environment, and the robustness of the findings to more complex, high-dimensional, and noisy real-world data remains an open question.
- The framework does not address the sensitivity of the identification to graph misspecification or provide methods for structure learning in this context.

## Confidence

- **High Confidence:** The core theoretical contribution of providing a unifying graphical framework for state variable selection in off-policy learning. The paper's identification criteria (nestedness, memorylessness, dynamic back-door) are formally defined and logically consistent.
- **Medium Confidence:** The claim that violations of these criteria lead to negative regret in policy iteration. The simulation study supports this, but the specific magnitude of the degradation may be sensitive to the chosen environment and policy iteration implementation details.
- **Low Confidence:** The practical utility of the framework for state selection in highly complex, partially observed, or dynamic environments where the ADMG is difficult to specify accurately.

## Next Checks

1. **Cross-Environment Robustness:** Apply the state selection framework to a different off-policy learning domain (e.g., medical treatment scheduling or robotics) with a known causal structure. Verify if the graphical criteria correctly identify valid state variables and if violations lead to performance degradation.

2. **Sensitivity to ADMG Specification:** Systematically perturb the assumed ADMG (e.g., add/remove edges, change edge directions) and assess the impact on the validity of the state selection and the resulting policy performance. This would quantify the framework's robustness to model misspecification.

3. **Real-World Data Application:** Apply the framework to a real-world dataset (e.g., from the Contextual Bandit Literature) where the causal structure is partially known. Compare the policies learned using the graphical framework to those learned using standard RL methods that ignore causal structure.