---
ver: rpa2
title: 'Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted
  Supervised Fine-Tuning'
arxiv_id: '2601.08146'
source_url: https://arxiv.org/abs/2601.08146
tags:
- circuit
- tuning
- heads
- depth
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of adapting large language models
  (LLMs) to low-resource languages, where labeled data is scarce and full-model fine-tuning
  can be unstable or cause catastrophic forgetting. The authors propose Circuit-Targeted
  Supervised Fine-Tuning (CT-SFT), a method that identifies a small set of task-relevant
  attention heads in a higher-resource language model using a modified Contextual
  Decomposition for Transformers (CD-T) procedure.
---

# Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via Circuit-Targeted Supervised Fine-Tuning

## Quick Facts
- **arXiv ID**: 2601.08146
- **Source URL**: https://arxiv.org/abs/2601.08146
- **Reference count**: 33
- **Primary result**: Circuit-Targeted SFT improves cross-lingual accuracy over full fine-tuning while updating only a small fraction of parameters and reducing catastrophic forgetting.

## Executive Summary
This paper addresses the challenge of adapting large language models to low-resource languages, where labeled data is scarce and full-model fine-tuning can be unstable or cause catastrophic forgetting. The authors propose Circuit-Targeted Supervised Fine-Tuning (CT-SFT), which identifies task-relevant attention heads in a higher-resource language model and adapts the model to the low-resource language by updating only these heads (plus LayerNorm). CT-SFT uses head-level gradient masking to restrict learning to discovered circuits, enabling efficient adaptation while preserving source-language competence.

## Method Summary
CT-SFT identifies task-relevant attention heads using a modified Contextual Decomposition for Transformers (CD-T) procedure, which quantifies the contribution of each head to task performance. The method then adapts the model to the low-resource language by updating only these heads and LayerNorm parameters, using head-level gradient masking to restrict learning to the discovered circuit. This approach allows the model to leverage shared mechanisms across languages while minimizing parameter updates and catastrophic forgetting.

## Key Results
- On NusaX-Senti, CT-SFT achieves 0.547 accuracy on Acehnese and 0.493 on Buginese at depth 2 with 100 target-language samples, compared to 0.428 and 0.454 for full fine-tuning.
- CT-SFT substantially reduces catastrophic forgetting, preserving source-language competence near the competence-tuning baseline (e.g., Indonesian accuracy remains near 0.757).
- An "editing-preserving trade-off" is observed: harder transfers benefit from editing circuit heads, while easier transfers favor preserving the source mechanism by updating near-zero relevance heads.

## Why This Works (Mechanism)
CT-SFT works by identifying and leveraging shared mechanisms across languages through circuit-targeted adaptation. By updating only task-relevant attention heads, the method preserves the overall model architecture while adapting to the specific needs of the low-resource language. This approach reduces the risk of catastrophic forgetting and enables efficient adaptation with limited data.

## Foundational Learning
- **Contextual Decomposition for Transformers (CD-T)**: A method for quantifying the contribution of each attention head to task performance, used to identify task-relevant circuits. *Why needed*: Enables identification of shared mechanisms across languages. *Quick check*: Verify that CD-T scores correlate with head importance for cross-lingual transfer.
- **Gradient masking**: A technique for restricting parameter updates to specific parts of the model during fine-tuning. *Why needed*: Allows targeted adaptation while preserving source-language competence. *Quick check*: Confirm that gradient masking correctly restricts updates to identified circuits.
- **Catastrophic forgetting**: The phenomenon where fine-tuning a model on new tasks causes it to forget previously learned knowledge. *Why needed*: Motivates the need for parameter-efficient adaptation methods. *Quick check*: Compare source-language accuracy before and after CT-SFT adaptation.

## Architecture Onboarding
- **Component map**: CD-T analysis -> Head relevance scoring -> Gradient masking -> Parameter updates
- **Critical path**: Identify task-relevant heads in source language → Apply gradient masking to restrict updates → Fine-tune on target language data
- **Design tradeoffs**: Parameter efficiency vs. adaptation flexibility; source-language preservation vs. target-language improvement
- **Failure signatures**: Poor cross-lingual transfer indicates incorrect head identification; catastrophic forgetting indicates insufficient source-language preservation
- **First experiments**:
  1. Ablate individual identified heads in source language to verify their importance
  2. Test CT-SFT on a broader set of language pairs and tasks
  3. Compare CT-SFT performance to other parameter-efficient fine-tuning methods

## Open Questions the Paper Calls Out
None

## Limitations
- The method's generalizability to other low-resource languages and task types is not fully established.
- The study does not compare CT-SFT against other parameter-efficient fine-tuning methods like LoRA or prefix tuning.
- The choice of "depth 2" as the optimal circuit depth is based on empirical performance curves without theoretical justification.

## Confidence
- **High Confidence**: CT-SFT achieves better cross-lingual accuracy than full fine-tuning in low-resource settings and substantially reduces catastrophic forgetting.
- **Medium Confidence**: The "editing-preserving trade-off" hypothesis is supported by qualitative trends but lacks rigorous quantification.
- **Low Confidence**: The assertion that identified circuits are truly "task-relevant" across languages is inferred from performance gains but not directly validated.

## Next Checks
1. Conduct an ablation study on head selection to verify the causal importance of identified heads for cross-lingual transfer.
2. Evaluate CT-SFT on a broader set of low-resource languages and tasks, and compare its performance to other parameter-efficient fine-tuning methods.
3. Systematically test whether the optimal circuit depth generalizes across different language pairs and tasks, and investigate dynamic depth selection methods.