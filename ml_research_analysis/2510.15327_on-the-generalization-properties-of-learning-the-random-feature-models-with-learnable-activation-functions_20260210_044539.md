---
ver: rpa2
title: On the Generalization Properties of Learning the Random Feature Models with
  Learnable Activation Functions
arxiv_id: '2510.15327'
source_url: https://arxiv.org/abs/2510.15327
tags:
- features
- number
- have
- learning
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the generalization properties of Random Feature
  models with Learnable Activation Functions (RFLAF). The key idea is to apply data-dependent
  sampling schemes to generate features more efficiently.
---

# On the Generalization Properties of Learning the Random Feature Models with Learnable Activation Functions

## Quick Facts
- arXiv ID: 2510.15327
- Source URL: https://arxiv.org/abs/2510.15327
- Reference count: 40
- Primary result: RFLAF with leverage weighted sampling achieves same performance as plain sampling with significantly fewer random features

## Executive Summary
This paper studies the generalization properties of Random Feature models with Learnable Activation Functions (RFLAF). The key contribution is applying data-dependent sampling schemes, specifically leverage weighted sampling using ridge leverage scores, to generate features more efficiently. The authors provide sharp theoretical bounds showing that the required number of random features for both regression and classification tasks improves significantly compared to uniform sampling. Empirically, the weighted RFLAF achieves the same performance as plainly sampled RFLAF with significantly fewer features, validating the theoretical findings.

## Method Summary
The method combines learnable activation functions with data-dependent feature sampling. The RFLAF model uses basis functions (RBF or B-splines) with learnable parameters to create an approximate kernel. The algorithm first finds an approximate kernel by solving a low-rank matrix sensing problem, then computes ridge leverage scores from this kernel to perform weighted sampling. This "find-then-sample" procedure decouples feature approximation error from generalization error. The final model uses weighted features sampled according to their importance scores, achieving the same performance as plain sampling with fewer features.

## Key Results
- In MSE loss regression, feature count bound improves from Ω(1/ϵ²) to Ω((1/ϵ)^(1/t)) for general t ≥ 1, and to Ω(1) for finite-rank Gram matrices
- For Lipschitz loss classification, bound improves from Ω(1/ϵ²) to Ω((1/ϵ²)^(1/t))
- Empirical results show LWS achieves same test loss as PS with 10-30× fewer features on Protein and CIFAR-10 datasets
- The theoretical framework provides the sharpest known bounds on required number of features for RFLAF models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using data-dependent leverage weighted sampling allows RFLAF to achieve low excess risk with significantly fewer random features compared to uniform sampling.
- **Mechanism:** Instead of sampling random features uniformly, the algorithm samples from a modified distribution q(w) ∝ l_λ(w), where l_λ(w) is the ridge leverage score. This prioritizes features that align closely with the data geometry and target function, reducing the "effective dimension" required for approximation.
- **Core assumption:** The effective dimension d_λ (derived from leverage scores) is significantly smaller than the ambient dimension, and leverage scores can be accurately approximated.
- **Evidence anchors:** Abstract states bounds improve from Ω(1/ϵ²) to Ω((1/ϵ)^(1/t)); Corollary 3.3 shows s = Ω(d_eK log d_eK) with leverage score sampling.
- **Break condition:** If ridge leverage scores are uniform (e.g., eigenvalues decay very slowly), d_λ becomes large, negating weighted sampling advantage.

### Mechanism 2
- **Claim:** Learning activation function parameters independently from output weights in a "find-then-sample" procedure effectively decouples feature approximation error from generalization error.
- **Mechanism:** The algorithm first solves a low-rank matrix sensing problem to find approximate activation parameters, constructs an approximate kernel, then uses this kernel to calculate leverage scores and sample features. This separation prevents bilinear optimization complexity from exploding the feature count bound.
- **Core assumption:** The approximate kernel learned in the first step is sufficiently close to the optimal kernel such that subsequent leverage scores remain valid.
- **Evidence anchors:** Section 4 describes the algorithm as first finding an approximate kernel, then applying leverage weighted sampling; Section 3.3 notes bilinear structure increases complexity but is reflected mainly in Rademacher complexity term.
- **Break condition:** If initial optimization for activation parameters lands in a poor local minimum, resulting kernel approximation will be inaccurate, leading to incorrect leverage scores.

### Mechanism 3
- **Claim:** Generalization error is bounded by a sum of estimation error, approximation error, and optimization error, where feature count complexity is primarily driven by kernel approximation term rather than sample size term.
- **Mechanism:** The analysis bounds excess risk as 4λ + O(1/h√n) + 2ε. By tightening the bound on feature count s via weighted sampling, the overall error remains controlled even as s decreases.
- **Core assumption:** Regularization parameter λ is chosen appropriately to balance bias and variance without causing underfitting.
- **Evidence anchors:** Abstract highlights sharpest bounds on required number of features; Section 3.1 emphasizes sharper bounds on three critical parameters including feature count.
- **Break condition:** If sample size n is insufficient relative to grid number N or width h, the O(1/h√n) term dominates, making reduction in feature count irrelevant to performance.

## Foundational Learning

- **Concept:** Ridge Leverage Scores
  - **Why needed here:** This is the mathematical tool used to determine the "importance" of a specific random feature direction relative to the dataset. Understanding it is required to implement the weighted sampling scheme.
  - **Quick check question:** Can you explain why a feature direction with a high leverage score might be more valuable to sample than one with a low score?

- **Concept:** Random Feature (RF) Models
  - **Why needed here:** RFLAF is an extension of standard RF models. One must understand the baseline trade-off between number of features and computational cost/kernel approximation quality.
  - **Quick check question:** How does the computational complexity of a Random Feature model scale with the number of samples n and features s compared to a standard kernel method?

- **Concept:** Bilinear Optimization / Matrix Sensing
  - **Why needed here:** The algorithm frames learning of activation parameters as a low-rank matrix sensing problem. Familiarity with this concept is necessary to understand the non-convex optimization step.
  - **Quick check question:** In a bilinear model f(x; a, v) = Σ a_i B_i(xᵀW) v, why is simultaneously solving for a and v more difficult than solving for one while holding the other fixed?

## Architecture Onboarding

- **Component map:** Input samples X → Feature pool W → Basis layer B_i → Weight matrix Q → Output linear combination
- **Critical path:** Implement Algorithm 1:
  1. Sample initial pool W
  2. Optimize approximate activation parameters using plain RF objective
  3. Compute ridge leverage scores to form sampling distribution
  4. Resample features using the weighted distribution and construct weight matrix
  5. Retrain final model using reduced feature set
- **Design tradeoffs:**
  - Accuracy vs. Speed: Increasing grid number N improves approximation error but increases Rademacher complexity
  - Approximation Cost vs. Inference Speed: Computing leverage scores adds upfront cost but reduces inference time significantly
  - Sampling Strategy: Weighted sampling provides tighter bounds than plain sampling but requires extra pre-calculation step
- **Failure signatures:**
  - Performance Collapse: Check if feature pool s is too small to accurately estimate leverage scores
  - Numerical Instability: Ensure regularization λ is not too small relative to eigenvalues of Gram matrix
  - No Gain over Baseline: Verify data spectrum exhibits fast decay; if eigenvalues are flat, leverage scores provide little benefit
- **First 3 experiments:**
  1. Replicate Figure 1: Compare LWS vs. PS on Protein dataset using RBF basis, plot Test Loss vs. Number of Features
  2. Ablation on Grid Number N: Run with N ∈ {16, 32, 64} to observe trade-off between activation approximation fidelity and generalization gap
  3. Spectrum Analysis: Visualize eigenvalues of approximate kernel to verify rapid decay justifying use of leverage scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can local Rademacher complexity analysis be applied to RFLAF to derive tighter bounds on sample size n?
- Basis in paper: Authors note that refined analysis deducing sharper bound on n remains to be explored because hypothesis class is not convex regarding joint parameters
- Why unresolved: Standard local Rademacher complexity analysis requires convexity in joint parameters (a, v), which is not present in bilinear RFLAF structure
- What evidence would resolve it: Theoretical framework extending local Rademacher complexity bounds to non-convex, bilinear function classes

### Open Question 2
- Question: Is the current dependency of sample size n on basis function width h (i.e., n = Ω(1/(ϵ²h²))) necessary for convergence?
- Basis in paper: Authors suspect bound on n is not completely tight and highlight finer analysis of sample size as potential future direction
- Why unresolved: Current bounds transfer complexity of learnable activation directly to sample size, potentially overestimating required data
- What evidence would resolve it: Theoretical proof showing sample complexity bound for RFLAF that is independent of or less sensitive to width h

### Open Question 3
- Question: Can computational overhead of initial low-rank matrix sensing step in Algorithm 1 be reduced?
- Basis in paper: Algorithm requires solving initial optimization problem to find approximate kernel before leverage weighted sampling
- Why unresolved: While paper proves statistical efficiency, pre-sampling optimization step adds computational cost that may negate speedup
- What evidence would resolve it: Algorithm capable of approximating ridge leverage scores without solving full initial regression problem, or proof of sublinear complexity of initialization step

## Limitations

- The theoretical improvements depend heavily on accurate estimation of ridge leverage scores, which may fail for datasets with slowly decaying eigenvalue spectra
- The non-convex optimization for activation parameters could get stuck in poor local minima, affecting the quality of the approximate kernel
- Limited empirical testing on very high-dimensional data where theoretical improvements should be most pronounced
- Computational overhead of initial low-rank matrix sensing step may negate speedup benefits in some regimes

## Confidence

- **Theoretical Claims:** Medium - Relies on specific assumptions about eigenvalue decay rates and kernel approximation quality that may not hold universally
- **Algorithmic Claims (Regression):** Medium-High - Protein dataset results are convincing and well-validated
- **Algorithmic Claims (Classification):** Medium - Limited testing and absence of strong baseline comparisons reduces confidence

## Next Checks

1. Test the break condition: deliberately use data with flat eigenvalue spectra to verify that weighted sampling provides no advantage over plain sampling
2. Vary the regularization parameter λ across several orders of magnitude to identify the stability threshold for leverage score computation
3. Implement a comparison against other feature selection methods (e.g., random Fourier features with explicit feature maps) to benchmark the relative efficiency of leverage weighted approach