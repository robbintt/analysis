---
ver: rpa2
title: 'From Failure to Mastery: Generating Hard Samples for Tool-use Agents'
arxiv_id: '2601.01498'
source_url: https://arxiv.org/abs/2601.01498
tags:
- tool
- hard
- data
- arxiv
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HardGen is a novel pipeline for generating challenging tool-use
  training data for LLM agents. It addresses the problem of existing data generation
  methods producing simple, homogeneous trajectories that fail to capture implicit
  logical dependencies required in real-world tasks.
---

# From Failure to Mastery: Generating Hard Samples for Tool-use Agents

## Quick Facts
- arXiv ID: 2601.01498
- Source URL: https://arxiv.org/abs/2601.01498
- Reference count: 40
- Key result: HardGen pipeline generates challenging tool-use training data, achieving 79.14% accuracy on BFCLv3 benchmark with 4B model, surpassing GPT-5.2, Gemini-3-Pro, and Claude-Opus-4.5

## Executive Summary
HardGen is a novel pipeline for generating challenging tool-use training data for LLM agents. It addresses the problem of existing data generation methods producing simple, homogeneous trajectories that fail to capture implicit logical dependencies required in real-world tasks. HardGen operates through three phases: (1) sampling failure-aware tool traces from a dynamic API Graph built on agent failure cases, (2) synthesizing advanced tools and hard queries from these traces to require multi-step reasoning and implicit dependency, and (3) refining Chain-of-Thought reasoning through a closed-loop feedback mechanism. The approach generates verifiable complex reasoning paths that target specific model weaknesses. Extensive experiments demonstrate that a 4B parameter model trained on HardGen-curated data achieves superior performance compared to several leading open-source and closed-source competitors, setting a new state-of-the-art record for its size with 79.14% accuracy on the BFCLv3 benchmark, surpassing models like GPT-5.2, Gemini-3-Pro, and Claude-Opus-4.5.

## Method Summary
HardGen generates challenging tool-use training data through a 3-phase pipeline: Phase I samples failure-aware tool traces from a dynamic API Graph built on agent failure cases; Phase II synthesizes advanced tools and hard queries that require multi-step reasoning and implicit dependency inference; Phase III refines Chain-of-Thought reasoning through a closed-loop feedback mechanism. The pipeline identifies failure APIs via self-evaluation on small models, constructs an API Graph encoding dependencies and constraints, then uses greedy sampling to prioritize tools minimizing graph distance to failure nodes. The Tool Maker abstracts executable traces into advanced tools with high-level parameters, while the Hard-query Generator creates queries that omit intermediate steps. A Reasoner-Verifier loop with K_max=3 iterations provides targeted feedback to improve CoT quality, retaining only fully verified trajectories.

## Key Results
- 4B parameter model trained on HardGen data achieves 79.14% accuracy on BFCLv3 benchmark, setting new SOTA for its size
- Surpasses leading models including GPT-5.2, Gemini-3-Pro, and Claude-Opus-4.5 on multi-turn tool-use tasks
- Ablation studies show +33% improvement over ToolACE-MT baseline and +8-10% gain from hard query generation
- Feedback-guided refinement yields +12.34% accuracy gains for 30B models over no-feedback baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeting failure cases produces harder training data than uniform sampling.
- Mechanism: Phase I evaluates baseline models on a self-evaluation query set to identify "failure APIs"—tools where both Qwen3-4B and Llama-3.2-3B produce incorrect results. These 1,204 failure APIs are structured into a dynamic API Graph encoding dependencies (D) and parameter constraints (P), then sampled via a greedy heuristic that prioritizes tools minimizing graph distance to target failure nodes.
- Core assumption: Failure patterns in small models indicate reasoning gaps that transfer to larger models and downstream tasks.
- Evidence anchors:
  - [abstract]: "HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces."
  - [section 3.2]: "A tool is designated as challenging if both models produce incorrect execution results during inference."
  - [corpus]: Weak direct corpus evidence; related work (PEARL, ToolGrad) addresses multi-hop tool use but not failure-driven sampling specifically.

### Mechanism 2
- Claim: Abstracting traces into "advanced tools" forces models to learn implicit dependency inference.
- Mechanism: The Tool Maker agent (A_T) takes an executable hard trace Γ_i and synthesizes an advanced tool T_adv that encapsulates the multi-step operation with high-level parameters (e.g., `buy_tickets_adv(cityA, cityB)` instead of explicit zipcode lookups). The Hard-query Generator (A_Q) then creates queries that omit intermediate steps, requiring the model to infer the tool chain.
- Core assumption: Queries that hide intermediate steps create "logical jumps" that cannot be solved via pattern matching alone.
- Evidence anchors:
  - [abstract]: "These traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries."
  - [section 3.3]: "Directly generating queries from execution traces, where all intermediate steps are explicitly enumerated, fails to cultivate this capability."
  - [corpus]: Related paper "Don't Adapt Small Language Models for Tools" notes schema misalignment as a failure mode, supporting the need for logical bridging.

### Mechanism 3
- Claim: Closed-loop verifier feedback improves CoT quality beyond single-pass generation.
- Mechanism: The Reasoner (A_R) attempts function calls with hints from advanced tool descriptions. When incorrect, the Verifier (A_V) diagnoses the error and generates corrective hints (Error^(k)_i) without revealing answers. The Reasoner re-attempts with augmented prompts up to K_max=3 iterations. Correct traces are retained only if all M function calls succeed.
- Core assumption: Models can self-correct reasoning when given targeted feedback, and verified traces are more valuable than unverified ones.
- Evidence anchors:
  - [abstract]: "closed-loop evaluation feedback steering the continuous refinement of the process"
  - [table 6]: Feedback-guided refinement yields +12.34% (30B), +10.21% (235B), +11.62% (671B) accuracy gains over no-feedback baselines.
  - [corpus]: PEARL uses adaptive RL for multi-hop tool use, partially supporting feedback-driven improvement.

## Foundational Learning

- Concept: **Implicit logical dependency**
  - Why needed here: The paper's core contribution is generating queries where intermediate tool calls must be inferred, not explicitly stated. Understanding this distinguishes "easy" pattern-matching tasks from "hard" reasoning tasks.
  - Quick check question: Given tools `get_zipcode(city)->zip` and `buy_tickets(zipA, zipB)`, why is "Buy tickets from city A to city B" harder than "Get zipcodes for A and B, then buy tickets"?

- Concept: **Graph-constrained sampling**
  - Why needed here: Phase I samples from an API Graph where a tool T_a is callable only if prerequisite tools D_Ta have been executed. This ensures trace executability.
  - Quick check question: If tool C requires outputs from tools A and B, what happens if a sampling policy ignores dependencies?

- Concept: **Iterative refinement with verifiers**
  - Why needed here: Phase III uses a Reasoner-Verifier loop where incorrect attempts receive targeted hints. Understanding this clarifies why K_max=3 balances quality vs. cost.
  - Quick check question: Why might a verifier hint "You're missing a dependency step" be better than "Call get_zipcode first"?

## Architecture Onboarding

- Component map: Self-evaluation query set → Failure API Set (T) → API Graph G=(T,D,P) → Hard traces Γ_i → Advanced tool T_adv → Hard query Q_hard → Reasoner attempts → Verifier feedback → Refined CoT → Verified trajectory output

- Critical path: Phase I failure detection → Phase II advanced tool synthesis → Phase III refinement → Verified trajectory output. If Phase I fails to identify meaningful failure APIs, downstream phases generate uninformative data.

- Design tradeoffs:
  - K_max=3 iterations balances correctness (89% at K=3 per Figure 5) vs. compute cost
  - Using Qwen3-30B-A3B-Thinking as backbone trades some quality for 10x faster inference vs. dense 32B models
  - Binary reward (format correct & answer correct) emphasizes structural compliance over partial credit

- Failure signatures:
  - Low multi-turn accuracy despite high single-turn suggests Phase II logical jumps are too hard or poorly specified
  - High trajectory rejection rate (>50%) indicates verifier hints may be unhelpful or K_max too low
  - Generalization gap between BFCLv3 and BFCLv4 suggests overfitting to in-distribution tool schemas

- First 3 experiments:
  1. **Baseline comparison**: Train Qwen3-4B with HardGen vs. ToolACE-MT data on BFCLv3 multi-turn subset. Expect +33% improvement per Table 3.
  2. **Ablation on hard queries**: Train with vs. without Q_hard generation. Expect +8-10% multi-turn gain per Table 4.
  3. **Verifier feedback analysis**: Compare K_max={1,2,3,4} on correctness rate. Expect diminishing returns after K=3 per Figure 5.

## Open Questions the Paper Calls Out

- **Question**: To what extent does HardGen generalize to entirely new API ecosystems or specialized domains beyond the 2,095 tools evaluated?
  - Basis in paper: [explicit] The authors state in the Limitations section: "Although our evaluation covers 2,095 tools across diverse domains, the extent to which HardGen generalizes to entirely new API ecosystems or specialized domains remains to be fully explored."
  - Why unresolved: Current evaluation focuses on existing BFCL/API-Bank benchmarks with established tool categories; systematic assessment of transfer to novel domains (e.g., medical, legal, scientific computing) has not been conducted.
  - What evidence would resolve it: Performance evaluations on held-out domain-specific API suites with substantially different structural patterns and dependency types than those in the training distribution.

- **Question**: How can HardGen's verification approach be adapted for proprietary APIs or tools with complex external dependencies that lack executable environments?
  - Basis in paper: [explicit] The authors acknowledge: "Our approach requires executable environments for verification, which may not be feasible for proprietary APIs or tools with complex external dependencies."
  - Why unresolved: The closed-loop feedback mechanism fundamentally depends on execution verification; without executable access, the refinement loop cannot validate function calls or diagnose errors.
  - What evidence would resolve it: A modified pipeline using mock environments, simulation-based verification, or formal specification checking that achieves comparable data quality without full API executability.

- **Question**: How does HardGen's effectiveness scale to models significantly larger than 4B parameters (e.g., 70B, 100B+)?
  - Basis in paper: [inferred] While the ablation study examines 0.6B–4B models showing improved gains with scale, the paper does not evaluate whether the 27,000-trajectory dataset provides sufficient coverage and diversity for training substantially larger models, nor whether failure-driven sampling remains beneficial when base capabilities differ.
  - Why unresolved: Larger models may exhibit different failure patterns and reasoning capabilities; the failure API set was identified using small models (Qwen3-4B, Llama-3.2-3B).
  - What evidence would resolve it: Training experiments on 70B+ parameter models with analysis of failure pattern transferability and dataset scaling requirements.

- **Question**: How robust is the Tool Maker's abstraction quality for highly irregular or domain-specific tool chains with non-standard dependency structures?
  - Basis in paper: [explicit] The authors note: "The abstraction quality of advanced tools relies on the Tool Maker's ability to correctly identify high-level operations from primitive tool sequences, which may occasionally produce suboptimal abstractions for highly irregular or domain-specific tool chains."
  - Why unresolved: No systematic analysis exists of abstraction failure modes or their downstream impact on training data quality and model performance.
  - What evidence would resolve it: Quantitative analysis of abstraction quality across diverse tool chain patterns, with correlation to downstream model performance degradation when suboptimal abstractions occur.

## Limitations

- **API Graph Construction**: The paper describes a dynamic API Graph built from failure cases but provides only conceptual details in Appendix G. The exact dependency edge generation, parameter constraint handling, and update mechanisms remain underspecified, limiting reproducibility without additional implementation guidance.

- **Advanced Tool Abstraction Quality**: While the approach claims to generate "logical jumps" through tool abstraction, there's no quantitative analysis of whether the synthesized advanced tools truly capture implicit dependencies versus creating unsolvable queries. The trade-off between challenge and solvability isn't empirically validated.

- **Transferability of Failure Patterns**: The core assumption that failure patterns in 4B-3B models indicate reasoning gaps that transfer to larger models lacks rigorous validation. It's unclear whether the identified failure APIs represent fundamental reasoning limitations or model-specific idiosyncrasies.

## Confidence

- **High Confidence**: The pipeline architecture (3-phase approach), the BFCLv3 benchmark results (79.14% accuracy), and the ablation studies showing HardGen's superiority over ToolACE-MT are well-supported by experimental data.

- **Medium Confidence**: The claims about HardGen generating truly "hard" samples requiring implicit logical reasoning are plausible but not rigorously proven. The mechanism descriptions are clear, but empirical validation of the logical complexity is limited.

- **Low Confidence**: The assumption that failure patterns in small models transfer to larger models, and that the generated data generalizes beyond the BFCLv3 benchmark, remain weakly supported without cross-model or cross-benchmark validation.

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate HardGen-trained models on failure cases from different base models (e.g., train on Qwen3-4B failures, test on Llama-3.2-3B or GPT-4o-mini failures) to verify that failure patterns transfer across architectures.

2. **Logical Complexity Analysis**: Conduct a qualitative study where human annotators assess whether HardGen-generated queries truly require implicit reasoning versus pattern matching, comparing against ToolACE-MT queries to validate the "logical jump" claim.

3. **Failure Pattern Transfer Validation**: Test whether the 1,204 identified failure APIs represent fundamental reasoning gaps by evaluating multiple model sizes (4B, 8B, 30B) on the same failure cases to determine if patterns are architecture-independent or model-specific.