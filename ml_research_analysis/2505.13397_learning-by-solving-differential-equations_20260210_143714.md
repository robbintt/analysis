---
ver: rpa2
title: Learning by solving differential equations
arxiv_id: '2505.13397'
source_url: https://arxiv.org/abs/2505.13397
tags:
- learning
- gradient
- methods
- adam
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores higher-order Runge-Kutta (RK) ODE solvers in
  deep learning, showing they follow gradient flow more closely than first-order methods
  like SGD/Adam, leading to more stable training. While vanilla RK4 shows competitive
  performance on simple tasks (e.g., MNIST), it struggles with generalization gaps
  in large-batch settings and increased wall-clock time when gradients don't fit in
  memory.
---

# Learning by solving differential equations

## Quick Facts
- **arXiv ID**: 2505.13397
- **Source URL**: https://arxiv.org/abs/2505.13397
- **Reference count**: 40
- **Primary result**: Higher-order Runge-Kutta ODE solvers follow gradient flow more closely than SGD/Adam, requiring modifications like preconditioning and momentum to match performance on complex tasks.

## Executive Summary
This paper explores higher-order Runge-Kutta (RK) ODE solvers in deep learning, showing they follow gradient flow more closely than first-order methods like SGD/Adam, leading to more stable training. While vanilla RK4 shows competitive performance on simple tasks (e.g., MNIST), it struggles with generalization gaps in large-batch settings and increased wall-clock time when gradients don't fit in memory. The authors propose three modifications: (1) preconditioning via a diagonal AdaGrad-like matrix to reduce ODE stiffness, (2) adaptive learning rates based on gradient magnitude changes (rescaled DAL-p), and (3) momentum applied to RK gradients. These modifications significantly improve performance—RK4 with momentum and adaptive learning rates even surpasses tuned Adam on MNIST and Fashion-MNIST. Results suggest RK methods are promising but require adaptation for complex workloads.

## Method Summary
The method applies classical 4th-order Runge-Kutta (RK4) solvers to the continuous gradient flow ODE $\dot{\theta} = -\nabla L(\theta)$ for neural network training. RK4 requires 4 gradient evaluations per step to approximate the ODE trajectory more accurately than first-order methods. To address stiffness and generalization issues, the authors introduce three modifications: diagonal preconditioning (modified AdaGrad), adaptive learning rates based on gradient change magnitude (DAL-p), and momentum applied to RK gradients. The approach is tested on image classification tasks (MNIST, Fashion-MNIST, CIFAR-10/100, ImageNet) using MLPs and CNNs.

## Key Results
- Vanilla RK4 shows competitive performance with tuned Adam on MNIST with small batches (batch size 16)
- Vanilla RK4 exhibits generalization gaps in full-batch settings, underperforming Adam
- RK4 with momentum and adaptive learning rates surpasses tuned Adam on MNIST and Fashion-MNIST
- Preconditioning helps RK4 handle stiff loss landscapes by smoothing the gradient field
- RK4 requires 4 gradient evaluations per step, increasing wall-clock time when gradients exceed memory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher-order ODE solvers (specifically Runge-Kutta) track the ideal gradient flow trajectory more closely than first-order methods (like SGD/Adam), potentially reducing training instability.
- **Mechanism:** Standard gradient descent acts as a 1st-order Euler method, accumulating local truncation error $O(h^2)$ which causes the path to deviate from the continuous steepest-descent curve. RK4 approximates the curve with $O(h^5)$ error, reducing "oscillations" and sticking closer to the true loss-minimizing path.
- **Core assumption:** The continuous gradient flow ODE $\dot{\theta} = -\nabla L(\theta)$ represents an ideal, stable trajectory that the optimizer should follow.
- **Evidence anchors:**
  - [abstract] "follow the gradient flow equation more precisely and more stably"
  - [section 2] "Gradient descent is a first-order solver... error... is of order $O(h^2)$"
  - [corpus] (Weak direct support; corpus focuses on Neural ODE solvers rather than ODE-based optimizers)
- **Break condition:** In large-batch settings, this high fidelity backfires. By perfectly tracking the flow, RK misses the "beneficial noise" and implicit regularization of 1st-order discretization errors, leading to a **generalization gap**.

### Mechanism 2
- **Claim:** Preconditioning the gradient field reduces ODE "stiffness," allowing RK methods to maintain performance in rugged loss landscapes.
- **Mechanism:** Neural loss landscapes often have high curvature (stiffness), causing gradient directions to change abruptly. Preconditioning transforms the ODE vector field using a matrix (approximating the inverse metric), smoothing the geometry and allowing the solver to take effective steps without destabilizing.
- **Core assumption:** A diagonal approximation of the accumulated gradient outer products (a modified AdaGrad) sufficiently captures the local curvature information needed to smooth the field.
- **Evidence anchors:**
  - [section 4.1] "Preconditioning... offers the flexibility to transform the gradient flow ODE into one which alleviates this stiffness problem."
  - [section 4.1] Lemma 4.1 shows loss decrease is preserved under symmetric positive definite preconditioning.
  - [corpus] (Not directly supported by corpus; corpus papers on PDE solvers focus on physics constraints rather than optimization dynamics)
- **Break condition:** If the preconditioner matrix becomes singular or estimates curvature poorly (e.g., sparse gradients), it may distort the gradient direction incorrectly.

### Mechanism 3
- **Claim:** Adaptive step sizes (specifically rescaled DAL) prevent divergence in stiff regions while maximizing progress in smooth regions.
- **Mechanism:** The adaptive rate monitors the rate of change of the gradient ($\|H(\theta)g(\theta)\|$). If the gradient changes rapidly (high stiffness), the step size $h$ is reduced to maintain stability; otherwise, it is increased.
- **Core assumption:** The local stiffness can be effectively approximated by the product of the Hessian and the gradient (or proxies thereof), and a cap is required to prevent unbounded step sizes.
- **Evidence anchors:**
  - [section 4.2] "learning rate should be inversely proportional to $\|H(\theta)g(\theta)\|$"
  - [figure 3] Shows rescaled DAL bridging the performance gap with Adam on Fashion-MNIST.
  - [corpus] (Weak support; corpus lacks specific references to DAL in optimization)
- **Break condition:** Computational overhead of estimating Hessian-vector products (if approximated poorly) or divergence if the "rescaling" cap $c$ is set too high for the specific workload.

## Foundational Learning

- **Concept: Gradient Flow ODE**
  - **Why needed here:** This paper reframes training not as discrete steps, but as sampling from a continuous Ordinary Differential Equation (ODE) where parameters flow "downhill" along the gradient. Understanding this shift is key to seeing why "better" solvers (RK) are proposed over "simple" ones (SGD).
  - **Quick check question:** Why does the paper equate standard Gradient Descent to the "Euler method"?

- **Concept: Order of a Solver (Local Truncation Error)**
  - **Why needed here:** The paper claims RK4 is "better" because it is 4th order. This refers to the mathematical precision of a single step. You must understand that higher order = exponentially lower error per step, which implies higher theoretical stability.
  - **Quick check question:** If Euler (Order 1) has error $O(h^2)$, what is the error for RK4 (Order 4)?

- **Concept: Stiffness**
  - **Why needed here:** A central theme is that RK methods struggle with "stiff" equations—where the loss landscape has sharp curves or conflicting scales. You need this concept to understand why vanilla RK fails on complex tasks and requires modifications like preconditioning.
  - **Quick check question:** In the context of this paper, does stiffness refer to the rigidity of the neural network weights or the rapid variation in the gradient vector field?

## Architecture Onboarding

- **Component map:**
  - **Multi-Stage Gradient Evaluation:** Unlike standard optimizers (1 backward pass), the RK4 optimizer requires a module to execute 4 forward/backward passes per step to compute intermediate slopes ($k_1$ to $k_4$).
  - **State Accumulator:** Requires storage for the preconditioner (running sum of squared gradients) and momentum vectors.
  - **Adaptive Controller:** Module to compute $h_{DALR}$ based on current gradient/Hessian stats (capped by constant $c$).

- **Critical path:**
  The sequential dependency of the RK stages. You cannot compute stage 2 ($k_2$) until stage 1 ($k_1$) is complete. This serial dependency prevents simple parallelization of the gradient calculations within a single step, increasing latency per step.

- **Design tradeoffs:**
  - **Compute vs. Memory:** RK4 requires 4x forward/backward passes per step. While wall-clock time is comparable to Adam if data fits in memory (IO-bound), it degrades severely if gradients must be swapped in/out of memory (Memory-bound).
  - **Stability vs. Generalization:** Vanilla RK maximizes stability (closely follows flow) but sacrifices generalization (misses beneficial noise). You must inject noise (small batches) or regularization (preconditioning/momentum) to restore it.

- **Failure signatures:**
  - **Silent Convergence to Sharp Minima:** Model trains with zero loss oscillation (perfect stability) but test accuracy is significantly worse than Adam (Generalization Gap).
  - **Wall-clock Blowup:** Step time increases drastically with batch size (unlike Adam) due to memory constraints during the multi-stage evaluation.

- **First 3 experiments:**
  1. **Vanilla RK4 Baseline:** Replace Adam with vanilla RK4 on MNIST (MLP). Tune *only* the learning rate. Goal: Verify if you can match Adam's accuracy with just the solver change (Abstract claims competitive performance).
  2. **Generalization Gap Check:** Run Vanilla RK4 on MNIST with full-batch vs. small-batch. Goal: Confirm the paper's finding that vanilla RK underperforms Adam in full-batch settings due to implicit regularization loss.
  3. **Ablation on Modifications:** Implement RK4 with only the "Modified AdaGrad" preconditioner (Eq. 12) on the full-batch task. Goal: Verify if this specific preconditioner bridges the generalization gap identified in step 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RK-based optimizers be effectively tailored or co-adapted with standard training heuristics (e.g., batch normalization, weight decay, schedules) to achieve competitive performance on complex workloads?
- Basis in paper: [explicit] The conclusion states that further investigation is needed into "how RK-based optimizers can be effectively tailored or how these heuristics might be co-adapted to allow for competitive performance on more complex workloads."
- Why unresolved: The authors focused on simple MLP workloads to isolate effects, noting that standard complex training tricks are often "detrimental" to RK methods without specific adaptation.
- What evidence would resolve it: Achieving accuracy comparable to tuned Adam or NAdamW on large-scale benchmarks like ImageNet ViT using a modified RK optimizer that integrates these heuristics.

### Open Question 2
- Question: Can the memory overhead and wall-clock time of higher-order RK methods be reduced for large-batch training where intermediate gradients exceed device memory?
- Basis in paper: [inferred] The paper notes that while RK is fast when data fits in memory, "RK’s wall-clock time sharply increases if gradient data exceeds memory capacity," making performance "prohibitive for large workloads."
- Why unresolved: The authors identify this I/O bottleneck as a major limitation but primarily benchmark scenarios where memory is sufficient or accept the time penalty.
- What evidence would resolve it: Development of a memory-efficient RK variant (e.g., using gradient checkpointing) that maintains wall-clock time parity with Adam on large-batch tasks.

### Open Question 3
- Question: Can computationally tractable implicit RK methods be developed for deep learning to handle ODE stiffness more effectively than explicit methods?
- Basis in paper: [explicit] The authors note that while "implicit RK methods generally manage stiffness better... the computational cost of solving their required non-linear systems... is often prohibitive for higher-order methods."
- Why unresolved: The paper attempts to fix stiffness via preconditioning and adaptive rates on explicit methods rather than solving the computational complexity of implicit solvers.
- What evidence would resolve it: An efficient implicit RK optimizer that converges on stiff neural network loss landscapes without the excessive per-step cost of solving non-linear systems.

## Limitations

- Memory constraints can make RK4 wall-clock time prohibitive for large-batch training when gradients exceed device memory
- Vanilla RK4 exhibits generalization gaps in full-batch settings, underperforming Adam due to missing beneficial noise
- Claims about large-scale vision tasks (CIFAR/ImageNet) are based on limited secondary experiments

## Confidence

- **High confidence**: Claims about vanilla RK4 showing competitive performance on MNIST/Fashion-MNIST with small batches, and the existence of a generalization gap in full-batch settings
- **Medium confidence**: Claims about the effectiveness of momentum and preconditioning modifications, as these are supported by ablation studies but could benefit from more extensive hyperparameter sweeps
- **Low confidence**: Claims about RK4 being "better" for large-scale vision tasks (CIFAR/ImageNet) based on the limited secondary experiments presented

## Next Checks

1. Reproduce the vanilla RK4 baseline on MNIST MLP with small (batch=16) and full (batch=60,000) batches to verify the reported generalization gap
2. Implement the RK4 momentum modification (Eq. 16) and confirm it closes the generalization gap on full-batch MNIST
3. Validate the preconditioning modification (Eq. 12) independently by applying it to a first-order method (e.g., SGD) to determine if improvements are specific to the RK framework