---
ver: rpa2
title: Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed
  Learning
arxiv_id: '2601.14942'
source_url: https://arxiv.org/abs/2601.14942
tags:
- communication
- semantic
- stage
- multi-modal
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of communication-efficient and
  robust multi-modal edge inference over bandwidth-limited wireless channels. The
  key difficulty is that distributed learning with multiple devices incurs high communication
  overhead, and inference robustness is limited when facing varying channels and noisy
  modalities.
---

# Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning

## Quick Facts
- **arXiv ID**: 2601.14942
- **Source URL**: https://arxiv.org/abs/2601.14942
- **Reference count**: 40
- **Primary result**: Achieves higher accuracy with ~10x-20x fewer training communication rounds compared to contrastive pre-training and training from scratch, while maintaining robustness to modality degradation and channel variation

## Executive Summary
This paper addresses the challenge of communication-efficient and robust multi-modal edge inference over bandwidth-limited wireless channels. The key difficulty is that distributed learning with multiple devices incurs high communication overhead, and inference robustness is limited when facing varying channels and noisy modalities. The proposed solution is a three-stage framework that leverages local self-supervised pre-training, server-side evidential fusion, and uncertainty-guided retransmission to achieve significant communication savings while maintaining high accuracy and robustness across varying conditions.

## Method Summary
The framework employs a three-stage approach to enable efficient multi-modal edge inference. First, devices perform local multi-modal self-supervised pre-training using InfoNCE loss to disentangle shared and modality-specific features without server communication. Second, the server performs supervised fine-tuning with evidential fusion to calibrate per-modality uncertainty and aggregate distorted features. Third, uncertainty-guided retransmission selectively requests additional features for uncertain samples, balancing accuracy and communication cost. The method uses evidential deep learning to model predictive uncertainty and makes decisions based on aggregated uncertainty scores from different modalities.

## Key Results
- Achieves higher accuracy than existing self-supervised and fully supervised baselines
- Reduces training communication rounds by approximately 10x compared to contrastive pre-training and more than 20x compared to training from scratch
- Maintains robustness to modality degradation and channel variation
- Demonstrates effectiveness on RGB-depth indoor scene classification task

## Why This Works (Mechanism)
The framework's effectiveness stems from three complementary mechanisms. Local self-supervised pre-training allows devices to learn useful representations without communication overhead, while evidential fusion at the server enables robust aggregation of uncertain or distorted features from different modalities. The uncertainty-guided retransmission mechanism provides an adaptive trade-off between communication cost and accuracy by selectively requesting additional information only for uncertain samples, rather than retransmitting all features.

## Foundational Learning

**Multi-modal Learning**: Understanding how to process and integrate information from multiple sensor types is essential for edge computing scenarios where devices have different sensing capabilities. Quick check: Can the model handle cases where one modality is completely missing?

**Self-supervised Learning**: InfoNCE loss enables learning useful representations without labeled data, reducing the need for extensive communication of labels between devices and server. Quick check: Does the local pre-training maintain quality when devices have limited data?

**Evidential Deep Learning**: Provides a principled way to quantify uncertainty in predictions, which is crucial for deciding when to request retransmission. Quick check: How does the uncertainty estimation behave under extreme channel conditions?

## Architecture Onboarding

**Component Map**: Device Local Pre-training -> Server Fine-tuning with Evidential Fusion -> Uncertainty-Guided Retransmission

**Critical Path**: The inference pipeline follows: feature extraction on devices → evidential fusion on server → uncertainty calculation → retransmission decision → final prediction

**Design Tradeoffs**: The framework trades increased local computation on devices (for self-supervised pre-training) against reduced communication with the server, while the uncertainty-guided retransmission adds latency for improved accuracy on uncertain samples

**Failure Signatures**: Poor performance may manifest as: high uncertainty scores across all samples (indicating model confidence issues), low accuracy despite frequent retransmissions (suggesting ineffective fusion), or communication costs approaching naive approaches (indicating ineffective retransmission filtering)

**First Experiments**: 1) Test with single modality to verify baseline performance, 2) Evaluate without uncertainty-guided retransmission to measure its contribution, 3) Assess performance under synthetic channel noise to validate robustness claims

## Open Questions the Paper Calls Out
None

## Limitations

- Evaluation is limited to a single dataset (RGB-depth indoor scene classification), raising questions about generalization to other modalities or tasks
- Communication savings comparisons rely on relative claims rather than absolute metrics, making it difficult to assess real-world impact
- The three-stage framework introduces significant complexity that may not be practical in resource-constrained edge environments

## Confidence

- **High confidence**: The theoretical framework for uncertainty-aware feature fusion and the core motivation for communication-efficient multi-modal edge inference are well-founded and clearly articulated
- **Medium confidence**: The claimed communication efficiency improvements (10x-20x reduction) are supported by experimental results but lack absolute metrics and broader validation across diverse datasets and network conditions
- **Medium confidence**: The robustness claims under varying channel conditions and noisy modalities are demonstrated on the primary dataset but require further validation across different types of degradation and wireless environments

## Next Checks

1. Evaluate the framework on diverse multi-modal datasets (e.g., audio-visual, multimodal medical imaging) to assess generalization across different modality types and tasks
2. Conduct experiments with varying network topologies and channel models, including scenarios with packet loss, variable latency, and different bandwidth constraints
3. Perform ablation studies to quantify the individual contributions of each framework component (self-supervised pre-training, evidential fusion, uncertainty-guided retransmission) to overall performance and communication efficiency