---
ver: rpa2
title: 'Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of
  Context'
arxiv_id: '2510.20229'
source_url: https://arxiv.org/abs/2510.20229
tags:
- image
- hallucinations
- hallucination
- arxiv
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why large vision-language models (LVLMs)
  hallucinate more in longer responses. The authors propose that the key factor is
  not response length itself, but rather the model's increased reliance on context
  for coherence and completeness.
---

# Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context

## Quick Facts
- arXiv ID: 2510.20229
- Source URL: https://arxiv.org/abs/2510.20229
- Reference count: 40
- LVLMs hallucinate more in longer responses due to increased context reliance for coherence and completeness, not autoregressive length accumulation

## Executive Summary
This paper challenges the common assumption that hallucinations in large vision-language models (LVLMs) accumulate naturally with response length. Instead, the authors propose that longer responses require greater contextual coherence and completeness, creating conflicting attention pressures that trigger hallucinations. Based on this insight, they develop HalTrapper, a three-step "induce-detect-suppress" framework that manipulates context to induce hallucinations, detects them using attention similarity and cross-prompt consistency, and suppresses them during decoding. Their approach achieves significant improvements in hallucination detection (up to 12% AUROC increase) and mitigation (up to 10% reduction in CHAIR hallucination metrics) across multiple benchmarks and models.

## Method Summary
HalTrapper is a training-free framework that operates in three phases: induction, detection, and suppression. For induction, it uses two approaches - Internal Grounding (IG) appends "There is also" after EOS to force additional generation and extract hallucination candidates, while External Expansion (EE) uses directional prompts to imagine objects outside the frame. Detection computes attention similarity between induced objects and caption objects (IGScore), and measures repetition across prompts (EEScore). Suppression employs Contrastive Contextual Decoding (CCD), where detected hallucinations are encoded as contrastive context tokens (CCT) and used to modify the softmax probabilities during re-generation, effectively suppressing hallucination candidates.

## Key Results
- Hallucinations are caused by context-driven coherence demands, not autoregressive length accumulation (evidence: context modification shifts hallucination positions independently of output length)
- IGScore achieves 90% accuracy in detecting hallucinations by measuring attention similarity between object tokens
- CCD reduces CHAIR hallucination scores by 10% and improves AMBER detection with 12% AUROC increase
- EE detection reveals that hallucinated objects are highly repetitive across different prompts (only 30% unique hallucinated objects)

## Why This Works (Mechanism)

### Mechanism 1: Context-Driven Hallucination (Not Length-Accumulated)
- Claim: Hallucinations in longer LVLM responses are caused by contextual demands (coherence and completeness), not autoregressive length accumulation.
- Mechanism: When models must maintain coherent, complete responses over longer outputs, they face conflicting attention pressures—focusing on prior outputs for consistency while seeking new content to avoid redundancy. This tension produces dispersed, noisy attention patterns that trigger hallucinations.
- Core assumption: Context modification should shift hallucination position independent of output length.
- Evidence anchors:
  - [abstract]: "the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses"
  - [section 3.2]: Cropping images or enriching prompts causes hallucinations to appear at earlier positions, contradicting length-accumulation hypothesis
  - [corpus]: Weak—neighbor papers focus on mitigation methods without challenging length vs. context causality
- Break condition: If hallucination frequency correlates primarily with token position regardless of context enrichment/compression, the mechanism weakens.

### Mechanism 2: Attention Similarity as Hallucination Signal
- Claim: Hallucinated objects exhibit higher intra-set attention similarity than non-hallucinated objects within the same response.
- Mechanism: When coherence demands conflict (consistency vs. novelty), attention disperses to ungrounded regions. Multiple hallucinations draw from the same fragmented visual regions, creating similar attention maps—unlike focused attention on real objects.
- Core assumption: Hallucinated object pairs should show measurably higher attention similarity than real object pairs.
- Evidence anchors:
  - [section 4.1]: "hallucinated objects exhibit higher attention similarity, while real objects show lower values"
  - [figure 3]: Quantitative distributions of S_H (hallucinated) vs S_N (non-hallucinated) attention similarity show clear separation
  - [corpus]: Sparse attention analysis appears in neighbor papers but not systematically linked to hallucination detection
- Break condition: If attention similarity distributions overlap significantly between hallucinated and real objects, detection via this signal degrades.

### Mechanism 3: Completeness-Driven Extrapolation Patterns
- Claim: Hallucinations follow repeatable, image-grounded patterns when models compensate for insufficient recognized content.
- Mechanism: When visual context is partially described but the model must generate comprehensive responses, it extrapolates beyond recognized content. These extrapolations are not random—they repeat across varied prompts for the same image, suggesting deterministic compensation strategies.
- Core assumption: Same images under different prompts should produce overlapping hallucinated objects.
- Evidence anchors:
  - [section 4.2]: "all models exhibit a high degree of repetitiveness in hallucinated objects, with objects appearing in only one response accounting for merely 30% on average"
  - [figure 4b]: Proportion analysis shows hallucinated objects frequently repeat across 5 different prompts
  - [corpus]: Not explicitly addressed in neighbor papers
- Break condition: If hallucinations are highly prompt-specific with minimal cross-prompt overlap, image-grounded extrapolation assumption weakens.

## Foundational Learning

- **Concept: Autoregressive Decoding in LVLMs**
  - Why needed here: The paper challenges the assumption that hallucinations accumulate naturally during autoregressive generation. Understanding p_θ(y_i|v, x, y_{<i}) is essential to grasp why context—not length—is the driver.
  - Quick check question: Can you explain why modifying input context (without changing target length) would shift hallucination positions if length-accumulation were the true cause?

- **Concept: Contrastive Decoding**
  - Why needed here: HalTrapper's suppression mechanism (CCD) extends standard contrastive decoding by encoding hallucination candidates as contrastive context tokens (CCT). Without this foundation, the softmax manipulation in equation (10) will be opaque.
  - Quick check question: How does equation (10) differ from standard contrastive decoding in equation (8)?

- **Concept: Attention Map Analysis in Vision Transformers**
  - Why needed here: The detection mechanism relies on computing attention similarity sim(A_{s,i}, A_{s,j}) between object tokens. Interpreting these maps is critical for understanding why hallucinated tokens cluster.
  - Quick check question: What would dispersed vs. focused attention look like visually when generating a hallucinated vs. real object?

## Architecture Onboarding

- **Component map:**
  - Generate caption with standard decoding -> Append "There is also" -> Extract induced object -> Compute attention similarity (IG) -> Generate 8 directional imagination prompts -> Compute EEScore -> Merge candidates -> Encode as CCT -> Apply CCD during re-generation

- **Critical path:**
  1. Generate initial caption with standard decoding
  2. Run IG induction → compute attention similarities → filter high-similarity objects as hallucination candidates
  3. Run EE induction with 8 directional prompts → identify repeated objects across prompts
  4. Merge candidates → construct CCT string with separator T_{sep}
  5. Apply CCD during re-generation with CCT in contrastive branch

- **Design tradeoffs:**
  - **Threshold sensitivity:** θ_{IG} (0.75–0.85 across models) balances precision vs. recall in detection; too aggressive loses real hallucinations, too permissive includes false positives
  - **CCT length N:** Fixed length requires truncation (priority: IG > EE) or padding with unrelated objects; mismatch may weaken contrastive signal
  - **EE direction count |D|=8:** More directions increase hallucination coverage but add inference cost

- **Failure signatures:**
  - **IG false negatives:** If reference object o^{ref}_s is real (not hallucinated), similarity scores may not flag actual hallucinations → low recall
  - **EE false positives:** Model may imagine objects actually present in image → requires reason-then-imagine filtering
  - **Over-suppression:** Aggressive α in CCD (equation 10) may reduce recall of legitimate objects

- **First 3 experiments:**
  1. **Reproduce attention similarity distributions:** Run LLaVA on 50 COCO images, compute S_H and S_N for hallucinated vs. real object pairs, verify separation matches Figure 3
  2. **Ablate IG vs. EE contributions:** Disable each branch independently on CHAIR benchmark (Table 6), confirm both contribute to CHAIR_S reduction
  3. **Stress-test hallucination position shift:** Apply image cropping and prompt enrichment from Section 3.2, verify PoScore distribution shifts earlier independent of response length

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the mechanisms of contextual coherence and completeness contribute to attribute-level and relational-level hallucinations?
- **Basis in paper:** [explicit] The Limitations section states that while the study addresses object-level hallucinations, "LVLMs are susceptible to a broader spectrum of hallucinations, including... hallucinations at the attribute and relational levels."
- **Why unresolved:** The current HalTrapper framework and analysis are designed specifically to detect and suppress objects, leaving the validity of the context-driven hypothesis for attributes (e.g., color, count) or relationships untested.
- **What evidence would resolve it:** Adapting the HalTrapper induction prompts to target specific attributes or spatial relations and measuring detection performance on corresponding benchmarks.

### Open Question 2
- **Question:** Can the context-driven hallucination hypothesis be generalized to open-ended, multi-turn conversational scenarios?
- **Basis in paper:** [explicit] The authors note in the Limitations section that evaluations "are mainly on image captioning benchmarks... [which] do not adequately cover more open-ended generative scenarios."
- **Why unresolved:** The current experiments rely on single-turn description tasks (COCO, AMBER), whereas real-world "longer responses" often occur in multi-turn dialogues where context accumulates differently.
- **What evidence would resolve it:** Validation of the HalTrapper framework on dialogue-centric benchmarks (e.g., MM-Vet) or visual storytelling datasets where context evolves over multiple turns.

### Open Question 3
- **Question:** Is the "dispersed attention" phenomenon a universal architectural failure mode or specific to the current generation of LVLMs?
- **Basis in paper:** [inferred] Section 4.1 identifies that hallucinated tokens show dispersed attention and high similarity, but the analysis is restricted to LLaVA, Qwen, and MiniGPT-4.
- **Why unresolved:** It remains unclear if this specific attention signature is a byproduct of the specific visual encoders or projector layers used in these models, or a fundamental property of autoregressive LVLMs.
- **What evidence would resolve it:** A cross-architecture study analyzing attention maps in models with fundamentally different visual alignment mechanisms (e.g., without Q-Former or with different LLM backbones).

## Limitations
- Attribute and relational hallucinations remain unaddressed despite being common in real applications
- Evaluation limited to single-turn captioning tasks, not covering multi-turn conversational scenarios
- Results based on specific model architectures (LLaVA, Qwen, MiniGPT-4) without cross-architecture validation

## Confidence
- Mechanism 1: Medium - Strong experimental evidence but limited cross-architecture validation
- Mechanism 2: High - Clear statistical separation in attention similarity distributions with quantitative analysis
- Mechanism 3: Medium - Good repeatability evidence but could benefit from larger-scale cross-prompt studies

## Next Checks
1. Verify that cropping images or enriching prompts shifts hallucination positions earlier, independent of response length (Section 3.2)
2. Reproduce attention similarity distributions showing higher S_H than S_N for hallucinated vs. real object pairs
3. Validate that CCD with α=1.0 and β=0.1 reduces CHAIR_S by approximately 10% on COCO images