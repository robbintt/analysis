---
ver: rpa2
title: 'Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning
  Model Robustness to Adversarial Attacks'
arxiv_id: '2511.20456'
source_url: https://arxiv.org/abs/2511.20456
tags:
- sensing
- ieee
- adversarial
- visited
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a systematic evaluation of CSI-based deep learning
  model robustness to adversarial attacks under diverse threat models. The study benchmarks
  white-box, black-box/transfer, and universal perturbations on three public datasets
  using compact temporal autoencoder models and larger deep architectures.
---

# Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks

## Quick Facts
- arXiv ID: 2511.20456
- Source URL: https://arxiv.org/abs/2511.20456
- Reference count: 40
- This work presents a systematic evaluation of CSI-based deep learning model robustness to adversarial attacks under diverse threat models, finding that smaller models are markedly less robust than larger ones, and that physically realizable signal-space perturbations significantly reduce attack success compared to unconstrained feature-space attacks.

## Executive Summary
This study provides the first comprehensive evaluation of adversarial robustness for CSI-based human sensing models, systematically benchmarking white-box, black-box/transfer, and universal perturbations across three public datasets. The research reveals critical trade-offs between model efficiency and robustness, demonstrating that while compact autoencoder-based models achieve comparable clean performance, they exhibit significantly higher vulnerability to transfer attacks. The paper also introduces physically constrained adversarial perturbations that dramatically reduce attack success rates, validating the importance of wireless propagation constraints in realistic threat models. Finally, it demonstrates that adversarial training improves mean robust accuracy with only moderate degradation in clean performance, establishing quantitative baselines for future secure sensing system design.

## Method Summary
The paper evaluates CSI-based deep learning models under multiple adversarial threat models using three public datasets (UT-HAR, NTU-HAR, NTU-HID). It benchmarks white-box attacks (PGD, DeepFool), black-box transfer attacks, and universal perturbations against two model families: large SenseFi baselines (LeNet, BiLSTM, ResNet18) and compact autoencoder-based models (TCN-AE, GRU-AE, SSM-AE). The evaluation includes physically realizable signal-space perturbations designed using wireless propagation constraints, and tests two defense strategies (PGD-AT and TRADES). Models are trained with Adam optimizer (LR=1e-3, weight decay=5e-4) for 100 epochs with early stopping, and adversarial examples are generated using ℓ₂-bounded perturbations at 10, 20, and 40 dB PSR.

## Key Results
- Smaller models, while efficient and equally performant on clean data, are markedly less robust to adversarial attacks than larger models
- Physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks
- Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance

## Why This Works (Mechanism)

### Mechanism 1: Physics-Guided Projection Neutralizes Gradient-Based Attacks
Imposing wireless propagation constraints on adversarial perturbations dramatically reduces attack success rates by projecting perturbations back onto the physically realizable CSI manifold, destroying the structured energy patterns that gradient-based attacks exploit.

### Mechanism 2: Model Capacity Provides Robustness Buffer
Larger deep learning models exhibit greater adversarial robustness than lightweight models due to more complex decision boundaries that distribute class representations more sparsely in feature space, increasing the minimum perturbation distance required to cross decision boundaries.

### Mechanism 3: Adversarial Training Improves Robust Accuracy with Controllable Clean Performance Cost
PGD-based adversarial training and TRADES reduce attack success rates by exposing models to attack patterns during training, shifting decision boundaries away from class borders while maintaining acceptable clean accuracy degradation.

## Foundational Learning

- Concept: **Channel State Information (CSI) tensor structure** - Understanding H ∈ ℂ^(Nr×Nt×K) with amplitude |H| and phase ∠H per subcarrier-antenna link is prerequisite for interpreting how perturbations propagate through the feature space.
  - Quick check question: Given a 3×30×250 CSI tensor from Intel 5300, what do each of the three dimensions represent?

- Concept: **Adversarial threat models (white-box vs. black-box)** - The paper evaluates robustness under varying attacker knowledge; distinguishing PGD (gradient-access) from transfer attacks (surrogate-only) determines which defense strategies apply.
  - Quick check question: In a black-box transfer attack, what information does the adversary have about the target model?

- Concept: **Perturbation-to-Signal Ratio (PSR) as attack budget** - PSR = -SNR quantifies perturbation strength; 10dB PSR means perturbation energy is 10% of signal energy, 40dB PSR means 1%.
  - Quick check question: If clean signal energy ‖x‖₂ = 100 and target PSR = 20dB, what is the maximum allowed perturbation norm ε?

## Architecture Onboarding

- Component map: Raw CSI -> preprocessing (normalization, optional smoothing) -> model forward pass -> loss computation -> (if training) adversarial example generation via inner PGD -> parameter update -> evaluation with ASR/Acc metrics

- Critical path: The pipeline processes CSI tensors through normalization, model inference, loss computation, and optional adversarial training loops, with evaluation metrics tracking both clean accuracy and attack success rates.

- Design tradeoffs:
  - Tiny vs. Large models: Tiny (28-82K params, efficient, less robust) vs. Large (227K-11M params, higher latency, more robust)
  - PGD-AT vs. TRADES: PGD-AT gives larger ASR reduction; TRADES gives better clean-robust balance
  - Unconstrained vs. physics-constrained attacks: Unconstrained provides upper-bound vulnerability; constrained provides realistic threat assessment

- Failure signatures:
  - ASR >80% on intra-family transfer with tiny models → expected behavior (shared latent vulnerability)
  - PGD-Corr ASR >5% → constraint implementation error or channel model mismatch
  - Clean F1 drops >10% after adversarial training → excessive ε budget or insufficient TRADES β tuning
  - UAP ASR remains high after correlation preservation → aggregation step missing or MMD weight too low

- First 3 experiments:
  1. **Baseline clean performance**: Train all 6 models on UT-HAR with standard cross-entropy; verify F1 matches Table 2 (TCN-AE ~93.8%, ResNet18 ~96.8%) to confirm implementation correctness.
  2. **White-box PGD attack sweep**: Run PGD ℓ₂ at 10/20/40dB PSR on trained models; verify ASR increases with budget and tiny models show higher vulnerability (Table 3 pattern).
  3. **Physics-constrained attack validation**: Implement Π_phys (Algorithm 1) and compare PGD-Corr ASR to unconstrained PGD; expect near-zero ASR (0.2-1.0%) confirming constraint effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
Does the use of engineered feature representations (e.g., BVP, Doppler profiles) amplify or mitigate adversarial vulnerability compared to raw CSI inputs? The authors state in Section 7.2 that "Future work should systematically assess whether feature engineering amplifies or mitigates vulnerability." This study focused exclusively on raw CSI tensors; the impact of hand-crafted features on the robustness-accuracy trade-off remains unknown.

### Open Question 2
How do consistency-based or regularization-based defenses compare to adversarial training in managing the trade-off between clean accuracy and robustness for CSI models? Section 7.6 lists the limitation that "We evaluate only a single adversarial defense... limits our understanding of how consistency-based or regularization-based defenses might alter the robustness-accuracy tradeoff."

### Open Question 3
How susceptible are CSI-based models to adaptive drift injection and temporal attacks in continuous, long-term deployment scenarios? Section 7.4 notes that static datasets obscure vulnerabilities and suggests "Future research should extend robustness evaluation... to include long-term temporal attacks, adaptive drift injection."

## Limitations

- Physical constraint effectiveness depends on accurate channel modeling assumptions (WSSUS, PDP statistics) that may not hold in highly dynamic environments
- Tiny model robustness findings rely on specific architectural choices (shared bottleneck) without exploring alternative compact designs
- Adversarial training improvements are evaluated only under ℓ₂-bounded attacks; generalization to other threat models remains unverified

## Confidence

- **High Confidence**: Model capacity robustness trends (Larger models consistently more robust than smaller ones across datasets)
- **Medium Confidence**: Physical constraint effectiveness (Limited by specific channel model assumptions and parameter choices)
- **Medium Confidence**: Adversarial training benefits (Standard AML methods applied to CSI domain, but threat model scope limited)

## Next Checks

1. Test physical constraints under non-WSSUS conditions (rapidly varying channels) to verify robustness to modeling assumptions
2. Evaluate adversarial training against different attack types (ℓ∞, semantic perturbations) to assess generalization
3. Compare tiny model robustness with alternative compact architectures (e.g., depthwise separable convolutions) to isolate architectural effects