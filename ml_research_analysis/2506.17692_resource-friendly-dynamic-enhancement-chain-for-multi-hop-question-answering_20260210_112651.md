---
ver: rpa2
title: Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering
arxiv_id: '2506.17692'
source_url: https://arxiv.org/abs/2506.17692
tags:
- question
- reasoning
- retrieval
- answer
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DEC, a resource-efficient framework for multi-hop
  question answering that mitigates hallucinations in lightweight LLMs by generating
  a logically coherent reasoning chain before retrieval. The approach employs structured
  question decomposition, dynamic context-aware query rewriting, and discriminative
  keyword extraction to enhance retrieval precision while reducing token consumption.
---

# Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering

## Quick Facts
- **arXiv ID**: 2506.17692
- **Source URL**: https://arxiv.org/abs/2506.17692
- **Reference count**: 37
- **Key outcome**: DEC achieves SOTA performance with 8B-parameter models, outperforming iterative methods by 27% fewer reasoning steps and reducing token consumption by up to 45% while maintaining accuracy

## Executive Summary
This work introduces DEC, a resource-efficient framework for multi-hop question answering that mitigates hallucinations in lightweight LLMs by generating a logically coherent reasoning chain before retrieval. The approach employs structured question decomposition, dynamic context-aware query rewriting, and discriminative keyword extraction to enhance retrieval precision while reducing token consumption. Experiments on three multi-hop QA datasets demonstrate DEC achieves state-of-the-art performance with 8B-parameter models, outperforming iterative methods by 27% fewer reasoning steps and reducing average token consumption per correct answer by up to 45% while maintaining accuracy.

## Method Summary
DEC addresses multi-hop question answering for 8B-parameter LLMs through a four-stage framework: (1) Question decomposition using a large LLM to generate a reasoning chain with sub-questions, (2) Dynamic query rewriting that leverages historical Q&A context to resolve coreferences, (3) Discriminative keyword extraction using a fine-tuned Llama-3.2-3B-Instruct model to identify retrieval-critical terms, and (4) Retrieval with E5-base embeddings, selecting top 2 relevant documents plus keyword-matched documents from top 10 candidates. The framework processes three datasets (HotpotQA, 2WikiMultiHopQA, MuSiQue) using Wikipedia snapshots, evaluating with Coverage Exact Match, Token-level F1, and Semantic Accuracy metrics.

## Key Results
- Achieves state-of-the-art performance with 8B-parameter models across three multi-hop QA datasets
- Reduces reasoning steps by 27% compared to iterative methods
- Decreases average token consumption per correct answer by up to 45% while maintaining accuracy

## Why This Works (Mechanism)
The framework's effectiveness stems from its plan-then-retrieve strategy that frontloads reasoning complexity. By generating a complete logical chain before retrieval, DEC ensures each retrieval step has a clear purpose tied to specific sub-questions. The dynamic query rewriting resolves coreferences using accumulated context, preventing ambiguous queries that lead to irrelevant retrievals. Discriminative keyword extraction focuses retrieval on critical terms rather than the full sub-question, reducing noise and improving precision. The hybrid retrieval strategy (top-2 relevance + keyword matching) balances recall and precision while minimizing document candidates per step.

## Foundational Learning
- **Multi-hop QA reasoning**: Understanding how questions requiring multiple evidence pieces are structured and answered. Why needed: DEC's core innovation addresses the complexity of tracking multiple reasoning steps.
- **RAG fundamentals**: Knowledge of retrieval-augmented generation and document selection strategies. Why needed: The retrieval component is central to DEC's performance gains.
- **LLM prompt engineering**: Techniques for structuring prompts to elicit specific outputs (decomposition, rewriting, extraction). Why needed: Each DEC stage relies on carefully crafted prompts.
- **Keyword extraction for retrieval**: Methods to identify discriminative terms that improve search precision. Why needed: The EK model is critical for reducing retrieval noise.
- **Evaluation metrics for QA**: Understanding CoverEM, F1, and semantic accuracy metrics. Why needed: These metrics define success criteria for the framework.

## Architecture Onboarding

### Component Map
LLM Decomposition -> Dynamic Query Rewriting -> Discriminative Keyword Extraction -> E5 Retrieval -> Answer Generation -> Feedback Loop (context accumulation)

### Critical Path
Question decomposition → Query rewriting → Keyword extraction → Document retrieval → Answer generation → Context accumulation

### Design Tradeoffs
- **Fixed reasoning chain vs. dynamic adaptation**: Plan-then-retrieve strategy trades flexibility for efficiency and reduced hallucinations
- **Keyword extraction overhead vs. retrieval precision**: Additional EK model fine-tuning improves retrieval quality but adds complexity
- **Top-2 selection vs. broader coverage**: Narrow document selection reduces computation but risks missing relevant information

### Failure Signatures
- Low keyword extraction accuracy (<96%) leading to poor retrieval
- Query rewriting failures to resolve coreferences resulting in ambiguous searches
- Insufficient document candidates (top-2 plus keyword matches) missing relevant information
- Initial decomposition errors propagating through all subsequent steps

### First Experiments
1. Test retrieval quality with E5-base embeddings on sample Wikipedia queries
2. Validate keyword extraction accuracy on held-out HotpotQA samples
3. Run end-to-end DEC pipeline on single query to verify component integration

## Open Questions the Paper Calls Out
The authors explicitly identify that DEC heavily relies on high-quality decomposition, noting that "if the initial decomposition or knowledge supplementation is insufficient or biased, it may lead to failure in subsequent reasoning." This creates an open question about whether the framework can be augmented with verification or self-correction mechanisms to mitigate failures caused by errors in the initial question decomposition.

## Limitations
- Heavy dependence on initial reasoning chain quality, with decomposition errors propagating through all subsequent steps
- Performance gains may be partially attributed to reduced search depth rather than fundamentally better retrieval
- Fixed selection strategy (top-2 plus keyword matches) could miss relevant information outside narrow scope
- Limited evaluation to Wikipedia-based corpora raises questions about domain generalization

## Confidence

**High confidence**: Token efficiency improvements (45% reduction in ATC) and overall methodology framework are well-supported by experimental results across three datasets with clearly defined metrics.

**Medium confidence**: The 27% reduction in reasoning steps compared to iterative methods requires careful interpretation, and the "state-of-the-art" claim needs context regarding specific model configurations used by competing methods.

**Low confidence**: The assertion that DEC "mitigates hallucinations" is primarily supported by improved accuracy metrics rather than direct hallucination detection or qualitative analysis of generated reasoning chains.

## Next Checks
1. **Keyword extraction validation**: Verify the fine-tuning process for the EK model by testing keyword accuracy on a held-out validation set, checking if reported >96% accuracy holds under different Wikipedia snapshots.
2. **Retrieval coverage analysis**: Examine whether the top-2 plus keyword-matched documents selection strategy maintains coverage across diverse question types by testing scenarios where relevant information might be in documents ranked 3-10 but not matching extracted keywords.
3. **Ablation study replication**: Replicate key ablation studies (removing reasoning chain, query rewriting, or keyword extraction) to verify each component contributes meaningfully to reported performance gains.