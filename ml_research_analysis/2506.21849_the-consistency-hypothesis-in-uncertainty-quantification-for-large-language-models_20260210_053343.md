---
ver: rpa2
title: The Consistency Hypothesis in Uncertainty Quantification for Large Language
  Models
arxiv_id: '2506.21849'
source_url: https://arxiv.org/abs/2506.21849
tags:
- dataset
- generations
- consistency
- similarity
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces and formalizes the consistency hypothesis
  in large language models (LLMs), which assumes that correct generations are more
  similar to each other than incorrect ones. Three mathematical statements (Sim-Any,
  Sim-Correct, Sim-Separate) are proposed along with statistical tests to verify these
  hypotheses.
---

# The Consistency Hypothesis in Uncertainty Quantification for Large Language Models

## Quick Facts
- arXiv ID: 2506.21849
- Source URL: https://arxiv.org/abs/2506.21849
- Reference count: 16
- Primary result: Consistency-based UQ methods achieve AUROC improvements up to 0.85 using geometric/harmonic mean aggregation of generation similarities

## Executive Summary
This paper introduces and formalizes the consistency hypothesis for LLM uncertainty quantification, proposing that correct generations are more similar to each other than incorrect ones. Three mathematical statements (Sim-Any, Sim-Correct, Sim-Separate) are proposed with statistical tests to verify these hypotheses across 8 benchmark datasets and 3 tasks. The study validates the hypothesis to varying degrees, with Sim-Any being most actionable for practical black-box UQ. Based on this foundation, new aggregation methods (geometric and harmonic mean) are proposed, outperforming baseline UQ approaches with AUROC improvements up to 0.85.

## Method Summary
The method involves generating 30 samples per instance (5 samples × 6 temperatures from 0.25 to 1.5) and computing pairwise similarities between all generations. Three mathematical statements formalize the consistency hypothesis: Sim-Any (aggregate similarity of any generation to all others), Sim-Correct (similarity among correct generations), and Sim-Separate (separation between correct and incorrect clusters). Statistical verification uses one-sided t-tests comparing mean similarities (μC vs μI) across ng groups of instances. For UQ, similarity aggregations (arithmetic, geometric, harmonic mean) are computed and thresholded to estimate confidence scores. The approach requires no ground truth labels at inference time, making it suitable for black-box deployment.

## Key Results
- The consistency hypothesis is validated across all datasets, with Sim-Any showing the most consistent verification (ρ(ng) typically > 0.7 for ng/n ≈ 0.1)
- Geometric and harmonic mean aggregation outperform arithmetic mean, achieving AUROC improvements of 0.02-0.05 across tasks
- Maximum relative group number θ* (4.1-4.7) indicates Sim-Any has the strongest hypothesis verification signal
- Performance varies by task: QA and Text-to-SQL show stronger hypothesis adherence than summarization

## Why This Works (Mechanism)

### Mechanism 1: Similarity-Based Clustering of Correct Generations
- **Claim**: Correct generations exhibit higher pairwise similarity to each other than incorrect generations do to correct ones.
- **Mechanism**: When an LLM has high confidence/competence on a query, multiple sampling converges toward semantically equivalent outputs. Incorrect generations, arising from model uncertainty or knowledge gaps, produce dissimilar hallucinations that scatter across the representation space.
- **Core assumption**: Model knowledge correlates with output consistency; sampling diversity at high temperature exposes uncertainty.
- **Evidence anchors**: [abstract]: "correct generations are more similar to each other than incorrect ones"; [section 2.2.1]: Formalizes Sim-Correct with SC_i = {s_j,k^i : j,k ∈ Y*_i, k≠j} and SI_i = {s_j,k^i : j∉Y*_i, k∈Y*_i}
- **Break condition**: Fails when correct answers are legitimately diverse (open-ended summarization) or when incorrect answers accidentally converge (systematic biases).

### Mechanism 2: Aggregated Similarity as Confidence Proxy (Sim-Any)
- **Claim**: Aggregating a generation's similarity to ALL other generations provides a practical confidence estimate without requiring correctness labels.
- **Mechanism**: Since correct generations are more similar to other correct generations, and incorrect generations are dissimilar to most others (including other incorrect ones), the mean similarity of any generation to all others correlates with correctness probability.
- **Core assumption**: The Sim-Any hypothesis holds: μC_i > μI_i for SC_i = {s_j,k^i : j∈Y*_i, k≠j} vs SI_i = {s_j,k^i : j∉Y*_i, k≠j}.
- **Evidence anchors**: [abstract]: "Sim-Any hypothesis as the most actionable"; [section 4.1, Figure 3]: Mean differences positive across all datasets for H1
- **Break condition**: Degraded when incorrect generations form coherent but wrong clusters (consistent hallucinations).

### Mechanism 3: Non-Linear Aggregation Improves Sensitivity
- **Claim**: Geometric and harmonic mean aggregation of pairwise similarities outperforms arithmetic mean for confidence estimation.
- **Mechanism**: Geometric/harmonic means emphasize low-similarity pairs (outliers), making them more sensitive to the presence of dissimilar generations. This conservative aggregation better separates correct from incorrect cases.
- **Core assumption**: Correctness correlates with minimum pairwise similarity, not just average.
- **Evidence anchors**: [section 5, Table 2]: Geometric/harmonic achieve AUROC 0.85 vs arithmetic 0.82 on CoQA; [section 4.3, Table 1]: Harmonic shows higher max relative group number for hypothesis verification
- **Break condition**: May over-penalize legitimate variation in creative tasks.

## Foundational Learning

- **Concept: Semantic Similarity Metrics (Jaccard, ROUGE, Sentence-BERT)**
  - **Why needed here**: Quantifies "consistency" between text generations; choice of metric directly affects hypothesis verification and UQ performance.
  - **Quick check question**: Given two SQL queries that produce identical results but differ syntactically, which similarity metric would score them highest?

- **Concept: Sampling Strategies (Temperature, Hybrid)**
  - **Why needed here**: Generating diverse samples is prerequisite for consistency-based UQ; sampling method affects hypothesis validity.
  - **Quick check question**: Why might hybrid sampling (multiple temperatures) validate the consistency hypothesis better than fixed-temperature sampling?

- **Concept: Statistical Hypothesis Testing (t-test, p-value thresholding)**
  - **Why needed here**: Paper formalizes verification via μC > μI tests; understanding significance and group-level aggregation is essential for applying the framework.
  - **Quick check question**: If the null hypothesis (μC ≤ μI) is rejected at p≤0.05 for 80% of groups with ng=10, what does this imply about dataset-level hypothesis validity?

## Architecture Onboarding

- **Component map**: Query → Multi-temperature sampling → Pairwise similarity matrix → Aggregation → Confidence score
- **Critical path**: Query → Multi-temperature sampling → Pairwise similarity matrix → Aggregation → Confidence score. The Sim-Any pathway is production-viable as it requires no ground truth labels.
- **Design tradeoffs**:
  - **Pairwise vs. Aggregated consistency**: Pairwise provides more data points for verification; aggregated is more practical for inference-time UQ.
  - **Similarity metric complexity**: Jaccard is fastest; sBERT captures semantics better but adds latency.
  - **Group size ng for verification**: Smaller groups → more granular but less statistically powered; paper suggests ng/n ≈ 0.01-0.1 as practical range.
- **Failure signatures**:
  - Low mean difference (Δμ < 0.1): Hypothesis weak; UQ will underperform
  - High variance in SC distribution: Task may have inherently diverse correct answers (summarization)
  - ρ(ng) curve flat or decreasing with larger ng: Dataset doesn't conform to hypothesis at instance level
- **First 3 experiments**:
  1. **Verify hypothesis on target dataset**: Run Algorithm 1 with ng=10-50, plot ρ(ng) vs ng/n. If ρ(10) < 0.6, consider alternative UQ methods.
  2. **Ablate similarity metrics**: Compare Jaccard, ROUGE-L, sBERT on mean difference Δμ and max relative group number θ*. Select metric with highest verification.
  3. **Benchmark aggregation functions**: Compare arithmetic, geometric, harmonic mean on AUROC/AUARC. If geometric/harmonic improve >2% over arithmetic, deploy with conservative aggregation.

## Open Questions the Paper Calls Out

- **Open Question 1**: What alternative aggregation functions (beyond arithmetic, geometric, and harmonic means) can effectively leverage the Sim-Any hypothesis for black-box uncertainty quantification?
  - **Basis in paper**: [explicit] The conclusion states, "An investigation of other aggregation methods... are potential avenues for future work."
  - **Why unresolved**: The paper limits its proposed UQ method to three simple aggregation types (arithmetic, geometric, harmonic), leaving the space of more complex or learned aggregation functions unexplored.
  - **What evidence would resolve it**: Empirical results from applying non-linear or learned aggregators (e.g., neural networks) to the similarity sets, showing improved AUROC/AUARC over the harmonic mean baseline.

- **Open Question 2**: Does the consistency hypothesis hold for generative tasks involving higher reasoning complexity or multi-modal inputs, where the variability of correct answers might differ from the studied tasks?
  - **Basis in paper**: [explicit] The conclusion suggests a "broader empirical study with tasks involving additional complexities" as a future avenue.
  - **Why unresolved**: The study is restricted to QA, summarization, and Text-to-SQL; it is unknown if the similarity distributions of correct/incorrect generations generalize to tasks like chain-of-thought reasoning or code generation.
  - **What evidence would resolve it**: Verification metrics ($\Delta\mu$, $\rho(n_g)$) calculated for complex reasoning benchmarks (e.g., MATH, BBH) or multi-modal datasets.

- **Open Question 3**: Is there a theoretically grounded method for selecting the optimal similarity metric ($s$) based on task structure, rather than relying on empirical ablation?
  - **Basis in paper**: [inferred] Tables 1 and 3 demonstrate that verification success varies significantly across metrics (Jaccard, Rouge, SBERT, SQL-type), but the choice relies on experimental trial rather than theoretical fit.
  - **Why unresolved**: The paper shows that SBERT works well for group numbers while Jaccard is robust, but does not define criteria for when semantic similarity outperforms lexical overlap.
  - **What evidence would resolve it**: A framework or set of heuristics that maps dataset properties (e.g., output length, vocabulary distinctness) to the similarity metric that maximizes the mean difference ($\Delta\mu$) between correct and incorrect clusters.

## Limitations

- **Dataset-specific hypothesis validity**: The consistency hypothesis demonstrates variable strength across datasets, with ρ(ng) ranging from 0.3 to 0.9, limiting UQ effectiveness for tasks with inherently diverse correct answers.
- **Temperature sampling constraints**: The paper employs multi-temperature sampling (0.25-1.5) but doesn't systematically explore how temperature ranges affect hypothesis verification or specify critical sampling parameters like top-p and top-k.
- **Semantic similarity metric sensitivity**: The choice of similarity metric significantly impacts verification results, but the paper doesn't establish criteria for metric selection across tasks.

## Confidence

**High confidence**: The statistical framework for hypothesis verification (Algorithm 1) is methodologically sound, with one-sided t-tests and verification fraction ρ(ng) providing rigorous assessment. Empirical AUROC improvements (0.72-0.85) are statistically significant across multiple datasets.

**Medium confidence**: The claim that Sim-Any is "most actionable" for practical UQ is supported by results but relies on assumptions about black-box deployment scenarios. While aggregation methods show consistent improvements,