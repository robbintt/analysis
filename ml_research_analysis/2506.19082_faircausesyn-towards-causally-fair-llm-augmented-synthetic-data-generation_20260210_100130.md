---
ver: rpa2
title: 'FairCauseSyn: Towards Causally Fair LLM-Augmented Synthetic Data Generation'
arxiv_id: '2506.19082'
source_url: https://arxiv.org/abs/2506.19082
tags:
- fairness
- data
- synthetic
- causal
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating synthetic health
  data that maintains causal fairness, going beyond counterfactual fairness approaches.
  The proposed FairCauseSyn method uses large language models (LLMs) to generate synthetic
  tabular data while enforcing causal fairness constraints through structural causal
  modeling and fairness evaluation metrics.
---

# FairCauseSyn: Towards Causally Fair LLM-Augmented Synthetic Data Generation

## Quick Facts
- arXiv ID: 2506.19082
- Source URL: https://arxiv.org/abs/2506.19082
- Authors: Nitish Nagesh; Ziyu Wang; Amir M. Rahmani
- Reference count: 5
- Primary result: Generated synthetic health data achieves <10% deviation from real data on causal fairness metrics, with near-zero direct and indirect effects when trained on causally fair predictors

## Executive Summary
FairCauseSyn addresses the challenge of generating synthetic health data that maintains causal fairness by going beyond counterfactual fairness approaches. The method uses large language models (LLMs) to generate synthetic tabular data while enforcing causal fairness constraints through structural causal modeling and fairness evaluation metrics. Evaluated on heart failure clinical data, the approach shows that synthetic data generated with causal fairness constraints reduces bias on sensitive attributes by 70% compared to real data, achieving near-zero direct and indirect effects while maintaining statistical fidelity.

## Method Summary
FairCauseSyn generates synthetic tabular health data using LLM prompts informed by real health data, then evaluates the generated data using causal fairness metrics including Total Variation (TV), Direct Effect (DE), Indirect Effect (IE), and Spurious Effect (SE). The method involves iterative prompt refinement with a constraint satisfaction module that evaluates each batch against fairness metrics. When trained on causally fair predictors, synthetic data reduces bias on sensitive attributes by 70% compared to real data. The approach uses structural causal modeling to decompose total discrimination into pathway-specific effects, enabling targeted bias mitigation while preserving legitimate causal relationships.

## Key Results
- Synthetic data deviates by less than 10% from real data on causal fairness metrics (TV, DE, IE, SE)
- Causally fair predictors trained on synthetic data reduce bias on sensitive attributes by 70% compared to real data
- Generated data achieves near-zero direct and indirect effects (DE ≈ -0.002, IE ≈ 0.008) when using FairAdapt causally fair model
- Spurious effect shows higher variability (SD ~0.05-0.06) compared to other metrics, indicating challenges in controlling confounding effects

## Why This Works (Mechanism)

### Mechanism 1: Causal Fairness Decomposition via Standard Fairness Model
- Claim: Decomposing total discrimination into pathway-specific effects enables targeted bias mitigation.
- Mechanism: The Standard Fairness Model (SFM) maps protected attributes (X), confounders (Z), mediators (W), and outcomes (Y) into distinct causal pathways. Total Variation (TV) is decomposed into Direct Effect (X→Y), Indirect Effect (X→W→Y), and Spurious Effect (confounding via Z), allowing isolation of bias sources.
- Core assumption: The causal graph structure correctly reflects real-world relationships among variables.
- Evidence anchors:
  - [abstract] "Causal fairness provides a more comprehensive evaluation framework by preserving causal structure"
  - [Section 2.2-2.3] Equations 1-3 formalize DE, IE, SE decomposition; TV = DE - IE - SE
  - [corpus] Related work "A pipeline for enabling path-specific causal fairness in observational health data" supports pathway-specific fairness analysis in health settings
- Break condition: If the causal graph is misspecified (e.g., missing confounders, incorrect mediator assignments), decomposition will misattribute effects and interventions may increase rather than reduce bias.

### Mechanism 2: LLM Prompt-Driven Synthetic Generation with Fairness Constraints
- Claim: LLMs can generate synthetic tabular data that approximates causal fairness properties of real data through iterative prompt refinement.
- Mechanism: Curated real data subsets inform prompt construction via in-context learning and schema-guided generation. A constraint satisfaction module evaluates each batch against fairness metrics; failing batches trigger prompt refinement until constraints are met.
- Core assumption: LLMs can internalize and reproduce statistical and causal relationships from tabular data presented in prompts.
- Evidence anchors:
  - [Section 2.4] "prompts are used to query the LLM, producing candidate synthetic samples... If constraints are unmet, iterative prompt refinement and data adaptation steps are invoked"
  - [Section 3.4.1] Synthetic data DE and IE closely mirrored real data (DE: -0.0429 vs -0.0477; IE: -0.0002 vs -0.0472)
  - [corpus] "FairTabGen" paper similarly demonstrates LLM-based fair synthetic tabular generation, suggesting convergent validity of the approach
- Break condition: If prompts lack sufficient causal context or the LLM fails to generalize relationships, generated data may satisfy surface statistics while violating underlying causal structure.

### Mechanism 3: Causally Fair Predictor Training Reduces Bias Amplification
- Claim: Training prediction models with causal fairness constraints on synthetic data yields lower DE and IE than training on real data.
- Mechanism: The FairAdapt framework adjusts features to remove discriminatory paths while preserving legitimate causal relationships. When trained on synthetic data already filtered through constraint satisfaction, the causally fair predictor achieves near-zero DE (-0.0020) and IE (0.0076).
- Core assumption: FairAdapt's causal adjustment correctly identifies and removes discriminatory pathways without introducing new biases.
- Evidence anchors:
  - [Table 1] Causally fair model on synthetic data: DE=-0.0020±0.0030, IE=0.0076±0.0108 vs real data DE=-0.0070±0.0016, IE=-0.0538±0.0054
  - [Section 3.4.3] "all three fairness components were further minimized... demonstrating improved mitigation of disparate treatment and impact"
  - [corpus] Weak direct corpus evidence for FairAdapt specifically; related work on causal fairness preprocessing exists but validation is limited
- Break condition: If the causal model omits relevant pathways, fair adaptation may over-correct legitimate relationships or under-correct hidden discrimination.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: The entire FairCauseSyn framework relies on SCM formalism to define variables, equations, and causal pathways before any fairness analysis or generation occurs.
  - Quick check question: Can you identify the four components of an SCM (V, U, F, P(u)) and explain what each represents in a healthcare dataset?

- Concept: Counterfactual vs. Causal Fairness
  - Why needed here: The paper explicitly positions itself against counterfactual fairness methods, arguing causal fairness provides broader pathway-level bias quantification.
  - Quick check question: What is the key distinction the paper draws between counterfactual fairness and causal fairness metrics?

- Concept: Fairness Metric Decomposition (TV, DE, IE, SE)
  - Why needed here: All evaluation and constraint satisfaction depends on correctly computing and interpreting these four metrics and their relationships.
  - Quick check question: If DE=0.05, IE=-0.02, and SE=0.01, what is the Total Variation? Which effect contributes most to discrimination?

## Architecture Onboarding

- Component map:
  - Real clinical data (D_real) -> Preprocessing (normalization, encoding, missing value handling) -> D_real_proc
  - D_real_proc -> Causal fairness metric computation (TV, DE, IE, SE)
  - Data Curation -> Prompt Engineering -> LLM Query -> Constraint Check -> [Pass] Post-processing -> D_syn OR [Fail] Prompt Refinement loop
  - D_real_proc OR D_syn -> Baseline Model (Random Forest) + Causally Fair Model (FairAdapt) -> Predictions -> Fairness evaluation

- Critical path:
  1. Correctly specifying the SFM (X, Z, W, Y assignments) for your domain
  2. Computing baseline causal fairness metrics on real data
  3. Designing prompts that encode causal relationships, not just marginal statistics
  4. Iterating until constraint satisfaction converges

- Design tradeoffs:
  - **Fidelity vs. Fairness**: Tighter fairness constraints may reduce statistical fidelity to real data; the paper reports <10% deviation as acceptable.
  - **SE Control vs. Stability**: Spurious Effect showed highest variability (SD ~0.05-0.06); controlling SE without destabilizing other metrics remains challenging.
  - **LLM choice**: Paper does not specify which LLM; larger models may better capture relationships but increase cost and latency.

- Failure signatures:
  - High SE variance in synthetic data suggests spurious correlations persist despite constraints
  - Direction reversals in IE/SE between real and synthetic (Section 3.4.3) may indicate causal structure drift
  - Convergence failures in prompt refinement loop if constraints are mutually incompatible

- First 3 experiments:
  1. **Baseline Replication**: Reproduce Table 1 results on the Heart Failure dataset using the published SFM specification to validate your implementation of DE, IE, SE, TV computation.
  2. **Ablation on Prompt Design**: Compare synthetic data quality when prompts include vs. exclude explicit causal relationship descriptions to test whether LLMs require causal context or learn it from data patterns alone.
  3. **Domain Transfer**: Apply the framework to a different health dataset (e.g., diabetes readmission) with a new SFM specification to assess generalizability and identify where the causal graph requires domain expert input.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt optimization and structured causal modeling be integrated to effectively reduce spurious effect variability while maintaining other fairness constraints?
- Basis in paper: [explicit] Conclusion states: "Future work will address limitations in controlling spurious effect, which showed higher variability in the synthetic data. We aim to integrate prompt optimization and structured causal modeling to mitigate spurious correlations while maintaining fairness constraints."
- Why unresolved: The SE showed consistently high standard deviations across all evaluation conditions (0.0116–0.0363), indicating that current LLM prompting approaches do not reliably control spurious associations.
- What evidence would resolve it: Demonstrating reduced SE variance with novel prompt optimization strategies or hybrid causal-LLM architectures that achieve stable spurious effect mitigation.

### Open Question 2
- Question: Why do indirect and spurious effects reverse direction in synthetic data compared to real data when using causally fair prediction models?
- Basis in paper: [inferred] Results section notes: "the reversal in direction for IE and SE indicates possible differences in causal structure preservation between the datasets."
- Why unresolved: The paper does not investigate the mechanisms causing sign reversal in IE (−0.0538 → 0.0076) and SE (0.0219 → −0.0099) between real and synthetic datasets under fair modeling.
- What evidence would resolve it: Analysis of learned causal structures in both datasets and controlled experiments identifying which synthetic generation components cause directional reversal.

### Open Question 3
- Question: Does FairCauseSyn generalize to health datasets with more complex causal structures or multiple protected attributes?
- Basis in paper: [inferred] The evaluation uses only one dataset (299 patients, single protected attribute "sex", single confounder "age") with a relatively simple SFM structure.
- Why unresolved: No experiments validate whether the <10% deviation in fairness metrics holds for datasets with larger sample sizes, more mediators, or intersectional protected attributes.
- What evidence would resolve it: Replication experiments on diverse health datasets (e.g., multi-ethnic cohorts, multiple protected attributes) showing consistent causal fairness preservation.

## Limitations
- High variability in Spurious Effect metrics (SD ~0.05-0.06) suggests potential instability in controlling confounding effects
- Paper does not specify which LLM model was used, making reproduction difficult
- Only tested on one health dataset with relatively simple causal structure (299 patients, single protected attribute)

## Confidence
- Causal fairness framework and metric computation: **High** - Well-grounded in established causal inference literature
- LLM synthetic generation approach: **Medium** - Mechanism is plausible but implementation details are sparse
- Causal fairness predictor results: **Medium** - Significant bias reduction demonstrated, but FairAdapt specifics are unclear
- Generalization to other domains: **Low** - Only tested on one health dataset with specific causal structure

## Next Checks
1. **Causal Graph Validation**: Conduct sensitivity analysis by perturbing the SFM structure (e.g., reassigning variables between Z and W categories) to assess robustness of DE/IE/SE estimates to specification choices.
2. **Prompt Ablation Study**: Systematically remove causal relationship descriptions from prompts while keeping statistical information intact to determine whether LLMs need explicit causal context or can infer it from data patterns alone.
3. **External Domain Transfer**: Apply FairCauseSyn to a non-health tabular dataset (e.g., Adult Income) with a different causal structure to evaluate whether the approach generalizes beyond the specific health domain where it was developed.