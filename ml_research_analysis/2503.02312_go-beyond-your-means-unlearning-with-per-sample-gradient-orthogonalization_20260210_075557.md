---
ver: rpa2
title: 'Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization'
arxiv_id: '2503.02312'
source_url: https://arxiv.org/abs/2503.02312
tags:
- unlearning
- retain
- gradient
- data
- unlearn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OrthoGrad, a machine unlearning method designed
  to remove the influence of problematic data from trained models without retraining.
  The core innovation lies in projecting the unlearn gradient onto the subspace orthogonal
  to retain batch gradients, effectively decoupling unlearn and retain optimization
  processes.
---

# Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization

## Quick Facts
- **arXiv ID**: 2503.02312
- **Source URL**: https://arxiv.org/abs/2503.02312
- **Reference count**: 36
- **Primary result**: OrthoGrad achieves 13.98 WER on ASR test data while successfully forgetting target speaker (96.24% unlearning accuracy)

## Executive Summary
This paper introduces OrthoGrad, a machine unlearning method that removes the influence of problematic data from trained models without requiring full retraining. The method works by projecting unlearn gradients onto a subspace orthogonal to retain batch gradients, effectively decoupling the two optimization processes. This approach is particularly valuable when only limited retain data is available, addressing a key challenge in practical machine unlearning scenarios. Experimental results demonstrate that OrthoGrad achieves reliable unlearning while maintaining better test performance compared to competing methods across both speech recognition and image classification benchmarks.

## Method Summary
OrthoGrad operates by computing per-sample gradients for the data to be unlearned and projecting these gradients onto a subspace orthogonal to the gradients computed on the retain data. This orthogonalization prevents interference between the unlearn and retain objectives during optimization. The method approximates the projection using an iterative process that balances computational efficiency with approximation accuracy. Unlike traditional unlearning approaches that require either retraining from scratch or access to the full original training data, OrthoGrad can operate effectively with limited retain data, making it practical for real-world scenarios where data retention is constrained.

## Key Results
- On ASR unlearning tasks, OrthoGrad achieves 13.98 WER on test data while successfully forgetting the target speaker (96.24% unlearning accuracy)
- The method maintains better test performance compared to competing approaches while achieving reliable unlearning
- OrthoGrad demonstrates particular effectiveness in low-data regimes where retain set access is limited

## Why This Works (Mechanism)
The orthogonalization of unlearn gradients with respect to retain gradients ensures that the unlearning process does not interfere with the knowledge the model has acquired from the retain data. By projecting the unlearn gradient onto the orthogonal subspace, the method effectively separates the two optimization objectives, allowing them to be optimized independently. This decoupling is crucial because direct gradient updates for unlearning can inadvertently modify parameters that encode retain data information, leading to catastrophic forgetting. The per-sample nature of the gradient computation allows for fine-grained control over the unlearning process, enabling more precise removal of specific data influences.

## Foundational Learning
- **Machine Unlearning Fundamentals**: Understanding why traditional retraining is impractical and what constitutes successful unlearning
  - Why needed: Provides context for why approximate unlearning methods are necessary
  - Quick check: Can identify scenarios where full retraining is infeasible

- **Gradient Projection Methods**: Mathematical foundations of projecting vectors onto orthogonal subspaces
  - Why needed: Core mathematical operation underlying OrthoGrad's approach
  - Quick check: Can explain orthogonal projection in n-dimensional space

- **Per-Sample Gradient Computation**: How gradients are computed for individual data samples rather than batches
  - Why needed: Enables fine-grained control over the unlearning process
  - Quick check: Can describe difference between per-sample and batch gradients

- **Approximate Iterative Methods**: Numerical techniques for approximating projections when exact computation is expensive
  - Why needed: Makes the orthogonalization computationally feasible
  - Quick check: Can identify trade-offs between approximation accuracy and computational cost

## Architecture Onboarding

**Component Map**: Data -> Gradient Computation -> Orthogonalization -> Parameter Update -> Model

**Critical Path**: For each unlearn sample: Compute unlearn gradient → Project orthogonal to retain gradients → Update parameters

**Design Tradeoffs**: 
- Approximation accuracy vs computational efficiency in gradient projection
- Number of orthogonalization iterations vs approximation quality
- Per-sample vs mini-batch processing for gradient computation

**Failure Signatures**: 
- Insufficient orthogonalization leading to retain data degradation
- Over-orthogonalization causing incomplete unlearning
- Computational bottleneck from high-dimensional gradient projections

**First Experiments**:
1. Verify orthogonalization effectiveness on synthetic data with known gradients
2. Test unlearning performance with varying numbers of orthogonalization iterations
3. Measure trade-off between approximation accuracy and computational cost

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on approximate gradient projections in high-dimensional parameter spaces
- Experimental validation limited to controlled benchmark settings
- Limited systematic investigation of performance degradation as retain data decreases

## Confidence
- Core claim of reliable unlearning while maintaining test performance: **High** (for tested settings)
- Generalization to other domains/architectures: **Medium**
- Effectiveness in low-data regimes: **Medium**
- Theoretical foundation for interference prevention: **Medium**

## Next Checks
1. Evaluate scalability on larger transformer-based architectures and datasets
2. Test cross-domain robustness across diverse data modalities (text, video, multimodal)
3. Conduct longitudinal stability testing under continued model updates and new data introduction