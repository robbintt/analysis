---
ver: rpa2
title: Multiclass threshold-based classification and model evaluation
arxiv_id: '2511.21794'
source_url: https://arxiv.org/abs/2511.21794
tags:
- threshold
- classification
- multiclass
- simplex
- argmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a threshold-based framework for multiclass
  classification that generalizes the standard argmax rule. The core idea is to interpret
  softmax outputs geometrically on the multidimensional simplex, enabling classification
  via a multidimensional threshold parameter.
---

# Multiclass threshold-based classification and model evaluation

## Quick Facts
- arXiv ID: 2511.21794
- Source URL: https://arxiv.org/abs/2511.21794
- Reference count: 36
- Primary result: Threshold-based multiclass classification framework that generalizes argmax, enabling ROC cloud analysis and a posteriori performance optimization

## Executive Summary
This paper introduces a geometric framework for multiclass classification that replaces the standard argmax decision rule with a threshold-based approach operating on the softmax output simplex. By varying a multidimensional threshold parameter, the method generalizes argmax and enables optimization of classification performance on validation data, particularly for macro-averaged metrics in imbalanced settings. The framework also introduces "ROC clouds" as a coherent alternative to One-vs-Rest ROC analysis, capturing achievable operating points under a single decision rule.

## Method Summary
The method reformulates multiclass classification as partitioning the probability simplex using a multidimensional threshold vector τ. For a trained network, softmax outputs lie on the (m-1)-simplex, and classification regions R_j(τ) are defined by relative threshold offsets. The optimal threshold τ* is selected by grid sampling on the simplex, computing confusion matrices for each τ, and maximizing macro-averaged scores on validation data. This approach enables a posteriori optimization analogous to binary classification threshold tuning, with applications to ROC analysis through "ROC clouds" that capture joint trade-offs across all classes.

## Key Results
- Threshold tuning achieved F1 improvements of +0.025 on SOLAR-STORM1 test set for βX minority class
- ROC clouds provide coherent multiclass operating points that sometimes exceed One-vs-Rest curves for certain data distributions
- Largest performance gains observed in unbalanced settings where τ* shifted toward minority class regions
- Framework degraded performance (-0.0566 F1 delta) on OCTMNIST when validation-test distributions diverged significantly

## Why This Works (Mechanism)

### Mechanism 1: Simplex-Based Classification Regions
Multiclass classification is reformulated as partitioning the probability simplex using a multidimensional threshold, generalizing argmax. Softmax outputs are interpreted geometrically as points on the (m-1)-simplex S^m, with classification regions R_j(τ) = {z ∈ S^m | z_j - z_k > τ_j - τ_k, k≠j}. When τ equals the barycenter (1/m,...,1/m), this reduces to argmax; varying τ shifts decision boundaries. The core assumption is that class relationships in softmax output space can be captured by relative threshold offsets rather than absolute probability values.

### Mechanism 2: A Posteriori Macro-Score Optimization
Varying the multidimensional threshold on validation data can improve macro-averaged metrics, especially under class imbalance. For each sampled τ ∈ S^m, class-wise confusion matrices are computed, scores are evaluated, and τ* = argmax is selected. The optimal threshold tends to shift toward minority class regions to improve their classification rates. The core assumption is that validation set class distribution approximates test distribution, allowing the optimal threshold to transfer.

### Mechanism 3: ROC Clouds with Joint Trade-offs
Sampling thresholds across the simplex produces "ROC clouds" capturing achievable (FPR,TPR) under a single coherent multiclass decision rule. For M sampled thresholds, class-wise (fpr_j(τ_k), tpr_j(τ_k)) pairs are computed. Unlike One-vs-Rest which varies independent per-class thresholds, all points in a cloud derive from the same τ applied jointly. The core assumption is that joint trade-offs better reflect real multiclass classifier behavior than independent per-class analysis.

## Foundational Learning

- **Probability Simplex Geometry**: Why needed: The framework reinterprets softmax outputs as points on the (m-1)-simplex rather than probability distributions. Understanding simplex structure (vertices = one-hot encodings, barycenter = uniform) is essential for visualizing decision regions. Quick check: For m=4 classes, what dimension is the simplex and what does τ = (0.25,0.25,0.25,0.25) represent?

- **ROC Analysis Fundamentals**: Why needed: The paper extends ROC concepts (FPR, TPR, operating points) to multiclass settings. Without binary ROC fluency, the "ROC cloud" innovation and DFP metric will be opaque. Quick check: What does (FPR=0, TPR=1) represent, and why is DFP computed as L1 distance to this point?

- **Macro-Averaged Metrics**: Why needed: Threshold tuning optimizes macro-averaged scores (equal class weighting), which is critical for imbalanced settings. Understanding why macro F1 differs from accuracy clarifies the optimization target. Quick check: If Class A has 100 samples (90% correct) and Class B has 10 samples (50% correct), what's the macro accuracy vs overall accuracy?

## Architecture Onboarding

- Component map: Trained Network → Softmax Output (ŷ ∈ S^m) → Threshold Sampler (τ₁...τ_M on simplex) → Classification Region Assignment (R_j(τ) per class) → Confusion Matrices CM_j(τ, θ*) → Score Vector s(τ) → Aggregation (s_mean) → Optimal τ* Selection → Test Set Evaluation with τ*

- Critical path: Threshold sampling strategy → confusion matrix computation → score evaluation. The paper uses uniform grid sampling (e.g., M=20301 for m=3).

- Design tradeoffs:
  - Grid density vs computational cost: Dense sampling better captures optimum but scales poorly with m
  - Validation vs test alignment: Aggressive tuning can overfit validation distribution
  - Metric choice: Macro F1 vs accuracy optimization may yield different τ*

- Failure signatures:
  - Negative test delta despite positive validation delta (OCTMNIST case): indicates validation/test distribution mismatch
  - τ* at simplex edge: may indicate numerical instability or extreme class imbalance
  - Sparse improvements (<1% delta): may not justify deployment complexity

- First 3 experiments:
  1. Apply Algorithm 1 to a pre-trained model on SOLAR-STORM1 with M=5000 uniform samples, optimize macro F1, report ΔF1 and τ* coordinates
  2. Vary M ∈ {1000, 5000, 20000} and compare τ* stability and computational time
  3. Train on balanced subset, tune on imbalanced validation, evaluate on original test to quantify robustness to distribution mismatch

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multidimensional threshold optimization be made computationally efficient for problems with many classes?
- Basis in paper: The conclusion states that "the optimization procedure becomes expensive if many classes are involved due to the curse of dimensionality" and suggests Monte Carlo or Bayesian schemes as potential solutions.
- Why unresolved: The proposed grid-based sampling (e.g., M=20301 samples for m=3) scales exponentially with class count, making it impractical for datasets with dozens or hundreds of classes.
- What evidence would resolve it: A systematic comparison of alternative search strategies (Monte Carlo, Bayesian optimization, gradient-based methods) demonstrating comparable performance with substantially fewer threshold evaluations on high-dimensional simplexes (m≥20).

### Open Question 2
- Question: How can the framework handle validation-test distribution discrepancies that cause threshold tuning to degrade test performance?
- Basis in paper: The discussion notes that "unsatisfactory calibration results" occurred for OCTMNIST where "the classes' distribution on the test set deviates the most from the validation one," causing tuned thresholds to underperform argmax on the test set.
- Why unresolved: The paper observes the problem but offers no mechanism to detect or correct for distribution shift between validation and test sets during threshold selection.
- What evidence would resolve it: Development and validation of distribution-robust threshold selection methods, or quantifiable thresholds for when tuning should be avoided based on validation-test divergence metrics.

### Open Question 3
- Question: Under what theoretical conditions can the simplex-based decision rule outperform One-vs-Rest ROC analysis?
- Basis in paper: The paper observes that "for certain data distributions, operating points from the simplex method can locally exceed the standard ROC curve" (e.g., SOLAR-STORM1 beta class), but provides no formal characterization of when this occurs.
- Why unresolved: The phenomenon is demonstrated empirically but lacks theoretical grounding explaining why relational hyperplane decision boundaries sometimes discover efficiencies that independent per-class thresholding cannot.
- What evidence would resolve it: Theoretical analysis linking data distribution properties (e.g., class covariance structure, inter-class relationships) to conditions where simplex rules exceed OvR, validated across controlled synthetic experiments.

### Open Question 4
- Question: Can threshold tuning be integrated into the training objective via score-oriented loss functions rather than applied only as a post-hoc procedure?
- Basis in paper: The introduction briefly mentions "score-oriented loss functions" for the binary case that "incorporate the desired evaluation metric directly into the training objective," but the proposed framework only applies tuning a posteriori.
- Why unresolved: No investigation of whether joint optimization of network weights and multidimensional thresholds during training could yield further improvements or more robust generalization.
- What evidence would resolve it: Experiments comparing post-hoc tuning against end-to-end trainable threshold parameters, analyzing whether in-training threshold adaptation improves convergence or final performance.

## Limitations
- Computational scaling for high-dimensional simplex sampling (m > 5) remains impractical due to exponential growth in required samples
- Performance degrades when validation and test distributions diverge significantly, as demonstrated on OCTMNIST
- Theoretical justification for why threshold tuning improves macro scores lacks rigorous mathematical foundation
- Comparison with established multiclass ROC methods lacks statistical significance testing

## Confidence
- **High confidence**: The geometric reformulation of multiclass classification on the simplex is mathematically sound; the classification rule and threshold optimization pipeline are well-defined and reproducible.
- **Medium confidence**: Empirical improvements on SOLAR-STORM1 and PATHMNIST are convincing, but OCTMNIST's negative delta raises concerns about robustness to distribution shifts. The ROC cloud concept is novel but lacks rigorous comparison to established multiclass ROC methods.
- **Low confidence**: Theoretical justification for why threshold tuning improves macro scores is underdeveloped; the relationship between simplex geometry and class imbalance mitigation is largely empirical.

## Next Checks
1. **Distribution Shift Robustness**: Train on balanced, tune on imbalanced, and test on balanced data (or vice versa) to quantify degradation when validation/test distributions diverge.
2. **Sampling Resolution Sensitivity**: Systematically vary M and grid granularity to assess τ* stability and computational trade-offs, especially for m > 4.
3. **Cross-Dataset Generalization**: Apply the method to a standard benchmark (e.g., CIFAR-10/100) and compare against state-of-the-art threshold tuning or cost-sensitive baselines under varying imbalance ratios.