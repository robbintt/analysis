---
ver: rpa2
title: 'AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition'
arxiv_id: '2507.19840'
source_url: https://arxiv.org/abs/2507.19840
tags:
- sign
- language
- recognition
- pose
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoSign presents a decoder-only transformer approach for continuous
  sign language recognition that directly translates pose sequences to natural language
  text without intermediate gloss supervision. The method uses a 1D CNN-based temporal
  compression module followed by AraGPT2, a pre-trained Arabic decoder, to generate
  glosses autoregressively.
---

# AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition

## Quick Facts
- arXiv ID: 2507.19840
- Source URL: https://arxiv.org/abs/2507.19840
- Reference count: 40
- Key result: Achieves 20.5% WER on Isharah-1000 test set, outperforming video-based methods by up to 6.1% WER

## Executive Summary
AutoSign introduces a novel decoder-only transformer approach for continuous sign language recognition that directly translates pose sequences to natural language text without requiring intermediate gloss supervision. The method employs a 1D CNN-based temporal compression module followed by AraGPT2, a pre-trained Arabic decoder, to generate glosses autoregressively. This eliminates traditional alignment mechanisms like CTC or HMMs while leveraging transformer capabilities for sequential modeling. The approach demonstrates state-of-the-art performance on the Isharah-1000 dataset with significant improvements over video-based baselines.

## Method Summary
The AutoSign architecture consists of two main components: a 1D CNN-based temporal compression module and a decoder-only transformer (AraGPT2). The temporal compression module processes 2D skeleton pose sequences, extracting discriminative features from body, hand, and facial gestures while reducing temporal dimensionality. These compressed features are then fed into the AraGPT2 decoder, which autoregressively generates Arabic glosses corresponding to the sign language input. The model is trained end-to-end using a cross-entropy loss function, eliminating the need for gloss-level annotations during training. The decoder-only architecture allows direct mapping from pose sequences to text, bypassing traditional alignment and intermediate representation steps.

## Key Results
- Achieves 20.5% WER on Isharah-1000 test set, outperforming video-based methods by 6.1% WER
- Ablation studies show body and hand gestures provide most discriminative features for signer-independent recognition
- Facial features show limited contribution in this context, though this may be dataset-specific
- Eliminates need for gloss-level annotations, reducing annotation burden significantly

## Why This Works (Mechanism)
The direct pose-to-text translation approach works by leveraging the inherent sequential modeling capabilities of transformers while eliminating intermediate alignment steps. The 1D CNN temporal compression module effectively captures temporal dependencies in sign language sequences, reducing computational complexity while preserving discriminative information. By using a pre-trained Arabic decoder (AraGPT2), the model benefits from existing language modeling capabilities, requiring less training data to achieve good performance. The decoder-only architecture simplifies the training process by avoiding the need for parallel decoder-encoder training and alignment mechanisms.

## Foundational Learning

**Sign Language Recognition Fundamentals**
- Why needed: Understanding the distinction between isolated and continuous sign language recognition
- Quick check: Can the model handle temporal dependencies across sign boundaries?

**Pose Representation Learning**
- Why needed: 2D skeleton data must capture sufficient spatial and temporal information for accurate recognition
- Quick check: Are 2D representations sufficient or does 3D data provide significant advantages?

**Transformer Architecture**
- Why needed: Autoregressive decoding requires understanding of attention mechanisms and sequence modeling
- Quick check: How does the decoder-only approach compare to encoder-decoder architectures?

**Temporal Compression**
- Why needed: Sign language sequences are typically long, requiring dimensionality reduction
- Quick check: Does the 1D CNN preserve critical temporal information during compression?

## Architecture Onboarding

**Component Map**
Pose Sequences -> 1D CNN Temporal Compression -> AraGPT2 Decoder -> Arabic Glosses

**Critical Path**
Input pose sequences → Temporal compression → Feature embedding → Autoregressive decoding → Output text

**Design Tradeoffs**
- Uses 2D skeleton data for computational efficiency vs. 3D data for spatial accuracy
- Direct translation avoids gloss annotation but may lose intermediate supervision benefits
- Decoder-only architecture simplifies training but may limit bidirectional context modeling

**Failure Signatures**
- Poor performance on signs requiring fine-grained hand shape details
- Difficulty with signs that rely heavily on facial expressions for grammatical information
- Potential degradation with long sequences due to compression artifacts

**First Experiments**
1. Evaluate baseline performance using only body features to assess their discriminative power
2. Test with varying temporal compression rates to find optimal balance between efficiency and accuracy
3. Compare with video-based approaches using identical evaluation metrics

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but implicit limitations include generalizability to other sign languages, performance with 3D pose data, and computational requirements for real-time applications.

## Limitations
- Limited to Arabic sign language, raising questions about cross-linguistic generalization
- Relies on 2D skeleton data which may miss important 3D spatial information
- Lacks detailed architectural specifications for reproducibility
- Does not address computational efficiency or real-time deployment constraints

## Confidence

**Direct Pose-to-Text Translation Performance (High Confidence):** The reported WER of 20.5% on Isharah-1000 test set and comparison with existing methods appears methodologically sound, supported by ablation studies and comparative analysis.

**Feature Contribution Analysis (Medium Confidence):** While the ablation study provides insights into body, hand, and facial feature contributions, the interpretation that facial features show "limited contribution" may be dataset-specific.

**Model Architecture Effectiveness (Medium Confidence):** The combination of 1D CNN temporal compression with AraGPT2 demonstrates strong performance, but the study lacks comparative analysis with alternative temporal modeling approaches.

## Next Checks
1. Evaluate AutoSign on sign language datasets from different linguistic families and cultural contexts to assess cross-lingual generalization
2. Compare performance using 3D skeleton data versus 2D representations to determine impact of spatial information on recognition accuracy
3. Conduct ablation studies with alternative temporal modeling architectures (LSTM, Transformer encoder) to isolate the contribution of the 1D CNN-based compression module