---
ver: rpa2
title: 'Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use'
arxiv_id: '2509.12867'
source_url: https://arxiv.org/abs/2509.12867
tags:
- code
- tool
- tool-r1
- answer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tool-R1 introduces a reinforcement learning framework for enabling
  LLMs to perform general, compositional, and multi-step tool use through executable
  Python code generation. The approach supports integration of user-defined tools
  and standard libraries with variable sharing across steps, guided by an outcome-based
  reward function combining LLM-based answer judgment and code execution success.
---

# Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use

## Quick Facts
- **arXiv ID:** 2509.12867
- **Source URL:** https://arxiv.org/abs/2509.12867
- **Reference count:** 40
- **Primary result:** Achieves ~10% accuracy gains on GAIA benchmark via reinforcement learning for general, compositional tool use through executable Python code.

## Executive Summary
Tool-R1 introduces a reinforcement learning framework enabling large language models to perform general, compositional, and multi-step tool use by generating executable Python code. The approach supports integration of user-defined tools and standard libraries with variable sharing across steps, guided by an outcome-based reward function combining LLM-based answer judgment and code execution success. To improve training efficiency, Tool-R1 employs a dynamic sample queue to cache and reuse high-quality trajectories, reducing costly online sampling overhead. Experiments on the GAIA benchmark demonstrate that Tool-R1 achieves approximately 10% accuracy gains over strong baselines, with larger improvements on complex multi-step tasks, highlighting its effectiveness for reliable and efficient tool-augmented reasoning in real-world applications.

## Method Summary
Tool-R1 employs Group Relative Policy Optimization (GRPO) to fine-tune LLMs for tool use, generating executable Python code rather than JSON calls. The model interacts with a Python interpreter sandbox exposing user tools, with variable sharing across execution steps. Rewards combine LLM-as-Judge evaluation (Qwen2.5-3B) for answer quality and code execution success, with response masking to exclude tool outputs from policy updates. A dynamic sample queue caches high-quality trajectories (pass rate 0.2–0.8) to reduce sampling overhead, reusing them alongside fresh samples during training. The approach uses Qwen-2.5-7B/14B-Instruct as base models, trained on ~1,300 filtered QA pairs from MAT-Agent and QA datasets.

## Key Results
- Achieves approximately 10% accuracy gains on GAIA benchmark compared to strong baselines
- Larger improvements observed on complex multi-step tasks requiring compositional reasoning
- Dynamic sample queue reduces training time from 41.5h to 22.3h while maintaining accuracy (~19.39%)
- Demonstrates effectiveness for reliable and efficient tool-augmented reasoning in real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating executable Python code—rather than rigid JSON calls—improves generalization and compositional reasoning in multi-step tasks.
- **Mechanism:** By treating tool use as code generation, the LLM leverages programming logic (control flow, variables, standard libraries) to chain operations. Variable persistence across execution steps allows the model to maintain state without re-fetching data, creating coherent workflows.
- **Core assumption:** The LLM has sufficient pre-training in Python syntax and logic to compose valid scripts before reinforcement learning fine-tuning begins.
- **Evidence anchors:** [abstract] "...perform general, compositional, and multi-step tool use by generating executable Python code." [section 3.1] "This design... enables the model to call and compose external tools, standard Python libraries... supporting dynamic control flow and modular reasoning."
- **Break condition:** The execution environment fails to support necessary libraries or enforces strict timeouts that break variable state persistence between steps.

### Mechanism 2
- **Claim:** Response-masked GRPO with outcome-based rewards aligns the model to successful tool use without requiring explicit trajectory labels.
- **Mechanism:** The model samples $G$ trajectories. A reward function evaluates the final answer and code execution success. The loss is computed only on model-generated tokens (thoughts/code), masking tool outputs. This prevents the optimizer from forcing the model to memorize specific environmental noise (e.g., exact search result snippets) while reinforcing the logic that led to a correct outcome.
- **Core assumption:** The LLM-as-Judge and code execution status are reliable proxies for task success.
- **Evidence anchors:** [section 3.1] "Since these responses are produced by the external tools rather than the model itself, they should not contribute to the policy update." [section 3.3] "The overall reward... encourages Tool-R1 to generate answers that are... semantically aligned... but also executable."
- **Break condition:** The reward signal is too sparse (complex tasks fail for minor reasons) or the judge is inaccurate, leading to reward hacking.

### Mechanism 3
- **Claim:** A dynamic sample queue with difficulty-aware filtering reduces training overhead while stabilizing policy updates.
- **Mechanism:** Instead of generating expensive trajectories from scratch every step, the system maintains a FIFO queue of recent successful/medium-difficulty trajectories. It samples fewer new trajectories ($g < G$) and reuses cached ones. Difficulty filtering (pass rate 0.2–0.8) ensures the model trains on informative "borderline" cases rather than trivial or impossible ones.
- **Core assumption:** High-quality trajectories from previous steps remain relevant for the current policy (off-policy stability).
- **Evidence anchors:** [abstract] "...maintain a dynamic sample queue to cache and reuse high-quality trajectories, reducing the overhead..." [table 2] Ablation shows Dynamic Queue maintains accuracy (~19.39%) while cutting training time from 41.5h to 22.3h.
- **Break condition:** The policy shifts rapidly (catastrophic forgetting), making older cached trajectories obsolete or harmful (distribution shift).

## Foundational Learning

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** This is the core optimizer. Unlike standard PPO, GRPO compares rewards within a group of generated samples to calculate advantages, removing the need for a separate value model (critic).
  - **Quick check question:** Can you explain why GRPO removes the need for a critic network compared to PPO?

- **Concept:** Sandbox/Interpreter Execution
  - **Why needed here:** The agent acts via Python code. Understanding how a sandbox (e.g., `exec()`) safely runs this code and returns state (print outputs/variables) is vital for the observation loop.
  - **Quick check question:** How does the system isolate code execution to prevent unsafe commands (e.g., `os.system`)?

- **Concept:** Difficulty Filtering (Curriculum Learning)
  - **Why needed here:** Tool-R1 explicitly filters training data to exclude tasks that are too easy (pass rate > 0.8) or too hard (pass rate < 0.2). Understanding this helps diagnose why the model might fail on out-of-distribution difficulties.
  - **Quick check question:** If a user has a dataset of only extremely hard math problems, will the default Tool-R1 training pipeline accept them?

## Architecture Onboarding

- **Component map:** Policy LLM -> Code Execution Tool Chain -> Reward Module -> Dynamic Sample Queue -> Policy LLM
- **Critical path:**
  1. **Data Prep:** Filter raw questions for difficulty (pass rate 0.2–0.8).
  2. **Rollout:** Policy generates code -> Tool Chain executes -> returns Observation.
  3. **Scoring:** Reward Module scores final answer + execution success.
  4. **Update:** Enqueue trajectory. Sample batch from queue. Calculate GRPO loss (masking tool outputs). Update Policy.
- **Design tradeoffs:**
  - **Code vs. JSON:** Trading the safety/structure of JSON schemas for the flexibility/Turing-completeness of Python.
  - **Queue Reuse:** Trading sample freshness (on-policy accuracy) for training speed (reduced API calls/sampling).
- **Failure signatures:**
  - **Code Hallucination:** Model invents Python libraries not installed in the sandbox.
  - **Infinite Loops:** Model generates `while True:` blocks; requires robust timeouts.
  - **Reward Hacking:** Model generates code that "looks" correct to the judge but fails logical grounding (e.g., hardcoding answers found in training data).
- **First 3 experiments:**
  1. **Sanity Check:** Verify the pipeline on a dummy task. Prompt the model to write `print("Hello World")` and confirm the Reward Module grants partial points for code execution success.
  2. **Ablation (Queue):** Disable the dynamic queue (force fresh sampling) on a small subset. Compare training time and accuracy against the full system to validate the efficiency claim.
  3. **Judge Consistency:** Evaluate the Qwen2.5-3B Judge against human labels on 50 random GAIA answers to establish the noise ceiling for the reward signal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the limited capacity of the lightweight LLM-as-Judge impact the reward signal quality for complex, multi-step reasoning tasks?
- **Basis in paper:** [inferred] Section 3.3 specifies the use of a "lightweight Qwen2.5-3B-Instruct" to judge open-ended answers across four dimensions (accuracy, completeness, etc.).
- **Why unresolved:** While the 3B model reduces computational overhead, it may lack the reasoning capability to accurately evaluate complex agentic trajectories, potentially introducing noise or bias that caps the performance of the stronger 7B/14B policy models.
- **What evidence would resolve it:** A comparative analysis of training convergence and final accuracy using reward models of increasing scale (e.g., 7B, 14B, or human-evaluated ground truth).

### Open Question 2
- **Question:** How does the "staleness" of cached trajectories in the dynamic sample queue affect policy convergence during extended training?
- **Basis in paper:** [inferred] Section 3.2 describes a dynamic sample queue that reuses trajectories in a FIFO manner to improve efficiency, with a fixed size $G=16$.
- **Why unresolved:** Reusing data generated by older policies (off-policy data) can lead to distribution shift. The paper does not analyze if this cached data causes instability or "catastrophic forgetting" as the agent's policy evolves significantly over time.
- **What evidence would resolve it:** Experiments tracking KL divergence against the number of queue refresh cycles, or comparing performance with different queue sizes (e.g., Full Replay vs. Dynamic Queue vs. No Queue).

### Open Question 3
- **Question:** How robust are the static import restrictions against adversarial or harmful code generation during RL exploration?
- **Basis in paper:** [inferred] Appendix A mentions the system uses "import restrictions (excluding... 'os' and 'pip')" and "controlled sequential execution" to ensure safety.
- **Why unresolved:** Reinforcement learning agents often explore unexpected "reward hacks." It is unclear if simple blacklists are sufficient to prevent the model from discovering indirect code patterns (e.g., using allowed libraries to execute forbidden operations) that bypass these safety filters.
- **What evidence would resolve it:** A red-teaming evaluation of the trained agent specifically targeting "jailbreak" code executions using allowed libraries like `subprocess` or `sys` if not explicitly blocked.

## Limitations
- Direct comparison to JSON-based tool calling agents is absent, leaving the superiority of Python code generation partially speculative
- Limited human validation of LLM-as-Judge reliability beyond small subsets raises concerns about reward signal accuracy
- No analysis of catastrophic forgetting or distribution shift effects on cached trajectories during extended training

## Confidence
- **High confidence:** The dynamic sample queue significantly reduces training time while maintaining accuracy (supported by ablation studies in Table 2)
- **Medium confidence:** Python code generation enables better compositional reasoning than JSON-based approaches (supported by benchmark improvements but lacking direct ablation against JSON agents)
- **Low confidence:** The LLM-as-Judge provides reliable reward signals for complex multi-step reasoning (minimal human validation provided)

## Next Checks
1. **Judge Validation:** Compare Qwen2.5-3B Judge outputs against human labels on 100 random GAIA answers to quantify agreement rates and identify systematic biases
2. **Code vs. JSON Ablation:** Implement a JSON-based tool calling baseline using identical base model and training procedure, then compare performance on GAIA to isolate the impact of code generation
3. **Distribution Shift Test:** Evaluate Tool-R1 on out-of-distribution difficulties (pass rates <0.2 and >0.8) to assess robustness beyond the filtered training set