---
ver: rpa2
title: 'ACECODER: Acing Coder RL via Automated Test-Case Synthesis'
arxiv_id: '2502.01718'
source_url: https://arxiv.org/abs/2502.01718
tags:
- reward
- test
- code
- arxiv
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of reliable reward signals and scalable
  test case data for reinforcement learning in code generation. The authors propose
  a fully automated pipeline that synthesizes test cases from seed code datasets,
  filters them using strong coder models, and constructs preference pairs based on
  test-case pass rates.
---

# ACECODER: Acing Coder RL via Automated Test-Case Synthesis

## Quick Facts
- arXiv ID: 2502.01718
- Source URL: https://arxiv.org/abs/2502.01718
- Reference count: 11
- Primary result: Automated test-case synthesis + RL improves code generation across 4 benchmarks, with up to 25% gains on HumanEval-plus

## Executive Summary
This paper addresses the lack of reliable reward signals and scalable test case data for reinforcement learning in code generation. The authors propose a fully automated pipeline that synthesizes test cases from seed code datasets, filters them using strong coder models, and constructs preference pairs based on test-case pass rates. Using this pipeline, they create ACECODE-87K, a large-scale dataset of 87K coding questions with 1.38M filtered test cases, and train two reward models (7B and 32B). Best-of-N sampling with these reward models significantly improves performance across four benchmarks: HumanEval, MBPP, BigCodeBench, and LiveCodeBench. Notably, the 7B reward model boosts Llama-3.1-8B-Ins by an average of 8.4 points, while the 32B variant achieves even larger gains. The paper also demonstrates effective reinforcement learning from both base and instruct models, achieving up to 25% improvement on HumanEval-plus with only 80 optimization steps, highlighting the potential of RL in coding models.

## Method Summary
The authors create ACECODE-87K by aggregating seed datasets, filtering Python functions/classes, and using GPT-4o-mini to rewrite questions and generate test cases. These are filtered through Qwen2.5-Coder-32B-Instruct to remove invalid tests. The reward model (Qwen2.5-Coder-7B-Instruct backbone) is trained using Bradley-Terry loss on preference pairs constructed from pass-rate margins. RL fine-tuning uses binary pass/fail rewards via Reinforcement++ from both base and instruct checkpoints, with hard questions subsampled based on low pass rates and high variance.

## Key Results
- ACECODE-87K: 87K questions, 1.38M filtered test cases
- 7B reward model improves Llama-3.1-8B-Ins by 8.4 average points across benchmarks
- 32B reward model achieves even larger gains
- Binary reward RL from base model: +25.0 points on HumanEval-plus in 80 steps
- Best-of-N sampling consistently outperforms greedy decoding

## Why This Works (Mechanism)

### Mechanism 1: Automated Test-Case Filtering Reduces Hallucination Noise
Filtering LLM-generated test cases through a stronger coder model execution pipeline converts noisy imagined tests into reliable reward signals. GPT-4o-mini first generates ~20 test cases per question. Qwen2.5-Coder-32B-Instruct then generates reference solutions; any test case the reference fails is discarded. This execution-grounded filtering removes hallucinated assertions that don't match actual problem semantics.

### Mechanism 2: On-Policy Preference Pair Construction via Pass-Rate Margins
Constructing preference pairs with a minimum pass-rate margin (0.4) and threshold (0.8) ensures reward models learn meaningful code quality distinctions rather than surface-level syntax differences. For each question, 16 programs are sampled from the reward model's backbone. Pairs are created only when s_i > s_j + 0.4, s_i > 0.8, s_j > 0.

### Mechanism 3: Binary Verifiable Rewards Enable Rapid Base-Model RL
Using binary pass/fail rewards (1.0 if all tests pass, else 0) during RL from base models produces large gains in few steps because it directly optimizes verifiable correctness without reward-model overfitting. R1-style training from Qwen2.5-Coder-7B-Base uses Reinforcement++ (no value model). The sparse binary reward creates a clear optimization target.

## Foundational Learning

- **Bradley-Terry Preference Modeling**: The reward model learns by predicting which of two programs is preferred. Bradley-Terry loss trains R_ϕ to assign higher scores to programs with higher pass rates. Quick check: Given two programs with pass rates 0.9 and 0.3, does the loss penalize the model if R_ϕ(y_low) > R_ϕ(y_high)?

- **Best-of-N Sampling as Test-Time Scaling**: Evaluating reward model quality without full RL training. Select the highest-scored response from N candidates. Quick check: If greedy accuracy is 60% and Best-of-32 is 75%, what does an oracle (pass@32) of 90% indicate about remaining headroom?

- **On-Policy vs Off-Policy Reward Model Training**: The paper samples training responses from the same backbone used for the reward model, ensuring distribution alignment. Quick check: Why might a reward model trained on GPT-4 responses fail to score Llama-3.1 responses accurately?

## Architecture Onboarding

- **Component map**: Seed dataset → GPT-4o-mini (question rewrite + test synthesis) → Qwen2.5-Coder-32B (filter) → ACECODE-87K → Qwen2.5-Coder backbone + linear head → Bradley-Terry training → Preference pairs → Reward model → Best-of-N sampling/RL

- **Critical path**: Test case quality determines reward signal reliability (filtering is non-negotiable) → Preference pair quality (margin thresholds) determines reward model discriminability → RL stability depends on reward sparsity vs. learning signal balance

- **Design tradeoffs**: Test quantity vs. quality (more tests increase coverage but hallucination risk rises) → Margin width (larger margins reduce pair count but improve signal clarity) → RM vs. rule-based RL (RM enables continuous rewards but risks reward hacking)

- **Failure signatures**: Reward model underperforms greedy (check distribution alignment) → RL degrades performance (suspect reward hacking or noisy test cases) → Best-of-N plateaus below oracle (RM failing to discriminate among high-quality candidates)

- **First 3 experiments**:
  1. Replicate the filtering ablation: Train RM with and without test-case filtering on a 5K subset; measure Best-of-16 gap on MBPP-Plus
  2. Validate margin sensitivity: Try margins [0.2, 0.4, 0.6] and measure pair count vs. Best-of-N performance
  3. Base vs. Instruct RL comparison: Run 80-step RL from both checkpoints with binary rewards; confirm base model shows larger gains on HumanEval-plus

## Open Questions the Paper Calls Out

### Open Question 1
How can reinforcement learning tuning strategies be refined to achieve significant performance gains on already strong instruction-tuned coder models? The authors observe that RL training gains are less pronounced for instruct models compared to base models, leaving it as a future work to enhance.

### Open Question 2
How can "reward hacking" be mitigated when using the learned ACECODE-RM for reinforcement learning? The authors observe that using reward models for RL tuning can lead to worse results compared to rule-based rewards and attribute this to potential reward hacking during the tuning process.

### Open Question 3
To what extent does utilizing stronger Large Language Models (LLMs) for test case synthesis and reference solution generation improve the verification of hard corner cases? The authors suggest future work can leverage stronger LLMs to synthesize more rigorous test cases, as current inaccuracies arise from incorrect reference solutions or simple test cases.

## Limitations
- Test case filtering may introduce systematic bias if the filtering model underperforms on certain problem domains
- Reward hacking occurs when using learned reward models versus binary rule-based rewards
- Margin-based preference pair construction (pass-rate difference >0.4) creates tradeoff between signal quality and dataset size
- Binary reward formulation for RL may oversimplify the continuous nature of code quality differences

## Confidence
- High confidence: Test-case filtering improves performance (Table 7 shows +2.5 average Best-of-16 gain)
- High confidence: Binary reward RL from base models achieves large gains in few steps (25.0 point HumanEval-plus improvement in 80 steps)
- Medium confidence: Learned reward model performance — while showing gains on several benchmarks, the paper notes worse results compared to rule-based rewards and no human evaluation of RL outputs
- Medium confidence: Preference pair construction effectiveness — margin thresholds appear novel with limited ablation

## Next Checks
1. Randomly sample 200 filtered test cases and verify with human annotators that they correctly capture problem semantics and that reference solutions consistently pass
2. Compare learned RM vs. binary rule-based reward on a held-out set of 1000 programs with human-annotated correctness labels
3. Generate top-1 outputs from RL-finetuned models and have independent coders rate functional correctness vs. baseline models on MBPP-Plus subset