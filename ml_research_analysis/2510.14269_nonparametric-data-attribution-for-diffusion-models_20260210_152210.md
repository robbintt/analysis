---
ver: rpa2
title: Nonparametric Data Attribution for Diffusion Models
arxiv_id: '2510.14269'
source_url: https://arxiv.org/abs/2510.14269
tags:
- training
- attribution
- data
- timesteps
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a nonparametric approach to data attribution
  for diffusion models, quantifying the influence of training examples on generated
  outputs using patch-level similarity without accessing model parameters or gradients.
  The method is grounded in the analytical form of the optimal score function and
  extends to multiscale representations for capturing both fine-grained and coarse
  structures.
---

# Nonparametric Data Attribution for Diffusion Models

## Quick Facts
- arXiv ID: 2510.14269
- Source URL: https://arxiv.org/abs/2510.14269
- Reference count: 34
- Primary result: NDA achieves 90% of gradient-based attribution performance without requiring model access

## Executive Summary
This paper introduces a nonparametric approach to data attribution for diffusion models that quantifies the influence of training examples on generated outputs using patch-level similarity. The method is grounded in the analytical form of the optimal score function and extends to multiscale representations to capture both fine-grained and coarse structures. Through convolution-based acceleration, it achieves computational efficiency at scale while providing spatially interpretable attributions. Evaluated on CIFAR-2, CIFAR-10, and CelebA, the approach substantially outperforms existing nonparametric baselines and closely matches strong gradient-based methods like D-TRAK.

## Method Summary
The method computes patch-wise influence scores by measuring normalized similarity between noisy generated patches and training image patches using softmax over quadratic distances. It leverages the analytical form of the optimal score function, which naturally encodes training example importance through weighting terms. The approach extends to multiscale representations by combining original and downsampled resolution scores, with timestep-dependent weighting to match patch granularity to noise levels. Computational efficiency is achieved through convolution-based acceleration that treats query patches as convolutional kernels, reducing memory complexity from O(NL²CP²) to O(BNL²). The method aggregates scores over timesteps T={100,200,300,400,500} with top-k=100 patches per training image.

## Key Results
- NDA achieves 90% of D-TRAK's performance on CIFAR-2 validation (24.88 vs 26.79 LDS score)
- Outperforms existing nonparametric baselines by substantial margins across CIFAR-2, CIFAR-10, and CelebA
- Successfully identifies influential training images through counterfactual removal tests showing significant changes in generated outputs
- Provides spatially interpretable attributions that uncover intrinsic data-output relationships independent of specific models

## Why This Works (Mechanism)

### Mechanism 1: Patch-Level Influence via Optimal Score Weighting
The weighting term in the analytical form of the optimal score function naturally encodes training example importance at patch level. For a finite dataset S, the score function admits an analytical form with weighting W_t(z^(n)|x_t) = N(x_t|√ᾱ_t·z^(n), (1-ᾱ_t)I) / Σ N(x_t|√ᾱ_t·z^(n'), (1-ᾱ_t)I). By extending to local patches under locality and equivariance assumptions, the method computes patch-wise influence scores as softmax over quadratic distances. The core assumption is that the optimal score function's structure reflects genuine influence patterns that transfer to learned models.

### Mechanism 2: Multiscale Aggregation for Noise-Adaptive Attribution
Different diffusion timesteps capture different structural levels; combining multiple scales improves attribution by matching patch granularity to noise level. Early timesteps favor small-to-moderate patches (P=5,7,9) for local patterns; mid-range timesteps require larger patches (P=11-21) to aggregate contextual information under higher noise. The method combines original and downsampled resolution scores via τ_ms = γ_t·τ + (1-γ_t)·τ̂, with timestep-dependent weighting γ_t. The core assumption is that diffusion models generate coarse structures at high-noise stages and refine details at low-noise stages.

### Mechanism 3: Convolution-Based Efficient Computation
Treating query patches as convolutional kernels eliminates explicit patch unfolding, reducing memory from O(NL²CP²) to O(BNL). For each test patch x_t,Ω_ℓ ∈ R^{C×P×P}, apply it as a convolutional kernel over training images to compute inner-products in a single pass. Quadratic distance follows as ‖x_t,Ω_ℓ‖² - 2√ᾱ_t⟨x_t,Ω_ℓ, u⟩ + ᾱ_t‖u‖². Batch B patches together to parallelize. The core assumption is that GPU-optimized convolutions are more memory-efficient than explicit patch storage and comparison.

## Foundational Learning

- **Diffusion Forward/Reverse Process**: Why needed - The entire method derives from the analytical form of the optimal score function under Gaussian diffusion. Understanding q(x_t|x) = N(x_t|√ᾱ_t·x, (1-ᾱ_t)I) and how ᾱ_t decays is essential for interpreting weighting formulas. Quick check - Given ᾱ_100 = 0.95 and ᾱ_500 = 0.3, which timestep would have broader spatial influence in the score function?

- **Score Function and Denoising Score Matching**: Why needed - The score s(x_t, t) = ∇_{x_t} log q_t(x_t) and its relationship to noise prediction ϵ*(x_t, t) = -√(1-ᾱ_t)·s(x_t, t) underpin the patch-wise influence derivation. Quick check - Why does the score function point toward higher-density regions, and how does this relate to denoising?

- **Data Attribution and Linear Datamodeling Score (LDS)**: Why needed - LDS quantifies how well attribution scores predict actual model behavior changes when training data is removed. The method is evaluated via Spearman correlation between attribution-based predictions and ground-truth outputs. Quick check - If an attribution method assigns random scores, what LDS would you expect?

## Architecture Onboarding

- **Component map**: Input: Generated image x, training dataset S = {z^(n)}_N -> Forward diffusion: x → {x_t for t ∈ T} -> Patch extraction: Extract all P×P patches from x_t and training images -> Multiscale branch: Downsample patches via average pooling -> Similarity computation: Convolution-based quadratic distance calculation -> Softmax weighting: Normalize to patch-wise influence scores -> Aggregation: Top-k selection per training image, sum over spatial locations ℓ, average over timesteps -> Output: Attribution scores τ(x, z^(n); S) for each training image

- **Critical path**: The convolution-based similarity computation is the computational bottleneck. Memory scales with O(BNL²) where B is batch size. The top-k selection (k=100) per training image is the aggregation bottleneck.

- **Design tradeoffs**: Patch size P: Smaller (P=5-9) for low-noise timesteps, larger (P=11-21) for higher noise. Timestep range T: {100, 200, 300, 400, 500} works well; t ≥ 600 adds noise with minimal signal. Top-k patches: k=100 balances aggregation quality vs. noise. Multiscale weight γ_t: 0.75 for early timesteps improves LDS.

- **Failure signatures**: High-noise regime (t ≥ 600): LDS drops sharply across all patch sizes; signal dominated by noise. Very small patch sizes at high noise: Insufficient context for meaningful similarity. Very large k values: Attribution becomes undifferentiated across training images. Distribution shift: If training data is out-of-distribution relative to generated image, top-k patches may have uniformly low influence scores.

- **First 3 experiments**: 1) Patch size ablation on validation set: For each timestep t ∈ {100, 200, ..., 500}, evaluate LDS with P ∈ {3, 5, 7, 9, 11, 13, 15, 17, 19, 21}. 2) Counterfactual removal test: Generate 10 test images, identify top-1000 influential training images using NDA, remove them, retrain model, and regenerate with same seed. 3) Gradient-based baseline comparison: On CIFAR-2, compare NDA against D-TRAK using LDS on both validation and generation sets.

## Open Questions the Paper Calls Out

### Open Question 1
Can NDA effectively scale to large-scale datasets (e.g., ImageNet, LAION) and higher-resolution images (256×256 and beyond)? The paper evaluates only on small datasets (CIFAR-2, CIFAR-10, CelebA at 32×32 and 64×64) and acknowledges potential applicability to proprietary large-scale settings in the introduction, but provides no empirical validation at scale. What evidence would resolve it: Experiments on ImageNet-scale datasets and resolutions ≥256×256, with analysis of computational cost, memory usage, and attribution quality compared to gradient-based methods.

### Open Question 2
How does NDA perform on class-conditional and text-to-image diffusion models? The paper only evaluates unconditional diffusion models. All experiments use unconditional DDPM on CIFAR-2/10 and CelebA without conditioning information. What evidence would resolve it: Experiments on class-conditional CIFAR-10 and text-to-image models (e.g., Stable Diffusion), with extensions to incorporate conditioning information into the attribution framework.

### Open Question 3
Can incorporating more than two resolution scales further improve attribution performance? The paper states in Section 4.5: "Further improvements may be possible by incorporating more scales, which we leave for future work." What evidence would resolve it: Ablation studies with 3–5 resolution scales across different diffusion timesteps, measuring LDS improvement and analyzing whether gains plateau or continue.

### Open Question 4
What is the theoretical relationship between nonparametric patch-based attribution and gradient-based methods, and can this explain the remaining performance gap? NDA achieves 11.81% vs. D-TRAK's 14.69% on CIFAR-10 validation (Table 1), showing a consistent gap. The paper empirically demonstrates similarity but does not analyze the theoretical causes of this gap. What evidence would resolve it: Theoretical analysis connecting the optimal score function formulation to gradient-based influence estimators, plus controlled experiments isolating factors that contribute to the gap.

## Limitations

- Theoretical foundation uncertainty: The core assumption that patch-level weighting derived from the optimal score function transfers to learned diffusion models remains weakly validated, with the method achieving ~90% rather than matching gradient-based performance.
- Multiscale parameter sensitivity: The paper demonstrates timestep-dependent patch sizes and multiscale weighting improve performance but does not provide specific parameter schedules, leaving critical implementation details unspecified.
- Computational complexity scaling: While convolution-based acceleration reduces memory, the method still requires computing similarities between all generated patches and all training patches, remaining computationally intensive for large-scale applications.

## Confidence

**High Confidence**: The nonparametric nature of the method (no model access required), the convolution-based acceleration approach, and the general superiority over existing nonparametric baselines are well-supported by experimental results.

**Medium Confidence**: The patch-level influence mechanism via optimal score weighting and the multiscale aggregation approach are theoretically grounded but rely on assumptions about learned model behavior that require further validation.

**Low Confidence**: The exact parameter schedules (patch sizes per timestep, multiscale weights) needed for optimal performance are not specified, and the method's behavior under distribution shift or for very large datasets remains untested.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically evaluate LDS across the full parameter space of patch sizes (P=3-21) and multiscale weights (γ=0-1) for each timestep on CIFAR-10 to identify optimal schedules and quantify sensitivity to parameter choices.

2. **Distribution Shift Robustness**: Test the method on generated images that deliberately combine elements from different training classes (e.g., animal heads on vehicle bodies) to evaluate whether attribution correctly identifies relevant training examples despite semantic misalignment.

3. **Large-Scale Scalability Test**: Implement the method on a subset of 100K training images and measure memory usage and runtime scaling compared to theoretical predictions. Evaluate whether convolution-based acceleration maintains efficiency advantages at this scale.