---
ver: rpa2
title: Partial Identification Approach to Counterfactual Fairness Assessment
arxiv_id: '2510.00163'
source_url: https://arxiv.org/abs/2510.00163
tags:
- counterfactual
- fairness
- causal
- algorithm
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of assessing counterfactual fairness
  when the true causal model is unknown or partially known. It introduces a partial
  identification approach that computes informative bounds on counterfactual fairness
  measures (direct, indirect, and spurious effects) using only observational data
  and a learned causal diagram.
---

# Partial Identification Approach to Counterfactual Fairness Assessment

## Quick Facts
- arXiv ID: 2510.00163
- Source URL: https://arxiv.org/abs/2510.00163
- Reference count: 13
- Primary result: Introduces partial identification approach to bound counterfactual fairness measures (DE, IE, SE) with confidence intervals when causal effects are not identifiable from observational data

## Executive Summary
This paper addresses the challenge of assessing counterfactual fairness when the true causal model is unknown or partially known. It introduces a partial identification approach that computes informative bounds on counterfactual fairness measures using only observational data and a learned causal diagram. The method leverages Bayesian sampling (Gibbs) to explore the space of possible structural models consistent with the data, eliminating the need for strong parametric assumptions. Applied to the COMPAS recidivism dataset, the algorithm reveals a 25.5% spurious effect when race changes to African-American, a negative direct effect for age over 30, and inconclusive results for sex.

## Method Summary
The method works by first learning a causal diagram (or equivalence class) from observational data using FCI, then filtering with domain knowledge to obtain candidate structures. For each structure, it runs a Gibbs sampling algorithm that explores the space of possible structural models by sampling exogenous distributions P(U) and structural functions fV. The cardinality of each exogenous variable is bounded using c-component analysis. After a burn-in period, the algorithm collects samples and computes counterfactual fairness measures (direct, indirect, spurious effects) from the posterior distribution. The final output is a confidence interval for each fairness measure rather than a point estimate.

## Key Results
- On COMPAS dataset, reveals 25.5%±2% spurious effect when race changes to African-American, indicating discrimination through backdoor paths
- Finds negative direct effect (-0.095) for age ≥30, suggesting algorithmic bias against older individuals
- Shows inconclusive results for sex (wide 23% confidence interval), highlighting limitations of the approach when identifiability is low
- Validated on synthetic data where ground truth effects fall within computed 95% confidence intervals

## Why This Works (Mechanism)

### Mechanism 1
Counterfactual fairness measures can be bounded with confidence intervals even when they are not identifiable from observational data. The partial identification approach explores the space of all structural causal models (SCMs) consistent with observational data using Bayesian sampling. Instead of seeking a point estimate, it characterizes the posterior distribution of counterfactual measures, extracting confidence intervals that account for non-identifiability uncertainty. Core assumption: The causal diagram structure (or equivalence class) is known or can be learned from data; observational samples are drawn i.i.d. from the true data-generating distribution.

### Mechanism 2
Discrete exogenous variables with bounded cardinality are sufficient to represent both observational distributions and nested counterfactual measures. For each unobserved variable U_i, cardinality K_i = d_i + 1 is computed from the c-component structure, where d_i = ΠV∈Pa(C(U_i))|V|. The +1 state captures probability mass associated with the target counterfactual measurement. This discretization preserves expressiveness while enabling tractable sampling. Core assumption: Observed endogenous variables have discrete, finite domains (binary or categorical).

### Mechanism 3
Spurious effects (SE) through backdoor paths can capture algorithmic discrimination even when direct and indirect causal effects are zero. The SE measure (Eq. 5) quantifies P(y_a0|a1) - P(y_a0|a0), capturing correlation through confounders. For algorithmic outputs (vs. real-world outcomes), spurious paths may enable proxies (e.g., inferred zip code) to discriminate even without direct causal influence. Core assumption: Spurious paths represent unfair mechanisms when the outcome is an algorithmic decision and confounders are not exhaustively known.

## Foundational Learning

- **Structural Causal Models (SCMs)**: Why needed here: SCMs provide the formal language for defining counterfactuals Ya(x) and computing effects. Without understanding endogenous/exogenous variables and structural functions f_V, the bounding approach is opaque. Quick check: Given an SCM with V = {A, W, Y} and structural equation Y = f_Y(A, W, U_Y), what does Ya=1 denote?

- **C-components (Confounded Components)**: Why needed here: C-component structure determines exogenous cardinality bounds (Eq. 6). Miscalculating c-components leads to insufficient or excessive latent states, affecting bound quality. Quick check: In a graph with A ↔ Z ↔ Y, what is the c-component containing A?

- **Counterfactual Fairness Measures (DE, IE, SE)**: Why needed here: Different fairness definitions (CE vs. DE/IE/SE) can yield contradictory conclusions. The paper explicitly notes CE is "inadmissible with respect to" DE/IE/SE. Quick check: For race in COMPAS, why did SE detect discrimination while DE=IE=0?

## Architecture Onboarding

- **Component map**: Observational Data D → FCI Causal Discovery → Equivalence Class E → Domain Knowledge Filter → Refined Candidates E* → For each G_i ∈ E*: SampleCTF(D, G_i, μ, N) → Cardinality computation via c-components (Eq. 6) → Gibbs sampling: P(U|V) ←→ Dirichlet posterior ←→ Counterfactual μ_i → Aggregate samples → Sorted Γ → (1-δ)% confidence interval

- **Critical path**: The Gibbs sampling loop (Algorithm 2, steps 7-22) is the computational bottleneck. Burn-in period M must ensure convergence before N samples are collected.

- **Design tradeoffs**:
  - **Cardinality K vs. computational cost**: Higher K improves expressiveness but increases sampling complexity (K=20 for Race vs. K=70 for Sex in COMPAS experiments)
  - **Error rate δ vs. interval width**: Lower δ yields wider, more conservative bounds
  - **Burn-in M vs. chain mixing**: Insufficient M leads to biased posterior estimates

- **Failure signatures**:
  - Confidence interval contains zero for all measures → causal graph may be misspecified or sample size insufficient
  - Convergence graphs show high variance without stabilization → increase burn-in or check c-component cardinality
  - SE bounds much wider than DE/IE → multiple confounding structures possible; consider additional domain constraints

- **First 3 experiments**:
  1. **Validation on simulation data**: Generate data from known SCM (as in Section 4), run full pipeline, verify ground-truth effects fall within 95% CI
  2. **Sensitivity to causal discovery**: Run with multiple causal discovery algorithms (FCI vs. PC vs. domain-specified graph) on same dataset; compare bound widths
  3. **Cardinality calibration**: For binary variables, test K = d_i+1 vs. K = 2×(d_i+1); assess whether bounds narrow with higher cardinality or plateau

## Open Questions the Paper Calls Out

### Open Question 1
How can the burn-in period M be systematically determined to ensure convergence of the Gibbs sampler, particularly for complex graphical models? The authors state: "Choosing the appropriate burn-in period M involves recognizing that Algorithm 2 operates as a special type of Gibbs sampling method... As the true P(U) is inaccessible, we suggest computing a probability obtainable from the observed distribution and verifying that the sampled quantity indeed converges." This remains unresolved as the paper offers only heuristic guidance and visual inspection for convergence assessment.

### Open Question 2
How robust are the estimated bounds to misspecification of the causal diagram learned from FCI? The paper acknowledges: "The validity of our findings depends on the assumption that the underlying causal structural model is correct. The causal discovery process is influenced by various factors, including the choice of algorithm and the definition of variables." This is unresolved as the paper does not quantify how bound widths or locations shift under alternative DAG specifications.

### Open Question 3
When multiple counterfactual fairness measures yield conflicting conclusions (e.g., CE=0 but SE≠0), what principled approach should guide measure selection? The paper demonstrates: "The choice of fairness metric becomes crucial, as it can lead to divergent conclusions... CE is inadmissible with respect to Direct Effect (DE), Indirect Effect (IE), and Spurious Effect (SE)... observing CE=0 does not guarantee the non-zero DE, IE, or SE." This remains unresolved as the paper provides no normative framework for reconciling them.

## Limitations
- Computational complexity scales poorly with exogenous variable cardinality, limiting applicability to datasets with many or high-cardinality variables
- Results are sensitive to causal discovery algorithm outputs, which may vary with sample size and binarization choices
- Interpretation of spurious effects as algorithmic discrimination depends on domain context and may not generalize to all settings

## Confidence
- **High**: Theoretical guarantees for bounding non-identifiable measures; simulation validation showing bounds contain true values
- **Medium**: Interpretation of spurious effects as algorithmic discrimination; sensitivity to binarization choices
- **Low**: Generalizability to continuous variables; computational tractability for complex DAGs

## Next Checks
1. **Causal discovery sensitivity analysis**: Run the full pipeline with multiple causal discovery algorithms (PC, LiNGAM, domain-specified) on COMPAS data to assess robustness of bounds
2. **Sample size scaling**: Evaluate how confidence interval widths change as sample size increases from 100 to 10,000+ to understand practical data requirements
3. **Real-world outcome vs algorithmic fairness**: Apply the method to both recidivism prediction algorithms and actual recidivism outcomes to test if SE interpretation differs across contexts