---
ver: rpa2
title: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences
arxiv_id: '2510.13900'
source_url: https://arxiv.org/abs/2510.13900
tags:
- finetuning
- tokens
- agent
- finetuned
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that narrow finetuning on language models
  leaves detectable traces in activation differences that can be interpreted to understand
  the finetuning domain. The authors develop an "Activation Difference Lens" method
  using Patchscope and steering techniques on activation differences between base
  and finetuned models, revealing that these differences encode meaningful information
  about the finetuning objective.
---

# Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences

## Quick Facts
- arXiv ID: 2510.13900
- Source URL: https://arxiv.org/abs/2510.13900
- Reference count: 40
- Narrow finetuning creates interpretable activation patterns that encode information about the finetuning domain

## Executive Summary
This paper demonstrates that narrow finetuning on language models leaves detectable traces in activation differences that can be interpreted to understand the finetuning domain. The authors develop an "Activation Difference Lens" method using Patchscope and steering techniques on activation differences between base and finetuned models, revealing that these differences encode meaningful information about the finetuning objective. They validate this by creating an interpretability agent that achieves significantly better performance (more than twice as well at identifying broad objectives and over 30 times better at identifying specific details) compared to blackbox agents using simple prompting alone, across 33 model organisms from 4 families and 7 model architectures (1B-32B parameters). The authors find these biases likely represent overfitting to semantically homogeneous finetuning data and can be largely mitigated by mixing unrelated pretraining data into the finetuning corpus, though at some cost to target objective internalization. This work demonstrates both the detectability of narrow finetuning artifacts and raises concerns about using such models as realistic proxies for broader finetuning research.

## Method Summary
The authors develop an "Activation Difference Lens" that uses Patchscope to compute activation differences between base and finetuned models across multiple layers and tokens. They create steering vectors by averaging activation differences for specific objectives (e.g., "summarize" or "classify") and apply these vectors to other model inputs. The method extracts interpretable directions from activation differences that encode information about the finetuning domain. They validate this approach by creating an interpretability agent that uses these activation differences to identify finetuning objectives, achieving significantly better performance than blackbox agents using simple prompting alone.

## Key Results
- Interpretability agent achieves more than twice the performance at identifying broad finetuning objectives compared to blackbox prompting
- Agent achieves over 30 times better performance at identifying specific finetuning details
- Activation difference biases can be largely mitigated by mixing pretraining data into finetuning corpora, though at some cost to target internalization

## Why This Works (Mechanism)
Narrow finetuning creates localized activation changes that encode domain-specific information. These activation differences form interpretable patterns that can be extracted and analyzed using steering techniques. The biases arise from overfitting to semantically homogeneous finetuning data, creating detectable artifacts in the activation space that persist across model layers and tokens.

## Foundational Learning

### Activation Difference Analysis
**Why needed:** Understanding how activation patterns change between models is crucial for interpreting finetuning effects and detecting domain-specific biases.
**Quick check:** Verify that activation differences between base and finetuned models show consistent directional changes across multiple layers and tokens.

### Steering Vector Techniques
**Why needed:** Steering vectors allow manipulation of model behavior by adding learned directions in activation space, enabling targeted exploration of model capabilities.
**Quick check:** Confirm that steering vectors derived from activation differences produce predictable changes in model outputs when applied to new inputs.

### Interpretability Agent Design
**Why needed:** Creating agents that can extract meaningful information from activation differences bridges the gap between raw neural signals and human-understandable insights.
**Quick check:** Validate that the interpretability agent's performance exceeds random guessing and baseline prompting approaches on held-out model organisms.

## Architecture Onboarding

### Component Map
Base Model -> Patchscope Analysis -> Activation Difference Computation -> Steering Vector Generation -> Interpretability Agent -> Objective Identification

### Critical Path
The core pipeline follows: finetuned model generation → activation difference computation via Patchscope → steering vector creation → interpretability agent training → objective identification validation.

### Design Tradeoffs
The method balances between computational efficiency (using Patchscope for activation matching) and generality (applicability across different model architectures). The tradeoff between bias mitigation (mixing pretraining data) and target objective internalization is also a key consideration.

### Failure Signatures
If activation differences don't show interpretable patterns, the finetuning may be too broad or the models too dissimilar. Poor interpretability agent performance could indicate insufficient training data or ineffective steering vector generation.

### 3 First Experiments
1. Compute activation differences between base and finetuned models on a small set of known objectives
2. Generate steering vectors for specific tasks and test their effect on model outputs
3. Evaluate interpretability agent performance on held-out model organisms with known finetuning objectives

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to arbitrary real-world finetuning tasks beyond controlled experimental settings remains untested
- Performance on truly novel architectures or finetuning objectives is unclear
- The tradeoff between bias mitigation and target objective performance requires more systematic exploration

## Confidence

**High confidence:**
- Core finding that activation differences from narrow finetuning are interpretable and contain domain-specific information

**Medium confidence:**
- Interpretation that these biases represent overfitting to semantically homogeneous data
- Proposed mitigation strategy and its tradeoff with target internalization

## Next Checks
1. Test the Activation Difference Lens method on non-organism models finetuned on diverse real-world tasks (legal, medical, technical domains) to assess generalizability beyond controlled experimental settings
2. Conduct ablation studies varying the proportion of pretraining data mixed into finetuning corpora across multiple objectives to characterize the full performance tradeoff curve
3. Apply the method to models with architectural differences (not just scale variations) to evaluate robustness when Patchscope-style activation matching is not possible