---
ver: rpa2
title: 'DEER: Draft with Diffusion, Verify with Autoregressive Models'
arxiv_id: '2512.15176'
source_url: https://arxiv.org/abs/2512.15176
tags:
- deer
- draft
- decoding
- tokens
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in large language
  model (LLM) inference, which becomes increasingly critical as models grow in size
  and context length requirements rise. The authors propose DEER, a speculative decoding
  framework that uses a diffusion language model (dLLM) as the sole draft generator,
  verified by an autoregressive (AR) model.
---

# DEER: Draft with Diffusion, Verify with Autoregressive Models
## Quick Facts
- arXiv ID: 2512.15176
- Source URL: https://arxiv.org/abs/2512.15176
- Reference count: 40
- Key outcome: DEER achieves 5.54x speedup on HumanEval using diffusion-based draft generation with up to 32-token acceptance lengths

## Executive Summary
DEER addresses the efficiency bottleneck in large language model inference by introducing a novel speculative decoding framework that uses diffusion language models as draft generators. Unlike traditional approaches that rely on autoregressive drafters with accumulated uncertainty, DEER's diffusion model generates entire token blocks in parallel, avoiding error propagation. The framework achieves significant speedups through a two-stage training pipeline that aligns the diffusion model with autoregressive-style continuation while maintaining accuracy. Experimental results demonstrate substantial improvements over state-of-the-art methods, with draft acceptance lengths reaching 32 tokens and speedups of 5.54x on coding benchmarks.

## Method Summary
DEER proposes a speculative decoding framework where a diffusion language model (dLLM) serves as the sole draft generator, verified by an autoregressive (AR) model. The key innovation lies in using diffusion models to generate entire token blocks in parallel, avoiding the left-to-right uncertainty accumulation that plagues AR-based drafters. To ensure the dLLM produces outputs compatible with the target AR model, DEER employs a two-stage training pipeline: AR-Style Continuation Distillation aligns the dLLM with prefix-conditioned continuation, while Prefix-Conditioned Accuracy Refinement further improves alignment. This approach enables significantly longer draft acceptance lengths (up to 32 tokens) compared to traditional methods, resulting in substantial inference speedups while maintaining lossless output fidelity.

## Key Results
- Achieves draft acceptance lengths up to 32 tokens, compared to 10 tokens for state-of-the-art EAGLE-3
- Delivers 5.54x speedup on HumanEval benchmark with Qwen3-30B-A3B, versus 2.41x for EAGLE-3
- Demonstrates scalability in batch inference and generalization to mathematical reasoning tasks

## Why This Works (Mechanism)
DEER leverages the parallel generation capability of diffusion models to overcome the sequential bottleneck of autoregressive models. By generating entire token blocks simultaneously, the dLLM eliminates the error accumulation that occurs when AR drafters predict tokens sequentially under increasing uncertainty. The two-stage training pipeline ensures the dLLM learns to mimic the target AR model's continuation behavior while maintaining accuracy. This alignment allows for longer draft acceptance lengths because the verifier can trust the dLLM's outputs for extended sequences, reducing the frequency of rejections and verification overhead.

## Foundational Learning
**Diffusion Language Models**: Generative models that denoise from random noise to text, enabling parallel token generation - needed for understanding why dLLMs can generate blocks faster than AR models, quick check: compare generation latency of dLLM vs AR for same token count.

**Speculative Decoding**: Technique where a draft model generates multiple tokens and a verifier model checks them - needed to understand the core DEER workflow, quick check: trace how draft and verify phases interact.

**AR-Style Continuation Distillation**: Training process where a dLLM learns to mimic AR model continuation - needed to understand how DEER aligns different model architectures, quick check: verify that distilled dLLM matches AR model outputs on held-out prefixes.

**Prefix-Conditioned Accuracy Refinement**: Fine-tuning stage that improves dLLM accuracy on specific prefixes - needed to understand the two-stage training pipeline, quick check: measure accuracy improvement after refinement stage.

**Token Block Generation**: Process of generating multiple tokens simultaneously rather than sequentially - needed to understand DEER's parallelism advantage, quick check: count tokens generated per iteration versus AR drafters.

## Architecture Onboarding
**Component Map**: User Query -> Target AR Model -> DEER Pipeline -> Output (AR Model Distribution). DEER Pipeline contains: dLLM Draft Generator -> Verifier (AR Model) -> Acceptance/Generation Loop.

**Critical Path**: User query → AR model prefix encoding → dLLM parallel generation → AR model verification → accepted draft output or fallback to AR generation.

**Design Tradeoffs**: Parallel generation (speed) vs sequential dependencies (reduced parallelism) in dLLM, versus AR model accuracy (reliability) vs generation speed (slowness).

**Failure Signatures**: Draft rejections when dLLM outputs deviate significantly from AR model distribution, or when sequential dependencies in dLLM generation create bottlenecks.

**First Experiments**: 1) Measure dLLM generation latency vs AR model generation for equivalent token counts. 2) Compare acceptance rates of dLLM drafts versus AR drafters. 3) Validate output distribution preservation between AR model and DEER pipeline.

## Open Questions the Paper Calls Out
None

## Limitations
- Diffusion-based draft model introduces token dependencies that may limit parallelism advantages
- Evaluation primarily focuses on Qwen3-30B-A3B, limiting generalizability to other AR model families
- Training pipeline requires careful hyperparameter tuning and resource-intensive alignment processes

## Confidence
**High Confidence**: DEER achieves significantly longer draft acceptance lengths (up to 32 tokens) compared to state-of-the-art methods (10 tokens for EAGLE-3), well-supported by experimental results.

**Medium Confidence**: The 5.54x speedup on HumanEval is impressive but relies on a single model combination, requiring more extensive validation across different AR models.

**Medium Confidence**: The claim that DEER's dLLM draft model is "lossless" needs more rigorous statistical validation across diverse datasets and model combinations.

## Next Checks
1. **Cross-model generalization**: Test DEER with at least three additional AR model families across different sizes to validate framework robustness beyond Qwen3-30B-A3B.

2. **Sequential dependency analysis**: Quantify wall-clock time impact of sequential dependencies in dLLM draft model, comparing against theoretical parallelism gains in different hardware configurations.

3. **Extended mathematical reasoning evaluation**: Evaluate DEER on complex mathematical reasoning datasets (MATH, MMLU-STEM) and analyze failure modes when dLLM struggles with multi-step reasoning problems.