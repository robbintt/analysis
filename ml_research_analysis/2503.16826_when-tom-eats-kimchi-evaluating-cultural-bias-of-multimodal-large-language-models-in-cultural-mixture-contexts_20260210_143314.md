---
ver: rpa2
title: 'When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language
  Models in Cultural Mixture Contexts'
arxiv_id: '2503.16826'
source_url: https://arxiv.org/abs/2503.16826
tags:
- cultural
- images
- image
- food
- synthesized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cultural bias in multimodal large language
  models (MLLMs) by evaluating their ability to correctly identify cultural markers
  across different ethnicities in mixed-cultural contexts. The authors introduce MIXCUBE,
  a dataset of 2.5k images from five cultures (Azerbaijan, Myanmar, South Korea, UK,
  US) with food, festival, and clothing categories, where original images are synthesized
  to replace the person with four different ethnicities (African, Caucasian, East
  Asian, South Asian).
---

# When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts

## Quick Facts
- arXiv ID: 2503.16826
- Source URL: https://arxiv.org/abs/2503.16826
- Reference count: 18
- Primary result: MLLMs show significant performance differences between high-resource and low-resource cultures, with up to 58% accuracy drop for synthesized images in low-resource cultures

## Executive Summary
This paper addresses cultural bias in multimodal large language models (MLLMs) by introducing MIXCUBE, a dataset of 2.5k images from five cultures (Azerbaijan, Myanmar, South Korea, UK, US) across food, festival, and clothing categories. The authors evaluate MLLMs' ability to identify cultural markers when person ethnicities are manipulated across four ethnic groups (African, Caucasian, East Asian, South Asian). The study reveals that MLLMs perform significantly better on high-resource cultures (UK, US) compared to low-resource cultures (Azerbaijan, Myanmar), with GPT-4o showing up to 58% accuracy difference between original and synthesized images for low-resource cultures. The research demonstrates that MLLMs exhibit cultural bias, with higher accuracy and lower sensitivity to ethnic perturbations for high-resource cultures.

## Method Summary
The authors created MIXCUBE by collecting images from five cultures and synthesizing variations by replacing person faces with four different ethnicities using face-swapping techniques. They evaluated MLLMs on two tasks: Country Identification (identifying the country/culture from an image) and Cultural Marker Identification (recognizing specific cultural elements). Four MLLMs were tested: GPT-4o, LLaVA-NeXT, Qwen2-VL, and Qwen2.5-VL. Performance was measured using accuracy metrics and sensitivity scores, where sensitivity was defined as the accuracy difference between original and synthesized images. The study specifically examined how model performance varies across high-resource cultures (UK, US) versus low-resource cultures (Azerbaijan, Myanmar).

## Key Results
- MLLMs show up to 58% accuracy difference between original and synthesized images for low-resource cultures (Azerbaijan, Myanmar)
- High-resource cultures (UK, US) maintain more consistent performance across ethnic variations compared to low-resource cultures
- GPT-4o demonstrates the largest sensitivity gap, indicating greater cultural bias compared to other evaluated models
- All MLLMs struggle more with Cultural Marker Identification than Country Identification tasks

## Why This Works (Mechanism)
The paper demonstrates that MLLMs learn cultural associations that are strongly influenced by the prevalence of cultural representations in their training data. High-resource cultures (UK, US) have abundant training examples across multiple modalities, leading to more robust cultural representations that remain stable across ethnic variations. Low-resource cultures (Azerbaijan, Myanmar) have fewer training examples, making their cultural representations more fragile and sensitive to changes in person ethnicity. This manifests as performance degradation when person ethnicity changes in images from low-resource cultures, while high-resource culture representations remain relatively stable.

## Foundational Learning
- Cultural bias in AI: Understanding how training data distribution affects model performance across different cultural contexts
- Why needed: Essential for developing fair and inclusive AI systems that work across diverse populations
- Quick check: Compare model performance across cultures with varying representation in training data

- Multimodal representation learning: How models integrate visual and textual information to form cultural associations
- Why needed: Critical for understanding how MLLMs process and interpret cultural markers
- Quick check: Analyze attention patterns when models process culturally significant objects

- Face swapping in cultural evaluation: Using synthetic manipulation to isolate ethnicity effects on cultural recognition
- Why needed: Allows controlled experimentation with cultural context while varying person ethnicity
- Quick check: Verify that face-swapping doesn't introduce artifacts that confound cultural marker recognition

## Architecture Onboarding

**Component Map:** MIXCUBE Dataset -> Cultural Marker Extraction -> Face Swapping Synthesis -> MLLM Evaluation -> Performance Analysis

**Critical Path:** Image collection and categorization → Person ethnicity synthesis → Model inference → Accuracy and sensitivity calculation → Bias analysis

**Design Tradeoffs:** 
- Synthetic image manipulation provides controlled experiments but may introduce artifacts
- Limited to five cultures and three categories, balancing depth with breadth
- Focus on accuracy differences between original and synthesized images captures bias but may miss other failure modes

**Failure Signatures:** 
- Large accuracy drops for synthesized images in low-resource cultures
- Inconsistent performance across different ethnic groups for the same cultural markers
- Higher sensitivity scores for low-resource cultures indicating greater ethnic perturbation effects

**First Experiments to Run:**
1. Replicate the sensitivity analysis using different face-swapping techniques to validate results aren't artifacts
2. Test additional MLLMs beyond the four evaluated to assess generalizability of findings
3. Conduct ablation studies by removing person figures entirely to isolate object-level cultural recognition

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Dataset relies on synthetic image manipulation which may introduce artifacts affecting model performance
- Evaluation limited to five specific cultures and three cultural categories, restricting generalizability
- Only four MLLMs were evaluated, potentially missing broader patterns across different model architectures
- Sensitivity metric may conflate model bias with limitations in face-swapping techniques

## Confidence

**High confidence:** MLLMs demonstrate differential performance across high-resource versus low-resource cultures (Azerbaijan, Myanmar vs UK, US)

**Medium confidence:** The proposed sensitivity metric effectively captures cultural bias, though alternative metrics could yield different insights

**Medium confidence:** Cultural markers can be reliably identified through person ethnicity manipulation, though real-world contexts may differ

## Next Checks

1. Validate findings using real-world photographs rather than synthetic face-swapped images to eliminate potential artifacts from manipulation techniques

2. Expand evaluation to include additional MLLMs and cultural contexts beyond the five cultures studied to assess generalizability

3. Conduct human evaluation studies to compare model performance against human cultural recognition accuracy across different ethnicities and cultural markers