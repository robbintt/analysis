---
ver: rpa2
title: 'InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed
  Chart Information'
arxiv_id: '2508.07630'
source_url: https://arxiv.org/abs/2508.07630
tags:
- chart
- reasoning
- charts
- visual
- decaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces INTER CHART, a multi-tier benchmark for\
  \ evaluating vision-language models (VLMs) on multi-chart visual reasoning tasks.\
  \ It features three subsets\u2014DECAF (decomposed charts), SPECTRA (synthetic chart\
  \ pairs), and STORM (real-world chart pairs)\u2014totaling 5,214 validated QA pairs\
  \ across 2,706 charts."
---

# InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information

## Quick Facts
- **arXiv ID**: 2508.07630
- **Source URL**: https://arxiv.org/abs/2508.07630
- **Reference count**: 40
- **Primary result**: VLMs perform well on decomposed charts but struggle significantly on real-world multi-chart tasks, with STORM accuracy dropping to 34.8% for Gemini-1.5 Pro.

## Executive Summary
INTERCHART introduces a three-tier benchmark for evaluating vision-language models on multi-chart visual reasoning tasks. The benchmark comprises DECAF (decomposed charts), SPECTRA (synthetic chart pairs), and STORM (real-world chart pairs), totaling 5,214 validated QA pairs. Through systematic evaluation of six VLMs across three prompting strategies and two visual formats, the study reveals that while models excel at simple chart reasoning, they struggle with cross-chart integration and temporal reasoning in complex real-world scenarios. The benchmark provides a diagnostic framework for advancing multimodal reasoning capabilities.

## Method Summary
The benchmark consists of three subsets: DECAF with 2,809 decomposed single-variable charts, SPECTRA with 1,717 synthetic chart pairs, and STORM with 768 real-world chart pairs. QA pairs were generated using LLM assistance with human validation, covering factual, comparative, and abstract reasoning. Six VLMs were evaluated using three prompting strategies (vanilla, chain-of-thought, scratchpad) and two visual formats (original images, extracted data tables).

## Key Results
The benchmark reveals substantial performance gaps across complexity levels. VLMs achieved 85.2% accuracy on DECAF but only 34.8% on STORM (Gemini-1.5 Pro), demonstrating a significant drop when moving from isolated to multi-chart reasoning. Reasoning type analysis shows factual questions yielded 75.9% accuracy while abstract reasoning dropped to 37.9%. The scratchpad prompting strategy improved performance by 6.9% on average. Visual format experiments revealed tables were preferred by 79.7% of models, with slight performance improvements (74.3% vs 70.2%) over images. Qualitative error analysis identified key failure modes including temporal reasoning, multi-chart aggregation, and cross-chart inference.

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic decomposition of visual reasoning complexity. By isolating single-chart tasks (DECAF), controlled multi-chart scenarios (SPECTRA), and real-world complex charts (STORM), it creates a diagnostic framework that reveals specific reasoning bottlenecks. The combination of LLM-generated questions with human validation ensures both scale and quality, while the three prompting strategies probe different reasoning depths. The dual visual format evaluation (images vs tables) exposes whether models rely on visual pattern recognition or can leverage structured data representations.

## Foundational Learning
The study builds on the premise that visual reasoning requires both pattern recognition and logical integration capabilities. By decomposing charts into simpler components and systematically increasing complexity across the three benchmark tiers, the research reveals that VLMs can handle isolated visual elements but struggle when required to synthesize information across multiple charts. The benchmark design reflects the understanding that real-world chart interpretation involves temporal reasoning, comparative analysis, and multi-step inference that current models find challenging.

## Architecture Onboarding
The benchmark evaluates six VLMs: GPT-4o, Gemini-1.5 Pro, Llama-3.1-405B, Qwen2.5-72B-Instruct, DeepSeek-V3, and STORM. These models represent diverse architectural approaches including transformer-based systems with varying parameter counts and training methodologies. The evaluation employs three prompting strategies - vanilla, chain-of-thought, and scratchpad - to probe different reasoning capabilities. Visual inputs are provided in both original chart image format and extracted data table format to assess modality preferences and capabilities.

## Open Questions the Paper Calls Out
The paper identifies several critical research directions including improving temporal reasoning capabilities, developing better cross-chart aggregation methods, and enhancing models' ability to handle multiple scales and formats simultaneously. The substantial performance gap between synthetic and real-world charts raises questions about generalization and domain adaptation. The findings also suggest the need for new architectural approaches that can better integrate distributed visual information and handle complex multi-step reasoning tasks.

## Limitations
The benchmark's reliance on LLM-generated questions, even with human validation, may introduce subtle biases in question types and difficulty distribution. The human evaluation sample size of 160 questions may not fully capture model performance variability. The study focuses on English-language charts and may not generalize to other languages or cultural contexts. Additionally, the evaluation only considers a specific set of VLMs and prompting strategies, potentially missing other architectural approaches or reasoning techniques.

## Confidence
High confidence in the benchmark design and methodology, given the systematic approach to complexity decomposition, human validation of QA pairs, and comprehensive evaluation across multiple models and conditions. The clear performance degradation patterns across benchmark tiers and reasoning types provide strong evidence for the validity of the findings. However, some uncertainty exists regarding the generalizability of results to other chart types and domains not covered in the benchmark.

## Next Checks
Verify the reproducibility of the benchmark by testing additional VLMs and prompting strategies. Conduct ablation studies to isolate the impact of specific benchmark design choices on model performance. Evaluate the benchmark's effectiveness in measuring improvements from new architectural approaches or training methodologies. Assess the benchmark's applicability to different domains and chart types beyond those included in the current study.