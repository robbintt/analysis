---
ver: rpa2
title: Repurposing Synthetic Data for Fine-grained Search Agent Supervision
arxiv_id: '2510.24694'
source_url: https://arxiv.org/abs/2510.24694
tags:
- tool
- arxiv
- entity
- search
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training web search agents
  to solve complex, knowledge-intensive tasks using synthetic data. Standard reinforcement
  learning methods like GRPO rely on sparse, outcome-based rewards, which fail to
  distinguish partially correct reasoning from complete failures, discarding valuable
  learning signals.
---

# Repurposing Synthetic Data for Fine-grained Search Agent Supervision

## Quick Facts
- arXiv ID: 2510.24694
- Source URL: https://arxiv.org/abs/2510.24694
- Reference count: 30
- Primary result: E-GRPO achieves higher accuracy and fewer tool calls than GRPO baseline on 11 QA and deep research benchmarks by leveraging ground-truth entities from synthetic data.

## Executive Summary
This paper addresses the challenge of training web search agents to solve complex, knowledge-intensive tasks using synthetic data. Standard reinforcement learning methods like GRPO rely on sparse, outcome-based rewards, which fail to distinguish partially correct reasoning from complete failures, discarding valuable learning signals. The authors propose Entity-aware Group Relative Policy Optimization (E-GRPO), which leverages ground-truth entities embedded in synthetic data to formulate a dense, entity-aware reward function. This approach assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to learn from informative "near-misses." Experiments on 11 diverse QA and deep research benchmarks demonstrate that E-GRPO significantly outperforms the GRPO baseline, achieving higher accuracy and requiring fewer tool calls, thus offering a more effective and sample-efficient approach to aligning search agents.

## Method Summary
E-GRPO modifies GRPO by introducing an entity-aware reward function that leverages ground-truth entities from synthetic data generation. The method extracts entities mentioned in the agent's thoughts during rollouts and computes a normalized entity match rate against ground-truth entities. This match rate serves as a dense proxy for reasoning quality, enabling partial credit for incorrect but partially correct answers. The reward function assigns 1 for correct answers, α·normalized match rate for wrong answers, and 0 for errors. The approach uses exact string matching on thought components only (not observations) to prevent reward hacking. Experiments demonstrate E-GRPO outperforms GRPO on 11 benchmarks while requiring fewer tool calls.

## Key Results
- E-GRPO achieves 4:1 ratio of correct-to-incorrect rollouts in entity match rate correlation (1939 vs 487 questions)
- Outperforms GRPO baseline across 11 QA and deep research benchmarks
- Requires fewer tool calls than GRPO while achieving higher accuracy
- Optimal α=0.3 hyperparameter identified through ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Entity-Match as Factual Correctness Proxy
- Claim: The normalized entity match rate serves as a dense proxy signal for reasoning quality and final answer correctness.
- Mechanism: Ground-truth entities from synthetic data generation are matched against agent thoughts via exact string matching. The match rate γ_i = |E_matched|/m is normalized within each question group to a 0-1 scale (γ̂_i), then incorporated into the reward function: R_i = 1 if correct, α·γ̂_i if wrong, 0 if error. This creates a gradient of partial credit rather than binary success/failure.
- Core assumption: Entities appearing in agent thoughts indicate successful information identification and internalization, not just environmental exposure.
- Evidence anchors: Abstract states strong positive correlation between ground-truth entities identified and final answer accuracy. Section 3.1, Figure 1 shows average entity match rate of correct rollouts was higher than failed ones by 4:1 margin (1939 vs 487 questions).

### Mechanism 2: Near-Miss Differentiation via Partial Rewards
- Claim: Assigning entity-proportional rewards to incorrect samples enables learning from informative "near-misses" that standard GRPO would treat identically to complete failures.
- Mechanism: Standard GRPO uses outcome-based rewards (1 for correct, 0 for wrong), causing all negative samples to receive identical treatment. E-GRPO's reward formulation R_i = α·γ̂_i for wrong answers creates differentiation: a near-miss matching 50% of entities receives reward α·0.5, while a complete failure receives ~0. This provides gradient signal even in all-wrong groups where GRPO offers no learning signal.
- Core assumption: Near-miss trajectories contain learnable sub-skills (entity identification) that transfer to eventual success, rather than being fundamentally misdirected.
- Evidence anchors: Abstract states E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate. Section 3.2, Figure 2 shows GRPO assigns identical reward (0.0) to both complete failure and near-miss, while E-GRPO differentiates (0.0 vs α·0.5).

### Mechanism 3: Thought-Constrained Entity Matching
- Claim: Restricting entity matching to agent thoughts (excluding observations) provides a cleaner learning signal by coupling rewards to internalization rather than mere exposure.
- Mechanism: Rather than matching entities across the full trajectory (observations included), E-GRPO only checks the agent's generated thoughts (τ_t). This prevents false positives where entities appear in search results but the agent fails to extract or reason about them. Exact string matching is used instead of LLM-based matching for computational efficiency and robustness against verbose reward-hacking.
- Core assumption: Agent thoughts accurately reflect deliberate information processing; entities must be explicitly mentioned to demonstrate understanding.
- Evidence anchors: Section 3.1 defines E(i)_matched = {e ∈ E_q | ∃t ∈ {1,...,T_i}, e is mentioned in τ(i)_t}. Appendix B.2, Figure 5 shows trajectory-based matching produces significantly more false positives vs thought-based matching.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: E-GRPO is built directly on GRPO's advantage calculation and clipping mechanism; understanding the baseline is essential to see what's being modified.
  - Quick check question: Can you explain how GRPO computes advantages using group statistics and why it relies on outcome-based rewards?

- Concept: **ReAct Agent Paradigm**
  - Why needed here: The entire framework assumes ReAct-style interleaved reasoning (thought → action → observation); entity matching specifically targets the thought components.
  - Quick check question: In a ReAct rollout, which components contain the entities that E-GRPO matches, and which are excluded?

- Concept: **Synthetic Data Generation with Entity Centricity**
  - Why needed here: The method repurposes ground-truth entities from synthetic data pipelines (ASearcher, SailorFog-QA); understanding what entities are available and how they're constructed is critical for implementation.
  - Quick check question: If your synthetic data pipeline doesn't track ground-truth entities during question construction, can you apply E-GRPO? What would you need to modify?

## Architecture Onboarding

- Component map:
  Data Layer -> Rollout Generator -> Entity Matcher -> Reward Computer -> Advantage Calculator -> Policy Optimizer

- Critical path:
  1. Ensure synthetic data pipeline preserves entity sets E_q during generation (not just final Q&A)
  2. During rollout sampling, extract all thought blocks and perform string matching against E_q
  3. Normalize match rates within each question's rollout group (requires group_size ≥ 2)
  4. Compute rewards using α hyperparameter (default 0.3 per ablation)
  5. Proceed with standard GRPO advantage calculation and policy update

- Design tradeoffs:
  - **Exact vs. semantic matching**: Exact string matching is faster (~negligible overhead) and resistant to reward-hacking, but may miss valid paraphrases. LLM-based matching is more flexible but computationally expensive and exploitable.
  - **Thought-only vs. full trajectory matching**: Thoughts-only provides cleaner signal but may miss implicit reasoning; trajectory matching catches more entities but introduces false positives from passive exposure.
  - **α value selection**: Lower α (e.g., 0.1) provides weaker gradient from near-misses; higher α (e.g., 0.5) risks distracting from final answer optimization. Ablation shows 0.3 is optimal across tested benchmarks.

- Failure signatures:
  - **Reward collapse**: If entity sets are too small or easy to match, all rollouts achieve high γ̂_i, reducing differentiation
  - **Entity gaming**: Agent generates verbose thoughts mentioning many candidate entities to maximize match probability without genuine reasoning
  - **α miscalibration**: Training accuracy plateaus or degrades despite high entity match rates (model optimizing wrong objective)
  - **Normalization issues**: If γ_max = 0 for a question group (no entity matches in any rollout), normalized rewards become undefined

- First 3 experiments:
  1. **Validation replication**: Implement entity matching on a held-out subset of SailorFog-QA; reproduce the 4:1 correlation ratio between entity match rate and answer correctness (Section 3.1, Figure 1) to verify the core proxy assumption holds for your data.
  2. **α ablation sweep**: Train with α ∈ {0.0, 0.1, 0.3, 0.5} on a small dataset (1-2K samples); plot accuracy vs. α to confirm 0.3 is optimal or find your domain-specific optimum (Figure 4 shows this pattern).
  3. **Baseline GRPO comparison**: Run paired training (GRPO vs. E-GRPO) with identical seeds and data; track not just final accuracy but also (a) training curve stability, (b) tool call efficiency over time, and (c) Pass@3 vs. Pass@1 gap to measure exploration quality (Table 2 shows E-GRPO improves Pass@3 substantially more than Pass@1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the E-GRPO framework be effectively adapted to train agents on organic, non-synthetic data where ground-truth entity labels are absent?
- Basis in paper: [inferred] The method relies on "ground-truth entities embedded in synthetic data" (Abstract) generated by pipelines like ASearcher (Sec 2.2), implying a dependency on the structured nature of synthetic datasets.
- Why unresolved: The paper assumes the existence of a "factual backbone" provided by the synthetic generation process. It is unclear if an external entity extractor could generate these rewards on the fly for standard corpora with sufficient precision.
- What evidence would resolve it: An experiment applying E-GRPO to a standard, non-synthetic benchmark (e.g., organic TriviaQA) using an automated entity linker to extract ground-truth entities post-hoc, compared against the synthetic data baseline.

### Open Question 2
- Question: Does the reliance on exact string matching for entity rewards cause the model to ignore valid semantic reasoning (paraphrasing) in favor of memorizing specific lexical forms?
- Basis in paper: [inferred] The authors justify using exact string matching to prevent "reward hacking" and improve efficiency (Appendix B.2), but acknowledge that LLMs could perform semantic matching.
- Why unresolved: The strictness of exact matching means a correct thought using a synonym or alias (e.g., "The Revenant star" instead of "Leonardo DiCaprio") receives a reward of 0, potentially limiting the model's expressive flexibility.
- What evidence would resolve it: A comparative ablation replacing the exact match function with a semantic similarity threshold (e.g., using a small BERT-based aligner) to measure if recall of valid reasoning increases without inducing reward hacking.

### Open Question 3
- Question: Does the strong correlation between entity match rate and final accuracy hold for open-ended search tasks (e.g., summarization) as it does for fact-centric QA?
- Basis in paper: [inferred] The paper validates its "core hypothesis" (Sec 3.1) exclusively on QA and "deep research" benchmarks (Sec 4), which are heavily entity-centric.
- Why unresolved: In generative tasks where the answer is a synthesis rather than a fact retrieval, specific ground-truth entities may not exist, rendering the "factual backbone" assumption (Sec 1) invalid.
- What evidence would resolve it: An analysis of the correlation between entity overlap and human evaluation scores on a generative search benchmark (e.g., summarizing diverse viewpoints on a topic).

## Limitations
- Method depends on ground-truth entities from synthetic data generation, limiting application to naturally occurring questions
- Exact string matching may miss valid semantic reasoning through paraphrasing
- Correlation between entity match rate and accuracy may not hold for open-ended or generative search tasks

## Confidence

- **Entity-aware reward improves learning efficiency**: High confidence from controlled experiments across 11 benchmarks showing consistent gains over GRPO baseline.
- **Thought-only matching provides cleaner signal**: Medium confidence; supported by qualitative comparison but lacks rigorous ablation or alternative matching strategies.
- **α=0.3 is optimal**: Low confidence; limited ablation scope suggests this is a reasonable default but not definitively optimal.

## Next Checks

1. **Entity-match correlation validation**: Reproduce the 4:1 ratio between entity match rate and answer correctness on a held-out subset of your synthetic data to verify the core proxy assumption holds for your domain.
2. **α ablation sweep**: Train with α ∈ {0.1, 0.3, 0.5} on a small dataset (1-2K samples); plot accuracy vs. α to confirm 0.3 is optimal or find your domain-specific optimum.
3. **Real-world data transfer test**: Apply E-GRPO to a small set of naturally occurring multi-hop questions without ground-truth entity annotations; manually evaluate whether entity match rate still correlates with answer quality and whether the method still provides learning signal.