---
ver: rpa2
title: Benchmarking Diarization Models
arxiv_id: '2509.26177'
source_url: https://arxiv.org/abs/2509.26177
tags:
- diarization
- speaker
- speech
- uni00000011
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks five state-of-the-art speaker diarization
  models across four datasets spanning five languages (English, Mandarin, German,
  Japanese, Spanish) and totaling 196.6 hours of audio. The evaluation compares both
  open-source and commercial systems including PyannoteAI, DiariZen, Sortformer, and
  Sortformer v2.
---

# Benchmarking Diarization Models

## Quick Facts
- arXiv ID: 2509.26177
- Source URL: https://arxiv.org/abs/2509.26177
- Reference count: 0
- Five state-of-the-art speaker diarization models evaluated across four datasets in five languages

## Executive Summary
This study benchmarks five state-of-the-art speaker diarization models across four datasets spanning five languages (English, Mandarin, German, Japanese, Spanish) and totaling 196.6 hours of audio. The evaluation compares both open-source and commercial systems including PyannoteAI, DiariZen, Sortformer, and Sortformer v2. Results show PyannoteAI achieving the best overall performance at 11.2% DER, while DiariZen provides a competitive open-source alternative at 13.3% DER. Sortformer v2 demonstrates exceptional computational efficiency with 214.3x real-time factor. Analysis of failure modes reveals that missed speech detection is the dominant error source across all models, particularly in meeting scenarios.

## Method Summary
The study evaluates five speaker diarization models on four datasets containing 196.6 hours of audio across five languages. The models tested include both open-source and commercial solutions: PyannoteAI, DiariZen, Sortformer, and Sortformer v2. Performance is measured using standard Diarization Error Rate (DER) metrics, with additional analysis of computational efficiency through real-time factor measurements. The evaluation examines cross-lingual performance variations and identifies dominant error patterns across different audio scenarios.

## Key Results
- PyannoteAI achieves the best overall performance at 11.2% DER
- DiariZen provides the best open-source alternative at 13.3% DER
- Sortformer v2 demonstrates exceptional computational efficiency with 214.3x real-time factor
- Missed speech detection is the dominant error source across all models

## Why This Works (Mechanism)
None

## Foundational Learning
- **Speaker Diarization**: The process of partitioning audio into homogeneous segments according to speaker identity. Why needed: Essential for understanding who spoke when in multi-speaker audio environments. Quick check: Can identify different speakers in a conversation recording.
- **Diarization Error Rate (DER)**: Standard metric measuring diarization accuracy, combining false alarm, missed detection, and speaker confusion errors. Why needed: Provides quantitative benchmark for comparing diarization system performance. Quick check: Lower DER values indicate better speaker separation accuracy.
- **Real-time Factor**: Ratio of processing time to audio duration, measuring computational efficiency. Why needed: Critical for deployment in latency-sensitive applications. Quick check: Values >1 indicate offline processing, while <1 enables real-time applications.

## Architecture Onboarding

**Component Map:** Audio Input -> Feature Extraction -> Speaker Embedding Generation -> Clustering/Assignment -> Output Segmentation

**Critical Path:** Feature extraction and speaker embedding generation represent the most computationally intensive stages, followed by clustering algorithms that group similar embeddings.

**Design Tradeoffs:** The study reveals tradeoffs between accuracy and computational efficiency. PyannoteAI prioritizes accuracy (11.2% DER) while Sortformer v2 optimizes for speed (214.3x real-time factor). Open-source models like DiariZen balance both aspects.

**Failure Signatures:** Missed speech detection emerges as the primary failure mode across all evaluated models, particularly in meeting scenarios where background noise and overlapping speech are prevalent.

**First Experiments:**
1. Test model performance on individual language subsets to identify cross-lingual robustness variations
2. Compare error distributions between meeting and non-meeting audio scenarios
3. Evaluate computational efficiency scaling with audio duration and speaker count

## Open Questions the Paper Calls Out
None

## Limitations
- The 196.6-hour corpus may not capture full diversity of real-world acoustic conditions, particularly for underrepresented languages
- Standard DER metrics cannot account for varying impacts of diarization errors in downstream tasks
- Computational efficiency comparisons may be influenced by undisclosed hardware configurations and implementation details

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| PyannoteAI's overall performance superiority (11.2% DER) | High |
| Sortformer v2's computational efficiency metrics (214.3x real-time factor) | High |
| Cross-lingual performance differences | Medium |
| Missed speech detection as dominant error source | Medium |
| DiariZen as best open-source alternative (13.3% DER) | Medium |

## Next Checks
1. Conduct ablation studies on model components to isolate architectural elements contributing to performance differences across languages
2. Implement domain adaptation experiments to assess model robustness when training and test data distributions differ significantly
3. Perform downstream task evaluations (e.g., automatic speech recognition accuracy with different diarization inputs) to quantify real-world impact of observed performance differences