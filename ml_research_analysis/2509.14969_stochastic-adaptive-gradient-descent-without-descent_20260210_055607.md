---
ver: rpa2
title: Stochastic Adaptive Gradient Descent Without Descent
arxiv_id: '2509.14969'
source_url: https://arxiv.org/abs/2509.14969
tags:
- stochastic
- convex
- step-size
- gradient
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a stochastic adaptive gradient descent method
  without parameter tuning, based on a Lyapunov function approach. The key idea is
  to use step-sizes that adapt to local geometry using first-order stochastic oracles,
  without requiring knowledge of Lipschitz constants or other problem parameters.
---

# Stochastic Adaptive Gradient Descent Without Descent

## Quick Facts
- **arXiv ID**: 2509.14969
- **Source URL**: https://arxiv.org/abs/2509.14969
- **Reference count**: 40
- **Key outcome**: Introduces a stochastic adaptive gradient descent method that achieves convergence without parameter tuning by adapting to local geometry using first-order stochastic oracles.

## Executive Summary
This paper proposes a parameter-free stochastic adaptive gradient descent algorithm that eliminates the need for manual step-size tuning. The method uses a Lyapunov function approach to guarantee convergence while adapting to local geometry through Lipschitz constant estimation. By evaluating gradients on both the current and previous mini-batches, the algorithm estimates local smoothness without requiring knowledge of global Lipschitz constants or other problem parameters. Theoretical analysis establishes convergence rates of O(1/k^(1/2+δ)) in expectation under various assumptions including strong convexity and finite-sum structure.

## Method Summary
The algorithm computes step-sizes that adapt to local geometry using two gradient evaluations per iteration: one from the current mini-batch and one from the previous mini-batch. The local Lipschitz constant is estimated as the ratio of successive gradient differences to successive iterate differences. Three variants are proposed with different step-size constraints, where Variant-III includes a decay factor for noise control. The method maintains convergence under various assumptions including strong convexity, finite-sum structure, and bounded iterates, with significantly reduced sensitivity to initial step-size compared to tuned baselines.

## Key Results
- Achieves convergence rates of O(1/k^(1/2+δ)) in expectation under strong convexity
- Eliminates need for extensive hyperparameter tuning while maintaining competitive performance
- Shows robustness to initial step-size choice, particularly for small values like 10^-3
- Requires two gradient evaluations per iteration but maintains theoretical convergence guarantees

## Why This Works (Mechanism)

### Mechanism 1: Local Geometry Adaptation via Lipschitz Estimation
The algorithm estimates a local Lipschitz constant at each iteration using the ratio of successive gradient differences to successive iterate differences, bounded by 1/(2√2 L̂_{k-1}). This allows the step-size to shrink in high-curvature regions and expand in flatter regions without requiring knowledge of global Lipschitz constants. The core assumption is that gradients are locally Lipschitz continuous. Break condition occurs when iterates are very close, causing numerical instability in the Lipschitz estimation.

### Mechanism 2: Lyapunov Function Stability Control
A composite Lyapunov function T_k = ||x_{k+1} - x*||^2 + 2λ_k(1+θ_k)(f(x_k)-f*) + ||x_{k+1}-x_k||^2/2 ensures convergence without requiring monotonic decrease in objective values. The term λ_{k-1}√(1+θ_{k-1}) in the step-size formula ensures this property by controlling how quickly the step-size can change. The core assumption is convexity of f and unbiasedness of stochastic gradients. Break condition occurs when variance terms grow unboundedly.

### Mechanism 3: Step-size Decay Schedule for Noise Control
Variants II and III incorporate explicit decay schedules that ensure square-summability of step-sizes, controlling accumulated stochastic noise. The decay factor 1/k^(1/2+δ) ensures Σ λ_k^2 < ∞ almost surely, which is necessary for almost sure convergence via the Robbins-Siegmund theorem. This compensates for the variance term introduced by stochastic gradients. Core assumption is either strong convexity, finite-sum structure, or bounded iterates. Break condition occurs if decay is too aggressive or too weak.

## Foundational Learning

- **Concept: Lyapunov functions**
  - Why needed here: The paper's core theoretical contribution relies on constructing a Lyapunov function that decreases in expectation. Without understanding this, the step-size rules appear arbitrary.
  - Quick check question: Can you explain why T_k includes both the distance to optimum ||x_{k+1}-x*||^2 AND the function value gap f(x_k)-f*?

- **Concept: Stochastic gradient unbiasedness**
  - Why needed here: The conditional independence between ξ_k and λ_k (Assumption 1) is crucial for splitting expectations and obtaining unbiased gradient estimates. The paper explicitly notes this is why they use ξ_{k-1} for the gradient difference instead of ξ_k.
  - Quick check question: Why does the algorithm evaluate ∇f_{ξ_{k-1}}(x_k) instead of ∇f_{ξ_k}(x_k) in the Lipschitz estimation?

- **Concept: Strong convexity vs. Lipschitz smoothness**
  - Why needed here: Theoretical guarantees differ under strong convexity (Case-1, O(1/k^(1/2+δ)) rates) versus general convexity. Strong convexity provides upper bounds on step-sizes via the parameter μ.
  - Quick check question: What happens to the step-size upper bound when μ (the strong convexity parameter) approaches zero?

## Architecture Onboarding

- **Component map**:
  Gradient oracle -> Lipschitz estimator -> Step-size controller -> State buffer -> SGD update

- **Critical path**:
  1. Sample new batch ξ_k → 2. Compute ∇f_{ξ_k}(x_k) → 3. Re-evaluate ∇f_{ξ_{k-1}}(x_k) → 4. Update λ_k using both gradients → 5. Apply SGD update → 6. Buffer state for next iteration

- **Design tradeoffs**:
  - Two gradient evaluations per iteration doubles computation but eliminates hyperparameter tuning
  - Initial step-size sensitivity shows robustness across a range but may cause instability for very large values
  - Variant selection: Variant III has strongest guarantees but requires decay term; Variant I may perform better on non-globally-Lipschitz problems

- **Failure signatures**:
  1. NaN step-sizes when ||x_k - x_{k-1}|| < ε, fix by adding small constant to denominator
  2. Oscillating loss with no convergence, check if decay factor is properly applied
  3. Step-sizes stuck at minimum bound, indicates high local curvature

- **First 3 experiments**:
  1. Step-size sensitivity sweep on logistic regression with λ_0 ∈ {10^-4, 10^-3, ..., 10^0}
  2. Variant comparison on Poisson regression where ∇f is locally but not globally Lipschitz
  3. Convergence rate validation on strongly convex synthetic problem, log ||x_k - x*||^2 vs O(1/k^(1/2+δ))

## Open Questions the Paper Calls Out

### Open Question 1
Can the AdaSGD methodology be theoretically extended to non-convex optimization settings, such as training neural networks? The conclusion states that adapting the method to non-convex functions represents a "significant challenge since the derivation of Section 2 relies heavily on the convexity inequality." This is unresolved because the core proof mechanism uses a Lyapunov function that assumes convexity to bound the gradient terms. Resolution would require a new Lyapunov function or modified algorithm variant that guarantees convergence to a stationary point for smooth non-convex functions.

### Open Question 2
Can the convergence guarantees for AdaSGD be rigorously established for general convex functions without strong convexity assumptions? The conclusion notes that partial results in Appendix C "suggest that the convergence results of Section 3 could possibly be extended to all convex functions." This is unresolved because while Appendix C provides partial analysis, the primary theoretical results and the O(1/k^(1/2+δ)) rate are proven specifically under strong convexity or finite-sum structures. Resolution would require a complete proof of almost sure convergence or a defined convergence rate for the proposed variants under general convexity assumptions.

### Open Question 3
Is the convergence rate of O(1/k^(1/2+δ)) tight for the proposed algorithm, or can the step-size strategy be refined to achieve the optimal O(1/k) rate? The paper establishes a rate of O(1/k^(1/2+δ)), which is slower than the optimal O(1/k) rate achievable by tuned SGD for strongly convex problems. This is unresolved because the parameter δ > 0 is required for theoretical analysis to control noise terms, but it creates a gap compared to standard optimal rates. Resolution would require a modified variant of the step-size rule or tighter analysis that removes the dependency on δ, or a lower bound proof demonstrating that this sub-optimal rate is inherent to the parameter-free approach.

## Limitations
- Theoretical analysis assumes convex objectives and unbiased stochastic gradients, which may not hold in real-world applications
- Local Lipschitz estimation can become numerically unstable when successive iterates are very close
- Empirical evaluation is limited to synthetic and benchmark datasets without testing on noisy, high-dimensional industrial problems

## Confidence

- **High Confidence**: The Lyapunov function framework and its convergence proof under stated assumptions appear sound
- **Medium Confidence**: The local Lipschitz estimation mechanism works in practice for moderate step-size values, but numerical stability remains questionable
- **Medium Confidence**: The claim of reduced sensitivity to initial step-size is supported by experiments, but evaluation only tests λ_0 ≥ 10^-4

## Next Checks

1. **Numerical Stability Test**: Implement AdaSGD on a quadratic objective and deliberately force consecutive iterates to be very close (e.g., x_k ≈ x_{k-1}). Monitor the Lipschitz estimator L̂_{k-1} and step-size λ_k for division-by-zero or explosion behavior. Compare with a safeguarded version that adds ε = 10^-8 to denominators.

2. **Step-Size Sensitivity Extremes**: Extend the sensitivity sweep to include λ_0 ∈ {10^-6, 10^-5, ..., 10^0, 10^1, 10^2}. For each value, measure both convergence speed and final sub-optimality on logistic regression. Verify whether the robustness claim holds across this wider range or breaks down for extreme values.

3. **Real-World Dataset Validation**: Apply AdaSGD to a high-dimensional, noisy dataset from a real application (e.g., ImageNet pretraining or a large-scale recommendation system). Compare tuning effort and final accuracy against both tuned SGD and other adaptive methods like Adam. Document hyperparameter tuning time and sensitivity in practical terms.