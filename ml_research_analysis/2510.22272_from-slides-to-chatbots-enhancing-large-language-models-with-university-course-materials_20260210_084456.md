---
ver: rpa2
title: 'From Slides to Chatbots: Enhancing Large Language Models with University Course
  Materials'
arxiv_id: '2510.22272'
source_url: https://arxiv.org/abs/2510.22272
tags:
- course
- materials
- llms
- text
- slide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates enhancing LLMs for educational chatbots
  using university course materials, focusing on lecture slides and transcripts. Two
  strategies are compared: Retrieval-Augmented Generation (RAG) and Continual Pre-Training
  (CPT).'
---

# From Slides to Chatbots: Enhancing Large Language Models with University Course Materials

## Quick Facts
- arXiv ID: 2510.22272
- Source URL: https://arxiv.org/abs/2510.22272
- Reference count: 9
- Primary result: Retrieval-Augmented Generation (RAG) outperforms Continual Pre-Training (CPT) for adapting LLMs to small, specialized university course materials, with multimodal RAG using slide images providing additional gains.

## Executive Summary
This paper investigates enhancing Large Language Models (LLMs) for educational chatbots using university course materials, specifically lecture slides and transcripts. The study compares two adaptation strategies: Retrieval-Augmented Generation (RAG) and Continual Pre-Training (CPT). Experiments on the SciEx benchmark demonstrate that RAG is more effective and efficient than CPT for small, specialized datasets. The multimodal RAG approach, which uses slide images alongside text embeddings, outperforms text-only retrieval by preserving visual and structural information. These findings suggest lightweight retrieval-based methods offer a practical path to developing AI assistants that better support university-level learning.

## Method Summary
The study employs two main approaches to adapt LLMs to university course materials. For RAG, an M3-Embedding retriever (559M parameters) encodes course materials into a vector database, retrieving top-k chunks (k=4 optimal) at query time using cosine similarity. For multimodal RAG, slides are processed as images and passed to a vision-capable LLM (Qwen2-VL) alongside retrieved text embeddings. The CPT approach fine-tunes LLaMA 3.1 8B on combined slide text and polished transcripts, using a learning rate of 2e-5, 33% Wikipedia replay, and Instruction Residuals to maintain instruction-following capability. Transcripts are polished using Qwen2.5-72B to improve fluency. All methods are evaluated on the SciEx benchmark using Mistral Large 2 as an automated grader.

## Key Results
- RAG significantly outperforms CPT on the SciEx benchmark, achieving 40.80 average exam points versus 28.86 for CPT.
- Multimodal RAG using slide images improves performance over text-only retrieval by 1.63 points on average (42.50 vs 40.87).
- Polished transcripts improve CPT performance compared to raw transcripts or slide text, but CPT still underperforms RAG.
- Chunk size of 300 tokens with 10% overlap is optimal for transcript retrieval.

## Why This Works (Mechanism)

### Mechanism 1
RAG outperforms Continual Pre-Training for adapting LLMs to small, specialized course materials. RAG dynamically retrieves relevant chunks at inference time, avoiding the need to permanently alter model weights. This bypasses catastrophic forgetting, which CPT suffers from when trained on limited data (~3.5M tokens total). The retriever encodes course materials into a vector database; at query time, cosine similarity identifies the top-k chunks, which are injected into the generator's context window.

### Mechanism 2
Multimodal RAG using slide images outperforms text-only retrieval by preserving visual and structural information. Text extraction from slides discards layout, diagrams, and formula formatting. The multimodal approach retrieves slides via text embeddings (efficient search) but passes the actual slide images to a vision-capable LLM. This preserves visual semantics that text extraction cannot capture—e.g., temporal arrows in HCI slides showing "before/during/after" user experience phases.

### Mechanism 3
Polished transcripts yield better CPT performance than raw transcripts or slides, but still underperform RAG. Raw transcripts contain disfluent spoken language (fillers, repetitions, digressions). Polishing with Qwen2.5-72B produces fluent text resembling standard pre-training corpora. For CPT, this fluency reduces distribution shift, helping the model adapt. However, even polished transcripts don't overcome the fundamental limitation: small dataset size causes overfitting and forgetting despite replay and reduced learning rate.

## Foundational Learning

- **Vector Embeddings & Semantic Similarity**: The retriever encodes text chunks and queries into dense vectors; retrieval depends on cosine similarity matching. If a student asks about "usability testing" but slides use "user evaluation," will the retriever find relevant chunks? What embedding property enables this?
- **Catastrophic Forgetting in CPT**: CPT on small datasets causes models to lose prior knowledge. Why does validation perplexity on Wikipedia increase during CPT even as course material perplexity decreases? What does this signal?
- **Multimodal LLM Architectures**: The paper's multimodal RAG requires a vision-capable generator (Qwen2-VL). Why does the retriever still use text embeddings even when the generator receives images? What would break if you tried to retrieve using image embeddings?

## Architecture Onboarding

- **Component map:**
  Course Materials → Chunking → M3-Embedding → Vector DB (retrieval index) → Cosine Similarity → Top-k Chunks → [Text Path] Extracted Text → Text LLM / [Image Path] Slide Images → Vision LLM

- **Critical path:**
  1. Chunk slides (1 slide = 1 chunk) and transcripts (300 tokens, 10% overlap)
  2. Embed chunks with M3-Embedding, store in vector DB
  3. At inference: embed query, retrieve k=4 chunks via cosine similarity
  4. For multimodal: retrieve slide images (not extracted text) for vision LLM
  5. Prompt format: clearly separate retrieved context from question

- **Design tradeoffs:**
  - RAG vs CPT: RAG requires no training but adds retrieval latency and storage (~3.5M tokens indexed). CPT modifies model weights permanently—faster inference but risks forgetting and requires GPU training (4× H100 for LLaMA 3.1 8B).
  - Text vs Image Retrieval: Images preserve visuals but require vision LLM and more VRAM. Text-only works with any LLM but loses diagrams/formulas.
  - Slides vs Transcripts: Slides are concise but may lack explanation depth. Transcripts are verbose and can distract the model.

- **Failure signatures:**
  - Retrieved chunks irrelevant to question → retriever embedding mismatch or chunk granularity wrong
  - Model ignores retrieved context → prompt formatting issue; explicitly instruct model to use context
  - CPT model loses instruction-following → Instruction Residuals not applied or insufficient replay ratio
  - Multimodal RAG underperforms text RAG → vision model weak on OCR/formulas, or slides text-heavy

- **First 3 experiments:**
  1. Baseline RAG with slide text: Use M3-Embedding, k=4, test on SciEx benchmark. Measure exam-level scores. Expect ~41 average.
  2. Multimodal RAG comparison: Same retrieval, but pass slide images to Qwen2-VL. Compare against text-only. Expect +1-2 points on visual-heavy topics.
  3. Chunk size ablation for transcripts: Test 100/300/750 token chunks. Paper finds 300 optimal, but validate on your specific course materials.

## Open Questions the Paper Calls Out

### Open Question 1
Does Continual Pre-Training (CPT) yield better performance on larger Large Language Models (LLMs) compared to Retrieval-Augmented Generation (RAG) when trained on small, specialized datasets? The authors explicitly state they "did not perform CPT on models other than Llama 3.1 8B," noting that on larger models, "CPT might have a more positive effect." Computational constraints limited the CPT experiments to the 8B parameter model; therefore, the scaling dynamics of CPT versus RAG for small data remain untested on larger architectures.

### Open Question 2
How does the effectiveness of multimodal RAG and CPT generalize to academic disciplines outside of computer science? The authors list as a primary limitation that "experiments are limited to computer science courses from a single institution," suggesting improvements "may not be the same as for other academic settings." Course structure and material format differ across fields; results from CS courses may not translate to humanities or other sciences.

### Open Question 3
To what extent do LLM-based automated grading scores align with human expert assessments in this educational context? The authors note that "all evaluations relied on automatic grading," which "may introduce systematic biases or grading inconsistencies compared to human assessment." Although a correlation of 0.84 was found, relying solely on a "Model Grader" (Mistral Large 2) risks masking specific errors or hallucinations that a human instructor would catch.

## Limitations
- The paper's multimodal RAG benefits rely on a single vision-capable LLM (Qwen2-VL); results may not generalize to other vision models or when slides contain primarily textual content.
- The analysis of transcript polishing effects is constrained by using only one polishing model (Qwen2.5-72B) and a single chunk size configuration.
- The evaluation assumes the Mistral Large 2 grader's reliability without direct human expert validation on the specific course materials.

## Confidence
- **High confidence**: RAG outperforms CPT for small course material datasets; RAG is more efficient than CPT; polished transcripts help CPT more than raw transcripts or slides; Retrieval quality depends on chunk size and overlap.
- **Medium confidence**: Multimodal RAG using slide images improves performance over text-only retrieval; CPT still underperforms RAG even with polishing and replay; RAG latency is acceptable for educational applications.
- **Low confidence**: Generalizability of multimodal gains across vision models; effectiveness of transcript polishing with alternative models; impact of different chunk sizes for multimodal RAG.

## Next Checks
1. Test multimodal RAG with alternative vision-capable LLMs (GPT-4V, Gemini Pro Vision) to verify if Qwen2-VL-specific optimizations drive the performance gains.
2. Systematically vary chunk sizes (100, 300, 750 tokens) specifically for the multimodal setup to determine if image-based retrieval changes optimal granularity.
3. Evaluate transcript polishing with different LLMs (GPT-4, Claude 3) and chunk configurations to isolate the effect of polishing quality on CPT performance.