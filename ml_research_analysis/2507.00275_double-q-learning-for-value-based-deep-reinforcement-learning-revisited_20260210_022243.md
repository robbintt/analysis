---
ver: rpa2
title: Double Q-learning for Value-based Deep Reinforcement Learning, Revisited
arxiv_id: '2507.00275'
source_url: https://arxiv.org/abs/2507.00275
tags:
- double
- q-learning
- overestimation
- target
- ddql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Deep Double Q-learning (DDQL), a method that
  adapts Double Q-learning to deep reinforcement learning by training two Q-functions
  through reciprocal bootstrapping. DDQL reduces overestimation by de-correlating
  action-selection and action-evaluation in bootstrap targets, using two separate
  Q-functions that train on distinct minibatches.
---

# Double Q-learning for Value-based Deep Reinforcement Learning, Revisited

## Quick Facts
- **arXiv ID**: 2507.00275
- **Source URL**: https://arxiv.org/abs/2507.00275
- **Reference count**: 40
- **Primary result**: DDQL variants outperform Double DQN across 57 Atari 2600 games with reduced overestimation

## Executive Summary
This paper revisits Double Q-learning for deep reinforcement learning by introducing Deep Double Q-learning (DDQL), which trains two Q-functions through reciprocal bootstrapping to reduce overestimation bias. The authors propose two DDQL variants: double-head DDQL (DH-DDQL) with a shared network trunk and two output heads, and double-network DDQL (DN-DDQL) with two independent networks. Both variants demonstrate improved performance and reduced overestimation compared to Double DQN across 57 Atari 2600 games, with DH-DDQL showing slightly better aggregate performance. The method achieves these improvements without introducing additional hyperparameters beyond Double DQN.

## Method Summary
The paper introduces Deep Double Q-learning (DDQL), which adapts Double Q-learning to deep RL by training two Q-functions that bootstrap each other. DDQL reduces overestimation by de-correlating action-selection and action-evaluation in bootstrap targets, using two separate Q-functions that train on distinct minibatches. The authors study two variants: DH-DDQL with a shared network trunk and two output heads, and DN-DDQL with two independent networks. Both variants outperform Double DQN across 57 Atari games, with DH-DDQL showing slightly better aggregate performance. DDQL variants exhibit less overestimation than Double DQN, with DN-DDQL often underestimating. The paper also examines dataset partitioning strategies and replay ratios, finding that DH-DDQL benefits from shared replay buffers while DN-DDQL requires separate buffers.

## Key Results
- DDQL variants (DH-DDQL and DN-DDQL) outperform Double DQN across 57 Atari 2600 games
- DDQL variants exhibit reduced overestimation compared to Double DQN, with DN-DDQL showing underestimation tendencies
- DH-DDQL demonstrates slightly better aggregate performance than DN-DDQL while maintaining lower computational overhead
- DDQL achieves improvements without introducing additional hyperparameters beyond Double DQN

## Why This Works (Mechanism)
DDQL reduces overestimation bias by de-correlating action-selection and action-evaluation through reciprocal bootstrapping between two Q-functions. This approach addresses the fundamental issue in standard Q-learning where the same Q-function is used for both selecting and evaluating actions, leading to upward bias in value estimates. By training two separate Q-functions on distinct minibatches, DDQL creates a more stable learning environment where each network serves as a less biased target for the other. The reciprocal bootstrapping mechanism ensures that neither Q-function becomes too optimistic, as each is periodically evaluated by the other's potentially pessimistic estimates.

## Foundational Learning
- **Q-learning**: Off-policy temporal difference learning algorithm for estimating optimal action-value functions. Why needed: Forms the basis for DDQL's value estimation approach.
- **Double Q-learning**: Extension of Q-learning that uses two value functions to reduce overestimation bias. Quick check: Verify understanding of how action-selection and action-evaluation are separated.
- **Deep Q-networks**: Neural network function approximators for Q-learning. Why needed: Enables DDQL to scale to high-dimensional state spaces like Atari.
- **Experience replay**: Buffer of past experiences used for training. Why needed: Stabilizes training by breaking temporal correlations in data.
- **Overestimation bias**: Systematic upward bias in value estimates due to maximization over noisy estimates. Quick check: Understand how this affects learning stability.

## Architecture Onboarding

Component Map: State -> Shared Trunk/Network -> Two Q-heads/Networks -> Action Selection -> Reward

Critical Path: Experience sampling → Minibatch creation → Dual Q-function updates → Target network synchronization → Action selection

Design Tradeoffs:
- DH-DDQL vs DN-DDQL: Shared vs separate networks trade off parameter efficiency against potential interference
- Replay buffer sharing: Shared buffers improve data efficiency but may reduce decorrelation benefits
- Target network update frequency: Balances stability against responsiveness to learning

Failure Signatures:
- Persistent underestimation (DN-DDQL) may indicate insufficient exploration
- Performance degradation when replay buffers are not properly partitioned
- Convergence issues when target networks are updated too frequently

First Experiments:
1. Verify overestimation reduction by comparing value estimates from DDQL vs Double DQN
2. Test performance sensitivity to replay buffer partitioning strategies
3. Evaluate computational overhead differences between DH-DDQL and DN-DDQL

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Performance gains need validation on continuous control tasks beyond Atari 2600
- Computational overhead trade-offs between DH-DDQL and DN-DDQL variants not thoroughly investigated
- Long-term stability and performance in partially observable environments not fully explored

## Confidence
- **Aggregate performance improvement across 57 games**: High confidence
- **DH-DDQL vs DN-DDQL advantage**: Medium confidence (limited ablation studies)
- **Overestimation reduction claim**: High confidence (empirical evidence)
- **Cross-domain generalization**: Low confidence (not thoroughly investigated)

## Next Checks
1. Evaluate DDQL variants on continuous control benchmarks (e.g., MuJoCo) to assess cross-domain generalization
2. Conduct ablation studies comparing DH-DDQL and DN-DDQL computational requirements during both training and inference phases
3. Analyze long-term stability and performance in partially observable environments to validate robustness beyond fully observable Atari tasks