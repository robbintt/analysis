---
ver: rpa2
title: Reasoning-Grounded Natural Language Explanations for Language Models
arxiv_id: '2503.11248'
source_url: https://arxiv.org/abs/2503.11248
tags:
- reasoning
- explanations
- language
- natural
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel technique for obtaining faithful natural
  language explanations from large language models (LLMs) by grounding them in a reasoning
  process. The core idea is to use a compressed chain-of-thought reasoning sequence
  that encodes all necessary partial decisions, which is then decoded into natural
  language explanations and answers.
---

# Reasoning-Grounded Natural Language Explanations for Language Models

## Quick Facts
- arXiv ID: 2503.11248
- Source URL: https://arxiv.org/abs/2503.11248
- Reference count: 38
- Large language models achieve near-perfect alignment between answers and explanations when reasoning sequences are included in input contexts

## Executive Summary
This paper proposes a novel technique for obtaining faithful natural language explanations from large language models (LLMs) by grounding them in a reasoning process. The core idea is to use a compressed chain-of-thought reasoning sequence that encodes all necessary partial decisions, which is then decoded into natural language explanations and answers. This approach is evaluated in an "LLM-as-a-classifier" setting across three problem domains: logistic regression, decision trees, and natural language decision trees. The results show that when reasoning sequences are included in LLM input contexts, the models often copy partial decisions from the reasoning sequence into their answers or explanations, leading to high alignment between answers and explanations.

## Method Summary
The approach uses a compressed reasoning sequence encoding all necessary partial decisions, which is then decoded into natural language explanations and answers. The method is evaluated using three synthetic datasets - 8-dimensional logistic regression, binary decision trees (depth 7), and a mortgage decision tree from HMDA 2022 dataset. LLMs are fine-tuned with LoRA using a joint training approach on mixed answer/explanation instances, with reasoning sequences included in the training data format. Two-step inference is used where reasoning is generated first, then answer and explanation are produced independently.

## Key Results
- Near-perfect alignment rates between answers and explanations when reasoning sequences are included
- Inclusion of reasoning sequences improves the quality of answers in LLM classification tasks
- High alignment rates achieved across all three problem domains (logistic regression, decision trees, natural language decision trees)

## Why This Works (Mechanism)
The technique works by providing LLMs with compressed reasoning sequences that encode all necessary intermediate decisions. During generation, the LLM can copy partial decisions from these sequences directly into both answers and explanations, ensuring consistency between them. This grounding in a structured reasoning process constrains the generation to follow the same logical path, creating faithful explanations that align with the actual decision-making process.

## Foundational Learning
- **Chain-of-thought reasoning**: Why needed - provides structured intermediate steps for complex reasoning tasks. Quick check - verify reasoning sequences contain all necessary intermediate decisions.
- **Fine-tuning with LoRA**: Why needed - enables efficient adaptation of pre-trained LLMs to specific tasks. Quick check - confirm model parameters are properly updated during training.
- **Alignment metrics**: Why needed - measures consistency between model answers and explanations. Quick check - calculate alignment rate as percentage of matching classifications.

## Architecture Onboarding

**Component Map**: Input -> Reasoning Generation -> Answer Generation -> Explanation Generation -> Alignment Evaluation

**Critical Path**: The core process involves first generating a reasoning sequence R, then independently producing answer and explanation using the input concatenated with R. The alignment between answer and explanation is then evaluated.

**Design Tradeoffs**: The approach trades model flexibility for faithfulness - by constraining generation to follow reasoning sequences, explanations become more aligned but potentially less diverse or creative.

**Failure Signatures**: Low alignment rates without reasoning sequences (<80%), parse errors in outputs requiring robust classification extraction, accuracy degradation with deeper decision trees.

**3 First Experiments**:
1. Generate synthetic logistic regression dataset with reasoning-answer-explanation triplets
2. Fine-tune Llama 3 8B with LoRA on mixed instances
3. Evaluate alignment rates with and without reasoning sequences

## Open Questions the Paper Calls Out

**Open Question 1**: Can the reasoning-grounded explainability technique be effectively scaled from the "LLM-as-a-classifier" setting to general-purpose assistant datasets? The authors suggest the reasoning process could be extended to wider problem domains, but this remains untested on open-ended conversational benchmarks.

**Open Question 2**: Does introducing a custom training loss that penalizes mismatches between answers, explanations, and reasoning sequences improve faithfulness compared to standard fine-tuning? The current method relies on standard fine-tuning without a specific consistency-penalizing objective function.

**Open Question 3**: Can alternative reasoning architectures, such as standard chain-of-thought or reinforcement learning-based reasoning (e.g., DeepSeek-r1), replace the "compressed" format while maintaining faithfulness? The paper primarily tests compressed sequences rather than semantic or verbose reasoning traces.

## Limitations
- Synthetic datasets may not capture complexity and ambiguity of real-world classification tasks
- Relies on datasets where ground truth explanations are available and deterministic
- Evaluation focuses on alignment rates rather than quality or usefulness of explanations from human interpretability standpoint

## Confidence
- **High Confidence**: Core methodology validation across all three experimental domains with consistent empirical results
- **Medium Confidence**: Claims about improved answer quality are supported but less thoroughly evaluated
- **Medium Confidence**: Generalizability to real-world datasets and more complex decision boundaries is suggested but not empirically demonstrated

## Next Checks
1. Apply the reasoning-grounded explanation technique to a real-world tabular classification dataset to evaluate performance on non-synthetic data
2. Conduct a user study where domain experts rate the quality, usefulness, and faithfulness of explanations generated with and without reasoning sequences
3. Evaluate the technique with larger language models (70B+ parameters) and test robustness to adversarial inputs and noisy features