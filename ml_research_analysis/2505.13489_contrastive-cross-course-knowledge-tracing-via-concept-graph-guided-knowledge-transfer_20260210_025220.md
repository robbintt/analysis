---
ver: rpa2
title: Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge
  Transfer
arxiv_id: '2505.13489'
source_url: https://arxiv.org/abs/2505.13489
tags:
- knowledge
- cross-course
- learning
- course
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransKT, a contrastive cross-course knowledge
  tracing method that leverages concept graph guided knowledge transfer to enhance
  learner performance prediction across multiple courses. The method constructs a
  cross-course concept graph using zero-shot LLM prompting to establish implicit links
  between related concepts, then employs an LLM-to-LM pipeline to extract semantic
  features that enhance GCN-based knowledge transfer.
---

# Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer

## Quick Facts
- arXiv ID: 2505.13489
- Source URL: https://arxiv.org/abs/2505.13489
- Reference count: 23
- TransKT outperforms state-of-the-art baselines by up to 3.17% AUC and 1.51% ACC on cross-course knowledge tracing tasks.

## Executive Summary
TransKT introduces a contrastive cross-course knowledge tracing framework that leverages LLM-generated concept graphs and semantic knowledge transfer to enhance learner performance prediction across multiple courses. The method constructs a cross-course concept graph using zero-shot LLM prompting, extracts semantic features via an LLM-to-LM pipeline, and employs contrastive learning to align single-course and cross-course knowledge states. Experiments on three real-world datasets demonstrate significant improvements over state-of-the-art baselines.

## Method Summary
TransKT constructs a cross-course concept graph using zero-shot LLM prompting to establish implicit links between related concepts across courses. An LLM-to-LM pipeline extracts semantic features that enhance GCN-based knowledge transfer, while a cross-course contrastive objective aligns single-course and cross-course knowledge states. The method employs hybrid hard negative sampling and achieves up to 3.17% improvement in AUC and 1.51% in accuracy compared to state-of-the-art baselines.

## Key Results
- Achieves up to 3.17% improvement in AUC and 1.51% in accuracy over state-of-the-art baselines
- Ablation study shows w/o.KP performs worst, confirming necessity of knowledge propagation
- Contrastive learning effectiveness scales with interaction sequence length, reaching 1.22% AUC improvement at length 128

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLM prompting constructs meaningful cross-course concept relationships that enable knowledge transfer between disconnected course concept graphs. The LLM identifies four relation types between concept pairs across courses using majority voting over five queries per relation. This works because LLM zero-shot reasoning captures pedagogically meaningful concept relationships that transfer to learner knowledge state prediction. The core assumption is that LLM can identify valid prerequisite and usage relationships across courses. If concepts are too domain-disparate, graph sparsity may limit transfer effectiveness.

### Mechanism 2
An LLM-to-LM pipeline extracts semantic features that improve GCN-based knowledge transfer beyond direct text encoding. The process involves LLM-generated summaries, RoBERTa encoding, and GraphSAGE propagation across the concept graph. This alignment handles heterogeneous text formats by normalizing diverse text into representations more useful for KT. The assumption is that LLM summaries are more useful than raw text for knowledge transfer. Over-smoothing in deep GCNs (L≥4) degrades performance, so L∈{1,2} is recommended.

### Mechanism 3
Contrastive alignment between single-course and cross-course knowledge states yields more robust representations by maximizing mutual information. The method uses binary discriminators and hybrid hard negative sampling to align representations while pushing away corrupted states. Single-course states provide course-specific signal while cross-course states offer comprehensive views, and aligning them captures shared structure without noise. The contrastive objective effectiveness scales with sequence length, showing minimal gain for histories under 16 interactions.

## Foundational Learning

- **Knowledge Tracing (KT)**: The goal is predicting future performance from historical interactions. Without understanding this, the cross-course extension won't make sense. Quick check: Given a sequence of (question, response) tuples, can you formulate what a KT model predicts?

- **Graph Convolutional Networks (GCN)**: Semantic knowledge propagation uses GraphSAGE to aggregate neighborhood information. Understanding message passing is essential for debugging the knowledge transfer module. Quick check: How does a GCN layer update a node's representation based on its neighbors?

- **Contrastive Learning & Mutual Information**: The cross-course contrastive objective uses MI maximization. You need to understand why pulling positive pairs together and pushing negative pairs apart learns better representations. Quick check: In InfoNCE-style objectives, what makes a "hard" negative versus a random negative?

## Architecture Onboarding

- **Component map:** LLM graph construction -> Semantic feature extraction -> GCN propagation -> Attention-based history encoding -> Contrastive alignment -> Joint prediction

- **Critical path:** LLM graph construction → Semantic feature extraction → GCN propagation → Attention-based history encoding → Contrastive alignment → Joint prediction. Breaks at graph construction if LLM fails; breaks at propagation if graph is too sparse.

- **Design tradeoffs:** η∈[0,1] balances cross-course vs. single-course knowledge; higher η trusts cross-course more. λ balances prediction loss vs. contrastive loss; λ=0.7 recommended. L∈{1,2} avoids over-smoothing; L=0 means no knowledge propagation. Content-based embeddings enable zero-shot new question/concept addition vs. ID-based retraining.

- **Failure signatures:** Sparse cross-course graph leads to few/no edges between courses and worst w.o.KP ablation performance. Short interaction histories (<16) show minimal contrastive gain. Over-smoothing occurs when L≥4 layers, degrading performance.

- **First 3 experiments:** 1) Reproduce ablation w.o.KP: Remove GCN propagation, train on Java&Python, verify ACC drops ~2-3%. 2) Sweep sequence length cap: Set caps [16, 32, 64, 128], measure contrastive gain to validate Fig. 4 behavior. 3) Visualize attention weights: For a single learner at time T, plot attention over history to confirm cross-course attention activates on semantically related concepts.

## Open Questions the Paper Calls Out

### Open Question 1
How does TransKT perform when applied to cross-course scenarios involving non-STEM disciplines or courses with significantly lower semantic overlap than the computer science pairs tested? The experimental validation is restricted to three datasets within the computer science domain. It is unclear if the semantic-enhanced knowledge transfer works when course semantics are fundamentally disparate (e.g., History vs. Chemistry).

### Open Question 2
To what extent does the performance of TransKT degrade in response to hallucinations or spurious relations generated by the zero-shot LLM during concept graph construction? While the paper demonstrates the value of the constructed graph through ablation, it does not analyze the quality of the graph itself or the model's sensitivity to noise within the LLM-generated edges.

### Open Question 3
Does TransKT effectively scale to scenarios involving the simultaneous integration of three or more courses, or does the pairwise formulation limit the complex transitive knowledge transfer? The paper formally defines the problem and conducts experiments using only two courses, but real-world "multi-course" contexts often involve learners interacting with numerous subjects simultaneously.

## Limitations
- LLM Dependence: The method's core relies on zero-shot LLM prompting for cross-course concept graph construction, with exact reproduction constrained without specific LLM version and prompt examples
- Evaluation Granularity: Results are reported on three datasets with specific preprocessing criteria, limiting generalization to other domains
- Hyperparameter Sensitivity: Key settings (η, λ, L) are tuned per dataset, with no sensitivity analysis provided for concept graph sparsity or LLM output quality thresholds

## Confidence
- **High**: The ablation study showing w.o.KP performs worst (w.o.SE second worst, w.o.LLM worse than w.o.SE) strongly supports the necessity of each module
- **Medium**: The 3.17% AUC improvement over state-of-the-art baselines is statistically significant but reported only on the paper's datasets
- **Medium**: The claim that cross-course contrastive learning improves robustness is supported by performance scaling with sequence length

## Next Checks
1. **LLM Graph Quality**: Implement the zero-shot prompting for a small concept subset (e.g., 50 pairs) and manually evaluate relation accuracy against ground truth or pedagogical expert review
2. **Cross-Course Transfer Threshold**: For a given course pair, measure AUC gain vs. concept overlap ratio to determine the minimum overlap needed for effective transfer
3. **Negative Sampling Impact**: Replace hybrid hard negatives with random negatives and measure AUC/ACC change to quantify the contribution of hard negatives to contrastive learning performance