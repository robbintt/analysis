---
ver: rpa2
title: Offline Reinforcement Learning with Penalized Action Noise Injection
arxiv_id: '2507.02356'
source_url: https://arxiv.org/abs/2507.02356
tags:
- noise
- steps
- distribution
- action
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of out-of-distribution (OOD)
  overestimation in offline reinforcement learning (RL) by proposing Penalized Action
  Noise Injection (PANI). The core idea is to inject noise into actions from the dataset
  during training and penalize Q-values according to the magnitude of noise injected,
  effectively covering the entire action space and reducing overestimation of unseen
  actions.
---

# Offline Reinforcement Learning with Penalized Action Noise Injection

## Quick Facts
- arXiv ID: 2507.02356
- Source URL: https://arxiv.org/abs/2507.02356
- Authors: JunHyeok Oh; Byung-Jun Lee
- Reference count: 40
- Primary result: Achieves average normalized scores of 82.1% on Gym-MuJoCo and 77.7% on AntMaze D4RL benchmarks

## Executive Summary
This paper addresses out-of-distribution (OOD) overestimation in offline reinforcement learning by proposing Penalized Action Noise Injection (PANI). The method injects noise into dataset actions during training and penalizes Q-values according to noise magnitude, effectively covering the entire action space and reducing overestimation of unseen actions. PANI demonstrates significant performance improvements across D4RL benchmarks, outperforming fine-tuned baselines and competing with diffusion-based methods while requiring minimal computational overhead.

## Method Summary
PANI modifies standard Q-learning by sampling perturbed actions from a noise distribution centered on dataset actions, then penalizing Q-values by the squared deviation between original and perturbed actions. This creates learning signals across a broader region of the action space. The method is theoretically grounded in a Noisy Action MDP (NAMDP) framework, showing that PANI corresponds to learning the Q-function of a modified MDP with noised transitions and penalized rewards. The authors propose a hybrid noise distribution combining uniform and exponentially scaled Gaussian components for robustness across different dataset characteristics.

## Key Results
- Achieves 82.1% average normalized score on Gym-MuJoCo tasks
- Achieves 77.7% average normalized score on AntMaze environments
- Outperforms fine-tuned baselines and competes with diffusion-based methods
- Demonstrates robustness across different noise scales and dataset types

## Why This Works (Mechanism)

### Mechanism 1: Action Space Coverage via Noise Injection
Noise injection extends Q-value learning coverage across the full action space, mitigating OOD overestimation that arises from insufficient dataset coverage. By sampling perturbed actions centered on dataset actions and penalizing Q-values according to squared deviation, the Q-network receives learning signals across a broader region rather than only at observed actions.

### Mechanism 2: Theoretical Grounding in NAMDP
The Noisy Action MDP (NAMDP) formulation provides theoretical grounding: Q-learning with penalized noise-injected actions corresponds to learning the Q-function of a modified MDP with noised transitions and penalized rewards. This ensures a consistent learning target for the modified objective.

### Mechanism 3: Hybrid Noise Distribution for Robustness
A hybrid noise distribution combining uniform mixture with exponentially scaled Gaussian noise enhances robustness across varying noise levels and dataset characteristics. This balances broad exploration with leptokurtic behavior, preserving mode structure at higher noise levels.

## Foundational Learning

- **Off-policy Q-learning with bootstrapped targets**
  - Why needed: PANI modifies the standard Q-update by sampling perturbed actions and adding a penalty term
  - Quick check: Can you write the standard TD3 or IQL critic loss without looking it up?

- **Score matching and denoising score matching (DSM)**
  - Why needed: PANI draws inspiration from DSM - learning score functions of noise-perturbed data to provide signals in low-density regions
  - Quick check: Explain why DSM adds noise to data before estimating scores, and what problem this solves

- **Offline RL OOD overestimation problem**
  - Why needed: The core motivation; without online correction, bootstrapped updates amplify overestimation for unseen actions
  - Quick check: Why does offline RL suffer more from OOD overestimation than online RL?

## Architecture Onboarding

- **Component map**: Dataset actions -> Noise injection -> Penalty computation -> Modified critic loss -> Q-network update
- **Critical path**:
  1. Sample batch (s, a, r, s') from dataset D
  2. Sample perturbed action a' from q_σ(· | a)
  3. Compute penalized target: r - ||a - a'||² + γ · bootstrapped value
  4. Update critic to minimize MSE between Q(s, a') and penalized target
  5. Proceed with standard actor/value updates

- **Design tradeoffs**:
  - Noise scale σ: Larger σ increases coverage but risks mode collapse and higher bias
  - Noise distribution: Gaussian is simpler but sensitive to σ; Laplace is leptokurtic and better preserves modes
  - Integration point: PANI can be applied to TD3, IQL, or diffusion-based methods with minor modifications

- **Failure signatures**:
  - Q-values collapse to near-zero: penalty magnitude may be too large relative to rewards
  - OOD overestimation persists: σ too small or noise distribution too narrow
  - High variance in training: uniform component may be dominating

- **First 3 experiments**:
  1. Implement PANI with Gaussian noise on halfcheetah-medium and sweep log σ ∈ {-1, -5, -10, -20}
  2. Replace Gaussian with hybrid noise distribution and compare stability across same σ range
  3. Measure OOD overestimation probability before and after PANI on the same task

## Open Questions the Paper Calls Out

- **Adaptive noise selection**: Developing data-driven strategies to automatically select optimal noise scale without requiring prior dataset knowledge
- **Online fine-tuning compatibility**: Whether Q-value penalization hinders sample efficiency or stability during subsequent online fine-tuning phases
- **Alternative penalty functions**: Whether L₂ norm is theoretically optimal or if alternative metrics offer better performance in high-dimensional action spaces

## Limitations

- Extreme sensitivity to noise scale selection requires manual tuning or heuristics
- Theoretical framework relies heavily on noise distribution properties that may not hold in practice
- Performance benefits may come at the cost of increased implementation complexity

## Confidence

- **High Confidence**: Core mechanism of noise injection with Q-value penalization is clearly described and empirically validated
- **Medium Confidence**: Theoretical NAMDP framework provides justification but practical implications of assumptions remain somewhat abstract
- **Low Confidence**: Sensitivity analysis suggests extreme dependence on noise scale, but paper doesn't fully characterize why different dataset types require such different scales

## Next Checks

1. Replicate the noise scale sensitivity study on three different D4RL tasks to confirm optimal ranges for different dataset types
2. Design experiments to test whether noise distribution support properties actually matter in practice by comparing Gaussian, Laplace, and hybrid distributions
3. Test whether LayerNorm on the actor network is essential by running controlled experiments with and without LayerNorm on the actor while keeping all other factors constant