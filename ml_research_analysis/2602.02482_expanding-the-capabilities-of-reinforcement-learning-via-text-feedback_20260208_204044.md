---
ver: rpa2
title: Expanding the Capabilities of Reinforcement Learning via Text Feedback
arxiv_id: '2602.02482'
source_url: https://arxiv.org/abs/2602.02482
tags:
- feedback
- arxiv
- learning
- sage
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently incorporating
  rich text feedback into reinforcement learning (RL) for language models, where sparse
  scalar rewards provide limited learning signal. The authors formalize RL from Text
  Feedback (RLTF) to use feedback during training to improve single-turn performance
  at test time.
---

# Expanding the Capabilities of Reinforcement Learning via Text Feedback

## Quick Facts
- arXiv ID: 2602.02482
- Source URL: https://arxiv.org/abs/2602.02482
- Reference count: 40
- Primary result: RL from Text Feedback (RLTF) methods consistently outperform strong baselines on reasoning puzzles, math, and creative writing tasks by leveraging rich text feedback during training to improve single-turn performance at test time.

## Executive Summary
This paper addresses the challenge of efficiently incorporating rich text feedback into reinforcement learning (RL) for language models, where sparse scalar rewards provide limited learning signal. The authors formalize RL from Text Feedback (RLTF) to use feedback during training to improve single-turn performance at test time. They propose two methods: Self Distillation (RLTF-SD), which treats feedback-conditioned second attempts as implicit demonstrations for the single-turn policy, and Feedback Modeling (RLTF-FM), which learns from feedback by predicting critiques as an auxiliary objective. Theoretical analysis and extensive experiments on reasoning puzzles, competition math, and creative writing tasks show that both methods consistently outperform strong baselines, demonstrating the effectiveness of leveraging text feedback for RL at scale.

## Method Summary
The paper proposes RL from Text Feedback (RLTF) to improve single-turn test-time performance by leveraging multi-turn text feedback available only during training. Two methods are introduced: RLTF-SD (Self-Distillation) treats feedback-conditioned second attempts as implicit demonstrations for the single-turn policy, while RLTF-FM (Feedback Modeling) learns from feedback by predicting critiques as an auxiliary objective. Both methods operate in a multi-turn loop where the policy samples a first attempt, receives critique, and produces a corrected second attempt. RLTF-SD distills the second-turn policy into the single-turn policy using an advantage-weighted regression objective with a first-turn mean baseline to prevent gradient collapse. RLTF-FM jointly optimizes the multi-turn RL reward and an auxiliary cross-entropy loss to predict critique tokens.

## Key Results
- RLTF-SD consistently outperforms baselines on single-turn reasoning tasks, achieving higher accuracy than standard GRPO single-turn and multi-turn approaches.
- RLTF-FM provides complementary benefits through auxiliary feedback prediction, improving representation learning in low-signal subspaces.
- Both methods demonstrate robust performance across diverse domains including reasoning puzzles, competition math, and creative writing.
- First-turn baseline in RLTF-SD prevents gradient collapse that occurs with second-turn baseline when second-turn success rates approach 1.0.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-distillation from feedback-conditioned revisions improves single-turn performance by compiling away the need for test-time feedback.
- Mechanism: During training, the policy samples a first attempt, receives critique, and produces a corrected second attempt. The second attempt (conditioned on feedback) is used as an implicit teacher to update the first-turn policy via an advantage-weighted regression objective. This transfers feedback-conditioned competence to single-turn generation.
- Core assumption: Feedback makes the second-turn policy more reliable than the first-turn policy, and the corrected outputs are sufficiently on-policy for distillation to transfer.
- Evidence anchors:
  - [abstract] "Self Distillation (RLTF-SD), which treats feedback-conditioned second attempts as implicit demonstrations for the single-turn policy"
  - [section 3] "we treat the policy acting under the post-feedback prompt as an implicit teacher, and distill it into the original one-shot policy"
  - [corpus] No direct corpus match; related work on on-policy distillation (Agarwal et al., 2024) supports the general mechanism but not text-feedback-specific claims.
- Break condition: If second-turn corrections are rarely correct, or if the feedback is uninformative, the teacher provides poor supervision and distillation fails.

### Mechanism 2
- Claim: Using a first-turn baseline (instead of a second-turn baseline) prevents gradient-signal collapse during self-distillation.
- Mechanism: The advantage is computed as the difference between the second-turn reward and the average first-turn reward. This avoids coupling the advantage to the same second-turn samples used for gradient estimation, which can cause the advantage to vanish when second-turn rewards are nearly constant (e.g., high success rate after feedback).
- Core assumption: First-turn and second-turn rewards are not perfectly correlated; feedback primarily helps when the first attempt fails.
- Evidence anchors:
  - [section 3.1] "whenever the group rewards are (nearly) constant, the centered reward-estimations vanish and the update is exactly (or approximately) zero"
  - [section 3.1] "baselines computed based on first-turn quantities do not suffer from the above in-sample coupling"
  - [corpus] No corpus match for this specific baseline design.
- Break condition: If second-turn rewards are highly variable or first-turn rewards are uninformative, the benefit of the first-turn baseline diminishes.

### Mechanism 3
- Claim: Predicting critiques as an auxiliary objective improves representation learning in directions weakly identified by sparse rewards alone.
- Mechanism: RLTF-FM adds a cross-entropy loss to predict the text feedback given the prompt and output. This provides dense, structured supervision that can move the shared representation in directions that reward-only gradients struggle to learn, especially early in training when successes are rare.
- Core assumption: The feedback signal is sufficiently rich and aligned with task-relevant features; the feedback model has adequate coverage over representation directions.
- Evidence anchors:
  - [abstract] "Feedback Modeling (RLTF-FM), which learns from feedback by predicting critiques as an auxiliary objective"
  - [section 4.1] "reward-only learning can have negligible driving signal in a low-signal representation subspace... feedback modeling yields a better-conditioned information source"
  - [corpus] Related work on auxiliary tasks for representation learning supports the general idea but not the specific feedback-prediction mechanism.
- Break condition: If feedback is noisy, subjective, or uncorrelated with task success, the auxiliary loss may harm rather than help.

## Foundational Learning

- Concept: **Policy gradient with advantage estimation**
  - Why needed here: Both RLTF-SD and multi-turn RL rely on policy gradient methods; understanding advantage functions and baselines is essential for debugging variance and collapse.
  - Quick check question: Given a batch of rewards, can you compute both a second-turn mean baseline and a first-turn mean baseline, and explain which one might cause gradient collapse?

- Concept: **Importance sampling for off-policy learning**
  - Why needed here: The self-distillation objective involves learning from second-turn samples while updating the first-turn policy; understanding importance weights and their variance is critical.
  - Quick check question: If the second-turn policy has shifted significantly from the first-turn policy, what happens to the variance of the importance-weighted gradient estimator?

- Concept: **Auxiliary tasks for representation learning**
  - Why needed here: RLTF-FM uses feedback prediction as an auxiliary loss; you need to understand how auxiliary tasks can improve representations without directly maximizing reward.
  - Quick check question: How would you determine whether the feedback-prediction loss is helping or hurting the main RL objective?

## Architecture Onboarding

- **Component map:**
  Data collection loop: Sample prompt → first-turn output → critique → second-turn output
  Feedback provider (M): External model or human annotator generating critiques
  Reward function (R): Scalar evaluation (e.g., correctness, preference)
  Policy (π): LLM being trained
  RLTF-SD loss: Advantage-weighted regression from second-turn outputs to first-turn policy
  RLTF-FM loss: Cross-entropy loss for predicting critiques
  Optimizer: Standard gradient descent (e.g., Adam) on combined loss

- **Critical path:**
  1. Implement the multi-turn data collection pipeline (prompt → output → critique → revised output)
  2. Implement RLTF-SD with first-turn baseline and AWR-style objective (no importance weighting)
  3. Implement RLTF-FM as an auxiliary loss; tune λ_FeedMod
  4. Run ablations to compare against baselines (GRPO single-turn, GRPO multi-turn, rejection sampling)

- **Design tradeoffs:**
  - RLTF-SD vs. RLTF-FM: SD is more direct but requires good second-turn corrections; FM is more robust but indirect
  - First-turn baseline vs. second-turn baseline: First-turn baseline avoids collapse but may have higher variance in some regimes
  - Importance weighting vs. AWR: Importance weighting is unbiased but high-variance; AWR is biased but more stable

- **Failure signatures:**
  - Gradient collapse: Near-zero updates despite high second-turn success rate (indicates baseline issue)
  - High variance: Unstable training curves, especially early on (may indicate importance-weighting issues or noisy feedback)
  - No single-turn improvement: Multi-turn accuracy improves but single-turn stays flat (indicates feedback is not being internalized)

- **First 3 experiments:**
  1. Replicate the Knights and Knaves experiment from the paper, comparing RLTF-SD, RLTF-FM, and GRPO multi-turn. Verify that both methods outperform the baseline on single-turn accuracy.
  2. Ablate the baseline choice in RLTF-SD: compare first-turn baseline, second-turn baseline, and improvement baseline (R1 - R0). Measure both single-turn accuracy and update magnitude.
  3. Ablate the feedback quality: replace rich critiques with correctness-only feedback ("Your previous answer was correct/incorrect"). Compare RLTF-SD and GRPO multi-turn to assess the value of rich text feedback.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust are RLTF-SD and RLTF-FM to noisy, subjective, or adversarial text feedback compared to the simulated feedback used in the study?
- **Basis in paper:** [explicit] The authors state in the conclusion that "real-world feedback may be noisy or subjective, likely requiring data curation and filtering."
- **Why unresolved:** The experiments utilize a strong simulated judge (Qwen3-235B-A22B-Instruct) to generate critiques, which likely provides cleaner and more consistent feedback than human annotators.
- **What evidence would resolve it:** Empirical evaluation on datasets containing human-generated critiques or experiments injecting synthetic noise/perturbations into the feedback channel to measure performance degradation.

### Open Question 2
- **Question:** What modifications are required to scale RLTF to truly long-horizon interactions that exceed model context windows?
- **Basis in paper:** [explicit] The paper notes that while methods generalize to arbitrary horizons, "truly long-horizon feedback interaction may require techniques such as summarization to address distribution shift and context limits."
- **Why unresolved:** The experimental validation primarily focuses on a 2-turn setting. The authors do not implement or test the summarization techniques necessary to manage state history over many turns.
- **What evidence would resolve it:** Experiments on complex tasks requiring >10 interaction turns, specifically comparing raw context appending against proposed summarization or hierarchical state management techniques.

### Open Question 3
- **Question:** Can the theoretical guarantees for RLTF-FM regarding representation learning be extended to the fully end-to-end online optimization setting?
- **Basis in paper:** [explicit] The authors acknowledge their theoretical analysis "focuses on representation learning near the base policy's distribution" and that "a fully end-to-end analysis would strengthen understanding."
- **Why unresolved:** The current theory relies on a "frozen-rollout" (batch) regime for tractability, which assumes data is drawn from a fixed distribution rather than the shifting distribution of an updating online policy.
- **What evidence would resolve it:** Theoretical proofs demonstrating that the "low-signal subspace" remains identifiable and that feedback modeling acts as a representation preconditioner even as the policy drifts during training.

### Open Question 4
- **Question:** Does RLTF provide complementary learning signal when combined with other fine-grained supervision methods like Process Reward Models (PRMs)?
- **Basis in paper:** [explicit] The conclusion lists "exploring interplay with other fine-grained supervision methods, such as process reward models" as a promising future direction.
- **Why unresolved:** It is currently unknown whether the dense text feedback overlaps with token-level reward signals from PRMs or if they address distinct deficiencies in sparse scalar rewards.
- **What evidence would resolve it:** Ablation studies comparing RLTF, PRMs, and a hybrid approach on reasoning benchmarks to see if the combination yields additive or multiplicative improvements.

## Limitations
- The advantage of RLTF-SD over standard multi-turn RL is not fully explained by baseline choice alone; the role of feedback richness versus training data quantity remains unclear.
- The ablation of feedback quality (rich critiques vs. correctness-only feedback) is only briefly mentioned, leaving open whether the method's success depends on access to high-quality, detailed critiques.
- Several methodological details remain underspecified, particularly regarding the exact implementation of verifiable rewards for reasoning tasks.

## Confidence
- RLTF-SD consistently outperforms baselines on single-turn tasks: Medium
- First-turn baseline prevents gradient collapse compared to second-turn baseline: Medium
- RLTF-FM provides complementary benefits through auxiliary feedback prediction: Low (limited ablation evidence)
- Rich text feedback is necessary for the methods to work: Low (only one ablation presented)

## Next Checks
1. **Gradient collapse verification:** Replicate the Knights and Knaves experiment comparing first-turn baseline vs. second-turn baseline in RLTF-SD. Plot both single-turn accuracy and the magnitude of policy gradient updates over training to confirm that the second-turn baseline causes vanishing gradients when second-turn success rate approaches 1.0.

2. **Feedback quality ablation:** Implement a version of the RLTF-SD loop where the critique provider only outputs "correct" or "incorrect" instead of detailed textual feedback. Compare performance against the full-text feedback condition and the standard GRPO multi-turn baseline to quantify the value of rich textual critiques.

3. **Importance weighting analysis:** In RLTF-SD, implement both versions of the distillation loss - with and without importance sampling weights (i.e., π_ref = π(·|x₀) vs. π_ref = π(·|x₁)). Measure training stability (loss variance, gradient norms) and final single-turn accuracy to confirm whether removing importance weights indeed improves stability without harming performance.