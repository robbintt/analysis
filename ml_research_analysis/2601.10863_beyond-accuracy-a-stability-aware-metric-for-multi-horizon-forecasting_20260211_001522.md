---
ver: rpa2
title: 'Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting'
arxiv_id: '2601.10863'
source_url: https://arxiv.org/abs/2601.10863
tags:
- forecast
- forecasts
- stability
- accuracy
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the forecast accuracy and coherence (AC)
  score, a novel evaluation metric for multi-horizon time series forecasting that
  simultaneously measures forecast accuracy and temporal stability across prediction
  horizons. Unlike traditional metrics focusing solely on point forecast accuracy,
  the AC score captures how consistently models predict the same future events from
  different forecast origins.
---

# Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting

## Quick Facts
- arXiv ID: 2601.10863
- Source URL: https://arxiv.org/abs/2601.10863
- Reference count: 11
- Novel evaluation metric that combines forecast accuracy and temporal stability across prediction horizons

## Executive Summary
This paper introduces the Forecast Accuracy and Coherence (AC) score, a novel evaluation metric for multi-horizon time series forecasting that simultaneously measures forecast accuracy and temporal stability across prediction horizons. Unlike traditional metrics focusing solely on point forecast accuracy, the AC score captures how consistently models predict the same future events from different forecast origins. When implemented as a differentiable objective function for training seasonal ARIMA models on the M4 Hourly dataset, the AC-optimized models achieved a 75% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

## Method Summary
The AC score combines an accuracy component based on the energy score and a stability component based on energy distance between successive forecasts. The accuracy component evaluates the distance between forecast distributions and ground truth outcomes using a weighted Euclidean norm, while the stability component measures distributional drift between consecutive forecasts targeting the same timestamp. The metric is implemented as a differentiable loss function in PyTorch, enabling gradient-based optimization of SARIMA model parameters. The stability-aware training explicitly penalizes revisions between forecasts of the same target from consecutive origins, reducing "vertical volatility" in the forecast ensemble.

## Key Results
- 75% reduction in forecast volatility for same target timestamps when optimizing SARIMA models with AC score
- 4.8% median improvement in forecast accuracy across 45 of 50 test series from M4 Hourly dataset
- Differentiable implementation enables direct optimization of stability-aware objectives through gradient descent

## Why This Works (Mechanism)

### Mechanism 1: Energy Score as Proper Scoring Rule for Probabilistic Accuracy
The energy score provides a theoretically grounded measure of forecast accuracy that evaluates entire predictive distributions rather than point estimates. The energy score `ES(F, y) = E||Ŷ_pred - y_true|| - 0.5 E||Ŷ - Ŷ'||` measures the expected distance between samples from the forecast distribution and the true outcome, penalized by the internal spread of the forecast. When applied to multi-horizon forecasts with weighted Euclidean distance, it naturally emphasizes specified horizons.

### Mechanism 2: Energy Distance Between Successive Forecasts Captures Vertical Instability
The stability component penalizes revisions between forecasts targeting the same timestamp from consecutive origins, reducing "vertical volatility." By marginalizing non-overlapping horizon components from consecutive forecast distributions and computing energy distance `D²(marg ft, marg ft+1)`, the metric quantifies distributional drift. Averaging over all origin pairs yields the stability score.

### Mechanism 3: Differentiable AC Score Enables Direct Optimization
Implementing the AC score as a differentiable loss function allows gradient-based optimization of model parameters for both accuracy and stability. The complete forecast generation pipeline (rolling forecasts → ensemble construction → AC score computation) is implemented in PyTorch. Gradients propagate through the SARIMA recurrence, adjusting autoregressive coefficients to minimize the combined objective.

## Foundational Learning

- **Energy Score and Energy Distance**: Mathematical foundations of both accuracy and stability components. Quick check: Given two samples from distribution F and one sample from distribution G, write the empirical estimator for squared energy distance.
- **Vertical vs Horizontal Forecast Stability**: The paper distinguishes vertical stability (same target, different origins) from horizontal stability (same origin, different horizons). Quick check: In a forecast ensemble matrix with rows as origins and columns as horizons, which direction corresponds to vertical stability?
- **Proper Scoring Rules**: The energy score is a strictly proper scoring rule, meaning it is minimized in expectation only by the true predictive distribution. Quick check: Why would a scoring rule that can be "gamed" by hedging be unsuitable for model training?

## Architecture Onboarding

- **Component map**: Forecast Ensemble Matrix -> Accuracy Module -> Stability Module -> AC Score Combiner -> Differentiable SARIMA
- **Critical path**: 1) Initialize SARIMA coefficients randomly in (-1, 1) 2) Generate rolling H-step forecasts across training data (batch size 32) 3) Construct forecast sample ensemble (k sample paths per origin) 4) Compute accuracy term via Eq. 15 (empirical energy score) 5) Compute stability term via Eq. 16 (empirical energy distance between consecutive origins) 6) Backpropagate combined loss; update coefficients via AdamW 7) Verify stationarity via roots of AR polynomial (Eq. 19)
- **Design tradeoffs**: λ selection (higher λ prioritizes stability but may reduce responsiveness), horizon weights w(j) (linear decay emphasizes short-horizon accuracy), sample count k (more samples improve approximation but increase compute), stationarity enforcement (post-hoc verification only)
- **Failure signatures**: Non-stationary learned coefficients (roots of AR polynomial inside unit circle), excessive stability at accuracy cost (if λ too high), gradient instability (deep unrolling of SARIMA recurrence), numerical issues in energy distance (when distributions are nearly identical)
- **First 3 experiments**: 1) Baseline replication on 5 series from M4 Hourly with λ=0.5 2) λ sensitivity sweep with λ ∈ {0.1, 0.3, 0.5, 0.7, 1.0} 3) Weighting scheme ablation comparing linear vs exponential vs uniform weights

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Narrow empirical validation on only 50 time series from M4 Hourly dataset with SARIMA models limits generalizability
- No sensitivity analysis for critical hyperparameter λ selection across different applications
- Stationarity constraint only verified post-hoc rather than enforced during optimization

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Mathematical formulation of energy score and distance | High |
| Empirical results on M4 Hourly dataset | Medium |
| λ=0.5 is universally appropriate | Low |

## Next Checks
1. **Generalization test**: Apply AC score optimization to diverse datasets (M4 Daily, M5 retail, energy demand) and model architectures (LSTM, N-BEATS, Transformer) to assess robustness across domains and model families.
2. **Hyperparameter sensitivity**: Conduct systematic sweeps of λ and horizon weighting schemes to characterize the accuracy-stability Pareto frontier and identify optimal settings for different operational contexts.
3. **Stability vs accuracy tradeoff analysis**: Quantify the explicit tradeoff between forecast volatility reduction and potential loss of responsiveness to genuine signal changes, particularly for datasets with known structural breaks or regime shifts.