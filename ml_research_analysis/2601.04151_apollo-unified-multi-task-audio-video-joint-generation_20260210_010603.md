---
ver: rpa2
title: 'Apollo: Unified Multi-Task Audio-Video Joint Generation'
arxiv_id: '2601.04151'
source_url: https://arxiv.org/abs/2601.04151
tags:
- audio
- video
- generation
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Apollo introduces a unified multi-task audio-video generation framework
  addressing challenges of audio-visual asynchrony, poor lip-speech alignment, and
  unimodal degradation. The core approach combines a single-tower DiT architecture
  with Omni-Full Attention for tight cross-modal fusion, progressive multi-task training
  with random modality masking, and a novel automated data pipeline producing a large-scale,
  high-quality audio-video dataset with dense captions.
---

# Apollo: Unified Multi-Task Audio-Video Joint Generation

## Quick Facts
- arXiv ID: 2601.04151
- Source URL: https://arxiv.org/abs/2601.04151
- Authors: Jun Wang; Chunyu Qiang; Yuxin Guo; Yiran Wang; Xijuan Zeng; Feng Deng
- Reference count: 15
- One-line primary result: Apollo achieves state-of-the-art performance on Verse-Bench with strong audio-visual alignment and unimodal quality, comparable to Veo 3.

## Executive Summary
Apollo introduces a unified multi-task audio-video generation framework addressing challenges of audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation. The core approach combines a single-tower DiT architecture with Omni-Full Attention for tight cross-modal fusion, progressive multi-task training with random modality masking, and a novel automated data pipeline producing a large-scale, high-quality audio-video dataset with dense captions. Apollo outperforms prior methods on the Verse-Bench, achieving performance comparable to Veo 3, with significant gains in audio-visual consistency (A V-A distance of 0.65 vs. 0.92 for JavisDiT) and unimodal quality.

## Method Summary
Apollo employs a single-tower 32-layer MM-DiT architecture (26B parameters) with Omni-Full Attention that performs joint attention across video, audio, and their captions by concatenating all four modality streams. The model uses MixD-RoPE with 3D position encoding for video and compatible 1D encoding for audio, sharing temporal IDs for cross-modal alignment. Training follows a progressive three-stage curriculum: Stage I pre-trains on large-scale diverse data, Stage II specializes with adaptive data rebalancing based on evaluation metrics, and Stage III fine-tunes on manually curated high-quality data. Random modality masking enables multi-task learning across five tasks (T2AV, TI2AV, TI2V, T2V, T2A) while preventing unimodal collapse. The framework is trained on a large-scale dataset (81M samples) with dense captions generated through automated pipelines including quality filtering, scene splitting, and consistency checks.

## Key Results
- Apollo achieves state-of-the-art performance on Verse-Bench, comparable to Veo 3
- Significant improvement in audio-visual consistency: A V-A distance of 0.65 vs. 0.92 for JavisDiT
- Maintains strong unimodal quality across both video (ID consistency, aesthetic scores) and audio (CLAP scores, Fréchet distance) generation tasks
- Demonstrates robust out-of-distribution generalization on unseen scenarios

## Why This Works (Mechanism)

### Mechanism 1: Omni-Full Attention for Cross-Modal Alignment
- Claim: Joint full attention across all modalities creates tighter audio-visual alignment than dual-tower or cross-attention approaches
- Mechanism: By concatenating all four input sequences (video tokens, video caption tokens, audio tokens, audio caption tokens) and computing full attention over the combined sequence, every token can directly attend to every other token
- Core assumption: Direct attention between modalities is superior to cross-attention bridges, depending on sufficient model capacity and attention head diversity
- Evidence anchors: [abstract] "Omni-Full Attention mechanism, achieving tight audio-visual alignment"; [section 3.2] Equations 3-6 show Q, K, V computed as element-wise products of all four modality projections

### Mechanism 2: Random Modality Masking Prevents Unimodal Collapse
- Claim: Training with random masking of modalities during attention enables a single model to handle multiple tasks while maintaining unimodal quality
- Mechanism: By selectively masking query and key for different modalities, the model learns both joint generation and unimodal generation within the same framework
- Core assumption: Shared representations across tasks transfer positively, depending on task compatibility and absence of gradient conflict
- Evidence anchors: [abstract] "random modality masking to joint optimization across tasks...preventing unimodal collapse"; [section 3.3] Equations 7-9 define modality-specific masking

### Mechanism 3: Progressive Training with Quality-Adaptive Data Curation
- Claim: A three-stage curriculum with adaptive data rebalancing yields more robust representations than single-stage training
- Mechanism: Stage I builds basic generation capabilities, Stage II uses evaluation metrics to identify weak capabilities and rebalance data mixtures, Stage III fine-tunes on manually curated high-quality data
- Core assumption: Capabilities build sequentially and metric-guided rebalancing correctly identifies weaknesses
- Evidence anchors: [abstract] "progressive multitask regime...and a multistage curriculum, yielding robust representations"; [section 3.3] Figure 5 shows progressive improvements

## Foundational Learning

- Concept: **Flow Matching / Rectified Flow**
  - Why needed here: Apollo uses conditional flow matching (Eq. 2) as the denoising objective, learning the velocity field that transforms noise to data distribution
  - Quick check question: Can you explain how flow matching's velocity prediction (x₁ - x₀) differs from standard diffusion's noise prediction?

- Concept: **3D Rotary Position Embeddings (RoPE)**
  - Why needed here: Apollo uses MixD-RoPE with 3D position encoding (temporal, width, height) for video and compatible 1D temporal encoding for audio, enabling cross-modal temporal alignment
  - Quick check question: Why would shared temporal position IDs between video and audio matter for lip-sync synchronization?

- Concept: **Multi-Task Learning with Task Interference**
  - Why needed here: Apollo jointly optimizes five tasks (T2AV, TI2AV, TI2V, T2V, T2A). Understanding gradient interference and task balancing is critical for debugging performance issues
  - Quick check question: If video quality degrades when adding audio generation tasks, what diagnostic steps would you take?

## Architecture Onboarding

- Component map:
  - Encoders (3D causal video encoder from CogVideoX, audio VAE, text encoder, TTS text encoder) -> Token concatenation with scale/norm -> 32-layer MM-DiT with Omni-Full Attention and MixD-RoPE -> Separate video and audio decoders

- Critical path:
  1. Input encoding → video latents (3Hz), audio latents (43Hz), text embeddings
  2. Token concatenation with scale/norm per modality
  3. Omni-Full Attention: Q, K, V = element-wise product of all modality projections (Eqs. 3-5)
  4. Output splitting → modality-specific hidden states → scale/norm + FFN
  5. Flow matching loss with modality-specific masking (Eqs. 7-9)

- Design tradeoffs:
  - **Single-tower vs. dual-tower**: Single tower enables deeper fusion; dual-tower leverages pretrained weights but has shallower interaction (Table 2 shows single-tower advantage)
  - **Full attention vs. factorized**: Full attention is O(N²) but simpler optimization; factorized is cheaper but increases learning complexity
  - **Joint vs. progressive training**: Progressive requires careful staging but yields better final performance (Figure 5)

- Failure signatures:
  - **Audio-video asynchrony**: High WER, low SyncNet confidence—check MixD-RoPE temporal ID alignment
  - **Unimodal degradation**: If modality masking is omitted, video-only or audio-only quality drops (Table 3)
  - **Training instability**: Loss divergence may indicate incorrect flow matching schedule or learning rate

- First 3 experiments:
  1. **Verify attention implementation**: Mask all modalities except video + video caption; confirm T2V results match baseline (Table 3)
  2. **Ablate modality masking**: Compare T2AV-only vs. full multi-task masking on unimodal metrics (ID, MOS, CLAP) to confirm collapse prevention
  3. **Validate MixD-RoPE alignment**: Measure AV-Alignment distance and SyncNet Confidence with vs. without shared temporal position IDs

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ablation studies for critical components like Omni-Full Attention versus simpler fusion methods, making it difficult to isolate which architectural innovations drive performance gains
- Progressive training methodology lacks detailed quantitative criteria for stage transitions and data rebalancing, raising questions about reproducibility
- Automated data pipeline has a 73% rejection rate that may introduce selection bias affecting generalization
- Single-tower architecture with full attention has unknown scalability limits for longer sequences and may face computational constraints in practical deployment

## Confidence
- **High Confidence**: Claims about dataset curation methodology, multi-task training setup with random modality masking, and specific benchmark metrics and results on Verse-Bench
- **Medium Confidence**: Claims about Omni-Full Attention mechanism superiority over dual-tower approaches and progressive training benefits
- **Low Confidence**: Claims about scalability to longer sequences and real-world deployment performance

## Next Checks
1. **Ablation Study on Fusion Mechanism**: Implement and compare Apollo's Omni-Full Attention against cross-attention fusion and dual-tower architectures while holding dataset and training procedures constant to isolate the architectural contribution to performance gains

2. **Progressive Training Sensitivity Analysis**: Systematically vary the criteria for stage transitions and data rebalancing ratios to determine the robustness of the progressive training methodology and identify the minimum effective staging configuration

3. **Out-of-Distribution Generalization Test**: Evaluate Apollo on audio-video samples with substantially different characteristics from the training distribution (e.g., different languages, speaker demographics, or acoustic environments) to assess real-world robustness claims