---
ver: rpa2
title: 'MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal
  Topical Matching'
arxiv_id: '2502.12852'
source_url: https://arxiv.org/abs/2502.12852
tags:
- languages
- images
- language
- lvlms
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MVL-SIB, a massively multilingual vision-language\
  \ benchmark that evaluates cross-modal topical matching across 205 languages\u2014\
  over 100 more than existing VL benchmarks. The authors hand-select images to represent\
  \ seven topics and pair them with professionally translated sentences from SIB-200\
  \ to create parallel VL evaluation data."
---

# MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching

## Quick Facts
- **arXiv ID**: 2502.12852
- **Source URL**: https://arxiv.org/abs/2502.12852
- **Reference count**: 40
- **Primary result**: MVL-SIB evaluates cross-modal topical matching across 205 languages, revealing that LVLMs struggle significantly with low-resource languages and do not benefit from multiple reference images.

## Executive Summary
This paper introduces MVL-SIB, a massively multilingual vision-language benchmark that evaluates cross-modal topical matching across 205 languages—over 100 more than existing VL benchmarks. The authors hand-select images to represent seven topics and pair them with professionally translated sentences from SIB-200 to create parallel VL evaluation data. They benchmark open-weight LVLMs and GPT-4o-mini on both cross-modal and text-only topic matching tasks. Results show that LVLMs struggle with cross-modal topic matching in lower-resource languages, performing near chance level on languages like N'Koo, and that VL support declines disproportionately relative to textual support for these languages. Open-weight LVLMs do not benefit from multiple reference images, indicating limited effectiveness in multi-image tasks. The study also correlates MVL-SIB performance with other multilingual VL benchmarks, confirming its reliability as a comprehensive probe of multilingual VL understanding.

## Method Summary
The authors constructed MVL-SIB by combining Flores (3,001 professionally translated sentences) with SIB-200 (1,004 parallel examples across 7 topics). For each topic, they hand-selected 10 images and verified image-topic alignment through LVLM classification. The benchmark includes 3,012 instances, each pairing one SIB sentence with 5 positive images and 4 same-category sentences, plus 3 negative images/sentences. Two main tasks were evaluated: Images-To-Sentence (I2S) where models select 1 of 4 sentences matching k reference images, and Sentences-To-Image (S2I) where models select 1 of 4 images matching k reference sentences. Text-only variants (T2S, S2T) replace images with topic labels. Models were evaluated using accuracy metrics, counting responses starting with the correct option letter. Inference used greedy decoding (temperature=0.0), with GPT-4o using 'low' image detail. The baseline mSigLIP model computed average cosine similarity between candidates and k references.

## Key Results
- LVLMs perform near-chance level on cross-modal topical matching for low-resource languages (Tier 1), such as N'Koo.
- VL support declines disproportionately relative to textual support for low-resource languages, with LVLMs showing greater performance drops in VL tasks versus text-only tasks.
- Open-weight LVLMs do not benefit from multiple reference images, indicating limited effectiveness in multi-image tasks.

## Why This Works (Mechanism)
None

## Foundational Learning
- **Cross-modal alignment**: Why needed: To evaluate how well models connect visual and textual representations across languages. Quick check: Compare VL task performance against text-only task performance.
- **Multilingual representation**: Why needed: To assess model capabilities across diverse linguistic contexts. Quick check: Evaluate performance across the three language tiers (Tier 0: 20 languages, Tier 1: 50 languages, Tier 2: 135 languages).
- **Topical matching**: Why needed: To test semantic understanding beyond exact word matching. Quick check: Verify model selects correct answer when topics are semantically related but lexically distinct.

## Architecture Onboarding

**Component Map**: MVL-SIB Dataset -> LVLM Models (Qwen2-VL, InternVL 2.5, Centurio-Qwen, GPT-4o-mini) -> Evaluation Pipeline (I2S, S2I, T2S, S2T tasks) -> Accuracy Metrics

**Critical Path**: Hand-selected images and translated sentences → Benchmark construction → Model inference with greedy decoding → Accuracy calculation → Cross-modal vs text-only comparison

**Design Tradeoffs**: The benchmark uses a relatively small dataset (3,012 instances) to maintain consistency across 205 languages, trading statistical power for comprehensive language coverage. The focus on topical matching rather than broader VL capabilities limits generalizability but provides a controlled evaluation of cross-modal semantic understanding.

**Failure Signatures**: 
- Models generate verbose explanations instead of single-letter answers
- Near-chance performance on low-resource languages
- Disproportionate performance drops in VL tasks versus text-only tasks for certain languages

**First Experiments**:
1. Evaluate a simple baseline (e.g., mSigLIP) to establish performance floor
2. Run text-only variants (T2S, S2T) to isolate VL performance degradation
3. Test with k=1 reference to establish baseline before evaluating multi-reference scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's construction relies on hand-selected images and manual verification steps that are not fully specified in the paper.
- The evaluation uses a relatively small dataset (3,012 instances) which may limit statistical power, particularly for detecting performance differences in low-resource languages.
- The study focuses on topical matching rather than broader VL capabilities, potentially limiting generalizability to other VL tasks.

## Confidence

**High Confidence**: The finding that LVLMs perform near-chance level on cross-modal topical matching for low-resource languages (Tier 1) is well-supported by the data. The comparison between VL and text-only performance showing disproportionate degradation for low-resource languages is robust.

**Medium Confidence**: The claim that open-weight LVLMs do not benefit from multiple reference images is supported, but the interpretation that this indicates "limited effectiveness in multi-image tasks" requires additional context about the specific task structure and could vary across different VL benchmarks.

**Medium Confidence**: The correlation analysis with other multilingual VL benchmarks supports MVL-SIB's reliability as a comprehensive probe, but the relatively small sample size and focus on topical matching may limit the strength of these conclusions.

## Next Checks
1. Reconstruct the full dataset by sourcing the hand-selected images per topic from permissive stock photo sites, ensuring semantic alignment with the visual examples provided in Appendix A.1, then rerun the complete benchmark evaluation pipeline.
2. Implement the missing sampling procedure specification for identifying languages with minimal deviation from tier means by creating a reproducible algorithm based on the description provided, then validate against the reported results.
3. Extend the analysis to include additional VL benchmarks beyond those mentioned to test the robustness of the correlation findings and assess whether the observed patterns hold across different types of VL tasks.