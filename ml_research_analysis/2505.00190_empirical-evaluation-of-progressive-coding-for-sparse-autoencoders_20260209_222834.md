---
ver: rpa2
title: Empirical Evaluation of Progressive Coding for Sparse Autoencoders
arxiv_id: '2505.00190'
source_url: https://arxiv.org/abs/2505.00190
tags:
- saes
- features
- loss
- matryoshka
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the computational challenge of training and
  using sparse autoencoders (SAEs) of different sizes for neural network interpretability.
  The authors explore two methods for creating progressive SAEs that allow flexible
  reconstruction at varying granularities: Matryoshka SAEs that jointly train nested
  representations with shared weights, and dictionary permutation of pretrained SAEs
  based on feature importance.'
---

# Empirical Evaluation of Progressive Coding for Sparse Autoencoders

## Quick Facts
- arXiv ID: 2505.00190
- Source URL: https://arxiv.org/abs/2505.00190
- Reference count: 15
- One-line primary result: Matryoshka SAEs outperform baseline SAEs and permuted versions across reconstruction metrics while enabling flexible granularity selection.

## Executive Summary
This paper addresses the computational challenge of training and using sparse autoencoders (SAEs) of different sizes for neural network interpretability. The authors explore two methods for creating progressive SAEs that allow flexible reconstruction at varying granularities: Matryoshka SAEs that jointly train nested representations with shared weights, and dictionary permutation of pretrained SAEs based on feature importance. They demonstrate that dictionary importance follows a power law across different model sizes, enabling effective feature prioritization. Matryoshka SAEs outperform both baseline SAEs and permuted versions across multiple metrics including reconstruction loss, recaptured language modeling loss, and representational similarity.

## Method Summary
The paper evaluates two progressive coding approaches: Matryoshka SAEs that jointly optimize multiple granularity levels with shared encoder/decoder weights, and dictionary permutation that reorders pretrained SAE features by importance without retraining. Both methods enable flexible reconstruction at varying latent sizes G ≤ N while maintaining consistent sparsity ratios. The Matryoshka approach trains with a weighted sum of reconstruction losses across all granularities, while the permutation method sorts features by E[activation²] and selects top G for inference. The authors use TopK activation functions, dictionary sizes {16K, 32K, 65K}, and evaluate on Gemma-2-2b layer 2 activations using FVU, recaptured LLM loss, and interpretability scores.

## Key Results
- Dictionary importance in pretrained SAEs follows power law distributions (exponents -0.54 to -1.27, R² 0.916-0.922)
- Matryoshka SAEs achieve lower reconstruction loss and higher recaptured language modeling loss than both baseline SAEs and permuted versions
- Permuted vanilla SAEs show better interpretability scores despite inferior reconstruction performance
- Feature splitting causes progressive coding performance to degrade at low G/N ratios, with 65K SAE underperforming native 16K SAE when both use G=16K

## Why This Works (Mechanism)

### Mechanism 1: Dictionary Power Law Enables Feature Prioritization
Feature importance in pretrained SAEs follows a power law distribution, enabling efficient subset selection for progressive coding. The eigenvalues of the decoder covariance matrix, mean squared activations E[activation²], and activation frequency all decay according to power laws. This hierarchical structure means a small subset of features captures most variance, so sorting features by importance and selecting the first G yields graceful degradation.

### Mechanism 2: Matryoshka SAEs Learn Nested Representations via Joint Optimization
Jointly training multiple granularity levels with shared encoder/decoder weights produces better progressive coders than post-hoc permutation. The loss function combines reconstruction losses across all granularities with shared weights. Since representations are nested (z₁:ₘ₁ ⊆ z₁:ₘ₂ ⊆ ... ⊆ z₁:ₘₖ), encoding is computed once and amortized. The model learns to place high-importance features in early dimensions because all granularities are optimized simultaneously.

### Mechanism 3: Permutation Invariance Permits Post-Hoc Progressive Coding
Pretrained SAEs can be converted to progressive coders by permuting features based on E[activation²], without retraining. SAE features are conditionally independent given the input—each activation depends only on its encoder row and bias. Permuting latent dimensions produces identical reconstructions. Sorting by E[activation²] prioritizes high-variance features, enabling subset selection at inference.

## Foundational Learning

- **Superposition Hypothesis**: Explains why SAEs need overcomplete dictionaries—networks represent more features than neurons via approximately orthogonal basis vectors. Quick check: Can you explain why neural networks might need N >> D basis vectors, and what the Johnson-Lindenstrauss lemma guarantees about approximate orthogonality?

- **TopK Activation Function**: The paper exclusively uses TopK, which fixes sparsity by zeroing all but the K largest activations. Understanding this is essential for interpreting the sparsity-fidelity tradeoffs. Quick check: Given a TopK SAE with dictionary size N=65,536 and K=256, what is the effective sparsity ratio, and how does this change if you select only the first G=16,384 features?

- **Representational Similarity Analysis (RSA)**: RSA bridges reconstruction loss and recaptured LLM loss by measuring second-order isometry between original and reconstructed activations. Quick check: Why might FVU and recaptured LLM loss diverge, and what does RSA correlation (~0.8 in this paper) tell you about their relationship?

## Architecture Onboarding

- **Component map**: Extract activations → Center via Bcenter → Encode to z using TopK → For progressive inference: truncate z to z₁:G → Decode using WDec[1:G, :] → Evaluate via FVU, recaptured CE loss, RSA

- **Critical path**: 1) Extract activations from target layer (e.g., Gemma-2-2b layer 2 residual stream) 2) Center via Bcenter, encode to z using TopK 3) For progressive inference: truncate z to z₁:G, decode using WDec[1:G, :] 4) Evaluate via FVU, recaptured CE loss, RSA

- **Design tradeoffs**: Matryoshka vs. Permutation: Matryoshka yields better reconstruction (lower FVU, higher recaptured loss) but lower interpretability (simulation correlation 0.57-0.74 vs. higher for permuted baselines). Sampled vs. Fixed granularities: Sampling mᵢ ~ U(1, N) during training improves progressive coding but adds complexity. Granularity ratio G/N: Performance degrades at low G/N due to feature splitting—permuted 65K SAE underperforms 32K SAE when both use G=16K.

- **Failure signatures**: Dead features: High k_aux values indicate dead features; mitigate with auxiliary loss (scale 1/32). Feature splitting artifacts: In Matryoshka, stepped activation patterns at granularity boundaries may indicate suboptimal nesting. Interpretability drop: Innermost granularities (e.g., 16K) more interpretable than outermost (e.g., 65K) in Matryoshka—suggests features aren't uniformly distributed.

- **First 3 experiments**: 1) Power law validation: On your target model, compute E[activation²] across 10⁵ tokens; fit log-log regression to confirm power law (R² > 0.85) before attempting permutation-based progressive coding. 2) Baseline comparison at matched sparsity: Train a 32K TopK SAE and a 65K Matryoshka SAE with M={16K, 32K, 65K}. Compare FVU and recaptured CE loss at G=16K and G=32K, keeping K/G constant. 3) Interpretability probe: Using sae-auto-interp or equivalent, compute simulation scores for Matryoshka vs. permuted SAEs. Focus on whether outermost granularities (indices 32K-65K) show degraded correlation compared to innermost (0-16K).

## Open Questions the Paper Calls Out

- Can efficient methods be developed to recombine or "reverse" feature-splitting during distillation from large to small SAEs? Future research could focus on developing efficient methods to recombine or 'reverse' this feature-splitting during the distillation process, potentially through feature clustering or adaptive merging strategies.

- Do the observed power law relationships and Matryoshka SAE benefits hold at much larger scales (tens of millions of features)? Our methods remain to be validated at larger scales and our experiments were conducted on relatively modest-sized SAEs compared to recent work, scaling to tens of millions of features.

- What causes the interpretability-reconstruction tradeoff between Matryoshka SAEs and permuted vanilla SAEs, and can it be bridged? Pruned vanilla SAEs are more interpretable, however. We discuss the origins and implications of this trade-off.

## Limitations

- Feature splitting causes progressive coding performance to degrade at low G/N ratios, with 65K SAE underperforming native 16K SAE when both use G=16K
- Power law analysis relies on activation statistics from a single model (Gemma-2-2b), limiting generalizability across architectures
- Matryoshka SAEs achieve superior reconstruction but reduced interpretability, creating a fundamental tradeoff that isn't fully characterized

## Confidence

**High confidence**: The Matryoshka SAE implementation details and reconstruction results are well-specified and reproducible. The power law distribution of feature importance (exponents -0.54 to -1.27, R² > 0.9) is directly measurable from decoder covariance matrices and activation statistics.

**Medium confidence**: The claim that permutation-based progressive coding works "without retraining" assumes stable feature importance rankings across inference conditions. While E[activation²] is measurable, the paper doesn't validate whether this remains predictive under distribution shifts or different prompting strategies.

**Low confidence**: The interpretability comparisons between Matryoshka and permuted SAEs rely on simulation correlation scores (0.57-0.74 range) that measure feature functionality but not human interpretability. The tradeoff between reconstruction quality and interpretability isn't fully characterized—the paper observes lower interpretability for Matryoshka but doesn't explain why this occurs or whether it's inherent to the joint optimization approach.

## Next Checks

1. **Cross-model power law validation**: Extract activation statistics from at least two additional models (different sizes, architectures, or training objectives) and verify that feature importance follows power law distributions with comparable exponents. Test whether the same feature importance ranking generalizes across models.

2. **Feature splitting threshold analysis**: Systematically measure the performance gap between 65K→16K progressive coding versus native 16K SAE as G/N decreases. Identify the critical G/N ratio where feature splitting artifacts dominate and evaluate whether alternative feature selection criteria (beyond E[activation²]) can mitigate this degradation.

3. **Interpretability mechanism probe**: Conduct ablation studies on Matryoshka training to isolate why innermost granularities are more interpretable than outermost. Test whether modifying the loss weighting scheme (c_m parameters) or adding interpretability-specific regularization can preserve the reconstruction advantages while improving functional interpretability scores.