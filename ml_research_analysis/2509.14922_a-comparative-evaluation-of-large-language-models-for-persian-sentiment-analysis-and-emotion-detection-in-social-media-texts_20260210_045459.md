---
ver: rpa2
title: A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis
  and Emotion Detection in Social Media Texts
arxiv_id: '2509.14922'
source_url: https://arxiv.org/abs/2509.14922
tags:
- sentiment
- emotion
- detection
- performance
- persian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four Large Language Models (LLMs)\u2014Claude\
  \ 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o\u2014for sentiment analysis\
  \ and emotion detection in Persian social media texts. Using balanced datasets of\
  \ 900 texts for sentiment (positive, negative, neutral) and 1,800 texts for emotion\
  \ (anger, fear, happiness, hate, sadness, surprise), the models achieved macro average\
  \ F1-scores between 0.7950 and 0.8079 for sentiment and 0.7379 to 0.8079 for emotion\
  \ detection."
---

# A Comparative Evaluation of Large Language Models for Persian Sentiment Analysis and Emotion Detection in Social Media Texts

## Quick Facts
- **arXiv ID:** 2509.14922
- **Source URL:** https://arxiv.org/abs/2509.14922
- **Reference count:** 3
- **Primary result:** GPT-4o achieved highest accuracy (80.67% sentiment, 80.94% emotion) among four LLMs evaluated on Persian social media texts

## Executive Summary
This study evaluates four large language models—Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o—for zero-shot sentiment analysis and emotion detection on Persian social media texts. Using balanced datasets of 900 texts for sentiment (positive, negative, neutral) and 1,800 texts for emotion (anger, fear, happiness, hate, sadness, surprise), the models achieved macro average F1-scores between 0.7950 and 0.8079 for sentiment and 0.7379 to 0.8079 for emotion detection. GPT-4o demonstrated the highest accuracy while Gemini 2.0 Flash offered the best cost-efficiency. The study reveals that all models performed better on sentiment than emotion detection, with surprise detection being particularly challenging due to frequent confusion with anger and happiness.

## Method Summary
The study employs zero-shot inference via API for sentiment analysis (3 classes) and emotion detection (6 classes) on Persian social media texts. Datasets were sourced from Kaggle (sentiment) and GitHub (ArmanEmo), cleaned to remove duplicates, filter by length, and standardize labels. Four LLMs were tested with temperature=0, batch size=20, and JSON output format. Evaluation metrics include macro-average F1-score, accuracy, precision, and recall per class, with statistical validation through bootstrap confidence intervals and McNemar's test. Prompts were iteratively refined to minimize hallucinations and ensure strict label compliance.

## Key Results
- GPT-4o achieved the highest accuracy (80.67% for sentiment, 80.94% for emotion detection)
- Gemini 2.0 Flash provided the best cost-efficiency among evaluated models
- Surprise detection showed lowest recall (0.37-0.69) across all models, frequently confused with anger and happiness
- All models performed better on sentiment (F1: 0.7950-0.8079) than emotion detection (F1: 0.7379-0.8079)

## Why This Works (Mechanism)
The evaluation leverages zero-shot capabilities of modern LLMs to perform complex sentiment and emotion classification without task-specific fine-tuning. The iterative prompt engineering process successfully reduced hallucinations and improved label compliance, enabling reliable evaluation across models. The use of balanced datasets and statistical validation methods provides robust comparative benchmarks for Persian NLP applications.

## Foundational Learning
- **Zero-shot learning**: Models classify without task-specific training; needed to evaluate general model capabilities; quick check: verify all models can classify without examples
- **Bootstrap confidence intervals**: Resampling method for uncertainty estimation; needed to assess result stability; quick check: run 1,000 resamples and verify confidence bounds
- **McNemar's test**: Statistical test for paired nominal data; needed to compare model classification differences; quick check: ensure p-values < 0.05 for significant differences
- **Macro-average F1-score**: Equal weighting across classes; needed for fair evaluation of imbalanced performance; quick check: calculate per-class F1 then average
- **Prompt engineering**: Iterative refinement to reduce hallucinations; needed for reliable zero-shot performance; quick check: monitor hallucination rate across iterations
- **Batch processing**: API calls with multiple texts; needed for efficiency and consistency; quick check: verify batch size=20 and temperature=0

## Architecture Onboarding
- **Component map**: Persian datasets (Kaggle + GitHub) -> Cleaning pipeline -> API inference (4 models) -> JSON parsing -> Metric computation -> Statistical validation
- **Critical path**: Data preparation → Prompt formulation → API inference → Result parsing → Evaluation → Statistical testing
- **Design tradeoffs**: Zero-shot inference offers flexibility but lower performance than fine-tuning; batch processing improves efficiency but may mask individual text issues
- **Failure signatures**: Hallucinations producing labels outside predefined sets; confusion between similar emotions (surprise/anger/happiness); degraded performance on minority classes
- **3 first experiments**: 1) Test prompt with single text to verify JSON output format; 2) Run small batch (5 texts) through all models to check API connectivity; 3) Compute baseline metrics on subset before full evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Single random seed (42) may not reflect variability across different sampling strategies
- Balanced class distributions don't reflect real-world data imbalances
- Exact prompt formulations referenced but not provided in paper text
- Zero-shot approach limits conclusions about fine-tuned model performance

## Confidence
- **High confidence** in relative model rankings (GPT-4o highest accuracy, Gemini 2.0 Flash best cost-efficiency)
- **Medium confidence** in absolute F1 scores due to prompt uncertainty and single-seed evaluation
- **Low confidence** in generalizability to imbalanced datasets or different Persian dialects/social media platforms

## Next Checks
1. Reconstruct and test the exact prompts from the GitHub repository to verify reported F1 scores match implementation
2. Run bootstrap resampling with multiple random seeds to assess stability of model rankings
3. Test model performance on an imbalanced subset (e.g., 200/300/400 per class) to evaluate robustness to real-world distributions