---
ver: rpa2
title: 'Pack and Force Your Memory: Long-form and Consistent Video Generation'
arxiv_id: '2510.01784'
source_url: https://arxiv.org/abs/2510.01784
tags:
- video
- arxiv
- generation
- training
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Pack and Force Your Memory: Long-form and Consistent Video Generation

## Quick Facts
- arXiv ID: 2510.01784
- Source URL: https://arxiv.org/abs/2510.01784
- Reference count: 11
- Primary result: Introduces MemoryPack and Direct Forcing for minute-level autoregressive video generation with improved temporal consistency

## Executive Summary
This paper addresses the fundamental challenge of error accumulation and temporal inconsistency in autoregressive video generation by introducing two complementary mechanisms: MemoryPack for hierarchical memory retrieval and Direct Forcing for training-inference alignment. The method achieves minute-scale video generation (up to 30 seconds tested) with significantly reduced flickering and drift compared to baselines, using only standard DiT architectures with targeted modifications. The approach decomposes context into short-term motion and long-term semantic streams, maintaining O(n) complexity while preserving positional consistency across segments.

## Method Summary
The method builds on a DiT backbone (FramePack-F1) and introduces MemoryPack modules that decompose context into FramePack (fixed compression for motion) and SemanticPack (learnable memory ψ updated via Memorize-Squeeze operations). MemoryPack maintains O(n) complexity through windowed attention and iterative memory updates. Direct Forcing uses a single-step approximation based on rectified flow trajectories to align training and inference distributions without distillation overhead. RoPE-based cross-segment positional consistency encoding treats the reference image as a CLS token to preserve absolute and relative positional signals across video segments.

## Key Results
- Achieves 10-15% improvements in consistency metrics (Background/Subject Consistency) over FramePack-F1 baseline
- Reduces error accumulation (∆M_drift) by 20-30% on 30-second videos
- Maintains VBench imaging quality while improving temporal coherence
- Demonstrates scalability to minute-level videos (up to 30 seconds tested)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Memory Retrieval via MemoryPack
- Claim: Jointly modeling short-term motion fidelity and long-term semantic coherence enables minute-level temporal consistency without quadratic complexity.
- Mechanism: MemoryPack decomposes context into two streams. FramePack applies fixed compression to recent frames for motion continuity. SemanticPack iteratively updates a learnable memory representation ψ through: (1) Memorize—windowed self-attention on historical segments producing compact embeddings; (2) Squeeze—cross-attention where memorized visual features query against text/image-guided memory. The update rule ψ_{n+1} = Squeeze(ψ_n, Memorize(x_n)) maintains O(n) complexity.
- Core assumption: Text prompts and reference images provide stable semantic anchors that persist across temporal drift; windowed attention captures sufficient local structure.
- Evidence anchors:
  - [abstract] "learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies"
  - [Section 3.1] Eq. 1 defines the iterative memory update; "computational complexity of SemanticPack is O(n)"
  - [corpus] Related work (LoL, VideoAR) confirms error accumulation and coherence loss are central autoregressive challenges; no direct corpus validation of this specific mechanism.
- Break condition: If semantic guidance (text/image) is ambiguous or contradicts visual content, ψ initialization may misanchor memory trajectory.

### Mechanism 2: Direct Forcing for Train-Inference Alignment
- Claim: Single-step approximation of inference outputs during training reduces exposure bias without distillation overhead.
- Mechanism: Building on rectified flow, the method defines xt = tx + (1-t)ε and velocity ut = x - ε. During training, instead of expensive multi-step sampling (Student Forcing), it computes: x̃₁ = x_t + Δt · v_θ(x_t, t) where Δt = 1-t. This one-step approximation serves as the conditional input for next-segment training, exposing the model to its own prediction distribution.
- Core assumption: Rectified flow's straighter trajectories make single-step approximation sufficiently accurate; errors are primarily distributional rather than architectural.
- Evidence anchors:
  - [abstract] "efficient single-step approximating strategy that improves training–inference alignment"
  - [Section 3.2] Eq. 7 defines approximation; "incurs no additional overhead, requires no distillation"
  - [corpus] Flowception addresses error accumulation via non-autoregressive frame insertion—different approach, same problem class. No corpus validation of single-step approximation effectiveness.
- Break condition: If pretrained backbone has poor flow straightness, single-step approximation deviates significantly from true inference distribution.

### Mechanism 3: RoPE-Based Cross-Segment Positional Consistency
- Claim: Encoding relative positions across segments via RoPE with a CLS-style image token mitigates flickering and discontinuities.
- Mechanism: Assign the reference image token the starting position index of the full video. RoPE's relative position property ensures that queries/keys maintain consistent positional relationships across segment boundaries, preserving both absolute and relative positional signals.
- Core assumption: Positional discontinuity—not just content drift—is a significant source of flickering.
- Evidence anchors:
  - [Section 3.1, RoPE Consistency] "segmentation causes even adjacent segments from the same video to be modeled independently, leading to the loss of cross-segment positional information"
  - [corpus] No direct corpus evidence for this specific RoPE application in video generation.
- Break condition: If video duration exceeds RoPE's effective context window or position indices overflow, encoding degrades.

## Foundational Learning

- **Rectified Flow / Flow Matching**:
  - Why needed here: Direct Forcing is built on rectified flow's ODE formulation; understanding velocity fields and straight trajectories is essential.
  - Quick check question: Can you explain why straighter ODE trajectories enable better single-step approximation?

- **Diffusion Transformers (DiT) Architecture**:
  - Why needed here: The backbone is DiT-based; MemoryPack modules insert into this architecture without modifying core blocks.
  - Quick check question: Where would cross-attention layers for SemanticPack connect in a standard DiT block?

- **Autoregressive Video Generation Challenges**:
  - Why needed here: The entire paper addresses error accumulation and exposure bias inherent to this paradigm.
  - Quick check question: What causes the train-inference mismatch in autoregressive generation?

## Architecture Onboarding

- **Component map**:
  Input text prompt + reference image → MemoryPack (FramePack + SemanticPack) → MM-DiT backbone → generated segment
  SemanticPack: Memorize (windowed self-attention) → Squeeze (cross-attention) → updated memory ψ

- **Critical path**:
  1. Initialize ψ₀ = concat(prompt features, image features)
  2. For each historical segment: Memorize → Squeeze → update ψ
  3. Combine FramePack and SemanticPack outputs as context
  4. MM-DiT generates next segment conditioned on context
  5. Direct Forcing: apply Eq. 7 to approximate inference output for next training step

- **Design tradeoffs**:
  - SemanticPack structure A (visual as query) vs B/C (text/image as query): A preserves dynamics better (Table 3)
  - Two-stage training: teacher forcing first, then Direct Forcing fine-tuning (output layers only)
  - Gradient accumulation across clips stabilizes distribution but reduces effective steps; mitigated via curriculum learning (short → long videos)

- **Failure signatures**:
  - Zero-initialized ψ (zero-MemoryPack): elevated error accumulation, reduced consistency (Table 2)
  - Direct Forcing on poor backbone: approximation quality degrades
  - Highly dynamic scenes: SemanticPack artifacts (acknowledged limitation)
  - Hour-long videos: long-term consistency still limited (acknowledged limitation)

- **First 3 experiments**:
  1. Sanity check: Train with teacher forcing only; verify baseline FramePack-F1 reproduction on 10s videos. Compare reported metrics (Table 1) to confirm implementation.
  2. MemoryPack ablation: Initialize ψ₀ = zeros; compare against text/image initialization on 30s videos. Expect degraded ∆SubjectConsistency and ∆BackgroundConsistency per Table 2.
  3. Direct Forcing validation: Compare 5-step Student Forcing vs single-step Direct Forcing on same data split; measure training time and ∆ImagingQuality drift. Expect faster convergence and lower error accumulation with Direct Forcing.

## Open Questions the Paper Calls Out
None

## Limitations
- Long-term consistency (minute-scale) is demonstrated empirically but not theoretically proven to scale beyond tested duration
- Performance degradation in highly dynamic scenes where rapid visual changes overwhelm SemanticPack memory
- Hour-long video generation remains a significant challenge not addressed by current approach

## Confidence
- **High Confidence**: Core architectural contributions (MemoryPack decomposition, Direct Forcing approximation, RoPE consistency encoding) are well-specified and internally consistent
- **Medium Confidence**: Quantitative improvements reported in Tables 1-4 appear substantial but lack confidence intervals or statistical significance testing
- **Low Confidence**: Claim of "minute-level temporal consistency" requires qualification as evaluation focuses on 30-second clips; single-step Direct Forcing effectiveness not empirically compared against multi-step alternatives

## Next Checks
1. **Statistical significance validation**: Replicate the VBench evaluation with confidence intervals across multiple runs to verify reported 10-15% improvements are robust
2. **Dynamic scene stress test**: Generate videos with rapid scene changes, camera motion, and object deformation to measure whether MemoryPack's SemanticPack can maintain coherence under high visual dynamics
3. **Cross-backbone generalization**: Apply Direct Forcing and MemoryPack modules to a different autoregressive video generation backbone (e.g., VideoAR or STARFlow-V) to test if improvements are due to architectural innovations rather than specific FramePack-F1 implementation