---
ver: rpa2
title: 'Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted
  DRL'
arxiv_id: '2512.23310'
source_url: https://arxiv.org/abs/2512.23310
tags:
- edge
- cloud
- latency
- splitwise
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Splitwise, a novel Lyapunov-assisted deep
  reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of
  large language models (LLMs) across edge and cloud environments. The key innovation
  is decomposing transformer layers into attention heads and feed-forward sub-blocks,
  exposing exponentially more partition choices than layer-wise schemes.
---

# Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL

## Quick Facts
- arXiv ID: 2512.23310
- Source URL: https://arxiv.org/abs/2512.23310
- Reference count: 35
- Primary result: Reduces end-to-end latency by 1.4x-2.8x and energy consumption by up to 41% compared to existing partitioners

## Executive Summary
Splitwise introduces a novel Lyapunov-assisted deep reinforcement learning framework for fine-grained, adaptive partitioning of large language models across edge and cloud environments. The system decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing exponentially more partition choices than traditional layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Experiments demonstrate significant performance improvements across multiple devices and model sizes while maintaining robustness through checkpoint-based recovery mechanisms.

## Method Summary
Splitwise employs a Lyapunov-assisted Proximal Policy Optimization (PPO) framework that learns to partition transformer layers at the sub-layer level (attention heads and feed-forward networks). The hierarchical DRL policy decomposes the exponentially large action space by making sequential decisions per layer, conditioned on previous layer embeddings. The reward function combines immediate performance metrics with a Lyapunov drift term that ensures queue stability. The system supports dynamic model loading and checkpointing at partition boundaries to enable fault tolerance and execution on resource-constrained edge devices. The architecture includes an edge-based partition controller, profiling engine, and communication manager working in conjunction with cloud execution runtimes.

## Key Results
- Reduces end-to-end latency by 1.4x-2.8x compared to existing partitioners
- Cuts energy consumption by up to 41% across tested configurations
- Lowers 95th-percentile latency by 53-61% relative to cloud-only execution
- Achieves these improvements while maintaining accuracy and modest memory requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained partitioning at the sub-layer level enables superior adaptation to fluctuating bandwidth compared to coarse, layer-wise partitioning.
- **Mechanism:** By decomposing transformer layers into $2^H \times 3$ configurations per layer, the policy can precisely tune the compute-to-communication ratio, offloading small, computationally heavy chunks to the cloud while retaining data-heavy operations on the edge.
- **Core assumption:** Serialization overhead of smaller activation tensors does not negate latency gains from fine-grained parallelism.
- **Evidence anchors:** Section 4.1 shows decomposition adds <0.3% overhead; hierarchical action space is explicitly defined in Section 3.1.
- **Break condition:** If network latency dominates bandwidth, frequent small transfers may degrade performance below that of single bulk transfers.

### Mechanism 2
- **Claim:** Integrating Lyapunov drift into the DRL reward function guarantees queue stability and reduces tail latency.
- **Mechanism:** The reward function minimizes "Drift-plus-Penalty" by penalizing actions that increase queue backlog, forcing the agent to prioritize draining the queue when congested.
- **Core assumption:** System operates where average arrival rate is strictly less than maximum service rate ($\lambda < \mu_{max}$).
- **Evidence anchors:** Equation 22 defines the reward combining drift and immediate cost; ablation study shows instability without Lyapunov drift.
- **Break condition:** Queue stability mechanism provides little value for stateless request-response workloads rather than streaming or batched processing.

### Mechanism 3
- **Claim:** Hierarchical action decomposition allows the DRL agent to solve the partitioning problem despite exponentially large action space.
- **Mechanism:** Instead of selecting from $\approx 10^{31}$ possibilities, the policy decomposes decisions per layer, reducing search space complexity from exponential-in-depth to linear-in-depth.
- **Core assumption:** Dependencies between layers can be captured by sequential hidden state embeddings.
- **Evidence anchors:** Section 3.1 describes hierarchical decomposition; asynchronous pipeline execution hides sequential decision latency.
- **Break condition:** Complex non-sequential dependencies across distant layers might yield suboptimal global partitioning.

## Foundational Learning

- **Concept:** Lyapunov Drift & Queue Stability
  - **Why needed here:** Understanding why Splitwise doesn't crash under heavy load requires knowing that "Drift" is a mathematical proxy for queue backlog growth.
  - **Quick check question:** If the queue backlog $Q(t)$ is high, does a high positive Lyapunov drift indicate the queue is draining or growing? (Answer: Growing; the policy seeks to minimize this).

- **Concept:** Transformer Architecture (MHA vs. FFN)
  - **Why needed here:** The paper relies on different compute-to-memory ratios between attention heads and feed-forward networks.
  - **Quick check question:** Which component typically requires more FLOPs but produces a compact representation suitable for transmission? (Answer: Attention mechanisms).

- **Concept:** Proximal Policy Optimization (PPO) & Actor-Critic
  - **Why needed here:** The paper uses a custom PPO architecture with dual critics for performance and stability.
  - **Quick check question:** Why does the Splitwise architecture need two critic networks instead of the standard one? (Answer: To decouple optimization of immediate performance metrics from long-term queue stability).

## Architecture Onboarding

- **Component map:** Input -> Edge Profiling -> Policy Net Decision -> Checkpointing -> Parallel Execution (Edge/Cloud) -> Result Aggregation
- **Critical path:**
  1. State Encoding: Samples queue length, bandwidth, workload stats, and resource availability
  2. Policy Inference: DRL agent generates partition strategy (which heads go where)
  3. Checkpointing: Data at partition boundaries is quantized and saved for fault tolerance
  4. Pipeline Execution: Computation overlaps with communication (Edge computes Layer $\ell$ while transmitting Layer $\ell-1$ results)

- **Design tradeoffs:**
  - Control Parameter $V$: High $V$ prioritizes queue stability but may sacrifice throughput; low $V$ optimizes for speed but risks overflow
  - Granularity: Fine-grained partitioning maximizes adaptation but increases scheduling overhead
  - Memory: Dynamic loading allows running 13B models on 8GB devices but introduces I/O latency if partition changes frequently

- **Failure signatures:**
  - Communication Failure: Triggers exponential backoff and resumes from last partition-boundary checkpoint
  - Queue Overflow: Triggered if $V$ is too low or arrival rate $\lambda > \mu_{max}$; manifests as increasing P99 latency
  - OOM: Occurs if partitioning policy tries to load too many shards simultaneously

- **First 3 experiments:**
  1. Overhead Validation: Profile latency of monolithic vs. decomposed execution to verify <0.3% overhead claim
  2. Stress Test (Ablation): Run at 8.5 req/s with Lyapunov drift disabled to reproduce "Unstable" result
  3. Network Adaptation: Simulate bandwidth drop from 100Mbps to 10Mbps and visualize partitioning heatmaps

## Open Questions the Paper Calls Out

- **Open Question 1:** Can early exit mechanisms or advanced compression techniques be integrated into Splitwise to further reduce communication overhead without destabilizing Lyapunov queue guarantees? The paper explicitly states this as future work for broader applicability.

- **Open Question 2:** How can the energy overhead of dynamic model loading be mitigated on extremely resource-constrained devices (e.g., Raspberry Pi) to prevent energy regression when running large models? The paper notes higher energy consumption on Raspberry Pi due to dynamic loading overhead.

- **Open Question 3:** Does the hierarchical decomposition generalize to emerging non-Transformer architectures (e.g., Mamba/RWKV) which lack distinct MHA-FFN sub-block structure? The paper aims to broaden applicability but doesn't address architectures with different dependency structures.

## Limitations

- The claim of negligible overhead (<0.3%) from fine-grained partitioning doesn't fully detail serialization/deserialization mechanisms for sub-layer blocks, which could become significant in practice.
- While Lyapunov drift guarantees queue stability theoretically, the paper doesn't explore scenarios where arrival rates exceed service capacity, which would break the stability assumption.
- The hierarchical action decomposition assumes sequential dependencies between layers are sufficient, but complex non-sequential dependencies might require different handling.

## Confidence

- **High:** Claims about latency reduction (1.4x-2.8x) and energy savings (up to 41%) are well-supported by experimental results across multiple devices and models.
- **Medium:** Queue stability guarantees via Lyapunov drift are theoretically sound but haven't been tested in extreme overload conditions.
- **Low:** The assumption that serialization overhead remains negligible with sub-layer partitioning needs more validation, especially on bandwidth-constrained networks.

## Next Checks

1. Measure actual serialization/deserialization overhead of sub-layer partitioning under varying bandwidth conditions to verify the <0.3% claim holds in practice.
2. Stress-test the system with arrival rates exceeding service capacity to identify failure modes when Lyapunov stability assumptions break down.
3. Test the hierarchical policy on transformer architectures with complex skip connections to evaluate if sequential decision-making remains optimal.