---
ver: rpa2
title: 'Antisocial behavior towards large language model users: experimental evidence'
arxiv_id: '2601.09772'
source_url: https://arxiv.org/abs/2601.09772
tags:
- page
- participants
- actual
- self-reported
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study experimentally tested whether people punish peers for
  using large language models (LLMs) by giving participants the option to destroy
  part of others' earnings. In a real-effort emoji-counting task, Phase I participants
  completed tasks with or without LLM assistance.
---

# Antisocial behavior towards large language model users: experimental evidence

## Quick Facts
- arXiv ID: 2601.09772
- Source URL: https://arxiv.org/abs/2601.09772
- Reference count: 40
- Participants destroyed 36% of LLM users' earnings on average

## Executive Summary
This experimental study reveals that people engage in costly antisocial punishment toward peers who use large language models (LLMs) to complete tasks. Using a real-effort emoji-counting task, researchers found that participants were willing to spend their own money to reduce the bonus payments of those who relied on LLMs. The punishment was substantial, averaging 36% of affected earnings, and increased systematically with the degree of LLM use. The study also uncovered a credibility gap: participants suspected dishonesty when zero use was self-reported, yet at high levels of actual use, self-reported reliance attracted less punishment than actual reliance.

## Method Summary
The study employed a two-phase experimental design using Amazon Mechanical Turk participants. In Phase I, participants completed a real-effort emoji-counting task either independently or with LLM assistance at varying levels. In Phase II, new participants observed the Phase I performers' work and could spend their own money to destroy portions of these performers' earnings. The punishment mechanism was designed to measure antisocial behavior toward LLM users, with earnings destruction serving as the primary dependent variable.

## Key Results
- Participants destroyed 36% of earnings on average for exclusive LLM users
- Punishment increased monotonically with actual LLM use intensity
- Self-reported zero use attracted more punishment than actual zero use due to suspicion of dishonesty

## Why This Works (Mechanism)
The study demonstrates that social sanctions for AI use are real, costly, and driven by perceived fairness violations. Participants viewed LLM use as potentially unfair advantage-taking, leading to costly punishment behavior. The credibility gap reveals that people actively monitor and suspect dishonesty about AI tool usage, while the monotonic relationship shows that actual rather than perceived behavior drives punishment intensity.

## Foundational Learning

**Experimental Economics**
- Why needed: Provides framework for measuring social preferences and costly punishment behaviors
- Quick check: Understand dictator and ultimatum game paradigms

**Real-Effort Tasks**
- Why needed: Creates ecologically valid measure of actual performance with and without AI assistance
- Quick check: Identify suitable real-effort tasks that can be feasibly augmented with AI

**Social Sanctions Theory**
- Why needed: Explains why individuals incur costs to punish perceived unfair behavior
- Quick check: Review literature on third-party punishment and fairness norms

## Architecture Onboarding

**Component Map**
Phase I performers -> Task completion (with/without LLM) -> Phase II observers -> Punishment decisions -> Earnings destruction

**Critical Path**
Task completion → Observer evaluation → Punishment decision → Earnings reduction

**Design Tradeoffs**
- Real-effort vs. abstract tasks: Real effort provides ecological validity but may not generalize
- Punishment mechanism: Costly destruction measures antisocial behavior but may not reflect all sanction types
- Sample source: MTurk provides scale but may limit external validity

**Failure Signatures**
- Low punishment rates might indicate weak fairness concerns or task misunderstanding
- Non-monotonic punishment patterns could suggest threshold effects or norm uncertainty
- Credibility gap reversal might indicate different social dynamics or task characteristics

**First Experiments**
1. Test alternative real-effort tasks to assess generalizability
2. Manipulate disclosure timing to isolate honesty effects
3. Vary observer incentives to measure punishment robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Online MTurk sample may not generalize to broader populations
- Emoji-counting task may not reflect real-world AI use scenarios
- Punishment mechanism represents stylized social sanction that may differ from workplace consequences

## Confidence

**High Confidence**: Costly punishment of LLM use (36% earnings destruction) is well-supported
**Medium Confidence**: Monotonic relationship between use and punishment severity
**Medium Confidence**: Credibility gap findings regarding self-reported versus actual use

## Next Checks
1. Replicate with alternative real-effort tasks (text editing, data categorization)
2. Pre-registered replication with representative population sample
3. Test mechanism by manipulating beliefs about skill acquisition and effort displacement