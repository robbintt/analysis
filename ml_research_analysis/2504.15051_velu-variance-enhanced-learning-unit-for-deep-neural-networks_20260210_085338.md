---
ver: rpa2
title: 'VeLU: Variance-enhanced Learning Unit for Deep Neural Networks'
arxiv_id: '2504.15051'
source_url: https://arxiv.org/abs/2504.15051
tags:
- velu
- activation
- arxiv
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeLU introduces a variance-aware activation function that combines
  ArcTan-ArcSin transformations, adaptive scaling based on local activation variance,
  and Wasserstein-2 regularization to improve training stability and generalization
  in deep neural networks. Unlike static activations like ReLU, Swish, or GELU, VeLU
  dynamically modulates its response based on input statistics without adding learnable
  parameters.
---

# VeLU: Variance-enhanced Learning Unit for Deep Neural Networks

## Quick Facts
- arXiv ID: 2504.15051
- Source URL: https://arxiv.org/abs/2504.15051
- Reference count: 40
- Introduces a variance-adaptive activation function with ArcTan-ArcSin transformations and Wasserstein-2 regularization

## Executive Summary
VeLU is a novel activation function designed to improve training stability and generalization in deep neural networks. It combines ArcTan-ArcSin transformations with adaptive scaling based on local activation variance and Wasserstein-2 regularization. Unlike static activations like ReLU, Swish, or GELU, VeLU dynamically modulates its response based on input statistics without adding learnable parameters. The method shows consistent performance gains across six architectures on twelve vision benchmarks.

## Method Summary
VeLU introduces a variance-aware activation function that combines ArcTan-ArcSin transformations, adaptive scaling based on local activation variance, and Wasserstein-2 regularization to improve training stability and generalization in deep neural networks. Unlike static activations like ReLU, Swish, or GELU, VeLU dynamically modulates its response based on input statistics without adding learnable parameters. Experiments across six architectures on twelve vision benchmarks show consistent performance gains over baselines.

## Key Results
- Achieves up to 3.12% absolute improvement in top-1 accuracy on CIFAR-100
- Achieves up to 3.85% absolute improvement in top-1 accuracy on Corel-10K
- Maintains competitive training time and memory usage compared to baselines

## Why This Works (Mechanism)
VeLU's effectiveness stems from its ability to adaptively scale activations based on local variance while maintaining smooth transitions through ArcTan-ArcSin transformations. The variance-adaptive mechanism allows the network to automatically adjust activation magnitudes based on input statistics, preventing vanishing/exploding gradients. The Wasserstein-2 regularization further stabilizes training by controlling the distribution of activations across layers, promoting smoother optimization landscapes.

## Foundational Learning
- **Activation Functions**: Non-linear transformations applied to neural network outputs; needed to introduce non-linearity and enable complex pattern learning; quick check: compare ReLU, Swish, and GELU outputs on sample data
- **Variance in Neural Networks**: Statistical measure of activation spread; needed to understand activation stability and gradient flow; quick check: compute activation variance across network layers
- **Wasserstein Distance**: Metric for comparing probability distributions; needed for regularization to control activation distribution; quick check: calculate Wasserstein-2 distance between activation distributions
- **ArcTan-ArcSin Transformations**: Mathematical functions providing smooth, bounded outputs; needed for stable activation scaling; quick check: plot ArcTan-ArcSin transformation curves
- **Adaptive Scaling**: Dynamic adjustment of activation magnitudes; needed for variance-aware response; quick check: verify scaling factor computation from local variance
- **Regularization**: Techniques to prevent overfitting; needed to improve generalization; quick check: compare training vs validation loss curves

## Architecture Onboarding

**Component Map**: Input -> ArcTan-ArcSin Transformation -> Variance Calculation -> Adaptive Scaling -> Output

**Critical Path**: The core computation path involves calculating local activation variance, applying ArcTan-ArcSin transformations, and scaling based on variance estimates. This path executes at every activation layer.

**Design Tradeoffs**: The method trades computational overhead for improved generalization. While more complex than ReLU, it avoids learnable parameters unlike attention-based mechanisms. The fixed hyperparameter λ requires careful tuning but provides stability.

**Failure Signatures**: Poor performance may manifest as training instability, gradient vanishing/exploding, or suboptimal convergence. Over-regularization (high λ) can lead to underfitting, while under-regularization may not provide stability benefits.

**Three First Experiments**:
1. Replace ReLU with VeLU in a simple CNN on CIFAR-10 and compare convergence curves
2. Compare activation distributions across layers with and without Wasserstein-2 regularization
3. Test sensitivity to the regularization parameter λ by training with different values

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance under dataset shifts and adversarial perturbations is unclear
- Results depend on a fixed hyperparameter (λ) for regularization, with sensitivity not fully explored
- Long-term stability and scaling to very deep architectures require further validation

## Confidence
- High: Theoretical motivation, mathematical formulation, and empirical results on standard vision tasks
- Medium: Generalization benefits and robustness under non-standard conditions
- Low: Long-term stability, scaling to very deep architectures, and sensitivity to hyperparameters

## Next Checks
1. Test VeLU's performance under adversarial attacks and out-of-distribution data to assess robustness beyond standard benchmarks
2. Conduct a systematic ablation study isolating the impact of ArcTan-ArcSin transformations vs. variance-adaptive scaling on accuracy gains
3. Evaluate the sensitivity of the Wasserstein-2 regularization parameter (λ) across different architectures and datasets to determine if adaptive tuning is needed