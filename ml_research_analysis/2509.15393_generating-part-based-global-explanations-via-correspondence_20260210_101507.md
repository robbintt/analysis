---
ver: rpa2
title: Generating Part-Based Global Explanations Via Correspondence
arxiv_id: '2509.15393'
source_url: https://arxiv.org/abs/2509.15393
tags:
- explanations
- image
- global
- images
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEPC (Global Explanations via Part Correspondence),
  a system that generates part-based global explanations for deep neural network image
  classifiers. The approach addresses the challenge of producing interpretable global
  insights from black-box models by leveraging user-defined part labels from a small
  annotated subset of images.
---

# Generating Part-Based Global Explanations Via Correspondence

## Quick Facts
- arXiv ID: 2509.15393
- Source URL: https://arxiv.org/abs/2509.15393
- Reference count: 10
- Primary result: Generates part-based global explanations for image classifiers using label transfer via Hyperpixel Flow and greedy set cover aggregation

## Executive Summary
This paper introduces GEPC (Global Explanations via Part Correspondence), a system that generates part-based global explanations for deep neural network image classifiers. The approach addresses the challenge of producing interpretable global insights from black-box models by leveraging user-defined part labels from a small annotated subset of images. GEPC uses Hyperpixel Flow to transfer these part labels to a larger dataset via visual correspondence, then generates local Minimal Sufficient Explanations (MSXs) using beam search. Finally, it aggregates these local explanations into global symbolic explanations using a greedy set cover algorithm.

## Method Summary
GEPC operates through three main stages: First, it transfers part labels from a small annotated subset to a larger dataset using Hyperpixel Flow, which matches multi-layer CNN features with geometric consistency voting. Second, it generates local MSXs by finding minimal superpixel subsets that preserve ≥90% of classifier confidence using beam search. Third, it aggregates these local explanations into global symbolic rules through greedy set cover, producing human-readable decision lists that maximize coverage of model predictions.

## Key Results
- Label transfer accuracy averaged 84.28% across 158 categories
- Relational explanations covered more images than part-based ones in CUB200 and Stanford datasets
- Cross-validation showed substantial coverage of model predictions with generated global explanations
- Method evaluated on three datasets: Stanford Cars, CUB-200, and PartImageNet using 70:30 train-test split

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperpixel Flow (HPF) enables label transfer across visually similar images by matching multi-layer deep features rather than raw pixels.
- Mechanism: HPF constructs a "hyperimage" by pooling and upsampling intermediate CNN outputs from multiple layers (low-level edges to high-level concepts). For each query image segment centroid, it computes appearance similarity (exponentiated cosine distance) weighted by Hough-space geometric consistency voting. This enforces that matched parts maintain similar spatial relationships across images.
- Core assumption: The pre-trained classifier's internal features encode semantically meaningful part information that transfers across class instances. Assumption: Visually similar images (same viewpoint, orientation) exist in the annotated gallery set.
- Evidence anchors:
  - [Abstract] "efficiently transfers [part labels] to a larger dataset"
  - [Section 3.1] "HPF performs matching over the deep features, motivated by the fact that different layers of the DNN model learn low-level features to high-level concepts"
  - [Section 4.2] "HPF using Resnet101 scores an average of 84.28% on the part labels transfer task for 158 categories"
- Break condition: Fails when query images have no visually similar annotated neighbors (different pose/viewpoint), or when HPF's geometric consistency assumption is violated by significant pose variation.

### Mechanism 2
- Claim: Beam search finds multiple Minimal Sufficient Explanations (MSXs)—superpixel subsets that preserve ≥90% of original prediction confidence—by systematically exploring combinatorial space.
- Mechanism: Starting from SLIC-generated superpixels, beam search iteratively evaluates candidate subsets by masking out non-selected regions and measuring classifier confidence. A subset qualifies as an MSX if: (1) the masked image achieves π_c(N_i) > 0.9 × π_c(original), and (2) no strict subset achieves this threshold. Multiple MSXs per image capture the fact that classifiers can rely on different feature combinations.
- Core assumption: The classifier's decision relies on localized regions rather than globally distributed patterns that cannot be isolated to discrete superpixels.
- Evidence anchors:
  - [Abstract] "generates local Minimal Sufficient Explanations (MSXs) using beam search"
  - [Section 3.2] "It has been shown that there are often multiple such MSXs for a single input image"
  - [Section 1] "Shitole et al. [2021] argue that a single saliency map provides an incomplete understanding"
- Break condition: Fails when superpixel boundaries do not align with semantically meaningful regions, or when the 90% threshold is too permissive/strict for the classifier's confidence distribution.

### Mechanism 3
- Claim: Greedy set cover approximates the NP-hard shortest decision list problem, producing human-readable global explanations that maximize dataset coverage.
- Mechanism: Symbolic MSXs (e.g., {Bird-Head, Bird-Wing, Bird-Beak}) are treated as set elements. Algorithm 1 iteratively selects the MSX covering the most uncovered images, removes those images from the uncovered set, and repeats. The result is a decision list interpretable as disjunctive normal form (OR of ANDs). Relational rules extend this by adding spatial predicates (e.g., "Bird-Head above Bird-Body").
- Core assumption: A relatively small set of symbolic rules can cover the majority of model predictions—implies the model learns systematic, recurring patterns rather than instance-specific features.
- Evidence anchors:
  - [Abstract] "aggregates these local explanations into global symbolic explanations using a greedy set cover algorithm"
  - [Section 3.3] "Since finding the shortest decision list is an NP-hard problem, we employ a greedy set cover algorithm"
  - [Section 4.3] "relational expressions cover more images in CUB200 and Stanford datasets than the part-based ones" (Figure 11)
- Break condition: Fails when local MSXs are highly heterogeneous (no common patterns), producing trivial global explanations with one rule per image. Also fails if greedy selection gets trapped in poor local optima missing more generalizable rules.

## Foundational Learning

- **Hypercolumns / Multi-scale Feature Aggregation**
  - Why needed here: HPF relies on understanding that different CNN layers capture different abstraction levels—early layers encode edges/textures, deeper layers encode semantic parts. Without this, the mechanism of matching across layers is opaque.
  - Quick check question: Given a ResNet, which layers would you expect to contain "beak" vs. "edge" information, and how would you combine them spatially?

- **Beam Search**
  - Why needed here: MSX generation requires systematic exploration of exponential superpixel combinations. Understanding beam width tradeoffs (coverage vs. computation) is critical for practical deployment.
  - Quick check question: If you have 20 superpixels and beam width 5, what is the maximum number of candidate MSXs evaluated at depth 3?

- **Set Cover Problem Complexity**
  - Why needed here: Global explanation aggregation is fundamentally a combinatorial optimization. Understanding why greedy is an approximation (ln(n) factor) helps set expectations for coverage quality.
  - Quick check question: Given 100 images and 50 unique symbolic MSXs, what is the worst-case approximation ratio of greedy vs. optimal set cover?

## Architecture Onboarding

- **Component map:**
  Annotation Module -> Retrieval Module -> Correspondence Module -> Segmentation Module -> Local Explanation Module -> Global Aggregation Module

- **Critical path:** Annotation quality → HPF accuracy → MSX validity → Global coverage. Errors propagate: poor part transfer corrupts symbolic MSXs, leading to misleading global rules. The 15% annotation sampling rate is a key leverage point.

- **Design tradeoffs:**
  - Annotation fraction (15%): Higher = better HPF accuracy, higher cost
  - Beam width: Wider = more MSXs found, exponential compute growth
  - Confidence threshold (0.9): Higher = stricter MSXs, potentially fewer discovered
  - Superpixel count: More granular = finer explanations, larger search space

- **Failure signatures:**
  - Low label transfer accuracy (<70%): Gallery images insufficiently similar—expand annotation or add pose clustering
  - MSXs containing mostly "background": Classifier not using semantic parts—may indicate dataset artifacts or overfitting
  - Global rules with single-image coverage: MSXs too specific—reduce superpixel granularity or increase beam width
  - Relational rules underperforming part-only: Spatial predicates too restrictive—relax relationship definitions

- **First 3 experiments:**
  1. **Baseline label transfer**: Measure HPF accuracy on PartImageNet validation split with 10%/15%/25% annotation fractions to quantify annotation sensitivity.
  2. **MSX coverage analysis**: For a single class (e.g., Bee-Eater), plot number of unique MSXs vs. beam width (3, 5, 10) to identify saturation point.
  3. **Global explanation robustness**: 5-fold cross-validation measuring coverage variance across splits—high variance indicates unstable rule discovery.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can the GEPC pipeline be effectively adapted for non-visual data modalities, such as gene expression analysis or question answering in texts?
  - Basis in paper: [explicit] The conclusion states, "We envision future work that explores the extensions of our general approach... to other tasks such as gene expression analysis, activity recognition in videos, and question answering from texts."
  - Why unresolved: The current implementation relies heavily on visual correspondence mechanisms (Hyperpixel Flow) and superpixel segmentation, which do not directly translate to non-spatial data structures like sequences or genetic profiles.
  - What evidence would resolve it: A demonstration of the MSX generation and set cover aggregation logic applied successfully to sequential or bioinformatics data.

- **Open Question 2**
  - Question: How can the global explanations be refined to serve as surrogate models that distinguish between classes, rather than identifying features potentially shared by multiple categories?
  - Basis in paper: [explicit] The authors note, "While global explanations shed light on what parts may be responsible for the model's decisions, they are not surrogate models in that the same explanation might hold for multiple classes."
  - Why unresolved: The current method aggregates minimal sufficient explanations (MSXs) based on coverage but lacks a mechanism to ensure the resulting decision list is discriminative across different classes.
  - What evidence would resolve it: An extension of the greedy set cover algorithm that incorporates negative constraints or contrastive examples to ensure rule uniqueness.

- **Open Question 3**
  - Question: How robust is the label transfer mechanism when the annotated gallery images differ significantly in viewpoint or orientation from the unlabeled query images?
  - Basis in paper: [inferred] The paper notes that "the HPF algorithm works well only on visually similar images," necessitating a nearest-neighbor step to find similar gallery images before transfer.
  - Why unresolved: The system relies on finding a visually similar annotated image to bootstrap the process. If the dataset has high pose variance and the 15% annotated subset lacks a similar pose, the Hyperpixel Flow may fail.
  - What evidence would resolve it: Ablation studies measuring label transfer accuracy and explanation quality when the gallery set has restricted pose diversity compared to the query set.

## Limitations

- The approach critically depends on visual similarity between annotated and unannotated images for label transfer, with 84.28% accuracy leaving room for error propagation.
- The assumption that semantically meaningful parts align with superpixel boundaries may not hold for all object classes or segmentation granularities.
- The greedy set cover algorithm provides only an approximate solution to finding the shortest explanation set, potentially missing more compact global explanations.

## Confidence

- **High Confidence**: The core mechanism of using Hyperpixel Flow for part label transfer across similar images is well-established in computer vision literature. The mathematical framework for beam search finding Minimal Sufficient Explanations is rigorous and reproducible.
- **Medium Confidence**: The effectiveness of greedy set cover for producing human-readable global explanations, while theoretically sound, depends heavily on the quality and diversity of local MSXs. The claim that relational explanations cover more images than part-based ones needs more statistical validation.
- **Low Confidence**: The practical utility of the generated explanations for non-expert users is not empirically validated. The paper does not demonstrate how these explanations improve human understanding or trust in model predictions.

## Next Checks

1. **Annotation Sensitivity Analysis**: Systematically vary the annotation fraction (10%, 15%, 20%, 25%) and measure impact on HPF accuracy and subsequent global explanation quality to determine the optimal tradeoff between annotation cost and explanation fidelity.

2. **Cross-Dataset Transferability**: Test whether global explanations generated on CUB-200 transfer to related datasets like iNaturalist, evaluating the generality of discovered part-based patterns across broader taxonomic contexts.

3. **Human Evaluation Study**: Conduct user studies comparing GEPC explanations against baseline saliency maps and other XAI methods, measuring both comprehension accuracy and user trust ratings to validate practical utility claims.