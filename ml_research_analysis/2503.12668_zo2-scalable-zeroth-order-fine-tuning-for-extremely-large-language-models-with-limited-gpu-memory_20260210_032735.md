---
ver: rpa2
title: 'ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models
  with Limited GPU Memory'
arxiv_id: '2503.12668'
source_url: https://arxiv.org/abs/2503.12668
tags:
- memory
- computation
- data
- framework
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO2, a scalable zeroth-order fine-tuning
  framework for extremely large language models (LLMs) with limited GPU memory. The
  core method leverages CPU offloading to transfer inactive parameters from GPU to
  CPU, exploiting the unique dual forward pass architecture of zeroth-order (ZO) optimization
  to minimize communication overhead.
---

# ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited GPU Memory

## Quick Facts
- arXiv ID: 2503.12668
- Source URL: https://arxiv.org/abs/2503.12668
- Authors: Liangyu Wang; Jie Ren; Hang Xu; Junxiao Wang; Huanyi Xie; David E. Keyes; Di Wang
- Reference count: 37
- Enables fine-tuning of 175B parameter models on 18GB GPU with minimal overhead

## Executive Summary
ZO2 introduces a novel zeroth-order optimization framework for fine-tuning extremely large language models with limited GPU memory. The key innovation leverages CPU offloading to transfer inactive parameters during the dual forward pass architecture of ZO optimization, minimizing communication overhead. By exploiting the unique structure of ZO methods, ZO2 achieves memory efficiency without accuracy loss or significant computational overhead, enabling practical fine-tuning of models like OPT-175B on consumer-grade hardware.

## Method Summary
ZO2 implements CPU offloading for inactive parameters during zeroth-order optimization's dual forward passes. The framework includes a random number generator state manager for accuracy preservation, dynamic scheduling to overlap computation and communication, reusable GPU memory blocks, and low-bit precision support in AMP mode. The method exploits the fact that zeroth-order methods require two forward passes per iteration, allowing parameter offloading between passes to free GPU memory. Dynamic scheduling ensures overlapping communication with computation, while the state manager preserves stochastic gradient estimation accuracy. The approach supports various batch sizes and sequence lengths while maintaining throughput.

## Key Results
- Enables fine-tuning of OPT-175B (175B parameters) on a single 18GB GPU with minimal overhead
- Achieves consistent memory usage reductions across model sizes
- Maintains or improves throughput in most cases compared to standard ZO methods
- Demonstrates absolutely no accuracy loss compared to standard ZO methods
- Shows robust performance across varying batch sizes and sequence lengths

## Why This Works (Mechanism)
The dual forward pass architecture of zeroth-order optimization creates a unique opportunity for memory optimization. By offloading inactive parameters to CPU during the first pass and retrieving them for the second pass, ZO2 minimizes communication overhead while maximizing GPU memory availability. The random number generator state manager ensures stochastic gradient estimation accuracy is preserved across CPU-GPU transfers. Dynamic scheduling overlaps communication with computation, effectively hiding transfer costs. Low-bit precision support further reduces memory requirements without compromising accuracy.

## Foundational Learning
- **Zeroth-order optimization**: Gradient-free optimization method needed for black-box systems and scenarios where gradients are unavailable or expensive to compute
- **Dual forward pass architecture**: Required in ZO methods to estimate gradients using finite differences, creating natural points for parameter management
- **CPU-GPU offloading**: Memory management technique that transfers data between devices to optimize resource utilization
- **Dynamic scheduling**: Runtime optimization that overlaps communication with computation to hide latency
- **Random number generator state management**: Ensures reproducibility and accuracy when stochastic operations cross device boundaries
- **Automatic mixed precision (AMP)**: Technique that uses lower precision for certain operations to reduce memory usage and increase throughput

## Architecture Onboarding

**Component map:**
Random Number Generator State Manager -> Dynamic Scheduler -> CPU Offloader -> GPU Memory Manager -> Parameter Updater -> AMP Low-bit Precision Module

**Critical path:**
Forward Pass 1 (inactive params offloaded) -> CPU transfer -> Forward Pass 2 (params retrieved) -> Parameter update

**Design tradeoffs:**
Memory vs. communication overhead (offload inactive parameters to save memory but incur transfer costs), precision vs. memory efficiency (low-bit precision reduces memory but may affect accuracy), overlap efficiency (dynamic scheduling tries to hide communication but effectiveness depends on hardware)

**Failure signatures:**
Accuracy degradation (likely from improper state management), memory overflow (insufficient CPU memory or poor scheduling), throughput collapse (communication overhead dominates computation), state inconsistency (random number generator state not properly preserved)

**Three first experiments:**
1. Single model layer offloading with fixed batch size to verify basic functionality
2. Memory usage profiling across different parameter subsets to optimize offloading strategy
3. Communication overhead measurement with varying CPU-GPU bandwidth to validate dynamic scheduling effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability beyond single-node CPU-GPU setups not addressed, limiting applicability to distributed training scenarios
- Claims of "absolutely no accuracy loss" based only on comparisons with standard ZO methods rather than full fine-tuning baselines
- Limited characterization of performance under heterogeneous hardware configurations and varying network conditions
- Does not extensively explore the impact on different task types and model architectures beyond the demonstrated cases

## Confidence
- High confidence in memory efficiency claims (supported by systematic ablation studies and consistent memory usage reductions across model sizes)
- Medium confidence in computational overhead claims (validated through throughput measurements but limited to specific hardware configurations)
- Medium confidence in accuracy preservation claims (based on controlled experiments but limited comparison scope)

## Next Checks
1. Evaluate ZO2 performance across heterogeneous CPU-GPU configurations with varying bandwidth constraints to validate the dynamic scheduler's robustness in diverse hardware environments
2. Conduct extensive ablation studies on the random number generator state manager's impact on different types of tasks and model architectures to verify universal accuracy preservation
3. Scale experiments to distributed multi-GPU setups and characterize communication overhead patterns to identify potential bottlenecks in large-scale deployments