---
ver: rpa2
title: 'Hyperbolic Residual Quantization: Discrete Representations for Data with Latent
  Hierarchies'
arxiv_id: '2505.12404'
source_url: https://arxiv.org/abs/2505.12404
tags:
- hyperbolic
- space
- hierarchical
- multitokens
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating discrete hierarchical
  representations for data with latent hierarchies. The authors propose Hyperbolic
  Residual Quantization (HRQ), which adapts the standard Residual Quantization (RQ)
  algorithm to operate in hyperbolic space rather than Euclidean space.
---

# Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies

## Quick Facts
- arXiv ID: 2505.12404
- Source URL: https://arxiv.org/abs/2505.12404
- Reference count: 40
- Primary result: HRQ achieves up to 20% improvement in recall@10 for hypernym generation compared to RQ, and consistently outperforms RQ in recommender system tasks

## Executive Summary
This paper addresses the challenge of creating discrete hierarchical representations for data with latent hierarchies. The authors propose Hyperbolic Residual Quantization (HRQ), which adapts the standard Residual Quantization (RQ) algorithm to operate in hyperbolic space rather than Euclidean space. By incorporating hyperbolic geometry through specialized embedding networks, residual computations, and distance metrics, HRQ aligns naturally with hierarchical branching structures. The method is evaluated on two scenarios: hierarchy modeling using WordNet hypernym trees, and hierarchy discovery using Amazon product reviews and MovieLens datasets, demonstrating substantial improvements over Euclidean RQ baselines.

## Method Summary
HRQ-VAE operates on the Poincaré ball model with Möbius addition/subtraction, hyperbolic distance, and exp/log maps for manifold-tangent space conversion. The model uses Riemannian SGD/Adam with reconstruction loss plus RQ codebook loss with α-weighted commitment term. It employs hyperbolic encoder/decoder networks and produces multitokens (k tokens per item) through hierarchical codebook lookup. The method is evaluated in two stages: first learning multitokens, then freezing them and training downstream transformer models for evaluation on WordNet hypernym prediction and sequential recommendation tasks.

## Key Results
- HRQ achieves up to 20% improvement in recall@10 for hypernym generation compared to RQ on WordNet
- Consistently outperforms RQ in recommender system tasks across Amazon and MovieLens datasets
- Demonstrates particular advantage at low dimensional embeddings (4-32 dimensions)

## Why This Works (Mechanism)

### Mechanism 1: Geometric Alignment Between Space Growth and Hierarchy Growth
Hyperbolic space's exponential volume expansion matches the exponential node growth in hierarchical trees, reducing representational distortion compared to Euclidean space. In the Poincaré ball model, distance from center grows exponentially, mirroring how hierarchies expand with few root nodes near center and many leaf nodes toward boundary. Core assumption: Data contains latent tree-like hierarchies that benefit from this alignment.

### Mechanism 2: Algebraic Closure via Möbius Operations Preserves Curvature
Computing residuals with Möbius subtraction and reconstructing with Möbius addition keeps all intermediate states on the manifold, preventing geometric distortion across quantization levels. Standard Euclidean subtraction would exit the manifold. Core assumption: Maintaining strict manifold membership throughout quantization improves downstream generalization.

### Mechanism 3: Hyperbolic Distance Induces Hierarchy-Aware Codebook Clustering
Using hyperbolic distance for nearest-codebook selection naturally clusters points along radial branches rather than Euclidean globular clusters, producing more semantically meaningful multitokens. The hyperbolic distance metric penalizes radial depth differences and angular separation differently than Euclidean distance. Core assumption: Radial sector organization translates to better semantic token hierarchies for downstream tasks.

## Foundational Learning

- **Poincaré Ball Model**: All HRQ operations occur in this model. Understanding curvature parameter c, the constraint ||x||² < 1/c, and why distance grows exponentially toward the boundary is essential. Quick check: Can you explain why two points near the boundary can have large hyperbolic distance even if their Euclidean distance is small?

- **Möbius Addition and Subtraction**: These replace standard vector arithmetic. They are non-commutative and depend on curvature c. Quick check: What happens to Möbius addition x ⊕c y when c → 0? (Hint: it should recover Euclidean addition.)

- **Exponential and Logarithmic Maps**: HRQ-VAE encodes Euclidean inputs to hyperbolic space via expc 0(x) and decodes back via logc 0(y). Misunderstanding these maps leads to incorrect gradient flow. Quick check: Given a point y on the manifold, what does logc x(y) represent geometrically?

## Architecture Onboarding

- **Component map**: Input → expc 0 → Hyperbolic Encoder EPc θ → HRQ module (k-level codebook with Möbius residuals, hyperbolic distance lookup) → Hyperbolic Decoder DPc θ → logc 0 → Output
- **Critical path**: 1) Correct implementation of exp/log maps (numerical stability near boundary), 2) Accurate hyperbolic distance computation (avoid division by zero when ||u||² → 1/c), 3) Proper gradient handling through Möbius operations (use Riemannian optimizer)
- **Design tradeoffs**: Curvature c (larger c increases exponential growth but risks overflow), Codebook depth k (more tokens capture finer hierarchy but increase sequence length), Hidden dimension h (HRQ outperforms RQ especially at low dimensions)
- **Failure signatures**: Exploding norms in latent vectors (likely incorrect exp map or learning rate too high), Codebook collapse (check α parameter in RQ loss), NaN in distance computation (points exceeding Poincaré ball boundary)
- **First 3 experiments**: 1) Sanity check on synthetic tree: verify multitoken prefixes correlate with tree depth, 2) Ablation on curvature: sweep c ∈ {0.1, 0.5, 1.0} on WordNet subset, 3) Dimension scaling: vary h ∈ {4, 8, 16, 32}, compare HRQ vs. RQ gap

## Open Questions the Paper Calls Out

1. Can HRQ improve performance in domains like image or audio processing where latent hierarchies are less explicit? The authors state exploring HRQ-VAE in these domains remains an exciting future direction.

2. Does combining HRQ with Hyper-VQ's regression-based codebook assignment yield superior discrete representations? The paper notes both can be combined together and considers it promising future work.

3. Do the discrete tokens generated by HRQ offer better semantic interpretability regarding data taxonomies compared to Euclidean RQ? While the paper claims HRQ leads to more interpretable models, experiments only evaluate quantitative metrics.

## Limitations

- Limited ablation studies isolating impact of Möbius operations versus Euclidean operations with hyperbolic embeddings
- Absence of systematic exploration of curvature parameter c values
- Experimental scope limited to relatively standard datasets with clear hierarchies
- No analysis of whether HRQ's advantages extend to more complex or noisy hierarchical structures

## Confidence

**High Confidence Claims:**
- HRQ produces multitokens that outperform Euclidean RQ counterparts on both hierarchy modeling and discovery tasks
- Hyperbolic distance metric induces radial clustering patterns consistent with hierarchical structure
- HRQ demonstrates particular advantage at low dimensional embeddings (4-32 dimensions)

**Medium Confidence Claims:**
- Exponential growth properties of hyperbolic space align naturally with hierarchical tree expansion
- Möbius arithmetic operations preserve curvature and improve representation quality
- Multitoken approach effectively captures hierarchical relationships

**Low Confidence Claims:**
- Specific numerical improvements are highly sensitive to dataset splits and hyperparameters
- Claims about superiority over all Euclidean baselines without extensive competitor analysis
- Generalization to arbitrary hierarchical structures beyond tested domains

## Next Checks

1. **Ablation on Geometric Operations**: Implement a variant of HRQ using Euclidean arithmetic while keeping hyperbolic embedding space and distance metric. Compare against full HRQ and standard RQ on WordNet hypernym prediction to isolate Möbius operation benefits.

2. **Synthetic Hierarchy Stress Test**: Generate synthetic hierarchical datasets with varying branching factors and depths. Apply HRQ and RQ, then measure reconstruction error, correlation between token prefixes and true hierarchy depth, and downstream classification accuracy.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary curvature parameter c across [0.1, 0.5, 1.0, 2.0] and codebook depth k across [2, 3, 4, 5] on Amazon Beauty dataset. Plot performance curves (Recall@10 vs. c, and Recall@10 vs. k) for both HRQ and RQ.