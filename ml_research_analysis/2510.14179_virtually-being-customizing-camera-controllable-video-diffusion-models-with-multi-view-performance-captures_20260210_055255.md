---
ver: rpa2
title: 'Virtually Being: Customizing Camera-Controllable Video Diffusion Models with
  Multi-View Performance Captures'
arxiv_id: '2510.14179'
source_url: https://arxiv.org/abs/2510.14179
tags:
- video
- camera
- generation
- data
- customization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for video generation with multi-view
  identity preservation and 3D camera control by fine-tuning video diffusion models
  on volumetric capture data. The method leverages 4D Gaussian Splatting to reconstruct
  dynamic human performances and generate diverse training videos with accurate camera
  annotations and relit lighting.
---

# Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures

## Quick Facts
- arXiv ID: 2510.14179
- Source URL: https://arxiv.org/abs/2510.14179
- Authors: Yuancheng Xu; Wenqi Xian; Li Ma; Julien Philip; Ahmet Levent Taşel; Yiwei Zhao; Ryan Burgert; Mingming He; Oliver Hermann; Oliver Pilarski; Rahul Garg; Paul Debevec; Ning Yu
- Reference count: 15
- Primary result: Fine-tunes video diffusion models on 4D Gaussian Splatting data to achieve multi-view identity preservation and 3D camera control, improving identity preservation (AdaFace 0.351) and camera control (translation error 0.324).

## Executive Summary
This paper presents a framework for video generation with multi-view identity preservation and 3D camera control by fine-tuning video diffusion models on volumetric capture data. The method leverages 4D Gaussian Splatting to reconstruct dynamic human performances and generate diverse training videos with accurate camera annotations and relit lighting. A two-stage training strategy first learns general camera-conditioned video generation, then customizes the model for specific subjects. The framework supports multi-subject generation, scene customization, real-life video adaptation, and control over motion and layout. Extensive experiments show improved multi-view identity preservation (AdaFace 0.351), better camera control (translation error 0.324), and enhanced lighting realism, advancing integration of video generation into virtual production pipelines.

## Method Summary
The framework fine-tunes CogVideoX (DiT-based video diffusion) in two stages: (1) Camera pretraining with ControlNet on RealEstate10K and HumanVid datasets using Plücker coordinates for 3D camera control, and (2) Identity customization via DreamBooth fine-tuning on 4DGS-rendered volumetric capture data. The method generates training videos by re-rendering 4DGS reconstructions with diverse camera trajectories and relighting using Lux Post Facto. Multi-subject generation is supported through either joint training or noise blending with segmentation masks. The approach enables customization of both subjects and scenes while maintaining precise camera control and identity preservation across viewpoints.

## Key Results
- Achieves state-of-the-art multi-view identity preservation with AdaFace score of 0.351
- Improves camera control with translation error of 0.324, outperforming AC3D baseline
- Supports multi-subject generation through noise blending, enabling efficient composition of independently customized models
- Enhances lighting realism through relit 4DGS training data with Poly Haven HDRIs

## Why This Works (Mechanism)

### Mechanism 1
Multi-view identity preservation is likely driven by training on synthetically re-rendered 4D Gaussian Splatting (4DGS) data rather than single-view images. The 4DGS reconstruction creates a unified 3D volumetric representation of the subject's performance. By rendering training videos from this representation using diverse, random camera trajectories, the diffusion model receives explicit supervision on how the subject's identity manifests from novel viewpoints, reducing overfitting to a single canonical angle. Core assumption: The 4DGS reconstruction accurately captures subject geometry and appearance without significant artifacts that would degrade the training distribution.

### Mechanism 2
3D camera control is achieved by conditioning the model on Plücker coordinates via a ControlNet architecture during the pretraining stage. Plücker coordinates represent 3D lines (rays) in space, providing a compact representation of camera extrinsics. By injecting these coordinates through a ControlNet (trained while the main DiT backbone is frozen), the model learns to correlate specific ray trajectories with visual flow and perspective shifts, separating camera motion from subject motion. Core assumption: The mapping from Plücker rays to pixel motion is learnable and consistent across the pretraining datasets and the customization domain.

### Mechanism 3
Multi-subject generation can be decomposed into independent single-subject customizations and merged at inference time via noise blending. Instead of training a monolithic model on all subjects simultaneously, the method generates a coarse layout video, creates spatio-temporal masks, and runs parallel denoising steps for each subject using their specific customized models. The latents are then blended pixel-wise according to the masks. Core assumption: The diffusion process is sufficiently localized such that latent regions can be optimized independently while maintaining global temporal consistency.

## Foundational Learning

- **Concept: 4D Gaussian Splatting (4DGS)**
  - **Why needed here:** This is the data engine. Unlike NeRFs, 4DGS uses discrete 3D Gaussians with time-dependent properties to represent dynamic scenes, allowing for real-time, high-fidelity rendering from arbitrary viewpoints, which is critical for generating the training data.
  - **Quick check question:** Can you explain how a 4D Gaussian differs from a 3D Gaussian in terms of the parameters defining its position and opacity over time?

- **Concept: Plücker Coordinates**
  - **Why needed here:** This is the control signal. Understanding how 6DOF camera pose is converted into these ray vectors is essential to understanding how the ControlNet "steers" the video generation.
  - **Quick check question:** How do Plücker coordinates represent a directed line in 3D space using a 6D vector, and why might this be more suitable for view synthesis than standard rotation matrices?

- **Concept: ControlNet & LoRA Fine-Tuning**
  - **Why needed here:** These are the architectural modification tools. The method uses ControlNet for camera control and DreamBooth/LoRA for identity preservation.
  - **Quick check question:** In the context of this paper, why is the camera ControlNet trained on a general dataset while the identity LoRA is trained on the specific 4DGS data?

## Architecture Onboarding

- **Component map:** Volumetric Capture (75-160 cams) -> 4DGS Reconstruction -> Rendering + Relighting (Lux Post Facto) -> Base Model: CogVideoX (DiT-based Video Diffusion) -> Conditioning: ControlNet branch accepting Plücker coordinates -> Customization: DreamBooth fine-tuning on 4DGS renders -> Inference: Noise Blending module using SA2VA masks

- **Critical path:** The quality of the 4DGS reconstruction is the primary bottleneck. If the reconstruction is low fidelity or temporally unstable, the model fine-tunes on garbage data, resulting in identity degradation.

- **Design tradeoffs:**
  - **Joint Training vs. Noise Blending:** Joint training yields better interaction realism (72.9% user preference) but requires retraining for every new combination. Noise blending is zero-shot for combinations but requires a segmentation step and may struggle with occlusions.
  - **Frontal-only vs. Multi-view:** Using only frontal views causes identity collapse in profile views; multi-view data is strictly required for 3D consistency.

- **Failure signatures:**
  - **Flat Lighting:** Occurs if the "Relit Data" is ablated.
  - **Profile Drift:** Identity changes when the head turns > 45 degrees if training data lacks multi-view coverage.
  - **Boundary Artifacts:** Occurs in noise blending if segmentation masks do not perfectly track subject motion.

- **First 3 experiments:**
  1. **Sanity Check (Camera):** Train the ControlNet on RealEstate10K and verify rotation/translation error drops (Target: TransErr ~0.31 vs AC3D baseline).
  2. **Ablation (Views):** Fine-tune the model on frontal-only data vs. multi-view data. Verify AdaFace score drop (e.g., from 0.351 to 0.327) on profile test cases.
  3. **Module Test (Noise Blending):** Generate a two-subject video using noise blending and compare the AdaFace score against the joint training baseline (Expect slight drop, e.g., 0.337 -> 0.320).

## Open Questions the Paper Calls Out

### Open Question 1
Can the method achieve high-fidelity identity preservation without per-subject fine-tuning, moving toward a zero-shot customization paradigm? Basis in paper: The Conclusion lists "the need for fine-tuning to fully leverage high-quality multi-view 4DGS data" as a primary limitation. Why unresolved: The current reliance on DreamBooth-style optimization creates a barrier for rapid deployment compared to inference-only methods. What evidence would resolve it: A benchmark demonstrating comparable AdaFace scores on unseen subjects using only inference-time conditioning, without weight updates.

### Open Question 2
Can inference-time composition techniques match the multi-subject identity fidelity of joint training? Basis in paper: Section 4.6 reports that noise blending achieves an AdaFace score of 0.320, which is lower than the 0.337 achieved by joint training. Why unresolved: There exists a trade-off between the flexibility of composing independently customized models and the coherence of training them together. What evidence would resolve it: Development of a blending strategy that closes the performance gap with joint training on standard multi-subject benchmarks.

### Open Question 3
How does the resolution bottleneck of the underlying video diffusion backbone (CogVideoX) limit the utilization of high-fidelity 4DGS inputs? Basis in paper: The authors note the "low resolution of the CogVideoX backbone, which underutilizes these higher-resolution inputs." Why unresolved: It is unclear if the identity preservation failures are due to the model architecture's resolution limits or the customization methodology itself. What evidence would resolve it: Re-evaluating the framework using a higher-resolution video diffusion backbone to measure gains in facial detail metrics.

## Limitations
- Data quality dependency on 4D Gaussian Splatting reconstruction fidelity, with potential identity degradation if reconstruction contains artifacts
- Real-world adaptation effectiveness for consumer-grade multi-view video input remains untested
- Interaction realism trade-off between joint training (72.9% user preference) and noise blending approaches

## Confidence
- **High Confidence:** Two-stage training strategy (camera pretraining followed by identity customization) and general framework architecture are well-supported by experimental results and ablation studies
- **Medium Confidence:** Claims about superiority of multi-view training data over frontal-only approaches are supported by ablation studies but could benefit from more extensive testing
- **Low Confidence:** Claims about real-life video adaptation effectiveness and practical utility of noise blending for complex multi-subject interactions lack quantitative validation

## Next Checks
1. **4DGS Quality Impact Study:** Systematically vary the quality of 4D Gaussian Splatting reconstructions and measure the corresponding impact on final video generation quality, particularly identity preservation scores and camera control accuracy.

2. **Cross-Dataset Generalization Test:** Evaluate the method's performance when trained on professional volumetric capture data but tested on consumer-grade multi-view video input with fewer cameras and uncontrolled lighting conditions, measuring degradation in AdaFace and camera control metrics.

3. **Hybrid Multi-Subject Training Evaluation:** Implement and evaluate a hybrid approach that uses joint training for subjects expected to interact closely while employing noise blending for subjects in separate spatial regions, comparing against pure joint training and pure noise blending baselines.