---
ver: rpa2
title: Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric
  LLM
arxiv_id: '2502.06635'
source_url: https://arxiv.org/abs/2502.06635
tags:
- data
- zhang
- steel-llm
- training
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Steel-LLM is a 1-billion-parameter Chinese-centric language model
  developed from scratch with limited computational resources (8 GPUs), demonstrating
  that high-quality LLMs can be built efficiently while maintaining full transparency.
  The model employs innovative techniques including Soft Mixture of Experts (Soft
  MoE) and an enhanced Feed-Forward Network to optimize performance within resource
  constraints.
---

# Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM

## Quick Facts
- arXiv ID: 2502.06635
- Source URL: https://arxiv.org/abs/2502.06635
- Reference count: 40
- Key outcome: 1-billion-parameter Chinese-centric LLM trained from scratch on 8 GPUs, achieving 41.90% on CEVAL and 36.08% on CMMLU

## Executive Summary
Steel-LLM is a 1-billion-parameter Chinese-centric language model developed from scratch with limited computational resources (8 GPUs), demonstrating that high-quality LLMs can be built efficiently while maintaining full transparency. The model employs innovative techniques including Soft Mixture of Experts (Soft MoE) and an enhanced Feed-Forward Network to optimize performance within resource constraints. Steel-LLM achieved competitive results on Chinese benchmarks, outperforming some early models from larger institutions. The complete training pipeline, datasets, and intermediate checkpoints were released, providing practical guidance for small-scale LLM development.

## Method Summary
Steel-LLM was trained from scratch using a combination of innovative architectural choices and efficient training techniques. The model uses Soft Mixture of Experts (Soft MoE) instead of traditional sparse MoE to enable full parameter training on limited GPU memory. An enhanced Feed-Forward Network with SwiGLU activation on both MLP layers was implemented. The training pipeline incorporated operator fusion (FlashAttention, CUDA-based RoPE, Triton-based cross-entropy loss) and mixed-precision (BF16) to achieve approximately 50% speed improvement while reducing memory usage by 10-14GB on A100 GPUs. The model was pretrained on 1 trillion tokens for 1.07 million steps, followed by supervised fine-tuning with ~700K Chinese entries and English datasets, and human preference alignment using Direct Preference Optimization (DPO).

## Key Results
- Achieved 41.90% on CEVAL and 36.08% on CMMLU Chinese benchmarks
- Maintained competitive performance on English MMLU (35.15%) despite Chinese focus
- Demonstrated effective multilingual capabilities through balanced data composition (80% Chinese, 20% English)
- Successfully trained from scratch with limited resources (8 GPUs, 1B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Soft Mixture of Experts (Soft MoE) for Resource-Constrained Training
Soft MoE replaces sparse MoE with fully differentiable expert selection using learnable dispatch/combine weight matrices. This enables effective parameter training on limited GPU memory by allowing dense gradient flow through all experts, avoiding discrete routing decisions that prevent full parameter updates in sparse MoE under memory constraints.

### Mechanism 2: Data Distribution Alignment Between Pretraining and Fine-Tuning
Maintaining consistent language ratios (80% Chinese / 20% English) across pretraining and fine-tuning preserves multilingual capabilities while improving target-language performance. The model's latent knowledge representations become anchored to specific language distribution patterns, and aligned distributions reinforce existing representations without destabilizing them.

### Mechanism 3: Operator Fusion and Mixed-Precision for Small-Scale Training Efficiency
Combining FlashAttention, CUDA-based RoPE, Triton-based cross-entropy loss, and BF16 mixed-precision yields ~50% training speed improvement while reducing memory by 10-14GB on A100 GPUs. Fused operators reduce intermediate activation storage and memory access overhead, while FSDP distributes parameter shards across GPUs enabling larger effective batch sizes.

## Foundational Learning

- **Concept: Soft Mixture of Experts (Soft MoE)**
  - Why needed here: Steel-LLM uses Soft MoE as its core FFN replacement. Understanding how dispatch/combine weights work through differentiable softmax is essential to debug training issues.
  - Quick check question: Given input tokens X and slot parameters Φ, can you compute the dispatch weights D and explain why they sum to 1 per column?

- **Concept: Rotary Position Embedding (RoPE)**
  - Why needed here: Steel-LLM adopts RoPE with FP32 precision locally for numerical stability, even though global training uses BF16. This mixed-precision choice affects implementation.
  - Quick check question: Why does RoPE encode relative position information rather than absolute position, and why might FP32 precision be necessary for the rotation matrix?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Steel-LLM uses DPO (not RLHF) for human preference alignment. DPO bypasses reward model training by directly optimizing on preference pairs.
  - Quick check question: In DPO, what is the role of the reference model, and why must both the reference and policy models be initialized from the same SFT checkpoint?

## Architecture Onboarding

- **Component map:** Input → Tokenizer (Qwen1.5 BPE, vocab 151,936) → Transformer Block × 18 layers → Output
- **Critical path:** 1. Data preprocessing (Data-Juicer with 21 text operators, 13 code operators) → 2. Tokenization and packing (max sequence 2048) → 3. Pretraining (1T tokens, 1.07M steps, AdamW, cosine LR with 2000 warmup) → 4. SFT (~700K Chinese entries + English datasets, 4 epochs) → 5. DPO (4:1 Chinese:English preference data, 3 epochs)
- **Design tradeoffs:** Chose Soft MoE to ensure all parameters receive gradients, trading FLOP efficiency for complete training at small scale. 1B parameters were constrained by resource limitations, compensated with architectural innovations. 20% English in fine-tuning balances multilingual capability preservation vs Chinese benchmark optimization.
- **Failure signatures:** If loss plateaus early in SFT: Check data distribution alignment with pretraining. If GPU OOM on 8×A100: Verify FSDP is enabled. If expert imbalance warnings appear: Should NOT happen with Soft MoE; if they do, dispatch weight computation may be incorrect.
- **First 3 experiments:** 1. Reproduce baseline pretraining (100B tokens) with checkpoint saving at 10B intervals; validate loss curve matches Appendix E shape. 2. Ablate Soft MoE: Replace with standard FFN (no experts) on same data; compare CEVAL/CMMLU scores to isolate Soft MoE contribution. 3. SFT data distribution sensitivity: Train three SFT variants (100% Chinese, 80/20, 50/50) and evaluate on both CEVAL and MMLU to validate distribution alignment hypothesis.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited benchmark scope with no comparisons against contemporary small-scale models like Llama-1B, MPT-7B, or Gemma-2B
- Soft MoE efficiency trade-offs lack direct comparison against sparse MoE or standard FFN baselines
- Data quality and distribution alignment claims rely heavily on single experimental comparisons without ablation studies

## Confidence

**High Confidence (Strong evidence, well-supported claims):**
- Soft MoE implementation details and training methodology
- Pretraining pipeline execution and checkpoint availability
- Basic architectural specifications (18 layers, 32 attention heads, 6 experts per MoE)

**Medium Confidence (Reasonable evidence but limited validation):**
- Data distribution alignment benefits (supported by single experiment)
- Resource efficiency gains from operator fusion (hardware-specific claims)
- Competitive benchmark performance (limited scope, no contemporary comparisons)

**Low Confidence (Weak evidence or major assumptions):**
- Soft MoE superiority over sparse MoE (no direct comparison)
- Optimal 80/20 language ratio for multilingual capability (single data point)
- Quality improvements from enhanced FFN (SwiGLU on both layers) without ablation

## Next Checks
1. **Soft MoE Ablation Study**: Implement and train Steel-LLM with standard FFN instead of Soft MoE using identical data and hyperparameters. Compare CEVAL/CMMLU performance to quantify the actual contribution of Soft MoE architecture versus other factors like data quality and training duration.

2. **Contemporary Benchmark Comparison**: Evaluate Steel-LLM against established small-scale models (Llama-1B, MPT-7B, Gemma-2B) on standard benchmarks including both Chinese and English datasets. This will establish whether the claimed competitive performance is maintained against models with more extensive validation.

3. **Data Distribution Sensitivity Analysis**: Conduct a systematic study varying the Chinese/English ratio in SFT (0%, 20%, 40%, 60%, 80%, 100%) while keeping pretraining distribution constant. Measure performance across CEVAL, CMMLU, and MMLU to determine the optimal ratio and validate whether 80/20 is genuinely optimal or merely sufficient.