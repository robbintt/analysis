---
ver: rpa2
title: Performance Evaluation of Large Language Models in Bangla Consumer Health Query
  Summarization
arxiv_id: '2505.05070'
source_url: https://arxiv.org/abs/2505.05070
tags:
- bangla
- llms
- arxiv
- summarization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the zero-shot performance of nine large language
  models (LLMs) for summarizing Bangla consumer health queries, a low-resource language
  task. Using the BanglaCHQ-Summ dataset of 2,350 annotated query-summary pairs, the
  models were assessed using ROUGE metrics and compared against a fine-tuned Bangla
  T5 model.
---

# Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization

## Quick Facts
- **arXiv ID**: 2505.05070
- **Source URL**: https://arxiv.org/abs/2505.05070
- **Reference count**: 36
- **Primary result**: Mixtral-8x22b-Instruct achieved highest ROUGE-1 (51.36) and ROUGE-L (49.17) scores in zero-shot Bangla health query summarization, rivaling fine-tuned Bangla T5

## Executive Summary
This study evaluates nine large language models for zero-shot Bangla consumer health query summarization, a low-resource language task. Using the novel BanglaCHQ-Summ dataset of 2,350 annotated query-summary pairs, the models were assessed using ROUGE metrics and compared against a fine-tuned Bangla T5 model. Mixtral-8x22b-Instruct emerged as the top performer in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2. The results demonstrate that zero-shot LLMs can rival fine-tuned models in producing high-quality summaries, even without task-specific training, highlighting their potential for addressing challenges in low-resource languages.

## Method Summary
The study evaluated nine LLMs using zero-shot prompting on the BanglaCHQ-Summ dataset containing 2,350 query-summary pairs. Models were assessed using ROUGE-1, ROUGE-2, and ROUGE-L metrics. A fine-tuned Bangla T5 model served as the baseline for comparison. The evaluation focused on extractive summarization quality without task-specific training for the zero-shot models.

## Key Results
- Mixtral-8x22b-Instruct achieved highest ROUGE-1 score of 51.36
- Mixtral-8x22b-Instruct achieved highest ROUGE-L score of 49.17
- Bangla T5 achieved highest ROUGE-2 score of 29.11 in fine-tuned baseline

## Why This Works (Mechanism)
The study demonstrates that zero-shot LLMs can effectively perform Bangla health query summarization without task-specific training, leveraging their multilingual pretraining and instruction-following capabilities. The models' ability to understand context and generate coherent summaries from consumer health queries indicates strong cross-lingual transfer learning capabilities.

## Foundational Learning
- **ROUGE metrics**: Why needed - standard evaluation for text summarization; Quick check - measure n-gram overlap between generated and reference summaries
- **Zero-shot learning**: Why needed - evaluate model capabilities without task-specific training; Quick check - assess performance using only prompt engineering
- **Fine-tuning**: Why needed - establish baseline performance with task-specific training; Quick check - compare against zero-shot approaches
- **Low-resource language processing**: Why needed - address challenges in languages with limited training data; Quick check - evaluate performance on Bangla, a less commonly supported language
- **Consumer health query understanding**: Why needed - process real-world health-related questions; Quick check - assess ability to handle medical terminology and context

## Architecture Onboarding
**Component map**: LLMs (Mixtral, Llama, etc.) -> Prompt processing -> Summary generation -> ROUGE evaluation -> Performance comparison
**Critical path**: Input query → LLM processing → Summary output → ROUGE metric calculation → Performance ranking
**Design tradeoffs**: Zero-shot vs. fine-tuned approaches - flexibility vs. task-specific optimization; Model size vs. performance - larger models show better results but higher computational cost
**Failure signatures**: Poor ROUGE scores indicate inability to capture key information; Large variance between ROUGE metrics suggests inconsistent performance across evaluation criteria
**3 first experiments**: 1) Test zero-shot performance on subset of queries to establish baseline; 2) Compare ROUGE scores across different prompt formulations; 3) Evaluate summary coherence through human assessment

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies solely on ROUGE metrics, which may not capture semantic quality or clinical accuracy
- BanglaCHQ-Summ dataset contains only 2,350 samples, limiting robustness of comparisons
- Study focuses exclusively on zero-shot and single fine-tuned model comparisons without exploring few-shot learning approaches

## Confidence
- **High**: Relative performance rankings of tested models on BanglaCHQ-Summ dataset using ROUGE metrics
- **Medium**: Claims about zero-shot LLMs rivaling fine-tuned models given single fine-tuned baseline
- **Low**: Broader claims about clinical utility or real-world applicability without human evaluation

## Next Checks
1. Conduct human evaluation studies with medical professionals to assess summary accuracy, completeness, and clinical relevance beyond automated metrics
2. Expand dataset size and diversity through additional annotations or synthetic data generation to strengthen statistical significance
3. Compare zero-shot performance against multiple fine-tuning approaches, including parameter-efficient fine-tuning and domain-specific pretraining