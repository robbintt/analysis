---
ver: rpa2
title: On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual
  Learning
arxiv_id: '2510.09181'
source_url: https://arxiv.org/abs/2510.09181
tags:
- alignment
- forgetting
- have
- adversarial
- erank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper uncovers a hidden adversarial mechanism in deep continual
  learning: new-task training implicitly attacks old-task knowledge by aligning updates
  with high-curvature directions in the old-task loss landscape. This alignment, termed
  "adversarial alignment," is counter-intuitive because it persists even when tasks
  have different data and high-curvature directions are sparse.'
---

# On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual Learning

## Quick Facts
- arXiv ID: 2510.09181
- Source URL: https://arxiv.org/abs/2510.09181
- Reference count: 40
- Key outcome: New-task training implicitly attacks old-task knowledge by aligning updates with high-curvature directions in the old-task loss landscape, causing catastrophic forgetting even in flat minima.

## Executive Summary
This paper reveals that catastrophic forgetting in deep continual learning arises from an implicit adversarial mechanism: new-task training gradients align with the most vulnerable (high-curvature) directions of the old-task loss landscape. This "adversarial alignment" persists even in flat minima due to a low-rank bias in deep network weights that confines both old-task and new-task gradients to the same low-dimensional subspace. The paper introduces backGP, which mitigates forgetting by projecting new-task gradients onto the nullspace of output gradients, reducing forgetting by 10.8% and improving accuracy by 12.7% over standard methods.

## Method Summary
The paper proposes backGP, a gradient projection method that extends standard GP by constraining both forward and backward propagation. While standard GP projects gradients onto the nullspace of input covariance, backGP additionally projects onto the nullspace of gradient covariance w.r.t. outputs. This dual projection neutralizes the residual alignment from backward propagation that standard GP leaves intact. The method requires computing gradient covariance matrices and their eigen-decompositions, using spectral regularization to maintain plasticity while enforcing orthogonality to old-task high-curvature directions.

## Key Results
- Adversarial alignment: New-task updates have disproportionately large projections onto the top 0.06% of old-task high-curvature directions
- Low-rank bias: Deep networks exhibit 10-100x more alignment than shallow networks due to Jacobian low-rank confinement
- backGP effectiveness: Reduces forgetting by 10.8% and improves accuracy by 12.7% over standard GP methods
- Backward alignment: Standard GP leaves residual alignment intact because gradients are also shaped by backward propagation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Catastrophic forgetting acts as an implicit adversarial attack because new-task training gradients align with the most vulnerable directions (high-curvature eigenvectors) of the old-task loss landscape.
- **Mechanism:** New-task updates preferentially project onto the "sharp" directions of the old-task Hessian, causing rapid increase in old-task loss compared to random perturbations.
- **Core assumption:** Old-task loss landscape has sparse high-curvature directions, making such precise alignment statistically unlikely by chance.
- **Evidence anchors:** Abstract defines "adversarial alignment," Section 1.2 shows disproportionate projections onto top 0.06% of eigenvectors, Figure 2 plots CDF showing actual vs. random updates.
- **Break condition:** If new-task updates were random or the old-task landscape were isotropic, amplification factor would approach 1 and forgetting would be linear.

### Mechanism 2
- **Claim:** Adversarial alignment is caused by "Low-Rank Bias" in model weights, confining both new-task gradients and old-task curvature directions to the same low-dimensional subspace.
- **Mechanism:** Training induces low-rank weight matrices, generating low-rank Jacobians that act as a bottleneck, projecting both new-task gradients and old-task Hessian eigenvectors into the same column space.
- **Core assumption:** Depth intensifies this effect; single-layer networks should exhibit significantly less alignment than deep networks.
- **Evidence anchors:** Section 1.4.2 shows alignment inversely proportional to Jacobian effective rank, Section 1.4.3 analyzes Jacobian as "common low-rank factor," Figure 4 shows phase transition in alignment when depth increases from 1 to 2.
- **Break condition:** If regularization forces weights to be high-rank, the common subspace expands to full rank, preventing forced alignment.

### Mechanism 3
- **Claim:** Existing Gradient Projection methods fail to fully mitigate forgetting because they only constrain the "forward" pass, leaving "backward" adversarial alignment intact.
- **Mechanism:** Standard GP projects new-task gradients onto the nullspace of old-task input covariance (forward pass), but gradients are also shaped by backward pass (transposed weights). backGP projects onto the nullspace of the gradient w.r.t. the output.
- **Core assumption:** Gradient covariance w.r.t. hidden outputs is computationally tractable and represents a valid constraint.
- **Evidence anchors:** Section 3.5 analyzes why forward GP leaves "residual adversariality," Section 1.5 proposes backGP, Table 1 shows backGP reduces forgetting significantly.
- **Break condition:** If gradient w.r.t. outputs is full-rank, nullspace is empty, and backGP cannot project updates, stalling training.

## Foundational Learning

- **Concept:** Hessian Eigenvalues and Loss Landscape Curvature
  - **Why needed here:** The core hypothesis relies on "high-curvature directions" being the target of the implicit attack. You must understand that eigenvalues of the Hessian represent the steepness of the loss landscape.
  - **Quick check question:** If a model resides in a "flat minimum" (low curvature), does the alignment mechanism strengthen or weaken? (Hint: The paper argues alignment persists even in flat minima because of the subspace confinement).

- **Concept:** Nullspace Projection
  - **Why needed here:** The solution (backGP) relies on projecting gradients into the nullspace of specific matrices. Understanding that projection ensures the update has zero component along the protected directions.
  - **Quick check question:** What happens to the learning update if the nullspace is dimension 0?

- **Concept:** Effective Rank (erank)
  - **Why needed here:** The paper uses "effective rank" to quantify the non-integer rank of a matrix with decaying singular values. This explains how a technically full-rank matrix can act low-rank in terms of alignment dynamics.
  - **Quick check question:** How does erank(A) behave as the singular values of A become more concentrated (low entropy)?

## Architecture Onboarding

- **Component map:** Continual Learner -> Alignment Monitor -> Projection Module -> Weight Update
- **Critical path:**
  1. Task 1 Training: Standard training. Store input activations and output gradients to build covariance matrices.
  2. Task N Training:
     - Compute forward projection F_i (standard GP)
     - Compute backward projection B_i (nullspace of output gradients)
     - Update weights using G_final = B_i · G_raw · F_i
  3. Evaluation: Track Forgetting (BWT) and Accuracy (ACC), specifically looking for drop in "residual alignment" α.

- **Design tradeoffs:**
  - Spectral Regularization vs. Alignment: The paper uses spectral regularization to maintain plasticity (high rank), but this potentially fights the "natural" low-rank convergence that causes alignment. backGP is required to enforce orthogonality within the high-rank space.
  - Plasticity vs. Stability: Aggressive projection (low epsilon_backGP) maximizes stability (zero forgetting) but kills plasticity. The system requires tuning thresholds (e.g., epsilon=0.2 to 0.4) to allow some learning.

- **Failure signatures:**
  - Residual Forgetting: Standard GP reduces alignment α from 10^3 to 10^2, but forgetting remains high. This indicates backward alignment is dominant.
  - Zero Plasticity: If ACC drops or immediate accuracy (immACC) is low, the nullspace projection is too strict (threshold too high).
  - Rank Collapse: If spectral regularization fails, weights become low-rank, alignment spikes, and backGP struggles to find an orthogonal direction.

- **First 3 experiments:**
  1. Verification of Adversarial Alignment: Train ResNet on CIFAR-100 (Task 1) and disjoint split (Task 2). Compute projection of Task 2 updates onto top eigenvectors of Task 1 Hessian. Plot CDF to confirm spike in alignment compared to random noise.
  2. BackGP Implementation Check: Implement backGP on simple Deep Linear Network (DLN) with synthetic data. Verify J^T J_GP ≈ 0 (orthogonality between old-task Hessian directions and new projected updates).
  3. Ablation on Projection Directions: Compare three settings on 10-Split CIFAR100: (A) No Protection, (B) Forward GP only, (C) Forward + Backward GP. Report BWT (Forgetting) and ACC to isolate contribution of "backward" mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does adversarial alignment arise in non-linear deep networks?
- Basis in paper: "We only theoretically study deep linear networks for technical tractability. How adversarial alignment arises in non-linear networks remains an open question." (p. 12)
- Why unresolved: Theoretical analysis relies on constant Jacobians in linear networks; non-linear activations introduce input-dependence and sparsity which may hinder low-rank bias propagation.
- Evidence to resolve: Theoretical derivation of alignment α for ReLU networks or empirical validation of alignment strength in non-linear architectures without linear assumptions.

### Open Question 2
- Question: Do later training steps exhibit "implicit power iteration" that strengthens alignment?
- Basis in paper: "Our theoretical result only addresses the first step... We conjecture the later dynamics involve... an implicit power iteration... akin to the generation of adversarial samples." (p. 12)
- Why unresolved: Current analysis is restricted to the first step of new-task training for mathematical tractability.
- Evidence to resolve: Analysis of multi-step training dynamics showing increasing alignment with top Hessian eigenvectors over time, rather than just initial steps.

### Open Question 3
- Question: Does the low-rank bias induce alignment with higher-order derivatives?
- Basis in paper: "We only study adversarial alignment in the second order... We conjecture that the low-rank bias will similarly induce low-rank Jacobians and pull the sharp directions of old-task higher-order derivatives..." (p. 12)
- Why unresolved: The decomposition of forgetting relies on second-order approximation, but higher-order terms contribute to actual forgetting.
- Evidence to resolve: Empirical measurement of alignment between new-task gradients and sharp directions of 3rd/4th order derivatives of old-task loss.

### Open Question 4
- Question: Does this mechanism explain expressiveness redistribution in pretraining-finetuning?
- Basis in paper: "This conjectures a preliminary model on how pretraining helps downstream-task finetuning... i.e., increasing expressiveness... along the few directions important... to the pretraining task." (p. 12)
- Why unresolved: The paper focuses on sequential distinct tasks; application to transfer learning paradigm is currently a qualitative conjecture.
- Evidence to resolve: Experiments verifying that finetuning efficiency correlates with redistribution of gradient norms toward pretraining high-curvature directions.

## Limitations

- The low-rank bias mechanism depends critically on specific weight initialization and training regimes; it's unclear whether this phenomenon persists under different architectures or training paradigms.
- The spectral regularization coefficient (3e-2 for first task) appears crucial for balancing plasticity vs. stability, but the sensitivity to this hyperparameter is not thoroughly explored.
- The computational cost of eigen-decomposition for the backward projection may scale poorly with network width and depth.

## Confidence

- **High:** The empirical observation of alignment between new-task updates and old-task high-curvature directions is well-supported by CDF plots and alignment metrics. The effectiveness of backGP in reducing forgetting is clearly demonstrated across multiple datasets.
- **Medium:** The theoretical link between low-rank bias and adversarial alignment is plausible but relies on several assumptions about Hessian structure and Jacobian properties that are difficult to verify in practice.
- **Low:** The broader claim about implications for pretraining-finetuning dynamics in foundation models is speculative and not directly tested in this paper.

## Next Checks

1. Verify the adversarial alignment mechanism on a transformer architecture (e.g., ViT) to test architecture dependence.
2. Conduct a systematic ablation study on the spectral regularization coefficient to determine its impact on the trade-off between plasticity and forgetting.
3. Measure the computational overhead of backGP (eigen-decomposition cost) and explore approximation methods for large-scale models.