---
ver: rpa2
title: 'JEDA: Query-Free Clinical Order Search from Ambient Dialogues'
arxiv_id: '2510.14169'
source_url: https://arxiv.org/abs/2510.14169
tags:
- order
- jeda
- query
- retrieval
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JEDA is a domain-initialized bi-encoder that retrieves clinical
  orders directly from ambient dialogue without LLM query rewriting. It uses a duplicate-safe
  contrastive objective to align heterogeneous conversational expressions (command-only,
  context-only, command+context, context+reasoning) to canonical orders, and supports
  a query-free mode that encodes a short rolling window of speech.
---

# JEDA: Query-Free Clinical Order Search from Ambient Dialogues

## Quick Facts
- **arXiv ID**: 2510.14169
- **Source URL**: https://arxiv.org/abs/2510.14169
- **Reference count**: 40
- **Primary result**: Recall@1 of 0.28 (vs 0.08 baseline) and MRR@10 of 0.44 (vs 0.15 baseline) on clinical order retrieval from ambient dialogue

## Executive Summary
JEDA is a domain-initialized bi-encoder that retrieves clinical orders directly from ambient dialogue without LLM query rewriting. It uses a duplicate-safe contrastive objective to align heterogeneous conversational expressions (command-only, context-only, command+context, context+reasoning) to canonical orders, and supports a query-free mode that encodes a short rolling window of speech. Initialized from PubMedBERT, it achieves strong retrieval performance while improving robustness to conversational disfluencies and ASR errors.

## Method Summary
JEDA uses a tied PubMedBERT bi-encoder trained with duplicate-safe Multiple Negatives Ranking loss. The training data consists of de-identified clinical encounters where signed orders are mapped to four query variants via constrained LLM guidance. The duplicate-safe masking prevents semantically equivalent queries from being treated as negatives. At inference, the model can operate in query-free mode by encoding short rolling windows of ambient dialogue and retrieving matching orders via cosine similarity against precomputed order embeddings.

## Key Results
- R@1 of 0.28 vs 0.08 baseline on held-out test set
- MRR@10 of 0.44 vs 0.15 baseline
- Even stronger performance in encounter-scoped retrieval
- Larger inter-order margins and tighter query-order coupling than baseline models

## Why This Works (Mechanism)

### Mechanism 1
Training on multiple linguistic formulations of the same intent (command-only, context-only, command+context, context+reasoning) creates a unified embedding space that generalizes across direct and ambient expressions. Each query variant provides a distinct supervisory signal—intent isolation, evidence grounding, their combination, and causal linkage—forcing the encoder to learn invariances tied to clinical logic rather than surface form.

### Mechanism 2
Masking duplicate positives within each minibatch prevents semantically equivalent queries from becoming false negatives, improving embedding geometry. Standard in-batch contrastive treats all non-target examples as negatives, but when multiple queries target the same order, this creates false negatives that push apart representations that should converge.

### Mechanism 3
Encoding a short rolling window of ambient dialogue provides noise resilience to ASR errors and conversational disfluencies. A window aggregates multiple turns, so local errors average out. The encoder, trained on context-only variants, learns to extract signal from noisy spans without requiring explicit query formulation.

## Foundational Learning

- **Bi-encoder architecture**: Separate query and document towers with tied weights and cosine similarity scoring. Why needed: JEDA is a bi-encoder; understanding this architecture is prerequisite. Quick check: Given query embedding q ∈ R^d and document embedding d ∈ R^d, what is the retrieval score and how does it differ from cross-encoder attention?
- **Multiple Negatives Ranking loss**: In-batch negatives and implicit negative sampling. Why needed: The core training objective uses MNR; without this, duplicate-safe masking rationale will be unclear. Quick check: In a batch of N (query, positive) pairs, how many negative samples does MNR implicitly provide for each query?
- **Domain-specific pretraining**: PubMedBERT initialization. Why needed: The paper attributes strong performance partly to biomedical initialization; understanding what PubMedBERT provides contextualizes gains. Quick check: What corpus is PubMedBERT pretrained on, and why might this help bridge "chest X-ray" and "imaging for pneumonia suspicion"?

## Architecture Onboarding

### Component map
1. **Training data pipeline**: Encounter transcripts + signed orders → LLM-guided record construction → 4 query variants per order with confidence scores
2. **Encoder**: Single PubMedBERT transformer (110M params), tied weights for query and order towers, L2-normalized output embeddings
3. **Training objective**: Duplicate-safe MNR loss with temperature τ=0.05 (scale=20), batch size 64, 5 epochs
4. **Inference**: Encode query/window → cosine similarity against precomputed order embeddings → top-K retrieval; optional encounter-scoped filtering

### Critical path
1. Verify order ontology canonicalization (SNOMED/LOINC/RxNorm grounding)—errors here poison all downstream metrics
2. Inspect LLM-generated training records for variant quality and confidence distribution; filter low-confidence pairs if needed
3. Confirm duplicate-safe masking is correctly implemented (M_ij = 0 for same-order pairs in batch)
4. Monitor embedding geometry during training: margin_pos_frac and Fisher ratio should increase; compactness should not collapse

### Design tradeoffs
- **Temperature**: s=10 (τ=0.1) favors recall and robustness; s=20 (τ=0.05) maximizes inter-order separation but may over-harden negatives. Paper recommends s=10 for recall, s=20 when downstream reranking exists.
- **Variant mix**: Full mix (4 variants) is strongest; if constrained, context+reasoning alone transfers best to ambient inputs.
- **Window size**: Not ablated; Assumption: deployment should tune W/N based on encounter density and ASR error rates.

### Failure signatures
1. **Low R@1 but high R@20**: Correct order in candidates but outranked by near-neighbors (same-family confusions); consider ontology-aware regularizers or reranking
2. **Context-only lagging other variants**: Sparse ambient cues or missing intent-bearing turns; may need longer windows or multi-hop context
3. **Geometric metrics stagnant**: Duplicate-safe masking may not be active; check batch composition for order diversity

### First 3 experiments
1. **Baseline replication**: Train PubMedBERT with standard MNR (no duplicate masking) on the same data; compare R@1 and margin_pos_frac to quantify masking contribution
2. **Variant ablation**: Train four single-variant models; evaluate on mixed test set to confirm context+reasoning transfers best and full mix is optimal
3. **Encounter-scoped stress test**: Apply encounter-level filtering to held-out encounters; measure strict vs. filtered R@1 gap to diagnose candidate pool incompleteness

## Open Questions the Paper Calls Out

### Open Question 1
Can ontology-aware regularizers or lightweight modular re-rankers specifically address the residual "same-family" confusion errors—where correct orders are outranked by clinically proximate alternatives—while preserving the system's real-time latency profile? Basis: The Discussion identifies "same-family confusions" as a residual error mode and proposes "ontology aware regularizers" and "modular re ranking" as future work.

### Open Question 2
Does the observed increase in retrieval accuracy and embedding separation translate into measurable downstream clinical benefits, such as reduced documentation time or lower error rates? Basis: Section 6.10 states, "Our evaluation focuses on retrieval and embedding geometry, not downstream clinical impact such as time saved or error reduction."

### Open Question 3
To what extent does the query-free embedding space generalize to multi-institutional settings with diverse nomenclature and distinct order catalogs? Basis: Section 7 lists "extending this formulation to multi institutional data" as a primary future direction.

## Limitations
- Evaluation focuses on retrieval and embedding geometry, not downstream clinical impact
- Proprietary clinical encounter data prevents independent replication
- Optimal window length and logit scale not ablated
- Same-family confusions remain a residual error mode

## Confidence

- **High**: Bi-encoder architecture, duplicate-safe masking mechanism, recall/MRR improvements over baseline
- **Medium**: Qualitative claims about inter-order margins and embedding geometry; generalization of variant mix benefits
- **Low**: Windowing robustness to ASR errors (not directly evaluated); optimal logit scale and variant mix for non-biomedical domains

## Next Checks

1. **Duplicate-safe masking isolation**: Train a baseline model with standard MNR loss (no masking) on the same data; measure relative changes in R@1 and geometric metrics to quantify masking contribution.

2. **Variant ablation on ambient queries**: Train four single-variant models (command-only, context-only, command+context, context+reasoning); evaluate each on context-only test queries to confirm which variant transfers best.

3. **Encounter-scoped completeness test**: Apply encounter-level filtering to held-out encounters; measure strict vs. filtered R@1 gap to quantify candidate pool incompleteness and diagnose retrieval errors.