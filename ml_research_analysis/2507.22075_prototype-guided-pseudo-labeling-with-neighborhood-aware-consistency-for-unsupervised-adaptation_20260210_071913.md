---
ver: rpa2
title: Prototype-Guided Pseudo-Labeling with Neighborhood-Aware Consistency for Unsupervised
  Adaptation
arxiv_id: '2507.22075'
source_url: https://arxiv.org/abs/2507.22075
tags:
- pseudo-labels
- clip
- class
- accuracy
- alpha
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ALPHA, a framework to enhance unsupervised
  CLIP adaptation by addressing the challenge of noisy pseudo-labels in fully unsupervised
  settings. ALPHA integrates two components: PICS, which assesses pseudo-label accuracy
  via in-class compactness and cross-class separation using prototype-based scoring,
  and NALR, which refines noisy pseudo-labels by exploiting neighborhood consistency
  through semantic alignment with LLM-generated text descriptions.'
---

# Prototype-Guided Pseudo-Labeling with Neighborhood-Aware Consistency for Unsupervised Adaptation

## Quick Facts
- **arXiv ID:** 2507.22075
- **Source URL:** https://arxiv.org/abs/2507.22075
- **Reference count:** 40
- **Primary result:** ALPHA achieves SOTA UDA performance with average accuracy improvements of +10.07% over zero-shot CLIP and +1.42% over DPA across 11 datasets.

## Executive Summary
This paper introduces ALPHA, a framework to enhance unsupervised CLIP adaptation by addressing the challenge of noisy pseudo-labels in fully unsupervised settings. ALPHA integrates two components: PICS, which assesses pseudo-label accuracy via in-class compactness and cross-class separation using prototype-based scoring, and NALR, which refines noisy pseudo-labels by exploiting neighborhood consistency through semantic alignment with LLM-generated text descriptions. The method also employs an adaptive weighting mechanism to dynamically modulate sample influence based on local semantic coherence. Extensive experiments on 11 diverse datasets demonstrate that ALPHA achieves state-of-the-art performance, with average accuracy improvements of +10.07% over zero-shot CLIP and +7.85% over CuPL, and +1.42% over DPA. Results highlight robust generalization, computational efficiency, and resilience to noisy data.

## Method Summary
ALPHA is a prototype-guided pseudo-labeling framework for unsupervised domain adaptation of CLIP. It operates by first generating pseudo-labels through zero-shot inference, then iteratively refining them using two key components: PICS (Prototype-Guided Inlier Score) for filtering clean samples based on compactness and separation metrics, and NALR (Neighborhood-Aware Label Refinement) for refining noisy samples using semantic consistency from LLM-generated text descriptions. The method fine-tunes only LayerNorm parameters and learnable text prototypes, using an adaptive weighting mechanism to balance clean and refined samples during training. Experiments show significant performance gains over baseline methods across diverse datasets.

## Key Results
- Achieves +10.07% average accuracy improvement over zero-shot CLIP
- Outperforms CuPL by +7.85% and DPA by +1.42% on average
- Demonstrates robust generalization across 11 diverse datasets
- Shows computational efficiency and resilience to noisy data

## Why This Works (Mechanism)
ALPHA works by addressing the fundamental challenge of noisy pseudo-labels in unsupervised adaptation. PICS identifies high-quality pseudo-labels by measuring in-class compactness and cross-class separation using prototype-based scoring, effectively filtering out mislabeled samples. NALR then refines the remaining noisy labels by leveraging semantic consistency from LLM-generated descriptions and neighborhood-aware adaptive weighting. This two-stage approach ensures that only reliable samples contribute to model updates, while noisy samples are corrected through semantic alignment. The adaptive weighting mechanism further enhances robustness by modulating sample influence based on local semantic coherence.

## Foundational Learning
- **Unsupervised Domain Adaptation (UDA):** Adapting models to new domains without labeled data. Why needed: Enables deployment in real-world scenarios where labeled data is scarce. Quick check: Compare performance on source vs. target domains.
- **Prototype-based Scoring:** Using class prototypes to measure sample quality. Why needed: Provides a principled way to assess pseudo-label reliability. Quick check: Monitor prototype convergence during training.
- **Neighborhood-aware Consistency:** Leveraging local semantic coherence for label refinement. Why needed: Exploits inherent structure in feature space to correct noisy labels. Quick check: Validate neighbor selection accuracy.

## Architecture Onboarding
- **Component Map:** CLIP ViT-B/32 -> PICS Filter -> NALR Refinement -> Loss Optimization
- **Critical Path:** Zero-shot inference → Memory Bank update → PICS filtering → NALR refinement → Model optimization
- **Design Tradeoffs:** Fine-tuning only LayerNorm and text prototypes reduces computational cost but may limit adaptation capacity. The choice of $k_n=3$ neighbors balances refinement quality and noise propagation risk.
- **Failure Signatures:** Training collapse on fine-grained datasets (use lower LR), over-filtering due to aggressive PICS thresholds, confirmation bias from noisy NALR neighbors.
- **First Experiments:** (1) Verify clean sample count evolution matches Fig 3 to ensure PICS filtering works as intended. (2) Confirm pseudo-label accuracy trajectory aligns with Fig 4 to validate NALR effectiveness. (3) Test model stability on StanfordCars/Food101 with reduced learning rate ($1e-6$).

## Open Questions the Paper Calls Out
None

## Limitations
- Sensitive to hyperparameter choices (e.g., $k_n$, $\tau$) which may affect performance
- Relies on CuPL-style LLM descriptions without explicit prompt details
- May struggle with highly fine-grained or ambiguous classes where semantic consistency is hard to establish

## Confidence
- **High:** Method is well-defined with clear training procedures and extensive experimental validation
- **Medium:** Some implementation details (LLM prompt structure, strong augmentation parameters) are delegated to prior work
- **Low:** Potential sensitivity to hyperparameter choices and reliance on referenced configurations

## Next Checks
1. Verify clean sample count evolution per epoch matches Fig 3 to ensure PICS filtering is functioning as intended
2. Confirm pseudo-label accuracy trajectory aligns with Fig 4 to validate NALR refinement effectiveness
3. Test model behavior on StanfordCars/Food101 with the specified reduced learning rate to confirm stability