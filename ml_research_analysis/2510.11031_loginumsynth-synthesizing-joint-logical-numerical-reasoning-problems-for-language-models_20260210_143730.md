---
ver: rpa2
title: 'LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for
  Language Models'
arxiv_id: '2510.11031'
source_url: https://arxiv.org/abs/2510.11031
tags:
- fact
- reasoning
- rule
- shot
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LogiNumSynth, a data synthesizer designed to
  generate joint logical-numerical reasoning problems for evaluating language models.
  The synthesizer supports fine-grained control over reasoning world richness, logical
  reasoning depth, and numerical complexity, enabling flexible generation of evaluation
  datasets.
---

# LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models

## Quick Facts
- arXiv ID: 2510.11031
- Source URL: https://arxiv.org/abs/2510.11031
- Reference count: 40
- Most models perform poorly on joint logical-numerical reasoning tasks; only top 2 achieve high accuracy

## Executive Summary
LogiNumSynth is a data synthesizer that generates joint logical-numerical reasoning problems for evaluating and training language models. The system provides fine-grained control over task difficulty through parameters governing world richness, logical reasoning depth, and numerical complexity. Evaluation of 29 models shows significant room for improvement in joint reasoning capabilities, while synthetic data proves effective for training small models on external reasoning benchmarks.

## Method Summary
The synthesizer generates problems through a four-stage pipeline: (1) world element initialization from entity/attribute/relationship pools, (2) reasoning DAG construction backward from query with configurable depth and rule complexity, (3) distraction synthesis, and (4) natural language generation via templates and LLM refinement. Training uses LoRA fine-tuning on small models (Llama3.2-1B, Qwen3-1.7B) with synthetic datasets emphasizing logical or numerical skills. Evaluation employs both answer accuracy and process accuracy metrics, where process accuracy is computed by extracting structured reasoning steps from model outputs and comparing them to gold-standard reasoning DAGs.

## Key Results
- Most evaluated models achieve low accuracy on joint logical-numerical reasoning tasks, with only top 2 models reaching high performance
- Synthetic data effectively improves model performance on external reasoning benchmarks, with Qwen3-1.7B gaining +10.20 on FOLIO and Llama3.2-1B gaining +10.50 on LogiQA
- Process accuracy evaluation reveals that models often fail at intermediate reasoning steps even when final answers are correct

## Why This Works (Mechanism)

### Mechanism 1: Controllable Complexity via Parameterized Synthesis
- Claim: LogiNumSynth enables systematic evaluation and targeted training by exposing fine-grained control over task difficulty dimensions (logical depth, numerical complexity, world richness).
- Mechanism: The synthesizer constructs problems through a four-stage pipeline: (1) world element initialization from pools, (2) reasoning DAG construction backward from query with configurable depth/rule complexity, (3) distraction synthesis, (4) natural language generation via templates + LLM refinement. Complexity is controlled by parameters like reasoning depth range, condition counts per rule, expression type distributions, and numerical operand ranges.
- Core assumption: Evaluation of reasoning capability benefits from decoupling it from world knowledge, and that controllable difficulty gradients expose model limitations more precisely than fixed benchmarks.

### Mechanism 2: Process-Level Diagnosis via Structured Output Comparison
- Claim: Evaluating process accuracy (intermediate reasoning steps) alongside answer accuracy provides more diagnostic insight into reasoning failures than end-to-end metrics alone.
- Mechanism: The synthesizer generates gold-standard reasoning DAGs during construction. During evaluation, models are prompted to output structured reasoning steps. A dual-extraction system (text parser + LLM-based structured extractor) converts model outputs to comparable DAG-like format. Each step is verified for: (1) rule condition satisfaction, (2) correct conclusion derivation. Partial credit is awarded for correct intermediate steps even if final answer is wrong.
- Core assumption: Reasoning processes can be faithfully extracted from natural language outputs and compared to formal gold standards, and that partial correctness in reasoning steps is meaningful for diagnosis.

### Mechanism 3: Cross-Domain Transfer via Targeted Synthetic Supervision
- Claim: Training on synthesized joint logical-numerical problems transfers to improvements on external, out-of-distribution reasoning benchmarks.
- Mechanism: Small models (Llama3.2-1B, Qwen3-1.7B) are fine-tuned on synthetic datasets designed to emphasize either logical (EN-Train) or numerical (EL-Train) skills. The training exposes models to structured multi-step reasoning patterns. Evaluation on diverse external benchmarks (GSM8K, MATH, RuleTaker, FOLIO, RuleArena, etc.) measures transfer.
- Core assumption: Synthetic reasoning patterns learned from artificial worlds generalize to real-world reasoning problems, and that parameter-efficient fine-tuning (LoRA) is sufficient to capture these patterns.

## Foundational Learning

- Concept: **Formal Logic Representations (Facts, Rules, Queries)**
  - Why needed here: The synthesizer operates on structured formal representations before converting to natural language. Understanding predicate-like facts (is(entity, attribute, value)), implication rules (condition → conclusion), and query formulation is essential for configuring and extending the system.
  - Quick check question: Can you explain why placeholder variables (Greek letters) in rule conditions must also appear in the conclusion for logically valid inferences?

- Concept: **Directed Acyclic Graphs (DAGs) for Reasoning**
  - Why needed here: The reasoning DAG constructor builds problems backward from the query, with nodes as facts/rules/intermediate conclusions. Understanding DAG structure is crucial for controlling reasoning depth and analyzing generated problems.
  - Quick check question: In the DAG construction algorithm, why is the synthesis strategy randomized at non-maximum depth (line 21: p←random([0,1])) but constrained at maximum depth?

- Concept: **Numerical Expression Types and Composition**
  - Why needed here: The paper defines four expression types (constant, retrieval, calculation, aggregation) that compose to create numerical complexity. Understanding these is necessary for interpreting rule conclusions and configuring numerical difficulty.
  - Quick check question: Given an aggregation expression max(calculation₁, retrieval₂), what information must be available during forward inference to compute its value?

## Architecture Onboarding

- Component map: World Elements → Query → DAG Construction → Fact/Rule Synthesis → Distraction Addition → Template Application → LLM Refinement → Natural Language Problem
- Critical path: The DAG constructor working backward from query ensures logical consistency, with world element selection and distraction synthesis adding complexity
- Design tradeoffs:
  - **Purity vs. Semantic Grounding**: Random element combinations ensure reasoning-only evaluation but produce semantically unnatural worlds (e.g., "Dave's cold attribute is 4")
  - **Template Diversity vs. Coherence**: 300+ templates provide variety, but LLM refinement may drift from formal meaning if poorly prompted
  - **Expressiveness vs. Complexity**: Current implementation limits to conjunction/implication and four expression types; extending adds capability but increases validation burden
- Failure signatures:
  - **Conflict detection false negatives**: Generated facts/rules that logically conflict but pass validation
  - **Extraction errors**: Process accuracy fails when structured output mode produces malformed JSON or mis-parses reasoning steps
  - **Template-refinement drift**: LLM "polishing" changes key terms (entity names, attribute values) breaking logical consistency
- First 3 experiments:
  1. **Basic synthesis validation**: Generate 100 samples at default configuration; manually inspect 10 for logical consistency and natural language fluency
  2. **Difficulty scaling test**: Run a single mid-sized model (e.g., Qwen3-8B) on EL-EN, HL-EN, EL-HN, HL-HN; verify performance degrades as expected with increasing difficulty
  3. **Process accuracy sanity check**: Evaluate DeepSeek-R1 or GPT-4o on 50 samples; compare process accuracy vs. answer accuracy to confirm process metrics capture partial reasoning failures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can process evaluation methods be improved to handle lengthy, interleaved, or intricately cross-referenced intermediate reasoning steps without introducing errors?
- **Basis in paper:** [explicit] The authors explicitly identify this as an open challenge in their limitations section: "Current process evaluation methods are still imperfect, and a small number of such errors are inevitable, especially when intermediate reasoning steps are lengthy, interleaved, or involve intricate cross-references that disrupt the logical flow."
- **Why unresolved:** The paper's dual-extraction approach (text-based parser + LLM-based structured extractor) cannot fully guarantee correct step-level verification for complex reasoning chains, limiting diagnostic precision.
- **What evidence would resolve it:** A process evaluation methodology achieving near-zero error rates on synthesized tasks with varying reasoning depths and cross-referencing patterns, validated against human-annotated reasoning steps.

### Open Question 2
- **Question:** To what extent does synthetic training on LogiNumSynth data transfer to larger-scale models (≥7B parameters) and general-purpose benchmarks beyond reasoning-specific tasks?
- **Basis in paper:** [explicit] The authors explicitly note as a limitation: "Due to limitations in computational resources, we only experimented with fine-tuning Llama3.2-1B-instruct and Qwen3-1.7B... Extending to a wider range of models, including larger-scale LLMs and diverse architectures, as well as more benchmarks such as standard general-purpose ones, would further validate the generality."
- **Why unresolved:** The training benefits demonstrated for 1.7B models on reasoning benchmarks may not generalize to larger models or broader capability evaluations.
- **What evidence would resolve it:** Fine-tuning results on models ranging from 7B to 70B+ parameters, evaluated on both reasoning benchmarks and general-purpose benchmarks like MMLU or BIG-bench, showing consistent improvements.

### Open Question 3
- **Question:** How can synthesized reasoning problems incorporate meaningful real-world semantic grounding while maintaining purity (decoupling reasoning from background knowledge)?
- **Basis in paper:** [explicit] The authors explicitly note: "The relationships in our synthesized worlds (e.g., 'a visits b') do not carry real-world semantic meaning... deeper semantic grounding remains an open challenge."
- **Why unresolved:** Current random combinations of world elements exclude background knowledge to isolate reasoning capability, but this creates an artificial gap between evaluation and real-world application requirements.
- **What evidence would resolve it:** A synthesis methodology that incorporates semantically meaningful entities and relationships while demonstrably preventing models from relying on pre-training knowledge rather than provided facts/rules, validated through controlled experiments comparing models with different knowledge bases.

## Limitations
- Semantic grounding of synthesized worlds remains weak, with random element combinations producing semantically unnatural scenarios
- Process accuracy evaluation depends on the reliability of structured output extraction, which may introduce measurement errors
- Transfer learning experiments are limited to small models and reasoning-specific benchmarks, leaving generalization to larger models and general tasks uncertain

## Confidence
- **High Confidence**: The mechanism of controllable complexity synthesis via parameterized DAG construction is well-specified and theoretically sound
- **Medium Confidence**: The process accuracy evaluation method is novel and described in detail, but its reliability depends on the robustness of the extraction pipeline
- **Medium Confidence**: The transfer learning results demonstrate improvements, but the magnitude and generality of these gains require further validation across more diverse benchmarks

## Next Checks
1. **Semantic Validity Test**: Manually evaluate 50 synthesized samples for logical consistency and natural language coherence to assess the semantic grounding of synthetic worlds
2. **Process Extraction Robustness**: Compare process accuracy scores obtained from multiple extraction methods (text parser vs. structured output vs. manual evaluation) on the same model outputs to quantify extraction error rates
3. **Transfer Generalization Test**: Evaluate models fine-tuned on synthetic data on a broader set of reasoning benchmarks (including non-mathematical, non-rule-based tasks) to assess the scope and limits of transfer learning