---
ver: rpa2
title: Answer Matching Outperforms Multiple Choice for Language Model Evaluation
arxiv_id: '2507.02856'
source_url: https://arxiv.org/abs/2507.02856
tags:
- answer
- question
- matching
- choice
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a critical flaw in current language model
  evaluation: multiple choice questions allow models to bypass generative reasoning
  by exploiting choice-only shortcuts, which undermines the assessment of their true
  generative capabilities. To address this, the authors propose "answer matching,"
  a generative evaluation method where models generate free-form answers to questions
  and then use another modern language model to determine if the response semantically
  matches a reference answer.'
---

# Answer Matching Outperforms Multiple Choice for Language Model Evaluation

## Quick Facts
- arXiv ID: 2507.02856
- Source URL: https://arxiv.org/abs/2507.02856
- Authors: Nikhil Chandak; Shashwat Goel; Ameya Prabhu; Moritz Hardt; Jonas Geiping
- Reference count: 40
- Key finding: Answer matching with reference answers achieves near-perfect human alignment vs. discriminative shortcuts in multiple choice

## Executive Summary
This paper identifies a fundamental flaw in current language model evaluation: multiple choice questions enable models to bypass generative reasoning through discriminative shortcuts that exploit statistical patterns in answer choices. To address this, the authors propose "answer matching," where models generate free-form answers and a modern language model determines semantic equivalence with a reference answer. Through human grading on MMLU-Pro and GPQA-Diamond, the study demonstrates that answer matching achieves near-perfect agreement with human evaluators using even small models, while both standard multiple choice evaluation and LLM-as-a-judge without references show significantly lower alignment. The approach not only provides more valid evaluations but also reveals more headroom for improvement and changes model rankings compared to multiple choice benchmarks.

## Method Summary
The answer matching framework generates free-form responses from candidate models using only the question text (no choices), then employs a separate matcher model to determine if the response semantically matches a reference answer extracted from the MCQ dataset. The evaluation uses Scott's π (inter-annotator agreement) to measure alignment with human grading. The pipeline involves filtering questions for specificity and unique answers, generating responses, applying the matcher with reference answers, and aggregating scores. The paper also demonstrates discriminative shortcuts by training choice-only classifiers that achieve high accuracy without seeing questions.

## Key Results
- Choice-only classifiers achieve 83% on TruthfulQA-v2, 93% on GoldenSwag, and 51% on MMMU-Pro without question access
- Qwen3-1.7B achieves Scott's π = 0.97 on MATH as a matcher vs. DeepSeek V3 achieving only π = 0.72 as a judge without reference
- R1-Distill-Llama-70B drops from 89.1% (MCQ) to 65.9% (generative) on MMLU-Pro, revealing 20%+ lower actual performance

## Why This Works (Mechanism)

### Mechanism 1
Multiple choice evaluations measure discriminative ability rather than generative capability. Models exploit statistical patterns distinguishing correct answers from distractors—odd one out heuristics, length differences, format irregularities—without understanding the question. Discrimination is easier than verification because the model need only identify which option is correct from a guaranteed-valid set, rather than assessing any arbitrary response.

### Mechanism 2
Providing a reference answer transforms evaluation from verification (determine correctness from scratch) to equivalence checking (determine if response matches reference). Verification requires the judge to possess or construct domain knowledge to assess correctness. Equivalence checking only requires semantic comparison—recognizing paraphrases, handling unit conversions, checking numerical equivalence. This explains why 1.7B-parameter models match human performance as matchers but fail as judges.

### Mechanism 3
MCQ evaluations systematically overestimate model capabilities because they conflate discriminative performance with generative ability. When forced to generate answers without choices, models cannot rely on heuristics and must produce correct knowledge directly. This reveals 20%+ lower actual performance, different model rankings, and substantial headroom before saturation.

## Foundational Learning

- **Discriminative vs. Generative Tasks**: Understanding that selecting from options is fundamentally different (and easier) than generating correct answers underpins the entire argument. *Quick check: Why can a model achieve 83% accuracy on TruthfulQA-v2 without seeing the question?*

- **Inter-Annotator Agreement Metrics (Scott's π)**: The paper uses Scott's π to measure alignment between evaluation methods and human ground truth. Understanding why this metric is necessary for valid comparison. *Quick check: Why is simple percentage agreement insufficient when model accuracy isn't 50%?*

- **Construct Validity in Evaluation Design**: The paper argues MCQ lacks construct validity for measuring generative capabilities. Understanding this psychometric concept helps determine when to apply answer matching vs. MCQ. *Quick check: If an evaluation measures something different than intended, what validity problem is this?*

## Architecture Onboarding

- **Component map**: Candidate Model -> generates free-form response -> Matcher Model -> outputs match/no-match using reference answer -> Human Annotation Layer (optional)

- **Critical path**: 1) Filter questions for specificity and unique answers 2) Generate free-form responses from candidate model 3) Apply matcher with reference answer to score correctness 4) Aggregate scores using Scott's π for alignment comparison

- **Design tradeoffs**: Small vs. large matcher (Qwen3-4B achieves near-human alignment at lower cost than frontier models); filtered vs. full dataset (filtering reduces size by >50% but ensures validity); temperature (use 0.3-0.6 for reproducibility)

- **Failure signatures**: Matcher systematically accepts vague responses (use stricter prompt or larger matcher); rankings change dramatically between matchers (verify with multiple matchers); filtered subset skews toward STEM (stratified sampling or subject-specific analysis needed)

- **First 3 experiments**: 1) Reproduce alignment on MATH using MATH-Verify for ground truth 2) Test choice-only shortcut by training classifier on choices only 3) Compare rankings across methods using 5+ models

## Open Questions the Paper Calls Out
None

## Limitations
- Requires reference answers extracted from existing MCQ datasets, limiting applicability to domains with structured datasets
- Human annotation phase introduces subjectivity that may not generalize across all domains
- Effectiveness depends on assumption that equivalence checking is easier than verification, which may not hold for complex reasoning or proof validation

## Confidence

**High Confidence**: Identification of discriminative shortcuts in MCQ evaluation and demonstration of improved alignment with human judgment using answer matching.

**Medium Confidence**: Claim that answer matching reveals more headroom for model improvement, as this interpretation requires additional validation.

**Low Confidence**: Generalizability of the approach across all domains and question types, as validation focuses on STEM-heavy datasets.

## Next Checks

1. **Domain Generalization Test**: Apply answer matching to non-STEM domains where equivalence checking may be more complex due to subjective or context-dependent answers.

2. **Matcher Model Scaling Analysis**: Systematically evaluate how matcher performance scales with model size and capability to identify minimum effective matcher size for different question types.

3. **Real-World Deployment Simulation**: Design a study where answer matching evaluates models on questions without pre-existing reference answers to assess framework handling of novel research problems.