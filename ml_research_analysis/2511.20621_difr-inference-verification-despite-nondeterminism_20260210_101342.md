---
ver: rpa2
title: 'DiFR: Inference Verification Despite Nondeterminism'
arxiv_id: '2511.20621'
source_url: https://arxiv.org/abs/2511.20621
tags:
- h200
- incorrect
- inference
- qwen3-30b-a3b
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiFR (Divergence-From-Reference), a method
  for verifying LLM inference outputs despite nondeterminism. The core idea is to
  synchronize random seeds between provider and verifier, enabling near-deterministic
  generation where token-level outputs themselves serve as auditable evidence of correct
  inference at zero provider overhead.
---

# DiFR: Inference Verification Despite Nondeterminism

## Quick Facts
- **arXiv ID**: 2511.20621
- **Source URL**: https://arxiv.org/abs/2511.20621
- **Reference count**: 40
- **Primary result**: Detects 4-bit quantization with AUC > 0.999 within 300 tokens using seed-synchronized sampling; achieves 25-75% communication overhead reduction via random activation projections

## Executive Summary
DiFR (Divergence-From-Reference) provides a method for verifying LLM inference outputs despite floating-point nondeterminism. The approach uses seed-synchronized random sampling to constrain the output space, making deviations from correct inference detectable through token-level comparison. The work introduces two complementary methods: Token-DiFR compares claimed tokens against reference predictions using shared randomness, while Activation-DiFR uses random orthogonal projections to compress activations into compact fingerprints for sample-efficient detection. The system achieves high detection rates (AUC > 0.999) for various misconfigurations while maintaining practical deployment characteristics.

## Method Summary
DiFR addresses the challenge of verifying LLM inference correctness in the presence of nondeterminism by synchronizing random seeds between provider and verifier. Token-DiFR leverages Gumbel-Max sampling where both parties use the same seed, making token selection deterministic given the logits. Deviations between provider and verifier outputs reveal inference errors through clipped post-Gumbel logit margins. Activation-DiFR bypasses seed synchronization by projecting high-dimensional activations through random orthogonal matrices into low-dimensional fingerprints, enabling detection with minimal token requirements. Both methods aggregate per-token scores into batch-level classifiers using threshold-based decision rules calibrated from honest configurations.

## Key Results
- Token-DiFR detects 4-bit quantization with AUC > 0.999 within 300 tokens
- Activation-DiFR achieves AUC > 0.999 for 4-bit quantization using just 2 tokens
- Activation-DiFR reduces communication overhead by 25-75% versus existing methods
- Combined approach provides defense-in-depth with temperature-zero spot-checking

## Why This Works (Mechanism)

### Mechanism 1: Seed-Synchronized Sampling Constrained Output Space
When provider and verifier share a PRNG seed, the set of valid token outputs contracts to a near-deterministic subset, making unauthorized deviations detectable. Gumbel-Max sampling adds fixed noise vectors (derived from the shared seed) to logits before argmax selection. Since the noise is deterministic given the seed, any deviation between provider and verifier outputs must arise from logit differences caused by the forward pass—systematic deviations (quantization, wrong weights) produce statistically distinguishable distributions of post-Gumbel margins.

### Mechanism 2: Post-Gumbel Logit Difference as Detection Statistic
The margin δlogit(t*, t̂, σ) between the provider's claimed token and the verifier's expected token under shared randomness provides a scalar score that aggregates into a reliable batch-level classifier. For each token position, compute z = logits + T·gumbel_noise. The margin δlogit = z[verifier_token] - z[claimed_token]. When tokens match, δ=0; when they differ, δ grows with how much the verifier's preferred token was suppressed. Clipping at high percentiles (99.9th or 99.999th) handles outliers. Aggregating mean margins across 100-10,000 tokens yields a batch statistic that distinguishes H0 (benign noise) from H1 (misconfiguration) via thresholding.

### Mechanism 3: Johnson-Lindenstrauss Projection for Activation Compression
Random orthogonal projections compress high-dimensional activation vectors (D≥4096) into low-dimensional fingerprints (k=2-64) while approximately preserving ℓ2 distances, enabling sample-efficient detection. Both provider and verifier generate the same random orthogonal projection matrix P (via shared seed σ'). They compute f = P·a and f̂ = P·â, then compare ∥f - f̂∥₂. The JL lemma guarantees distance preservation with high probability for sufficiently large k. Since quantization or wrong weights produce activation drift, the distance statistic detects deviations.

## Foundational Learning

- **Concept: Gumbel-Max Trick for Discrete Sampling**
  - Why needed here: Token-DiFR relies on Gumbel-Max sampling to make token selection deterministic given a seed. Without understanding this, the mechanism of seed-synchronized verification is opaque.
  - Quick check question: Given logits [2.0, 1.0, 0.5] and Gumbel noise [0.1, 1.5, -0.2] at temperature 1.0, which token is selected?

- **Concept: Floating-Point Non-Associativity in Parallel Reductions**
  - Why needed here: The core motivation is that GPU inference produces nondeterministic results due to varying reduction orders across batches, kernels, and hardware. Understanding this explains why exact reproducibility fails.
  - Quick check question: Why does (a+b)+c ≠ a+(b+c) in IEEE 754 floating-point arithmetic? What hardware factors change the effective reduction order?

- **Concept: AUC at Fixed FPR as Detection Metric**
  - Why needed here: The paper reports AUC@FPR=1% to emphasize low false-positive operating points relevant to deployment. Interpreting these curves is essential for evaluating tradeoffs.
  - Quick check question: If a detector has AUC=0.95 at FPR=1%, what does this imply about its true positive rate when configured to flag only 1% of honest batches as suspicious?

## Architecture Onboarding

- **Component map**:
  ```
  Provider Side:
    └─ Inference Engine (vLLM)
       ├─ Forward Pass → Logits + Activations
       ├─ Gumbel-Max Sampler (seeded) → Token Output
       └─ (Optional) Activation Projector → Fingerprint f
  
  Verifier Side:
    └─ Trusted Reference Implementation
       ├─ Prefill Pass on [prompt + claimed_tokens] → Recomputed Logits + Activations
       ├─ Token-DiFR Scorer → δlogit per token → Batch mean
       └─ Activation-DiFR Scorer → ∥f - f̂∥₂ per token → Batch mean
  
  Decision:
    └─ Threshold Classifier (τ calibrated from pooled honest configs)
  ```

- **Critical path**:
  1. Provider generates output with seed σ and optionally records activation fingerprints
  2. Verifier receives (prompt, output, seed, [fingerprints])
  3. Verifier runs single prefill pass to recompute logits/activations
  4. Verifier computes Token-DiFR margins and/or Activation-DiFR distances
  5. Verifier aggregates scores and compares to threshold τ

- **Design tradeoffs**:
  | Choice | Pros | Cons |
  |--------|------|------|
  | Token-DiFR only | Zero communication overhead; no provider modifications | Requires seed sync; needs 100s-1000s of tokens for small deviations |
  | Activation-DiFR only | 2-4 tokens suffice; no seed sync required | 0.1-7+ bytes/token overhead; requires provider instrumentation |
  | Both combined | Maximal sensitivity; defense-in-depth | Highest implementation complexity |
  | Clipping at 99.9th percentile | Better for quantization detection | Worse for rare bugs (need 99.999th) |
  | Pooled calibration (multiple GPUs) | Robust to provider hardware variation | Widens H0, reduces sensitivity to small deviations |

- **Failure signatures**:
  - **Cross-entropy gaming**: Adversary tunes temperature to match reference cross-entropy. Detection: Token-DiFR margins remain elevated (Figure 2).
  - **Heterogeneous calibration contamination**: Pooling HuggingFace runs with vLLM for Qwen3-30B-A3B produces H0 tail overlap with FP8-KV quantization. Detection: Use vLLM-only pool or separate thresholds per implementation.
  - **Seed manipulation**: Provider uses wrong seed. Detection: Token-DiFR exact-match rate drops from ~99% to ~85% (Table 6).
  - **MoE routing variability**: Tensor-parallel MoE shows larger benign noise. Detection: Characterize noise per architecture; may need larger batches or activation-based methods.

- **First 3 experiments**:
  1. **Establish baseline noise**: Run 2000 prompts through your inference stack on reference hardware (bf16, temp=1.0, top-p=0.95, top-k=50). Record Token-DiFR margins and cross-entropy. Compute mean and 99.9th percentile. This defines your H0.
  2. **Inject controlled misconfigurations**: Repeat with 4-bit model quantization and FP8 KV-cache quantization. Plot margin distributions against H0. Confirm separability at 100, 300, 1000 token batch sizes. Target: AUC>0.99 at 300 tokens for 4-bit.
  3. **Test Activation-DiFR compression curve**: For a subset of tokens (every J-th token), vary projection dimension k ∈ {2, 8, 32}. Measure detection AUC vs. bytes/token overhead. Identify Pareto frontier for your target AUC threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Token-DiFR be extended to verify speculative decoding and other advanced sampling procedures with comparable detection efficiency?
- **Basis in paper**: [explicit] "We have not evaluated our approach on other sampling procedures such as speculative decoding... the wide variety of speculative decoding implementations would each require slightly modified verification algorithms."
- **Why unresolved**: vLLM 0.10.0 lacks speculative decoding support, and each variant (speculative sampling, EAGLE, etc.) has different acceptance/resampling rules requiring custom metadata logging.
- **What evidence would resolve it**: Implementing and evaluating verification algorithms for at least one speculative decoding method (e.g., Leviathan et al. 2023), showing detection performance and any additional metadata requirements.

### Open Question 2
- **Question**: What adversarial strategies beyond temperature-tuning could defeat Token-DiFR, and can detection remain robust against adaptive adversaries?
- **Basis in paper**: [inferred] The paper demonstrates cross-entropy's vulnerability to temperature-tuning attacks but does not systematically explore adversarial robustness of Token-DiFR beyond this simple case.
- **Why unresolved**: Only one adversarial attack was tested. Sophisticated adversaries might manipulate logits, selectively report outputs, or exploit the calibration process.
- **What evidence would resolve it**: Formal adversarial analysis or empirical evaluation against a suite of adaptive attacks targeting seed-synchronized verification.

### Open Question 3
- **Question**: How should verifiers define "acceptable numerical variation" when pooling heterogeneous honest implementations with substantially different score distributions?
- **Basis in paper**: [explicit] The HuggingFace implementation for Qwen3-30B-A3B "yielded much larger benign Token-DiFR gaps than the vLLM-based configurations... it is ultimately up to the end user whether to treat those differences as acceptable."
- **Why unresolved**: The paper excludes HuggingFace from pooled results but provides no principled framework for deciding which implementations belong in the null distribution.
- **What evidence would resolve it**: A systematic methodology or threshold criterion for determining inclusion in pooled honest baselines, validated across multiple inference stacks.

### Open Question 4
- **Question**: Can verification techniques be adapted for closed-weights model APIs without requiring model weight access?
- **Basis in paper**: [explicit] "Token-DiFR and Activation-DiFR require access to model weights... cannot verify inference from providers serving closed-weights models."
- **Why unresolved**: The fundamental approach requires re-computation with trusted weights, which is impossible for proprietary APIs.
- **What evidence would resolve it**: A modified protocol using distributional tests, API logit access, or cryptographic commitments that provides verification guarantees without full weight access.

## Limitations

- **Calibration Pool Sensitivity**: Token-DiFR performance degrades when calibration sets include heterogeneous implementations, requiring careful selection of null distributions
- **Hardware Mismatch Amplification**: Activation-DiFR shows significantly increased communication overhead (7.25-30 bytes/token) when provider and verifier hardware differ substantially
- **False Positive Risk**: The paper reports AUC@1% FPR but doesn't provide concrete estimates of operational false positive rates in production settings

## Confidence

**High Confidence**:
- The core mathematical framework of Token-DiFR (Gumbel-Max with seed synchronization producing deterministic margins) is sound and well-specified
- Activation-DiFR's use of Johnson-Lindenstrauss projections for distance preservation is theoretically grounded
- Empirical results showing AUC > 0.999 for 4-bit quantization detection within 300 tokens are reproducible based on the methodology described

**Medium Confidence**:
- Claims about operational deployment readiness, particularly regarding false positive rates at scale and the effectiveness of temperature-zero spot-checking as a mitigation
- The calibration procedure's robustness across diverse hardware and implementation combinations
- The practical overhead implications for third-party API integration

**Low Confidence**:
- Claims about adversarial robustness against sophisticated gaming strategies (e.g., providers who can perform full-precision inference but selectively cheat)
- Long-term effectiveness as LLM implementations evolve and introduce new sources of nondeterminism
- The trade-off between detection sensitivity and communication overhead in heterogeneous deployment scenarios

## Next Checks

1. **Calibration Pool Sensitivity Analysis**: Systematically evaluate Token-DiFR performance across different calibration pool compositions (vLLM-only vs. mixed implementations, single GPU vs. multi-GPU, different quantization schemes). Measure AUC degradation as the calibration set becomes more heterogeneous, and develop guidelines for constructing robust calibration pools that minimize false positives while maintaining detection sensitivity.

2. **Adversarial Provider Simulation**: Design and implement an adversarial provider that can perform inference at full precision but selectively introduces errors on seed-synchronized queries. Test whether temperature-zero spot-checking effectively detects this class of attack, and quantify the false negative rate under various attack strategies (random token corruption, targeted output manipulation, etc.).

3. **Production Deployment Benchmark**: Deploy a scaled-down version of DiFR (using a smaller model like Llama-3.1-8B) on a real API-like infrastructure where multiple providers use different hardware and implementation combinations. Measure actual false positive/negative rates under realistic load conditions, and evaluate the operational overhead of seed synchronization and activation fingerprinting in a production environment.