---
ver: rpa2
title: Inference in Spreading Processes with Neural-Network Priors
arxiv_id: '2509.02073'
source_url: https://arxiv.org/abs/2509.02073
tags:
- where
- algorithm
- spreading
- overlap
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of inferring the initial state of
  spreading processes on graphs, such as epidemics, when the initial sources are determined
  by an unknown function of node features. Unlike standard models that assume random
  initial sources, the authors propose a "Neural Sources Spreading" model where the
  initial state is generated by a simple neural network (specifically, a single-layer
  perceptron) acting on known node-wise covariate variables.
---

# Inference in Spreading Processes with Neural-Network Priors

## Quick Facts
- arXiv ID: 2509.02073
- Source URL: https://arxiv.org/abs/2509.02073
- Reference count: 0
- Key outcome: A hybrid BP-AMP algorithm for inferring initial spreading states on graphs where sources are determined by neural network priors on node features, revealing statistical-to-computational gaps for binary weights.

## Executive Summary
This paper addresses the problem of inferring initial states in spreading processes on graphs when the initial sources are determined by an unknown function of node features. Unlike standard models assuming random initial sources, the authors propose a "Neural Sources Spreading" model where the initial state is generated by a single-layer perceptron acting on known node-wise covariates. The key methodological contribution is a hybrid belief propagation and approximate message passing (BP-AMP) algorithm that combines inference from spreading dynamics with information from the neural network prior.

The authors demonstrate that incorporating neural-network prior information significantly enhances inference performance. For Gaussian weights in the perceptron, the algorithm consistently outperforms baselines using only spreading or feature information. More interestingly, for Rademacher (binary) weights, the model exhibits first-order phase transitions creating a statistical-to-computational gap: perfect recovery becomes theoretically possible at certain parameter regimes, but the BP-AMP algorithm fails to achieve it despite the theoretical possibility. The authors characterize these transitions through free energy analysis and identify conditions under which hard phases exist where Bayes-optimal inference is possible but computationally intractable.

## Method Summary
The BP-AMP algorithm iteratively couples belief propagation for sparse spreading interactions on the graph with approximate message passing for the dense neural network prior. The algorithm maintains two sets of messages: BP messages for graph interactions and AMP messages for the covariate interactions. In each iteration, BP updates are computed using the current AMP-derived priors, then AMP updates use the resulting BP marginals. The process continues until convergence, with damping applied to ensure stability. The algorithm derives specific denoising functions for both the spreading process and the perceptron prior, combining them to compute the final posterior beliefs for each node's initial state.

## Key Results
- Incorporating neural-network prior information significantly improves inference performance compared to using spreading dynamics alone
- For Gaussian weights, the algorithm smoothly bridges the performance gap between BP-only and AMP-only baselines
- For Rademacher (binary) weights, the model exhibits first-order phase transitions creating a statistical-to-computational gap
- The algorithm can verify Bayes-optimality through Nishimori conditions, where expected overlap matches mean overlap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining Belief Propagation (BP) and Approximate Message Passing (AMP) allows efficient Bayesian inference on hybrid graphs containing both sparse spreading dynamics and dense neural network priors.
- **Mechanism:** The algorithm decouples the posterior into two modules. BP handles the **sparse** graph interactions (spreading), assuming a locally tree-like structure. AMP handles the **dense** interactions (perceptron prior on covariates), assuming high-dimensional Gaussian convergence. These modules exchange marginals ($\nu_i$ from BP to AMP, $\eta_i$ from AMP to BP) in a fixed-point iteration.
- **Core assumption:** The spreading graph is locally tree-like (replica symmetry holds), and the covariate matrix $F$ is i.i.d. Gaussian with $N, M \to \infty$.
- **Evidence anchors:**
  - [abstract] "derive a hybrid belief propagation and approximate message passing (BP-AMP) algorithm that jointly handles both the sparse spreading interactions and the dense neural-network prior."
  - [section IV] "Using the cavity method, we merge these components to derive a hybrid BP-AMP algorithm..."
  - [corpus] Weak direct evidence in provided corpus; "Covariate-dependent Graphical Model Estimation" discusses neural nets for graphs but lacks the specific BP-AMP hybridization mechanism.
- **Break condition:** If the graph has short loops or the covariate matrix is highly structured (violating i.i.d. assumptions), the decoupling approximation fails.

### Mechanism 2
- **Claim:** A first-order phase transition in the free entropy creates a statistical-to-computational gap, preventing the algorithm from finding the globally optimal solution even when perfect recovery is theoretically possible.
- **Mechanism:** For Rademacher (binary) weights, the free entropy landscape develops multiple local maxima (metastable states). The algorithm converges to a suboptimal "partial recovery" fixed point because the basin of attraction for the "perfect recovery" fixed point is inaccessible from a random initialization, despite the latter having higher free entropy.
- **Core assumption:** The prior distribution of the neural network weights is Rademacher (binary), rather than Gaussian.
- **Evidence anchors:**
  - [abstract] "...model can exhibit first-order phase transitions... creating a statistical-to-computational gap where even the BP-AMP algorithm... fails to achieve it."
  - [section VI.A] "This defines a computationally hard phase... Bayes-optimal recovery is possible, but our algorithm fails to find it."
  - [corpus] Not directly evidenced in the provided corpus neighbors.
- **Break condition:** Switching the weight prior to Gaussian eliminates the first-order transition, restoring the algorithm's ability to reach the global optimum smoothly.

### Mechanism 3
- **Claim:** Validating inference requires checking the Nishimori conditions, which verify that the algorithm's estimated uncertainty matches the true posterior error.
- **Mechanism:** In a Bayes-optimal setting, the expected overlap between the estimate and ground truth ($O$) must equal the expected mean overlap computed from the posterior ($MO$). If $O \neq MO$, the algorithm is either overconfident or underconfident, indicating a failure in the approximation (e.g., replica symmetry breaking) or convergence to a metastable state.
- **Core assumption:** The model parameters used for inference exactly match the generative process (Bayes-optimal setting).
- **Evidence anchors:**
  - [section III.A] "...the Nishimori conditions have to hold... expected overlap... and the expected mean overlap... must coincide."
  - [appendix C1] "We plot the difference between overlap and mean overlap... as expected... [it] diminishes when we increase N."
  - [corpus] Weak/General; "Quantifying Uncertainty..." relates broadly to uncertainty but not this specific statistical physics condition.
- **Break condition:** If the model is misspecified (e.g., wrong spreading parameter $\lambda$), the conditions are expected to break, indicating systematic bias.

## Foundational Learning

- **Concept: Belief Propagation (BP) on Factor Graphs**
  - **Why needed here:** This is the engine for the "spreading" half of the model. Understanding how messages pass on the graph $G$ is required to grasp how local transition probabilities constrain global states.
  - **Quick check question:** If a graph has many short loops, does BP's accuracy increase or decrease?

- **Concept: Approximate Message Passing (AMP)**
  - **Why needed here:** This is the engine for the "neural prior" half. It handles the dense matrix multiplication ($F \cdot u$) efficiently by tracking only the first two moments (mean and variance) of the distribution.
  - **Quick check question:** What role does the "Onsager reaction term" play in the AMP update equations?

- **Concept: Free Entropy (Bethe Free Energy)**
  - **Why needed here:** Section VI relies on comparing free entropy values to detect phase transitions. You must understand that the system seeks to maximize this quantity, and multiple maxima imply computational traps.
  - **Quick check question:** In a first-order transition, why does an algorithm typically remain in a "metastable" state even if a higher-entropy state exists?

## Architecture Onboarding

- **Component map:**
  1. **Inputs:** Covariates $F$ (Dense), Graph $G$ (Sparse), Observations $O$.
  2. **BP Module:** Computes $\nu_i$ (marginals of initial states) using local graph neighbors.
  3. **AMP Module:** Computes $\eta_i$ (priors from covariates) and estimates weights $a_a, v_a$.
  4. **Hybrid Iterator:** Exchanges $\nu_i$ and $\eta_i$; multiplies them to form the final posterior belief $\chi_i$.

- **Critical path:**
  Initialize messages $\to$ **BP Step** (Update $m_{i \to j}$ using $\eta_i$) $\to$ Compute $\nu_i$ $\to$ **AMP Step** (Update $g_o, f_a$ using $\nu_i$) $\to$ Compute $\eta_i$ $\to$ Check Convergence.

- **Design tradeoffs:**
  - **Gaussian vs. Rademacher Weights:** Gaussian priors are computationally easier (no hard phase) but may model binary biological states less accurately than Rademacher priors, which introduce computational traps.
  - **Random vs. Informed Initialization:** Random initialization tests algorithmic tractability ("can we find the solution?"), while Informed initialization (using ground truth) finds the information-theoretic limit ("does a solution exist?").

- **Failure signatures:**
  - **Divergence of overlaps:** If $O_{t=0} \neq MO_{t=0}$ (Nishimori condition violation), the algorithm is hallucinating certainty or stuck in a spin-glass phase.
  - **Non-convergence:** Oscillating messages typically indicate the sparse graph is not locally tree-like or parameters are in a chaotic phase.

- **First 3 experiments:**
  1. **Sanity Check (Gaussian):** Run BP-AMP on a Random Regular Graph with Gaussian weights and sensor observations. Verify that performance smoothly bridges the gap between "BP-only" and "AMP-only" baselines (replicate Fig 2).
  2. **Locate the Hard Phase (Rademacher):** Fix $\kappa$ and scan the sensor fraction $\rho$. Identify the region where Random Initialization fails but Informed Initialization succeeds (the gap between $\rho_{IT}$ and $\rho_c$ in Fig 5).
  3. **Scaling Analysis:** Run the algorithm for increasing system sizes $N$ near the transition point to confirm that the Nishimori conditions are restored in the thermodynamic limit (verify Fig 12).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the presence of replica symmetry breaking (RSB) affect the performance of the BP-AMP algorithm in the low-source-density regime?
- **Basis in paper:** [explicit] The "Related Works" section and Section IV explicitly state that the paper avoids the low-density regime where RSB is expected, leaving this regime "for future study."
- **Why unresolved:** The current analysis relies on the Replica Symmetric (RS) assumption for the cavity method. At very low source densities, RSB is conjectured to occur, causing standard BP to fail to converge or produce incorrect marginals.
- **What evidence would resolve it:** An extension of the algorithm (e.g., using Survey Propagation) or a state evolution analysis that remains valid under RSB conditions, tested specifically at low values of the source density $\delta$.

### Open Question 2
- **Question:** Can the inference framework be generalized to multi-layer neural network priors while maintaining tractability?
- **Basis in paper:** [explicit] Section VII (Conclusion) explicitly lists exploring "more complex neural-network priors, such as multi-layer architectures" as a direction for future work.
- **Why unresolved:** The current derivation relies on a single-layer perceptron, which allows for a specific coupling of AMP and BP. Multi-layer networks introduce non-Gaussian effective fields and complex dependencies between layers that require combining different AMP iterations.
- **What evidence would resolve it:** A derivation of the AMP updates for a multi-layer prior integrated with the BP spreading equations, demonstrating convergence and improved performance over the single-layer model.

### Open Question 3
- **Question:** Can the model parameters (e.g., transmission rates $\lambda$, graph topology) be learned unsupervised within this framework?
- **Basis in paper:** [explicit] Section VII notes the assumption of known model parameters is a limitation and identifies the "case where these parameters must be learned" as a natural extension.
- **Why unresolved:** The current Bayesian setting assumes the likelihood and prior parameters $\Theta$ are known. Learning them requires maximizing the free entropy (or likelihood) with respect to these parameters, which adds computational complexity to the message-passing fixed-point equations.
- **What evidence would resolve it:** An extension of the BP-AMP algorithm incorporating an Expectation-Maximization (EM) step or gradient descent to estimate $\Theta$ simultaneously with the node states.

### Open Question 4
- **Question:** Can the statistical-to-computational gap observed with binary weights be rigorously proven?
- **Basis in paper:** [explicit] Section VII states that "developing rigorous mathematical proofs for the more general hardness conjectures presented here remains an important open question."
- **Why unresolved:** The paper identifies a gap between the information-theoretic threshold ($\rho_{IT}$) and the algorithmic threshold ($\rho_c$) empirically and via the replica method. However, a rigorous proof that no polynomial-time algorithm can succeed in the "hard" phase is missing.
- **What evidence would resolve it:** A formal reduction from a known computationally hard problem or a rigorous lower bound proof that matches the observed algorithmic transition $\rho_c$.

## Limitations

- The analysis relies on replica-symmetric (RS) cavity assumptions, which may fail in low-source-density regimes where replica symmetry breaking becomes significant.
- The hard phase characterization depends on free entropy landscape geometry, but theoretical guarantees for convergence to global optima remain incomplete.
- The algorithm's performance in heterogeneous graph structures beyond random regular graphs has not been validated.

## Confidence

- **High confidence:** The hybrid BP-AMP framework and its connection to Bayes-optimal inference (Gaussian weight case). The mechanism of combining sparse graph messages with dense covariate information is well-grounded in existing AMP and BP theory.
- **Medium confidence:** The characterization of first-order phase transitions and statistical-to-computational gaps for Rademacher weights. While the free entropy analysis is sound, the empirical verification across parameter regimes could be more extensive.
- **Low confidence:** The practical scalability and convergence guarantees in real-world graphs with community structure or temporal dynamics, which are not addressed in the theoretical analysis.

## Next Checks

1. **Convergence Verification:** Run the BP-AMP algorithm on a range of system sizes (N = 5,000 to 50,000) and verify that Nishimori conditions hold in the thermodynamic limit, confirming Bayes-optimality.
2. **Hard Phase Exploration:** Systematically map the phase diagram for Rademacher weights by varying (ρ, δ) pairs, measuring both overlap and free entropy to precisely locate the statistical-to-computational gap boundaries.
3. **Graph Structure Robustness:** Test the algorithm on heterogeneous graphs (e.g., power-law degree distributions, community structure) to evaluate whether the BP-AMP framework maintains performance outside the random regular graph assumption.