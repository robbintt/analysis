---
ver: rpa2
title: Do Vision-Language Models Understand Visual Persuasiveness?
arxiv_id: '2511.17036'
source_url: https://arxiv.org/abs/2511.17036
tags:
- visual
- message
- image
- persuasive
- persuasiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vision-language models exhibit a recall-oriented bias in visual\
  \ persuasion judgment, over-predicting high persuasiveness while underperforming\
  \ on low-level and mid-level feature discrimination. Human-aligned high-level semantic\
  \ cues\u2014particularly the presence of message-relevant key objects\u2014are the\
  \ strongest predictors of persuasiveness, whereas models over-rely on generic human\
  \ presence."
---

# Do Vision-Language Models Understand Visual Persuasiveness?

## Quick Facts
- **arXiv ID:** 2511.17036
- **Source URL:** https://arxiv.org/abs/2511.17036
- **Reference count:** 40
- **Primary result:** Vision-language models over-predict high persuasiveness and under-rely on message-relevant visual cues, requiring explicit object-grounded rationales to improve alignment with human judgment.

## Executive Summary
Vision-language models (VLMs) exhibit a systematic recall-oriented bias when judging visual persuasiveness, over-predicting high persuasiveness while underperforming on low-level and mid-level feature discrimination. Human-aligned high-level semantic cues—particularly the presence of message-relevant key objects—are the strongest predictors of persuasiveness, whereas models over-rely on generic human presence. Knowledge-injection strategies improve performance only when concise, object-grounded rationales are provided; instruction-based or scaffolded reasoning alone fail to correct the bias. These findings highlight that VLMs' core limitation is not object recognition but linking visual cues to communicative intent, necessitating explicit, rationale-driven reasoning for accurate persuasive understanding.

## Method Summary
The study evaluates VLMs on a high-consensus subset of the PVP dataset (562 images) filtered for "almost perfect" annotator agreement. Binary persuasiveness judgments are made using minimal reasoning, with feature extraction at three levels: low-level (color, brightness), mid-level (saliency metrics via DeepGaze IIE), and high-level (key objects via OWL-ViT, human presence via YOLOv8). Logistic regression quantifies feature influence via odds ratios, while interventions include prompt engineering and knowledge injection with object-grounded rationales. The analysis compares human vs. model feature importance and measures precision/recall/F1 across prompting strategies.

## Key Results
- VLMs show recall-oriented bias: near-perfect recall (>98%) but poor precision (<52%) due to over-predicting high persuasiveness.
- High-level semantic alignment (key object presence) is the dominant predictor of human judgments (OR ≈ 3.28), while low/mid-level features have weak discriminative power.
- Object-grounded rationales significantly improve VLM precision and F1 scores (+13.52 to +27.91), but only when concise and message-aligned; generic human presence cues are over-weighted by models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-level semantic alignment between message nouns and visually present objects is the dominant driver of human persuasiveness judgments, not low-level perceptual or mid-level compositional cues.
- Mechanism: Humans judge persuasiveness primarily by checking whether semantically relevant key objects (extracted from message nouns) are visually depicted and contextually aligned with the persuasive intent. Low-level color/brightness and mid-level saliency provide background context but lack discriminative power.
- Core assumption: Persuasion operates through the peripheral route of Elaboration Likelihood Model, but semantic grounding is still the primary signal even in high-consensus scenarios.
- Evidence anchors:
  - [abstract] "high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment"
  - [Section 4.2] "Key Object Presence increases the odds of being judged persuasive by more than threefold (OR≈3.28)"
  - [Section 4.2] "human presence has no significant influence for humans, yet models consistently overreact to it"
  - [corpus] Related work on VLM pragmatic inference (arXiv:2502.09120) confirms VLMs struggle with speaker intent inference from visual-linguistic cues
- Break condition: If low-level or mid-level features showed comparable odds ratios (>2.0) with key-object presence, semantic alignment would not be dominant.

### Mechanism 2
- Claim: VLMs' recall-oriented bias stems from misinterpreting visual cues as causally diagnostic when they are merely correlational, particularly over-weighting human presence.
- Mechanism: VLMs learn statistical associations from training data where human presence often co-occurs with persuasive content (ads, campaigns). This creates a spurious correlation that models treat as causal, leading to false positives on images with people but lacking message-relevant objects.
- Core assumption: Training data contains biased co-occurrence patterns between generic visual features and persuasive labels.
- Evidence anchors:
  - [abstract] "models over-react to human presence and under-attend to message-relevant grounding"
  - [Table 1] VLMs show Human OR of 1.56-2.48 (statistically significant) vs. Human ground truth OR of 1.22 (non-significant)
  - [Section 4.2] "models may conflate persuasive intent, revealing a systematic divergence"
  - [corpus] ColorBench (arXiv:2504.10514) shows VLMs have inconsistent color perception—similar pattern of surface-level feature over-reliance
- Break condition: If FP (false positive) images showed lower human presence rates than TP (true positive) images, the mechanism would not hold.

### Mechanism 3
- Claim: Object-grounded rationales improve VLM performance because they explicitly bridge the gap between object recognition and communicative intent understanding.
- Mechanism: Concise rationales that explain why specific objects support/undermine the message provide explicit grounding signals that VLMs cannot self-generate. This external knowledge injection compensates for the model's inability to reason about causal relationships between objects and persuasion.
- Core assumption: VLMs can recognize objects but cannot autonomously infer their persuasive function without explicit scaffolding.
- Evidence anchors:
  - [abstract] "concise, object-grounded rationales significantly improve precision and F1 scores"
  - [Table 2] Key-Object Rationale (Informed) achieves ΔF1 of +13.52 to +27.91 across models
  - [Section 5.2] "knowing what the key object is does not imply understanding why it matters"
  - [corpus] Neuropsychological tests (arXiv:2504.10786) show VLMs have widespread visual deficits in elemental reasoning—supports claim that explicit grounding is needed
- Break condition: If label-agnostic rationales achieved comparable gains to label-informed rationales, the mechanism would not require causal explanation but only object identification.

## Foundational Learning

- Concept: **Elaboration Likelihood Model (ELM) - Central vs. Peripheral Routes**
  - Why needed here: The paper's theoretical foundation assumes visual persuasion operates primarily through peripheral processing (superficial cues like color, composition). Understanding this distinction explains why semantic alignment still dominates even in "peripheral" persuasion.
  - Quick check question: If a viewer carefully analyzes how each object supports an argument, which ELM route are they using? Would this dataset's high-consensus judgments reflect central or peripheral processing?

- Concept: **Odds Ratios (OR) and Logistic Regression Interpretation**
  - Why needed here: The paper uses OR to quantify how visual features affect persuasiveness judgments. An OR of 3.28 for key objects means their presence triples the odds of high persuasiveness ratings.
  - Quick check question: If Human Presence OR for GPT-5 is 1.56 with p<0.05, but Human Presence OR for ground truth is 1.22 with p=0.356, what does this tell you about how GPT-5 differs from humans?

- Concept: **Saliency Maps and Attention Modeling**
  - Why needed here: Mid-level compositional features rely on saliency prediction (DeepGaze IIE) to measure where humans look. Understanding that saliency ≠ persuasion judgment explains why these features have weak discriminative power.
  - Quick check question: Why might high saliency concentration at an image's center not predict persuasiveness, even though it predicts where viewers look first?

## Architecture Onboarding

- Component map:
  - **Low-Level Feature Extractor**: Hasler-Süsstrunk colorfulness, CIELAB color entropy, L* brightness from raw images
  - **Mid-Level Feature Extractor**: DeepGaze IIE saliency → A@p, Hsal, CBI, T3 metrics
  - **High-Level Feature Extractor**: spaCy noun extraction → OWL-ViT object detection; YOLOv8 person detection
  - **VLM Under Test**: GPT-5, Gemma3, Qwen2.5-VL, LLaVA variants (binary judgment + optional reasoning)
  - **Knowledge Injector**: GPT-5 as rationale generator (agnostic/informed variants)
  - **Evaluation Pipeline**: Logistic regression (OR, AME) + classification metrics (Acc/Prec/Rec/F1)

- Critical path:
  1. Dataset filtering: PVP → Fleiss' κ banding → "Almost perfect" subset → low/high binary labels (562 images)
  2. Feature extraction: Parallel extraction of all three VPF levels per image-message pair
  3. Baseline evaluation: VLM binary judgment with minimal reasoning
  4. Knowledge injection: Augment input with detected objects ± rationales
  5. Analysis: Compare human vs. model OR/AME; measure ΔF1 across injection strategies

- Design tradeoffs:
  - **High-consensus filtering**: Reduces noise but limits generalization to ambiguous/subjective persuasion cases
  - **Binary judgment vs. scalar scoring**: Sacrifices granularity for robustness and interpretability
  - **Label-informed rationales as oracle**: Shows upper bound but not practical for deployment (requires ground truth labels)
  - **DeepGaze IIE for saliency**: Trained on general attention, not persuasion-specific viewing patterns

- Failure signatures:
  - **Recall-oriented bias**: Near-perfect recall (>98%) with poor precision (<52%) → models default to "high" predictions
  - **Human presence over-weighting**: Non-significant human feature in ground truth becomes significant in model predictions
  - **Instruction-following failures**: LLaVA variants omitted from analysis due to unstable outputs (Table 1 note)
  - **Unguided reasoning backfire**: Knowledge-Conditioned Chain hurts GPT-5 (ΔF1 -2.96) and Gemma3 (-10.46) by amplifying recall bias

- First 3 experiments:
  1. **Reproduce baseline bias quantification**: Run binary judgment on robust subset with GPT-5-mini. Expected: Recall >95%, Precision <50%. Verify FP/TP feature distributions match Table 1 Panel C pattern (FP brighter, more colorful, lower entropy than TP).
  2. **Test feature-level intervention effects**: Implement Aligned Key-Object Context injection. Compare: (a) object presence only, (b) object + bounding boxes, (c) object + confidence scores. Expected: All variants show ΔF1 ≈ -0.5 to -2.0 (features alone not actionable).
  3. **Probe rationale quality gap**: Generate both agnostic and informed rationales for same image-message pairs. Measure BERTScore-F1, ROUGE-L, SBERT-Cosine between them. Expected: Moderate semantic overlap (~0.35-0.40 BERTScore-F1) with divergent reasoning patterns, confirming "what ≠ why" gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VLMs maintain discriminative performance on visual persuasion tasks when human judgments exhibit low agreement, or does the recall-oriented bias observed in high-consensus settings become more severe?
- Basis in paper: [explicit] "Generalization to 'low-agreement' data—where human interpretations widely vary—remains a necessary next step."
- Why unresolved: The study deliberately filtered for high-consensus items (Fleiss' κ "Almost perfect"), leaving unclear whether VPF-based interventions help when persuasive cues are inherently ambiguous or subjective.
- What evidence would resolve it: Evaluate the same VPF-aware knowledge injection strategies on the discarded low-agreement subset of the PVP dataset, comparing precision/recall trade-offs against the high-consensus baseline.

### Open Question 2
- Question: Does integrating individual personality traits (e.g., Big Five, Need for Cognition) into VPF representations improve VLM alignment with human persuasiveness judgments?
- Basis in paper: [explicit] "Our study did not account for individual personality traits, which can significantly influence perceptions of persuasion."
- Why unresolved: The PVP dataset includes annotators with distinct psychological profiles, but the study aggregated judgments rather than modeling individual-level variation—leaving the personalization dimension unexplored.
- What evidence would resolve it: Train or prompt VLMs with personality-conditioned VPF features and measure whether predictions correlate more strongly with subgroup-specific human labels.

### Open Question 3
- Question: Can persuasion-oriented saliency models—trained to predict attention patterns specific to persuasive contexts—provide better compositional features for VLM persuasion understanding than general-purpose saliency predictors like DeepGaze IIE?
- Basis in paper: [explicit] "DeepGaze IIE-style saliency captures where attention concentrates, but it was trained on general visual attention, which may not be the same cognitive process as the attention used for judging persuasiveness."
- Why unresolved: Mid-level compositional features showed weak discriminative power; the authors speculate this stems from using general saliency rather than persuasion-specific attention models.
- What evidence would resolve it: Collect eye-tracking data from humans making persuasiveness judgments, train a specialized saliency model, and re-evaluate whether mid-level VPFs become predictive.

### Open Question 4
- Question: Can VLMs learn to generate concise, object-grounded rationales that approach the effectiveness of oracle (label-informed) rationales without access to ground-truth labels during generation?
- Basis in paper: [explicit] "A dataset and metrics for generating concise, object-grounded, label-agnostic rationales are needed."
- Why unresolved: The label-informed (oracle) rationale generator yielded the largest F1 gains, but the agnostic generator showed only moderate similarity to oracle rationales (BERTScore F1 ≈ 0.38), indicating models cannot yet infer "why" without label hints.
- What evidence would resolve it: Develop and benchmark a rationale generation model trained to maximize semantic alignment with human-authored rationales, measuring downstream F1 gains without oracle access.

## Limitations
- High-consensus filtering limits generalization to ambiguous or subjective persuasion cases.
- Label-informed rationale injection is an oracle approach requiring ground truth labels, not deployable in practice.
- Results are based on English-language messages, limiting cross-linguistic generalizability.

## Confidence
- **High confidence:** The recall-oriented bias in VLM persuasiveness judgments and the superiority of key object presence as a predictor (supported by odds ratios >3.0).
- **Medium confidence:** The causal interpretation of human presence over-weighting, as it relies on correlational analysis without ablation studies isolating this feature's contribution.
- **Medium confidence:** The effectiveness of object-grounded rationales, though results depend on the quality of the rationale generator and may not transfer to open-weight models.

## Next Checks
1. **Feature Ablation Study:** Systematically remove human presence, key objects, and color/saliency features from model inputs to quantify their individual contributions to persuasiveness judgments and test the recall-oriented bias mechanism.
2. **Cross-Dataset Generalizability:** Evaluate the same VLM models and feature extraction pipeline on a different persuasion dataset (e.g., ColorBench or multimodal misinformation data) to test whether semantic alignment remains the dominant predictor.
3. **Open-Model Replication:** Replace GPT-5 with open-weight VLMs (Qwen2.5-VL, Gemma3) for the knowledge-injection experiments to verify whether the precision gains from object-grounded rationales are model-agnostic or specific to proprietary systems.