---
ver: rpa2
title: Pruning Large Language Models by Identifying and Preserving Functional Networks
arxiv_id: '2508.05239'
source_url: https://arxiv.org/abs/2508.05239
tags:
- pruning
- functional
- networks
- llms
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses structured pruning of large language models
  (LLMs) to reduce computational cost while preserving performance. Inspired by functional
  brain networks in neuroscience, it proposes to identify and preserve functional
  networks within LLMs using Independent Component Analysis (ICA).
---

# Pruning Large Language Models by Identifying and Preserving Functional Networks

## Quick Facts
- **arXiv ID:** 2508.05239
- **Source URL:** https://arxiv.org/abs/2508.05239
- **Reference count:** 14
- **Primary result:** Outperforms state-of-the-art structured pruning methods on multiple benchmarks while reducing model size by 10-30%

## Executive Summary
This paper introduces a neuroscience-inspired approach to structured pruning of large language models (LLMs) by identifying and preserving functional networks within the model architecture. Drawing from functional brain network theory, the authors apply Independent Component Analysis (ICA) to decompose neuron activation patterns into statistically independent functional networks. By preserving key neurons within these identified networks rather than pruning based on individual importance scores, the method achieves better performance preservation during structured pruning. Experimental results demonstrate superior performance compared to existing methods like FLAP across multiple benchmarks including PIQA, WinoGrande, and HellaSwag while maintaining lower perplexity on Wikitext-2.

## Method Summary
The method treats LLMs as digital brains and applies spatial Independent Component Analysis (CanICA) to neuron activation signals extracted from MLP Gate and Up projections in transformer blocks. The process involves z-score normalization of neuron signals, decomposition into 128 independent components per layer, and thresholding to create binary masks identifying critical neurons. Multiple calibration data batches are processed and their masks aggregated via logical OR to ensure robustness. The final pruning removes neurons not preserved in the aggregated mask, with optional bias compensation. The approach is validated on Vicuna-7B, LLaMA2-7B, and ChatGLM3-6B at 10%, 20%, and 30% pruning rates.

## Key Results
- Outperforms FLAP on 5 out of 7 benchmark tasks at 20% pruning
- Achieves better perplexity scores on Wikitext-2 across all tested pruning rates
- Demonstrates consistent improvements across Vicuna-7B, LLaMA2-7B, and ChatGLM3-6B models
- Shows optimal performance with 128 ICA components, with degradation at higher component counts

## Why This Works (Mechanism)

### Mechanism 1: Preservation of Functional Cohesion
The method assumes that specific capabilities in LLMs rely on coordinated interactions between specific neurons, analogous to cognitive tasks in neuroscience. By identifying these groups via ICA and preserving them collectively, the local functional architecture remains intact even as the model shrinks. This works because artificial neurons form statistically independent functional groups that contribute to the final output.

### Mechanism 2: Spatial Signal Decomposition via CanICA
CanICA decomposes neuron activation signals into independent components, identifying which neurons belong to which functional network. The method treats neuron activations as spatial signals and extracts source matrices representing functional networks. Thresholding these rows generates binary masks for preservation, allowing the isolation of meaningful functional groups.

### Mechanism 3: Robustness through Iterative Aggregation
Running the identification process on small batches of calibration data and aggregating masks via OR operation prevents accidental pruning of neurons only activated by specific input patterns. This approach ensures the final mask is robust across diverse inputs by preserving neurons that participate in any functional network identified across the data distribution.

## Foundational Learning

- **Independent Component Analysis (ICA)**: A mathematical technique for separating mixed signals into statistically independent components. Needed here as the core engine for identifying functional networks; differs from PCA by focusing on independence rather than variance maximization.
- **Structured vs. Unstructured Pruning**: Structured pruning removes entire neurons/channels physically for hardware efficiency, while unstructured pruning zeros out individual weights. Physical removal enables hardware acceleration through reduced memory and computation.
- **Digital Brain Analogy**: The methodology maps LLM neurons to fMRI voxels and token sequences to time series, with neuron activations corresponding to BOLD signals. This analogy frames the pruning problem as preserving functional brain networks.

## Architecture Onboarding

- **Component map:** Calibration Data -> MLP Layer Extraction -> Z-score Normalization -> CanICA Decomposition -> Thresholding -> Mask Aggregation -> Physical Pruning
- **Critical path:** Thresholding of the source signal matrix (S) in the CanICA step determines which functional networks are preserved and directly impacts the final sparsity and performance.
- **Design tradeoffs:** Local (layer-by-layer) vs. Global (across all layers) approach - local used for computational tractability; 128 components found optimal - higher counts cause non-convergence.
- **Failure signatures:** High perplexity with low benchmark scores indicates over-pruning; CanICA non-convergence suggests too many components; bias term instability shows sensitivity to output distribution shifts.
- **First 3 experiments:** 1) Reproduce Vicuna-7B 20% pruning to verify perplexity drop (~16.3 vs FLAP ~16.6); 2) Ablate calibration size (40, 480, 1500 samples) to verify sweet spot; 3) Test component sensitivity (64, 128, 256) on single layer to observe mask density changes.

## Open Questions the Paper Calls Out

- **Global vs Local Strategy:** Would implementing a global functional network strategy across all layers significantly reduce performance loss compared to the layer-by-layer local approach? The paper suggests global strategy would better maintain functional coherence but is computationally infeasible with current ICA implementations.

- **Non-linear Decomposition Methods:** Can non-linear decomposition methods like auto-encoders more effectively capture functional networks for pruning than the linear ICA method used? The authors note ICA may not adequately capture non-linear dependencies and suggest deep neural networks as future direction.

- **Functional Network Interpretability:** Do the functional networks identified via ICA correspond to specific, interpretable linguistic or functional behaviors in LLMs? The authors express interest in assessing the behaviors of these networks to provide mechanistic interpretation.

## Limitations

- The existence of statistically independent functional networks within LLMs remains an empirical observation rather than theoretically proven
- The critical thresholding mechanism for converting source signals to binary masks is underspecified and could significantly affect results
- Claims about hardware efficiency gains lack actual measurements of inference speedup or memory reduction
- All experiments use decoder-only transformer models, limiting generalization to other architectures

## Confidence

- **High Confidence:** Experimental results showing perplexity and benchmark performance improvements are well-documented with specific numbers across multiple models and pruning rates
- **Medium Confidence:** Theoretical justification for why ICA-based preservation works better than magnitude-based pruning is plausible but not rigorously proven
- **Low Confidence:** Claims about computational efficiency gains and hardware acceleration are unsupported by actual measurements

## Next Checks

1. **Cross-Domain Stability Test:** Apply the same pruning methodology to models calibrated on different datasets (Code, Scientific Literature, Conversational Data) and measure performance degradation to assess whether functional networks are domain-specific or universal.

2. **Hardware Benchmarking:** Implement the pruned models on actual GPU hardware and measure wall-clock inference time, memory usage, and throughput compared to baseline and unstructured pruning approaches.

3. **Network Stability Analysis:** Track how the identified functional networks change when varying calibration sample size (40 vs 1500 vs 3200) and pruning rate (10% vs 30%) to quantify the stability of the CanICA decomposition across different operating points.