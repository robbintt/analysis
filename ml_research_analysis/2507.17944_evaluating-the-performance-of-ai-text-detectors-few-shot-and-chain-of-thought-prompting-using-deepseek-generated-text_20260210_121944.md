---
ver: rpa2
title: Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought
  Prompting Using DeepSeek Generated Text
arxiv_id: '2507.17944'
source_url: https://arxiv.org/abs/2507.17944
tags:
- text
- accuracy
- detectors
- content
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of AI text detectors, few-shot,
  and chain-of-thought prompting on DeepSeek-generated text. Six AI detectors were
  tested on 49 human-written and 49 DeepSeek-generated Q&A pairs, with additional
  adversarial paraphrasing and DeepThink modes.
---

# Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text

## Quick Facts
- arXiv ID: 2507.17944
- Source URL: https://arxiv.org/abs/2507.17944
- Authors: Hulayyil Alshammari; Praveen Rao
- Reference count: 40
- Primary result: Six AI detectors tested on 49 human-written and 49 DeepSeek-generated Q&A pairs, with adversarial paraphrasing and DeepThink modes showing significant detection accuracy degradation, while few-shot and chain-of-thought prompting achieved high recall.

## Executive Summary
This study evaluates six commercial AI text detectors (AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, GPTZero) and prompt-based approaches (few-shot and chain-of-thought) on DeepSeek-v3 generated text. The detectors were tested on original human and AI-generated text, followed by adversarial paraphrasing and humanization attacks. Commercial detectors showed near-perfect accuracy on original text but significant degradation under adversarial attacks, with GPTZero dropping to 52% accuracy on humanized text. Few-shot prompting with five examples achieved 96% AI recall and 100% human recall, while chain-of-thought prompting achieved 92.6% AI recall and 90% human recall. The results highlight the evolving challenge of AI detection and the need for adaptive methods as LLM models improve.

## Method Summary
The study used 49 human-written Q&A pairs from pre-LLM era sources (Quora, Academia Stack Exchange) and generated 49 DeepSeek-v3 responses to the same questions. The evaluation included four phases: human text detection, DeepSeek text detection, paraphrased text detection, and DeepThink text detection. Adversarial transformations included paraphrasing with QuillBot (standard mode), DeepThink mode outputs, and paraphrasing DeepThink outputs (standard and humanized modes). Few-shot prompting tested 0-5 examples for classification, while chain-of-thought prompting used five linguistic criteria: generic vs. specific content, emotional authenticity, structural perfection vs. imperfections, neutral/impersonal vs. personal voice, and narrative flow & creative juxtapositions.

## Key Results
- Commercial detectors (Copyleaks, QuillBot) achieved near-perfect accuracy (95-99%) on original DeepSeek text but dropped significantly under adversarial attacks
- Humanization was the most effective attack, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero
- Few-shot prompting with five examples achieved 96% AI recall and 100% human recall
- Chain-of-thought prompting achieved 92.6% AI recall and 90% human recall
- AI Text Classifier and GPT-2 performed poorly overall, with inconsistent results across conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting with ≥2 examples enables accurate AI/human text classification without model fine-tuning
- Mechanism: In-context examples establish decision boundaries by demonstrating stylistic distinctions between human and machine prose within the prompt itself, allowing the LLM to pattern-match against these exemplars
- Core assumption: The LLM being used as a detector has sufficient representational capacity to capture the stylistic variance from minimal examples
- Evidence anchors:
  - [abstract] "Few-shot prompting with five examples achieved 96% AI recall and 100% human recall"
  - [section] "Both the two-shot and three-shot classifiers achieved flawless detection on our dataset... correctly labeled all 24 human-written passages (100% specificity) and all 25 AI-generated samples (100% sensitivity)"
  - [corpus] Neighbor papers (arxiv 2510.16091, 2507.07539) confirm few-shot prompting improves LLM classification across domains, but generalization to unseen LLM generators remains unvalidated
- Break condition: Performance degrades if AI-generated text comes from a fundamentally different architecture or if paraphrasing obscures surface-level stylistic markers the few-shot examples relied upon

### Mechanism 2
- Claim: Chain-of-thought (CoT) prompting with explicit linguistic criteria yields interpretable AI detection with high accuracy
- Mechanism: CoT forces the model to reason through structured criteria (generic vs. specific content, emotional authenticity, structural perfection, tone, narrative flow) before classification, making detection decisions traceable to specific features
- Core assumption: Human and AI text differ systematically along the identified linguistic dimensions, and the model can reliably detect these differences
- Evidence anchors:
  - [abstract] "chain-of-thought prompting achieved 92.6% AI recall and 90% human recall"
  - [section] "The CoT classifier showed a good mix between preserving original writing and spotting AI output... correctly identifying 25 AI-generated passages while just misclassifying 2 as human"
  - [corpus] Corpus evidence on CoT for detection tasks is sparse; related work (arxiv 2503.10095) evaluates CoT for mental health prediction but not text attribution
- Break condition: Assumes the linguistic criteria generalize across LLM families; may fail if newer models explicitly optimize for "human-like" narrative imperfections or emotional voice

### Mechanism 3
- Claim: Adversarial paraphrasing—particularly "humanization" modes—significantly degrades commercial detector accuracy by altering surface statistical patterns
- Mechanism: Detectors rely on perplexity, burstiness, and token distribution patterns; paraphrasing disrupts these signals by introducing human-like variance, causing detectors calibrated on raw LLM outputs to misclassify
- Core assumption: Detection tools are trained primarily on unmodified LLM outputs and lack robustness to distributional shifts introduced by deliberate humanization
- Evidence anchors:
  - [abstract] "The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero"
  - [section] Table V shows DeepThink-paraphrased (Humanize mode) reducing GPTZero from 94.1% to 52%, Copyleaks from 99.7% to 71%, QuillBot from 95.4% to 58%
  - [corpus] Neighbor paper (arxiv 2508.00619) confirms "existing AIG text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough."
- Break condition: Detectors trained on adversarially-augmented datasets may recover accuracy; this study does not test such detectors

## Foundational Learning

- **Concept: Few-shot in-context learning**
  - Why needed here: Understanding how examples in prompts establish classification boundaries without weight updates
  - Quick check question: Given 3 labeled examples of human vs. AI text, can you articulate what features the model might generalize from them?

- **Concept: Recall vs. precision in detection**
  - Why needed here: The paper reports recall at a 50% threshold; understanding false negatives (missed AI) vs. false positives (flagged humans) is critical for detector selection
  - Quick check question: If a detector flags 45/50 AI samples correctly but also flags 10/50 human samples, what are its recall and specificity?

- **Concept: Adversarial robustness in ML**
  - Why needed here: Paraphrasing attacks exploit detector brittleness; understanding distributional shift explains why accuracy collapses post-humanization
  - Quick check question: Why would a detector trained on GPT-3 outputs struggle with paraphrased DeepSeek text?

## Architecture Onboarding

- **Component map:**
  Human/AI text corpus → [Detector API or LLM prompt] → Classification score → Threshold decision → Recall/accuracy metrics
  - Commercial detectors (Copyleaks, GPTZero, QuillBot): Black-box APIs returning probability scores
  - Prompt-based detectors (few-shot/CoT on DeepSeek): LLM as classifier with engineered prompts

- **Critical path:**
  1. Assemble balanced corpus (human pre-LLM era + LLM-generated)
  2. Apply adversarial transformations (paraphrasing, humanization)
  3. Submit to each detector, record scores
  4. Compute recall at 50% threshold; track false positives

- **Design tradeoffs:**
  - Commercial detectors: Higher convenience, subscription costs, token limits, opaque internals
  - Prompt-based detection: Full control, interpretable reasoning, requires prompt engineering effort, depends on host LLM quality

- **Failure signatures:**
  - "AI Text Classifier" returned no results for paraphrased text—API silently fails
  - GPT-2 and Content Detector AI showed inconsistent performance across conditions
  - All top detectors collapsed on humanized DeepThink text (52–71% accuracy)

- **First 3 experiments:**
  1. Replicate the 2-shot classification on a held-out subset of the corpus to verify the "perfect accuracy" claim generalizes
  2. Test whether CoT criteria transfer to other LLM families (e.g., Claude, Gemini) by generating new samples and evaluating the same prompt
  3. Stress-test top detectors with a second-pass humanization to determine if accuracy continues to degrade or plateaus

## Open Questions the Paper Calls Out

- **Open Question 1**: What criteria should guide optimal exemplar selection for few-shot AI text detection?
  - Basis in paper: [explicit] Authors state "careful exemplar selection, not just quantity, is essential to fully capturing the stylistic variance of AI-generated text" after observing diminishing returns beyond 2-3 examples
  - Why unresolved: The study demonstrates few-shot effectiveness but does not investigate what makes certain exemplars more effective than others for detection tasks
  - What evidence would resolve it: Systematic comparison of few-shot performance using different exemplar selection strategies (e.g., diversity-based, domain-matched, difficulty-stratified) on DeepSeek-generated text

- **Open Question 2**: Which specific linguistic features should detectors prioritize to resist humanization attacks?
  - Basis in paper: [explicit] "The most effective adversarial attack was text humanization, which reduced the accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero."
  - Why unresolved: The study quantifies accuracy drops under humanization but does not analyze which features the humanization alters or which features detectors could rely on instead
  - What evidence would resolve it: Linguistic feature analysis comparing original, paraphrased, and humanized DeepSeek text, combined with ablation studies on feature importance for detection

- **Open Question 3**: Can detection methods maintain consistent accuracy across emerging LLMs without requiring per-model re-evaluation?
  - Basis in paper: [inferred] The paper fills a gap for DeepSeek but implies each new LLM (ChatGPT, Copilot, Gemini, DeepSeek) requires separate detector evaluation. The authors note "the evolving challenge of AI detection and the need for adaptive methods as LLM models improve."
  - Why unresolved: No universal detection approach is proposed; current detectors appear to require re-testing for each new LLM release
  - What evidence would resolve it: Cross-model evaluation where detectors trained or calibrated on one LLM (e.g., GPT-4) are tested on multiple other LLMs including DeepSeek without retraining

- **Open Question 4**: How well do these findings generalize to non-Q&A text domains and non-English languages?
  - Basis in paper: [inferred] The study uses only 49 Q&A pairs from specific domains (academia, business, history, life, entertainment). Related work section notes "detectors' accuracy decreased when tested with code, languages other than English, and paraphrased text."
  - Why unresolved: The constrained domain and format limit claims about detector performance on academic essays, creative writing, technical documentation, or multilingual content
  - What evidence would resolve it: Replication of the evaluation methodology across diverse text genres and languages using DeepSeek-generated samples

## Limitations

- Perfect accuracy claims for few-shot (2-5 examples) and high recall for CoT prompting may not generalize to other LLM families or adversarial attacks beyond paraphrasing
- The study focuses only on surface-level paraphrasing without testing more sophisticated attacks like semantic-preserving rewrites or style transfer
- Commercial detector performance is highly variable and subject to API updates, making exact replication challenging
- The study does not test detectors trained on adversarially-augmented data, which would be necessary to evaluate true robustness

## Confidence

- **High confidence**: Original text detection accuracy for commercial detectors (Copyleaks/QuillBot near-perfect), basic few-shot prompting effectiveness (2+ examples achieve strong results)
- **Medium confidence**: CoT prompting criteria generalization, adversarial paraphrasing impact (though paraphrasing effectiveness is well-established in literature)
- **Low confidence**: Perfect accuracy claims for specific few-shot configurations, generalizability across LLM families and more sophisticated attacks

## Next Checks

1. Replicate few-shot classification on a held-out corpus subset to verify the perfect accuracy claims generalize beyond the reported dataset
2. Test CoT prompting criteria on samples from other LLM families (Claude, Gemini) to evaluate cross-model generalization
3. Apply a second-pass humanization attack to determine if detector accuracy continues degrading or plateaus at the reported 52-71% range