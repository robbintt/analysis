---
ver: rpa2
title: Building a Functional Machine Translation Corpus for Kpelle
arxiv_id: '2505.18905'
source_url: https://arxiv.org/abs/2505.18905
tags:
- kpelle
- language
- translation
- dataset
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first publicly available English-Kpelle
  dataset for machine translation, containing over 2,000 sentence pairs covering daily
  communication, religious texts, and educational materials. By fine-tuning Meta's
  NLLB model on two versions of the dataset, the authors achieved BLEU scores up to
  30 in the Kpelle-to-English direction, demonstrating that data augmentation improves
  translation quality for this low-resource African language.
---

# Building a Functional Machine Translation Corpus for Kpelle

## Quick Facts
- **arXiv ID**: 2505.18905
- **Source URL**: https://arxiv.org/abs/2505.18905
- **Reference count**: 8
- **Primary result**: First English-Kpelle MT dataset (2,000+ sentence pairs) with NLLB achieving up to 30 BLEU in Kpelle→English direction

## Executive Summary
This paper presents the first publicly available English-Kpelle machine translation dataset, containing over 2,000 sentence pairs across daily communication, religious texts, and educational materials. The authors fine-tuned Meta's NLLB model on two versions of the dataset, achieving BLEU scores up to 30 in the Kpelle-to-English direction. The results demonstrate that data augmentation improves translation quality for this low-resource African language, with performance aligning with NLLB-200 benchmarks on other African languages. The dataset supports broader NLP applications including speech recognition and language modeling.

## Method Summary
The authors created a parallel corpus by manually translating English sentences into Kpelle across three domains: daily communication, religious texts, and educational materials. They fine-tuned Meta's NLLB model on this dataset using standard transformer-based training procedures. Two versions of the dataset were used for fine-tuning, with data augmentation techniques applied to improve model performance. BLEU scores were used as the primary evaluation metric to measure translation quality in both translation directions.

## Key Results
- First publicly available English-Kpelle dataset with 2,000+ sentence pairs
- NLLB model achieved BLEU scores up to 30 in Kpelle-to-English translation
- Data augmentation techniques improved translation quality
- Performance comparable to NLLB-200 benchmarks on other African languages

## Why This Works (Mechanism)
The success stems from leveraging transfer learning through NLLB's pre-trained multilingual model, which provides a strong foundation for low-resource language pairs. The domain-specific corpus covering practical communication scenarios ensures relevance and usability. Data augmentation techniques help compensate for the limited dataset size by creating synthetic training examples that expose the model to varied linguistic patterns. The transformer architecture's attention mechanisms effectively handle the structural differences between English and Kpelle, while the multilingual training objective enables cross-linguistic transfer from related languages.

## Foundational Learning
- **Transfer Learning**: Pre-training on large multilingual corpora provides a strong starting point for low-resource languages - needed because Kpelle lacks extensive training data; check by comparing with random initialization
- **Data Augmentation**: Techniques like back-translation and synonym replacement increase effective dataset size - needed to address the 2,000+ sentence limitation; check by measuring performance gains
- **BLEU Score**: Automated metric for evaluating translation quality by comparing n-gram overlap - needed for reproducible evaluation; check by comparing human vs. automated scores
- **Transformer Architecture**: Self-attention mechanisms handle long-range dependencies in translation - needed for capturing complex linguistic relationships; check by comparing with RNN-based models
- **Low-Resource NLP**: Specialized approaches for languages with limited digital resources - needed due to Kpelle's scarcity of training data; check by measuring resource requirements
- **Multilingual Models**: Joint training across multiple languages enables transfer learning - needed to leverage knowledge from related languages; check by ablating multilingual vs. bilingual training

## Architecture Onboarding

**Component Map**
NLLB pre-trained model -> Fine-tuning pipeline -> Data augmentation module -> BLEU evaluation

**Critical Path**
Data collection → Preprocessing → Fine-tuning → Evaluation → Results analysis

**Design Tradeoffs**
- Dataset size vs. coverage: Limited sentences vs. broader domain representation
- Manual translation vs. automatic methods: Higher quality but slower vs. faster but noisier
- BLEU-only evaluation vs. human assessment: Scalable but potentially incomplete vs. thorough but time-consuming

**Failure Signatures**
- Low BLEU scores indicating poor translation quality
- Domain-specific performance degradation outside training domains
- Overfitting to limited training data showing poor generalization
- Vocabulary coverage issues for out-of-domain terms

**3 First Experiments**
1. Fine-tune NLLB on the full dataset and measure baseline BLEU scores
2. Apply back-translation augmentation and compare performance gains
3. Test model generalization on unseen religious and educational text samples

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Dataset size limited to 2,000+ sentence pairs constrains linguistic diversity coverage
- Evaluation relies primarily on BLEU scores, potentially missing translation quality nuances
- Focus on three specific domains may limit generalizability to other contexts
- Single model architecture (NLLB) without comparison to alternative approaches

## Confidence
**High confidence**: Creation of first English-Kpelle dataset and achievement of measurable BLEU scores (up to 30) are well-supported findings.

**Medium confidence**: Data augmentation effectiveness and competitive performance claims relative to other African languages are reasonable but would benefit from more extensive experimentation and direct comparative analysis.

## Next Checks
1. Conduct human evaluation studies to validate BLEU score improvements and assess translation quality across different Kpelle dialects and linguistic phenomena.

2. Expand the dataset with additional domains and sentence pairs while measuring the relationship between dataset size and translation performance.

3. Compare NLLB performance against other translation architectures (e.g., Transformer variants) and training approaches to establish the optimal methodology for Kpelle machine translation.