---
ver: rpa2
title: Rethinking Repetition Problems of LLMs in Code Generation
arxiv_id: '2505.10402'
source_url: https://arxiv.org/abs/2505.10402
tags:
- code
- generation
- repetition
- test
- stmt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formally defines structural repetition in code generation
  and proposes RPG, a grammar-based decoding approach to mitigate both structural
  and content repetitions. RPG uses a pushdown automaton to detect repetitive patterns
  in grammar rules and strategically decays the likelihood of tokens contributing
  to repetitions.
---

# Rethinking Repetition Problems of LLMs in Code Generation

## Quick Facts
- arXiv ID: 2505.10402
- Source URL: https://arxiv.org/abs/2505.10402
- Reference count: 38
- Key outcome: RPG achieves up to 11.3% improvement in Pass@1 on HumanEval(-ET) and MBPP(-ET) benchmarks while reducing structural repetition metrics (TR-N, TR-S) and improving compiler correctness (CCP)

## Executive Summary
This paper addresses the persistent problem of code repetition in LLM code generation, particularly structural repetition where different code segments share the same grammar-level patterns. The authors propose RPG, a grammar-based decoding approach that uses a pushdown automaton to detect repetitive grammar-rule patterns and applies exponential decay to the likelihood of tokens contributing to these repetitions. The approach is evaluated on a new dataset, CodeRepetEval, and shows significant improvements over existing methods while also reducing generation time compared to greedy decoding.

## Method Summary
RPG operates by first mapping generated tokens to their underlying grammar rules using a pushdown automaton, then detecting repeated grammar-rule subsequences through suffix array and LCP array computation. When repetitions are detected, RPG applies exponential decay to the logits of tokens contributing to the repetition patterns, with the decay factor λ^(count) where count tracks the number of repetitions. The method is implemented as a decoding-time intervention that works with existing LLMs like CodeLlama-7B without requiring model retraining.

## Key Results
- RPG achieves up to 11.3% improvement in Pass@1 on HumanEval(-ET) and MBPP(-ET) benchmarks
- Significantly reduces repetition metrics (TR-N, TR-S) compared to baseline methods
- Improves compiler correctness (CCP) on CodeRepetEval dataset
- Reduces average generation time (13.68s vs 33.87s for greedy on benchmarks)
- Shows consistent performance across different model sizes and programming languages

## Why This Works (Mechanism)

### Mechanism 1: Grammar-Based Structural Abstraction
Mapping generated tokens to grammar rules exposes structural repetitions that content-based methods miss. The pushdown automaton processes each token to reduce it to corresponding grammar rule states, merging adjacent tokens mapping to the same grammar symbol. This transforms token sequences into grammar-rule sequences where structural patterns become detectable regardless of content variation.

### Mechanism 2: Efficient Repetition Detection via Suffix/LCP Arrays
Suffix arrays combined with LCP arrays enable O(n log n) detection of repeated grammar-rule subsequences. The merged grammar-rule sequence is processed through suffix array construction and LCP computation, where positions with LCP[i] > 0 indicate consecutive repeated patterns of length ≥ LCP[i].

### Mechanism 3: Exponential Decay Penalization on Repetition Count
Applying exponential decay λ^(count) to token logits breaks the self-reinforcement loop sustaining structural repetition. Each token contributing to detected repetitions receives a penalty weight that reduces its selection probability, cumulatively forcing the model toward alternative generation paths.

## Foundational Learning

- **Context-Free Grammars and Pushdown Automata**
  - Why needed here: The entire detection mechanism relies on reducing token sequences to grammar rule sequences via PDA transitions
  - Quick check question: Can you trace how the Python grammar rule `if_stmt: 'if' test ':' suite ('elif' test ':' suite)*` would be processed by a PDA token-by-token?

- **Suffix Arrays and Longest Common Prefix Arrays**
  - Why needed here: These data structures enable efficient detection of repeated patterns in the grammar-rule sequence
  - Quick check question: Given string "ababcab", what is the suffix array? Which LCP values would indicate consecutive repetitions?

- **LLM Token Sampling and Logit Manipulation**
  - Why needed here: RPG operates at decoding time by modifying token scores before sampling
  - Quick check question: How does multiplying logits by λ^count differ from subtracting a constant repetition penalty?

## Architecture Onboarding

- **Component map:**
Generated tokens (x₁...xₜ) -> [PDA State Tracker] -> Grammar rule sequence R̂₁:ₜ (merged) -> [Suffix Array Builder] -> Sorted suffix indices -> [LCP Array Computer] -> Consecutive repetition detection -> [Penalty Calculator] -> Count-based exponential decay weights -> [Logit Adjuster] -> Modified scores s'(xₜ) -> [Token Sampler] -> Next token selection

- **Critical path:** The PDA-to-LCP chain must complete within token generation latency. The reported average time reductions suggest the overhead is acceptable and may actually reduce total time by preventing endless generation.

- **Design tradeoffs:**
  - λ selection: Paper uses 0.9 as default. Lower λ = stronger suppression but risk of premature termination; higher λ = weaker suppression with residual repetition
  - Grammar scope: Only statement-level reductions are used (not expression-level), trading detection granularity for computational efficiency
  - Language portability: Requires grammar rules for target language; paper demonstrates Go extension but does not quantify effort

- **Failure signatures:**
  - Premature EOS: Generated code terminates mid-function (suggests λ too low or false positive detection)
  - Residual repetition: Repetition continues despite penalty (suggests λ too high or grammar-rules not capturing pattern)
  - Compilation errors after penalty: Breaking repetition may leave syntactically incomplete code

- **First 3 experiments:**
  1. **Reproduction on single scenario**: Run RPG on CodeRepetEval "Code Generation Benchmarks" subset with CodeLlama-7B, verify TR-S reduction from ~0.64 to ~0.39 per Table 1
  2. **Ablation on λ**: Sweep λ ∈ {0.6, 0.8, 0.9, 0.95, 1.0} on HumanEval, plot Pass@1 vs TR-S to identify optimal operating point (Figure 5 suggests 0.9 but shows room for tuning)
  3. **Cross-language validation**: Adapt RPG to a grammar from a different language (e.g., Java), construct 50 test cases with known repetition patterns, verify detection rates and CCP improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying mechanistic causes of structural self-reinforcement in LLMs during code generation?
- Basis in paper: [explicit] The authors state in Section 9 (Limitations) that "the potential reasons why LLMs induce structural repetitions in code generation remain unclear" and leave this analysis for future work
- Why unresolved: The study focuses on the mitigation strategy (RPG) rather than investigating the internal model weights, attention mechanisms, or training data artifacts that trigger the repetition loop
- What evidence would resolve it: An interpretability analysis identifying specific attention heads or neuron circuits that activate disproportionately during repetitive loops, or a data analysis correlating repetition rates with specific training code structures

### Open Question 2
- Question: How robust is the grammar-based detection mechanism against tokenization misalignments in languages with complex syntax?
- Basis in paper: [inferred] The paper acknowledges in Appendix E that mapping LLM tokens to grammar terminals involves complex scenarios (e.g., one-to-many tokens) but does not quantify failure rates in these edge cases
- Why unresolved: The evaluation primarily focuses on Python and Go, which have relatively clean syntax, leaving the performance on languages with more ambiguous or context-heavy syntax untested
- What evidence would resolve it: An ablation study measuring the detection failure rate (false negatives) on obfuscated code or code using languages like C++ with extensive preprocessor macros or Perl with context-sensitive grammar

### Open Question 3
- Question: Can the penalization decay factor λ be optimized dynamically rather than statically to preserve necessary code patterns?
- Basis in paper: [inferred] Figure 5 and Appendix B show that performance varies significantly with λ, and the authors note "there is still room for further improvements with the better hyper-parameter setup"
- Why unresolved: The current method relies on a fixed decay factor (λ=0.9), which may be too aggressive for some contexts and too lenient for others, potentially suppressing necessary code structures
- What evidence would resolve it: A comparative study where λ is adjusted dynamically based on the repetition count or the semantic similarity of the repeated blocks, showing improved Pass@1 scores over the static baseline

## Limitations

- Grammar coverage limitations: RPG's effectiveness depends on comprehensive grammar rule coverage and may struggle with dynamically typed languages or those with ambiguous syntax
- Computational overhead trade-offs: While RPG reduces generation time in tested scenarios, the suffix array and LCP computation add O(n log n) complexity that could become prohibitive for very long sequences
- Hyperparameter sensitivity: RPG relies on λ=0.9 as a default decay rate, but performance varies significantly with this parameter and lacks systematic hyperparameter tuning across different model sizes

## Confidence

**High confidence**: The core mechanism of grammar-based repetition detection is well-founded and the experimental results on repetition metrics (TR-N, TR-S) are reproducible. The mathematical framework for suffix array detection and exponential decay penalization is sound.

**Medium confidence**: The code generation correctness improvements (Pass@1) are substantial but may be partially attributed to the specific evaluation setup. The computational efficiency claims hold for the tested scenarios but may not generalize. The grammar reduction approach appears effective but has not been systematically validated across diverse language families.

**Low confidence**: The universal applicability of RPG across different LLM architectures, programming languages, and generation contexts has not been established. The long-term effects of exponential decay on generation diversity and creativity remain unexplored.

## Next Checks

1. **Cross-language generalization test**: Implement RPG for a statically-typed object-oriented language (Java/C++) and evaluate on a comparable repetition dataset. Measure detection accuracy and CCP improvement to assess whether the Python-centric approach generalizes to languages with different grammar structures.

2. **Hyperparameter robustness analysis**: Conduct a systematic λ sweep across multiple model sizes (7B, 13B, 34B) and evaluate the Pareto frontier between repetition reduction and code correctness. Report confidence intervals and statistical significance to quantify the reliability of reported improvements.

3. **Real-world deployment stress test**: Apply RPG to an open-source codebase generation task where the model must extend existing functions. Measure not just repetition metrics but also integration correctness, style consistency, and whether the grammar-based penalties interfere with maintaining existing code patterns.