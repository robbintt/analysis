---
ver: rpa2
title: 'A Non-Asymptotic Theory of Seminorm Lyapunov Stability: From Deterministic
  to Stochastic Iterative Algorithms'
arxiv_id: '2502.14208'
source_url: https://arxiv.org/abs/2502.14208
tags:
- have
- theorem
- where
- seminorm
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a non-asymptotic theory for solving fixed-point
  equations with seminorm-contractive operators in both deterministic and stochastic
  settings. The key idea is to leverage the quotient space structure induced by a
  seminorm to extend classical fixed-point and Lyapunov stability theorems to this
  broader setting.
---

# A Non-Asymptotic Theory of Seminorm Lyapunov Stability: From Deterministic to Stochastic Iterative Algorithms

## Quick Facts
- arXiv ID: 2502.14208
- Source URL: https://arxiv.org/abs/2502.14208
- Reference count: 40
- Primary result: Establishes non-asymptotic theory for seminorm-contractive operators in both deterministic and stochastic settings

## Executive Summary
This paper develops a unified framework for analyzing fixed-point iterations when the operator is contractive with respect to a seminorm rather than a norm. The key insight is to leverage the quotient space structure induced by a seminorm to extend classical fixed-point and Lyapunov stability theorems to this broader setting. The theory applies to both deterministic fixed-point iterations and stochastic approximation algorithms with Markovian noise, providing finite-sample convergence rates and Lyapunov stability conditions. The framework is particularly valuable for analyzing average-reward reinforcement learning algorithms like TD(λ) and Q-learning, where traditional Hurwitzness assumptions fail.

## Method Summary
The paper's core approach is to construct a quotient space where the seminorm becomes a true norm, allowing classical fixed-point and Lyapunov stability theorems to be applied. For deterministic algorithms, the authors prove geometric convergence to the kernel of the seminorm using the Banach fixed-point theorem in the quotient space. For stochastic settings, they develop a smoothed Lyapunov function using infimal convolution to handle Markovian noise, providing finite-sample bounds that depend on the mixing time of the underlying Markov chain. The theory is applied to average-reward reinforcement learning by recognizing that the Bellman operator is contractive with respect to the span seminorm, enabling analysis without Hurwitzness assumptions.

## Key Results
- Fixed-point theorem showing geometric convergence to kernel affine subspace for deterministic seminorm-contractive operators
- Finite-sample convergence rates of O(1/k) for decaying stepsizes in stochastic settings
- Lyapunov equation characterization for stability of linear systems with respect to seminorms
- Application to average-reward RL with sample complexity of O(ε⁻²) for TD(λ) and Q-learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Seminorm-contractive operators guarantee geometric convergence to a fixed-point affine subspace, rather than a single point.
- **Mechanism:** The paper constructs a **quotient space** $\mathbb{R}^d/\text{ker}(p)$ where the seminorm $p(\cdot)$ becomes a true norm. An operator $T$ that is contractive with respect to the seminorm in $\mathbb{R}^d$ becomes a standard norm-contraction in this quotient space. The classical Banach fixed-point theorem is then applied in this space to guarantee convergence to a unique equivalence class $[x^*]$, which corresponds to an affine subspace in the original space.
- **Core assumption:** The operator $T$ is a $\gamma$-contraction with respect to the seminorm $p$ (i.e., $p(T(x)-T(y)) \le \gamma p(x-y)$).
- **Evidence anchors:**
  - [abstract] Mentions leveraging "quotient space structure induced by a seminorm."
  - [Section 2.1] Lemma 2.1 explicitly states $(\mathbb{R}^d/\text{ker}(p), p)$ forms a Banach space.
  - [corpus] Corpus evidence regarding "quotient space" methods for seminorms is weak/absent; most related work focuses on standard Lyapunov stability.
- **Break condition:** If the operator $T$ is not contractive with respect to the specific seminorm, or if the kernel $\text{ker}(p)$ is not a linear subspace (violating seminorm definition).

### Mechanism 2
- **Claim:** Finite-sample convergence rates for Markovian Stochastic Approximation (SA) are achieved using a smoothed Lyapunov function.
- **Mechanism:** The analysis relies on a novel Lyapunov function $M_E(\cdot)$ constructed via the **infimal convolution** of the seminorm-square and a smoothing kernel (generalized Moreau envelope). This function approximates $p^2/2$ but is smooth. This allows the derivation of a one-step recursive bound (drift inequality) involving the mixing time of the Markov chain, separating the bias term and the variance term.
- **Core assumption:** The Markov chain $\{Y_k\}$ is uniformly ergodic (mixing); the operator is Lipschitz in the seminorm; martingale difference noise.
- **Evidence anchors:**
  - [Section 4.2] Theorem 4.1 provides the explicit bound $\tilde{O}(1/k)$ for decaying stepsizes.
  - [Section 4.4] Definition 4.2 defines the infimal convolution; Eq. (15) shows the one-step drift inequality derived from it.
  - [corpus] The paper "Bias-Optimal Bounds for SGD: A Computer-Aided Lyapunov Analysis" supports the general viability of Lyapunov analysis for non-asymptotic bounds, though it focuses on SGD.
- **Break condition:** If the mixing time of the underlying Markov chain is infinite (non-ergodic), the bounds derived from the mixing time $t_k$ become invalid.

### Mechanism 3
- **Claim:** Average-reward Reinforcement Learning algorithms can be analyzed as special cases of seminorm-contractive SA.
- **Mechanism:** In average-reward RL, the Bellman operator is typically not a norm-contraction. However, it is a contraction with respect to the **span seminorm** ($p_{\text{span}}(x) = \max x_i - \min x_i$). By mapping the RL problem to the seminorm-SA framework, the paper derives finite-sample guarantees for TD($\lambda$) and Q-learning without requiring the restrictive "Hurwitzness" assumption usually needed for Linear SA or unique fixed points.
- **Core assumption:** The underlying MDP is communicating/unichain to ensure the existence of a stationary distribution and the span-semicontraction property (Assumption 5.1/5.2).
- **Evidence anchors:**
  - [abstract] Explicitly mentions application to average reward RL for TD($\lambda$) and Q-learning.
  - [Section 5.2.2] Theorem 5.1 provides the convergence bounds for TD($\lambda$) derived from the general SA theory.
  - [corpus] The paper "Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning..." in the corpus directly addresses this specific application area, confirming the relevance of the mechanism.
- **Break condition:** If the MDP structure prevents the Bellman operator from being a span-semicontraction (e.g., multichain average reward without proper handling), the specific rates may not hold.

## Foundational Learning

- **Concept: Seminorms and Kernels**
  - **Why needed here:** The entire theory relies on relaxing the "positive definiteness" of a norm. A seminorm allows non-zero vectors to have zero "length" if they lie in the kernel. This is essential for modeling problems like average-reward RL where the solution is unique only up to a constant (the kernel of the span seminorm).
  - **Quick check question:** Why does $p(x)=0$ not imply $x=0$ in a seminorm, and what does the set $\{x \mid p(x)=0\}$ represent geometrically?

- **Concept: Quotient Space ($\mathbb{R}^d/E$)**
  - **Why needed here:** To recover the power of the Banach Fixed-Point Theorem. The paper maps the original "messy" space where convergence is weak to a quotient space where the seminorm becomes a norm and the operator becomes a strict contraction.
  - **Quick check question:** If $E$ is the kernel of a seminorm, how is the distance between two elements $[x]$ and $[y]$ in the quotient space $\mathbb{R}^d/E$ defined?

- **Concept: Mixing Time ($t_{\delta}$)**
  - **Why needed here:** In stochastic settings with Markovian noise (not i.i.d.), the algorithm cannot simply assume fresh samples. Mixing time quantifies how long one must wait for the Markov chain to "forget" its past, dictating the effective step-size constraints and convergence speed.
  - **Quick check question:** How does the mixing time $t_k$ relate to the step-size $\alpha_k$ in the convergence bounds of Theorem 4.1?

## Architecture Onboarding

- **Component map:** Deterministic Core -> Stochastic Layer -> Analysis Engine -> Application Wrapper
- **Critical path:**
  1.  **Identify Seminorm:** For your specific problem (e.g., linear system, RL), find a seminorm $p$ such that your operator is contractive.
  2.  **Check Stability:** Verify if the "unstable" modes of your system (eigenvalues $\ge 1$) lie within the kernel of your chosen seminorm (Theorem 3.1).
  3.  **Select Stepsize:** Choose constant $\alpha$ (for geometric convergence to a neighborhood) or decaying $\alpha_k \approx 1/k$ (for exact convergence at $\tilde{O}(1/k)$ rate).
  4.  **Monitor Convergence:** Track the seminorm of the error $p(x_k - x^*)$, not the standard norm, as vector components may diverge.

- **Design tradeoffs:**
  - **Relaxed Assumptions vs. Granularity:** You gain the ability to analyze non-Hurwitz systems or average-reward RL (relaxed assumptions), but you only get convergence guarantees in the seminorm sense (reduced granularity). The iterate might diverge in the standard Euclidean norm while converging in the seminorm.
  - **Constant vs. Decaying Stepsizes:** Constant stepsizes offer fast initial geometric decay but plateau at an asymptotic error proportional to $\alpha$. Decaying stepsizes converge to the true solution but slower ($\tilde{O}(1/k)$).

- **Failure signatures:**
  - **Diverging Components:** You observe specific coordinates of $x_k$ growing to infinity despite the loss decreasing. *Diagnosis:* This is expected behavior for seminorm-convergent systems (see Section 2.2 example). *Fix:* Apply the projection step onto the kernel $E$ mentioned in Section 4.2 to stabilize the iterate.
  - **No Convergence:** The seminorm error $p(x_k - x^*)$ does not decrease. *Diagnosis:* The operator is not contractive w.r.t the chosen seminorm, or the Markov chain is not mixing fast enough relative to the step-size.

- **First 3 experiments:**
  1.  **Validation of Linear Theory:** Implement a simple linear system $x_{k+1} = Ax_k$ where $A$ is unstable in standard norm but stable in a seminorm (e.g., matrix with spectral radius $>1$ but one eigenvector in the kernel). Plot $p(x_k)$ vs. $\|x_k\|_2$ to verify geometric vs. divergent behavior.
  2.  **Average Reward TD($\lambda$):** Implement Algorithm 1 on a simple MDP (e.g., RiverSwim). Compare convergence speed (in span seminorm) between constant stepsize and $1/k$ decaying stepsize to validate Theorem 5.1.
  3.  **Projection Ablation:** Run the Q-learning experiment (Algorithm 2) *without* the suggested projection step. Observe if the values drift. Then implement the projection step (minimization over kernel) to verify empirical stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the finite-sample analysis be extended to asynchronous Q-learning in the average reward setting?
- Basis in paper: [explicit] Section 5.3.1 explicitly states that characterizing the convergence behavior of asynchronous Q-learning in this setting "remains a direction for future research."
- Why unresolved: The current analysis relies on synchronous updates; asynchronous sampling introduces dependencies that complicate the seminorm-contraction analysis.
- What evidence would resolve it: A convergence proof or finite-sample bounds for asynchronous Q-learning using the seminorm framework.

### Open Question 2
- Question: How can the projection step required for empirical stability be efficiently computed for general seminorms?
- Basis in paper: [inferred] Section 4.2 notes that solving for the argmin in the projection step "is not always straightforward, depending on the seminorm and the kernel space."
- Why unresolved: While a closed-form solution exists for the span seminorm in Q-learning, a general efficient method for other seminorms is not provided.
- What evidence would resolve it: An algorithm or complexity analysis demonstrating efficient projection computation for a general class of seminorms.

### Open Question 3
- Question: Can the theory be adapted to handle Markovian noise that is geometrically ergodic rather than uniformly ergodic?
- Basis in paper: [inferred] Assumption 4.2(2) requires the Markov chain to be uniformly ergodic to control the mixing time.
- Why unresolved: Uniform ergodicity is a stronger assumption than often necessary for RL environments (which may only be geometrically ergodic).
- What evidence would resolve it: A modification of the mixing time analysis in Section 4.1 that accommodates geometric rates.

## Limitations

- The theory fundamentally relies on the operator being contractive with respect to a seminorm, which may not hold for all practical problems
- The convergence guarantees are in seminorm rather than standard norm, meaning iterates may diverge in standard coordinates while converging in the seminorm sense
- For stochastic settings, the bounds depend on mixing time assumptions that may be difficult to verify in practice

## Confidence

- **High confidence:** Deterministic fixed-point theorem (Theorem 2.1), Lyapunov equation characterization (Theorem 3.1)
- **Medium confidence:** Stochastic approximation bounds (Theorem 4.1), as they depend on mixing time assumptions that may be conservative
- **Medium confidence:** RL applications, as they require the specific structure of average-reward problems

## Next Checks

1. **Empirical verification of seminorm vs norm convergence:** Implement the linear system example from Section 2.2 and plot both $p(x_k)$ and $\|x_k\|_2$ to demonstrate the divergence in standard norm while converging in seminorm

2. **Mixing time verification:** For a simple Markov chain example, empirically estimate mixing time and compare against theoretical bounds required by Theorem 4.1

3. **Hurwitzness relaxation test:** Implement TD(λ) on an average-reward MDP with known Hurwitzness violations to verify convergence where standard linear SA would fail