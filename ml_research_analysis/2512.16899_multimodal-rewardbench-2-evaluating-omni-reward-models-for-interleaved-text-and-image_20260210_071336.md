---
ver: rpa2
title: 'Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text
  and Image'
arxiv_id: '2512.16899'
source_url: https://arxiv.org/abs/2512.16899
tags:
- image
- response
- text
- reasoning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Multimodal RewardBench 2 (MMRB2) addresses the lack of comprehensive\
  \ evaluation for reward models on omni models handling interleaved text and images.\
  \ The benchmark spans four tasks\u2014text-to-image generation, image editing, interleaved\
  \ generation, and multimodal reasoning\u2014with 1,000 expert-annotated preference\
  \ pairs per task."
---

# Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image

## Quick Facts
- **arXiv ID**: 2512.16899
- **Source URL**: https://arxiv.org/abs/2512.16899
- **Reference count**: 40
- **Primary result**: MMRB2 benchmark shows state-of-the-art multimodal reward models achieve 75-80% human agreement on challenging interleaved text-image evaluation tasks, with significant gaps remaining compared to human performance.

## Executive Summary
Multimodal RewardBench 2 (MMRB2) addresses the critical gap in evaluating reward models for omni models that handle interleaved text and images. The benchmark provides 1,000 expert-annotated preference pairs across four tasks: text-to-image generation, image editing, interleaved generation, and multimodal reasoning. Using an ensemble filtering strategy with nine multimodal judges, MMRB2 curates high-consensus preference pairs that maximize discriminative signal. Evaluation reveals substantial room for improvement: while Gemini 3 Pro achieves 75-80% accuracy, commonly-used GPT-4o reaches only 59%, and the best open-source model Qwen3-VL-32B achieves 64%.

## Method Summary
MMRB2 evaluates reward models through four multimodal tasks using 1,000 preference pairs per task, curated via ensemble filtering where nine diverse judges evaluate each pair in both orderings. Pairs with ≥90% judge agreement are discarded as too easy. Human experts annotate remaining pairs using 7-point Likert scales, with majority vote determining ground truth preferences. The benchmark includes responses from 23 models and specialized agents, with ground truth used for multimodal reasoning and human preference for generation tasks. Evaluation employs positional consistent dual evaluation to mitigate bias, and validation shows strong correlation between benchmark scores and downstream Best-of-N sampling performance.

## Key Results
- Gemini 3 Pro achieves 75-80% accuracy on MMRB2, while GPT-4o attains only 59% and Qwen3-VL-32B reaches 64%
- Judges show 27.7-49.3% performance gaps favoring image-containing responses in multimodal reasoning tasks
- Same-model pairs (within one model's outputs) are 5-13 percentage points harder than different-model pairs
- Best-of-N sampling with MMRB2 scores strongly correlates with downstream task success, validating the benchmark
- Test-time scaling provides only modest improvements (0.8-1.2%) for multimodal reward models versus text-only LLMs

## Why This Works (Mechanism)

### Mechanism 1
Ensemble filtering concentrates human annotation effort on informative preference pairs where models disagree. Nine multimodal judges evaluate each pair in both orderings, discarding pairs where ≥90% agree as "too easy." This leaves a filtered set where human annotation provides maximal signal about fine-grained model differences.

### Mechanism 2
Positional consistent dual evaluation mitigates systematic position bias in judge predictions. Each pair is evaluated twice—once in original order and once reversed. Both judgments are retained as independent data points, penalizing judges with strong position bias (preferring whichever response appears first).

### Mechanism 3
Preference training improves reward model accuracy but trained models underperform frontier MLLM judges due to distribution shift. Models trained on human preference data (e.g., EditReward) outperform untrained baselines (+9.4% for EditReward vs. Qwen2.5-VL-7B on editing) but underperform larger MLLM judges because training data comes from earlier-generation models.

## Foundational Learning

- **Reward models and RLHF pipeline**: Why needed—MMRB2 evaluates reward models that score candidate responses for quality alignment with human preferences—critical for RLHF post-training. Quick check—Can you explain how a reward model's output is used during PPO training of a language model?
- **Interleaved multimodal generation**: Why needed—Unlike text-to-image or image-to-text tasks, interleaved generation requires models to produce arbitrary sequences of text and images (e.g., visual storytelling with captions). Quick check—What makes evaluating interleaved outputs harder than evaluating single-modality outputs?
- **Best-of-N sampling as downstream validation**: Why needed—The paper validates MMRB2 by showing correlation between benchmark scores and downstream task performance when using reward models for Best-of-N selection. Quick check—In Best-of-N sampling, how does the choice of reward model affect the expected quality of the selected output?

## Architecture Onboarding

- **Component map**: Prompt curation (stratified sampling from 21 benchmarks + synthesized tasks) → Response generation (23 models/agents across 4 tasks) → Ensemble filtering (9 judges × 2 orderings → filter pairs with ≥90% agreement) → Human annotation (3 annotators per pair via Surge AI; 7-point Likert → binary preference) → Pair construction (reasoning: correct answer + correct reasoning vs. alternatives) → Evaluation (positional consistent dual evaluation → judge-human agreement)

- **Critical path**: Start with text-to-image task (simplest modality, largest existing literature) → Validate your judge implementation against reported GPT-4o baseline (~60% agreement) → Verify positional consistency: check that reversing pair order produces compatible judgments → Run correlation analysis with downstream Best-of-N to confirm benchmark validity

- **Design tradeoffs**: Same-model vs. different-model pairs (57.4% same-model for fine-grained discrimination vs. 42.6% different-model for capability gap detection) → Ground truth vs. preference-based evaluation (reasoning uses ground truth for objective verification vs. generation uses human preference for practical relevance) → Judge ensemble diversity vs. consistency (9 diverse judges reduce systematic bias but increase filtering variance)

- **Failure signatures**: Position bias (judge systematically prefers first response → dual evaluation agreement drops) → Modality bias in reasoning (judge prefers image-containing responses regardless of correctness → 27.7-49.3% gaps) → Same-model discrimination failure (judge struggles with fine-grained differences within one model's outputs → 5-13% gap vs. different-model pairs) → Distribution shift (reward model trained on older data underperforms on frontier model outputs → VQAScore 58.3% vs. Gemini 3 Pro 74.4% on text-to-image)

- **First 3 experiments**: 1) Reproduce baseline judge performance: Run GPT-4o and Qwen3-VL-32B on text-to-image subset (1,000 pairs). Target: ~60% and ~64% agreement respectively. Deviation >5% suggests implementation issue. 2) Validate positional consistency: For each judge, compute agreement between forward and reverse orderings on same pairs. Target: <10% disagreement indicates acceptable position bias. 3) Test modality bias in reasoning task: Compare judge accuracy on text-only preferred vs. image-containing preferred pairs. High gap (>30%) indicates modality bias requiring prompt engineering or calibration.

## Open Questions the Paper Calls Out

- **What alternative scaling methods can substantially improve multimodal reward model performance beyond the modest gains (0.8–1.2%) achieved through test-time sampling with majority voting?** The authors show K=9 majority voting yields only ~1% improvement for GPT/Gemini and none for Qwen3-VL, unlike text-only LLMs where such scaling is highly effective.

- **How can the strong bias toward image-containing responses in multimodal reasoning tasks be mitigated?** The analysis reveals judges exhibit a strong bias toward responses that include images, with performance gaps of 27.7–49.3% between pairs where annotators preferred image-containing responses and those where the preferred response contained only text.

- **What approaches can close the 5–13 percentage point gap between judge performance on same-model pairs versus different-model pairs?** Current judges struggle with subtle quality distinctions within a single model's output distribution, a capability essential for model self-improvement via Best-of-N sampling.

## Limitations

- **Distribution shift challenge**: Preference-trained reward models underperform frontier MLLM judges because training data comes from earlier-generation models, creating a fundamental gap that simple fine-tuning may not resolve
- **Modality bias in reasoning**: Judges show strong bias toward image-containing responses regardless of correctness, with 27.7-49.3% performance gaps that current evaluation methods cannot fully mitigate
- **Same-model discrimination difficulty**: Judges achieve 5-13 percentage points lower accuracy on same-model pairs versus different-model pairs, indicating difficulty with fine-grained quality distinctions within a single model's outputs

## Confidence

- **High confidence**: Human performance bounds (>90% accuracy), judge accuracy rankings (Gemini 3 Pro > GPT-5 > GPT-4o), and the core finding that existing reward models underperform frontier MLLM judges
- **Medium confidence**: The ensemble filtering methodology's effectiveness, the positional consistency mitigation, and the downstream correlation claims (due to potential confounding factors in Best-of-N evaluation)
- **Low confidence**: The generalizability of findings beyond the specific tasks and models evaluated, and whether the 1,000 pairs per task provide sufficient coverage for comprehensive reward model evaluation

## Next Checks

1. **Position bias verification**: Run positional consistent dual evaluation on a subset of pairs and measure agreement between forward and reverse orderings. Target: <10% disagreement rate indicates successful bias mitigation.

2. **Modality bias analysis**: Separate evaluation accuracy for multimodal reasoning pairs where human preference was for text-only vs. image-containing responses. Target: Gap >30% confirms the reported modality bias requiring calibration.

3. **Distribution shift test**: Compare preference-trained reward models (EditReward, HPSv3) against untrained baselines on a held-out test set from frontier models only. Target: Trained models match or exceed frontier judge performance, validating that distribution shift explains the current accuracy gap.