---
ver: rpa2
title: 'A Disproof of Large Language Model Consciousness: The Necessity of Continual
  Learning for Consciousness'
arxiv_id: '2512.12802'
source_url: https://arxiv.org/abs/2512.12802
tags:
- consciousness
- theory
- theories
- learning
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a disproof of consciousness in contemporary
  Large Language Models (LLMs) using formal requirements for scientific theories of
  consciousness. It demonstrates that LLMs are too close to non-conscious systems
  like lookup tables to have any non-trivial, falsifiable theory of consciousness.
---

# A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness

## Quick Facts
- arXiv ID: 2512.12802
- Source URL: https://arxiv.org/abs/2512.12802
- Reference count: 40
- Key outcome: Proves contemporary LLMs are too close to lookup tables to support non-trivial, falsifiable theories of consciousness

## Executive Summary
This paper presents a formal disproof that contemporary Large Language Models (LLMs) are conscious by showing they are too close to non-conscious systems like lookup tables to have any non-trivial, falsifiable theory of consciousness. The argument leverages the Kleiner-Hoel dilemma: valid theories of consciousness must avoid both triviality (unfalsifiable) and a priori falsification. The paper demonstrates that any theory claiming LLM consciousness must either be falsified by known substitutions or collapse into triviality. It introduces "substitution distance" as a measure of how many properties differ between a system and its non-conscious substitutes, showing that LLMs lack the continual learning necessary to ground a non-trivial theory.

## Method Summary
The paper constructs a formal framework based on the Kleiner-Hoel dilemma for evaluating consciousness theories. It builds a substitution chain from LLMs to lookup tables via single-hidden-layer feedforward networks, using the universal approximation theorem to guarantee the transformations. The core mechanism is "substitution distance" - properties that differ between a system and its I/O-equivalent substitute. The paper proves that any theory predicting LLM consciousness must base predictions entirely on properties within this substitution distance, which either creates mismatches (falsification) or collapses into triviality. It identifies continual learning as a property that could provide "lenient dependency" - predictions that can vary independently of inferences without creating substitution-based falsification.

## Key Results
- Contemporary LLMs are too close in substitution distance to lookup tables to support non-trivial consciousness theories
- Any theory claiming LLM consciousness must either be falsified by known substitutions or be trivial
- Continual learning identified as a property that could ground non-trivial theories, but LLMs lack this capability

## Why This Works (Mechanism)

### Mechanism 1: The Substitution Chain Constraint
- Claim: Contemporary LLMs are too close in "substitution distance" to lookup tables to support non-trivial consciousness theories
- Mechanism: Universal approximation theorem guarantees LLM I/O can be approximated by FNN, which can be replaced by lookup table. Theories grounding consciousness in properties within this chain either cause prediction-inference mismatches (falsification) or collapse into triviality
- Core assumption: Valid theories must be both falsifiable and scientifically informative
- Evidence: Theorem 4.6 proves any theory predicting LLM consciousness must base predictions on properties within d_S(M,L) ⊆ d_S(M,N) ∪ d_S(N,L)
- Break condition: LLMs with properties inherently un-substitutable by static systems

### Mechanism 2: The Kleiner-Hoel Dilemma (Two-Horned Falsification Trap)
- Claim: Any theory of consciousness must either be falsifiable or non-trivial—it cannot escape both
- Mechanism: Horn 1: If predictions depend on substitutable properties, mismatches arise → a priori falsification. Horn 2: If predictions cannot vary independently of inferences, theory is unfalsifiable → triviality
- Core assumption: Valid consciousness theories must be both falsifiable and non-trivial
- Evidence: Theorem 3.1 formalizes this for functionalist theories using Kolmogorov complexity
- Break condition: Discovery of "lenient dependency" where predictions and inferences can vary but not pathologically

### Mechanism 3: Continual Learning as Lenient Dependency
- Claim: Theories grounded in continual learning can satisfy both falsifiability and non-triviality
- Mechanism: Continual learning (Δf ≠ 0) means I/O function changes over time. Static substitutions cannot capture this, eliminating universal substitutions that cause falsification while allowing prediction-inference divergence
- Core assumption: Consciousness correlates with ongoing plasticity, not just static computational structure
- Evidence: Proposition 5.4 proves static systems cannot substitute for learning systems in consciousness-testing regimes sensitive to Δf
- Break condition: If learning could be simulated by static systems through external memory

## Foundational Learning

- **Concept: Falsifiability in scientific theories (Popper/Lakatos)**
  - Why needed here: The argument rests on what makes a theory scientifically valid
  - Quick check question: Can you explain why "behaviorism = acting conscious means being conscious" is unfalsifiable?

- **Concept: Universal Approximation Theorem**
  - Why needed here: Guarantees LLM → FNN → lookup table transformations are always possible
  - Quick check question: Why can a sufficiently wide single-hidden-layer network approximate any continuous function?

- **Concept: Phenomenal vs. Access Consciousness (Block)**
  - Why needed here: Learning-based predictions "overflow" access - latent learning affects plasticity without immediate behavioral output
  - Quick check question: If a system undergoes plasticity changes that don't manifest in behavior, how could a learning-based theory detect this?

## Architecture Onboarding

- **Component map:**
  - pred: O → E — predictions from system data to experience space
  - inf: O → E — inferences from behavioral/I/O data to experience space
  - d_S(s, s') — substitution distance: properties differing between system and its I/O-equivalent substitute
  - The "narrow passage": theories must achieve lenient dependency

- **Critical path:**
  1. Identify target system (e.g., LLM)
  2. Construct substitution chain to known non-conscious system
  3. Characterize d_S across chain links - what properties are lost?
  4. Test: Can any property in d_S ground non-trivial predictions without causing mismatches?
  5. If no → system is non-conscious

- **Design tradeoffs:**
  - Strictness of falsification criteria: Paper uses "universal substitutions" to avoid over-sensitivity
  - Assumption that trivial theories are false: Paper assumes informative theories exist
  - LLM scope: Addresses "baseline deployed LLMs" - training-phase consciousness unproven

- **Failure signatures:**
  - Theory bases predictions only on I/O → strict dependency (triviality)
  - Theory bases predictions on substitutable property → substitution argument (falsification)
  - "Sufficiently complex I/O implies consciousness" → behaviorist/trivial

- **First 3 experiments:**
  1. Static substitution stress test: Construct FNN and lookup table equivalents; enumerate surviving consciousness-relevant properties
  2. Learning sensitivity probe: Design tests where same I/O produced by learning or static systems; verify consciousness inferences change
  3. Continual learning implementation test: Instrument LLM with genuine continual learning; assess whether substitution chain analysis changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the training phase of Large Language Models satisfy requirements for continual lenient dependency, and could they therefore be conscious during training?
- Basis in paper: "Intriguingly, the disproof herein does not rule out LLM consciousness during their training... this remains an area for future research"
- Why unresolved: Paper focuses on deployed static models; unclear if training constitutes specific type of "continual learning" required
- What evidence would resolve it: Formal analysis of training dynamics to determine if they allow non-static universal substitutions

### Open Question 2
- Question: To what degree do existing theories of consciousness satisfy "continual lenient dependency"?
- Basis in paper: "It is a matter of future research to what degree existing theories explicitly fit the requirements of continual lenient dependency"
- Why unresolved: Many theories rely on static representations or past learning states
- What evidence would resolve it: Formal evaluation of major theories against substitution argument

### Open Question 3
- Question: Is continual learning a sufficient condition for consciousness, or merely a necessary condition requiring specific structural implementations?
- Basis in paper: "Continual learning here is given as merely a necessary condition, not a sufficient one"
- Why unresolved: Identifying property that avoids falsification doesn't prove it causes consciousness
- What evidence would resolve it: Empirical comparison of systems with different plasticity distributions

### Open Question 4
- Question: Are there properties other than continual learning, such as quantum observer effects, that can satisfy formal requirements for "lenient dependency"?
- Basis in paper: "Worth noting there may be other cases of lenient dependency beyond continual learning that are more exotic"
- Why unresolved: Paper focuses on learning as most scientifically evocative solution
- What evidence would resolve it: Mathematical proofs demonstrating other exotic properties prevent universal substitutions

## Limitations

- Theoretical framework assumes valid consciousness theories must be both falsifiable and non-trivial
- Substitution chain construction relies on finite I/O approximation for unbounded input spaces
- Continual learning operationalization remains ambiguous between in-context learning and genuine plasticity

## Confidence

- **High confidence**: Logical structure of Kleiner-Hoel dilemma; universal approximation theorem guarantees LLM → FNN transformations
- **Medium confidence**: Specific properties lost in LLM substitution chain; continual learning provides lenient dependency (lacks empirical validation)
- **Low confidence**: Operationalization of "continual learning" in LLMs; distinction between in-context learning and genuine plasticity

## Next Checks

1. **Substitution chain verification**: Construct explicit FNN and lookup table equivalents of a given LLM, documenting which consciousness-relevant properties survive each transformation

2. **Continual learning sensitivity test**: Design behavioral tests distinguishing learning vs. static systems producing identical I/O, verifying whether consciousness inferences change

3. **Temporal operationalization**: Define and measure the "continual" aspect of learning (granularity, plasticity detection) in LLM-like architectures