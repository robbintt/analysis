---
ver: rpa2
title: 'Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression'
arxiv_id: '2505.18166'
source_url: https://arxiv.org/abs/2505.18166
tags:
- pruning
- accuracy
- self-distillation
- fine-tuning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two post-pruning retraining strategies for
  compressing large language models (LLMs) for edge deployment. The authors use a
  fixed layer-wise L2-norm pruning on only the MLP blocks of the OLMo2-7B-SFT model
  and then compare two recovery approaches: (i) fine-tuning with cross-entropy loss
  (L2PFT), which requires labeled data, and (ii) self-distillation with KL-divergence
  (L2PSD), which uses only teacher logits.'
---

# Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression

## Quick Facts
- **arXiv ID:** 2505.18166
- **Source URL:** https://arxiv.org/abs/2505.18166
- **Reference count:** 29
- **Primary result:** KL-divergence self-distillation outperforms cross-entropy fine-tuning by 3-5% accuracy at 50% parameter retention in MLP-pruned LLMs

## Executive Summary
This paper compares two post-pruning retraining strategies for compressing large language models (LLMs) for edge deployment. The authors use a fixed layer-wise L2-norm pruning on only the MLP blocks of the OLMo2-7B-SFT model and then compare two recovery approaches: (i) fine-tuning with cross-entropy loss (L2PFT), which requires labeled data, and (ii) self-distillation with KL-divergence (L2PSD), which uses only teacher logits. Under identical pruning schedules on the CommonsenseQA dataset, KL-based self-distillation consistently matches or exceeds CE fine-tuning, achieving 3–5% higher test accuracy at 50% retention. The results show that the choice of loss function is critical for compressed model recovery, with self-distillation offering superior performance and reduced prediction uncertainty, making it a practical alternative for resource-constrained, data-sparse edge environments.

## Method Summary
The study employs structured L2-norm pruning on MLP blocks (UP, GATE rows; DOWN columns) of OLMo2-7B-SFT, progressively reducing parameters from 100% to 50% retention. Two recovery strategies are compared: (1) L2PFT - iterative fine-tuning with cross-entropy loss using original labeled data, and (2) L2PSD - iterative self-distillation with KL-divergence using precomputed teacher logits. Both methods use the same pruning schedule and are evaluated on CommonsenseQA with two-shot prompting. The KL loss includes temperature scaling (T) and T² normalization for stable training. Models are evaluated at multiple retention levels with data ablations (25%, 50%, 75%, 100% of training data), measuring test accuracy, train accuracy, inference FLOPs, wall-clock time, and Shannon entropy of output distributions.

## Key Results
- KL-based self-distillation matches or exceeds CE fine-tuning in test accuracy, generally by 3-5% at 50% parameter retention
- Self-distillation produces flatter output entropy curves, maintaining better-calibrated uncertainty as compression increases
- CE fine-tuning shows overfitting patterns with larger train-test accuracy gaps and increasingly confident (low-entropy) predictions at higher retention levels
- Self-distillation achieves these gains without requiring labeled data, only teacher logits from the frozen model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** KL-divergence self-distillation outperforms cross-entropy fine-tuning for post-pruning recovery in compressed LLMs.
- **Mechanism:** Soft targets from teacher logits preserve inter-class relationships that hard labels discard. The KL loss transfers this richer information structure to the pruned student, enabling better generalization when capacity is constrained.
- **Core assumption:** The unpruned teacher model retains sufficient accuracy to provide meaningful soft targets.
- **Evidence anchors:** [abstract] "KL-based distillation matches or exceeds CE fine-tuning in test accuracy"; [Section 5] "KL-divergence-based self-distillation ultimately