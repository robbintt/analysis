---
ver: rpa2
title: 'OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for
  Diverse Scenarios'
arxiv_id: '2501.01384'
source_url: https://arxiv.org/abs/2501.01384
tags:
- dialogue
- spoken
- data
- audio
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniChat, a multi-turn spoken dialogue system
  that leverages scalable synthetic data to handle diverse real-world conversation
  scenarios including emotional expressions, audio events, and background music. The
  authors created ShareChatX, the first large-scale synthetic spoken dialogue dataset
  spanning these complex scenarios, by using large language models to generate dialogue
  scripts and controllable text-to-speech to synthesize speech.
---

# OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data for Diverse Scenarios

## Quick Facts
- arXiv ID: 2501.01384
- Source URL: https://arxiv.org/abs/2501.01384
- Reference count: 7
- Primary result: OmniChat achieves state-of-the-art on DailyTalk (METEOR: 14.24, BERTScore: 86.99) and superior emotion prediction (F1e: 75.46) using scalable synthetic data

## Executive Summary
OmniChat is a multi-turn spoken dialogue system that generates context-aware responses from speech input across diverse real-world scenarios including emotional expressions, audio events, and background music. The system leverages a large-scale synthetic dataset (ShareChatX) created using LLMs and controllable TTS, combined with a heterogeneous feature fusion module (Mix-Former) that optimally integrates speech content, emotion, and audio features. Through extensive experimentation, the authors demonstrate that mixing synthetic and real data at a 20% sampling ratio yields optimal performance, with speech-only models outperforming ASR-based approaches at sufficient scale.

## Method Summary
OmniChat uses Llama-3.1-8B-Instruct as a frozen backbone with LoRA adapters, processing raw audio (16kHz) through three parallel expert encoders: Whisper for speech content, emotion2vec for emotional representations, and BEATs for non-speech audio. The Mix-Former module employs window-level Q-Transformers with learned weights to fuse these heterogeneous features into LLM-compatible representations. Training uses a dual-stage approach: pre-training on synthetic data (ShareChatX) followed by fine-tuning on real-world data (DailyTalk) with a 0.2 synthetic-to-real sampling ratio. The system generates both text responses and style parameters, with speech synthesis handled by an external controllable TTS.

## Key Results
- OmniChat achieves state-of-the-art performance on DailyTalk with METEOR 14.24 and BERTScore 86.99
- Superior emotion prediction with F1e score of 75.46 on DailyTalk test set
- Optimal synthetic-to-real data ratio of α=0.2, with performance degrading at higher ratios
- Speech-only models outperform ASR-augmented baselines at 20K+ synthetic samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mix-Former enables context-aware weighting of heterogeneous audio features for optimal dialogue responses
- Mechanism: Three attribute-specific Q-Formers encode expert features into window-level representations, with learnable weight modules applying sigmoid-activated weights per window before concatenation and projection to LLM input space
- Core assumption: Different dialogue scenarios require different feature importance profiles
- Evidence anchors: Table 5 shows Mix-Former improves METEOR from 15.4 to 15.8; AV-Dialog uses similar multi-modal fusion approaches
- Break condition: If all expert features have uniform importance across scenarios, weighting provides no benefit

### Mechanism 2
- Claim: Synthetic data mixed at α=0.2 improves real-world dialogue performance by providing scenario diversity
- Mechanism: LLM generates dialogue scripts with style parameters, TTS synthesizes speech, dual verification ensures quality, then synthetic samples are mixed with real data at 0.2 ratio during training
- Core assumption: Synthetic data quality is sufficient to generalize without introducing harmful artifacts
- Evidence anchors: Table 4 shows α=0.2 achieves best ROUGE-L (13.67) and F1e (75.46); speech-only models outperform text+speech at 20K+ scale
- Break condition: If synthetic data distribution diverges significantly from real-world distribution, excessive synthetic data causes negative transfer

### Mechanism 3
- Claim: Multi-expert feature extraction with frozen specialized encoders captures complementary acoustic dimensions
- Mechanism: Whisper extracts speech content (Fs), emotion2vec extracts emotional representations (Fe), BEATs captures non-speech audio (Fb), features are temporally aligned before fusion
- Core assumption: Specialized pretrained encoders provide better feature representations than single general-purpose audio encoder
- Evidence anchors: Section 4.1 describes temporal alignment; Table 3 shows OmniChat outperforms Qwen2-Audio on ShareChat-Audio/Music
- Break condition: If a single foundation model can capture all acoustic dimensions, multi-expert overhead becomes unnecessary

## Foundational Learning

- Concept: Q-Former (Query-based alignment)
  - Why needed here: Bridges frozen audio encoders and frozen LLM by learning small trainable queries that compress audio features into LLM-compatible representations
  - Quick check question: Can you explain why Q-Former uses a fixed number K of trainable queries rather than variable-length outputs?

- Concept: Window-level feature processing
  - Why needed here: Enables fine-grained feature weighting (L≈0.33 seconds per window) to handle rapid context changes within a single utterance
  - Quick check question: What happens to temporal resolution if window size L is too large for emotion-heavy dialogues?

- Concept: Synthetic data verification pipeline
  - Why needed here: Ensures speaker consistency and transcription accuracy before mixing synthetic samples into training; low-quality synthetic data degrades rather than helps
  - Quick check question: Why is ASR WER verification necessary if the text content was generated by an LLM?

## Architecture Onboarding

- Component map:
  Raw audio (16kHz) -> Three parallel expert encoders (Whisper, emotion2vec, BEATs) -> Window-level Q-Formers with weight modules -> Concatenation -> Linear projection -> Llama-3.1-8B-Instruct (frozen) with LoRA adapters -> Response tokens + style tokens -> External TTS

- Critical path:
  1. Audio input quality directly affects all three expert features
  2. Window alignment (L=17, K=1) determines fusion granularity
  3. Sampling ratio α controls synthetic/real balance during training
  4. LoRA adapter tuning (query/value weights only) is the sole trainable component in LLM

- Design tradeoffs:
  - Larger K (more queries per window) -> richer representation but higher compute
  - Higher α (more synthetic) -> better scenario coverage but risk of distribution shift
  - Pre-training on general data (E-PT) before scenario-specific fine-tuning (A-FT) -> better language understanding but requires two-stage training

- Failure signatures:
  - Emotion predictions significantly lower than baseline -> check emotion2vec feature extraction or weight module saturation
  - BLEU improves but ROUGE-L/semantics degrade -> model may be overfitting to synthetic phrasing patterns
  - Performance plateaus at 10K-20K samples -> insufficient data for speech-only model to surpass ASR-assisted baseline

- First 3 experiments:
  1. Ablation on Mix-Former vs. simple concatenation using ShareChat-Music test set
  2. Sweep α ∈ {0.1, 0.15, 0.2, 0.25, 0.3} on DailyTalk validation to confirm optimal ratio
  3. Scale experiment: train on 5K, 10K, 20K synthetic samples to determine minimum viable data scale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the benefits of large-scale synthetic data for dialogue understanding be effectively transferred to end-to-end spoken dialogue systems that generate speech directly without a separate TTS module?
- Basis in paper: The authors state in the Limitations section: "In the future, we will explore the application of synthetic data in developing end-to-end spoken dialogue systems."
- Why unresolved: The current OmniChat architecture decouples response generation from speech synthesis, so the efficacy of synthetic data for training models that directly output audio waveforms remains untested
- What evidence would resolve it: Training an end-to-end audio-to-audio model on ShareChatX and evaluating its performance against cascaded systems using metrics for both content accuracy and speech naturalness

### Open Question 2
- Question: What specific mechanisms can be implemented to further reduce the risk of accidental guidance or offensive content in spoken dialogue systems trained on synthetic data?
- Basis in paper: The Ethical Discussion notes that while synthetic data helps consistency, "We plan to explore how to further reduce the risk of accidental guidance in voice dialogue systems in the future."
- Why unresolved: The paper identifies the risk of complex, diverse conversations leading to accidental inappropriate guidance but does not propose or test specific mitigation algorithms beyond the inherent control offered by synthetic generation
- What evidence would resolve it: A comparative safety evaluation measuring the rate of accidental guidance or unsafe suggestions in models trained on ShareChatX versus those trained on filtered real-world data

### Open Question 3
- Question: Does the optimal synthetic-to-real data sampling ratio (α = 0.2) generalize to other distinct real-world datasets or low-resource languages outside of DailyTalk?
- Basis in paper: The paper determines that α = 0.2 is the ideal balance for the DailyTalk dataset, but it does not validate if this specific ratio is a universal constant or dependent on the specific characteristics of the real-world target dataset
- Why unresolved: The finding is derived from a single dataset (DailyTalk); it is unclear if different domains or languages with different syntactic structures would require a different balance to avoid the performance decline seen at ratios α > 0.2
- What evidence would resolve it: Conducting the same mixing ratio experiments (varying α) on diverse real-world benchmarks to see if the peak performance consistently occurs at 0.2

### Open Question 4
- Question: Does the "synthetic gap" (domain shift between AI-generated and human speech) limit the model's ability to handle highly spontaneous or overlapping speech not present in the ShareChatX generation pipeline?
- Basis in paper: The authors acknowledge that ShareChatX is generated by LLMs and TTS, which, while "high-fidelity," may lack the disfluencies, interruptions, and channel noise inherent in "in-the-wild" data, yet they test on DailyTalk (controlled environment)
- Why unresolved: While the model generalizes to DailyTalk, it is unresolved how well the synthetic training prepares the system for "in-the-wild" conversational dynamics that are difficult to simulate
- What evidence would resolve it: Evaluating OmniChat on noisy, in-the-wild datasets to measure degradation in ASR comprehension and response relevance compared to controlled test sets

## Limitations
- Synthetic data distribution alignment remains uncertain for truly diverse, out-of-distribution scenarios
- Mix-Former weight generalization to unseen scenario types has not been tested
- Scalability of multi-expert fusion may become bottlenecked with increasing scenario complexity

## Confidence

- **High Confidence**: Mix-Former mechanism for context-aware feature weighting is well-supported by ablation studies and aligns with established multi-modal fusion literature; optimal α=0.2 ratio is empirically validated
- **Medium Confidence**: Speech-only models outperforming ASR-augmented baselines at 20K+ scale is supported by scale experiments, but exact threshold may vary by domain; superiority of expert encoders over unified models is shown but not yet proven across all scenario types
- **Low Confidence**: Long-term stability of synthetic-to-real data mixing ratios beyond α=0.3 is not explored; assumption that frozen expert encoders capture all necessary acoustic dimensions may break down in highly complex, multi-source audio environments

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate OmniChat on a held-out real-world dataset (e.g., Switchboard) with diverse audio conditions not present in ShareChatX to assess synthetic data generalization limits

2. **Dynamic Window Size Experiment**: Sweep L from 10 to 30 frames to determine if variable window sizing improves emotion prediction in rapid-turn dialogues compared to the fixed L=17 setting

3. **Expert Encoder Ablation in Noise**: Systematically remove each expert (Whisper, emotion2vec, BEATs) and test performance on noisy audio (additive white Gaussian noise at SNRs 0-20dB) to quantify robustness contributions of each branch