---
ver: rpa2
title: 'Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via
  Synthetic Instruction Data'
arxiv_id: '2509.03501'
source_url: https://arxiv.org/abs/2509.03501
tags:
- video
- data
- referring
- frame
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Strefer addresses the challenge of enhancing Video Large Language\
  \ Models (Video LLMs) with fine-grained spatiotemporal referring and reasoning capabilities,\
  \ enabling AI companions to interpret spatial and temporal references in dynamic\
  \ environments. The core method involves a synthetic instruction data generation\
  \ framework that automatically produces diverse, high-quality video instruction-response\
  \ pairs by pseudo-annotating temporally dense, object-centric video metadata\u2014\
  including subjects, objects, their locations as masklets, and action descriptions\
  \ and timelines."
---

# Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data

## Quick Facts
- arXiv ID: 2509.03501
- Source URL: https://arxiv.org/abs/2509.03501
- Reference count: 40
- Models trained on Strefer-generated data outperform baselines across multiple benchmarks: VideoRefer-BenchD improves from 3.28 to 3.34, VideoRefer-BenchQ from 0.665 to 0.672, and QVHighlights from 0.5288 to 0.5390

## Executive Summary
Strefer addresses the challenge of enhancing Video Large Language Models (Video LLMs) with fine-grained spatiotemporal referring and reasoning capabilities. The approach uses a synthetic instruction data generation framework that automatically produces diverse, high-quality video instruction-response pairs by pseudo-annotating temporally dense, object-centric video metadata. This enables models to resolve complex queries involving object-centric events, temporal references, and gestural cues. Experiments demonstrate significant improvements in spatial and temporal disambiguation across multiple benchmarks, establishing a foundation for perceptually grounded, instruction-tuned Video LLMs.

## Method Summary
Strefer generates synthetic instruction data through a modular pipeline that includes entity recognition, referring masklet generation, video clipping, transcription, and question-answer pair generation. The core innovation is the Referring Masklet Generator, which overcomes first-frame dependency issues by reordering frames to prioritize middle-heavy sampling and using bidirectional tracking with SAM2. The system produces instruction-response pairs covering 11 task types across 8 groups (G1-G8), which are combined with a base recipe of existing instruction-tuning data. The trained model incorporates specialized modules including a Region-Language Connector for spatial reasoning and Timestamp Conversion for temporal grounding.

## Key Results
- VideoRefer-BenchD mask-referred description improves from 3.28 to 3.34 (GPT-4o scoring 0-5)
- VideoRefer-BenchQ mask-referred QA accuracy improves from 0.665 to 0.672
- QVHighlights timestamp-based QA accuracy improves from 0.5288 to 0.5390

## Why This Works (Mechanism)

### Mechanism 1: Modular Tracking for Late-Appearing Entities
Strefer overcomes the "first-frame dependency" of standard video object segmentation by decoupling detection from tracking and prioritizing "middle-heavy" frame sampling. The Referring Masklet Generator employs a bidirectional strategy, reordering frames to prioritize the middle of the video, detecting generalized nouns using GroundingDINO, and propagating masks forward and backward using SAM2. It finally assigns complex referring expressions to these tracks using RexSeek.

### Mechanism 2: Forced Visual Grounding via Pronoun Replacement
Synthetic instruction data improves spatial disambiguation by strategically degrading linguistic specificity, forcing the model to rely on visual masklets. The Video Instruction Data Generator replaces specific entity names with generic pronouns or regional tokens in the question prompt. Because the text alone is now ambiguous, the model must attend to the provided masklet token to resolve the reference, reducing "foreground bias."

### Mechanism 3: Discrete Tokenization of Continuous Time
Large Language Models struggle with numerical timestamps; mapping continuous time to a discrete vocabulary of temporal tokens improves temporal grounding. The architecture incorporates a "Timestamp Conversion" module that divides a video into segments, creating tokens that map specific timestamps to discrete representations rather than text strings.

## Foundational Learning

- **Concept: Referring Expression Comprehension (REC) in Video**
  - Why needed: Video REC must track objects across frames while appearance changes or occlusions occur, linking text queries to specific trajectories over time
  - Quick check: How does a "masklet" differ from a static "mask," and why is temporal consistency required to interpret "What did he do after dropping the cup?"

- **Concept: Visual Token Compression (Q-Former / Connector)**
  - Why needed: Video LLMs cannot process raw pixel sequences directly due to context length limits; visual features must be projected into the LLM's embedding space
  - Quick check: If a video has 1000 frames, why can't we feed all raw pixels into the LLM, and how does a "Region-Language Connector" extract specific info from just a masked region?

- **Concept: Pseudo-Labeling / Synthetic Data Generation**
  - Why needed: Strefer relies entirely on generated data rather than human annotation; understanding how teacher models generate labels for student models is critical to evaluating data quality
  - Quick check: Why might a synthetic data pipeline generate "negative questions" (asking about entities not present), and how does this prevent the model from hallucinating?

## Architecture Onboarding

- **Component map:** Entity Recognizer -> Video Clipper -> Masklet Generator (GroundingDINO + SAM2 + RexSeek) -> Transcriber -> QA Generator -> Model (Visual Encoder -> Region-Language Connector + Timestamp Conversion -> LLM Backbone)

- **Critical path:** The Referring Masklet Generator is the most fragile component, relying on a 4-step cascade (Sample, Detect, Track, Assign). If initial frame selection fails to find a frame with the object, subsequent SAM2 tracking will produce empty or noisy masklets, degrading training data quality.

- **Design tradeoffs:**
  - Visual Prompting vs. Architectural Changes: Visual prompting is "plug-and-play" but performs worse on fine-grained tasks compared to training specialized connectors, which require architecture changes and re-training
  - Data Volume vs. Specificity: Adding targeted, high-quality instruction data (e.g., G7 with 27K samples) outperforms generic scaling

- **Failure signatures:**
  - Hallucinated Descriptions: If the Video Transcriber hallucinates actions, the model will generate instruction data confirming those hallucinations
  - Motion Blur Tracking Loss: Heavy motion blur breaks SAM2 tracking, resulting in fragmented spatial supervision

- **First 3 experiments:**
  1. Ablate the Data Sources: Train the model using the "Base Recipe" and incrementally add groups G1-G8 to identify which task types drive specific benchmark improvements
  2. Probe the Referring Pipeline: Run the Referring Masklet Generator on held-out videos where objects disappear and reappear, comparing generated masklet coverage against ground truth
  3. Visual Prompting Baseline: Evaluate the trained model with visual prompting overlays vs. the Region-Language Connector to verify if architectural overhead is justified

## Open Questions the Paper Calls Out

- **Can incorporating visual prompting techniques (SoM, NumberIt) during fine-tuning, while preserving the original Video LLM architecture, yield larger performance gains than applying them only at inference time?**
- **How can synthetic instruction-tuning data from multi-stage modular pipelines be effectively filtered to mitigate error propagation from imperfect pseudo-annotations?**
- **Can mask-level instruction data be transformed into formats for other spatial references (points, boxes, scribbles) to train models with broader spatial grounding capabilities?**
- **What is the optimal training data mixture to balance fine-grained spatiotemporal understanding against broader temporal abstraction skills?**

## Limitations
- The synthetic data generation pipeline's dependency on multiple teacher models introduces potential noise propagation, with the Referring Masklet Generator's 4-step cascade being particularly fragile
- Fixed temporal discretization (M=31 tokens) may inadequately represent videos with rapid action changes or those requiring fine-grained temporal resolution
- Architectural changes are evaluated only within the Strefer framework without ablation showing benefits for other video LLMs

## Confidence
- **High Confidence:** The core claim that synthetic instruction data improves spatial and temporal reasoning capabilities is well-supported by quantitative improvements across multiple benchmarks
- **Medium Confidence:** The effectiveness of forced visual grounding via pronoun replacement is supported qualitatively but lacks systematic evaluation of masklet reliance
- **Low Confidence:** Claims about establishing a foundation for perceptually grounded, instruction-tuned Video LLMs lack empirical evidence for generalization beyond the NExT-QA corpus

## Next Checks
1. **Track Failure Analysis:** Systematically evaluate the Referring Masklet Generator's failure rate on a diverse video corpus, measuring what percentage of entities cannot be tracked due to initialization failures or motion blur
2. **Generalization Stress Test:** Train Strefer on a different video domain (e.g., surveillance footage, sports highlights) and evaluate whether learned spatial-temporal reasoning generalizes beyond the NExT-QA corpus
3. **Architectural Ablation with Visual Prompting:** Implement SoM and NumberIt visual prompting baselines on the Strefer-trained model and compare their performance against the Region-Language Connector and Timestamp Conversion modules