---
ver: rpa2
title: 'Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs'
arxiv_id: '2505.20155'
source_url: https://arxiv.org/abs/2505.20155
tags:
- uni00000013
- pruning
- uni00000051
- pangu
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Pangu Light addresses the challenge of accelerating large language\
  \ models (LLMs) through structured pruning while mitigating performance degradation\
  \ from aggressive width and depth reductions. The framework introduces novel weight\
  \ re-initialization strategies\u2014Cross-Layer Attention Pruning (CLAP) and Stabilized\
  \ LayerNorm Pruning (SLNP)\u2014to stabilize pruned networks and preserve crucial\
  \ information, enabling effective performance recovery."
---

# Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs

## Quick Facts
- **arXiv ID**: 2505.20155
- **Source URL**: https://arxiv.org/abs/2505.20155
- **Reference count**: 29
- **Primary result**: Achieves 2.1x speedup with 98.9% accuracy retention on Pangu-32B

## Executive Summary
Pangu Light addresses the challenge of accelerating large language models (LLMs) through structured pruning while mitigating performance degradation from aggressive width and depth reductions. The framework introduces novel weight re-initialization strategies—Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)—to stabilize pruned networks and preserve crucial information, enabling effective performance recovery. Additionally, it incorporates specialized optimizations like Post-RMSNorm absorption tailored for Pangu's Sandwich-Norm architecture and Ascend NPU hardware. Experimental results demonstrate that Pangu Light-32B achieves an average score of 81.6 and throughput of 2585 tokens/s on Ascend NPUs, outperforming Qwen3-32B (80.9 score, 2225 tokens/s). The framework retains 98.9% accuracy with 2.1x speedup, offering a superior accuracy-efficiency trade-off compared to baselines like PUZZLE.

## Method Summary
Pangu Light accelerates LLMs through structured pruning combined with weight re-initialization techniques. The method uses multi-axis importance scoring (L2-norm of RMSNorm activations for channels, L2-norm of head outputs for attention, magnitude for FFN, cosine distance BI for layers) to guide pruning decisions. CLAP transfers high-importance attention parameters from pruned layers to predecessors by jointly ranking KV groups across consecutive layers. SLNP rescales RMSNorm affine parameters after channel pruning to preserve output magnitude statistics. Post-RMSNorm absorption eliminates normalization overhead by fusing dynamic scaling into preceding linear projections. Recovery training employs knowledge distillation from the original model with cosine learning rate decay (1e-5→1e-7) over 300B tokens.

## Key Results
- Pangu Light-32B achieves average score of 81.6 and throughput of 2585 tokens/s on Ascend NPUs
- Outperforms Qwen3-32B (80.9 score, 2225 tokens/s) while retaining 98.9% accuracy
- Achieves 2.1x speedup through joint width and depth pruning with CLAP and SLNP re-initialization
- Demonstrates superior accuracy-efficiency trade-off compared to PUZZLE baseline

## Why This Works (Mechanism)

### Mechanism 1: Cross-Layer Attention Pruning (CLAP)
- Claim: Transferring high-importance attention parameters from pruned layers to their predecessors mitigates information loss during depth reduction
- Mechanism: When layer l+1 is marked for pruning, KV groups from both layer l and l+1 are jointly ranked by importance scores (averaged from surviving query heads). Top-K groups are retained in layer l, with parameters from l+1's selected groups explicitly transferred rather than discarded
- Core assumption: Critical attention computations are distributed across consecutive layers; some non-redundant features exist in pruned layers that predecessor layers lack
- Evidence anchors:
  - [Section 3.2.1]: "CLAP aims to preserve and transfer critical attention capabilities from a pruned layer by selectively merging its most important Key-Value (KV) groups' parameters into the preceding layer"
  - [Table 3]: Adding CLAP to Minitron baseline improves average score from 30.2 to 33.1 (+2.9 points)
  - [corpus]: Limited direct corpus support; neighbor papers focus on general pruning strategies rather than cross-layer parameter transfer specifically
- Break condition: May provide diminishing returns when layers have highly dissimilar attention patterns, or when pruning very shallow networks where layer l already operates near capacity

### Mechanism 2: Stabilized LayerNorm Pruning (SLNP)
- Claim: Rescaling RMSNorm affine parameters after channel pruning preserves output magnitude statistics, improving post-pruning training stability
- Mechanism: Computes rescaling scalar c_l = ||γ_orig||_2 / ||γ_pruned||_2 for each RMSNorm layer, then applies γ_new = c_l × γ_pruned. This counteracts the L2-norm reduction that occurs when dimensions are removed
- Core assumption: The output scale of normalization layers is a learned signal that downstream layers depend on; abrupt changes to this scale destabilize training dynamics
- Evidence anchors:
  - [Section 3.2.2]: "We observed that re-establishing the output scale of these normalization layers via direct weight re-initialization is crucial for model stability"
  - [Table 3]: Adding SLNP on top of CLAP yields additional +0.7 improvement (33.1 → 33.8)
  - [Figure 3]: Parameter analysis shows γ distributions remain consistent pre/post pruning when SLNP is applied
  - [corpus]: No direct corpus evidence for this specific re-initialization strategy
- Break condition: If channel pruning ratios vary significantly across layers, uniform rescaling may not capture layer-specific adaptation needs

### Mechanism 3: Post-RMSNorm Absorption
- Claim: Dynamic Post-RMSNorm operations can be converted to static channel-wise scaling that fuses into preceding linear projections, eliminating normalization overhead without accuracy loss
- Mechanism: Computes expected inverse scaling magnitude s̄_inv from calibration data, then absorbs it into affine parameters: γ_abs = s̄_inv × γ. The resulting element-wise scaling (γ_abs ⊙ x) fuses directly into the preceding projection weight matrix columns
- Core assumption: At convergence, activation variance entering Post-RMSNorm layers has stabilized, making dynamic normalization less critical than during training
- Evidence anchors:
  - [Section 3.3]: "eliminating these Post-RMSNorm layers can yield a throughput enhancement of up to 6% on Ascend NPUs"
  - [Table 4]: Norm Absorption achieves 59.0 average score vs. 59.9 for full Sandwich-Norm (only -0.9 degradation), matching DyT performance
  - [corpus]: No corpus papers specifically address Post-RMSNorm absorption for Sandwich-Norm architectures
- Break condition: Requires calibration data representative of inference distribution; may degrade if input distributions shift significantly post-deployment

## Foundational Learning

- Concept: **RMSNorm vs LayerNorm**
  - Why needed here: Pangu uses RMSNorm with learnable affine parameters (γ); understanding that ||γ|| directly affects output magnitude is essential for SLNP
  - Quick check question: Given RMSNorm(x) = γ ⊙ x / sqrt(mean(x²) + ε), what happens to output magnitude if γ dimensions are pruned without compensation?

- Concept: **Grouped Query Attention (GQA)**
  - Why needed here: CLAP must respect GQA structure—query heads are grouped by shared KV projections, so pruning/transfer operates at the KV-group level, not individual heads
  - Quick check question: Why can't query heads from different KV groups be freely combined during CLAP's joint ranking?

- Concept: **Knowledge Distillation for Recovery**
  - Why needed here: Post-pruning, the student model recovers capability via distillation from the original teacher (full logits, full vocabulary)
  - Quick check question: Why use the original unpruned model as teacher rather than a separately trained model of the target size?

## Architecture Onboarding

- Component map:
  - Calibration data → Importance scoring → Pruning decisions → CLAP (depth) → SLNP (width) → Norm absorption → Distillation recovery

- Critical path: Calibration data → Importance scoring → Pruning decisions → CLAP (depth) → SLNP (width) → Norm absorption → Distillation recovery

- Design tradeoffs:
  - Aggressive joint pruning (depth + width) enables higher acceleration but increases recovery difficulty—CLAP/SLNP specifically target this regime
  - Norm absorption trades training-time stability (Post-RMSNorm helps early training) for inference efficiency (zero-cost at deployment)
  - Calibration dataset size vs. pruning quality: larger C improves importance estimates but increases preprocessing overhead

- Failure signatures:
  - Sudden accuracy collapse post-pruning without CLAP/SLNP: indicates joint pruning exceeded model's recovery capacity
  - Inconsistent γ distributions post-SLNP: check rescaling computation or channel importance ranking
  - Norm absorption causes >2% accuracy drop: calibration data may not match inference distribution

- First 3 experiments:
  1. Baseline ablation: Apply Minitron-style pruning without CLAP or SLNP to an 11B experimental model; measure post-distillation accuracy to establish lower bound
  2. CLAP-only test: Add CLAP to baseline; verify KV group transfer is correctly merging parameters (inspect attention weight shapes before/after)
  3. Full pipeline validation: Apply all components (CLAP + SLNP + norm absorption) to Pangu-38B targeting 32B; compare throughput on Ascend NPU against Qwen3-32B while monitoring benchmark scores (target: ≥81.0 average)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CLAP and SLNP re-initialization strategies generalize effectively to LLM architectures without Sandwich-Norm (e.g., standard Pre-LN or Post-LN transformers)?
- Basis in paper: [explicit] Conclusion states: "Future work may explore the extension of these principles to other model architectures and hardware platforms"
- Why unresolved: All experiments used Pangu models with Sandwich-Norm; SLNP specifically addresses RMSNorm γ parameter scaling, and Post-RMSNorm absorption is architecture-specific
- What evidence would resolve it: Apply Pangu Light to models like LLaMA, Mistral, or Qwen (Pre-RMSNorm only) and compare performance recovery rates

### Open Question 2
- Question: Can Pangu Light's structured pruning be combined with quantization for compound efficiency gains?
- Basis in paper: [explicit] Conclusion explicitly mentions "integration of Pangu Light with other compression techniques like quantization for even greater efficiency"
- Why unresolved: No experiments combined pruning with quantization; pruning alone achieves 2.1x speedup, but potential interactions with INT8/INT4 quantization remain unexplored
- What evidence would resolve it: Apply INT4/INT8 quantization to pruned Pangu Light models and measure whether accuracy degradation is additive, multiplicative, or mitigated

### Open Question 3
- Question: Why does parameter merging for FFN blocks fail to yield improvements when CLAP succeeds for attention?
- Basis in paper: [inferred] Section 3.2.1 states: "We also explored applying a similar strategy of parameter merging for FFN blocks, but found it did not yield significant performance improvements"
- Why unresolved: No analysis provided for why attention parameter transfer works but FFN doesn't; potential structural or functional differences unexplored
- What evidence would resolve it: Ablation studies analyzing FFN weight distributions across layers, or alternative FFN re-initialization strategies

### Open Question 4
- Question: How sensitive are the importance metrics and pruning outcomes to the choice and size of calibration dataset?
- Basis in paper: [inferred] Importance scores depend on calibration set C, but dataset composition, size, and representativeness are not analyzed for robustness
- Why unresolved: Only one calibration configuration used; no analysis of whether different domains or sizes would yield different pruning decisions
- What evidence would resolve it: Systematic evaluation with varying calibration dataset sizes and domain compositions

## Limitations
- Architecture-specific optimizations (Post-RMSNorm absorption, Sandwich-Norm) limit generalizability to other LLM architectures like GPT-style models
- Heavy dependency on specific 300B-token training regimen with prescribed data composition for knowledge distillation recovery
- Performance claims (2585 tokens/s) are specific to Ascend NPUs and may not translate to other hardware platforms

## Confidence
- **High Confidence**: Core mechanism of weight re-initialization (SLNP) to stabilize pruned networks is well-supported by magnitude compensation theory and experimental evidence (+0.7 score improvement)
- **Medium Confidence**: CLAP's cross-layer parameter transfer shows promising results (+2.9 score improvement), but assumption about consecutive layers sharing critical attention features requires further validation
- **Low Confidence**: Post-RMSNorm absorption's generalizability beyond Pangu's Sandwich-Norm architecture and Ascend NPU hardware is uncertain; 6% throughput claim is hardware-specific

## Next Checks
1. **Architecture Generalization Test**: Apply Pangu Light's pruning pipeline (excluding Post-RMSNorm absorption) to a different LLM architecture (e.g., Llama or Mistral) and evaluate whether SLNP alone provides comparable stability benefits (+2.9 score improvement)

2. **Calibration Data Sensitivity Analysis**: Systematically vary the size and composition of calibration dataset C (e.g., 10K vs. 100K samples, uniform vs. task-specific sampling) and measure the impact on pruning quality and final accuracy to establish calibration requirements

3. **Hardware Portability Assessment**: Implement the core pruning pipeline (CLAP + SLNP) on GPU hardware and compare throughput and accuracy against the Ascend NPU results to quantify hardware-specific performance variations