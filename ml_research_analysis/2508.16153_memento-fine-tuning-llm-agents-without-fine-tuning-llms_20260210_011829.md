---
ver: rpa2
title: 'Memento: Fine-tuning LLM Agents without Fine-tuning LLMs'
arxiv_id: '2508.16153'
source_url: https://arxiv.org/abs/2508.16153
tags:
- memento
- memory
- arxiv
- agents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Memento introduces a memory-based learning paradigm for LLM agents
  that achieves continual adaptation without fine-tuning underlying models. The approach
  formalises deep research agents as a memory-augmented Markov Decision Process (M-MDP),
  enabling case-based reasoning through episodic memory.
---

# Memento: Fine-tuning LLM Agents without Fine-tuning LLMs
## Quick Facts
- arXiv ID: 2508.16153
- Source URL: https://arxiv.org/abs/2508.16153
- Reference count: 18
- Memento achieves 87.88% Pass@3 on GAIA validation and 66.6% F1 on DeepResearcher without fine-tuning LLMs

## Executive Summary
Memento introduces a memory-based learning paradigm that enables continual adaptation of LLM agents without updating underlying model parameters. The approach formalizes deep research agents as memory-augmented Markov Decision Processes (M-MDPs), allowing case-based reasoning through episodic memory retrieval and adaptation. By alternating between case-based planning and tool-based execution, Memento retrieves and adapts past experiences to guide decision-making in new tasks.

The method demonstrates state-of-the-art performance across multiple benchmarks including GAIA, DeepResearcher, and SimpleQA, achieving 4.7%-9.6% absolute improvements on out-of-distribution tasks compared to baseline approaches. This memory-augmented approach enables effective continual learning while avoiding the computational costs and potential degradation associated with traditional fine-tuning methods.

## Method Summary
Memento operates by maintaining an episodic memory bank of past experiences, which it uses to inform decision-making in new tasks. When encountering a new task, the agent first retrieves relevant past experiences from memory, then adapts these experiences to create a plan for the current task. The system alternates between case-based planning (retrieving and adapting past experiences) and tool-based execution (using the planned approach to interact with tools and gather information). This creates a self-improving loop where successful experiences are stored in memory for future use, while the underlying LLM remains frozen.

## Key Results
- Achieves 87.88% Pass@3 on GAIA validation set, outperforming state-of-the-art training-based methods
- Reaches 66.6% F1 score and 80.4% PM on DeepResearcher benchmark
- Demonstrates 4.7%-9.6% absolute improvements on out-of-distribution tasks compared to baselines
- Achieves 79.40% on GAIA test set, maintaining top-1 performance across benchmarks

## Why This Works (Mechanism)
Memento works by leveraging the inherent generalization capabilities of large language models while adding a memory layer that captures task-specific patterns over time. The memory-augmented MDP framework allows the system to treat past experiences as valuable assets that can be retrieved and adapted rather than learning entirely from scratch. This approach combines the broad reasoning capabilities of LLMs with the efficiency of case-based reasoning, enabling the agent to build expertise through experience accumulation without the need for parameter updates.

## Foundational Learning
**Markov Decision Processes (MDPs)**: Why needed - Provides mathematical framework for modeling sequential decision-making in uncertain environments. Quick check - Can you define the five components (S, A, P, R, γ) of an MDP?

**Episodic Memory**: Why needed - Stores complete sequences of agent experiences for later retrieval and adaptation. Quick check - What distinguishes episodic memory from semantic memory in cognitive science?

**Case-based Reasoning**: Why needed - Enables problem-solving by adapting solutions from similar past experiences. Quick check - How does case-based reasoning differ from rule-based reasoning?

**Memory-augmented Neural Networks**: Why needed - Extends traditional neural networks with external memory for better information retention. Quick check - What are the key differences between LSTM and memory-augmented approaches?

**Tool Use in LLMs**: Why needed - Allows agents to interact with external APIs and environments to gather information. Quick check - What are the main categories of tools that LLM agents typically use?

## Architecture Onboarding
**Component Map**: User Query -> Memory Retrieval -> Case Adaptation -> Tool Execution -> Result Storage -> Memory Update

**Critical Path**: The critical execution path follows: memory retrieval of relevant experiences → case-based planning through adaptation → tool-based execution of the plan → storage of successful experiences back into memory.

**Design Tradeoffs**: Memento trades computational overhead during inference (due to retrieval operations) for improved performance and continual learning capability. The frozen LLM parameters avoid catastrophic forgetting but require efficient memory management strategies.

**Failure Signatures**: System may fail when: memory retrieval returns irrelevant experiences, case adaptation produces incorrect plans, or tool execution encounters unexpected API behaviors. Performance degradation can occur with insufficient diverse experiences in memory.

**3 First Experiments**: 
1. Single-task memory accumulation: Run agent on repeated instances of the same task type to measure memory utilization
2. Memory ablation test: Compare performance with and without memory retrieval to quantify contribution
3. Retrieval quality analysis: Evaluate precision@k of memory retrieval against ground truth relevant experiences

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead from case-based reasoning and retrieval operations, especially for long trajectories
- Performance heavily dependent on quality and diversity of stored experiences, creating potential cold-start problems
- Memory management scalability concerns for long-term deployment with growing experience databases

## Confidence
- **High Confidence**: Core methodology of memory-augmented MDPs and case-based reasoning is technically sound and well-grounded in literature
- **Medium Confidence**: Outperformance of training-based methods needs careful validation across diverse domains to rule out benchmark-specific optimizations
- **Low Confidence**: Long-term effectiveness of continual learning without parameter updates remains uncertain in real-world, non-stationary environments

## Next Checks
1. **Scalability Test**: Measure inference latency and memory usage as experience database scales from 100 to 10,000+ cases, quantifying performance-overhead trade-off
2. **Robustness Evaluation**: Test system on adversarial task sequences designed to expose case-based reasoning limitations, including tasks with superficial similarities but fundamental differences
3. **Cross-Domain Transfer**: Evaluate performance when experience accumulates across multiple domains (research, coding, creative writing) to assess effectiveness without domain-specific fine-tuning