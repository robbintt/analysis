---
ver: rpa2
title: 'FedEM: A Privacy-Preserving Framework for Concurrent Utility Preservation
  in Federated Learning'
arxiv_id: '2503.06021'
source_url: https://arxiv.org/abs/2503.06021
tags:
- privacy
- data
- learning
- fedem
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedEM addresses gradient leakage attacks in federated learning
  by introducing controlled data perturbations instead of adding noise to gradients.
  The method formulates a joint optimization problem that minimizes model loss while
  ensuring perturbations remain within specified bounds.
---

# FedEM: A Privacy-Preserving Framework for Concurrent Utility Preservation in Federated Learning

## Quick Facts
- **arXiv ID**: 2503.06021
- **Source URL**: https://arxiv.org/abs/2503.06021
- **Reference count**: 40
- **Primary result**: Data-level perturbations outperform gradient-level noise for privacy-utility trade-off in federated learning

## Executive Summary
FedEM addresses gradient leakage attacks in federated learning by introducing controlled data perturbations instead of adding noise to gradients. The method formulates a joint optimization problem that minimizes model loss while ensuring perturbations remain within specified bounds. Experiments on MNIST, FashionMNIST, and CIFAR-10 datasets show FedEM outperforms traditional LDP methods in both privacy protection and model utility. With similar perturbation levels, FedEM achieves higher test accuracy (MNIST: 97.23% vs 79.47%) and stronger privacy metrics (MSE: 1.18 vs 0.63).

## Method Summary
FedEM applies controlled perturbations to local data before gradient computation, breaking the gradient-to-data inversion mapping that reconstruction attacks rely on. Each client runs N perturbation iterations: updating δ via sign gradients, projecting to norm bounds, then computing gradients on perturbed data. The server aggregates these gradients using FedSGD. Unlike LDP's post-hoc noise addition, FedEM's task-aware perturbations create structured privacy protection. The framework balances privacy and utility through constrained bilevel optimization where both model parameters and perturbations are jointly optimized.

## Key Results
- MNIST: 97.23% accuracy vs 79.47% (LDP) at similar privacy levels
- MNIST: Test-MSE 1.18 vs 1.22 (LDP) when comparing at comparable utility levels
- CIFAR-10: FedEM doesn't significantly outperform FedSGD baseline (1.76 vs 1.91 Test-MSE)

## Why This Works (Mechanism)

### Mechanism 1: Data-Level Perturbation Breaks Gradient-to-Data Inversion
- **Claim**: Perturbing input data directly disrupts the gradient-to-data mapping that reconstruction attacks rely on, while preserving gradient informativeness for legitimate training.
- **Core assumption**: Perturbation is sufficiently large to disrupt reconstruction fidelity but sufficiently bounded to preserve semantic content for learning.
- **Evidence**: [abstract] "effectively mitigates gradient leakage attacks while maintaining model performance"

### Mechanism 2: Error Minimization Framework Redirected for Defense
- **Claim**: The error minimization optimization structure, originally designed for training corruption, can be repurposed for privacy when the inner minimization target shifts from "poison effectiveness" to "privacy-preserving perturbation."
- **Core assumption**: Sign-based gradient updates efficiently navigate the perturbation space.
- **Evidence**: [Section 3.3] "optimization objectives are fundamentally different"

### Mechanism 3: Constrained Bilevel Optimization Balances Privacy-Utility Trade-off
- **Claim**: Joint optimization over model parameters and perturbations with norm constraints creates a Pareto frontier where both objectives can be satisfied better than LDP's sequential approach.
- **Core assumption**: Task-aware perturbations are more efficient privacy protectors than random noise per unit of utility degradation.
- **Evidence**: [Table 2] At comparable utility (96.40% vs 97.23%), FedEM achieves MSE 1.18 vs DP-Clip's 1.22

## Foundational Learning

- **Concept: Gradient Leakage Attacks (DLG)**
  - **Why needed here**: FedEM's entire purpose is to defend against DLG-style reconstruction.
  - **Quick check question**: If I gave you a gradient vector and the model architecture, could you explain how an attacker might reconstruct the original input?

- **Concept: Adversarial Perturbations and Error Minimization**
  - **Why needed here**: FedEM borrows the mathematical structure from adversarial training and error minimization attacks.
  - **Quick check question**: What is the difference between adversarial training (maximizing loss) and error minimization (minimizing loss), and why does FedEM choose the latter?

- **Concept: Local Differential Privacy vs. Data-Level Perturbation**
  - **Why needed here**: The paper positions itself against LDP.
  - **Quick check question**: Why might adding noise to a gradient be less efficient than adding structured perturbations to the input that generated it?

## Architecture Onboarding

- **Component map**: Server -> broadcasts θ^(t-1) and δ_k -> Clients -> perturbation loop (N iterations) -> gradient computation -> upload g_k -> Server -> aggregates and updates θ
- **Critical path**: 1) Server broadcasts parameters to clients, 2) Each client runs N perturbation iterations updating δ_k, 3) Client computes gradient on perturbed data, 4) Server aggregates gradients, 5) Repeat for M rounds
- **Design tradeoffs**: N (perturbation iterations) vs compute, ρ_min/ρ_max bounds vs privacy-utility balance, FedSGD vs FedAVG for attack amplification
- **Failure signatures**: Accuracy collapse (ρ_max too large), weak privacy protection (ρ_min effectively zero), non-convergence (learning rate issues), CIFAR-10 limitations
- **First 3 experiments**:
  1. Reproduce MNIST baseline comparison: FedEM vs FedSGD vs LDP with 97.23% vs 79.47% accuracy gap
  2. Ablate perturbation iterations (N): Test N ∈ {1, 2, 5, 10} on MNIST to confirm non-monotonic trade-off
  3. Stress test with different attack strengths: Vary DLG reconstruction iterations to measure robustness differences

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can more effective data perturbation techniques be developed to improve privacy protection on complex datasets like CIFAR-10?
- **Basis**: Authors state privacy protection on CIFAR-10 doesn't significantly outperform FedSGD
- **Why unresolved**: Current error-minimization perturbations may be insufficient for color images with complex features
- **What evidence would resolve it**: Improved MSE/SSIM metrics on CIFAR-10 with novel perturbation strategies

### Open Question 2
- **Question**: How does FedEM perform against adversarial attacks beyond gradient leakage?
- **Basis**: Authors note future studies could explore performance against other attack types
- **Why unresolved**: FedEM was designed and evaluated specifically against DLG-based gradient leakage
- **What evidence would resolve it**: Empirical evaluation against membership inference, property inference, and model inversion attacks

### Open Question 3
- **Question**: Can FedEM provide formal differential privacy guarantees while preserving its utility advantages over LDP methods?
- **Basis**: Paper demonstrates superior empirical privacy-utility trade-offs but lacks formal DP bounds
- **Why unresolved**: FedEM uses norm-constrained perturbations rather than calibrated noise
- **What evidence would resolve it**: Derivation of privacy budget bounds for FedEM's perturbation mechanism

## Limitations
- Architecture details (layers, activations) not specified for any dataset
- Exact perturbation bounds (ρ_min, ρ_max) unclear beyond ρ_max=8 initialization
- Weak privacy protection on CIFAR-10 suggests dataset-dependent limitations
- Non-monotonic effects of perturbation iterations observed but not theoretically explained

## Confidence
- **High**: Data-level perturbation disrupting gradient-to-data inversion (supported by experimental MSE/accuracy gaps)
- **Medium**: Error minimization framework redirection claim (theoretical plausibility but limited corpus validation)
- **Medium**: Constrained bilevel optimization balancing privacy-utility trade-off (supported by MNIST/FashionMNIST results but CIFAR-10 exception)

## Next Checks
1. **Architecture Sensitivity Test**: Reproduce MNIST experiments with different neural network depths to determine if privacy gains depend on model capacity
2. **Perturbation Bound Sensitivity**: Systematically vary ρ_min from 0 to ρ_max/2 to identify exact point where privacy protection begins degrading
3. **Cross-Dataset Transferability**: Apply FedEM trained on MNIST to FashionMNIST and CIFAR-10 without retraining perturbation parameters