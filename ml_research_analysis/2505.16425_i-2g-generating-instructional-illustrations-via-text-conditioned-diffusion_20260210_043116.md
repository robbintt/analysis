---
ver: rpa2
title: '$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion'
arxiv_id: '2505.16425'
source_url: https://arxiv.org/abs/2505.16425
tags:
- image
- arxiv
- diffusion
- text
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating instructional
  illustrations from procedural text, a key problem in natural language processing
  where purely textual instructions often fail to convey complex physical actions
  and spatial relationships. The authors propose a language-conditioned diffusion
  framework that decomposes instructional content into goals and sequential steps,
  then conditions visual generation on these linguistic elements.
---

# $I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion

## Quick Facts
- **arXiv ID:** 2505.16425
- **Source URL:** https://arxiv.org/abs/2505.16425
- **Reference count:** 19
- **Primary result:** Proposes language-conditioned diffusion framework for procedural text-to-image generation with constituency parsing and pairwise coherence model, achieving better alignment on HTStep, CaptainCook4D, and WikiAll datasets.

## Executive Summary
This paper addresses the challenge of generating instructional illustrations from procedural text, where purely textual instructions often fail to convey complex physical actions and spatial relationships. The authors propose a language-conditioned diffusion framework that decomposes instructional content into goals and sequential steps, then conditions visual generation on these linguistic elements. Their approach introduces three key innovations: a constituency parser-based text encoding mechanism to handle lengthy instructions, a pairwise discourse coherence model for maintaining consistency across instruction sequences, and a novel evaluation protocol specifically designed for procedural language-to-image alignment. Experiments across three instructional datasets demonstrate that their method significantly outperforms existing baselines in generating visuals that accurately reflect the linguistic content and sequential nature of instructions.

## Method Summary
The authors propose a diffusion-based framework for generating instructional illustrations from procedural text. Their approach conditions Stable Diffusion XL on goal descriptions and sequential step instructions, using constituency parsing to segment lengthy step descriptions into coherent clauses for more effective text encoding. To maintain visual coherence across generated images, they employ a pairwise diffusion model that samples adjacent step pairs and applies an adjacency-masked attention mechanism to enable cross-step consistency. The framework is optimized using a reward-based preference learning approach that combines caption generation and text similarity models. The method is evaluated on three instructional datasets (HTStep, CaptainCook4D, and WikiAll) using both automated metrics (KL divergence, Chi-square tests) and human evaluation.

## Key Results
- The model achieves better text-image alignment as measured by KL divergence and Chi-square tests compared to baseline models like Stable Diffusion variants, with lower scores indicating better alignment
- Constituency parser-based text encoding and pairwise diffusion model improve performance on lengthy and multi-step instructions
- The custom evaluation protocol combining human judgment with caption generation and text similarity models provides more accurate assessment of instructional image generation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise latent fusion with adjacency-masked attention appears to improve cross-step visual coherence in procedural image sequences.
- Mechanism: The model samples adjacent step pairs $(v_i, s_i)$ and $(v_j, s_j)$, encodes images into latents $z_i, z_j$ via VAE, concatenates them, and applies a custom attention mask $\hat{M}$ that restricts attention primarily to each latent's own slice and its temporal neighbor. This allows step $j$ to inherit object identity and environment context from step $i$ without requiring a computationally expensive full joint model.
- Core assumption: Procedural tasks exhibit visual continuity where objects, tools, or settings persist across consecutive steps (e.g., same mixing bowl appearing in multiple cooking steps).
- Evidence anchors:
  - [abstract] "employing a pairwise diffusion model to maintain coherence across generated images"
  - [section 4.1-4.2] Mathematical formulation of pairwise factorization $\prod_{i<j} p(v_i, v_j | g, s_i, s_j)$ and attention mask design
  - [corpus] Weak direct evidence; CookAnything (FMR 0.52) addresses similar multi-step recipe generation but uses different approach
- Break condition: When procedural steps have non-sequential dependencies (e.g., parallel branches), when actions are abstract/implicit with no clear visual manifestation, or when extended sequences exceed practical pairwise coverage.

### Mechanism 2
- Claim: Constituency parser-based clause segmentation likely preserves more semantic content from lengthy instructional text than standard single-pass encoding.
- Mechanism: A constituency parser segments each step description into coherent clauses (e.g., verb phrases). Each clause is encoded separately using OpenCLIP-ViT/G (for steps) or CLIP-ViT/L (for goals), then concatenated. This bypasses the truncation or compression that standard encoders apply to long sequences.
- Core assumption: Instructional step texts often contain multiple semantic units (objects, actions, conditions) that exceed typical encoder context windows, and clause boundaries align with meaningful semantic boundaries.
- Evidence anchors:
  - [abstract] "constituency parsing for effective text encoding"
  - [section 4.3] "employ a constituency parser to segment each step into coherent clauses... Each clause is then separately encoded and concatenated"
  - [corpus] No directly comparable clause-based encoding in neighbor papers; most use standard CLIP/LLM encoders
- Break condition: When constituency parsing produces malformed segments, when instructions are already concise (adding overhead without benefit), or when clause boundaries do not align with visual entities.

### Mechanism 3
- Claim: Reward-based preference optimization using caption-generation and text-similarity models may better align generated images with instructional text than standard diffusion fine-tuning.
- Mechanism: The framework uses a frozen LLM to sample $(g, s)$ pairs, generates images via diffusion, then computes alignment rewards using caption models (BLIP2, LLaVA) and a decoder-only text encoder for similarity. Gradients flow backward through sampling steps to update diffusion parameters $\phi$ only, improving fidelity without retraining the language backbone.
- Core assumption: The reward model (caption + similarity pipeline) accurately reflects human judgment of instruction-image alignment, and gradient-based feedback can propagate meaningful signal through the diffusion sampling process.
- Evidence anchors:
  - [abstract] "custom evaluation protocol that combines human judgment with caption generation and text similarity models"
  - [section 4.4-5.2] Preference optimization objective $J(\theta) = \mathbb{E}[r(\text{sample}(\theta, g, s, x_T), g, s)]$ and human evaluation correlation analysis
  - [corpus] RAFT, ImageReward, and related methods (cited in Related Work) use reward-based diffusion fine-tuning but not specifically for procedural alignment
- Break condition: When reward model has systematic biases (paper notes MLLMs assign high scores for object-name matches even when actions differ), when reward signal is sparse or noisy, or when optimization causes mode collapse.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPM) and Classifier-Free Guidance (CFG)**
  - Why needed here: The backbone is Stable Diffusion XL, and understanding the forward/reverse process equations (Eq. 2-3) and CFG mixing (Eq. 4) is essential for modifying the conditioning mechanism.
  - Quick check question: Given noisy sample $z_t = \sqrt{\bar{\alpha}_t}z_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$, what does the diffusion model learn to predict?

- **Concept: Self-Attention and Attention Masking in Transformers**
  - Why needed here: The cross-image consistency mechanism modifies standard self-attention with a custom adjacency mask $\hat{M}$ to enforce temporal ordering across latent pairs.
  - Quick check question: How does masking the attention matrix $\text{Softmax}(\hat{M} \odot QK^\top / \sqrt{d})$ change which tokens can attend to each other?

- **Concept: Constituency Parsing in NLP**
  - Why needed here: The text encoding pipeline relies on constituency parsing to segment instructions into clauses before encoding, assuming clause boundaries align with visual entities.
  - Quick check question: What is the difference between constituency parsing (phrase structure) and dependency parsing, and why might constituency be preferred for clause-level segmentation?

## Architecture Onboarding

- **Component map:** Input: Goal text g, Step texts S = {s_1, ..., s_n} -> LLM (frozen) -> Sample/enhance (g, S) representations -> Constituency Parser -> Segment steps into clauses -> Text Encoders -> CLIP-ViT/L (goals) + OpenCLIP-ViT/G (steps) -> Pairwise Sampler -> Select adjacent pairs (v_i, s_i), (v_j, s_j) -> VAE Encoder -> Latents z_i, z_j -> Latent Fusion + Adjacency Masked Attention -> Cross-step consistency -> SDXL Diffusion Backbone -> Denoising with CFG -> VAE Decoder -> Generated image pairs -> Reward Model: Caption (BLIP2/LLaVA) + Text Similarity -> Feedback -> Gradient Update -> Diffusion parameters φ only

- **Critical path:** The adjacency-masked attention (Section 4.2) is the novel contribution—understanding how $\hat{M}$ restricts attention to temporal neighbors is essential. The constituency parsing pipeline (Section 4.3) handles the long-text problem. The reward model (Section 5.2) determines optimization direction.

- **Design tradeoffs:**
  - Pairwise vs. full joint model: Pairwise avoids combinatorial explosion but may miss long-range dependencies (assumption: procedural tasks are locally coherent)
  - CFG strength $w$: Higher values improve fidelity but reduce diversity
  - Caption model choice: BLIP2 vs. LLaVA trade off speed vs. detail in generated captions for reward computation
  - Encoder choice: OpenCLIP-ViT/G handles longer text but adds computational cost vs. standard CLIP

- **Failure signatures:**
  - Word-trigger bias: Specific words (e.g., "steak") consistently generate similar figures regardless of context (shown in Figure 5)
  - Non-visual actions: Abstract instructions (e.g., "let it rest") produce irrelevant or generic images
  - Cross-step uniformity: Generated images for different steps become overly similar when text conditioning is insufficiently differentiated
  - Reward gaming: Model optimizes for captioner biases (object-name matching) rather than true instructional alignment

- **First 3 experiments:**
  1. **Baseline comparison on held-out split:** Run I²G against SD1.5, SD2.1, SDXL, and StackedDiffusion on HTStep/CaptainCook4D test sets. Measure KL divergence and Chi-square scores using the caption+similarity evaluation pipeline. Expected: I²G should show lower KL/Chi2 values indicating better alignment distribution.
  2. **Ablation on attention mask design:** Compare full adjacency mask vs. unrestricted attention vs. independent generation (no cross-step attention). Measure cross-image consistency (e.g., object persistence across steps) via human evaluation. This isolates the contribution of the masked attention mechanism.
  3. **Reward model correlation with human judgment:** Generate images for 100 random instruction pairs, collect human alignment scores, and correlate with CLIPScore, ImageBind, and the proposed caption+similarity pipeline. Validate that the chosen reward model better reflects human judgment than alternatives.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can abstract or implicit actions that lack clear visual manifestations be effectively represented in instructional image generation?
- Basis in paper: [explicit] The conclusion states the approach "struggles with abstract or implicit actions that do not manifest themselves as clear visual features."
- Why unresolved: The current diffusion-based framework relies on direct text-to-visual mappings, which cannot generate concrete imagery for actions without salient physical correlates.
- What evidence would resolve it: Demonstrated success on a benchmark dataset enriched with implicit actions (e.g., "remember to," "avoid," "carefully"), showing improved alignment scores over the current method.

### Open Question 2
- Question: How can visual consistency be maintained across longer sequences (e.g., more than 10-15 steps) without degradation?
- Basis in paper: [explicit] The conclusion notes that "uniformity across consecutive images can still be improved, especially for extended multi-step processes."
- Why unresolved: The pairwise factorization and masked attention mechanism localize dependencies but may accumulate errors or drift over long trajectories.
- What evidence would resolve it: Quantitative consistency metrics (e.g., object identity preservation, environment continuity) on tasks with 20+ steps, compared against full-sequence joint modeling approaches.

### Open Question 3
- Question: How can keyword-triggered visual bias (e.g., "steak" always producing a similar figure) be reduced?
- Basis in paper: [explicit] Appendix A.3 documents that "certain words consistently triggers similar figures, regardless of how much additional instructional text we include."
- Why unresolved: Pretrained diffusion models may have strong priors associated with specific nouns that override contextual conditioning.
- What evidence would resolve it: Controlled experiments varying context while keeping key nouns fixed, measuring diversity of generated scenes and fidelity to full step descriptions.

### Open Question 4
- Question: Can action-level alignment, beyond object matching, be reliably evaluated by current multimodal models?
- Basis in paper: [inferred] Section 5.2 notes MLLMs "assign high scores when the object names in the text match, even if the actions described differ."
- Why unresolved: Existing caption-based or CLIP-based metrics conflate object presence with action correctness, failing to penalize missing or incorrect verbs.
- What evidence would resolve it: Development of an action-focused benchmark with verb-variation pairs, showing that a proposed metric correlates more strongly with human judgments of action fidelity than existing MLLM-based scores.

## Limitations

- The approach struggles with abstract or implicit actions that do not manifest themselves as clear visual features
- Certain words consistently trigger similar figures regardless of how much additional instructional text is included
- Uniformity across consecutive images can still be improved, especially for extended multi-step processes

## Confidence

- **High Confidence**: The overall architecture design (pairwise diffusion + constituency parsing) is technically sound and the experimental methodology (KL divergence, Chi-square tests) is appropriate for evaluating text-image alignment
- **Medium Confidence**: The cross-step coherence mechanism works as claimed, though its effectiveness may be limited to locally coherent procedural tasks rather than complex multi-branch instructions
- **Medium Confidence**: The constituency parser improves text encoding for lengthy instructions, but the extent of improvement over standard truncation methods requires more direct comparison
- **Low Confidence**: The reward-based optimization actually captures human notions of instructional alignment rather than optimizing for captioner biases

## Next Checks

1. **Cross-task generalization test**: Evaluate I²G on non-sequential procedural datasets (e.g., parallel cooking steps or multi-branch assembly instructions) to assess whether pairwise attention breaks down when temporal continuity assumptions fail
2. **Parser ablation study**: Compare clause-based encoding against standard truncation and sliding window approaches on the same instruction sets to quantify the actual benefit of constituency parsing for text encoding
3. **Reward model bias analysis**: Generate images for instruction pairs where object names are present but actions differ (e.g., "cut the steak" vs. "serve the steak"), then measure whether the reward model consistently favors object-name matches over action-context alignment, and correlate these scores with human judgments