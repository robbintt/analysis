---
ver: rpa2
title: Long Context Automated Essay Scoring with Language Models
arxiv_id: '2509.10417'
source_url: https://arxiv.org/abs/2509.10417
tags:
- arxiv
- scoring
- length
- language
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates long-context language models for automated\
  \ essay scoring, addressing the challenge of processing lengthy student essays that\
  \ exceed typical transformer limits. Five models with architectural modifications\u2014\
  XLNet, Longformer, ModernBERT, Mamba, and Llama\u2014were fine-tuned on the ASAP\
  \ 2.0 dataset using both traditional classification and generative scoring approaches."
---

# Long Context Automated Essay Scoring with Language Models

## Quick Facts
- arXiv ID: 2509.10417
- Source URL: https://arxiv.org/abs/2509.10417
- Reference count: 10
- Primary result: Mamba-130m achieves 0.797 QWK, outperforming human rater baseline of 0.745

## Executive Summary
This study evaluates long-context language models for automated essay scoring (AES), addressing the challenge of processing lengthy student essays that exceed typical transformer limits. Five models with architectural modifications—XLNet, Longformer, ModernBERT, Mamba, and Llama—were fine-tuned on the ASAP 2.0 dataset using both traditional classification and generative scoring approaches. Mamba-130m, despite its smaller size, achieved the highest overall quadratic weighted kappa (QWK) of 0.797, demonstrating competitive performance with linear complexity models. All models outperformed human rater agreement (0.745), with Mamba showing particular efficiency advantages for long sequences due to its state-space architecture requiring only linear computational scaling.

## Method Summary
The study fine-tuned five long-context models (XLNet, Longformer, ModernBERT, Mamba, and Llama) on the ASAP 2.0 dataset (17,307 training essays, 7,421 test essays) using two regimes: traditional classification with appended classification heads, and generative scoring with QLoRA adapters for Llama. Models were evaluated using Quadratic Weighted Kappa (QWK) with 10% of training data held out as a development set. Mamba required frozen SSM layers to prevent model collapse, while other models used standard fine-tuning procedures with Adam optimization, learning rate 10^-6, and linear scheduling over 10 epochs.

## Key Results
- Mamba-130m achieved the highest overall QWK of 0.797 despite having only 130M parameters
- Longformer reached 0.798 QWK, slightly outperforming Mamba
- Traditional models (DeBERTa-Base) scored 0.790 QWK
- All models outperformed human rater agreement baseline of 0.745
- Mamba demonstrated 2-8x inference speed improvements due to linear computational scaling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba's state-space architecture enables competitive AES performance with linear computational scaling, achieving 0.797 QWK despite having only 130M parameters.
- Mechanism: Replaces transformer attention layers with discretized state-space models defined by h_t = Ah_{t-1} + Bx_t (Equation 3a). Each Mamba layer combines SSM blocks with convolutional layers, linear projections, and activation functions. The linear recurrence relation eliminates quadratic attention computation, scaling as O(n) rather than O(n²).
- Core assumption: Linear complexity translates to practical efficiency gains for long sequences without sacrificing scoring accuracy.
- Evidence anchors:
  - [abstract]: "Mamba-130m, despite its smaller size, achieved the highest overall quadratic weighted kappa (QWK) of 0.797...Mamba showing particular efficiency advantages for long sequences due to its state-space architecture requiring only linear computational scaling."
  - [section 2.6]: "The Mamba blocks can be computed with linear complexity, making them well-suited for long context tasks...optimized implementations may achieve 2-8x speed improvements compared to transformer-based models."
  - [corpus]: Related work validates efficiency claims but Mamba-specific AES validation remains limited in corpus neighbors.

### Mechanism 2
- Claim: Longformer's hybrid attention mechanism (local sliding window + selective global attention) achieves the highest QWK (0.798) by preserving rubric-relevant organizational features.
- Mechanism: Local attention operates via sliding window over adjacent tokens, while global attention applies exclusively to special tokens ([CLS], [SEP], [MASK]). This reduces attention computation from O(n²) to O(n×w) where w is window size, enabling 4k context length.
- Core assumption: Organizational scoring rubric elements requiring long-range dependencies can be captured through sparse global token connections.
- Evidence anchors:
  - [abstract]: "This raises serious validity concerns as it undermines the model's ability to fully capture and evaluate organizational elements of the scoring rubric, which requires long contexts to assess."
  - [section 2.2]: "The Longformer model attempts to reconcile the need for local attention with a selective form of global attention...by only using attention selectively, the computational burden is mitigated."
  - [corpus]: Neighbor paper "Empirical Analysis of the Effect of Context in AES" confirms context length significantly impacts transformer-based scoring quality.

### Mechanism 3
- Claim: Recurrent attention with cached hidden states (XLNet/Transformer-XL) extends effective context to L×D tokens (6,000-12,000) without architectural modifications.
- Mechanism: Attention keys and values incorporate previous segment hidden states via stop-gradient concatenation: h̃^{n-1}_{τ+1} = [SG(h^{n-1}_τ) ∘ h^{n-1}_{τ+1}]. Information flows across segments through cached states, though maximum effective context is bounded by segment length × network depth.
- Core assumption: Graded essays exhibit discourse coherence that propagates information across segment boundaries through recurrent connections.
- Evidence anchors:
  - [section 2.3]: Mathematical formulation (Equations 1a-1e) showing recurrence built into key and value computation; "the output of any token is only a function of at most L×D of the previous tokens."
  - [section 2.3]: "These models have recently been discussed for essays, where the long context was useful in accurately annotating the argumentative components of essays."
  - [corpus]: Ormerod et al. (2023) neighbor reference confirms XLNet effectiveness for essay argumentation annotation tasks.

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: Primary evaluation metric; all performance claims reference QWK scores. Understanding that QWK measures agreement beyond chance (1=perfect, 0=random, -1=inverse) is essential for interpreting model comparison.
  - Quick check question: If Model A scores 0.790 QWK and human raters score 0.745, what does this indicate about Model A's reliability?

- Concept: Transformer Attention Complexity
  - Why needed here: All architectural modifications in this paper address the O(n²) attention scaling problem. Without this foundation, you cannot evaluate why Mamba's O(n) complexity matters or why truncation was historically necessary.
  - Quick check question: Given BERT's 512-token limit, what happens to memory requirements if you double input length to 1,024 tokens?

- Concept: Fine-tuning vs. Frozen Weights
  - Why needed here: Mamba requires partial freezing (SSM layers frozen, only L_in/L_out trained) to prevent model collapse. Understanding which parameters to update is critical for successful deployment.
  - Quick check question: Why might fine-tuning all Mamba parameters cause training instability, while freezing SSM weights succeeds?

## Architecture Onboarding

- Component map:
  - Input Processing: Tokenizer-specific subword encoding → Word embeddings + Positional embeddings (RoPE for ModernBERT/Llama) → Context length bound (512-8k depending on model)
  - Encoder Variants: (1) Standard transformer attention [DeBERTa], (2) Sliding window + global attention [Longformer], (3) Recurrent attention with segment caching [XLNet], (4) State-space recurrence [Mamba], (5) RoPE-scaled attention [ModernBERT/Llama]
  - Task Heads: Classification head (traditional models) or generative output (Llama with QLoRA adapters on L_q, L_k, L_v)
  - Output: Score prediction mapped to rubric scale

- Critical path:
  1. Assess essay length distribution against model context limits (Table 1: avg 342-426 words → ~450-550 tokens)
  2. Select architecture based on length requirements: DeBERTa (512) < Longformer (4k) < ModernBERT/XLNet/Mamba (8k effective)
  3. Configure fine-tuning regime: Traditional (classification head) vs. Generative (QLoRA with instruction template)
  4. Optimize QWK on 10% held-out development set with early stopping
  5. Validate against human baseline (0.745 QWK threshold)

- Design tradeoffs:
  - Accuracy vs. Efficiency: Longformer (0.798 QWK, 149M params) slightly outperforms Mamba (0.797 QWK, 130M params), but Mamba offers 2-8x inference speedup
  - Model Size vs. Context Length: Llama-3.2-8B (8B params, 0.792 QWK) underperforms smaller encoders, suggesting parameter count alone doesn't determine AES quality
  - Truncation vs. Full Context: DeBERTa (512 limit) achieves 0.790 QWK but risks validity concerns for organizational rubric elements

- Failure signatures:
  - Model Collapse (Mamba): Full fine-tuning causes training divergence; freeze SSM, conv, and L_gate layers
  - Memory Overflow: Batch size reduction from 4→1 may be required for long essays; prioritize gradient accumulation over OOM
  - Prompt Sensitivity (Generative): Rubric phrasing variations significantly impact QWK; use 20 paraphrase variations and optimize on development set

- First 3 experiments:
  1. Baseline Establishment: Fine-tune DeBERTa-Base (512 tokens, truncated) on ASAP 2.0 with classification head. Target: ≥0.790 QWK. This validates data pipeline and provides truncation baseline.
  2. Long Context Ablation: Fine-tune Longformer (4k) and Mamba-130m (8k effective) on identical data without truncation. Compare QWK scores and training time per epoch. Expect Mamba to match Longformer (±0.001 QWK) with faster wall-clock time.
  3. Parameter Efficiency Test: Apply QLoRA to Llama-3.2-8B with 10% development set for rubric optimization. If QWK <0.790, prioritize encoder models over generative approaches for pure scoring tasks.

## Open Questions the Paper Calls Out

- Can ensemble methods combining state-space models (like Mamba) with attention-based transformers yield superior performance compared to single-model approaches?
  - Basis: Discussion section states that performance differences "suggest a potential for ensemble approaches."
  - Why unresolved: Study evaluated each model in isolation
  - Evidence needed: Experiments blending Mamba and Longformer predictions to see if ensemble QWK exceeds 0.798

- What specific training dynamics cause model collapse when fully fine-tuning Mamba models for classification, and can this be mitigated?
  - Basis: Methods section notes "Full model training seemed to readily lead to model collapse," forcing authors to freeze SSM weights
  - Why unresolved: Authors worked around instability rather than investigating root cause
  - Evidence needed: Ablation study testing various learning rates or regularization techniques for stable full-parameter fine-tuning

- How does the quality of feedback provided by generative models (like Llama) compare to their scoring accuracy when evaluated against human criteria?
  - Basis: Discussion acknowledges generative models "offer the promising capability of providing feedback"
  - Why unresolved: Study measured performance solely via QWK, not qualitative feedback utility
  - Evidence needed: Dual evaluation measuring both QWK score and validity/explanatory power of generated feedback

## Limitations
- Model collapse when fully fine-tuning Mamba requires partial freezing of SSM layers
- Memory constraints may require batch size reduction to 1 for long essays
- Truncation remains necessary for models with limited context (DeBERTa at 512 tokens)

## Confidence
- QWK as evaluation metric: High - explicitly stated and consistently applied
- Mamba performance claims: Medium - based on single study, though efficiency claims align with known architecture properties
- Human baseline comparison: High - clearly stated (0.745 QWK) and used as threshold

## Next Checks
1. Verify that ASAP 2.0 dataset split maintains 90/10 ratio with stratified sampling by grade/prompt
2. Confirm Mamba-130m fine-tuning with frozen SSM, conv, and L_gate layers produces stable training without model collapse
3. Test whether ensemble of Longformer and Mamba predictions achieves QWK > 0.798 on held-out test set