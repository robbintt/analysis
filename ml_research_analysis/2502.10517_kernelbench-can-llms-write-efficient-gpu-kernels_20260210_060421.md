---
ver: rpa2
title: 'KernelBench: Can LLMs Write Efficient GPU Kernels?'
arxiv_id: '2502.10517'
source_url: https://arxiv.org/abs/2502.10517
tags:
- kernels
- kernel
- torch
- pytorch
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KernelBench, a framework for evaluating language
  models' ability to generate efficient GPU kernels for machine learning workloads.
  It includes 250 tasks spanning individual operations, operator sequences, and full
  ML architectures, with a new metric "fastp" measuring the percentage of kernels
  that are both correct and achieve a speedup threshold p over PyTorch baselines.
---

# KernelBench: Can LLMs Write Efficient GPU Kernels?

## Quick Facts
- arXiv ID: 2502.10517
- Source URL: https://arxiv.org/abs/2502.10517
- Reference count: 40
- Primary result: Current LLMs struggle to outperform PyTorch baselines in more than 20% of GPU kernel generation tasks

## Executive Summary
KernelBench introduces a comprehensive framework for evaluating language models' ability to generate efficient GPU kernels for machine learning workloads. The benchmark includes 250 tasks spanning individual operations, operator sequences, and full ML architectures, with a novel metric "fast_p" measuring the percentage of kernels that are both correct and achieve a speedup threshold over PyTorch baselines. Experiments reveal that even state-of-the-art models like OpenAI-o1 and DeepSeek-R1 struggle to generate efficient kernels, with iterative refinement and hardware-specific guidance shown to improve performance. The study highlights the persistent challenge of functional correctness, as most failures stem from execution errors or incorrect outputs rather than performance issues.

## Method Summary
The framework evaluates LLMs on GPU kernel generation through three task categories: individual operations (e.g., softmax, attention), operator sequences (e.g., multi-head attention), and full ML architectures (e.g., transformers). Each task requires generating CUDA kernels that are functionally correct and achieve specified performance improvements over PyTorch Eager baselines. The evaluation uses a custom metric "fast_p" that measures the percentage of kernels meeting both correctness and speedup criteria. The study employs iterative refinement with execution and profiling feedback, and tests various prompt engineering strategies including hardware-specific information and optimization examples.

## Key Results
- State-of-the-art LLMs achieve "fast_p" scores below 20% on most benchmarks
- Iterative refinement with execution feedback significantly improves kernel quality
- Providing hardware-specific information and optimization examples helps models generate better kernels
- Functional correctness remains the primary challenge, with most failures due to execution errors

## Why This Works (Mechanism)
The framework's effectiveness stems from its comprehensive evaluation of both correctness and performance, combined with iterative refinement capabilities. By providing execution and profiling feedback, models can progressively improve their kernels through multiple attempts. The hardware-specific guidance helps bridge the gap between abstract kernel descriptions and practical GPU implementation requirements.

## Foundational Learning
- **GPU kernel programming**: Understanding CUDA programming and GPU architecture is essential for generating efficient kernels
- **Performance profiling**: Knowledge of profiling tools and metrics helps identify bottlenecks and optimization opportunities
- **Iterative refinement**: Understanding how to incorporate feedback into successive kernel generations improves outcomes
- **Hardware constraints**: Awareness of GPU memory hierarchies and computational resources guides efficient implementation
- **Functional correctness verification**: Methods for validating kernel outputs against expected results are crucial for quality assessment

## Architecture Onboarding
- **Component map**: LLM -> Kernel Generator -> CUDA Compiler -> GPU Execution -> Profiler -> Feedback Loop -> LLM
- **Critical path**: Kernel generation → Compilation → Execution → Profiling → Refinement
- **Design tradeoffs**: Performance vs. correctness, generalization vs. hardware-specific optimization, single vs. iterative generation
- **Failure signatures**: Compilation errors, runtime crashes, incorrect outputs, performance regressions
- **First experiments**: 1) Test single kernel generation without feedback, 2) Apply iterative refinement with execution feedback, 3) Compare hardware-specific vs. generic prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Single execution environment may not generalize across different GPU architectures
- Fixed speedup threshold (1.5x) may not capture nuanced performance improvements
- Focus on CUDA limits applicability to other GPU programming models
- Limited expert kernel comparison may not represent optimal solutions

## Confidence
- **High confidence**: Current LLMs struggle to outperform PyTorch baselines
- **Medium confidence**: Effectiveness of iterative refinement and hardware-specific guidance
- **Low confidence**: Generalizability across different GPU architectures and programming models

## Next Checks
1. Replicate experiments across multiple GPU architectures (e.g., AMD, Intel) to assess hardware dependency
2. Test alternative performance metrics beyond fixed speedup thresholds
3. Evaluate impact of varying prompt engineering strategies on kernel generation success rates