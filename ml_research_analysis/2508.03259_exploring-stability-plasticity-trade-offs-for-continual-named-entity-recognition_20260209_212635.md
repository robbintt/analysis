---
ver: rpa2
title: Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition
arxiv_id: '2508.03259'
source_url: https://arxiv.org/abs/2508.03259
tags:
- entity
- types
- learning
- cner
- ma-f1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continual named entity recognition
  (CNER), where a model must sequentially learn to recognize new entity types without
  catastrophic forgetting of previously learned ones. The authors propose a Stability-Plasticity
  Trade-off (SPT) method that balances old and new knowledge from both representation
  and weight perspectives.
---

# Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition

## Quick Facts
- arXiv ID: 2508.03259
- Source URL: https://arxiv.org/abs/2508.03259
- Reference count: 40
- Primary result: Proposed SPT method achieves state-of-the-art performance in continual named entity recognition by balancing stability (retaining old knowledge) and plasticity (acquiring new knowledge)

## Executive Summary
This paper addresses continual named entity recognition (CNER), where models must sequentially learn to recognize new entity types without catastrophic forgetting of previously learned ones. The authors propose a Stability-Plasticity Trade-off (SPT) method that balances old and new knowledge from both representation and weight perspectives. From the representation side, they enhance knowledge distillation by incorporating pooling operations to consolidate representation dimensions. From the weight perspective, they dynamically merge weights of old and new models using a weight-guided selective mechanism that prioritizes significant weights. Additionally, they develop a confidence-based pseudo-labeling approach to handle semantic shift of the non-entity type. Extensive experiments across ten CNER settings on three benchmark datasets demonstrate that SPT significantly outperforms previous CNER approaches, achieving new state-of-the-art performance.

## Method Summary
The SPT method operates through three complementary mechanisms: Pooled Knowledge Distillation (PKD) that consolidates attention representations through pooling operations to retain global knowledge while allowing local adaptation; Weight-Guided Selective Mechanism (WSM) that uses Fisher information to identify and selectively merge only the most important weights from the old model; and Confidence-Based Pseudo-Labeling that recovers old entity labels masked as non-entities in new datasets. The model is trained on CoNLL2003, I2B2, and OntoNotes5 datasets using BERT-base-cased encoder with standard classifier head, batch size 8, learning rate 4e-4, and Î»=2. Training proceeds sequentially where each new model state is selectively merged with the previous state using Fisher information to identify critical weights.

## Key Results
- SPT achieves state-of-the-art performance across three datasets (CoNLL2003, I2B2, OntoNotes5) with significant improvements in both stability and plasticity
- Pooled Knowledge Distillation (PKD) outperforms standard Knowledge Distillation by consolidating representation dimensions while maintaining adaptability
- Weight-Guided Selective Mechanism effectively protects critical old knowledge while allowing new entity types to be learned
- Confidence-based pseudo-labeling successfully recovers old entity types masked as non-entities in new datasets

## Why This Works (Mechanism)

### Mechanism 1: Pooled Knowledge Distillation (PKD)
- **Claim:** Standard Knowledge Distillation (KD) enforces rigid representation alignment, potentially over-constraining the model; introducing pooling operations appears to relax this constraint, allowing the model to adapt to new entity types (plasticity) while retaining aggregated statistical knowledge of old types (stability).
- **Mechanism:** Traditional KD minimizes the distance between attention scores of the old and new models. This method aggregates (pools) these attention scores across sequence and head dimensions (Equation 3) before calculating the loss. By distilling "pooled" statistics rather than exact token-level attention maps, the new model retains the *global* characteristics of old representations without being forced to replicate the exact *local* activations of the previous task.
- **Core assumption:** The linguistic knowledge required for Named Entity Recognition (NER) can be preserved in aggregated attention statistics rather than requiring precise, token-level attention preservation.
- **Evidence anchors:**
  - [abstract] "...introduce a pooling operation into the original KD, permitting a level of plasticity by consolidating representation dimensions."
  - [section] Section III-B1, Equation (3): "LPKD strikes a suitable balance between excessive rigidity... and extreme leniency."
  - [corpus] Weak direct corpus support for this specific pooling mechanism; neighbors focus on prompt tuning or generative frameworks.
- **Break condition:** If the performance on new entity types (plasticity) is significantly lower than baselines, the pooling may be too aggressive (loss of specific semantic detail).

### Mechanism 2: Weight-Guided Selective Mechanism (WSM)
- **Claim:** Uniformly averaging weights between old and new models may bias the model toward old knowledge or dilute new knowledge; selecting only "significant" weights to merge may better preserve critical old circuits while allowing new circuits to form.
- **Mechanism:** After training on a new task, the model does not simply average parameters ($\theta_{balanced} = \alpha\theta_{old} + (1-\alpha)\theta_{new}$). Instead, it calculates the Fisher information of the old model weights to identify parameters critical to old entity types. It selectively fuses only the top-$\gamma$% most important weights, leaving the rest as they are in the new model (Equation 6).
- **Core assumption:** Assumption: Fisher information effectively identifies weights crucial for remembering old entity types, and non-significant weights can be safely left un-merged to maximize plasticity.
- **Evidence anchors:**
  - [abstract] "...dynamically merge weights... using a weight-guided selective mechanism that prioritizes significant weights."
  - [section] Section III-B2: "This mechanism selectively incorporates essential weights from the old model... into the new model."
  - [corpus] Corpus mentions "Pareto Continual Learning" for dynamic trade-offs, supporting the intuition of selective/balanced updates.
- **Break condition:** If "Old Entity Type" performance collapses in later steps, the Fisher-based selection threshold ($\gamma$) is likely too low, failing to protect critical old knowledge.

### Mechanism 3: Confidence-Based Pseudo-Labeling
- **Claim:** In Continual NER, tokens belonging to old entity types are often labeled as "Non-Entity" (O) in new datasets; treating them as true negatives causes the model to unlearn old entities. Recovering these labels via pseudo-labeling mitigates this "semantic shift."
- **Mechanism:** The frozen old model infers labels on the new training data. For tokens labeled "O" in the ground truth, if the old model predicts an old entity type with high confidence (entropy below a dynamic threshold), the training target is replaced with the old model's prediction (Equation 8).
- **Core assumption:** The old model's predictions on the new data are sufficiently accurate to serve as ground truth, and entropy is a reliable proxy for prediction confidence.
- **Evidence anchors:**
  - [abstract] "...handle the semantic shift of the non-entity type... using the old model to predict entity types..."
  - [section] Section III-B3, Figure 2: Visualizes how old entities (e.g., [ORG]) are masked as [O] and recovered by pseudo-labels.
  - [corpus] "KoGNER" and "GenCNER" in corpus also highlight handling entity confusion/knowledge, aligning with the need for label refinement.
- **Break condition:** If the model hallucinates old entities in new data where they don't exist, the confidence threshold ($\tau$) is too low (false positives from the old teacher).

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** This is the central problem the paper solves. A neural network, when fine-tuned on Task B, typically loses the ability to perform Task A because weights relevant to Task A are overwritten.
  - **Quick check question:** Can you explain why simply training a model on new entity types causes it to "forget" previously learned entity types?

- **Concept: Knowledge Distillation (KD)**
  - **Why needed here:** The paper modifies standard KD for the representation perspective. You must understand that KD typically forces a "student" model to mimic a "teacher" model's outputs to preserve knowledge, often causing rigidity (excessive stability).
  - **Quick check question:** In standard KD, what is the risk if you force the new model to match the old model's probabilities exactly?

- **Concept: Semantic Shift (in NER)**
  - **Why needed here:** Specific to this domain. The "Non-Entity" or "Outside" (O) tag changes meaning between tasks. In Task 1, "Apple" is an Org; in Task 2 data, "Apple" might be labeled "O" because Org is not in the current label set.
  - **Quick check question:** Why is the "O" (non-entity) class problematic in continual learning compared to other classes?

## Architecture Onboarding

- **Component map:** BERT-base-cased encoder (12 layers, 12 heads) -> Pooled Knowledge Distillation module -> Weight-Guided Selective Mechanism -> Confidence-based Pseudo-labeling -> Linear classifier head

- **Critical path:**
  1. **Inference:** Pass current data through frozen $M_{t-1}$ to generate Pseudo-Labels for "O" tokens.
  2. **Training:** Train $M_t$ using combined Ground Truth + Pseudo-Labels.
  3. **Loss:** Calculate $L_{PKD}$ by pooling attention maps of $M_t$ and $M_{t-1}$.
  4. **Post-Processing:** After training step $t$, compute Fisher info for $M_t$. Merge weights with $M_{t-1}$ using Equation (6) to create the starting point for step $t+1$.

- **Design tradeoffs:**
  - **Pooling Intensity:** Equation (1) is too rigid; Equation (2) is too loose; Equation (3) is the "balanced" choice.
  - **Weight Fusion ($\alpha$ vs $\gamma$):** $\alpha$ balances old/new; $\gamma$ determines *how many* weights to protect. Lower $\gamma$ = more plasticity but higher risk of forgetting.

- **Failure signatures:**
  - **Rigid Output:** Model detects old entities perfectly but fails completely on new ones -> "Excessive Stability" (reduce KD weight $\lambda$ or increase pooling).
  - **Generic Predictions:** Model predicts only new entities and marks all old ones as "O" -> "Excessive Plasticity" (increase $\gamma$ or reduce learning rate).

- **First 3 experiments:**
  1. **Baseline Check:** Implement "Vanilla Weight Fusion" (Eq 4) vs. "Selective Mechanism" (Eq 6) on the I2B2 dataset to validate that selective merging improves "New Entity" scores without dropping "Old Entity" scores.
  2. **Ablation on Pooling:** Compare $L_{KD}$ (No pooling) vs. $L_{PKD}$ (Pooling) to confirm that pooling actually improves the trade-off (Table V logic).
  3. **Hyperparameter $\lambda$:** Sweep $\lambda$ (e.g., 0.5, 2.0, 5.0) to find the sweet spot where distillation aids retention without stifling new learning (Table VI).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does the Stability-Plasticity Trade-off (SPT) method generalize to other sequence labeling tasks beyond Named Entity Recognition?
- Basis in paper: [explicit] The conclusion states, "Looking ahead, our goal is to extend the application of our approach to other sequence labeling tasks, exploring its potential in a wider range of contexts."
- Why unresolved: The current study limits evaluation strictly to NER datasets (CoNLL, I2B2, OntoNotes, and CMeEE), leaving the method's efficacy on tasks with different labeling structures (e.g., Chunking, Slot Filling) unverified.
- Evidence: Experimental results applying SPT to non-NER sequence labeling benchmarks under continual learning settings.

### Open Question 2
- Question: How does the SPT method perform in more complex continual learning environments, such as those involving domain shifts or noisy data?
- Basis in paper: [explicit] The authors state they "plan to investigate the scalability and robustness of the SPT method in more complex continual learning environments."
- Why unresolved: The current experimental settings involve adding new entity types within relatively consistent data domains, whereas real-world application often involves simultaneous domain shifts and noisy labels.
- Evidence: Evaluation metrics from scenarios where the input data domain changes alongside the introduction of new entity types (e.g., crossing from news to biomedical domains).

### Open Question 3
- Question: Is Fisher information the optimal metric for calculating weight importance in the weight-guided selective mechanism?
- Basis in paper: [inferred] Section III-B2 uses Fisher information to determine weight importance, but the paper does not compare this against other importance metrics or explain why Fisher is specifically superior for the semantic shift problem in CNER.
- Why unresolved: While Fisher information is a standard proxy for importance, its specific superiority over alternatives (like gradient magnitude or learned attention scores) in the context of fusing weights for CNER is assumed but not proven.
- Evidence: An ablation study comparing the performance of the weight-guided selective mechanism using Fisher information versus other importance estimation techniques.

## Limitations

- The greedy sampling algorithm for creating continual learning scenarios is referenced to CFNER's Appendix B but not fully specified, affecting reproducibility
- Fisher information computation uses diagonal approximation without clear details on batch aggregation, potentially limiting precision
- Evaluation focuses on fixed step sequences (2-step, 4-step, 10-step) rather than arbitrary task arrival patterns
- Limited validation of pseudo-label quality showing how many tokens are correctly recovered versus false positives introduced

## Confidence

**High Confidence:** The core architectural components (pooled knowledge distillation, weight-guided selective mechanism, confidence-based pseudo-labeling) are well-specified with clear mathematical formulations and the experimental methodology is rigorous with appropriate baselines and ablation studies.

**Medium Confidence:** The claim that SPT achieves superior stability-plasticity trade-offs is supported by extensive experiments across three datasets, though the exact mechanisms by which pooling operations achieve "relaxed" constraints versus standard KD could benefit from additional qualitative analysis of attention patterns.

**Low Confidence:** The claim about handling "semantic shift of the non-entity type" through pseudo-labeling, while intuitively sound, lacks direct empirical validation showing how many tokens are recovered versus false positives introduced, and the dynamic threshold mechanism's robustness across different entity distributions is not fully explored.

## Next Checks

1. **Fisher Information Computation Validation:** Implement the exact Fisher information calculation procedure used in the paper, including batch-wise aggregation and diagonal approximation details, then verify whether the identified important weights align with known critical parameters for entity recognition (e.g., weights affecting [PER], [LOC] entity boundaries).

2. **Pooling Operation Sensitivity Analysis:** Systematically vary the pooling dimensions and aggregation functions in the LPKD component (beyond the three equations shown) to determine whether the reported "balanced" choice truly represents an optimal point or if the performance gains are robust to pooling configuration changes.

3. **Pseudo-Label Quality Assessment:** Implement an oracle evaluation of the confidence-based pseudo-labeling by comparing recovered pseudo-labels against ground truth in the new datasets, measuring both true positive recovery rates for old entities and false positive rates where the old model hallucinates entities that don't exist in the new context.