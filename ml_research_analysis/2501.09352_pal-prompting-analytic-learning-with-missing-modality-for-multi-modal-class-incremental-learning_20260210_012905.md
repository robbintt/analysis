---
ver: rpa2
title: 'PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental
  Learning'
arxiv_id: '2501.09352'
source_url: https://arxiv.org/abs/2501.09352
tags:
- learning
- prompt
- missing
- modality
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAL addresses multi-modal class-incremental learning (MMCIL) under
  missing-modality scenarios, where subsets of modalities (e.g., visual, text) may
  be absent during training or testing. It combines modality-specific prompt learning
  with analytic learning, using prompt tuning to preserve holistic representations
  and reformulating MMCIL as a Recursive Least-Squares problem for closed-form solutions.
---

# PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2501.09352
- **Source URL:** https://arxiv.org/abs/2501.09352
- **Reference count:** 40
- **Primary result:** Achieves 72.59% accuracy on UPMC-Food101 with 70% missing modality and 4.92% forgetting rate

## Executive Summary
PAL addresses multi-modal class-incremental learning under missing-modality scenarios by combining prompt tuning with analytic learning. The method uses modality-specific prompts to maintain holistic representations when data is incomplete, while reformulating classifier updates as a Recursive Least-Squares problem to prevent catastrophic forgetting. Experiments show PAL outperforms state-of-the-art baselines on UPMC-Food101 and N24News datasets, particularly excelling when large portions of modalities are missing during training or testing.

## Method Summary
PAL integrates prompt tuning with analytic learning to handle multi-modal class-incremental learning with missing modalities. During training, modality-specific prompt pools are prepended to a frozen ViLT backbone, with prompts selected via a query-key mechanism. The model simultaneously learns to reconstruct missing modality embeddings using a reconstruction loss. For classifier updates, PAL uses Recursive Least-Squares (RLS) to analytically solve for weights, avoiding iterative gradient updates that cause forgetting. This combines the representation flexibility of prompt tuning with the stability of analytic learning, while handling missing data through dummy inputs and reconstruction objectives.

## Key Results
- Achieves 72.59% accuracy on UPMC-Food101 under 70% missing modality condition
- Maintains low forgetting rate of 4.92% across incremental tasks
- Outperforms ACIL and RebQ baselines by significant margins in both accuracy and forgetting metrics

## Why This Works (Mechanism)

### Mechanism 1: Holistic Representation via Modality-Specific Prompts
The model compensates for absent sensory inputs by projecting available data into a shared embedding space using learned prompt tokens. PAL attaches modality-specific prompt pools to a frozen ViLT backbone, with a query-key mechanism selecting specific prompts based on available input. A reconstruction loss forces the model to reconstruct missing modality's query embedding from the available one, ensuring prompts encapsulate cross-modal knowledge even when data is incomplete.

### Mechanism 2: Catastrophic Forgetting Mitigation via Recursive Least-Squares (RLS)
PAL prevents forgetting by solving for classifier weights analytically rather than through iterative gradient updates. The method reformulates classifier training as a Ridge Regression problem, maintaining a correlation matrix and updating weights using a closed-form RLS update rule. This ensures the solution at step k is mathematically equivalent to joint training on all previous steps.

### Mechanism 3: Alleviation of Underfitting in Analytic Learning
Prompt tuning provides necessary representation plasticity that purely frozen analytic learning lacks. Standard analytic learning faces underfitting due to reliance on frozen backbones. PAL introduces a "Prompt Tuning via BP" phase before the analytic update, allowing input representation to adapt dynamically to new classes while keeping the bulk of the model frozen.

## Foundational Learning

- **Concept: Recursive Least Squares (RLS) / Ridge Regression**
  - Why needed here: The core classifier update relies on matrix inversion identities rather than gradient descent
  - Quick check question: Can you derive the update step for a linear regressor when a new data point arrives, without retraining on old data?

- **Concept: Prompt Tuning in Transformers**
  - Why needed here: Understanding how prepending learnable tokens shifts attention mechanism in a frozen ViLT model is essential for the "Prompt Module"
  - Quick check question: How does changing the input token sequence affect the self-attention output if the weights W_Q, W_K, W_V are frozen?

- **Concept: Multi-Modal Fusion (ViLT)**
  - Why needed here: The method assumes a specific architecture where image patches and text tokens are processed in a single stream
  - Quick check question: How does ViLT handle interaction between image patches and text tokens compared to dual-encoder architectures?

## Architecture Onboarding

- **Component map:** Input -> Frozen ViLT Backbone -> Prompt Module (with Key-Query matching) -> Classifier (MLP with Upsampling) -> Output
- **Critical path:**
  1. Input Processing: Tokenize available modalities; mask missing ones with dummy inputs
  2. Prompt Selection: Use backbone outputs to query prompt pools; prepend selected prompts
  3. BP Phase: Train Prompts + Classifier on current task Tk
  4. AL Phase: Freeze everything. Extract features. Update Classifier weights using RLS
- **Design tradeoffs:**
  - Prompt Length vs. Noise: Increasing prompt length helps fit complex tasks but introduces noise if too long
  - Upsampling Size: Larger up-sampling captures more info but increases compute cost for matrix inversion
- **Failure signatures:**
  - Singular Matrix: If regularization Î· is too low or feature dimension is high, R calculation may explode
  - Error Accumulation: If prompts fail to reconstruct missing modalities, classifier receives garbage features
- **First 3 experiments:**
  1. Sanity Check: Run inference on UPMC-Food101 with 0% vs 90% missing data to establish robustness baseline
  2. Ablation (AL vs BP): Disable RLS update and train only with BP to measure forgetting reduction contribution
  3. Prompt Stress Test: Vary prompt pool size on 50% missing modality split to verify capacity saturation point

## Open Questions the Paper Calls Out
- The paper explicitly states plans to extend PAL for tri-modal learning and beyond
- The current architecture is restricted to dual-modal scenarios (image-text), utilizing modality-specific prompt pools designed for two streams
- The prompt tuning mechanism's dependency on ViLT's specific attention mechanisms for prompt insertion is not validated against other transformers

## Limitations
- Prompt tuning hyperparameters are underspecified (exact training epochs not provided)
- RLS update relies on matrix inversion that could become ill-conditioned over many incremental steps
- Results only shown on UPMC-Food101 and N24News; generalization to other multi-modal domains remains unverified

## Confidence
- **High confidence**: The analytic learning mechanism (RLS reformulation) is mathematically sound and well-explained
- **Medium confidence**: The prompt-based holistic representation claim works well empirically but lacks rigorous analysis of when prompts fail
- **Medium confidence**: The reconstruction loss as cross-modal knowledge transfer mechanism is plausible but not extensively validated

## Next Checks
1. Systematically vary prompt pool sizes (10, 50, 100, 128) on a 50% missing modality split to identify exact point where additional prompts stop improving performance
2. Monitor correlation matrix condition numbers across incremental steps on UPMC-Food101 to identify when numerical errors might accumulate
3. Evaluate PAL on a different multi-modal dataset (e.g., medical imaging with text reports) to test generalization beyond food and news domains