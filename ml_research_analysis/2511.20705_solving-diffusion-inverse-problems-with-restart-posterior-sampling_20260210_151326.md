---
ver: rpa2
title: Solving Diffusion Inverse Problems with Restart Posterior Sampling
arxiv_id: '2511.20705'
source_url: https://arxiv.org/abs/2511.20705
tags:
- reps
- problems
- daps
- diffusion
- restart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RePS, a diffusion-based framework for solving
  inverse problems with both linear and non-linear measurement models. RePS leverages
  restart posterior sampling, an extension of restart-based sampling previously used
  for unconditional diffusion, to solve posterior inference problems.
---

# Solving Diffusion Inverse Problems with Restart Posterior Sampling

## Quick Facts
- **arXiv ID**: 2511.20705
- **Source URL**: https://arxiv.org/abs/2511.20705
- **Reference count**: 40
- **Primary result**: RePS achieves faster convergence and superior reconstruction quality on linear and non-linear inverse problems compared to diffusion-based baselines including DAPS.

## Executive Summary
This paper introduces RePS, a diffusion-based framework for solving inverse problems with both linear and non-linear measurement models. RePS leverages restart posterior sampling, an extension of restart-based sampling previously used for unconditional diffusion, to solve posterior inference problems. The method uses a conditioned ODE applicable to any differentiable measurement model and introduces a simplified restart strategy that contracts approximation errors. Unlike some prior approaches, RePS avoids backpropagation through the score network, reducing computational cost. Experiments on linear and non-linear inverse problems demonstrate that RePS achieves faster convergence and superior reconstruction quality compared to existing diffusion-based baselines, including state-of-the-art methods like DAPS.

## Method Summary
RePS implements a conditioned ordinary differential equation (ODE) solver for posterior sampling in inverse problems. The method approximates the posterior score function using Tweedie-style expectations, reformulating the ODE update in terms of the posterior expectation E[X₀|xₜ,y] rather than requiring explicit Jacobian computation. At each integration step, RePS solves a MAP estimation problem via gradient descent to find the measurement-consistent solution. The key innovation is a restart sampling strategy that periodically injects noise to contract accumulated approximation errors while maintaining the low discretization error characteristic of ODE solvers. The approach works with any differentiable measurement model without requiring closed-form solutions, and avoids backpropagation through the score network to reduce computational cost.

## Key Results
- RePS achieves 31.89 PSNR on ImageNet super-resolution (4×) versus 30.71 for DAPS
- On non-linear phase retrieval, RePS achieves 25.56 PSNR on ImageNet versus 22.53 for DAPS
- RePS shows superior sample diversity in multi-modal posterior settings like phase retrieval, with multiple samples producing visually distinct yet measurement-consistent reconstructions

## Why This Works (Mechanism)

### Mechanism 1: Posterior Score via Tweedie-style Expectation
The posterior score can be expressed as (E[Ẋ₀|ẋₜ,y] - ẋₜ)/σ²(t), avoiding explicit Jacobian computation of the score network. Rather than computing ∇ₓₜ log p(y|ẋₜ;θ) directly (which requires lifting measurements to noisy latent space), RePS reformulates the ODE update in terms of the posterior expectation E[Ẋ₀|ẋₜ,y]. This is approximated by the MAP estimate via argmin of (1/2)||y - h(ẋ)||² + (λ/2)||ẋ - E[Ẋ₀|ẋₜ]||², solved with gradient descent per integration step.

### Mechanism 2: Restart Sampling Contracts Approximation Errors
Interleaving ODE integration with periodic noise injection achieves both low discretization error and error contraction. ODE samplers accumulate approximation errors deterministically, while SDE samplers inject small noise per step, contracting errors stochastically but with higher discretization error. RePS runs multiple ODE steps (typically 10) from σᵣ to σ₀, then "restarts" by adding noise: ẋᵣ ~ N(ẋ₀, σᵣ²I). This obliterates accumulated errors while preserving ODE's low discretization error.

### Mechanism 3: Gradient Descent on Measurement Consistency Per Step
Optimizing the MAP objective with gradient descent at each ODE step handles arbitrary differentiable measurement models without closed-form solutions. At each noise level, RePS computes E[Ẋ₀|ẋₜ] via single score network forward pass, then performs N gradient descent steps on the composite loss. This decouples the score network from the measurement model.

## Foundational Learning

- **Score-based diffusion models and SDE/ODE formulations**: RePS builds directly on the reverse-time ODE: dẋₜ = -σ̇(t)σ(t)∇ₓₜ log p(ẋₜ)dt. Understanding how score functions parameterize this ODE is prerequisite to grasping the conditioned ODE modification. Quick check: Can you explain why Eq. (2) and Eq. (3) both sample from the same marginal distribution, but differ in their stochasticity?

- **Inverse problems as Bayesian inference**: The core formulation p(x₀|y) ∝ p(x₀)p(y|x₀) underlies all diffusion-based inverse methods. RePS's contribution is a specific posterior sampling strategy, not the inverse problem formulation itself. Quick check: For a linear measurement y = Ax + n with n ~ N(0, σ²I), what is p(y|x₀)?

- **Tweedie's formula and denoising score matching**: Eq. (10) derives from Tweedie-style arguments connecting the score ∇ₓₜ log p(ẋₜ|y) to the posterior mean E[Ẋ₀|ẋₜ,y]. This is the theoretical bridge enabling the no-backprop formulation. Quick check: In the unconditional case, what is E[Ẋ₀|ẋₜ] expressed in terms of the score function?

## Architecture Onboarding

- **Component map**: Pre-trained Score Network s_θ(x, σ) → [Score Evaluation] → E[Ẋ₀|ẋₜ] (Tweedie estimate) → [MAP Optimization, Eq. 12] → ẋ_MAP (gradient descent, N steps) → [Conditioned ODE Step, Eq. 11] → ẋₜ₋Δₜ → [Restart Scheduler] ← Annealing: σᵣₑₛₜₐᵣₜ → σₘᵢₙ

- **Critical path**: (1) Initialize ẋₜ ~ N(0, σ²ₘₐₓI); (2) For each restart level σᵣ: run 10 ODE steps from σᵣ to σ₀, each requiring N gradient descent iterations on Eq. (12); (3) After reaching σ₀, renoise to next σᵣ₊₁; (4) Repeat until σᵣ reaches σₘᵢₙ

- **Design tradeoffs**:
  - ODE steps per restart: More steps → lower discretization error but more error accumulation. Paper finds 10 optimal.
  - Gradient descent steps (N): More steps → better MAP solutions but higher compute. Paper uses 10-20 depending on task.
  - σᵣₑₛₜₐᵣₜ value: Higher → more exploration but slower convergence. Task-specific tuning required.
  - λ (measurement consistency weight): Controls prior vs. likelihood balance. Improper values cause reconstructions that ignore measurements or overfit noise.

- **Failure signatures**:
  - Blurry reconstructions: λ too high (measurement constraint dominates), or N too small (insufficient optimization).
  - Non-measurement-consistent outputs: λ too low, or gradient descent diverging.
  - Slow convergence with no quality gain: σᵣₑₛₜₐᵣₜ too high or too many restart levels.
  - Mode collapse in diverse tasks: Restart noise insufficient (σₘᵢₙ too low) or too few ODE steps between restarts.

- **First 3 experiments**:
  1. Verify baseline ODE vs. SDE behavior: Implement conditioned ODE without restarts, conditioned SDE (Euler-Maruyama), and compare PSNR/LPIPS vs. NFE on a single linear task (e.g., Gaussian deblur). Reproduce Figure 2 pattern—ODE faster initial convergence, SDE better asymptotically.
  2. Ablate ODE steps per restart: Fix NFE=1000, vary ODE steps {1, 5, 10, 20, 30} between restarts. Confirm performance peaks around 5-10 steps per Figure 5.
  3. Test nonlinear measurement model: Implement phase retrieval (FFT magnitude only), tune λ and N via grid search. Verify multiple samples from same measurement yield diverse reconstructions (Figure 7 behavior).

## Open Questions the Paper Calls Out

### Open Question 1
Can the restart noise level schedule (σ_restart) and the likelihood weighting parameter (λ) be theoretically determined or adaptively learned to remove the need for per-task hyperparameter tuning? The authors state they "tune σ_restart separately for each task" and that λ is "a tunable parameter, fixed for all time." The current work relies on manual grid search for these parameters to achieve state-of-the-art results, treating them as task-specific constants rather than dynamically adjusted variables.

### Open Question 2
Why does RePS underperform variational baselines like RED-diff on specific non-linear tasks such as non-linear deblurring? Table 1 shows RED-diff achieves higher PSNR (30.86) and lower FID (43.84) compared to RePS (29.02 PSNR, 51.97 FID) on the FFHQ non-linear deblurring task. The paper focuses on outperforming DAPS and does not provide an analysis of why the restart posterior sampling strategy fails to surpass variational approaches on this specific non-linear operator.

### Open Question 3
How does approximating the posterior mean by the MAP estimate (mode) affect the preservation of multiple modes in the posterior distribution? In Section 3.1, the method approximates the intractable posterior mean E[Ẋ₀|ẋₜ, y] with the MAP estimate to formulate the ODE step. Using the mode as a proxy for the mean is a strong assumption that may bias the sampler toward single high-probability regions, potentially collapsing distinct solutions in ambiguous inverse problems (e.g., phase retrieval).

## Limitations
- The theoretical justification for MAP approximation of the posterior mean relies on strong assumptions about posterior regularity and measurement model differentiability
- Hyperparameter sensitivity remains significant, with task-specific tuning of λ, N, and σ_restart required for optimal performance
- The claim that restart sampling universally contracts approximation errors lacks rigorous proof, with empirical evidence limited to specific architectures and noise schedules

## Confidence

- **High confidence**: Linear measurement models (super-resolution, inpainting, deblurring) - extensive empirical validation across multiple datasets
- **Medium confidence**: Nonlinear measurement models (phase retrieval, HDR) - fewer comparative baselines and limited hyperparameter exploration reported
- **Medium confidence**: Generalizability claims - while results span multiple tasks, the specific combination of VE-SDE, polynomial schedules, and restart parameters may not transfer to other diffusion architectures without retraining

## Next Checks

1. **Ablation study reproducibility**: Verify the ODE steps per restart trade-off (Figure 5) across both linear and nonlinear measurement models to confirm the 5-10 step sweet spot is architecture-independent
2. **Gradient descent convergence analysis**: Measure MAP optimization convergence rates across different N values to establish whether 10-20 steps consistently achieves sufficient accuracy or if early stopping occurs
3. **Cross-architecture generalization**: Test RePS with non-VE-SDE diffusion models (e.g., DDPM or NCSN++) to validate whether the Tweedie-based score formulation and restart strategy remain effective without architectural modifications