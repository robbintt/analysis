---
ver: rpa2
title: Improving the Convergence Rate of Ray Search Optimization for Query-Efficient
  Hard-Label Attacks
arxiv_id: '2512.21241'
source_url: https://arxiv.org/abs/2512.21241
tags:
- gradient
- attacks
- ars-opt
- attack
- pars-opt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of query-efficient hard-label
  black-box adversarial attacks, where only the top-1 predicted label is accessible.
  The authors propose ARS-OPT, a momentum-based optimization algorithm inspired by
  Nesterov's Accelerated Gradient, which proactively estimates gradients with respect
  to a future ray direction inferred from accumulated momentum.
---

# Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks

## Quick Facts
- arXiv ID: 2512.21241
- Source URL: https://arxiv.org/abs/2512.21241
- Reference count: 4
- The authors propose ARS-OPT and PARS-OPT, achieving O(1/T^2) convergence rate and outperforming 13 state-of-the-art methods in query efficiency for hard-label black-box attacks.

## Executive Summary
This paper addresses the challenge of query-efficient hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible. The authors propose ARS-OPT, a momentum-based optimization algorithm inspired by Nesterov's Accelerated Gradient, which proactively estimates gradients with respect to a future ray direction inferred from accumulated momentum. This approach enables more accurate directional updates and faster, more stable optimization. The authors further enhance ARS-OPT by incorporating surrogate-model priors into gradient estimation, resulting in PARS-OPT with improved performance. Theoretical analysis establishes an O(1/T^2) convergence rate under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that ARS-OPT and PARS-OPT outperform 13 state-of-the-art approaches in query efficiency, with PARS-OPT achieving the best mean ℓ2 distortion and attack success rates.

## Method Summary
The method addresses hard-label black-box adversarial attacks by reformulating the problem as finding the minimum ℓ2-norm perturbation through ray search optimization. ARS-OPT implements a momentum-based approach where gradients are estimated at a lookahead position (interpolated between current direction and accumulated momentum) rather than the current position. The algorithm maintains two coupled sequences - main and auxiliary - with different gradient estimators to enable stable accelerated convergence. PARS-OPT extends ARS-OPT by incorporating surrogate-model priors into gradient estimation, orthogonalizing them with random vectors to improve performance when surrogate transferability is high. The approach uses binary search to find boundary distances and sign-based finite differences for gradient estimation.

## Key Results
- ARS-OPT achieves O(1/T^2) convergence rate under standard assumptions, compared to O(ln(T)/T) for prior methods
- ARS-OPT and PARS-OPT outperform 13 state-of-the-art approaches in query efficiency on ImageNet and CIFAR-10
- PARS-OPT achieves the best mean ℓ2 distortion and attack success rates among all evaluated methods
- ARS-OPT maintains stability and performance even with fewer queries per iteration compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating gradients at a lookahead position rather than the current position accelerates convergence.
- Mechanism: The algorithm computes an interpolated direction $\tilde{\theta}_t = (1-\alpha_t)\theta_t + \alpha_t m_t$ by blending the current direction $\theta_t$ with accumulated momentum $m_t$, then estimates gradients at this predicted future location. This anticipates the optimization trajectory, analogous to Nesterov's insight of "looking ahead" before stepping.
- Core assumption: The optimization landscape is smooth enough that the momentum direction correlates with productive descent directions.
- Evidence anchors:
  - [abstract] "proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum"
  - [section 4] "rather than relying solely on the current slope, the walker looks ahead to anticipate the upcoming terrain and adjust the direction of motion accordingly"
  - [corpus] Weak direct support; neighbor papers focus on gradient estimation but not specifically lookahead mechanisms.
- Break condition: Highly non-convex landscapes with frequent local minima where momentum misleads the lookahead.

### Mechanism 2
- Claim: Maintaining two coupled sequences (main and auxiliary) with different gradient estimators enables stable accelerated convergence.
- Mechanism: The main sequence $\theta_t$ updates using a biased gradient estimator $g_1(\tilde{\theta}_t)$ optimized for directional updates, while the auxiliary momentum sequence $m_t$ updates using an unbiased estimator $g_2(\tilde{\theta}_t)$ that preserves convergence guarantees. The unbiased estimator corrects for the bias scaling factor $\frac{d}{\frac{2}{\pi}(q-1)+1}$ derived from the expected projection magnitude.
- Core assumption: $g_2(\tilde{\theta}_t)$ remains an unbiased estimator of $\nabla f(\tilde{\theta}_t)$; the interpolation coefficient $\alpha_t$ satisfies $\alpha_t^2 = \zeta_t \gamma_t(1-\alpha_t)$.
- Evidence anchors:
  - [abstract] "showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization"
  - [section 4, Theorem 4.1] Derives that $\mathbb{E}[g_1(\tilde{\theta}_t)] = \frac{1}{d}(\frac{2}{\pi}(q-1)+1) \cdot g$, proving the unbiased correction factor
  - [corpus] Not explicitly addressed in neighbors.
- Break condition: Gradient estimation noise overwhelms the signal; insufficient queries per iteration cause $g_2$ to violate unbiasedness.

### Mechanism 3
- Claim: Incorporating surrogate-model priors into gradient estimation reduces query cost when surrogate gradients correlate with target gradients.
- Mechanism: PARS-OPT computes prior directions $k_{t,i} = \nabla_\theta h(\theta_t, \lambda_t)$ from surrogate models, orthogonalizes them with random vectors via Gram-Schmidt, and incorporates them into both $g_1$ and $g_2$ estimators. The dimensionality correction factor adjusts from $d$ to $(d-s)$ to account for the $s$ prior directions.
- Core assumption: Surrogate model gradients have non-trivial alignment with target model gradients (measured by $\hat{D}_t$).
- Evidence anchors:
  - [abstract] "incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance"
  - [section 4, PARS-OPT] "An ideal prior would be the gradient of $\hat{f}(\theta)$ derived from a surrogate model"
  - [corpus] Prior-OPT (Ma et al. 2025) establishes the transfer-based prior framework this builds upon.
- Break condition: Surrogate models have low transferability to target (e.g., different architectures, adversarial training); priors mislead search.

## Foundational Learning

### Concept: Zeroth-order optimization
- Why needed here: The entire method relies on estimating gradients from finite differences when only function values (via binary search) are available.
- Quick check question: Given only sign(f(x+εu) - f(x)), how would you estimate the gradient direction?

### Concept: Nesterov's Accelerated Gradient
- Why needed here: ARS-OPT adapts NAG's lookahead principle to discrete optimization; understanding momentum interpolation is essential.
- Quick check question: Why does NAG compute gradients at θ - αv (lookahead) rather than θ?

### Concept: Hard-label attack setting
- Why needed here: Only top-1 labels are available; no confidence scores or gradients. This forces binary search for boundary distance.
- Quick check question: With only a binary indicator Φ(x_adv), how do you find the minimum perturbation distance?

## Architecture Onboarding
- Component map: Binary search module -> Lookahead positioner -> Gradient estimator (dual) -> Prior integrator (PARS-OPT only) -> Dual sequence updater
- Critical path: Initialize θ_0 → Binary search for f(θ_0) → Compute lookahead → Estimate g_1, g_2 → Update both sequences → Repeat
- Design tradeoffs:
  - More prior vectors (s) vs. random vectors (q-s): More priors help when transferability is high, hurt when it's low
  - Larger q: Better gradient estimation but more queries per iteration
  - Step size $\hat{L}$: Too large causes instability; too small wastes queries
- Failure signatures:
  - Distortion plateaus early: Momentum trapped in local region; reduce α_t
  - Distortion increases: Gradient estimation too noisy; increase q
  - PARS-OPT underperforms ARS-OPT: Poor prior quality (low $\hat{D}_t$); surrogate lacks transferability
- First 3 experiments:
  1. Reproduce ARS-OPT vs. Sign-OPT on CIFAR-10 with identical query budgets; plot distortion curves to verify O(1/T²) vs. O(ln(T)/T) gap.
  2. Ablate prior quality: Use randomly initialized surrogates vs. pretrained surrogates; measure $\hat{D}_t$ and final distortion to confirm prior alignment hypothesis.
  3. Stress test momentum: Run on adversarially trained models (high curvature boundaries) and compare ARS-OPT-S (subspace) vs. full-space ARS-OPT to assess robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the acceleration techniques in (P)ARS-OPT be extended to ℓ∞-norm or ℓ1-norm constrained hard-label attacks?
- Basis in paper: [explicit] The paper states RayS "removes gradient estimation entirely by using hierarchical search, but is limited to untargeted ℓ∞-norm attacks," while this work reformulates the problem specifically as finding "the minimum ℓ2-norm perturbation" (Eq. 3).
- Why unresolved: The theoretical analysis and gradient estimation framework are derived specifically for ℓ2 geometry; different norm constraints fundamentally alter the ray search formulation and boundary geometry.
- What evidence would resolve it: A theoretical extension showing convergence guarantees for ℓ∞/ℓ1 settings, plus empirical comparisons against norm-specific baselines on standard benchmarks.

### Open Question 2
- Question: What is the actual convergence rate of the practical Algorithm 1, given that the O(1/T²) guarantee applies only to an idealized version?
- Basis in paper: [explicit] Theorem 4.2 establishes the convergence guarantee for "the idealized version of Algorithm 1," and the authors acknowledge "Algorithm 1 is a practical approximation of an idealized version presented in Appendix A."
- Why unresolved: The practical algorithm uses estimated values (̂Dt, ∥∇̂ft∥2) rather than exact quantities, and performs operations like gradient clipping that may violate theoretical assumptions.
- What evidence would resolve it: Empirical convergence curves comparing the gap between idealized and practical versions, or tighter analysis accounting for estimation errors.

### Open Question 3
- Question: How robust is (P)ARS-OPT under violations of the smoothness and convexity assumptions made in the theoretical analysis?
- Basis in paper: [explicit] Theorem 4.2 requires "f(·) is smooth and convex," yet adversarial loss landscapes in deep networks are typically non-convex with discontinuous decision boundaries.
- Why unresolved: Real-world adversarial optimization operates on complex, non-smooth boundaries; the theory provides no guarantees when assumptions are violated.
- What evidence would resolve it: Systematic experiments measuring convergence degradation on synthetically perturbed non-smooth/non-convex landscapes, or analysis under relaxed assumptions.

## Limitations
- The O(1/T²) convergence rate claim is theoretical and assumes convexity; real adversarial landscapes are highly non-convex, so practical convergence may be significantly slower.
- The effectiveness of PARS-OPT heavily depends on the transferability of surrogate models to target models, which varies substantially across architectures and adversarial training regimes.
- The binary search step for boundary distance estimation introduces substantial query overhead, particularly in high-dimensional spaces, which may offset gains from accelerated convergence.

## Confidence
- **High confidence**: ARS-OPT outperforms Sign-OPT and DUV in query efficiency on standard benchmarks (CIFAR-10, ImageNet). The dual-sequence momentum mechanism with lookahead is sound and demonstrably improves stability.
- **Medium confidence**: The O(1/T²) convergence rate holds in practice, given that the theoretical analysis assumes convexity but real adversarial landscapes are non-convex.
- **Low confidence**: PARS-OPT consistently outperforms ARS-OPT across all target models and attack settings; surrogate prior quality and transferability vary substantially.

## Next Checks
1. Conduct a controlled ablation study comparing ARS-OPT-S (subspace) vs full-space ARS-OPT on adversarially trained models to assess robustness to curvature variations.
2. Measure surrogate gradient transferability (correlation between ∇h and ∇f) across different architecture pairs to establish when PARS-OPT is expected to help vs hurt.
3. Profile query distribution: fraction used for binary search vs gradient estimation in ARS-OPT vs Prior-OPT to quantify the practical cost of the momentum-based acceleration.