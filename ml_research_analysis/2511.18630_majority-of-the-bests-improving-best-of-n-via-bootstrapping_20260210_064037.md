---
ver: rpa2
title: 'Majority of the Bests: Improving Best-of-N via Bootstrapping'
arxiv_id: '2511.18630'
source_url: https://arxiv.org/abs/2511.18630
tags:
- answer
- reward
- adaptive
- distribution
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve the Best-of-N (BoN) selection
  algorithm by estimating the distribution of BoN's outputs via bootstrapping and
  selecting the mode. The key idea is that, while BoN often fails to reliably select
  the correct answer with imperfect rewards, the correct answer is frequently the
  most probable outcome of BoN.
---

# Majority of the Bests: Improving Best-of-N via Bootstrapping

## Quick Facts
- **arXiv ID**: 2511.18630
- **Source URL**: https://arxiv.org/abs/2511.18630
- **Reference count**: 40
- **Primary result**: MoB improves Best-of-N selection by estimating BoN's output distribution via bootstrapping and selecting the mode, outperforming BoN in 25/30 experimental setups.

## Executive Summary
This paper addresses the fundamental limitation of Best-of-N (BoN) selection: with imperfect reward models, BoN's single selection is often wrong despite the correct answer being the most probable outcome. MoB (Majority of the Bests) solves this by estimating the distribution of BoN outputs using bootstrapping and selecting the mode. The method shows consistent improvements across five benchmarks, three base LLMs, and two reward models. The theoretical foundation relies on m-out-of-n bootstrapping to properly estimate extreme value distributions, with an adaptive procedure for selecting the optimal subsample size that eliminates hyperparameter tuning.

## Method Summary
MoB generates N outputs per question using a base LLM, scores them with a reward model, then uses bootstrapping to estimate the distribution of BoN winners. Instead of selecting the single highest-reward output, MoB resamples subsets of size m (typically √N) with replacement, runs BoN on each subset, and aggregates the frequency of each answer winning. The answer with highest frequency under this estimated distribution is selected. An adaptive procedure can automatically determine the optimal m by minimizing the distance between distributions estimated at different subsample sizes.

## Key Results
- MoB outperforms BoN in 25 out of 30 experimental setups across five benchmarks
- Consistent improvements observed across three base models (Qwen2.5-3B, Llama-3.1-8B, Gemma-2-9B) and two reward models
- The adaptive m selection performs nearly as well as the optimal fixed m without requiring hyperparameter tuning
- Theoretical results support the consistency of the bootstrapping approach under mild tail conditions

## Why This Works (Mechanism)

### Mechanism 1: Mode Selection over Stochastic Sampling
- **Claim**: Selecting the mode of BoN's output distribution yields higher accuracy than a single BoN sample when rewards are imperfect.
- **Mechanism**: BoN with imperfect rewards exhibits stochastic behavior; the correct answer is often the most probable outcome (mode) but not selected with certainty in a single run. MoB estimates this distribution π_m via bootstrapping and selects the most frequent winner, thereby reducing the variance inherent in a single selection.
- **Core assumption**: The correct answer appears as the mode of the BoN distribution more often than incorrect answers.
- **Evidence anchors**: [abstract], [Section 1], [corpus]

### Mechanism 2: m-out-of-n Bootstrapping
- **Claim**: Estimating the BoN output distribution requires resampling subsets of size m < N rather than standard bootstrapping.
- **Mechanism**: BoN selects the maximum reward, where standard bootstrapping fails because the original maximum appears in ~63.2% of resamples, biasing the distribution. Using m < N (where m → ∞ and m/N → 0) introduces sufficient variance to approximate the true tail behavior of the reward distribution.
- **Core assumption**: The distribution of rewards has mild tail properties satisfying Theorem 1 conditions.
- **Evidence anchors**: [Section 4.2], [Section 4.2], [corpus]

### Mechanism 3: Adaptive Subsample Size Selection
- **Claim**: An adaptive procedure for selecting m eliminates the need for hyperparameter tuning while maintaining near-optimal performance.
- **Mechanism**: The optimal m balances signal strength (large m) against estimation error (small m). The method minimizes the distance ||π̂_m,N - π̂_qm,N||_1 for candidates m_j = ⌊q^j N⌋, approximating the unknown minimization of error against the true distribution.
- **Core assumption**: The intrinsic structure of the problem allows a fixed schedule of candidates to capture the optimal trade-off point.
- **Evidence anchors**: [Section 4.3], [Section 4.3, Figure 5], [corpus]

## Foundational Learning

- **Concept: Best-of-N (BoN)**
  - **Why needed here**: MoB is a wrapper around BoN. You must understand that BoN selects the single output with the highest score from N samples and that this process is noisy with imperfect rewards.
  - **Quick check question**: If I have 100 samples and the reward model is perfect, does MoB improve over BoN? (Answer: Marginally or No, because BoN is already near-perfect).

- **Concept: Bootstrapping**
  - **Why needed here**: MoB uses resampling with replacement to simulate multiple BoN trials. Understanding that this creates an empirical distribution of "winners" is central.
  - **Quick check question**: Why can't we just run BoN on the full dataset k times? (Answer: We only have one fixed set of generated samples; bootstrapping simulates variance without generating more).

- **Concept: Mode vs. Mean**
  - **Why needed here**: The paper explicitly targets the *mode* (most frequent value) of the distribution, not the average. This aligns with discrete answer spaces (e.g., multiple choice).
  - **Quick check question**: In a set of BoN winners [A, A, B, C, A], what is the mode?

## Architecture Onboarding

- **Component map**: Generator -> Scorer -> MoB Core (Sorter -> Resampler -> Aggregator)
- **Critical path**: Generate N completions (GPU bound) → Score N completions (GPU/CPU bound) → Run MoB selection logic (CPU bound, negligible cost)
- **Design tradeoffs**: Fixed m vs. Adaptive: Adaptive requires computing estimates for log(N) candidates, slightly more CPU overhead than fixed m=√N, but removes a hyperparameter. Computational Budget: MoB adds almost zero latency compared to the generation step (B=10,000 bootstrap iterations are computationally free relative to LLM inference).
- **Failure signatures**: Dominance of Single Output: If one output has a reward massively higher than all others, MoB will always select it, behaving exactly like BoN. Broken Reward Model: If rewards are anti-correlated with correctness, MoB will confidently select the wrong mode.
- **First 3 experiments**: 1) Sanity Check (Synthetic): Generate data with known noise distributions. Verify that MoB succeeds where BoN fails when 0.5 < BoN Success Prob < 0.9. 2) m Sensitivity Analysis: On a validation set, sweep m from N^0.2 to N^0.8. Plot accuracy to verify the "U-shape" or trade-off curve implies an optimal m < N. 3) Efficiency Validation: Implement the closed-form calculation from Appendix B. Compare wall-clock time of the bootstrap loop vs. the closed-form calculation to verify the claimed efficiency.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can MoB's bootstrapped selection signal be utilized to implement efficient early stopping criteria during parallel LLM generation? The paper proposes the concept but provides no implementation or experimental validation of an early stopping mechanism.

- **Open Question 2**: Can MoB be effectively extended to open-ended generation tasks that lack a discrete final answer? The current algorithm relies on majority voting over discrete outputs, a mechanism that fails when answers are continuous or semantically diverse.

- **Open Question 3**: How does MoB perform when integrated into complex inference frameworks like tree search? The current evaluation is restricted to sample-and-marginalize setups and does not cover interaction with tree search algorithms.

## Limitations

- The empirical claims rely heavily on specific reward models that may not generalize to other reward model architectures or domains.
- Theoretical guarantees assume specific tail properties of the reward distribution that may not hold for all real-world datasets.
- The paper focuses exclusively on single-answer multiple-choice tasks, leaving open questions about MoB's effectiveness on open-ended generation tasks or multi-answer scenarios.

## Confidence

- **High Confidence** (Experimental Claims): The claim that MoB outperforms BoN in 25/30 experimental setups is well-supported by the presented results.
- **Medium Confidence** (Theoretical Claims): The bootstrapping theory provides a reasonable mathematical foundation, but the conditions required create tension with practical implementation.
- **Medium Confidence** (Mechanism Claims): The core claim that selecting the mode of BoN's output distribution improves accuracy is plausible and supported by results.

## Next Checks

1. **Robustness to Reward Model Quality**: Systematically vary the quality of the reward model and measure the point at which MoB's advantage over BoN diminishes. This would validate the claim that MoB works best when "0.5 < BoN Success Probability < 0.9."

2. **Cross-Domain Generalization**: Apply MoB to non-mathematical benchmarks like medical question answering or code generation to verify the claim that the method generalizes beyond the tested domains.

3. **Theoretical Bound Validation**: Conduct experiments that test the theoretical bounds in Theorem 1 by varying N and m systematically, then measuring the actual convergence of the estimated distribution to the true distribution.