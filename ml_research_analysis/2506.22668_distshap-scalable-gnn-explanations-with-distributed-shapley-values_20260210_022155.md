---
ver: rpa2
title: 'DistShap: Scalable GNN Explanations with Distributed Shapley Values'
arxiv_id: '2506.22668'
source_url: https://arxiv.org/abs/2506.22668
tags:
- graph
- edges
- gpus
- distshap
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining graph neural network
  (GNN) predictions at scale. The core problem is that attributing predictions to
  specific edges in large graphs requires evaluating millions of subgraphs, which
  is computationally infeasible with existing methods.
---

# DistShap: Scalable GNN Explanations with Distributed Shapley Values

## Quick Facts
- **arXiv ID:** 2506.22668
- **Source URL:** https://arxiv.org/abs/2506.22668
- **Reference count:** 40
- **Primary result:** DistShap achieves higher explanation fidelity than state-of-the-art methods while reducing runtime by orders of magnitude through distributed Shapley value computation across GPUs.

## Executive Summary
This paper addresses the fundamental challenge of explaining graph neural network predictions at scale. Traditional Shapley value-based explanations require evaluating millions of subgraphs, making them computationally infeasible for large graphs. DistShap introduces a distributed algorithm that leverages multiple GPUs to accelerate Shapley value computation through computational graph replication, batched inference, and distributed least squares solving. The method achieves near-linear speedup with GPU count and enables explaining GNNs on graphs that would otherwise be intractable.

## Method Summary
DistShap computes edge importance scores for GNN predictions using Shapley values distributed across GPUs. The algorithm extracts the computational graph (edges within L hops) for each target node, replicates this subgraph across all GPUs, generates coalition samples in parallel with paired complementary subgraphs, performs batched GNN inference using block-diagonal adjacency matrices, and solves distributed weighted least squares problems using Conjugate Gradients Least Squares (CGLS) to compute Shapley values. This approach eliminates inter-GPU communication during model prediction while maintaining high explanation fidelity.

## Key Results
- DistShap achieves 11.1× to 13.5× speedup when scaling from 8 to 128 GPUs
- Consistently outperforms baselines (GNNExplainer, GraphLIME, PGExplainer, GraphMask, PGExplainer-β) in Fidelity+ and Fidelity- scores across six real-world datasets
- Scales to graphs with up to 37.8M edges in computational graphs using 128 NVIDIA A100 GPUs
- Model prediction dominates runtime (~90%+), while sampling and CGLS are comparatively fast

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-uniform coalition sampling with complementary pairs improves Shapley approximation quality while enabling load-balanced GPU distribution.
- **Mechanism:** The algorithm samples coalitions preferentially from small and large coalition sizes using Kernel SHAP weights, assigning higher probability to sizes where marginal contributions are more observable. Each GPU generates both members of complementary sample pairs (subgraph with k edges and its complement with n-k edges), ensuring each GPU processes exactly k/p samples and nk/2p edges regardless of which specific coalitions are sampled.
- **Core assumption:** Marginal contributions of edges are more informative at coalition extremes (very sparse or very dense subgraphs), and the paired symmetric sampling accelerates convergence to true Shapley values.
- **Evidence anchors:**
  - [Section 3.2.2] "This distribution favors sampling coalitions with very few or very many edges, as these tend to provide more informative signals for explaining the prediction."
  - [Section 3.2.2] "This pairing of complementary samples helps the algorithm converge more quickly to the true Shapley values."
  - [corpus] Weak direct corpus evidence; related work on Shapley acceleration focuses on tree models (GPUTreeShap) or Spark-based approaches limited to ~50 players.

### Mechanism 2
- **Claim:** Block-diagonal batched inference maximizes GPU utilization without inter-GPU communication during model prediction.
- **Mechanism:** Multiple copies of the computational graph's adjacency matrix are stacked along the diagonal of a larger sparse matrix. A corresponding block-diagonal mask matrix applies edge selections for each sample. Element-wise product yields batched subgraphs, enabling simultaneous GNN forward passes. Each GPU processes its local batch independently since computational graphs are replicated.
- **Core assumption:** The computational graph fits in GPU memory (empirically validated up to 37.8M edges for Reddit dataset requiring 1.04GB), and batch size can be tuned to saturate GPU compute without exceeding memory.
- **Evidence anchors:**
  - [Section 3.3] "Batching improves parallel efficiency but comes at the cost of increased memory usage."
  - [Figure 12] Shows batch size 50 provides optimal runtime-memory tradeoff on ogbn-products and Reddit datasets.
  - [corpus] No direct corpus evidence for this specific batching strategy in GNN explanation context.

### Mechanism 3
- **Claim:** Distributed Conjugate Gradients Least Squares (CGLS) solver outperforms direct methods for Shapley computation at scale.
- **Mechanism:** Rather than explicitly forming and communicating the n×n Gram matrix M^T W M (requiring O(n²) communication), CGLS iteratively solves the weighted least-squares problem using only matrix-vector products. Each GPU stores k/p rows of mask matrix M locally; all-reduce operations synchronize n-dimensional vectors and scalars each iteration. Communication cost is O(i·log p) latency + O(i·n) bandwidth for i iterations.
- **Core assumption:** The mask matrix M (50% sparse) is sufficiently well-conditioned that CGLS converges in fewer than n iterations, making iterative communication cheaper than the O(n²) direct method communication.
- **Evidence anchors:**
  - [Section 3.4] "We show that Algorithm 1 is faster than solving the weighted least-squares problem using a parallel direct method despite the additional latency cost."
  - [Figure 13] CGLS achieves identical fidelity to direct WLS while being significantly faster as player count increases.
  - [corpus] Related distributed Shapley work uses Spark/PySpark but is limited to small player counts; CGLS is standard for large-scale least squares but not previously applied to GNN explanation.

## Foundational Learning

- **Concept: Shapley Values and Marginal Contributions**
  - **Why needed here:** Core mathematical framework DistShap approximates. Understanding that Shapley values decompose predictions as weighted averages of marginal contributions across all possible coalitions is essential for interpreting the sampling and regression objectives.
  - **Quick check question:** For a computational graph with 100 edges, why can't we compute exact Shapley values, and what does the weighted least-squares formulation in Eq. 3 approximate?

- **Concept: GNN Computational Graphs and Message Passing**
  - **Why needed here:** The explanation target is edges within the L-hop computational graph that contribute to a node's prediction. Understanding that predictions only depend on this local subgraph (not the full graph) explains why replication is memory-feasible.
  - **Quick check question:** For a 3-layer GCN predicting node v, which edges influence the prediction, and why does DistShap only sample from this subset?

- **Concept: Data Parallelism and All-Reduce Communication**
  - **Why needed here:** DistShap's scalability hinges on minimizing inter-GPU communication. Understanding 1D row partitioning, when all-reduce is needed (norm computations, matrix-vector products with transposed masks), and Hockney's communication model enables predicting scaling bottlenecks.
  - **Quick check question:** In Algorithm 1, why do lines 6 and 11 require all-reduce operations while lines 9 and 13-14 do not?

## Architecture Onboarding

- **Component map:**
  CPU: Extract computational graphs Gc(v) -> GPU_i: Replicate Gc(v) + Generate local mask rows -> GPU_i: Batch mask × adjacency → batched subgraphs -> GPU_i: Batched GNN forward pass → predictions ŷ_p -> All GPUs: Distributed CGLS with all-reduce -> CPU: Rank edges by Shapley values, return top-k

- **Critical path:** Model prediction (batched GNN inference) dominates runtime (~90%+ of total time per Figure 9). The 600K forward passes per node explanation is the primary bottleneck; sampling and CGLS are comparatively fast.

- **Design tradeoffs:**
  - **Batch size:** Larger batches improve GPU utilization but linearly increase memory. Paper finds 50 optimal on A100 80GB (Figure 12).
  - **Sample count:** More samples improve fidelity with diminishing returns (saturation at ~600K for large graphs per Figure 6).
  - **GPU count vs. communication:** Model prediction scales near-linearly; CGLS communication overhead grows with GPU count but remains ~2% of CGLS runtime even at 128 GPUs.
  - **Replication vs. partitioning:** Replicating computational graphs eliminates sampling/prediction communication but limits applicability to graphs fitting in GPU memory.

- **Failure signatures:**
  - **OOM during replication:** Computational graph exceeds GPU memory (likely with deep GNNs >3 layers on dense graphs). Signature: memory error before sampling begins.
  - **OOM during batching:** Batch size too large. Signature: memory error during forward pass. Remedy: reduce batch size.
  - **CGLS non-convergence:** Poorly conditioned mask matrix. Signature: iteration count approaches n without convergence. Not observed in paper experiments.
  - **Speedup plateau:** Communication begins dominating computation. Signature: sub-linear scaling when adding GPUs beyond problem-appropriate count (observed slight slowdown from 64→128 GPUs for sampling in Figure 11).

- **First 3 experiments:**
  1. **Baseline fidelity validation:** On a small dataset (Coauthor-CS), run DistShap with k=60K samples on single GPU. Compute Fidelity+ for top-k edges (k∈{10,20,30,40,50}) and compare against paper's Figure 7 values. This validates implementation correctness before scaling.
  2. **Batch size sweep:** On ogbn-products with a node having ~100K edges in computational graph, sweep batch sizes {10, 25, 50, 75, 100} on single GPU. Measure prediction time and memory to reproduce Figure 12's optimum, confirming hardware-specific tuning needs.
  3. **Weak scaling test:** Fix samples-per-GPU ratio (e.g., 50K samples per GPU), scale from 8→16→32→64→128 GPUs on Reddit dataset. Measure prediction time, CGLS time, and total time. Verify near-linear scaling matches Figure 9; identify where communication overhead begins degrading returns.

## Open Questions the Paper Calls Out
- **Question:** Can DistShap efficiently explain predictions from deeper GNN architectures (3+ layers) without encountering memory bottlenecks from larger computational graphs?
  - **Basis in paper:** [explicit] "With deeper GNNs, our approach of replicating the computational graph on GPUs could lead to memory bottlenecks. Our future work will focus on explaining deeper GNNs..."
  - **Why unresolved:** All experiments use two-layer GNNs; deeper architectures expand computational graphs substantially, potentially exceeding GPU memory capacity.
  - **What evidence would resolve it:** Experiments on 4-6 layer GNNs showing memory usage and scalability patterns; benchmarks on heterophilic graphs where deeper GNNs are beneficial.

- **Question:** How does DistShap perform on link prediction and graph classification tasks compared to node classification?
  - **Basis in paper:** [inferred] The paper evaluates only node classification across all six datasets. The method assumes a single target node v with a defined computational graph; link prediction and graph classification have different prediction formulations.
  - **Why unresolved:** The methodology focuses exclusively on node-level predictions; extending to edge-level or graph-level predictions may require adapting the coalition definition and Shapley value computation.
  - **What evidence would resolve it:** Comparative experiments on link prediction benchmarks (e.g., ogbl-ddi) and graph classification datasets (e.g., molecular property prediction) with Fidelity+ and Fidelity- metrics.

## Limitations
- **Memory constraints:** The replication strategy requires computational graphs to fit in GPU memory, limiting applicability to very deep GNNs or extremely dense graphs.
- **Communication overhead:** While CGLS scales well, communication overhead grows with GPU count and may dominate at extreme scales beyond 128 GPUs.
- **Single-target focus:** Current implementation explains one node at a time; batch processing multiple target nodes could improve efficiency but would require different architectural approaches.

## Confidence
- **High Confidence:** Core algorithmic mechanisms (non-uniform sampling, batched inference, distributed CGLS) and their implementation details are well-specified and reproducible.
- **Medium Confidence:** Scaling results and performance improvements are validated on the Perlmutter supercomputer but may vary on different hardware architectures or with network heterogeneity.
- **Medium Confidence:** Fidelity improvements over baselines are statistically significant across datasets, though the specific ranking of methods may shift with different random seeds or hyperparameter settings.

## Next Validation Checks
1. **Weak scaling validation:** Replicate the weak scaling experiment (fixed samples-per-GPU) across 8→128 GPUs on a medium-sized dataset (ogbn-products) to verify near-linear speedup and identify communication bottlenecks.
2. **Memory limit exploration:** Test DistShap on computational graphs approaching GPU memory limits (70-80GB) with varying GNN depths (2→4→6 layers) to identify when replication strategy fails.
3. **Baselines on equal footing:** Implement and run all baseline methods (GNNExplainer, GraphLIME, PGExplainer, GraphMask, PGExplainer-β) with identical computational graph extraction and sample budgets to verify Fidelity+ improvements are method-specific rather than implementation-dependent.