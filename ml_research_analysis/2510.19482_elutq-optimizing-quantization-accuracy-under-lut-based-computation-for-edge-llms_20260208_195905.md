---
ver: rpa2
title: 'ELUTQ: Optimizing Quantization Accuracy under LUT-Based Computation for Edge
  LLMs'
arxiv_id: '2510.19482'
source_url: https://arxiv.org/abs/2510.19482
tags:
- quantization
- memory
- table
- llama3
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ELUTQ introduces Hierarchical Linear Quantization (HLQ) to improve
  low-bit quantization of LLMs. HLQ uses non-uniform quantization with multiple per-bit
  scale factors to better fit weight distributions while remaining hardware-friendly.
---

# ELUTQ: Optimizing Quantization Accuracy under LUT-Based Computation for Edge LLMs

## Quick Facts
- arXiv ID: 2510.19482
- Source URL: https://arxiv.org/abs/2510.19482
- Authors: Xin Nie; Liang Dong; Haicheng Zhang; Jiawang Xiao; G. Sun
- Reference count: 40
- Primary result: 2-bit LLaMA3.1-8B achieves 15.08 perplexity on C4, 1.5× faster than AWQ on RTX 3090

## Executive Summary
ELUTQ introduces Hierarchical Linear Quantization (HLQ) to improve low-bit quantization of LLMs. HLQ uses non-uniform quantization with multiple per-bit scale factors to better fit weight distributions while remaining hardware-friendly. The method includes an efficient alternating optimization algorithm and a lightweight finetuning pipeline that avoids full weight retraining. ELUTQ enables 2-bit LLaMA3.1-8B to achieve 15.08 perplexity on C4, outperforming GPTQ, AWQ, and OmniQuant. The framework supports bit-serial LUT-based GEMM, delivering 1.5× speedup over AWQ on RTX 3090. ELUTQ quantizes LLaMA3.1-70B in 40 hours using 64 GB CPU and 48 GB GPU memory, significantly reducing hardware requirements. It achieves higher throughput than prior methods on CPUs and GPUs, with gains increasing for larger models and lower bit-widths.

## Method Summary
ELUTQ proposes Hierarchical Linear Quantization (HLQ) for low-bit LLM quantization. HLQ uses non-uniform quantization with per-bit scale factors to better approximate bell-shaped weight distributions. The method employs an alternating optimization algorithm to jointly optimize bit patterns and scales, plus a two-stage finetuning pipeline (block-wise reconstruction followed by end-to-end scale-only updates). For efficient inference, ELUTQ implements bit-serial LUT-based GEMM that eliminates dequantization overhead. The system layer supports lazy loading, chunked computation, and disk offloading to enable 70B model quantization on consumer hardware with minimal memory footprint.

## Key Results
- 2-bit LLaMA3.1-8B achieves 15.08 perplexity on C4, outperforming GPTQ, AWQ, and OmniQuant
- 1.5× speedup over AWQ on RTX 3090 using bit-serial LUT-based GEMM
- Quantizes LLaMA3.1-70B in 40 hours using only 64 GB CPU and 48 GB GPU memory
- Higher throughput than prior methods on CPUs and GPUs, with gains increasing for larger models and lower bit-widths

## Why This Works (Mechanism)

### Mechanism 1
HLQ's hierarchical per-bit scales better approximate bell-shaped weight distributions than uniform quantization. Instead of a single scale factor, HLQ assigns independent scales to each bit plane: Ŵ = Σ(sⱼ · bⱼ) + z. This allows the quantizer to allocate representational capacity non-uniformly, concentrating precision near zero where LLM weights cluster. Alternating optimization (bit-pattern selection + least-squares reconstruction) jointly optimizes scales and zero-points. Core assumption: LLM weights follow bell-shaped/Gaussian-like distributions that uniform grids approximate poorly at low bit-widths.

### Mechanism 2
Bit-serial LUT-based GEMM eliminates dequantization overhead, enabling true linear speedup with lower bit-widths. Weights are offline-decomposed into q single-bit planes. At runtime, for activation groups of size g, all 2^g possible dot products are precomputed in a lookup table. Matrix multiplication becomes table lookups followed by summation, bypassing the unpack-to-FP16 step required by dequantization-based methods. Core assumption: Table lookup + sum is faster than dequantize + multiply-accumulate, especially as bit-width decreases.

### Mechanism 3
Lazy loading + disk offloading + chunked computation enables 70B model quantization on consumer hardware. Transformer blocks are quantized independently; only one block loads at a time (lazy loading). Hidden states are batched from disk rather than fully resident in CPU RAM. Large weight matrices are split into chunks for parameter search, scaling memory linearly with partition granularity. Core assumption: Disk I/O overhead is small relative to quantization compute time for large models.

## Foundational Learning

- Concept: Uniform vs Non-uniform Quantization
  - Why needed here: HLQ is a non-uniform scheme; understanding why uniform quantization fails at low bits is prerequisite to appreciating the design motivation.
  - Quick check question: Given weights clustered near zero with a few outliers, would uniform or non-uniform quantization give lower reconstruction error at 2 bits?

- Concept: Bit-serial Computation
  - Why needed here: ELUTQ's inference kernel decomposes multi-bit weights into single-bit planes processed serially; this is the foundation of the LUT-based speedup.
  - Quick check question: How many table lookups are required to compute the dot product of a 4-bit weight vector with an activation vector of group size 8?

- Concept: Alternating Optimization
  - Why needed here: HLQ's scales and zero-points are optimized by alternating between discrete selection (which bit pattern) and continuous regression (what scales).
  - Quick check question: In Algorithm 1, what would happen if you skipped the LSE step and only updated bit patterns?

## Architecture Onboarding

- Component map: Load block from disk -> HLQ alternating optimization -> Block-wise reconstruction -> Offload to CPU -> End-to-end tuning -> Convert to BCQ -> Bit-serial LUT-based inference

- Critical path:
  1. Load one transformer block from disk
  2. For each linear layer: run HLQ alternating optimization to get W_int, s, z
  3. Block-wise reconstruction: freeze W_int, optimize s, z to minimize block output error
  4. Offload quantized block to CPU; repeat for all blocks
  5. End-to-end tuning: load quantized model, update only scales s on calibration data
  6. Deploy: convert HLQ → BCQ format, run bit-serial LUT-based inference

- Design tradeoffs:
  - BPW vs accuracy: HLQ at 2-bit uses 2.37 BPW (vs 2.25 for uniform) due to per-bit scales; ~0.1GB extra for LLaMA3.1-8B but no added compute cost.
  - Group size g: Smaller g → finer-grained scales → better accuracy but more metadata; paper uses g=128 as default.
  - T_max: More iterations don't always help; Figure 6 shows diminishing returns beyond ~10 steps.

- Failure signatures:
  - Perplexity >1000 at 2-bit: Likely using RTN/naive uniform instead of HLQ + finetuning.
  - Inference slower at 2-bit than 3-bit: Using dequantization-based GEMM instead of LUT-based; AWQ shows this pattern.
  - OOM during quantization: Not using lazy loading/chunking; check if full model is loaded into GPU.

- First 3 experiments:
  1. **Sanity check**: Quantize LLaMA3.1-8B W2G128 with HLQ-only (no finetuning). Expect C4 perplexity ~17-20 (Figure 5). Compare against RTN baseline (~1000+).
  2. **Pipeline validation**: Run full ELUTQ (block-wise + E2E tuning) on LLaMA3.1-8B. Target: C4 perplexity ~15.08 (Table 1). Profile peak GPU/CPU memory.
  3. **Inference benchmark**: Deploy quantized model on RTX 3090. Measure tokens/s for 2-bit vs 3-bit. Expect 2-bit faster; if slower, verify LUT-based kernel is active (not dequantization fallback).

## Open Questions the Paper Calls Out

### Open Question 1
Can HLQ be effectively extended to weight-activation quantization to further enhance inference efficiency and reduce memory consumption? The authors state in Section 6: "The current study focuses exclusively on weight only quantization. Extending our framework to weight-activation quantization could further enhance inference efficiency and reduce memory consumption." The Bit-serial LUT-based GEMM paradigm is designed for weight-only scenarios; incorporating dynamic activations requires fundamentally different handling of the lookup table structure.

### Open Question 2
What algorithmic enhancements could close the accuracy gap between HLQ and codebook-based non-uniform quantization methods? Figure 1(a) shows ELUTQ "remains slightly behind codebook-based non-uniform approaches" for 2-bit quantization on average accuracy. The paper does not investigate why HLQ's hierarchical linear structure underperforms codebook methods at equivalent bit-widths, nor explores hybrid approaches.

### Open Question 3
Why does increasing the alternating optimization steps (T_max) not lead to continuous perplexity improvement, and what determines the optimal stopping point? Figure 6 shows that "increasing T_max does not necessarily lead to continuous improvement" for both 3-bit and 2-bit settings across LLaMA3.1-8B and Qwen3-8B. The paper empirically sets T_max=10 without explaining the non-monotonic convergence behavior or providing theoretical analysis of the alternating optimization landscape.

## Limitations

- HLQ remains slightly behind codebook-based non-uniform approaches in accuracy at 2-bit quantization
- Memory overhead from LUT tables (2^g entries) could cause cache pressure for larger group sizes
- Disk I/O overhead (5-25% of tuning time) may become prohibitive with larger calibration datasets

## Confidence

- **High confidence**: The core HLQ quantization mechanism and its superiority over uniform quantization for bell-shaped weight distributions. The perplexity results (15.08 on C4 for 2-bit LLaMA3.1-8B) are well-supported by ablation studies.
- **Medium confidence**: The memory efficiency claims for 70B model quantization (40 hours vs 280+ hours for competitors). While Table 5 shows dramatic hardware requirements reduction, the disk I/O overhead could become prohibitive.
- **Low confidence**: The absolute inference speedup numbers (1.5× over AWQ on RTX 3090). The comparison depends heavily on specific kernel implementations and hardware characteristics that may not generalize across platforms.

## Next Checks

1. **Ablation of alternating optimization**: Run HLQ quantization with T_max=1 (no alternating optimization) versus T_max=10 on LLaMA3.1-8B. Measure the perplexity difference on C4 to quantify the contribution of the alternating optimization step beyond simple bit-pattern selection.

2. **Memory vs. speed tradeoff analysis**: Implement ELUTQ with varying activation group sizes (g=64, 128, 256) and measure both inference throughput and GPU memory usage. This will reveal whether the claimed speedup holds when accounting for LUT memory pressure and cache effects.

3. **Cross-platform inference validation**: Deploy the quantized 2-bit LLaMA3.1-8B on both RTX 3090 and a representative edge device (e.g., Apple M2). Compare tokens/second against AWQ and GPTQ to verify that the LUT-based speedup is consistent across hardware architectures, or identify platform-specific bottlenecks.