---
ver: rpa2
title: 'Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from
  LLMs Using Adaptive RMU'
arxiv_id: '2506.16548'
source_url: https://arxiv.org/abs/2506.16548
tags:
- unlearning
- score
- llms
- language
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of unlearning sensitive factual
  information from large language models (LLMs), addressing concerns around privacy,
  copyright compliance, and security. The authors apply an adaptive version of Representation
  Misdirection Unlearning (RMU), which steers model activations toward a random direction
  during training on forget data while preserving knowledge on retain data through
  a regularization term.
---

# Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU

## Quick Facts
- arXiv ID: 2506.16548
- Source URL: https://arxiv.org/abs/2506.16548
- Authors: Arjun Dosajh; Mihika Sanghi
- Reference count: 4
- Primary result: Achieved 4th place on SemEval-2025 Task 4 leaderboard for both 1B and 7B models using adaptive RMU

## Executive Summary
This paper presents an approach to unlearning sensitive factual information from large language models, addressing privacy, copyright, and security concerns. The authors adapt Representation Misdirection Unlearning (RMU) by steering model activations toward random directions during training on forget data while preserving general capabilities through regularization on retain data. Through systematic experimentation across different decoder layers, they identify optimal layer ranges for unlearning factual knowledge while maintaining overall model performance. The method demonstrates strong results on the SemEval-2025 Task 4 benchmark, achieving 4th place with competitive membership inference attack robustness and MMLU scores.

## Method Summary
The method adapts RMU for unlearning by computing a forget loss that pushes activations at target layers toward a scaled random unit vector, while a retain loss preserves knowledge by regularizing against a frozen reference model. The authors systematically test different consecutive three-layer combinations to identify optimal unlearning locations, finding that middle-to-later layers provide the best trade-off. An adaptive scaling factor is introduced to adjust the forget loss magnitude during training. The approach focuses on efficient fine-tuning by updating only the target layers and their immediate predecessors, rather than full model retraining.

## Key Results
- Achieved 4th place on official SemEval-2025 Task 4 leaderboard for both 1B and 7B models
- Best performance achieved from middle-to-later layers: 12-14 for 1B model, 24-26 for 7B model
- Strong membership inference attack (MIA) robustness while maintaining competitive MMLU benchmark scores
- Successfully unlearned sensitive information including PII, addresses, and SSNs from targeted layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Directing forget-set activations toward a random direction disrupts the model's ability to retrieve targeted factual knowledge.
- **Mechanism:** The forget loss computes MSE between model activations at a target layer and a scaled random unit vector. For each token in forget samples, the loss pushes activations toward c·u, where c is a scaling hyperparameter.
- **Core assumption:** Factual knowledge retrieval depends on specific activation patterns that can be systematically disrupted without full retraining.
- **Evidence anchors:** [abstract]: "steers model activations toward a random direction during training on forget data"; [Section 4.1, Eq. 1]: Formal definition of forget loss with random unit vector and scaling coefficient c
- **Break condition:** If forget-set activations are already near-random, the loss provides no gradient signal.

### Mechanism 2
- **Claim:** Regularizing retain-set activations against a frozen reference model preserves general capabilities while unlearning proceeds.
- **Mechanism:** The retain loss computes MSE between the unlearned model's activations and the frozen original model's activations on retain data. The weighted combination balances destruction of targeted knowledge against preservation of general knowledge.
- **Core assumption:** Retain-set and forget-set knowledge are sufficiently separable at the activation level that one can be disrupted while the other is preserved.
- **Evidence anchors:** [abstract]: "preserving knowledge on retain data through a regularization term"; [Section 4.1, Eq. 2-3]: Retain loss formula and weighted combination with α parameter
- **Break condition:** If forget and retain sets share significant representational overlap, regularization may restore forgotten knowledge.

### Mechanism 3
- **Claim:** Factual knowledge is more effectively unlearned from middle-to-later decoder layers.
- **Mechanism:** The authors systematically test all consecutive three-layer combinations and find layers 12-14 (1B) and 24-26 (7B) achieve the best trade-off between forgetting targeted information and maintaining general capabilities.
- **Core assumption:** Different decoder layers encode qualitatively different information types, with factual/episodic details concentrated in later layers.
- **Evidence anchors:** [abstract]: "unlearning from middle-to-later layers (12,13,14 for the 1B model; 24,25,26 for the 7B model) provides the best trade-off"; [Section 7]: "factual knowledge (e.g., phone numbers, Social Security Numbers, and addresses) is more effectively unlearned from later decoder layers"
- **Break condition:** Layer-wise specialization may vary across architectures; findings may not transfer directly to encoder-decoder or non-OLMo models.

## Foundational Learning

- **Concept:** Transformer decoder layer functions (hidden states, residual streams, layer normalization)
  - **Why needed here:** RMU operates directly on intermediate activations; understanding what each layer block computes is essential for interpreting why layer selection matters.
  - **Quick check question:** Can you explain why activations at layer 12 might encode different information than activations at layer 2?

- **Concept:** Loss function composition and gradient flow
  - **Why needed here:** The method combines two competing losses; understanding how gradients from each term interact during backpropagation is critical for debugging training dynamics.
  - **Quick check question:** What happens to the retain loss gradient if α is set too low? Too high?

- **Concept:** Membership Inference Attacks (MIA) and privacy leakage metrics
  - **Why needed here:** The final evaluation explicitly includes MIA robustness; understanding how attackers exploit confidence scores helps interpret why certain layer choices improve privacy.
  - **Quick check question:** Why would higher loss variance on forget-set inputs indicate successful unlearning from a privacy perspective?

## Architecture Onboarding

- **Component map:** Tokenized forget-set (D_f) and retain-set (D_r) samples -> Forward pass through unlearned model (M_u) and frozen reference model (M_f) -> Activation extraction at target layer ℓ -> Forget loss (MSE vs. scaled random vector) + α-weighted retain loss (MSE vs. frozen activations) -> Weight update for layers ℓ-2, ℓ-1, ℓ -> Unlearned model output

- **Critical path:** 
  1. Prepare forget/retain datasets with proper tokenization
  2. Initialize random unit vector u (sampled once, fixed throughout training)
  3. Select target layer range (primary hyperparameter)
  4. Set α (retain weight) and β (adaptive scaling factor)
  5. Train with alternating forget/retain batch sampling

- **Design tradeoffs:**
  - **Layer selection:** Earlier layers → better MMLU retention but poor MIA robustness; later layers → strong MIA protection with modest MMLU decline
  - **α value:** Higher α preserves capabilities but risks under-forgetting; lower α accelerates forgetting but may cause catastrophic collapse
  - **Random vector initialization:** Fixed u provides consistency but creates dependence on initial seed

- **Failure signatures:**
  - **Catastrophic forgetting:** MMLU drops below threshold (0.371 for 7B model)
  - **Under-forgetting:** Task aggregate score near zero despite training
  - **MIA vulnerability:** Score near 0.5 indicates no privacy improvement
  - **Training instability:** NaN losses if adaptive scaling produces extreme values

- **First 3 experiments:**
  1. **Layer sweep:** Replicate Table 2 by testing all consecutive three-layer combinations on a validation split; plot task aggregate vs. MIA vs. MMLU to verify the "later layers = better privacy" finding.
  2. **α sensitivity analysis:** Fix optimal layer range, vary α ∈ {0.1, 0.5, 1.0, 2.0, 5.0}; observe trade-off curve between forget effectiveness and retain preservation.
  3. **Architecture transfer test:** Apply same layer-selection heuristic to a different decoder-only model (e.g., Llama-1B); assess whether the proportional layer relationship holds across architectures.

## Open Questions the Paper Calls Out
None

## Limitations
- The mechanism by which random-direction activation steering specifically disrupts factual knowledge retrieval remains incompletely characterized
- Layer-wise specialization assumption (factual knowledge in later layers) lacks cross-architecture validation and has weak theoretical justification
- Method's performance on non-English datasets, multi-modal models, or encoder-decoder architectures remains unknown
- Adaptive scaling hyperparameter β sensitivity and optimal range are not explored

## Confidence
- **Mechanism 1 (Random direction steering):** Medium confidence - mathematical formulation is rigorous but empirical validation of why this specifically disrupts factual knowledge retrieval is limited
- **Mechanism 2 (Retain regularization):** High confidence - dual-objective optimization approach is well-established in the unlearning literature
- **Mechanism 3 (Layer-wise specialization):** Low confidence - empirical findings are specific to tested models, lack cross-architecture validation, and theoretical justification is minimal
- **Overall effectiveness:** Medium confidence - 4th place leaderboard ranking demonstrates competitive performance, but definitive conclusions about relative contribution of each component are premature without complete ablation studies

## Next Checks
1. **Cross-architecture layer mapping:** Apply the layer-selection heuristic (last ~20% of layers) to a different decoder-only architecture (e.g., Llama-1B) and measure whether the proportional relationship between layer index and unlearning effectiveness holds.

2. **Information localization ablation:** Conduct targeted experiments where forget-set samples contain only factual information versus only conceptual information, then measure unlearning effectiveness at different layer ranges.

3. **Random vector sensitivity analysis:** Systematically vary the initialization seed for the random unit vector u across multiple runs and measure variance in unlearning effectiveness.