---
ver: rpa2
title: Focus on Likely Classes for Test-Time Prediction
arxiv_id: '2505.03819'
source_url: https://arxiv.org/abs/2505.03819
tags:
- image
- e-03
- classes
- e-02
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether focusing on likely classes of
  a single in-domain sample can improve model predictions. The authors propose two
  test-time fine-tuning methods: Decreasing outputs of Out-of-Focus classes (doFo)
  and Increasing outputs for Focus classes (iFo).'
---

# Focus on Likely Classes for Test-Time Prediction

## Quick Facts
- arXiv ID: 2505.03819
- Source URL: https://arxiv.org/abs/2505.03819
- Reference count: 40
- One-line primary result: iFo improves prediction accuracy in majority of over 70 model-dataset pairs across text and image domains with average gain of 0.28 (p-value 0.004)

## Executive Summary
This paper proposes a test-time fine-tuning approach that focuses on likely classes to improve model predictions. The method identifies uncertain samples where the top two classes are close in probability, then applies a single gradient descent step to amplify features shared among these focus classes. The approach, called Increasing outputs for Focus classes (iFo), outperforms existing test-time adaptation methods by an average of 0.28 accuracy points across diverse model-dataset pairs. The method requires no auxiliary data, augmentations, or batching, operating on single in-domain samples.

## Method Summary
The method identifies uncertain samples using the probability gap between top-1 and top-2 classes (Δ₁,₂ < 0.16). For these samples, it performs a single gradient descent step to maximize the weighted sum of logits for the top-2 focus classes. The loss function L_iFo = -Σ_{c∈F} p̃_c(X)·f_c(X) amplifies features contributing positively to multiple likely classes simultaneously. A large learning rate (0.01-0.04) enables effective optimization in a single step, providing 8x speedup compared to multi-step approaches while maintaining comparable accuracy.

## Key Results
- iFo achieves average accuracy gain of 0.28 across over 70 model-dataset pairs
- Statistically significant improvement with p-value of 0.004
- Outperforms entropy minimization baseline (Tent) by 0.26 accuracy points
- Works across diverse domains including ImageNet and multiple text datasets

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Gating
Optimization is beneficial only when the model exhibits genuine uncertainty between its top predictions. A forward pass produces probability estimates. Uncertainty is quantified as Δ₁,₂ = p_top₁ - p_top₂. If Δ₁,₂ < 0.16, the model is uncertain and optimization proceeds; otherwise, the prediction is returned unchanged. This gates computation to cases where the correct class is likely among top candidates but not confidently selected.

### Mechanism 2: Shared Feature Amplification
Features shared among likely classes are more reliable discriminators under uncertainty than class-specific features. Identify focus classes F (top-2). Apply loss L_iFo = -Σ_{c∈F} p̃_c(X)·f_c(X) to maximize weighted logits of focus classes. Gradients amplify features contributing positively to multiple focus classes simultaneously.

### Mechanism 3: Logit-Space Optimization
Direct logit maximization aligns optimization strength with uncertainty rationale better than entropy minimization. Entropy minimization has non-linear coefficients approaching zero at maximum uncertainty, weakening updates when they are most needed. iFo uses constant coefficients (α=1), providing stronger updates at high uncertainty.

## Foundational Learning

- **Concept**: Test-Time Adaptation (TTA)
  - **Why needed here**: The method modifies model behavior at inference without training data, labels, or batching—only the single test sample.
  - **Quick check question**: What information can be used during TTA? Answer: Only the current test sample; no batch statistics, augmentations, or auxiliary data.

- **Concept**: Softmax Calibration and Uncertainty Quantification
  - **Why needed here**: The method relies on softmax probability differences as uncertainty proxies despite known miscalibration issues.
  - **Quick check question**: Why use Δ₁,₂ rather than entropy? Answer: Δ₁,₂ directly measures decision margin, indicating how much change flips predictions.

- **Concept**: Feature-Logit Coupling via Weights
  - **Why needed here**: Understanding how features contribute to class logits through weight parameters is essential for the theoretical motivation.
  - **Quick check question**: What is the "double growth effect"? Answer: Shared features contributing to multiple focus classes receive amplified gradients because updates from each class accumulate.

## Architecture Onboarding

- **Component map**: Input X → Forward pass f(X;θ) → Logits [f_c(X)] → Softmax p_c(X) → Uncertainty: Δ₁,₂ = p_top₁ - p_top₂ → If Δ₁,₂ < 0.16 → Focus classes F = {top-2} → Loss: L_iFo = -Σ p̃_c·f_c for c∈F → Single step: θ ← θ - η·∇L → Return: argmax_c f_c(X)

- **Critical path**:
  1. Forward pass must produce stable (if poorly calibrated) softmax outputs
  2. Uncertainty threshold d₁,₂=0.16 works across domains but may need tuning
  3. Learning rate selection: sweet spot 0.01–0.04
  4. Model reset between samples is mandatory—high LR corrupts model if accumulated

- **Design tradeoffs**:
  - Single-step (η≈0.02) vs multi-step (η≈0.005, 8 iterations): Similar accuracy, single-step 8× faster
  - All parameters vs normalization layers only: Paper optimizes all; restriction may improve stability but reduces gains
  - Top-2 vs top-k focus classes: Default top-2; more classes dilutes shared-feature signal
  - Weighted vs unweighted loss: Probability weighting helps more at higher LR

- **Failure signatures**:
  1. No prediction changes → LR too small or threshold too restrictive
  2. Performance degradation → LR too large (>0.05) or applied to confident predictions
  3. Inconsistent gains across architectures → WideResNet, AlexNet show anomalous behavior
  4. Instruction-tuned models → Noisy results with LR spread 5e-5 to 0.5

- **First 3 experiments**:
  1. Validate uncertainty criterion: On held-out data, compute top-k accuracy at different d₁,₂ thresholds. Confirm low d₁,₂ correlates with low top-1 accuracy, high top-2 accuracy, and large gap between them.
  2. Learning rate sweep: For a single model-domain pair, test LR from 1e-5 to 0.1. Plot ΔAcc and count of configurations with gains. Identify sweet spot (expected ~0.01–0.04).
  3. Compare vs entropy baseline: Implement Tent-style entropy minimization with identical uncertainty gating and single-step optimization on same sample subset. Verify iFo outperforms to isolate contribution of logit-space optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does decreasing outputs for out-of-focus classes (doFo) fail to improve accuracy while increasing outputs for focus classes (iFo) succeeds?
- Basis in paper: The authors state in the Limitations section that "empirical findings suggest that the assumption that reducing features of unlikely classes is beneficial does not hold consistently."
- Why unresolved: The paper suggests the failure might be due to aggressive hyperparameters or the invalid assumption that low-probability class features are irrelevant, but it does not isolate the specific mechanism of failure.
- What evidence would resolve it: An ablation study analyzing gradient interactions between shared and unique features during suppression, or isolating the "unlearning" effect on shared features.

### Open Question 2
- Question: Is there a fundamental asymmetry where enhancing strongly activated features is beneficial at test time but detrimental during offline training?
- Basis in paper: The Conclusion offers the conjecture that "reducing reliance on a few strongly activated features is beneficial during regular offline training..., whereas enhancing such features can be advantageous at test time."
- Why unresolved: This is proposed as a high-level rationale for the empirical results, but is not empirically tested in the paper.
- What evidence would resolve it: Comparative experiments training models with an iFo-like objective (feature amplification) and evaluating their test-time robustness versus standard cross-entropy models.

### Open Question 3
- Question: Can improving the calibration of uncertainty estimates significantly boost the accuracy gains of the iFo method?
- Basis in paper: The Limitations section notes the method depends on softmax probabilities which are "often poorly calibrated" and states "Applying calibration techniques might improve results."
- Why unresolved: The paper uses raw softmax differences as a proxy for uncertainty but acknowledges these may be poorly calibrated, leaving the interaction between calibration and method performance unexplored.
- What evidence would resolve it: Experiments combining iFo with standard calibration techniques (e.g., temperature scaling) to measure the correlation between calibration error and accuracy improvement Δ Acc.

### Open Question 4
- Question: Do general theorems exist to formally characterize why single-step optimization on shared features improves prediction robustness?
- Basis in paper: Section 6 (Limitations) states the theoretical model "lacks general theorems necessary to conclusively answer the research question."
- Why unresolved: The current theoretical motivation relies on a simple three-class model and intuition regarding "double growth," which is insufficient to prove generalizability.
- What evidence would resolve it: Mathematical proofs relating the "sharedness" of features in high-dimensional spaces to generalization bounds or convergence guarantees for the proposed loss function.

## Limitations

- Architecture sensitivity: Method shows anomalous behavior on wide ResNet and AlexNet architectures
- Instruction-tuned model instability: High variance in learning rate sensitivity for instruction-tuned language models
- Single-step vs multi-step trade-off: Theoretical analysis suggests multi-step could amplify gains but risks exploding coefficients

## Confidence

**High Confidence Claims:**
- Statistical significance of iFo's accuracy gain (p-value 0.004) across 70+ model-dataset pairs
- Uncertainty gating improving computational efficiency
- Superiority of logit-space optimization over entropy minimization

**Medium Confidence Claims:**
- "Double growth effect" for shared features among focus classes
- Stability of single-step optimization compared to multi-step approaches
- Generalizability across diverse domains

**Low Confidence Claims:**
- Optimal learning rate ranges for specific architectures
- Performance on instruction-tuned language models
- Benefits of focusing on more than top-2 classes

## Next Checks

1. **Architecture-Specific Validation**: Test iFo on a broader range of CNN architectures (MobileNet, EfficientNet, ViT variants) to establish which architectures benefit most and identify potential architectural constraints or requirements.

2. **Cross-Domain Calibration Study**: Evaluate softmax calibration quality before and after iFo optimization across all tested domains to determine if the method introduces or reduces calibration errors, and whether uncertainty quantification remains reliable.

3. **Multi-Step Stability Analysis**: Systematically vary the number of optimization steps (1-10) with both small and large learning rates to identify the precise conditions under which "exploding coefficients" occur and establish safe operational boundaries.