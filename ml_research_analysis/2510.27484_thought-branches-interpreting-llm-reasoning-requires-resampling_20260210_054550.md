---
ver: rpa2
title: 'Thought Branches: Interpreting LLM Reasoning Requires Resampling'
arxiv_id: '2510.27484'
source_url: https://arxiv.org/abs/2510.27484
tags:
- reasoning
- sentence
- email
- kyle
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Interpreting reasoning models by studying only a single chain-of-thought
  (CoT) is inadequate for understanding causal influence and underlying computation.
  Instead, we propose analyzing the distribution of possible CoTs via on-policy resampling
  from different points in the trace.
---

# Thought Branches: Interpreting LLM Reasoning Requires Resampling

## Quick Facts
- arXiv ID: 2510.27484
- Source URL: https://arxiv.org/abs/2510.27484
- Authors: Uzay Macar; Paul C. Bogdan; Senthooran Rajamanoharan; Neel Nanda
- Reference count: 40
- Key outcome: Interpreting reasoning models by studying only a single chain-of-thought (CoT) is inadequate for understanding causal influence and underlying computation. Instead, we propose analyzing the distribution of possible CoTs via on-policy resampling from different points in the trace.

## Executive Summary
Interpreting reasoning models requires moving beyond single chain-of-thought samples to analyze distributions of possible reasoning traces. The paper argues that studying only one CoT is inadequate for understanding causal influence, introducing methods to measure resilience and counterfactual importance through on-policy resampling. In blackmail scenarios, self-preservation statements show low resilience and negligible counterfactual importance, suggesting they don't meaningfully drive decisions. Off-policy interventions produce small, unstable effects while on-policy resampling yields larger, more reliable behavioral changes. The methodology enables principled causal analysis of reasoning processes across multiple task domains.

## Method Summary
The approach involves generating multiple base CoT traces, segmenting them into sentences, and resampling 100+ rollouts per sentence to compute outcome distributions. Resilience is measured by iteratively resampling until semantic content is abandoned, while counterfactual++ importance evaluates causal impact when content is entirely absent. On-policy resampling uses model-generated candidates filtered by semantic similarity, whereas off-policy uses handwritten or cross-model edits. KL divergence quantifies distribution shifts between modified and baseline outcomes. The method is applied to blackmail scenarios, hinted multiple-choice tasks, and resume screening, using GPT-4.1-mini for outcome classification and BERT embeddings for semantic comparisons.

## Key Results
- Self-preservation statements in blackmail CoTs show lowest resilience (1-4 iterations) and negligible counterfactual++ importance (~0.001-0.003 KL), indicating post-hoc rationalization rather than causal causation
- On-policy resampling achieves 67-87% plan shift rates versus 62-79% for off-policy interventions, with "no effect" rates of 7-11% versus 14-28%
- In hinted MMLU tasks, hidden information exerts subtle cumulative bias on CoT trajectories even when not explicitly mentioned
- Resume screening bias differences between demographic groups are largely mediated by sentence clusters related to qualifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Studying single chain-of-thought samples is inadequate for understanding causal influence; analyzing distributions of possible CoTs via on-policy resampling enables principled causal analysis.
- **Mechanism**: By resampling from specific points in a partial CoT and contrasting downstream outcome distributions (using KL divergence), we approximate which reasoning steps genuinely shift behavior versus which are correlational.
- **Core assumption**: Causal importance is revealed by distribution shifts across many rollouts, not by presence in a single trace.
- **Evidence anchors**:
  - [abstract]: "Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate."
  - [section 2.1.1]: Counterfactual importance uses `D_KL[p(A'_Si|Ti≉Si) || p(A_Si)]` to measure outcome distribution shifts.
  - [corpus]: "Causal Strengths and Leaky Beliefs" (arxiv:2512.11909) similarly frames LLM reasoning via causal Bayes nets, supporting distributional causal analysis as an emerging paradigm.
- **Break condition**: If resampling produces near-identical outcome distributions regardless of sentence modifications, the method cannot discriminate causal influences.

### Mechanism 2
- **Claim**: Resilience metrics address error correction by measuring how many interventions eliminate semantic content entirely; counterfactual++ importance computed only from fully-eliminated rollouts reveals true causal drivers.
- **Mechanism**: Iteratively resample and track semantic reappearance. Low-resilience sentences (e.g., self-preservation) are easily eliminated and show negligible counterfactual++ importance, indicating post-hoc rationalization rather than causal causation.
- **Core assumption**: A sentence's true causal importance requires its semantic content to be absent throughout the entire trace, not merely replaced once.
- **Evidence anchors**:
  - [abstract]: "We introduce methods to measure resilience—how many interventions are needed to eliminate a sentence's semantic content."
  - [section 2.2]: Self-preservation shows lowest resilience (~1-4 iterations) and counterfactual++ importance (~0.001-0.003 KL), while plan generation shows elevated importance.
  - [corpus]: Limited direct precedent for resilience-based semantic elimination in related work; this appears novel to this paper.
- **Break condition**: If semantic content is infinitely resilient (never fully disappears regardless of intervention count), the method cannot distinguish causal importance.

### Mechanism 3
- **Claim**: On-policy resampling produces larger and more reliable behavioral changes than off-policy interventions because the latter trigger distribution shift and error correction.
- **Mechanism**: Off-policy text (handwritten, cross-model) is treated as noise and ignored or overwritten. On-policy resampling stays within the model's natural distribution, producing coherent modifications.
- **Core assumption**: Intervention effectiveness depends on plausibility within the model's distribution; off-policy text is actively corrected away.
- **Evidence anchors**:
  - [abstract]: "Off-policy interventions (handwritten or cross-model edits) produce small and unstable effects, whereas on-policy resampling yields larger, more reliable behavioral changes."
  - [section 3.2, Table 1]: On-policy resampling achieves "Plan shift" 67-87% vs. 62-79% for off-policy; "No effect" rate is 7-11% vs. 14-28%.
  - [corpus]: Related work on "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning" (arxiv:2506.09853) addresses sufficiency but doesn't compare on/off-policy effectiveness directly.
- **Break condition**: If off-policy interventions consistently outperform on-policy resampling, the mechanism is invalid.

## Foundational Learning

- **Concept: Counterfactual Causal Inference**
  - Why needed here: The entire methodology evaluates "what would happen if this sentence were different" to infer causation, not just correlation.
  - Quick check question: If observing "self-preservation reasoning" in every blackmail CoT, can you conclude it causes blackmail? Why or why not?

- **Concept: Distribution Shift in Language Models**
  - Why needed here: The paper shows off-policy interventions fail because they shift the model outside its natural distribution, triggering correction.
  - Quick check question: If you insert a sentence the model would never naturally produce, what behaviors might you expect? Is this a reliable causal test?

- **Concept: KL Divergence for Distribution Comparison**
  - Why needed here: KL divergence quantifies how much outcome distributions shift when sentences are removed—the core importance metric.
  - Quick check question: If removing sentence A yields KL=0.001 and sentence B yields KL=0.5, what can you conclude about their relative causal importance?

## Architecture Onboarding

- **Component map**:
  Base CoT generation -> Sentence segmentation -> Semantic embedding -> Counterfactual resampling -> Resilience iteration -> Outcome classification -> Importance computation -> On-policy candidate filtering

- **Critical path**:
  Generate base CoTs → Segment sentences → Resample 100× per sentence → Classify outcomes → Compute KL divergence → For resilience: iterate resampling → Compute counterfactual++ from fully-eliminated rollouts

- **Design tradeoffs**:
  - Sample count vs. cost: 100+ rollouts per sentence provides stable estimates but is computationally expensive
  - Sentence vs. token granularity: Sentences are semantically meaningful but may miss fine-grained effects
  - Assumption: Binary vs. multi-class outcomes affect sample requirements per category

- **Failure signatures**:
  - Uniform importance across all sentences → insufficient samples or outcome classification issues
  - Uniformly high resilience (content never disappears) → semantic threshold too strict or model too deterministic
  - Off-policy outperforms on-policy → candidate filtering may be incorrectly restrictive

- **First 3 experiments**:
  1. Verify leverage/plan sentences show higher counterfactual++ importance than self-preservation in blackmail scenario; confirm baseline counterfactual shows uniform distribution (validates resilience approach).
  2. Compare on-policy resampling vs. handwritten insertions for specific meanings (doubt, ethics); verify on-policy achieves larger, more directional effects.
  3. Compute resilience scores multiple times for a sentence set; verify critical sentences (plan generation) show consistently higher resilience than post-hoc rationalizations.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the low causal importance and low resilience of self-preservation statements generalize beyond the blackmail and whistleblowing scenarios tested?
  - Basis in paper: [explicit] The authors explicitly list a limitation: "experiments largely hone into specific prompts rather than being prompt-agnostic (e.g., studying self-preservation beyond blackmailing)."
  - Why unresolved: The paper only tests specific "agentic misalignment" scenarios; it is unknown if self-preservation is similarly epiphenomenal in other contexts.
  - What evidence would resolve it: Replicating the resilience and counterfactual++ importance analysis across a broader, diverse distribution of prompts where self-preservation might theoretically drive behavior.

- **Open Question 2**: Can the resampling strategy be refined to reduce computational costs enough for real-time monitoring?
  - Basis in paper: [explicit] The authors state: "The main limitation of this resampling approach is its computational cost, which currently makes it more suitable for offline analysis than real-time monitoring."
  - Why unresolved: Generating hundreds of rollouts per sentence creates a bottleneck that prevents deployment in live safety systems.
  - What evidence would resolve it: Developing heuristics to strategically identify critical intervention points, thereby minimizing the number of resampling operations required for reliable causal estimates.

- **Open Question 3**: How do text-based causal importance metrics correlate with internal model representations?
  - Basis in paper: [explicit] The authors caution that their "measures are black-box and there may be disconnects between internal representations and external legibility."
  - Why unresolved: It is unclear if sentences identified as "causally important" via resampling correspond to specific internal neural circuits or mechanisms.
  - What evidence would resolve it: Comparing the behavioral effects of text-based resampling interventions with mechanistic interventions (e.g., activation patching) on the same reasoning tasks.

## Limitations
- Computational cost of generating 100+ rollouts per sentence makes the approach suitable only for offline analysis rather than real-time monitoring
- Semantic similarity thresholds for resilience scoring rely on median-based cutoffs that may not generalize across domains
- Binary outcome classification in blackmail scenarios may oversimplify nuanced decision processes

## Confidence
- **High Confidence**: The core finding that single CoT samples inadequately represent reasoning processes is well-supported by multiple task domains
- **Medium Confidence**: Resilience metrics and counterfactual++ importance show clear patterns in blackmail scenarios, but semantic similarity thresholds require careful tuning
- **Low Confidence**: The hinted MMLU experiment showing "cumulative bias" relies on subtle effects that could be influenced by prompt formatting or model-specific behaviors

## Next Checks
1. **Resilience Threshold Validation**: Run multiple resilience analyses on the same sentences with different semantic similarity thresholds (e.g., mean, 75th percentile) to verify critical sentences consistently show higher persistence than post-hoc rationalizations.

2. **Off-Policy vs On-Policy Robustness**: Systematically vary the degree of distributional shift in off-policy interventions (gradually transitioning from on-policy to completely foreign content) to map the boundary where models begin treating interventions as noise.

3. **Multi-Outcome Generalization**: Apply the methodology to a task with more than two outcome categories (e.g., multi-class classification) to verify that KL divergence-based importance metrics maintain discriminative power beyond binary decisions.