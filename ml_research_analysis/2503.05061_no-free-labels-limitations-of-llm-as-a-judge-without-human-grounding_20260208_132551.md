---
ver: rpa2
title: 'No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding'
arxiv_id: '2503.05061'
source_url: https://arxiv.org/abs/2503.05061
tags:
- answer
- reference
- question
- judge
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-as-a-Judge performance is highly sensitive to the provided
  reference answers. When judges cannot correctly answer a question themselves, they
  struggle to grade responses accurately unless given a correct human-written reference.
---

# No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding

## Quick Facts
- arXiv ID: 2503.05061
- Source URL: https://arxiv.org/abs/2503.05061
- Reference count: 21
- Primary result: LLM-as-a-Judge performance critically depends on reference answer correctness, not source.

## Executive Summary
This paper reveals fundamental limitations of using LLM judges to evaluate responses without proper reference grounding. The key finding is that LLM judges struggle to grade responses accurately when they cannot answer questions themselves, unless provided with correct human-written reference answers. Reference correctness matters more than whether the reference is human-written or model-generated. This dependence poses serious risks for evaluating difficult domains or models more capable than the judge itself.

## Method Summary
The authors evaluated LLM-as-a-Judge performance across multiple reference conditions (None, Self, Human, Wrong, Random) on two benchmarks: CMT-Bench and BFF-Bench. Judge models were tested both as candidates (to determine which questions they could answer) and as judges (to evaluate responses). Performance was measured using Cohen's Kappa agreement with human annotators, with results stratified by whether the judge answered correctly as a candidate. The study tested correctness grading on math, reasoning, and business/finance questions using various judge and candidate model pairs.

## Key Results
- Providing correct reference answers significantly improves judge agreement with humans, especially for questions the judge cannot answer
- Reference correctness is more important than source - verified synthetic references match human-written reference performance
- Using incorrect ("Wrong") references actively degrades performance below the no-reference baseline
- LLM judges are fundamentally limited in evaluating models more capable than themselves

## Why This Works (Mechanism)

### Mechanism 1: Reference-Grounded Comparison Reduces Task Complexity
Providing a verified correct reference answer improves LLM Judge accuracy by simplifying the evaluation task from generating-and-then-judging to only comparing. A judge model without a reference must internally generate the correct answer and then compare it to the candidate response. This compounds potential errors. A reference answer short-circuits the generation step, allowing the model to focus its capacity on semantic and factual comparison.

### Mechanism 2: Performance is Bounded by Judge's Intrinsic Knowledge
An LLM Judge's ability to grade a response is fundamentally limited by its own ability to answer the question correctly. If a judge model cannot answer a question, it lacks the internal knowledge or reasoning path to evaluate the correctness of another model's response. This limitation is not overcome by using its own generated response as a reference.

### Mechanism 3: Reference Correctness is More Critical Than Source
The benefit of a reference comes from its accuracy, not whether it was human-written or model-generated. A verified correct model-generated reference provides a similar performance improvement to a human-written one. The judge model uses the reference as a grounding point for truth; the source is irrelevant if the content is accurate.

## Foundational Learning

**LLM-as-a-Judge Framework**
- Why needed here: This is the core system being evaluated, where a separate "judge" model evaluates the output of a "candidate" model.
- Quick check question: What are the two main roles in the LLM-as-a-Judge framework? (Answer: judge model and candidate model)

**Cohen's Kappa (Îº)**
- Why needed here: This is the primary metric used to measure agreement between the LLM Judge and human annotators, accounting for chance agreement.
- Quick check question: What does a Cohen's Kappa of 0.5 indicate compared to 0.0? (Answer: 0.5 indicates moderate agreement beyond chance, 0.0 indicates agreement no better than random chance)

**Reference Types (None, Self, Human, Wrong, Random)**
- Why needed here: The paper's main contribution is analyzing how different reference types affect judge performance.
- Quick check question: According to the paper, which reference type performed worse than "None" or "Random"? (Answer: "Wrong" reference)

## Architecture Onboarding

**Component map**: Candidate Models (Gemma 2, Qwen 2.5, Phi-4, etc.) generate responses; Judge Models (same set, except Gemma) grade these responses; Benchmark Dataset (CMT-Bench, BFF-Bench) contains questions and human-verified "gold" references; Human Annotators provide ground-truth correctness labels.

**Critical path**: Create high-quality, verified reference answers. The process is: *Draft Question -> Expert-Write Reference -> Verify Reference -> Generate Candidate Responses -> Judge with Reference -> Compare to Human Label*.

**Design tradeoffs**: Human-Written vs. Verified Synthetic References: Human-written references are costly but reliable. A viable alternative is using model-generated references that are then human-verified for correctness, saving time while maintaining accuracy. Aggregate vs. Subset Performance: A model may show high overall agreement, masking poor performance on the difficult subset of questions it cannot answer.

**Failure signatures**: A high False Negative Rate (FNR) when using an incorrect ("Wrong") reference; the judge will incorrectly mark valid responses as incorrect. Self-preference bias, where a model grades its own responses more leniently, is exacerbated when using its own unverified responses as a reference.

**First 3 experiments**:
1. Establish a Baseline: Run the judge model on the benchmark without any reference answers ("None") and measure Cohen's Kappa against human labels. Separate results into two subsets: questions the model answered correctly vs. incorrectly.
2. Test Reference Types: On the same benchmark, run the judge with "Self," "Human," and "Random" references. Confirm that "Human" yields the highest Kappa, especially on the "answered incorrectly" subset.
3. Probe Reference Correctness: Create a small set of "Wrong" references (human-edited to be incorrect) and compare performance against a set of verified synthetic references (e.g., from GPT-4). Confirm that verified synthetic references approach "Human" performance, while "Wrong" references severely degrade it.

## Open Questions the Paper Calls Out

**Open Question 1**: Do the findings regarding reference necessity generalize to subjective domains like creative writing or safety, where "correctness" is not binary?
- Basis in paper: The paper focuses on business, finance, and reasoning tasks with unambiguous answers (BFF-Bench), acknowledging that trends in other domains are unknown.
- Why unresolved: The mechanisms driving judge failure on difficult reasoning tasks (inability to verify facts) may differ from those in subjective tasks, where stylistic preferences or instruction-following dominate.
- Evidence to resolve: Replicating the reference-intervention experiments on open-ended creative benchmarks (e.g., AlpacaEval creative writing) to observe if human references yield similar agreement gains.

**Open Question 2**: What is the specific trade-off between improved agreement and increased self-preference bias when using verified synthetic references?
- Basis in paper: The Conclusion states, "although using verified references may improve the overall agreement metrics, it may come at the cost of increased bias. Specifically, using a model's own references exacerbates the self-preference bias."
- Why unresolved: The paper recommends verifying references to boost accuracy but explicitly flags this bias risk without quantifying the severity of the trade-off in their results.
- Evidence to resolve: A comparative study measuring self-preference rates when judges use their own verified references versus human-written or externally verified references.

**Open Question 3**: Can automated verification methods (e.g., using a stronger model) replace human verification of references without degrading judge performance?
- Basis in paper: The paper recommends verifying references but relies on human annotators for this, while noting in the Limitations section that "collecting accurate, high-quality human annotations... is not always the case."
- Why unresolved: The authors demonstrate that correctness is key, but leave open the practical question of how to scalably ensure correctness without reintroducing the bottleneck of human labeling.
- Evidence to resolve: An experiment comparing judge alignment when references are verified by a stronger "teacher" model versus human annotators.

## Limitations

- The findings rely on specific judge models (primarily Qwen 2.5 7B and Phi-4) and benchmarks (CMT-Bench, BFF-Bench), which may not generalize to all model families or domains.
- The study focuses on correctness grading rather than quality grading, leaving open whether these limitations extend to more nuanced evaluation tasks.
- The human-written references were manually corrected, but the specific verification process and potential remaining errors are not detailed.

## Confidence

**High Confidence**: The core finding that providing correct reference answers significantly improves LLM Judge agreement with humans, particularly for questions the judge cannot answer itself. This is supported by direct experimental comparisons across multiple reference conditions and models.

**Medium Confidence**: The claim that verified synthetic references can match human-written reference performance. While the paper shows this for one experimental condition, broader validation across different model families and reference generation methods would strengthen this claim.

**Medium Confidence**: The assertion that LLM-as-a-Judge is fundamentally limited for evaluating models more capable than the judge itself. This follows logically from the data but would benefit from testing with models of explicitly known capability gaps.

## Next Checks

1. **Cross-Model Verification**: Test the reference dependence hypothesis with a broader range of judge models spanning different capability levels (e.g., Claude, Gemini) to confirm the pattern holds beyond Qwen and Phi-4.

2. **Reference Generation Pipeline**: Implement and evaluate a systematic pipeline for creating verified synthetic references: (1) generate candidate reference with strong model, (2) have human verify correctness, (3) use verified reference for judging. Measure performance against pure human-written references.

3. **Capability Gap Analysis**: Design an experiment with explicitly tiered models (e.g., judge using Llama 3.1 8B, candidates using Claude 3.5 Sonnet) to quantify how performance degrades as the capability gap increases, confirming the limitation for evaluating more capable models.