---
ver: rpa2
title: Learning to Attribute with Attention
arxiv_id: '2504.13752'
source_url: https://arxiv.org/abs/2504.13752
tags:
- attribution
- attention
- context
- sources
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of identifying which preceding
  tokens influence a language model's generation, a task traditionally expensive due
  to the need for ablations. The core idea is to learn a surrogate model that uses
  attention weights from different heads as features, with learnable coefficients
  indicating each head's utility for attribution.
---

# Learning to Attribute with Attention

## Quick Facts
- arXiv ID: 2504.13752
- Source URL: https://arxiv.org/abs/2504.13752
- Reference count: 40
- Primary result: AT2 achieves comparable attribution quality to methods requiring many ablations while being significantly faster, outperforming baselines like average attention and gradient methods.

## Executive Summary
This paper addresses the problem of identifying which preceding tokens influence a language model's generation, a task traditionally expensive due to the need for ablations. The core idea is to learn a surrogate model that uses attention weights from different heads as features, with learnable coefficients indicating each head's utility for attribution. This allows for efficient attribution after a one-time training phase. The method, called AT2, achieves comparable attribution quality to methods requiring many ablations while being significantly faster, particularly outperforming naive attention averaging and gradient-based methods.

## Method Summary
The method learns coefficients for attention heads that best predict token attribution effects. It trains a linear surrogate model where attention weights from each head are weighted by learnable coefficients, optimized to predict the probability changes under token ablations. The training requires ground truth from ablations but only once per dataset, after which the learned coefficients can be applied to new examples without additional ablations. The approach treats attribution as a regression problem where the goal is to predict how much each token contributes to the generation probability.

## Key Results
- AT2 outperforms naive attention averaging and gradient-based attribution methods on both top-k drop and Linear Datamodeling Score (LDS) metrics
- Performance is comparable to example-specific surrogate modeling with 256 ablations but with much lower computational cost
- Coefficients learned on generic datasets generalize to task-specific datasets, eliminating the need for task-specific training in many cases

## Why This Works (Mechanism)

### Mechanism 1
Attention heads vary in their utility for attribution, and learning head-specific coefficients improves attribution quality over naive averaging. Each attention head is assigned a learnable coefficient θ_ℓh, and the attribution score is computed as a weighted sum. Heads with high coefficients contribute more to the final attribution. This works because attention patterns from specific heads correlate with the causal influence of tokens on generation.

### Mechanism 2
A linear surrogate model can approximate the relationship between source ablations and generation probability. The surrogate model ˆf_θ(v,X,S,Y) = ⟨w_θ(X,S,Y), v⟩ predicts the log-probability of generating Y when ablating sources according to binary vector v. Coefficients are optimized to minimize prediction error against ground-truth ablation effects. This relies on the assumption that the effect of ablating multiple sources is approximately the sum of individual source effects.

### Mechanism 3
Coefficients learned on one dataset generalize to unseen examples and tasks without retraining. Training uses examples with ablations, and coefficients θ are optimized across all examples. At inference, no ablations are needed—only attention weight extraction and linear combination. This works because head utility for attribution is relatively stable across examples and tasks within the same model.

## Foundational Learning

- **Token attribution via ablation**: Understanding that influential tokens cause large probability drops when removed is essential since AT2 uses ablation effects as ground-truth labels. Quick check: If removing token A reduces generation probability by 0.3 log-prob and removing token B reduces it by 0.1, which has higher attribution?

- **Attention weights in transformers**: AT2 treats attention weights as features, so you must understand that attention weights represent how much each token in the context attends to each other token during generation. Quick check: In a transformer with L layers and H heads, what is the shape of the attention weight tensor for a single generation step?

- **Surrogate modeling**: The entire AT2 framework builds on approximating an expensive function (ablation effects) with a cheap, interpretable proxy. Quick check: Why might a linear surrogate model be preferred over a neural network surrogate for attribution?

## Architecture Onboarding

- **Component map**: Attention extractor -> Coefficient matrix θ -> Surrogate predictor -> Training loop
- **Critical path**: 1) Forward pass generates Y with saved attention weights. 2) For each source s, aggregate attention weights from X→Y. 3) Compute attribution scores: w_θ = Σ θ_ℓh · Attn_ℓh. 4) (Training only) Sample ablations, compute p_LM(Y|ABLATE(X,S,v)), optimize θ.
- **Design tradeoffs**: Token vs. sentence sources (training on tokens transfers to sentences but reverse may not hold); Saved vs. recomputed attention (FlashAttention doesn't store attention matrices; recomputation adds ~0.1s overhead); Training dataset size (2,000 examples with 32 ablations each is sufficient).
- **Failure signatures**: Low LDS but high top-k drop (model may identify influential sources but fail on fine-grained predictions); Coefficients collapsing to uniform (training signal insufficient or loss function misconfigured); Large gap between general and task-specific (head roles not transferable across domains).
- **First 3 experiments**: 1) Baseline comparison: Run AT2 on 100 examples from target dataset, compare LDS and top-k drop against average attention and gradient baselines. 2) Ablation study: Train AT2 with varying numbers of ablations per example (8, 16, 32, 64) to identify cost-quality tradeoff. 3) Transfer test: Train on generic dataset (e.g., Dolly 15k), evaluate on domain-specific dataset to assess generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can the AT2 framework be extended to account for higher-order interactions between tokens in the input sequence? The paper notes that attention weights only consider first-order effects and don't consider effects from interactions between tokens. Evidence to resolve: A modified version including interaction terms that demonstrates improved Linear Datamodeling Score without sacrificing efficiency.

### Open Question 2
What additional features, beyond attention weights, could improve the accuracy of attribution without significantly increasing computational cost? The paper suggests attention is an imperfect proxy and that more accurate attributions may need additional features. Evidence to resolve: An ablation study comparing AT2 against surrogate models trained on alternative features that identifies a feature set offering higher Top-k drop scores with comparable latency.

### Open Question 3
Do the learned attention head coefficients transfer effectively across different model architectures or scales? While the paper demonstrates transfer across datasets, it doesn't test if coefficients learned on one model apply to another. Evidence to resolve: An experiment applying coefficients trained on a larger model (e.g., Llama-70B) to a smaller counterpart (e.g., Llama-8B) and measuring the degradation of attribution quality metrics.

## Limitations

- Attention weights only consider first-order effects and don't account for interactions between tokens in the input sequence
- The linear surrogate assumption may break down when token interactions are strongly non-additive
- FlashAttention implementations don't store attention matrices, requiring recomputation that adds computational overhead

## Confidence

**High confidence** in: The core experimental results showing AT2 outperforms naive attention averaging and gradient-based methods while being significantly faster than ablation-heavy approaches.

**Medium confidence** in: The transferability claims across datasets and tasks, as the corpus lacks evidence on how stable coefficients are across fundamentally different domains or model architectures.

**Low confidence** in: The claim that AT2's attribution quality is "comparable" to example-specific surrogate modeling with 256 ablations, as the LDS scores show overlapping confidence intervals suggesting the difference may not be practically significant.

## Next Checks

1. **Cross-architecture coefficient transfer test**: Train AT2 coefficients on one model architecture (e.g., Llama-3.1-8B) and evaluate attribution quality on a different architecture (e.g., GPT-3.5). Measure degradation in LDS and top-k drop to quantify architectural generalization limits.

2. **Non-linearity stress test**: Create synthetic examples where token interactions are deliberately non-additive (e.g., "A and B are true" vs. "A or B is true" scenarios). Compare AT2's attribution accuracy against ground-truth influence on these examples to validate the linear surrogate assumption.

3. **Attention extraction overhead quantification**: Measure the actual wall-clock time difference between AT2 with saved vs. recomputed attention on sequences of varying lengths (128, 512, 2048 tokens). Benchmark against the claimed 0.1s overhead to determine practical scalability limits.