---
ver: rpa2
title: Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music
  Generation
arxiv_id: '2505.03314'
source_url: https://arxiv.org/abs/2505.03314
tags:
- music
- diffusion
- wavelet
- generation
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel diffusion model for controllable
  symbolic music generation that combines Transformer-Mamba blocks with learnable
  wavelet transform. The model represents symbolic music as image-like pianorolls
  and uses classifier-free guidance to generate music with target chords.
---

# Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation

## Quick Facts
- arXiv ID: 2505.03314
- Source URL: https://arxiv.org/abs/2505.03314
- Reference count: 28
- Primary result: Proffusion-WM achieves Overlapping Area similarity of 0.939 and Chord F1 score of 0.541, outperforming baselines in both objective metrics and subjective listening tests

## Executive Summary
This paper introduces a novel diffusion model for controllable symbolic music generation that combines Transformer-Mamba blocks with learnable wavelet transform. The model represents symbolic music as image-like pianorolls and uses classifier-free guidance to generate music with target chords. The approach outperforms existing baselines in both objective metrics (Overlapping Area similarity scores of 0.939 vs 0.862 for strongest baseline, chord F1 score of 0.541) and subjective listening tests across five evaluation dimensions. The learnable wavelet transform improves denoising capabilities while the hybrid architecture leverages both global and local information processing. The model successfully generates high-quality, controllable symbolic music that aligns well with specified chord progressions.

## Method Summary
The method converts symbolic music to 2D pianoroll tensors (2, 128, 128) and uses a diffusion model with a U-Net architecture containing Transformer-Mamba blocks and learnable wavelet nodes in skip connections. Chord progressions are encoded via a pre-trained chord VAE and injected through cross-attention. The model employs classifier-free guidance with dropout 0.2 and guidance scale 5 during inference. Training uses the POP909 dataset with 1000 diffusion steps, Adam optimizer (lr=5e-5), and batch size 16. The learnable wavelet transform decomposes features into frequency bands to better preserve high-frequency musical details like note onsets and offsets during the denoising process.

## Key Results
- Proffusion-WM achieves Overlapping Area similarity of 0.939 vs 0.862 for strongest baseline
- Chord F1 score of 0.541 demonstrates strong controllability adherence
- Subjective listening tests show superior performance across all five evaluation dimensions (Humanness, Harmony, Rhythm, Richness, Overall Preference)

## Why This Works (Mechanism)

### Mechanism 1: Image-like Pianoroll Representation Bypasses Discrete Data Limitation
Representing symbolic music as 2D pianorolls enables diffusion models (designed for continuous data) to generate discrete symbolic music. MIDI events are converted to (2, 128, 128) tensors where pitch occupies the vertical axis and time occupies the horizontal axis, transforming the discrete token sequence problem into a continuous image-like generation task that standard diffusion can handle. The pianoroll representation preserves sufficient musical structure (onsets, durations, polyphony) for the model to learn meaningful distributions.

### Mechanism 2: Hybrid Transformer-Mamba Block Combines Global and Local Processing
The sequential combination of Transformer (global attention) followed by Mamba (selective linear-time SSM) captures both long-range dependencies and local sequential patterns more effectively than either alone. Features enter Transformer first for global context, then flatten to sequence format for Mamba's input-dependent selection mechanism. Mamba's parameters B, C, Δ depend on input, allowing selective filtering of irrelevant information while maintaining O(L) complexity.

### Mechanism 3: Learnable Wavelet Transform Preserves High-Frequency Musical Detail During Denoising
Replacing standard skip connections with Learnable Wavelet Nodes (LWN) improves reconstruction of abrupt transitions (note onsets/offsets) that blur during diffusion's reverse process. The 2D-DWT decomposes features into low/high frequency bands via learnable filters. High-frequency components capture edges and transients. Self-supervised wavelet loss prevents degradation to generic convolution.

## Foundational Learning

- **Diffusion Models (Forward/Reverse Process, Noise Scheduling)**
  - Why needed here: The entire generative framework is DDPM-based. Understanding how β_t controls noise injection and why the reverse process approximates p(x₀) is essential for debugging sample quality.
  - Quick check question: Can you explain why classifier-free guidance modifies the score estimate using both conditional and unconditional models?

- **State Space Models (S4, Discretization, Selection Mechanism)**
  - Why needed here: Mamba extends S4 with input-dependent parameters. You must understand how the continuous ODE h'(t)=Ah(t)+Bx(t) discretizes and why the selection mechanism enables content-aware filtering.
  - Quick check question: Why does Mamba achieve O(L) complexity while Transformers achieve O(L²)?

- **Discrete Wavelet Transform (Multi-Resolution Analysis, Filter Banks)**
  - Why needed here: The LWN implements 2D-DWT with learnable filters. Understanding approximation vs. detail coefficients is necessary to interpret what the model learns.
  - Quick check question: What do the four subbands F_ll, F_hl, F_lh, F_hh represent in a 2D wavelet decomposition of a pianoroll?

## Architecture Onboarding

- **Component map:** Input MIDI -> Pianoroll tensor (2, 128, 128) -> Chord VAE encoding -> Cross-attention in U-Net -> ResBlock -> Transformer-Mamba block -> Learnable Wavelet Node -> Output pianoroll

- **Critical path:**
  1. Chord VAE encoding must match training distribution or cross-attention misaligns
  2. Wavelet filter initialization must satisfy biorthogonality constraints or L_wavelet diverges early
  3. Classifier-free guidance dropout probability (0.2) affects controllability vs. diversity tradeoff

- **Design tradeoffs:**
  - Adding Mamba improved Pitch Range OA (0.920→0.955) but slightly hurt Note Duration OA (0.919→0.909)—likely capacity reallocation to global features
  - 1000 diffusion steps chosen for quality; inference slower than autoregressive but parallelizable
  - Guidance scale (ω=5): Higher values increase chord adherence but may reduce musical naturalness

- **Failure signatures:**
  - Blurry pianorolls: Wavelet loss not converging; check L_wavelet magnitude vs. L_diffusion
  - Poor chord adherence: Cross-attention weights not attending to condition; verify VAE latent distribution
  - Note duration artifacts: Mamba may over-emphasize global structure; try reducing Mamba hidden dimension

- **First 3 experiments:**
  1. Ablate wavelet loss weight: Run with L_wavelet coefficient [0.0, 0.1, 1.0, 10.0] and measure OA/F1 to find regularization sweet spot
  2. Compare Transformer-Mamba order: Swap to Mamba→Transformer and evaluate whether global-local processing order matters for musical attributes
  3. Vary guidance scale: Test ω ∈ [1, 3, 5, 7, 10] and plot chord F1 vs. subjective "Humanness" score to characterize controllability-naturalness frontier

## Open Questions the Paper Calls Out
- Would integrating discrete wavelet transform in the U-Net encoder and inverse wavelet transform in the decoder improve denoising performance compared to the current skip-connection approach?
- Can the trade-off between global (Pitch Range, IOI) and local (Note Duration) attribute modeling introduced by Mamba be mitigated through architectural modifications or hybrid weighting schemes?
- Does the model generalize to musical genres beyond pop music, given that training and evaluation were conducted exclusively on the POP909 dataset?

## Limitations
- The learnable wavelet transform's effectiveness relies critically on the wavelet loss regularization working as intended
- Architecture specifications (number of blocks, channel dimensions, depth of U-Net) are underspecified, making exact replication difficult
- Subjective listening test methodology lacks detail on participant expertise and number of raters

## Confidence
- **High confidence**: The core methodology (pianoroll representation, Transformer-Mamba hybrid architecture, classifier-free guidance) is technically sound and well-established
- **Medium confidence**: The learnable wavelet transform's contribution is supported by ablation results but depends critically on the wavelet loss functioning as designed
- **Low confidence**: Subjective evaluation results lack methodological transparency, making it difficult to assess their reliability

## Next Checks
1. Ablate wavelet loss weight: Run experiments with wavelet loss coefficients [0.0, 0.1, 1.0, 10.0] to identify optimal regularization strength and verify filters learn meaningful frequency decompositions
2. Compare Transformer-Mamba order: Swap processing order to Mamba→Transformer and evaluate whether complementary information processing depends on sequence
3. Verify cross-attention conditioning: Implement gradient-based attribution or attention visualization to confirm chord condition latents from VAE are actually being used during generation