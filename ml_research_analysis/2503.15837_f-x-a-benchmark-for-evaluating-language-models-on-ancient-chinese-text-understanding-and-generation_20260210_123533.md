---
ver: rpa2
title: "F\xF9x\xEC: A Benchmark for Evaluating Language Models on Ancient Chinese\
  \ Text Understanding and Generation"
arxiv_id: '2503.15837'
source_url: https://arxiv.org/abs/2503.15837
tags:
- chinese
- tasks
- task
- generation
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "F\xF9x\xEC is a benchmark for evaluating large language models\
  \ on ancient Chinese text understanding and generation. It addresses the gap in\
  \ assessing models' generative capabilities in classical Chinese by introducing\
  \ 21 diverse tasks spanning both comprehension and generation."
---

# Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation

## Quick Facts
- arXiv ID: 2503.15837
- Source URL: https://arxiv.org/abs/2503.15837
- Reference count: 29
- Fùxì introduces 21 tasks spanning ancient Chinese comprehension and generation, revealing significant capability gaps particularly in format-constrained generation tasks.

## Executive Summary
Fùxì is a benchmark designed to evaluate large language models on ancient Chinese text understanding and generation tasks. It addresses a critical gap in current LLM evaluation by focusing on both comprehension and open-ended generation in classical Chinese. The benchmark introduces 21 diverse tasks organized into four categories: multiple-choice reading comprehension, open-ended question answering, comprehension-based generation, and pattern-constrained generation. Using specialized evaluation metrics combining rule-based verification with fine-tuned LLM evaluators, Fùxì reveals significant performance gaps between understanding and generation tasks, with models particularly struggling in format-constrained generation requiring deep cultural knowledge.

## Method Summary
Fùxì uses zero-shot primary evaluation with optional 5-shot in-context learning, testing models across 21 ancient Chinese tasks. Evaluation strategies vary by task type: accuracy for multiple-choice and open-ended QA, BLEU for translation tasks, and rule-based format verification for pattern-constrained generation. A key innovation is the LLM Evaluator—Qwen2-7B-Instruct fine-tuned on 2,000 human-annotated examples with binary correctness labels and reasoning outputs. The benchmark evaluates 10+ models including GPT-4o, Qwen-Max, GLM-4-Plus, InternLM2, LLaMA3.1, Qwen2/Qwen2.5 series, and Xunzi-Qwen2-7b, using datasets from WYWEB, daizhige database, and Erya dataset.

## Key Results
- Comprehension tasks show strong performance (85.23% average on RC tasks) while generation tasks lag significantly (majority below 50% accuracy)
- Ci poetry generation reveals fundamental capability gaps, with most models scoring below 10% on format adherence
- Parameter scaling shows strong positive correlation with knowledge-intensive task performance, with 72B models outperforming 1.5B counterparts by 22.06% on RC and 31.85% on QA tasks
- Xunzi-Qwen2-7b demonstrates instruction-following degradation from fine-tuning, scoring 0% on pattern-constrained generation

## Why This Works (Mechanism)

### Mechanism 1
Hybrid evaluation combining rule-based verification with fine-tuned LLM evaluators enables reliable automated assessment of open-ended ancient Chinese generation tasks. Rule-based metrics handle deterministic constraints while the fine-tuned Qwen2-7B-Instruct evaluator judges semantic correctness and cultural authenticity, trained on 2,000 human-annotated examples.

### Mechanism 2
Model performance on ancient Chinese tasks scales with parameter count, particularly for knowledge-intensive tasks requiring memorization of cultural knowledge. Larger models encode more specialized cultural knowledge during pre-training, showing strong positive correlation with model size for book author/dynasty identification tasks.

### Mechanism 3
Pattern-constrained generation tasks expose a fundamental capability gap that persists despite model scaling or in-context learning. These tasks require simultaneous adherence to formal constraints while maintaining semantic coherence, with most models failing format adherence (<10% on Ci Generation).

## Foundational Learning

- **Classical Chinese literary forms (词牌, 对联, 格律)**: Understanding tonal patterns, rhythmic structures, and formal constraints is essential for diagnosing model failures in format-constrained generation. *Quick check*: Can you explain why "望江南" cipai constrains line lengths and tonal patterns, and how this differs from couplet requirements?

- **LLM evaluation paradigms (BLEU, LLM-as-judge, rule-based verification)**: Fùxì uses three distinct evaluation strategies depending on task type. Understanding when each is appropriate is essential for extending the benchmark. *Quick check*: Why would BLEU be unsuitable for evaluating poetry generation format correctness, and what specific rule-based checks would you implement instead?

- **Cross-lingual transfer and Chinese-specific tokenization**: LLaMA-31-8b scored near-zero on Chinese tasks while LLaMA-31-8b-Chinese showed clear improvements. Understanding vocabulary adaptation is critical for architectural decisions. *Quick check*: What specific modifications would be needed to adapt a model trained primarily on English/Latin-script data for classical Chinese character-level constraints?

## Architecture Onboarding

- **Component map**: Task Categories (21 tasks) → Evaluation Strategy Selection → Metric Application → Model Inference
- **Critical path**: 1) Task selection and data curation, 2) Evaluation metric implementation per task type, 3) LLM Evaluator fine-tuning (2,000 human-annotated samples), 4) Zero-shot evaluation of target models, 5) Optional 5-shot ICL evaluation
- **Design tradeoffs**: BLEU vs. LLM evaluation (objective vs. subjective), format correctness vs. aesthetic quality (benchmark prioritizes format adherence), task diversity vs. evaluation reliability (including subjective tasks reduces evaluator agreement)
- **Failure signatures**: Language mixing (LLaMA-31-8b), instruction-following degradation (Xunzi-Qwen2-7b), format collapse (prose-like content instead of structured poetry), hallucination cascade (smaller models invent non-existent sources)
- **First 3 experiments**: 1) Establish baseline on comprehension tasks (compare against Qwen2.5-72b 85.23% RC average), 2) Validate LLM evaluator alignment (measure agreement drift from κ = 0.764), 3) Test format-constrained generation in isolation (focus on Couplet Generation before Ci Generation)

## Open Questions the Paper Calls Out

1. How can automated evaluation metrics be developed to better capture the aesthetic and stylistic nuances of classical Chinese poetry generation, beyond format correctness? The authors state they "prioritize format correctness as the fundamental capability, leaving aesthetic evaluation for future work."

2. What training strategies can preserve instruction-following capabilities in models while specializing them for ancient Chinese text processing? Xunzi-Qwen2-7b scored 0% on pattern-constrained generation tasks due to instruction-following degradation from fine-tuning.

3. How can the significant performance gap between comprehension and generation tasks (approximately 42 percentage points average) in ancient Chinese be systematically reduced? The paper documents the gap but does not propose interventions to address it.

## Limitations
- LLM evaluator shows reduced reliability on subjective tasks (κ = 0.567 for allegorical saying QA) and demonstrated brittleness when penalizing valid multi-form poetry responses
- Benchmark focuses on format adherence over aesthetic quality, potentially allowing structurally correct but semantically incoherent outputs to score highly
- Performance gap between understanding (85.23% average) and generation tasks (majority below 50%) suggests current evaluation methodologies may inadequately capture generative capabilities

## Confidence

- **High Confidence**: Claims about parameter scaling effects on knowledge-intensive tasks and zero-shot performance differentials between model families
- **Medium Confidence**: Claims about LLM evaluator reliability (showing task-dependent variability) and the fundamental capability gap in pattern-constrained generation
- **Low Confidence**: Claims about cultural knowledge encoding mechanisms and the sufficiency of current evaluation metrics for capturing semantic quality in creative tasks

## Next Checks

1. **Evaluator Agreement Validation**: Sample 100 LLM evaluator judgments across all task categories and conduct independent human annotation to measure agreement drift from the reported κ = 0.764, particularly focusing on subjective generation tasks.

2. **Cross-Lingual Transfer Analysis**: Test non-Chinese models (LLaMA-31-8b, GLM-4-Plus) on the benchmark with and without Chinese-specific tokenization to quantify the vocabulary adaptation contribution versus architectural scaling benefits.

3. **Format Constraint Knowledge Isolation**: Create a diagnostic test suite isolating tonal pattern recognition and structural constraint adherence from semantic generation—evaluate whether models can correctly identify format violations even when unable to generate compliant text.