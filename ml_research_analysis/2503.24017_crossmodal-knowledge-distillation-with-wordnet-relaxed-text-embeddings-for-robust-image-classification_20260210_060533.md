---
ver: rpa2
title: Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for
  Robust Image Classification
arxiv_id: '2503.24017'
source_url: https://arxiv.org/abs/2503.24017
tags:
- teacher
- text
- student
- embeddings
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-teacher crossmodal knowledge distillation
  framework that leverages WordNet-relaxed text embeddings to mitigate label leakage
  and improve student model performance. By replacing exact class names with semantically
  richer WordNet expansions, the method addresses the limitations of direct textual
  inputs, which often fail to capture complex visual semantics and lead to artificial
  teacher accuracy.
---

# Crossmodal Knowledge Distillation with WordNet-Relaxed Text Embeddings for Robust Image Classification

## Quick Facts
- arXiv ID: 2503.24017
- Source URL: https://arxiv.org/abs/2503.24017
- Reference count: 25
- Authors: Chenqi Guo; Mengshuo Rong; Qianli Feng; Rongfan Feng; Yinglong Ma
- One-line primary result: Multi-teacher crossmodal distillation using WordNet-relaxed embeddings achieves state-of-the-art or near-state-of-the-art student accuracy (up to 83.33% on CIFAR100) while mitigating label leakage.

## Executive Summary
This paper addresses a critical limitation in crossmodal knowledge distillation: the artificial accuracy boost from direct textual label inputs, which undermines student model generalization. The authors propose replacing exact class names with WordNet-relaxed embeddings that incorporate semantically richer, hypernym-based expansions. By doing so, they reduce reliance on superficial textual cues and encourage teachers to focus on robust visual features. Experiments across six public datasets demonstrate significant improvements in student model accuracy, with CIFAR100 reaching 83.33%. The approach also enhances interpretability by minimizing textual shortcuts in teacher predictions.

## Method Summary
The method introduces a multi-teacher crossmodal distillation framework where each teacher is paired with WordNet-relaxed textual embeddings instead of direct class names. WordNet expansion generates semantically diverse synonyms and hypernyms, creating prompts that better capture the visual complexity of classes. During distillation, teachers generate soft targets from these relaxed embeddings, which are then used to train the student model. This design mitigates label leakage and encourages the student to learn more generalizable visual features. The framework is evaluated across six datasets, showing consistent performance gains and reduced dependence on textual shortcuts.

## Key Results
- Achieved state-of-the-art or near-state-of-the-art student accuracy across six datasets.
- Best student accuracy reached 83.33% on CIFAR100.
- Reduced reliance on textual shortcuts in teacher predictions, improving interpretability.

## Why This Works (Mechanism)
The core mechanism relies on replacing exact class names with WordNet-relaxed embeddings, which introduce semantic diversity and reduce artificial teacher accuracy. By expanding class names into hypernyms and synonyms, the method forces teachers to focus on robust visual features rather than superficial textual cues. This mitigates label leakage and enhances crossmodal knowledge transfer, leading to improved student model generalization.

## Foundational Learning

### WordNet and Semantic Expansion
**Why needed**: WordNet provides a structured lexical database for generating semantically richer class descriptions.
**Quick check**: Verify WordNet coverage for your dataset's class names and assess expansion quality.

### Crossmodal Knowledge Distillation
**Why needed**: Leverages pre-trained vision-language models to transfer knowledge from textual to visual modalities.
**Quick check**: Ensure compatibility between teacher and student model architectures.

### Label Leakage Mitigation
**Why needed**: Prevents artificial accuracy boosts from direct textual inputs, which undermine generalization.
**Quick check**: Compare teacher accuracy with and without WordNet-relaxed embeddings.

## Architecture Onboarding

### Component Map
Pre-trained Teacher Model -> WordNet-Relaxed Text Embeddings -> Student Model

### Critical Path
1. Generate WordNet-relaxed embeddings for class names.
2. Use relaxed embeddings to guide teacher predictions.
3. Distill soft targets from teachers to train the student model.

### Design Tradeoffs
- **Semantic richness vs. alignment**: WordNet expansions introduce diversity but may reduce alignment with exact class labels.
- **Computational overhead**: Multi-teacher architecture and WordNet expansion generation increase training time.

### Failure Signatures
- Over-reliance on textual cues in teacher predictions.
- Inconsistent WordNet expansions leading to poor alignment with visual features.

### First Experiments
1. Compare student accuracy with exact vs. WordNet-relaxed embeddings.
2. Evaluate teacher accuracy to detect label leakage.
3. Assess interpretability by analyzing teacher prediction distributions.

## Open Questions the Paper Calls Out
None

## Limitations
- WordNet-based expansions may not generalize well to non-English or specialized domains (e.g., medical imaging).
- Computational overhead of multi-teacher architecture and WordNet expansion generation is not quantified.
- Limited comparisons with existing crossmodal distillation methods leave claims of superiority unverified.

## Confidence

**High Confidence**: Effectiveness of WordNet-relaxed embeddings in reducing label leakage and improving student accuracy is well-supported by experimental results across six datasets.

**Medium Confidence**: Interpretability analyses suggesting reduced reliance on textual shortcuts are promising but could benefit from deeper qualitative validation.

**Low Confidence**: Claims about superiority over existing crossmodal distillation approaches are not fully contextualized, as comparisons are limited to specific datasets and metrics.

## Next Checks
1. Evaluate the method on non-English datasets or specialized domains (e.g., medical imaging) to assess cross-linguistic and domain-specific robustness.
2. Quantify the computational overhead of the multi-teacher architecture and WordNet expansion generation to determine scalability for large-scale applications.
3. Conduct ablation studies to isolate the contributions of WordNet-relaxed embeddings versus other components (e.g., teacher model choice, distillation strategy) to the observed performance gains.