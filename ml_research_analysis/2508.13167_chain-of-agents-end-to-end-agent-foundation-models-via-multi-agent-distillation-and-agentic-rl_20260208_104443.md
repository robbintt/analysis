---
ver: rpa2
title: 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation
  and Agentic RL'
arxiv_id: '2508.13167'
source_url: https://arxiv.org/abs/2508.13167
tags:
- code
- search
- https
- answer
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-Agents introduces a unified end-to-end agent foundation
  model paradigm that replaces multi-agent frameworks with a single model capable
  of dynamic tool and role orchestration. It uses multi-agent distillation to convert
  expert system trajectories into training data, then applies agentic reinforcement
  learning for optimization.
---

# Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL

## Quick Facts
- arXiv ID: 2508.13167
- Source URL: https://arxiv.org/abs/2508.13167
- Reference count: 40
- Primary result: Achieves 55.3% on GAIA, 47.9% on LiveCodeBench v5, and 59.8% on AIME25 while reducing inference cost by 84.6% versus traditional multi-agent systems

## Executive Summary
Chain-of-Agents introduces a unified end-to-end agent foundation model paradigm that replaces multi-agent frameworks with a single model capable of dynamic tool and role orchestration. It uses multi-agent distillation to convert expert system trajectories into training data, then applies agentic reinforcement learning for optimization. The method achieves state-of-the-art performance across diverse benchmarks including web, code, and math tasks while significantly reducing inference costs through elimination of message-passing overhead.

## Method Summary
The method distills multi-agent system trajectories into a single foundation model through supervised fine-tuning, then refines capabilities using agentic reinforcement learning. It extracts reasoning patterns from expert systems like OAgents, reformats them into Chain-of-Agents sequences, and trains a unified model to dynamically activate different tool agents and role-playing agents. The approach uses observation masking during SFT to prevent environmental noise propagation and DAPO for RL optimization on verifiable tasks.

## Key Results
- Web agent achieves 55.3% on GAIA, 11.1% on BrowseComp, and 18.0% on HLE
- Code agent achieves 47.9% on LiveCodeBench v5 and 32.7% on CodeContests
- Math agent achieves 59.8% on AIME25
- Reduces inference cost by 84.6% compared to traditional multi-agent systems
- End-to-end models show greater benefits from test-time scaling than traditional multi-agent systems

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent distillation transfers collaborative reasoning patterns from expert systems into a single foundation model. The approach monitors a state-of-the-art multi-agent system during task execution, recording each agent activation, reasoning state, and output as sequential steps. These trajectories are reformatted into CoA-compatible sequences with special tokens, then used for supervised fine-tuning with observation masking to prevent environmental noise propagation.

### Mechanism 2
Dynamic agent orchestration within a unified decoding process reduces inference overhead while maintaining collaborative reasoning. The model maintains a persistent reasoning state that conditions on previous actions and observations. At each step, it samples which agent role to activate next via learned policy, enabling the model to switch between planning, tool execution, and verification modes within a single forward pass.

### Mechanism 3
Agentic reinforcement learning on verifiable tasks refines tool-selection policies beyond supervised initialization. After SFT provides cold-start capabilities, RL training uses DAPO with rollouts through real tool environments. The reward function uses binary correctness rather than intermediate process rewards, with strategic sampling focusing on challenging cases where tool usage provides genuine value.

## Foundational Learning

- **Multi-Agent Coordination Patterns**: Understanding how role specialization, message passing, and state sharing enable complex task decomposition is essential before attempting to compress these patterns into a single model. Can you explain why traditional multi-agent systems require explicit message protocols between agents, and what information must be preserved when collapsing this into autoregressive generation?

- **Knowledge Distillation at Sequence Level**: The paper extends sequence-level knowledge distillation from single-model settings to multi-agent trajectories. Understanding how teacher-student distribution matching works for token sequences helps interpret why observation masking prevents environmental noise propagation. When distilling multi-agent trajectories, should the student model learn to predict tool outputs or only the agent's reasoning steps? Why?

- **Verifiable Reward Design for RL**: The paper relies on outcome-based rewards rather than process rewards. Understanding the trade-offs between sparse binary rewards and dense intermediate rewards is critical for debugging RL training failures. For a code generation task with 50 test cases, would you use an all-or-nothing reward or a partial reward proportional to test cases passed? What failure modes does each introduce?

## Architecture Onboarding

- **Component map**: Trajectory Synthesis Pipeline (OAgents execution → trajectory extraction → progressive filtering → CoA format conversion) -> SFT Training Loop (ReAct data + distilled trajectories → observation-masked cross-entropy loss) -> RL Training Loop (Query filtering → environment rollouts → DAPO optimization) -> Inference Runtime (Single model forward pass with dynamic agent activation)

- **Critical path**: 1) Set up OAgents environment and verify it can solve representative tasks from your target domain, 2) Run trajectory extraction on generated tasks, ensuring average hops fall in 5-20 range, 3) Apply four-stage filtering; expect ~10-20% retention rate, 4) Train SFT model for 2-2.5 epochs with observation masking enabled, 5) Validate SFT model achieves reasonable cold-start performance before RL, 6) Configure RL environment with tool sandbox, 7) Start RL with DAPO, monitoring reward curves and response length growth

- **Design tradeoffs**: Data volume vs. quality (paper uses ~60k SFT trajectories and ~47k RL queries but emphasizes filtering over quantity), Context window vs. training speed (expands context from 16k to 32k mid-training), Reward sparsity vs. shaping (binary rewards avoid reward hacking but may slow convergence)

- **Failure signatures**: SFT model generates invalid tool calls (indicates insufficient observation masking), RL reward plateaus early (may indicate query filtering threshold is too strict or too loose), Model overuses single tool (suggests biased training data distribution), Inference produces truncated responses (context window exceeded during generation)

- **First 3 experiments**: 1) Trajectory quality ablation: Train SFT models on progressively filtered datasets and measure performance gap on held-out GAIA subset, 2) RL cold-start threshold sweep: Vary pass rate filtering threshold for RL query selection and track training stability, 3) Tool generalization test: Train code agent, then evaluate zero-shot on GAIA with web search and crawl tools added to prompt

## Open Questions the Paper Calls Out

### Open Question 1
What are the fundamental mechanisms that enable end-to-end agent models to derive greater performance benefits from test-time scaling strategies compared to traditional multi-agent systems? The paper demonstrates this phenomenon but does not analyze the architectural or training factors that cause this differential scaling behavior.

### Open Question 2
What training paradigm optimally balances format strictness and flexibility to achieve robust zero-shot generalization across diverse tool interfaces? The paper identifies a correlation but does not establish whether format-strict training is necessary, sufficient, or simply one factor among many for generalization.

### Open Question 3
How does the quality and architectural design of the teacher multi-agent system affect the downstream performance of AFMs trained via multi-agent distillation? It is unclear whether OAgents' specific design choices are encoded into AFM, or whether simpler/different teacher systems would yield equivalent or superior distilled models.

## Limitations

- Heavy dependency on OAgents configuration details that are not fully specified in the paper
- Generalization claims beyond trained domains need more extensive validation across diverse tool combinations
- Binary reward structure may not provide sufficient gradient signal for complex multi-step reasoning

## Confidence

- High Confidence: Performance claims on established benchmarks (GAIA, LiveCodeBench, AIME25)
- Medium Confidence: The 84.6% inference cost reduction and multi-agent distillation mechanism
- Low Confidence: Zero-shot tool generalization claims based on single experiment

## Next Checks

1. **Trajectory Quality Impact Study**: Systematically vary the four-stage filtering thresholds and measure performance degradation on GAIA to quantify each filtering stage's contribution

2. **Reward Signal Ablation**: Compare DAPO training with alternative reward structures (partial rewards for code test cases, process rewards for intermediate steps) to determine if binary rewards are optimal

3. **Cross-Domain Transfer Validation**: Train the web agent exclusively on web tasks, then evaluate zero-shot on code benchmarks with Python interpreter tool added to the prompt to validate generalization claims