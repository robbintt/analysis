---
ver: rpa2
title: Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation
  Framework
arxiv_id: '2504.20213'
source_url: https://arxiv.org/abs/2504.20213
tags:
- implies
- proof
- reasoning
- proofs
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the logical reasoning capabilities of large
  language models (LLMs) through the task of constructing formal proofs in Boolean
  logic. The authors address the scarcity of real-world proof data by proposing an
  efficient randomized procedure for synthesizing valid proofs and introducing a Template
  Transformation technique for data augmentation.
---

# Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework

## Quick Facts
- **arXiv ID**: 2504.20213
- **Source URL**: https://arxiv.org/abs/2504.20213
- **Reference count**: 35
- **Primary result**: Fine-tuned Llama 8B achieves 98% accuracy on depth-7 proofs, outperforming GPT-4o's 70% few-shot performance

## Executive Summary
This paper investigates whether large language models can effectively learn formal logic through the task of constructing Boolean logic proofs. The authors address the scarcity of real-world proof data by developing an efficient randomized procedure for synthesizing valid proofs and introducing template transformation for data augmentation. Through systematic experimentation with Llama models, they demonstrate that fine-tuning enables strong logical reasoning performance, with an 8B parameter model achieving 98% accuracy on depth-7 proofs. The study reveals important relationships between model capacity, proof complexity, and augmentation effectiveness, showing that larger models generalize better to deeper proofs while augmentation techniques benefit smaller models most significantly.

## Method Summary
The authors propose a data-driven framework for training LLMs on formal logic reasoning tasks, specifically Boolean logic proof construction. They address the scarcity of real-world proof data through a randomized proof generation procedure that synthesizes valid proofs efficiently. To augment the limited training data, they introduce Template Transformation, a technique that applies probabilistic transformations to proof templates to create diverse training examples. The framework is evaluated through systematic fine-tuning of Llama models across different parameter scales, comparing performance against GPT-4o's few-shot learning baseline. The evaluation protocol tests generalization to deeper proofs than those seen during training, providing insight into the models' reasoning capabilities versus pattern memorization.

## Key Results
- Llama 8B fine-tuned model achieves 98% accuracy on depth-7 proofs, significantly outperforming GPT-4o's 70% few-shot learning accuracy
- Larger model capacity correlates with better generalization to deeper proofs not seen during training
- Template transformation with 70% probability provides optimal performance gains across model scales
- Smaller models benefit more from proof augmentation techniques than larger models
- Proof augmentation particularly enhances handling of complex expressions and long proofs

## Why This Works (Mechanism)
The success of fine-tuned LLMs in formal logic stems from their ability to learn structural patterns in proof construction rather than mere pattern matching. By synthesizing diverse proof examples through randomized generation and augmenting them with template transformations, the models are exposed to a rich variety of proof structures and logical transformations. This comprehensive training approach enables the models to develop abstract understanding of logical reasoning principles, allowing them to generalize to proofs of greater depth than encountered during training. The effectiveness of template transformation at specific probabilities suggests that introducing controlled variability helps models learn invariant logical principles rather than memorizing specific proof sequences.

## Foundational Learning
- **Propositional Logic**: Why needed - forms the logical foundation for proof tasks; Quick check - models can evaluate truth tables correctly
- **Formal Proof Structure**: Why needed - understanding proof syntax and inference rules; Quick check - models generate syntactically valid proofs
- **Template Transformation**: Why needed - enables data augmentation and generalization; Quick check - models maintain correctness after transformations
- **Proof Depth Generalization**: Why needed - tests abstract reasoning versus memorization; Quick check - models handle proofs deeper than training examples
- **Few-shot Learning**: Why needed - provides baseline for comparison; Quick check - GPT-4o achieves baseline performance metrics
- **Parameter Scaling**: Why needed - determines optimal model size for task; Quick check - performance improves with model size up to 8B parameters

## Architecture Onboarding

**Component Map**: Randomized Proof Generator -> Template Transformer -> Training Pipeline -> Evaluation Framework -> Performance Analysis

**Critical Path**: The essential workflow flows from proof generation through template transformation to model training and evaluation. The randomized proof generation creates the foundational dataset, which is then augmented through template transformations to increase diversity and generalization potential. The training pipeline fine-tunes Llama models on this augmented dataset, followed by evaluation against depth-generalization benchmarks and performance analysis comparing against few-shot baselines.

**Design Tradeoffs**: The framework prioritizes data efficiency through synthetic generation over collecting real-world proofs, trading potential authenticity for scalability. Template transformation introduces controlled noise to enhance generalization but requires careful probability tuning to avoid corrupting proof validity. The focus on Boolean logic with limited operators enables clean experimental control but constrains real-world applicability. Fine-tuning smaller Llama models trades parameter count against training efficiency while maintaining competitive performance.

**Failure Signatures**: Models may memorize proof patterns rather than learning abstract logical principles, evidenced by poor generalization to unseen proof depths. Over-aggressive template transformation can corrupt proof validity, causing accuracy degradation. Insufficient proof diversity in training data leads to overfitting on specific proof structures. Performance plateaus despite increasing model size suggest fundamental limitations in logical reasoning capabilities rather than capacity constraints.

**First Experiments**:
1. Validate proof generation produces syntactically and semantically correct proofs across all depths
2. Test template transformation preserves proof validity while increasing diversity metrics
3. Establish baseline performance of Llama models without fine-tuning on held-out proof validation

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on propositional logic with limited operators (AND, OR, NOT), constraining generalizability to more complex logical domains
- Synthetic proof generation may produce proofs that systematically differ from human-written proofs in structure or complexity distribution
- The optimal template transformation probability (70%) was empirically determined for this specific domain and may not generalize across different logical systems
- The study does not investigate whether models learn abstract logical principles or merely memorize proof patterns from the training distribution

## Confidence

**Major claim clusters confidence:**
- Fine-tuning effectiveness: **High** - Strong quantitative results with clear comparisons
- Template transformation utility: **High** - Consistent performance improvements across model scales
- Model capacity correlation: **Medium** - Results show correlation but limited exploration of underlying mechanisms
- Generalization capabilities: **Low** - Constrained by synthetic data and limited logical operators

## Next Checks
1. Evaluate model performance on human-written formal proofs from established theorem provers to assess genuine logical reasoning versus pattern matching
2. Test generalization to first-order logic and extended propositional operators (implication, biconditional, exclusive OR) to determine scalability of approach
3. Conduct ablation studies on proof depth ranges to identify whether models memorize specific proof patterns or learn abstract logical transformations