---
ver: rpa2
title: 'Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier
  vs Context Effects'
arxiv_id: '2601.20102'
source_url: https://arxiv.org/abs/2601.20102
tags:
- cultural
- medical
- context
- accuracy
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a counterfactual benchmark to test how cultural
  cues affect medical question-answering accuracy in large language models (LLMs).
  By augmenting 150 MedQA items into 1,650 variants with culturally related identifiers,
  contextual details, or both for three groups, and a neutral control, the researchers
  confirmed the clinical invariance of the correct answer with clinician review.
---

# Counterfactual Cultural Cues Reduce Medical QA Accuracy in LLMs: Identifier vs Context Effects

## Quick Facts
- arXiv ID: 2601.20102
- Source URL: https://arxiv.org/abs/2601.20102
- Reference count: 24
- Key outcome: Cultural identifiers and contexts in medical QA significantly degrade LLM accuracy, with largest effects when both are combined.

## Executive Summary
This study introduces a counterfactual benchmark to assess how cultural cues affect medical question-answering accuracy in large language models (LLMs). Researchers augmented 150 MedQA items into 1,650 variants with culturally related identifiers, contextual details, or both for three groups (Indigenous Canadian, Middle-Eastern Muslim, Southeast Asian), plus a neutral control. The correct clinical answer was verified as invariant by clinician review. Four models (GPT-5.2, Llama-3.1-8B, DeepSeek-R1, MedGemma 4B/27B) were evaluated using option-only and explanation-based prompts. Results show that cultural cues significantly reduce accuracy (Cochran's Q, p<10⁻¹⁴), with the largest degradation when both identifier and context co-occur (up to 3-7 percentage points), while neutral edits have minimal effect. More than half of culturally grounded explanations lead to incorrect answers, linking cultural reasoning to diagnostic failure.

## Method Summary
The study built a counterfactual evaluation framework using 150 MedQA test items, generating 1,650 variants through few-shot LLM augmentation (9 cultural variants per item plus neutral control). Cultures targeted were Indigenous Canadian, Middle-Eastern Muslim, and Southeast Asian, with augmentation types including identifier-only, context-only, and both combined. Clinician review verified clinical invariance of correct answers. Five LLMs (GPT-5.2, Llama-3.1-8B, DeepSeek-R1, MedGemma 4B/27B) were evaluated under option-only and explanation+option prompting. Outputs were parsed to extract answers, and LLM-as-judge with human-validated rubric (κ=0.76) flagged culturally grounded explanations. Accuracy, flip rate, and harmful flip rate were computed per condition, with Cochran's Q testing significance.

## Key Results
- Cultural cues significantly reduce medical QA accuracy (Cochran's Q, p<10⁻¹⁴).
- Largest degradation occurs with combined identifier and context augmentation (3-7 percentage points).
- Neutral edits have minimal effect on accuracy.
- Over half of culturally grounded explanations lead to incorrect answers, linking cultural reasoning to diagnostic failure.

## Why This Works (Mechanism)
The study demonstrates that non-medically-decisive cultural cues can trigger diagnostic reasoning pathways in LLMs that override correct clinical answers, revealing a form of cultural bias that manifests as reduced accuracy when culturally specific identifiers or contexts are present in medical questions.

## Foundational Learning
- **Cultural bias in medical AI**: Systematic error patterns linked to patient demographic or cultural attributes; needed to identify and mitigate fairness issues in clinical LLM deployment.
- **Counterfactual evaluation**: Testing model behavior under modified inputs that isolate specific variables; quick check: generate variants with single attribute changes and measure impact.
- **Cochran's Q test**: Non-parametric test for differences across related samples; needed for comparing accuracy across multiple cultural conditions; quick check: verify p-values for significance across conditions.
- **LLM-as-judge with rubric**: Using LLMs to classify outputs based on predefined criteria; needed for scalable detection of culturally grounded explanations; quick check: compute inter-annotator agreement (κ) on sample labels.
- **Clinical invariance verification**: Ensuring answer correctness is preserved across counterfactual variants; needed to isolate cultural effects from clinical content changes; quick check: have clinicians review subset for answer consistency.
- **Semantic diversity via embeddings**: Measuring variation in generated scenarios using cosine similarity; needed to assess augmentation quality and avoid repetitive contexts; quick check: target average similarity ~0.31.

## Architecture Onboarding
- **Component map**: MedQA items -> LLM augmentation pipeline -> 1,650 variants -> LLM inference (5 models × 2 prompts) -> Answer parsing -> LLM-as-judge rubric -> Accuracy/flip analysis
- **Critical path**: Augmentation generation → Clinician invariance validation → LLM inference → Answer extraction → Cultural reasoning classification → Statistical analysis
- **Design tradeoffs**: Automated LLM augmentation offers scalability but risks repetitive contexts; clinician review ensures clinical validity but limits sample size; explanation-based prompting captures reasoning but increases parsing complexity.
- **Failure signatures**: Incorrect augmentation introducing clinical bias (detect via clinician review), explanation parsing failures (detect via malformed output logs), inconsistent cultural reasoning labels (detect via low κ agreement).
- **Exactly 3 first experiments**:
  1. Generate 50 augmented items and have two clinicians independently verify answer invariance.
  2. Run LLM-as-judge rubric on 100 explanations and compute κ agreement with human labels.
  3. Execute complete pipeline on 300 samples using GPT-4o substitute to compare accuracy patterns.

## Open Questions the Paper Calls Out
- **What specific mitigation strategies can effectively reduce cultural bias in medical LLMs without compromising clinically relevant personalization?** The study motivates future research on bias mitigation but does not develop remediation algorithms.
- **Do these findings generalize to a broader range of cultural groups, intersectional identities, and non-Western clinical settings?** The study was limited to three specific groups and the MedQA dataset.
- **How does LLM-based cultural augmentation affect scenario diversity, and can it be improved to avoid repetitive stereotypical contexts?** The authors note that LLM augmentation can reduce diversity and repeat contexts.

## Limitations
- GPT-5.2 is unavailable, requiring substitution with closest available model (likely GPT-4o), which may affect absolute accuracy values.
- Cultural augmentation generation depends on unspecified few-shot prompts, introducing variability in how cultural elements are generated.
- LLM-as-judge rubric is not completely detailed, creating uncertainty about exact labeling criteria.

## Confidence
- **High confidence**: Directional finding that cultural cues reduce medical QA accuracy, supported by significant Cochran's Q tests across multiple models.
- **Medium confidence**: Specific magnitude of accuracy degradation and relative model sensitivities may vary with model substitutions and prompt variations.
- **Low confidence**: Exact distribution of error types depends heavily on specific rubric implementation and may not replicate precisely.

## Next Checks
1. Have at least two independent clinicians review 50 randomly selected augmented items to confirm clinical invariance of correct answers.
2. Apply LLM-as-judge rubric to a held-out sample of 100 culturally modified explanations and compare labels with human annotators, targeting κ≥0.70.
3. Run complete analysis pipeline using GPT-4o instead of unavailable GPT-5.2 on a subset of 300 samples to assess model substitution impact.