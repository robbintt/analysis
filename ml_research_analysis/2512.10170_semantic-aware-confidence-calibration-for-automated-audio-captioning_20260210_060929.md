---
ver: rpa2
title: Semantic-Aware Confidence Calibration for Automated Audio Captioning
arxiv_id: '2512.10170'
source_url: https://arxiv.org/abs/2512.10170
tags:
- confidence
- audio
- semantic
- calibration
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of overconfident predictions in
  automated audio captioning models, which limits their reliability despite strong
  performance on standard metrics. The authors propose a framework that integrates
  confidence prediction into audio captioning by augmenting a Whisper-based model
  with a learned confidence head that estimates uncertainty from decoder hidden states.
---

# Semantic-Aware Confidence Calibration for Automated Audio Captioning

## Quick Facts
- arXiv ID: 2512.10170
- Source URL: https://arxiv.org/abs/2512.10170
- Authors: Lucas Dunker; Sai Akshay Menta; Snigdha Mohana Addepalli; Venkata Krishna Rayalu Garapati
- Reference count: 11
- Primary result: Semantic-aware confidence calibration dramatically improves both calibration (CLAP-based ECE of 0.071) and caption quality (BLEU-4 from 0.066 to 0.115) on Clotho v2

## Executive Summary
This paper addresses the problem of overconfident predictions in automated audio captioning models, which limits their reliability despite strong performance on standard metrics. The authors propose a framework that integrates confidence prediction into audio captioning by augmenting a Whisper-based model with a learned confidence head that estimates uncertainty from decoder hidden states. They redefine caption correctness using semantic similarity (CLAP and FENSE) rather than n-gram overlap, enabling meaningful Expected Calibration Error (ECE) computation. Experiments on Clotho v2 show that confidence-guided beam search with semantic evaluation achieves dramatically improved calibration (CLAP-based ECE of 0.071 vs. 0.488 for greedy decoding) while simultaneously improving caption quality (BLEU-4 from 0.066 to 0.115, CIDEr from 0.150 to 0.290). The results demonstrate that semantic similarity provides a more meaningful foundation for confidence calibration than traditional n-gram metrics.

## Method Summary
The authors augment a Whisper-small model fine-tuned for audio captioning with a confidence prediction head that maps decoder hidden states to confidence scores. Training uses a combined loss: standard caption generation plus MSE between mean predicted confidence and semantic correctness (measured via CLAP similarity to reference captions). They employ temperature scaling for post-hoc calibration optimization. At inference, confidence-guided beam search re-ranks candidates using a combination of log-likelihood, length penalty, and predicted confidence. Semantic correctness is defined using CLAP and FENSE similarity metrics with a threshold of 0.6, enabling computation of CLAP-based Expected Calibration Error (ECE) as the primary calibration metric.

## Key Results
- Confidence-guided beam search achieves CLAP-based ECE of 0.071 vs. 0.488 for greedy decoding
- Caption quality improves: BLEU-4 from 0.066 to 0.115, CIDEr from 0.150 to 0.290
- CLAP similarity increases from 0.600 to 0.685
- Temperature scaling reduces CLAP-ECE from 0.227 to 0.071
- Confidence scores correlate with human judgments of caption reliability

## Why This Works (Mechanism)

### Mechanism 1: Semantic Correctness Redefinition Enables Meaningful Calibration
Traditional n-gram metrics fail to capture semantic equivalence, making calibration measurements meaningless; semantic similarity provides a more coherent correctness signal. CLAP embeddings map audio and text into a shared multimodal space; a caption is classified as "correct" if `max(cos(E_text(pred), E_text(ref))) ≥ 0.6`. This replaces BLEU thresholding with semantic grounding. Core assumption: CLAP/FENSE similarity correlates with human judgment of semantic correctness in your target domain. Evidence: "redefining correctness through semantic similarity... enabling Expected Calibration Error (ECE) computation that reflects true caption quality rather than surface-level text overlap." Break condition: If semantic similarity metrics do not correlate with human judgment in your domain, the calibration signal becomes unreliable.

### Mechanism 2: Decoder Hidden States Encode Predictive Uncertainty
The final-layer decoder hidden state contains sufficient information to estimate caption reliability. A three-layer MLP (768→384→192→1 with ReLU, dropout 0.1, sigmoid) maps the decoder's final hidden state to a confidence score. Training uses MSE loss between mean token confidence and semantic correctness score. Core assumption: Uncertainty is primarily decoder-side; encoder ambiguity (e.g., unclear audio) is partially captured through decoder representations. Evidence: "confidence prediction head that estimates uncertainty from decoder hidden states" and "ct = σ(MLP(h_t^(L))) where h_t^(L) is the decoder hidden state at the final layer for token t." Break condition: If model architecture does not expose decoder hidden states, or if uncertainty is primarily encoder-driven without decoder propagation.

### Mechanism 3: Confidence-Guided Reranking Explores the Likelihood-Confidence Trade-off
Incorporating predicted confidence into beam search scoring yields both higher-quality and better-calibrated captions. Final score combines log-likelihood, length penalty (α=1.0), and confidence (β=0.3): `score(b) = logp(b)/|b|^α + β·c̄_b`. Reranking selects among beam candidates using this joint objective. Core assumption: Likelihood and confidence are not perfectly correlated; there exist high-likelihood, low-confidence candidates worth downweighting. Evidence: "confidence-guided beam search... achieves dramatically improved calibration (CLAP-based ECE of 0.071) compared to greedy decoding baselines (ECE of 0.488)" and "High-confidence predictions (>0.7) typically describe unambiguous audio events, while lower-confidence predictions often involve complex scenes." Break condition: If confidence and likelihood are perfectly aligned, the reranking term provides no signal gain.

## Foundational Learning

- **Expected Calibration Error (ECE)**: Primary metric for evaluating whether confidence scores align with actual correctness; lower ECE = better calibration. Quick check: Given ECE bins samples by confidence, why would a model with uniform 1.0 confidence have high ECE?
- **CLAP (Contrastive Language-Audio Pretraining) Embeddings**: Provides the semantic similarity function that defines "correctness" for calibration; maps audio and text into shared space. Quick check: If two captions have identical meaning but different words (e.g., "water flowing" vs. "liquid pouring"), what should their CLAP similarity be?
- **Temperature Scaling**: Post-hoc calibration technique that softens (T>1) or sharpens (T<1) softmax probability distributions. Quick check: Why is temperature optimized on a validation set rather than training set?

## Architecture Onboarding

- **Component map**: Audio (16kHz, max 30s) → Whisper encoder → decoder generates tokens → per-token hidden state → confidence MLP → per-token confidence score
- **Critical path**: 1) Audio → Whisper encoder → decoder generates tokens; 2) At each token: hidden state → confidence MLP → per-token confidence score; 3) Training: MSE between mean sequence confidence and semantic correctness score (λ=0.15); 4) Inference: Beam search (size 5) → rerank by `logp/length + 0.3×confidence`
- **Design tradeoffs**: Correctness threshold τ=0.6 is arbitrary (paper acknowledges this); different applications may need different thresholds. Loss weight λ=0.15 and reranking weight β=0.3 are validation-tuned hyperparameters. Beam size 5 balances quality vs. computation.
- **Failure signatures**: Confidence stuck at 1.0 → confidence head not learning (check gradient flow). CLAP ECE improves but FENSE ECE doesn't → metric-specific overfitting. Traditional ECE remains high (~0.5) even with beam search → expected behavior (n-gram metrics are the problem, not the model).
- **First 3 experiments**: 1) Baseline reproduction: Run greedy decoding on evaluation split, verify ECE ≈ 0.488, BLEU-4 ≈ 0.066; 2) Ablate confidence head: Train with only temperature scaling (no MLP), compare ECE to full system; 3) Threshold sensitivity: Sweep τ ∈ {0.5, 0.55, 0.6, 0.65, 0.7} and plot ECE vs. threshold to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
How does calibration performance vary across different semantic similarity thresholds, and can an optimal threshold be learned adaptively for different audio content types? The authors acknowledge "our semantic correctness threshold (τ= 0.6) is somewhat arbitrary; different applications may require different thresholds." This remains unresolved because the paper fixes τ= 0.6 without exploring sensitivity analysis or adaptive thresholding strategies. What evidence would resolve it: Systematic experiments varying τ across values (e.g., 0.4-0.8) on validation data, or a learned threshold module conditioned on audio features.

### Open Question 2
Does the confidence calibration framework generalize to other audio captioning datasets (AudioCaps, SoundDescs) with different acoustic characteristics and annotation styles? The authors state "we evaluate only on Clotho; generalization to other datasets (AudioCaps, larger-scale data) requires validation." This remains unresolved because Clotho contains specific environmental sounds (15-30 seconds) that may not represent the full diversity of audio captioning scenarios. What evidence would resolve it: Replication of the calibration experiments on AudioCaps and other benchmarks, reporting CLAP-based ECE and quality metrics.

### Open Question 3
Can incorporating audio-side uncertainty estimates (e.g., from CLAP audio embeddings) improve calibration beyond decoder-side confidence alone? Future work proposes "audio-grounded confidence using CLAP embeddings of the audio itself, not just text" and notes the confidence head "may not capture all sources of uncertainty (e.g., encoder-side ambiguity)." This remains unresolved because the current confidence head operates solely on decoder hidden states. What evidence would resolve it: Ablation studies comparing decoder-only vs. encoder-decoder confidence estimation, measuring ECE improvements when audio embeddings inform confidence prediction.

## Limitations
- **Threshold Dependence**: The semantic correctness threshold τ=0.6 is empirically chosen and may not generalize across domains or audio captioning datasets
- **Metric Correlation Stability**: CLAP similarity correlation with human judgments demonstrated only for Clotho environmental audio, may not hold for speech-based or music captioning tasks
- **Confidence Head Generalization**: Confidence prediction head trained on Clotho-specific audio-caption pairs may become unreliable on out-of-distribution audio without fine-tuning

## Confidence
- **High Confidence**: Confidence-guided beam search improves both caption quality and calibration on Clotho v2; Traditional n-gram metrics fail to capture semantic equivalence in audio captioning; Semantic similarity metrics (CLAP, FENSE) correlate better with human judgments than BLEU/CIDEr
- **Medium Confidence**: Decoder hidden states contain sufficient information to estimate caption uncertainty; Confidence-Guided beam search improves calibration without sacrificing quality; Temperature scaling provides effective post-hoc calibration
- **Low Confidence**: The specific confidence-guided scoring formula (β=0.3) is optimal across all audio captioning scenarios; The semantic correctness threshold τ=0.6 generalizes to other domains; Confidence scores can reliably detect all types of model uncertainty (including encoder-side ambiguity)

## Next Checks
1. **Cross-Dataset Generalization Test**: Apply the trained confidence calibration model to a different audio captioning dataset (e.g., AudioCaps or AudioVisual Events) and evaluate whether CLAP-based ECE remains low and whether caption quality metrics maintain improvement.
2. **Human Judgment Validation**: Conduct a human evaluation study where raters assess semantic correctness of captions using the same CLAP threshold (τ=0.6) and verify that confidence scores correlate with human-perceived reliability.
3. **Uncertainty Type Analysis**: Create controlled test cases with different uncertainty sources (e.g., noisy audio, ambiguous scenes, rare events) and analyze whether confidence scores appropriately reflect these distinct uncertainty types.