---
ver: rpa2
title: 'SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis'
arxiv_id: '2506.02096'
source_url: https://arxiv.org/abs/2506.02096
tags:
- data
- arxiv
- reasoning
- difficulty
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SynthRL, a pipeline for automatically synthesizing
  more challenging training data for vision-language models trained with verifiable
  rewards. The core idea is to select seed questions that are too easy for the target
  model, use a strong VLM to generate harder variants while preserving answers, and
  verify both correctness and increased difficulty via Monte Carlo rollouts.
---

# SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis

## Quick Facts
- **arXiv ID**: 2506.02096
- **Source URL**: https://arxiv.org/abs/2506.02096
- **Reference count**: 40
- **Primary result**: SynthRL synthesizes harder visual math questions that improve model performance across five out-of-domain benchmarks

## Executive Summary
SynthRL introduces a novel pipeline for automatically generating more challenging training data for vision-language models by leveraging verifiable rewards. The method identifies easy questions from existing datasets, uses a strong VLM to create harder variants while preserving correct answers, and employs Monte Carlo rollouts to verify both correctness and increased difficulty. Applied to MMK12, this approach synthesizes 3.3K harder questions from 8K seeds, resulting in consistent performance gains across multiple visual math reasoning benchmarks, particularly on the most difficult samples.

## Method Summary
The SynthRL pipeline operates through three main stages: (1) difficulty assessment using a target VLM to identify easy questions from seed datasets, (2) data synthesis where a strong VLM generates harder variants while preserving the original answers, and (3) verification using Monte Carlo rollouts to ensure both correctness and increased difficulty. The key innovation is the use of verifiable rewards throughout the process, allowing the system to iteratively improve the quality and challenge level of synthetic training data. This approach addresses the common problem of visual reasoning models overfitting to easy questions in existing datasets.

## Key Results
- Synthesized 3.3K harder questions from ~8K seed questions in the MMK12 dataset
- Models trained on augmented data show consistent gains across five out-of-domain visual math reasoning benchmarks
- Largest performance improvements observed on the hardest evaluation samples
- Ablation studies confirm that aligned verification and data augmentation are critical to achieving these gains

## Why This Works (Mechanism)
The core mechanism relies on the observation that many existing visual reasoning datasets contain questions that are too easy for current models, limiting their ability to generalize to harder problems. By using a strong VLM to generate harder variants while preserving correct answers, SynthRL creates a curriculum of increasing difficulty. The Monte Carlo verification ensures that synthesized questions are both solvable and genuinely harder than their seeds, preventing the introduction of noise or degenerate examples. This creates a feedback loop where the model is continuously challenged with appropriately difficult examples that push its reasoning capabilities.

## Foundational Learning
- **Visual Language Model (VLM) Architecture**: Needed to understand how VLMs process image-text pairs for reasoning tasks; Quick check: Can the VLM accurately parse both visual elements and mathematical relationships?
- **Monte Carlo Rollouts**: Essential for verifying question difficulty and answer correctness probabilistically; Quick check: Does the variance in rollouts converge to stable difficulty assessments?
- **Question Difficulty Measurement**: Critical for identifying easy seeds and validating synthesized harder variants; Quick check: Is the difficulty metric correlated with actual model performance gaps?
- **Verifiable Reward Design**: Required to ensure synthetic data maintains mathematical correctness while increasing complexity; Quick check: Can the reward system distinguish between genuinely harder questions and simply longer ones?
- **Data Augmentation in Vision-Language Tasks**: Important for understanding how synthetic data integrates with existing training pipelines; Quick check: Does augmented data maintain domain distribution or introduce artifacts?
- **Curriculum Learning Principles**: Relevant for understanding how progressive difficulty impacts model generalization; Quick check: Is there an optimal difficulty progression that maximizes learning efficiency?

## Architecture Onboarding

**Component Map**: Seed Dataset -> Difficulty Assessment VLM -> Question Generator VLM (34B) -> Monte Carlo Verifier -> Augmented Dataset

**Critical Path**: The pipeline's critical path runs from seed question selection through difficulty assessment, generation, and verification. Each stage depends on the previous one, with the verification step serving as a quality gate that can reject or accept generated questions. The 34B-parameter generator VLM is the computational bottleneck, as its inference cost scales with the number of synthesis attempts.

**Design Tradeoffs**: The primary tradeoff is between synthesis quality and computational cost. Using a larger generator VLM produces better questions but increases inference costs. The Monte Carlo verification adds accuracy but requires multiple rollouts per question. Alternative designs could use smaller generators with more iterations or different verification mechanisms, but this would likely compromise the difficulty-aware nature of the synthesis.

**Failure Signatures**: The system can fail if the generator VLM produces questions that are ill-posed or outside the mathematical domain, if the verification step cannot accurately assess difficulty, or if the difficulty metric becomes saturated. Performance degradation typically manifests as plateaus in benchmark improvements or increased variance across different test sets.

**First Experiments**:
1. Validate that the difficulty assessment VLM can accurately rank questions by difficulty on held-out data
2. Test the generator VLM's ability to create harder variants while preserving answers on a small sample
3. Run Monte Carlo verification on synthesized questions to establish baseline difficulty measurement accuracy

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation scope limited to mathematical reasoning domains without testing broader visual reasoning applicability
- Method depends critically on access to sufficiently capable verifier models
- Computational overhead from Monte Carlo rollouts scales with the number of synthesized samples
- Performance gains demonstrated using a 34B-parameter generator VLM, raising scalability concerns for smaller models

## Confidence
- **High confidence**: Core synthesis methodology and general pattern of improved performance on test benchmarks
- **Medium confidence**: Specific difficulty-aware gains on harder evaluation samples, as this relies on accuracy of difficulty assessment method
- **Medium confidence**: Relative importance of aligned verification versus data augmentation, as shown through ablation studies

## Next Checks
1. Test transferability of SynthRL-synthesized data to non-mathematical visual reasoning tasks to assess domain generality
2. Evaluate performance using smaller generator models (e.g., 7B or 13B parameters) to understand practical scalability constraints
3. Conduct ablation studies varying the number of Monte Carlo rollouts to quantify tradeoff between verification accuracy and computational cost