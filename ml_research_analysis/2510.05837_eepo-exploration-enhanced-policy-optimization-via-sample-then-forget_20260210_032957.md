---
ver: rpa2
title: 'EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget'
arxiv_id: '2510.05837'
source_url: https://arxiv.org/abs/2510.05837
tags:
- eepo
- exploration
- policy
- entropy
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the exploration-exploitation dilemma in reinforcement
  learning with verifiable rewards (RLVR) for large language models, where policies
  tend to over-exploit dominant high-reward modes leading to entropy collapse and
  poor generalization. The proposed Exploration-Enhanced Policy Optimization (EEPO)
  introduces a sample-then-forget mechanism that divides rollouts into two stages:
  first sampling half the trajectories, then performing a lightweight unlearning step
  to suppress these responses, and finally sampling the remaining trajectories from
  the updated model.'
---

# EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget
## Quick Facts
- **arXiv ID**: 2510.05837
- **Source URL**: https://arxiv.org/abs/2510.05837
- **Reference count**: 12
- **Primary result**: Achieves 24.3-33.0% average relative improvements on mathematical reasoning benchmarks over GRPO baselines

## Executive Summary
This paper addresses the exploration-exploitation dilemma in RLVR for LLMs, where policies tend to over-exploit dominant high-reward modes leading to entropy collapse and poor generalization. The proposed EEPO introduces a sample-then-forget mechanism that divides rollouts into two stages: first sampling half the trajectories, then performing a lightweight unlearning step to suppress these responses, and finally sampling the remaining trajectories from the updated model. This approach promotes exploration of alternative modes beyond dominant behaviors. EEPO achieves substantial performance gains on mathematical reasoning tasks while maintaining training efficiency comparable to standard GRPO.

## Method Summary
EEPO introduces a novel sample-then-forget mechanism to address the exploration-exploitation dilemma in RLVR. The approach works by first sampling half of the trajectories from the current policy, then applying a lightweight unlearning step to suppress these sampled responses from being repeated, and finally sampling the remaining trajectories from the updated model. This two-stage process encourages the policy to explore alternative response modes beyond the dominant high-reward behaviors that typically emerge in standard RLVR training. The unlearning step is designed to be computationally efficient while effectively promoting exploration of diverse solution strategies.

## Key Results
- EEPO achieves average relative improvements of 24.3% on Qwen2.5-3B and 33.0% on Llama3.2-3B-Instruct across five mathematical reasoning benchmarks
- Outperforms GRPO baselines by 10.4% on average across tested tasks
- Maintains training efficiency comparable to standard GRPO while improving exploration capabilities

## Why This Works (Mechanism)
The core insight is that standard RLVR algorithms tend to converge to a narrow set of high-reward responses, creating an exploration-exploitation trade-off where the policy entropy collapses and generalization suffers. EEPO's sample-then-forget mechanism actively prevents this collapse by suppressing responses that have already been explored, forcing the policy to discover alternative solution paths. The two-stage rollout process ensures that the model doesn't get stuck in local optima by continuously encouraging exploration of diverse behaviors while still optimizing for reward.

## Foundational Learning
**Entropy Collapse**: Why needed - prevents policy from converging to narrow high-reward modes; Quick check - monitor policy entropy during training to ensure it doesn't approach zero
**Exploration-Exploitation Trade-off**: Why needed - balances discovering new solutions versus optimizing known good ones; Quick check - compare diversity of generated responses before and after applying EEPO
**Unlearning in RL**: Why needed - enables efficient suppression of explored behaviors without full retraining; Quick check - verify that unlearning step effectively reduces probability of previously sampled responses

## Architecture Onboarding
**Component Map**: Environment -> Policy Network -> Two-Stage Sampling (Stage 1 -> Unlearning -> Stage 2) -> Reward Computation -> Policy Update
**Critical Path**: The unlearning step between the two sampling stages is critical - if too aggressive, exploration suffers; if too weak, entropy collapse returns
**Design Tradeoffs**: Computational overhead of unlearning versus exploration gains; balance between suppression strength and maintaining useful learned behaviors
**Failure Signatures**: If unlearning is too strong, policy may explore random behaviors with no reward structure; if too weak, results resemble standard GRPO with no exploration benefits
**First Experiments**: 1) Test unlearning strength sensitivity by varying suppression parameters; 2) Compare single-stage versus two-stage sampling without unlearning; 3) Measure entropy evolution across training epochs

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical gains may be sensitive to hyperparameter tuning differences between EEPO and baseline GRPO
- Sample-then-forget mechanism introduces additional variance through unlearning step that could affect stability
- Effectiveness on tasks with continuous or high-dimensional action spaces remains unclear as evaluation focuses on discrete mathematical reasoning
- Computational overhead characterization lacks detail across different model scales and batch sizes

## Confidence
**High Confidence**: The core mechanism of suppressing explored trajectories to promote alternative mode exploration is theoretically sound and aligns with established exploration-exploitation principles in RL.

**Medium Confidence**: The empirical improvements over GRPO baselines are well-documented for the tested mathematical reasoning tasks, but generalizability to other domains requires validation.

**Low Confidence**: The claim of maintaining training efficiency comparable to GRPO is supported but lacks detailed computational complexity analysis across varying batch sizes and model scales.

## Next Checks
1. Conduct ablation studies removing the unlearning step to quantify its specific contribution to performance gains versus the baseline two-stage sampling approach.
2. Test EEPO on non-mathematical reasoning tasks (e.g., code generation, instruction following) to assess domain generalization and potential degradation in performance on tasks requiring consistent policy behavior.
3. Measure wall-clock training time and memory overhead across different batch sizes and model scales to provide comprehensive efficiency characterization beyond simple comparison to GRPO.