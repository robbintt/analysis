---
ver: rpa2
title: 'FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large
  Language Models'
arxiv_id: '2506.14824'
source_url: https://arxiv.org/abs/2506.14824
tags:
- fednano
- client
- federated
- multimodal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedNano addresses the challenge of deploying large-scale Multimodal
  Large Language Models (MLLMs) in federated learning environments, where centralized
  training is infeasible due to privacy constraints and distributed data. The key
  innovation is a framework that centralizes the LLM on the server while enabling
  lightweight client-side adaptation through modality-specific NanoAdapters.
---

# FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2506.14824
- Source URL: https://arxiv.org/abs/2506.14824
- Reference count: 22
- Key outcome: FedNano reduces client-side storage by 95% and communication overhead to 0.01% of model parameters while outperforming federated learning baselines on ScienceQA and IconQA benchmarks.

## Executive Summary
FedNano addresses the challenge of deploying large-scale Multimodal Large Language Models (MLLMs) in federated learning environments, where centralized training is infeasible due to privacy constraints and distributed data. The key innovation is a framework that centralizes the LLM on the server while enabling lightweight client-side adaptation through modality-specific NanoAdapters. This design reduces client-side storage by over 95% and communication overhead to just 0.01% of model parameters by transmitting only compact adapter updates. FedNano also introduces Fisher Merging with diagonal FIM approximation to improve global aggregation under non-IID data. Experiments on ScienceQA and IconQA show that FedNano outperforms existing federated learning baselines, achieving higher accuracy while maintaining minimal communication and computational costs, thus enabling scalable, privacy-preserving MLLM deployment.

## Method Summary
FedNano is a federated learning framework that centralizes the frozen LLM on the server while deploying lightweight NanoAdapters on clients for efficient multimodal model adaptation. The server holds the full MLLM backbone, while each client runs a "NanoEdge" module containing frozen modality encoders, a connector, and trainable low-rank NanoAdapters for vision and text. During training, clients perform forward passes through their local modules and upload only NanoAdapter parameters to the server, which completes LLM inference and aggregation. The framework employs Fisher Merging with diagonal Fisher Information Matrix approximation to weight client updates by parameter importance, improving robustness under non-IID data distributions. Communication overhead is reduced to 0.01% of model parameters by transmitting only adapter updates, and client storage is reduced by 95% by avoiding full model deployment.

## Key Results
- Client-side storage reduced by 95% compared to full MLLM deployment
- Communication overhead limited to 0.01% of model parameters through NanoAdapter updates
- Outperforms FedAvg, FedProx, and other federated baselines on ScienceQA and IconQA benchmarks
- Fisher Merging consistently achieves highest average accuracy in highly non-IID settings (α = 0.1)

## Why This Works (Mechanism)

### Mechanism 1: Server-Centralized LLM with External Client Adapters
Centralizing the frozen LLM on the server while deploying only lightweight NanoAdapters on clients enables FL for MLLMs where full model deployment is infeasible. The LLM never leaves the server. Clients receive a frozen modality encoder (e.g., Vision Transformer), a frozen connector/projector, and two trainable low-rank adapters (NanoAdapter-I for vision, NanoAdapter-T for text). These adapters interface with the LLM only through the connector output, requiring no structural access to LLM internals. During forward passes, client-side computation stops at adapter output; the server completes LLM inference.

### Mechanism 2: Low-Rank NanoAdapters for Parameter-Efficient Communication
Low-rank decomposition in NanoAdapters reduces communication to 0.01% of model parameters while preserving task-specific adaptation capacity. Each NanoAdapter uses a down-projection (reducing dimensionality) followed by an up-projection (restoring it), mirroring LoRA. Only adapter parameters (θk) are trained locally and uploaded. With rank-64 adapters on LLaVA-1.5-7B, uploads are ~1.05M parameters vs. ~180.89M for FedDPA-F.

### Mechanism 3: Fisher-Guided Aggregation for Non-IID Robustness
Fisher Merging with diagonal FIM approximation improves global aggregation under heterogeneous client data by weighting updates by parameter importance. Instead of uniform averaging (FedAvg), FedNano computes per-client Fisher Information Matrices Fk as precision matrices under Laplace approximation. Global parameters are: θglobal = Σ(|Dk|·Fk·θk) / Σ(|Dk|·Fk). Diagonal approximation reduces FIM from O(|θ|²) to O(|θ|), computed from squared gradients during backprop.

## Foundational Learning

- **Federated Learning (FedAvg, FedProx)**: Why needed here: FedNano builds on FL fundamentals—local training, periodic aggregation, non-IID challenges. Without this baseline, Fisher Merging's advantages over FedAvg are unclear.
- **Parameter-Efficient Fine-Tuning (LoRA)**: Why needed here: NanoAdapters directly implement LoRA-style low-rank decomposition. Understanding rank vs. capacity tradeoffs is essential for configuring adapters.
- **Multimodal LLM Architecture (Encoders → Connector → LLM)**: Why needed here: FedNano's design hinges on where to insert adapters—externally at the connector-LLM interface rather than inside LLM layers.

## Architecture Onboarding

- **Component map**:
  - Server: Frozen LLM backbone (e.g., LLaVA-1.5-7B, ~7B params), aggregation module (Fisher Merging)
  - Client (NanoEdge): Frozen Vision Transformer encoder, Text Embedding Layer, Modality Projector; Trainable NanoAdapter-I (vision), NanoAdapter-T (text), each with low-rank A and B matrices
  - Communication: Only NanoAdapter parameters (Δθ) and diagonal FIM values are transmitted per round

- **Critical path**:
  1. Server distributes initialized NanoAdapter weights to all clients
  2. Each client runs local epochs: encode multimodal inputs → apply NanoAdapters → send adapter outputs to server
  3. Server completes LLM forward pass, returns loss/gradients to client
  4. Client updates NanoAdapters via backprop, computes diagonal FIM from squared gradients
  5. Client uploads {θk, Fk} to server
  6. Server aggregates via Fisher Merging (Eq. 1), broadcasts new global θ

- **Design tradeoffs**:
  - Adapter rank: Higher rank → better accuracy but more communication; rank-64 used in experiments
  - FIM precision: Full FedNano (extra passes) vs. FedNano-EF (approximated during training)—Table 7 shows ~0.5-0.6% accuracy tradeoff
  - Communication frequency: More frequent rounds amplify FedNano's advantage

- **Failure signatures**:
  - Client storage exceeds budget: Verify only NanoEdge deployed (not full MLLM); check encoder freezing
  - Aggregation underperforms FedAvg: Ensure FIM is computed correctly (diagonal from squared gradients); check for missing |Dk| weighting
  - Accuracy plateaus despite more rounds: Adapter rank may be too low; client data may be too heterogeneous

- **First 3 experiments**:
  1. Reproduce Table 2 baseline: Run FedNano vs. FedAvg/FedProx on ScienceQA with MiniGPT-4, 5 clients, α=1, rank-64. Verify avg accuracy improvement.
  2. Communication ablation: Measure upload size (params) and round time for FedNano vs. FedDPA-F on LLaVA-1.5-7B. Confirm ~99% reduction.
  3. Non-IID stress test: Run Table 3 experiment with α=0.1 vs. α=5 on IconQA. Confirm FedNano's advantage increases under high heterogeneity.

## Open Questions the Paper Calls Out

- **How can adaptive mechanisms be designed to dynamically adjust NanoAdapter configurations for clients with highly heterogeneous hardware capabilities?**
  The current assumption that all clients possess similar hardware may not hold and suggests future research explore adaptive mechanisms for resource constraints.

- **What architectural or optimization strategies are needed to maintain collaboration effectiveness when clients have incomplete modalities (e.g., missing visual or text data)?**
  Practical deployments may involve incomplete modality settings, which requires "new strategies to ensure effective collaboration" without full modality availability.

- **Can differential privacy be integrated into FedNano without sacrificing the computational and communication efficiency that defines the framework?**
  Integrating differential privacy is a promising direction but highlights the challenge of achieving it "without sacrificing the computational and communication efficiency."

## Limitations
- The diagonal approximation of the Fisher Information Matrix may not fully capture parameter importance for complex multimodal tasks
- Architecture assumes all LLM inference can be centralized, which may not generalize to tasks requiring on-device autoregressive generation
- Evaluation focuses on ScienceQA and IconQA benchmarks; performance on other multimodal tasks or domains remains unverified

## Confidence

- **High Confidence**: The parameter efficiency claims (95% storage reduction, 0.01% communication overhead) are well-supported by the experimental design and align with established LoRA principles
- **Medium Confidence**: The Fisher Merging approach shows consistent advantages over FedAvg in the reported experiments, but its superiority depends on the quality of diagonal FIM approximation
- **Medium Confidence**: The architecture's feasibility hinges on the assumption that clients can execute forward passes through encoders and adapters without LLM access

## Next Checks

1. **Robustness to FIM Approximation**: Compare Fisher Merging with full vs. diagonal FIM on a subset of ScienceQA to quantify accuracy loss from approximation
2. **Scalability to Larger Models**: Test FedNano with larger MLLMs (e.g., LLaVA-1.5-13B) to assess whether parameter efficiency gains scale with model size
3. **Generalization to New Tasks**: Evaluate FedNano on a non-VQA multimodal task (e.g., image captioning) to verify architecture flexibility beyond the tested benchmarks