---
ver: rpa2
title: Reinforcement Learning for Robotic Safe Control with Force Sensing
arxiv_id: '2512.02022'
source_url: https://arxiv.org/abs/2512.02022
tags:
- learning
- reinforcement
- force
- control
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces force and tactile sensing into reinforcement
  learning to improve the safety and reliability of robotic manipulation, especially
  during sim-to-real transfer. The authors integrate force/torque and touch sensors
  into the observation space and reward function, and add a safe control strategy
  to prevent abnormal behaviors such as excessive force or collision.
---

# Reinforcement Learning for Robotic Safe Control with Force Sensing

## Quick Facts
- arXiv ID: 2512.02022
- Source URL: https://arxiv.org/abs/2512.02022
- Reference count: 34
- The paper integrates force/torque and touch sensors into reinforcement learning for safe robotic manipulation, improving sim-to-real transfer and training efficiency.

## Executive Summary
This paper introduces force and tactile sensing into reinforcement learning to enhance the safety and reliability of robotic manipulation, particularly during sim-to-real transfer. The authors integrate force/torque and touch sensors into the observation space and reward function, and add a safe control strategy to prevent abnormal behaviors such as excessive force or collision. Experiments on an object pushing task show that using force/torque sensors alone or in combination with touch sensors significantly improves training efficiency and success rate in simulation. In real-world tests, the force-based strategy maintains high success rates despite domain gaps, and effectively prevents failures caused by collision. The approach enhances exploration efficiency, ensures safety, and supports robust transfer from simulation to real-world environments.

## Method Summary
The method uses DDPG with Hindsight Experience Replay as the base algorithm, augmented with force/torque and touch sensors. The observation space includes binarized force/torque indicators and tactile contact signals. The reward function combines the sparse task reward with auxiliary force-based rewards that penalize excessive force and reward successful contact. A safety gain term directly modulates actions based on force feedback to prevent hazardous outputs. The policy is trained in MuJoCo simulation and transferred to a real UR5e robot without domain randomization, relying on the force-based safety strategy to handle sim-to-real gaps.

## Key Results
- Force/torque sensing alone or combined with touch sensing significantly improves training efficiency and success rate in simulation.
- Force-based corrective actions maintain high success rates during real-world deployment despite domain gaps.
- The safety strategy effectively prevents collisions and abnormal behaviors such as excessive force or object scrolling.

## Why This Works (Mechanism)

### Mechanism 1: Force-Based Reward Shaping
The method adds auxiliary force-based rewards to the sparse task reward, creating a denser signal that guides exploration toward safer contact behaviors. Two reward terms are used: one penalizes excessive force/torque, and another rewards successful tactile contact. This reshapes the reward landscape to discourage unsafe behaviors during policy learning.

### Mechanism 2: Safety Gain Corrective Action
During action selection, a safety term is added that directly modulates the policy's output based on real-time force feedback. This reactive layer prevents hazardous outputs before they occur by opposing dangerous forces, operating alongside the learned policy.

### Mechanism 3: State Augmentation with Binary Force Indicators
Discretized force/touch indicators are added to the observation space, allowing the policy to condition behavior on contact state. This provides interpretable state features without requiring precise calibration, as the sensors are difficult to calibrate accurately.

## Foundational Learning

- **Concept: DDPG (Deep Deterministic Policy Gradient)**
  - Why needed here: Base algorithm for continuous-action robotic control; maintains actor (policy) and critic (Q-function) networks trained off-policy.
  - Quick check question: Can you explain why DDPG uses target networks and experience replay for stability?

- **Concept: Hindsight Experience Replay (HER)**
  - Why needed here: Addresses sparse reward problem by relabeling failed trajectories with alternative goals, dramatically improving sample efficiency for manipulation tasks.
  - Quick check question: Given a failed trajectory that moved object to position B instead of goal A, how would HER relabel this for training?

- **Concept: Reward Shaping in RL**
  - Why needed here: The paper's core contribution adds auxiliary rewards; understanding when shaping preserves optimal policy vs. introduces bias is critical.
  - Quick check question: If a shaped reward causes the agent to avoid the goal because the shaping term is negative near it, what went wrong?

## Architecture Onboarding

- **Component map:**
  Sensors → [F/T Sensor, Touch Sensor, Camera (pose)] → Observation Builder → Adds I_ft, I_touch to state vector → Actor Network (π_θ) → Proposes action → Safety Modulator → a_t = π_θ + k_ft * O_ft + noise → Robot (UR5e) → Executes action → Reward Calculator → r_d + r_ft + r_touch → Replay Buffer + HER → Stores (s, a, r, s', g) tuples → Critic Network (Q_φ) + Actor → Updated via gradients

- **Critical path:** The safety gain `k_ft` and reward coefficients (`c_ft`, `c_touch`) are the most sensitive hyperparameters. Incorrect values cause either unsafe behavior or overly conservative policies that fail to complete tasks.

- **Design tradeoffs:**
  - Binaries vs. continuous force signals: Paper chooses binarized for calibration robustness; loses information but simplifies learning.
  - Reactive safety layer vs. learned safety: Paper uses both — reactive `k_ft` term plus reward shaping. Reactive is immediate but non-adaptive; learned is adaptive but requires training.
  - Sim-to-real transfer: Policy trained in MuJoCo; transferred directly without domain randomization. Force sensing compensates for some sim-to-real gap but visual pose estimation errors still cause failures.

- **Failure signatures:**
  - Oscillation/jitter: `k_ft` too high or force sensor noise amplified.
  - Policy never converges: Reward coefficients poorly balanced; auxiliary rewards dominate task reward.
  - Collisions still occur in real world: Thresholds calibrated in sim don't transfer; need real-world tuning.
  - Robot moves without pushing object: Visual pose estimation error; touch sensor not properly integrated into state estimation.

- **First 3 experiments:**
  1. Baseline comparison: Train DDPG+HER with `r_d` only (no force rewards, no safety gain) on simulated pushing task. Confirm low success rate and observe abnormal behaviors (oscillation, excessive force, object sliding/scrolling).
  2. Ablation on sensor modalities: Compare four reward conditions (force+touch, force-only, touch-only, neither) to isolate each sensor's contribution. Expect touch to help early training, force to improve final success rate.
  3. Sim-to-real transfer test: Deploy best policy to real UR5e with actual F/T and touch sensors. Measure success rate with and without safety gain `k_ft` active; verify that emergency stops decrease with force-based strategy.

## Open Questions the Paper Calls Out

### Open Question 1
How can tactile and force sensing be effectively combined to perform object pose estimation when visual methods fail due to occlusion? The authors note that visual detection often fails due to occlusion, and identifying how to "better combine tactile and force sensing for objects' pose estimation" is a "major research direction." The current experiments relied on visual ArUco markers for pose estimation, and the paper does not propose a method for inferring pose purely through haptic feedback.

### Open Question 2
Can this control strategy scale to more complex manipulation tasks using high-density tactile sensor arrays? The Limitations section notes that tactile sensors can be fabricated as arrays to realize "multimodal perception" and process "more complicated contact-rich tasks," suggesting the current single-sensor setup is a baseline. The current study utilizes discrete, threshold-based tactile data (boolean variables) for a specific pushing task; the efficacy of the algorithm with high-resolution, continuous tactile data remains untested.

### Open Question 3
Is the proposed safe control method robust enough for "more complex and dynamic situations" without manual parameter tuning? The Conclusion states the future work involves "finding better ways to combine force control and reinforcement learning" to face "more complex and dynamic situations," implying the current fixed safety gains and thresholds may not generalize well. The paper utilizes fixed safety constraints and gains; it is unclear if this approach adapts safely to varying dynamics or environments without re-calibration.

## Limitations
- Binary discretization of force/touch signals may lose critical information about gradual force buildup before collisions.
- Safety gain `k_ft` is tuned empirically without theoretical guarantees of stability across different robot dynamics or task geometries.
- Sim-to-real transfer only tested on one task (object pushing); generalization to tasks with different force profiles or object properties is unknown.

## Confidence
- **High confidence:** Force/torque rewards improve training efficiency and success rate in simulation (validated by ablation experiments).
- **Medium confidence:** Force-based corrective actions prevent real-world collisions (supported by real-robot tests, but limited sample size).
- **Low confidence:** Binary force indicators are sufficient for complex contact-rich tasks (only tested on simple pushing; related work uses continuous force feedback).

## Next Checks
1. **Force signal discretization test:** Compare binary vs. continuous force/torque inputs in the observation space to quantify information loss impact on safety and performance.
2. **Generalization across tasks:** Apply the same framework to a different contact-rich task (e.g., insertion or peg-in-hole) to test whether force/touch sensing generalizes beyond object pushing.
3. **Noise robustness analysis:** Introduce varying levels of force sensor noise in simulation and measure degradation in success rate and safety metrics to establish operational boundaries for the safety gain `k_ft`.