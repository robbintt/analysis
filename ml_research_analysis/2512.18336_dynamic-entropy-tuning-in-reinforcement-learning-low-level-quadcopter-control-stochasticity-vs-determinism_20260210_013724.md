---
ver: rpa2
title: 'Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control:
  Stochasticity vs Determinism'
arxiv_id: '2512.18336'
source_url: https://arxiv.org/abs/2512.18336
tags:
- entropy
- agent
- stochastic
- deterministic
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares stochastic and deterministic reinforcement
  learning algorithms for low-level quadcopter control, focusing on the impact of
  dynamic entropy tuning. Stochastic policies, such as those trained with Soft Actor-Critic
  (SAC), optimize a probability distribution over actions and incorporate entropy
  to encourage exploration.
---

# Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism

## Quick Facts
- arXiv ID: 2512.18336
- Source URL: https://arxiv.org/abs/2512.18336
- Reference count: 12
- Primary result: Stochastic policies with dynamic entropy tuning achieve higher rewards and better generalization than deterministic methods for low-level quadcopter control

## Executive Summary
This paper compares stochastic and deterministic reinforcement learning algorithms for low-level quadcopter control, focusing on the impact of dynamic entropy tuning. Stochastic policies, such as those trained with Soft Actor-Critic (SAC), optimize a probability distribution over actions and incorporate entropy to encourage exploration. Deterministic policies, like those trained with Twin Delayed Deep Deterministic Policy Gradient (TD3), select single actions per state and rely on external noise for exploration. The study finds that stochastic algorithms with dynamic entropy tuning achieve higher rewards, more stable learning, and better generalization to unseen states compared to deterministic methods. Dynamic entropy tuning enables efficient exploration without catastrophic forgetting, whereas deterministic algorithms suffer from performance decay when using external noise. Simulation results demonstrate that the stochastic approach with dynamic entropy tuning provides superior adaptability and control in both small and large training environments.

## Method Summary
The study implements SAC and TD3 on a gym-pybullet-drones CrazyFlie2.0 X-layout model for low-level quadcopter stabilization. The observation space consists of 12 dimensions including Euler angles, velocities, angular rates, and position errors. The action space is 4-dimensional motor RPM commands normalized to [-1,1]. SAC uses dynamic entropy tuning where the entropy coefficient α is optimized online to balance exploration and exploitation, while TD3 relies on external Gaussian noise for exploration. Both methods use twin critics with 2 hidden layers (400, 300 nodes) and are trained for 4 million steps with a replay buffer of 1 million transitions. The reward function combines a quadratic error term with a Gaussian shaping term to encourage stability near the target position.

## Key Results
- Stochastic SAC with dynamic entropy achieved significantly higher rewards than deterministic TD3 in both small (±0.5m) and large (±2m) training environments
- Deterministic TD3 showed performance decay after reaching peak reward when using external noise, demonstrating catastrophic forgetting
- SAC with dynamic entropy successfully stabilized the quadcopter at positions outside the trained environment bounds, while TD3 failed in these out-of-distribution scenarios

## Why This Works (Mechanism)

### Mechanism 1
Dynamic entropy tuning enables automatic exploration-exploitation balancing that adapts to training progress. The entropy coefficient (α) is optimized online via gradient descent to minimize the gap between current policy entropy H(π) and a target entropy H₀ (typically set to -dim(action space)). When the policy becomes too deterministic, α increases to encourage exploration; when entropy is sufficient, α decreases to favor exploitation. This mechanism assumes the relationship between entropy coefficient and actual policy stochasticity is differentiable and approximately monotonic during training.

### Mechanism 2
Stochastic policies with intrinsic entropy avoid catastrophic forgetting that plagues deterministic policies with external action noise. Deterministic TD3 adds external Gaussian noise for exploration; this is "wasteful" because it re-explores already-visited action regions and destabilizes learned policies. SAC's entropy term is coupled to the policy optimization objective, so exploration is goal-directed rather than random perturbation. This assumes catastrophic forgetting in deterministic policies is caused primarily by external noise structure, not fundamental algorithm limitations.

### Mechanism 3
Maximum entropy training produces policies that generalize to out-of-distribution initial conditions. By jointly maximizing reward and entropy, the policy learns a broader region of the state-action space rather than converging to a single trajectory. This provides "coverage" that transfers to unseen starting positions. This assumes generalization gains are not simply due to SAC's higher sample efficiency providing more environment interactions.

## Foundational Learning

- **Entropy in RL policies**: Measures action distribution uncertainty; essential for understanding why entropy tuning affects exploration-exploitation balance. Quick check: Would a deterministic policy have high or low entropy? (Answer: Low/zero)

- **Actor-Critic architecture**: Both SAC and TD3 use this framework where the actor outputs actions and the critic evaluates state-action values. Quick check: Which network outputs actions and which outputs value estimates?

- **Exploration vs. exploitation tradeoff**: The fundamental dilemma where agents must balance trying new actions (exploration) with using known good actions (exploitation). Quick check: What happens if an agent only exploits? Only explores?

## Architecture Onboarding

- **Component map**: State observation → Actor forward pass → Action sampling (with entropy) → Environment step → Reward + next state → Buffer store → Sample batch → Critic update → Actor update (with entropy bonus) → α update

- **Critical path**: The sequence from state observation through policy output to environment interaction and learning updates

- **Design tradeoffs**: Static vs. dynamic entropy (simpler vs. adaptive), training environment size (larger bounds test exploration but may slow convergence), external noise addition (SAC with noise caused catastrophic forgetting)

- **Failure signatures**: Reward curve peaks then decays (catastrophic forgetting), α drops to near-zero early (target entropy may be too low), agent crashes on extreme initial positions (insufficient exploration)

- **First 3 experiments**: 1) Reproduce small-environment comparison for 1M steps to validate reward curve shapes, 2) Ablate entropy mechanism by running SAC with static α vs dynamic α, 3) Test generalization by evaluating both policies on a grid of initial positions spanning inside and outside training bounds

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text. However, based on the methodology and results, several implicit questions arise: How does dynamic entropy tuning affect sample efficiency in high-level navigation tasks compared to low-level control? Does the dynamic entropy approach maintain superiority when transferred from simulation to physical hardware? Is the heuristic of setting target entropy to negative action space dimension optimal for all flight phases?

## Limitations
- Limited direct corpus validation of the dynamic entropy tuning mechanism claims
- Catastrophic forgetting mechanism relies on inferred evidence from reward curves without ablation studies
- Generalization claims based on a single out-of-distribution test case without systematic statistical analysis
- Only two exploration noise levels tested for deterministic methods without exploring full noise parameter space

## Confidence
- High confidence in SAC implementation and reward function specifications
- Medium confidence in mechanism claims linking dynamic entropy to exploration-exploitation balance and catastrophic forgetting prevention
- Low confidence in generalization claims without systematic out-of-distribution testing across multiple task variations

## Next Checks
1. Run SAC with static entropy (fixed α=0.2) vs dynamic entropy for 4M steps to isolate entropy tuning effects
2. Evaluate both policies on a 5x5 grid of initial positions spanning ±2m in x and y to quantify generalization statistically
3. Test TD3 with progressively smaller external noise (σ=0.1, 0.05, 0.01) to determine if catastrophic forgetting is noise-dependent