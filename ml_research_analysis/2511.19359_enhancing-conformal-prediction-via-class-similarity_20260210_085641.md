---
ver: rpa2
title: Enhancing Conformal Prediction via Class Similarity
arxiv_id: '2511.19359'
source_url: https://arxiv.org/abs/2511.19359
tags:
- prediction
- size
- class
- average
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a class-similarity-based regularization method
  for conformal prediction that improves both the semantic coherence and efficiency
  of prediction sets. The method augments the conformal score function with a penalty
  term that discourages including semantically dissimilar classes in prediction sets.
---

# Enhancing Conformal Prediction via Class Similarity

## Quick Facts
- arXiv ID: 2511.19359
- Source URL: https://arxiv.org/abs/2511.19359
- Authors: Ariel Fargion; Lahav Dabah; Tom Tirer
- Reference count: 40
- Primary result: Proposes class-similarity-based regularization that reduces prediction set size by up to 33% while maintaining coverage guarantees

## Executive Summary
This paper introduces a class-similarity-based regularization method for conformal prediction that improves both semantic coherence and efficiency of prediction sets. The approach augments standard conformal scores with a penalty term that discourages including semantically dissimilar classes in prediction sets. Theoretically, the authors prove that this method reduces the expected number of unique semantic groups in prediction sets and surprisingly also reduces average prediction set size under common conditions. They extend this to a model-specific variant that automatically learns class similarity from the classifier's embedding space, eliminating the need for human-defined semantic partitions. Extensive experiments across CIFAR-100, Living-17, and Mini-ImageNet demonstrate consistent improvements over standard conformal methods.

## Method Summary
The method introduces two variants: Model-Agnostic Class Similarity (MA-CS) and Model-Specific Class Similarity (MS-CS). MA-CS adds a binary penalty term d(y, ŷ(x)) = I{g(y)≠g(ŷ(x))} to any conformal score function, where g(y) is the semantic group of class y. MS-CS automatically learns class similarity by computing centered cosine similarity between class mean embeddings in the model's feature space. Both variants tune a regularization parameter λ using a held-out validation split. The key insight is that penalizing semantically dissimilar classes can simultaneously improve group coherence and reduce set size when the classifier has high in-group accuracy.

## Key Results
- Achieves up to 33% reduction in average prediction set size compared to standard conformal methods
- Reduces the average number of superclasses in prediction sets by up to 25%
- Maintains marginal coverage guarantees at target levels (α ∈ {0.05, 0.1})
- MS-CS variant consistently outperforms MA-CS by leveraging learned class relationships
- Validated across multiple datasets (CIFAR-100, Living-17, Mini-ImageNet) and CP methods (LAC, RAPS, SAPS)

## Why This Works (Mechanism)

### Mechanism 1: Binary Out-of-Group Penalty
Adding a binary penalty term to any conformal score function reduces the expected number of unique groups in prediction sets and, under common conditions, also reduces average set size. The augmented score s_λ(x,y) = s(x,y) + λ·d(y, ŷ(x)) penalizes classes from different semantic groups than the predicted class by adding λ to their scores. Since calibration scores shift by ≤λ (Lemma 4.1), out-of-group classes face a stricter threshold, reducing their inclusion. The penalty creates asymmetric filtering: in-group classes pass at threshold q̂, out-of-group classes must pass at q̂−λ. Core assumption: A valid partition of classes into semantic groups exists, and modern classifiers have reasonably high in-group accuracy (p₀ not small, p₁n₀ ≪ p₀n₁ per Theorem 4.5 discussion).

### Mechanism 2: Model-Specific Soft Similarity from Embeddings
Using model-perceived class similarity from learned embeddings eliminates the need for human semantic partitions and can further reduce prediction set size. Compute class mean embeddings h_c from training data, center by global mean h_G, then define similarity M_{c,c'} = ⟨h_c−h_G, h_c'−h_G⟩ / (‖h_c−h_G‖‖h_c'−h_G‖). The penalty d_MS(y,y') = 1−M_{y,y'} is now continuous in [0,2], allowing finer-grained penalization. Classes the model confuses get lower penalties; clearly separated classes get higher penalties. Core assumption: Neural collapse phenomenon holds—within-class samples concentrate around class means in feature space, and inter-class mean relationships generalize to test data.

### Mechanism 3: Quantile Shift Dynamics
The reduction in average set size emerges from the asymmetric relationship between calibration quantile shift and score distribution differences between in-group and out-of-group classes. The penalty shifts the calibration scores s_λ(x_i,y_i) = s(x_i,y_i) + λ·I{g(y_i)≠g(ŷ(x_i))}. By Lemma 4.1, q̂ ≤ q̂_λ ≤ q̂+λ. When p₁n₀ ≪ p₀n₁ (typical case: out-of-group errors are rare, in-group has few classes), the derivative analysis shows the net effect at small λ is removal of out-of-group classes exceeding the gain from slightly higher threshold including more in-group classes. Core assumption: Assumptions 1-3 from Section 4 hold—CDFs F₀, F₁ and "size-biased" quasi-CDFs are absolutely continuous with well-defined densities at quantile q₀.

## Foundational Learning

- **Conformal Prediction Score Functions**: Why needed: The method augments existing score functions (LAC, RAPS, SAPS) with a penalty term; understanding base scores is prerequisite to grasping how the penalty modifies behavior. Quick check: Given softmax output [0.7, 0.2, 0.1] for classes [A, B, C] and true label B, what is the LAC score s(x,B)?
- **Exchangeability and Coverage Guarantees**: Why needed: The paper's central contribution preserves CP's coverage guarantee; understanding why any score function maintains this guarantee under exchangeability is essential. Quick check: Why does adding a penalty term that depends on ŷ(x) (which itself depends on x) not violate the exchangeability assumption?
- **Neural Collapse and Feature Space Geometry**: Why needed: The MS-CS variant relies on class mean embeddings forming meaningful clusters; neural collapse theory explains why this structure emerges in well-trained networks. Quick check: In a classifier exhibiting neural collapse, what geometric relationship do you expect between class means and the global mean?

## Architecture Onboarding

- **Component map**: Base CP module -> Penalty module -> Hyperparameter tuner (for λ) [MS-CS only: Similarity matrix builder]
- **Critical path**: 
  1. Pre-computation (MS-CS): Build similarity matrix M from training embeddings—O(C·n_train·p) for mean computation, O(C²·p) for similarity matrix
  2. Calibration phase: Compute s_λ(x_i,y_i) for all calibration samples, find (1−α) quantile q̂_λ
  3. Deployment: For test x, compute ŷ(x), then s_λ(x,y) for all y, return {y : s_λ(x,y) ≤ q̂_λ}
- **Design tradeoffs**: 
  - MA-CS vs MS-CS: MA-CS requires known semantic groups but is interpretable; MS-CS requires training data access but works on any dataset
  - λ selection: Split calibration data (50/50) for λ tuning vs use full calibration for q̂ and risk overfitting λ—paper uses 50/50 split
  - Penalty granularity: Binary penalty is simpler but coarse; soft penalty from M is flexible but requires storage of C×C matrix
- **Failure signatures**: 
  - Coverage violation: Should never happen under exchangeability; check calibration/test data are i.i.d.
  - Increased set size: λ too large—examine Figure 2 behavior, λ should be in the decreasing region before minimum
  - MS-CS worse than MA-CS: Model poorly trained or embeddings don't cluster by class—visualize class means in 2D (t-SNE)
  - Empty prediction sets: Extremely large λ can push all scores above threshold—check that at least ŷ(x) has penalty 0
- **First 3 experiments**:
  1. Baseline reproduction: Run standard LAC on CIFAR-100 with ResNet50, verify average set size ~3.68 at α=0.05 (Table 1). This validates your CP pipeline.
  2. MA-CS λ sweep: Implement binary penalty with CIFAR-100's 20 superclasses, sweep λ ∈ [0, 0.5], plot both set size and #superclasses. Verify U-curve for size and monotonic decrease for superclasses.
  3. MS-CS similarity visualization: Compute class mean embeddings for CIFAR-100, visualize the 100×100 similarity matrix. Compare to ground-truth superclass matrix (Figure 5) to verify model captures semantic structure.

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal strategy for constructing the class similarity matrix M in the model-specific variant, and how does the choice of similarity metric affect prediction set efficiency? The authors propose cosine similarity of centered class means in feature space but do not compare against alternative similarity metrics or provide theoretical guidance on optimal construction. What evidence would resolve it: Systematic comparison of different similarity matrix construction strategies across diverse datasets and architectures, potentially combined with theoretical analysis relating similarity matrix properties to efficiency gains.

### Open Question 2
Under what conditions do the density ratio terms a = f̃₀(q₀)f₁(q₀) and b = f̃₁(q₀)f₀(q₀) remain approximately equal, and when does this approximation break down? The theoretical analysis relies on the simplifying assumption that a ≈ b to derive the intuitive condition p₁n₀ ≪ p₀n₁, but the validity of this assumption across real-world data distributions remains unproven. What evidence would resolve it: Empirical estimation of a and b across diverse datasets and classifiers to characterize when a ≈ b holds, combined with theoretical analysis identifying sufficient conditions for this approximation.

### Open Question 3
How does the proposed penalty affect class-conditional coverage and other stronger coverage guarantees beyond marginal coverage? While marginal coverage guarantees transfer to the penalized score function, the penalty's selective effect on out-of-group predictions could potentially create systematic coverage gaps for specific classes without violating marginal guarantees. What evidence would resolve it: Formal analysis of the penalty's effect on class-conditional coverage, or empirical evaluation of conditional coverage across subgroups defined by both class membership and semantic group membership.

### Open Question 4
Can the regularization parameter λ be selected theoretically rather than empirically, and is there an optimal λ that balances efficiency gains against potential coverage distortions? The empirical results show λ affects both efficiency and group coherence in non-monotonic ways (Figure 2), suggesting complex trade-offs that are not captured by current theory. What evidence would resolve it: Theoretical analysis characterizing the optimal λ as a function of class distribution properties (p₀, p₁, n₀, n₁) and the density ratio terms, or development of principled selection criteria that do not require held-out validation data.

## Limitations

- λ selection sensitivity: Method's effectiveness heavily depends on selecting appropriate λ value, with narrow optimal range and performance degradation when λ is too large
- Assumption dependencies: Theoretical guarantee of set size reduction relies on specific conditions (p₁n₀ ≪ p₀n₁, continuous CDFs) that may not hold for imbalanced semantic groups or poor classifiers
- MS-CS embedding quality: Model-specific variant assumes neural collapse structure in feature space; performance could diminish on datasets where embeddings don't exhibit expected structure

## Confidence

- **High confidence**: Coverage guarantee preservation (follows from conformal prediction theory under exchangeability), monotonic reduction in average #superclasses (Proposition 4.2), basic experimental methodology
- **Medium confidence**: Set size reduction claims (theoretical conditions are specific and may not generalize), MS-CS variant performance (limited ablation studies on embedding quality impact)
- **Low confidence**: Optimal λ selection strategy robustness, performance on datasets without clear semantic partitions, generalization to non-image domains

## Next Checks

1. **λ sensitivity analysis**: Systematically vary λ across multiple orders of magnitude on CIFAR-100, measuring both set size and coverage. Identify the break point where set size starts increasing and quantify performance degradation from suboptimal λ selection.

2. **Cross-dataset generalization**: Apply the method to datasets with different semantic structures (e.g., CIFAR-10 with hierarchical relationships, or a dataset with overlapping semantic groups). Evaluate whether the theoretical conditions for set size reduction hold and whether performance gains persist.

3. **Embedding quality ablation**: For MS-CS, compare performance when using: (a) penultimate layer features, (b) features from an earlier layer, (c) randomly initialized embeddings. This would quantify how much the method relies on learned semantic structure versus arbitrary embedding geometry.