---
ver: rpa2
title: 'PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition
  in Low-resource Pashto Language'
arxiv_id: '2505.10055'
source_url: https://arxiv.org/abs/2505.10055
tags:
- text
- pashto
- dataset
- performance
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PsOCR, a large-scale synthetic dataset for
  evaluating Large Multimodal Models (LMMs) on Optical Character Recognition (OCR)
  in the low-resource Pashto language. To address the lack of annotated Pashto text
  data, the authors generated one million synthetic images annotated at word, line,
  and document levels, covering 1,000 font families and diverse layouts.
---

# PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language

## Quick Facts
- arXiv ID: 2505.10055
- Source URL: https://arxiv.org/abs/2505.10055
- Reference count: 40
- Primary result: PsOCR dataset introduced; Gemini best (CER: 0.10, WER: 0.31), Qwen-7B top open-source (CER: 0.34, WER: 0.73)

## Executive Summary
This paper introduces PsOCR, a large-scale synthetic dataset for evaluating Large Multimodal Models (LMMs) on Optical Character Recognition (OCR) in the low-resource Pashto language. To address the lack of annotated Pashto text data, the authors generated one million synthetic images annotated at word, line, and document levels, covering 1,000 font families and diverse layouts. A benchmark subset of 10,000 images was used to assess seven open-source and four closed-source LMMs under zero-shot settings. Results show that Gemini achieved the best performance (CER: 0.10, WER: 0.31), while Qwen-7B was the top-performing open-source model (CER: 0.34, WER: 0.73). The study highlights the effectiveness of current LMMs for OCR in cursive, low-resource languages and provides a foundation for future research in similar scripts. The PsOCR dataset is publicly available.

## Method Summary
The PsOCR dataset was created by generating one million synthetic Pashto text images with word, line, and document-level bounding box annotations. The pipeline used a Pashto text corpus, HTML/CSS rendering with randomized styling (1,000 fonts, 11-30px sizes, 66K color combinations), and Selenium screenshot capture to produce PNG images. A benchmark subset of 10,000 images was evaluated under zero-shot settings using seven open-source and four closed-source LMMs, with custom prompts to extract raw text without translation or commentary. OCR performance was measured using Character Error Rate (CER) and Word Error Rate (WER).

## Key Results
- Gemini achieved the best overall performance (CER: 0.10, WER: 0.31)
- Qwen-7B was the top open-source model (CER: 0.34, WER: 0.73)
- All models showed higher WER than CER, indicating word-boundary fragility in Pashto's cursive script
- Synthetic data generation proved effective for evaluating LMMs on low-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Controlled Synthetic Data Generation for Low-Resource Scripts
- Claim: Synthetic OCR datasets can substitute for scarce real-world annotated data when systematic variability is introduced across fonts, colors, layouts, and sizes.
- Mechanism: Text corpus → HTML rendering with randomized CSS styling → Selenium screenshot capture → JSON annotations with bounding boxes. The pipeline introduces controlled diversity (1,000 fonts, 11-30px sizes, 66K color combinations) while preserving ground-truth alignment.
- Core assumption: Models trained or evaluated on synthetic data with sufficient font and layout variation will transfer to real-world documents.
- Evidence anchors:
  - [abstract]: "PsOCR, consisting of one million images...covering variations across 1,000 unique font families, colors, image sizes, and layouts"
  - [section 3.3]: "Each HTML page was captured as a PNG image screenshot, resulting in one million images of varying dimensions, aspect ratios, and visual styles"
  - [corpus]: QARI-OCR and synthocr-gen papers confirm synthetic data generation is an emerging pattern for Perso-Arabic scripts, though transfer efficacy remains understudied.
- Break condition: Performance degrades significantly on handwritten text, textured backgrounds, or perspective distortions—none of which are included in PsOCR (acknowledged in Section 6).

### Mechanism 2: Multi-Granularity Annotations for Architecture Flexibility
- Claim: Providing bounding box annotations at page, line, and token levels enables diverse model architectures (CNN-based, transformer-based, detection-focused) to use the same dataset.
- Mechanism: Each image has three annotation layers with (X, Y, width, height) bounding boxes. Token-level captures space-separated chunks; line-level captures individual text lines; page-level captures full documents.
- Core assumption: Different OCR architectures benefit from different annotation granularities without requiring separate datasets.
- Evidence anchors:
  - [abstract]: "annotated with bounding boxes at word, line, and document levels, suitable for training and evaluating models based on different architectures, including CNNs and Transformers"
  - [section 3.4.1]: "This structured and rich annotation schema significantly enhances the dataset's applicability across various OCR scenarios and training methodologies"
  - [corpus]: Weak—neighbor papers do not systematically compare multi-granularity vs. single-granularity annotations.
- Break condition: Architecture-specific pre-processing requirements (e.g., fixed input dimensions) may still require annotation transformation or resampling.

### Mechanism 3: Zero-Shot LMM Evaluation via Prompt Engineering
- Claim: Large multimodal models can perform OCR on unseen scripts through zero-shot prompting, but output quality is highly sensitive to prompt design and post-processing.
- Mechanism: Custom prompts per model → image input → raw text extraction → manual/semi-automatic cleaning → CER/WER evaluation. Prompts explicitly instruct against translation, description, or commentary.
- Core assumption: Zero-shot performance reflects inherent multilingual vision-language capabilities rather than task-specific training.
- Evidence anchors:
  - [abstract]: "Gemini achieves the best overall performance, with a CER of 0.10 and WER of 0.31"
  - [section 4.4]: "GPT-4o occasionally generated erroneous or incomplete responses due to content flagged as inappropriate by its internal safety filters...accounting for approximately 4% of the total benchmark"
  - [corpus]: LogicOCR and E-ARMOR confirm LMM OCR evaluation is an active area, but prompt sensitivity for low-resource scripts is underreported.
- Break condition: Models revert to translation, summarization, or refusal when prompts are ambiguous—Llama consistently produced contaminated outputs despite prompt refinement.

## Foundational Learning

- **Concept: Character Error Rate (CER) vs. Word Error Rate (WER)**
  - Why needed here: These are the primary metrics for OCR evaluation. CER measures character-level edit distance; WER measures word-level. Pashto's cursive script and ambiguous word boundaries cause WER to exceed CER consistently across all models.
  - Quick check question: If a model outputs "helloo" for ground-truth "hello", what are the CER and WER?

- **Concept: Zero-Shot Evaluation**
  - Why needed here: All models were evaluated without fine-tuning on Pashto data. This tests generalization but does not represent peak performance achievable with adaptation.
  - Quick check question: What is the difference between zero-shot, few-shot, and fine-tuned evaluation?

- **Concept: Perso-Arabic Script Challenges**
  - Why needed here: Pashto is RTL, cursive, uses ligatures, and has context-dependent character forms (initial, medial, final, isolated). These properties make word boundary detection and character segmentation harder than Latin scripts.
  - Quick check question: Why might a model achieve low CER but high WER on a cursive script?

## Architecture Onboarding

- **Component map:**
  Text Corpus → Preprocessing (cleaning, chunking) → HTML Generation → CSS Styling (fonts, colors, layouts) → Selenium Rendering → PNG Images → Annotation Generator (bbox at 3 levels) → JSON Output

- **Critical path:**
  1. Font selection and filtering (proprietary/duplicate removal)
  2. Color contrast validation (luminance ratio ≥ 6)
  3. Prompt engineering per model (preventing translation/commentary)
  4. Post-processing of model outputs (handling refusals, malformed responses)

- **Design tradeoffs:**
  - Synthetic vs. real data: Synthetic enables scale and perfect annotations but lacks real-world noise (skew, handwriting, textures)
  - Controlled noise vs. absolute cleanliness: Authors intentionally preserved minor noise for robustness training
  - API vs. local inference: Proprietary models via API; open-source locally—introduces variability in latency, cost, and reproducibility

- **Failure signatures:**
  - Llama: Consistently includes commentary despite explicit instructions
  - GPT-4o: ~4% refusals due to safety filters
  - Florence: Zero similarity scores on text metrics (possible tokenization or output formatting issue)
  - All models: WER > CER indicates word-boundary fragility

- **First 3 experiments:**
  1. **Baseline replication:** Run Qwen-7B on the 10K benchmark with the provided prompt; verify CER ~0.34, WER ~0.73
  2. **Font sensitivity analysis:** Subset evaluation on top-10 vs. bottom-10 fonts (Table 6); quantify CER delta
  3. **Prompt ablation:** Test minimal prompt ("Extract text from image") vs. engineered prompt; measure output contamination rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LMMs effectively generalize to handwritten Pashto text, and does fine-tuning on the PsOCR synthetic dataset improve performance on real-world handwritten samples?
- Basis in paper: [explicit] The Conclusion explicitly states the limitation that the dataset contains only computer-generated text and announces ongoing work on a large-scale handwritten Pashto OCR dataset.
- Why unresolved: The current benchmark evaluates zero-shot capabilities exclusively on synthetic images, leaving the domain gap between synthetic print and natural handwriting unexplored.
- What evidence would resolve it: A comparative evaluation of LMMs on the proposed handwritten dataset, reporting Character Error Rate (CER) and Word Error Rate (WER) for both zero-shot and fine-tuned scenarios.

### Open Question 2
- Question: How does the introduction of complex background textures and diverse lighting conditions impact the robustness of LMMs in extracting Pashto text?
- Basis in paper: [explicit] The Conclusion identifies the lack of background patterns and specific lighting variations as a limitation and proposes developing a more realistic dataset version to address this.
- Why unresolved: The current evaluation uses images with plain backgrounds and controlled luminance ratios (≥6), which may overestimate model performance in uncontrolled, real-world environments.
- What evidence would resolve it: Performance metrics (CER, WER) from the proposed "realistic" dataset variant, specifically analyzing degradation in scores as visual noise and lighting complexity increase.

### Open Question 3
- Question: To what extent can fine-tuning on the 1-million-image PsOCR training set recover the performance deficits observed in lower-performing open-source models like Llama and Florence?
- Basis in paper: [inferred] The authors release a massive training set but limit their experimental analysis to zero-shot settings, leaving the utility of the dataset for supervised learning unverified.
- Why unresolved: While Qwen-7B performed reasonably well zero-shot, models like Llama (CER 1.31) performed poorly; it remains unknown if the provided training data can optimize these architectures for Pashto.
- What evidence would resolve it: Experimental results comparing zero-shot baselines against models fine-tuned on the PsOCR training split, specifically tracking the reduction in error rates for the struggling models.

## Limitations
- Synthetic data may not capture real-world document complexities like handwriting, perspective distortions, or textured backgrounds
- Zero-shot evaluation does not represent peak performance achievable through fine-tuning or few-shot adaptation
- Model-specific prompt engineering details are partially documented, affecting reproducibility

## Confidence

- **High Confidence**: The PsOCR dataset construction methodology (synthetic generation pipeline, annotation schema) and benchmark results for the 10K subset. The dataset is publicly available and the evaluation pipeline is reproducible.
- **Medium Confidence**: The relative performance ranking of LMMs on Pashto OCR (Gemini > GPT-4o > Claude > Grok > Florence > Qwen-7B > Llama). While results are consistent across metrics, prompt sensitivity and model-specific behaviors introduce variability.
- **Low Confidence**: Claims about LMMs being "capable of recognizing and extracting text" from Pashto documents in general, as this extrapolates from synthetic data performance to real-world applicability without validation on authentic documents.

## Next Checks

1. **Real Document Validation**: Evaluate the top-performing models (Gemini, GPT-4o, Qwen-7B) on a small set of real Pashto documents (scanned pages, receipts, signage) to measure synthetic-to-real transfer gap.

2. **Fine-tuning Benchmark**: Fine-tune the best open-source model (Qwen-7B) on a subset of PsOCR (e.g., 100K images) and re-evaluate on the 10K benchmark to quantify improvement over zero-shot performance.

3. **Prompt Sensitivity Analysis**: Systematically vary prompt phrasing (minimal vs. engineered, with/without explicit prohibitions on translation/commentary) and measure output contamination rates and OCR accuracy for each model.