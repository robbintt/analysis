---
ver: rpa2
title: Can Cross Encoders Produce Useful Sentence Embeddings?
arxiv_id: '2502.03552'
source_url: https://arxiv.org/abs/2502.03552
tags:
- retrieval
- layer
- used
- sentence
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The prevailing approach uses cross encoders for reranking and dual
  encoders for initial retrieval, as cross encoders are too slow for initial search.
  The paper shows that embeddings from earlier layers of cross encoders can be used
  for search, often outperforming dual encoder embeddings.
---

# Can Cross Encoders Produce Useful Sentence Embeddings?

## Quick Facts
- arXiv ID: 2502.03552
- Source URL: https://arxiv.org/abs/2502.03552
- Reference count: 9
- Primary result: Early-layer cross-encoder embeddings outperform early-layer dual-encoder embeddings for retrieval

## Executive Summary
The prevailing approach uses cross encoders for reranking and dual encoders for initial retrieval, as cross encoders are too slow for initial search. This paper shows that embeddings from earlier layers of cross encoders can be used for search, often outperforming dual encoder embeddings. This enables knowledge distillation from cross encoders to dual encoders, creating faster models with minimal accuracy loss. Experiments show that embeddings from earlier cross encoder layers contain a stronger signal for retrieval than earlier dual encoder layers, and a 2-layer dual encoder infused with cross encoder weights achieves 5.15x speedup while maintaining similar accuracy.

## Method Summary
The method extracts sentence embeddings from early layers of cross-encoders by pairing each sentence with itself and mean-pooling token embeddings (excluding padding/SEP tokens). These layer-wise embeddings are evaluated for retrieval performance. To create efficient dual encoders, the approach copies the embedding layer and first encoder layer from the cross-encoder into a 2-layer dual encoder, randomly initializes the second layer, and trains with standard contrastive loss using only BM25 hard negatives. This distillation process requires no complex additional procedures.

## Key Results
- Early-layer cross-encoder embeddings achieve ≥80% of final dual-encoder Hits@10 on 5/10 datasets
- A 2-layer dual encoder infused with cross-encoder weights achieves 5.15x speedup while maintaining similar accuracy
- Cross-encoder training appears to extract IR-relevant information in earlier layers than dual-encoder training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Earlier layers of cross encoders contain stronger IR-relevant signals than earlier layers of dual encoders, despite shared pretraining origins.
- Mechanism: CE training appears to extract information relevant for IR in earlier layers than DE training. The authors hypothesize that later CE layers compute differences between sentence pairs, leaving earlier layers to encode individual sentence representations useful for retrieval.
- Core assumption: The benefit stems from CE training dynamics, not from architectural differences alone.
- Evidence anchors: [abstract] "embeddings from earlier layers of CEs can in fact be used within an information retrieval pipeline"; [section 3.3] "CE embeddings from lower layers show surprisingly good performance, on many datasets"

### Mechanism 2
- Claim: Self-pairing (passing a sentence paired with itself) enables extraction of single-sentence embeddings from CE architectures.
- Mechanism: Since CEs require sentence pairs at input, the authors pair each sentence with itself, then mean-pool over tokens (excluding padding/SEP tokens) to extract layer-wise embeddings.
- Core assumption: Self-pairing produces embeddings representative of the sentence's semantic content without interference from cross-sentence comparison dynamics.
- Evidence anchors: [section 3.1] "to embed a single sentence, we simply pair it with itself"; [section 4.3] CE embedding layer achieved ≥80% of final DE Hits@10 on 5/10 datasets

### Mechanism 3
- Claim: CE early-layer weights can be transferred to initialize a shallow DE, achieving comparable accuracy with 5x inference speedup.
- Mechanism: Copy the embedding layer and encoder layer 0 from CE to a 2-layer DE; randomly initialize layer 1; train with standard DE loss using only BM25 hard negatives (no additional CE distillation needed).
- Core assumption: The CE's early-layer representations encode transferable semantic structure that DE training can refine without extensive hard-negative mining.
- Evidence anchors: [section 3.2] "Weights from the embedding layer in BERT... and layer 0 from the BERT encoder are both copied into a DE model from the CE"; [section 4.4] "DE-2 CE is only 0.99% worse than the baseline DE across 12 datasets... speedup for inference is on average 5.15x"

## Foundational Learning

- Concept: **Cross Encoder vs. Dual Encoder architectures**
  - Why needed here: The entire paper assumes understanding that CEs process sentence pairs jointly (allowing cross-attention), while DEs encode sentences independently and compare via vector similarity.
  - Quick check question: Given a corpus of 1M documents and a query, which architecture requires O(N) forward passes at inference?

- Concept: **Knowledge distillation in IR**
  - Why needed here: The DE-2-CE model is a form of distillation; understanding how teacher-student relationships work clarifies why early-layer weight transfer differs from logit-based distillation.
  - Quick check question: If you copy weights directly, are you doing distillation or transfer learning? What's the practical difference?

- Concept: **Mean pooling over token embeddings**
  - Why needed here: The extraction method (Figure 1) relies on mean pooling to produce sentence-level representations from token-level hidden states.
  - Quick check question: Why might mean pooling underperform on sentences with many padding tokens or dominant function words?

## Architecture Onboarding

- Component map: Cross-encoder -> Self-pairing -> Layer-wise embeddings -> Mean pooling -> Cosine similarity for retrieval
- Critical path: 1. Select CE/DE pair with same pretrained backbone 2. Extract layer-wise embeddings via self-pairing 3. Evaluate retrieval metrics at each layer 4. Copy early-layer weights to shallow DE 5. Train DE with standard sentence-transformer pipeline 6. Benchmark against baseline DE + reranking
- Design tradeoffs:
  - Layer count: 2-layer DE maximizes speedup but may underperform on complex tasks; 12-layer DE-12-CE showed minimal gain over DE-2-CE
  - Hard negative strategy: Using only BM25 negatives simplifies training but may limit accuracy vs. multi-source distillation
  - Model pair selection: Must share pretrained backbone; not all CE/DE pairs have compatible architectures
- Failure signatures:
  - DE-2-Rand underperforms DE-2-CE significantly → CE weight initialization is critical, not just architecture
  - Late-layer CE embeddings degrade → confirms they encode pair-differences, not individual semantics
  - Large accuracy gaps on specific datasets (e.g., fiqa, eli5) → may require task-specific tuning or deeper models
- First 3 experiments:
  1. Replicate layer-wise extraction on msmarco-MiniLM pair; plot Hits@10 by layer for CE vs. DE to validate early-layer CE advantage
  2. Train DE-2-CE with CE weight initialization; compare to DE-2-Rand baseline on 3 held-out datasets
  3. Measure inference latency (ms/query) for baseline DE-12 vs. DE-2-CE on identical hardware; verify ~5x speedup claim

## Open Questions the Paper Calls Out

- Question: Is there an optimal architecture or layer configuration for infusing cross-encoder (CE) knowledge into dual encoders (DEs) beyond the specific 2-layer setup tested?
  - Basis in paper: [explicit] The authors state, "There is a huge range of architectures for infusing a DE with CE knowledge. We explored a small fraction of that space, and not exhaustively."
  - Why unresolved: The paper only evaluated a specific 2-layer DE initialization and a 12-layer comparison, leaving the broader architectural search space unexplored.
  - What evidence would resolve it: A systematic ablation study varying the number of layers transferred and the initialization strategies across multiple benchmarks.

- Question: Does the efficacy of CE-derived embeddings generalize to larger, state-of-the-art model architectures where fine-tuning code may be unavailable?
  - Basis in paper: [explicit] The limitations section notes, "Our work covers limited sets of models... This is especially the case for the infusion of CE knowledge into DEs, since the fine-tuning code for mixed bread (mxbai) models was not available."
  - Why unresolved: The experiments focused on MiniLM models, and the authors could not verify if the early-layer signal strength holds for larger architectures like mxbai.
  - What evidence would resolve it: Replicating the knowledge infusion method on the mxbai model pair or similar large-scale architectures.

- Question: Are the early-layer embedding capabilities of cross-encoders applicable to non-English languages?
  - Basis in paper: [explicit] The authors acknowledge, "All of our work was done on English text, and we did not study how it applies in other languages."
  - Why unresolved: It is unclear if the early-layer "strong signal" for information retrieval is a language-agnostic property of BERT-based CEs or specific to English.
  - What evidence would resolve it: Applying the layer-wise analysis and distillation techniques to multilingual datasets and models.

## Limitations

- The approach focuses exclusively on models sharing the same pretrained backbone (MiniLM-L-12), leaving unclear whether the observed layer-wise advantages generalize to other architectures or training regimens.
- The self-pairing mechanism for extracting single-sentence embeddings from CEs lacks direct corpus validation for potential biases in specific retrieval tasks.
- The approach's effectiveness on non-MSMARCO-style retrieval tasks (e.g., multi-hop question answering, passage ranking with heterogeneous sources) remains untested.

## Confidence

- **High Confidence**: The empirical observation that early-layer CE embeddings outperform early-layer DE embeddings on retrieval tasks (5/10 datasets with ≥80% of final DE performance) is well-supported by the experimental data and straightforward to reproduce.
- **Medium Confidence**: The mechanism explaining why CE training extracts IR-relevant signals earlier than DE training is plausible but not definitively proven, as the experiments compare models with different training objectives rather than isolating architectural effects.
- **Medium Confidence**: The claim that CE early-layer weights can be transferred to create efficient DEs with minimal accuracy loss is supported by the 5.15x speedup result, though the approach's generalizability beyond MiniLM pairs remains uncertain.

## Next Checks

1. Apply the layer-wise extraction and DE-2-CE training approach to a different CE/DE pair (e.g., BERT-based models) to verify whether the early-layer advantage persists across architectures.

2. Systematically evaluate whether self-pairing introduces retrieval biases by testing on tasks where self-similarity artifacts would be particularly problematic (e.g., duplicate detection vs. semantic similarity ranking).

3. Train DE-2-CE models with 4, 6, and 8 layers to determine whether the 2-layer design is optimal or whether modest increases in depth yield better accuracy-speed tradeoffs on complex retrieval tasks.