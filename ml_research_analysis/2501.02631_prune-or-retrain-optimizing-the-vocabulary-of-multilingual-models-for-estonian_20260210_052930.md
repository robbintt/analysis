---
ver: rpa2
title: 'Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian'
arxiv_id: '2501.02631'
source_url: https://arxiv.org/abs/2501.02631
tags:
- vocabulary
- training
- tokenizer
- language
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated two approaches for optimizing the vocabulary
  of multilingual models for Estonian: retraining a tokenizer from scratch and pruning
  unused tokens. Both methods aimed to improve computational efficiency and potentially
  enhance performance on downstream tasks.'
---

# Prune or Retrain: Optimizing the Vocabulary of Multilingual Models for Estonian

## Quick Facts
- arXiv ID: 2501.02631
- Source URL: https://arxiv.org/abs/2501.02631
- Reference count: 8
- Primary result: Vocabulary pruning reduces model size by 23% without performance degradation, while retraining tokenizers requires more extensive training to maintain downstream performance

## Executive Summary
This study evaluates two approaches for optimizing multilingual model vocabularies for Estonian: retraining tokenizers from scratch and pruning unused tokens. Both methods aim to improve computational efficiency and potentially enhance downstream task performance. The research compares these approaches on Named Entity Recognition (NER) tasks using Estonian datasets, examining trade-offs between model size reduction, tokenization efficiency, and task performance.

## Method Summary
The study employs two main approaches: vocabulary pruning and tokenizer retraining. For pruning, the researchers removed tokens from a pretrained model that were not present in the Estonian NER datasets (2.5K and 9.7K sentences). The pruning process removed approximately 33% of tokens, resulting in a 23% reduction in model size. For retraining, they created new tokenizers with varying vocabulary sizes (32K and 64K) using WordPiece tokenization. Both pruned and retrained models underwent continued training with LoRA adapters to improve performance on Estonian data.

## Key Results
- Pruning 33% of tokens (retaining 67%) reduced model size by 23% with no negative impact on NER performance
- Retraining with a 32K vocabulary reduced tokens per word by 20% but significantly degraded NER performance (from 82.78 to 71.85 F1)
- Continued LoRA training improved masked language modeling accuracy but did not benefit NER performance in pruned models

## Why This Works (Mechanism)
Vocabulary pruning works by removing tokens that are unused in downstream tasks, reducing model size without affecting the model's ability to process the required tokens. The pruned model retains all tokens needed for the specific task while eliminating redundant parameters. Retraining with smaller vocabularies reduces tokens per word, potentially improving efficiency, but requires sufficient training to maintain task performance. The LoRA continued training provides task-specific adaptation, improving masked language modeling while not necessarily translating to downstream task improvements.

## Foundational Learning
- **WordPiece tokenization**: A subword tokenization method that breaks words into smaller units, needed for handling morphologically rich languages like Estonian; quick check: verify tokens can reconstruct original words
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning technique that adds low-rank matrices to pretrained weights; needed to adapt models to Estonian without full fine-tuning; quick check: confirm adapter dimensions match model architecture
- **Vocabulary pruning**: Process of removing unused tokens from pretrained models; needed to reduce model size while maintaining task performance; quick check: ensure pruned tokens don't appear in downstream datasets

## Architecture Onboarding
**Component Map**: Pretrained model -> Pruning/Retraining -> LoRA Adapter -> Downstream Task
**Critical Path**: Tokenization → Model processing → Task-specific output
**Design Tradeoffs**: Model size vs. performance (pruning favors size, retraining needs balance); efficiency vs. accuracy (smaller vocabularies improve efficiency but may hurt accuracy)
**Failure Signatures**: Performance degradation after pruning indicates critical tokens were removed; performance degradation after retraining suggests insufficient training or suboptimal hyperparameters
**3 First Experiments**: 1) Test pruning on additional downstream tasks beyond NER, 2) Evaluate different pruning thresholds (less than 67% retention), 3) Compare LoRA continued training duration and configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to one downstream task (NER) and one language (Estonian), limiting generalizability
- Pruning approach retains 67% of tokens but doesn't explore more aggressive pruning or how threshold was determined
- Retraining shows degraded performance despite reducing tokens per word, suggesting insufficient training or suboptimal hyperparameters
- LoRA continued training improved masked language modeling but not NER, raising questions about adequacy of training duration/methodology

## Confidence
- **High confidence**: Vocabulary pruning can reduce model size by 23% with no negative impact on NER performance
- **Medium confidence**: Retraining tokenizers with smaller vocabularies reduces tokens per word but requires more extensive training to maintain downstream performance
- **Medium confidence**: Continued training with LoRA improves masked language modeling but does not benefit NER performance in pruned models

## Next Checks
1. Test vocabulary pruning thresholds below 67% retention to determine the minimum viable vocabulary size without performance degradation
2. Evaluate both pruning and retraining approaches across multiple downstream tasks (beyond NER) and multiple languages to assess generalizability
3. Conduct ablation studies varying continued training duration, learning rates, and LoRA configurations to optimize performance recovery for both approaches