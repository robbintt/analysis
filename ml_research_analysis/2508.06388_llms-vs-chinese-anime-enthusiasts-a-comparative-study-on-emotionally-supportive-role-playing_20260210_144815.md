---
ver: rpa2
title: 'LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive
  Role-Playing'
arxiv_id: '2508.06388'
source_url: https://arxiv.org/abs/2508.06388
tags:
- human
- role-playing
- emotional
- character
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ChatAnime, the first Emotionally Supportive
  Role-Playing (ESRP) dataset, to evaluate how Large Language Models (LLMs) can provide
  emotional support while maintaining specific anime character traits. The dataset
  includes 20 well-known anime characters, 60 emotion-centric real-world scenario
  questions, 2,400 human-written answers, 24,000 LLM-generated answers, and over 132,000
  human annotations.
---

# LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing

## Quick Facts
- arXiv ID: 2508.06388
- Source URL: https://arxiv.org/abs/2508.06388
- Authors: Lanlan Qiu; Xiao Pu; Yeqi Feng; Tianxing He
- Reference count: 18
- Primary result: Top-performing LLMs surpass human fans in role-playing and emotional support quality, while humans lead in response diversity

## Executive Summary
This study introduces ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset, to evaluate how Large Language Models (LLMs) can provide emotional support while maintaining specific anime character traits. The dataset comprises 20 well-known anime characters, 60 emotion-centric real-world scenario questions, 2,400 human-written answers, 24,000 LLM-generated answers, and over 132,000 human annotations. An evaluation system with 9 fine-grained metrics across three dimensions (basic dialogue, role-playing, and emotional support) plus a diversity metric was designed.

The research demonstrates that LLMs can effectively provide emotional support while maintaining character consistency, with top models outperforming human enthusiasts in both role-playing and emotional support quality. However, human respondents maintain an advantage in generating diverse responses, suggesting complementary strengths between human and machine approaches to emotionally supportive role-playing.

## Method Summary
The study developed a comprehensive evaluation framework using the ChatAnime dataset, which includes well-known Chinese anime characters interacting with emotion-centric scenarios. The dataset was created through a multi-stage process involving scenario design, human answer generation, LLM response generation, and extensive human annotation. The evaluation system measures performance across three main dimensions: basic dialogue quality, role-playing fidelity, and emotional support effectiveness, with additional metrics for response diversity. Human annotators evaluated responses across 9 fine-grained metrics, resulting in over 132,000 annotations.

## Key Results
- Top-performing LLMs surpassed human anime enthusiasts in role-playing fidelity and emotional support quality
- Human respondents maintained superior performance in generating response diversity
- The evaluation framework successfully distinguished between different aspects of emotionally supportive role-playing
- LLMs demonstrated consistent performance across different emotional scenarios while maintaining character authenticity

## Why This Works (Mechanism)
The success of LLMs in emotionally supportive role-playing stems from their ability to process and synthesize vast amounts of character-specific dialogue patterns and emotional response templates. LLMs can maintain character consistency through learned personality embeddings while simultaneously accessing emotional support strategies from their training data. The structured evaluation framework with 9 fine-grained metrics provides clear performance boundaries that LLMs can optimize toward, explaining their superior performance in measured dimensions compared to human respondents who may have more variable approaches.

## Foundational Learning
This study builds upon several foundational areas in AI and human-computer interaction. First, it extends the field of character-consistent dialogue generation by introducing emotional support as a key evaluation dimension. Second, it contributes to affective computing by creating a structured framework for evaluating emotional support quality in conversational AI. Third, it advances human-AI collaboration research by demonstrating complementary strengths between human and machine approaches to emotionally supportive role-playing. The ChatAnime dataset itself represents a novel contribution to specialized conversational datasets that combine character fidelity with emotional intelligence requirements.

## Architecture Onboarding
Component map: ChatAnime dataset -> LLM models -> 9 evaluation metrics -> 3-dimensional scoring framework

Critical path: Scenario question → Character response generation → Human annotation → Metric calculation → Comparative analysis

Design tradeoffs: Structured evaluation metrics provide quantitative comparison but may miss nuanced aspects of emotional support; focus on Chinese anime limits generalizability

Failure signatures: Over-reliance on human annotation consistency; cultural specificity of scenarios and characters; potential bias in evaluation metrics

First experiments:
1. Test evaluation framework with different cultural anime characters to assess metric validity
2. Conduct ablation studies removing specific metrics to understand their contribution
3. Compare results with alternative evaluation approaches (peer review, expert assessment)

## Open Questions the Paper Calls Out
1. How can LLMs be further optimized to balance emotional support quality with response diversity?
2. What are the long-term effects of emotionally supportive role-playing interactions on users' mental health and well-being?
3. How can the evaluation framework be adapted for other cultural contexts and entertainment domains beyond Chinese anime?
4. What are the ethical implications of using AI for emotional support in sensitive situations?
5. How can human-LLM collaboration be optimized to leverage the complementary strengths identified in this study?

## Limitations
- Dataset creation relies heavily on subjective human annotations for emotional support quality
- Cultural specificity to Chinese anime limits generalizability to other contexts
- Structured evaluation may not capture nuanced aspects of effective emotional support
- 132,000+ annotations raise questions about consistency and reliability across raters
- Focus on text-based interactions excludes non-verbal emotional support elements
- Limited exploration of how different LLM architectures affect emotional support quality

## Confidence
High confidence: LLMs surpass human enthusiasts in role-playing and emotional support quality (supported by structured evaluation metrics)
Medium confidence: Humans maintain advantage in response diversity (more straightforward finding but may reflect evaluation criteria)
Low confidence: Generalizability to non-Chinese contexts and other entertainment domains

## Next Checks
1. Conduct a blind evaluation with a larger, more diverse group of human raters to assess inter-rater reliability and potential cultural biases in the current annotation process
2. Test the same LLMs and evaluation framework on anime characters and scenarios from different cultural contexts to assess generalizability
3. Implement longitudinal testing where the same emotional support conversations are evaluated over multiple exchanges to assess how well the current metrics capture sustained emotional support quality