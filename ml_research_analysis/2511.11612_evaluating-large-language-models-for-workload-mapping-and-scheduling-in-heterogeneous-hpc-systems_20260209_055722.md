---
ver: rpa2
title: Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous
  HPC Systems
arxiv_id: '2511.11612'
source_url: https://arxiv.org/abs/2511.11612
tags:
- reasoning
- scheduling
- task
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 21 publicly available large language models
  (LLMs) on heterogeneous HPC workload mapping and scheduling using only natural-language
  input. Each model was tasked with assigning tasks to nodes, computing makespan,
  and explaining its reasoning, without formal optimization frameworks.
---

# Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems

## Quick Facts
- arXiv ID: 2511.11612
- Source URL: https://arxiv.org/abs/2511.11612
- Reference count: 22
- Models tested: 21 publicly available LLMs on natural-language HPC scheduling task

## Executive Summary
This study evaluates 21 publicly available large language models on heterogeneous HPC workload mapping and scheduling using only natural-language input. Each model was tasked with assigning tasks to nodes, computing makespan, and explaining its reasoning without formal optimization frameworks. Three models (o3-mini, GPT-4.1 Mini, and Gemini Pro 2.5) exactly reproduced the analytical optimum of 9 h 20 s, while twelve others achieved near-optimal results within two minutes. All models produced feasible task-to-node mappings and coherent reasoning, though only about half strictly adhered to all constraints. Nineteen models generated partially executable verification code. The results show that leading LLMs can perform structured reasoning and explainability in combinatorial optimization, but struggle with precise timing and constraint enforcement. LLMs are thus promising as explainable co-pilots for hybrid optimization systems, rather than autonomous solvers.

## Method Summary
The study evaluated 21 LLMs via GWDG-hosted API using a single-prompt approach with chain-of-thought instruction. Models received a natural-language description of a 3-node heterogeneous system (NodeA: 32 CPUs, 128 GB RAM, GPU, 10 Gbps; NodeB: 64 CPUs, 256 GB RAM, CPU-only, 5 Gbps; NodeC: 16 CPUs, 64 GB RAM, SSD, 2 Gbps) and a 4-task workflow with dependencies (Task1→Task2; Task2+Task3→Task4). Each model produced task-to-node mappings, makespan calculations, reasoning traces, and optionally verification code. Evaluation metrics included makespan accuracy (ground truth: 9 h 20 s), constraint adherence (resources, features, dependencies, data transfer), reasoning coherence, code generation, and response time. Inference parameters were fixed at temperature=0.5 and top_p=50% across all models.

## Key Results
- Three models (o3-mini, GPT-4.1 Mini, Gemini Pro 2.5) exactly reproduced the analytical optimum makespan of 9 h 20 s
- Twelve models achieved near-optimal makespans within two minutes of the optimum
- All 21 models produced feasible task-to-node mappings with balanced node utilization
- Only 10 of 21 models strictly adhered to all constraints without violation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can transform natural language HPC scheduling descriptions into feasible task-to-node mappings without formal optimization frameworks
- Mechanism: Pattern recognition from training data enables semantic parsing of resource constraints and dependency structures; models apply heuristic scheduling logic learned from diverse optimization-related corpora
- Core assumption: Pre-training coverage includes sufficient scheduling, logistics, and resource allocation examples for transfer to HPC domain
- Evidence anchors: All models produced feasible task-to-node mappings and coherent reasoning; every model successfully achieved full task coverage and balanced node utilization

### Mechanism 2
- Claim: LLMs approximate constraint satisfaction probabilistically rather than enforcing constraints deterministically, causing arithmetic and dependency errors
- Mechanism: Next-token prediction prioritizes semantic coherence over mathematical precision; transfer-time calculations and temporal dependencies lack verification mechanisms during generation
- Core assumption: LLM inference does not perform explicit constraint propagation or backtracking
- Evidence anchors: Only about half strictly adhered to all constraints; frequent errors included incorrect transfer-time arithmetic, ignored dependency ordering, or minor resource over-allocations

### Mechanism 3
- Claim: Chain-of-thought prompting elicits interpretable reasoning traces that expose decision logic even when solutions are suboptimal
- Mechanism: Explicit instruction to provide step-by-step reasoning causes models to verbalize constraint tracking, trade-off considerations, and assignment justifications
- Core assumption: Reasoning transparency is achievable and valuable regardless of solution optimality
- Evidence anchors: 18 provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred; LLMs articulate decision logic, trade-offs, and assumptions in natural language

## Foundational Learning

- Concept: **Makespan and Critical Path in DAG Scheduling**
  - Why needed here: The analytical optimum (9h 20s) is determined by the longest dependency chain plus transfer delays; understanding this is essential to evaluate LLM makespan calculations
  - Quick check question: If Task 4 depends on both Task 2 (ends at 5h) and Task 3 (ends at 5h) on different nodes, and Task 3 outputs 20GB to transfer, when can Task 4 start?

- Concept: **Heterogeneous Resource Matching (Features + Capacity)**
  - Why needed here: Fixed feature requirements (GPU → NodeA, SSD → NodeC) constrain the search space; LLMs must respect both feature and capacity constraints simultaneously
  - Quick check question: Task 1 requires GPU and 8 CPUs; which node(s) can it run on, and what resources remain after assignment?

- Concept: **Data Transfer Latency Calculation**
  - Why needed here: Inter-node data transfer adds to makespan; formula is (Data_GB × 8) / min(src_rate, dst_rate) / 3600 hours
  - Quick check question: Calculate transfer time for 5GB from NodeA (10 Gbps) to NodeC (2 Gbps) in seconds.

## Architecture Onboarding

- Component map: [Natural Language Prompt] → [LLM Semantic Parser] → [Heuristic Scheduler] → [Reasoning Generator] → [Output]
- Critical path: Prompt clarity → Feature-constraint extraction (GPU/SSD assignment) → Dependency ordering → Transfer delay calculation → Makespan computation. Errors cascade from arithmetic mistakes in step 4.
- Design tradeoffs:
  - Single-prompt vs. multi-turn: Single-prompt is faster but limits refinement; multi-turn enables correction but adds latency
  - LLM-only vs. hybrid: LLM-only provides interpretability but no guarantees; hybrid (LLM + MILP solver) adds reliability but complexity
  - Temperature setting: Lower (0.5 used) reduces variability but may miss creative solutions; higher increases diversity at cost of consistency
- Failure signatures:
  - Arithmetic drift: Transfer times off by 10-60 seconds due to unit conversion errors
  - Premature task start: Task 4 begins before both dependencies arrive
  - Conservative serialization: Independent tasks (Task 1, Task 3) run sequentially instead of parallel
  - Context truncation: Later constraints in long prompts ignored
  - Resource over-allocation: Node capacity exceeded by simultaneous task assignments
- First 3 experiments:
  1. **Baseline reproduction**: Run the exact 4-task prompt on o3-mini, GPT-4.1 Mini, Gemini Pro 2.5 with temperature=0.5; verify makespan=9h20s and constraint adherence; document reasoning patterns
  2. **Scale test**: Expand to 8-task workflow with 3-node system; measure where constraint adherence drops below 50%; identify error type distribution (arithmetic vs. dependency vs. capacity)
  3. **Hybrid validation pipeline**: LLM generates initial mapping + reasoning → extract structured representation → pass to MILP solver (PuLP) for validation → return corrected schedule with LLM explanation of changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM scheduling performance degrade as workflow complexity (task count, dependency depth, node count) increases beyond the small 4-task, 3-node benchmark studied here?
- Basis in paper: Future work section states: "Future work will expand this benchmark toward larger and more complex workflow graphs."
- Why unresolved: The study only evaluated a single small-scale scenario with 4 tasks and 3 nodes; scalability to realistic HPC workflows with hundreds of tasks and nodes remains untested
- What evidence would resolve it: Systematic evaluation across a spectrum of workflow sizes, measuring makespan accuracy, constraint adherence, and reasoning coherence as problem scale increases

### Open Question 2
- Question: What hybrid architectures can effectively combine LLM-based natural-language reasoning with formal optimization solvers to achieve both interpretability and quantitative precision?
- Basis in paper: Conclusion states: "explore hybrid LLM architectures... LLMs could provide natural-language reasoning and constraint translation, while formal solvers ensure quantitative precision."
- Why unresolved: The paper identifies LLMs as promising co-pilots but does not implement or evaluate any specific hybrid system design
- What evidence would resolve it: Prototype implementations of LLM-solver pipelines with empirical comparison against standalone LLM and standalone solver baselines on scheduling benchmarks

### Open Question 3
- Question: Can automated constraint-verification pipelines reliably detect and correct the arithmetic errors, dependency violations, and unit-conversion mistakes that LLMs frequently produce?
- Basis in paper: Future work section: "integrate automated constraint-verification pipelines." Also, findings show only 10 of 21 models satisfied all constraints
- Why unresolved: The paper documents failure patterns but does not propose or test any correction or validation mechanisms
- What evidence would resolve it: Development of verification tools that parse LLM outputs, identify specific violation types, and measure correction success rates

### Open Question 4
- Question: How sensitive are LLM scheduling results to prompt phrasing, temperature settings, and multi-turn refinement loops versus single-shot prompting?
- Basis in paper: The paper notes results "varied across repeated runs with identical prompts" and used fixed temperature (0.5), but did not systematically vary these parameters or explore iterative refinement
- Why unresolved: Prompt engineering choices were not ablated; the contribution of prompt structure to success remains unclear
- What evidence would resolve it: Controlled experiments varying prompt formulations, temperature values, and comparing single-shot versus iterative refinement approaches

## Limitations

- Dataset Scope: Evaluation confined to a single 4-task DAG on a 3-node heterogeneous system; generalization to larger, more complex workflows remains untested
- Constraint Verification: "Constraint adherence" scoring is qualitative with no explicit rubric or inter-rater agreement reported
- Prompt Sensitivity: Results depend on prompt wording, temperature, and top-p settings; no ablation study on prompt structure or inference parameters conducted

## Confidence

- High Confidence: Feasibility of task-to-node mappings and interpretability of reasoning traces (supported by consistent outputs across 21 models)
- Medium Confidence: Near-optimal makespan results for top models (o3-mini, GPT-4.1 Mini, Gemini Pro 2.5), as analytical optimum is verifiable but may not generalize
- Low Confidence: Constraint adherence rates (only 10/21 models fully compliant) due to lack of quantitative scoring criteria and potential subjectivity in manual checks

## Next Checks

1. **Scaling Test**: Evaluate models on a 10-task DAG with mixed dependencies and resource heterogeneity; measure where constraint adherence and makespan accuracy drop significantly
2. **Prompt Ablation**: Vary temperature (0.1, 0.5, 1.0) and top-p (0.9, 0.95) on a fixed model (e.g., o3-mini) to quantify sensitivity of reasoning quality and makespan
3. **Hybrid Pipeline Validation**: Implement an automated LLM → MILP validation loop for the original 4-task scenario; compare corrected schedules and document LLM explanations of changes