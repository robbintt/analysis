---
ver: rpa2
title: Reward Modeling for Scientific Writing Evaluation
arxiv_id: '2601.11374'
source_url: https://arxiv.org/abs/2601.11374
tags:
- evaluation
- score
- reasoning
- work
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes reward models (SCIRM and SCIRM-REF) for scientific
  writing evaluation, addressing the challenge of adapting LLMs to diverse and task-specific
  evaluation criteria. The key idea is a two-stage training framework: first optimizing
  scientific evaluation preferences, then refining reasoning capabilities.'
---

# Reward Modeling for Scientific Writing Evaluation

## Quick Facts
- arXiv ID: 2601.11374
- Source URL: https://arxiv.org/abs/2601.11374
- Reference count: 40
- Primary result: SCIRM-REF achieves 0.83 accuracy on related work evaluation and 0.69 on review utility evaluation, outperforming baselines.

## Executive Summary
This paper introduces reward models (SCIRM and SCIRM-REF) for scientific writing evaluation, addressing the challenge of adapting large language models to diverse, task-specific evaluation criteria. The key innovation is a two-stage training framework that first optimizes scientific evaluation preferences and then refines reasoning capabilities through self-verification. This approach enables models to reason over explicit evaluation constitutions and adhere to dynamic criteria across tasks. Experimental results show that SCIRM-REF achieves strong performance on related work evaluation (0.83 accuracy) and review utility evaluation (0.69), while also generalizing effectively to unseen tasks like novelty alignment (0.74) and revision evaluation (0.78).

## Method Summary
The method employs a two-stage Group Relative Policy Optimization (GRPO) training framework on Qwen2.5-7B with LoRA (rank 64). Stage 1 optimizes preference-following using tiered rewards based on format compliance and score correctness, plus a length penalty to prevent reasoning-skipping. Stage 2 refines reasoning through self-verification, where models re-evaluate their initial reasoning against the constitution and receive rewards for self-correction (+1.0) or penalties for regression (-1.0). The models are trained on a combined dataset of 65,357 instances from related work evaluation (binary rubrics) and review utility evaluation (1-5 scales), and evaluated on both seen and unseen tasks using formatted output extraction.

## Key Results
- SCIRM-REF achieves 0.83 accuracy on related work evaluation and 0.69 on review utility evaluation
- Models generalize to unseen tasks, achieving 0.74 accuracy on novelty alignment and 0.78 on revision evaluation
- SCIRM-MASKED (trained without Actionability/Grounding aspects) still outperforms baselines on these unseen aspects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage GRPO training improves scientific evaluation by first teaching preference-following, then refining reasoning through self-verification.
- Mechanism: Stage 1 optimizes a reward function r(y, s*, S) that provides graduated penalties/rewards based on format compliance and score correctness. Stage 2 introduces reflection where the model re-evaluates its initial reasoning against the constitution, receiving highest reward (1.0) for self-correction and harshest penalty (-1.0) for regressing from correct to incorrect.
- Core assumption: Models can learn to reason over explicit criteria when incentivized through structured rewards rather than imitation alone.
- Evidence anchors:
  - [abstract]: "two-stage training framework that initially optimizes scientific evaluation preferences and then refines reasoning capabilities"
  - [section 3.2]: Reward functions r(y, s*, S) and rp(si, sf, s*) explicitly defined; SCIRM-REF achieves 0.74 on novelty alignment vs SCIRM's 0.61 (Table 1)
  - [corpus]: "Dynamic and Generalizable Process Reward Modeling" mentions cross-domain generalization challenges in PRMs, supporting the multi-stage approach
- Break condition: If reasoning tokens are truncated or length penalty k is set too aggressively, models may game rewards by outputting minimal reasoning.

### Mechanism 2
- Claim: Conditioning on explicit evaluation constitutions at both training and inference enables adaptation to novel tasks without retraining.
- Mechanism: Prompts include (q, c, e) tuplesâ€”query, criteria with scoring rubric, and examples per score. The model learns to parse and apply arbitrary constitutions rather than internalizing fixed rubrics.
- Core assumption: The model's learned "how to evaluate" skill transfers across domains when criteria are presented in-context.
- Evidence anchors:
  - [abstract]: "fine-tuning for each individual task is costly and impractical"
  - [section 3.2]: Prompt x = (q, c, e); examples provided for each scoring rubric in evaluation
  - [corpus]: Related work on PRMs notes struggle with cross-domain generalization when relying on heuristic approaches
- Break condition: If constitutions become too long or contain contradictory criteria, models may fail to resolve conflicts without explicit conflict-resolution training.

### Mechanism 3
- Claim: Joint training across diverse scientific tasks with multiple scoring rubrics improves robustness to evaluation schema variation.
- Mechanism: Training data combines related work evaluation (binary rubrics: 0-1) and review utility evaluation (ordinal rubrics: 1-5). Models learn that rubric structure varies and must attend to the provided schema rather than assuming fixed output spaces.
- Core assumption: Exposure to heterogeneous rubrics during training prevents overfitting to any single scoring paradigm.
- Evidence anchors:
  - [section 3.1]: Tables 3-4 show rubric diversity (binary vs 1-5 scales); "joint training across diverse tasks"
  - [section 5]: SCIRM-MASKED (trained without Actionability/Grounding aspects) still outperforms baselines on these unseen aspects
  - [corpus]: Limited direct evidence; corpus papers focus on single-task evaluation
- Break condition: If new tasks require fundamentally different output formats (e.g., continuous scores, multi-label), model may fail without format extension.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**:
  - Why needed here: Core RL algorithm used for both training stages; differs from standard PPO in how advantages are computed from grouped samples.
  - Quick check question: Can you explain how GRPO computes advantages from multiple rollouts per prompt vs single-response methods?

- **Reward Modeling Paradigms (Pointwise vs Pairwise)**:
  - Why needed here: SCIRM uses pointwise scoring conditioned on explicit criteria, unlike pairwise preference models that compare responses.
  - Quick check question: Why would pairwise models struggle with scientific evaluation tasks that require absolute quality assessment?

- **Constitutional AI Principles**:
  - Why needed here: The paper explicitly contrasts its inference-time constitution approach with Constitutional AI's training-time internalization.
  - Quick check question: What is the tradeoff between internalizing constitutions during training vs providing them at inference?

## Architecture Onboarding

- **Component map**:
  Base model (Qwen2.5-7B with LoRA) -> GRPO training (stage 1 and stage 2) -> vLLM rollout (4 samples per instance) -> Reward computation (combined r(y, s*, S) - f(L, T) or rp(si, sf, s*)) -> Output parsing (regex extraction from <reasoning> and <score> tags)

- **Critical path**:
  1. Prepare evaluation datasets with (query, criteria, examples, gold_score) tuples
  2. Stage 1 GRPO training with tiered rewards and length penalty
  3. Generate initial predictions for stage 2 data, remove scores, append reflection prompt
  4. Stage 2 GRPO training with correction-focused rewards
  5. Evaluate on held-out tasks using formatted output extraction

- **Design tradeoffs**:
  - 7B model size limits reasoning depth vs computational cost (acknowledged in Limitations)
  - LoRA reduces memory but may limit full adaptation vs full fine-tuning
  - Joint training improves generalization but may dilute task-specific performance

- **Failure signatures**:
  - Models generating only scores without reasoning (reward hacking)
  - Outputs truncated at max length (length penalty too weak)
  - Adjacent score confusion on instances with annotator disagreement (noted in error analysis)

- **First 3 experiments**:
  1. Replicate SCIRM stage 1 on a single evaluation aspect (e.g., Verifiability Extraction) to validate reward function implementation.
  2. Ablate length penalty to confirm it prevents reasoning-skipping behavior.
  3. Test SCIRM-MASKED setup: train on subset of aspects, evaluate on held-out aspects to verify generalization claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SCIRM-REF be effectively deployed as a reward signal for reinforcement learning to improve scientific text generation models across diverse tasks?
- Basis in paper: [explicit] The conclusion states: "A promising direction for future work is to improve scientific text generation across diverse tasks using reinforcement learning, where rewards are provided by our models."
- Why unresolved: The current work only validates SCIRM-REF for evaluation; its utility as a training signal for generation has not been tested.
- What evidence would resolve it: Experiments using SCIRM-REF rewards to train scientific text generators, measuring improvement in generated text quality across multiple aspects.

### Open Question 2
- Question: Would scaling beyond 7B parameters substantially improve evaluation accuracy, reasoning, and generalization to unseen tasks?
- Basis in paper: [explicit] The limitations state: "We hypothesize that larger models could better capture task-specific evaluation nuances, generalize more effectively, and exhibit more accurate reasoning."
- Why unresolved: Computational constraints of GRPO restricted experiments to 7B-scale models only.
- What evidence would resolve it: Training and evaluating SCIRM variants at 13B, 70B, or larger scales, comparing performance on both seen and unseen tasks.

### Open Question 3
- Question: How robust is the two-stage training framework to low inter-annotator agreement in ground-truth data?
- Basis in paper: [inferred] The error analysis notes that "some instances do not have full annotator agreement in RevUtil dataset" and models "can confuse two adjacent scores" on such instances.
- Why unresolved: The paper does not analyze performance stratified by annotation agreement levels, leaving unclear how noisy labels affect training.
- What evidence would resolve it: Ablation studies measuring model performance on high-agreement vs. low-agreement test subsets, or experiments with artificially injected label noise.

## Limitations
- 7B parameter size limits reasoning depth and may not capture task-specific nuances as effectively as larger models
- Dataset composition incompletely specified, particularly instance distribution across tasks and rubric schema variation
- Generalization claims based on only two held-out tasks (novelty alignment, revision evaluation)
- Length penalty parameters (k value, max length T) are only implicitly defined

## Confidence

- **High confidence**: SCIRM-REF achieves superior accuracy (0.83 related work, 0.69 review utility) over baselines on the evaluation tasks it was trained on. The two-stage GRPO training framework is correctly implemented and produces measurable improvements.
- **Medium confidence**: The model's generalization to unseen aspects (0.74 novelty alignment, 0.78 revision evaluation) demonstrates transfer capability, though the small number of held-out tasks limits the strength of this claim. The length penalty mechanism effectively prevents reward hacking, but optimal hyperparameters are not specified.
- **Low confidence**: Claims about the mechanism by which two-stage training improves reasoning are inferential rather than directly measured. The SCIRM-MASKED ablation's success in handling unseen aspects is promising but not thoroughly explored.

## Next Checks

1. **Dataset transparency audit**: Request complete breakdown of instance counts per task and aspect, plus explicit schema specifications for all rubrics used in training and evaluation.
2. **GRPO hyperparameter sensitivity analysis**: Systematically vary learning rate, batch size, KL coefficient, and length penalty k to determine optimal settings and assess robustness to hyperparameter choice.
3. **Extended generalization test**: Evaluate the trained models on at least 3-5 additional unseen scientific evaluation tasks with novel rubric structures to more rigorously test cross-domain transfer claims.