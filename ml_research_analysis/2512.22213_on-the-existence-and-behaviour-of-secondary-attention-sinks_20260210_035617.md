---
ver: rpa2
title: On the Existence and Behaviour of Secondary Attention Sinks
arxiv_id: '2512.22213'
source_url: https://arxiv.org/abs/2512.22213
tags:
- sink
- sinks
- secondary
- attention
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and analyzes secondary attention sinks\u2014\
  tokens that become attention sinks in middle layers and persist for variable depths,\
  \ unlike primary sinks that emerge early and persist throughout. The study spans\
  \ 11 model families and reveals that secondary sinks are formed by specific MLP\
  \ layers that map token representations to align with the primary sink direction."
---

# On the Existence and Behaviour of Secondary Attention Sinks

## Quick Facts
- **arXiv ID**: 2512.22213
- **Source URL**: https://arxiv.org/abs/2512.22213
- **Reference count**: 7
- **Primary result**: Secondary attention sinks emerge in middle layers, formed by specific MLP modules that map semantically uninformative tokens to align with primary sink direction, with their strength and lifetime determined by the ℓ2-norm of MLP outputs.

## Executive Summary
This paper identifies and analyzes secondary attention sinks—tokens that become attention sinks in middle layers and persist for variable depths, unlike primary sinks that emerge early and persist throughout. The study spans 11 model families and reveals that secondary sinks are formed by specific MLP layers that map token representations to align with the primary sink direction. The ℓ2-norm of these MLP outputs determines both the sink score and lifetime of secondary sinks. The findings show that larger models exhibit more deterministic and frequent sink levels, with secondary sinks compensating for the decay of primary sink strength in middle layers.

## Method Summary
The study analyzes reasoning traces from AIME24 and Math datasets processed through 11 model families. Researchers hook into model layers to extract hidden states, keys, values, and attention weights. Secondary sinks are detected when token hidden states achieve cosine similarity >0.95 with the BOS token's hidden state in middle layers. The analysis includes PCA on MLP inputs, t-SNE clustering of token representations in early layers, and token swapping experiments to verify formation mechanisms. Sink scores are computed using attention weight sums, and sink lifetimes are measured by persistence across layers.

## Key Results
- Secondary sinks emerge in middle layers (typically layer 11-22) and persist for 2-22 layers, unlike primary sinks that persist throughout
- The ℓ2-norm of MLP outputs at sink formation layer determines both sink score (log-linear relationship) and lifetime (monotonically increasing with plateaus)
- Models exhibit "sink levels" where tokens transformed at the same layer show different depths of influence based on MLP output norm
- Secondary sinks appear primarily in semantically uninformative tokens that cluster separately from normal tokens by layer 19
- The primary (BOS) sink weakens in middle layers, coinciding spatially with secondary sink emergence

## Why This Works (Mechanism)

### Mechanism 1: MLP-Driven Secondary Sink Formation
- **Claim**: Specific middle-layer MLP modules transform semantically uninformative tokens into secondary attention sinks by mapping their representations to align with the primary sink direction.
- **Mechanism**: At a critical layer $l_{start}$, the MLP receives token representations with low cosine similarity to the BOS sink. The MLP selectively amplifies components aligned with the BOS sink direction, producing outputs with high cosine similarity (>0.95) and elevated $\ell_2$-norms. Multiple orthogonal principal components (PC1–PC6) all map to the same sink direction, suggesting the MLP encodes a convergent mapping.
- **Core assumption**: The "linear representation hypothesis" holds that tokens sharing a common key direction can retrieve the same sink direction through linear transformation.
- **Evidence anchors**: [abstract] "these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink"; [Section 4] Figure 4 shows cosine similarity increasing from ~0.2 (MLP input) to ~0.9+ (MLP output); Figure 5 confirms multiple PCs converge to BOS-aligned outputs.

### Mechanism 2: Norm-Determined Sink Strength and Lifetime
- **Claim**: The $\ell_2$-norm of the MLP output at $l_{start}$ determines both the attention sink-score and how many layers the secondary sink persists.
- **Mechanism**: Higher MLP output norms produce higher sink-scores (log-linear relationship) and longer lifetimes (monotonically increasing with distinct linear regimes and plateaus). This creates differentiated "sink levels"—tokens transformed by the same $l_{start}$ but with different norms exhibit different depths of influence.
- **Core assumption**: Sink influence propagates through residual stream; norm magnitude controls attenuation rate.
- **Evidence anchors**: [abstract] "The ℓ2-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for"; [Section 5] Figure 7 (mid, right) shows log-linear relationship between sink-score ratio and MLP norm; lifetime increases monotonically with log-norm.

### Mechanism 3: Compensatory Primary Sink Decay
- **Claim**: Secondary sinks emerge when the primary (BOS) sink weakens in middle layers, suggesting a compensatory mechanism.
- **Mechanism**: The BOS sink-score exhibits a valley-shaped profile across network depth, reaching minimum in middle layers. At this minimum, secondary sinks emerge to maintain attention redistribution. The primary sink's reduced strength coincides spatially with secondary sink creation.
- **Core assumption**: Attention redistribution to sink tokens serves a functional role (e.g., mitigating over-mixing, providing reference points) that the network maintains.
- **Evidence anchors**: [abstract] "The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks"; [Section 5] Figure 7 (left) shows BOS sink-score valley with secondary sinks emerging at minimum.

## Foundational Learning

- **Concept: Attention Sinks** - Why needed: The entire paper builds on understanding that certain tokens receive disproportionately high attention despite limited semantic relevance. Without this, "secondary sink" has no reference point.
- **Concept: MLP Layer Function in Transformers** - Why needed: The paper's core finding is that specific MLP layers create secondary sinks. Understanding MLP operations (up-projection, activation, down-projection) is essential for interpreting Figure 4's MLP-stage analysis.
- **Concept: Cosine Similarity and Vector Alignment** - Why needed: The paper uses cosine similarity >0.95 as a detection threshold and shows progressive alignment through MLP stages. Understanding vector alignment is critical for the PCA analysis.

## Architecture Onboarding

- **Component map**: Primary sink tokens (BOS, chat template tokens) -> Secondary sink tokens (emerge at $l_{start}$) -> Critical MLP layers ($l_{start}$) -> Pre-$l_{start}$ layers (early clustering)
- **Critical path**: 1) Early layers (1 to $l_{start}$-3): Build representations that distinguish future sinks from normal uninformative tokens 2) $l_{start}$ MLP: Projects tokens onto sink direction with norm proportional to sink properties 3) Post-$l_{start}$ layers: Sink persists based on initial norm, gradually attenuating
- **Design tradeoffs**: Models with larger rotary base (lower RoPE frequency) more likely to exhibit secondary sinks (Qwen families), but not deterministic—CodeLlama has large base but no secondary sinks; Math post-training amplifies secondary sink phenomenon; larger models show fewer but more distinct sink levels
- **Failure signatures**: No secondary sinks detected in LLaMA-3.1, Phi-4-reasoning, Mathstral, CodeLlama despite similar architectures; secondary sinks suppressed when swapping activations at layers ≥19; models without valley in BOS sink-score unlikely to develop secondary sinks
- **First 3 experiments**: 1) Detection pipeline: Generate reasoning traces on AIME24/Math datasets; compute cosine similarity between each token's hidden state and BOS token per layer; threshold at >0.95 to identify secondary sink candidates 2) Ablation study: Swap MLP outputs at $l_{start}$ between secondary sink tokens and average tokens; measure suppression rate (expect 60-80% at layers 19-21) 3) Cross-model comparison: Run detection on Qwen2.5-14B (has secondary sinks at layer 22) vs. LLaMA-3.1-8B (none); compare BOS sink-score valley depth and MLP output norm distributions at corresponding middle layers

## Open Questions the Paper Calls Out

- **Question**: What is the causal effect of secondary sinks on text generation quality and downstream task performance?
- **Basis**: The conclusion states, "its effect on the text generation and downstream performance remains an open question."
- **Why unresolved**: The paper focuses on characterizing the existence, formation, and properties of secondary sinks, but does not perform ablation studies to determine if they are beneficial, detrimental, or neutral to model reasoning capabilities.

- **Question**: Why does post-training on reasoning data specifically amplify the secondary sink phenomenon?
- **Basis**: The paper notes, "Investigating... why post-training processes amplify their effects, is a promising direction for future research," observing that sinks strengthen significantly after math/reasoning training.
- **Why unresolved**: While the correlation between reasoning data and sink strength is observed, the specific optimization dynamics or data features driving this amplification are not isolated.

- **Question**: Why do models like CodeLlama fail to exhibit secondary sinks despite using large rotary bases similar to Qwen models?
- **Basis**: The authors note that despite using very large rotary bases, "CodeLlama and several other model families... do not exhibit such secondary sinks. The underlying cause of this discrepancy remains an open question."
- **Why unresolved**: The paper validates that secondary sinks appear in Qwen families with large bases, but contradicts prior work suggesting large rotary bases generally facilitate multiple sinks, leaving the architectural reason for CodeLlama's absence of sinks unexplained.

## Limitations
- The study only analyzes 2 datasets (AIME24 and Math), potentially limiting generalizability to other domains
- Secondary sink phenomenon appears in only 6 of 11 model families tested, with no clear deterministic architectural predictor
- The exact conditions under which a token becomes eligible for secondary sink transformation remain unclear

## Confidence

- **High Confidence**: The existence of secondary attention sinks in middle layers; the directional alignment of MLP outputs with primary sink direction; the compensatory relationship between primary sink decay and secondary sink emergence
- **Medium Confidence**: The mechanism by which specific MLP layers transform token representations into sink directions; the log-linear relationship between MLP output norm and sink properties; the clustering behavior of future sink tokens in early layers
- **Low Confidence**: The functional necessity of secondary sinks in the overall model behavior; the precise architectural features that determine secondary sink formation; the generalizability of the 6-level sink pattern observed in Qwen3-14B

## Next Checks
1. **Cross-Domain Validation**: Test secondary sink detection on non-math/non-reasoning datasets (e.g., code generation, summarization, dialogue) to assess whether the phenomenon is domain-specific or universal across LLM applications
2. **Architectural Ablation Study**: Systematically vary MLP layer sizes, rotary base frequencies, and post-training objectives in a controlled model to identify the minimal conditions necessary for secondary sink formation
3. **Functional Necessity Experiment**: Compare model performance with and without secondary sinks (via token swapping or MLP ablation at lstart) on downstream tasks to determine if secondary sinks provide computational advantages beyond attention redistribution