---
ver: rpa2
title: Rule-Guided Reinforcement Learning Policy Evaluation and Improvement
arxiv_id: '2503.09270'
source_url: https://arxiv.org/abs/2503.09270
tags:
- rules
- policy
- rule
- policies
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LEGIBLE, a framework for evaluating and improving
  deep reinforcement learning (RL) policies using domain knowledge expressed as rules.
  The approach mines rules from trained RL policies, generalizes them using metamorphic
  relations to express symmetries and domain knowledge, and guides policy execution
  with these rules to detect weaknesses and improve performance.
---

# Rule-Guided Reinforcement Learning Policy Evaluation and Improvement

## Quick Facts
- **arXiv ID:** 2503.09270
- **Source URL:** https://arxiv.org/abs/2503.09270
- **Reference count:** 17
- **One-line primary result:** Rule-guided execution of DQN policies improves cumulative rewards by up to 273% and outperforms random testing in detecting policy weaknesses.

## Executive Summary
This paper presents LEGIBLE, a framework for evaluating and improving deep reinforcement learning (RL) policies using domain knowledge expressed as rules. The approach mines rules from trained RL policies, generalizes them using metamorphic relations to express symmetries and domain knowledge, and guides policy execution with these rules to detect weaknesses and improve performance. The key contribution is demonstrating that enforcing rule-based generalizations during policy execution can reveal weaknesses where policies fail to generalize learned behaviors to related situations.

## Method Summary
LEGIBLE trains DQN policies, mines rules from their behavior using LIME for feature importance and RIPPER for symbolic rule learning, generalizes these rules using metamorphic relations that capture domain symmetries, and enforces these generalized rules during policy execution by masking actions with negative rules and following positive rules when triggered. The framework evaluates performance by comparing cumulative rewards under rule-guided versus baseline execution, using Welch's t-test to detect statistically significant improvements that indicate policy weaknesses.

## Key Results
- Rule-guided execution improves cumulative rewards by up to 273% compared to non-guided execution
- The method outperforms random testing baselines in detecting policy weaknesses
- Greedy composition of improving rule sets yields optimal performance across 11 PAC-Man and highway driving environments

## Why This Works (Mechanism)

### Mechanism 1: Weakness Detection via Counterfactual Enforcement
- **Claim:** If enforcing a generalized rule increases cumulative reward, the original policy likely failed to generalize a learned heuristic to related states.
- **Mechanism:** The system mines a rule (e.g., "avoid action A if feature X is present") and generalizes it using domain relations (e.g., "rotate 90 degrees"). It then overrides the agent's action selection using this new rule. A statistically significant reward increase using a Welch test indicates the agent had a "weakness" in the generalized scenario.
- **Core assumption:** The policy's value function captures local optima well but fails to generalize across symmetries or domain shifts that the symbolic rules capture.

### Mechanism 2: Feature-Importance Weighted Rule Induction
- **Claim:** Prioritizing features with high explainability scores (LIME) during rule mining creates surrogate rules that more accurately reflect the agent's internal decision boundaries.
- **Mechanism:** Instead of optimizing rule growth solely for coverage (standard RIPPER), the system maximizes the product of FOIL's information gain and LIME's feature importance weight ($imp(\otimes, a, f_i)$). This forces the symbolic rules to rely on features the neural network actually uses.
- **Core assumption:** LIME effectively identifies causal features in the RL policy; high importance weights correlate with the agent's decision logic.

### Mechanism 3: Runtime Action Masking
- **Claim:** Blocking specific actions via negative rules (setting Q-values to $-\infty$) improves policy robustness by preventing exploration of known low-value states.
- **Mechanism:** During execution, the system identifies negative rules matching the current state. It sets the Q-values of the associated "avoid" actions to negative infinity before selecting the argmax action. This effectively prunes the action space dynamically.
- **Core assumption:** The underlying Q-function is accurate enough that the remaining unmasked actions contain a viable optimal path.

## Foundational Learning

- **Concept: Metamorphic Relations (MRs)**
  - **Why needed here:** MRs are the core vehicle for injecting domain knowledge (e.g., "rotating the map 90 degrees rotates the optimal action 90 degrees"). Without understanding MRs, you cannot define the generalization logic.
  - **Quick check question:** Can you define a feature relation tuple $\langle a, a', i, j, R \rangle$ for a generic grid-world agent where moving North is equivalent to moving East after a clockwise rotation?

- **Concept: LIME (Local Interpretable Model-agnostic Explanations)**
  - **Why needed here:** This technique generates the "feature importance" weights required to prioritize which conditions (features) should appear in the mined symbolic rules.
  - **Quick check question:** If an agent avoids a ghost, would LIME assign a high positive or negative weight to the feature representing "distance to ghost"?

- **Concept: Q-Learning & Value Functions**
  - **Why needed here:** LEGIBLE relies on access to Q-values to identify "avoided" actions (low Q-values) for negative rule mining and to mask actions during enforcement.
  - **Quick check question:** In Algorithm 1, why do we set Q-values to $-\infty$ rather than removing the action from the action set entirely?

## Architecture Onboarding

- **Component map:** Data Collector -> Explainer (LIME) -> Learner (RIPPER) -> Generalizer -> Evaluator/Monitor
- **Critical path:** The quality of the **Generalized Rules ($G$)** is the bottleneck. If the MRs are manually defined incorrectly, or if the base rules are inaccurate, the runtime enforcement will fail.
- **Design tradeoffs:**
  - **Accuracy vs. Coverage:** The paper imposes strict accuracy thresholds (0.9) for rules, sacrificing coverage. This ensures enforcement is safe but might miss edge cases.
  - **Manual vs. Auto:** MRs are manually defined (domain knowledge), whereas rules are automatically mined. This is a hybrid neuro-symbolic approach.
- **Failure signatures:**
  - **Total Blockage:** Algorithm 1 falls back to random action because all actions are masked by conflicting negative rules.
  - **Performance Collapse:** Reward drops significantly, indicating the "generalized" rule was actually invalid for the specific environment dynamics (e.g., asymmetric map).
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Run the agent with *random* rule enforcement (Random Testing/RT) to ensure that *any* perturbation doesn't just naturally improve scores (placebo effect).
  2. **Ablation on MRs:** Compare performance of "Random Rules" (RR) vs. "Metamorphic Rules" (MT) to quantify the specific value added by your domain knowledge (MRs).
  3. **Extended Training Check:** Compare "Rule-Guided Execution" against a policy trained for 2x the steps. If the rule-guided version beats the extended-training version, you confirm the mechanism adds value beyond just "more data."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can generalized rules be seamlessly integrated into the training phase of reinforcement learning policies?
- **Basis in paper:** [explicit] The conclusion states an intention to work on "a more seamless integration of generalized rules into RL policies through rule-guided training."
- **Why unresolved:** The current approach applies rules post-training via runtime enforcement (Algorithm 1) rather than influencing the learning updates or network weights directly.

### Open Question 2
- **Question:** Can metamorphic testing (MT) be effectively combined with search-based testing to evaluate RL agents more comprehensively?
- **Basis in paper:** [explicit] The authors plan to "study how to combine MT with existing work that focuses on the environment, like search-based testing."
- **Why unresolved:** The paper focuses on an agent-centric view (changes in decisions), whereas search-based testing focuses on finding challenging environment states; the interaction between these paradigms is unexplored.

### Open Question 3
- **Question:** How can domain knowledge regarding temporal dependencies, such as restraining bolts, be integrated into the LEGIBLE framework?
- **Basis in paper:** [explicit] The conclusion proposes investigating "how to integrate other types of domain knowledge, for example, knowledge about temporal dependencies between actions via restraining bolts."
- **Why unresolved:** The current method relies on instantaneous state-action rules and feature relations, lacking the capacity to express or verify constraints over time sequences.

### Open Question 4
- **Question:** Can the rule-guided execution mechanism be adapted to improve policies trained with on-policy algorithms like PPO?
- **Basis in paper:** [inferred] The appendix notes that "rule-guided execution does not work well for on-policy algorithms, like PPO," despite rules being mineable from them.
- **Why unresolved:** The current enforcement logic (blocking actions via Q-values) is tailored for Q-learning; PPO policies rely on stochastic probability distributions that may be degraded by hard constraints.

## Limitations
- The framework relies on manually defined Metamorphic Relations, which must accurately capture domain symmetries or risk degrading performance
- LIME's effectiveness in identifying causal features may be limited for complex pixel-based inputs, potentially leading to spurious rules
- Runtime action masking could cause total blockage or performance collapse if negative rules are over-generalized

## Confidence

- **High Confidence:** The core mechanism of detecting weaknesses by enforcing generalized rules and measuring reward changes is well-supported by experimental results showing consistent improvements across 11 environments.
- **Medium Confidence:** The feature-importance weighted rule mining approach is theoretically sound, but the specific implementation details (LIME sampling size, RIPPER integration) are underspecified, making exact replication challenging.
- **Medium Confidence:** The runtime action masking mechanism is clearly defined, but the risk of over-generalization leading to performance collapse or total blockage is a valid concern that requires careful MR definition.

## Next Checks

1. **MR Validation:** Conduct an ablation study comparing performance using randomly generated rules versus rules generalized using the proposed MRs. This will quantify the specific value added by the domain knowledge encoded in the MRs.

2. **LIME Sensitivity Analysis:** Vary the LIME sampling size and observe its impact on the mined rules' accuracy and the subsequent policy improvement. This will assess the robustness of the feature-importance weighting approach.

3. **Negative Rule Over-Generalization:** Systematically increase the strictness of negative rule enforcement (e.g., higher accuracy thresholds) and monitor for performance degradation or total blockage. This will identify the limits of the action masking mechanism.