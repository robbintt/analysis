---
ver: rpa2
title: 'Rapfi: Distilling Efficient Neural Network for the Game of Gomoku'
arxiv_id: '2503.13178'
source_url: https://arxiv.org/abs/2503.13178
tags:
- search
- mixnet
- value
- policy
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Rapfi, an efficient Gomoku AI agent that outperforms
  CNN-based agents in limited computation environments. The key innovation is Mixnet,
  a compact neural network that decomposes the board into local line patterns and
  uses a pattern-based codebook distilled from CNNs.
---

# Rapfi: Distilling Efficient Neural Network for the Game of Gomoku

## Quick Facts
- arXiv ID: 2503.13178
- Source URL: https://arxiv.org/abs/2503.13178
- Reference count: 15
- Primary result: Outperforms CNN-based Gomoku agents under limited computation without GPUs, ranking first among 520 agents on Botzone

## Executive Summary
Rapfi presents an efficient Gomoku AI that achieves state-of-the-art performance under computational constraints by combining knowledge distillation, pattern-based feature extraction, and incremental updates. The key innovation is Mixnet, which replaces expensive convolutional operations with a precomputed codebook of local line patterns, enabling fast inference on CPUs. When combined with carefully tuned evaluation and search algorithms, Rapfi surpasses existing open-source Gomoku AIs while using significantly less computational resources.

## Method Summary
Rapfi uses knowledge distillation from a ResNet teacher to train a compact Mixnet student that decomposes the board into local 11-cell line patterns. A mapping network is trained to convert these patterns into feature vectors, which are then exported to a static codebook for fast lookup during inference. The system maintains accumulators for incremental updates when the board changes minimally between search steps, reducing computation by orders of magnitude. Dynamic convolution and grouped feed-forward heads help recover global context lost by the local decomposition approach.

## Key Results
- Mixnet achieves similar accuracy to ResNet-20b with ~10x less computation (~580k vs 5.3M FLOPs)
- Under fixed computational resources, Rapfi outperforms Katagomo, the strongest open-source AlphaZero-based Gomoku AI
- Ranked first among 520 Gomoku agents on Botzone and won GomoCup 2024 championship

## Why This Works (Mechanism)

### Mechanism 1: Pattern-based feature extraction
The system replaces 2D convolutions with pattern lookups by decomposing the board into 11-cell line segments. A mapping network is trained offline to convert all possible patterns to feature vectors, which are exported to a codebook. At runtime, features are retrieved via memory access rather than matrix multiplication, preserving accuracy while reducing FLOPs.

### Mechanism 2: Incremental updates
Since a move changes only one board point, affecting at most 4×11 local line patterns, the system maintains accumulators for feature maps and applies deltas when moves occur. This minimizes redundant computation during depth-first traversal by adding new features and subtracting old ones rather than recalculating the entire board.

### Mechanism 3: Global context recovery
The pattern-based approach loses global context, so the policy head uses dynamic convolution where weights are generated from global average pooled features. The value head groups the board into 3×3 regions and uses multiplicative pooling ("Star Blocks") to aggregate local wins into global assessments.

## Foundational Learning

- **Depth-First Search vs Best-First Search**: Understanding why incremental updates favor DFS (which retraces steps) over MCTS (which jumps between branches) is crucial for appreciating the efficiency gains. Quick check: Why does maintaining a persistent state accumulator favor algorithms that traverse branches fully before backtracking?

- **Knowledge Distillation**: The Mixnet uses a larger ResNet to generate labels, accepting lower policy accuracy (1.21 vs 1.03) for significantly reduced computation. Quick check: What tradeoff is being accepted when student policy loss exceeds teacher policy loss?

- **Receptive Fields in CNNs**: The paper replaces 2D spatial logic with 1D line logic. Quick check: How does the mapping network (5 layers of 3×3 Dir Conv) ensure it captures necessary length-11 patterns before being baked into a codebook?

## Architecture Onboarding

- **Component map**: Input -> Binary Board Plane -> Line Pattern Extractor -> Codebook (Lookup) -> Directional Features -> Sum features -> ReLU -> Incremental Accumulator -> Policy Head (Dynamic Conv) -> Value Head (Regional Grouping + Star Blocks)

- **Critical path**: The Codebook Export process is the highest-risk integration point. If the mapping network is not trained to convergence or export introduces errors, the inference agent will behave unpredictably.

- **Design tradeoffs**: Codebook size vs pattern length (longer patterns capture more context but explode memory usage), quantization (16-bit integers speed inference but cap precision), and model size (Large Mixnet offers better accuracy but may negate incremental update benefits).

- **Failure signatures**: Tactical blindness (missing obvious threats if pattern length is too short), slow MCTS (if base lookup overhead is too high), and memory overflow (cache misses destroying speed advantage).

- **First 3 experiments**:
  1. Verify incremental consistency by comparing full fresh lookup vs. incremental update for random moves
  2. Benchmark MCTS vs α-β search on fixed positions to confirm 4x node visit advantage
  3. Ablate pattern length (9 vs 11) to verify receptive field assumptions

## Open Questions the Paper Calls Out
The paper acknowledges limitations including the model's shallowness and scalability to larger networks, noting that large MixNet underperforms smaller variants with α-β search, suggesting codebook update costs may diminish scalability benefits.

## Limitations
- Shallowness of current architecture limits scalability to larger networks
- Reliance on distillation from CNN teacher raises questions about independent learning capability
- Pattern-based approach may not generalize well to games with different strategic structures

## Confidence
- High: The fundamental mechanism of codebook-based feature extraction with incremental updates is sound
- Medium: Knowledge distillation approach and architectural choices are plausible but depend on specific implementation details
- Low: Claims about achieving "first place" rankings require verification of exact computational constraints

## Next Checks
1. Implement incremental update consistency verification by comparing full-board computation vs incremental updates for 1000 random positions
2. Conduct pattern length sensitivity analysis by training Mixnet variants with lengths 9, 11, and 13
3. Measure actual node visit counts and time per node for both MCTS and α-β search to verify claimed 4x speedup advantage