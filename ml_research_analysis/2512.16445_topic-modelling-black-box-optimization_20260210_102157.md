---
ver: rpa2
title: Topic Modelling Black Box Optimization
arxiv_id: '2512.16445'
source_url: https://arxiv.org/abs/2512.16445
tags:
- optimization
- topic
- black-box
- sabbo
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of selecting the optimal number
  of topics in Latent Dirichlet Allocation (LDA) topic modeling by formulating it
  as a discrete black-box optimization problem. The authors compare four optimization
  approaches: two evolutionary methods (Genetic Algorithm and Evolution Strategy)
  and two learned amortized methods (Preferential Amortized Black-Box Optimization
  and Sharpness-Aware Black-Box Optimization).'
---

# Topic Modelling Black Box Optimization

## Quick Facts
- arXiv ID: 2512.16445
- Source URL: https://arxiv.org/abs/2512.16445
- Reference count: 24
- Compares evolutionary vs amortized optimizers for LDA topic number selection

## Executive Summary
This paper tackles the problem of selecting the optimal number of topics in Latent Dirichlet Allocation (LDA) topic modeling by framing it as a discrete black-box optimization task. The authors evaluate four optimization methods: two evolutionary approaches (Genetic Algorithm and Evolution Strategy) and two learned amortized methods (Preferential Amortized Black-Box Optimization and Sharpness-Aware Black-Box Optimization). Using perplexity as the evaluation metric across four text corpora, the study demonstrates that while all methods eventually achieve similar final perplexity scores, the amortized approaches are significantly more sample- and time-efficient. The Sharpness-Aware method typically identifies near-optimal topic numbers after just one evaluation, while the evolutionary methods require nearly the full evaluation budget to approach the same performance.

## Method Summary
The paper formulates LDA topic number selection as a discrete black-box optimization problem where the objective is to minimize perplexity. Four optimization approaches are compared: Genetic Algorithm (GA), Evolution Strategy (ES), Preferential Amortized Black-Box Optimization (PABBO), and Sharpness-Aware Black-Box Optimization (SABBO). Each method is given a budget of 20 evaluations to search for the optimal number of topics across four different text corpora. The amortized methods learn a policy from historical optimization data that can be applied to new problems, while the evolutionary methods use traditional population-based search strategies. All methods share the same evaluation metric (perplexity) and search space (discrete topic numbers).

## Key Results
- All four optimization methods eventually converge to similar final perplexity levels
- Amortized optimizers (PABBO and SABBO) are significantly more sample-efficient than evolutionary methods
- SABBO typically finds near-optimal configurations after just one evaluation
- PABBO identifies competitive configurations within a few evaluations while GA and ES require nearly all 20 evaluations

## Why This Works (Mechanism)
The success of amortized optimization methods stems from their ability to learn generalizable policies from historical optimization data. Unlike evolutionary methods that start optimization from scratch for each new problem instance, amortized methods leverage knowledge from previous optimization runs to make more informed decisions about which configurations to evaluate. This prior knowledge allows them to bypass the initial exploration phase that evolutionary methods must undergo, leading to faster convergence. The sharpness-aware approach additionally considers the sensitivity of the objective function to perturbations, helping it identify robust solutions more quickly.

## Foundational Learning
- **Latent Dirichlet Allocation (LDA)**: A generative probabilistic model for discovering abstract topics in text corpora. Why needed: This is the topic modeling technique being optimized.
- **Perplexity metric**: A standard measure of how well a probability model predicts a sample. Why needed: Used as the objective function to evaluate topic model quality.
- **Black-box optimization**: Optimization of functions where gradients are unavailable or expensive to compute. Why needed: Topic number selection is a discrete optimization problem without analytical gradients.
- **Amortized optimization**: Learning a policy that can be applied to optimize new, related problems. Why needed: Enables transfer of optimization knowledge across similar problems.
- **Evolutionary algorithms**: Population-based optimization methods inspired by biological evolution. Why needed: Traditional approach for black-box optimization that serves as baseline.
- **Discrete optimization**: Optimization over discrete rather than continuous variables. Why needed: Topic numbers are discrete values (2, 3, 4, etc.).

## Architecture Onboarding

**Component Map:**
Corpus -> Topic Model (with N topics) -> Perplexity Evaluation -> Optimizer -> Next Topic Number Proposal

**Critical Path:**
Initialization → First Evaluation → Policy Update (for amortized) / Population Update (for evolutionary) → Next Proposal → Evaluation → Convergence Check

**Design Tradeoffs:**
- Sample efficiency vs computational overhead (amortized methods require pretraining)
- Exploration vs exploitation (balancing search breadth vs depth)
- Computational cost per evaluation vs number of evaluations needed
- Generalizability across different corpora vs specialization for specific domains

**Failure Signatures:**
- Premature convergence to suboptimal topic numbers
- High variance in perplexity across different random seeds
- Inefficient exploration leading to wasted evaluations
- Sensitivity to hyperparameter settings in the optimization algorithm

**First 3 Experiments to Run:**
1. Evaluate each method's performance on a simple synthetic corpus with known optimal topic structure
2. Compare sample efficiency curves showing perplexity improvement over evaluation count
3. Test method robustness by running multiple trials with different random seeds

## Open Questions the Paper Calls Out
None

## Limitations
- Only perplexity is used as the objective function, potentially missing semantic quality aspects
- Results based on only four text corpora, limiting generalizability
- No computational overhead analysis for amortized methods' pretraining requirements
- Limited range of topic numbers evaluated may not capture full optimization landscape complexity

## Confidence
- Superior sample efficiency of amortized methods for LDA topic selection: **High**
- Convergence to similar final perplexity levels across all methods: **Medium**
- Generalization of results to other black-box optimization problems: **Low**
- Practical deployment advantages without computational overhead analysis: **Low**

## Next Checks
1. Replicate experiments using topic coherence metrics (UMass/UCI) alongside perplexity to verify semantic quality of discovered topics
2. Conduct experiments with expanded topic number search spaces (2-100 topics) to test method performance in more complex optimization landscapes
3. Perform comprehensive computational cost analysis comparing wall-clock time including pretraining for amortized approaches to determine true efficiency advantages