---
ver: rpa2
title: Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific
  Articles
arxiv_id: '2509.21028'
source_url: https://arxiv.org/abs/2509.21028
tags:
- article
- articles
- author
- reasoning
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciTrek is a long-context reasoning benchmark using full-text scientific
  articles, generating questions and answers via SQL queries over article metadata.
  It requires models to synthesize information across multiple articles using operations
  like counting, sorting, and filtering.
---

# Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles

## Quick Facts
- **arXiv ID:** 2509.21028
- **Source URL:** https://arxiv.org/abs/2509.21028
- **Reference count:** 32
- **Primary result:** SciTrek benchmark reveals current LLMs struggle with multi-article scientific reasoning, especially as context length increases.

## Executive Summary
SciTrek is a long-context reasoning benchmark that evaluates language models on synthesizing information across full-text scientific articles. The benchmark generates verifiable questions and answers using SQL queries over article metadata, requiring models to perform operations like counting, sorting, and filtering. Experiments show that both open-weight and proprietary models struggle with this task, with performance degrading predictably as context length increases. The benchmark provides fine-grained error analysis and is designed to be scalable to contexts up to 1M tokens.

## Method Summary
The benchmark constructs question-answer pairs by automatically generating SQL queries over structured article metadata (titles, authors, references) from 662 scientific articles across 8 subjects. Questions are converted to natural language and validated through round-trip SQL conversion. Models are evaluated at fixed context lengths (64K-1M tokens) using both full-text articles and condensed database tables. The evaluation uses exact match and F1 metrics, with optional supervised fine-tuning or GRPO training on a 19,543-question training set.

## Key Results
- Model performance degrades predictably as context length increases, from 61.0% (64K) to 46.5% (128K) for o4-mini.
- Most models struggle with basic operations: counting (especially references), sorting, and handling negation conditions.
- Database table contexts yield significantly higher accuracy than full-text contexts, confirming retrieval vs. reasoning separation.
- GRPO training improves reasoning traces but not accuracy on fine-grained operations like counting.

## Why This Works (Mechanism)

### Mechanism 1: SQL-Grounded Verifiable Reasoning
Automatically generating questions via SQL queries over article metadata produces verifiable reasoning chains with ground-truth answers. Article metadata is structured into database tables, and SQL queries encode explicit reasoning operations. Query execution yields deterministic answers, eliminating annotation ambiguity.

### Mechanism 2: Multi-Skill Stress Testing at Scale
The benchmark isolates specific reasoning skills (aggregation, sorting, filtering, relational filtering) across increasing context lengths to expose systematic failure modes. Questions are categorized by skill templates and evaluated at 64K-1M tokens, enabling fine-grained diagnosis of model weaknesses.

### Mechanism 3: Context-Length Degradation Signal
Model performance degrades predictably as context length increases, revealing capacity rather than reasoning limitations. Models are evaluated at fixed token lengths, showing monotonic performance decline even when database-table contexts yield higher accuracy, isolating retrieval/attention failures from reasoning failures.

## Foundational Learning

- **Concept: SQL Query Semantics → Reasoning Operations**
  - Why needed: Understanding how SQL primitives map to cognitive operations is prerequisite to interpreting error analysis.
  - Quick check: Given `SELECT COUNT(*) FROM articles WHERE reference_count > 50`, what reasoning step does each clause represent?

- **Concept: Attention Degradation in Long Contexts**
  - Why needed: The benchmark's core diagnostic depends on recognizing how transformer attention mechanisms lose fidelity beyond training distribution lengths.
  - Quick check: Why might a model correctly aggregate 5 articles but fail on 50, even with identical question structure?

- **Concept: Exact Match vs. F1 for Structured Outputs**
  - Why needed: Evaluation metrics must align with output type (lists, counts, names); exact match penalizes formatting errors differently than F1.
  - Quick check: If a model returns "Kim, Lee" when ground truth is "Lee, Kim", which metric captures the partial correctness?

## Architecture Onboarding

- **Component map:** Semantic Scholar API → PDF download → Marker (PDF→Markdown) → token-bucket assembly (64K-1M) → Database Generator → SQL Template Engine → Question Generator → Evaluation Harness

- **Critical path:** SQL template design → question validation → context assembly at target length → model inference. Validation step (bidirectional SQL↔NL) has 17.1% rejection rate.

- **Design tradeoffs:** Metadata-only vs. full-text reasoning (scalability vs. ecological validity), automated vs. human validation (cost vs. ambiguity), fixed vs. adaptive context (comparison simplicity vs. capacity testing).

- **Failure signatures:** NULL overuse (weak models default to "NULL"), format mismatch (author lists vs. counts), negation blindness (near-zero performance on NOT conditions), partial-list truncation (incomplete aggregations).

- **First 3 experiments:**
  1. Baseline replication: Run Qwen2.5-7B-Instruct-1M zero-shot on F-128 subset; verify exact match ≈2.8% and NULL-rate ≈80% on filtering tasks.
  2. Ablation on context type: Compare F-128 (full-text) vs. D-128 (database tables) for same model; expect ~4× improvement.
  3. Negation-focused probe: Isolate 20 questions with NOT operators; test whether chain-of-thought prompting improves performance.

## Open Questions the Paper Calls Out

### Open Question 1
Can the benchmark methodology be expanded to evaluate models on domain-specific content reasoning and figure interpretation rather than just metadata?
- Basis: Limitations section states future work will explore interpreting figures and domain-specific reasoning.
- Why unresolved: Current study restricts scope to metadata elements for scalability and verifiability.
- What evidence would resolve it: A new version containing questions derived from figure data and methodology sections.

### Open Question 2
Does the SQL-based construction methodology generalize effectively to non-scientific domains with structured entities?
- Basis: Conclusion claims methodology "generalizes beyond titles, authors, and references" but provides no experimental validation.
- Why unresolved: Experiments and dataset construction were exclusively confined to scientific articles.
- What evidence would resolve it: Successful application to other domains (legal documents, financial reports) yielding similar utility.

### Open Question 3
Why does reinforcement learning (GRPO) improve abstract reasoning logic without enhancing accuracy in fine-grained operations like counting?
- Basis: Analysis notes GRPO improves coherent reasoning traces but doesn't fix inaccuracies in specific steps, particularly counting references.
- Why unresolved: Paper observes the performance gap but doesn't identify specific training dynamics causing this dissociation.
- What evidence would resolve it: An ablation study analyzing token-level attention during RL training.

## Limitations
- Reliance on structured metadata rather than full-text comprehension limits ecological validity.
- 17.1% rejection rate in SQL-to-NL conversion suggests nontrivial annotation ambiguity.
- Fixed context buckets don't test models at their true capacity limits.
- Skill categorization lacks external validation for generalizability.

## Confidence
- **High:** SQL-grounded verifiable reasoning (supported by deterministic query execution)
- **Medium:** Context-length degradation as attention limit signal (ablation supports but doesn't conclusively prove mechanism)
- **Low:** Skill isolation diagnostic value (related work supports but no direct validation)

## Next Checks
1. **Negation robustness probe:** Isolate 50 questions with explicit negation operators; test whether chain-of-thought prompting improves performance versus zero-shot.
2. **Semantic comprehension extension:** Add 100 questions requiring interpretation of experimental methodology descriptions; compare performance drop to SQL-grounded questions.
3. **Capacity-accuracy tradeoff:** For models showing monotonic degradation, measure exact-match vs. token-length curves to determine if degradation follows theoretical attention window decay.