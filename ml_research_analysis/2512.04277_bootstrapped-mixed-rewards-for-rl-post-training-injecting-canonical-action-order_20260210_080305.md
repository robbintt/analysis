---
ver: rpa2
title: 'Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action
  Order'
arxiv_id: '2512.04277'
source_url: https://arxiv.org/abs/2512.04277
tags:
- order
- reward
- fine-tuned
- cell
- solver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate whether scalar rewards that hint at a canonical
  solving order can improve RL post-training performance on Sudoku, even when the
  base model is fine-tuned only on randomly ordered solution sequences. They combine
  cell accuracy with an ordering reward that encourages alignment with solver trajectories,
  using bootstrapped scaling to balance their magnitudes.
---

# Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order
## Quick Facts
- arXiv ID: 2512.04277
- Source URL: https://arxiv.org/abs/2512.04277
- Reference count: 7
- The authors investigate whether scalar rewards that hint at a canonical solving order can improve RL post-training performance on Sudoku, even when the base model is fine-tuned only on randomly ordered solution sequences.

## Executive Summary
The paper introduces a bootstrapped mixed-reward framework to enhance reinforcement learning (RL) post-training for Sudoku-solving models. The method combines cell accuracy rewards with ordering rewards that encourage alignment with canonical solver trajectories. By applying bootstrapped scaling to balance reward magnitudes, the authors show significant improvements in test accuracy over models fine-tuned only on random solution sequences. The approach demonstrates that even coarse ordering signals, provided solely as scalar rewards, can steer models toward solver-like reasoning without modifying the underlying training data or architecture.

## Method Summary
The authors augment standard RL post-training with a dual-reward structure: cell accuracy and ordering reward. The ordering reward is computed by comparing the model's action sequence to a known solver trajectory, encouraging canonical solving order. To balance the potentially disparate scales of the two rewards, they employ a bootstrapped scaling mechanism that dynamically adjusts their relative magnitudes during training. This mixed-reward RL is applied to a model initially fine-tuned on Sudoku solutions with randomly ordered steps, aiming to recover solver-like performance without altering the supervised training setup.

## Key Results
- On 9×9 Sudoku, GRPO with mixed rewards outperforms cell-only optimization.
- The best 0.75:0.25 cell-to-order mixture reaches 0.496 test accuracy.
- This improves over the random-order fine-tuned baseline by 0.214 absolute points and recovers ~90% of the gap to the solver-order fine-tuned model.

## Why This Works (Mechanism)
The ordering reward provides a coarse, domain-specific signal that guides the model toward solver-like action sequences. By aligning the model's behavior with a canonical solving trajectory, the reward encourages systematic reasoning rather than random or local search patterns. The bootstrapped scaling ensures that both accuracy and ordering signals contribute meaningfully during learning, preventing one from overwhelming the other. This combination allows the model to refine its solving strategy post-training, effectively bridging the gap between random and solver-ordered supervision.

## Foundational Learning
- **Reinforcement Learning (RL) for sequence tasks**: Needed to understand how scalar rewards can shape model behavior over action sequences. Quick check: model's performance improves when rewards are structured to guide behavior.
- **Reward shaping and scaling**: Critical for balancing multiple reward signals so both contribute to learning. Quick check: mixing ratios affect performance; bootstrapped scaling adapts dynamically.
- **Canonical solving order in Sudoku**: Provides a reference trajectory for measuring ordering reward. Quick check: solver trajectories are reproducible and verifiable.
- **Bootstrapped reward scaling**: Adjusts reward magnitudes during training to maintain effective learning dynamics. Quick check: scaling improves stability and final accuracy.
- **Post-training RL on pre-fine-tuned models**: Explores whether RL can refine reasoning without retraining from scratch. Quick check: model benefits from RL even after supervised fine-tuning.

## Architecture Onboarding
**Component Map**: Pre-fine-tuned model -> RL environment (Sudoku) -> Reward function (cell accuracy + ordering) -> Bootstrapped scaling -> Policy update
**Critical Path**: Model generates action sequence → Reward function computes cell accuracy and ordering penalties → Bootstrapped scaling balances rewards → Policy gradient updates model parameters
**Design Tradeoffs**: Using scalar rewards instead of full trajectories trades off fine-grained supervision for scalability and simplicity; bootstrapped scaling adds complexity but improves stability.
**Failure Signatures**: If ordering reward dominates, model may overfit to solver trajectories and lose flexibility; if cell accuracy dominates, ordering benefits may be lost.
**First Experiments**: 1) Train with only cell accuracy reward to establish baseline. 2) Add ordering reward with fixed scaling to test its standalone effect. 3) Apply bootstrapped scaling to mixed rewards and compare performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The effectiveness of the ordering reward may be specific to Sudoku, where a clear canonical solver exists, and may not generalize to less structured domains.
- Bootstrapped scaling lacks comparison to simpler alternatives like fixed reward coefficients, leaving open whether its performance gain is essential.
- No ablations on cell accuracy alone are provided to confirm the ordering reward's decisive role versus hyperparameter tuning.

## Confidence
- Mixed rewards outperform cell-only optimization on 9×9 Sudoku: **High**
- Ordering signals can steer models toward solver-like reasoning without altering training data or architecture: **Medium**
- Recovers ~90% of the gap to the solver-order fine-tuned model: **Medium**

## Next Checks
1. Apply the bootstrapped mixed-reward framework to a different combinatorial reasoning task (e.g., Kakuro or Latin squares) to test domain transfer.
2. Conduct ablations isolating the effect of bootstrapped scaling versus fixed scalar coefficients on reward effectiveness.
3. Analyze intermediate model outputs during inference to verify that improved accuracy corresponds to closer alignment with canonical solver trajectories rather than superficial pattern matching.