---
ver: rpa2
title: 'REAL: Benchmarking Abilities of Large Language Models for Housing Transactions
  and Services'
arxiv_id: '2507.03477'
source_url: https://arxiv.org/abs/2507.03477
tags:
- block
- real
- names
- llms
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces REAL, the first benchmark suite designed
  to evaluate large language models (LLMs) in housing transactions and services. REAL
  contains 5,316 high-quality questions across 4 topics: memory, comprehension, reasoning,
  and hallucination, organized into 14 categories.'
---

# REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services

## Quick Facts
- arXiv ID: 2507.03477
- Source URL: https://arxiv.org/abs/2507.03477
- Reference count: 29
- Introduces REAL benchmark evaluating LLMs on housing transactions and services with 5,316 questions across 4 topics

## Executive Summary
This paper introduces REAL, the first comprehensive benchmark suite designed to evaluate large language models in the domain of housing transactions and services. The benchmark contains 5,316 high-quality questions organized into 14 categories covering memory, comprehension, reasoning, and hallucination detection. REAL addresses the critical gap in evaluating LLMs' capabilities in real-world real estate scenarios where accurate knowledge and reliable responses are essential for high-stakes transactions.

Experiments on 12 mainstream LLMs reveal significant performance limitations, with models achieving only 50% accuracy on core tasks and exhibiting approximately 40% hallucination rates when handling fictional entities. These results demonstrate that current LLMs still fall short of matching human real estate agents' capabilities in this domain, highlighting the need for continued research and development in domain-specific language model applications.

## Method Summary
The REAL benchmark was constructed using a systematic approach that involved generating questions across four main topics: memory, comprehension, reasoning, and hallucination detection. The questions were designed to evaluate LLMs' knowledge of real estate properties, understanding of natural language descriptions, ability to reason about client behavior, and capacity to detect hallucinations. The benchmark includes 5,316 questions organized into 14 categories, covering various aspects of housing transactions and services. Evaluation was conducted on 12 mainstream LLMs, with performance measured across the different task types and categories.

## Key Results
- LLMs achieve only 50% average accuracy on memory, comprehension, and reasoning tasks in real estate domain
- Hallucination rates average around 40% for fictional names, with higher rates when presented with non-existent entities
- Current models demonstrate significant gaps in matching human real estate agents' capabilities in housing transactions

## Why This Works (Mechanism)
The REAL benchmark works by systematically evaluating LLMs across multiple dimensions of real estate knowledge and reasoning capabilities. The mechanism relies on well-structured questions that test both factual knowledge and higher-order reasoning skills required in housing transactions. By including hallucination detection tasks, the benchmark addresses the critical issue of model reliability in high-stakes scenarios where invented information could have serious consequences. The multi-topic approach ensures comprehensive coverage of the real estate domain, from basic property knowledge to complex client interaction scenarios.

## Foundational Learning

1. **Real Estate Domain Knowledge** (why needed: Models must understand property types, market dynamics, and transaction processes; quick check: Can accurately describe different property categories and their typical features)
2. **Natural Language Comprehension** (why needed: Agents need to interpret client descriptions and preferences; quick check: Can extract key requirements from complex property descriptions)
3. **Reasoning About Client Behavior** (why needed: Models must infer client needs and predict preferences; quick check: Can recommend suitable properties based on stated preferences)
4. **Hallucination Detection** (why needed: Critical for preventing misinformation in high-stakes transactions; quick check: Can identify when presented with fictional entities)
5. **Memory and Recall** (why needed: Agents need to remember property details and market information; quick check: Can accurately recall specific property features)
6. **Multi-turn Dialogue Understanding** (why needed: Real estate involves complex, ongoing conversations; quick check: Can maintain context across multiple questions about the same property)

## Architecture Onboarding

Component map: REAL Benchmark -> Evaluation Framework -> 12 LLMs -> Performance Metrics

Critical path: Question generation -> Model input processing -> Response generation -> Answer evaluation -> Performance aggregation

Design tradeoffs: Comprehensive coverage vs. evaluation complexity; realistic scenarios vs. controlled testing conditions; binary evaluation vs. nuanced assessment of responses

Failure signatures: Low accuracy on reasoning tasks indicates insufficient domain understanding; high hallucination rates suggest inadequate fact-checking mechanisms; poor performance on memory tasks reveals knowledge gaps

First experiments:
1. Test model performance on basic property identification questions
2. Evaluate reasoning capabilities with client preference scenarios
3. Measure hallucination detection accuracy with fictional entity introduction

## Open Questions the Paper Calls Out

None

## Limitations

- Reliance on simulated real estate agents for data collection may not fully represent actual real estate practice
- Binary correct/incorrect evaluation framework may oversimplify the nuanced nature of real estate knowledge
- Does not address cultural or regional variations in real estate practices that could affect model performance
- Lacks temporal analysis of how quickly real estate market changes impact model knowledge accuracy

## Confidence

Real estate knowledge gap (High): Well-supported by experimental results across 12 mainstream models with comprehensive evaluation methodology

Hallucination detection capabilities (Medium): Concerning 40% hallucination rate, but binary evaluation may oversimplify complex issue

Benchmark representativeness (Medium): Covers 4 main topics and 14 categories but may not fully capture real estate domain complexity

## Next Checks

1. Conduct field study with actual real estate agents to validate REAL benchmark's question quality and difficulty level against real-world scenarios

2. Implement longitudinal study tracking model performance on REAL over time as real estate markets evolve

3. Develop more nuanced evaluation framework for hallucination detection that considers severity and context of hallucinations