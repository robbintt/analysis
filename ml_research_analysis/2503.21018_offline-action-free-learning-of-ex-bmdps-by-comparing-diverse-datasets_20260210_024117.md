---
ver: rpa2
title: Offline Action-Free Learning of Ex-BMDPs by Comparing Diverse Datasets
arxiv_id: '2503.21018'
source_url: https://arxiv.org/abs/2503.21018
tags:
- pred
- then
- latent
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses action-free representation learning in Exogenous
  Block Markov Decision Processes (Ex-BMDPs), where observations contain both controllable
  latent states and temporally-correlated noise features. Prior work showed this is
  theoretically impossible without action labels, but this paper demonstrates that
  with offline video data from multiple agents with different policies, the problem
  becomes tractable.
---

# Offline Action-Free Learning of Ex-BMDPs by Comparing Diverse Datasets

## Quick Facts
- arXiv ID: 2503.21018
- Source URL: https://arxiv.org/abs/2503.21018
- Reference count: 40
- Primary result: CRAFT achieves 86.4-97.7% encoder accuracy from diverse offline datasets, with sample complexity depending only on controllable state sizes

## Executive Summary
This paper tackles action-free representation learning in Exogenous Block Markov Decision Processes (Ex-BMDPs), where observations contain both controllable latent states and temporally-correlated noise. Prior work proved this impossible without action labels, but this paper shows that offline video data from multiple agents with different policies enables tractable learning. The proposed CRAFT algorithm identifies latent states by clustering observation pairs based on agent identity prediction, leveraging the fact that controllable features transition differently under different policies while exogenous noise remains invariant.

## Method Summary
CRAFT operates on offline datasets from two agents with different policies, splitting trajectories into observation pairs (x_h, x_{h+1}). For each timestep h, it trains a log-odds predictor f_h to distinguish which agent generated each pair, using loss L(f)=Σln(1+e^(-f)) for τA + Σln(1+e^(f)) for τB. Observation pairs are clustered by discretized f_h values, then merged if their observation sets are statistically indistinguishable via binary classifiers. The final encoder ϕ'_h is trained via supervised classification on the identified state datasets. The method relies on policy diversity (agents differ by multiplicative factor e^α) and coverage assumptions (every latent transition observed sufficiently often).

## Key Results
- Achieves 86.4-97.7% encoder accuracy on toy environment with 500-5000 trajectories per agent
- Sample complexity depends only on controllable latent state sizes, not exogenous noise space size
- Outperforms baselines that predict agent identity from single observations or paired observations
- Theoretical bounds show dependence on policy diversity α, coverage ν, discretization grid size |Ξ|, and hypothesis class sizes |Φ|, |G|

## Why This Works (Mechanism)

### Mechanism 1: Policy Diversity Creates Discriminative Signal
When two agents operate with sufficiently different policies, observation pairs associated with different controllable latent transitions become separable by agent identity. Controllable latent features transition with different probabilities under different policies while exogenous noise features evolve identically regardless of agent. The clustering signal vanishes if policies are identical or one agent never visits certain transitions.

### Mechanism 2: Log-Odds Clustering Avoids Error Accumulation
Predicting log-odds ratios rather than hard binary classifications allows robust clustering without compounding errors across timesteps. CRAFT trains f_h to predict log(Pr[agent A]/Pr[agent B]) for each observation pair. All pairs from the same latent transition cluster around the same log-odds value. Critically, these f_h models are trained once on the full dataset—not recursively—preventing error propagation.

### Mechanism 3: Merge Detection Via Distinguishability Testing
After clustering observation pairs by f_h values, CRAFT trains binary classifiers between resulting observation sets. If classifier loss > 0.5 (random chance), the sets are deemed to represent the same latent state and are merged. Classifier hypothesis class G_h must have sufficient capacity to perfectly distinguish observations from different latent states.

## Foundational Learning

- **Exogenous Block MDPs (Ex-BMDPs)**: Framework assumes observations decompose into controllable latent states and action-independent exogenous noise. Quick check: Can you identify which parts of your observation space change when the agent acts vs. which parts evolve independently of actions?

- **Coverage Assumptions in Offline RL**: The ν parameter ensures every reachable latent transition is observed often enough. Without this, some states would never be learned. Quick check: Does your offline dataset contain examples of all states/actions you eventually want to represent?

- **Log-Odds and Logistic Regression**: CRAFT trains models to output log-odds of agent identity using the loss in Equation 13. Understanding why minimizing this loss yields calibrated log-odds is essential. Quick check: Why is predicting log-odds preferable to predicting probabilities directly for clustering?

## Architecture Onboarding

- **Component map**: Data preprocessing → Log-odds predictors f_h → Cluster identification → Merge classifiers → Final encoders ϕ'_h
- **Critical path**: Train all f_h models independently → At h=1, identify clusters → For h > 1: for each identified predecessor state, identify successor clusters from its pairs, then merge across predecessors → Train final encoders on assembled per-state datasets
- **Design tradeoffs**: Discretization granularity (|Ξ|) finer bins separate more transitions but require more samples; q_thresh threshold too high misses rare transitions, too low includes noise clusters; hypothesis class size (|Φ|, |G|) larger classes improve realizability but worsen sample complexity
- **Failure signatures**: Encoder accuracy plateaus below 100% even with more data → likely policy diversity α too small or hypothesis class underfitting; number of discovered states grows without bound → q_thresh too low or merge classifiers failing; performance degrades at later timesteps → coverage ν or ν' insufficient for deep horizons
- **First 3 experiments**: Reproduce toy environment (H=30, M=128) with varying trajectories per agent (500, 1000, 5000); verify encoder accuracy matches Table 1 (86.4% → >99.9%); ablate policy diversity by reducing agent B's bias from 3/4 toward 1/2; stress test merge classifiers by constructing environment where multiple predecessors lead to same successor state

## Open Questions the Paper Calls Out

### Open Question 1
Can CRAFT be extended to handle stochastic or near-deterministic latent dynamics? The current theoretical analysis and sample complexity bounds rely strictly on deterministic transitions s*_{h+1} = T_{h+1}(s*_h, a_h). Evidence needed: A modified version of Theorem 3.1 that provides sample-complexity bounds allowing for a small probability of deviation from deterministic dynamics per episode.

### Open Question 2
How does the sample complexity and algorithm design change when leveraging data from more than two agents? The current algorithm and theoretical bounds are formulated specifically for the comparison between two datasets (τ_A and τ_B). Evidence needed: An algorithmic extension that compares multiple datasets simultaneously, along with analysis showing how the policy diversity parameter α scales with the number of agents.

### Open Question 3
Is the "noise-independent policy" assumption necessary for the correctness of the learned representation? The identifiability of the exogenous features relies on the differing dynamics of the controllable states; if policies depend on exogenous noise, they might induce spurious correlations that the algorithm could mistakenly attribute to controllable dynamics. Evidence needed: A theoretical proof of correctness under relaxed policy assumptions, or conversely, a counter-example construction showing where policy-dependence on noise leads to incorrect latent state decoding.

## Limitations
- Assumes very specific Ex-BMDP structure with deterministic controllable features and independent Markov chain noise
- Requires two agents with sufficiently different policies (α > 0), which may not be satisfied in practice
- Algorithm performance critically depends on coverage parameter ν, difficult to verify without access to true latent state space
- Theoretical framework relies on idealized conditions that may not hold in practical applications

## Confidence

- **High confidence**: Clustering mechanism based on log-odds prediction works as described when policy diversity assumption holds; theoretical sample complexity bounds are correctly derived
- **Medium confidence**: Merge detection mechanism performs reliably in practice, though effectiveness depends on classifier hypothesis class capacity; discretization approach provides reasonable approximation
- **Low confidence**: Algorithm scales to high-dimensional observation spaces with complex exogenous noise structures; coverage requirements can be satisfied in practical offline datasets

## Next Checks
1. Systematically vary the bias difference between agents (from α=0 to α=ln(3)) and measure how encoder accuracy degrades to identify minimum viable policy diversity
2. Construct environments with ambiguous successor states that could plausibly arise from multiple predecessors, and measure false positive/false negative rates in merge detection
3. Design experiments to empirically estimate whether the ν and ν' coverage conditions are satisfied in practice, particularly for deeper timesteps where coverage becomes more challenging