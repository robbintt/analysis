---
ver: rpa2
title: Multimodal Arabic Captioning with Interpretable Visual Concept Integration
arxiv_id: '2510.03295'
source_url: https://arxiv.org/abs/2510.03295
tags:
- arabic
- image
- captioning
- visual
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLCAP is an Arabic image captioning framework that integrates CLIP-based
  visual label retrieval with multimodal text generation. Unlike prior work, it grounds
  captioning in interpretable Arabic visual concepts extracted using three multilingual
  encoders (mCLIP, AraCLIP, Jina V4).
---

# Multimodal Arabic Captioning with Interpretable Visual Concept Integration

## Quick Facts
- arXiv ID: 2510.03295
- Source URL: https://arxiv.org/abs/2510.03295
- Authors: Passant Elchafei; Amany Fashwan
- Reference count: 9
- Primary result: VLCAP integrates CLIP-based visual label retrieval with multimodal generation, ranking first in cosine similarity and second in LLM-as-a-judge among competing teams.

## Executive Summary
VLCAP is an Arabic image captioning framework that grounds generation in interpretable visual concepts retrieved using multilingual CLIP encoders (mCLIP, AraCLIP, Jina V4). Unlike prior end-to-end approaches, it builds a hybrid Arabic vocabulary from training captions and 21K translated general domain labels, then uses top-k retrieved labels to construct prompts for vision-language models (Qwen-VL, Gemini Pro Vision). This staged approach enables culturally coherent and contextually accurate Arabic captions. The system ranked first in cosine similarity and second in LLM-as-a-judge among participating teams, demonstrating strong semantic adequacy and human preference.

## Method Summary
VLCAP employs a two-stage pipeline for Arabic image captioning. First, it constructs a curated Arabic visual vocabulary from translated general domain concepts and frequent content words extracted from training captions. Three multilingual CLIP encoders (mCLIP, AraCLIP, Jina V4) retrieve top-k relevant visual labels per image by cosine similarity ranking. Second, these labels are transformed into fluent Arabic prompts and fed to vision-language models (Qwen-VL or Gemini Pro Vision) to generate captions. The approach uses six encoder-decoder configurations, enabling systematic evaluation of different visual and linguistic components.

## Key Results
- mCLIP + Gemini Pro Vision achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%)
- AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%)
- VLCAP ranked first in cosine similarity and second in LLM-as-a-judge among participating teams
- Different encoder-decoder pairs excel on different metrics, with no single configuration optimizing all objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving explicit visual labels before generation improves semantic grounding compared to end-to-end captioning.
- Mechanism: CLIP-based encoders compute cosine similarity between image embeddings and a curated Arabic vocabulary (~21K concepts). Top-k labels serve as interpretable anchors representing "what is seen," which are then formatted as prompts for the downstream VLM.
- Core assumption: The vocabulary sufficiently covers culturally relevant visual concepts, and CLIP similarity correlates with human-perceived relevance.
- Evidence anchors:
  - [abstract] "VLCAP grounds generation in interpretable Arabic visual concepts extracted with three multilingual encoders... top-k retrieved labels are transformed into fluent Arabic prompts."
  - [section 3.2] "Cosine similarity ranking is applied to select the top-k matched labels per image, which serve as interpretable visual representations."
  - [corpus] CONCAP (2507.20411) demonstrates similar retrieval-augmented captioning benefits for multilingual settings, supporting the general mechanism.
- Break condition: If vocabulary coverage is sparse for domain-specific images, retrieved labels may be irrelevant or misleading, degrading caption quality.

### Mechanism 2
- Claim: Decoupling visual recognition from linguistic generation allows independent optimization and improves interpretability.
- Mechanism: Stage 1 extracts visual labels; Stage 2 generates captions using a VLM conditioned on both the image and the retrieved labels. This separation enables tuning each component separately (e.g., different CLIP variants vs. different VLMs).
- Core assumption: The VLM can effectively integrate both visual and textual signals without catastrophic interference.
- Evidence anchors:
  - [abstract] "This interpretable pipeline enables culturally coherent and contextually accurate Arabic captions."
  - [section 3] "Unlike prior work that mainly adapts English datasets or relies on end-to-end models, our approach explicitly grounds captioning in Arabic visual concepts."
  - [corpus] No directly comparable decoupled architectures found; related work (JEEM, ArtRAG) focuses on end-to-end or RAG-augmented approaches but not staged label-then-generate pipelines.
- Break condition: If the VLM over-relies on retrieved labels and ignores image features, captions may hallucinate or miss visual details not in the vocabulary.

### Mechanism 3
- Claim: Encoder choice influences different quality dimensions (lexical similarity vs. human preference).
- Mechanism: mCLIP yields higher BLEU-1 and cosine similarity (lexical alignment), while AraCLIP yields higher LLM-judge scores (perceived relevance/fluency), suggesting label quality affects downstream evaluation differently depending on metric.
- Core assumption: LLM-as-a-judge correlates with human preference for cultural and contextual appropriateness.
- Evidence anchors:
  - [abstract] "mCLIP + Gemini Pro Vision achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%)."
  - [section 4] "AraCLIP, despite lower BLEU-1 and cosine similarity scores, achieves the highest LLM Judge Score, indicating that captions... are often judged as more contextually relevant."
  - [corpus] Weak corpus support; no comparable encoder comparison studies found for Arabic captioning.
- Break condition: If evaluation metrics are misaligned with deployment goals, selecting the "best" configuration may optimize the wrong objective.

## Foundational Learning

- Concept: CLIP-style multimodal embeddings
  - Why needed here: Understanding how image-text similarity is computed is essential for debugging label retrieval quality.
  - Quick check question: Given an image embedding and three candidate label embeddings, how would you determine which label best matches the image?

- Concept: Two-stage vs. end-to-end architectures
  - Why needed here: VLCAP's design trades off end-to-end optimization for interpretability and modularity.
  - Quick check question: What information might be lost when decoupling visual recognition from generation that an end-to-end model could preserve?

- Concept: Prompt construction for VLMs
  - Why needed here: The retrieved labels are transformed into Arabic prompts; prompt quality directly affects generation.
  - Quick check question: If your retrieved labels include both relevant and irrelevant terms, how should you structure the prompt to emphasize useful information?

## Architecture Onboarding

- Component map: Visual Vocabulary Builder → Label Extractor → Prompt Constructor → Caption Generator
- Critical path: Image → Label Extractor → Prompt Constructor → Caption Generator → Output. Vocabulary quality and label relevance are bottlenecks.
- Design tradeoffs:
  - mCLIP: Better lexical alignment (higher BLEU/cosine) but may miss culturally specific concepts.
  - AraCLIP: Better human/LLM preference but lower lexical scores—potentially more natural but less literal.
  - Jina V4: Balanced but not best on any single metric.
  - Gemini vs. Qwen: Gemini favors semantic coherence; Qwen with AraCLIP favors human-judged relevance.
- Failure signatures:
  - Low BLEU but high LLM-judge: Captions may be fluent but lexically divergent from references (acceptable for some use cases).
  - High BLEU but low LLM-judge: Captions may be literal but culturally or contextually poor.
  - Empty or generic labels retrieved: Vocabulary gap—consider domain-specific vocabulary expansion.
- First 3 experiments:
  1. **Baseline comparison**: Run all six configurations on a held-out validation set; log BLEU, cosine similarity, and LLM-judge scores to identify Pareto-optimal combinations.
  2. **Ablation on k**: Vary top-k (e.g., 10, 25, 50 labels) and measure impact on caption quality and latency; too few labels may miss concepts, too many may introduce noise.
  3. **Error analysis on label retrieval**: Manually inspect 50 images where caption quality is low; categorize whether failures stem from missing vocabulary terms, wrong label ranking, or VLM