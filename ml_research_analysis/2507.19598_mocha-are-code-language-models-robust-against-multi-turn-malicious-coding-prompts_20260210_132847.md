---
ver: rpa2
title: 'MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding
  Prompts?'
arxiv_id: '2507.19598'
source_url: https://arxiv.org/abs/2507.19598
tags:
- code
- prompts
- malicious
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOCHA, a benchmark designed to evaluate the
  robustness of code language models against adversarial and multi-turn malicious
  prompts. The authors propose a novel code decomposition attack framework, where
  harmful tasks are broken into benign-looking subtasks across multiple conversational
  turns to evade safety filters.
---

# MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?

## Quick Facts
- arXiv ID: 2507.19598
- Source URL: https://arxiv.org/abs/2507.19598
- Authors: Muntasir Wahed; Xiaona Zhou; Kiet A. Nguyen; Tianjiao Yu; Nirav Diwan; Gang Wang; Dilek Hakkani-TÃ¼r; Ismini Lourentzou
- Reference count: 32
- Code language models show significant vulnerabilities to multi-turn malicious prompts, with rejection rates as low as 13.0% across threat categories.

## Executive Summary
This paper introduces MOCHA, a benchmark designed to evaluate code language models' robustness against adversarial and multi-turn malicious prompts. The authors propose a code decomposition attack framework where harmful tasks are broken into benign-looking subtasks across multiple conversational turns to evade safety filters. MOCHA includes 10.5K high-fidelity malicious coding prompts spanning 13 threat categories and both single- and multi-turn attacks. Empirical results show that both open- and closed-source models exhibit significant vulnerabilities, especially under multi-turn scenarios, with rejection rates ranging from 13.0% to 54.5%. Fine-tuning models using MOCHA improves rejection rates by up to 32.4% without additional supervision while maintaining coding utility.

## Method Summary
The MOCHA benchmark introduces a novel code decomposition attack framework that breaks harmful tasks into multiple benign-looking subtasks across conversational turns. The authors create 10.5K malicious coding prompts spanning 13 threat categories, including code obfuscation, backdoors, credential theft, and spam generation. The evaluation methodology includes human-annotated labels, coverage metrics, and generalizability tests against external adversarial datasets. The paper demonstrates that multi-turn attacks are significantly more effective than single-turn attacks, with models showing vulnerability gaps of 10-30 percentage points. MOCHA fine-tuning is shown to improve safety without sacrificing coding utility, achieving up to 32.4% improvement in rejection rates.

## Key Results
- Both open- and closed-source code language models show significant vulnerabilities to multi-turn malicious prompts, with rejection rates ranging from 13.0% to 54.5%
- Multi-turn attacks are 10-30 percentage points more effective than single-turn attacks in bypassing safety filters
- MOCHA fine-tuning improves rejection rates by up to 32.4% without additional supervision and maintains coding utility
- The benchmark generalizes well to external adversarial datasets, demonstrating effectiveness in equipping models to handle unseen threats

## Why This Works (Mechanism)
The effectiveness of MOCHA stems from its code decomposition attack framework, which exploits the sequential nature of conversational interactions. By breaking malicious tasks into multiple benign-looking subtasks, the attack evades traditional safety filters that typically inspect individual prompts rather than conversational context. The multi-turn approach leverages the model's tendency to maintain context across interactions, allowing harmful code generation to emerge only after several seemingly innocent exchanges. This temporal decomposition makes detection significantly more challenging than single-turn attacks.

## Foundational Learning
**Code Language Models**: AI models trained on programming code that can generate, complete, and analyze code snippets.
- Why needed: Understanding the target system's capabilities and limitations
- Quick check: Can the model generate functional code from natural language descriptions?

**Adversarial Prompting**: Techniques that manipulate model outputs through carefully crafted inputs to produce unintended behaviors.
- Why needed: The attack methodology relies on understanding how to craft prompts that bypass safety mechanisms
- Quick check: Does the model reject obviously harmful prompts while accepting obfuscated versions?

**Multi-turn Conversations**: Sequential interactions where context is maintained across multiple exchanges.
- Why needed: The core attack strategy exploits temporal decomposition across conversation turns
- Quick check: Can the model maintain context and build upon previous responses?

**Safety Filters**: Mechanisms designed to prevent models from generating harmful or malicious content.
- Why needed: Understanding what defenses are being evaded is crucial for evaluating attack effectiveness
- Quick check: What percentage of single-turn malicious prompts are rejected?

**Code Obfuscation**: Techniques that make code difficult to understand while maintaining functionality.
- Why needed: Many malicious coding tasks involve generating obfuscated code to hide harmful intent
- Quick check: Can the model detect obfuscated malicious code patterns?

## Architecture Onboarding

**Component Map**: User Prompts -> Model Safety Filter -> Code Generation -> Output Evaluation
- User interaction initiates multi-turn conversation
- Safety filter attempts to detect malicious intent
- Code generation produces output based on prompts
- Evaluation measures rejection rates and harm potential

**Critical Path**: Prompt Decomposition -> Multi-turn Interaction -> Context Building -> Malicious Output Generation
- Attacker decomposes harmful task into benign subtasks
- Model maintains context across multiple turns
- Accumulated context enables generation of harmful code
- Output evaluation reveals safety filter bypass

**Design Tradeoffs**: Safety vs. Utility balance
- Stricter safety filters may reduce legitimate coding assistance
- Multi-turn monitoring requires additional computational resources
- Fine-tuning for safety may impact general coding performance

**Failure Signatures**: 
- Low rejection rates for multi-turn attacks (13.0-54.5%)
- Significant performance gap between single-turn and multi-turn scenarios
- Model continuation of harmful tasks despite partial context awareness

**First Experiments**:
1. Test baseline model rejection rates on single-turn vs. multi-turn malicious prompts
2. Evaluate MOCHA fine-tuning impact on both safety and coding utility
3. Measure generalizability to external adversarial datasets

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the long-term effectiveness of MOCHA-based defenses against evolving attack strategies, the potential for adaptive attackers to develop new obfuscation techniques, and the real-world deployment challenges of balancing safety improvements with coding utility. The authors also note the need for broader validation across diverse coding tasks and model architectures.

## Limitations
- Adaptive attackers may develop new obfuscation techniques that circumvent MOCHA-based defenses
- Human judgment introduces subjectivity and potential inconsistencies in categorizing malicious intent
- Real-world deployment effectiveness beyond controlled experiments remains untested
- The benchmark may not capture all possible multi-turn malicious coding strategies

## Confidence

**High Confidence**: Empirical results demonstrating model vulnerabilities are robust, supported by extensive testing across multiple models and prompt variations. The observed improvement in rejection rates (up to 32.4%) through MOCHA fine-tuning is well-documented and reproducible.

**Medium Confidence**: Generalizability claim to external adversarial datasets is supported by experimental evidence but may not fully account for all possible attack variations. Effectiveness of the decomposition attack framework is validated but its completeness in representing real-world attack strategies is uncertain.

**Medium Confidence**: Assertion that MOCHA maintains coding utility while improving safety is based on internal benchmarks but requires broader validation across diverse coding tasks and model architectures.

## Next Checks
1. **Long-term Robustness Testing**: Conduct extended evaluations over multiple months to assess whether MOCHA fine-tuning maintains effectiveness against evolving attack strategies and newly discovered prompt engineering techniques.

2. **Real-world Deployment Assessment**: Implement MOCHA-enhanced models in production coding environments to evaluate performance trade-offs between safety and utility under actual user interactions and diverse coding scenarios.

3. **Cross-model Generalization Study**: Test MOCHA's effectiveness across a wider range of code language models, including smaller and specialized models, to determine if the safety improvements scale proportionally with model size and architecture differences.