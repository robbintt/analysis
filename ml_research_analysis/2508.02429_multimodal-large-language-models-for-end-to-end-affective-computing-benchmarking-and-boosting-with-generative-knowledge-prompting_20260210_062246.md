---
ver: rpa2
title: 'Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking
  and Boosting with Generative Knowledge Prompting'
arxiv_id: '2508.02429'
source_url: https://arxiv.org/abs/2508.02429
tags:
- audio
- arxiv
- mllms
- multimodal
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks state-of-the-art open-source MLLMs on six
  multimodal affective computing datasets covering sentiment analysis, emotion recognition,
  and humor detection. The evaluation compares MLLMs with traditional methods, revealing
  significant performance variability across tasks and datasets.
---

# Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting

## Quick Facts
- arXiv ID: 2508.02429
- Source URL: https://arxiv.org/abs/2508.02429
- Reference count: 40
- This paper benchmarks state-of-the-art open-source MLLMs on six multimodal affective computing datasets covering sentiment analysis, emotion recognition, and humor detection.

## Executive Summary
This paper benchmarks state-of-the-art open-source MLLMs on six multimodal affective computing datasets covering sentiment analysis, emotion recognition, and humor detection. The evaluation compares MLLMs with traditional methods, revealing significant performance variability across tasks and datasets. To improve MLLM performance, the authors propose a hybrid approach combining generative knowledge prompting (using zero-shot extraction of audio/video descriptions) with supervised fine-tuning. This strategy achieves notable improvements in accuracy and F1 scores across multiple tasks, especially in multi-class classification. Ablation studies show that dataset balance and modality dominance influence performance, with text generally being most effective.

## Method Summary
The study evaluates seven open-source MLLMs (HumanOmni, Qwen2.5Omni, VideoLLaMA2-AV, Ola, MiniCPM-o, PandaGPT, Emotion-LLaMA) on six multimodal affective computing datasets using SFT with LoRA fine-tuning. The proposed boosting strategy involves two stages: first generating descriptive text from audio/video inputs via zero-shot inference, then concatenating these descriptions with original text and fine-tuning the LLM. Models use different audio encoders (Whisper vs BEATs), with Whisper-based models showing superior audio-only performance. Training uses 4× RTX 4090 48G GPUs with FlashAttention-2 and BF16, with two-stage training for some models and single-stage LoRA for others.

## Key Results
- MLLMs significantly outperform traditional methods on multimodal sentiment analysis, with HumanOmni achieving 73.4% Acc7 on CMU-MOSI
- Generative knowledge prompting improves multi-class classification accuracy by 1-2% across multiple datasets
- Text modality dominates performance, with MLLMs leveraging their strong language understanding on text-dominant datasets
- Audio encoder choice critically impacts performance, with Whisper-based models outperforming BEATs-based models in audio-only tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generative Knowledge Prompting (GKP) enhances affective computing performance by explicitly extracting and integrating latent semantic cues from audio and video into the textual context before fine-tuning.
- **Mechanism:** The model first uses zero-shot inference to convert raw audio/video into descriptive text (e.g., "shouting," "concerned expression"). These descriptions are concatenated with the original text. During subsequent Supervised Fine-Tuning (SFT), the LLM learns to attend to these explicit semantic descriptions rather than relying solely on implicit feature alignment from the encoders.
- **Core assumption:** The MLLM possesses sufficient zero-shot capability to accurately describe affective cues (e.g., tone of voice, facial changes) in the description generation phase.
- **Evidence anchors:** [Abstract]: "...extracts descriptive cues from audio and video inputs and integrates them into model training, significantly improving performance—for example, increasing Acc7 from 53.9 to 55.9..." [Section III-C]: "...this approach helps the MLLM pay more refined attention to affective cues."

### Mechanism 2
- **Claim:** The choice of audio encoder pre-training objective (specifically speech-to-text) critically determines the model's effectiveness in unimodal audio sentiment analysis.
- **Mechanism:** Models utilizing Whisper-based audio encoders (trained on speech-to-text) outperform those using general audio encoders (like BEATs or ImageBind) in audio-only sentiment tasks. The mechanism is likely that speech-to-text training forces the encoder to capture prosodic and phonetic details closely aligned with semantic meaning, which transfers better to sentiment tasks than generic acoustic event detection.
- **Core assumption:** The alignment between the audio feature space and the LLM's semantic space is tighter when the encoder is already trained to map audio to language tokens.
- **Evidence anchors:** [Section V-D]: "Deep analysis reveals that all three models [HumanOmni, Qwen2.5Omni, MiniCPM-o] employ Whisper as the audio encoder... performance in the standalone audio modality is comparable to that of the text modality."

### Mechanism 3
- **Claim:** MLLMs leverage text-modality dominance to achieve high benchmark scores on datasets where text is the primary sentiment carrier, making architecture less critical than dataset modality balance.
- **Mechanism:** On datasets like CMU-MOSI (dominated by text), the LLM backbone's strong language comprehension drives performance. Conversely, on datasets with balanced modality contributions (CH-SIMS), the quality of cross-modal fusion becomes the differentiating factor.
- **Core assumption:** The LLM backbone is sufficiently pre-trained to handle sentiment reasoning solely from text tokens provided in the prompt.
- **Evidence anchors:** [Section V-A]: "...attributed to the dominant role of the text modality in this dataset... MLLMs can fully leverage their robust language understanding."

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The study fine-tunes large models (7B parameters) on consumer-grade GPUs (RTX 4090). LoRA allows updating only a small percentage of parameters (0.X%), making the benchmark feasible without full parameter updates.
  - **Quick check question:** Can you explain why freezing the original weight matrix $W$ and injecting trainable rank-decomposition matrices $A$ and $B$ reduces memory overhead during backpropagation?

- **Concept: Zero-Shot Description Generation**
  - **Why needed here:** The proposed "boosting" strategy relies on the model's ability to describe what it sees/hears without specific training labels (zero-shot). Understanding the limits of this capability is key to knowing why the "Optimized" strategy works.
  - **Quick check question:** How does the model's zero-shot capability differ from its fine-tuned state, and why is the zero-shot mode used for *generating* the descriptions rather than the final prediction?

- **Concept: Multimodal Alignment / Projection**
  - **Why needed here:** The paper evaluates models with different connectors (e.g., MLP2xGeLU vs. STC Convolution). Understanding how Vision/Audio encoders map features into the LLM's embedding space is necessary to interpret the performance differences between architectures like HumanOmni and VideoLLaMA2.
  - **Quick check question:** Why might a simple linear projection fail to capture temporal dynamics in video compared to a Spatial-Temporal Convolution (STC) connector?

## Architecture Onboarding

- **Component map:** Raw Video Frames + Raw Audio Waveform + Text Tokenizer -> Visual (SigLIP/CLIP) and Audio (Whisper/BEATs) Encoders -> Projectors (MLP/STC) -> LLM Backbone (Qwen2.5/Llama-2) -> Knowledge Loop (Novel): Zero-shot description generator -> Text concatenation -> SFT

- **Critical path:**
  1. Feature Extraction: Audio/Video -> Encoders -> Tensors
  2. Knowledge Generation (Hybrid Strategy): Run inference on Audio/Video Tensors to generate text descriptions ("Audio Description", "Video Description")
  3. Prompt Construction: Concatenate [Text] + [Audio Description] + [Video Description]
  4. SFT: Train the LLM (via LoRA) using the concatenated prompt to predict sentiment labels

- **Design tradeoffs:**
  - Two-Stage vs. Single-Stage Training: Some models (VideoLLaMA2) use a two-stage approach (align first, then LLM), while others (Qwen2.5Omni) use end-to-end LoRA. Two-stage offers better modality alignment but adds complexity.
  - Prompt Redundancy: Adding descriptions to distinct datasets (CMU-MOSEI) showed marginal gains or potential interference ("information redundancy"), suggesting the tradeoff is only favorable when the base modalities are ambiguous.

- **Failure signatures:**
  - Majority Class Collapse: On CMU-MOSEI, the model outputs the label "0" (Neutral) for almost all inputs due to the 21.7% label imbalance combined with the model's prior bias.
  - Audio-Visual Disconnect: If the video description is generic (e.g., "A person sitting") but the audio is emotional (e.g., "Screaming"), the text-dominant LLM might prioritize the generic video description, diluting the sentiment signal.

- **First 3 experiments:**
  1. Unimodal Stress Test: Run HumanOmni on CH-SIMS using only Audio, only Video, and only Text to verify encoder robustness (replicate Figure 3). *Goal:* Ensure the audio encoder (Whisper) is actually contributing, not just the text.
  2. Knowledge Ablation: Train the model with the proposed hybrid strategy (Prompt+SFT), then train a baseline (SFT only), and finally a "Noisy Knowledge" version (shuffling descriptions). *Goal:* Isolate the utility of the generative descriptions.
  3. Imbalance Sensitivity: Fine-tune on a subset of CMU-MOSEI with rebalanced classes vs. the original distribution. *Goal:* Confirm if the "Acc2" failure is purely data-driven or architecture-dependent.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does the generative knowledge prompting strategy yield marginal improvements or potential interference on datasets like CMU-MOSEI and CH-SIMS v2?
- **Basis in paper:** [inferred] Page 6 notes improvements were marginal on CMU-MOSEI and CH-SIMS v2, hypothesizing that "additional descriptive information... may even slightly interfere" due to redundancy when features are already distinct.
- **Why unresolved:** The paper identifies the phenomenon but does not isolate the specific features of these datasets that cause redundancy nor proposes a mechanism to filter or select when knowledge prompting is beneficial.
- **What evidence would resolve it:** Ablation studies quantifying the semantic overlap between generated descriptions and raw modalities, or experiments testing retrieval-based versus generative knowledge on these specific datasets.

### Open Question 2
- **Question:** How can MLLM fine-tuning be adapted to mitigate the bias towards majority classes (e.g., predicting "neutral" labels) when trained on imbalanced datasets?
- **Basis in paper:** [inferred] Page 5 identifies that a "significant issue of samples distribution imbalance" (21.7% labeled as 0) on CMU-MOSEI causes models to "tend to output 0 labels," degrading binary accuracy.
- **Why unresolved:** The study highlights this bias as a key failure mode but relies on standard LoRA fine-tuning without testing intervention methods like re-weighting or data augmentation to correct the imbalance.
- **What evidence would resolve it:** Experiments applying class-balanced loss functions or resampling strategies during the LoRA fine-tuning phase on the CMU-MOSEI dataset.

### Open Question 3
- **Question:** To what extent does the pre-training objective of specific audio encoders (e.g., speech-to-text vs. general audio) limit the extraction of affective cues compared to visual or textual modalities?
- **Basis in paper:** [inferred] Page 7 concludes that the "degree of adaptation between the pre-training tasks of the audio encoder and downstream sentiment analysis tasks directly affects performance," noting Whisper (speech-to-text) aligns well with text but may miss non-verbal cues.
- **Why unresolved:** The benchmark compares models with fixed encoders (Whisper vs. BEATs) but does not decouple the encoder architecture from the fusion mechanism to isolate the causal factors of performance gaps.
- **What evidence would resolve it:** A controlled study swapping audio encoders (e.g., Whisper vs. BEATs) within the same baseline MLLM architecture to measure the performance delta on audio-centric tasks.

## Limitations

- **Dataset Scope and Generalization:** The evaluation covers six datasets with varying modality dominance and label distributions, which may not generalize to all affective computing scenarios.
- **Modality-Specific Encoder Performance:** The analysis reveals significant performance differences based on audio encoder choice but lacks deeper analysis of the underlying mechanisms.
- **Zero-Shot Description Quality:** The effectiveness of generative knowledge prompting hinges on the zero-shot capability of the MLLM to accurately describe audio and video content, but the quality of these descriptions is not validated.

## Confidence

**High Confidence:** The finding that text-dominant datasets (CMU-MOSI) yield higher MLLM performance due to strong language understanding is well-supported by consistent results across multiple models and metrics.

**Medium Confidence:** The superiority of Whisper-based audio encoders over BEATs-based encoders is supported by empirical results but lacks deeper analysis of the underlying mechanisms.

**Low Confidence:** The claim that generative knowledge prompting consistently improves performance is the least robust, with mixed results across datasets and inadequate explanation of when and why it succeeds or fails.

## Next Checks

1. **Description Quality Validation:** Implement an automated evaluation of the zero-shot generated descriptions to measure their relevance to affective content. Compare description quality scores against downstream performance to establish whether better descriptions correlate with better results.

2. **Cross-Dataset Generalization Study:** Apply the exact same fine-tuning and knowledge prompting strategy across all six datasets using identical hyperparameters. Measure performance variance and conduct statistical significance testing to determine whether improvements are consistent or dataset-specific.

3. **Encoder Architecture Ablation:** Conduct a controlled experiment replacing Whisper with BEATs in HumanOmni while keeping all other components constant. Similarly, replace BEATs with Whisper in VideoLLaMA2-AV. Measure the isolated impact of encoder choice on audio-only and multimodal performance.