---
ver: rpa2
title: Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge
arxiv_id: '2510.18196'
source_url: https://arxiv.org/abs/2510.18196
tags:
- score
- b-inst
- contrastive
- range
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new bias in LLM-as-a-judge systems called
  "score range bias," where judges produce outputs that are sensitive to the predefined
  score ranges, leading to inconsistent evaluations. The bias is observed across different
  model sizes and families (Llama-3 and Qwen-2.5), with models tending to favor specific
  scores regardless of the quality of the summaries.
---

# Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge

## Quick Facts
- **arXiv ID**: 2510.18196
- **Source URL**: https://arxiv.org/abs/2510.18196
- **Reference count**: 35
- **Key outcome**: Contrastive decoding mitigates score range bias in LLM-as-a-judge systems, achieving up to 11.3% relative improvement in Spearman correlation with human judgments across different score ranges.

## Executive Summary
This paper identifies a new bias in LLM-as-a-judge systems called "score range bias," where judges produce outputs that are sensitive to the predefined score ranges, leading to inconsistent evaluations. The bias is observed across different model sizes and families (Llama-3 and Qwen-2.5), with models tending to favor specific scores regardless of the quality of the summaries. The paper proposes using contrastive decoding, motivated by the observation that similar biases exist across models from the same family, to mitigate this issue. Contrastive decoding adjusts the model outputs by combining a main model and an assistant model, canceling out the shared biases.

## Method Summary
The paper proposes contrastive decoding to mitigate score range bias in LLM-as-a-judge systems. The method uses a main model and an assistant model to generate scores, then adjusts the main model's logits by subtracting weighted assistant model logits (log p_main − λ log p_asst). The approach is tested on the SummEval benchmark using Llama-3.1 and Qwen-2.5 models for coherence, relevance, and consistency evaluation across four score ranges (0-4, 1-5, 2-6, 3-7). Hyperparameters λ and temperature are tuned per range using a 10% held-out development set.

## Key Results
- Score range bias causes models to disproportionately favor specific scores (e.g., Llama-3 models favor 4, Qwen-2.5 models favor 2)
- Contrastive decoding achieves up to 11.3% relative improvement in Spearman correlation with human judgments
- The approach works across different model sizes and families, with improvements observed for both greedy and contrastive decoding methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM judges exhibit score range bias—systematic preferences for specific scores that persist regardless of input quality.
- **Mechanism:** When prompted to score within a range (e.g., 2-6), models develop token-level priors for certain numerals. In the paper, Llama-3 models disproportionately output "4" while Qwen-2.5 models favor "2" (Figure 2). This occurs because next-token prediction is influenced by the presence of specific tokens in the prompt, creating distributional skews unrelated to actual quality assessment.
- **Core assumption:** The bias manifests at the logit level and is consistent across inference instances.
- **Evidence anchors:**
  - [abstract] "LLM judge outputs are highly sensitive to pre-defined score ranges"
  - [section 4.2] "Llama family models (3B and 8B) tend to output score of 4... Qwen 2.5 family models tend to output score of 2"
  - [corpus] Paper 114138 ("Exploring the Effects of Alignment on Numerical Bias") confirms LLM evaluators exhibit numerical bias where "certain evaluation scores are generated disproportionately often"
- **Break condition:** If score distributions matched human annotation distributions (e.g., Figure 2 human bars), the mechanism would not apply.

### Mechanism 2
- **Claim:** Models from the same family encode similar score range biases, enabling cross-model cancellation.
- **Mechanism:** Shared training data, architecture, and tokenization create correlated logit patterns across model sizes. Figure 3 shows Qwen-2.5 3B, 7B, and 14B all have highest logits for Score 2, though the bias magnitude decreases with scale (max logit: 3B≈25, 7B≈30, 14B≈34). This correlation allows one model's bias to predict and cancel another's.
- **Core assumption:** Family-shared biases are directionally aligned (same favored score) even if magnitude differs.
- **Evidence anchors:**
  - [abstract] "similar biases exist among models from the same family"
  - [section 4.2] "Qwen-2.5 3B, 7B, 8B, and 14B models encode similar biases where Score 2 is the highest"
  - [corpus] Paper 67757 demonstrates related bias structures in vision-language models, suggesting shared encoder components propagate bias
- **Break condition:** If different family members favored different scores (e.g., 3B prefers 2, 7B prefers 4), contrastive decoding would amplify rather than cancel bias.

### Mechanism 3
- **Claim:** Contrastive decoding cancels shared bias by subtracting weighted assistant model logits from main model logits.
- **Mechanism:** The formula `log p_main − λ log p_asst` amplifies differences while suppressing commonalities. When both models over-assign probability mass to Score 2, subtraction reduces that peak, redistributing probability toward underrepresented scores. The λ parameter aligns logit magnitude differences (smaller models have lower max logits).
- **Core assumption:** The assistant model's bias direction matches the main model's, but its magnitude is controllable via λ.
- **Evidence anchors:**
  - [abstract] "contrastive decoding... achieving up to 11.3% relative improvement on average in Spearman correlation"
  - [section 3] Equation 1 and λ inclusion motivated by logit range differences across model sizes
  - [corpus] Paper 112747 ("SDCD") applies contrastive decoding to hallucination mitigation, showing the approach generalizes beyond scoring tasks
- **Break condition:** If λ is poorly tuned (too high: overcorrection destroys signal; too low: insufficient bias removal), correlation degrades.

## Foundational Learning

- **Concept: Logit-level decoding manipulation**
  - Why needed here: Contrastive decoding operates on pre-softmax logits, not output text. Understanding that `p = softmax(logits/temperature)` is essential for tuning λ and t.
  - Quick check question: If temperature increases from 1.0 to 5.0, does the output distribution become sharper or flatter?

- **Concept: Correlation metrics for evaluation alignment**
  - Why needed here: The paper uses Pearson, Spearman, and Kendall to measure judge-human agreement. Spearman captures rank correlation robust to outliers—critical when score distributions are skewed.
  - Quick check question: If a judge assigns scores [1,2,3,4,5] and humans assign [2,3,4,5,6], what is the Spearman correlation?

- **Concept: Direct assessment vs. pairwise comparison in LLM evaluation**
  - Why needed here: The paper targets direct assessment (single-output scoring), which is harder than pairwise (relative preference). Knowing why direct assessment underperforms motivates the bias investigation.
  - Quick check question: Why might pairwise comparison be more robust to score range bias than direct assessment?

## Architecture Onboarding

- **Component map:**
  Input (document + summary + rubric) → Main Model → logits_main → Contrastive Layer: adjusted_logits = logits_main - λ × logits_asst → Temperature scaling: adjusted_logits / t → Softmax → sampled score

- **Critical path:** The λ hyperparameter is the most sensitive—paper uses grid search over [0.01, 0.1, 0.5, 1.0] per range. Temperature t also requires per-range tuning (Tables 3-5).

- **Design tradeoffs:**
  - Assistant model selection: Smaller models (1B vs 3B) show marginal differences (Table 1), but smaller = faster inference. Trade-off: smaller models may have noisier bias patterns.
  - Score range selection: Contrastive decoding enables searching beyond 1-5, but requires re-tuning hyperparameters per range.
  - Inference cost: Two forward passes required; however, paper notes compatibility with speculative decoding (both use main+assistant architecture).

- **Failure signatures:**
  - Negative correlations (e.g., Qwen-3B in Table 2: Spearman = -0.221 for 0-4 range): indicates model is inverting quality judgments.
  - Zero correlation (Llama-1B for 2-6 range): model outputting constant score regardless of input.
  - High variance across ranges: if greedy decoding shows wildly different correlations per range but contrastive doesn't stabilize, λ tuning failed.

- **First 3 experiments:**
  1. **Reproduce score distribution shift:** Run greedy decoding on Llama-8B across all 4 ranges (0-4, 1-5, 2-6, 3-7). Plot score histograms. Confirm bias pattern (e.g., peak at 4 for Llama).
  2. **Validate family-shared bias:** Run same experiment on assistant model (Llama-3B). Check if histogram peaks align. If peaks differ, contrastive decoding won't work for this pair.
  3. **Contrastive decoding sweep:** Fix main=8B, assistant=3B. Grid search λ∈[0.01,0.1,0.5,1.0] and t∈[0.5,1.0,2.0] on held-out 10% dev set. Report Spearman per setting. Verify improvement over greedy baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- **Statistical Significance and Effect Size** - While the paper reports up to 11.3% relative improvement in Spearman correlation, the absolute correlation gains are modest (typically 0.02-0.05 points). The confidence intervals for these improvements are not reported, making it difficult to assess whether observed differences are statistically significant or could arise from random variation in the evaluation dataset.
- **Generalizability Beyond Summarization** - All experiments focus on single-document summarization evaluation. The score range bias mechanism may manifest differently for other tasks (reasoning, code generation, multi-document summarization) where the evaluation rubric structure varies substantially.
- **Hyperparameter Sensitivity** - The paper uses grid search for λ and temperature parameters, but the sensitivity of results to these choices is not thoroughly characterized. The reported improvements could be partially attributed to careful hyperparameter tuning rather than the contrastive decoding approach itself.

## Confidence

**High Confidence** (mechanism well-supported by evidence):
- Score range bias exists and affects multiple model families consistently
- Contrastive decoding can mathematically cancel shared biases when present
- The bias manifests at the logit level and is predictable across model scales

**Medium Confidence** (evidence present but limitations acknowledged):
- The magnitude of improvement (11.3%) generalizes across different score ranges
- Family-shared bias is the primary driver of contrastive decoding success
- The approach is compatible with speculative decoding acceleration

**Low Confidence** (claims with limited empirical support):
- Contrastive decoding would work equally well for non-summative tasks
- The method generalizes to other forms of LLM bias beyond score range bias
- The improvements would scale to significantly larger model sizes (>70B parameters)

## Next Checks

1. **Statistical Validation Study**: Run bootstrap resampling on the SummEval dataset to establish 95% confidence intervals for all reported correlation coefficients and their improvements. This will determine whether the 11.3% improvement is statistically distinguishable from noise.

2. **Cross-Task Generalization Test**: Apply the same methodology to at least two other LLM-as-a-judge tasks (e.g., code generation evaluation and mathematical reasoning grading) to verify whether score range bias manifests similarly and whether contrastive decoding provides comparable improvements.

3. **Ablation on Hyperparameter Tuning**: Fix λ and temperature parameters to their mean values across all ranges (rather than per-range tuning) and measure correlation degradation. This will quantify how much of the improvement comes from the contrastive decoding approach versus careful hyperparameter optimization.