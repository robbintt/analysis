---
ver: rpa2
title: Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification
  with Large Language Models
arxiv_id: '2511.05752'
source_url: https://arxiv.org/abs/2511.05752
tags:
- semantic
- text
- feature
- classification
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a hybrid text classification method integrating
  large language model feature extraction, multi-scale feature pyramid fusion, and
  graph neural network modeling. The approach addresses limitations of single-scale
  representations by capturing hierarchical semantic information and structured dependencies
  within text.
---

# Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models

## Quick Facts
- arXiv ID: 2511.05752
- Source URL: https://arxiv.org/abs/2511.05752
- Authors: Xiangchen Song; Yulin Huang; Jinxu Guo; Yuchen Liu; Yaxuan Luan
- Reference count: 25
- Key outcome: Accuracy 0.913, F1-Score 0.905, AUC 0.947, Precision 0.908 on AG News dataset

## Executive Summary
This study presents a hybrid text classification method that integrates large language model feature extraction, multi-scale feature pyramid fusion, and graph neural network modeling. The approach addresses limitations of single-scale representations by capturing hierarchical semantic information and structured dependencies within text. Experimental results on the AG News dataset demonstrate superior performance compared to baseline models, showing strong robustness across different learning rates.

## Method Summary
The method processes text through three key stages: (1) A large language model encoder extracts contextual hidden states from multiple layers, providing semantically rich features that capture long-distance dependencies; (2) A feature pyramid network fuses representations from different LLM layers to integrate semantic information at multiple scales, balancing global context with local details through learnable upsampling and lateral connections; (3) The fused features are transformed into graph structures where nodes represent semantic units and edges capture latent relations, followed by graph neural network layers that propagate and aggregate neighborhood information to capture non-sequential semantic dependencies.

## Key Results
- Achieved accuracy of 0.913, F1-score of 0.905, AUC of 0.947, and precision of 0.908 on AG News dataset
- Outperformed baseline models including BERT, Transformer, 1DCNN, and LSTM+CNN architectures
- Demonstrated strong robustness with optimal learning rate of 1×10⁻⁴, showing stable performance across hyperparameter variations

## Why This Works (Mechanism)

### Mechanism 1: LLM Contextual Feature Extraction
- Claim: Deep semantic representations from the LLM provide a feature foundation that captures long-distance dependencies better than single-scale methods
- Mechanism: The LLM encoder maps input sequences to hidden representations H = f_LLM(E), where contextual dependencies are encoded across token positions. These representations serve as input for subsequent multi-scale processing
- Core assumption: Pre-trained LLMs transfer semantic knowledge to classification tasks without domain-specific fine-tuning
- Evidence anchors:
  - [abstract]: "the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation"
  - [section III]: Formalization H = f_LLM(E) with hidden dimension h_d
  - [corpus]: Weak direct corpus evidence for this specific LLM-pyramid-GNN integration; related work focuses on parameter-efficient adaptation rather than frozen feature extraction
- Break condition: If the target domain vocabulary or structure diverges significantly from LLM pretraining corpora without adaptation

### Mechanism 2: Multi-Scale Feature Pyramid Fusion
- Claim: Feature pyramids integrate semantic information at different levels, balancing global context with local details
- Mechanism: Features from multiple LLM layers are fused via F_l = φ(W_l · H_l + Up(F_{l+1})), propagating high-level semantics downward while preserving lower-level granularity. The final representation F = Φ(F_1, ..., F_L) combines all scales
- Core assumption: Text contains hierarchical semantic structure (phrases, sentences, paragraphs) that single-scale models lose
- Evidence anchors:
  - [abstract]: "the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details"
  - [section I]: "phrases, sentences, and paragraphs belong to different semantic levels... A single-scale model may cause information loss"
  - [corpus]: MS-DGCNN++ uses parallel multi-scale processing for geometric structures; HGM-Net addresses hierarchical semantic coherence in patent text
- Break condition: If input texts are too short to exhibit meaningful multi-scale hierarchy

### Mechanism 3: GNN Structured Semantic Modeling
- Claim: Graph representations capture latent semantic relations and logical dependencies that sequential models miss
- Mechanism: Fused features are transformed into graph G = (V, E) where nodes are semantic units. GNN layers aggregate neighborhood information: h_v^(k+1) = σ(W·h_v^(k) + Σ c_uv·W·h_u^(k)). A readout function produces the final text representation
- Core assumption: Text contains non-sequential dependencies (synonymy, logical links) that benefit from explicit graph modeling
- Evidence anchors:
  - [abstract]: "graph neural networks are employed to capture latent semantic relations and logical dependencies in the text"
  - [section IV]: "the method proposed in this study achieves the best performance on all metrics... This advantage comes from the joint effect of deep feature extraction... multi-scale fusion... and structured modeling"
  - [corpus]: TF-DWGNet uses tensor fusion with GNN for multi-omics dependencies; corpus shows GNN effectiveness depends heavily on graph construction quality
- Break condition: If the graph construction method fails to encode meaningful semantic relationships (e.g., poor node definition or edge criteria)

## Foundational Learning

- Concept: Feature Pyramid Networks
  - Why needed here: Understanding how multi-scale fusion combines coarse global features with fine local details through up-sampling and lateral connections
  - Quick check question: Given features at two scales (global: 64-dim, local: 256-dim), how would you fuse them while preserving both types of information?

- Concept: Graph Neural Network Message Passing
  - Why needed here: Understanding how node representations update by aggregating neighbor information across graph layers
  - Quick check question: If a node has 3 neighbors with features [0.1, 0.5], [0.3, 0.2], [0.2, 0.4], what would a simple mean aggregation produce?

- Concept: LLM Hidden State Extraction
  - Why needed here: Understanding that different transformer layers capture different levels of abstraction (lower: syntax, higher: semantics)
  - Quick check question: Why might you want features from multiple layers rather than just the final layer?

## Architecture Onboarding

- Component map:
  - Input text → LLM Encoder → Multi-layer hidden states {H_1, ..., H_L}
  - Hidden states → Feature Pyramid (upsampling + fusion) → Fused representation F
  - F → Graph construction (nodes = semantic units, edges = semantic relations)
  - Graph → GNN layers (K iterations of message passing) → Node embeddings
  - Node embeddings → READOUT → Text vector z → Linear classifier → Predictions

- Critical path:
  1. LLM must produce semantically meaningful hidden states (garbage in, garbage out)
  2. Pyramid fusion must preserve both global and local signals (information bottleneck)
  3. Graph construction must encode real semantic relationships (structural failure cascades)
  4. GNN depth must balance expressiveness vs. over-smoothing

- Design tradeoffs:
  - More pyramid levels: richer hierarchy vs. increased memory and fusion complexity
  - Deeper GNN: longer-range dependency capture vs. over-smoothing where node features become indistinguishable
  - Graph sparsity: computational efficiency vs. missing important semantic edges
  - Learning rate: paper shows 1×10⁻⁴ optimal; 1×10⁻³ causes oscillation, 1×10⁻⁵ slows convergence

- Failure signatures:
  - Accuracy plateaus below baseline (BERT/Transformer): check if LLM features are frozen vs. fine-tuned; frozen may underperform on domain shift
  - High variance across learning rates: pyramid weight initialization may be unstable
  - GNN outputs converge to uniform values: reduce GNN depth or add residual connections
  - Short texts underperform: pyramid may add noise without meaningful hierarchy

- First 3 experiments:
  1. Ablation study: Run (a) LLM+Pyramid only, (b) LLM+GNN only, (c) full model to isolate each component's contribution on validation set
  2. Graph construction analysis: Visualize the constructed graphs for sample texts to verify edges capture meaningful semantic relations; if edges appear random, revise construction criteria
  3. Scale sensitivity test: Stratify test set by text length (short/medium/long) and measure performance gaps; expect pyramid benefits to scale with text complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating contrastive pretraining enable effective cross-domain adaptation for this framework?
- Basis in paper: [explicit] The conclusion proposes incorporating "contrastive pretraining" in future research to enable stronger "domain transferability by aligning semantic representations across heterogeneous corpora."
- Why unresolved: The current study validates the method solely on the AG News dataset, leaving the framework's ability to generalize to disparate domains like finance or medicine unverified
- What evidence would resolve it: Experimental results showing maintained classification accuracy when the model is transferred from the news domain to datasets with distinct semantic structures

### Open Question 2
- Question: How can the framework be optimized to handle the computational costs associated with long documents?
- Basis in paper: [explicit] The authors explicitly identify the need to design "lightweight or sparse GNN variants to reduce computational cost when processing long documents or large graphs."
- Why unresolved: While the method improves accuracy, the integration of Large Language Models with Graph Neural Networks suggests high resource intensity, limiting potential real-time application
- What evidence would resolve it: A comparative analysis of inference latency and memory usage against the baseline model on datasets with significantly longer text sequences

### Open Question 3
- Question: Does dynamic graph construction based on interaction saliency improve relational modeling over the current static approach?
- Basis in paper: [explicit] The conclusion suggests "dynamic graph construction based on interaction saliency" as a method to enhance relational modeling while maintaining interpretability
- Why unresolved: The paper does not detail how the current graph structure handles noise or irrelevant semantic links, nor does it quantify the benefit of dynamically adjusting edges
- What evidence would resolve it: Ablation studies comparing the performance of static versus saliency-based dynamic edges on texts with complex or ambiguous semantic dependencies

## Limitations
- The specific LLM architecture and layer selection for feature extraction are not detailed, requiring assumptions for reproduction
- Graph construction methodology (node definition and edge formation criteria) remains underspecified, critical for GNN performance
- Validation is limited to AG News dataset, raising questions about generalizability to other text classification domains
- Computational complexity of combining LLM feature extraction, multi-scale fusion, and GNN processing may limit practical deployment

## Confidence
- **High Confidence**: The core mechanism of multi-scale feature pyramid fusion integrating global and local semantic information is well-supported by both the theoretical framework and experimental results
- **Medium Confidence**: The claim that GNN effectively captures latent semantic relations beyond sequential modeling is supported by ablation studies but depends critically on graph construction quality, which is underspecified
- **Medium Confidence**: The LLM's ability to provide semantically rich features without domain-specific fine-tuning is plausible but unverified without knowing the specific LLM architecture and pretraining corpus alignment with AG News topics

## Next Checks
1. **Architecture Dependency Test**: Reproduce the complete pipeline using different LLM backbones (BERT, RoBERTa, DistilBERT) to determine whether performance gains are architecture-dependent or generic to the multi-scale + GNN framework
2. **Graph Construction Sensitivity**: Systematically vary graph construction parameters (node definition, edge criteria, sparsity) to quantify their impact on final performance and identify the minimum viable graph quality for the approach to work
3. **Domain Transfer Validation**: Apply the trained model to a different text classification dataset (e.g., IMDb reviews, 20 Newsgroups) without fine-tuning the LLM features to assess whether the frozen feature extraction approach generalizes beyond AG News