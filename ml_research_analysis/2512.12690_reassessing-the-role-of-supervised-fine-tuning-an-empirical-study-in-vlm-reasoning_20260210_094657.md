---
ver: rpa2
title: 'Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM
  Reasoning'
arxiv_id: '2512.12690'
source_url: https://arxiv.org/abs/2512.12690
tags:
- reasoning
- grpo
- data
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reexamines the prevailing belief that reinforcement
  learning (RL) is inherently superior to supervised fine-tuning (SFT) for improving
  reasoning capabilities in vision-language models (VLMs). Through a systematic, controlled
  comparison of SFT and RL using identical training data and matched optimization
  setups, the study finds that the relative effectiveness of SFT and RL is conditional,
  depending strongly on model capacity, data scale, and data distribution.
---

# Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning

## Quick Facts
- **arXiv ID:** 2512.12690
- **Source URL:** https://arxiv.org/abs/2512.12690
- **Reference count:** 40
- **Primary result:** SFT can match or exceed RL for reasoning in VLMs when data distribution matches base model and for weaker models.

## Executive Summary
This paper challenges the prevailing belief that reinforcement learning (RL) is inherently superior to supervised fine-tuning (SFT) for improving reasoning capabilities in vision-language models (VLMs). Through a systematic, controlled comparison of SFT and RL using identical training data and matched optimization setups, the study finds that the relative effectiveness of SFT and RL is conditional, depending strongly on model capacity, data scale, and data distribution. SFT is shown to be unexpectedly powerful for weaker models and highly data-efficient, achieving comparable or better reasoning performance with only 2K samples compared to RL with 20K. Additionally, SFT demonstrates stronger cross-modal transferability to pure-text tasks when trained on data with distributions similar to the base model. The study also identifies a pervasive issue of deceptive rewards in RL, where higher rewards do not necessarily correlate with improved reasoning accuracy. These findings challenge the "RL over SFT" narrative and highlight the need for a more balanced post-training pipeline that leverages the complementary strengths of both paradigms.

## Method Summary
The study compares SFT and RL (GRPO/DAPO) for multimodal reasoning in VLMs across model capacity, data scale, and transferability. Training uses ~50K SFT samples and ~30K RL samples derived from reasoning datasets (Geometry3K, GeoQA, TQA, etc.) via rejection sampling from strong VLMs. SFT uses LLaMA-Factory (batch_size=128, lr=1e-5, 3 epochs), while RL uses EasyR1 (batch_size=512, lr=1e-6, 2 epochs, 10 rollouts). Rewards combine accuracy (λ=0.9) and format compliance (λ=0.1). Models include Qwen2-VL-2B/7B and Qwen2.5-VL-3B/7B, evaluated on math reasoning benchmarks (MathVista, MathVision, MathVerse, We-Math) and cross-modal transfer tasks (GPQA, MATH-500, MMLU).

## Key Results
- SFT achieves comparable or better reasoning performance than RL with only 2K samples versus 20K RL samples
- SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs due to signal availability gaps in RL
- SFT demonstrates stronger cross-modal transferability when training data distribution matches the base model's pretraining distribution
- RL training exhibits deceptive rewards where higher reward values fail to correlate with improved reasoning accuracy

## Why This Works (Mechanism)

### Mechanism 1: Signal Availability Gap in RL for Weak Models
- Claim: SFT outperforms RL on weaker models because RL requires diverse response samples (mix of correct/incorrect) to generate meaningful gradient updates, which weak models fail to produce.
- Mechanism: In GRPO/DAPO, advantage estimation contrasts rewards across sampled trajectories. When a weak model generates predominantly incorrect responses, all trajectories receive similar low rewards, producing vanishing or unstable gradients. SFT bypasses this by directly providing correct reasoning trajectories for imitation, requiring no exploration.
- Core assumption: The signal availability explanation is inferred from performance patterns rather than directly measured through gradient analysis.
- Evidence anchors:
  - [abstract] "SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs"
  - [section] Page 6: "weaker models produce predominantly incorrect responses, yielding little to no positive feedback for policy updates. This scarcity of valid learning signals results in unstable or vanishing gradients"
  - [corpus] Not directly validated in neighbor papers; related work discusses SFT/RL comparisons but doesn't address signal mechanism
- Break condition: If weak models can be prompted or initialized to produce sufficiently diverse responses, RL may match SFT performance

### Mechanism 2: Distribution Alignment Preserves Cross-Modal Transfer
- Claim: SFT maintains cross-modal transferability when training data distribution matches the base model's pretraining distribution, preventing catastrophic forgetting.
- Mechanism: SFT data distilled from the same model family (Qwen2.5-VL-7B/32B → Qwen2.5-VL-3B/7B) maintains distributional proximity, reducing interference with pretrained knowledge. Heterogeneous data sources (e.g., Eureka-Distill) cause accuracy drops on OOD benchmarks.
- Core assumption: Distributional similarity is the causal factor; alternative explanations (data quality, reasoning style) not fully ruled out.
- Evidence anchors:
  - [abstract] "SFT demonstrates stronger cross-modal transferability to pure-text tasks when trained on data matching the base model's distribution"
  - [section] Figure 6(a): QwenVL-Distill preserves HallBench/MMBench-EN performance while Eureka-Distill degrades it
  - [corpus] Weak direct validation; neighbor papers don't examine distribution effects
- Break condition: If base model has robust anti-forgetting mechanisms (e.g., experience replay, elastic weight consolidation), heterogeneous data may not cause degradation

### Mechanism 3: Reward-Accuracy Decoupling (Deceptive Rewards)
- Claim: RL training can exhibit reward overfitting where optimization increases reward values without improving—and sometimes degrading—actual reasoning accuracy.
- Mechanism: Verifiable rewards create pressure toward surface patterns that trigger the reward function (e.g., output format compliance, partial answer matching) without genuine reasoning. SFT cold-start initialization constrains the policy to reasonable trajectories before RL exploration, reducing reward hacking.
- Core assumption: The divergence indicates overfitting rather than benchmark-reward misalignment or evaluation artifact.
- Evidence anchors:
  - [abstract] "higher reward values fail to correlate with improved reasoning accuracy"
  - [section] Figure 7: Reward curves increase monotonically while MathVerse/We-Math accuracy plateaus then declines after ~100 steps
  - [corpus] Not examined in neighbor papers; this appears novel to this study
- Break condition: If reward function perfectly captures reasoning quality (e.g., process-based verification), deceptive reward behavior may be eliminated

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: Core RL algorithm compared against SFT; understanding its group-based advantage estimation explains why weak models struggle (need diverse samples for contrast)
  - Quick check question: Given 8 sampled responses from a weak model where 7/8 are incorrect, how would the advantage signal quality differ from a strong model producing 4/8 correct?

- **Concept: Verifiable Rewards (RLVR)**
  - Why needed here: Explains how rewards are computed (accuracy + format) and why they can become decoupled from actual reasoning capability
  - Quick check question: If a model learns to output correct format but wrong answers, would the reward function (λ=0.9 accuracy, 0.1 format) catch this? What about the inverse?

- **Concept: Distribution Shift in Fine-Tuning**
  - Why needed here: Central to understanding why SFT data source matters for cross-modal transfer; mismatched distributions cause forgetting
  - Quick check question: Why might SFT data generated by a 32B model cause more degradation when training a 3B model vs. data from a 7B model?

## Architecture Onboarding

- **Component map:**
  Base VLM (Qwen2-VL/Qwen2.5-VL) → SFT Path (LLaMA-Factory, 50K samples, 3 epochs) or RL Path (EasyR1/GRPO, 30K samples, 2 epochs) → Reasoning Model or SFT+RL Combined

- **Critical path:**
  1. Select appropriate training paradigm based on model capacity (SFT for <7B, RL viable for ≥7B)
  2. If using RL, verify response diversity before committing (sample 10 responses per prompt, check correct/incorrect ratio)
  3. Match SFT data distribution to base model family when cross-modal transfer is required
  4. Monitor reward-accuracy correlation during RL; stop if divergence appears

- **Design tradeoffs:**
  - SFT-only: Higher data efficiency (2K ≈ 20K RL), better for weak models, faster iteration; but plateaus at scale
  - RL-only: Better scaling with data/model size, exploration benefits; but requires strong base, susceptible to reward hacking
  - SFT→RL: Best of both for strong models; adds complexity, cold-start benefit diminishes for very strong bases (<1% gain on Qwen2.5-VL-7B)

- **Failure signatures:**
  - RL reward increasing but benchmark accuracy flat/declining → reward overfitting
  - Strong training performance, weak OOD transfer → distribution mismatch in SFT data
  - RL gradients unstable/vanishing → insufficient response diversity (weak model symptom)
  - SFT+RL underperforms SFT-only → likely on already-saturated model or hyperparameter mismatch

- **First 3 experiments:**
  1. **Baseline capacity check:** Run base model on 4 math benchmarks (MathVista, MathVision, MathVerse, We-Math) to establish capability ranking; if avg <40%, prioritize SFT
  2. **Data efficiency probe:** Train SFT with 2K samples vs. GRPO with 20K samples on same data subset; expect SFT to match or exceed RL on weaker models
  3. **Reward-accuracy tracking:** During GRPO training, log both training reward and held-out benchmark accuracy every 20 steps; plot correlation to detect deceptive reward onset

## Open Questions the Paper Calls Out
None

## Limitations
- Conclusions depend heavily on specific training pipeline and data preparation procedures not fully specified
- Unknown exact details of rejection sampling process used to generate reasoning trajectories
- Potential bias in how "weaker" vs "stronger" models are defined
- Lack of ablation studies isolating effects of data distribution versus data quality

## Confidence

- **High confidence**: SFT's superior data efficiency (2K samples achieving results comparable to 20K RL samples) and its effectiveness for weaker models are strongly supported by the controlled experiments.
- **Medium confidence**: The distributional alignment explanation for cross-modal transfer requires further validation, as alternative explanations (data quality, reasoning style) are not fully ruled out.
- **Medium confidence**: The reward-accuracy decoupling finding is well-documented in the results, but the exact threshold where overfitting occurs and its dependence on reward function design remains unclear.

## Next Checks
1. **Gradient signal analysis**: For weak models, measure the distribution of advantage values during GRPO training to directly quantify signal availability, rather than inferring from performance patterns.
2. **Distributional control experiment**: Train SFT models on identically high-quality data but with deliberately mismatched distributions to isolate distribution effects from data quality effects on cross-modal transfer.
3. **Reward function ablation**: Test alternative reward formulations (e.g., process-based rewards, multi-reward combinations) to determine whether deceptive rewards are inherent to RLVR or specific to the λ=0.9 accuracy weighting used.