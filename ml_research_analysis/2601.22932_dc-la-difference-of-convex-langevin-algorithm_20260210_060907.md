---
ver: rpa2
title: 'DC-LA: Difference-of-Convex Langevin Algorithm'
arxiv_id: '2601.22932'
source_url: https://arxiv.org/abs/2601.22932
tags:
- prox
- dc-la
- convex
- assumption
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the problem of sampling from a Gibbs distribution\
  \ with a non-smooth, non-log-concave potential of the form V = f + r\u2081 - r\u2082\
  , where f is Lipschitz smooth and r\u2081, r\u2082 are convex functions (DC structure).\
  \ The authors propose a novel algorithm called DC-LA (Difference-of-Convex Langevin\
  \ Algorithm) that leverages the DC structure of the regularizer and Moreau smoothing\
  \ techniques to make the problem tractable for sampling."
---

# DC-LA: Difference-of-Convex Langevin Algorithm

## Quick Facts
- **arXiv ID**: 2601.22932
- **Source URL**: https://arxiv.org/abs/2601.22932
- **Reference count**: 40
- **Primary result**: Novel DC-LA algorithm for sampling from non-smooth, non-log-concave DC distributions, with convergence guarantees under distant dissipativity

## Executive Summary
This paper proposes DC-LA (Difference-of-Convex Langevin Algorithm) for sampling from Gibbs distributions with non-smooth, non-log-concave potentials of the form V = f + r₁ - r₂, where f is Lipschitz smooth and r₁, r₂ are convex functions. The key innovation is to smooth r₁ and r₂ separately using Moreau envelopes, then apply a proximal Langevin algorithm to the resulting augmented potential. The algorithm achieves convergence in q-Wasserstein distance under a distant dissipativity condition, improving upon previous work by handling non-weakly convex DC regularizers. Numerical experiments demonstrate accurate sampling in synthetic settings and reliable uncertainty quantification in Computed Tomography applications with DICNN priors.

## Method Summary
DC-LA leverages the DC structure of the regularizer by applying Moreau envelopes separately to r₁ and r₂, creating smooth approximations r^λ₁ and r^λ₂. The algorithm then performs a proximal Langevin step where the gradient step handles f - r^λ₂ and the proximal step operates on r^λ₁. This decomposition enables treating the convex and concave portions appropriately while maintaining well-defined proximal operators. The method requires hyperparameters λ (smoothing parameter) and γ (step size), with theoretical constraints linking them to ensure convergence. For complex priors like DICNNs, the proximal operators are approximated using fixed-point iterations.

## Key Results
- DC-LA achieves W_q convergence to target distribution with error bounds combining discretization (O(γ^{1/(2q)})) and smoothing (O(λ^{1/q})) terms
- Handles non-weakly convex DC regularizers inaccessible to previous Langevin-type algorithms
- Outperforms ULA and PSGLA in synthetic experiments while avoiding over-sharpening artifacts
- Provides reliable uncertainty quantification in CT reconstruction with DICNN priors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Smoothing each DC component separately via Moreau envelopes yields a well-defined differentiable regularizer approximation
- **Mechanism**: The DC structure r = r₁ - r₂ with convex r₁, r₂ enables independent application of Moreau envelopes to each component, producing r^λ₁ and r^λ₂ that are (1/λ)-smooth. This avoids the ill-posedness of applying Moreau smoothing directly to a DC function, which Appendix G shows produces multi-valued, discontinuous proximal operators.
- **Core assumption**: Both r₁ and r₂ are proper convex functions (Assumption 1).
- **Evidence anchors**: [abstract] "By leveraging the DC structure of r, we can smooth out r by applying Moreau envelopes to r₁ and r₂ separately."

### Mechanism 2
- **Claim**: Redistributing the concave portion (-r₂) to the forward step and the convex portion (r₁) to the backward step stabilizes the sampling dynamics
- **Mechanism**: DC programming principles suggest treating f - r₂ in the forward step and r₁ in the proximal step. This decomposition ensures the proximal operator operates on a convex function (which is well-defined) while the gradient step handles smooth components.
- **Core assumption**: r₁ is G₁-Lipschitz (Assumption 3); r₂ is either Lipschitz, Hölder-smooth, or smooth (Assumption 4).
- **Evidence anchors**: [Section 3] "In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity."

### Mechanism 3
- **Claim**: Distant dissipativity (strong convexity at infinity) suffices for Wasserstein convergence without requiring global log-concavity
- **Mechanism**: The condition ⟨∇f(x) + u₁ - u₂ - ∇f(y) - v₁ + v₂, x-y⟩ ≥ μ||x-y||² for ||x-y|| ≥ R₀ ensures the drift contracts outside a bounded region. Combined with stability results from Renaud et al. (2025a), this yields W_q(p_{X_k}, π) ≤ A_λ ρ^{kγ}_λ + B_λ γ^{1/(2q)} + Cλ^{1/q}.
- **Core assumption**: V is (μ, R₀)-distant dissipative (Assumption 2).
- **Evidence anchors**: [Theorem 8] Convergence bound with three error terms.

## Foundational Learning

- **Concept: Proximal Operator and Moreau Envelope**
  - **Why needed here**: Core building blocks of DC-LA; understanding Prox_{γg}(x) = argmin_y{g(y) + 1/(2γ)||x-y||²} and how g^λ(x) smooths nonsmooth g is essential.
  - **Quick check question**: Given convex g, compute Prox_{γ||·||₁}(x) and explain why ||·||_1^λ is smooth.

- **Concept: Wasserstein Distance**
  - **Why needed here**: The convergence metric; W_q measures distributional distance via optimal transport couplings.
  - **Quick check question**: Why does W_q convergence imply stronger distributional approximation than weak convergence?

- **Concept: DC (Difference-of-Convex) Programming**
  - **Why needed here**: Motivates the algorithm structure; the DC decomposition r = r₁ - r₂ enables separating convex and concave treatments.
  - **Quick check question**: Is ||x||₁ - ||x||₂ a valid DC function? Identify r₁ and r₂.

## Architecture Onboarding

- **Component map**:
  ```
  X_k → [Gradient step: X_k - γ∇f(X_k) + γ∇r^λ₂(X_k)] → [Add noise: + √(2γ)Z_{k+1}] 
       → Y_{k+1} → [Proximal step: Prox_{γr^λ₁}(Y_{k+1})] → X_{k+1}
  ```

- **Critical path**: Implementing Prox_{λr₂} and Prox_{(γ+λ)r₁} correctly. For DICNN priors, these lack closed forms and require fixed-point iterations (Section 5.2 uses 1 iteration).

- **Design tradeoffs**:
  - Smaller λ → better approximation (Cλ^{1/q} term shrinks) but drift Lipschitz constant (2/λ + L_f) grows → requires smaller γ → slower mixing.
  - Smaller γ → smaller discretization error (B_λ γ^{1/(2q)}) but more iterations needed for convergence.
  - Assumption: Lemma 12 shows λ, γ must satisfy joint constraints for finite moments.

- **Failure signatures**:
  - Exploding ||X_k||: γ too large relative to λ (violates γ ≤ μλ²/2(2+λL_f)² for q=1).
  - Poor mixing: λ too small or r₁, r₂ insufficiently regular.
  - Variance underestimation: Posterior samples miss modes (Section 5.2 notes missed structure in lower-left region).

- **First 3 experiments**:
  1. **Sanity check**: Implement DC-LA for V(x) = ½(x-μ)ᵀΣ(x-μ) + τ(||x||₁ - ||x||₂) in 2D. Compare sample histogram to true density (Figure 1). Verify KL divergence decreases with more chains.
  2. **Ablation on (λ, γ)**: Sweep λ ∈ {10⁻⁴, 10⁻³, 10⁻²}, γ ∈ {10⁻⁴, 10⁻³, 10⁻²}. Plot KL divergence vs. (λ, γ) to find stable operating region (Figure 11 shows intermediate values work best).
  3. **Comparison to baselines**: Run ULA on V (nonsmooth), ULA on V^λ (Moreau ULA), PSGLA on V. Compare W₂ distance to DC-LA samples. Expect DC-LA to achieve lower KL while avoiding PSGLA's over-sharpening.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does DC-LA maintain convergence guarantees when using inexact proximal operators for the DC components?
- **Basis in paper:** [Explicit] The authors note that "bounded approximation errors in computing proximal operators lead to controlled bias; extending our analysis to this setting is a promising direction for future work."
- **Why unresolved:** The theoretical analysis assumes exact proximal mappings, but for complex priors like DICNNs, these operators must be approximated (e.g., via fixed-point iterations), introducing unquantified bias.
- **What evidence would resolve it:** A proof of convergence establishing non-asymptotic bounds for DC-LA under a bounded error assumption for the proximal steps of $r_1$ and $r_2$.

### Open Question 2
- **Question:** How do the uncertainty quantification maps generated by DC-LA improve performance in downstream clinical tasks?
- **Basis in paper:** [Explicit] The Conclusion states: "Future work will explore the practical use of these variance images in downstream applications, e.g., [Jun et al., 2025]."
- **Why unresolved:** While the paper visualizes variance maps for CT reconstruction, it does not quantify their utility for tasks such as lesion detection or robust decision-making.
- **What evidence would resolve it:** Quantitative results showing that incorporating DC-LA variance estimates improves diagnostic accuracy or anomaly detection rates compared to point estimates.

### Open Question 3
- **Question:** Is explicit augmentation of the likelihood $f$ required to ensure convergence in ill-conditioned inverse problems?
- **Basis in paper:** [Inferred] Section 5.2 mentions that for ill-conditioned $A$, Lemma 4 does not apply directly, and one *could* augment $f$ with a radial part to ensure distant dissipativity, but the authors "leave the likelihood as-is."
- **Why unresolved:** There is a disconnect between the theory (requiring distant dissipativity) and the CT experiments (where the likelihood may not satisfy this condition without augmentation), leaving the necessity of this modification unclear.
- **What evidence would resolve it:** An ablation study comparing DC-LA convergence and sample quality with and without the radial likelihood augmentation on ill-conditioned datasets.

## Limitations

- Dependence on accurate proximal operators for r₁ and r₂, particularly challenging for complex priors like DICNNs
- Distant dissipativity assumption, while less restrictive than global convexity, may still exclude important non-convex distributions
- Discretization and smoothing error bounds are theoretical and may be loose in practice

## Confidence

- **High confidence**: The DC structure exploitation mechanism (Mechanism 1) and the separate Moreau smoothing approach are well-grounded theoretically with clear proofs in the appendix.
- **Medium confidence**: The convergence rate analysis under distant dissipativity (Mechanism 3) is theoretically sound but relies on technical conditions that may be difficult to verify in practice.
- **Medium confidence**: The numerical results demonstrate practical effectiveness, though the CT experiment relies on pretrained models not directly available from the cited source.

## Next Checks

1. **Verify proximal operator implementation**: Test DC-LA on a simple 2D example with known proximal forms (e.g., ℓ₁ and ℓ₂ norms) and compare empirical KL divergence to theoretical bounds across different (λ, γ) pairs.
2. **Assess sensitivity to proximal approximation quality**: In the CT experiment, vary the number of fixed-point iterations for the DICNN proximal (1, 5, 10 iterations) and measure impact on posterior variance and reconstruction quality.
3. **Test boundary conditions**: Implement counterexamples where r₂ lacks Lipschitz or Hölder smoothness to verify that DC-LA fails gracefully or produces degraded samples as predicted by the theory.