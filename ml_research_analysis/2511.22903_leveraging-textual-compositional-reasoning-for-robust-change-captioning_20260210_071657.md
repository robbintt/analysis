---
ver: rpa2
title: Leveraging Textual Compositional Reasoning for Robust Change Captioning
arxiv_id: '2511.22903'
source_url: https://arxiv.org/abs/2511.22903
tags:
- image
- compositional
- reasoning
- cortex
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORTEX improves change captioning by integrating explicit compositional
  reasoning from VLM-generated text with visual features. It employs an RTE module
  to extract structured, relational descriptions and an ITDA module for static and
  dynamic alignment of visual and textual features.
---

# Leveraging Textual Compositional Reasoning for Robust Change Captioning

## Quick Facts
- arXiv ID: 2511.22903
- Source URL: https://arxiv.org/abs/2511.22903
- Reference count: 40
- Primary result: CORTEX improves change captioning by integrating VLM-generated compositional reasoning with visual features, achieving 57.4 BLEU-4 on CLEVR-Change.

## Executive Summary
CORTEX introduces a novel approach to change captioning by combining explicit compositional reasoning from Vision Language Models with visual feature alignment. The framework extracts structured textual descriptions of objects and their spatial relationships from single images using carefully crafted prompts, then aligns these textual cues with visual features through static and dynamic cross-attention mechanisms. This dual-modality approach enables fine-grained relational reasoning about changes that are ambiguous in visual features alone, achieving state-of-the-art performance across three benchmarks while serving as a plug-and-play enhancement for existing visual-only methods.

## Method Summary
CORTEX consists of three modules: an image-level change detector (SCORER/SMART/DIRL), a Relation-Reasoning Extractor (RTE) that generates compositional sentences from single images using structured prompts with a frozen VLM (InternVL2), and an Inter- and Intra-Scene Text-Visual Alignment (ITDA) module that performs static (intra-scene) and dynamic (cross-scene) alignment between visual and textual features using cross-attention. The RTE generates multiple sentences per image describing object attributes and spatial relationships, which are encoded with BERT and concatenated. ITDA aligns before/after visual features with before/after text using L2 losses (L_sa, L_da), and the aligned features are decoded into captions. The framework adds only ~3.8M parameters and can enhance any existing visual-only change detector.

## Key Results
- CORTEX achieves best scores of 57.4 BLEU-4, 43.0 METEOR, and 76.2 CIDEr on CLEVR-Change
- Consistent improvements across three baselines: SCORER (+2.0 CIDEr), SMART (+3.2 CIDEr), DIRL (+5.4 CIDEr on semantic change)
- Ablation confirms both RTE and ITDA modules contribute significantly to performance gains
- VLM-generated compositional cues are essential, with direct paired-image VLM comparison achieving only 12.3 CIDEr

## Why This Works (Mechanism)

### Mechanism 1: Structured Prompt-Based VLM Extraction
Carefully designed prompts force VLMs to generate explicit compositional reasoning sentences encoding object attributes and spatial relationships that are only implicit in raw pixels. The RTE module uses a prompt requiring each sentence to describe one object's color, shape, size, AND its spatial relationship to at least one other object. This structured output constraint guides InternVL2 to produce relational descriptions (e.g., "To the left of the blue sphere, there is a small red block") rather than generic captions. Core assumption: VLMs trained on large-scale image-text data can accurately capture spatial layouts and object semantics from single images, even if they struggle with direct pairwise comparison.

### Mechanism 2: Dual Cross-Attention Alignment (Static + Dynamic)
Cross-attention between visual and textual modalities in both intra-scene and cross-scene configurations enables fine-grained relational reasoning about changes. Static alignment (Equation 1) attends visual features to same-scene text, embedding compositional context. Dynamic alignment (Equation 5) attends before-scene visual features to after-scene text (and vice versa), highlighting what changed. L2 losses (L_sa, L_da) enforce semantic consistency. Core assumption: Textual descriptions from both scenes contain complementary information: same-scene text provides compositional grounding; cross-scene text highlights differences.

### Mechanism 3: Plug-and-Play Visual Enhancement Architecture
RTE+ITDA modules can enhance any existing visual-only change detector as a drop-in augmentation, preserving baseline strengths while adding compositional reasoning. RTE generates text offline (preprocessing); ITDA is a separate module accepting f_icd from any baseline and producing text-augmented f_itda. Only ~3.8M additional parameters; λ controls alignment strength (10⁻³–10⁻⁴). Core assumption: Visual-only methods capture low-level appearance differences but systematically miss compositional structure; textual cues are complementary, not redundant.

## Foundational Learning

- **Concept: Vision-Language Models for single-image vs. pairwise reasoning**
  - Why needed here: Understanding that VLMs like InternVL2 are optimized for single-image captioning, NOT direct comparison of image pairs
  - Quick check question: "Why does Table 8 show direct VLM comparison (paired images) achieving only 12.3 CIDEr vs. 49.5 for the auxiliary approach?"

- **Concept: Cross-attention for multimodal feature fusion**
  - Why needed here: ITDA uses Attn(Q, K, V) where Q comes from text and K,V from image (or vice versa), projecting between modalities
  - Quick check question: "In Equation 1, why is t^n_bef used as Q while f_bef serves as both K and V?"

- **Concept: Static vs. dynamic alignment semantics**
  - Why needed here: Static = intra-scene grounding (what exists); dynamic = cross-scene contrast (what changed)
  - Quick check question: "Why does dynamic alignment use before-scene visual features with after-scene text, rather than before-scene visual with before-scene text?"

## Architecture Onboarding

- **Component map:**
  Input: (I_bef, I_aft) → ResNet-101 → [f_bef, f_aft] → Change Detector → f_icd
  Parallel: (I_bef, I_aft) → InternVL2 + Structured Prompt → [T_bef^1..N, T_aft^1..M]
                                                            ↓
                                         BERT → [t_bef^1..N, t_aft^1..M] → Concat → f_rte
  Fusion (ITDA):
    f_bef + {t_bef^n} → Static Attn → f_s(t→i)_bef
    f_bef + {t_aft^m} → Dynamic Attn → f_d(t→i)_bef
    (same for after scene)
    Concat → f_itda → Transformer Decoder → Caption
  Losses: L_total = L_cap + λ(L_sa + L_da), λ ∈ {10⁻³, 10⁻⁴}

- **Critical path:** RTE text generation (offline, 3.94s/img) → ITDA alignment → Decoder. If RTE produces systematic spatial errors, all downstream outputs inherit these mistakes.

- **Design tradeoffs:**
  1. Offline preprocessing cost (3.94s/img) vs. fast online inference (0.008s/img)
  2. Multiple sentences per scene (N, M vary dynamically) vs. single caption — more robust to individual errors but higher variance
  3. BERT text encoder (better compositional understanding, Table S3) vs. CLIP (slightly weaker)

- **Failure signatures:**
  1. VLM spatial relation errors → "left/right" confusion in final caption (Figures S8–S10 show 3 error categories)
  2. Attribute identification errors → wrong color/size/shape in output
  3. Direct paired-image VLM input → CIDEr collapses to 12.3 (Table 8)
  4. Generic prompts (vs. compositional) → -0.3 to -1.2 CIDEr (Table 6)
  5. Missing either L_sa or L_da → ~0.5–1.0 CIDEr drop (Table 5)

- **First 3 experiments:**
  1. Reproduce DIRL baseline on CLEVR-Change validation. Target: ~125 CIDEr (total). Verify visual-only performance before adding modules.
  2. Generate RTE text for 100 image pairs using the provided prompt with InternVL2. Manually score accuracy/relevance/fluency (target: >3.5/5 per Table S4 criteria).
  3. Integrate ITDA with λ=10⁻⁴. Monitor both L_sa and L_da decreasing during training. Target: +2–5 CIDEr over baseline on validation set.

## Open Questions the Paper Calls Out
- Can knowledge distillation effectively reduce the computational overhead of VLM-based text extraction without sacrificing reasoning quality?
- To what extent do VLM hallucinations in spatial relations impact final caption accuracy?
- Does the framework generalize to specialized domains like medical imaging where VLMs may lack specific training?

## Limitations
- Performance critically depends on VLM-generated compositional descriptions with only moderate accuracy (3.90–4.22/5 in human evaluation)
- All three benchmarks use synthetic or surveillance imagery, limiting validation of generalization to natural scenes
- Alignment losses contribution is empirically chosen without systematic exploration of λ values

## Confidence
- **High confidence**: The plug-and-play architecture works as claimed, with consistent improvements across three different baselines and three datasets
- **Medium confidence**: The compositional reasoning mechanism works, but VLM-generated text quality limitations may constrain real-world performance
- **Low confidence**: Claims about "fine-grained relational reasoning" are not empirically validated beyond quantitative metrics

## Next Checks
1. Generate 100 RTE descriptions on CLEVR-Change validation set, manually annotate spatial relation and attribute errors, then trace these errors through ITDA alignment to final captions to quantify error amplification
2. Apply CORTEX to a small subset of natural image change detection datasets (e.g., CDNet, ChangeDetection.net) to assess generalization beyond synthetic domains
3. Systematically vary λ from 10⁻² to 10⁻⁵ and analyze correlation between alignment loss magnitude and semantic consistency metrics (e.g., object-attribute matching between text and detected changes)