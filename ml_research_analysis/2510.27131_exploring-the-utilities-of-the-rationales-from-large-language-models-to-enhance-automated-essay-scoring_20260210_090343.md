---
ver: rpa2
title: Exploring the Utilities of the Rationales from Large Language Models to Enhance
  Automated Essay Scoring
arxiv_id: '2510.27131'
source_url: https://arxiv.org/abs/2510.27131
tags:
- scoring
- score
- essay
- ensemble
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared essay-based scoring with rationale-based scoring
  for automated essay evaluation using the ASAP Prompt 6 dataset. The findings show
  that essay-based scoring generally outperformed rationale-based scoring, achieving
  a higher QWK of 0.848 versus 0.823.
---

# Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring

## Quick Facts
- arXiv ID: 2510.27131
- Source URL: https://arxiv.org/abs/2510.27131
- Authors: Hong Jiao; Hanna Choi; Haowei Hua
- Reference count: 40
- Primary result: Rationale-based scoring improves F1 for underrepresented score 0 while ensemble modeling achieves highest QWK of 0.870

## Executive Summary
This study explores how LLM-generated rationales can enhance automated essay scoring by providing complementary information to traditional essay-based models. The research compares seven encoder architectures fine-tuned on essays, GPT-4.1 rationales, and GPT-5 rationales, then combines predictions using seven ensemble methods. While essay-based scoring generally outperformed rationale-based scoring in overall QWK, rationale-based models demonstrated superior F1 accuracy for the underrepresented score 0 category. The study found that stacking ensemble learning, which combines predictions from all 21 models using Ridge regression, achieved the highest QWK of 0.870, surpassing previous benchmarks.

## Method Summary
The study used the ASAP Prompt 6 dataset (1,800 essays, scores 0-4) with a 70/10/20 split for training, validation, and testing. GPT-4.1 and GPT-5 generated rationales using zero-shot prompts based on the scoring rubric. Seven encoder-only transformers (BERT-base, DeBERTa-base, DeBERTa-v3-large, DistilBERT, ELECTRA-large, RoBERTa-base, RoBERTa-large) were fine-tuned on each input type. Predictions from 21 models were combined using seven ensemble methods including stacking, tiered, elite, and weighted median approaches. The primary evaluation metric was Quadratic Weighted Kappa (QWK), supplemented by Spearman correlation and per-class F1 scores.

## Key Results
- Essay-based scoring outperformed rationale-based scoring in overall QWK (0.848 vs 0.823)
- Rationale-based scoring achieved significantly higher F1 scores for score 0 (0.50-0.60 vs 0.00-0.10)
- Ensemble modeling combining essay and rationale-based models achieved highest QWK of 0.870
- ELECTRA-large on essays achieved highest individual QWK of 0.8495
- GPT-5 rationales were shorter (129.5 words) than GPT-4.1 rationales (310.3 words) and performed worse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rationale-based scoring provides auxiliary information that improves accuracy for underrepresented score classes, even when overall QWK is lower than essay-based scoring.
- **Mechanism:** LLM-generated rationales explicitly articulate evaluation criteria aligned with rubrics, creating discriminative signals for minority classes that essay-based models struggle with due to sparse training examples. This functions analogously to how items with lower discrimination parameters in IRT can provide more information for certain ability ranges.
- **Core assumption:** Rationales capture evaluative patterns distinct from raw essay text, particularly for boundary cases.
- **Evidence anchors:** [abstract] "Rationale-based scoring led to higher scoring accuracy in terms of F1 scores for score 0 which had less representation due to class imbalance issues." [section p.8-9] Tables 3-4 show rationale-based models achieving F1_0 of 0.50-0.60 versus most essay-based models at 0.00. [corpus] Limited direct support; neighbor papers focus on scoring accuracy broadly rather than class imbalance specifically.
- **Break condition:** If rationales are too succinct (as with GPT-5's constrained prompting), discriminative signal degrades—QWK dropped to 0.78-0.80 range.

### Mechanism 2
- **Claim:** Stacking ensemble learning optimally combines predictions from heterogeneous models by learning weights from validation performance rather than applying fixed rules.
- **Mechanism:** A Ridge regression meta-learner takes predictions from 21 base models (7 encoder architectures × 3 input types: essays, GPT-4.1 rationales, GPT-5 rationales) and learns optimal linear combination weights through cross-validated regularization. This captures complementary error patterns—essay models excel at well-represented scores while rationale models provide minority-class signal.
- **Core assumption:** Base model predictions contain uncorrelated errors that can be exploited by a meta-learner.
- **Evidence anchors:** [abstract] "Ensemble modeling of essay and rationale-based models yielded the highest QWK of 0.870, surpassing previous literature benchmarks." [section p.10] Table 8 shows Stacking Ensemble achieving QWK of 0.8703 with F1_0 of 0.6154, the best across all configurations. [corpus] Neighbor paper "Rank-Then-Score" similarly finds ensemble approaches improve AES, though via ranking rather than stacking.
- **Break condition:** If base models are highly correlated (e.g., all same architecture), stacking gains diminish; tiered or elite ensembles may perform comparably.

### Mechanism 3
- **Claim:** Encoder-only transformer models fine-tuned on rationales underperform those fine-tuned on essays in overall QWK, suggesting rationales lose information present in original text.
- **Mechanism:** Rationales are compressed summaries (~130-310 words) versus original essays (mean 150 words, max 454). Information loss occurs during LLM generation, particularly for nuanced content features. ELECTRA-large on essays achieved 0.8495 QWK versus 0.7937 on GPT-4.1 rationales.
- **Core assumption:** Rationales sufficiently preserve score-predictive information for the target task.
- **Evidence anchors:** [section p.8] "In general, models based on rationales generated by GPT-4.1 performed worse than models based on essays." [section p.7] Rationale length distributions show GPT-5 rationales averaged 129.5 words versus 310.3 for GPT-4.1, with GPT-5 models performing worse. [corpus] Neighbor "Comparison of Scoring Rationales Between LLMs and Human Raters" explores rationale quality but doesn't directly address information loss.
- **Break condition:** When rationales are over-compressed or prompt engineering is suboptimal, performance degrades further.

## Foundational Learning

- **Concept: Quadratic Weighted Kappa (QWK)**
  - **Why needed here:** QWK is the primary evaluation metric, accounting for agreement severity—not just accuracy. A score prediction off by 1 point is penalized less than being off by 3 points.
  - **Quick check question:** If a model predicts score 4 for an essay that received score 0, how does QWK penalize this versus predicting score 3?

- **Concept: Class Imbalance and F1 Score**
  - **Why needed here:** Score 0 has only 44 essays (2.4% of data). Standard accuracy metrics mask poor performance on minority classes; F1 per class reveals this.
  - **Quick check question:** Why would a model that never predicts score 0 still achieve high overall accuracy on this dataset?

- **Concept: Stacking Ensemble Meta-Learning**
  - **Why needed here:** The best results come from a Ridge regression meta-learner combining 21 base models. Understanding how stacking differs from simple averaging is critical.
  - **Quick check question:** What advantage does learning ensemble weights from validation data have over using fixed performance-based weights?

## Architecture Onboarding

- **Component map:** Essays/GPT-4.1 rationales/GPT-5 rationales -> 7 encoder transformers (BERT-base, DeBERTa-base/large, DistilBERT, ELECTRA-large, RoBERTa-base/large) -> Classification heads -> 21 base models -> 7 ensemble methods -> Final predictions

- **Critical path:**
  1. Generate rationales via GPT-4.1/GPT-5 API with temperature 0.2/1.0 respectively
  2. Fine-tune each encoder on each input type (21 models total)
  3. Extract predictions on held-out test set (360 essays)
  4. Train Stacking ensemble (Ridge regression with CV regularization) on validation predictions
  5. Evaluate on test set using QWK, Spearman correlation, and per-class F1

- **Design tradeoffs:**
  - GPT-4.1 rationales (longer, more informative) vs GPT-5 (succinct, potentially over-compressed)
  - Stacking (highest QWK, learned weights) vs Tiered (domain knowledge for extreme scores)
  - Single best model (ELECTRA-large on essays, QWK 0.8495) vs full ensemble (QWK 0.8703, but 21× complexity)

- **Failure signatures:**
  - F1_0 = 0.0000 indicates model never correctly predicts score 0 (common in essay-only models)
  - Token truncation warnings when rationale length exceeds 512 tokens (occurred for 113-189 GPT-4.1 rationales)
  - GPT-5 QWK dropped to 0.7283 with overly succinct prompting—rationale quality degrades

- **First 3 experiments:**
  1. **Baseline replication:** Train ELECTRA-large on essays only, verify QWK ≈ 0.849. Check F1_0 to confirm minority-class weakness.
  2. **Ablation study:** Remove rationale-based models from ensemble one at a time. Quantify QWK and F1_0 degradation to validate complementary signal claim.
  3. **Prompt engineering sweep:** Vary GPT-5 rationale length constraints (current: ≤512 tokens). Test if allowing longer rationales recovers performance gap versus GPT-4.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing explicit data augmentation strategies improve the scoring accuracy of language models trained on essays and rationales, particularly for underrepresented score categories?
- Basis in paper: [explicit] The authors state, "Future exploration can explicitly implement data augmentation methods to improve scoring accuracy with LMs based on essays and rationales respectively."
- Why unresolved: The study identified class imbalance (notably for score 0) but did not utilize rebalancing techniques in the current methodology.
- Evidence: Comparative results showing QWK and F1-score improvements for minority classes after applying augmentation techniques (e.g., back-translation) to the training data.

### Open Question 2
- Question: Can high-quality sentence embeddings from Sentence-Transformers (e.g., all-MiniLM, E5) outperform the current ELECTRA and DeBERTa embeddings when used in supervised machine learning models?
- Basis in paper: [explicit] The paper suggests that "high-quality sentence embeddings from Sentence-Transformers families... can be explored in future studies."
- Why unresolved: Supervised models trained on current embeddings yielded lower QWKs (0.72–0.78) than the fine-tuned LMs, likely because the current encoders are not optimized for semantic similarity.
- Evidence: A performance comparison (QWK) between gradient boosting models trained on Sentence-Transformer embeddings versus the current baseline embeddings.

### Open Question 3
- Question: Does allowing GPT-5 to generate more elaborated rationales improve the performance of rationale-based scoring models?
- Basis in paper: [explicit] The authors note, "We will experiment with other options that GPT-5 rationale can be more elaborated," hypothesizing that current constraints may have hindered performance.
- Why unresolved: GPT-5 rationale-based models underperformed compared to GPT-4.1, which the authors speculate was due to the "too strict" succinctness constraint added to the prompt.
- Evidence: An ablation study varying the verbosity constraints of the GPT-5 prompt to measure the correlation between rationale length/density and scoring accuracy.

## Limitations
- GPT-5 rationale generation produced shorter outputs (129.5 words) and lower QWK scores (0.7283-0.8000) than GPT-4.1, with prompt engineering trade-offs not fully explored
- Class imbalance effects are difficult to generalize since score 0 has only 44 essays (2.4% of data)
- Ensemble performance gains assume complementary error patterns across 21 models, but inter-model correlations and individual contributions aren't reported

## Confidence
- **High confidence**: Essay-based scoring outperforming rationale-based scoring in overall QWK; stacking ensemble achieving best results
- **Medium confidence**: Rationale-based models specifically improving F1 for score 0; encoder-only models losing information from essay compression
- **Low confidence**: Optimal ensemble composition; generalizability across different essay prompts or scoring rubrics

## Next Checks
1. **Ablation study**: Remove rationale-based models from the ensemble one at a time and measure QWK/F1_0 degradation to quantify their marginal contribution
2. **Class imbalance robustness**: Replicate the analysis with different train/test splits to verify F1_0 improvements for score 0 are consistent
3. **Prompt engineering sweep**: Systematically vary GPT-5 rationale length constraints and verbosity instructions to identify optimal balance between informativeness and conciseness