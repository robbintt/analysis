---
ver: rpa2
title: 'Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise
  Mixture-of-Experts'
arxiv_id: '2505.22582'
source_url: https://arxiv.org/abs/2505.22582
tags:
- languages
- language
- experts
- moe-lpr
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LayerMoE, a method for efficiently expanding
  the multilingual capabilities of large language models (LLMs) through layer-wise
  allocation of experts in a Mixture-of-Experts (MoE) architecture. The key challenge
  addressed is catastrophic forgetting of old languages while learning new ones.
---

# Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts

## Quick Facts
- **arXiv ID:** 2505.22582
- **Source URL:** https://arxiv.org/abs/2505.22582
- **Reference count:** 16
- **Primary result:** Achieves 60% fewer experts than state-of-the-art while improving performance on both old and new languages in multilingual LLM expansion

## Executive Summary
This paper addresses the challenge of efficiently expanding large language models to new languages while preventing catastrophic forgetting of previously learned languages. The authors propose LayerMoE, a method that analyzes layer-wise representation similarity between languages to allocate experts proportionally—fewer experts for highly similar layers (language-agnostic features) and more for dissimilar layers (language-specific features). Additionally, LayerMoE adds binary classifiers in front of router networks for high-similarity layers to explicitly guide old-language tokens to frozen experts. Experiments on Qwen1.5-1.8B and Llama-3.2-3B demonstrate superior parameter efficiency and performance compared to existing approaches in both single-language and lifelong multilingual expansion settings.

## Method Summary
LayerMoE operates in two stages to expand LLMs to new languages while preserving old-language capabilities. First, it computes cosine similarity between hidden states of old and new language tokens across all layers using sampled data. Based on these similarity scores, it allocates experts inversely proportional to similarity (higher similarity gets fewer experts). The original model parameters are frozen to prevent catastrophic forgetting. Second, it adds binary classifiers to the top-K highest-similarity layers to explicitly route old-language tokens to frozen experts. Training proceeds with a combined loss function including next-token prediction, load balancing, language-prior routing, and classification objectives. The method targets efficient multilingual expansion by leveraging the observation that different layers capture different levels of language-specific versus language-agnostic features.

## Key Results
- Achieves 60% fewer experts than MoE-LPR while maintaining or improving performance in single-language expansion
- Outperforms state-of-the-art with 33.3% fewer experts in lifelong language expansion settings
- Improves Old-avg scores (preserving old languages) from 45.36 to 45.80 while reducing expert count from 72 to 30
- Demonstrates effectiveness across multiple benchmarks including ARC-C, MMLU, HellaSwag, and Belebele

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Allocating experts proportional to layer-wise language representation similarity improves parameter efficiency while maintaining or improving performance.
- **Mechanism:** The authors compute cosine similarity between hidden states (post-attention) of old and new language tokens per layer. Layers with higher cross-language similarity receive fewer new experts; lower-similarity layers receive more. The allocation formula inverts similarity and normalizes across layers.
- **Core assumption:** High similarity layers capture language-agnostic features that transfer across languages; low similarity layers require language-specific capacity.
- **Evidence anchors:**
  - [abstract] "we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts"
  - [section 3.1] Figure 1 shows similarity patterns: "higher similarity in the middle and last few layers, and lower similarity in the 0~4 layers and 17~21 layers"
  - [corpus] Neighbor paper "Multilingual Routing in Mixture-of-Experts" analyzes similar layer-wise routing phenomena, suggesting cross-paper consistency
- **Break condition:** If all layers have uniformly high similarity (e.g., very similar language pairs), the allocation signal collapses and the method reduces to uniform allocation.

### Mechanism 2
- **Claim:** A pre-router classifier on high-similarity layers explicitly guides old-language tokens to frozen experts, reducing routing confusion and forgetting.
- **Mechanism:** A linear classifier (h×2) predicts whether a token belongs to old or new languages. If classified as old, the token bypasses the router and goes directly to the frozen original expert. This targets layers where high similarity makes the router's job harder.
- **Core assumption:** High similarity between old and new language representations confuses the learned router; explicit language identification is easier than implicit routing.
- **Evidence anchors:**
  - [abstract] "we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens"
  - [section 3.3] Equation 13 shows the conditional routing based on classifier output
  - [section 5.2] Ablation shows removing classifier increases forgetting: "the forgetting of old languages becomes worse (↓ 0.47 in G1, ↓ 0.67 in G2)"
- **Break condition:** If classifier accuracy is poor (e.g., visually similar scripts, code-mixed text), forced routing to old experts may harm new-language performance without benefiting old-language preservation.

### Mechanism 3
- **Claim:** Two-stage training with frozen original parameters prevents catastrophic forgetting while allowing new language acquisition.
- **Mechanism:** Stage-1 freezes all original parameters, training only new experts and routers on new-language data. Stage-2 trains only routers and classifiers on mixed data with LPR loss encouraging old-language tokens to route to original experts.
- **Core assumption:** The original model encodes sufficient old-language knowledge; preserving these weights directly maintains capability.
- **Evidence anchors:**
  - [section 2] "the parameters of the original dense model are frozen to avoid catastrophic forgetting"
  - [section 5.1] Table 1 shows LayerMoE achieves 45.80 Old-avg vs. 45.36 for MoE-LPR (6×24) with 60% fewer experts
  - [corpus] Related work references MoE-LPR and LoRAMoE as prior approaches with similar frozen-backbone strategies
- **Break condition:** If the original model has weak old-language capability, freezing preserves mediocrity rather than competence.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) routing fundamentals**
  - **Why needed here:** LayerMoE modifies expert allocation and routing; understanding standard top-K routing with load balancing is prerequisite.
  - **Quick check question:** Given router scores [0.1, 0.3, 0.4, 0.2] with K=2, which experts are selected and what are their weights?

- **Concept: Catastrophic forgetting in continual learning**
  - **Why needed here:** The entire method is designed around preserving old-language performance while adding new languages.
  - **Quick check question:** Why does fine-tuning on new-language data degrade old-language performance in standard LLMs?

- **Concept: Representation similarity metrics (cosine similarity)**
  - **Why needed here:** The allocation algorithm depends entirely on computing similarity between hidden states across languages.
  - **Quick check question:** If two languages have cosine similarity of 0.95 in layer 10 and 0.3 in layer 2, which layer should receive more experts?

## Architecture Onboarding

- **Component map:**
  - Dense base model (frozen) -> New experts (layer-specific allocation) -> Router network -> Classification network (high-similarity layers only) -> Loss components (L_NTP, L_balance, L_LPR, L_CLS)

- **Critical path:**
  1. Compute similarity S per layer using sampled tokens from old and new languages (Eq. 9)
  2. Allocate experts using inverted-similarity formula (Eq. 10)
  3. Stage-1: Train new experts + routers on new-language data only
  4. Identify top-K high-similarity layers (K=7 for single-expansion, K=5 for lifelong)
  5. Stage-2: Train routers + classifiers on mixed data with combined loss (Eq. 12)

- **Design tradeoffs:**
  - **Total expert budget δ:** Lower values improve efficiency but cap new-language capacity. Paper uses δ=72 (matching MoE-LPR 3×24) for fair comparison.
  - **Number of classifier layers (K):** More classifiers better preserve old languages but may constrain new-language routing. Table 4 shows top-7 optimal for G0→G1.
  - **Assumption:** Similarity computed on 100K sampled tokens; smaller samples may introduce noise.

- **Failure signatures:**
  - **Random allocation baseline:** "w/random" in Table 3 shows New-avg drops 0.60–0.71 points
  - **No classifier baseline:** "w/o classifier" shows Old-avg drops 0.47–0.67 points
  - **Excessive classifiers:** Top-24 (all layers) underperforms top-7 on new-language metrics
  - **Tokenization mismatch:** G2 languages (Bengali, Hindi, Nepali) show smaller gains (2.45 vs. 5.8 points) due to UTF-8 encoding inefficiency

- **First 3 experiments:**
  1. **Reproduce similarity analysis:** Sample 100K tokens each from 2 languages your model knows well; plot layer-wise cosine similarity to verify non-uniform patterns.
  2. **Single-expansion pilot:** Expand to 1 new language with δ=24 experts (small budget); compare LayerMoE allocation vs. uniform baseline on a held-out benchmark.
  3. **Classifier ablation:** Add classifiers to top-3 vs. top-7 vs. random-7 layers; measure Old-avg retention to validate the similarity-guided placement hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the optimal total number of new experts be determined dynamically rather than relying on fixed baselines?
- **Basis in paper:** [explicit] The authors acknowledge they "set the total number of new experts according to the baseline 'MoE-LPR (3*24)' for a fair comparison rather than exploring the optimal total number."
- **Why unresolved:** The current work focuses on the *distribution* (allocation) of a fixed budget of experts, leaving the determination of the budget size itself to manual tuning based on prior art.
- **What evidence would resolve it:** An ablation study varying the total expert count ($\delta$) combined with an automated search strategy (e.g., scaling laws) to identify the most parameter-efficient configuration for a specific language group.

### Open Question 2
- **Question:** What are the underlying factors causing inconsistent performance gains across different benchmarks and languages?
- **Basis in paper:** [explicit] The Limitations section states that while average improvements exist, "improvements in different benchmarks and languages are inconsistent. The inherent reasons for the inconsistency are not yet clear."
- **Why unresolved:** The aggregate success metrics (Old-avg/New-avg) mask variance; understanding why specific tasks or languages benefit less from the layer-wise allocation is necessary for robustness.
- **What evidence would resolve it:** A fine-grained analysis correlating the entropy of hidden states for specific languages/tasks with the allocated expert density to identify under-resourced representations.

### Open Question 3
- **Question:** How does the sequential order of language group expansion influence the final model's performance in lifelong learning?
- **Basis in paper:** [inferred] The results show that the learning order (G0→G1→G2 vs. G0→G2→G1) yields different results, which the authors suggest "indicates that the learning order... may have non-trivial influences."
- **Why unresolved:** The paper identifies the phenomenon but does not propose a theory or mechanism to explain why one expansion order results in better retention or new language acquisition than another.
- **What evidence would resolve it:** Experiments systematically permuting the order of language groups (e.g., similar scripts first vs. diverse scripts first) to isolate the effect of linguistic similarity on sequential expert routing.

## Limitations

- The method's efficiency gains rely on stable and predictable layer-wise similarity patterns that may not hold across all language pairs or model scales
- Classifier-based routing in high-similarity layers could fail with ambiguous cases like code-switched text or languages with shared scripts
- The long-term stability and optimality of routing strategy in lifelong expansion scenarios (multiple phases) is not well-established
- Performance gains show variance across different benchmarks and languages, with some showing minimal improvement

## Confidence

**High confidence:** The core mechanism of using layer-wise similarity to guide expert allocation is well-supported by experimental evidence, with clear ablation results showing degradation when the classifier is removed or allocation is randomized.

**Medium confidence:** The efficiency claims (60% fewer experts) are compelling within the controlled experimental setting using Qwen1.5-1.8B, but may not generalize to larger models or more diverse language sets.

**Low confidence:** The long-term stability of the routing strategy in lifelong expansion scenarios is not well-established, and the paper doesn't address potential compounding effects from multiple expansion phases.

## Next Checks

1. **Similarity stability analysis:** Recompute layer-wise similarity using multiple random samples (varying seeds, different domains) and measure allocation variance. Test whether small changes in similarity produce large changes in expert allocation and performance.

2. **Edge case routing evaluation:** Create synthetic test cases with code-switched text, languages with shared scripts (e.g., Hindi/Nepali), or ambiguous tokens. Measure classifier accuracy and routing quality in these scenarios to identify failure modes.

3. **Multi-phase expansion stress test:** Implement a three-phase expansion (G0→G1→G2→G3) and track cumulative routing complexity, classifier performance, and parameter efficiency across phases. Evaluate whether the initial similarity-based allocation remains optimal or requires recalibration.