---
ver: rpa2
title: Question Answering on Patient Medical Records with Private Fine-Tuned LLMs
arxiv_id: '2501.13687'
source_url: https://arxiv.org/abs/2501.13687
tags:
- task
- patient
- llms
- fine-tuned
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-stage approach for answering questions
  about patient medical records using fine-tuned LLMs. The first stage identifies
  relevant FHIR resources for a given query, and the second stage generates answers
  using these resources.
---

# Question Answering on Patient Medical Records with Private Fine-Tuned LLMs

## Quick Facts
- arXiv ID: 2501.13687
- Source URL: https://arxiv.org/abs/2501.13687
- Reference count: 31
- Key outcome: Fine-tuned Llama-3.1-8B and Mistral-NeMo models outperformed GPT-4 family by 0.55% F1 on classification and 42% on METEOR for answer generation while being 250x smaller

## Executive Summary
This paper presents a two-stage approach for answering questions about patient medical records using fine-tuned LLMs. The method first classifies FHIR resources as relevant or irrelevant to a query, then generates answers using only those resources. Fine-tuned Llama-3.1-8B and Mistral-NeMo models significantly outperformed GPT-4 family models while being substantially smaller and more privacy-preserving. The study also examined sequential fine-tuning, dataset size effects, and LLM self-evaluation bias.

## Method Summary
The approach uses task decomposition into binary classification (identifying relevant FHIR resources) and answer generation. Synthetic patient data is generated using Synthea, then GPT-4 creates training queries and answers. Models are fine-tuned using QLoRA (rank 16, 5 epochs) on Llama-3.1-8B and Mistral-NeMo-12B. Task 1 uses F1 for evaluation while Task 2 uses METEOR score. The models are available at https://huggingface.co/genloop.

## Key Results
- Fine-tuned Llama-3.1-8B achieved 95.52% F1 on Task 1 vs. GPT-4 baseline
- Fine-tuned models achieved 42% improvement on METEOR for Task 2 answer generation
- Sequential fine-tuning caused catastrophic forgetting (67% F1 drop)
- LLM self-evaluation bias inflated performance by 13 points in non-blind settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition into classification-then-generation improves medical QA performance over end-to-end approaches.
- Mechanism: Separating FHIR resource relevance classification from answer generation allows each model to specialize, reducing context window noise by filtering irrelevant resources before processing.
- Core assumption: FHIR resources can be deterministically labeled as relevant/irrelevant for a given query, and this binary classification is learnable.
- Evidence anchors: Task formulation as binary classification [section 3], EHRNavigator related work using multi-agent systems, and task separation in approach.
- Break condition: If relevant information spans many small resources with partial overlap, binary classification may over-filter and lose context needed for generation.

### Mechanism 2
- Claim: QLoRA fine-tuning enables 8B-parameter models to outperform GPT-4 on domain-specific medical tasks.
- Mechanism: Low-rank adaptation (LoRA) adds trainable parameters to attention layers while freezing base weights. QLoRA further reduces memory via 4-bit quantization. Fine-tuning on task-specific synthetic data adapts the model to FHIR schema and medical terminology patterns.
- Core assumption: Synthetic data generated by GPT-4 sufficiently represents the distribution of real patient queries and FHIR resource patterns.
- Evidence anchors: 250x smaller models outperforming GPT-4 family [abstract], LoRA rank=16 and 5 epochs specification [section 4.2].
- Break condition: If real patient queries differ significantly from GPT-4-generated synthetic queries, fine-tuned models may underperform.

### Mechanism 3
- Claim: Sequential fine-tuning causes task interference, with later tasks degrading earlier task performance.
- Mechanism: Fine-tuning on Task 2 after Task 1 causes "catastrophic forgetting"—model weights shift to optimize Task 2 loss, overwriting Task 1 representations. F1 drops from 95.52% to 31.62% after sequential training.
- Core assumption: Task 1 and Task 2 require overlapping but distinct representations; weight sharing causes interference.
- Evidence anchors: 67% F1 drop after sequential training [section 5.1.2], reverse fine-tuning resulting in low METEOR score [section 5.2.3].
- Break condition: Multi-task learning or adapter-based approaches may preserve both tasks better than sequential full fine-tuning.

## Foundational Learning

- Concept: **FHIR resource structure**
  - Why needed here: The entire approach depends on understanding FHIR resource types (Procedure, Medication, Observation, Condition, etc.) and their JSON schema. Task 1 classifies relevance per resource.
  - Quick check question: Can you identify which FHIR resource type would contain a patient's most recent hemoglobin lab result?

- Concept: **Parameter-efficient fine-tuning (PEFT/LoRA/QLoRA)**
  - Why needed here: The paper achieves its results using QLoRA with rank=16 on an A100 40GB GPU. Understanding how LoRA adapters work is essential for reproducing or modifying the approach.
  - Quick check question: If LoRA rank=16 adds ~0.1% trainable parameters, what happens to inference latency compared to the base model?

- Concept: **LLM evaluation metrics (F1, METEOR)**
  - Why needed here: Task 1 uses F1 for binary classification; Task 2 uses METEOR for text generation. Understanding why METEOR was chosen (stemming, synonymy matching) helps interpret the 42% improvement claim.
  - Quick check question: Why would exact-match metrics like BLEU perform poorly for patient-facing medical answers?

## Architecture Onboarding

- Component map: Synthea → FHIR JSON → filtered resources → GPT-4 query/answer generation → train/test split → Task 1 model → Task 2 model → Inference pipeline
- Critical path: User query → Task 1 filters resources → Task 2 generates answer
- Design tradeoffs:
  - Separate models avoid task interference but double deployment complexity vs. unified model approach
  - Synthetic data enables rapid iteration but may not cover edge cases vs. real patient data
  - LoRA rank selection balances capacity vs. overfitting risk
- Failure signatures:
  - Task 1 produces false negatives → relevant resources filtered out → Task 2 generates incomplete/wrong answers
  - Sequential training used accidentally → Task 1 performance collapses → many resources misclassified
  - LLM-as-judge evaluation without blind protocol → GPT-4o self-bias inflates perceived performance
- First 3 experiments:
  1. Reproduce Task 1 fine-tuning with 5,000-example synthetic dataset on Llama-3.1-8B, validate F1 >95% on held-out test set
  2. Test Task 1 model on manually crafted edge-case queries (ambiguous, multi-resource, temporal) to identify classification gaps
  3. Run blind vs. non-blind LLM-as-judge evaluation on Task 2 outputs to quantify self-bias in your evaluation pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-task learning (MTL) or continual pre-training (CPT) produce a single unified model that matches or exceeds the performance of separately fine-tuned models for FHIR resource retrieval and answer generation?
- Basis in paper: Section 6.3 states plans to investigate MTL strategies and CPT to develop a single model capable of handling both tasks simultaneously.
- Why unresolved: Current approach uses separate models; sequential fine-tuning experiments showed significant task interference with 67% F1 drop.

### Open Question 2
- Question: What is the optimal training dataset size for FHIR QA fine-tuning, and at what point does performance plateau or degrade?
- Basis in paper: Section 5.2.2 states deeper study is required on more size variations and improvement limits.
- Why unresolved: Only two dataset sizes (500 and 4,900 examples) were tested, showing 4.39–4.55% improvement for the larger set.

### Open Question 3
- Question: Why do different model architectures exhibit opposite responses to sequential fine-tuning, and can architectural features predict which strategy will be effective?
- Basis in paper: Section 5.2.3 states the effect of sequential fine-tuning varies across models with no conclusive pattern.
- Why unresolved: Observed architectural differences but could not explain them; Mistral NeMo performed better with direct fine-tuning while LLaMA 3.1 performed better with sequential fine-tuning.

## Limitations
- Synthetic data generation relies entirely on GPT-4 outputs without validation against real patient queries or clinical expert review
- Sequential fine-tuning causes catastrophic forgetting (67% F1 drop) that only partially recovers with mitigation strategies
- LLM-as-judge evaluation reveals significant bias (13-point inflation) suggesting reported gains may be overstated

## Confidence
- **High confidence**: Task decomposition approach works as described; separate models outperform sequential fine-tuning; QLoRA fine-tuning on synthetic data achieves stated metrics
- **Medium confidence**: Outperformance claims vs. GPT-4 family models are technically correct but may reflect evaluation bias; real-world generalization remains unverified
- **Low confidence**: Clinical validity of synthetic data, long-term stability of fine-tuned models, and privacy guarantees beyond stated "on-premise deployment"

## Next Checks
1. Conduct blind evaluation of Task 2 outputs using multiple external LLM judges (Claude, GPT-4) with inter-rater reliability metrics
2. Test Task 1 model on manually crafted edge cases: temporal queries ("symptoms last month"), ambiguous queries ("recent problems"), and multi-resource queries requiring cross-referencing
3. Perform ablation study on LoRA rank hyperparameter (rank 8, 16, 32) to identify optimal tradeoff between performance and overfitting risk