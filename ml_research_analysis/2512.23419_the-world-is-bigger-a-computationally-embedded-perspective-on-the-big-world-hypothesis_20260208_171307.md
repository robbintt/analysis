---
ver: rpa2
title: The World Is Bigger! A Computationally-Embedded Perspective on the Big World
  Hypothesis
arxiv_id: '2512.23419'
source_url: https://arxiv.org/abs/2512.23419
tags:
- agent
- learning
- interactivity
- environment
- automaton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a computationally-embedded perspective to formalize
  the big world hypothesis in continual learning. It introduces universal-local environments
  where agents are simulated as automata within the environment's state-space, leading
  to implicit capacity constraints.
---

# The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis

## Quick Facts
- arXiv ID: 2512.23419
- Source URL: https://arxiv.org/abs/2512.23419
- Reference count: 40
- Agents in universal-local environments face implicit capacity constraints; deep linear networks sustain higher interactivity than deep nonlinear networks.

## Executive Summary
This paper proposes a computationally-embedded perspective to formalize the big world hypothesis in continual learning. It introduces universal-local environments where agents are simulated as automata within the environment's state-space, leading to implicit capacity constraints. The paper defines interactivity as a measure of an agent's ability to adapt behavior based on past experience, using algorithmic complexity. An RL algorithm is developed to maximize interactivity by steering the agent toward experiences that are predictable with learning but unpredictable without it. Experiments show that deep linear networks sustain higher interactivity as capacity increases, while deep nonlinear networks struggle to maintain adaptive behavior. This demonstrates that interactivity-seeking agents naturally face the key challenges of continual learning: finite capacity limits and the need for ongoing adaptation.

## Method Summary
The method implements a self-predicting agent in a universal-local environment where the agent only observes its own actions. A linear value function is trained with TD(0) to predict future actions, while a deep policy network (linear or ReLU) is updated via meta-gradients to maximize agent-relative interactivity. Interactivity is approximated as the difference between static and dynamic TD errors: static errors use frozen value function parameters, while dynamic errors update parameters online. The policy is trained to maximize this difference, encouraging actions that are predictable to a learning agent but unpredictable otherwise. The environment is "environment-free" - the agent's actions are the only observations.

## Key Results
- Deep linear networks sustain higher interactivity as capacity increases, while deep nonlinear networks struggle to maintain adaptive behavior
- Interactivity collapses to near-zero for deep ReLU networks after ~10,000 timesteps (Figure 4)
- Interactivity scales with both network depth and width for linear networks (Figure 5)
- The self-prediction task demonstrates implicit capacity constraints without explicit regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding an agent as an automaton within the environment's state-space creates implicit capacity constraints without requiring explicit constraints.
- Mechanism: A universal-local environment (computationally universal + uniformly local) simulates the agent as a boundaried Markov process. Since the agent's internal state-space |Θ| is finite and the environment is unbounded, there exist input-output sequences the agent cannot realize. The agent is always smaller than its environment.
- Core assumption: The environment can be modeled as a computationally universal system where any algorithm can be simulated through state transitions (Church-Turing thesis applies).
- Evidence anchors:
  - [Abstract] "represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained"
  - [Section 4.1] Proposition 3: "The capacity of an embedded automaton is upper bounded by the size of its internal state space, |Θ|, which is finite"
  - [corpus] Weak direct corpus support; related work on high-capacity value functions (arxiv:2505.23150) addresses capacity but not embedding constraints
- Break condition: If the environment's state-space is bounded or the agent's internal state-space is unbounded, the implicit capacity constraint dissolves.

### Mechanism 2
- Claim: Interactivity, defined via algorithmic complexity, measures an agent's capability for continually adaptive behavior.
- Mechanism: Interactivity = K(bt:t+T-1|ε) - K(bt:t+T-1|b0:t-1), where K is conditional algorithmic complexity. High interactivity requires future behavior that is complex (high unconditional K) but predictable from past experience (low conditional K). This forces the agent to continually learn rather than converge.
- Core assumption: Algorithmic complexity meaningfully captures behavioral complexity for individual sequences, and the interactivity thesis (speculative) holds: interactivity measures adaptive capability.
- Evidence anchors:
  - [Abstract] "measures an agent's ability to continually adapt its behaviour by learning new predictions"
  - [Section 4.2] Definition 7 formalizes interactivity; Section 4.3 states "interactivity measures a capability for continually adaptive behaviour" as a thesis
  - [corpus] No direct corpus validation of interactivity measure; intrinsic motivation literature (cited: Schmidhuber 1991, Still & Precup 2012) uses Shannon information, not algorithmic
- Break condition: If the agent can compute exact algorithmic complexity (uncomputable in general), or if behavioral sequences are compressible without learning, the measure fails to incentivize adaptation.

### Mechanism 3
- Claim: Approximating interactivity via prediction errors from a value function enables tractable reinforcement learning.
- Mechanism: Agent-relative complexity replaces Kolmogorov complexity with temporal difference (TD) errors from a value function. Conditional complexity uses dynamic TD errors (parameters updated during roll-out); unconditional complexity uses static TD errors (frozen reference parameters). The policy maximizes the difference between static and dynamic prediction errors.
- Core assumption: TD errors from a bounded function approximator correlate with algorithmic complexity for the behavioral sequences encountered.
- Evidence anchors:
  - [Abstract] "deep linear networks sustain higher interactivity as capacity increases, while deep nonlinear networks struggle"
  - [Section 5] Definition 8 formalizes agent-relative complexity; Figure 4 shows deep ReLU networks fail while deep linear networks sustain interactivity
  - [corpus] Adjusting model size in continual learning (arxiv:2408.07588) addresses capacity but not this specific approximation
- Break condition: If the value function converges (stops changing), interactivity collapses to zero. If the policy becomes fixed, the value function can converge, eliminating interactivity.

## Foundational Learning

- Concept: **Kolmogorov/Algorithmic Complexity**
  - Why needed here: Interactivity is defined as a difference of conditional algorithmic complexities; understanding K(x|y) as the shortest program computing x given y is essential to grasp the objective.
  - Quick check question: Given strings x = "0101010101" and y = "01", can you explain why K(x|y) < K(x)?

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Proposition 2 proves an embedded automaton with boundary-space coinciding with input-space interacts with a POMDP; the agent only observes part of the environment state.
  - Quick check question: What distinguishes a POMDP from a fully observable MDP, and why must an agent maintain internal state in the former?

- Concept: **Temporal Difference Learning (TD)**
  - Why needed here: The agent-relative interactivity approximation uses TD(0) errors; you must understand δt = bt+1 + γv(bt+1) - v(bt) and semi-gradient updates.
  - Quick check question: In TD(0) learning, why is the update called "semi-gradient" rather than "full gradient"?

## Architecture Onboarding

- Component map:
  1. **Universal-local environment** (Ω, Ξ, T): Infinite countable state-space, finite symbol set, polynomial-time computable transition function. Decomposes into identical boundaried Markov processes.
  2. **Embedded automaton** (Ω|X, Ω|Y, Ω|Θ, u, π): Input space (observations), output space (actions), internal state-space (parameters), update function u (learning rule), output function π (policy).
  3. **Value function v**: Predicts discounted sum of future behavior bt+1, bt+2, ...; trained with TD(0).
  4. **Policy π**: Maps (observation, internal state) → action; trained via meta-gradients to maximize agent-relative interactivity.
  5. **Static vs. dynamic parameter tracking**: Static uses θref (frozen); dynamic updates θ along roll-out.

- Critical path:
  1. Define action dimension d (paper uses d=1000), horizon T (paper uses T=10).
  2. Initialize policy θ0 and value function W0 randomly; sample initial action b0.
  3. Roll-out T actions using current policy πθ.
  4. Compute static TD errors: freeze W, accumulate δ² along roll-out.
  5. Compute dynamic TD errors: update W at each step, accumulate δ² along roll-out.
  6. Compute interactivity: J(θ) = Σ(static δ² - dynamic δ²).
  7. Update policy θ via gradient step on -J(θ).
  8. Take action, update value function with single TD(0) step.
  9. Repeat from step 3.

- Design tradeoffs:
  - **Linear vs. nonlinear policy**: Deep linear sustains interactivity; deep ReLU collapses (Figure 4). Paper suggests nonlinear networks exhibit loss of plasticity.
  - **Horizon T**: Longer horizons increase effective depth of meta-gradient computation (O(T·D²) complexity). T=10 used in experiments.
  - **Width vs. depth**: Figure 5 shows depth increases interactivity more than width; deeper networks have more capacity for rapid parameter change.
  - **Optimizer**: RMSProp preferred over Adam for stability (paper finding).

- Failure signatures:
  - Interactivity collapses to near-zero and stays flat (Figure 4, ReLU curve).
  - Actions lack predictable structure (Figure 4, left: ReLU actions appear random/noisy).
  - Value function parameters stop changing → interactivity = 0 by definition.
  - Meta-gradient instability with deep nonlinear networks (Section B.2 notes curvature issues).

- First 3 experiments:
  1. **Replicate Figure 4**: Implement self-predicting agent with d=1000, T=10. Compare deep linear (depth 4) vs. deep ReLU (depth 4). Confirm linear sustains interactivity while ReLU collapses over ~10k steps.
  2. **Ablate capacity**: Systematically vary network depth (1, 2, 4, 8) and width (100, 500, 1000) for linear policy. Plot sustained interactivity vs. capacity to validate Figure 5 scaling behavior.
  3. **Introduce external observations**: Modify the self-predicting setup to include observations from a simple external environment (e.g., sinusoidal input). Test whether interactivity-seeking transfers to predicting environment dynamics, validating the auxiliary objective hypothesis in Discussion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does an agent's ability to sustain high interactivity imply it possesses the capacity to learn any goal-directed behavior, such as maximizing cumulative discounted reward?
- Basis in paper: [explicit] In the Discussion, the authors speculate that "if an agent can sustain a particular level of interactivity, then it possesses the capacity to learn any behaviour with equal or lower interactivity—including those that maximize the cumulative sum of expected discounted reward."
- Why unresolved: This is stated as a "speculation" or refinement of the interactivity thesis, not a proven theorem.
- What evidence would resolve it: A theoretical proof linking interactivity bounds to general learning capacity, or empirical demonstrations where agents with high sustained interactivity successfully master standard RL tasks.

### Open Question 2
- Question: Can interactivity be effectively operationalized as an auxiliary objective or intrinsic reward in conventional reinforcement learning environments?
- Basis in paper: [explicit] The Discussion notes that "interactivity can also serve as an auxiliary objective for agents in conventional reinforcement learning environments" to incentivize directed exploration.
- Why unresolved: The paper only evaluates interactivity in an "environment-free" self-prediction task; applying it to external environments requires addressing the computational cost of learning models to calculate meta-gradients.
- What evidence would resolve it: Experiments applying the interactivity-seeking algorithm to standard RL benchmarks (e.g., Atari) showing improved sample efficiency or exploration compared to curiosity-driven baselines.

### Open Question 3
- Question: What specific architectural or algorithmic modifications allow deep nonlinear networks to sustain interactivity effectively?
- Basis in paper: [explicit] The Discussion suggests the evaluation task incentivizes "alternative sequence architectures, learning algorithms designed for continual learning... or better procedures for selecting hyperparameters" to fix the failure of deep ReLU networks.
- Why unresolved: The experiments showed deep nonlinear networks struggle to maintain adaptive behavior, but the paper does not identify a specific remedy.
- What evidence would resolve it: Empirical results demonstrating that specific regularization methods (e.g., spectral normalization) or alternative architectures (e.g., LSTMs) achieve high interactivity in the proposed synthetic benchmark.

## Limitations

- The interactivity thesis remains speculative with no empirical validation that high interactivity implies adaptive capability
- The paper only evaluates in an environment-free self-prediction task, not on standard continual learning benchmarks
- The algorithmic complexity approximation via TD errors lacks theoretical grounding and may fail for complex behavioral sequences

## Confidence

**High confidence**: The mechanism of implicit capacity constraints through embedding (Mechanism 1) is mathematically sound given the computational universality assumption. The experimental observation that deep linear networks sustain higher interactivity than deep ReLU networks is directly demonstrated and reproducible.

**Medium confidence**: The definition and computation of interactivity as a difference of conditional algorithmic complexities is formally correct, but its practical utility as a measure of adaptive capability remains unproven. The approximation using TD errors is a reasonable heuristic but lacks theoretical grounding.

**Low confidence**: The claim that interactivity-seeking agents "naturally face the key challenges of continual learning" extrapolates from a self-prediction task to general continual learning without validation. The discussion of how this applies to real-world continual learning scenarios is speculative.

## Next Checks

1. **Validate the interactivity thesis**: Design experiments where agents trained with interactivity objective are compared against standard continual learning baselines (e.g., EWC, MAS) on benchmark tasks like permuted MNIST or incremental sine waves. Measure not just interactivity but actual learning performance and forgetting.

2. **Test environmental universality**: Implement the self-predicting agent in environments with varying degrees of computational universality (e.g., finite-state vs. universal computation). Verify that the implicit capacity constraints and interactivity dynamics scale as predicted across this spectrum.

3. **Characterize approximation breakdown**: Systematically vary the complexity of behavioral sequences and measure the correlation between actual algorithmic complexity and the TD error approximation. Identify conditions under which the approximation fails and characterize the nature of the failure.