---
ver: rpa2
title: The Effect of State Representation on LLM Agent Behavior in Dynamic Routing
  Games
arxiv_id: '2506.15624'
source_url: https://arxiv.org/abs/2506.15624
tags:
- game
- agents
- agent
- behavior
- route
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically investigates how different natural language
  state representations affect the strategic behavior of large language model (LLM)
  agents in dynamic routing games. The authors propose a framework characterizing
  representations along three axes: action informativeness, reward informativeness,
  and prompting style.'
---

# The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games

## Quick Facts
- arXiv ID: 2506.15624
- Source URL: https://arxiv.org/abs/2506.15624
- Reference count: 10
- Primary result: Summarized state representations with regret feedback lead LLM agents to more stable, equilibrium-like behavior in dynamic routing games compared to full-chat representations.

## Executive Summary
This study systematically investigates how different natural language state representations affect the strategic behavior of large language model (LLM) agents in dynamic routing games. The authors propose a framework characterizing representations along three axes: action informativeness, reward informativeness, and prompting style. Applying this framework to a repeated atomic selfish routing game with Braess's paradox, they find that summarized state representations lead to more stable and equilibrium-like behavior compared to full-chat representations. Specifically, providing agents with summarized historical information, regret-based feedback rather than raw payoffs, and limited information about others' actions results in behavior closer to theoretical equilibrium predictions and with more stable game play. These findings offer practical guidance for researchers designing LLM agents for strategic decision-making in dynamic environments.

## Method Summary
The authors evaluate LLM agents in repeated atomic selfish routing games (Game A: 2 routes, Game B: 3 routes with Braess's paradox) across 8 state representations varying along 3 axes (action informativeness, reward informativeness, prompting style). The study uses n=18 GPT-4o agents, T=40 rounds per game, and computes metrics including mean agents on focal route, mean payoff/regret per agent, mean switches per agent, and Kendall's τ between round number and equilibrium deviation score. The experimental design employs a 2×2×2 factorial structure across the representation axes, with temperature=1 and chain-of-thought prompting.

## Key Results
- Summarized representations lead to significantly lower regret and more stable behavior compared to full-chat representations
- Regret-based feedback produces more equilibrium-convergent behavior than raw payoff feedback, particularly in games with dominant strategies
- Limiting visibility to only own actions results in more stable play compared to full visibility of others' actions
- Full-chat representations cause higher switch counts and less stable convergence, likely due to recency bias and context window effects

## Why This Works (Mechanism)

### Mechanism 1: Summarization-Induced Stability
Summarized history prompts lead to more stable, equilibrium-convergent behavior than full-chat transcripts. Summarization acts as a noise filter, compressing unstructured long histories into structured signals that enforce a meta-view of the game state rather than reactive turn-taking.

### Mechanism 2: Regret as a Gradient Signal
Providing regret (the gap between realized payoff and optimal payoff) leads to faster convergence and lower variance than raw payoffs. Regret serves as a pre-computed "error signal" that reduces cognitive load by explicitly quantifying opportunity cost.

### Mechanism 3: Strategic Myopia via Action Visibility
Limiting visibility to only the agent's own actions (hiding others' choices) stabilizes play. Full visibility of others' actions triggers complex, often flawed, "anticipatory" reasoning resulting in non-equilibrium play.

## Foundational Learning

- **Concept: Braess's Paradox**
  - Why needed here: This is the specific game environment used. Understanding that adding capacity (a bridge route) can *increase* cost for everyone is essential to interpreting why agents might fail to find the equilibrium.
  - Quick check question: If agents converge to the "bridge" route in Game B, are they at a Pareto optimal outcome? (Answer: No, they are at a Nash Equilibrium that is worse than the equilibrium in Game A).

- **Concept: No-Regret Learning**
  - Why needed here: The paper compares LLMs to algorithms like EXP3 and MWU. You must understand that "low regret" implies an agent is performing nearly as well as the best fixed strategy in hindsight.
  - Quick check question: Does achieving zero regret mean an agent won every round? (Answer: No, it means they did as well as the best single strategy would have done over the whole period).

- **Concept: Context Window & "Lost in the Middle"**
  - Why needed here: Explains the failure mode of "Full-Chat" representations. As history grows, relevant data may be buried in the middle of the prompt, degrading performance.
  - Quick check question: Why might an LLM ignore its instructions from Round 1 when it is Round 40? (Answer: The instructions are at the "start" of the long context, which models often weight less effectively than the start/end).

## Architecture Onboarding

- **Component map:** Python simulation environment -> State Encoder (maps history to prompt based on 3 axes) -> GPT-4o agents via LangChain with temperature=1

- **Critical path:** The prompt construction pipeline. The system must dynamically inject the specific historical summary (e.g., S-RO format) for each of the 18 agents before every API call.

- **Design tradeoffs:** Regret computation increases orchestration compute overhead but reduces token cost/complexity for the LLM. Summaries are token-efficient and stable but lose granular conversational nuance.

- **Failure signatures:** High Switch Count indicates "myopic" chasing or instability. Equilibrium Deviation indicates failure to process regret signals or identify dominant strategies.

- **First 3 experiments:**
  1. Baseline Replication: Run Game A and B with S-RO (Summary, Regret, Own-actions) and verify convergence to 50/50 split (Game A) and Bridge route (Game B) using 18 agents.
  2. Ablation on Feedback: Run Game B with S-P (Summary, Payoff, Own-actions). Observe if agents fail to identify the dominant strategy without the explicit "regret" gradient.
  3. Context Stress Test: Run a 100-round game with F-PO (Full-Chat). Monitor token usage and observe if agent performance degrades as the context window fills.

## Open Questions the Paper Calls Out

### Open Question 1
How does history truncation depth affect LLM agent convergence and stability in games with longer time horizons and larger state spaces? The authors retained full history in all experiments since the 40-round games fit within the context window, leaving the effects of partial history unexplored.

### Open Question 2
Do the effects of state representation axes generalize to other game classes with different equilibrium structures? Only routing games with Braess's paradox were tested; these have simple, well-characterized equilibria that may not represent more complex strategic environments.

### Open Question 3
How do sequential reasoning models (e.g., o1, o3, DeepSeek R1) respond to the same state representation variations? Only GPT-4o was tested; reasoning models may process summarized vs. full-chat prompts differently due to their extended chain-of-thought capabilities.

### Open Question 4
To what extent do LLM agent behaviors under different state representations align with documented human behavioral biases in strategic settings? Qualitative analysis in Appendix C is described as "only anecdotal observations"; no systematic comparison to biases like loss aversion or anchoring was conducted.

## Limitations

- Unknown trial counts make it impossible to assess statistical significance or compute proper error bars for reproduction
- Context window dynamics handling is not specified for potential token limit issues in later rounds
- JSON parsing reliability is not described for handling invalid or malformed agent outputs
- Only one game class (routing games with Braess's paradox) was tested, limiting generalizability
- No systematic comparison to human behavioral biases in strategic settings was conducted

## Confidence

- **High confidence** in the core finding that summarized representations outperform full-chat for routing games, supported by multiple metrics
- **Medium confidence** in the specific mechanism explanations (recency bias, attention dynamics) as the paper provides behavioral evidence but limited ablations to isolate individual cognitive processes
- **Low confidence** in generalizability beyond routing games, as the study is confined to a specific class of congestion games with deterministic costs

## Next Checks

1. **Context stress test**: Run a 100-round simulation with full-chat representation and log token usage per round to verify whether context window issues emerge and correlate with performance degradation.

2. **Ablation of regret computation**: Implement a version where regret is not pre-computed but agents must infer opportunity costs from raw payoffs, measuring impact on convergence time and variance.

3. **Cross-game transferability**: Test the same representation framework on a different strategic environment (e.g., market entry game or coordination game) to assess whether summarized representations provide similar benefits outside routing contexts.