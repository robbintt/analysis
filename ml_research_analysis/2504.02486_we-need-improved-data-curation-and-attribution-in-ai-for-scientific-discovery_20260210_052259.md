---
ver: rpa2
title: We Need Improved Data Curation and Attribution in AI for Scientific Discovery
arxiv_id: '2504.02486'
source_url: https://arxiv.org/abs/2504.02486
tags:
- data
- synthetic
- datasets
- real
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the challenges of data curation and attribution
  in AI-driven scientific discovery. The authors analyze dataset adoption rates and
  synthetic data prevalence across platforms like HuggingFace and Zenodo, finding
  that 75% of datasets are underutilized and synthetic data remains largely untracked.
---

# We Need Improved Data Curation and Attribution in AI for Scientific Discovery

## Quick Facts
- arXiv ID: 2504.02486
- Source URL: https://arxiv.org/abs/2504.02486
- Reference count: 19
- Datasets on HuggingFace: 75% underutilized, synthetic data largely untracked

## Executive Summary
This study examines critical challenges in data curation and attribution for AI-driven scientific discovery. Through analysis of dataset adoption patterns across platforms like HuggingFace and Zenodo, the authors reveal that 75% of datasets are underutilized while synthetic data remains largely untracked. The research demonstrates that distinguishing synthetic from real data is becoming increasingly difficult across multiple scientific modalities including molecules, text, single-cell RNA-seq, and IR spectra. To address these issues, the authors propose automated metadata curation through agentic workflows and watermarking of human-generated data for provenance tracking, showing that watermarking just 40% of human-generated data could maintain model robustness while ensuring balanced integration of synthetic and human-generated content.

## Method Summary
The authors conducted a comprehensive analysis of dataset adoption rates across major scientific data platforms, examining usage patterns and tracking synthetic data prevalence. They employed generative models to test the ability to distinguish synthetic from real data across multiple scientific modalities including molecular structures, textual data, single-cell RNA sequencing, and infrared spectra. The research team developed automated metadata curation workflows using agentic AI systems and implemented watermarking techniques for human-generated data provenance. Through simulation studies, they evaluated the impact of different watermarking coverage levels (0-100%) on model performance and data quality preservation.

## Key Results
- 75% of datasets on platforms like HuggingFace and Zenodo are underutilized
- Synthetic data remains largely untracked across scientific domains
- Watermarking 40% of human-generated data can sustain model robustness while promoting balanced synthetic-human data integration

## Why This Works (Mechanism)
The approach works by addressing the fundamental problem of data provenance and traceability in AI-driven scientific discovery. By implementing watermarking for human-generated data, the system creates a verifiable signal that helps maintain data quality while allowing synthetic data to supplement training. The automated metadata curation through agentic workflows reduces the manual burden of data organization while improving discoverability and attribution. The mechanism leverages the fact that human-generated data typically has higher quality and reliability than synthetic alternatives, so preserving its signal while allowing synthetic data to fill gaps creates a more robust training ecosystem.

## Foundational Learning

**Data Provenance** - Understanding data origin and lineage is crucial for scientific reproducibility and trust. Quick check: Verify data source documentation and modification history.

**Synthetic Data Detection** - Models must distinguish between human-generated and synthetic data to maintain quality standards. Quick check: Test model performance with mixed datasets containing known synthetic portions.

**Metadata Curation** - Proper metadata enables dataset discovery and appropriate use. Quick check: Validate metadata completeness against FAIR data principles.

**Watermarking Techniques** - Digital watermarks provide traceable signatures without compromising data utility. Quick check: Confirm watermark robustness against common preprocessing operations.

**Agentic Workflows** - Autonomous systems can automate repetitive curation tasks while maintaining consistency. Quick check: Monitor workflow decision patterns for unexpected biases.

## Architecture Onboarding

**Component Map**: Data Ingestion -> Metadata Extraction -> Watermark Application -> Storage -> Agentic Curation -> Distribution

**Critical Path**: Human-generated data flows through watermarking, metadata extraction, and agentic curation before distribution. Synthetic data follows similar paths but skips watermarking.

**Design Tradeoffs**: Watermarking intensity vs. data utility (stronger marks reduce utility), automation vs. human oversight (fully automated risks missing edge cases), coverage percentage (balancing robustness vs. computational cost).

**Failure Signatures**: Watermark degradation during preprocessing, metadata extraction failures on non-standard formats, agentic workflow deadlocks on ambiguous cases, distribution bottlenecks from excessive validation.

**First Experiments**:
1. Test watermark persistence across common data transformations (normalization, tokenization, compression)
2. Measure metadata extraction accuracy on heterogeneous scientific data formats
3. Evaluate agentic workflow performance on datasets with missing or incomplete metadata

## Open Questions the Paper Calls Out
- How to balance watermark strength with downstream model performance across different scientific modalities
- Optimal percentage of human-generated data to watermark for maintaining robustness while enabling synthetic data integration
- Long-term effects of synthetic data accumulation on model behavior and scientific validity
- Standardization approaches for watermarking across different data types and scientific domains

## Limitations
- Focus on technical solutions without addressing broader policy and incentive structures
- Limited real-world deployment testing beyond controlled experiments
- Potential privacy concerns with persistent data watermarks
- Computational overhead of automated curation workflows not fully quantified

## Confidence

| Claim | Confidence |
|-------|------------|
| 75% dataset underutilization rate | High |
| Synthetic data detection difficulty | High |
| Watermarking effectiveness at 40% coverage | Medium |
| Automated curation workflow efficiency | Medium |
| Cross-domain applicability of solutions | Low |

## Next Checks

1. Deploy watermarking system on a production scientific dataset repository and measure adoption rates
2. Conduct longitudinal study on model performance using watermarked vs. non-watermarked training data
3. Evaluate privacy implications of persistent watermarks on sensitive scientific data