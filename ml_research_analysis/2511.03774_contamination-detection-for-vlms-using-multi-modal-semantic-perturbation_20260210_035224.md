---
ver: rpa2
title: Contamination Detection for VLMs using Multi-Modal Semantic Perturbation
arxiv_id: '2511.03774'
source_url: https://arxiv.org/abs/2511.03774
tags:
- contaminated
- contamination
- clean
- performance
- perturbed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting test-set contamination
  in vision-language models (VLMs), where models memorize benchmark data instead of
  learning genuine reasoning capabilities. The authors propose a novel method called
  multi-modal semantic perturbation that generates perturbed versions of image-question
  pairs by altering visual semantics while preserving overall composition, testing
  whether models fail to generalize to these controlled variations.
---

# Contamination Detection for VLMs using Multi-Modal Semantic Perturbation

## Quick Facts
- **arXiv ID**: 2511.03774
- **Source URL**: https://arxiv.org/abs/2511.03774
- **Reference count**: 40
- **Primary result**: Multi-modal semantic perturbation method successfully detects test-set contamination in vision-language models with performance drops on perturbed data while clean models maintain or improve performance

## Executive Summary
This paper addresses the critical problem of test-set contamination in vision-language models (VLMs), where models memorize benchmark data instead of learning genuine reasoning capabilities. The authors propose a novel multi-modal semantic perturbation approach that generates perturbed versions of image-question pairs by altering visual semantics while preserving overall composition. This method tests whether models can generalize to controlled variations, with contaminated models showing performance drops while clean models maintain or improve performance. The approach satisfies three key requirements: it works without clean model access, detects contamination across different fine-tuning strategies, and produces detection signals that correlate with contamination degree.

## Method Summary
The proposed method generates perturbed image-question pairs by first changing the correct answer choice, then using an LLM to generate a caption describing the image with the new answer. This caption guides a diffusion model to create a semantically perturbed image that preserves overall composition but alters specific visual elements corresponding to the new answer. The perturbation operates at the semantic level, ensuring that the modified examples remain relevant to the original task while testing the model's generalization ability. By comparing model performance on original versus perturbed examples, the method can detect contamination: contaminated models show performance drops on perturbed data while clean models maintain or improve performance. The approach is designed to be practical (requiring no clean model access), reliable (working across different fine-tuning strategies), and consistent (correlating with contamination degree).

## Key Results
- Multi-modal semantic perturbation successfully detects contamination across multiple VLM architectures and benchmarks
- Contaminated models show significant performance drops on perturbed data while clean models maintain or improve performance
- The method outperforms existing contamination detection approaches like multi-modal leakage, circular evaluation, and choice confusion on RealWorldQA and MMStar benchmarks

## Why This Works (Mechanism)
The method works by exploiting the fundamental difference between genuine reasoning and memorization. When VLMs memorize training data, they learn specific associations between image features and correct answers. By generating semantically perturbed examples that maintain overall image composition but change specific visual elements corresponding to different answer choices, the method creates controlled variations that test whether the model has truly learned reasoning capabilities or merely memorized specific patterns. Clean models, having learned genuine reasoning, can handle these semantic variations and often perform better on perturbed examples that may be easier to reason about. Contaminated models, having memorized specific image-answer pairs, fail to generalize to these controlled perturbations and show performance degradation. The multi-modal nature of the approach ensures that perturbations affect both visual and semantic aspects, making it difficult for models to compensate through language-only reasoning.

## Foundational Learning

**Vision-Language Models (VLMs)**: Neural networks that process both visual and textual inputs to perform tasks requiring joint understanding of images and language. Needed to understand the target models being evaluated. Quick check: Can the model process both image and text inputs simultaneously?

**Test-Set Contamination**: When benchmark data leaks into training sets, causing models to memorize rather than learn. Needed to understand the problem being solved. Quick check: Does the model show suspiciously high performance on known benchmarks?

**Semantic Perturbation**: Controlled modification of input meaning while preserving overall structure. Needed to understand how the detection method works. Quick check: Does the perturbed example maintain task relevance while changing specific details?

**Diffusion Models**: Generative models that create images through iterative denoising processes. Needed to understand the image generation component. Quick check: Can the model generate realistic images from text prompts?

**Large Language Models (LLMs)**: Models trained on vast text corpora for generating and understanding language. Needed to understand the caption generation component. Quick check: Can the model generate coherent captions describing images with specified answer choices?

## Architecture Onboarding

**Component Map**: Image-Question Pair -> Answer Choice Modification -> LLM Caption Generation -> Diffusion Model Image Generation -> Perturbed Example -> Model Evaluation

**Critical Path**: The core detection pipeline follows: (1) original image-question pair, (2) answer modification, (3) LLM caption generation, (4) diffusion-based image perturbation, (5) model evaluation on original vs. perturbed pairs. Performance drops on perturbed examples indicate contamination.

**Design Tradeoffs**: The method trades computational overhead (generating multiple perturbed versions) for practical contamination detection without clean model access. Using LLM captions provides semantic guidance but may introduce bias. Diffusion models enable semantic-level perturbations but require significant compute resources.

**Failure Signatures**: Method may fail when LLM captions introduce semantic drift, when perturbed examples become unfair or too difficult, or when models can compensate through language-only reasoning. Computational overhead may limit scalability for large-scale evaluation.

**First Experiments**:
1. Test contamination detection on VLMs with known contamination levels to establish baseline performance
2. Evaluate method sensitivity to perturbation magnitude and semantic fidelity
3. Compare detection accuracy against existing contamination detection methods

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Reliance on LLMs for caption generation may introduce semantic drift or bias affecting perturbation quality
- No thorough examination of whether perturbed examples remain fair and representative of original task distribution
- Significant computational overhead from generating multiple perturbed versions per example

## Confidence
- **High confidence**: The core methodology of using semantic perturbations to detect memorization effects is technically sound and well-justified
- **Medium confidence**: The three key requirements (practicality, reliability, consistency) are demonstrated on tested benchmarks, but broader validation is needed
- **Medium confidence**: Claims about outperforming existing methods are supported by experiments, though comparison baselines could be expanded

## Next Checks
1. Test the method's effectiveness on VLMs trained on entirely different domains (e.g., medical imaging, satellite imagery) to assess domain generalization
2. Conduct ablation studies to quantify the impact of LLM-generated captions on perturbation quality and detection accuracy
3. Perform human evaluation studies to verify that perturbed examples maintain task validity and don't introduce unfair difficulty increases