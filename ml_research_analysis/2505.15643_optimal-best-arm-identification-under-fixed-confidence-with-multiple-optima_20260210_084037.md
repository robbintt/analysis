---
ver: rpa2
title: Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima
arxiv_id: '2505.15643'
source_url: https://arxiv.org/abs/2505.15643
tags:
- arms
- optimal
- multiple
- identification
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of best-arm identification in
  stochastic multi-armed bandits when multiple arms achieve the same optimal expected
  reward. The Track-and-Stop algorithm, known for its strong empirical performance
  in the unique-optimal-arm case, lacks a complete theoretical justification in the
  presence of multiple optimal arms.
---

# Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima

## Quick Facts
- arXiv ID: 2505.15643
- Source URL: https://arxiv.org/abs/2505.15643
- Reference count: 18
- One-line primary result: Modified Track-and-Stop algorithm achieves instance-optimal sample complexity for best-arm identification with multiple optimal arms

## Executive Summary
This paper addresses a fundamental gap in stochastic multi-armed bandit theory by providing the first formal proof of optimality for the Track-and-Stop algorithm when multiple arms achieve the same optimal expected reward. The work introduces a novel generalized likelihood ratio stopping rule that ensures δ-PAC guarantees even with non-unique optimal arms. By deriving a new information-theoretic lower bound that explicitly accounts for multiple optima, the paper demonstrates that the modified algorithm matches this bound, closing a longstanding theoretical gap and offering insights for designing efficient exploration strategies in modern bandit problems.

## Method Summary
The method modifies the Track-and-Stop algorithm with a generalized likelihood ratio (GLR) stopping rule specifically calibrated for multiple optimal arms. The algorithm uses tracking-based sampling rules (C-Tracking or D-Tracking) to enforce forced exploration, ensuring empirical arm pull proportions converge to theoretically optimal allocations. The stopping criterion computes a GLR statistic that tests whether a hypothesized set of optimal arms contains all true optima, stopping when this statistic exceeds a carefully calibrated threshold. The approach maintains δ-PAC guarantees while achieving instance-optimal sample complexity, with the threshold designed to control false identification probability across all parameter configurations and time steps.

## Key Results
- Modified Track-and-Stop algorithm achieves instance-optimal sample complexity even when multiple arms share the optimal reward
- New information-theoretic lower bound explicitly characterizes problem difficulty with multiple optimal arms
- Stopping rule with calibrated threshold ensures δ-PAC guarantees for any number of optimal arms
- Theoretical analysis demonstrates automatic adaptation to problem structure with multiple optima

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A generalized likelihood ratio (GLR) stopping rule with a specifically calibrated threshold ensures a fixed confidence level (δ-PAC guarantee) even when multiple optimal arms exist.
- Mechanism: The stopping rule computes a GLR statistic Z(t) that tests the null hypothesis "the set {b₁,...,bₘ} contains all optimal arms" against alternatives where a non-candidate arm a is superior. The algorithm stops when Z(t) exceeds a threshold β(t, δ). The threshold is explicitly designed to control the probability of falsely rejecting the null across all possible parameter configurations and all time steps, ensuring P(false identification) ≤ δ/K^M. The proof uses a change-of-measure argument combined with a time-uniform concentration bound.
- Core assumption: Reward distributions belong to a one-parameter exponential family, enabling tractable KL divergence computation.
- Evidence anchors: [abstract]: "propose a modified stopping rule that ensures instance-optimality"; [section VI]: "Pμ(τδ < ∞, âτδ ∉ {1,...,M}) ≤ δ/K^M"; [corpus]: Related work on fixed-confidence BAI (e.g., EVaR-Optimal, DP-BAI) similarly relies on thresholded stopping rules for δ-PAC guarantees.
- Break condition: Fails if reward distributions fall outside the exponential family (KL divergences become intractable) or if the threshold calibration does not account for the union bound over all (M+1)-arm tuples.

### Mechanism 2
- Claim: An information-theoretic lower bound, derived via a change-of-measure argument, correctly characterizes the intrinsic difficulty of the problem with multiple optima.
- Mechanism: Theorem 4 establishes a lower bound on sample complexity: E[τδ] ≥ T*(μ)·kl(1-δ∥δ). The complexity term T*(μ) is defined via an optimization problem involving a supremum over allocation weights w and a minimum over all non-optimal arms a. This formulation captures the fundamental requirement: the algorithm must gather enough evidence to reject the hypothesis that any suboptimal arm a is superior to all arms in the hypothesized optimal set. The key insight is that with multiple optima, the "alternative" hypothesis space is more complex.
- Core assumption: The algorithm is δ-PAC, meaning it must output an optimal arm with probability ≥ 1-δ.
- Evidence anchors: [abstract]: "new information-theoretic lower bound explicitly accounting for multiple optimal arms"; [section IV]: Equation (12) and the surrounding derivation provide the formal bound; [corpus]: The use of change-of-measure arguments for lower bounds is a standard technique in the BAI literature, as cited in the paper's references [2, 14].
- Break condition: The bound is asymptotic (δ→0). It may not be tight for non-asymptotic regimes or if the KKT conditions used to solve the optimization problem are not satisfied.

### Mechanism 3
- Claim: A tracking-based sampling rule, by enforcing forced exploration, ensures that the empirical arm pull proportions converge to the theoretically optimal proportions.
- Mechanism: The sampling rule (C-Tracking or D-Tracking, Section V.A) actively guides exploration. It targets the optimal allocation w*(μ), which is the solution to the optimization problem in the lower bound. "Forced exploration" (e.g., sampling an arm if its count falls below √t) is critical to prevent an arm from being starved of samples, which could lead to non-convergence of its empirical mean and a failure to identify the correct optimal set. The proof shows that this ensures Ni(t)/t → w*i for all arms i.
- Core assumption: The forced exploration is sufficient to ensure consistent estimation of all arm means.
- Evidence anchors: [section V.A]: "Both sampling rules are guaranteed to ensure that the empirical sampling proportions converge to the optimal allocation"; [corpus]: The Track-and-Stop strategy is a well-established method for achieving optimal sample complexity in fixed-confidence BAI, as referenced from Garivier & Kaufmann (2016).
- Break condition: Fails if the forced exploration schedule is too weak, causing some arm's empirical mean to remain inconsistent, or if the optimization for w* is computationally intractable for large K.

## Foundational Learning

- Concept: **Kullback-Leibler (KL) Divergence**
  - Why needed here: KL divergence is the fundamental measure of "distance" between probability distributions used throughout the paper. It quantifies the information required to distinguish between the true model and an alternative, forming the core of both the lower bound (Theorem 4) and the GLR statistic.
  - Quick check question: For two Bernoulli distributions with means μ=0.5 and λ=0.8, what does d(μ, λ) represent in the context of this paper?

- Concept: **One-Parameter Exponential Family**
  - Why needed here: The theoretical results are derived specifically for this class of distributions (e.g., Bernoulli, Gaussian with known variance, Poisson). This assumption is crucial because it provides a closed-form, convex relationship between the mean μ and the natural parameter θ, which simplifies the KL divergence computation and the optimization in the lower bound.
  - Quick check question: Why is the assumption that d(μ, λ) is convex in λ essential for the analysis of the lower bound?

- Concept: **δ-PAC (Probably Approximately Correct) Algorithm**
  - Why needed here: This is the formal guarantee provided by the algorithm. It defines the performance requirement: the algorithm must return an arm from the true optimal set A* with probability at least 1-δ. All sample complexity analysis is predicated on this condition.
  - Quick check question: If δ = 0.1 and the true optimal set is A* = {arm1, arm2}, what is the required property of the algorithm's output?

## Architecture Onboarding

- Component map: Sampling Rule (C-Tracking or D-Tracking) -> GLR Stopping Module -> Recommendation Module
- Critical path:
  1. **Initialization**: Pull each arm once. Initialize empirical means.
  2. **Sampling Loop**: At each step t, use the Sampling Rule (e.g., D-Tracking) to select an arm Aₜ. Observe reward. Update statistics.
  3. **Stopping Check**: Compute Z(t). If Z(t) > β(t, δ), exit loop.
  4. **Recommendation**: Identify the set {b₁,...,bₘ} that maximized the statistic at stopping time. Return any arm from this set as the answer.

- Design tradeoffs:
  - **Threshold Selection**: The paper provides a specific form β(t, δ) = log(Ct^α/δ). In practice, tuning constants C and α could trade off between the empirical stopping time and the robustness of the δ-PAC guarantee.
  - **Sampling Rule Choice**: C-Tracking guarantees tracking but can be less efficient. D-Tracking is more practical but relies on a heuristic forced exploration condition. The choice depends on implementation complexity vs. empirical performance.
  - **Computational Cost**: Computing the supremum in the lower bound and the GLR statistic requires solving optimization problems. This cost scales with the number of arms K and the hypothesized size of the optimal set M.

- Failure signatures:
  - **Non-termination**: Z(t) never exceeds β(t, δ). This could happen if the threshold is too conservative or if the sampling rule fails to provide data to distinguish arms.
  - **Premature Stopping with Error**: Algorithm stops and returns a suboptimal arm with probability > δ. This indicates a failure in the threshold calibration or a breakdown in the theoretical assumptions (e.g., non-exponential family rewards).
  - **Slow Convergence**: The algorithm stops correctly but E[τδ] is significantly larger than T*(μ)log(1/δ). This suggests the sampling rule is not tracking the optimal proportions effectively.

- First 3 experiments:
  1. **Reproduce Theoretical Bound**: Implement the Bernoulli bandit case with M=2 optimal arms. Vary δ ∈ {0.1, 0.01, 0.001} and plot E[τδ]/log(1/δ). Verify convergence to T*(μ) as δ→0, confirming the asymptotic optimality claim.
  2. **Ablate Sampling Rule**: Compare C-Tracking vs. D-Tracking. Measure empirical sample complexity and CPU time. This validates the paper's claim that both rules lead to optimal performance while informing the practical trade-off.
  3. **Stress Test with Non-Exponential Rewards**: Test the algorithm with reward distributions outside the exponential family (e.g., uniform, heavy-tailed). Monitor the error rate P(âτδ ∉ A*). This probes the "Break Condition" of the core theoretical assumptions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the modified Track-and-Stop strategy and its instance-optimality guarantees be extended to structured bandit models, such as combinatorial or contextual settings, where multiple optimal actions may exist?
- Basis in paper: [explicit] The conclusion explicitly states a direction for future work is to "extend these insights to more structured bandit models, such as combinatorial or contextual bandits."
- Why unresolved: The current analysis focuses on standard stochastic multi-armed bandits; structured settings introduce dependencies between arms or contexts that alter the information-theoretic lower bounds and sampling complexity.
- Evidence would resolve it: A derivation of new lower bounds for these structures and a proof that an adapted algorithm satisfies them.

### Open Question 2
- Question: How can adaptive algorithms be designed to improve empirical performance (e.g., in finite-time regimes) while preserving the instance-optimal guarantees provided for the modified Track-and-Stop algorithm?
- Basis in paper: [explicit] The conclusion identifies the need to "develop adaptive algorithms that offer both optimal guarantees and improved empirical performance."
- Why unresolved: While the paper proves asymptotic optimality (as $\delta \to 0$), practical applications often require better finite-sample efficiency which standard Track-and-Stop may not always provide.
- Evidence would resolve it: Empirical benchmarks showing reduced sample complexity in non-asymptotic settings or finite-time upper bounds that improve upon the current analysis.

### Open Question 3
- Question: Does the instance-optimality of the proposed stopping rule hold for reward distributions outside the one-parameter exponential family, such as heavy-tailed distributions?
- Basis in paper: [inferred] The theoretical analysis (specifically Lemma 2 and Appendix A) relies heavily on the convexity properties of the KL divergence specific to exponential families like Gaussian, Bernoulli, and Poisson.
- Why unresolved: The convexity of the function $f(\lambda)$ in Lemma 2 is only proven for common exponential families; it is unclear if the optimization steps in the lower bound proof hold for general distributions.
- Evidence would resolve it: A generalization of Lemma 2 to non-exponential families or a counter-example showing the breakdown of the derived lower bound.

## Limitations

- Theoretical guarantees rely on strong assumptions including one-parameter exponential family rewards and known number of optimal arms M
- Lower bound derivation is asymptotic (δ→0) and may not be tight for non-asymptotic regimes
- Computational complexity of optimal allocation and stopping statistic grows with number of arms K and hypothesized optimal set size M
- Empirical validation limited to Bernoulli instances, leaving gaps for other reward types

## Confidence

- **High**: Core theoretical framework under stated assumptions (exponential family, δ-PAC requirement)
- **Medium**: Practical implementation details and computational aspects
- **Low**: Generalization beyond specified setting (non-exponential families, unknown M)

## Next Validation Checks

1. Test the algorithm on non-exponential family rewards (e.g., uniform distributions) to assess robustness of the δ-PAC guarantee.
2. Implement the optimal allocation computation w*(μ) with numerical solver, measure runtime complexity for K=50.
3. Conduct ablation studies varying M relative to true number of optimal arms to quantify performance degradation.