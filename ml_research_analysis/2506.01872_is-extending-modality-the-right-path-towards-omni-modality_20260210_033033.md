---
ver: rpa2
title: Is Extending Modality The Right Path Towards Omni-Modality?
arxiv_id: '2506.01872'
source_url: https://arxiv.org/abs/2506.01872
tags:
- modality
- fine-tuning
- language
- merging
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates modality extension as a path to omni-modality
  in large language models. The authors examine three questions: whether modality
  extension compromises language abilities, if model merging can integrate modality-specific
  models, and if omni-modality fine-tuning improves knowledge sharing.'
---

# Is Extending Modality The Right Path Towards Omni-Modality?

## Quick Facts
- **arXiv ID:** 2506.01872
- **Source URL:** https://arxiv.org/abs/2506.01872
- **Authors:** Tinghui Zhu; Kai Zhang; Muhao Chen; Yu Su
- **Reference count:** 40
- **Primary result:** Modality fine-tuning degrades reasoning and instruction-following while enhancing multimodal knowledge; weighted model merging better preserves language capabilities than standard averaging.

## Executive Summary
This paper investigates whether extending LLMs with additional modalities (image, video, audio) is an effective path to achieving true omni-modality. Through extensive experiments, the authors find that while modality fine-tuning successfully extends capabilities, it systematically degrades core language abilities, particularly reasoning and instruction-following. They demonstrate that weighted model merging based on parameter shifts better preserves base LLM capabilities than standard averaging, though still underperforms specialized models. The study concludes that current approaches fall short of achieving true omni-modality, suggesting the need for more refined strategies that balance capability extension with knowledge preservation.

## Method Summary
The authors evaluate modality extension by fine-tuning Qwen2-7B-Instruct with image, video, and audio data, then merging these models using weighted averaging based on parameter shift magnitudes. They compare weighted merging against standard averaging and assess both approaches against specialized models on text and multimodal benchmarks. The parameter shift-based weighting computes the average absolute difference between original and fine-tuned parameters, applies softmax normalization, and manually assigns weights to the base model and each modality expert. The study also examines omni-modality fine-tuning efficiency compared to modality-specific training.

## Key Results
- Modality fine-tuning extends capabilities but compromises reasoning (GPQA, MATH) and instruction-following (IFEval) performance.
- Weighted model merging based on parameter shift preserves base LLM capabilities better than standard averaging.
- Omni-modality fine-tuning requires significantly more data than modality-specific fine-tuning while achieving lower performance.
- Larger models show less degradation due to parameter redundancy that can absorb modality-specific updates.

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Preservation Trade-off in Modality Fine-Tuning
- **Claim:** Modality fine-tuning extends capabilities but systematically degrades core language abilities, particularly reasoning and instruction-following.
- **Mechanism:** Visual modalities inject new parametric knowledge through fine-tuning data, but parameter updates disrupt reasoning pathways. Larger models mitigate degradation through redundant parameters that absorb modality-specific updates without affecting core abilities.
- **Core assumption:** The observed degradation is caused by parameter interference rather than data distribution shift or evaluation artifacts.
- **Evidence anchors:**
  - [abstract]: "modality fine-tuning effectively extends capabilities but compromises core language abilities, particularly reasoning and instruction-following"
  - [Section 4.2]: "modality fine-tuning severely degrades reasoning performance across all tested domains"
  - [Section 4.2]: "larger models possess more redundant parameters... These idle parameters may absorb the effects of modality fine-tuning"
  - [corpus]: Weak direct support; related work on multimodal robustness (OmniEval, WorldSense benchmarks) focuses on evaluation rather than mechanisms
- **Break condition:** If fine-tuning explicitly incorporates instruction-following and reasoning data in appropriate proportions, degradation may be mitigated. Also breaks when model scale provides sufficient parameter redundancy.

### Mechanism 2: Parameter-Shift-Weighted Model Merging
- **Claim:** Weighted averaging based on parameter shift magnitude preserves base LLM capabilities while integrating multimodal functionality.
- **Mechanism:** Compute Δ_avg (average parameter shift from original to fine-tuned model). Apply softmax to normalize shifts into weights α_i. Manually assign α_0 to base LLM, rescale other weights by (1-α_0). This prioritizes models with greater specialization while anchoring to original capabilities.
- **Core assumption:** Parameter shift magnitude correlates with modality-specific importance, and linear interpolation in weight space preserves functional capabilities.
- **Evidence anchors:**
  - [Section 5.3]: "greater specialization in a modality results in more substantial parameter deviations"
  - [Table 4]: Weighted merging shows +1.2% on MMLU-Pro vs. -9.3% absolute drop for full omni-modality fine-tuning
  - [Section 5.5]: "merged model performs on par with or better than the original LLM across evaluated domains. This suggests that future efforts... should focus on addressing reasoning degradation"
  - [corpus]: T3 (Test-Time Model Merging in VLMs) demonstrates model merging for medical imaging, supporting transferability of the approach
- **Break condition:** When modalities require fundamentally incompatible representations (e.g., fine-tuning drives weights in divergent directions—see t-SNE visualization in Figure 4), merging cannot reconcile conflicts.

### Mechanism 3: Modality-Specific vs. Omni-Modality Training Efficiency Gap
- **Claim:** Modality-specific fine-tuning achieves better performance with less data than omni-modality joint training.
- **Mechanism:** Specialized models optimize for statistical properties of a single modality without interference from competing objectives. Omni-modality training introduces gradient conflicts across modalities, requiring more data to achieve sub-optimal equilibrium.
- **Core assumption:** The comparison controls for model architecture and base LLM quality; performance gap reflects training paradigm rather than data quality differences.
- **Evidence anchors:**
  - [Table 6]: LLaVA-Next uses 1.3M image data vs. NextGPT's 4.5M, yet achieves 57.6 vs. 48.4 on VizWiz
  - [Section 6.1.2]: "omni-modal fine-tuning is currently less effective and efficient than modality-specialized models"
  - [Figure 3]: After ~1,000 fine-tuning steps, MMLU/MMLU-Pro decrease while MMMU improves—modality trade-off emerges
  - [corpus]: No direct corpus support for efficiency comparison; related benchmarks (OmniEval, WorldSense) evaluate capability but not training efficiency
- **Break condition:** If modality-specific data is carefully balanced and curriculum-designed, or if architectural modifications (e.g., modality-specific adapters) isolate gradient flows, joint training may approach specialized performance.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Fine-Tuning**
  - **Why needed here:** The paper's central finding—that modality fine-tuning degrades reasoning—requires understanding why neural networks lose previously learned capabilities when trained on new objectives.
  - **Quick check question:** Can you explain why gradient updates on modality-specific data would interfere with weights responsible for logical reasoning?

- **Concept: Linear Mode Connectivity / Weight Interpolation**
  - **Why needed here:** Model merging assumes that linear combinations of weights preserve functionality. This is not guaranteed—loss barriers can exist between fine-tuned checkpoints.
  - **Quick check question:** Why might averaging two fine-tuned models produce worse results than either individual model, even if both share the same initialization?

- **Concept: Attention Head Specialization**
  - **Why needed here:** The paper's head-masking analysis (Figure 2) shows modality fine-tuning affects all heads, not just specific ones. Understanding attention head roles clarifies why selective preservation is difficult.
  - **Quick check question:** If shallow layers handle semantic understanding and deep layers handle reasoning, what does it mean when masking any head degrades multimodal performance?

## Architecture Onboarding

- **Component map:** Base LLM (frozen or fine-tuned) -> Modality Encoders (E_m1, E_m2, ...) -> Modality Projectors (P_m1, P_m2, ...) -> Encoded tokens T = {t_text, t_image, t_video, t_audio} -> LLM forward pass -> Text output

- **Critical path:**
  1. Modality-specific fine-tuning updates LLM weights (not just projectors)
  2. Compute Δ_avg = avg|θ_original - θ_fine-tuned| for each candidate model
  3. Apply softmax normalization → weights α_i
  4. Set base LLM weight α_0 manually (paper uses implicit tuning)
  5. Merge: θ_merged = α_0·θ_0 + (1-α_0)·Σα_i·θ_i
  6. Evaluate on both textual benchmarks (MMLU, IFEval, GPQA, MATH) and multimodal benchmarks (MMMU, Video-MME)

- **Design tradeoffs:**
  - **Freeze LLM vs. fine-tune LLM:** Freezing preserves language abilities but limits modality integration; fine-tuning enables deeper integration but risks degradation
  - **Standard vs. weighted merging:** Weighted preserves more capabilities but requires computing parameter shifts; standard is simpler but sub-optimal
  - **Merging vs. joint training:** Merging is lightweight and preserves base model; joint training is conceptually unified but currently less efficient

- **Failure signatures:**
  - Reasoning scores (GPQA, MATH, HumanEval) drop >5% absolute → excessive fine-tuning or insufficient α_0
  - MMMU/Video-MME below individual model performance → merging weights poorly calibrated
  - Instruction-following (IFEval) collapses → fine-tuning data lacks instruction diversity

- **First 3 experiments:**
  1. **Baseline degradation measurement:** Take Qwen2-7B-Instruct, fine-tune on image-only data (LLaVA format), evaluate on MMLU-Pro, GPQA, MATH, IFEval. Document per-domain degradation rates.
  2. **Parameter shift analysis:** Compute Δ_avg for Qwen2-VL, LLaVA-Video, and LLaVA-OneVision relative to base. Correlate shift magnitude with training data volume and performance changes.
  3. **Merging weight sweep:** Fix base weight α_0 ∈ {0.1, 0.3, 0.5, 0.7}, merge all three modality models, evaluate on full textual + multimodal benchmark suite. Identify Pareto frontier for knowledge vs. reasoning preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can omni-modality fine-tuning strategies be optimized to match the efficiency and performance of modality-specific models?
- Basis in paper: [explicit] Section 6.1.2 states that "Further research is needed to optimize omni-modality fine-tuning strategies, ensuring they can effectively balance generalization and efficiency without excessive data consumption."
- Why unresolved: Current omni-modality models like NextGPT require significantly more training data (e.g., 4.5M samples) than specialized models like LLaVA-Next (1.3M samples) yet still underperform on specific benchmarks.
- What evidence would resolve it: A training regime that enables an omni-modal model to achieve state-of-the-art performance on multimodal benchmarks using a data budget comparable to modality-specific models.

### Open Question 2
- Question: Can the inclusion of instruction-following data during modality fine-tuning preserve reasoning and instruction-following capabilities?
- Basis in paper: [inferred] Section 4.2 observes that modality fine-tuning degrades instruction following and reasoning, suggesting that "incorporating instruction-following data during fine-tuning may be necessary."
- Why unresolved: The paper demonstrates that current fine-tuning paradigms function primarily as modality extensions, often sacrificing core LLM abilities like reasoning (GPQA, MATH) and instruction adherence (IFEval).
- What evidence would resolve it: Experiments showing that a specific ratio or curriculum of instruction-following data mixed with modality-specific data maintains or improves IFEval and reasoning scores post-training.

### Open Question 3
- Question: Is there a small-step fine-tuning regimen that can simultaneously improve multimodal capabilities without degrading textual understanding in merged models?
- Basis in paper: [inferred] Section 6.2.2 identifies a "modality trade-off" where the optimal fine-tuning steps for multimodal tasks differ from those for textual tasks.
- Why unresolved: The authors found that while textual performance (MMLU) peaks around 100 steps, multimodal performance (MMMU) continues to improve up to 1000 steps, causing a conflict where optimizing one degrades the other.
- What evidence would resolve it: A dynamic training schedule or regularization technique that allows performance on MMLU/MMLU-Pro to remain stable or improve while MMMU scores increase over longer training durations.

## Limitations

- The paper does not explicitly isolate and control for confounding factors such as dataset composition, fine-tuning duration, or evaluation metric sensitivity.
- Lack of ablation studies for individual modality contributions and no investigation into architectural modifications (e.g., adapters) that might mitigate degradation.
- The base LLM weight (α₀) in model merging is treated as a hyperparameter without systematic sensitivity analysis.

## Confidence

- **High:** Weighted model merging preserves language capabilities better than standard averaging (supported by quantitative results in Table 4).
- **Medium:** Modality fine-tuning degrades reasoning and instruction-following (mechanism plausible but causation not fully isolated).
- **Low:** Omni-modality fine-tuning is inherently less efficient than modality-specific fine-tuning (efficiency claim relies on implicit data quality equivalence assumption).

## Next Checks

1. **Causal Attribution Study:** Perform ablation experiments where base LLM reasoning benchmarks are evaluated after fine-tuning only the projector layers versus fine-tuning the full LLM. This would isolate whether reasoning degradation stems from parameter interference or representation drift.

2. **α₀ Sensitivity Analysis:** Systematically sweep α₀ values (0.1 to 0.9) for weighted model merging and plot the Pareto frontier of multimodal capability versus reasoning preservation. This would quantify the trade-off space and identify optimal configurations.

3. **Architectural Modification Comparison:** Implement and evaluate modality-specific adapters (instead of full LLM fine-tuning) and compare reasoning degradation, merging compatibility, and training efficiency against the current approach.