---
ver: rpa2
title: 'Elite Polarization in European Parliamentary Speeches: a Novel Measurement
  Approach Using Large Language Models'
arxiv_id: '2507.06658'
source_url: https://arxiv.org/abs/2507.06658
tags:
- polarization
- political
- elite
- party
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for measuring elite polarization
  using large language models (LLMs) to analyze parliamentary speeches. The approach
  detects when politicians mention each other, identifies the speaker and addressee,
  and evaluates the sentiment toward these political entities.
---

# Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models

## Quick Facts
- arXiv ID: 2507.06658
- Source URL: https://arxiv.org/abs/2507.06658
- Reference count: 18
- Method achieves sensitivity >80% and low false discovery rates in detecting elite polarization from parliamentary speeches

## Executive Summary
This paper introduces a novel method for measuring elite polarization using large language models (LLMs) to analyze parliamentary speeches. The approach detects when politicians mention each other, identifies the speaker and addressee, and evaluates the sentiment toward these political entities. By aggregating hundreds of thousands of such mentions, it creates an index of mutual out-party hostility. The method demonstrates high accuracy (sensitivity >80%) and low false discovery rates when validated against human coding. The resulting polarization scores show strong face validity, responding to events like electoral campaigns, crises, and government changes in Hungary, the UK, and Italy.

## Method Summary
The method uses zero-shot prompting with a 6-step chain-of-thought approach via OpenAI API to extract political actor mentions and evaluate sentiment toward each actor on a -5 to +5 scale. An R-based preprocessing pipeline removes formal address sentences from speeches. The LLM outputs structured CSV data with entity mention, context, rationale, and sentiment score. Entity normalization classifies heterogeneous actor names into categories and resolves aliases using date context. The Elite Polarization Score (EPS) aggregates dyadic sentiment data over time using a weighted formula. A "Supergold" validation protocol combines human coding with a second LLM review to create a sensitive benchmark for measuring LLM accuracy.

## Key Results
- LLM-based approach achieves sensitivity >80% and false discovery rate of 4.2% when validated against human coding
- EPS scores respond to major political events including electoral campaigns, government changes, and crises in Hungary, UK, and Italy
- The method successfully measures elite polarization across different languages without translation, enabling cross-country time-series analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shifting the unit of analysis from full speeches to individual political-entity references enables measurement of directed inter-party affect, not just general debate tone.
- **Mechanism:** A chain-of-thought prompt guides an LLM to first identify all political actors in a speech, then separately evaluate the speaker's sentiment toward each actor on a -5 to +5 scale, and finally explain its reasoning.
- **Core assumption:** LLMs can reliably distinguish sentiment about a political actor from sentiment about that actor's situation or the broader issue being discussed.
- **Evidence anchors:** [abstract] "I identify when politicians mention one another... and assess the emotional temperature behind these evaluations." [section 5.1] The prompt structure explicitly separates actor identification from sentiment analysis.

### Mechanism 2
- **Claim:** Zero-shot LLM-based sentiment analysis provides cross-linguistic portability without task-specific training data for each language.
- **Mechanism:** Pre-trained LLMs leverage their multilingual pre-training and self-attention mechanisms to process speeches in their original language (e.g., Hungarian), avoiding error propagation from machine translation.
- **Core assumption:** The LLM's internal emotion mapping (e.g., -5 to +5 scale) is consistent and comparable across different political cultures and languages.
- **Evidence anchors:** [abstract] "lays the groundwork for a twenty-year, EU-wide time-series dataset on elite polarization." [section 1] "LLMs can operate seamlessly across different languages without additional training."

### Mechanism 3
- **Claim:** A "Supergold" validation protocol that combines human coding with a second LLM's review creates a more sensitive benchmark than human coding alone.
- **Mechanism:** Human coders and the primary LLM both code a sample. Unmatched AI results are checked by a second LLM to catch valid AI detections missed by humans, forming a Supergold standard.
- **Core assumption:** A second LLM can accurately adjudicate potential false positives from the primary LLM, and that this combined standard approximates "ideal sensitivity."
- **Evidence anchors:** [abstract] "demonstrates high accuracy (sensitivity >80%) and low false discovery rates when validated against human coding." [section 5.2] Details the Supergold creation process.

## Foundational Learning

- **Concept: Affective vs. Ideological Polarization**
  - **Why needed here:** The paper explicitly distinguishes its measure (elite out-party hostility) from ideological polarization (policy distances).
  - **Quick check question:** In the paper's framework, is a party that strongly dislikes an ideologically similar out-party more or less polarized than one that moderately dislikes an ideologically distant out-party?

- **Concept: Zero-Shot and Chain-of-Thought Prompting**
  - **Why needed here:** The method relies on prompting an LLM without examples (zero-shot) to perform a complex, multi-step task.
  - **Quick check question:** What are the two key ways the prompt avoids using examples, and what is the stated reason for this design choice?

- **Concept: Precision, Recall, Sensitivity, and False Discovery Rate in NLP Evaluation**
  - **Why needed here:** The paper reports sensitivity (>80%) and false discovery rates to validate its method.
  - **Quick check question:** According to the paper, why was a "Supergold" standard necessary for measuring sensitivity, instead of using human coding directly?

## Architecture Onboarding

- **Component map:** Data Preprocessor -> Entity & Sentiment Extractor (Core) -> Entity Resolver/Classifier -> Aggregation Engine -> EPS Time Series
- **Critical path:** Data Preprocessor -> Entity & Sentiment Extractor -> Entity Resolver -> Aggregation Engine -> EPS Time Series. The prompt design and LLM selection in the Extractor are the most critical and fragile steps.
- **Design tradeoffs:**
  - **Accuracy vs. Cost:** ChatGPT 4o-mini used for large UK corpus for cost reasons, while 4o used for smaller Hungary/Italy corpora
  - **Recall vs. Precision:** Prompt engineered for high sensitivity (recall) to capture all mentions, with Step 1.2 designed to reduce false discoveries (precision)
  - **Replicability vs. Complexity:** Requesting LLM rationale enhances replicability but makes prompt incompatible with smaller/older models
- **Failure signatures:**
  - Low sensitivity if LLM misses political references, manifesting as low mention counts per speech
  - Sentiment confusion if LLM systematically misattributes sentiment about policies to parties
  - Entity resolution errors if resolver incorrectly links politicians to wrong parties during party switches
- **First 3 experiments:**
  1. **Prompt Ablation on a Pilot Corpus:** Remove one part of chain-of-thought prompt and re-run on 100 speeches, compare sensitivity against human-coded sample
  2. **Cross-LLM Consistency Check:** Run same 300-speech sample through two different LLMs, compare sentiment distributions and EPS scores
  3. **Targeted Validation of Entity Resolution:** Manually verify 500 resolved name-to-party mappings, focusing on periods of party splits or MP defections

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific structural or agentic conditions (e.g., electoral cycles, government status, mass support) facilitate the growth of elite polarization, and is this growth a stable characteristic or a reactive accumulation?
- **Basis in paper:** The author states that because mechanisms are understudied, "there remains 'little to no research identifying the mechanisms underlying polarization' (Iyengar et al., 2019, p. 133)," and asks "What conditions facilitate the growth of elite polarization?"
- **Why unresolved:** The paper establishes a method to measure the level of polarization but notes that the field lacks the longitudinal data to determine the causes (structural vs. reactive) and the specific impact of variables like government status or crises.
- **What evidence would resolve it:** Time-series cross-section analysis using the new EPS index to correlate spikes with specific events (elections, scandals) and structural conditions.

### Open Question 2
- **Question:** At what specific intensity level does elite polarization transition from healthy democratic competition to a predictor of democratic backsliding or institutional erosion?
- **Basis in paper:** The author notes that while we know excessive polarization is harmful, "we only have an educated guess as to what extent elite polarization should be developed to have these processes start."
- **Why unresolved:** The paper provides an index with a theoretical range of -5 to 5 but lacks the comparative historical data to identify a specific "tipping point" where polarization creates existential threats to the regime.
- **What evidence would resolve it:** Longitudinal studies correlating EPS scores with indices of democratic quality or instances of constitutional hardball across multiple countries.

### Open Question 3
- **Question:** Does the exclusion of in-party sentiment from the Elite Polarization Score introduce bias in cross-country comparisons, particularly between disciplined two-party systems and fragmented multi-party systems?
- **Basis in paper:** The paper explicitly operationalizes polarization using only out-party hostility, arguing that in-party evaluation is heuristically less important and empirically censored.
- **Why unresolved:** The "aggregated hostility" index may treat a cohesive party with high out-group hate the same as a fractured party with high out-group hate but low internal cohesion, masking systemic differences.
- **What evidence would resolve it:** A comparative study combining this EPS index with internal party cohesion metrics or surveys of elite intra-party sentiment.

## Limitations
- Cross-country comparability depends on untested assumptions about LLM sentiment scale consistency across political cultures
- The method cannot detect implicit polarization (coded language, dog whistles) or distinguish strategic rhetorical positioning from genuine sentiment
- Entity resolution errors during periods of party switches, mergers, or MP defections can misattribute dyadic sentiment

## Confidence
- **High Confidence**: Technical implementation of entity extraction and sentiment scoring works as described, supported by detailed prompt structure and validation protocol
- **Medium Confidence**: Face validity demonstrations support measure's relevance but don't prove it captures the same construct as other polarization measures
- **Low Confidence**: Cross-country and cross-linguistic comparability of sentiment scores remains theoretically plausible but empirically unverified beyond single-country validation samples

## Next Checks
1. **Cross-Linguistic Sentiment Calibration**: Have human coders blind to country context rate sentiment on the same -5 to +5 scale for 100 speeches from each country, compare LLM vs human score distributions
2. **Temporal Stability Analysis**: Calculate EPS variance during periods of stable political configurations to identify measurement noise vs genuine sentiment shifts
3. **Strategic Rhetoric Detection**: Manually examine speeches with extreme EPS scores (top and bottom 5%) to determine whether they represent genuine hostility or strategic parliamentary positioning