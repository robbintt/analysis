---
ver: rpa2
title: Mathematical reasoning and the computer
arxiv_id: '2502.07850'
source_url: https://arxiv.org/abs/2502.07850
tags:
- which
- have
- proof
- mathematics
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This article provides an overview of how computers are changing
  mathematics beyond just computation. It discusses three main areas: neural networks
  helping mathematicians discover new theorems and counterexamples (such as predicting
  knot signatures and Kazhdan-Lusztig polynomials), automated and interactive theorem
  provers verifying complex mathematical results (including the four-color theorem
  and modern research-level proofs), and large language models like ChatGPT engaging
  with mathematical problems (though currently limited by tendency to make false assertions).'
---

# Mathematical reasoning and the computer

## Quick Facts
- arXiv ID: 2502.07850
- Source URL: https://arxiv.org/abs/2502.07850
- Authors: Kevin Buzzard
- Reference count: 5
- Key outcome: Computers are transforming mathematics through neural networks discovering conjectures, theorem provers verifying proofs, and language models engaging with problems, though true research-level reasoning remains challenging.

## Executive Summary
This article explores how computers are changing mathematics beyond traditional computation, focusing on three transformative areas: neural networks discovering mathematical relationships, automated theorem provers verifying complex proofs, and large language models engaging with mathematical problems. The author provides concrete examples including neural networks predicting knot signatures and Kazhdan-Lusztig polynomials, formal verification of the four-color theorem and modern research results, and LLM attempts at mathematical reasoning. While these tools show significant promise, the paper emphasizes that true mathematical reasoning at research level remains a challenge, with current systems excelling in areas where computation is feasible but struggling with abstract, continuous, or highly structural domains.

## Method Summary
The paper synthesizes existing research on computational approaches to mathematical reasoning, drawing from specific case studies and implementations. Neural network approaches involve training on tables of mathematical objects with their invariants, then analyzing feature importance to guide conjecture formation. Interactive theorem provers require translating mathematical definitions and proofs into formal languages for computer verification, with automation tools bridging gaps between different formal systems. Large language models generate mathematical content through token prediction, with integration attempts using ITPs for validation. The methods span supervised and reinforcement learning paradigms, formal proof systems, and natural language processing, all aimed at augmenting human mathematical reasoning capabilities.

## Key Results
- Neural networks successfully predicted knot signatures from continuous invariants and identified key structural features (extremal reflections) in Bruhat graphs
- Interactive theorem provers verified the four-color theorem and modern research-level results like the Liquid Tensor Experiment with significant human effort
- Large language models can generate plausible mathematical proofs but require external verification to prevent false assertions from cascading into invalid arguments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks can guide mathematical discovery by identifying correlations between mathematical invariants that humans can then formalize into theorems.
- Mechanism: A neural network is trained on tables of mathematical objects (knots, Bruhat graphs) with their associated invariants. The network learns to predict one invariant from others; analyzing which input features drive predictions reveals which invariants are mathematically connected.
- Core assumption: The mathematical relationship is learnable from the data representation chosen, and continuity-like properties exist where small input changes don't drastically alter outputs.
- Evidence anchors:
  - [section 2.2]: "The team found that an appropriate neural network was able to accurately predict the signature of a knot (a discrete, cohomological invariant) given the data of the continuous invariants in the tables."
  - [section 2.3]: "Humans managed to distill from the network the idea that so-called extremal reflections in the graph were playing an important role in the prediction."
  - [corpus]: Related papers discuss pattern recognition in higher mathematics but lack direct validation of this specific conjecture-generation mechanism.
- Break condition: The target mathematical structure lacks computational representations or exhibits high sensitivity to small input perturbations (e.g., elliptic curve ranks from Weierstrass coefficients).

### Mechanism 2
- Claim: Interactive theorem provers (ITPs) can verify research-level mathematics by requiring humans to formalize proofs in a computer-checkable language while tactics automate routine steps.
- Mechanism: Humans translate mathematical definitions and proofs into ITP code (e.g., Lean, Coq). The ITP kernel verifies each step against axioms; tactics (small programs) fill in tedious algebraic manipulations. External tools (ATPs, computer algebra systems, LLMs) can be called as "hammers" to propose intermediate steps.
- Core assumption: The mathematical theory can be formalized in the ITP's type theory or logic, and the time investment for formalization is tractable.
- Evidence anchors:
  - [section 3.2]: "In 2022 Bloom–Mehta formalisation of Bloom's 2021 resolution of a conjecture of Erdős and Graham... Thomas Bloom knew nothing about Lean, but he was introduced to it by Bhavik Mehta and between them they formalised the full proof before the paper had even got a referee's report."
  - [section 3.2]: "The Liquid Tensor Experiment... one notable consequence was that the dependency on the theory of stable homotopy groups of spheres was removed during the formalization process."
  - [corpus]: MathlibLemma paper confirms folklore lemma gaps remain a barrier, suggesting formalization works but library coverage is incomplete.
- Break condition: The mathematics relies heavily on geometric intuition that resists formalization, or the formalization effort exceeds available person-hours.

### Mechanism 3
- Claim: Large language models can generate mathematical proofs but require external verification to prevent false assertions from cascading into invalid arguments.
- Mechanism: LLMs predict next tokens in sequences trained on internet-scale text. When prompted for proofs, they generate plausible-looking reasoning but lack grounding in logical truth. Pairing LLMs with ITPs provides immediate feedback: generated code either compiles (proof valid) or fails.
- Core assumption: The ITP provides a sufficiently narrow and unambiguous target language that LLMs can learn to emit syntactically correct and semantically meaningful proof steps.
- Evidence anchors:
  - [section 4]: "A big problem with these systems when it comes to writing mathematics in a 'natural language' such as English is that they will happily assert false statements."
  - [section 4]: "Both Meta and OpenAI have produced work in this direction recently... Both systems have managed to automatically generate Lean code corresponding to proofs of theorems at olympiad level."
  - [corpus]: Hallucination detection papers confirm this remains an active problem; RealMath benchmark indicates current LLMs struggle with research-level mathematics.
- Break condition: The theorem requires proof steps that are rare in training data, or the ITP's formalization of the problem is itself ambiguous.

## Foundational Learning

- Concept: **First-order logic and formal proof systems**
  - Why needed here: ATPs operate on first-order theories; understanding the distinction between axioms, inference rules, and theorems clarifies what these systems can and cannot prove.
  - Quick check question: Can you explain why the statement (ab)⁻¹ = b⁻¹a⁻¹ follows from group axioms rather than being an axiom itself?

- Concept: **Type theory basics (dependent types, propositions-as-types)**
  - Why needed here: Modern ITPs like Lean and Coq are based on dependent type theory; understanding how types encode specifications is essential for reading or writing formal proofs.
  - Quick check question: What does it mean for a proof to "inhabit" a type in the Curry-Howard correspondence?

- Concept: **Neural network training paradigms (supervised learning vs. reinforcement learning)**
  - Why needed here: The paper describes both approaches—supervised learning for pattern discovery (knot invariants) and reinforcement learning for counterexample search (graph conjectures).
  - Quick check question: Why might reinforcement learning be better suited for finding counterexamples than for predicting invariants?

## Architecture Onboarding

- Component map:
  Data layer -> Learning layer -> Formalization layer -> Automation layer -> Interface layer
  Mathematical databases -> Neural networks/RL agents -> ITPs with libraries -> Tactics/hammers -> Natural language/ITP code

- Critical path:
  1. Identify a mathematical domain with computable invariants and existing data tables
  2. Train neural network to predict target invariant; analyze feature importance
  3. Formulate conjecture from network insights; attempt human proof
  4. Formalize proof in ITP for verification; use tactics/hammers to accelerate

- Design tradeoffs:
  - Natural language vs. ITP output: Natural language is accessible but unverifiable; ITP output is verifiable but requires formalization expertise
  - ATP vs. ITP: ATPs prove simple-theory theorems automatically but struggle with complex structures; ITPs handle research-level math but require human guidance
  - Data representation choice: Raw mathematical objects (e.g., Weierstrass coefficients) may lack continuity; transformed representations (e.g., modular form coefficients) may preserve structure better

- Failure signatures:
  - Neural network achieves low accuracy → chosen representation may lack learnable structure; try alternative encodings
  - ITP formalization takes 10× paper-writing time → missing library lemmas ("folklore"); contribute to shared libraries first
  - LLM generates syntactically correct but semantically wrong proof → increase ITP feedback during training; use curriculum learning from simpler theorems

- First 3 experiments:
  1. Replicate the knot signature prediction experiment using public knot tables and a simple feedforward network; measure accuracy vs. number of invariants provided
  2. Install Lean 4 and mathlib; formalize a short proof (e.g., infinitude of primes) to experience the formalization bottleneck firsthand
  3. Query an LLM (GPT-4 or similar) for proofs of theorems at increasing difficulty levels; manually verify each step and catalog failure modes (arithmetic errors, unjustified leaps, definition mismatches)

## Open Questions the Paper Calls Out

- Will high-level automated mathematical reasoning rely primarily on natural language models or formal ITP-backed systems? The author notes it's unclear whether success will come from natural language systems, ITP-backed systems, or hybrid approaches, given current limitations of both approaches.

- Does the time and effort required to formalize mathematical proofs differ significantly between subfields, such as geometry versus arithmetic? The author explicitly asks whether formalization time varies between subfields and identifies this as an important area of inquiry.

- Can neural network-based discovery tools be applied to abstract mathematical domains that lack large computational datasets? The text notes current applicability is limited to areas where computation is possible and tables exist, suggesting these tools might not be useful for abstract domains like perfectoid spaces.

## Limitations

- Current computational approaches work well for problems with finite or computable representations but face fundamental limitations with problems involving continuous structures or undecidable questions
- Neural network approaches can suggest conjectures but lack formal proof mechanisms and depend heavily on data representation choices
- ITP formalization requires substantial human expertise and time investment, with significant barriers including missing library lemmas and the need for collaboration between domain experts and formal methods specialists

## Confidence

- **Medium confidence**: Neural networks can guide mathematical discovery by identifying correlations between invariants
- **Medium confidence**: ITPs can verify research-level mathematics with human guidance  
- **Low confidence**: LLMs can meaningfully engage with mathematical problems through natural language

## Next Checks

1. Replicate the knot signature prediction experiment using open-source knot tabulation software (e.g., KnotInfo) to generate training data, then train a simple neural network to predict signatures from continuous invariants. Measure whether similar input structures produce similar outputs to verify the continuity assumption required for learning.

2. Attempt to formalize a short but non-trivial mathematical proof (e.g., quadratic reciprocity or the infinitude of primes in arithmetic progressions) in Lean or Coq. Document the time investment and identify which steps require human insight versus what could potentially be automated by tactics or hammers.

3. Test current LLMs (GPT-4, Claude, or similar) on a graded set of mathematical problems ranging from olympiad to research level. For each problem, record not just whether the final answer is correct, but systematically catalog the types of errors made (arithmetic mistakes, unjustified logical leaps, definition confusion) to quantify the gap between plausible-sounding and mathematically valid reasoning.