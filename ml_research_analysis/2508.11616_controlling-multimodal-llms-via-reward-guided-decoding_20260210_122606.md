---
ver: rpa2
title: Controlling Multimodal LLMs via Reward-guided Decoding
arxiv_id: '2508.11616'
source_url: https://arxiv.org/abs/2508.11616
tags:
- object
- reward
- mrgd
- decoding
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces multimodal reward-guided decoding (MRGD),\
  \ a method to control multimodal large language models (MLLMs) at inference time.\
  \ MRGD builds two reward models\u2014one for object hallucination reduction and\
  \ another for object recall\u2014and uses them to guide the MLLM's decoding process."
---

# Controlling Multimodal LLMs via Reward-guided Decoding

## Quick Facts
- arXiv ID: 2508.11616
- Source URL: https://arxiv.org/abs/2508.11616
- Reference count: 40
- Primary result: MRGD achieves up to 70% reduction in object hallucinations with only 6.5% drop in recall compared to greedy decoding

## Executive Summary
This paper introduces multimodal reward-guided decoding (MRGD), a method to control multimodal large language models (MLLMs) at inference time. MRGD builds two reward models—one for object hallucination reduction and another for object recall—and uses them to guide the MLLM's decoding process. By adjusting the weight of each reward model, users can dynamically trade off object precision and recall, while varying the search breadth allows control over compute usage versus visual grounding quality. Experiments on standard object hallucination benchmarks show MRGD significantly outperforms existing hallucination mitigation methods, achieving up to 70% reduction in object hallucinations with only a 6.5% drop in recall compared to greedy decoding, while enabling fine-grained controllability not possible with fine-tuning or prompting approaches.

## Method Summary
MRGD operates by sampling k candidate completions at each decoding iteration, scoring each candidate with a combined reward function (w·r_hal + (1-w)·r_rec), and selecting the highest-scoring candidate to append to the context. The r_hal reward model is trained on preference data using a Bradley-Terry loss to output scalar rewards indicating hallucination likelihood. The r_rec reward model estimates object recall using an open-vocabulary object detector (OWLv2) and semantic matching via Sentence-BERT embeddings. Rewards are evaluated at sentence boundaries (T sentences) to reduce noise from incomplete text. The method enables dynamic trade-offs between precision and recall by adjusting the weight w at inference time.

## Key Results
- MRGD achieves up to 70% reduction in object hallucinations on standard benchmarks
- Only 6.5% drop in recall compared to greedy decoding while maintaining high precision
- Enables controllable trade-offs between precision and recall through weight adjustment
- Outperforms existing hallucination mitigation methods across multiple datasets
- Sample-efficient: MRGD with k=5 outperforms rejection sampling with k=30

## Why This Works (Mechanism)

### Mechanism 1: Iterative Reward-Guided Best-of-k Search
Sampling multiple candidate completions and selecting the highest-scoring one via learned reward functions reduces hallucinations more effectively than standard decoding. At each iteration, the MLLM samples k candidate completions, evaluates each with a combined reward score, selects the maximum, and adds it to the context. This transforms generation into a search problem over partial outputs. The core assumption is that the reward model generalizes from full-response training to evaluate partial responses meaningfully.

### Mechanism 2: Linear Combination of Specialized Rewards for Precision-Recall Trade-off
Separately trained reward models for object precision (hallucination reduction) and recall enable controllable trade-offs at inference time. Two reward models are combined: r_hal (trained on preference data to prefer non-hallucinated responses) and r_rec (computed via object detector + semantic matching). The combined score s = w·r_hal + (1-w)·r_rec allows users to interpolate between precision and recall by adjusting w ∈ [0,1] on-the-fly. This addresses the assumption that hallucination reduction and recall are partially conflicting objectives.

### Mechanism 3: Sentence-Level Reward Evaluation
Evaluating rewards at sentence boundaries (rather than token-level) improves reward signal quality and reduces noise from incomplete text. The reward model evaluates outputs every T sentences (using "." as delimiter). This avoids penalizing syntactically incomplete prefixes and aligns evaluation with semantically coherent units. The core assumption is that hallucinations typically manifest at sentence granularity, making mid-sentence evaluation unnecessarily noisy.

## Foundational Learning

- **Bradley-Terry Preference Modeling**
  - Why needed: r_hal is trained using preference pairs (y⁺, y⁻) with a Bradley-Terry loss to output scalar rewards
  - Quick check: Given preference data, can you explain why ranking loss rather than regression is used?

- **Autoregressive Decoding with Search**
  - Why needed: MRGD modifies standard autoregressive decoding by introducing search over candidates at each step
  - Quick check: How does best-of-k differ from beam search in terms of diversity and compute?

- **Open-Vocabulary Object Detection + Semantic Matching**
  - Why needed: r_rec uses OWLv2 (open-vocab detector) and Sentence-BERT embeddings to estimate object recall without ground-truth captions
  - Quick check: Why use semantic similarity (threshold τ=0.5) rather than exact string matching for object recall?

## Architecture Onboarding

- Component map: Image + prompt -> MLLM (samples k candidates) -> r_hal (scores each) -> r_rec (scores each) -> combined score -> select best -> repeat until EOS
- Critical path: Base MLLM generates candidates, two reward models score each candidate, combined score selects best completion, process repeats until EOS
- Design tradeoffs:
  - k (candidate count): Higher k improves grounding but increases latency ~linearly
  - T (evaluation period): Lower T (more frequent) improves sample efficiency but slightly increases compute
  - w (reward weight): Controls precision-recall; optimal w varies by benchmark
  - Preference data mix: Adding RLAIF-V without POVID degrades performance
- Failure signatures:
  - High hallucination despite w=1.0: Check r_hal validation accuracy
  - Low recall despite w=0.0: Verify object detector coverage on target domain
  - Excessive latency: Reduce k or increase T; batch reward evaluations
- First 3 experiments:
  1. Baseline sanity check: Run greedy vs MRGD (w=1.0, k=10, T=1) on 100 images; expect ~40-50% hallucination reduction
  2. Ablate reward components: Run MRGD with r_hal only, r_rec only, and combined (w=0.5); plot precision-recall curve
  3. Compute-quality sweep: Vary k∈{3,5,10,30} and T∈{1,2,3}; measure hallucination rate vs latency

## Open Questions the Paper Calls Out

- Can MRGD be effectively extended to mitigate non-object hallucinations, such as errors in attributes, counts, spatial relationships, and negation?
- How does MRGD perform on discriminative hallucination tasks (e.g., POPE) compared to generative captioning tasks?
- Can gradient-based optimization methods outperform or complement the proposed search-based candidate sampling in terms of compute efficiency and output quality?

## Limitations

- Reward model generalization to partial outputs during iterative decoding is unproven beyond tested benchmarks
- Sentence-level evaluation may miss hallucinations that occur mid-sentence or in attributive phrases
- 10x-20x latency increase represents a significant practical constraint for resource-constrained applications

## Confidence

- **High**: The basic MRGD mechanism works as described for standard object hallucination benchmarks
- **Medium**: Sentence-level evaluation improvement and specific optimal hyperparameters are well-supported
- **Low**: Reward model fidelity when evaluating partial outputs during iterative decoding is plausible but not rigorously validated

## Next Checks

1. Generate partial outputs of varying lengths (1-5 sentences) from the test set and evaluate whether r_hal and r_rec maintain consistent quality scores compared to their full-response training
2. Apply MRGD trained on COCO-style datasets to a distinctly different visual domain (medical imaging, satellite imagery, or fine-grained object recognition)
3. Create a diagnostic dataset with hallucinations specifically embedded in attributive phrases or compound sentences to test whether T=1 sentence-level evaluation catches these versus token-level evaluation