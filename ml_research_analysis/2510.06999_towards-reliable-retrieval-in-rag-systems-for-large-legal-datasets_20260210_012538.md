---
ver: rpa2
title: Towards Reliable Retrieval in RAG Systems for Large Legal Datasets
arxiv_id: '2510.06999'
source_url: https://arxiv.org/abs/2510.06999
tags:
- legal
- retrieval
- document
- information
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses a critical failure mode in Retrieval-Augmented
  Generation (RAG) systems called Document-Level Retrieval Mismatch (DRM), where retrieval
  systems select information from incorrect source documents due to high lexical and
  structural similarity in legal documents. To mitigate DRM, the authors propose Summary-Augmented
  Chunking (SAC), a simple and computationally efficient technique that prepends document-level
  synthetic summaries to each text chunk, injecting crucial global context lost during
  standard chunking.
---

# Towards Reliable Retrieval in RAG Systems for Large Legal Datasets

## Quick Facts
- arXiv ID: 2510.06999
- Source URL: https://arxiv.org/abs/2510.06999
- Reference count: 20
- Primary result: SAC reduces Document-Level Retrieval Mismatch by up to 50% in legal RAG systems

## Executive Summary
This study addresses a critical failure mode in Retrieval-Augmented Generation (RAG) systems called Document-Level Retrieval Mismatch (DRM), where retrieval systems select information from incorrect source documents due to high lexical and structural similarity in legal documents. To mitigate DRM, the authors propose Summary-Augmented Chunking (SAC), a simple and computationally efficient technique that prepends document-level synthetic summaries to each text chunk, injecting crucial global context lost during standard chunking. Experiments on the LegalBench-RAG benchmark demonstrate that SAC significantly reduces DRM (by up to 50%) and improves text-level retrieval precision and recall. Surprisingly, generic summarization strategies outperform expert-guided legal domain knowledge approaches. SAC provides a practical, scalable solution for enhancing retrieval reliability in large-scale legal document databases, bringing us closer to reliable AI systems for legal applications.

## Method Summary
The paper introduces Summary-Augmented Chunking (SAC), which enhances each text chunk with a document-level synthetic summary to reduce Document-Level Retrieval Mismatch. The method uses gpt-4o-mini to generate concise summaries (~150 characters) of entire legal documents, then prepends these summaries to text chunks created through recursive splitting (~500 characters). These augmented chunks are embedded using thenlper/gte-large and indexed for retrieval. The approach is evaluated against standard chunking baselines on the LegalBench-RAG benchmark, measuring DRM reduction, precision, and recall improvements.

## Key Results
- SAC reduces DRM by up to 50% compared to standard chunking baselines
- Text-level retrieval precision and recall improve significantly with SAC
- Generic summarization strategies outperform expert-guided legal domain knowledge approaches
- The method is computationally efficient and scales well for large legal datasets

## Why This Works (Mechanism)

### Mechanism 1: Document-Level Disambiguation Through Global Context Injection
Prepended document summaries provide unique "fingerprints" that distinguish similar legal documents in embedding space. Standard chunking isolates text segments from their parent document context, making chunks from different NDAs semantically indistinguishable due to shared boilerplate language. By adding entity-rich summaries, each chunk inherits a document-specific semantic signature that helps the retriever match the correct source document to queries.

### Mechanism 2: Alignment of Broad User Queries with Formal Legal Language via Generic Summaries
Generic summaries capture broad semantic cues that align better with natural language user queries than dense legal terminology. While expert-guided summaries use precise legal identifiers (Governing Law, specific clauses), generic summaries provide conceptual descriptions that bridge the semantic gap between user questions and formal legal text, making them more robust across varied query phrasings.

### Mechanism 3: Dense Semantic Search over Hybrid Retrieval for Context-Enriched Chunks
Pure dense semantic search better handles the combined structure of summary-plus-chunk than hybrid retrieval with BM25. While BM25 slightly improves document selection, it reduces text-level precision and recall by creating conflicts between keyword matching for summary entities and semantic similarity for natural language chunks. Dense retrieval preserves both document identity and passage relevance signals.

## Foundational Learning

- **Chunking Strategies (Fixed-Size vs. Semantic vs. Recursive)**: Understanding trade-offs is critical as standard chunking fragments legal clauses and causes context loss. The choice impacts the "isolated chunk" problem SAC solves. *Quick check: If a legal document has a key clause split across two chunks from a fixed-size splitter, how would a retriever using only those chunks likely fail?*

- **Embedding Models and Semantic Similarity**: The core retriever uses thenlper/gte-large to project text into vector space. Different models have varying capacities and biases that explain why generic summaries might outperform expert ones for a given model. *Quick check: Why might an embedding model create similar vectors for two chunks from different NDAs that share boilerplate text?*

- **Retrieval Metrics: Precision, Recall, and Document-Level Mismatch (DRM)**: Standard RAG evaluation blends retrieval and generation, but this paper isolates retrieval with specific metrics: precision (is retrieved text relevant?), recall (did we find all relevant text?), and DRM (did we get the right document?). *Quick check: A system retrieves a relevant clause from Contract A in response to a query about Contract B. How would precision, recall, and DRM each evaluate this result?*

## Architecture Onboarding

- **Component map**: Summarizer (gpt-4o-mini) -> Recursive Chunker -> Augmenter -> Embedding Model (thenlper/gte-large) -> Vector Database (FAISS) -> Dense Retriever

- **Critical path**: The Summarizer → Augmenter → Embedding Model chain is most critical. Poor summaries or inability of the embedding model to blend summary and chunk signals causes the entire mechanism to fail.

- **Design tradeoffs**:
  1. **Summary Length (150 vs. 300 chars)**: Longer summaries carry more detail but may dominate embeddings, hurting snippet-level precision
  2. **Generic vs. Expert Summaries**: Generic summaries are more robust to varied queries; expert summaries are more precise for technical queries but may be less robust with current embedding models
  3. **Dense vs. Hybrid Retrieval**: Pure dense retrieval optimizes for passage relevance; hybrid retrieval improves document selection but reduces passage precision

- **Failure signatures**:
  - High DRM (>20-30%): Summaries lack sufficient document-differentiating signal
  - Low precision despite low DRM: Summary overwhelms chunk content in embedding or chunk size is too small
  - Expert summaries underperforming: Embedding model struggles with dense legal language or queries are too broad

- **First 3 experiments**:
  1. **Baseline DRM Measurement**: Measure DRM, precision, and recall with standard recursive chunking on a small labeled subset of your legal corpus
  2. **Generic SAC Validation**: Implement full SAC pipeline with generic prompt and recommended hyperparameters, compare against baseline
  3. **Domain-Specific Prompt Iteration**: Adapt generic prompt to include key identifiers from your document types, compare performance against baseline and generic SAC

## Open Questions the Paper Calls Out
None

## Limitations
- Key finding that generic summaries outperform expert ones may not generalize beyond the tested embedding model and benchmark
- Study doesn't address failure modes where summaries become generic enough to fail document disambiguation
- Computational overhead of generating summaries for every chunk could become significant at enterprise scale

## Confidence

- **High Confidence**: SAC's core mechanism of reducing DRM through context injection is well-supported by experimental results
- **Medium Confidence**: Claim that generic summaries outperform expert ones is supported but may be model- and dataset-dependent
- **Medium Confidence**: Assertion that SAC provides a "practical, scalable solution" is reasonable but lacks real-world deployment validation

## Next Checks

1. **Cross-Model Validation**: Test SAC with multiple embedding models (OpenAI embeddings, Cohere) to determine if generic summarization advantage holds across architectures

2. **Query Profile Analysis**: Conduct experiments with varying query types (broad vs. technical) to identify conditions where expert-guided summaries might outperform generic ones

3. **Cost-Benefit Analysis at Scale**: Measure total computational overhead and latency impact when deploying SAC in production environments with millions of documents