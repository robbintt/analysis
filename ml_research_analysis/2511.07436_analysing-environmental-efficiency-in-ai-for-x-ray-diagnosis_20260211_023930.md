---
ver: rpa2
title: Analysing Environmental Efficiency in AI for X-Ray Diagnosis
arxiv_id: '2511.07436'
source_url: https://arxiv.org/abs/2511.07436
tags:
- llms
- carbon
- footprint
- x-ray
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks 14 AI model configurations for Covid-19 detection
  in chest X-rays, comparing accuracy and environmental impact. Local discriminative
  models (Covid-Net, DenseNet, ResNet, VGG) were deployed alongside OpenAI and Claude
  LLMs in a Mendix application.
---

# Analysing Environmental Efficiency in AI for X-Ray Diagnosis

## Quick Facts
- arXiv ID: 2511.07436
- Source URL: https://arxiv.org/abs/2511.07436
- Reference count: 40
- Primary result: Small discriminative models outperform LLMs in both accuracy and environmental efficiency for medical image classification tasks

## Executive Summary
This study benchmarks 14 AI model configurations for Covid-19 detection in chest X-rays, comparing accuracy and environmental impact. Local discriminative models (Covid-Net, DenseNet, ResNet, VGG) were deployed alongside OpenAI and Claude LLMs in a Mendix application. Results showed Covid-Net achieved the highest accuracy (95.5%) with the lowest carbon footprint (99.9% less than GPT-4.5-Preview). Smaller models like VGG reduced carbon footprint by 74% but showed lower confidence. LLMs struggled with classification accuracy (52.3% without knowledge bases) and consumed significantly more energy despite faster response times. Knowledge bases improved LLM accuracy by up to 27% but had mixed environmental impacts.

## Method Summary
The study deployed four local discriminative models (Covid-Net, DenseNet, ResNet, VGG) via ONNX runtime in a Mendix application for binary Covid-19 classification on chest X-rays. Five LLM configurations (GPT-4.5-Preview, o4-Mini, GPT-4.1-Nano, Claude-3.5 Sonnet) were accessed via API with prompts requesting probabilistic outputs. Knowledge bases were built using cosine similarity retrieval on classification layer embeddings from Covid-Net and DenseNet. Carbon footprint was calculated using a specific equation combining power consumption, PUE, carbon intensity, and manufacturing emissions. Models were evaluated on the Covidx CXR-3 dataset using accuracy, time, carbon footprint, specificity, sensitivity, PPV, and confidence distributions.

## Key Results
- Covid-Net achieved highest accuracy (95.5%) with lowest carbon footprint (99.9% less than GPT-4.5-Preview)
- Smaller models like VGG reduced carbon footprint by 74% but showed lower confidence and 10% lower PPV
- LLMs struggled with classification accuracy (52.3% without knowledge bases) despite faster response times
- Knowledge bases improved LLM accuracy by up to 27% but had mixed environmental impacts

## Why This Works (Mechanism)

### Mechanism 1: Task-Specialized Architecture Efficiency
Discriminative models are architecturally constrained to output fixed-format probabilities (binary classification), whereas LLMs generate text tokens autoregressively. The constrained output format eliminates the computational overhead of token-by-token generation and attention over large context windows. This works when the task can be formulated as a closed classification problem with discrete labels. Evidence shows small neural networks consume less energy during inference than generative models.

### Mechanism 2: Knowledge Base Augmentation via Embedding Similarity
Pre-computed embeddings from discriminative model classification layers are stored in a vector database. At inference, the query image is embedded, cosine similarity retrieves the k-most similar labeled cases, and this context is injected into the LLM prompt. This improves LLM classification accuracy by up to 27% when the embedding space preserves semantic similarity relevant to the classification task. Retrieval-augmented generation with cosine similarity retrieval has been demonstrated to improve radiology Q&A performance.

### Mechanism 3: Confidence-Accuracy-Environment Trade-off Curve
Smaller models (e.g., VGG vs. Covid-Net) have reduced parameter capacity, leading to poorly calibrated probability outputs—probabilities cluster around decision boundaries rather than exhibiting high confidence on correct predictions. This creates a tradeoff where optimizing solely for carbon footprint reduction can degrade model confidence and positive predictive value, creating hidden safety costs. Models with high accuracy but low confidence separation indicate unreliable probability outputs.

## Foundational Learning

- **Discriminative vs. Generative Model Output Constraints**
  - Why needed: The paper's central comparison assumes understanding that discriminative models output fixed probability vectors while LLMs generate variable-length text
  - Quick check: Given a chest X-ray input, what does Covid-Net output vs. what does GPT-4.5 output?

- **Carbon Footprint Calculation Components**
  - Why needed: The paper uses a specific equation combining power consumption, PUE, carbon intensity, and manufacturing emissions
  - Quick check: What four variables does the carbon footprint equation require, and which are omitted for LLMs?

- **Cosine Similarity for Retrieval**
  - Why needed: Knowledge base augmentation relies on cosine similarity to find semantically similar X-ray embeddings
  - Quick check: Why might DenseNet and Covid-Net embeddings produce different retrieval results for the same query image?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Chest X-ray image upload → Mendix application
  - Local Path: ONNX runtime → Discriminative model (Covid-Net/DenseNet/ResNet/VGG) → Probability output
  - LLM Path: Image encoding → API call (OpenAI/Anthropic) → Text probability parsing
  - RAG Path: Discriminative model embedding layer → Vector store → Cosine similarity retrieval → Context injection → LLM prompt
  - Carbon Tracking: Timer on inference duration → Equation 1 calculation → Per-memory normalization (Equation 3)

- **Critical path:**
  1. Deploy discriminative model in ONNX format within application container
  2. Validate output format matches expected probability schema
  3. For LLM integration, structure prompt to enforce probability-only output format
  4. For RAG, pre-compute embeddings for knowledge base images using discriminative model classification layer

- **Design tradeoffs:**
  - Covid-Net vs. VGG: Covid-Net has 3.5x higher carbon footprint but 10% higher PPV and better confidence calibration
  - Local vs. API: Local models require application memory (~40-45% of instance) but avoid network latency and per-query API energy; API models shift compute to external infrastructure with ~1000x higher carbon per query
  - RAG augmentation: Improves LLM accuracy but adds embedding storage overhead and variable carbon impact depending on retrieval latency

- **Failure signatures:**
  - LLM hallucination without image: GPT returning probabilities even when image was not sent—always validate prompt token counts
  - Positive diagnosis bias: Smaller discriminative models (ResNet, VGG, DenseNet) show sensitivity >> specificity, indicating false positive tendency
  - Confidence miscalibration: Models with high accuracy but low confidence separation indicate unreliable probability outputs

- **First 3 experiments:**
  1. Replicate carbon footprint measurement for a single discriminative model (Covid-Net) on local infrastructure, validating Equation 1 against actual power meter readings
  2. Test LLM prompt variations to measure sensitivity of output probability to system instruction framing
  3. Build a minimal RAG pipeline with DenseNet embeddings on a 100-image subset, measuring accuracy gain vs. retrieval latency trade-off

## Open Questions the Paper Calls Out

### Open Question 1
Does the environmental efficiency of small discriminative models remain superior to LLMs when accounting for the full lifecycle, specifically the carbon footprint of model training? The authors note their analysis was constrained to model inference and recommend investigating the training stage to determine if the "collaborative nature of third-party LLMs could demonstrate greater environmental sustainability."

### Open Question 2
Can local discriminative models outperform generative AI in multi-class classification scenarios involving multiple distinct pathologies? The authors acknowledge the study was limited to binary classification and state that "future work should determine whether the superior accuracy in binary classification can also outperform generative models in multi-classification scenarios."

### Open Question 3
To what extent does fine-tuning vision-capable LLMs on specific medical datasets improve their diagnostic accuracy and reliability compared to the few-shot or knowledge-base approaches tested? The authors identify the lack of image fine-tuning for ChatGPT as a limitation, noting that "future work should explore the capabilities of fine-tuning LLMs and analysing their accuracy to local models."

## Limitations

- Carbon measurement accuracy depends on infrastructure assumptions and real-world PUE/carbon intensity variations
- Model generalization limited to single binary classification dataset, performance on multi-class or cross-domain datasets unknown
- LLM confidence calibration not measured (e.g., expected calibration error), limiting assessment of clinical reliability
- Knowledge base representativeness not characterized in detail

## Confidence

- High Confidence: Discriminative models outperform LLMs on both accuracy and carbon efficiency for this specific binary classification task
- Medium Confidence: The 99.9% carbon reduction claim is plausible given the cited calculations but depends on precise infrastructure assumptions
- Low Confidence: The claim that RAG improves LLM accuracy by up to 27% lacks mechanistic explanation

## Next Checks

1. **Carbon measurement validation:** Measure actual power consumption of Covid-Net inference on local hardware using a power meter, comparing against Equation 1 predictions to validate the 99.9% carbon reduction claim
2. **Cross-dataset generalization test:** Evaluate the four discriminative models and GPT-4.5 on a separate chest X-ray dataset (e.g., CheXpert) to assess whether accuracy gaps persist across domains
3. **Calibration analysis:** Compute expected calibration error (ECE) for all models' probability outputs to quantify confidence reliability beyond raw accuracy metrics