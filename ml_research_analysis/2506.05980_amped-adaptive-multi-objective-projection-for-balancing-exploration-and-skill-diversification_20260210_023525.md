---
ver: rpa2
title: 'AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill
  Diversification'
arxiv_id: '2506.05980'
source_url: https://arxiv.org/abs/2506.05980
tags:
- skill
- skills
- exploration
- learning
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMPED tackles the challenge of jointly maximizing exploration and
  skill diversity in skill-based reinforcement learning, where these objectives often
  conflict. The method introduces adaptive multi-objective gradient projection to
  balance exploration (via entropy and RND) and diversity (via AnInfoNCE) objectives,
  and uses a skill selector to adaptively choose skills during fine-tuning.
---

# AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification

## Quick Facts
- arXiv ID: 2506.05980
- Source URL: https://arxiv.org/abs/2506.05980
- Reference count: 40
- Primary result: Outperforms strong baselines on URLB with statistically significant improvements in return.

## Executive Summary
AMPED addresses the challenge of jointly maximizing exploration and skill diversity in skill-based reinforcement learning, where these objectives often conflict. The method introduces adaptive multi-objective gradient projection to balance exploration (via entropy and RND) and diversity (via AnInfoNCE) objectives, and uses a skill selector to adaptively choose skills during fine-tuning. This approach outperforms strong baselines on the Unsupervised Reinforcement Learning Benchmark, achieving statistically significant improvements in return. Ablation studies confirm each component—RND, AnInfoNCE, gradient surgery, and skill selection—contributes to performance. Theoretical and empirical evidence shows that greater skill diversity reduces fine-tuning sample complexity when paired with a greedy selector.

## Method Summary
AMPED is a skill-based RL method that pretrains a skill-conditioned policy using unsupervised objectives before fine-tuning on downstream tasks. During pretraining, it balances exploration (particle-based entropy and RND) and diversity (AnInfoNCE) objectives through adaptive multi-objective gradient projection (PCGrad) to resolve conflicts between these objectives. The method then employs a skill selector during fine-tuning that adaptively chooses from learned skills using an ε-greedy strategy. This approach is evaluated on the Unsupervised Reinforcement Learning Benchmark (URLB) across multiple domains including Maze, Walker, Quadruped, and Jaco.

## Key Results
- Outperforms strong baselines on URLB with statistically significant improvements in return
- Demonstrates that balancing exploration and diversity through gradient projection improves skill learning
- Shows that greater skill diversity reduces fine-tuning sample complexity when paired with a greedy selector
- Ablation studies confirm the contribution of each component (RND, AnInfoNCE, gradient surgery, skill selection)

## Why This Works (Mechanism)
AMPED works by explicitly resolving the conflict between exploration and diversity objectives during skill pretraining. The adaptive multi-objective gradient projection (PCGrad) technique projects conflicting gradients to their average direction when they point in opposite directions, allowing both objectives to be optimized simultaneously rather than having one dominate. This creates a more diverse skill set that better covers the state space. During fine-tuning, the skill selector adaptively chooses the most appropriate skills for each task, with evidence showing that more diverse skill sets lead to faster task learning and reduced sample complexity.

## Foundational Learning
- **Unsupervised RL pretraining**: Learn skills without task rewards before fine-tuning on specific tasks - needed to create reusable skill libraries; quick check: pretraining converges to stable skill distribution
- **Intrinsic motivation objectives**: Use entropy, RND, and diversity measures instead of task rewards during pretraining - needed to encourage exploration without external guidance; quick check: intrinsic reward signals are non-zero and drive policy changes
- **Multi-objective optimization**: Balance competing objectives (exploration vs. diversity) during training - needed because these goals often conflict; quick check: gradient conflict ratio decreases during pretraining
- **Skill selection strategies**: Choose appropriate skills during fine-tuning - needed to leverage pretrained skills for specific tasks; quick check: skill selector converges to use 2-4 skills per task

## Architecture Onboarding
**Component map**: Observation -> Encoder -> Skill Policy -> Skill Selector -> Environment
**Critical path**: State → Encoder → Skill-conditioned policy → Action
**Design tradeoffs**: Fixed vs. adaptive skill count; exploration methods (entropy vs RND vs both); gradient conflict resolution (PCGrad vs no projection)
**Failure signatures**: Gradient conflict persists despite PCGrad (check projection application); skill selector underperforms random selection (likely sparse reward issue); high variance across seeds
**First experiments**: 1) Verify PCGrad projects conflicting gradients when dot product < 0; 2) Confirm intrinsic rewards drive exploration (non-zero particle entropy, RND loss); 3) Check skill selector converges to 2-4 skills per task by fine-tuning end

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Can the framework be extended to dynamically adjust the number of skills during pre-training based on environment coverage rather than treating skill count as a fixed hyperparameter?
- **Basis in paper**: [explicit] Appendix K notes that a fixed number of skills is "not ideal across different environments" and suggests developing mechanisms to "dynamically adjust the number of skills according to the environment's requirements."
- **Why unresolved**: The current implementation requires manually tuning the skill dimension (e.g., 16 skills) for different domains, which may lead to suboptimal coverage in complex environments or redundancy in simpler ones.
- **What evidence would resolve it**: An adaptive variant of AMPED that automatically adds or prunes skills during training to maximize coverage efficiently, outperforming fixed-skill configurations.

### Open Question 2
- **Question**: How does specific environment structure (e.g., maze topology vs. open space) systematically influence the degree of gradient conflict between exploration and diversity objectives?
- **Basis in paper**: [explicit] Appendix K highlights that gradient conflict is "strongly task-dependent" and calls for a "systematic characterization of how environment structure influences exploration-diversity gradient interactions."
- **Why unresolved**: The paper observes varying conflict ratios across Walker, Quadruped, and Jaco domains but does not provide a theoretical or empirical model linking these differences to specific environmental features.
- **What evidence would resolve it**: A study analyzing gradient conflict ratios across a spectrum of procedurally generated environments with controlled topological features.

### Open Question 3
- **Question**: Does the gradient-surgery approach remain effective in high-dimensional, pixel-based observation spaces where exploration challenges differ significantly from state-based inputs?
- **Basis in paper**: [explicit] Appendix J states that while METRA was designed for pixel-based settings, "extending our framework to pixel-based domains is a natural next step" for future work.
- **Why unresolved**: The current evaluation is restricted to state-based inputs (URLB), and it is unclear if the computational overhead or gradient dynamics scale effectively to high-dimensional visual inputs.
- **What evidence would resolve it**: Benchmark results showing AMPED's performance and gradient conflict metrics on pixel-based control tasks compared to methods like METRA.

## Limitations
- Theoretical claims about reduced sample complexity are based on a simple bandit model that may not fully capture real-world fine-tuning dynamics
- The framework requires manual tuning of the skill dimension for different domains
- Limited evidence across diverse environments to assess long-term stability of the exploration-diversity balance
- The relationship between gradient conflict and specific environmental features is not systematically characterized

## Confidence
- **High Confidence**: Claims about AMPED's superior performance on URLB benchmarks compared to baselines, supported by statistical significance tests and rliable aggregation
- **Medium Confidence**: Claims about the effectiveness of the skill selector and the relationship between skill diversity and fine-tuning sample complexity, as these are primarily empirical observations with limited theoretical grounding
- **Low Confidence**: Claims about the long-term stability of the exploration-diversity balance, as the paper does not provide extensive evidence across diverse environments or extended training horizons

## Next Checks
1. Conduct a more rigorous theoretical analysis to validate the claims about sample complexity reduction, potentially using more complex models that better capture fine-tuning dynamics
2. Perform extensive hyperparameter sensitivity analysis to ensure the observed improvements are robust to different hyperparameter choices
3. Test AMPED on a wider range of environments beyond URLB to assess its generalizability and long-term stability across diverse task domains