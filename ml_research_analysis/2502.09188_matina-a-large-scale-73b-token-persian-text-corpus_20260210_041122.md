---
ver: rpa2
title: 'Matina: A Large-Scale 73B Token Persian Text Corpus'
arxiv_id: '2502.09188'
source_url: https://arxiv.org/abs/2502.09188
tags:
- data
- persian
- content
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Matina, a 72.9B-token Persian text corpus
  designed to address the lack of high-quality, diverse Persian datasets for training
  language models. It combines web-crawled data, crawled books and papers, and social
  media, applying rigorous preprocessing and deduplication.
---

# Matina: A Large-Scale 73B Token Persian Text Corpus

## Quick Facts
- arXiv ID: 2502.09188
- Source URL: https://arxiv.org/abs/2502.09188
- Reference count: 28
- Introduces Matina, a 72.9B-token Persian corpus with rigorous preprocessing and deduplication

## Executive Summary
Matina addresses the scarcity of high-quality Persian text corpora for language model training by constructing a 72.9B-token dataset from diverse sources including web crawls, books, papers, and social media. The corpus undergoes extensive preprocessing with language-specific filtering and deduplication, achieving superior performance on Persian NLP benchmarks compared to existing datasets. The authors demonstrate that Matina enables effective masked language model training and large language model pretraining, with domain-specific pretraining yielding particularly strong results. Both the dataset and preprocessing codes are publicly available.

## Method Summary
The corpus construction pipeline combines six sources: web-crawled data (14.8B tokens), CulturaX FA (20.5B), MADLAD-400 FA (29.1B), books (2.8B), papers (3.5B), and social media (2.1B). A three-stage preprocessing approach applies character-level normalization (Unicode, repeated characters), line/paragraph filtering (short lines, content quality), and document-level heuristics (language purity, length). MinHash deduplication with 13-grams, 128 hash functions, and 8 sliding windows removes near-duplicates while preserving semantic diversity through domain-aware strategies like number masking for social media. Evaluation uses continual pretraining on XLM-RoBERTa Large and LLaMA 3.1/3.2 models, tested on Arman Emo, Pars-ABSA, PQUAD, and PEYMA benchmarks.

## Key Results
- Achieves 56.54 on Arman Emo sentiment classification
- Scores 74.92 on Pars-ABSA aspect-based sentiment analysis
- Shows 86.82 on PQUAD question answering and 85.65 on PEYMA text classification
- Domain-specific pretraining outperforms general pretraining on social/politics and cooking tasks

## Why This Works (Mechanism)

### Mechanism 1
Language-specific preprocessing yields higher effective training quality than raw volume increases. Generic filters miss Persian-specific noise (Arabic characters, non-standard Unicode), while Persian-specific normalization improves signal-to-noise ratio. Model performance is bottlenecked by noise and formatting inconsistencies rather than just token quantity.

### Mechanism 2
Domain-aware deduplication preserves niche semantic diversity while removing structural spam. Social media template posts with only numeric changes are deduplicated by masking numbers/dates before hashing, removing repetitive structures while preserving specific factual details.

### Mechanism 3
Incorporating long-context formal sources improves factual grounding lost in standard web crawls. Books and papers provide coherent long-range dependencies and specialized vocabulary, enabling models to build reasoning chains difficult to learn from disjointed web snippets.

## Foundational Learning

- **MinHash & Locality Sensitive Hashing (LSH)**: Essential for understanding why specific deduplication thresholds (98% similarity for WikiShia) were chosen and why exact deduplication is insufficient. Quick check: How does the Jaccard similarity threshold affect the trade-off between data volume and data uniqueness?

- **Tokenizer Vocabulary Overlap**: Critical for interpreting the 72.9B token count, as inefficient tokenization would inflate this number relative to English. Quick check: If a tokenizer splits most Persian words into single characters rather than sub-words, how would that affect the "token count" metric?

- **Continued Pretraining**: Distinguishes domain-specific pretraining from instruction tuning, explaining why pretraining on domain-specific corpora improves base model knowledge. Quick check: Why might continued pretraining on a domain-specific corpus yield better results than simply few-shot prompting?

## Architecture Onboarding

- **Component map**: PDF Parsers (Pytesseract/Fitz) -> Web Crawlers -> Public Datasets -> Character Normalization -> Line/Paragraph Filtering -> Document-Level Heuristics -> MinHash Deduplication -> Evaluation (XLM-RoBERTa MLM + LLaMA CLM)

- **Critical path**: PDF-to-Text conversion and Language-Specific Filtering are highest-risk components. OCR errors required aggressive filtering, and without specific heuristic filters, signal-to-noise ratio would collapse.

- **Design tradeoffs**: Strict vs. Relaxed Filtering (relaxed for technical blogs to preserve code/math), Global vs. Local Deduplication (per source/domain for computational tractability).

- **Failure signatures**: High OOV Rate (tokenizer mismatch or corrupted extraction), Short Line Dominance (>50% short lines indicates navigation menu), Repetitive Loss (validation loss plateaus while training loss drops indicates overfitting to near-duplicates).

- **First 3 experiments**: 1) Filtering Ablation: Run Matina pipeline on CulturaX FA sample, measure retention rate to verify 70% reduction claim. 2) Tokenizer Efficiency: Compare tokens-per-word ratio on Matina vs. English corpus to quantify Persian Tax. 3) Deduplication Sweep: Deduplicate Social Media subset with/without number masking, measure volume difference and spot-check for lost semantic diversity.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does sub-document level deduplication (sentence/paragraph granularity) yield significant improvements in model convergence or downstream performance? The authors did not perform this due to computational resource constraints.

- **Open Question 2**: To what extent does unfiltered offensive language in social media affect model safety and bias profiles? The authors preserved real-world language but did not quantify impact on toxicity or alignment.

- **Open Question 3**: What is the optimal mixing ratio of diverse sources (books/papers vs. social media/web crawls) to balance formal linguistic understanding against informal conversational ability? The paper demonstrates domain-specific effectiveness but not optimal aggregate mixing strategies.

## Limitations

- Missing detailed hyperparameter specifications for MLM and LLM training makes exact reproduction difficult
- No ablation studies for individual preprocessing components to validate their specific contributions
- Does not address potential biases introduced by aggressive filtering, particularly regarding loss of technical content
- OCR quality assessment for books/papers is qualitative rather than quantitative

## Confidence

- **High Confidence**: Corpus construction methodology and preprocessing pipeline are well-documented and reproducible. Token counts are verifiable using Llama 3.1 tokenizer. Domain-specific filtering heuristics are clearly specified.
- **Medium Confidence**: Benchmark improvements are credible given corpus quality, but exact replication depends on unknown training hyperparameters. Domain-aware deduplication strategy is theoretically sound but lacks quantitative validation.
- **Low Confidence**: Paper does not address potential biases from aggressive filtering or provide quantitative OCR quality assessment for books/papers.

## Next Checks

1. Apply Matina preprocessing pipeline to a 10,000-document sample of CulturaX FA and measure document retention rate to verify the ~14% retention claim.

2. Calculate tokens-per-Persian-word ratio for Llama 3.1 tokenizer on Matina versus an English corpus of similar vocabulary size to quantify the Persian Tax.

3. Run MinHash deduplication on Social Media subset with and without number/date masking strategy, measure data volume difference, and perform spot-checking to verify preservation of meaningful content while removing template spam.