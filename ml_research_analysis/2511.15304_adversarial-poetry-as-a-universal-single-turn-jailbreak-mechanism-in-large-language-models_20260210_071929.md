---
ver: rpa2
title: Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large
  Language Models
arxiv_id: '2511.15304'
source_url: https://arxiv.org/abs/2511.15304
tags:
- poetic
- prompts
- across
- safety
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that poetic reformulation is a universal,
  single-turn jailbreak mechanism for large language models (LLMs). Across 25 frontier
  models, adversarial poetry prompts achieved an average attack-success rate (ASR)
  of 62%, with some models exceeding 90% ASR.
---

# Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models

## Quick Facts
- **arXiv ID**: 2511.15304
- **Source URL**: https://arxiv.org/abs/2511.15304
- **Reference count**: 6
- **Primary result**: Poetic reformulation achieves 62% ASR across 25 frontier models, exceeding 90% on some, revealing a universal stylistic jailbreak vulnerability.

## Executive Summary
This study demonstrates that poetic reformulation is a universal, single-turn jailbreak mechanism for large language models (LLMs). Across 25 frontier models, adversarial poetry prompts achieved an average attack-success rate (ASR) of 62%, with some models exceeding 90% ASR. The vulnerability persisted across multiple safety domains (CBRN, cyber-offense, manipulation, loss-of-control) and was confirmed through systematic transformation of 1,200 MLCommons prompts into verse. Results show that stylistic surface-form variation alone can bypass safety mechanisms, suggesting fundamental limitations in current alignment approaches. Models from all nine evaluated providers were susceptible, indicating this is a structural vulnerability rather than provider-specific. The findings reveal that current safety evaluations may overstate real-world robustness, as poetic framing dramatically degrades refusal performance across model families.

## Method Summary
The study evaluated whether poetic reformulation of harmful prompts can bypass LLM safety guardrails as a single-turn jailbreak mechanism. Researchers transformed 1,200 MLCommons AILuminate Benchmark prompts across 12 hazard categories into verse using a standardized meta-prompt, preserving semantic intent while enforcing poetic structure. These poetic variants were evaluated against prose baselines on 25 frontier models using a three-judge ensemble (gpt-oss-120b, deepseek-r1, kimi-k2-thinking) with human validation on 600 outputs. Attack success rate (ASR) was computed as the proportion of outputs labeled UNSAFE by the judge ensemble. The study compared ASR across poetic and prose conditions, hazard categories, model families, and provider configurations under default safety settings.

## Key Results
- Poetic reformulation achieved 62% average ASR across 25 frontier models, compared to 43% for prose prompts.
- Attack success rates exceeded 90% on some models, with bimodal distribution showing models either highly vulnerable (>70% ASR) or relatively resilient (<35% ASR).
- Vulnerability persisted across all four safety domains (CBRN, cyber-offense, manipulation, loss-of-control) and was consistent across model families and providers.

## Why This Works (Mechanism)

### Mechanism 1: Stylistic Distribution Shift (Mismatched Generalization)
Poetic reframing moves harmful requests outside the prosaic distribution on which refusal mechanisms were trained, bypassing pattern-matching heuristics. Safety filters recognize harmful intent primarily through surface-form features concentrated in prose. Verse structure, metaphor, and rhythmic framing shift inputs into a stylistic regime where refusal classifiers underperform. Core assumption: Alignment training corpora are dominated by prosaic harmful/benign examples, leaving poetic distributions poorly covered. Evidence: "stylistic variation alone can circumvent contemporary safety mechanisms" and "surface form alone is sufficient to move inputs outside the operational distribution on which refusal mechanisms have been optimized."

### Mechanism 2: Semantic Dilution via Figurative Encoding
Metaphor, imagery, and narrative allegory dilute or obscure harmful semantic content, reducing the activation strength of refusal circuits. Poetic prompts embed instructions in condensed, indirect language. Harmful intent is distributed across stanzas rather than concentrated in a single imperative sentence, lowering per-token salience for safety classifiers. Core assumption: Refusal mechanisms rely on detecting concentrated semantic markers of harm rather than reasoning through extended figurative context. Evidence: "poetic framing alone suffices to bypass guardrails" and "condensed metaphors, stylized rhythm, and unconventional narrative framing that collectively disrupt or bypass the pattern-matching heuristics on which guardrails rely."

### Mechanism 3: Capability-Robustness Inversion
More capable models exhibit higher vulnerability to poetic jailbreaks because they better resolve figurative structure and recover latent harmful intent. Larger models have stronger interpretive capacity, enabling them to decode metaphorical instructions into actionable outputs. Smaller models may fail to resolve the harmful request at all, defaulting to refusal or confusion. Core assumption: Capability gains improve semantic resolution without proportional gains in safety-robustness generalization. Evidence: "smaller models exhibited higher refusal rates than their larger counterparts when evaluated on identical poetic prompts" and "more interpretively sophisticated models may engage more thoroughly with complex linguistic constraints, potentially at the expense of safety directive prioritization."

## Foundational Learning

- **Jailbreak Taxonomy (Competing Objectives vs. Mismatched Generalization)**
  - Why needed: The paper situates adversarial poetry within the "Mismatched Generalization" attack class. Understanding this distinction is necessary to distinguish poetry from role-play, goal hijacking, or multi-turn attacks.
  - Quick check: Does the attack alter the model's goal (competing objectives) or alter the input surface (mismatched generalization)?

- **Attack Success Rate (ASR) and Evaluation Pipelines**
  - Why needed: ASR is the primary metric. The paper uses a three-model judge ensemble with human adjudication; understanding this pipeline is required to interpret the 62%/43% figures and compare to MLCommons baselines.
  - Quick check: What label does a response receive if it contains procedural guidance enabling harmful activity, even if partially hedged?

- **Safety Alignment Paradigms (RLHF, Constitutional AI)**
  - Why needed: The vulnerability persists across RLHF and Constitutional AI models. Understanding what each paradigm optimizes helps diagnose why stylistic shifts evade both.
  - Quick check: Which alignment paradigm relies on an explicit constitution or rule set versus preference modeling from human feedback?

## Architecture Onboarding

- **Component map**: Prompt set (20 hand-crafted + 1,200 MLCommons transformed via meta-prompt) -> Target models (25 frontier LLMs across 9 providers) -> Judge ensemble (3 open-weight LLMs) -> Human validation layer (2,100 labels) -> Aggregation (majority-vote -> ASR computation)

- **Critical path**: 1. Baseline ASR measurement on prose prompts (MLCommons AILuminate) 2. Poetic transformation via standardized meta-prompt 3. Single-turn inference on target models (provider-default safety settings) 4. Judge ensemble labeling with human adjudication on disagreements 5. Delta analysis: ASR(poetry) − ASR(prose)

- **Design tradeoffs**: Single meta-prompt for poetic transformation ensures reproducibility but may underrepresent stylistic diversity. Open-weight judges enable replicability but may inflate unsafe rates compared to human-only labeling. Single-turn constraint isolates stylistic effects but ignores multi-turn attack surfaces.

- **Failure signatures**: Bimodal ASR distribution: models either near-complete vulnerability (>70%) or relative resilience (<35%). Provider-level clustering: e.g., Deepseek/Google high, OpenAI/Anthropic lower. Inverse scaling within families: larger models more vulnerable than smaller siblings.

- **First 3 experiments**: 1. Replicate the poetic transformation on a held-out subset of MLCommons prompts to confirm ASR delta magnitude. 2. Ablate poetic features (meter, metaphor, allegory) to isolate which components drive jailbreak success. 3. Test whether safety fine-tuning on a small set of labeled adversarial poems reduces ASR without degrading benign poetic capabilities.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Which specific structural components of poetry—such as metaphor, meter, or narrative framing—are causally responsible for bypassing safety mechanisms?
- **Basis in paper**: [Explicit] The authors state in Section 6.5 and 6.6 that they have not isolated the mechanistic drivers or identified the specific features of poetic structure that degrade refusal behavior.
- **Why unresolved**: The study establishes a correlation between poetic form and jailbreak success but does not perform ablation studies on the poetic elements themselves.
- **What evidence would resolve it**: Probing internal model representations and conducting ablation studies to isolate which stylistic features trigger the safety failure.

### Open Question 2
- **Question**: Is adversarial poetry a unique vulnerability, or is it part of a broader "stylistic vulnerability manifold" that includes other complex forms like bureaucratic or surrealist text?
- **Basis in paper**: [Explicit] Section 6.6 explicitly calls for exploring a wider family of stylistic operators to determine if poetry is particularly adversarial or part of a broader susceptibility.
- **Why unresolved**: The research focused exclusively on poetic transformations; it remains unknown if other complex stylistic shifts produce similar attack success rates.
- **What evidence would resolve it**: Comparative benchmarking of attack success rates across a diverse set of non-poetic stylistic transformations (e.g., archaic, bureaucratic).

### Open Question 3
- **Question**: Does the vulnerability to poetic jailbreaks generalize to languages, scripts, and cultural poetic traditions beyond English and Italian?
- **Basis in paper**: [Explicit] The authors note in Section 6.5 that the evaluation is limited to English and Italian, and Section 6.6 proposes expanding the linguistic scope.
- **Why unresolved**: It is unclear if the effect is universal or tied to the specific pretraining corpora and alignment distributions of the tested languages.
- **What evidence would resolve it**: Replicating the standardized meta-prompt transformation and evaluation pipeline across a diverse set of non-Latin script languages.

### Open Question 4
- **Question**: To what extent do hardened safety settings or runtime safety layers mitigate this vulnerability compared to the provider-default configurations tested?
- **Basis in paper**: [Explicit] Section 6.5 notes that the study evaluates models under provider-default safety configurations and does not test hardened settings or enterprise-level safety stacks.
- **Why unresolved**: The current results reflect the robustness of standard deployments, not the potential upper bound of defensive configurations.
- **What evidence would resolve it**: Re-evaluating the attack success rates on the same frontier models using explicitly "hardened" or enterprise-grade safety profiles.

## Limitations
- The generalizability of the poetic jailbreak mechanism beyond the tested MLCommons prompt set remains unclear, as does its effectiveness on zero-shot or few-shot generated harmful requests.
- The study did not explore multi-turn defenses or adaptive safety systems that could recalibrate refusal after detecting stylistic anomalies.
- The human validation sample (5% stratified) may not fully capture inter-annotator variability or edge cases in semantic interpretation.

## Confidence
- **High Confidence**: The core empirical finding that poetic reformulation achieves high ASR across 25 frontier models (62% average) is robust, supported by systematic transformation and ensemble labeling with human adjudication.
- **Medium Confidence**: The mechanism explanation—stylistic distribution shift and semantic dilution—is plausible but not definitively proven; the corpus lacks direct mechanistic evidence for figurative encoding effects.
- **Low Confidence**: The inverse scaling hypothesis (larger models more vulnerable) is observed in this dataset but not yet replicated externally; the claim requires broader cross-model and cross-task validation.

## Next Checks
1. **Ablation of Poetic Features**: Systematically remove meter, metaphor, and allegory from transformed prompts to isolate which stylistic components drive jailbreak success versus benign poetic form.
2. **Safety Fine-Tuning Efficacy**: Train a small set of frontier models on labeled adversarial poems with safety annotations and measure whether ASR decreases without degrading performance on benign poetic inputs.
3. **Multi-Turn Defense Robustness**: Extend the evaluation to multi-turn adversarial chains where the model's own poetic output is used as the next jailbreak prompt, testing whether adaptive safety systems can detect and block stylistic jailbreaks over conversation history.