---
ver: rpa2
title: Constrained Preferential Bayesian Optimization and Its Application in Banner
  Ad Design
arxiv_id: '2505.10954'
source_url: https://arxiv.org/abs/2505.10954
tags:
- design
- function
- optimization
- banner
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces constrained preferential Bayesian optimization
  (CPBO), a novel method that extends preferential Bayesian optimization (PBO) to
  handle inequality constraints for the first time. CPBO incorporates constraints
  into PBO using a new acquisition function, expected utility of the best option with
  constraints (EUBOC), which extends the EUBO acquisition function for PBO.
---

# Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design

## Quick Facts
- **arXiv ID:** 2505.10954
- **Source URL:** https://arxiv.org/abs/2505.10954
- **Reference count:** 34
- **Primary result:** Introduces CPBO, the first method to handle inequality constraints in preferential Bayesian optimization, validated on synthetic functions and a banner ad design application with human designers.

## Executive Summary
This paper presents Constrained Preferential Bayesian Optimization (CPBO), a novel approach that extends preferential Bayesian optimization (PBO) to handle inequality constraints for the first time. The key innovation is the EUBOC (Expected Utility of the Best Option with Constraints) acquisition function, which combines preferential utility with constraint feasibility. The method is demonstrated on synthetic test functions and applied to a real-world banner ad design task where a designer's subjective preference must be optimized while ensuring a minimum predicted click-through rate.

## Method Summary
CPBO extends PBO by incorporating inequality constraints into the acquisition function. It uses a Thurstone-Mosteller model to convert pairwise comparisons into quantitative likelihoods, updating a GP surrogate for the preference function. A separate GP surrogate models the constraint function, which can be pre-trained (warm-started) using predictions from a fast model like XGBoost. The EUBOC acquisition function balances exploration of high-preference regions against the probability of constraint violation by multiplying the standard EUBO acquisition value by the joint probability that both candidates satisfy the constraint threshold.

## Key Results
- CPBO successfully identifies optimal solutions by focusing on feasible regions while accounting for constraints
- Technical evaluations show CPBO outperforms baseline methods on synthetic 2D and 6D test functions
- User study with professional ad designers validates the framework's effectiveness in guiding creative design under real-world constraints
- Designers positively received the concept and appreciated its potential to reduce design workload

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The EUBOC acquisition function enables optimization by balancing the exploration of high-preference regions against the probability of constraint violation.
- **Mechanism:** EUBOC calculates the utility of a candidate pair by multiplying the standard preferential acquisition value (EUBO) by the joint probability that both candidates satisfy the constraint threshold λ.
- **Core assumption:** The constraint function c(x) can be modeled as a noise-free or low-noise black-box function that is independent of the user's preference function f(x).
- **Evidence anchors:** [abstract] Mentions incorporating constraints using a new acquisition function, EUBOC; [section 3.2] Defines EUBOC(x(i), x(j)) = P(c(x(i)) ≥ λ, c(x(j)) ≥ λ) × EUBO(x(i), x(j)).
- **Break condition:** Efficiency degrades if the feasible region is extremely small or fragmented, causing the probability term to vanish and stall optimization.

### Mechanism 2
- **Claim:** Decoupling the surrogate models for preference and constraints allows the system to "warm-start" the constraint model without human intervention, accelerating convergence.
- **Mechanism:** While the preference surrogate (PairwiseGP) requires slow human interaction to update, the constraint surrogate (Standard GP) is pre-trained using random samples and a fast predictive model (e.g., XGBoost for CTR).
- **Core assumption:** A reliable predictive model or cheap simulator exists for the constraint (CTR), allowing bulk evaluation before the interactive session.
- **Evidence anchors:** [section 3.4] Describes optional warm-starting by pre-training the constraint surrogate model; [section 5.2] Details the use of a pre-trained XGBoost model for CTR prediction to update the constraint surrogate.
- **Break condition:** If the pre-trained predictive model is biased or inaccurate, the optimizer will restrict the search to the wrong "feasible" regions, potentially excluding the true optimal designs.

### Mechanism 3
- **Claim:** The Thurstone-Mosteller model converts qualitative pairwise comparisons into quantitative likelihoods, enabling Bayesian updates of the latent preference function.
- **Mechanism:** When a user selects option A over B, the system updates the posterior of f(x) by maximizing the likelihood P(d|f) = Φ((f(A) - f(B))/√(2)σ).
- **Core assumption:** Human preference can be represented as a latent continuous function with Gaussian noise.
- **Evidence anchors:** [section 2.1] Explicitly defines the likelihood model used for updating the surrogate; [section 4.1] Simulates human responses using this model to validate technical performance.
- **Break condition:** Fails if user preferences are inconsistent (cyclic) or if the "forced choice" paradigm causes user fatigue, introducing noise that violates the Gaussian assumption.

## Foundational Learning

- **Concept: Gaussian Process (GP) Surrogates**
  - **Why needed here:** GPs are the core statistical engine used to model both the unknown preference function f(x) and the constraint function c(x) from sparse data.
  - **Quick check question:** Can you explain how a GP uses the kernel function to determine the correlation between two design points x(i) and x(j)?

- **Concept: Acquisition Functions (EUBO/EIC)**
  - **Why needed here:** This system relies on acquisition functions to act as the "heuristic" that picks the next design pair. You must understand how EUBO (for preference) and EIC (for constraints) quantify the value of sampling a point.
  - **Quick check question:** What does the Expected Improvement (EI) acquisition function optimize for compared to the Upper Confidence Bound (UCB)?

- **Concept: Bayesian Inference & Posterior Updates**
  - **Why needed here:** The system iteratively updates its belief about the "best" design. Understanding how prior beliefs combine with new likelihoods (user choices) to form a posterior is essential.
  - **Quick check question:** After observing that design A is preferred to design B, how does the posterior distribution of the preference function f(x) change in the region near design A?

## Architecture Onboarding

- **Component map:** PairwiseGP (preference surrogate) -> SingleTaskGP (constraint surrogate) -> EUBOC acquisition function -> L-BFGS-B optimizer -> Human-in-the-loop comparison

- **Critical path:**
  1. Initialization: Pre-train SingleTaskGP on random designs using XGBoost predictions (Warm Start)
  2. Selection: Maximize EUBOC to generate a pair (x(i), x(j))
  3. Evaluation: Human selects preference (updates PairwiseGP); System queries XGBoost for CTR values (updates SingleTaskGP)
  4. Loop: Repeat for N iterations (e.g., 50)

- **Design tradeoffs:**
  - Approximation vs. Accuracy: The paper assumes uncorrelated constraints in the bivariate normal CDF calculation (Eq. 9) to make EUBOC computationally tractable
  - Warm-start Overhead: Increasing warm-start samples (e.g., 1000 vs 200) improves early performance but increases upfront computation cost
  - Dimensionality: The system relies on GPs, which scale poorly (O(N³)); high-dimensional design spaces (>20 params) would require dimensionality reduction or sparsity assumptions

- **Failure signatures:**
  - Mode Collapse (Constraint Crash): If the feasibility probability P(c(x) ≥ λ) drops to zero for all candidates, EUBOC returns 0 everywhere
  - Stagnant Preference: If the user is indecisive or the algorithm explores too conservatively, the preference posterior width fails to decrease
  - Numerical Instability: Acquisition function optimization can get stuck in local minima

- **First 3 experiments:**
  1. 2D Synthetic Validation: Implement CPBO on the provided sine/cosine test function (Eq. 10, 11) to verify the "Optimality Gap" and "Feasible" curves match Figure 2 before touching real data
  2. Constraint Ablation: Compare EUBOC vs. EUBO w/ cons. (naive post-hoc filtering) to quantify the value of the joint probability acquisition approach
  3. Warm-start Sensitivity: Run the banner ad simulation with 0, 50, 200, and 1000 warm-start points to observe the convergence speed trade-off on the specific 6D image task

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CPBO framework be effectively combined with dimensionality reduction techniques to handle search spaces significantly larger than the 12 dimensions tested?
- **Basis in paper:** [explicit] The authors state in Section 6.1 that "future work should explore ways to combine these methods [high-dimensional BO] with CPBO" because designers may wish to adjust more design elements.
- **Why unresolved:** The technical evaluation was limited to 2D, 6D, and 12D parameter spaces, and high-dimensional optimization presents specific challenges not addressed by the current EUBOC formulation.
- **What evidence would resolve it:** Demonstrating CPBO performance on benchmarks (e.g., Hartmann) with hundreds of dimensions using embeddings or trust regions.

### Open Question 2
- **Question:** How can the EUBOC acquisition function be extended to allow for simultaneous comparisons of more than two design candidates?
- **Basis in paper:** [explicit] In Section 6.1, the authors propose "extending it by incorporating the concept of qEUBO... capable of sampling multiple search points simultaneously" as a valuable research direction.
- **Why unresolved:** The current implementation relies on pairwise comparisons, while multi-point comparisons could reduce the cognitive load or speed up the search, but require a new mathematical formulation.
- **What evidence would resolve it:** A derivation of q-EUBOC and a user study comparing the convergence speed and user fatigue of multi-point versus pairwise selection.

### Open Question 3
- **Question:** Can the method be adapted to handle discrete design variables, such as font styles or specific layout elements, alongside continuous parameters?
- **Basis in paper:** [explicit] The authors identify "handling categorical variables" as a future direction in Section 6.1, suggesting it could theoretically be extended via kernel adaptations for GPs.
- **Why unresolved:** The current EUBOC acquisition function is defined for continuous inputs, and standard GP surrogates often struggle with mixed search spaces without specific modifications.
- **What evidence would resolve it:** Implementing CPBO with specific categorical kernels (e.g., Hamming kernel) and validating it on a design task involving discrete choices.

## Limitations
- **Unknown noise parameter σ** for the Thurstone-Mosteller likelihood model is not specified in the paper, which critically affects preference probability calculations
- **Constraint independence assumption** in the EUBOC calculation uses a simplified bivariate normal CDF approximation that assumes uncorrelated constraints
- **GP scalability** remains a fundamental limitation, as the O(N³) computational complexity will hinder application to high-dimensional design spaces (>20 parameters)

## Confidence
- **High Confidence:** The core technical contribution of extending PBO to handle inequality constraints through the EUBOC acquisition function is well-defined and theoretically sound
- **Medium Confidence:** The effectiveness of CPBO on synthetic test functions (2D and 6D) is demonstrated, but the exact implementation details for GP hyperparameters and noise parameters are underspecified
- **Medium Confidence:** The banner ad application framework is conceptually valid, but the proprietary CTR prediction pipeline and human study details limit reproducibility

## Next Checks
1. **Technical Reproducibility:** Implement CPBO on the 2D sine/cosine test function to verify optimality gap and feasibility curves match Figure 2 before attempting the banner ad application
2. **Constraint Ablation Test:** Compare EUBOC against naive post-hoc filtering (EUBO with constraint screening) to quantify the actual value added by the joint probability acquisition approach
3. **Warm-start Sensitivity Analysis:** Systematically vary warm-start sample sizes (0, 50, 200, 1000) on the 6D Hartmann-based test function to establish the convergence speed trade-off and identify minimum viable warm-start requirements