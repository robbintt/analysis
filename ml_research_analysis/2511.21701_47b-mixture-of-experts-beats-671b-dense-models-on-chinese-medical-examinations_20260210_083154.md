---
ver: rpa2
title: 47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations
arxiv_id: '2511.21701'
source_url: https://arxiv.org/abs/2511.21701
tags:
- medical
- performance
- clinical
- arxiv
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a comprehensive benchmark for evaluating
  large language models (LLMs) on Chinese medical examination questions across seven
  specialties and two professional levels. A dataset of 2,800 carefully curated multiple-choice
  questions was used to assess 27 state-of-the-art models, including both dense and
  mixture-of-experts architectures.
---

# 47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations

## Quick Facts
- **arXiv ID:** 2511.21701
- **Source URL:** https://arxiv.org/abs/2511.21701
- **Reference count:** 40
- **Primary result:** Mixtral-8x7B (47B active params) achieves 74.25% accuracy, outperforming DeepSeek-R1-671B (64.07%) on Chinese medical exam benchmark.

## Executive Summary
This paper presents a comprehensive benchmark evaluating 27 state-of-the-art LLMs on Chinese medical examination questions across seven specialties and two professional levels. The dataset comprises 2,800 carefully curated multiple-choice questions, revealing that mixture-of-experts architectures like Mixtral-8x7B significantly outperform much larger dense models such as DeepSeek-R1-671B. Performance varies systematically across specialties, with cardiovascular and neurology showing higher accuracy than gastroenterology and nephrology. Notably, no consistent correlation was found between model size and performance, highlighting the effectiveness of sparse architectures for specialized medical knowledge tasks.

## Method Summary
The study evaluates LLMs on 2,800 Chinese medical multiple-choice questions across seven specialties (cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, respiratory) and two professional levels (attending vs senior physician). Models are assessed using standardized inference parameters (temperature=0.0, max_tokens=2048) with automated answer extraction and validation. Statistical analysis includes McNemar's test for pairwise comparisons, Kruskal-Wallis H-test for multi-group comparisons, and Spearman's ρ for scaling trends, with 95% bootstrap confidence intervals and Bonferroni correction.

## Key Results
- Mixtral-8x7B achieves 74.25% overall accuracy, outperforming DeepSeek-R1-671B (64.07%) despite having only 47B active parameters versus 671B
- No consistent correlation between model size and performance; MoE models achieve mean accuracy of 54.0% vs 36.0% for dense models (p < 0.001)
- Performance varies by specialty: cardiovascular/neurology (higher) vs gastroenterology/nephrology (lower), suggesting corpus representation effects
- Minimal performance gap between attending and senior physician levels (3.3% average), raising questions about genuine reasoning versus pattern matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture-of-Experts architectures outperform dense models on specialized medical question-answering despite having fewer active parameters.
- Mechanism: Sparse activation patterns selectively route inputs to domain-relevant expert networks, which may align with the modular organization of medical knowledge into specialties. Each specialty requires distinct knowledge bases and reasoning patterns; MoE routing can activate specialized subnetworks rather than diluting computation across all parameters.
- Core assumption: Expert routing in MoE models naturally partitions along semantic or domain boundaries present in medical training corpora.
- Evidence anchors:
  - [abstract] "we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures"
  - [Section IV.D] "MoE models achieved a mean accuracy of 54.0% (SD: 17.6%) compared to 36.0% (SD: 2.5%) for dense models, representing a significant 18.0% advantage (95% CI: 14.5-21.5%, p <0.001)"
  - [corpus] Related work on MoE for medical QA (Zhou et al., 2025) supports expert routing alignment with domain-specific knowledge, though evidence remains limited
- Break condition: If expert routing fails to specialize—e.g., all experts activated uniformly, or routing collapses to a single expert—the efficiency advantage disappears and performance may degrade below dense baselines.

### Mechanism 2
- Claim: Performance varies systematically across medical specialties due to differences in training corpus representation and domain complexity.
- Mechanism: Well-established fields (cardiovascular, neurology) likely have higher publication volumes and more standardized terminology in pretraining data, enabling better parameter-efficient retrieval. Conversely, gastroenterology and nephrology may be underrepresented or require more multi-step reasoning.
- Core assumption: Training corpus composition reflects real-world publication and discourse patterns across medical domains.
- Evidence anchors:
  - [abstract] "models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains"
  - [Section V.B] "The consistently higher performance on cardiovascular and neurology questions may reflect greater representation of these specialties in training corpora"
  - [corpus] Cross-lingual transfer studies (Wang et al., 2023; Zhu et al., 2023) show performance degradation when training data lacks domain coverage, supporting corpus representation as a factor
- Break condition: If a specialty is well-represented in training data but still underperforms, other factors (reasoning complexity, terminology ambiguity) may dominate; corpus representation alone is insufficient.

### Mechanism 3
- Claim: Difficulty level (attending vs. senior physician) shows minimal impact on model performance, suggesting models rely on pattern matching rather than expertise-calibrated reasoning.
- Mechanism: Senior-level questions may not require fundamentally different reasoning but rather deeper factual knowledge or case experience. LLMs pattern-match against training examples without hierarchical expertise representation, flattening performance across difficulty levels.
- Core assumption: The attending/senior distinction primarily reflects knowledge depth and case familiarity, not qualitatively different reasoning architectures.
- Evidence anchors:
  - [abstract] "minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization"
  - [Section IV.C] "minimal performance gap between attending and senior physician levels, averaging only 3.3% (95% CI: 2.8-3.8%)"
  - [corpus] No direct corpus evidence on difficulty stratification in medical LLM evaluation; related benchmarks (MedMCQA, MedQA) typically do not differentiate expertise levels
- Break condition: If senior-level questions require multi-step diagnostic chains that models cannot compress into pattern matching, performance should drop sharply; the flat curve suggests either weak difficulty calibration in the dataset or model shortcut exploitation.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) Routing**
  - Why needed here: The paper's central claim hinges on MoE efficiency; understanding sparse activation and expert selection is required to interpret why 47B active parameters outperform 671B dense models.
  - Quick check question: Given a 8-expert MoE with top-2 routing, what fraction of parameters are active per forward pass?

- Concept: **Benchmark Calibration via Bloom's Taxonomy**
  - Why needed here: The dataset design (35% recall, 40% application, 25% reasoning) structures cognitive demand; understanding this helps diagnose whether errors stem from knowledge gaps or reasoning failures.
  - Quick check question: If a model scores 80% on recall questions but 40% on analytical reasoning, what does this suggest about its capabilities?

- Concept: **Statistical Significance in Model Comparison (McNemar's Test)**
  - Why needed here: The paper uses McNemar's test for pairwise model comparisons; understanding paired classification evaluation prevents overinterpreting small accuracy differences.
  - Quick check question: Two models differ by 2% accuracy on 2,800 questions—is this necessarily significant, and what test would you use?

## Architecture Onboarding

- Component map:
  Input Layer → Routing Network (MoE-specific) → Expert Networks → Output Layer → Evaluation Pipeline

- Critical path: Question encoding → Expert routing → Specialized computation → Answer token → Post-processing validation. Failure at any stage (especially routing collapse or invalid output format) cascades to incorrect scoring.

- Design tradeoffs:
  - **MoE vs. Dense**: MoE offers better parameter efficiency but requires careful routing regularization; dense models are simpler but scale poorly.
  - **Temperature = 0.0**: Ensures reproducibility but eliminates diversity; may underrepresent model uncertainty.
  - **Single-choice format**: Enables automated evaluation but simplifies clinical reality (no open-ended diagnosis, no multi-turn reasoning).

- Failure signatures:
  - **Routing collapse**: Low expert utilization variance → degraded performance toward dense baseline
  - **Terminology confusion (23% of errors)**: Similar-sounding Chinese medical terms misclassified
  - **Guideline-specific errors (18%)**: China-specific protocols not captured in Western-centric training data
  - **Invalid output format**: Model generates explanation instead of single letter → marked incorrect

- First 3 experiments:
  1. **Routing analysis**: Log expert activation patterns per specialty; test whether cardiovascular vs. nephrology questions route to distinct expert clusters. Expect low inter-specialty routing overlap if MoE specialization is working.
  2. **Ablation by difficulty level**: Separate accuracy on attending vs. senior questions per specialty; if gap widens in specific domains (e.g., gastroenterology), indicates domain-specific reasoning bottlenecks.
  3. **Error stratification**: Classify incorrect responses by error type (terminology, guideline, multi-step reasoning); correlate with model size and architecture to identify systematic failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does model performance change when evaluating Chinese medical competencies using case-based reasoning, multi-modal inputs (text and imaging), or sequential decision-making tasks rather than single-choice questions?
- **Basis in paper:** [explicit] The authors state in Section V.D (Limitations) that "Future evaluations should incorporate more diverse assessment modalities... including case-based reasoning scenarios, multi-modal inputs combining text with medical imaging, and sequential decision-making tasks."
- **Why unresolved:** The current study relied exclusively on 2,800 single-choice questions, which may not fully capture the complexity of clinical workflows or diagnostic depth required in real-world practice.
- **What evidence would resolve it:** A follow-up benchmark using open-ended clinical vignettes and imaging data to compare against the multiple-choice results.

### Open Question 2
- **Question:** Does the integration of external medical knowledge bases or iterative reasoning tools (e.g., retrieval-augmented generation) reduce the error rates in underperforming specialties like gastroenterology?
- **Basis in paper:** [explicit] Section V.D notes that the static evaluation without tool use "may underestimate their potential capabilities," suggesting that "Integration with medical databases... and multi-turn interaction capabilities could substantially improve model performance."
- **Why unresolved:** Models were tested in isolation without access to reference materials or the ability to query guidelines, limiting the assessment to parametric knowledge.
- **What evidence would resolve it:** A comparative study measuring the performance delta between baseline models and tool-augmented agents on the same examination questions.

### Open Question 3
- **Question:** Is the superior performance on cardiovascular and neurology questions driven by greater representation in training corpora or by the inherent structure of knowledge in these fields?
- **Basis in paper:** [explicit] Section V.B (Discussion) suggests that high performance "may reflect greater representation of these specialties in training corpora," whereas lower performance in gastroenterology "suggests either underrepresentation... or inherent complexity."
- **Why unresolved:** The paper establishes a correlation between specialty and performance but lacks a causal analysis linking specific pretraining data distributions to these outcomes.
- **What evidence would resolve it:** A statistical analysis correlating the frequency of specialty-specific tokens in the training sets of the evaluated models against their benchmark accuracy.

### Open Question 4
- **Question:** Do high accuracy scores reflect genuine medical reasoning or pattern matching, given the minimal performance gap between attending and senior physician difficulty levels?
- **Basis in paper:** [explicit] Section V.A states that the minimal degradation between difficulty levels "raises questions about whether models truly understand medical concepts or primarily rely on pattern matching from training data."
- **Why unresolved:** Standard accuracy metrics do not distinguish between correct answers derived from logical deduction versus those retrieved via superficial feature matching.
- **What evidence would resolve it:** An adversarial evaluation where questions are rephrased to disrupt common linguistic patterns while preserving the underlying clinical logic.

## Limitations
- Dataset and prompt template transparency: The exact 2,800-question dataset and prompt wording are not publicly released, preventing exact replication.
- Single-answer evaluation: The benchmark measures multiple-choice selection rather than clinical reasoning depth, potentially overestimating real-world capabilities.
- Chinese medical domain specificity: Models trained on Western medical literature may struggle with China-specific guidelines and terminology.

## Confidence
- **Mixtral-8x7B vs. DeepSeek-R1-671B performance comparison (High):** Statistical testing is rigorous (McNemar's test, bootstrap CIs, Bonferroni correction), and the accuracy gap (74.25% vs 64.07%) is substantial with clear p-values.
- **No correlation between model size and performance (Medium):** While Spearman correlation analysis is presented, the limited sample of 27 models across heterogeneous architectures makes definitive claims about scaling laws premature.
- **Specialty-specific performance differences (High):** The pattern (cardiovascular/neurology > gastroenterology/nephrology) is consistent across models with appropriate statistical validation.
- **MoE architecture superiority mechanism (Low-Medium):** The theoretical justification for expert routing alignment is plausible but lacks direct empirical validation (no routing pattern analysis shown).

## Next Checks
1. **Routing Pattern Analysis:** Examine expert activation distributions across specialties for MoE models to confirm that cardiovascular questions route to distinct expert sets versus nephrology questions. Low inter-specialty routing overlap would validate the MoE specialization hypothesis.

2. **Difficulty Calibration Validation:** Re-analyze attending vs senior performance gaps after stratifying by question type (recall vs application vs reasoning). If gaps widen specifically for analytical reasoning questions, it would confirm that difficulty calibration is functioning as intended.

3. **Cross-Lingual Transfer Test:** Evaluate the same models on English medical QA benchmarks (MedQA-USMLE) using identical prompt templates. Significant performance degradation would highlight the Chinese medical knowledge specificity and corpus representation limitations.