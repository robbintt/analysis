---
ver: rpa2
title: Learning GUI Grounding with Spatial Reasoning from Visual Feedback
arxiv_id: '2509.21552'
source_url: https://arxiv.org/abs/2509.21552
tags:
- cursor
- grounding
- step
- target
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GUI-CURSOR, a method that reframes GUI grounding
  as an interactive search task rather than a static coordinate prediction. By controlling
  a virtual cursor on the GUI and providing visual feedback at each step, the model
  learns to align predicted coordinates with actual screen locations.
---

# Learning GUI Grounding with Spatial Reasoning from Visual Feedback

## Quick Facts
- arXiv ID: 2509.21552
- Source URL: https://arxiv.org/abs/2509.21552
- Reference count: 40
- Primary result: GUI-CURSOR achieves 93.9% accuracy on ScreenSpot-v2 and 56.5% on ScreenSpot-Pro

## Executive Summary
This paper introduces GUI-CURSOR, a novel approach to GUI grounding that reframes the task as an interactive search rather than static coordinate prediction. By controlling a virtual cursor and providing visual feedback at each step, the model learns to align predicted coordinates with actual screen locations through reinforcement learning with dense trajectory-based rewards. The method significantly outperforms previous approaches on two established benchmarks.

## Method Summary
GUI-CURSOR transforms GUI grounding into an interactive cursor control task where the model learns to navigate toward target elements using visual feedback. The approach employs reinforcement learning with a dense reward function that penalizes incorrect or inefficient cursor movements while rewarding progress toward the target. This interactive paradigm allows the model to iteratively refine its understanding of spatial relationships within the GUI.

## Key Results
- Achieves state-of-the-art accuracy of 93.9% on ScreenSpot-v2 benchmark
- Outperforms previous methods by 6.4% on ScreenSpot-Pro (56.5% accuracy)
- Solves 95% of tasks within two steps while adaptively using more steps for difficult cases

## Why This Works (Mechanism)
The method succeeds by leveraging interactive feedback loops that allow the model to refine spatial understanding through direct manipulation. The trajectory-based reward system provides rich supervision signals that capture not just end-state correctness but the quality of intermediate reasoning steps, enabling more efficient learning of spatial relationships.

## Foundational Learning
- **Reinforcement Learning**: Required for training through interaction; quick check: agent learns policy mapping states to actions
- **Spatial Reasoning**: Essential for understanding GUI layouts; quick check: model can localize elements relative to each other
- **Visual Grounding**: Core task of connecting language to visual elements; quick check: accurate mapping from instructions to screen locations
- **Trajectory-based Rewards**: Novel reward formulation; quick check: rewards depend on path quality, not just final position
- **Interactive Search**: Problem reformulation; quick check: iterative refinement through feedback

## Architecture Onboarding

Component Map: Input GUI + Instruction -> Vision Encoder -> Instruction Encoder -> Joint Representation -> Policy Network -> Cursor Actions -> Visual Feedback Loop

Critical Path: Visual features and instruction embeddings are combined into a joint representation that feeds into a policy network. The policy outputs cursor actions, which generate new visual states providing feedback for the next decision cycle.

Design Tradeoffs: The interactive approach trades computational efficiency (multiple steps per grounding) for improved accuracy and robustness compared to single-step prediction methods.

Failure Signatures: Performance degradation occurs with ambiguous references, cluttered interfaces, or when visual feedback is delayed or noisy.

First Experiments:
1. Validate baseline performance on ScreenSpot-v2 without interactive feedback
2. Test impact of trajectory-based reward design by comparing with standard sparse rewards
3. Evaluate step efficiency by measuring average steps needed across task difficulty levels

## Open Questions the Paper Calls Out
None

## Limitations
- Performance remains modest (56.5%) on challenging ScreenSpot-Pro benchmark
- Does not explicitly handle multi-target grounding scenarios
- Current design assumes unique, clearly identifiable targets on the GUI

## Confidence

High: Effectiveness of interactive search paradigm and trajectory-based rewards for improving grounding accuracy

Medium: Generalization to diverse and dynamic GUI environments based on controlled benchmark evaluation

Low: Claims about real-world deployment without testing on varied, unconstrained user interfaces

## Next Checks

1. Evaluate GUI-CURSOR on dynamically changing GUIs where target positions shift between interactions to test robustness against layout variability

2. Test the model's performance on instructions with ambiguous or multi-target references to assess handling of complex grounding scenarios

3. Conduct ablation studies isolating the impact of the trajectory-based reward and step-by-step cursor control to quantify their individual contributions to performance gains