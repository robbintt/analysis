---
ver: rpa2
title: Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate
  Speech
arxiv_id: '2506.03009'
source_url: https://arxiv.org/abs/2506.03009
tags:
- legal
- statutory
- language
- knowledge
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models (LLMs) can be\
  \ conditioned on different levels of legal abstraction to detect punishable hate\
  \ speech under German law. The authors propose conditioning approaches ranging from\
  \ fundamental rights (freedom of speech under Article 5 of the Basic Law) to statutory\
  \ law (\xA7 130 of the Criminal Code) down to case law definitions and examples."
---

# Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech

## Quick Facts
- **arXiv ID:** 2506.03009
- **Source URL:** https://arxiv.org/abs/2506.03009
- **Reference count:** 11
- **Primary result:** LLMs show significant performance gaps compared to legal experts when detecting punishable hate speech, even when conditioned on multiple levels of legal abstraction

## Executive Summary
This study investigates how large language models can be conditioned on different levels of legal abstraction to detect punishable hate speech under German law. The authors systematically test conditioning approaches ranging from fundamental rights (freedom of speech under Article 5 of the Basic Law) through statutory law (ยง 130 of the Criminal Code) down to specific case law definitions and examples. Experiments conducted on 150 manually created hate speech samples reveal that despite conditioning on multiple legal abstraction levels, LLMs still perform significantly worse than legal experts and exhibit inconsistent behavior, particularly when dealing with abstract legal concepts.

The research demonstrates that while models can identify target groups reasonably well, they struggle substantially with classifying target conduct. Notably, models conditioned on abstract legal knowledge often contradict themselves and hallucinate answers, whereas those using concrete legal knowledge show inconsistent performance patterns. The findings suggest that current LLMs lack the deep task understanding necessary for reliable legal assessment of hate speech, highlighting fundamental limitations in their ability to reason about complex legal concepts and apply them consistently to real-world scenarios.

## Method Summary
The study employs a systematic approach to test LLM performance on hate speech detection by conditioning models at different levels of legal abstraction. Researchers created 150 manually crafted hate speech samples covering various protected groups and violation types. They tested multiple state-of-the-art models including Command R+, Llama 3.1, Phi-3, and Qwen 2, conditioning each with different legal knowledge levels from fundamental rights to case law examples. The evaluation compares model performance against legal experts and analyzes how conditioning on abstract versus concrete legal knowledge affects classification accuracy and consistency.

## Key Results
- LLMs perform significantly worse than legal experts in hate speech classification tasks
- Models conditioned on abstract legal concepts show more hallucinations and self-contradictions compared to those using concrete legal knowledge
- While models can identify target groups reasonably well, they struggle substantially with classifying target conduct

## Why This Works (Mechanism)
The study reveals that LLMs struggle with hate speech detection due to their inability to properly reason about legal concepts and apply them consistently. When prompted with abstract legal knowledge, models tend to generate inconsistent responses and hallucinate legal reasoning, suggesting that higher-level legal abstractions exceed their current reasoning capabilities. Conversely, models using concrete legal examples show more stable behavior but still lack the nuanced understanding required for accurate hate speech classification. The mechanism appears to be that LLMs can process explicit, concrete examples more reliably than abstract legal principles, but neither approach provides sufficient legal reasoning capability for expert-level performance.

## Foundational Learning
**Hate Speech Legal Framework**: Understanding German hate speech laws (ยง 130 Criminal Code) and protected characteristics - needed to properly evaluate model performance against legal standards; quick check: verify model responses align with actual statutory requirements
**Legal Abstraction Levels**: Knowledge of how fundamental rights, statutory law, and case law relate hierarchically - needed to understand conditioning approaches; quick check: ensure prompts correctly represent each abstraction level
**LLM Hallucination Patterns**: Recognition of when models generate false or contradictory legal reasoning - needed to identify performance limitations; quick check: cross-reference model outputs with actual legal precedents

## Architecture Onboarding
**Component Map**: Prompt Design -> Model Selection -> Conditioning Application -> Output Generation -> Expert Evaluation
**Critical Path**: Legal Prompt Design -> Model Conditioning -> Hate Speech Classification -> Expert Performance Comparison
**Design Tradeoffs**: Abstract legal conditioning provides broader coverage but causes more hallucinations vs. concrete legal examples offer stability but limited generalization
**Failure Signatures**: Self-contradictory reasoning, hallucinated legal citations, inconsistent target group identification, inability to classify target conduct
**3 First Experiments**: 1) Test same models on simplified legal concepts to establish baseline reasoning capability; 2) Compare performance across different model families using identical legal prompts; 3) Evaluate model performance on real-world hate speech data vs. manually created samples

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 150 manually created samples may not capture full diversity of real-world hate speech scenarios
- GPT-4 used for generating example prompts and responses introduces potential contamination between training and evaluation data
- Lack of human performance baseline makes it difficult to establish fundamental capability limits of current LLMs

## Confidence
- **High confidence**: Models perform significantly worse than legal experts in hate speech classification tasks
- **Medium confidence**: Abstract legal conditioning leads to more hallucinations and contradictions compared to concrete legal knowledge
- **Medium confidence**: Models can identify target groups reasonably well but struggle with classifying target conduct

## Next Checks
1. Replicate experiments with a larger, more diverse dataset of real-world hate speech cases to validate findings beyond the manually created samples
2. Conduct ablation studies comparing different levels of legal abstraction conditioning across multiple model families to isolate model-specific effects
3. Implement human performance baselines using legal professionals to establish more accurate performance gaps and identify specific areas for model improvement