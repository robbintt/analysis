---
ver: rpa2
title: 'HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control'
arxiv_id: '2602.02268'
source_url: https://arxiv.org/abs/2602.02268
tags:
- graph
- attention
- transformer
- sparse
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HopFormer introduces a sparse graph Transformer that injects structure
  exclusively through head-specific n-hop masked attention, without positional encodings
  or architectural modifications. This design provides explicit, interpretable control
  over receptive fields and enables genuinely sparse attention with linear computational
  cost in mask sparsity.
---

# HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control

## Quick Facts
- arXiv ID: 2602.02268
- Source URL: https://arxiv.org/abs/2602.02268
- Authors: Sanggeon Yun; Raheeb Hassan; Ryozo Masukawa; Sungheon Jeong; Mohsen Imani
- Reference count: 40
- Introduces sparse graph Transformer with hop-controlled attention, no positional encodings or structural modifications

## Executive Summary
HopFormer presents a sparse graph Transformer that achieves competitive performance without relying on positional encodings or structural modifications. By injecting structure exclusively through head-specific n-hop masked attention, it offers explicit and interpretable control over receptive fields. The design enables genuinely sparse attention with linear computational cost relative to mask sparsity. Experiments demonstrate that localized attention is often sufficient for small-world graphs, challenging the assumption that dense global attention is always necessary.

## Method Summary
HopFormer implements sparse graph Transformers using head-specific n-hop masked attention, avoiding positional encodings and architectural changes. Structure is injected purely through attention masks that control hop distance. This yields linear computational cost with mask sparsity and interpretable receptive field control. The model is evaluated on both node- and graph-level tasks, comparing against graph Transformers with complex structural encodings.

## Key Results
- Sparse attention with explicit hop control yields competitive or superior performance compared to dense attention models
- Localized attention is sufficient for small-world graphs, while global attention offers diminishing returns on graphs with weaker small-world effects
- Linear computational cost with mask sparsity achieved without sacrificing accuracy

## Why This Works (Mechanism)
HopFormer's effectiveness stems from injecting structure via hop-specific attention masks rather than positional encodings or architectural modifications. This design allows for explicit control over receptive fields, making the attention mechanism both interpretable and efficient. The sparse attention strategy leverages the inherent structure of graphs, particularly small-world properties, to achieve high performance with reduced computational overhead.

## Foundational Learning
- **Sparse attention masks**: Needed to control receptive field and reduce computation; check by visualizing attention sparsity patterns
- **Hop distance in graphs**: Needed to define locality and structure; check by mapping node distances in example graphs
- **Small-world graph properties**: Needed to justify localized attention sufficiency; check by measuring clustering coefficient and average path length
- **Linear computational cost**: Needed to ensure scalability; check by profiling runtime with varying mask sparsity

## Architecture Onboarding

**Component Map**: Input Graph -> Hop-specific Attention Masks -> Sparse Multi-head Attention -> Feed-forward Network -> Output

**Critical Path**: Graph input is transformed via hop-specific attention masks into sparse multi-head attention, followed by standard feed-forward layers.

**Design Tradeoffs**: HopFormer trades explicit structural encodings for interpretability and sparsity control. This reduces model complexity but may limit performance on graphs with irregular structure.

**Failure Signatures**: Poor performance on graphs with low small-world effects or highly irregular structures; high memory usage if sparsity is not maintained.

**First Experiments**:
1. Evaluate hop window size impact on Cora citation network accuracy
2. Compare memory usage between sparse and dense attention on large graphs
3. Visualize attention sparsity patterns across different hop distances

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Claims about global attention being unnecessary are not extensively validated on graphs with highly irregular structures or low small-world effects
- Sparsity analysis is limited to specific datasets; generalization to other graph types is uncertain
- Memory and scalability on graphs with millions of nodes/edges not fully characterized

## Confidence
- **High** confidence in core claim: Sparse attention with explicit hop control yields competitive performance and interpretability
- **Medium** confidence in broader claim: Global attention is often unnecessary, due to limited graph type and sparsity regime testing
- **Medium** confidence in efficiency claims: Real-world scaling behavior not fully explored

## Next Checks
1. Evaluate HopFormer on large-scale, heterogeneous, or low-small-world-effect graphs to test sparse attention limits
2. Conduct ablation studies on different hop window sizes and sparsity levels to quantify accuracy, interpretability, and computational trade-offs
3. Benchmark memory and runtime scaling on graphs with millions of nodes/edges to validate linear cost claims in practice