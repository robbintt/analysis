---
ver: rpa2
title: Increasing the Thinking Budget is Not All You Need
arxiv_id: '2512.19585'
source_url: https://arxiv.org/abs/2512.19585
tags:
- reasoning
- thinking
- budget
- configurations
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how reasoning budgets (compute allocated to
  thinking tokens) interact with different reasoning strategies in Large Language
  Models. The authors compare configurations like vanilla, self-consistency, summary,
  and reflection across varying thinking budgets using three models (Qwen3-8B, Qwen3-4B,
  DeepSeek-R1-Distill-Llama-8B) on AIME24/AIME25 benchmarks.
---

# Increasing the Thinking Budget is Not All You Need

## Quick Facts
- arXiv ID: 2512.19585
- Source URL: https://arxiv.org/abs/2512.19585
- Reference count: 10
- Primary result: Simply increasing the thinking budget does not guarantee better performance; strategy choice matters more than raw compute allocation

## Executive Summary
This paper examines how reasoning budgets (compute allocated to thinking tokens) interact with different reasoning strategies in Large Language Models. The authors compare configurations like vanilla, self-consistency, summary, and reflection across varying thinking budgets using three models (Qwen3-8B, Qwen3-4B, DeepSeek-R1-Distill-Llama-8B) on AIME24/AIME25 benchmarks. They find that simply increasing the thinking budget does not guarantee better performance; instead, configurations like Summary (ensemble + consolidation) and self-reflection often outperform baseline approaches. Summary consistently yields the highest accuracy, while self-consistency struggles due to extraction issues. Judge-LLM setups also show promise by refining or regenerating answers based on correctness feedback. The results suggest that optimal compute allocation depends on strategy choice rather than just raw budget size.

## Method Summary
The study evaluates reasoning strategies by varying thinking budgets (compute allocated to thinking tokens) across three models: Qwen3-8B, Qwen3-4B, and DeepSeek-R1-Distill-Llama-8B. Configurations tested include vanilla (no reasoning), self-consistency (ensemble of reasoning paths with majority vote), summary (ensemble + consolidation), and reflection (self-critique and revision). The analysis uses AIME24 and AIME25 benchmarks to measure accuracy across different budget levels. The experiments systematically compare how each strategy performs as the thinking budget increases, with particular attention to answer extraction mechanisms and validation processes.

## Key Results
- Summary configuration consistently yields the highest accuracy across all budget levels
- Self-consistency struggles due to formatting and extraction issues during majority voting
- Judge-LLM setups show promise by refining or regenerating answers based on correctness feedback
- Performance improvements depend more on strategy choice than raw thinking budget size

## Why This Works (Mechanism)
The paper demonstrates that different reasoning strategies have varying effectiveness at different thinking budget levels due to their distinct mechanisms for handling uncertainty and self-correction. Summary configurations work well because they consolidate multiple reasoning paths into a coherent final answer, reducing noise from individual attempts. Self-consistency fails when extraction mechanisms cannot properly identify and aggregate individual reasoning outputs. Judge-LLM setups can improve answers through external validation but may prematurely accept incorrect answers if the model is over-confident in its own validation. The effectiveness of each strategy depends on how well it leverages additional compute for error correction versus generating more potential errors.

## Foundational Learning
- **Thinking budget allocation**: Why needed - determines how much compute is available for reasoning tokens; Quick check - measure token usage across different strategies
- **Reasoning strategy diversity**: Why needed - different approaches handle uncertainty differently; Quick check - compare accuracy variance across strategies
- **Answer extraction mechanisms**: Why needed - critical for ensemble methods like self-consistency; Quick check - test regex extraction reliability on different output formats
- **Self-reflection capabilities**: Why needed - enables error correction in reflection-based approaches; Quick check - measure improvement rate from reflection steps
- **External validation frameworks**: Why needed - provides objective correctness assessment; Quick check - compare judge-LLM accuracy versus ground truth
- **Ensemble consolidation methods**: Why needed - combines multiple reasoning paths effectively; Quick check - test different consolidation algorithms

## Architecture Onboarding

**Component map:** User Query -> Model + Thinking Budget -> Reasoning Strategy (Vanilla/Self-Consistency/Summary/Reflection) -> Answer Extraction -> Judge-LLM (optional) -> Final Answer

**Critical path:** Query Input → Model Processing → Reasoning Strategy Execution → Answer Generation → Output

**Design tradeoffs:** The main tradeoff is between computational cost (higher thinking budgets) and accuracy gains, which vary significantly by strategy. Summary strategies require more compute but provide better consolidation, while self-consistency is computationally efficient but suffers from extraction issues.

**Failure signatures:** Self-consistency fails when regex extraction cannot properly parse individual reasoning outputs; reflection strategies fail when the model is over-confident and accepts incorrect initial answers; vanilla approaches fail to leverage additional compute for complex problems.

**3 first experiments:** 1) Test different answer extraction methods (regex vs semantic parsing) for self-consistency, 2) Compare consolidation algorithms within the summary strategy, 3) Calibrate judge-LLM confidence thresholds for rejecting incorrect answers.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Do the benefits of summary-based configurations over vanilla scaling generalize to non-mathematical reasoning tasks and proprietary model families?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that experiments were restricted to a "limited set of datasets [AIME] and LLMs" (Qwen and DeepSeek) and that a larger sample is necessary to validate findings.
- Why unresolved: The observed superiority of the Summary strategy might be an artifact of the specific mathematical benchmarks or the architecture of the specific open-weights models tested.
- What evidence would resolve it: Replicating the budget-strategy interaction analysis on diverse domains (e.g., coding, logical entailment) and different model families (e.g., GPT, Llama).

### Open Question 2
- Question: Can robust answer extraction mechanisms enable Self-Consistency to outperform the Summary configuration?
- Basis in paper: [explicit] The authors note that Self-Consistency struggled to provide improvement, specifically citing that "formatting issues might be problematic in this step" during the regex-based extraction of individual results.
- Why unresolved: It is unclear if the poor performance of Self-Consistency is intrinsic to the strategy or a technical failure of the answer parsing implementation.
- What evidence would resolve it: A comparative study using semantic parsing or high-reliability extraction tools to ensure valid votes are counted in the Self-Consistency configuration.

### Open Question 3
- Question: How can Judge-LLM setups be calibrated to avoid prematurely validating incorrect initial answers?
- Basis in paper: [inferred] The authors found that in the Judge+Reflection configuration, "most of the times the first answer is assess as valid," which negates the potential benefits of the reflection step.
- Why unresolved: The model appears over-confident in its own validation ("assessed as valid"), preventing the self-correction mechanism from triggering.
- What evidence would resolve it: Testing calibration techniques or external verifier modules that lower the threshold for rejecting an answer in the Judge loop.

## Limitations
- Limited to three reasoning strategies across only two benchmark datasets (AIME24/AIME25)
- Experimental scope restricted to specific open-weight models (Qwen and DeepSeek families)
- Does not address computational cost-benefit tradeoffs or performance across diverse reasoning domains
- Self-consistency performance issues not thoroughly investigated beyond noting extraction problems

## Confidence
- Core claim (strategy choice > raw budget): Medium
- Summary strategy superiority: High (within tested conditions)
- Judge-LLM promise: Low (limited implementation details)

## Next Checks
1. Test identified strategies across diverse reasoning domains (mathematical, logical, commonsense) to assess generalizability
2. Conduct ablation studies to isolate which components of the Summary strategy drive its superior performance
3. Perform cost-benefit analysis comparing computational overhead versus accuracy improvements across different budget ranges and strategy combinations