---
ver: rpa2
title: Residualized Similarity for Faithfully Explainable Authorship Verification
arxiv_id: '2510.05362'
source_url: https://arxiv.org/abs/2510.05362
tags:
- interpretable
- system
- features
- similarity
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Residualized Similarity (RS), a method that
  combines interpretable linguistic features with neural models for authorship verification
  while maintaining explainability. RS predicts the error in similarity scores from
  interpretable systems (like Gram2vec) using a neural model, allowing high performance
  with traceable explanations.
---

# Residualized Similarity for Faithfully Explainable Authorship Verification

## Quick Facts
- arXiv ID: 2510.05362
- Source URL: https://arxiv.org/abs/2510.05362
- Authors: Peter Zeng; Pegah Alipoormolabashi; Jihu Mun; Gourab Dey; Nikita Soni; Niranjan Balasubramanian; Owen Rambow; H. Schwartz
- Reference count: 24
- Primary result: RS matches state-of-the-art neural model performance while providing interpretable confidence scores

## Executive Summary
This paper introduces Residualized Similarity (RS), a method that combines interpretable linguistic features with neural models for authorship verification while maintaining explainability. RS predicts the error in similarity scores from interpretable systems (like Gram2vec) using a neural model, allowing high performance with traceable explanations. Evaluated across four datasets, RS matches state-of-the-art neural model performance while providing interpretable confidence scores.

## Method Summary
RS combines interpretable Gram2vec features with neural embeddings to predict authorship verification while maintaining explainability. The method computes cosine similarity between standardized interpretable feature vectors, then trains a neural model to predict the residual (error) between this similarity score and the ground truth label. The final prediction is the sum of the interpretable similarity and predicted residual. An attention layer over both interpretable features and neural embeddings enables the model to learn optimal weighting during training while preserving interpretability at inference.

## Key Results
- RS achieves AUC improvements up to 19 points over interpretable baselines while preserving explainability
- Attention-based RS outperforms appended variants across all datasets (Fanfiction: 0.87 vs 0.77 AUC)
- INTCONF values remain above 0.5 in most cases, indicating faithful explanations
- RS maintains robust performance across multiple neural architectures and interpretable feature systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A neural network can predict the error (residual) in an interpretable system's similarity score, and combining these improves accuracy while maintaining explainability.
- Mechanism: The interpretable system (Gram2vec) computes cosine similarity between standardized feature vectors for two documents. The neural network is trained via MSE loss to predict the difference between this score and the ground truth label (±1). At inference, the final score is the sum: interpretable similarity + predicted residual.
- Core assumption: The interpretable system's errors are learnable patterns that a neural model can capture without completely overriding the interpretable signal.
- Evidence anchors:
  - [abstract]: "RS predicts the error in similarity scores from interpretable systems (like Gram2vec) using a neural model, allowing high performance with traceable explanations."
  - [section 3.2]: "RS is trained to predict a residual r(i) = y(i) - sim(f(d1), f(d2))... The final similarity score is a simple summation of the cosine similarity from our interpretable system and the predicted residual."
  - [corpus]: Paper 31963 (LM²otifs) similarly combines interpretable motifs with neural detection for explainable machine-generated text identification.
- Break condition: If predicted residuals consistently have large magnitude (approaching ±2), INTCONF approaches 0 and the system degrades to a black-box neural model.

### Mechanism 2
- Claim: An attention layer over both interpretable features and neural embeddings enables the model to learn optimal weighting for residual prediction during training.
- Mechanism: During training only, an attention layer receives four embeddings (two interpretable vectors, two neural embeddings) and learns how much to weight each signal when predicting residuals. At inference, only the predicted residual is used—no attention computation needed.
- Core assumption: Both interpretable features and neural embeddings contain complementary information that improves residual prediction accuracy beyond either alone.
- Evidence anchors:
  - [section 3.2, Figure 2]: "Next, an attention layer is placed over all four embeddings, for RS to learn how much to weigh the interpretable features and the neural embeddings when making the residual prediction."
  - [section 5.2, Table 2]: RS with attention substantially outperforms both Only-Neural and Appended variants across all datasets (Fanfiction: 0.87 vs 0.74/0.77).
  - [corpus]: Paper 83785 aligns chemical concepts with predictions via language models using similar attention-based integration strategies.
- Break condition: If attention weights concentrate entirely on neural embeddings, the model ignores interpretable features during training, potentially learning shortcuts that don't generalize.

### Mechanism 3
- Claim: The relative contribution of the interpretable system to the final prediction can be quantified, providing a per-prediction measure of explainability.
- Mechanism: INTCONF measures the proportion of the final decision attributable to the interpretable system. For same-author predictions: (1 + sim_g2v) / (1 + sim_g2v + |residual|). For different-author: (1 - sim_g2v) / (1 - sim_g2v + |residual|). Values range from 0 (neural-dominant) to 1 (interpretable-dominant).
- Core assumption: When residuals are small, the interpretable features provide faithful explanations; larger residuals proportionally reduce explainability.
- Evidence anchors:
  - [section 3.3]: "INTCONF represents the relative contribution of Gram2vec to the overall similarity score, composed of the Gram2vec-derived score and the residual predicted by RS."
  - [section 5.3, Figure 3]: "We observe in most cases, we have an INTCONF of greater than 0.5, and a mean of .65."
  - [corpus]: Weak corpus evidence—no direct analogs to this specific interpretability quantification metric found in related papers.
- Break condition: If INTCONF approaches 0 for most predictions, the system provides no faithful explanations despite using interpretable features.

## Foundational Learning

- Concept: Cosine Similarity for Vector Comparison
  - Why needed here: RS fundamentally relies on cosine similarity between document vectors. The interpretable system's score, ground truth labels (±1), and residual calculations all depend on understanding this normalized dot product measure.
  - Quick check question: Given vectors [1,0] and [0,1], what is their cosine similarity? (Answer: 0—they're orthogonal)

- Concept: Residual Learning in Regression
  - Why needed here: The core innovation is predicting residuals (corrections) rather than direct outputs. Understanding that residuals represent prediction error (actual - predicted) and that learning corrections can be more stable than learning full mappings is essential.
  - Quick check question: If ground truth is +1 and the interpretable similarity is 0.3, what residual should the neural model predict? (Answer: 0.7)

- Concept: Siamese Networks with Contrastive Loss
  - Why needed here: The neural baselines and RS's neural component use Siamese architectures where two documents pass through shared encoder weights, trained to push same-author pairs together and different-author pairs apart in embedding space.
  - Quick check question: Why must both document encoders in a Siamese network share weights? (Answer: To produce comparable embeddings in the same feature space)

## Architecture Onboarding

- Component map:
Documents (d1, d2)
    │
    ├─► Interpretable Branch:
    │   Gram2vec → Standardize → Cosine Sim → s
    │
    ├─► Neural Branch [Training]:
    │   Encoder (LUAR/etc.) → Attention(4 emb) → Regression Head → r_pred
    │
    └─► Inference: final_score = s + r_pred → threshold comparison

- Critical path:
  1. Preprocess documents (≥20 words for Reddit/Amazon/Fanfiction; ≥100 chars for Pikabu)
  2. Extract interpretable features via Gram2vec, z-score per dataset
  3. Compute interpretable cosine similarity `s`
  4. At inference: predict residual `r_pred` via trained neural model
  5. Combine: `final_score = s + r_pred`
  6. Apply threshold for same/different author decision

- Design tradeoffs:
  - Attention vs. concatenation: Attention (Table 2) outperforms appending (Fanfiction: 0.87 vs 0.77) but adds training complexity.
  - Interpretable system choice: Gram2vec vs. ELFEN vs. combined (Table 3)—combined slightly improves baselines but RS performance is similar across all three.
  - LoRA vs. full fine-tuning: LoRA reduces memory and unexpectedly improved performance (Appendix A), possibly due to regularization effects.

- Failure signatures:
  - High-magnitude residuals (|r_pred| > 1.5): INTCONF drops toward 0; system becomes effectively black-box.
  - Cross-domain degradation (Table 5): Reddit-trained model on Fanfiction drops to 0.71 AUC vs. 0.87 in-domain.
  - Feature standardization errors: Z-scoring must use dataset-level statistics; per-pair standardization breaks the similarity scale.

- First 3 experiments:
  1. Replicate LUAR + Gram2vec RS on Reddit (target: AUC ~0.80). Use 50K/10K/10K splits, AdamW (lr=5e-5), 10 epochs with early stopping, LoRA fine-tuning per Appendix A.
  2. Ablate attention vs. appended architecture on same Reddit data (compare Table 2 variants) to validate mechanism necessity for your infrastructure.
  3. Analyze INTCONF distribution on test set (aim for mean >0.5 per Figure 3) to quantify accuracy-explainability trade-offs before deployment.

## Open Questions the Paper Calls Out
The paper explicitly states in the Limitations section that they leave "human-computer interface (HCI) issues" regarding how to choose which features to present to the user to future work.

## Limitations
- Missing details on attention layer architecture and regression head configuration prevent exact replication
- Cross-domain generalization degrades significantly (AUC drops from 0.87 to 0.71 when moving from Reddit to Fanfiction)
- The theoretical claim that RS maintains faithful explainability when residuals are small lacks systematic edge case analysis

## Confidence

- **High Confidence:** The fundamental mechanism of residual prediction and its mathematical formulation are well-specified and reproducible. The AUC improvements over interpretable baselines (19-point gains) are clearly demonstrated across four datasets.
- **Medium Confidence:** The attention-based architecture shows superior performance, but the exact implementation details needed for faithful reproduction are missing. The interpretability quantification is conceptually sound but lacks validation of faithfulness in edge cases.
- **Low Confidence:** Cross-domain generalization claims are supported by Table 5 results, but the paper doesn't analyze why Reddit-trained models fail on Fanfiction or provide mitigation strategies.

## Next Checks
1. **Attention Architecture Replication:** Implement the four-embedding attention mechanism and compare performance against the appended variant on Reddit data to validate whether attention is essential for the reported gains.
2. **INTCONF Edge Case Analysis:** Systematically examine test predictions where |residual| > 1.0 and INTCONF < 0.3 to determine if Gram2vec features still provide meaningful explanations or if the system becomes effectively black-box.
3. **Cross-Domain Transfer Study:** Train on Reddit, evaluate on Fanfiction (and vice versa), then analyze feature importance differences to understand domain-specific patterns that cause performance degradation.