---
ver: rpa2
title: 'Staying on the Manifold: Geometry-Aware Noise Injection'
arxiv_id: '2509.20201'
source_url: https://arxiv.org/abs/2509.20201
tags:
- noise
- manifold
- space
- data
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes geometry-aware noise injection strategies
  for data augmentation that respect the intrinsic structure of data manifolds during
  neural network training. The authors develop three approaches: projecting ambient
  Gaussian noise onto tangent spaces, generating geodesic noise via the exponential
  map, and simulating Brownian motion along manifolds.'
---

# Staying on the Manifold: Geometry-Aware Noise Injection

## Quick Facts
- **arXiv ID:** 2509.20201
- **Source URL:** https://arxiv.org/abs/2509.20201
- **Reference count:** 40
- **Key outcome:** Geometry-aware noise injection strategies that respect intrinsic data manifold structure improve generalization and robustness compared to standard ambient space noise.

## Executive Summary
This paper proposes three geometry-aware noise injection strategies for data augmentation that respect the intrinsic structure of data manifolds during neural network training. The authors develop tangent space projection, geodesic noise via exponential map, and intrinsic Brownian motion approaches. They demonstrate these strategies improve generalization and robustness compared to standard ambient space noise, particularly on highly curved manifolds like Swiss rolls. For MNIST, geometry-aware Brownian motion in the latent space outperforms ambient noise and even surpasses classifiers trained on original images when data is scarce.

## Method Summary
The authors develop three noise injection strategies that respect manifold geometry: (1) projecting ambient Gaussian noise onto tangent spaces, (2) generating geodesic noise via the exponential map, and (3) simulating intrinsic Brownian motion along manifolds. These methods are evaluated on synthetic parameterized manifolds (Swiss roll, spheroids, tori, biconcave disc) and MNIST digits. For MNIST, they train a convolutional autoencoder to learn a latent space representation, then apply Brownian motion in this space before decoding. The approach extends to deformed manifolds using neural ODE flows and learned manifolds via autoencoders.

## Key Results
- Geometry-aware Brownian motion achieves best/competitive results on Swiss roll (0.46× baseline error) and DeformedSphere (0.92× baseline error)
- On MNIST subsamples, Brownian motion in latent space outperforms ambient noise and surpasses classifiers trained on original images
- Tangent space projection regularizes only the tangential component of gradients, providing targeted regularization
- Geodesic noise shows robustness to noise intensity on highly curved manifolds

## Why This Works (Mechanism)

### Mechanism 1: Tangent Space Projection as Targeted Regularization
Projecting ambient Gaussian noise onto tangent space constrains only tangential component of learned function's gradient. Ambient Gaussian noise regularizes full gradient, but projecting to tangent space (via P = I - Σn_i n_i^T) penalizes only on-manifold variations while leaving normal-direction regularization implicit. This works when manifold is locally flat within noise scale σ.

### Mechanism 2: Exponential Map Preserves Manifold Validity
Using exponential map to project tangent noise onto manifold ensures augmented samples remain valid manifold points even at larger noise scales. Sample velocity in tangent space, then compute x̃ = Exp_x(ϵ^⊤) = γ_{ϵ^⊤/‖ϵ^⊤‖}(‖ϵ^⊤‖), where γ follows geodesic ODE. This wraps flat tangent space onto curved manifold geometry.

### Mechanism 3: Intrinsic Brownian Motion for Chart-Independent Augmentation
Brownian motion generated by Laplace-Beltrami operator provides intrinsic, coordinate-free noise respecting manifold geometry. The stochastic process du_k(t) incorporates metric derivatives in drift and uses g^{-1} in noise term, ensuring chart independence.

## Foundational Learning

- **Concept: Riemannian manifolds and tangent spaces**
  - Why needed here: Understanding projection matrices P = I - Σn_i n_i^T that decompose vectors into tangential/normal components
  - Quick check: Given a point on a 2D surface embedded in R³, can you identify the basis vectors spanning its tangent plane?

- **Concept: Geodesics and Christoffel symbols**
  - Why needed here: Geodesic noise requires solving ¨α_k(t) = -Σ Γ^k_{ij} ˙α_i ˙α_j to compute Exponential map
  - Quick check: What is the relationship between geodesics and shortest paths on a curved surface?

- **Concept: Pullback metrics and induced geometry**
  - Why needed here: Deformed manifolds require computing ǧ = J_u(T)^⊤ J_u(T) via automatic differentiation
  - Quick check: If X: R^d → R^D parameterizes a manifold, how does the Jacobian J_X relate to the induced metric?

## Architecture Onboarding

- **Component map:** Input x ∈ M → [Compute J_x, g_x, Γ^k_{ij}] → [Sample noise strategy] → Augmented x̃
  - Tangent: ϵ^⊤ = Pϵ, add directly
  - Geodesic: Solve geodesic ODE from ϵ^⊤
  - Brownian: Integrate SDE with drift + noise
  - For learned manifolds: x → Encoder → z ∈ latent space → [Brownian motion in Z] → Decoder → x̃

- **Critical path:** Implement metric computation J^T X J_X, implement Christoffel symbol computation for geodesic ODE, implement projection matrix P for tangent noise, for Brownian motion implement Eq. 17 with Euler-Maruyama integration

- **Design tradeoffs:** Tangent noise is fastest but loses validity at larger σ on curved manifolds; geodesic noise is most accurate but computationally expensive; Brownian motion offers good balance with stochastic nature tolerating coarser discretization; latent space approach enables high-dimensional applications but depends on autoencoder quality

- **Failure signatures:** Off-manifold artifacts in augmented images, label boundary crossing (digit morphing), performance degradation with increasing σ for ambient noise, no improvement on simple/near-Euclidean manifolds

- **First 3 experiments:** Replicate Swiss roll experiment comparing ambient vs. Brownian noise across σ²; train autoencoder on MNIST subsample implementing latent-space Brownian motion; deform a sphere using neural ODE flow and verify metric pullback computation

## Open Questions the Paper Calls Out

### Open Question 1
Does the relative difference between ambient space dimension and intrinsic manifold dimension amplify performance gap between geometry-aware and standard Gaussian noise? The authors expect high ambient-to-manifold dimension ratios to make Gaussian noise more "perpendicular" to the manifold, but this wasn't empirically verified across varying dimensions.

### Open Question 2
Can using more expressive manifold approximators like flow matching enable geometry-aware noise to outperform baselines on full datasets rather than subsampled ones? The current autoencoder approximation creates a performance gap the noise injection cannot fully close when data is abundant.

### Open Question 3
How can intrinsic Brownian motion be modified to remain within semantic class boundaries to prevent label noise? The current stochastic process follows data manifold geometry but ignores decision boundary geometry, leading to corrupted labels when noise crosses between class clusters.

## Limitations
- Computational cost scales poorly with ambient dimensionality due to local Riemannian geometry computation at every training step
- Performance depends heavily on accuracy of manifold parameterizations or learned representations
- Brownian motion and geodesic paths can cross decision boundaries between classes, introducing label noise

## Confidence
- **High confidence:** Mathematical framework for Riemannian noise injection is sound with clear differential geometry derivations
- **Medium confidence:** MNIST latent-space results show promise but 0.6% accuracy gap suggests autoencoder reconstruction limits effectiveness
- **Low confidence:** Effectiveness on highly curved manifolds remains partially demonstrated; tangent space approach underperforms on Swiss roll

## Next Checks
1. Implement geometry-aware noise injection on CIFAR-10 using pretrained classifier feature space and compare augmentation performance against ambient noise
2. Quantify fraction of MNIST latent-space Brownian motion samples crossing digit class boundaries using nearest-neighbor classification
3. Systematically vary autoencoder latent dimension and measure correlation between reconstruction MSE and geometry-aware noise effectiveness on MNIST classification