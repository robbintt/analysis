---
ver: rpa2
title: A Training-free LLM Framework with Interaction between Contextually Related
  Subtasks in Solving Complex Tasks
arxiv_id: '2503.23053'
source_url: https://arxiv.org/abs/2503.23053
tags:
- subtask
- subtasks
- interaction
- execution
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IFD, a training-free framework that enables
  information exchange between contextually related subtasks through interaction requests.
  The core method uses a subtask trajectory memory and interaction handler to allow
  subtasks to query completed subtasks during execution.
---

# A Training-free LLM Framework with Interaction between Contextually Related Subtasks in Solving Complex Tasks

## Quick Facts
- arXiv ID: 2503.23053
- Source URL: https://arxiv.org/abs/2503.23053
- Authors: Hongjia Liu; Jinlong Li
- Reference count: 6
- Primary result: Training-free framework IFD achieves 48% success rate on WebShop and 52% single-pass success rate on HotpotQA by enabling information exchange between contextually related subtasks

## Executive Summary
This paper introduces IFD (Interaction Framework for Decomposition), a training-free method that enables information exchange between contextually related subtasks during complex task solving. The framework extends standard task decomposition with interaction requests, allowing executing subtasks to query completed subtasks for specific information. A new `finish[overview]` action generates concise descriptions of subtask outcomes, replacing goal-based representations to improve interaction accuracy. IFD demonstrates state-of-the-art performance on WebShop (48% success rate, 76.4% average reward) and HotpotQA (52% success rate with GPT-4) while reducing failed interactions by 76% compared to goal-based approaches.

## Method Summary
IFD is a training-free framework consisting of four components: (1) a dynamic planner that generates subtasks based on previous subtasks and their overviews; (2) a ReAct-style executor with extended action space including `interact[d, r]` and `finish[overview]`; (3) subtask trajectory memory storing execution traces and overviews; (4) an interaction handler that resumes completed subtasks to process requests. The framework uses GPT-3.5-turbo for planning and execution, with GPT-4 for HotpotQA evaluation. Subtasks can query completed subtasks via interaction requests, which trigger resumption of stored trajectories to generate responses. The method is evaluated on WebShop (100 test examples) and HotpotQA (100 test examples) benchmarks.

## Key Results
- IFD achieves 48% success rate on WebShop benchmark (76.4% average reward), outperforming baselines including Reflexion (33.2%) and ReAct (28.8%)
- On HotpotQA, IFD achieves 52% success rate with single-pass inference using GPT-4, compared to Reflexion's 52% with 4 retries
- Overview-based representation reduces failed interaction rate from 14.4% to 3.4% compared to goal-based approaches
- IFD reduces total interactions by 50.6% while maintaining higher accuracy

## Why This Works (Mechanism)

### Mechanism 1
Subtask interaction via requests reduces information loss and redundant operations in contextually related subtasks. An `interact[d, r]` action allows executing subtasks to query completed subtask `d` with request `r`. The target subtask resumes from stored trajectory, processes the request through additional execution steps, and returns a response as an observation. This compensates for static decomposition limitations when subtasks share contextual dependencies that cannot be fully anticipated during planning.

### Mechanism 2
Overview-based subtask representation enables more accurate interaction targeting than goal-based representation. Upon completion, each subtask generates an `overview` via `finish[overview]` action—a concise description of execution process and outcomes. Subsequent subtasks use overviews (not goals) to determine which completed subtask to query and what to request. This works because goals represent prior intentions that may diverge from actual outcomes, while overviews capture ground-truth execution state.

### Mechanism 3
Trajectory memory enables stateless resumption of completed subtasks for handling interaction requests. Memory `M = {(i, Γi, Oi)}` stores trajectory `Γi` and overview `Oi` per subtask. Upon receiving request `r`, interaction handler retrieves `Γd`, performs additional execution `Γ'd = LLMi(r, Γd)`, and returns response via `finish[res]`. This assumes stored trajectories contain sufficient context to resume reasoning without full environment state reconstruction.

## Foundational Learning

- **Concept: ReAct-style execution (interleaved reasoning, actions, observations)**
  - Why needed here: IFD extends ReAct's action space with `interact` and `finish`; understanding the base pattern is prerequisite.
  - Quick check question: Can you trace how a ReAct executor decides between `think`, `act`, and processing `observe` steps?

- **Concept: Dynamic task decomposition**
  - Why needed here: IFD's planner generates subtasks based on completed subtasks and overviews rather than static pre-planning.
  - Quick check question: How does dynamic planning differ from static decomposition when a subtask fails?

- **Concept: In-context learning for action space extension**
  - Why needed here: New actions (`interact`, `finish`) are introduced via prompt engineering without model fine-tuning.
  - Quick check question: If you add a new action to an LLM agent via prompt, what determines whether the model will use it correctly?

## Architecture Onboarding

- **Component map:**
  - Planner → Dynamic subtask generation using `[Ti]`, `[Oi]` context
  - Executor → ReAct-style loop with extended action space `A ∪ {interact, finish}`
  - Subtask trajectory memory M → Dictionary `{id: (trajectory, overview)}`
  - Interaction handler → LLM-based request processor that resumes target subtask

- **Critical path:**
  1. Planner generates subtask Ti+1 from main task T + previous subtasks/overviews
  2. Executor begins ReAct loop, receives overviews `[Oi]` as context
  3. If executor generates `interact(d, r)`, handler retrieves `Γd` from M, executes `LLMi(r, Γd)`, returns response
  4. Executor continues until `finish[overview]`, stores `(Γi+1, Oi+1)` in M
  5. Loop to step 1 until main task complete

- **Design tradeoffs:**
  - Memory footprint vs. interaction capability: Storing full trajectories enables complex resumption but scales with task length
  - Overview conciseness vs. completeness: Overly brief overviews may omit interaction-relevant information
  - Single-pass vs. retry methods: IFD achieves 52% on HotpotQA single-pass vs. Reflexion's 52% with 4 retries—trading compute for simplicity

- **Failure signatures:**
  - High failed interaction rate (>10%) → Overviews may be too terse or goals being used instead
  - Redundant subtask execution → Interaction not triggering; check `interact` action prompt formatting
  - Stale responses → Environment state changed; trajectory memory insufficient

- **First 3 experiments:**
  1. Replicate WebShop baseline comparison (ReAct vs. IFD) on 20 examples to validate interaction mechanism
  2. Ablate overview vs. goal on HotpotQA subset (50 examples); verify failed interaction rate difference
  3. Measure interaction overhead: count average interactions per task, trajectory memory size, and latency impact

## Open Questions the Paper Calls Out

### Open Question 1
Can structured representations (e.g., JSON, code) for the subtask overview improve performance compared to natural language descriptions? The conclusion states future work involves "investigating better structured representations for the overview component." Ablation studies comparing success rate and context length efficiency of IFD using natural language overviews versus structured data schemas on multi-hop reasoning benchmarks would resolve this.

### Open Question 2
Can fine-tuning LLMs specifically for the interaction mechanism improve the framework's efficiency and success rates compared to the current training-free approach? The conclusion identifies "integrating interaction mechanisms into LLMs through fine-tuning" as a direction for future work. Training a model on interaction trajectories and comparing its ability to handle complex requests against the training-free GPT-4 baseline would resolve this.

### Open Question 3
How can the framework automatically determine whether a task requires subtask interaction to avoid unnecessary computational overhead? The discussion notes IFD was not evaluated on ALFWorld because "interaction between subtasks is unnecessary" there, implying the method is currently applied broadly rather than selectively. A method or metric that predicts the "contextual relatedness" of subtasks and a comparison of IFD's performance on tasks flagged as "high relatedness" vs. "low relatedness" would resolve this.

## Limitations
- Effectiveness is constrained by subtask independence conditions—the core interaction mechanism provides no benefit when subtasks are truly independent
- Trajectory-based resumption may produce stale responses when environment state changes between subtask completion and interaction requests
- Paper does not address trajectory memory pruning or staleness detection mechanisms

## Confidence
- High confidence: WebShop performance claims (48% success rate, 76.4% average reward) - directly measured on benchmark with clear metrics
- Medium confidence: HotpotQA single-pass results (52% success rate) - impressive but lack comparison to retry-based baselines
- Medium confidence: Overview vs. goal representation claims - well-supported by Table 4 statistics but limited to one benchmark

## Next Checks
1. Test interaction mechanism on benchmark with mixed dependency structures (partially dependent and independent subtasks) to validate break condition identification and measure overhead in irrelevant interaction scenarios
2. Implement environment state change simulation to measure trajectory-based resumption accuracy degradation and identify staleness thresholds
3. Measure interaction latency and memory scaling by instrumenting framework on progressively longer tasks (100, 500, 1000 steps) to validate practical deployment constraints