---
ver: rpa2
title: Tiny Reward Models
arxiv_id: '2507.09973'
source_url: https://arxiv.org/abs/2507.09973
tags:
- https
- arxiv
- reward
- language
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tiny Reward Models introduces small, bidirectional masked language
  models (MLMs) as efficient alternatives to large decoder-based reward models. By
  combining FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer
  freezing, TinyRM achieves competitive performance on reasoning and safety preference
  modeling tasks while using up to 175 times fewer parameters.
---

# Tiny Reward Models

## Quick Facts
- arXiv ID: 2507.09973
- Source URL: https://arxiv.org/abs/2507.09973
- Reference count: 18
- Primary result: Small bidirectional MLMs (as few as 400M parameters) rival capabilities of models over 175 times larger on reasoning and safety preference modeling tasks

## Executive Summary
Tiny Reward Models introduces small, bidirectional masked language models (MLMs) as efficient alternatives to large decoder-based reward models. By combining FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing, TinyRM achieves competitive performance on reasoning and safety preference modeling tasks while using up to 175 times fewer parameters. The approach demonstrates that lightweight MLMs can serve as effective reward models, with parameter-efficient tuning methods like DoRA proving particularly effective for reasoning tasks. While challenges remain in conversational preference modeling, the results highlight the promise of bidirectional architectures for creating scalable, deployable preference learning systems with significantly reduced inference costs.

## Method Summary
TinyRM uses small bidirectional MLMs (ModernBERT-Base or Large) trained with FLAN-style cloze prompting, where preference judgments are cast as masked language modeling tasks. For reasoning tasks, the model uses DoRA (rank 128) with bottom layers frozen; for chat and safety tasks, standard fine-tuning is used with different layer freezing configurations. The input format presents both chosen and rejected responses simultaneously in a structured prompt, and the model predicts which response is preferred by predicting a specific token at the [MASK] position. Training uses Decoupled AdamW optimizer with weight decay of 1e-5, batch size of 256, and 1 epoch with linear decay.

## Key Results
- TinyRM with 400M parameters achieves competitive performance with models over 175 times larger on reasoning and safety tasks
- DoRA provides significant performance gains over full-rank fine-tuning for reasoning tasks
- FLAN-style cloze prompting significantly outperforms alternatives for encoder-based models
- TinyMLMs trained on Skywork reasoning and safety tasks outperform models 175x their size on those domains
- Struggles with conversational (Chat) tasks where large LLMs typically excel

## Why This Works (Mechanism)

### Mechanism 1: Bidirectional Attention for Comparative Understanding
- Claim: MLMs with bidirectional attention can more efficiently evaluate preferences by simultaneously attending to the full context of a prompt and both response options.
- Mechanism: Encoder-based models access contextual information in both directions, resulting in richer internal representations compared to left-to-right processing of decoder models.
- Core assumption: Preference modeling is fundamentally an NLU task suited to bidirectional architectures.
- Evidence anchors: Abstract showing performance rivaling 175x larger models; section 2.2 explaining bidirectional advantage; weak corpus support.
- Break condition: Performance degrades in open-ended conversational tasks requiring implicit context generation.

### Mechanism 2: Eliciting Latent Capabilities via Parameter-Efficient Tuning
- Claim: Reasoning abilities can be more effectively elicited through low-rank updates (DoRA) combined with layer freezing than through full fine-tuning.
- Mechanism: DoRA decomposes weights into magnitude and direction, applying low-rank updates only to direction while freezing lower layers preserves general pre-trained representations.
- Core assumption: High-level reasoning capabilities are latent within pre-trained models and can be surfaced with targeted updates.
- Evidence anchors: Abstract mentioning DoRA and layer freezing; section 3.1 showing DoRA's advantage for reasoning; section 4 speculating on reasoning ability beyond parameter count.
- Break condition: Fails when adapting to domains requiring learning new factual knowledge rather than re-weighting existing capabilities.

### Mechanism 3: Instruction-Formatted Cloze Task
- Claim: Rephrasing preference classification as instruction-following with masked token prediction is more effective than classification heads.
- Mechanism: Models are fine-tuned on structured prompts to predict semantically meaningful tokens rather than using randomly initialized classification layers.
- Core assumption: Pre-trained semantic understanding of concepts like "safer" can be mapped to preference labels via existing vocabulary.
- Evidence anchors: Abstract mentioning FLAN-style prompting; section 3.2 showing cloze approach outperforms alternatives.
- Break condition: Performance sensitive to prompt design and will degrade with ambiguous instructions or nuanced judgments.

## Foundational Learning

- Concept: **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: The paper provides a more efficient reward model component for the RLHF pipeline. Understanding the RM's role as a proxy for human judgment during policy optimization is essential context.
  - Quick check question: What is the primary role of a reward model in the standard RLHF training loop?

- Concept: **Bidirectional vs. Autoregressive Architectures**
  - Why needed here: The core claim is that bidirectional encoders are better suited for preference modeling than autoregressive decoders. Understanding the difference in attention mechanisms is critical.
  - Quick check question: How does the attention mechanism of a bidirectional encoder differ from that of an autoregressive decoder, and what does this imply for processing a prompt and two response options?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT) / DoRA**
  - Why needed here: The paper uses DoRA as a key technique for achieving strong performance in reasoning tasks. Understanding PEFT methods is necessary to grasp the proposed method.
  - Quick check question: What is the primary goal of methods like LoRA and DoRA, and how does DoRA specifically modify the standard LoRA approach?

## Architecture Onboarding

- Component map: ModernBERT encoder-only Transformer with modified training pipeline and output interpretation. Input: `[Instruction] + [Response A] + [Response B] + [Preference Question] + [MASK]`. Model processes bidirectionally. Prediction: probability distribution over vocabulary at `[MASK]` position, deriving preference from likelihood of specific tokens.

- Critical path: Understanding data formatting is essential.
  1. Take preference pair: `(instruction, chosen_response, rejected_response)`
  2. Format into FLAN-style template: "Instruction: {instruction} Option A: {chosen_response} Option B: {rejected_response} Which option is better? The better option is [MASK]."
  3. Train to maximize probability of token "A" at `[MASK]` position for chosen-rejected pairs.

- Design tradeoffs: Efficiency vs. generality tradeoff - domain-specific specialists are highly efficient but less capable as generalists. "All-At-Once" model underperforms specialists. Another tradeoff: evaluation protocol differs from official RewardBench API.

- Failure signatures: Explicitly noted failure in open-ended conversational (Chat) tasks. Small bidirectional specialists struggle with this domain likely due to lack of conversational fine-tuning. Expect poor performance on subjective, multi-turn dialogue where "best" response depends on nuanced persona or conversational flow.

- First 3 experiments:
  1. Establish Baseline with Classification Head: Fine-tune ModernBERT-Base using standard `[CLS]` token and linear classification head to reproduce finding that this method is suboptimal.
  2. Implement FLAN-Style Cloze Trainer: Retrain same model using prompt template and masked language modeling objective, comparing performance to baseline on held-out preference pairs.
  3. Ablate PEFT vs. Full Fine-Tuning: Train two reasoning specialists - one with full fine-tuning and one using DoRA with layer freezing - comparing accuracy on reasoning benchmark to confirm PEFT effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-specific TinyRM specialists be effectively merged into a single generalist model without performance degradation?
- Basis: [explicit] Authors hope to reconcile specialists into a single generalist model, noting weight-averaging attempts failed.
- Why unresolved: Paper reports weight-averaging specialists did not result in a model performing well across all domains.
- What evidence would resolve it: Successful application of model merging techniques (e.g., task vectors, advanced weight averaging) yielding single model maintaining high accuracy across Chat, Reasoning, and Safety domains.

### Open Question 2
- Question: What specific mechanisms enable small encoder-based models to maintain rich representations and reasoning capabilities when tuned with parameter-efficient methods like DoRA?
- Basis: [explicit] Authors express interest in mechanisms through which small encoder-based models maintain rich representations, admitting work provides only speculative glimpses.
- Why unresolved: Paper lacks ablation studies and mechanistic interpretability analysis to explain why DoRA specifically boosts reasoning performance in small MLMs.
- What evidence would resolve it: Ablation studies and representational analysis comparing DoRA against full finetuning to identify how internal representations shift during reasoning tasks.

### Open Question 3
- Question: What are the scaling laws governing encoder-based LMs on the reward modeling task?
- Basis: [explicit] Authors note interest in scaling laws governing performance of encoder-based LMs on reward modeling task, citing need for more checkpoints.
- Why unresolved: Current work is preliminary with only basic scaling attempt in appendix, insufficient to characterize trend line behaviors.
- What evidence would resolve it: Comprehensive study training models across wide range of parameter counts (e.g., 100M to several billion) to plot performance scaling curves against compute and data.

### Open Question 4
- Question: Does dual-context evaluation setup artificially inflate TinyRM performance compared to standard single-context inference?
- Basis: [inferred] Paper notes models trained and evaluated with visibility of both options, which is not case for official RewardBench evaluation.
- Why unresolved: Authors admit comparison is not "apples-to-apples," unclear if efficiency gains hold under standard evaluation constraints where model cannot compare options side-by-side.
- What evidence would resolve it: Evaluating TinyRM using official RewardBench protocol where model scores completions individually without seeing alternative option.

## Limitations

- Evaluation protocol gap: Paper evaluates RewardBench using both response options visible simultaneously, while official API scores them separately, making results incomparable to published leaderboard.
- Domain coverage limitations: Tiny models struggle significantly with conversational preference modeling, performing notably worse than on reasoning and safety tasks.
- Task complexity ceiling: Paper does not test whether tiny models can handle more complex scenarios involving multi-turn conversations or highly nuanced judgments.

## Confidence

**High Confidence**: Experimental results showing tiny models achieving competitive performance with 175x fewer parameters on reasoning and safety tasks. Methodology clearly described and reproducible.

**Medium Confidence**: Claim that bidirectional attention is fundamentally better suited for preference modeling than autoregressive attention. Results support this but corpus lacks direct comparative studies between encoder and decoder architectures for this specific task.

**Medium Confidence**: Assertion that parameter-efficient tuning methods like DoRA are more effective than full fine-tuning for reasoning tasks. Paper shows this works but doesn't provide mechanistic explanation for why.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate tiny reasoning and safety specialists on conversational preference tasks to quantify performance gap and understand domain limitations more precisely.

2. **Ablation Study on Attention Mechanisms**: Train decoder-based reward models of similar size and compare their performance directly with tiny MLMs on same tasks to validate claim about architectural advantages.

3. **Scalability Analysis**: Test whether larger tiny models (beyond 400M parameters) continue to show same efficiency advantages, or whether there's threshold where decoder-based models become more competitive.