---
ver: rpa2
title: Towards a Comparative Framework for Compositional AI Models
arxiv_id: '2507.02940'
source_url: https://arxiv.org/abs/2507.02940
tags:
- compositional
- figure
- compositionality
- each
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparative framework for compositional
  AI models, focusing on two key properties: compositional generalisation and compositional
  interpretability. The authors use the DisCoCirc framework to create models that
  can parse and reason about natural language texts, then test these models on various
  aspects of compositionality using a dataset derived from the bAbI tasks.'
---

# Towards a Comparative Framework for Compositional AI Models

## Quick Facts
- **arXiv ID:** 2507.02940
- **Source URL:** https://arxiv.org/abs/2507.02940
- **Reference count:** 40
- **One-line primary result:** Quantum circuit-based and classical neural network models show similar performance on productivity and substitutivity tasks (within 5%), but differ significantly on systematicity tasks (at least 10% difference).

## Executive Summary
This paper presents a comparative framework for compositional AI models using the DisCoCirc framework to parse and reason about natural language texts. The authors test quantum circuit-based and classical neural network models on various aspects of compositionality using a dataset derived from bAbI tasks. The framework enables both compositional generalization (handling novel combinations of known elements) and compositional interpretability (analyzing model behavior through diagram fragments). The study reveals that while both model types perform similarly on productivity and substitutivity, quantum models demonstrate superior systematicity and less overfitting to training data.

## Method Summary
The method employs DisCoCirc, a compositional framework that maps grammatical structures to computational spaces using functors. Text is parsed into diagrams where words are boxes and grammatical dependencies are wires, then mapped to either quantum circuits or classical neural networks. The models are trained on a modified bAbI Task 6 dataset and evaluated on four compositionality aspects: productivity (handling longer inputs), systematicity (handling recombined inputs), substitutivity (handling synonyms), and overgeneralization (handling corrupted data). A unique "Valid AB" validation scheme combines hard training samples with easy test samples to prevent overfitting.

## Key Results
- Both quantum and classical models perform similarly on productivity and substitutivity tasks (within 5% of each other)
- Quantum models significantly outperform classical models on systematicity tasks (at least 10% difference)
- Quantum models show less overfitting to training data compared to neural models
- Model interpretability is demonstrated by analyzing diagram fragments, revealing the model learns a more general question ("Is somebody in the park?") rather than the specific question intended

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing a strict compositional syntax (DisCoCirc) enables semantic generalisation when the task is compositional.
- **Mechanism:** The model architecture is constrained by a syntax category derived from grammar. Text is parsed into diagrams, then a functor maps these to semantic categories. The model learns to combine atoms according to grammatical structure rather than treating sentences as atomic units.
- **Core assumption:** The reading comprehension task is semantically compositional relative to the defined syntax.
- **Evidence anchors:** Abstract mentions compositional nature gives rise to generalisation; Section 2.1.3 defines syntactic compositionality via commuting diagram; corpus supports generating circuits from text.
- **Break condition:** If the task contains non-compositional elements, syntax constraints may prevent finding correct solutions.

### Mechanism 2
- **Claim:** The choice of monoidal product (tensor for quantum vs direct sum for classical) induces different inductive biases regarding systematicity and exception handling.
- **Mechanism:** Quantum models using tensor products are less prone to memorizing specific training instances and more likely to overgeneralize by applying learned compositional rules. Classical models using direct sums can more easily memorize exceptions.
- **Core assumption:** The non-Cartesian nature of the tensor product limits encoding specific exceptions independently of general rules.
- **Evidence anchors:** Abstract notes quantum models are more robust on systematicity; Section 4.4 shows quantum models overgeneralize while neural models overfit.
- **Break condition:** If dataset is small or lacks diversity, neural model's memorization might yield higher training accuracy but fail to generalize.

### Mechanism 3
- **Claim:** Compositional interpretability is achieved by analyzing diagram fragments rather than the full black box.
- **Mechanism:** The model's compositional structure allows isolating semantic representations of specific words or sub-phrases by evaluating their overlap relative to specific assertions.
- **Core assumption:** The semantic category supports a meaningful measure of overlap that correlates with task performance.
- **Evidence anchors:** Abstract demonstrates compositional interpretability by decomposing trained model; Section 5.2 details comparison of diagram fragments.
- **Break condition:** Interpretability via fragments fails if internal representations are entangled in a way that does not respect diagram structure.

## Foundational Learning

- **Concept: Monoidal Categories & Functors**
  - **Why needed here:** The entire DisCoCirc framework relies on mapping grammatical structures to computational spaces using functors.
  - **Quick check question:** Can you explain the difference between a function mapping data points and a functor mapping structure-preserving relationships between categories?

- **Concept: Compositional Generalisation (Productivity vs. Systematicity)**
  - **Why needed here:** The paper evaluates models specifically on their ability to handle longer inputs (Productivity) and recombined inputs (Systematicity).
  - **Quick check question:** If a model sees "Alice likes Bob" and "Bob likes Alice" but fails on "Charlie likes Dave" (where Charlie and Dave are known nouns), which aspect of compositionality is it failing?

- **Concept: Ansatz Design (Quantum vs. Classical)**
  - **Why needed here:** Implementing the "boxes" requires choosing a parameterised circuit or neural sub-network.
  - **Quick check question:** What is the "Sandwich expansion" used for in the context of DisCoCirc frames?

## Architecture Onboarding

- **Component map:** Parser (Syntax) -> Functor (Semantics) -> Evaluator
- **Critical path:**
  1. Define grammar signature for specific bAbI task (People, Objects, Locations)
  2. Select semantic category (CPM(Hilb_2) for quantum, NN for classical)
  3. Implement "Sandwich" expansion to flatten higher-order frames into standard boxes
  4. Optimize parameters to maximize overlap for correct assertions

- **Design tradeoffs:**
  - **Quantum (⊗):** Better for systematicity and handling exceptions via overgeneralisation; harder to simulate/classically scale
  - **Classical (⊕):** Faster training and inference; prone to overfitting training data and failing on unseen noun combinations

- **Failure signatures:**
  - **"Ordering Confusion":** Unitary quantum models may struggle to "forget" previous states, leading to errors in distinguishing "A moves to B then C" vs "A moves to C then B"
  - **"Rote Memorization":** Neural models achieving near-perfect training accuracy but random guessing on systematic test splits
  - **"Identity Collapse":** Models clustering all noun embeddings together if loss function does not sufficiently penalize incorrect overlaps

- **First 3 experiments:**
  1. **Overgeneralisation Check:** Train on dataset with 10-20% corrupted labels. Verify if Quantum model recovers compositional rule while Neural model fits noise.
  2. **Systematicity Split:** Train on subset of noun pairs (Group A) and test on held-out pairs (Group B). Compare accuracy drops between architectures.
  3. **Fragment Analysis:** Train model on productivity task, then probe by computing overlaps for "Identity" vs "Movement" fragments to see if it learned "Is X in Y?" vs "Is *anyone* in Y?" heuristic.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of explicit loss function specification and detailed diagram rewrite rules creates uncertainty for faithful reproduction
- Claims about quantum vs classical differences in systematicity primarily supported by source paper's analysis rather than independent evidence
- Interpretability analysis relies on model's internal structure being semantically meaningful, which may not hold for all linguistic phenomena

## Confidence
- **High Confidence:** The compositional framework (DisCoCirc) and its application to bAbI tasks are well-specified and reproducible
- **Medium Confidence:** Comparative results between quantum and classical models are methodologically sound but rely heavily on source paper's analysis
- **Low Confidence:** Interpretability claims are theoretically grounded but would benefit from more diverse linguistic examples

## Next Checks
1. Implement the "Sandwich expansion" diagram rewrites and verify they preserve semantic meaning across quantum and classical representations
2. Conduct ablation studies removing the extended validation scheme to quantify its impact on overfitting prevention
3. Test the framework on a non-bAbI compositional task (e.g., simple arithmetic word problems) to assess generalizability beyond current domain