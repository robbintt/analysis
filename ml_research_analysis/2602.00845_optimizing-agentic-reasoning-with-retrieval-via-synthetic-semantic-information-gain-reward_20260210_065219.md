---
ver: rpa2
title: Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information
  Gain Reward
arxiv_id: '2602.00845'
source_url: https://arxiv.org/abs/2602.00845
tags:
- information
- reasoning
- gain
- retrieval
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoReasoner introduces a principled framework for optimizing agentic
  reasoning with retrieval by redefining information gain as uncertainty reduction
  over belief states. The core innovation is a synthetic semantic information gain
  reward computed from model outputs without requiring manual retrieval annotations.
---

# Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward

## Quick Facts
- arXiv ID: 2602.00845
- Source URL: https://arxiv.org/abs/2602.00845
- Reference count: 40
- Key outcome: InfoReasoner achieves up to 5.4% average accuracy gain while reducing retrieval turns by 66.7% on HotpotQA

## Executive Summary
InfoReasoner introduces a principled framework for optimizing agentic reasoning with retrieval by redefining information gain as uncertainty reduction over belief states. The core innovation is a synthetic semantic information gain reward computed from model outputs without requiring manual retrieval annotations. This reward guides retrieval policies to maximize epistemic progress by concentrating probability mass on correct semantic answer classes. The method employs Group Relative Policy Optimization (GRPO) and semantic clustering via bidirectional textual entailment to estimate belief distributions. Experiments across seven QA benchmarks demonstrate consistent improvements over strong baselines, achieving up to 5.4% average accuracy gain while requiring fewer retrieval turns (66.7% reduction on HotpotQA). The framework provides both theoretical guarantees (non-negativity, telescoping additivity, monotonicity) and practical scalability for optimizing agentic reasoning systems.

## Method Summary
InfoReasoner optimizes agentic reasoning with retrieval by introducing a synthetic semantic information gain reward that captures epistemic progress through uncertainty reduction. The framework computes this reward by first clustering answers into semantic classes using bidirectional textual entailment, then estimating belief distributions over these clusters from model outputs. The information gain reward measures the reduction in entropy between belief states across retrieval turns, encouraging policies that concentrate probability mass on correct answer classes. This reward guides a retrieval policy optimized via Group Relative Policy Optimization (GRPO), which compares policy performance against a group of baseline trajectories. The semantic clustering approach eliminates the need for manual retrieval annotations while providing a principled measure of epistemic progress that is non-negative, telescoping, and monotonic.

## Key Results
- Achieves up to 5.4% average accuracy improvement across seven QA benchmarks compared to strong baselines
- Reduces average retrieval turns by 66.7% on HotpotQA while maintaining or improving accuracy
- Demonstrates consistent performance gains across diverse QA tasks including HotpotQA, MedQA-USMLE, and FEVER

## Why This Works (Mechanism)
InfoReasoner works by redefining information gain in terms of uncertainty reduction over belief states rather than traditional KL-divergence between probability distributions. The synthetic reward captures epistemic progress by measuring how retrieval actions concentrate probability mass on correct semantic answer classes. By clustering answers into semantic groups and tracking belief distributions over these clusters, the framework can quantify the information gained from each retrieval turn without requiring manual annotations. The GRPO optimization ensures that the retrieval policy learns to select actions that maximize expected information gain relative to a group of baseline trajectories, providing stable learning signals even in sparse reward environments.

## Foundational Learning
- **Semantic Clustering via Textual Entailment**: Groups semantically similar answers using bidirectional textual entailment, needed to estimate belief distributions over answer spaces; quick check: validate clustering quality using semantic similarity metrics
- **Belief State Estimation**: Represents uncertainty as probability distributions over semantic answer clusters, needed to quantify epistemic progress; quick check: verify entropy reduction correlates with accuracy improvement
- **Group Relative Policy Optimization**: Compares policy performance against group baselines rather than absolute rewards, needed for stable learning in sparse reward environments; quick check: analyze policy variance across different baseline groups
- **Synthetic Information Gain Reward**: Computes epistemic progress from model outputs without manual annotations, needed to scale to large-scale reasoning tasks; quick check: compare synthetic rewards against oracle information gain where available
- **Entropy Reduction as Epistemic Progress**: Measures information gain as reduction in uncertainty over belief states, needed to provide interpretable learning signals; quick check: validate that entropy reduction correlates with downstream task performance

## Architecture Onboarding

**Component Map**: Retriever -> Belief Estimator -> Reward Computer -> Policy Optimizer -> Action Selector

**Critical Path**: The core workflow follows: retrieval action selection → document retrieval → answer generation → semantic clustering → belief state estimation → information gain reward computation → policy update via GRPO. The semantic clustering and belief estimation components are critical as they enable the synthetic reward computation that drives the entire optimization process.

**Design Tradeoffs**: The framework trades computational overhead for annotation-free learning by computing synthetic rewards from model outputs rather than using manual annotations. This design enables scalability but introduces potential noise in the reward signal. The semantic clustering approach provides interpretable semantic classes but may struggle with nuanced distinctions. GRPO provides stable learning in sparse reward environments but requires careful hyperparameter tuning for baseline group selection.

**Failure Signatures**: Poor semantic clustering quality leads to incorrect belief state estimation and noisy reward signals. Inadequate clustering granularity (too coarse or too fine) prevents effective epistemic progress measurement. Policy convergence issues may arise from suboptimal baseline group selection in GRPO. Performance degradation occurs when model outputs fail to capture true uncertainty or when semantic distinctions are too subtle for entailment-based clustering.

**3 First Experiments**:
1. Validate semantic clustering quality on a held-out dataset by measuring cluster purity and semantic coherence scores
2. Compare synthetic information gain rewards against oracle rewards computed from ground-truth information gain where available
3. Analyze policy learning curves under different baseline group sizes in GRPO to determine optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Semantic clustering via textual entailment may struggle with nuanced semantic distinctions and domain-specific terminology
- Synthetic reward computation assumes model output distributions adequately capture epistemic uncertainty, which may not hold for all tasks
- Initial overhead from multiple retrieval turns may limit applicability in time-sensitive scenarios despite significant turn reduction
- GRPO-based optimization requires careful hyperparameter tuning and may face convergence challenges in complex reasoning environments

## Confidence
- **High**: Theoretical properties of information gain reward (non-negativity, telescoping additivity, monotonicity) are mathematically sound
- **Medium**: Empirical improvements across seven benchmarks are consistent but represent incremental gains (1.1-5.4%) over strong baselines
- **Medium**: Retrieval turn reduction is well-demonstrated on HotpotQA but needs validation across more diverse tasks
- **Low**: Generalizability to non-QA domains and tasks with complex answer structures remains unproven

## Next Checks
1. **Cross-Domain Robustness**: Evaluate InfoReasoner on non-QA tasks such as scientific reasoning, mathematical problem-solving, or multi-step planning to assess generalization beyond question answering
2. **Semantic Clustering Quality**: Conduct ablation studies measuring the impact of different semantic clustering algorithms (e.g., sentence transformers vs. entailment-based clustering) on final performance
3. **Reward Function Sensitivity**: Systematically vary synthetic reward computation parameters (e.g., KL-divergence vs. entropy reduction, different uncertainty estimation methods) to determine sensitivity of policy optimization to design choices