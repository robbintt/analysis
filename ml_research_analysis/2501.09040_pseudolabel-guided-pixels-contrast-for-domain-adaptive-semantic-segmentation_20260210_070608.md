---
ver: rpa2
title: Pseudolabel guided pixels contrast for domain adaptive semantic segmentation
arxiv_id: '2501.09040'
source_url: https://arxiv.org/abs/2501.09040
tags:
- segmentation
- semantic
- learning
- domain
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses unsupervised domain adaptation for semantic
  segmentation by proposing a novel framework called Pseudo-label Guided Pixel Contrast
  (PGPC). The method integrates pixel-to-pixel and pixel-to-prototype contrastive
  learning, using class prototypes as positive samples and pixel representations as
  negative samples.
---

# Pseudolabel guided pixels contrast for domain adaptive semantic segmentation

## Quick Facts
- **arXiv ID:** 2501.09040
- **Source URL:** https://arxiv.org/abs/2501.09040
- **Reference count:** 40
- **Primary result:** PGPC achieves state-of-the-art UDA semantic segmentation with 71.8% mIoU on GTA5→Cityscapes

## Executive Summary
This paper introduces Pseudo-label Guided Pixel Contrast (PGPC), a framework that integrates pixel-to-pixel and pixel-to-prototype contrastive learning for unsupervised domain adaptation in semantic segmentation. The method leverages class prototypes as positive samples and pixel representations as negative samples, while employing entropy-based filtering to select reliable target pixels for contrastive learning. PGPC achieves significant improvements over existing UDA approaches, demonstrating relative gains of 5.1% and 4.6% mIoU on GTA5→Cityscapes and SYNTHIA→Cityscapes benchmarks respectively.

## Method Summary
PGPC combines supervised source training with self-training on target images, enhanced by a contrastive loss component. The framework uses a student-teacher architecture where the teacher generates pseudo-labels for target images via EMA. After a warmup period, PGPC computes contrastive loss using reliable pixels (selected by low entropy) as anchors, source class prototypes as positives, and cross-class pixels (including target negatives) as negatives. The total loss is a weighted combination of source CE loss, target CE loss, and contrastive loss, with hyperparameters λt=1.0 and λc=0.1.

## Key Results
- Achieves 71.8% mIoU on GTA5→Cityscapes, outperforming DAFormer by 5.1%
- Achieves 52.2% mIoU on SYNTHIA→Cityscapes, outperforming DAFormer by 4.6%
- Demonstrates consistent improvements across multiple UDA methods when PGPC is applied
- Ablation studies confirm the importance of target negative pixels and entropy-based selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating pixel-to-prototype and pixel-to-pixel contrastive learning improves feature discriminability across domains compared to either method alone.
- **Mechanism:** Class prototypes serve as positive samples (pulling target pixels toward category centers), while cross-class pixel representations serve as negative samples (pushing them away from competing category feature spaces).
- **Core assumption:** Source domain prototypes approximate meaningful feature centers that transfer to the target domain distribution.
- **Evidence anchors:** [abstract] "integrates pixel-to-pixel and pixel-to-prototype contrastive learning"; [section 1, page 3] "gains the capability to not only discern pixel representations and the distinctions between pixels belonging to diverse categories but also capture the similarities between pixel representations and the prototypes of the same category".
- **Break condition:** If source and target domain feature distributions have minimal overlap, prototypes may not serve as meaningful attractors for target pixels.

### Mechanism 2
- **Claim:** Using per-pixel prediction entropy to select reliable target pixels reduces noise from incorrect pseudo-labels while maintaining sufficient training signal.
- **Mechanism:** The teacher model generates softmax probabilities; entropy is computed per pixel. Only pixels in the lowest η% (entropy-wise) are used as anchor pixels in contrastive learning.
- **Core assumption:** Low-entropy predictions correlate with correct pseudo-labels across both majority and minority classes.
- **Evidence anchors:** [section 3.2, page 7] "employ per-pixel prediction entropy to filter high confident pixels for contrastive learning"; [table 7, page 18] Ablation shows global entropy (71.8 mIoU) outperforms class-wise confidence (70.3 mIoU).
- **Break condition:** If early-training teacher predictions are systematically biased (e.g., always uncertain on minority classes), entropy filtering may under-utilize critical classes.

### Mechanism 3
- **Claim:** Unreliable target pixels can still contribute as negative samples for classes they confidently reject, improving negative sample diversity.
- **Mechanism:** For each pixel, the model ranks class probabilities. If class c ranks among the r lowest-probability classes (r=4 in experiments), that pixel is used as a negative sample for class c.
- **Core assumption:** Low-ranked class predictions indicate genuine dissimilarity rather than model miscalibration.
- **Evidence anchors:** [section 3.3, page 9] "operate under the assumption that the model can effectively distinguish between dissimilar classes while potentially encountering confusion primarily among a subset of classes that exhibit similarity"; [table 6, page 17] Ablation shows adding target negative pixels improves mIoU from 70.0 to 71.8.
- **Break condition:** If model predictions are poorly calibrated (e.g., uniform across all classes), low-ranked classes may not indicate true dissimilarity.

## Foundational Learning

- **Concept: InfoNCE Contrastive Loss**
  - Why needed here: The core training signal for PGPC; measures relative similarity between anchor, positive, and negative samples.
  - Quick check question: Can you explain why InfoNCE uses a softmax over similarity scores rather than directly maximizing positive similarity?

- **Concept: Exponential Moving Average (EMA) Teacher**
  - Why needed here: Generates stable pseudo-labels for target images; updated as EMA of student weights.
  - Quick check question: Why might an EMA teacher produce more stable pseudo-labels than simply using the student model directly?

- **Concept: Memory Bank for Prototypes/Negatives**
  - Why needed here: Stores pixel representations across batches to compute stable prototypes and provide diverse negative samples beyond mini-batch limits.
  - Quick check question: What happens to prototype quality if the memory bank is too small or not updated with momentum?

## Architecture Onboarding

- **Component map:**
  - Encoder (MiT-B5) -> Segmentation head -> Projection head -> Memory banks
  - Teacher model (EMA of student) -> Pseudo-labels generation
  - Memory banks -> Store source pixel representations for prototype computation and negative sampling

- **Critical path:**
  1. Source images → supervised CE loss (Ls)
  2. Target images → teacher generates pseudo-labels → student trained with weighted CE loss (Lt)
  3. After warmup (20k iterations): contrastive loss (Lc) computed using reliable anchor pixels, source prototypes (positive), and cross-class pixels (negative)
  4. Total loss: L = Ls + λt·Lt + λc·Lc (λt=1.0, λc=0.1 recommended)

- **Design tradeoffs:**
  - Global vs. class-wise entropy: Global entropy performs better (71.8 vs 70.3 mIoU) but may undersample minority classes
  - Memory bank size: More negative samples improve performance (512 > 256 > 128) but increase memory/compute
  - Warmup period: 20k iterations stabilizes training but delays contrastive learning benefits

- **Failure signatures:**
  - Performance plateaus early: Check if λc is too high (dominates segmentation loss)
  - Minority class collapse: Verify η is not too low (over-filtering reliable pixels)
  - Training instability: Ensure warmup iterations complete before enabling Lc

- **First 3 experiments:**
  1. **Baseline verification:** Train DAFormer without PGPC on GTA5→Cityscapes; confirm ~68.3 mIoU to establish reproducibility
  2. **Ablation on negative sampling:** Compare (a) source negatives only vs. (b) source + target negatives; expect +1.0-1.8 mIoU gain from target negatives
  3. **Hyperparameter sweep on η:** Test η ∈ {0.4, 0.5, 0.6} to find optimal reliable pixel proportion; paper reports η=0.5 best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational resource consumption of the PGPC framework be reduced during training without compromising segmentation accuracy?
- Basis in paper: [explicit] The Conclusion explicitly states that the method "requires a significant amount of computational resources during training," identifying resource reduction as a "viable direction for improvement."
- Why unresolved: The contrastive learning component, specifically the maintenance of memory banks and pixel-wise comparisons, increases training time by approximately 33% compared to baselines like HRDA.
- What evidence would resolve it: A modified training regime or approximation method that lowers GPU memory usage and training duration while maintaining the 71.8% mIoU performance on the GTA5→Cityscapes benchmark.

### Open Question 2
- Question: How can the framework be modified to effectively eliminate error accumulation caused by erroneous pseudo-labels?
- Basis in paper: [explicit] The Discussion and Conclusion note that shortcomings in the results are attributed to "erroneous pseudo-labeling," and efforts should be directed toward "denoising pseudo-labels."
- Why unresolved: While the method uses entropy to select reliable pixels, it cannot fully avoid noisy labels, which distorts the feature clusters in the embedding space.
- What evidence would resolve it: A denoising strategy that identifies and corrects or discards high-entropy false positives, resulting in tighter feature clusters in the t-SNE visualization and improved segmentation of minority classes.

### Open Question 3
- Question: Can the proportion of reliable pixels (η) and the contrastive loss weight (λc) be dynamically adapted rather than empirically fixed?
- Basis in paper: [inferred] Section 4.3 ablation studies show that the model is sensitive to η and λc, with performance varying by >1.5% mIoU based on the chosen values.
- Why unresolved: The authors tuned these hyperparameters empirically for specific datasets (e.g., η=0.5 for GTA5), but static values may be suboptimal for different domain shifts or training stages.
- What evidence would resolve it: An adaptive scheduling algorithm for η or λc that automatically adjusts these values during training, achieving stable or superior performance across both GTA5→Cityscapes and SYNTHIA→Cityscapes without manual retuning.

## Limitations
- **Computational intensity:** The framework requires significant computational resources during training due to contrastive learning components
- **Memory bank complexity:** The memory bank implementation details are underspecified, particularly queue sizes and update frequencies
- **Hyperparameter sensitivity:** The model shows sensitivity to η and λc values, requiring careful tuning for different datasets

## Confidence

- **High Confidence:** The core mechanism of combining pixel-to-prototype and pixel-to-pixel contrastive learning is well-explained and supported by ablation studies showing consistent improvements over baseline methods
- **Medium Confidence:** The entropy-based reliable pixel selection strategy shows promising results but the optimal η value (0.5) may be dataset-dependent and requires tuning
- **Low Confidence:** The negative sampling strategy using low-ranked class predictions is novel but lacks corpus validation and may be sensitive to model calibration issues

## Next Checks

1. Implement the memory bank with varying queue sizes (128, 256, 512 per class) to determine optimal configuration for balancing performance and computational cost
2. Conduct a systematic ablation study on the rank threshold r (try r ∈ {3, 4, 5}) to verify its impact on minority class performance and overall mIoU
3. Test the model's robustness by training on SYNTHIA→Cityscapes with different warmup periods (10k, 20k, 30k iterations) to identify the minimum effective warmup duration