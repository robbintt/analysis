---
ver: rpa2
title: 'CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data
  for Micro-Gesture Recognition on the iMiGUE Dataset'
arxiv_id: '2506.16385'
source_url: https://arxiv.org/abs/2506.16385
tags:
- pose
- clip
- micro-gesture
- visual
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenging problem of micro-gesture recognition
  by introducing CLIP-MG, a pose-guided semantic attention architecture that integrates
  skeletal pose features with RGB data. The core method uses pose information to generate
  semantic queries that guide CLIP's visual attention toward subtle gesture-related
  regions, combined with gated multi-modal fusion to adaptively merge pose and visual
  representations.
---

# CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset

## Quick Facts
- **arXiv ID**: 2506.16385
- **Source URL**: https://arxiv.org/abs/2506.16385
- **Reference count**: 40
- **Primary result**: Achieves 61.82% Top-1 accuracy on iMiGUE micro-gesture recognition dataset

## Executive Summary
This paper addresses the challenging problem of micro-gesture recognition by introducing CLIP-MG, a pose-guided semantic attention architecture that integrates skeletal pose features with RGB data. The core method uses pose information to generate semantic queries that guide CLIP's visual attention toward subtle gesture-related regions, combined with gated multi-modal fusion to adaptively merge pose and visual representations. Evaluated on the iMiGUE dataset, the model achieves a Top-1 accuracy of 61.82%, outperforming several single-modality baselines and matching state-of-the-art approaches. Ablation studies confirm that each component—pose guidance, semantic query generation, and gated fusion—contributes significantly to performance, demonstrating the value of integrating multimodal and semantic information for fine-grained gesture recognition.

## Method Summary
CLIP-MG processes 8 sampled RGB frames (224×224) through a frozen CLIP ViT-B/16 visual encoder (last 2 blocks unfrozen) and 32 OpenPose skeleton frames through a 3D CNN skeleton encoder. The skeleton encoder rasterizes 18 joints as 2D Gaussians (σ=2.5px) on 256×256 heatmaps, processes them through 3 conv stages (64→128→256), and projects to 512-dim features. Pose-guided query generation computes spatial weights based on joint proximity, applies gated fusion with learnable confidence scores, and uses cross-attention to pool visual tokens. A two-layer MLP classifier produces final predictions. The model uses cross-entropy loss and is evaluated on a cross-subject split (37 train / 35 test subjects).

## Key Results
- Achieves 61.82% Top-1 accuracy on iMiGUE dataset, outperforming single-modality baselines
- Pose-only baseline achieves 45.30%, visual-only achieves 45.30%, demonstrating complementary information
- Ablation studies show each component (pose guidance: +16.52 pp, cross-attention: +8.65 pp, gated fusion: +1.74 pp) contributes significantly

## Why This Works (Mechanism)

### Mechanism 1: Spatial Prior Filtering
Skeleton features act as a spatial prior to filter noise in vision transformers. The architecture computes relevance scores for each visual patch based on Euclidean distance to skeletal joints, suppressing background patches before query generation. Assumes micro-gestures manifest visually near skeletal joints.

### Mechanism 2: Adaptive Confidence Gating
Gated fusion prevents noisy pose estimates from degrading visual features. A learnable gate vector modulates visual features and semantic query, allowing the network to fall back to pure visual cues when pose data is ambiguous or occluded. Assumes pose data reliability varies and requires adaptive confidence scoring.

### Mechanism 3: Semantic Cross-Attention Pooling
Semantic cross-attention pools disparate visual tokens into unified gesture representation. The pose-weighted query acts as a probe in CLIP's final transformer layer, attending over gated visual tokens to aggregate spatially distributed features into a compact vector for classification. Assumes a single vector can encapsulate complex spatio-temporal gesture semantics.

## Foundational Learning

- **Vision Transformers and Patch Embeddings**: Needed to understand how CLIP processes images into 196 patch tokens. Quick check: How does the relevance score w_{t,p} modify the contribution of a specific image patch during weighted mean pooling?

- **Cross-Attention (Query, Key, Value)**: Core to the innovation of using pose-derived vectors as queries to interrogate visual keys and values. Quick check: In section 3.5, what represents the Query (Q) and what represents the Keys (K) and Values (V) in the cross-attention equation?

- **Long-Tailed Data Distributions**: Explains why simple accuracy is a brittle metric given iMiGUE's heavy imbalance (28 of 32 classes are tail classes). Quick check: Why might standard Cross-Entropy loss lead to a model that ignores subtle tail gesture classes in favor of majority classes?

## Architecture Onboarding

- **Component map**: Visual Encoder (Frozen CLIP ViT-B/16) -> Skeleton Encoder (3D CNN) -> Query Generator -> Gated Fusion -> Cross-Attention Layer -> Classifier

- **Critical path**: Synchronization between sampling 8 RGB frames and averaging 32 Pose frames into 8 windows. Misalignment destroys subtle signal.

- **Design tradeoffs**: Frozen vs. fine-tuned CLIP (reduces overfitting but limits adaptation to micro-gesture textures); single query vs. multiple queries (computationally cheap but risks losing distinct spatial relationships).

- **Failure signatures**: Static output (same class for all inputs suggests gating zeroing out pose signal); overfitting on identity (recognizing who rather than what suggests overreliance on global [CLS] token).

- **First 3 experiments**: 1) Verify alignment by visualizing attention weights on RGB images; 2) Run visual-only vs. pose-only baselines to confirm 16.52% gap; 3) Monitor gating scalar distribution during training to verify adaptive learning.

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation ambiguities including potential gated fusion equation typo and unspecified hyperparameters limit reproducibility
- Cross-subject evaluation protocol makes model vulnerable to subject-specific biases not explicitly addressed
- Reliance on OpenPose skeleton quality introduces potential failure point without reported pose estimation accuracy metrics

## Confidence

**High Confidence**: Core architectural contribution (pose-guided semantic query with gated fusion) and quantitative impact (61.82% accuracy, outperforming baselines); ablation studies showing individual component contributions.

**Medium Confidence**: Mechanism explanations for pose guidance (spatial prior filtering) and gated fusion (adaptive confidence); primarily architectural assertions rather than rigorously proven.

**Low Confidence**: Discussion of tail-class performance and data imbalance handling; long-tail distribution acknowledged but not systematically addressed through rebalancing or loss weighting strategies.

## Next Checks

1. **Alignment Verification**: Visualize pose-derived attention weights overlaid on RGB frames to confirm highlighted regions correspond to skeletal joints and moving body parts. Misalignment would invalidate the entire guidance mechanism.

2. **Gate Behavior Analysis**: Monitor the gating scalar g during training - plot its distribution over time and across different inputs. If the gate saturates to 0 or 1 for most inputs, the adaptive fusion claim is unsupported.

3. **Cross-Subject Generalization**: Compute per-subject accuracy and analyze variance across 72 subjects. Aggregate metric (61.82%) may mask poor performance on specific subjects, indicating learning of subject-specific rather than gesture-specific features.