---
ver: rpa2
title: A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language
  Models
arxiv_id: '2508.20583'
source_url: https://arxiv.org/abs/2508.20583
tags:
- graph
- node
- reasoning
- station
- subway
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current graph-language model benchmarks fail to properly assess
  multimodal reasoning capabilities, as strong performance can be achieved using either
  graph or text information alone. To address this gap, we introduce CLEGR, a synthetic
  benchmark designed to require joint reasoning over graph structure and textual semantics.
---

# A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models

## Quick Facts
- arXiv ID: 2508.20583
- Source URL: https://arxiv.org/abs/2508.20583
- Authors: Soham Petkar; Hari Aakash K; Anirudh Vempati; Akshit Sinha; Ponnurangam Kumarauguru; Chirag Agarwal
- Reference count: 40
- Primary result: Current graph-language model benchmarks fail to assess multimodal reasoning capabilities, with GLMs providing negligible gains over soft-prompted LLMs

## Executive Summary
Current benchmarks for Graph-Language Models (GLMs) fail to properly evaluate multimodal reasoning because strong performance can be achieved using either graph or text information alone. The paper introduces CLEGR, a synthetic benchmark designed to require joint reasoning over graph structure and textual semantics. Evaluations reveal that GLMs provide negligible performance gains over simple soft-prompted LLMs across all tasks, including zero-shot transfer and larger graph sizes. Linear probing confirms that graph encoders capture most task-relevant information, rendering the LLM component functionally redundant. These findings indicate that current GLMs do not effectively integrate graph and language modalities, highlighting the need for more sophisticated architectural approaches.

## Method Summary
The study evaluates Graph-Language Models on existing text-attributed graph benchmarks (Cora, CiteSeer, Computers, Photo, History, Arxiv) and introduces CLEGR, a synthetic subway network benchmark with 1,000 graphs and 54,000 questions. Models tested include TEA-GLM, GraphToken (with GraphSAGE/GAT backbones), and soft-prompted LLMs. The evaluation uses exact match accuracy and includes linear probing experiments to test whether graph encoders capture all task-relevant information. CKA analysis measures representational similarity between GLMs and soft-prompted baselines. All experiments use frozen LLM backbones with 1 epoch training and AdamW optimization (LR=0.001, batch size=1).

## Key Results
- GLMs achieve negligible performance gains over soft-prompted LLM baselines across all tasks
- Linear probing on graph tokens matches GLM performance on structurally-sufficient datasets
- Soft-prompted LLMs achieve comparable results to GLMs on semantically-sufficient datasets
- CKA analysis shows high representational similarity between GLMs and soft-prompted baselines on most datasets

## Why This Works (Mechanism)

### Mechanism 1
Existing graph-language benchmarks can be solved using unimodal information alone, failing to test true multimodal integration. Datasets fall into two categories: (1) semantically-sufficient, where textual features contain all discriminative information, and (2) structurally-sufficient, where graph topology alone suffices for classification. Neither requires integrating both modalities.

### Mechanism 2
Current GLMs fail to meaningfully integrate graph structure, primarily relying on LLM textual processing regardless of architectural complexity. Graph encoder outputs are projected into soft tokens prepended to LLM inputs, but the LLM appears to process these as generic tokens rather than structural information.

### Mechanism 3
Representation similarity between GLMs and soft-prompted LLMs correlates with performance similarity, revealing when structural information is actually utilized. CKA analysis measures representational overlap. When GLMs learn different representations (lower CKA in middle layers), it occurs precisely when datasets are structurally-sufficient.

## Foundational Learning

- **Soft Prompting**: The paper uses soft-prompted LLMs as the primary baseline to isolate whether GLM improvements come from structural reasoning or just additional trainable parameters. If a model with 10 learnable soft tokens achieves 75% accuracy, and a model with a 3-layer GNN encoder + 10 graph tokens also achieves 75%, what can you conclude about the GNN's contribution?

- **Linear Probing**: Used to test whether graph encoder representations contain all task-relevant information without LLM processing. High probe accuracy indicates the LLM may be functioning as an expensive decoder. A linear probe on frozen graph tokens achieves 85% accuracy while the full GLM achieves 87%. Does this suggest the LLM is performing complex reasoning?

- **Text-Attributed Graphs (TAGs)**: The primary data structure in this workâ€”nodes have textual features (e.g., paper titles/abstracts), edges have semantic relationships. GLMs aim to jointly reason over both structure and text. In a citation network where nodes are papers with abstracts and edges indicate citations, what information would be lost by (a) using only the abstracts, (b) using only the citation graph?

## Architecture Onboarding

- **Component map**: GNN Encoder (Mg) -> Projection Layer (MP) -> LLM Backbone (Ml) -> Graph Tokens
- **Critical path**: 
  1. Encode graph: `h = GNN(X, edge_index)`
  2. Pool for graph-level tasks: `h_graph = mean_pool(h)` or use node embeddings directly
  3. Project to LLM space: `graph_tokens = Linear(h)`
  4. Concatenate: `input = [graph_tokens] + [text_tokens] + [question_tokens]`
  5. Generate: `output = LLM(input)`
- **Design tradeoffs**: 
  - GNN backbone choice: GraphSAGE (neighbor sampling, scalable) vs. GAT (attention-based, expressive but slower)
  - Token count: More tokens can encode more structural information but increase sequence length
  - Frozen vs. fine-tuned LLM: Paper freezes LLM; fine-tuning could improve integration but risks catastrophic forgetting
  - Pre-training strategy: TEA-GLM pre-trains GNN with contrastive learning aligned to LLM embeddings; GraphToken trains end-to-end
- **Failure signatures**: 
  - Graph encoder not utilized: GLM accuracy matches soft-prompt baseline
  - Retrieval failures: G-Retriever shows degraded performance
  - Scaling mismatch: If GLM doesn't outperform baseline as graph size increases
- **First 3 experiments**: 
  1. Establish baseline parity: Train soft-prompted LLM and GLM on CLEGR-Facts; verify similar accuracy
  2. Test structural reasoning gap: Evaluate both models on CLEGR-Reasoning; if gap is minimal, the GNN isn't contributing meaningfully
  3. Ablate graph encoder: Replace GNN with random projections or zero outputs; if performance doesn't drop, the graph structure isn't being utilized

## Open Questions the Paper Calls Out

### Open Question 1
Can GLMs be designed to genuinely integrate graph structure with language semantics, rather than relying primarily on textual processing? The paper concludes that current GLMs "provide negligible performance gains over soft-prompted LLM baselines" and their capabilities are "primarily driven by the LLM's textual processing capabilities rather than an interplay of graph and text modalities."

### Open Question 2
Would the findings generalize to real-world graphs with noisy, ambiguous, or large-scale structures? The authors state: "As GLMs continue to develop, their capacity to represent graphs are likely to increase, and our current evaluation may not encompass all their capabilities."

### Open Question 3
Do the results hold for a wider range of GLM architectures and LLM backbones (e.g., multimodal transformers, newer LLMs)? The evaluation covers representative GLMs with Llama3/Phi backbones, but the space of possible architectures is vast and rapidly evolving.

## Limitations

- The synthetic nature of CLEGR may not capture the full complexity of real-world text-attributed graphs like social networks or biomedical knowledge graphs
- The comparison methodology assumes soft-prompted LLM baselines adequately capture unimodal performance ceilings, potentially underestimating specialized unimodal models
- CKA similarity analysis may miss non-linear relationships or complementary information encoded differently across architectures

## Confidence

**High Confidence (8-10/10)**: The core finding that existing benchmarks allow unimodal solutions and that CLEGR requires joint reasoning is well-supported by systematic analysis. The performance parity between GLMs and soft-prompted baselines across multiple architectures and datasets provides strong evidence for the central claim about current GLM limitations.

**Medium Confidence (5-7/10)**: The interpretation of CKA similarity patterns as evidence for multimodal integration relies on several assumptions about representation space and model behavior. While the correlation with task performance is suggestive, alternative explanations exist for the observed patterns.

**Low Confidence (1-4/10)**: The generalizability of CLEGR's findings to other graph domains and real-world applications remains uncertain. The synthetic nature of the benchmark, while methodologically sound, limits confidence in extrapolating results to production scenarios involving diverse graph structures and reasoning patterns.

## Next Checks

1. **Cross-domain validation**: Apply the same evaluation methodology to real-world text-attributed graphs from domains like social networks, citation networks, or biological networks. Compare performance patterns to validate whether CLEGR's findings extend beyond synthetic subway networks.

2. **Specialized unimodal baselines**: Implement and evaluate dedicated graph neural networks and language models separately on CLEGR tasks. This would establish whether the soft-prompted LLM baseline is appropriately challenging and whether specialized unimodal models could outperform general-purpose GLMs.

3. **Fine-tuning analysis**: Repeat the core experiments with fine-tuned rather than frozen LLM backbones. This would test whether the observed GLM limitations persist when models can adapt their internal representations to better integrate graph and language information.