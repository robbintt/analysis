---
ver: rpa2
title: Pretraining Scaling Laws for Generative Evaluations of Language Models
arxiv_id: '2509.24012'
source_url: https://arxiv.org/abs/2509.24012
tags:
- scaling
- wang
- compute
- chen
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies pretraining scaling laws for generative evaluations
  using pass-at-k metrics on math benchmarks. The authors fit and predict performance
  using three scaling laws based on pretraining compute, model parameters plus tokens,
  and gold reference likelihoods.
---

# Pretraining Scaling Laws for Generative Evaluations of Language Models

## Quick Facts
- arXiv ID: 2509.24012
- Source URL: https://arxiv.org/abs/2509.24012
- Authors: Rylan Schaeffer; Noam Levi; Brando Miranda; Sanmi Koyejo
- Reference count: 40
- Key outcome: Study of pretraining scaling laws for generative evaluations using pass-at-k metrics on math benchmarks, showing that increasing k reduces irreducible error and affects scaling exponents

## Executive Summary
This paper investigates pretraining scaling laws for generative evaluations using pass-at-k metrics on math benchmarks. The authors propose and validate three scaling laws based on pretraining compute, model parameters plus tokens, and gold reference likelihoods. They demonstrate that the number of attempts per problem (k) significantly influences scaling behavior by reducing irreducible error and steepening scaling exponents. The study provides theoretical insights into compute-optimal scaling for generative tasks and shows that all three scaling laws perform comparably in prediction, with the gold reference likelihood law showing exceptional parameter stability across orders of magnitude of compute.

## Method Summary
The authors analyze pretraining scaling laws for generative evaluations using pass-at-k metrics on math benchmarks (GSM8K and MATH). They fit three different scaling laws to empirical data: one based on pretraining compute, another on model parameters plus tokens, and a third using gold reference likelihoods. The scaling laws are compared through prediction performance and parameter stability analysis across approximately five orders of magnitude of compute. The authors also provide theoretical proofs connecting the different scaling laws, including demonstrating that the compute scaling law is the compute-optimal envelope of the parameters-and-tokens scaling law. The analysis focuses on how the hyperparameter k (number of attempts per problem) affects scaling behavior and irreducible error.

## Key Results
- Increasing k reduces irreducible error and steepens scaling exponents
- All three scaling laws (compute, parameters+tokens, gold reference likelihood) perform comparably in prediction
- Gold reference likelihood scaling law shows exceptional parameter stability across ~5 orders of magnitude of compute
- Compute scaling law is the compute-optimal envelope of parameters-and-tokens scaling law with quantifiable misallocation penalty
- For GSM8K, irreducible error vanishes by k≈100; for MATH, significant irreducible error remains even at k=10,000

## Why This Works (Mechanism)
The mechanism behind these scaling laws centers on the relationship between evaluation methodology and scaling behavior. By introducing the pass-at-k metric, the authors create a more nuanced evaluation framework that captures the iterative nature of generative model outputs. The reduction in irreducible error with increasing k occurs because generative models can explore multiple solution paths, and some fraction of these attempts will succeed even if individual attempts have limited probability of success. The steepening of scaling exponents reflects the fact that as models scale, their ability to generate correct solutions improves multiplicatively across multiple attempts. The theoretical connection between scaling laws emerges from the fundamental relationship between compute, parameters, and the information-theoretic capacity of models to represent solutions.

## Foundational Learning

### Scaling Laws
**Why needed:** Understanding how model performance scales with size and compute is essential for efficient resource allocation
**Quick check:** Verify that power-law relationships hold across multiple orders of magnitude of compute

### Pass-at-k Metrics
**Why needed:** Traditional accuracy metrics don't capture the iterative nature of generative model outputs
**Quick check:** Ensure k attempts provide statistically independent samples

### Compute-Optimal Scaling
**Why needed:** Maximizing performance per unit compute is crucial for practical model deployment
**Quick check:** Compare actual scaling to theoretical optimal envelope

### Irreducible Error
**Why needed:** Understanding fundamental limits helps set realistic expectations for model performance
**Quick check:** Verify that irreducible error decreases monotonically with k

## Architecture Onboarding

### Component Map
Pretraining Compute -> Model Parameters -> Tokens Processed -> Performance Metrics -> Pass-at-k Evaluation

### Critical Path
The critical path is: Pretraining Compute → Model Parameters → Tokens Processed → Performance Metrics. Each component must scale appropriately for optimal performance.

### Design Tradeoffs
The primary tradeoff is between evaluation cost (proportional to k) and prediction accuracy. Higher k reduces irreducible error but increases computational cost of evaluation.

### Failure Signatures
Failure occurs when scaling laws overfit to specific benchmarks, when k is too low to capture generative capabilities, or when theoretical assumptions about compute optimality break down in practice.

### First Experiments
1. Validate scaling laws on additional generative benchmarks beyond math problems
2. Conduct ablation studies on different values of k to find optimal trade-off
3. Test theoretical predictions about compute-optimal scaling in actual training scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses specifically on math benchmarks, potentially limiting generalizability to more open-ended generative tasks
- The theoretical proofs assume idealized conditions that may not hold in practical model development scenarios
- The study assumes pass-at-k metrics adequately capture the full generative evaluation landscape, which may not generalize to all task types

## Confidence
- High confidence: The empirical finding that increasing k reduces irreducible error and affects scaling exponents is well-supported by the data and theoretically grounded
- Medium confidence: The prediction performance comparison between different scaling laws depends heavily on the specific benchmarks and evaluation setup used
- Medium confidence: The theoretical connections between scaling laws are mathematically rigorous but may not fully capture practical constraints

## Next Checks
1. Validate the scaling laws on additional generative benchmarks beyond math problems, including more open-ended tasks like story generation or code completion, to test generalizability
2. Conduct ablation studies on different values of k to determine the optimal trade-off between evaluation cost and prediction accuracy across various task types
3. Test the theoretical predictions about compute-optimal scaling in actual model training scenarios with realistic computational constraints and hyperparameter tuning limitations