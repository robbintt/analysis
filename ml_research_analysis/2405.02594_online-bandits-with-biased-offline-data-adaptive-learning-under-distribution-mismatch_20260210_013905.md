---
ver: rpa2
title: 'Online Bandits with (Biased) Offline Data: Adaptive Learning under Distribution
  Mismatch'
arxiv_id: '2405.02594'
source_url: https://arxiv.org/abs/2405.02594
tags:
- offline
- data
- regret
- bound
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic multi-armed and combinatorial bandits
  with potentially biased offline data. Without auxiliary information, no non-anticipatory
  policy can outperform vanilla UCB, even with offline data.
---

# Online Bandits with (Biased) Offline Data: Adaptive Learning under Distribution Mismatch

## Quick Facts
- **arXiv ID:** 2405.02594
- **Source URL:** https://arxiv.org/abs/2405.02594
- **Reference count:** 40
- **Primary result:** MIN-UCB and MIN-COMB-UCB adaptively incorporate biased offline data when beneficial, improving regret bounds over vanilla UCB

## Executive Summary
This paper studies stochastic multi-armed and combinatorial bandits where offline data distribution may differ from online distribution. The key insight is that valid bias bounds are necessary and sufficient to outperform vanilla UCB when using potentially biased offline data. The proposed MIN-UCB and MIN-COMB-UCB policies adaptively incorporate offline data only when beneficial by computing both vanilla UCB and warm-start UCB, then selecting the minimum. The framework achieves improved regret bounds over existing baselines, with tight instance-dependent and instance-independent regret bounds derived. Applications include dynamic pricing and social influence maximization.

## Method Summary
The paper proposes MIN-UCB for multi-armed bandits and MIN-COMB-UCB for combinatorial bandits. Both algorithms compute two confidence bounds at each step: vanilla UCB_t(a) and warm-start UCB^S_t(a) that incorporates offline data with bias correction. The arm/action selection uses argmax min{UCB_t(a), UCB^S_t(a)}, automatically falling back to pure online learning when offline data is unreliable. The bias correction term is TS(a)·V(a)/(N_t(a)+TS(a)), where V(a) is a user-provided valid bias bound. The approach requires both P^off and P^on to be 1-subGaussian for concentration inequalities.

## Key Results
- Without valid bias bounds, no non-anticipatory policy can outperform vanilla UCB, even with offline data
- MIN-UCB achieves improved regret bounds when ω(a) = V(a) + (μ^off(a) - μ^on(a)) < Δ(a) for sub-optimal arms
- The direction of bias (overestimation vs. underestimation) affects regret improvement, not just bias magnitude
- Instance-independent regret bound improvement depends on min_a TS(a), while instance-dependent bounds are tight
- MIN-COMB-UCB extends the framework to combinatorial settings with semi-bandit feedback

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Valid bias bounds V are necessary and sufficient to outperform vanilla UCB when using potentially biased offline data.
- **Mechanism:** The bias bound V(a) upper-bounds |μ^off(a) - μ^on(a)|. The algorithm constructs a corrected confidence radius rad^S_t(a) = √(2log(2t/δ_t)/(N_t(a)+TS(a))) + TS(a)/(N_t(a)+TS(a))·V(a). When ω(a) = V(a) + (μ^off(a) - μ^on(a)) < Δ(a), offline data provides a saving term Sav_0(a) = TS(a)·Δ(a)·max{1 - ω(a)/Δ(a), 0}^2.
- **Core assumption:** The DM is provided with a valid (possibly conservative) bias bound V before online learning begins.
- **Evidence anchors:** Impossibility result showing any policy improving on I_P incurs Ω(T^{1-β}) regret on I_Q; "Best Arm Identification with Possibly Biased Offline Data" confirms impossibility without bias bounds in BAI setting.
- **Break condition:** If V(a) = ∞ for all arms (no non-trivial knowledge), MIN-UCB reduces to vanilla UCB with no benefit from offline data.

### Mechanism 2
- **Claim:** The min{UCB_t(a), UCB^S_t(a)} selection rule adaptively incorporates offline data only when beneficial.
- **Mechanism:** At each step, compute both vanilla UCB and warm-start UCB^S. The min operation automatically favors the tighter (lower) confidence bound. When offline data is unreliable, UCB^S_t(a) exceeds UCB_t(a) and the algorithm falls back to pure online learning.
- **Core assumption:** Both P^off and P^on are 1-subGaussian, enabling concentration inequalities for both offline and online samples.
- **Evidence anchors:** Shows that when N_t(a) exceeds the saving threshold, A_t ≠ a with certainty; dual-UCB min mechanism appears novel to this work.
- **Break condition:** When ω(a) ≥ Δ(a) for all sub-optimal arms, rad^S_t(a) remains inflated and min selects vanilla UCB for all arms.

### Mechanism 3
- **Claim:** The direction of bias (overestimation vs. underestimation of sub-optimal arms) affects regret improvement, not just bias magnitude.
- **Mechanism:** The saving term depends on ω(a) = V(a) + (μ^off(a) - μ^on(a)). When μ^off(a) < μ^on(a) (pessimistic estimate of sub-optimal arm), ω(a) decreases, making Sav_0(a) larger. This accelerates elimination of sub-optimal arms.
- **Core assumption:** The discrepancy direction is captured in ω(a) ∈ [0, 2V(a)], allowing asymmetric effects.
- **Evidence anchors:** "the performance depends not only on the value of V and discrepancy... but also on the direction of the discrepancy"; Optimistic bias (v ≥ 0.5) causes UCBS and MONUCB to fail; MIN-UCB adapts. Pessimistic bias maintains stable improvement for all v.
- **Break condition:** When offline data systematically overestimates sub-optimal arms and V is tight, the algorithm may still succeed, but looser V or larger bias magnitude causes fallback to vanilla UCB.

## Foundational Learning

**Concept:** Upper Confidence Bound (UCB) Principle
- **Why needed here:** MIN-UCB builds directly on vanilla UCB. The optimism-in-face-of-uncertainty principle underlies both UCB_t and UCB^S constructions.
- **Quick check question:** Why does UCB_t(a) = R̂_t(a) + √(2log(2t/δ_t)/N_t(a)) achieve O(∑_{a:Δ(a)>0} log(T)/Δ(a)) regret?

**Concept:** Stochastic Multi-armed Bandit Regret
- **Why needed here:** Need to understand instance-dependent bounds (involving Δ(a)) vs. instance-independent bounds (O(√(KT log T))). The paper provides tight bounds for both.
- **Quick check question:** What is the instance-independent (minimax) regret bound for vanilla UCB?

**Concept:** Combinatorial Bandits with Semi-bandit Feedback
- **Why needed here:** MIN-COMB-UCB extends to combinatorial settings. Need to understand base arms vs. actions, bounded smoothness, and (α, β)-optimization oracles.
- **Quick check question:** How does CombUCB1 achieve O(m√(KT log T)) regret for linear combinatorial bandits?

## Architecture Onboarding

**Component map:**
- Offline Data Store -> Dual UCB Calculator -> MIN Selector -> Statistics Updater
- Bias Bound Input -> Dual UCB Calculator -> MIN Selector
- Online Horizon T -> Statistics Updater -> MIN Selector

**Critical path:**
1. Initialize: Compute X̂(a) from offline data; pull each arm once
2. For t = K+1 to T: compute both UCBs, select via min, observe R_t, update
3. Saving realized when ω(a) < Δ(a) for sub-optimal arms

**Design tradeoffs:**
- **V conservatism vs. improvement:** Tighter V yields larger Sav_0(a); V = ∞ guarantees no harm but no gain
- **Uniform vs. heterogeneous TS(a):** Instance-independent bound uses τ* from LP (17); heterogeneous TS(a) can limit τ*
- **Confidence parameter δ_t:** Smaller δ_t increases exploration; paper uses δ_t = 1/(2Kt²)

**Failure signatures:**
- **Invalid V (too small):** If |μ^off(a) - μ^on(a)| > V(a), confidence intervals become unreliable; no correctness guarantee
- **Optimistic bias with tight deadline:** Large bias overestimating sub-optimal arms may cause persistent sub-optimal pulls if V is too small
- **Sparse offline data:** Small min_a TS(a) limits τ*, reducing instance-independent improvement

**First 3 experiments:**
1. **Bias magnitude sweep:** Fix TS = 1000, vary v ∈ {0.1, 0.2, ..., 1.0} with optimistic bias. Verify MIN-UCB transitions at ω(a) = Δ(a) (v = 0.5) from outperforming to matching PURE-UCB.
2. **Direction comparison:** Same |v| for optimistic vs. pessimistic bias. Confirm pessimistic bias maintains stable improvement; optimistic bias degrades as v increases.
3. **Offline data size effect:** Vary TS ∈ {100, 1000, 10000} with fixed v = 0.4. Confirm larger TS yields larger regret reduction when ω(a) < Δ(a).

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can the MIN-UCB framework be extended to online Markov Decision Processes (MDPs) and contextual bandits while retaining the regret benefits of bias correction?
- **Basis in paper:** [explicit] The authors state, "Interesting future directions include extending our framework to other online learning models, such as online Markov decision processes and contextual bandits."
- **Why unresolved:** The current theoretical analysis is restricted to multi-armed and combinatorial bandits, leaving the interaction between state transitions or contexts and biased offline data in more complex models unaddressed.
- **What evidence would resolve it:** Derivation of regret bounds for an MDP or contextual algorithm that adaptively utilizes potentially biased offline data.

**Open Question 2**
- **Question:** How does utilizing alternative metrics for distribution drift (other than valid bias bounds) affect the adaptive utilization of offline data?
- **Basis in paper:** [explicit] The authors note, "It is also intriguing to study our setting with other metrics on the distribution drift."
- **Why unresolved:** The paper exclusively relies on the valid bias bound V (a difference in means) as auxiliary input; the implications of using probabilistic divergences (e.g., KL divergence, Wasserstein distance) are not explored.
- **What evidence would resolve it:** An algorithm design and accompanying regret analysis that utilizes a different drift metric to determine when offline data is "sufficiently close" to the online distribution.

**Open Question 3**
- **Question:** What is the performance degradation of MIN-UCB when the provided bias bound V is invalid (underestimates the true distribution mismatch)?
- **Basis in paper:** [inferred] The paper assumes V(a) ≥ |μ^(off)(a) - μ^(on)(a)| is valid (Page 9). It lacks an analysis of robustness when this upper bound is misspecified.
- **Why unresolved:** If V is too small, the algorithm may incorrectly deem offline data "informative," potentially leading to linear regret rather than matching the vanilla UCB performance.
- **What evidence would resolve it:** A theoretical analysis or empirical study quantifying the regret penalty when the auxiliary input V is strictly smaller than the true bias.

## Limitations

- The algorithm's performance critically depends on the validity of the bias bound V(a). If the provided bounds are too conservative (V = ∞) or invalid (too small), the theoretical guarantees fail.
- The instance-independent regret bound improvement is limited by min_a TS(a), making the algorithm less effective when offline data is sparse for some arms.
- The combinatorial extension requires strong assumptions (bounded smoothness, optimization oracles) that may not hold in all practical settings.

## Confidence

- **High:** Core impossibility result (no improvement without bias bounds), basic MIN-UCB regret analysis for well-specified V
- **Medium:** Tight instance-dependent regret bound with heterogeneous TS(a), combinatorial extension claims
- **Low:** Practical performance with invalid or overly conservative V bounds

## Next Checks

1. Test MIN-UCB with intentionally invalid bias bounds (V(a) < |μ^off(a) - μ^on(a)|) to quantify failure modes and regret inflation
2. Implement the heterogeneous TS(a) case with varying offline sample sizes to verify the instance-independent bound tightness
3. Benchmark MIN-COMB-UCB on a concrete combinatorial setting (e.g., cascading bandits) with realistic oracle assumptions