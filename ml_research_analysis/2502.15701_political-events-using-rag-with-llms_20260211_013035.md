---
ver: rpa2
title: Political Events using RAG with LLMs
arxiv_id: '2502.15701'
source_url: https://arxiv.org/abs/2502.15701
tags:
- political
- events
- event
- system
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a RAG-based system for extracting political
  events from news articles using large language models (LLMs). The approach leverages
  RAG to augment LLMs with external data retrieval, enhancing contextual understanding
  and accuracy for domain-specific political event extraction tasks.
---

# Political Events using RAG with LLMs

## Quick Facts
- arXiv ID: 2502.15701
- Source URL: https://arxiv.org/abs/2502.15701
- Authors: Muhammad Arslan; Saba Munawar; Christophe Cruz
- Reference count: 40
- Primary result: RAG-based system achieves 0.87 accuracy in extracting political event properties from news articles

## Executive Summary
This paper presents a retrieval-augmented generation (RAG) system for extracting political events from news articles using the Llama2 large language model. The approach combines external document retrieval with LLM-based event extraction, achieving high accuracy in identifying actors, actions, recipients, and locations from political news. The system demonstrates improved efficiency over classical NLP techniques while maintaining credibility through source attribution. A manually curated test set of 50 health-related political events validated the approach's effectiveness.

## Method Summary
The system implements a RAG pipeline using Llama2 LLM with a VectorStoreIndex of pre-indexed news articles. User queries are embedded and matched against the index to retrieve relevant documents, which are then provided as context to Llama2 for generating structured event outputs. The extraction follows an 8-property schema (actor, action, recipient, instrument, reason, time, location, reporter) and leverages the LLM's pre-trained capabilities without requiring task-specific fine-tuning. The approach uses langchain for orchestration and HuggingFace for LLM access.

## Key Results
- Achieved 0.87 accuracy in identifying named entities and event attributes on a 50-event test set
- Successfully extracted structured political event properties including actors, actions, recipients, and locations
- Demonstrated improved efficiency over classical NLP techniques with dynamic dataset updates

## Why This Works (Mechanism)

### Mechanism 1: External Knowledge Augmentation via Retrieval
RAG improves political event extraction by dynamically retrieving relevant news articles to ground LLM responses in domain-specific context. User query embedding is matched against a VectorStoreIndex of pre-indexed news articles, with top-k relevant documents passed as context to Llama2 for structured event output generation. This assumes retrieved documents contain the information needed to answer the query and that semantic similarity correlates with information relevance.

### Mechanism 2: Schema-Constrained Event Extraction
LLMs can extract structured event properties without task-specific training when guided by explicit schema definitions and prompts. The system uses an 8-property schema (actor, action, recipient, instrument, reason, time, location, reporter) with system prompts and query wrapper prompts instructing Llama2 to identify and extract specific event properties. This assumes LLM pre-training includes sufficient exposure to political event descriptions and structured information extraction patterns.

### Mechanism 3: Pre-trained LLM as Generalist Extractor
Pre-trained LLMs reduce the need for domain-specific model training by leveraging generalized language understanding capabilities. Llama2's pre-trained weights provide baseline linguistic competence while RAG supplements domain knowledge without requiring fine-tuning on political corpora. This assumes general language competence transfers to political event extraction with minimal task-specific adaptation.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architectural pattern combining generative LLMs with external retrieval to address knowledge cutoffs and domain gaps
  - Quick check question: How does RAG differ from fine-tuning in how it updates a model's accessible knowledge?

- **Concept: Vector Embeddings and Similarity Search**
  - Why needed here: Enables semantic retrieval by converting text to dense vectors; query embedding matched against document embeddings via cosine similarity
  - Quick check question: What could cause a relevant document to receive a low similarity score to a query?

- **Concept: Event Schema Design for IE**
  - Why needed here: Defines the structured output format; the 8-property schema constrains LLM output to consistent, parseable event records
  - Quick check question: Which event properties are mandatory vs. optional in the schema described?

## Architecture Onboarding

- **Component map:** Data layer (News Category Dataset) -> Embedding layer (HuggingFaceEmbeddings) -> Index layer (VectorStoreIndex) -> LLM layer (Llama2) -> Orchestration layer (langchain + llama_index) -> Prompt layer (System prompts + query wrapper prompts)

- **Critical path:** 1. Load documents → 2. Generate embeddings → 3. Build VectorStoreIndex → 4. Query → 5. Retrieve relevant chunks → 6. LLM generates response with retrieved context → 7. Output structured event properties

- **Design tradeoffs:** Llama2 vs. other LLMs (accessibility vs. performance); dataset recency vs. availability (2020-2022 vs. fresher data); index size vs. latency (larger corpora improve recall but increase retrieval time); single-source vs. multi-source (news only vs. social media and government sources)

- **Failure signatures:** Slow queries on large indices (may require GPU scaling or approximate nearest neighbor search); low retrieval precision when query vocabulary mismatches corpus language; incomplete event extraction when source text lacks explicit actor/action mentions; cosine similarity threshold too strict, filtering out partially relevant documents

- **First 3 experiments:**
  1. Baseline replication: Recreate the 50-event evaluation using same dataset and Llama2; verify ~0.87 accuracy on named entity and attribute extraction
  2. Retrieval ablation: Log retrieved documents per query; measure retrieval precision independently from LLM extraction quality to isolate retrieval vs. generation failures
  3. Latency scaling test: Incrementally increase corpus size (e.g., 1K, 10K, 50K documents) and measure query latency; identify practical throughput limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-source RAG architectures be developed to integrate diverse political information channels beyond news articles?
- Basis in paper: The authors state that to achieve comprehensive insights, "multi-source RAG architectures need to be explored" to incorporate sources like social media and government websites
- Why unresolved: The current system relies exclusively on a static dataset of news articles, limiting the scope of extracted political events
- What evidence would resolve it: A modified system architecture successfully ingesting and synthesizing data from social media and government reports alongside news articles

### Open Question 2
- Question: How can real-time RAG solutions be implemented to allow analysts to observe political dynamics without manual intervention?
- Basis in paper: The paper notes that manual data scraping is suboptimal and "real-time RAG solutions should be investigated"
- Why unresolved: The existing implementation requires manual dataset updates, preventing the immediate analysis of unfolding events
- What evidence would resolve it: A streaming pipeline architecture that automatically updates the vector store with live news feeds

### Open Question 3
- Question: What computational strategies can mitigate slow response times when scaling RAG systems to extensive political datasets?
- Basis in paper: The authors highlight that "employing extensive datasets with RAG may lead to slower response times" and suggest further research is needed
- Why unresolved: The study utilized a small dataset, and the authors note that scaling hardware (GPUs) is costly for resource-limited organizations
- What evidence would resolve it: Benchmarks comparing indexing strategies or approximation methods that maintain high accuracy (0.87) while reducing retrieval latency on large corpora

## Limitations

- Reliance on news article corpora limits generalizability to other political event sources like social media or government documents
- 0.87 accuracy figure based on small, manually curated test set of only 50 health-related political events
- Evaluation lacks metrics for retrieval quality (precision/recall) and system latency measurements
- Does not address error analysis for cases where the system fails

## Confidence

- **High confidence**: The RAG architectural approach for augmenting LLMs with external retrieval is well-established and the mechanism is sound
- **Medium confidence**: The specific implementation using Llama2, news article dataset, and the 8-property schema is described clearly, but the evaluation evidence is limited by small sample size
- **Low confidence**: Claims about system benefits (efficiency, dynamic updates, credibility) are largely theoretical without empirical comparison to classical NLP techniques or detailed latency measurements

## Next Checks

1. **Retrieval Quality Isolation**: Log and analyze the retrieved documents for each query to measure retrieval precision and recall independently from LLM extraction quality, identifying whether failures stem from retrieval or generation

2. **Latency and Scalability Testing**: Systematically measure query response times across increasing corpus sizes (e.g., 1K, 10K, 50K documents) to establish practical throughput limits and identify scaling bottlenecks

3. **Cross-Domain Evaluation**: Test the system on political events from sources beyond news articles (e.g., social media, government reports) to assess generalizability and identify domain-specific failure patterns