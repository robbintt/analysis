---
ver: rpa2
title: 'LIFT: Interpretable truck driving risk prediction with literature-informed
  fine-tuned LLMs'
arxiv_id: '2510.22333'
source_url: https://arxiv.org/abs/2510.22333
tags:
- risk
- driving
- truck
- prediction
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel interpretable prediction framework
  using literature-informed fine-tuned large language models (LIFT LLM) for truck
  driving risk prediction. The framework integrates a literature processing pipeline
  that automatically constructs a domain knowledge base from 299 research papers and
  a fine-tuned LLM to achieve precise truck driving risk prediction and robust interpretation.
---

# LIFT: Interpretable truck driving risk prediction with literature-informed fine-tuned LLMs

## Quick Facts
- arXiv ID: 2510.22333
- Source URL: https://arxiv.org/abs/2510.22333
- Reference count: 15
- This study introduces a novel interpretable prediction framework using literature-informed fine-tuned large language models (LIFT LLM) for truck driving risk prediction.

## Executive Summary
This study introduces LIFT LLM, a novel interpretable prediction framework for truck driving risk prediction that integrates a literature processing pipeline and fine-tuned large language models. The framework automatically constructs a domain knowledge base from 299 research papers and achieves superior performance compared to benchmark models, with 26.7% higher recall and 10.1% higher F1-score. The system demonstrates its capability in identifying key variables and complex variable combinations contributing to high-risk scenarios, validated by PERMANOVA tests. The research highlights the potential of LIFT LLM in data-driven knowledge discovery and its applicability in truck safety management.

## Method Summary
The LIFT LLM framework consists of a literature processing pipeline that extracts domain knowledge from academic papers, converts structured trajectory and traffic data into hierarchical text prompts, and fine-tunes a Qwen2.5-7B-Instruct model using LoRA adaptation. The system employs a 1:1 train/test split with SMOTE balancing on a real-world truck driving risk dataset from Dongguan, China. The framework achieves trip-scale risk prediction through supervised fine-tuning on textualized data while maintaining interpretability through literature-informed knowledge bases.

## Key Results
- LIFT LLM achieved 26.7% higher recall and 10.1% higher F1-score compared to benchmark models
- The framework demonstrated superior capability in identifying key variables and complex variable combinations contributing to high-risk scenarios
- PERMANOVA tests validated the identified variable combinations with p-values < 0.001
- Literature-informed knowledge bases significantly improved interpretation stability compared to models without domain knowledge

## Why This Works (Mechanism)

### Mechanism 1
Supervised fine-tuning (SFT) with domain-specific textualization adapts general LLMs to trip-scale risk prediction. The framework converts structured trajectory and traffic data into hierarchical text prompts (System/User) and applies Low-Rank Adaptation (LoRA) to update model weights, minimizing negative log-likelihood on the risk classification task. This aligns the model's latent space with the specific distribution of truck driving risks. Core assumption: Tokenization of continuous numerical variables preserves semantic relationships required for risk inference. Evidence: Fine-tuned version achieves F1 0.76 vs 0.04 for not fine-tuned model. Break condition: If tokenization destroys magnitude information, SFT fails to converge on regression-like precision.

### Mechanism 2
Literature-informed knowledge bases stabilize interpretation against data distribution shifts. Instead of relying solely on current training batch distribution, the LIFT LLM retrieves static knowledge derived from 299 academic papers, injecting fixed causal priors into the prompt context. This grounds the model's reasoning in established theory rather than immediate sample noise. Core assumption: Causal factors identified in historical literature remain valid for the specific operational design domain. Evidence: Removing literature knowledge base causes constant, narrow explanations; including it broadens and stabilizes factor identification. Break condition: If literature contains conflicting or outdated causal claims, "stability" becomes consistent hallucination or bias.

### Mechanism 3
Semantic reasoning enables discovery of complex, non-linear variable combinations (scenarios). The LLM processes input variables as semantic entities and, by querying the model to identify "key combinations" in natural language, can propose multi-factor interaction effects that standard feature importance methods might miss. Core assumption: LLM's internal representation of "risk" allows synthesis of variable interactions that mimic human expert reasoning. Evidence: LLM identifies combinations validated by PERMANOVA tests with p-values < 0.001. Break condition: If LLM hallucinates statistically non-existent variable combinations, validation step must reject them.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Full fine-tuning of 7B+ parameter model is computationally expensive and prone to catastrophic forgetting. LoRA allows learning specific "dialect" of truck risk data without overwriting base reasoning skills.
  - Quick check question: Can you explain why we update the delta-weight matrix (ΔW) as a product of two smaller matrices (B and A) rather than updating the full weight matrix W directly?

- **Concept: PERMANOVA (Permutational Multivariate Analysis of Variance)**
  - Why needed here: Used to validate "discovered" scenarios. Unlike standard ANOVA, PERMANOVA works on distance matrices and doesn't assume normal distribution, crucial for validating complex, non-linear clusters of risk factors.
  - Quick check question: Why is a permutation-based test necessary when comparing centroids of "risk" vs. "non-risk" groups in high-dimensional variable space?

- **Concept: Prompt Engineering (System vs. User Prompts)**
  - Why needed here: Architecture relies on caching "System Prompt" containing heavy literature knowledge base to optimize inference speed. Separating static knowledge (System) from dynamic data (User) is critical for system's throughput.
  - Quick check question: In the KV-cache of a transformer, why does separating immutable system prompt from variable user prompt improve inference efficiency during batch processing?

## Architecture Onboarding

- **Component map:** Data Processor -> Literature Processing Pipeline -> Inference Core (LIFT LLM) -> Result Evaluator
- **Critical path:** The Literature Processing Pipeline is the most novel and fragile component. If PDF-to-Markdown conversion fails to parse tables or filtering LLM rejects relevant papers, resulting Knowledge Base will be incomplete, crippling model's interpretability.
- **Design tradeoffs:**
  - Recall vs. Precision: LIFT LLM optimizes for high Recall (0.95) at expense of Precision (0.64), deliberate choice for safety-critical systems where missing risk is worse than false alarm
  - Generalization vs. Specificity: Automated KB allows generalization across domains but requires significant upfront processing (299 papers) compared to hard-coded expert rules
- **Failure signatures:**
  - Constant Explanations: Model outputs "s_std_s" as cause for every crash → Literature Knowledge Base likely failed to load or was omitted
  - Low Recall (<0.5): Model fails to predict risks → Check SMOTE balancing or LoRA rank settings
  - Hallucinated Variables: Model invents factors not in input data → Temperature likely too high or system prompt constraints loose
- **First 3 experiments:**
  1. Ablation on Context: Run Inference Core with empty Knowledge Base vs. full 137-paper KB to measure delta in explanation diversity and stability
  2. Temperature Sensitivity Sweep: Test Task 2 interpretation at τ=0.01, 0.5, 1.0 to quantify trade-off between creativity and consistency
  3. Baseline Race: Train Random Forest on exact same dataset and compare Feature Importance rankings against LIFT LLM's frequency-based rankings

## Open Questions the Paper Calls Out

### Open Question 1
Does pretraining an LLM exclusively on domain-specific literature yield superior interpretability and performance compared to the LIFT framework's fine-tuning approach? The paper notes this could be explored but remains untested due to computational costs. Evidence would require comparative study benchmarking domain-pretrained LLM against LIFT framework on same dataset using F1-score and explanation consistency metrics.

### Open Question 2
How can the rate of hallucinations in risk explanation outputs be quantitatively measured and minimized in safety-critical prediction tasks? While the study uses literature knowledge base and low temperature settings to mitigate randomness, it lacks specific metric to quantify factual accuracy of generated natural language explanations. Evidence would require development of "hallucination score" metric based on factual alignment with provided knowledge base.

### Open Question 3
What is the optimal structure and composition for the literature knowledge base to maximize prediction accuracy and explanation robustness? The current pipeline uses JSON summary of 137 academic papers, but impact of including non-academic industry data or altering structural format is unknown. Evidence would require ablation studies testing model performance when knowledge base is augmented with industry logs or restructured using knowledge graphs.

## Limitations
- Reliance on automated literature processing may introduce selection bias or miss critical domain knowledge if filtering LLM incorrectly rejects relevant papers
- 1:1 train/test split with SMOTE balancing may overestimate performance on imbalanced real-world data
- Domain specificity to truck driving in Dongguan, China limits generalizability to different traffic cultures or vehicle types

## Confidence

- **High Confidence:** Task 1 performance metrics (recall, F1-score) - well-defined ground truth, standard evaluation
- **Medium Confidence:** Literature processing pipeline effectiveness - automated filtering step not fully validated
- **Medium Confidence:** PERMANOVA validation of variable combinations - appropriate statistical method but complex interaction interpretation
- **Low Confidence:** Generalization to other domains - single dataset, specific geographic context

## Next Checks

1. Test the literature processing pipeline on a held-out set of papers to measure precision/recall of relevant paper identification
2. Evaluate model performance on a naturally imbalanced test set (without SMOTE) to assess real-world applicability
3. Apply the framework to a different ODD (e.g., highway vs. urban, different country) to test domain transfer capability