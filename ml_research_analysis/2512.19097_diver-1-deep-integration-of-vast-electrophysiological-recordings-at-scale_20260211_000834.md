---
ver: rpa2
title: 'DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale'
arxiv_id: '2512.19097'
source_url: https://arxiv.org/abs/2512.19097
tags:
- diver
- ieeg
- scaling
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic scaling law analysis for
  electrophysiology foundation models (EFMs), addressing the challenge of scaling
  EEG and iEEG models in data-constrained regimes. The authors introduce DIVER-1,
  a family of models trained on the largest and most diverse corpus to date (59.3k
  hours from 17.7k+ subjects) with up to 1.82B parameters.
---

# DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale

## Quick Facts
- **arXiv ID**: 2512.19097
- **Source URL**: https://arxiv.org/abs/2512.19097
- **Reference count**: 40
- **Primary result**: First systematic scaling law analysis for electrophysiology foundation models, showing data-constrained regimes favor smaller models trained longer

## Executive Summary
This paper presents the first systematic scaling law analysis for electrophysiology foundation models (EFMs), addressing the challenge of scaling EEG and iEEG models in data-constrained regimes. The authors introduce DIVER-1, a family of models trained on the largest and most diverse corpus to date (59.3k hours from 17.7k+ subjects) with up to 1.82B parameters. Through controlled experiments, they discover that unlike language models, EFM performance is dominated by data scale and training duration rather than model size under fixed compute budgets - smaller models trained longer outperform larger models trained briefly. Guided by these insights, DIVER-1 achieves state-of-the-art performance across iEEG and EEG benchmarks, establishing a concrete scaling playbook for future EFM development.

## Method Summary
DIVER-1 is a family of masked autoencoder foundation models for electrophysiology data that operate on raw EEG/iEEG signals. The models use Spatio-Temporal Conditional Positional Encoding (STCPE) to handle heterogeneous channel configurations, Any-variate Attention mechanisms, and multi-domain reconstruction objectives (Time, FFT, STFT). The models are trained on 59.3k hours from 17.7k+ subjects across diverse datasets including sleep, emotion, seizure detection, and general EEG/iEEG recordings.

## Key Results
- Scaling analysis reveals EFM performance is dominated by data scale and training duration rather than model size under fixed compute budgets
- Smaller models trained for extended epochs consistently outperform larger models trained briefly
- DIVER-1 achieves state-of-the-art performance across iEEG and EEG benchmarks
- Data-constrained regime requires tailored scaling strategies rather than direct application of language model scaling laws

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: In data-constrained electrophysiology regimes, allocating compute to extended training (epochs) on smaller models yields better downstream performance than training large models briefly.
- **Mechanism**: The authors argue that Ephys data has limited supply of "unique tokens" ($U_D$) relative to potential model capacity. Repeated passes over fixed data allow extraction of diminishing but non-zero signal returns. Ephys models hit a "half-life" of parameter utility ($R^*_N$) faster than language models, meaning larger models overfit without sufficient data diversity.
- **Core assumption**: Intrinsic dimensionality of Ephys data manifold is low enough that smaller models can capture signal if they see data enough times to overcome noise and subject variance.
- **Evidence anchors**: [Section 4.1] "EFMs exhibit smaller $R^*_N$ values... compared to language models"; [Abstract] "smaller models trained for extended epochs consistently outperform larger models trained briefly"; [Corpus: BaRISTA] Challenge of generalizing across subjects supports need for data diversity/iteration over raw capacity.

### Mechanism 2
- **Claim**: Spatio-Temporal Conditional Positional Encoding (STCPE) allows handling heterogeneous, variable-channel electrode configurations better than fixed positional encodings.
- **Mechanism**: Standard positional encodings rely on fixed channel axes, breaking permutation equivariance. STCPE uses sliding window of local patches processed by lightweight transformer (MOIRAI block) to generate positional embeddings dynamically based on content and local context, ensuring representation invariance to arbitrary electrode ordering.
- **Core assumption**: Local spatiotemporal context provides sufficient information to infer relative position without global indices.
- **Evidence anchors**: [Section 2.2] "STCPE replaces convolution... computing channel-permutation-equivariant positional encodings"; [Section 4.3] "ablation of STCPE... induced performance degradation"; [Corpus: Multi-variate parallel attention] Shift toward mechanisms handling heterogeneous channel configurations.

### Mechanism 3
- **Claim**: Multi-domain reconstruction (Time, FFT, STFT) improves learned representations by forcing encoder to model spectral features explicitly.
- **Mechanism**: Standard masked autoencoding focuses on pixel/time-point reconstruction. Adding reconstruction heads for FFT and STFT forces model to learn filters and attention patterns capturing frequency dynamics alongside raw voltage fluctuations, acting as data augmentation and regularization.
- **Core assumption**: Frequency domain characteristics are essential for downstream tasks targeted by this model.
- **Evidence anchors**: [Section 2.4] "Multi-domain reconstruction objective encourages comprehensive learning of temporal and spectral properties"; [Table 18] Ablation study shows performance drop when removing FFT/STFT reconstruction.

## Foundational Learning

**Data-Constrained Scaling Laws (Muennighoff et al.)**
- **Why needed here**: Paper relies on this framework to derive "compute-optimal frontier." Must understand how "repeated data" ($R_D$) interacts with "unique tokens" ($U_D$) to predict loss.
- **Quick check question**: Given a fixed dataset, does doubling model size or doubling epochs typically reduce loss more in DIVER-1 regime? (Answer: Doubling epochs)

**Rotary Position Embeddings (RoPE)**
- **Why needed here**: DIVER-1 uses RoPE within "Any-variate attention" to handle temporal relationships and relative distances, crucial for STCPE logic.
- **Quick check question**: Why does RoPE generalize to variable sequence lengths better than absolute positional encodings?

**Masked Autoencoders (MAE)**
- **Why needed here**: Pretraining objective is 50% random patch masking. Understanding encoder-decoder split is necessary to implement training loop.
- **Quick check question**: In standard Vision Transformer MAE, why do we only encode visible patches? (DIVER-1 processes whole grid but uses registers/masks; checking specific implementation is key)

## Architecture Onboarding

**Component map**: Raw Signal -> Patchify (1s or 0.1s) -> CNN Patch Encoder -> Add Spectral Embedding (FFT of patch) + Position/Modality Embedding + STCPE -> MOIRAI Blocks (Transformer) with Any-variate Attention (RoPE + bias) + Spatio-temporal Registers -> Multi-output Projection (Linear layers to Raw, FFT, STFT)

**Critical path**: STCPE is most complex integration point. It sits before main transformer encoder but uses lightweight MOIRAI block internally. Ensuring sliding window operation is memory-efficient is key.

**Design tradeoffs**:
- **Patch Size (1s vs 0.1s)**: 0.1s preserves high-frequency resolution but increases sequence length (compute); 1s is efficient but may blur transient spikes. Paper uses different loss weights ($\lambda$) for each.
- **Input Resampling**: During training, randomly sample $C' \le 32$ channels and $N' \le 30$ patches. This prevents overfitting to specific montage layouts but requires careful data loader engineering.

**Failure signatures**:
- **Loss Plateau at Low Values**: If training large model (e.g., 1.8B params) on limited data, loss may drop fast then stagnate while smaller model continues improving. Fix: Reduce model size.
- **Channel Permutation Sensitivity**: If results change significantly when shuffling channel order in input array, STCPE is not functioning or was disabled.

**First 3 experiments**:
1. **Scaling Sweep**: Train "Tiny" (13M) and "Small" (50M) model on 10% data subset. Plot loss vs. FLOPs to verify "smaller-longer" trend locally before full training.
2. **Ablation of STCPE**: Compare full DIVER-1 vs. version replacing STCPE with standard absolute positional encodings on subject-independent cross-validation split.
3. **Modality Transfer**: Pretrain on EEG only, then freeze encoder and linear-probe on iEEG (and vice versa) to test if "Any-variate" attention truly bridges modality gap.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question**: Do data-constrained scaling laws identified for EEG and iEEG generalize to other neural recording modalities such as MEG or fMRI?
- **Basis in paper**: [explicit] Impact Statement states: "Our conclusions are specific to EEG and iEEG under modeling and training settings studied here, and scaling behavior may differ for other modalities such as MEG or fMRI."
- **Why unresolved**: This study systematically characterized scaling only for EEG and iEEG; other modalities have different signal characteristics, temporal resolutions, and data availability constraints.
- **What evidence would resolve it**: Controlled scaling experiments across compute, data, and model size dimensions applied to MEG and fMRI datasets, comparing fitted scaling law parameters against those reported for EEG/iEEG.

**Open Question 2**
- **Question**: What causes substantially different scaling law exponents between EEG (α≈3.45) and iEEG (α≈0.38), and does this reflect fundamental differences in signal structure or experimental design artifacts?
- **Basis in paper**: [inferred] Appendix C.5 reports EEG model-size exponent α=3.4480 compared to iEEG's α=0.3773, with authors noting this is "unusually large" and partially attributing it to XXL model overfitting and fixed 32-epoch training procedure.
- **Why unresolved**: EEG scaling law fit was notably weaker (R²=0.50 vs 0.82 for iEEG), and multiple potential causes were proposed but not disentangled: signal quality differences, trial-to-trial variability, and methodological differences in how epoch losses were sampled.
- **What evidence would resolve it**: Re-running EEG scaling experiments with per-epoch model training (matching iEEG methodology) and systematically varying signal quality through controlled noise injection to isolate contribution of each factor.

**Open Question 3**
- **Question**: How does compute-optimal frontier shift when jointly pretraining on multiple modalities (EEG + iEEG) versus single-modality training?
- **Basis in paper**: [inferred] Appendix D.4 shows joint EEG+iEEG pretraining underperforms EEG-only at 2 epochs but outperforms at 16 epochs, suggesting modality heterogeneity creates initial impediment requiring extended training to overcome - but this interaction with scaling frontier was not quantified.
- **Why unresolved**: Scaling law experiments were conducted separately for each modality; joint training introduces additional complexity in how compute should be allocated.
- **What evidence would resolve it**: Fitting data-constrained scaling laws on joint-modality models across varying epoch counts and model sizes to derive multi-modal compute-optimal frontier.

**Open Question 4**
- **Question**: What is optimal aspect ratio for very large EFM models, and does instability observed in 1.83B parameter model stem from violating aspect ratio stability constraints?
- **Basis in paper**: [explicit] Appendix C.2 states: "Another possibility is that aspect ratio of DIVER XXL/I/1s (256) places it outside region where loss remains stable. Future work should therefore evaluate larger models within aspect-ratio regime where stable loss behavior is maintained."
- **Why unresolved**: XXL model exhibited higher loss at low epoch counts than smaller models, but whether this is due to insufficient training, aspect ratio, or fundamental scaling limits remains unclear.
- **What evidence would resolve it**: Systematic evaluation of large models with varying depth-to-width ratios while controlling for parameter count, measuring loss stability across training.

## Limitations

**Data Scaling Ceiling**: Scaling laws derived from 59.3k hours across 17.7k subjects; cannot predict transition threshold to model-size-dominated behavior without substantially larger datasets.

**Task Representativeness**: State-of-the-art results across 8 benchmarks may not span full diversity of real-world clinical applications; performance on seizure detection and emotion classification may not generalize to other neurological conditions.

**Generalization to Novel Montages**: Though STCPE handles channel permutation, performance on completely novel electrode configurations beyond 17.7k+ subjects' montages remains untested; "any-variate" attention mechanism's robustness to extreme montage variations is theoretical.

## Confidence

**High Confidence**: Core empirical finding that smaller models trained longer outperform larger models trained briefly on fixed compute budgets is well-supported by controlled experiments (Section 4.1). Ablation studies for STCPE and multi-domain reconstruction show clear, reproducible performance impacts.

**Medium Confidence**: Scaling law analysis and data-constrained regime characterization are internally consistent but rely on assumptions about relationship between unique tokens and model capacity; transition points identified may shift with different data distributions or task sets.

**Low Confidence**: Theoretical mechanism explaining why Ephys data is inherently more "data-constrained" than text requires further validation; claim about limited "unique tokens" is plausible but not directly measured or proven across diverse electrophysiological datasets.

## Next Checks

1. **Scaling Law Extrapolation Test**: Train DIVER-1 variants on datasets of 100k+ hours (when available) to empirically determine whether data-constrained scaling laws persist or transition to model-size-dominated behavior. Measure loss curves and downstream task performance across 3x and 10x data scale increases.

2. **Montage Generalization Benchmark**: Create systematic evaluation suite testing model on electrode configurations not seen during pretraining - including novel channel counts, spatial arrangements, and bipolar vs. referential montages. Compare performance degradation against models using standard positional encodings.

3. **Computational Efficiency Analysis**: Implement resource-constrained version of DIVER-1 that removes either STCPE or multi-domain reconstruction. Measure trade-off between inference speed, memory usage, and task performance across edge devices (CPU-only) and clinical deployment scenarios.