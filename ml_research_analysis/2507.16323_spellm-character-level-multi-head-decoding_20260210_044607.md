---
ver: rpa2
title: 'SpeLLM: Character-Level Multi-Head Decoding'
arxiv_id: '2507.16323'
source_url: https://arxiv.org/abs/2507.16323
tags:
- spellm
- arxiv
- output
- token
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpeLLM, a method that replaces standard token-level
  output prediction in LLMs with parallel character-level predictions using multiple
  independent linear heads. This approach decouples input and output vocabularies,
  allowing for large input vocabularies without increasing output computational cost.
---

# SpeLLM: Character-Level Multi-Head Decoding

## Quick Facts
- arXiv ID: 2507.16323
- Source URL: https://arxiv.org/abs/2507.16323
- Reference count: 40
- Replaces token-level prediction with parallel character-level predictions using multiple independent linear heads

## Executive Summary
SpeLLM introduces a novel approach to large language model decoding by replacing standard token-level output prediction with parallel character-level predictions using multiple independent linear heads. This architecture decouples input and output vocabularies, allowing for large input vocabularies without increasing computational cost. The method employs a self-distillation approach to convert existing LLMs to SpeLLM by training only new character-level heads while fine-tuning the last few transformer layers. Experimental results demonstrate competitive performance on downstream tasks while achieving runtime improvements.

## Method Summary
SpeLLM modifies the standard LLM decoding approach by implementing character-level predictions instead of token-level predictions. The architecture uses multiple independent linear heads to predict characters in parallel, which allows for decoupling of input and output vocabularies. This decoupling enables the use of large input vocabularies without incurring additional computational overhead. The conversion process employs a self-distillation approach where existing LLMs are transformed into SpeLLM models by training only the new character-level heads while fine-tuning the last few transformer layers.

## Key Results
- Achieves competitive performance on downstream tasks (BoolQ, ARC-Easy, GSM8K, CNN/Daily Mail) with four LLMs (Llama3.2-3B, Llama3-8B, Gemma2-2B, and Gemma2-9B)
- Reduces runtime by 5.1% on average across evaluated models
- Matches at least partially 94.89% of tokens without AutoCorrect and 97.57% with it
- Shows potential benefits for underrepresented languages by reducing dependency on large token vocabularies

## Why This Works (Mechanism)
The decoupling of input and output vocabularies through character-level prediction allows SpeLLM to maintain computational efficiency while handling diverse linguistic inputs. Multiple independent linear heads enable parallel character prediction, which reduces sequential dependencies during decoding. The self-distillation approach leverages existing LLM knowledge while adapting the output layer for character-level prediction, preserving learned representations while enabling the new decoding strategy.

## Foundational Learning
- **Character-level vs token-level prediction**: Character-level prediction provides finer granularity than token-level prediction, allowing more precise control over output generation and reducing vocabulary size requirements. Quick check: Compare vocabulary sizes between token-based and character-based approaches.
- **Multi-head decoding architecture**: Multiple independent linear heads enable parallel prediction of different output positions, reducing sequential dependencies and improving computational efficiency. Quick check: Measure speedup gains from parallelization versus single-head approaches.
- **Self-distillation in model conversion**: Self-distillation allows knowledge transfer from pre-trained token-level models to character-level models by using the original model's outputs as soft targets. Quick check: Verify knowledge preservation during conversion through performance comparison.
- **Vocabulary decoupling**: Separating input and output vocabularies enables flexible handling of diverse languages without computational penalties. Quick check: Test performance across languages with varying morphological complexity.
- **AutoCorrect integration**: AutoCorrect improves token matching accuracy by correcting character-level predictions, enhancing overall output quality. Quick check: Compare token matching rates with and without AutoCorrect.
- **Transformer layer fine-tuning**: Fine-tuning only the last few transformer layers preserves most learned representations while adapting the model to character-level prediction. Quick check: Analyze performance changes when varying the number of fine-tuned layers.

## Architecture Onboarding

Component map: Input sequence -> Transformer layers (partially fine-tuned) -> Multiple independent linear heads -> Character-level predictions -> AutoCorrect (optional) -> Output tokens

Critical path: Input tokens pass through the transformer layers, where the last few layers are fine-tuned. The output from these layers is then processed by multiple independent linear heads that predict characters in parallel. Optional AutoCorrect processing can be applied to improve output quality.

Design tradeoffs: The primary tradeoff involves balancing the benefits of character-level prediction (vocabulary independence, precision) against potential overhead from multiple heads and the complexity of the conversion process. The architecture favors flexibility and efficiency over simplicity.

Failure signatures: Potential failures include degradation in output quality if the fine-tuning process is insufficient, performance bottlenecks if the multiple heads cannot effectively parallelize, and conversion failures if the self-distillation process doesn't properly transfer knowledge from the original model.

Three first experiments:
1. Baseline comparison: Evaluate standard token-level LLM performance on downstream tasks without any modifications
2. Single-head character prediction: Implement a simplified version with only one character prediction head to establish baseline character-level performance
3. Ablation study: Test SpeLLM with varying numbers of heads (2, 4, 8) to determine optimal parallelization level

## Open Questions the Paper Calls Out
None

## Limitations
- Runtime improvements limited to evaluation on four relatively small models (3B-9B parameters), with uncertainty about scalability to larger models
- Lack of ablation studies prevents clear understanding of individual component contributions to overall performance
- Evaluation focused primarily on English-language tasks, leaving theoretical claims about benefits for underrepresented languages unverified

## Confidence
- **High**: Competitive performance on evaluated downstream tasks (directly measured and reported)
- **Medium**: Runtime improvement claim (5.1% reduction demonstrated but limited to small model set)
- **Low**: Benefits for underrepresented languages (supported only by theoretical arguments, not empirical evidence)

## Next Checks
1. Evaluate runtime improvements across a broader range of model sizes, particularly large-scale models (>30B parameters)
2. Conduct ablation studies to isolate the contributions of multiple heads versus character-level prediction
3. Test SpeLLM's performance and vocabulary benefits on non-English languages with complex morphology or limited training data