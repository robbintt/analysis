---
ver: rpa2
title: Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams
arxiv_id: '2512.14989'
source_url: https://arxiv.org/abs/2512.14989
tags:
- national
- reasoning
- chemistry
- local
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 40 multimodal large language models on a benchmark
  of Olympiad-style chemistry problems, USNCO-V, revealing significant limitations
  in scientific reasoning despite advances in generic multimodal understanding. Chain-of-Thought
  prompting markedly improves accuracy and interpretability, shifting models from
  pattern matching to structured, comparative reasoning.
---

# Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams

## Quick Facts
- arXiv ID: 2512.14989
- Source URL: https://arxiv.org/abs/2512.14989
- Reference count: 40
- 40 multimodal large language models evaluated on Olympiad-style chemistry problems, revealing significant limitations in scientific reasoning despite advances in generic multimodal understanding

## Executive Summary
This study presents a comprehensive evaluation of 40 multimodal large language models on USNCO-V, a benchmark of Olympiad-style chemistry problems. The research reveals a significant gap between models' generic multimodal understanding capabilities and their ability to perform scientific reasoning in chemistry contexts. Chain-of-Thought prompting emerges as a critical technique, substantially improving both accuracy and interpretability by shifting models from pattern matching to structured, comparative reasoning approaches.

The study's ablation analyses reveal that visual input can sometimes degrade performance, indicating fundamental challenges in vision-language integration for scientific tasks. Through occlusion-based saliency analysis, the researchers demonstrate that CoT prompting leads to more systematic attention to chemically relevant features. These findings highlight the need for more sophisticated multimodal reasoning strategies in scientific AI development and suggest that current models, while proficient at generic understanding, struggle with the structured reasoning required for complex scientific problem-solving.

## Method Summary
The study evaluates 40 multimodal large language models on USNCO-V, a benchmark comprising 118 Olympiad-style chemistry problems with accompanying visual diagrams. Models are tested across various prompting strategies including Chain-of-Thought (CoT) and few-shot learning. Performance is measured through accuracy metrics, while interpretability is assessed through occlusion-based saliency analysis to understand feature attention patterns. The research includes systematic ablation studies to isolate the effects of visual input and prompting strategies on model performance.

## Key Results
- Chain-of-Thought prompting significantly improves accuracy and interpretability across tested models
- Visual input can sometimes degrade performance, indicating misaligned vision-language integration
- Smaller models benefit from few-shot prompting while mid-tier models gain most from CoT scaffolding
- Occlusion analysis shows CoT prompts models to attend to chemically relevant features more systematically

## Why This Works (Mechanism)
Chain-of-Thought prompting works by providing structured reasoning frameworks that guide models through step-by-step problem-solving processes. This scaffolding helps models move beyond pattern matching to engage in comparative and analytical reasoning required for scientific problems. The mechanism appears to activate different cognitive pathways in the model, enabling more systematic feature attention and reasoning about chemical relationships rather than surface-level pattern recognition.

## Foundational Learning
- **Multimodal Learning**: Understanding how models integrate visual and textual information is crucial for scientific reasoning tasks. Quick check: Can the model correctly interpret chemical diagrams without accompanying text?
- **Chain-of-Thought Prompting**: This technique structures reasoning processes, essential for complex problem-solving. Quick check: Does the model show improved step-by-step reasoning when prompted?
- **Vision-Language Integration**: The ability to combine visual and linguistic understanding is fundamental for scientific tasks. Quick check: How does performance change when visual inputs are added or removed?
- **Scientific Reasoning**: Moving beyond pattern matching to genuine analytical thinking is critical for chemistry problems. Quick check: Can the model explain reasoning rather than just provide answers?

## Architecture Onboarding
**Component Map**: Vision Encoder -> Language Model -> Reasoning Module -> Output
**Critical Path**: Visual Input → Vision Encoder → Feature Fusion → Language Model Processing → Chain-of-Thought Reasoning → Answer Generation
**Design Tradeoffs**: Vision encoders optimized for generic understanding may lack chemical domain specificity; larger models show diminishing returns on scientific reasoning tasks; prompt engineering can compensate for architectural limitations
**Failure Signatures**: Performance degradation with visual input suggests fusion mechanism issues; pattern matching without reasoning indicates insufficient scaffolding; attention concentrated on irrelevant features suggests poor feature integration
**First Experiments**:
1. Test model performance with and without visual inputs to isolate vision-language integration effects
2. Compare Chain-of-Thought versus direct answering approaches across model sizes
3. Apply occlusion analysis to understand feature attention patterns and reasoning quality

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark representativeness limited to single chemistry olympiad format
- Unclear generalizability to other scientific domains and problem types
- Causal mechanisms behind vision-language integration failures remain unexplained

## Confidence
**High Confidence**:
- Chain-of-Thought prompting consistently improves both accuracy and interpretability across tested models
- Significant performance gaps exist between generic multimodal understanding and scientific reasoning capabilities
- Smaller models benefit from few-shot prompting while mid-tier models gain most from CoT scaffolding

**Medium Confidence**:
- Visual input can hinder performance in certain configurations
- CoT prompts lead to more systematic attention to chemically relevant features
- Current models exhibit pattern matching rather than true scientific reasoning

**Low Confidence**:
- Generalizability of findings to other scientific domains
- Causal mechanisms behind vision-language integration failures
- Relationship between attention patterns and reasoning quality

## Next Checks
1. **Cross-Domain Validation**: Test the same models and prompting strategies on multimodal problems from physics and biology olympiads to assess generalizability of the performance patterns observed in chemistry.

2. **Ablation Architecture Study**: Systematically vary vision encoder architectures and fusion mechanisms while holding prompting strategies constant to isolate whether performance degradation stems from architectural limitations versus training data gaps.

3. **Reasoning Quality Assessment**: Develop and apply fine-grained rubrics to distinguish between correct answers achieved through genuine scientific reasoning versus pattern matching, validating the claim that CoT shifts models toward structured reasoning.