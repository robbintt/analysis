---
ver: rpa2
title: 'Beyond Transcription: Mechanistic Interpretability in ASR'
arxiv_id: '2508.15882'
source_url: https://arxiv.org/abs/2508.15882
tags:
- layer
- whisper
- layers
- arxiv
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work applies interpretability methods to modern ASR systems,
  revealing internal mechanisms behind hallucinations, repetitions, and contextual
  errors. Using logit lens, linear probing, and activation patching adapted from LLM
  analysis, the authors show that: (1) decoder residual streams encode hallucination
  signals with up to 93.4% accuracy; (2) encoder layers capture speaker gender (94.6%),
  acoustic conditions (90%), and semantic categories (up to 97% per pair); (3) repetition
  hallucinations localize to specific cross-attention components, with one head in
  layer 18 suppressing 78.1% of cases; (4) encoders contribute semantic bias, not
  just acoustic processing.'
---

# Beyond Transcription: Mechanistic Interpretability in ASR

## Quick Facts
- arXiv ID: 2508.15882
- Source URL: https://arxiv.org/abs/2508.15882
- Reference count: 40
- This work applies interpretability methods to modern ASR systems, revealing internal mechanisms behind hallucinations, repetitions, and contextual errors

## Executive Summary
This study pioneers the application of mechanistic interpretability methods to Automatic Speech Recognition (ASR) systems, adapting techniques from large language model analysis to uncover internal representations beyond transcription. The authors demonstrate that ASR models encode rich semantic and contextual information throughout their architecture, not just acoustic features. Using adapted LLM interpretability tools including logit lens, linear probing, and activation patching, they reveal specific mechanisms underlying hallucinations, repetitions, and contextual errors. The findings show that encoder layers contribute semantic bias rather than purely acoustic processing, while decoder components actively suppress hallucinations through targeted attention mechanisms.

## Method Summary
The authors applied three core interpretability techniques from LLM analysis to ASR: logit lens examines the evolution of token predictions across decoder layers to identify hallucination signals; linear probing uses frozen linear classifiers to detect what features are encoded in different layers; and activation patching measures the causal contribution of specific components by swapping activations between clean and corrupted inputs. These methods were adapted to handle speech-token representations, with logit lens applied to decoder outputs, linear probing conducted on encoder and decoder residual streams, and activation patching performed on cross-attention mechanisms to identify hallucination suppression pathways.

## Key Results
- Decoder residual streams encode hallucination signals with up to 93.4% accuracy detection
- Encoder layers capture speaker gender (94.6%), acoustic conditions (90%), and semantic categories (up to 97% per pair)
- Repetition hallucinations localize to specific cross-attention components, with one head in layer 18 suppressing 78.1% of cases
- Encoder layers generate grammatically coherent but ungrounded text, while final layers produce accurate transcriptions

## Why This Works (Mechanism)
The ASR models learn hierarchical representations where early layers capture acoustic features while deeper layers extract semantic and contextual information. Cross-attention mechanisms in the decoder can both generate and suppress hallucinations by attending to different context windows. The encoder-decoder architecture allows semantic information to flow bidirectionally, with encoders contributing contextual bias and decoders refining predictions through attention mechanisms. Logit lens reveals that intermediate layers produce plausible but ungrounded text, explaining how hallucinations emerge during decoding.

## Foundational Learning
- Logit lens: Projects residual streams to token space at each layer to track prediction evolution - needed to identify when hallucinations emerge during decoding; quick check: verify predictions stabilize by final layer
- Linear probing: Trains frozen classifiers on intermediate representations to detect encoded features - needed to map what information is available at each layer; quick check: accuracy above chance indicates feature presence
- Activation patching: Swaps activations between clean and corrupted examples to measure causal contributions - needed to identify which components actively suppress hallucinations; quick check: performance drop indicates causal role
- Cross-attention mechanisms: Allow decoder to attend to encoder outputs for context - needed to understand how semantic information flows and hallucinations are controlled; quick check: attention weights show which encoder positions are referenced
- Residual streams: Carry information through transformer layers - needed to understand information flow and feature extraction; quick check: dimensionality matches token vocabulary

## Architecture Onboarding
Component map: Speech signal -> Encoder (acoustic features) -> Cross-attention -> Decoder (semantic refinement) -> Output
Critical path: Input speech -> Encoder layers 1-18 -> Cross-attention in decoder layers -> Output predictions
Design tradeoffs: End-to-end vs hybrid architectures affect interpretability accessibility; attention mechanisms enable both hallucination generation and suppression
Failure signatures: Hallucinations appear in intermediate layers via logit lens; repetitions localize to specific cross-attention heads; semantic errors correlate with encoder bias
First experiments: 1) Apply logit lens to clean vs hallucinated examples, 2) Linear probe encoder layers for speaker attributes, 3) Activation patch cross-attention heads in repetition cases

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Adaptation of LLM interpretability methods to ASR architecture lacks validation and ablation studies
- High linear probing accuracy conflates correlation with causation for feature usage
- Hallucination detection claims lack precision/recall metrics and false positive analysis
- Single model and dataset limit generalizability across ASR architectures and domains

## Confidence
- **High confidence**: Basic feasibility of applying interpretability tools to ASR (techniques work technically)
- **Medium confidence**: Encoder representations correlate with speaker attributes and acoustic conditions (shown via linear probing)
- **Low confidence**: Claims about specific hallucination mechanisms and intervention strategies (limited ablation and validation)

## Next Checks
1. Ablation study: Remove identified hallucination-related components and measure actual ASR performance degradation
2. Cross-model validation: Apply same interpretability pipeline to different ASR architectures (end-to-end, hybrid) and datasets
3. Causality verification: Use causal mediation analysis to confirm that identified representations actually drive observed behaviors rather than merely correlating with them