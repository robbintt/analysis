---
ver: rpa2
title: 'TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only
  Decision Tasks'
arxiv_id: '2510.13878'
source_url: https://arxiv.org/abs/2510.13878
tags:
- llms
- language
- reasoning
- reward
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TextBandit, a benchmark to evaluate large
  language models' (LLMs) ability to make sequential decisions under uncertainty using
  only natural language feedback. Unlike traditional methods, the benchmark requires
  LLMs to infer latent reward structures from purely textual cues like "you earned
  a token" without access to numerical probabilities.
---

# TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks

## Quick Facts
- **arXiv ID**: 2510.13878
- **Source URL**: https://arxiv.org/abs/2510.13878
- **Reference count**: 2
- **Primary result**: Qwen3-4B achieves 89.2% best-arm selection rate on 2-arm bandit tasks using only natural language feedback

## Executive Summary
TextBandit is a benchmark designed to evaluate large language models' ability to perform sequential decision-making under uncertainty using only natural language feedback. The benchmark presents models with multi-armed bandit problems where reward probabilities are hidden and outcomes are communicated through text like "you earned a token" rather than numerical values. Across 500 trials with 2-5 arms and varying reward structures, the study found that smaller models (particularly Qwen3-4B) can outperform both larger LLMs and traditional decision-making algorithms when reasoning about probabilistic outcomes from purely linguistic cues.

## Method Summary
The evaluation uses a text-only multi-armed bandit environment where LLMs receive natural language feedback about outcomes rather than numerical probabilities. Models make sequential decisions across 25 iterations per episode, with the environment providing binary outcomes converted to text descriptions. The study compares four open-source LLMs (Qwen3-4B, Qwen3-8B, Llama-3.1-8B, Phi-2) against traditional algorithms like Thompson Sampling and UCB. Performance is measured through best-arm selection rate, cumulative reward, and cumulative regret, with no chain-of-thought prompting enabled to isolate raw reasoning capabilities.

## Key Results
- Qwen3-4B achieved 89.2% best-arm selection rate, significantly outperforming larger models and traditional baselines
- Larger models (Qwen3-8B, Llama-3.1-8B) showed degraded performance, with Qwen3-8B taking exceptionally long due to excessive reasoning attempts
- All models experienced sharp performance decline as arm count increased (Qwen3-4B: 89.2% → 6.53% from 2-arm to 5-arm)
- Traditional algorithms (Thompson Sampling: 51.1%, UCB: 47.6%) underperformed compared to the smallest tested LLM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can perform probabilistic reasoning using only natural language feedback, without explicit numerical probabilities or reward signals.
- Mechanism: The model accumulates a textual history of outcomes (e.g., "Slot machine 1 won," "Slot machine 2 lost") across iterations. By processing this sequential narrative, the model implicitly tracks relative success frequencies and forms an internal preference for the higher-reward arm. The paper frames this as probabilistic reasoning "emerging from language alone."
- Core assumption: The model's pre-training has encoded sufficient statistical pattern recognition that transfers to this text-only sequential decision task, enabling implicit frequency estimation from binary linguistic outcomes.
- Evidence anchors:
  - [abstract]: "probabilistic reasoning is able to emerge from language alone"
  - [section 4.1]: "No explicit numerical cues or probabilistic information are provided... The objective of the model is to maximize cumulative reward... by learning which arm is better solely from this binary language feedback."
  - [corpus]: OpenEstimate (arXiv:2510.15096) addresses LLM reasoning under uncertainty with real-world data, supporting the broader investigation of non-numeric probabilistic reasoning.
- Break condition: Performance degrades sharply as the number of arms increases (e.g., Qwen3-4B drops from 89.22% on 2-arm to 6.53% on 5-arm), suggesting the mechanism does not scale with task complexity.

### Mechanism 2
- Claim: Larger models may perform worse than smaller models on simple decision tasks due to excessive internal deliberation ("overthinking") when chain-of-thought scaffolding is removed.
- Mechanism: Models trained for complex reasoning (e.g., Qwen3-8B, Llama-3.1-8B) continue attempting elaborate inference even in simple reinforcement environments. Without CoT to externalize this reasoning, the deliberation becomes internal noise, leading to slower decisions and suboptimal arm selection. The smaller Qwen3-4B defaults to a more direct exploitation strategy.
- Core assumption: The inverse scaling effect is caused by training on complex reasoning tasks, which introduces biases or overfitting that interfere with simple reward-maximization heuristics.
- Evidence anchors:
  - [section 6]: "Larger models may have been trained for complex reasoning which is why when they encounter simpler tasks, they tend to overthink things which leads to a drop in performance."
  - [section 5.6]: "Qwen3-8B took an exceptionally large amount of time when testing because it kept trying to reason instead of giving a concise input."
  - [corpus]: Corpus evidence is weak or missing for direct overthinking claims; no neighbor papers explicitly address this phenomenon.
- Break condition: This effect may not hold when chain-of-thought prompting is enabled (acknowledged by authors as a limitation), or for models outside the tested 2.7B–8B parameter range.

### Mechanism 3
- Claim: LLMs without explicit uncertainty quantification follow a Thompson-Sampling-like exploration-exploitation pattern: initial exploration followed by premature lock-in on a perceived best arm.
- Mechanism: Early in the episode, the model samples multiple arms to gather outcome data. After accumulating "a moderate amount of outputs," it converges on what it believes is the optimal arm—even if that arm is suboptimal. This mirrors Thompson Sampling's balance but lacks formal posterior updates, making the model susceptible to early noise.
- Core assumption: LLMs do not maintain explicit uncertainty estimates; instead, they make near-deterministic choices based on accumulated success history in the prompt context.
- Evidence anchors:
  - [section 5.6]: "The LLMs will sample the options first and choose the ones they believe are the most successful... they believe that their chosen one is the best after receiving a moderate amount of outputs."
  - [section 5.6]: "Their method is similar to the Thompson Sampling method, where they balance exploration with exploitation."
  - [corpus]: Zhang et al. (2025, cited in paper) document LLMs halting exploration as they receive more information, solidifying suboptimal choices from noise.
- Break condition: Lock-in on suboptimal arms occurs when early stochastic outcomes mislead the model; performance collapses as arm count increases (more exploration required).

## Foundational Learning

- Concept: **Multi-Armed Bandit Problem**
  - Why needed here: This is the core decision-making framework. You must understand the explore-exploit tradeoff, regret, and why identifying the optimal arm under uncertainty is non-trivial.
  - Quick check question: If you have two slot machines with unknown win rates, how do you decide which to play over 25 rounds?

- Concept: **Exploration-Exploitation Tradeoff**
  - Why needed here: The paper's central tension. Models must balance trying new arms (exploration) vs. leaning on known successful arms (exploitation). Poor balance causes high regret.
  - Quick check question: Why would always choosing the arm that has won most often so far be a suboptimal strategy?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The authors explicitly disable CoT to isolate raw single-shot reasoning. Understanding CoT helps explain why larger models struggled—they were "overthinking" without an outlet.
  - Quick check question: What might happen if you forced a model to verbalize its reasoning before each decision in this bandit task?

## Architecture Onboarding

- Component map:
  - LLM Agent -> Bandit Environment -> Feedback Generator -> History Buffer -> LLM Agent

- Critical path:
  1. Construct initial prompt (task instruction + empty or minimal history)
  2. LLM outputs arm choice
  3. Environment samples outcome based on hidden arm probability
  4. Feedback generator emits text; history buffer updates
  5. Repeat for 25 iterations per episode, 500 episodes total
  6. Compute metrics across all runs

- Design tradeoffs:
  - **CoT disabled vs. enabled**: Disabling isolates raw language-based adaptation but may underestimate model capabilities; enabling could improve exploration but introduces confounds from scaffolding
  - **Static vs. dynamic reward structures**: Static rewards simplify evaluation but limit real-world applicability; dynamic/non-stationary bandits are left for future work
  - **Model scale**: Smaller models (4B) may be better suited for fast-feedback, low-context tasks; larger models (8B+) may require structured reasoning aids

- Failure signatures:
  - **Random-like behavior**: Consistently low best-arm selection rate near chance (e.g., Phi-2 at 25.4%)
  - **Overthinking / timeout**: Excessive generation time, verbose or non-terminating outputs (observed in Qwen3-8B)
  - **Premature lock-in**: Early convergence on a suboptimal arm with low subsequent switching
  - **Scaling collapse**: Sharp accuracy drop as arm count increases (all models degrade from 2-arm to 5-arm)

- First 3 experiments:
  1. **2-arm baseline replication**: Run Qwen3-4B and Qwen3-8B on the 2-arm setup (30%/65%) for 100 episodes of 25 iterations. Confirm best-arm selection rates approximate reported values (~89% vs. ~37%)
  2. **Arm-count scaling test**: Evaluate all four models on 3-arm, 4-arm, and 5-arm configurations. Plot best-arm selection rate vs. arm count to quantify degradation slope
  3. **CoT ablation**: Enable simple chain-of-thought prompting ("Think step by step about which machine to choose") for Qwen3-8B and Llama-3.1-8B on the 2-arm task. Compare decision latency and accuracy to the no-CoT baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of Chain-of-Thought (CoT) reasoning improve best-arm selection rates, or does it exacerbate the "overthinking" and latency issues observed in larger models?
- Basis in paper: [explicit] The authors explicitly removed CoT to isolate single-shot capabilities and list "incorporating chain-of-thought prompting" as a primary direction for future work to understand how scaffolds affect the exploration-exploitation balance.
- Why unresolved: The current study intentionally restricted intermediate reasoning steps to force the model to rely on implicit priors, leaving the impact of explicit reasoning untested.
- What evidence would resolve it: A comparison of TextBandit scores between vanilla prompting and CoT prompting across the same model set (e.g., Qwen3-4B vs. Qwen3-8B).

### Open Question 2
- Question: How do frontier-scale models (e.g., GPT-4, Qwen-32B) perform on TextBandit, and do they reverse or reinforce the inverse scaling trend observed between 4B and 8B models?
- Basis in paper: [explicit] Section 7 (Limitations) states that larger-scale models were not tested due to computational limits, noting that "it is possible that these larger models may exhibit different behaviors, such as a more stable converge."
- Why unresolved: The study only covers models between 2.7B and 8B parameters; the finding that the smallest capable model (Qwen3-4B) outperformed the 8B variants is counter-intuitive and untested at higher scales.
- What evidence would resolve it: Benchmarking frontier models (e.g., Claude 3.5, GPT-4o) on the identical TextBandit protocol to see if best-arm selection rates improve or decline with increased parameter count.

### Open Question 3
- Question: Can LLMs maintain effective probabilistic reasoning in non-stationary bandit environments where reward probabilities shift dynamically over time?
- Basis in paper: [explicit] Section 7 identifies the static reward structure as a limitation and suggests that "future work could expand upon having... dynamic adaption" and "non-stationary environments."
- Why unresolved: The current benchmark uses fixed probabilities (e.g., 30% vs. 65%); real-world decision-making often requires re-learning when the underlying "truth" changes, a capability not assessed here.
- What evidence would resolve it: A modified TextBandit task where the optimal arm switches mid-session, measuring the LLM's ability to detect the shift and update its strategy (regret analysis).

## Limitations
- The inverse scaling effect (smaller models outperforming larger ones) is intriguing but potentially dataset-specific, with no evidence of robustness across different reward structures or arm counts
- The lack of uncertainty quantification in LLM decisions is a critical limitation: without formal posterior tracking, models are vulnerable to premature lock-in on suboptimal arms due to early stochastic outcomes
- The disabling of Chain-of-Thought, while methodologically sound for isolating raw reasoning, may systematically disadvantage larger models that benefit from explicit reasoning scaffolding

## Confidence

- **High Confidence**: The empirical observation that Qwen3-4B outperforms larger models and traditional baselines on the 2-arm bandit task with natural language feedback. The methodology and results are clearly specified and reproducible.
- **Medium Confidence**: The interpretation that this demonstrates "probabilistic reasoning emerging from language alone." While supported by the results, alternative explanations (e.g., frequency counting, pattern matching) remain equally plausible without deeper analysis.
- **Low Confidence**: The claim that larger models "overthink" and perform worse on simple tasks. This requires stronger evidence beyond execution time observations and anecdotal reports of verbose outputs.

## Next Checks

1. **Prompt Template Ablation**: Systematically vary prompt formatting (history placement, instruction clarity, few-shot examples) to determine whether Qwen3-4B's superior performance is robust to prompt engineering or specific to the exact template used.
2. **Dynamic Reward Structure**: Replace static reward probabilities with non-stationary bandits (e.g., reward probabilities that drift over time) to test whether language-only reasoning can adapt to changing environments or is limited to fixed distributions.
3. **Uncertainty-Aware Extensions**: Implement simple uncertainty quantification methods (e.g., temperature-based sampling, multiple completions with voting) to determine if explicit uncertainty tracking can improve performance and reduce premature lock-in, particularly for larger models.