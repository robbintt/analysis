---
ver: rpa2
title: Towards Efficient Multi-Objective Optimisation for Real-World Power Grid Topology
  Control
arxiv_id: '2502.00034'
source_url: https://arxiv.org/abs/2502.00034
tags:
- grid
- topology
- power
- expert
- multi-objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a two-phase, efficient and scalable multi-objective
  optimisation method for power grid topology control, combining reinforcement learning
  with rapid planning to generate day-ahead plans. The approach addresses the challenge
  of managing congestion and maintaining stable supply in modern electric grids by
  optimising competing objectives: minimising N-1 load flow and reducing switching
  frequency.'
---

# Towards Efficient Multi-Objective Optimisation for Real-World Power Grid Topology Control

## Quick Facts
- **arXiv ID:** 2502.00034
- **Source URL:** https://arxiv.org/abs/2502.00034
- **Reference count:** 21
- **Primary result:** Two-phase RL method generates day-ahead power grid plans in 4-7 minutes with 100% in-distribution success and 75% out-of-distribution success

## Executive Summary
This paper presents a scalable two-phase approach for multi-objective power grid topology control that addresses the challenge of congestion management while maintaining system stability. The method combines reinforcement learning with rapid planning to generate day-ahead operational plans, overcoming the computational limitations of standard multi-objective RL algorithms when applied to real-world grid sizes. Using real-world data from TenneT, the approach demonstrates strong performance in balancing competing objectives: minimizing N-1 load flow violations and reducing switching frequency.

## Method Summary
The proposed approach decouples reinforcement learning training from multi-objective planning through a two-phase methodology. In Phase 1, a single RL agent is trained on a scalarized utility function that balances N-1 load flow minimization and switching frequency reduction. In Phase 2, this trained agent is used within a rapid planning framework that generates candidate topologies by sweeping through discrete switching constraints. The system handles the large action space (~100k topologies) by decomposing complex topological actions into unitary substation steps, allowing the agent to navigate the configuration space incrementally. This design enables the generation of day-ahead plans within 4-7 minutes while maintaining computational efficiency.

## Key Results
- Achieves 100% success rate for in-distribution days and solves more than 75% of out-of-distribution scenarios
- Outperforms expert baselines with superior hypervolume analysis showing better spread and quality of solutions
- Maintains moderate switching levels while optimizing N-1 load flow performance
- Demonstrates practical computational efficiency suitable for operational planning in transmission system operations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling RL training from planning allows scaling to real-world grid sizes where standard MORL fails
- **Mechanism:** Instead of learning a distinct policy for every trade-off, the system trains a single RL agent on scalarized utility, then uses a separate planning phase to sweep through discrete constraints
- **Core assumption:** The trained agent generalizes to produce high-quality topologies when queried during planning
- **Evidence anchors:** Abstract and section 3.2 describe the two-phase methodology overcoming MORL inefficiencies
- **Break condition:** If the agent overfits to training utility weights, resulting Pareto front may be sparse or sub-optimal

### Mechanism 2
- **Claim:** Decomposing actions into unitary substation steps enables navigation of large action spaces
- **Mechanism:** Agent interacts by modifying one substation at a time rather than making single complex decisions
- **Core assumption:** Optimal topologies can be reached via sequences of unitary steps
- **Evidence anchors:** Section 4 describes decomposition of target topologies into unitary actions
- **Break condition:** Unitary sequences may lead to invalid intermediate states the agent cannot recover from

### Mechanism 3
- **Claim:** Rewarding topological stability implicitly optimizes switching frequency
- **Mechanism:** Agent receives rewards based on consecutive future timestamps where topology remains stable
- **Core assumption:** Immediate future stability correlates with day-ahead switching minimization
- **Evidence anchors:** Section 3.3 describes Râ‚‚ reward based on consecutive stable timestamps
- **Break condition:** Rapid grid fluctuations may render stability-optimized solutions unsafe at future timestamps

## Foundational Learning

- **Concept: Pareto Front (PF) & Non-Dominated Solutions**
  - **Why needed here:** Core output is a set of trade-off plans, not a single solution
  - **Quick check question:** If Plan A has lower load flow but higher switching than Plan B, can we definitively say Plan A is better?

- **Concept: N-1 Contingency Criterion**
  - **Why needed here:** Primary safety constraint; agent must keep N-1 load flow below 1.0
  - **Quick check question:** What does it mean physically if N-1 load flow exceeds 1.0?

- **Concept: Scalarization in Multi-Objective RL**
  - **Why needed here:** Phase 1 converts multiple objectives into single scalar reward
  - **Quick check question:** How do utility function weights potentially bias the resulting Pareto Front?

## Architecture Onboarding

- **Component map:** Environment (DC flow simulation) -> Phase 1 Agent (PPO/AlphaZero) -> Phase 2 Planner (non-ML logic)
- **Critical path:** Data ingestion -> Offline training -> Online Single-Step Planning (4-7 minutes)
- **Design tradeoffs:** SSA (simpler, greedy) vs AZA (complex, plans full day); DC approximation for speed vs potential accuracy loss
- **Failure signatures:** OOD degradation (performance drops on unseen scenarios); Sim-to-real gap (DC plans may violate AC constraints)
- **First 3 experiments:** 1) Baseline validation vs Reference Strategy on 31 in-distribution days; 2) Ablation study on reward weights; 3) OOD stress test to identify failure patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can framework extend to three or more objectives while maintaining efficiency?
- **Basis:** Conclusion suggests incorporating topological depth by conditioning on discrete values
- **Why unresolved:** Extension requires conditioning policies on discrete topological depth values and merging Pareto Fronts
- **Evidence needed:** Experiments showing three-objective optimization with Pareto Fronts including topological depth

### Open Question 2
- **Question:** Can method generate diverse plans representing same Pareto Front point?
- **Basis:** Conclusion notes current algorithm returns one plan per trade-off point
- **Why unresolved:** Planning phase doesn't produce multiple equivalent alternatives
- **Evidence needed:** Demonstration of multiple topologically distinct plans achieving identical objective values

### Open Question 3
- **Question:** How to improve out-of-distribution generalization?
- **Basis:** Paper notes OOD degradation attributed to limited training exposure
- **Why unresolved:** Root causes and mitigation strategies not investigated
- **Evidence needed:** Experiments showing improved OOD success through training modifications

## Limitations
- Simulation-to-reality gap due to DC power flow approximation missing reactive power constraints
- 25% failure rate on out-of-distribution days represents scenarios where RL may be systematically disadvantaged
- Action space constraints may miss globally optimal solutions requiring coordinated multi-substation changes

## Confidence
- **High confidence:** Computational efficiency claims (4-7 minute planning) directly supported by methodology
- **Medium confidence:** 100% in-distribution and 75% OOD success rates depend on specific dataset composition
- **Medium confidence:** Hypervolume improvements are well-defined but practical significance varies by operator

## Next Checks
1. Test generated plans using AC power flow simulation to quantify DC-to-real performance gap
2. Systematically identify and evaluate specific out-of-distribution scenarios where RL approach fails
3. Calculate actual cost savings from reduced switching and improved N-1 performance to verify "millions of euros annually" claim