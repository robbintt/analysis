---
ver: rpa2
title: A Greek Government Decisions Dataset for Public-Sector Analysis and Insight
arxiv_id: '2512.05647'
source_url: https://arxiv.org/abs/2512.05647
tags:
- decisions
- boilerplate
- public
- document
- greek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large-scale dataset of Greek government
  decisions from the Diavgeia transparency platform, comprising 1 million documents
  with high-quality Markdown text extracted from PDFs. The dataset includes normalized
  metadata, canonical identifiers, and is released with a fully reproducible extraction
  pipeline.
---

# A Greek Government Decisions Dataset for Public-Sector Analysis and Insight

## Quick Facts
- arXiv ID: 2512.05647
- Source URL: https://arxiv.org/abs/2512.05647
- Authors: Giorgos Antoniou; Giorgos Filandrianos; Aggelos Vlachos; Giorgos Stamou; Lampros Kollimenos; Konstantinos Skianis; Michalis Vazirgiannis
- Reference count: 40
- 1 million Greek government decisions extracted from PDFs with high-quality Markdown text, released with reproducible pipeline and metadata

## Executive Summary
This paper introduces a large-scale dataset of Greek government decisions from the Diavgeia transparency platform, comprising 1 million documents with high-quality Markdown text extracted from PDFs. The dataset includes normalized metadata, canonical identifiers, and is released with a fully reproducible extraction pipeline. Beyond the core dataset, the authors perform boilerplate analysis to explore administrative patterns and design a retrieval-augmented generation (RAG) task for citizen-facing question answering. The RAG system achieves 66.6% accuracy on automated evaluations and 85.0% accuracy on manual multi-document queries. The dataset and pipeline are publicly available, offering valuable resources for legal NLP, public-sector AI, and transparency research.

## Method Summary
The authors constructed a dataset of 1 million Greek government decisions from the Diavgeia transparency platform (year 2021). They extracted high-quality Markdown text from PDFs using PyMuPDF4LLM, which preserves document structure while being significantly faster than OCR. Documents were indexed in Elasticsearch 7.11 using a Greek language analyzer and BM25 scoring. A RAG system was implemented using BM25 retrieval with GPT-4o-mini generation, storing conversations in Redis. For boilerplate detection, the authors used all-MiniLM-L6-v2 embeddings with k-NN retrieval and GPT-5 segmentation. The system was evaluated both automatically (semantic equivalence ≥70%) and manually (100 queries).

## Key Results
- RAG system achieves 66.6% accuracy on automated semantic equivalence evaluation
- Manual evaluation shows 85.0% accuracy for multi-document queries
- Boilerplate detection achieves 0.0097 reconstruction error and 75.60% extraction rate
- Dataset comprises 1 million documents with 4GB compressed Markdown corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PDF-to-Markdown extraction via PyMuPDF4LLM provides efficient, LLM-compatible text while preserving document structure.
- Mechanism: Direct PDF extraction bypasses OCR overhead; Markdown output retains headings, lists, and structural cues that improve downstream tokenization and retrieval segmentation.
- Core assumption: Source PDFs contain extractable text streams (not purely scanned images) and encoding artifacts are tolerable.
- Evidence anchors:
  - [abstract]: "high-quality raw text extracted from PDFs... released with raw extracted text in Markdown format"
  - [section 3.2]: "PyMuPDF4LLM... considerably faster... output is given in a Markdown format, which is considered optimal for training language models"
  - [corpus]: Plutus benchmark (arXiv:2502.18772) highlights challenges in Greek financial NLP due to domain-specific data scarcity—this dataset addresses that gap.
- Break condition: If PDFs are predominantly image-based scans, direct extraction degrades; OCR becomes necessary.

### Mechanism 2
- Claim: Semantic embeddings outperform lexical matching for boilerplate detection in administrative documents.
- Mechanism: Dense embeddings (all-MiniLM-L6-v2) cluster templated documents by shared meaning rather than surface overlap; LLMs then segment boilerplate from content.
- Core assumption: Documents from the same template share semantic patterns (referenced entities, statutes) even with lexical variation.
- Evidence anchors:
  - [section 4.1]: "We intentionally avoid lexical similarity metrics such as keyword overlap, BM25 scoring... these techniques overweight surface-form correspondence"
  - [section 4.2]: GPT-5 achieves 0.0097 reconstruction error and 75.60% boilerplate extraction rate
  - [corpus]: Weak direct evidence for Greek administrative boilerplate—this appears to be a novel application.
- Break condition: If templates vary semantically as much as lexically, embedding clusters won't correlate with boilerplate structure.

### Mechanism 3
- Claim: BM25-based retrieval with LLM generation suffices for single-document Q&A but struggles with multi-document numerical aggregation.
- Mechanism: Lexical retrieval fetches top-k documents; LLM synthesizes answers with ADA citations. Performance varies by task type.
- Core assumption: Relevant documents contain query keywords; aggregation requires explicit numeric fields.
- Evidence anchors:
  - [section 5.3]: "Accuracy of 66.6% indicates the RAG system successfully retrieves and generates correct information for approximately two-thirds of queries"
  - [section 5.4]: Manual evaluation shows 100% signer/topic accuracy but only 60% on arithmetic aggregation
  - [corpus]: Swiss Parliaments Corpus (arXiv:2506.07726) demonstrates RAG-based correction pipelines—similar architectural patterns.
- Break condition: Semantic mismatches between queries and document phrasing cause retrieval failures; implicit calculations exceed LLM reasoning.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) architecture**
  - Why needed here: The paper's citizen-facing Q&A system depends on understanding how retrieval and generation components interact.
  - Quick check question: Can you explain why the paper uses BM25 instead of dense retrieval, and what tradeoff that introduces?

- Concept: **Embedding-based similarity vs. lexical matching**
  - Why needed here: Boilerplate detection relies on semantic clustering; understanding when to prefer each approach is critical.
  - Quick check question: Why would BM25 fail to identify boilerplate that shares meaning but not keywords?

- Concept: **Greek language NLP constraints**
  - Why needed here: Morphological richness and moderate resource status affect tokenization, model selection, and evaluation.
  - Quick check question: What considerations apply when adapting models trained primarily on English data to Greek administrative text?

## Architecture Onboarding

- Component map:
  - Diavgeia API → PDF/JSON download → PyMuPDF4LLM extraction → Markdown files
  - Metadata + content concatenation → Elasticsearch (Greek analyzer, BM25)
  - all-MiniLM-L6-v2 embeddings → vector DB → k-NN retrieval → LLM segmentation
  - Query + conversation history → Elasticsearch retrieval → context assembly → GPT-4o-mini generation → Redis storage

- Critical path:
  1. Verify PDF extraction quality on sample documents (check for encoding artifacts)
  2. Index subset and validate retrieval with known queries
  3. Test end-to-end RAG on manual evaluation queries before scaling

- Design tradeoffs:
  - PyMuPDF vs. OCR: ~2500× faster but may miss content in scanned PDFs
  - BM25 vs. dense retrieval: Simpler, interpretable, but fails on paraphrased queries
  - GPT-5-mini vs. GPT-5 for boilerplate: Faster but 17% lower extraction rate

- Failure signatures:
  - **Retrieval gaps**: Extra or missing documents when organization names are ambiguous
  - **Aggregation errors**: Large numerical discrepancies (28% in ΛΑΙΚΟ case) indicate implicit calculation failures
  - **Incomplete extraction**: Signer lists partially missing when documents span multiple pages

- First 3 experiments:
  1. Run extraction pipeline on 100 random PDFs; manually inspect Markdown quality for structural preservation and encoding issues.
  2. Index 10,000 documents; query with 20 test questions and measure retrieval recall against known-relevant ADAs.
  3. Evaluate boilerplate detection on 50 document pairs using the cross-swapping reconstruction test; compare GPT-5-mini vs. GPT-5.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited quantitative validation of PDF extraction quality across all document types
- Single-modality BM25 retrieval may fail on paraphrased or semantically equivalent queries
- Evaluation scope limited to 100 manual queries, affecting generalizability

## Confidence
- **High Confidence**: Dataset construction methodology, extraction pipeline reproducibility, basic RAG architecture implementation
- **Medium Confidence**: Performance metrics on automated evaluation, boilerplate detection methodology, manual evaluation results
- **Low Confidence**: Claims about PyMuPDF4LLM extraction quality across all document types, effectiveness of BM25-only retrieval for diverse query types, generalizability of manual evaluation results

## Next Checks
1. **Extraction Quality Validation**: Run PyMuPDF4LLM extraction on a stratified sample of 200 documents (varying ages, sizes, and suspected quality levels). Manually compare extracted Markdown against original PDFs for structural preservation, completeness, and encoding artifacts. Calculate extraction accuracy and identify failure modes.

2. **Retrieval Method Comparison**: Implement a dense retrieval baseline using all-MiniLM-L6-v2 embeddings (already computed for boilerplate detection). Evaluate both BM25 and dense retrieval on 50 query-document pairs with known relevance. Compare recall@5 and precision@5 to quantify the lexical matching limitation.

3. **Extended Manual Evaluation**: Conduct manual evaluation on 200 additional queries, stratified by complexity (single vs. multi-document, numerical vs. categorical). Measure accuracy by query type to identify systematic failure patterns, particularly for numerical aggregation and implicit calculations.