---
ver: rpa2
title: 'Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches'
arxiv_id: '2505.12259'
source_url: https://arxiv.org/abs/2505.12259
tags:
- ability
- evaluation
- arxiv
- answer
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Teach2Eval addresses the problem of effectively evaluating large
  language models by introducing an indirect teaching-based framework. Instead of
  directly testing LLMs on static benchmarks, it evaluates how well a model can guide
  weaker student models to complete tasks, converting open-ended problems into multiple-choice
  questions.
---

# Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches

## Quick Facts
- arXiv ID: 2505.12259
- Source URL: https://arxiv.org/abs/2505.12259
- Authors: Yuhang Zhou; Xutian Chen; Yixin Cao; Yuchen Ni; Yu He; Siyu Tian; Xiang Liu; Jian Zhang; Chuanjun Ji; Guangnan Ye; Xipeng Qiu
- Reference count: 14
- Primary result: Student model performance gains after multi-turn teaching serve as reliable proxy for evaluating teacher model abilities, with Kendall's Tau 0.90+ alignment to existing leaderboards

## Executive Summary
Teach2Eval introduces an indirect evaluation framework that assesses LLM capabilities by measuring how effectively they can teach weaker student models to complete tasks. Instead of direct testing on static benchmarks, teachers guide students through multiple turns of instruction while solving multiple-choice questions derived from open-ended tasks. This approach enables scalable, automated assessment while avoiding data leakage and memorization issues. Experiments on 26 leading LLMs show strong correlation with existing evaluation metrics and offer fine-grained capability analysis for training guidance.

## Method Summary
The method converts open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, then evaluates teacher models based on their ability to guide weaker student models to solve these questions across multiple interaction turns. Student performance improvement serves as the proxy metric for teacher capability. The framework decomposes teacher abilities into four dimensions: Application (direct accuracy), Judgment (correctness in evaluating student responses), Guidance (improvement after incorrect answers), and Reflection (multi-turn improvement). The evaluation uses 60 datasets converted to MCQs with distractors from weak model errors, and runs T=3 guidance turns with a pool of four weak student models.

## Key Results
- Student model performance gains serve as reliable proxy for evaluating teacher model abilities, with higher-order skills (guidance, reflection) showing stronger correlations with overall effectiveness
- Strong alignment with existing leaderboards: Kendall's Tau 0.79-0.82 and Spearman 0.90-0.92 with Chatbot Arena and LiveBench
- Effectively mitigates data contamination issues: distilled models show improved Application Ability but diminished Comprehensive Ability, while stronger models remain stable
- Multi-dimensional assessment enables fine-grained capability analysis for training guidance

## Why This Works (Mechanism)

### Mechanism 1: Teaching Performance as Proxy for Cognitive Ability
Student model improvement after guided learning serves as a reliable proxy for evaluating teacher model capabilities. The teacher must demonstrate multiple interacting abilities—understanding the problem, judging student correctness, generating actionable guidance, and reflecting across turns. The aggregation of student performance gains captures these distributed capabilities without directly testing each one.

Core assumption: Teaching ability correlates with general model capability, and higher-order skills (guidance, reflection) are more diagnostic than simple application accuracy.

Evidence anchors:
- "Results demonstrate that student model performance gains serve as a reliable proxy for evaluating teacher model abilities, with higher-order skills like guidance and reflection showing strong correlations with overall effectiveness."
- Spearman correlations: Application (0.85), Judgment (0.87), Guidance (0.94), Reflection (0.90) — higher-order abilities show stronger correlation.

### Mechanism 2: MCQ Standardization Preserves Difficulty While Enabling Automation
Converting open-ended tasks to multiple-choice questions through teacher-generated feedback enables scalable, automated evaluation while preserving task difficulty. Distractor options are generated from weak model errors, preserving realistic confusion patterns. Teachers generate guidance without seeing MCQ options, ensuring they solve the open-ended problem rather than selecting from choices.

Core assumption: MCQ format with weak-model-derived distractors preserves sufficient signal about reasoning difficulty and learning potential.

Evidence anchors:
- "By converting open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, Teach2Eval enables scalable, automated, and multi-dimensional assessment."
- Uses weak model incorrect answers with GPT-4o as rewriter to ensure proper formatting.

### Mechanism 3: Interactive Teaching Circumvents Memorization
Requiring models to teach weaker models reduces the effectiveness of memorization and data contamination as shortcuts to high scores. Contaminated models may recognize questions and recall answers, but teaching requires diagnosing specific student errors, generating contextual explanations, and adapting guidance across turns. Memorized answers don't transfer to these meta-cognitive demands.

Core assumption: Teaching requires deeper understanding than answering; contamination typically affects surface-level answer recall more than explanatory reasoning patterns.

Evidence anchors:
- "This method enables scalable, automated, and multi-dimensional assessment while avoiding data leakage and memorization issues."
- "most distilled models exhibit improved Application Ability but diminished Comprehensive Ability, whereas Qwen2.5-32B and Phi-4, stronger models, remain largely unchanged — demonstrating that Teach2Eval effectively mitigates evaluation issues caused by data contamination."

## Foundational Learning

- **Bloom's Taxonomy (Cognitive Levels)**
  - Why needed here: The paper explicitly maps teacher abilities to four levels—Application, Judgment, Guidance, Reflection—derived from Bloom's taxonomy. Understanding this hierarchy is essential for interpreting why higher-order abilities show stronger correlation with Comprehensive Ability.
  - Quick check question: In this framework, why is "Guidance Ability" considered higher-order than "Application Ability," and what does their correlation difference (0.94 vs. 0.85) suggest?

- **The Feynman Technique Principle**
  - Why needed here: The paper draws explicit inspiration from the Feynman Technique: "explaining or teaching concepts in a simple, concise manner deepens one's understanding." This philosophical assumption underlies the entire evaluation design.
  - Quick check question: What claim does the Feynman Technique make about the relationship between teaching ability and understanding, and how does Teach2Eval operationalize this for LLM evaluation?

- **Kendall's Tau and Spearman Correlation**
  - Why needed here: Validation relies on rank correlation metrics against Chatbot Arena (Kendall's Tau 0.79, Spearman 0.91) and LiveBench (0.82, 0.92). Understanding what these measure—and what they don't—is critical for interpreting how strongly Teach2Eval aligns with human preference rankings.
  - Quick check question: A Kendall's Tau of 0.79 between Teach2Eval and Chatbot Arena indicates what about the relationship between these two ranking systems, and what limitations might remain?

## Architecture Onboarding

- **Component map:**
  Data Pipeline: 60 datasets → weak model wrong answers → GPT-4o rewrites to MCQ format → Qwen-family models classify difficulty (5 levels)
  Student Pool: 4 weak models (LLaMA3.2-1B, Qwen2.5-1.5B, MiniCPM-2B, InternLM2.5-1.8B)
  Interaction Loop: Question + MCQ options → Student initial answer → Teacher judgment + guidance (teacher sees NO options) → Student re-answers with history → Repeat T turns
  Metrics: CA = average ΔP_T across students; decomposed into AA, JA, GA, RA

- **Critical path:**
  1. MCQ data construction with authentic distractors
  2. Establish student baseline accuracy at turn 0
  3. Run T=3 guidance turns per question per student-teacher pair
  4. Compute per-question improvement, aggregate to CA
  5. Validate: correlate CA rankings with Chatbot Arena/LiveBench

- **Design tradeoffs:**
  - Turn count: T=3 balances signal vs. cost; Figure 6a shows plateau by turn 3 for most models
  - Student model selection: Must be weak enough to show improvement but instruction-following enough to incorporate guidance
  - Prompt isolation: Teacher must NOT see MCQ options; otherwise evaluation tests option selection, not guidance quality
  - Distractor source: Weak model errors vs. synthetic distractors — paper uses real errors for ecological validity

- **Failure signatures:**
  - Kendall's Tau < 0.70 vs. Chatbot Arena (misaligned evaluation signal)
  - Student models showing < 2% improvement across all turns (guidance ineffective or student ceiling hit)
  - High AA (> 75%) but low CA (< 5%) — classic contamination pattern
  - Negative Reflection Ability — model makes guidance worse over time

- **First 3 experiments:**
  1. Reproduce correlation validation: Run Teach2Eval on 5–10 models from Table 4, compute Kendall's Tau vs. Chatbot Arena Reasoning. Verify τ > 0.75.
  2. Student ablation: Remove one weak model at a time, recompute CA rankings. Confirm correlation variance < 0.02 (robustness to student selection).
  3. Convergence check: Run 5 representative models for T=6 turns. Plot improvement curve; verify plateau by turn 3 as in Figure 6a.

## Open Questions the Paper Calls Out
- **Multimodal Extension**: The authors state this approach can be applied to the multimodal domain, which they consider part of future work, but the current study is restricted to text-based LLMs due to resource constraints.
- **Beyond Helpful Domain**: While experiments covered helpful tasks, the authors reference 3H criteria (Helpful, Honest, Harmless) without testing the latter two domains, noting the method can easily be extended to other domains.
- **GPT-4o Dependency**: The use of GPT-4o as rewriter and reviewer for MCQ conversion raises questions about whether this introduces bias that favors models sharing architectural traits or training data with the rewriter.

## Limitations
- **Unverified Contamination Resistance**: The teaching-based evaluation claims to mitigate data contamination, but lacks controlled contamination experiments to definitively prove the framework detects memorization.
- **MCQ Conversion Fidelity**: Without direct validation of whether MCQ conversion preserves original task difficulty and reasoning complexity, there's risk of systematic underestimation or overestimation of model capabilities.
- **Student Model Selection Sensitivity**: The framework's dependence on finding appropriate weak models that are both instruction-following and measurably improvable remains partially characterized.

## Confidence
- **High Confidence**: Correlation with existing leaderboards (Kendall's Tau 0.79-0.82, Spearman 0.90-0.92), mechanism of multi-turn guidance, and basic framework design
- **Medium Confidence**: Claims about contamination resistance, effectiveness of MCQ conversion for preserving task difficulty, and generalizability to domains beyond tested 60 datasets
- **Low Confidence**: Exact sensitivity to student model selection, behavior in extreme contamination scenarios, and performance on tasks requiring long-form generation

## Next Checks
1. **Contamination Experiment**: Systematically contaminate a subset of teacher models with training data from the evaluation datasets. Run Teach2Eval to verify that high Application Ability (AA) no longer correlates with high Comprehensive Ability (CA) as predicted by Insight 1.

2. **MCQ Conversion Validation**: Select 10 open-ended tasks and their MCQ-converted versions. Run both versions with the same teacher models and compare CA scores. If conversion systematically alters difficulty, scores should diverge significantly.

3. **Student Model Robustness**: Implement the ablation study by removing each weak student model one at a time and recomputing CA rankings. Verify that Kendall's Tau between full and reduced student pool rankings remains above 0.90.