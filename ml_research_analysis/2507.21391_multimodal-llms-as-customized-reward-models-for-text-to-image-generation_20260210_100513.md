---
ver: rpa2
title: Multimodal LLMs as Customized Reward Models for Text-to-Image Generation
arxiv_id: '2507.21391'
source_url: https://arxiv.org/abs/2507.21391
tags:
- llav
- reward
- a-reward
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLaVA-Reward, an efficient multimodal reward
  model that leverages pre-trained multimodal large language models (MLLMs) for evaluating
  text-to-image generation. Unlike existing methods requiring complex instructions
  or token-based scoring, LLaVA-Reward directly utilizes MLLM hidden states from text-image
  pairs and incorporates a Skip-connection Cross Attention (SkipCA) module to enhance
  bidirectional visual-textual reasoning.
---

# Multimodal LLMs as Customized Reward Models for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2507.21391
- Source URL: https://arxiv.org/abs/2507.21391
- Authors: Shijie Zhou; Ruiyi Zhang; Huaisheng Zhu; Branislav Kveton; Yufan Zhou; Jiuxiang Gu; Jian Chen; Changyou Chen
- Reference count: 40
- Primary result: Introduces LLaVA-Reward, an efficient multimodal reward model that achieves state-of-the-art performance across alignment, safety, and artifact evaluation for text-to-image generation

## Executive Summary
This paper introduces LLaVA-Reward, an efficient multimodal reward model that leverages pre-trained multimodal large language models (MLLMs) for evaluating text-to-image generation. Unlike existing methods requiring complex instructions or token-based scoring, LLaVA-Reward directly utilizes MLLM hidden states from text-image pairs and incorporates a Skip-connection Cross Attention (SkipCA) module to enhance bidirectional visual-textual reasoning. The model supports multiple evaluation perspectives (alignment, fidelity, safety, ranking) and is fine-tuned using preference data through Bradley-Terry ranking loss or cross-entropy loss.

## Method Summary
LLaVA-Reward employs a pre-trained MLLM and extracts hidden states from text-image pairs to create a compact reward representation. The key innovation is the Skip-connection Cross Attention (SkipCA) module, which enables efficient bidirectional visual-textual reasoning without requiring complex instruction tuning. The model is fine-tuned on preference data using either Bradley-Terry ranking loss for pairwise comparisons or cross-entropy loss for multi-class classification. This approach supports various evaluation perspectives including alignment, fidelity, safety, and ranking tasks. The model can be integrated into text-to-image generation pipelines to provide rewards for diffusion-based inference-time scaling, improving overall generation quality.

## Key Results
- Achieves 71.1% pairwise accuracy on TIFA 160 benchmark for alignment evaluation
- Demonstrates 87.2% F1 score for safety evaluation
- Shows significant efficiency improvements over VQA-based approaches while maintaining superior performance across multiple evaluation metrics

## Why This Works (Mechanism)
LLaVA-Reward works by leveraging the rich semantic representations already learned by pre-trained MLLMs, avoiding the need for complex instruction tuning required by traditional reward models. The SkipCA module enables efficient bidirectional reasoning between visual and textual modalities by maintaining skip connections that preserve information flow. By using hidden states rather than requiring the model to generate textual responses, the approach reduces computational overhead while maintaining expressiveness. The preference-based fine-tuning allows the model to learn nuanced quality distinctions from human preferences rather than relying on synthetic or rule-based training signals.

## Foundational Learning
- **Multimodal Large Language Models (MLLMs)**: Why needed - Provide pre-trained visual-textual reasoning capabilities; Quick check - Verify the MLLM can accurately describe text-image relationships
- **Skip-connection Cross Attention (SkipCA)**: Why needed - Enables efficient bidirectional reasoning while preserving information; Quick check - Confirm skip connections improve attention effectiveness
- **Bradley-Terry Ranking Loss**: Why needed - Enables pairwise preference learning for relative quality assessment; Quick check - Validate ranking accuracy on known preference pairs
- **Hidden State Extraction**: Why needed - Provides compact, efficient reward representations; Quick check - Compare hidden state effectiveness against full model outputs
- **Preference-based Fine-tuning**: Why needed - Learns nuanced quality distinctions from human judgments; Quick check - Test on diverse preference datasets

## Architecture Onboarding
**Component Map**: Input Text/Image -> MLLM Encoder -> SkipCA Module -> Hidden State Extraction -> Reward Prediction
**Critical Path**: The MLLM encoder and SkipCA module form the critical path, as they process the multimodal input and enable the bidirectional reasoning essential for reward prediction
**Design Tradeoffs**: Uses hidden states instead of full model outputs for efficiency, but may lose some fine-grained information; SkipCA adds complexity but improves reasoning quality
**Failure Signatures**: Poor performance on edge cases with unusual text-image relationships; degradation when visual-textual alignment is weak; sensitivity to input resolution and quality
**First Experiments**: 1) Test pairwise accuracy on controlled text-image pairs with known relationships; 2) Evaluate safety detection on adversarial prompts; 3) Measure computational efficiency compared to VQA-based baselines

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation primarily compares against VQA-based reward models, potentially missing other competitive approaches
- Performance on diverse, real-world datasets beyond reported benchmarks is unclear
- Limited discussion of potential biases in training data and handling of edge cases
- Absolute computational requirements for deployment not fully characterized

## Confidence
- **High confidence**: Architectural innovation (SkipCA module) and core concept of using MLLM hidden states for reward modeling
- **Medium confidence**: Reported benchmark performance improvements due to limited comparison scope
- **Medium confidence**: Efficiency claims as absolute metrics and real-world deployment scenarios lack thorough exploration

## Next Checks
1. Cross-dataset generalization: Test LLaVA-Reward on diverse, held-out text-to-image datasets not used in training or baseline comparisons
2. Ablation studies: Systematically evaluate the SkipCA module and hidden state usage contributions to performance gains
3. Real-world deployment analysis: Evaluate performance and computational efficiency on actual text-to-image generation pipelines, measuring end-to-end impact