---
ver: rpa2
title: 'A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents'
arxiv_id: '2510.18608'
source_url: https://arxiv.org/abs/2510.18608
tags:
- learning
- more
- robotic
- tasks
- robotics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a compositional paradigm for foundation models
  to create more adaptable, efficient, and intelligent AI systems. The authors address
  the limitations of current foundation models, which struggle with dynamic real-world
  scenarios without full retraining and show diminishing returns from simple scaling.
---

# A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents

## Quick Facts
- **arXiv ID:** 2510.18608
- **Source URL:** https://arxiv.org/abs/2510.18608
- **Reference count:** 16
- **Primary result:** HAM achieves 55.17% accuracy vs 47.56% SD-LoRA and 36.02% InfLoRA on CUB200 with 50 tasks

## Executive Summary
This paper introduces a compositional paradigm for foundation models that addresses key limitations in current AI systems, particularly their inability to handle dynamic real-world scenarios without full retraining. The authors propose combining Continual Learning and Compositionality principles to create more adaptable, efficient, and intelligent AI systems. Their approach is validated through experiments showing that hierarchical adapter merging achieves superior accuracy and faster training compared to baseline methods, while their Weighted Small Adapter (WSA) method demonstrates dramatic improvements in robotic manipulation tasks with significantly reduced training time.

## Method Summary
The proposed compositional paradigm centers on two main techniques: Hierarchical Adapter Merging (HAM) for image classification and Weighted Small Adapter (WSA) for robotic manipulation. HAM trains separate LoRA adapters per task, groups them by similarity, and performs hierarchical merging to reduce interference. WSA uses small pre-trained models combined with attention mechanisms to dynamically scale components based on context. The approach leverages Continual Learning principles to maintain knowledge across tasks while using compositionality to enable zero-shot adaptation to new task combinations through modular skill recombination.

## Key Results
- HAM achieves 55.17% accuracy on CUB200 with 50 tasks, outperforming SD-LoRA (47.56%) and InfLoRA (36.02%)
- HAM trains faster at 170.61 seconds vs 318.22 seconds (SD-LoRA) and 196.89 seconds (InfLoRA)
- WSA achieves 0.91 success rate with 0.60 reward per step in 14 hours, outperforming InstructRL (0.0 success rate, 40h) and OpenVLA (0.0 success rate, 92h)

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical merging of task-specific adapters reduces interference compared to flat merging approaches. LoRA adapters trained per experience are first combined within similarity-based groups, then group adapters merge into a unified model. This two-stage process preserves task-specific knowledge by containing interference within related clusters before global consolidation. Core assumption: Adapters for semantically similar tasks exhibit lower mutual interference than randomly paired adapters. Evidence: HAM achieves 55.17% accuracy vs. 47.56% SD-LoRA and 36.02% InfLoRA on CUB200 with 50 tasks.

### Mechanism 2
Lightweight adapter composition with attention-based routing outperforms monolithic VLA models under constrained compute. Small pre-trained models are combined using adapters and attention mechanisms that dynamically scale components based on context, enabling efficient skill selection without full model fine-tuning. Core assumption: Pre-trained visual and policy representations contain sufficient domain knowledge that can be accessed via lightweight routing rather than end-to-end retraining. Evidence: WSA achieves 0.91 success rate with 0.60 reward/step in 14 hours vs. InstructRL (0.0, 40h) and OpenVLA (0.0, 92h).

### Mechanism 3
Compositional systems enable zero-shot adaptation to new task combinations without additional training. Independent skill modules can be recombined dynamically by changing orchestration logic, allowing novel task compositions from learned primitives. Core assumption: Skills factorize compositionally—complex behaviors decompose into reusable sub-skills that combine predictably. Evidence: The paper claims combinations "may be performed with little to no training phases, exploiting FMs full potential."

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The entire HAM methodology builds on LoRA adapters as the unit of composition. Without understanding low-rank decomposition of weight updates, you cannot reason about merging strategies.
  - Quick check question: Can you explain why merging two LoRA adapters (W1 + W2) is parameter-equivalent to sequential fine-tuning only when rank and target modules match?

- **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The paper frames adapter composition as a solution to forgetting. Understanding why naive fine-tuning destroys prior knowledge is essential to evaluate whether hierarchical merging actually addresses the root problem.
  - Quick check question: What happens to task A accuracy when you fine-tune a model on task B without regularization or architectural isolation?

- **Attention-Based Feature Gating**
  - Why needed here: The WSA robotic architecture uses attention to dynamically weight pre-trained components. Understanding how attention selectively routes information is prerequisite to debugging failure cases.
  - Quick check question: Given query Q and keys K, how does softmax(QK^T/√d) create a context-dependent weighting over value vectors V?

## Architecture Onboarding

- **Component map:**
  - HAM Pipeline: Task stream → Per-experience LoRA training → Similarity computation → Group-level merge → Global merge → Unified adapter
  - WSA Pipeline: Observation → Pre-trained visual encoder → Adapter modules → Attention-based routing → Policy head → Action output
  - Orchestration Layer: (Assumption: Task context determines which adapter combination to activate—details sparse in paper)

- **Critical path:**
  1. Establish baseline single-adapter performance per task
  2. Implement similarity metric for adapter grouping (paper does not specify metric—cosine similarity on adapter weights is reasonable starting point)
  3. Build hierarchical merge logic with interference monitoring
  4. Integrate attention router for dynamic composition
  5. Validate compositional generalization on held-out task combinations

- **Design tradeoffs:**
  - Merge granularity: More groups = finer isolation but higher merge complexity
  - Adapter rank: Higher rank = more capacity per task but greater interference risk
  - Pre-trained backbone choice: Larger backbones (7B VLA) provide richer features but the paper shows they underperform under compute constraints

- **Failure signatures:**
  - Accuracy drops on earlier tasks after merging → interference not isolated, reconsider grouping strategy
  - Attention collapses to single adapter regardless of context → routing not learning meaningful distinctions, check reward structure
  - Success rate 0.0 on robotics tasks (as with OpenVLA baseline) → backbone may lack task-relevant features; verify pre-training coverage

- **First 3 experiments:**
  1. Reproduce HAM image classification results on CUB200 split into 10 tasks (not 50) to validate merging logic before scaling
  2. Ablate similarity-based grouping by comparing against random grouping to quantify interference reduction
  3. Test WSA on a single Meta-World task before multi-task composition to isolate routing failures from skill acquisition failures

## Open Questions the Paper Calls Out

- **How can compositional agents effectively abstract from low-level policies to perform high-level reasoning and planning?**
  - Basis in paper: The conclusion states that for agents to evolve, they "need to be able to abstract from these policies and reason over their capabilities in order to come up with plans."
  - Why unresolved: The current experimental results focus on success rates in manipulation and classification accuracy, which do not inherently demonstrate abstract reasoning capabilities.
  - What evidence would resolve it: Successful completion of long-horizon tasks requiring the novel composition of learned skills without explicit retraining.

- **Does the efficiency of compositional adapters (WSA) in simulated tasks generalize to dynamic, real-world physical environments?**
  - Basis in paper: The abstract identifies "dynamic, real-world scenarios" as a major struggle for current FMs, while the experimental validation relies on dataset metrics and likely simulation.
  - Why unresolved: The paper demonstrates superior simulation efficiency (14 hours) but does not validate the "Sim2Real" gap for the proposed compositional methods.
  - What evidence would resolve it: Benchmark results showing the WSA method maintaining high success rates (e.g., >0.90) on physical robotic hardware in unstructured environments.

- **What specific mechanisms can optimally orchestrate the dynamic composition of multiple specialized models without training phases?**
  - Basis in paper: Section III suggests that systems could adapt "by changing the way in which the models are orchestrated," and notes such combinations may occur with "little to no training phases."
  - Why unresolved: The paper validates a hierarchical merging method (HAM) but does not detail a mechanism for dynamic, inference-time orchestration of distinct modules.
  - What evidence would resolve it: A defined orchestration algorithm that selects and composes modules in real-time to handle unseen task distributions.

## Limitations

- The paper does not specify critical hyperparameters for the hierarchical merging process, including the similarity metric, LoRA rank parameters, and group merging algorithm.
- Architectural details for the attention-based routing in WSA are sparse, creating barriers to faithful reproduction.
- The paper lacks empirical evidence for zero-shot compositional generalization on novel task combinations.

## Confidence

- **HAM image classification results**: Medium confidence - specific accuracy improvements are verifiable but similarity-based grouping mechanism lacks detailed specification
- **WSA robotics performance**: Medium confidence - concrete training time and success rate comparisons but sparse architectural details
- **Compositional generalization claims**: Low confidence - theoretical mechanism explained but limited empirical evidence provided

## Next Checks

1. **Ablation study on grouping strategy**: Reproduce HAM with three variants—similarity-based grouping, random grouping, and no grouping—to quantify the actual interference reduction benefit claimed by the hierarchical approach.

2. **Attention routing verification**: Implement WSA on a single robotic task with attention routing disabled to isolate whether success comes from adapter composition or the underlying policy learning.

3. **Compositional generalization test**: Hold out specific task combinations during training, then evaluate zero-shot performance on these novel combinations to directly validate the compositional adaptation claims.