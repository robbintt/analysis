---
ver: rpa2
title: 'Fairness of Deep Ensembles: On the interplay between per-group task difficulty
  and under-representation'
arxiv_id: '2501.14551'
source_url: https://arxiv.org/abs/2501.14551
tags:
- difficulty
- fairness
- performance
- ensembles
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether homogeneous deep ensembles can
  improve fairness by reducing performance disparities across demographic subgroups.
  The authors explore the interplay between per-group task difficulty and under-representation
  using synthetic and real datasets.
---

# Fairness of Deep Ensembles: On the interplay between per-group task difficulty and under-representation

## Quick Facts
- arXiv ID: 2501.14551
- Source URL: https://arxiv.org/abs/2501.14551
- Reference count: 37
- Primary result: Homogeneous deep ensembles improve fairness by reducing performance disparities without sacrificing overall accuracy, with optimal balance ratios shifting toward harder subgroups as task difficulty increases

## Executive Summary
This paper demonstrates that homogeneous deep ensembles can significantly improve fairness across demographic subgroups by reducing performance disparities without the typical trade-off of lowering overall accuracy. Through controlled synthetic experiments and real-world vision datasets, the authors show that ensembles disproportionately benefit underperforming subgroups by smoothing out variance in predictions. Crucially, they reveal that traditional balanced sampling is suboptimal when task difficulty varies between groups, finding that over-representing the more difficult subgroup can minimize performance gaps and maximize overall accuracy. The work provides practical guidance for designing fairer ML systems by highlighting the interplay between per-group task difficulty and representation.

## Method Summary
The study uses synthetic binary classification tasks with controlled difficulty (via label noise or rotated decision boundaries) and real vision datasets (CelebA hair-color prediction, CheXpert lung opacity classification) where subgroups are defined by gender. The method involves training homogeneous ensembles of identical architectures (FC networks for synthetic, ImageNet-pretrained ResNet-50 for real) with different random initializations. Ensembles of size 1-20 are constructed and predictions are averaged. A key innovation is using a proxy evaluator (single model trained on balanced data) to identify which subgroup is "harder," then adjusting sampling ratios in the training data to over-represent that group. The approach systematically explores how ensemble size and data balance affect both individual subgroup performance and overall accuracy.

## Key Results
- Homogeneous ensembles consistently improve both individual subgroup performance and overall accuracy, achieving positive-sum fairness
- The optimal sampling ratio shifts from 50-50 toward the harder subgroup as task difficulty increases (e.g., from 50-50 to 80-20 in high-noise scenarios)
- Ensemble size has diminishing returns but consistently reduces performance gaps while improving all groups' accuracy
- The "harder" subgroup (lower baseline accuracy) experiences greater relative improvement from ensembling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Homogeneous deep ensembles disproportionately improve performance for subgroups with higher task difficulty, narrowing the accuracy gap without handicapping the "easy" group.
- **Mechanism:** Ensembling reduces variance in predictions. Under-performing groups exhibit higher variance or sensitivity to initialization, and averaging multiple models stabilizes these high-variance subgroups more significantly than already stable, high-accuracy subgroups.
- **Core assumption:** The performance disparity observed in a single model is partially driven by variance that can be smoothed out, rather than purely systematic bias or lack of capacity.
- **Evidence anchors:** [abstract] "homogeneous ensembles consistently improve both individual subgroup performance... particularly benefiting under-performing subgroups"; [section] Page 5, Figure 2d shows "relative improvement for the under-served population also grew as the relative task difficulty increased."
- **Break condition:** If the subgroup gap is caused by strictly systematic errors (e.g., a missing feature for a specific demographic) rather than variance, ensembling will fail to mitigate the bias.

### Mechanism 2
- **Claim:** Ensembling achieves "positive-sum fairness," avoiding the "leveling-down" effect where fairness is bought at the cost of overall accuracy.
- **Mechanism:** Standard bias mitigation (like re-weighting) often forces a model to prioritize a minority group at the expense of the majority, reducing global utility. Ensembling instead adds capacity and robustness, demonstrating that all groups gain accuracy, but the disadvantaged group gains *more*, lifting the "floor" without lowering the "ceiling."
- **Core assumption:** The model is under-parameterized or unstable such that a single random initialization cannot simultaneously fit the optimal boundaries for both the easy and hard subgroups.
- **Evidence anchors:** [abstract] "achieved without sacrificing overall performance, which is a common trade-off observed in bias mitigation strategies"; [section] Page 5: "ensembles decrease the gap in performance while increasing the individual performance of both groups."
- **Break condition:** If the ensemble size grows too large and overfits the majority group, the benefits for the minority group may saturate or degrade relative to the majority.

### Mechanism 3
- **Claim:** When task difficulty is unequal, a 50-50 dataset balance is suboptimal; the optimal sampling ratio requires over-representing the "harder" subgroup.
- **Mechanism:** A harder subgroup (e.g., one with label noise) provides a weaker learning signal per sample. To compensate for this lower "information density," the optimization landscape requires more samples from the hard group to shift the decision boundary sufficiently. A balanced dataset effectively under-represents the hard group in terms of *usable* information.
- **Core assumption:** Task difficulty can be approximated by the performance gap in a controlled (balanced) setting, and this difficulty stems from noise or boundary complexity rather than data scarcity.
- **Evidence anchors:** [abstract] "perfectly balanced datasets may be suboptimal when task difficulty varies... over-representing the more difficult subgroup can minimize performance gaps"; [section] Page 6, Figure 3: "As we increase the noise percentage... the ideal balance ratio will be one with more females than males" (up to 80-20).
- **Break condition:** If the "hard" subgroup is not just noisy but fundamentally out-of-distribution or conceptually distinct, simply over-sampling it may cause the model to learn spurious correlations for that group.

## Foundational Learning

- **Concept:** **Positive-Sum Fairness vs. Leveling-Down**
  - **Why needed here:** Many fairness techniques degrade total accuracy to achieve parity (leveling-down). Understanding that this mechanism seeks to raise the floor (positive-sum) is critical for interpreting the results.
  - **Quick check question:** Does adding a model to the ensemble improve the accuracy of the *worst* group without dropping the accuracy of the *best* group?

- **Concept:** **Task Difficulty vs. Representation**
  - **Why needed here:** The paper decouples these two sources of bias. A group might perform poorly because it has few samples (Representation) or because the task is inherently harder for it (Difficulty, e.g., noise).
  - **Quick check question:** If you have 50-50 data split but a 10% accuracy gap, is the issue representation or difficulty?

- **Concept:** **Homogeneous vs. Heterogeneous Ensembles**
  - **Why needed here:** The paper explicitly uses *homogeneous* ensembles (same architecture, different init). This distinguishes the work from methods requiring diverse architectures.
  - **Quick check question:** Do we need to train a ResNet and a ViT together, or just 20 ResNets with different seeds? (Answer: The latter).

## Architecture Onboarding

- **Component map:** Proxy Evaluator -> Data Loader/Sampler -> Ensemble Trainer -> Aggregator
- **Critical path:** Determining the "Difficulty Delta." If you misidentify which group is harder, or by how much, you will over/under-sample incorrectly. The paper uses performance on a balanced set as the proxy for difficulty.
- **Design tradeoffs:**
  - **Inference Cost vs. Fairness:** An ensemble of 20 models requires 20x the inference compute. The paper shows diminishing returns but recommends sufficient size to smooth variance.
  - **Balanced vs. Optimal Data:** Using a balanced dataset is safer if difficulty is unknown; custom sampling ratios (e.g., 80-20) maximize performance but require careful validation to avoid overfitting the noisy group.
- **Failure signatures:**
  - **V-Shaped Gap Curves:** In Figure 4 (CheXpert), the gap drops then rises (V-shape). If the ensemble size is not tuned, the originally disadvantaged group might outperform the advantaged one, flipping the bias.
  - **Synthetic-Real Mismatch:** The "Rotating Boundary" synthetic test uses L1 regularization to induce difficulty. If your real-world regularization differs, the specific angle-to-balance heuristic may not transfer.
- **First 3 experiments:**
  1. **Difficulty Proxy Check:** Train a single model on a perfectly balanced subset. Verify that a performance gap exists (indicating difficulty difference) rather than just random variance.
  2. **Ensemble Scaling:** On a fixed dataset (balanced or slightly imbalanced), train ensembles of size $M \in \{1, 5, 10, 20\}$. Plot accuracy for *both* groups separately to confirm positive-sum behavior (both lines go up).
  3. **Sampling Ablation:** Keeping ensemble size fixed (e.g., $M=20$), sweep the minority-to-majority sampling ratio (e.g., 50-50, 60-40, 70-30). Plot "Overall Accuracy" and "Gap" on the same chart to find the intersection point (optimal fairness/performance).

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the positive-sum fairness effects of homogeneous deep ensembles generalize to machine learning domains and tasks beyond the binary image classification settings studied?
  - **Basis in paper:** [explicit] The authors state in the conclusion that "Substantial work remains to be done to understand if and how these findings generalize to other ML domains and tasks, opening up a valuable avenue of research."
  - **Why unresolved:** The study restricted its scope to synthetic binary classification and specific vision tasks (CelebA, CheXpert). It is unknown if the interplay between ensemble size, difficulty, and representation holds for NLP, reinforcement learning, or multi-class problems.
  - **What evidence would resolve it:** Results from experiments replicating this methodology on non-vision datasets (e.g., tabular fairness benchmarks) or multi-class classification tasks.

- **Open Question 2:** Can a more robust measure of "task difficulty" be developed that disentangles inherent group difficulty from sample variability or annotation uncertainty?
  - **Basis in paper:** [explicit] The limitations section notes the authors used performance gaps in balanced datasets as a proxy for difficulty. They suggest "Future research should account for this factor... Ideally, one would want a measure of difficulty which directly captures how often experts would fail."
  - **Why unresolved:** The current proxy assumes differences in balanced performance equate to difficulty, but this may conflate difficulty with data quality issues like lower sample variability or labeling errors.
  - **What evidence would resolve it:** Analysis using datasets with multiple expert annotators to correlate model performance gaps with human inter-rater reliability or expert error rates.

- **Open Question 3:** Do the fairness benefits of deep ensembles persist when using modern, non-CNN architectures such as Vision Transformers (ViTs)?
  - **Basis in paper:** [explicit] The limitations section explicitly states, "More complex architectures could be considered (like Visual Transformers for image classification, for example) to ensure that our conclusions also hold for larger vision models."
  - **Why unresolved:** ViTs possess different inductive biases and ensemble behaviors compared to the ResNets (CNNs) and fully connected layers used in the paper.
  - **What evidence would resolve it:** Comparative experiments using ViT ensembles on the same fairness benchmarks to observe if the relationship between ensemble size and performance gaps remains consistent.

## Limitations

- **Architectural details:** Key hyperparameters for both synthetic (FC) and real (ResNet-50) experiments are unspecified, making exact reproduction challenging
- **Real-world transferability:** Synthetic scenarios (label noise, rotated boundaries) provide controlled evidence but may not fully capture complex real-world bias patterns
- **Optimal balance heuristics:** The paper provides empirical optimal ratios for specific difficulty manipulations but doesn't establish a general formula for determining ideal sampling ratios in unknown conditions

## Confidence

- **High confidence:** Ensemble homogeneity reduces performance gaps (Mechanism 1) - supported by multiple datasets and ablation studies
- **High confidence:** Positive-sum fairness is achievable without degrading overall accuracy (Mechanism 2) - consistently demonstrated across experiments
- **Medium confidence:** Over-representation of harder subgroups is optimal (Mechanism 3) - strong synthetic evidence but fewer real-world validations
- **Medium confidence:** L1 regularization induces boundary difficulty in rotated scenario - mechanism is plausible but specific parameter sensitivity is unclear

## Next Checks

1. **Difficulty proxy validation:** Train single models on multiple balanced datasets (different seeds) to verify that performance gaps are consistent and not due to random variance
2. **Ensemble size scaling:** Systematically vary ensemble size (1, 5, 10, 20) on a fixed dataset to confirm both positive-sum behavior and diminishing returns
3. **Sampling ratio sensitivity:** Fix ensemble size and sweep minority-to-majority ratios across multiple noise levels to verify the V-shaped gap curves and optimal ratio shifts