---
ver: rpa2
title: Exploiting Edge Features for Transferable Adversarial Attacks in Distributed
  Machine Learning
arxiv_id: '2507.07259'
source_url: https://arxiv.org/abs/2507.07259
tags:
- surrogate
- attacks
- target
- feature
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates a novel vulnerability in distributed deep
  learning systems, where an attacker can intercept intermediate features transmitted
  between edge and cloud nodes. The authors propose a method to reconstruct the shape
  of these serialized feature tensors using covariance matrix analysis and develop
  a feature-aware surrogate training strategy.
---

# Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning

## Quick Facts
- arXiv ID: 2507.07259
- Source URL: https://arxiv.org/abs/2507.07259
- Authors: Giulio Rossolini; Fabio Brau; Alessandro Biondi; Battista Biggio; Giorgio Buttazzo
- Reference count: 40
- Primary result: Intermediate feature leakage in distributed ML systems enables up to 96% success rate in transferable adversarial attacks

## Executive Summary
This paper investigates a novel vulnerability in distributed deep learning systems where attackers can intercept intermediate features transmitted between edge and cloud nodes. The authors demonstrate that by reconstructing the shape of these serialized feature tensors through covariance matrix analysis and incorporating intermediate feature distillation into surrogate model training, they can significantly improve the transferability of black-box adversarial attacks. Their method achieves up to 96% success rate against ResNet56 targets, compared to only 69% without feature knowledge. The research reveals that intermediate feature leakage poses a serious security risk in partitioned inference systems and highlights the need for new defenses in edge AI environments.

## Method Summary
The authors propose a feature-aware surrogate training strategy that leverages intercepted intermediate features from distributed deep learning systems. Their approach combines covariance-based tensor shape reconstruction to infer the structure of serialized features with a feature distillation component that guides surrogate model training. By incorporating intermediate feature information into the surrogate training process, the method improves the alignment between surrogate and target models, resulting in more effective transferable adversarial attacks. The attack methodology operates in the black-box setting where the attacker has no access to the target model's architecture or parameters, relying instead on intercepted feature transmissions to inform the surrogate training process.

## Key Results
- Feature-aware surrogate models achieve up to 96% success rate in transferable attacks against ResNet56 targets
- Surrogate models without feature knowledge only achieve 69% attack success rate
- Covariance-based tensor shape reconstruction enables accurate inference of intermediate feature dimensions
- Feature distillation during surrogate training significantly improves transferability compared to standard training approaches

## Why This Works (Mechanism)
The attack exploits the information leakage that occurs when distributed deep learning systems partition computation between edge and cloud nodes. When intermediate features are transmitted between these components, they contain rich information about the model's internal representations. By intercepting and analyzing these features, attackers can reconstruct the tensor shapes and use this knowledge to create more aligned surrogate models. The feature distillation component ensures that the surrogate model learns to produce similar intermediate representations as the target model, which directly translates to better transferability of adversarial examples. This approach overcomes the typical limitations of black-box attacks by providing the surrogate training process with privileged information about the target model's feature space.

## Foundational Learning
- **Covariance matrix analysis**: Used to infer tensor shapes from serialized features; needed to reconstruct feature dimensions without prior architectural knowledge; quick check: verify covariance patterns match expected tensor structures
- **Feature distillation**: Technique for aligning intermediate representations between models; needed to improve surrogate-target model alignment; quick check: measure feature similarity metrics (e.g., MSE, correlation) between models
- **Transferable adversarial attacks**: Black-box attack methodology where adversarial examples crafted on surrogate models remain effective against target models; needed to operate without target model access; quick check: validate attack success rate across different target architectures
- **Partitioned inference systems**: Distributed ML architectures where computation is split between edge and cloud nodes; needed to identify the specific vulnerability context; quick check: confirm feature transmission occurs at known partition points
- **Serialized feature tensors**: Intermediate representations transmitted between system components; needed as the attack vector; quick check: ensure features are actually being transmitted in plaintext or weakly protected form
- **Surrogate model training**: Process of creating a substitute model to approximate target model behavior; needed to enable black-box attack generation; quick check: verify surrogate achieves comparable accuracy to target on validation data

## Architecture Onboarding

Component map: Edge device -> Feature extraction -> Serialized feature transmission -> Cloud processing -> Final prediction

Critical path: Feature interception at transmission point -> Shape reconstruction via covariance analysis -> Feature-aware surrogate training -> Adversarial example generation -> Attack deployment

Design tradeoffs: The attack trades computational overhead of feature analysis and surrogate training against improved attack success rates. The authors prioritize attack effectiveness over computational efficiency, accepting the additional complexity of covariance analysis and feature distillation.

Failure signatures: Attack failure occurs when covariance analysis cannot accurately reconstruct feature shapes, when feature distributions are too complex for reliable analysis, or when the surrogate model fails to align with the target despite feature knowledge. Network-level security measures that prevent feature interception would also cause failure.

First experiments:
1. Test covariance-based shape reconstruction accuracy across different model architectures and feature dimensionalities
2. Evaluate feature distillation effectiveness by measuring intermediate representation similarity between surrogate and target models
3. Measure attack success rate improvements when incrementally adding feature knowledge to surrogate training

## Open Questions the Paper Calls Out
None

## Limitations
- Attack methodology relies on specific architectural assumptions about distributed learning system partitioning
- Covariance-based shape reconstruction effectiveness varies with feature dimensionality and structure
- Practical feasibility depends on ability to intercept and analyze intermediate feature transmissions
- Limited exploration of diverse model architectures and compression techniques

## Confidence

High confidence in core finding that intermediate feature leakage enables more effective transferable attacks, supported by quantitative experimental results showing substantial improvement in attack success rates.

Medium confidence in generalizability of covariance-based shape reconstruction technique, as effectiveness may be architecture-dependent with limited exploration of diverse models.

Low confidence in practical feasibility of attack in real-world distributed systems, as assumes unprotected feature transmission without addressing potential network-level security controls.

## Next Checks

1. Test feature shape reconstruction method across broader range of deep learning architectures (transformers, vision transformers, varying intermediate feature dimensionalities) to assess generalizability.

2. Evaluate attack effectiveness when intermediate features undergo compression schemes (quantization, sparsification, pruning) commonly used in edge-to-cloud communication.

3. Investigate whether feature-aware surrogate training remains effective with partial or noisy access to intermediate features, simulating realistic interception scenarios with packet loss or encryption artifacts.