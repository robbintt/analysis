---
ver: rpa2
title: Agent-centric Information Access
arxiv_id: '2502.19298'
source_url: https://arxiv.org/abs/2502.19298
tags:
- llms
- retrieval
- information
- expert
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for agent-centric information access,
  where large language models (LLMs) function as knowledge agents dynamically ranked
  and queried based on their demonstrated expertise. Unlike traditional document retrieval,
  this approach requires inferring expertise on the fly, rather than relying on static
  metadata.
---

# Agent-centric Information Access

## Quick Facts
- arXiv ID: 2502.19298
- Source URL: https://arxiv.org/abs/2502.19298
- Reference count: 40
- This paper proposes a framework for agent-centric information access, where large language models (LLMs) function as knowledge agents dynamically ranked and queried based on their demonstrated expertise.

## Executive Summary
This paper introduces a novel paradigm for information access that shifts from document retrieval to agent-based knowledge retrieval. The framework treats specialized LLMs as knowledge agents that can be dynamically selected and queried based on inferred expertise rather than static metadata. The authors identify key challenges including efficient expert selection, cost-effective querying, response aggregation, and robustness against adversarial manipulation. They propose a scalable evaluation framework using RAG over clustered document collections to simulate thousands of specialized agents.

## Method Summary
The framework operates through a user agent that maintains a belief model about expertise, dynamically selecting knowledge agents for each query based on predicted relevance and cost. Rather than training distinct models, expertise is simulated using a shared base LLM combined with domain-specific document clusters accessed through RAG. The system implements cost-aware query budgeting with tiered or adaptive strategies to optimize efficiency. A scalable evaluation framework leverages retrieval-augmented generation and clustering techniques to construct and assess thousands of specialized models.

## Key Results
- Proposes agent-centric information access framework where LLMs function as dynamically ranked knowledge agents
- Identifies key challenges: expert selection, cost-effective querying, response aggregation, and adversarial robustness
- Introduces scalable evaluation framework using RAG and clustering to simulate thousands of specialized models
- Shifts paradigm from document retrieval to agent-based knowledge retrieval

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The proposed system functions by dynamically ranking specialized LLMs ("knowledge agents") using a continuously updated "belief model" rather than static metadata.
- **Mechanism:** A user agent maintains a belief state regarding the expertise of available knowledge agents. Upon receiving a query, the agent consults this belief model to select a subset of agents, balancing predicted relevance against query cost. Feedback from the response updates the belief model for future iterations.
- **Core assumption:** Expertise can be reliably inferred from past interactions or "demonstrated expertise" without access to internal training data or standardized model cards.
- **Evidence anchors:**
  - [abstract] "...inferring expertise on the fly, rather than relying on static metadata..."
  - [section] Figure 1 description (Page 2): "A belief model on expertise determines which knowledge agents... should be queried... continuously updated based on user interactions."
  - [corpus] Neighbor papers like *GeneGPT* and *AgriLens* demonstrate domain-specific retrieval, supporting the feasibility of specialized agents, but do not validate the specific "belief model" ranking dynamics proposed here.
- **Break condition:** If agents provide inconsistent or adversarial feedback that destabilizes the belief convergence, the ranking mechanism may fail to identify true experts.

### Mechanism 2
- **Claim:** The evaluation framework operates by simulating thousands of specialized experts using Retrieval-Augmented Generation (RAG) over clustered document collections.
- **Mechanism:** Instead of training distinct neural weights for thousands of agents, the system uses a shared base LLM. "Expertise" is simulated by restricting the retriever for each agent to a specific document cluster (e.g., clustering ClueWeb09 via K-Means). The LLM generates answers based only on documents retrieved from its assigned cluster.
- **Core assumption:** Access to a restricted document corpus via RAG is a sufficient proxy for the behavior of a truly fine-tuned domain expert model.
- **Evidence anchors:**
  - [abstract] "...leveraging retrieval-augmented generation and clustering techniques to construct and assess thousands of specialized models."
  - [section] Section 4.3 (Page 7): "Each simulated expert consists of: A (shared) underlying LLM... [and] A unique document collection, defining the domain expertise."
  - [corpus] *MiRAGE* and *AgriLens* validate the use of RAG for domain-specific tasks, supporting the simulation technique as a viable proxy for specialization.
- **Break condition:** If the base LLM "leaks" parametric knowledge (ignoring the restricted context) or if the clustering is too coarse, the simulated expertise boundaries dissolve.

### Mechanism 3
- **Claim:** The architecture optimizes for efficiency by implementing cost-aware query budgeting, limiting the number of agents invoked per query.
- **Mechanism:** The system uses a tiered or adaptive querying strategy. It predicts the utility of querying an agent against its cost (latency/fees). It may query a small set of high-confidence agents first and stop if consensus is reached, or employ "progressive search" techniques to minimize redundancy.
- **Core assumption:** The system can accurately predict the marginal information gain of querying an additional agent before actually paying the cost of the query.
- **Evidence anchors:**
  - [abstract] "...selecting a small subset of relevant models, querying them efficiently..."
  - [section] Section 3.1.3 (Page 4): "...implement a tiered querying strategy... [or] adaptive querying strategy, where it starts with a small number... and only expands... if responses are inconsistent."
  - [corpus] Weak direct validation; neighbor papers focus on capability rather than cost budgeting.
- **Break condition:** If the utility prediction model is poorly calibrated, the system may either exhaust its budget on low-quality agents or stop early with insufficient information.

## Foundational Learning

- **Concept: Distributed Information Retrieval (DIR) & Federated Search**
  - **Why needed here:** The paper explicitly positions itself as an evolution of DIR. You must understand "Resource Selection" (choosing which collection to query) to understand why "Expert Selection" in multi-LLM systems is harder (lack of standardized metadata).
  - **Quick check question:** How does "resource selection" in federated search differ from "expert selection" when the resources (LLMs) lack static metadata?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** This is the core technology used to simulate the expert agents in the evaluation framework. Understanding how retrieval context grounds generation is critical to understanding the paper's experimental setup.
  - **Quick check question:** In the paper's simulation, what defines the boundary of an "expert" agent?

- **Concept: Clustering (K-Means / Hashing Vectorizer)**
  - **Why needed here:** The framework relies on clustering large corpora (ClueWeb) to create synthetic "domains." You need to understand how document clusters serve as proxies for proprietary knowledge bases.
  - **Quick check question:** Why does the paper suggest using a "Hashing Vectorizer" instead of TF-IDF for clustering massive datasets like ClueWeb?

## Architecture Onboarding

- **Component map:**
  - User Agent -> Belief Model (ranking logic) -> Coordinator/Orchestrator -> Knowledge Agents (Shared LLM + Dedicated Retriever + Clustered Document KB) -> Global Index

- **Critical path:**
  1. Query Ingestion: User Agent receives $q$.
  2. Expert Ranking: Agent uses Belief Model to rank available Knowledge Agents (Section 4.1).
  3. Selection: Agent selects top $N$ agents based on budget $b$.
  4. Retrieval (Simulation): Selected agents retrieve docs from their specific clusters.
  5. Synthesis: LLM generates answer using retrieved context; User Agent aggregates responses.

- **Design tradeoffs:**
  - Simulation Fidelity vs. Efficiency: The paper notes the trade-off between building $K$ separate indices (high fidelity, high cost) vs. one global index (lower fidelity, high efficiency).
  - Prompting vs. Filtering: To prevent knowledge leakage, the paper trades off between "Prompt Engineering" (cheap, unreliable) and "Query Filtering" (expensive, robust) (Section 4.3).

- **Failure signatures:**
  - Knowledge Leakage: Base LLM answers correctly without using retrieved context, invalidating the "simulated expert" assumption.
  - Monoculture/Bias: The Belief Model converges on a single "dominant" agent, reinforcing bias (Section 3.3.1).
  - Stale Beliefs: The Belief Model fails to adapt to temporal changes in expertise (Section 3.1.4).

- **First 3 experiments:**
  1. Baseline Verification: Confirm that simulated experts (RAG + Cluster) perform better on in-domain queries than out-of-domain queries to validate the simulation method.
  2. Leakage Test: Query simulated experts on topics *outside* their cluster's scope but *inside* the base LLM's parametric memory to measure the extent of knowledge leakage.
  3. Budget Impact: Evaluate ranking accuracy (nDCG) vs. query budget size ($b$) to find the point of diminishing returns in the cost-performance trade-off.

## Open Questions the Paper Calls Out
None

## Limitations
- The "belief model" mechanism for dynamic expertise inference lacks empirical validation and demonstration
- The simulation framework using RAG over clustered documents may not accurately represent truly fine-tuned domain experts
- Cost-aware querying strategy assumes accurate utility prediction without providing supporting evidence
- Adversarial robustness is acknowledged as a challenge but not addressed with concrete solutions

## Confidence
- **Medium Confidence:** The general framework architecture (belief model + knowledge agents + aggregation) is well-specified and logically coherent. The use of RAG for simulation is technically sound.
- **Low Confidence:** The dynamic expertise inference mechanism lacks empirical validation. The cost prediction and utility optimization claims are theoretical without supporting experiments. The adversarial robustness section identifies problems but provides no solutions.
- **Medium-Low Confidence:** The evaluation methodology's fidelity - whether simulated experts truly represent specialized LLMs - remains unproven.

## Next Checks
1. **Expertise Inference Validation:** Design an experiment where knowledge agents provide inconsistent or adversarial feedback to test whether the belief model converges correctly or gets destabilized, directly testing the break condition identified in Mechanism 1.

2. **Simulation Fidelity Test:** Measure the performance gap between truly fine-tuned domain experts and RAG-simulated experts on the same tasks to quantify how well the clustering + RAG approach captures specialized knowledge.

3. **Budget-Utility Calibration:** Conduct experiments varying query budgets systematically to identify whether the predicted marginal utility of additional agents matches observed performance gains, testing the core assumption of Mechanism 3.