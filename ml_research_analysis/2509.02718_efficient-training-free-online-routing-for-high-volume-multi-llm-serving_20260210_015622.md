---
ver: rpa2
title: Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving
arxiv_id: '2509.02718'
source_url: https://arxiv.org/abs/2509.02718
tags:
- cost
- performance
- perf
- routing
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first training-free online routing algorithm
  for high-volume, budget-constrained multi-LLM serving. The algorithm uses Approximate
  Nearest Neighbor Search (ANNS) to efficiently estimate query features from historical
  data, then performs a one-time optimization over a small set of initial queries
  to learn routing weights that guide subsequent routing decisions.
---

# Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving

## Quick Facts
- **arXiv ID**: 2509.02718
- **Source URL**: https://arxiv.org/abs/2509.02718
- **Reference count**: 40
- **One-line primary result**: First training-free online routing algorithm for budget-constrained multi-LLM serving that outperforms 8 baselines with 3.55x performance improvement.

## Executive Summary
This paper introduces a training-free online routing algorithm for efficiently serving sequential queries across multiple LLMs under token budget constraints. The method uses Approximate Nearest Neighbor Search to estimate query features from historical data, then learns routing weights through one-time optimization over a small initial query subset. The algorithm achieves competitive performance without requiring model retraining or fine-tuning, making it highly practical for real-world deployment. Extensive experiments demonstrate significant improvements over existing methods across three benchmarks.

## Method Summary
The algorithm operates in three phases: feature estimation using HNSW to find nearest neighbors in historical data, an observation phase where initial queries are routed randomly to collect performance data, and weight learning using L-BFGS-B optimization to determine routing weights. For subsequent queries, the system routes based on a scoring function that balances estimated performance against weighted costs. The method requires only one optimization step and works efficiently with sequential query arrivals, achieving competitive ratios approaching the offline optimum under natural assumptions.

## Key Results
- Achieves 3.55x improvement in average performance compared to baselines
- Delivers 1.85x better cost efficiency (performance per cost)
- Provides nearly 4.25x higher throughput (queries served)
- Demonstrates robustness across varying query volumes, arrival orders, and budget splitting strategies
- Requires only one optimization over a small query subset, making it highly efficient

## Why This Works (Mechanism)
The algorithm leverages historical query-response data to build an HNSW index for fast approximate nearest neighbor search. This enables efficient estimation of expected performance and cost vectors for incoming queries without expensive model inference. The one-time optimization over initial queries learns routing weights that balance performance gains against cost constraints. By using these weights to guide subsequent routing decisions, the system achieves near-optimal performance without the computational overhead of online learning or model retraining. The competitive ratio analysis provides theoretical guarantees that the solution approaches the offline optimum.

## Foundational Learning

**Approximate Nearest Neighbor Search (ANNS)**: Why needed - Enables fast estimation of query features from historical data without expensive model inference. Quick check - Verify HNSW returns reasonable neighbors by checking distance distributions and neighbor quality.

**Primal-Dual Analysis**: Why needed - Provides theoretical guarantees about competitive ratio performance. Quick check - Review dual function formulation and verify constraint satisfaction in the optimization.

**Budget-Constrained Optimization**: Why needed - Ensures routing decisions respect per-model token budget limits. Quick check - Monitor cumulative cost vs. budget during routing to verify constraints are maintained.

## Architecture Onboarding

**Component Map**: Historical Data + HNSW Index -> Feature Estimator -> Observation Phase -> Weight Optimizer (L-BFGS-B) -> Router (scoring function) -> LLM Selection

**Critical Path**: Query arrives → ANNS feature estimation → Score calculation (performance - cost × weight) → LLM selection if budget permits → Response collection → Cost update

**Design Tradeoffs**: Static routing weights offer efficiency but may underperform with distribution shifts vs. dynamic weights that adapt but require more computation. Random routing during observation provides unbiased data but wastes early queries vs. more sophisticated exploration strategies.

**Failure Signatures**: Early budget exhaustion indicates cost underestimation; degenerate routing weights suggest optimization failure; poor neighbor quality degrades estimator accuracy; random routing phase underperforms on small datasets.

**First Experiments**: 1) Test estimator accuracy by comparing predicted vs. actual costs on held-out historical data. 2) Verify optimization convergence by monitoring weight stability and objective value. 3) Benchmark routing performance with varying observation phase sizes (0.01 to 0.05 fraction).

## Open Questions the Paper Calls Out
None

## Limitations
- Estimator accuracy heavily depends on historical data quality and representativeness
- Static routing weights may become outdated with significant query distribution shifts
- Implementation details of baselines lack full specification, complicating independent validation

## Confidence

**High Confidence**: Theoretical competitive ratio of 1-o(1) compared to offline optimum based on primal-dual analysis framework.

**Medium Confidence**: Empirical performance improvements of 3.55x in performance, 1.85x in cost efficiency, and 4.25x in throughput across three benchmarks.

**Low Confidence**: Absolute magnitude of improvements across all possible query distributions and model configurations, particularly with non-stationary streams or model behavior drift.

## Next Checks

1. **Estimator Robustness Test**: Conduct experiments with intentionally shifted query distributions compared to historical data to measure degradation in cost/performance estimates and subsequent routing impact.

2. **Dynamic Weight Adaptation**: Implement periodic re-optimization of routing weights (e.g., every 10% of query stream) and compare performance against static weights on long, non-stationary query streams.

3. **Baseline Implementation Replication**: Faithfully implement the strongest baseline method using only paper descriptions and run head-to-head comparison on RouterBench to verify claimed performance gaps.