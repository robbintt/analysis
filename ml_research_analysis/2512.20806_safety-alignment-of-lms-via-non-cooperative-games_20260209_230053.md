---
ver: rpa2
title: Safety Alignment of LMs via Non-cooperative Games
arxiv_id: '2512.20806'
source_url: https://arxiv.org/abs/2512.20806
tags:
- attacker
- defender
- safety
- judge
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdvGame, a novel framework for safety alignment
  of language models (LMs) that frames the problem as a non-cooperative game between
  an Attacker LM and a Defender LM. Rather than using sequential adversarial training,
  both LMs are jointly trained via online reinforcement learning, allowing each to
  adapt continuously to the other's strategies.
---

# Safety Alignment of LMs via Non-cooperative Games

## Quick Facts
- arXiv ID: 2512.20806
- Source URL: https://arxiv.org/abs/2512.20806
- Reference count: 40
- Introduces AdvGame framework achieving state-of-the-art safety alignment performance

## Executive Summary
This paper introduces AdvGame, a novel framework for safety alignment of language models that frames the problem as a non-cooperative game between an Attacker LM and a Defender LM. Rather than using sequential adversarial training, both LMs are jointly trained via online reinforcement learning, allowing each to adapt continuously to the other's strategies. The method employs preference-based reward signals derived from pairwise comparisons rather than point-wise scores, providing more robust supervision and reducing reward hacking vulnerabilities. AdvGame uses separate models for Attacker and Defender without parameter sharing, distinguishing it from self-play approaches.

## Method Summary
AdvGame employs a non-cooperative game framework where an Attacker LM and Defender LM are jointly trained through online reinforcement learning. The Attacker generates adversarial prompts to elicit harmful responses from the Defender, while the Defender learns to refuse such requests. Both models receive rewards based on pairwise preference judgments from a separate Judge model, rather than absolute reward signals. The training uses off-policy sampling with exponential moving average updates to stabilize learning. Key variants include AdvGame-DPO-MD (using Direct Preference Optimization) and AdvGame-IPO-MD (using Implicit Preference Optimization). The framework maintains separate models for Attacker and Defender, unlike self-play approaches, and employs continuous online training rather than sequential rounds.

## Key Results
- AdvGame variants achieve state-of-the-art performance on safety benchmarks while preserving utility on Qwen2.5-7B and Llama3.1-8B models
- The Defender LM shows greater robustness against adaptive attacks, particularly black-box methods generating human-readable adversarial prompts
- The trained Attacker LM becomes a strong general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models
- Ablation studies demonstrate that actively training the Attacker, using pairwise preference judging, and employing off-policy sampling with EMA are critical for effective adversarial training

## Why This Works (Mechanism)
The framework works by creating a competitive dynamic where the Attacker continuously discovers novel attack vectors while the Defender learns to counter them in real-time. Unlike sequential adversarial training where the Defender only responds to past attacks, the joint training allows the Defender to develop generalized safety mechanisms rather than overfitting to specific attack patterns. The pairwise preference-based reward system provides richer supervision signals that capture relative quality rather than absolute thresholds, making it harder for either player to exploit reward functions through reward hacking.

## Foundational Learning
- **Non-cooperative game theory**: Needed because safety alignment requires modeling adversarial interactions between attacker and defender agents; quick check: verify payoff matrices capture the strategic tension
- **Online reinforcement learning**: Required for continuous adaptation where both agents learn simultaneously rather than in isolated phases; quick check: monitor convergence stability across training iterations
- **Preference-based learning**: Essential for obtaining relative quality judgments that are more robust than absolute reward signals; quick check: validate pairwise comparisons reduce reward hacking
- **Adversarial prompting**: Core threat model where carefully crafted inputs bypass safety mechanisms; quick check: measure attack success rate against baseline models
- **Red teaming methodologies**: Provides systematic approach to discovering model vulnerabilities; quick check: assess discovered failure modes' diversity and severity
- **Zero-sum game dynamics**: Ensures that Defender improvements directly translate to Attacker degradation, creating efficient learning pressure; quick check: verify training loss trends show opposing patterns

## Architecture Onboarding

**Component Map**
Attacker LM -> Judge -> Defender LM -> Judge -> Reward Signal -> Both LMs

**Critical Path**
1. Attacker generates adversarial prompt
2. Prompt passed to Defender
3. Judge evaluates Defender response quality
4. Reward signal computed via pairwise preference
5. Both LMs update via RL algorithms
6. Repeat with new adversarial prompts

**Design Tradeoffs**
- Separate vs. shared parameters: Separate models enable true adversarial dynamics but double computational cost
- Pairwise vs. pointwise rewards: Pairwise provides richer supervision but requires more preference data
- Online vs. sequential training: Online enables continuous adaptation but requires stable learning algorithms
- Judge quality vs. training efficiency: Better judges improve alignment but increase human annotation burden

**Failure Signatures**
- Defender overfitting to specific attack patterns rather than learning general safety
- Attacker converging to repetitive, low-diversity attack strategies
- Judge model introducing systematic bias in preference judgments
- Reward hacking where agents exploit loopholes in preference evaluation

**3 First Experiments**
1. Train AdvGame with only sequential adversarial rounds (no joint RL) to establish baseline performance
2. Replace pairwise judging with pointwise rewards to measure impact on safety alignment quality
3. Test Defender robustness against holdout attacks not seen during training to validate generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of maintaining and training two separate LMs simultaneously may limit scalability
- Reliance on pairwise preference judgments introduces potential human bias and scalability challenges in preference collection
- Evaluation primarily focuses on English-language safety benchmarks, leaving multilingual effectiveness uncertain

## Confidence

**High Confidence**: The core methodology of framing safety alignment as a non-cooperative game between Attacker and Defender LMs is technically sound and well-supported by experimental results. The demonstration that AdvGame variants outperform baseline methods on safety benchmarks is robust.

**Medium Confidence**: Claims about AdvGame producing a deployable red-teaming agent require further validation across diverse target models and threat scenarios. The generalizability of the Attacker LM beyond the specific training setup needs additional empirical support.

**Medium Confidence**: While ablation studies identify key components (active Attacker training, pairwise judging, off-policy sampling), the relative importance of these factors across different model scales and domains remains to be fully established.

## Next Checks

1. **Cross-Lingual Safety Transfer**: Evaluate whether the Defender LM trained primarily on English safety benchmarks maintains robustness when tested against adversarial attacks in non-English languages, particularly for languages with limited safety training data.

2. **Resource Efficiency Analysis**: Conduct systematic experiments measuring the wall-clock time, GPU memory usage, and carbon footprint of AdvGame compared to sequential adversarial training methods across different model sizes (7B, 13B, 34B parameters).

3. **Attacker Transferability Assessment**: Test the Attacker LM as a red-teaming agent against a diverse set of proprietary and open-source models not seen during training, measuring both attack success rates and the diversity of failure modes discovered.