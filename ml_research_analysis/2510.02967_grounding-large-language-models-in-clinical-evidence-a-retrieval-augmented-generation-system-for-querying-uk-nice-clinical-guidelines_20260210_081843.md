---
ver: rpa2
title: 'Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented
  Generation System for Querying UK NICE Clinical Guidelines'
arxiv_id: '2510.02967'
source_url: https://arxiv.org/abs/2510.02967
tags:
- system
- context
- guidelines
- clinical
- nice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops and evaluates a retrieval-augmented generation
  system for querying UK NICE clinical guidelines. The system addresses the challenge
  of navigating extensive clinical documents by combining semantic search with LLM-based
  question answering.
---

# Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines

## Quick Facts
- arXiv ID: 2510.02967
- Source URL: https://arxiv.org/abs/2510.02967
- Authors: Matthew Lewis; Samuel Thio; Amy Roberts; Catherine Siju; Whoasif Mukit; Rebecca Kuruvilla; Zhangshu Joshua Jiang; Niko Möller-Grell; Aditya Borakati; Richard JB Dobson; Spiros Denaxas
- Reference count: 10
- Primary result: RAG system achieves 98.7% accuracy with GPT-4.1 while reducing unsafe responses by 67% compared to standalone LLMs

## Executive Summary
This study develops a retrieval-augmented generation (RAG) system for querying UK NICE clinical guidelines, addressing the challenge of navigating extensive clinical documents. The system combines semantic search with LLM-based question answering to provide accurate, evidence-based responses. Clinical evaluation by NHS experts demonstrated 98.7% accuracy while significantly reducing unsafe responses, establishing RAG as a scalable approach for safe deployment of generative AI in healthcare.

## Method Summary
The system employs a hybrid retrieval approach using Voyage-3-Large embeddings and BM25 to retrieve relevant passages from NICE guidelines, followed by LLM-based question answering with grounding in retrieved evidence. The architecture was evaluated on a dataset of 36 expert-curated questions across clinical specialties, with clinical accuracy assessed by 7 NHS experts. Performance was compared against standalone medical-specific models and traditional keyword search methods, demonstrating superior reliability and safety metrics.

## Key Results
- RAG architecture increases faithfulness from 34.7% to 99.5% for O4-Mini, outperforming medical-specific models
- Clinical evaluation confirms 98.7% accuracy with GPT-4.1 while reducing unsafe responses by 67%
- Hybrid retrieval achieves MRR of 0.814 and 99.1% recall at top 10 results
- System demonstrates cost-effectiveness compared to large medical-specific models

## Why This Works (Mechanism)
The RAG approach grounds LLM responses in authoritative clinical evidence by retrieving relevant guideline passages before generating answers. This mitigates hallucination risks inherent in standalone LLMs by constraining responses to evidence present in the guidelines. The hybrid retrieval combining semantic embeddings with BM25 ensures both semantic relevance and keyword matching, improving coverage across diverse clinical queries. Clinical evaluation confirms that evidence grounding significantly improves both accuracy and safety compared to ungrounded generation.

## Foundational Learning
- **Retrieval-Augmented Generation**: Why needed - prevents hallucination by grounding responses in retrieved evidence; Quick check - verify retrieval system returns relevant passages before generation
- **Hybrid Search**: Why needed - combines semantic understanding with keyword matching for comprehensive coverage; Quick check - evaluate both semantic similarity and keyword overlap metrics
- **Clinical Guideline Structure**: Why needed - understanding document organization enables effective passage selection; Quick check - confirm passage boundaries align with clinical concept boundaries
- **Medical Terminology**: Why needed - accurate retrieval requires understanding clinical language variants; Quick check - test retrieval with synonyms and related clinical terms
- **Evaluation Metrics**: Why needed - proper assessment requires both technical and clinical validation; Quick check - cross-validate automated metrics with expert clinical review

## Architecture Onboarding

**Component Map**: User Query -> Hybrid Retriever (Voyage-3-Large + BM25) -> Passage Selector -> LLM Generator -> Answer with Citations

**Critical Path**: Query processing → Hybrid retrieval → Top-k passage selection → LLM generation with context → Answer formatting with citations

**Design Tradeoffs**: Hybrid retrieval balances semantic relevance (Voyage-3-Large) with keyword matching (BM25) at computational cost; top-k selection trades comprehensiveness for latency; citation inclusion adds transparency but may reduce fluency

**Failure Signatures**: 
- Retrieval misses: LLM generates unsupported answers (hallucination)
- Over-retrieval: Context dilution reduces answer precision
- Citation errors: LLM cites incorrect passages or fabricates citations
- Domain mismatch: System struggles with non-NICE guideline formats

**First Experiments**:
1. Query "What is the first-line treatment for hypertension?" and verify passages retrieved cover NICE guidelines
2. Test "How should diabetic ketoacidosis be managed?" to evaluate retrieval of emergency protocols
3. Query "What are the criteria for mechanical ventilation?" to assess retrieval of decision-making algorithms

## Open Questions the Paper Calls Out
None

## Limitations
- Small evaluation dataset (36 questions) and limited expert panel (7 NHS experts assessing 20 questions)
- System tested exclusively on NICE guidelines, limiting generalizability to other document types or international guidelines
- Controlled retrieval backend makes it difficult to isolate model-specific performance contributions
- Clinical evaluation lacks statistical power for robust safety claims

## Confidence

**High Confidence**: Retrieval performance metrics (MRR, recall) and core RAG architecture implementation are well-documented and reproducible

**Medium Confidence**: Clinical evaluation results showing 98.7% accuracy with GPT-4.1, given the small expert sample size (n=7) and limited question set (n=20)

**Medium Confidence**: Comparative performance against medical-specific models, as the controlled retrieval backend makes attribution of performance gains uncertain

**Low Confidence**: Generalizability to other clinical domains, document types, or healthcare systems beyond NICE guidelines

## Next Checks

1. Expand clinical evaluation to include a larger, more diverse panel of healthcare professionals (target n=20-30) assessing answers across 100+ clinically relevant questions spanning multiple specialties

2. Conduct cross-domain validation by testing the RAG system on different guideline corpora (e.g., American Heart Association guidelines, WHO recommendations) to assess portability and performance consistency

3. Perform ablation studies comparing RAG performance with and without retrieval augmentation across multiple LLM architectures to isolate the contribution of the RAG framework versus model-specific capabilities