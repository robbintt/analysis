---
ver: rpa2
title: Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion
  Classification and ACL Tear Detection
arxiv_id: '2508.17567'
source_url: https://arxiv.org/abs/2508.17567
tags:
- radimagenet
- breast
- performance
- imagenet
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the RadImageNet transfer learning framework by
  conducting a systematic investigation of CNN architectures for medical imaging classification.
  The authors experiment with various classifier architectures, unfreezing strategies,
  and hyperparameters for ACL tear and breast lesion malignancy detection tasks.
---

# Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection

## Quick Facts
- arXiv ID: 2508.17567
- Source URL: https://arxiv.org/abs/2508.17567
- Reference count: 40
- Primary result: 1D convolutional classifiers with skip connections and partial backbone unfreezing yield optimal performance, with ImageNet pretraining outperforming RadImageNet

## Executive Summary
This study systematically investigates CNN architectures for medical imaging classification, focusing on transfer learning from general to medical domains. The authors experiment with various classifier architectures, unfreezing strategies, and hyperparameters for ACL tear detection and breast lesion malignancy classification. They find that 1D convolutional classifiers with skip connections, ResNet50 backbones, and partial backbone unfreezing (top 3-5 layer groups) yield optimal downstream performance. Notably, their results contradict previous findings by showing ImageNet pretraining statistically significantly outperforms RadImageNet pretraining for both tasks, suggesting classifier architecture and optimization choices may matter more than pretraining data selection.

## Method Summary
The study employs ResNet50 backbones with either ImageNet or RadImageNet pretraining weights, modified with various classifier heads including Linear, Conv, and ConvSkip architectures. Models are trained using SGD with Nesterov momentum (0.9), cosine annealing learning rate schedules, and dropout (0.5). Key experimental variables include backbone unfreezing depth (0-7 layer groups), classifier architecture, kernel sizes, and learning rates. The ACL tear detection task uses MRNet knee MRI data (1,021 exams), while breast lesion malignancy classification uses Kaggle breast ultrasound images (780 images). Both datasets use stratified 75/15/10 train/validation/test splits, with primary evaluation via AUC and statistical comparison using DeLong tests.

## Key Results
- 1D convolutional classifiers with skip connections ("ConvSkip") significantly outperform standard Linear classifiers, achieving AUC improvements of 0.313 points
- Partial backbone unfreezing of top 3-5 ResNet50 layer groups optimizes the trade-off between feature retention and domain adaptation
- ImageNet pretraining statistically significantly outperforms RadImageNet pretraining for both ACL tear detection (p<0.05) and breast malignancy classification (p<0.05)
- Best models achieve test AUCs of 0.9969 for ACL detection and 0.641 for breast malignancy classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partial backbone unfreezing optimizes the trade-off between feature retention and domain adaptation.
- **Mechanism:** By unfreezing the top 3-5 layer groups of the ResNet50 backbone while keeping earlier layers frozen, the model adapts high-level spatial features to the medical domain (MRI/Ultrasound) without destroying the low-level generic feature extractors (edges, textures) learned from the source dataset.
- **Core assumption:** Assumption: The optimal number of unfrozen layers (3-5 groups) is robust across similar medical imaging modalities and dataset sizes.
- **Evidence anchors:**
  - [abstract] "ResNet50 pre-trained backbones, and partial backbone unfreezing yields optimal downstream medical classification performance."
  - [section 5.2] "Best model performance... occurs for 3-5 unfrozen ResNet50 backbone layer groups... too much unfreezing might make learning harder [due to] gradient dispersion."
- **Break condition:** Unfreezing too many layers (e.g., >7 groups) introduces gradient dispersion and optimization instability on small datasets.

### Mechanism 2
- **Claim:** 1D-Convolutional classifiers with skip connections ("ConvSkip") outperform standard linear or fully connected heads for extracting spatial dependencies from flattened feature vectors.
- **Mechanism:** A 1D convolutional layer acts on the flattened backbone output to capture local relationships among feature channels, while the skip connection stabilizes gradient flow and preserves information, preventing the "bottleneck" often caused by simple linear projections.
- **Core assumption:** The feature vectors extracted by the backbone contain spatially adjacent dependencies that 1D convolutions can exploit better than global average pooling followed by dense layers.
- **Evidence anchors:**
  - [abstract] "1-dimensional convolutional classifiers with skip connections... yield optimal downstream medical classification performance."
  - [table 5] Mixed-model analysis shows Linear classifiers significantly underperform (Estimate: -0.313), while ConvSkip shows a positive improvement over the baseline Conv classifier.
- **Break condition:** If the backbone outputs highly fragmented or non-sequential features without local correlation, the 1D convolutional inductive bias provides no benefit over dense layers.

### Mechanism 3
- **Claim:** ImageNet pretraining provides statistically significantly better initialization than RadImageNet for these specific tasks when using PyTorch implementations.
- **Mechanism:** The paper suggests ImageNet's diversity (varied textures/scenes) or potential implementation discrepancies in RadImageNet PyTorch weights may hinder convergence. Specifically, the authors found RadImageNet resulted in performance plateaus in later epochs, whereas ImageNet allowed continued convergence.
- **Core assumption:** Assumption: The observed performance gap is intrinsic to the pretraining datasets/weights and not solely an artifact of the specific hyperparameter search space used in this study.
- **Evidence anchors:**
  - [abstract] "We do not find evidence confirming RadImageNet pre-training to provide superior downstream performance... ImageNet pretraining yields statistically significant better results."
  - [corpus] While neighbors like [arXiv 2509.02710] adapt foundation models for breast MRI, this paper specifically isolates the CNN backbone source, contradicting prior domain-specific pretraining assumptions.
- **Break condition:** This finding may not generalize if the RadImageNet weights are rigorously tuned for PyTorch independently, or if the downstream task involves anatomies heavily represented in RadImageNet but absent in ImageNet (though the paper notes RadImageNet lacked breast pathology images).

## Foundational Learning

### Concept: Transfer Learning (Backbone vs. Head)
- **Why needed here:** The entire methodology relies on separating the model into a feature extractor (Backbone) and a classifier (Head). Understanding what to freeze and what to train is the core experimental variable.
- **Quick check question:** Why would you freeze the bottom layers of a network but unfreeze the top layers when transfer learning to a new domain?

### Concept: Skip Connections (Residual Connections)
- **Why needed here:** The optimal "ConvSkip" architecture relies on element-wise addition to merge pathways. Without understanding this, the choice of classifier architecture seems arbitrary.
- **Quick check question:** How does adding the input of a layer to its output help mitigate the vanishing gradient problem?

### Concept: AUC (Area Under the ROC Curve)
- **Why needed here:** This is the primary evaluation metric used for all statistical comparisons (DeLong tests).
- **Quick check question:** Why is AUC a better metric than raw Accuracy for medical binary classification tasks where negative samples might vastly outnumber positive ones?

## Architecture Onboarding

### Component map:
256x256 Grayscale MRI/Ultrasound -> Augmented (Rotation/Shift/Shear) -> Normalized (ImageNet stats) -> ResNet50 Backbone -> ConvSkip Head -> Softmax

### Critical path:
Data Normalization -> Partial Unfreezing Configuration -> ConvSkip Head Construction -> SGD with Momentum (0.9) + Cosine Annealing LR

### Design tradeoffs:
- **ImageNet vs. RadImageNet:** ImageNet offers better convergence/diversity vs. RadImageNet's theoretical domain proximity (which failed to yield superior results here)
- **Unfreezing Depth:** 3-5 layers offers the "Goldilocks" zone; 0 is too rigid (underfitting), >7 is too unstable (gradient dispersion)

### Failure signatures:
- **Plateauing Validation AUC:** Observed with RadImageNet weights in ACL task after initial gains (Figure 8)
- **Training Instability:** Observed specifically with InceptionV3 + RadImageNet PyTorch weights (Appendix D.2)
- **Overfitting:** High training AUC but stagnant/lower validation AUC; mitigated by increasing weight decay (0.4) and dropout

### First 3 experiments:
1. **Baseline Validation:** Train a "Linear" classifier on top of a frozen ResNet50 (ImageNet weights) to establish a performance floor
2. **Head Architecture Ablation:** Compare "Linear" vs. "ConvSkip" heads using the optimal "unfreezetop5" setting for 5 epochs to verify the paper's claim regarding classifier importance
3. **Pretraining Source Test:** Run the best configuration (ConvSkip + unfreezetop5) using ImageNet weights vs. RadImageNet weights to replicate the statistical significance finding (DeLong test)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can concatenating features from both RadImageNet and ImageNet backbones enhance performance compared to single-source pretraining?
- **Basis in paper:** [explicit] The authors propose future work should investigate combining dataset strengths, specifically suggesting feature concatenation as a "simple first approach."
- **Why unresolved:** The current experiments only evaluated models initialized with weights from one source at a time.
- **What evidence would resolve it:** Empirical results from models utilizing fused feature sets from both pretrained backbones on the ACL and breast tasks.

### Open Question 2
- **Question:** Is the observed superiority of ImageNet pretraining dependent on the specific deep learning framework (PyTorch vs. TensorFlow) used for the weights?
- **Basis in paper:** [explicit] The authors note their contradictory results regarding RadImageNet may stem from "discrepancies between RadImageNet PyTorch vs. TensorFlow weights."
- **Why unresolved:** This study utilized PyTorch implementations while the original RadImageNet findings were based on TensorFlow, introducing a potential confounding variable.
- **What evidence would resolve it:** A controlled comparison using identical architectures and optimization settings across both frameworks.

### Open Question 3
- **Question:** Does expanding RadImageNet to include breast imaging modalities improve its transfer learning efficacy for breast lesion classification?
- **Basis in paper:** [explicit] The authors state the lack of breast ultrasound images in RadImageNet likely contributed to its inferior performance on the breast malignancy task.
- **Why unresolved:** The current dataset lacks this specific anatomical data, preventing a test of modality-specific transfer.
- **What evidence would resolve it:** Re-evaluating performance after pretraining on an augmented RadImageNet dataset that includes breast pathology images.

## Limitations

- **Architecture Specification Gap:** The ConvSkip classifier architecture is partially underspecified, with exact layer dimensions, skip connection placement, and number of FC layers remaining unclear
- **Limited Hyperparameter Search:** The study uses a constrained hyperparameter search space that may have limited discovery of alternative optimal configurations
- **Framework Dependency:** The ImageNet vs. RadImageNet comparison is based solely on PyTorch implementations, leaving open the possibility that different frameworks or training procedures might yield different results

## Confidence

- **High Confidence:** Classifier architecture impact (ConvSkip vs. Linear) is well-supported by mixed-model analysis showing Linear classifiers underperforming by 0.313 AUC points (p<0.05)
- **Medium Confidence:** ImageNet vs. RadImageNet comparison is statistically significant but may be implementation-dependent; optimal hyperparameters are task-specific
- **Low Confidence:** Transferability of unfreezing layer counts (3-5 groups) across different medical imaging tasks and backbone architectures is assumed but not empirically validated

## Next Checks

1. **Architecture Replication:** Implement the exact ConvSkip architecture with specified layer dimensions and skip connection placement, then reproduce the breast lesion classification AUC of 0.9641 on the Kaggle dataset
2. **Pretraining Implementation Test:** Train the same ConvSkip + unfreezetop5 configuration using RadImageNet weights from multiple sources (original Keras vs. independently-trained PyTorch) to isolate whether the ImageNet superiority is dataset-specific or implementation-specific
3. **Unfreezing Sensitivity Analysis:** Systematically vary the number of unfrozen layer groups (1-10) on both tasks to confirm the 3-5 group "Goldilocks zone" and identify the gradient dispersion threshold