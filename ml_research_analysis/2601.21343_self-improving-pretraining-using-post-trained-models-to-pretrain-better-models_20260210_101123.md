---
ver: rpa2
title: 'Self-Improving Pretraining: using post-trained models to pretrain better models'
arxiv_id: '2601.21343'
source_url: https://arxiv.org/abs/2601.21343
tags:
- pretraining
- quality
- suffix
- rollouts
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Improving Pretraining, a method that
  enhances language model pretraining by leveraging a post-trained judge model to
  provide superior supervision signals. The core idea is to treat pretraining as a
  sequence generation task, where the model generates high-quality suffixes given
  prefixes, and uses the judge to evaluate and reward generations for quality, safety,
  and factuality.
---

# Self-Improving Pretraining: using post-trained models to pretrain better models

## Quick Facts
- **arXiv ID:** 2601.21343
- **Source URL:** https://arxiv.org/abs/2601.21343
- **Reference count:** 40
- **Primary result:** Up to 86.3% win rate in generation quality using self-improving pretraining with post-trained judge models

## Executive Summary
This paper introduces Self-Improving Pretraining (SIP), a novel approach that enhances language model pretraining by leveraging a post-trained judge model to provide superior supervision signals. The method treats pretraining as a sequence generation task where the model generates high-quality suffixes given prefixes, using the judge to evaluate and reward generations for quality, safety, and factuality. The approach dynamically balances between using original suffixes, rewrites, and rollouts from the current policy, adapting as training progresses. Experiments demonstrate significant improvements over standard pretraining across multiple evaluation dimensions, including generation quality, factuality, and safety.

## Method Summary
Self-Improving Pretraining works by leveraging a frozen post-trained judge model to supervise the pretraining of a new language model. The approach treats pretraining as a sequence generation task where the model generates suffixes given prefixes, and uses the judge to evaluate and reward these generations. A three-phase curriculum is employed: Phase 1 uses only original suffixes from the corpus, Phase 2 introduces generated rewrites, and Phase 3 incorporates rollouts from the current policy. The method dynamically balances between original suffixes, rewrites, and rollouts based on the judge's feedback and the current training stage. This creates a feedback loop where the pretraining process itself is guided by a more capable model, leading to improved quality, safety, and factuality in the resulting model.

## Key Results
- **86.3% win rate** in generation quality over standard pretraining baselines
- **36.2% relative improvement** in factuality metrics using judge-based evaluation
- **18.5% relative improvement** in safety metrics compared to standard pretraining

## Why This Works (Mechanism)
The method works by creating a self-improving loop where a post-trained judge model provides high-quality supervision signals during pretraining. By treating pretraining as a sequence generation task and using the judge to evaluate and reward generations, the approach ensures that the model learns from superior examples rather than just the original corpus data. The dynamic balancing between original suffixes, rewrites, and rollouts allows the model to gradually adapt to more challenging supervision signals as training progresses. The judge model acts as a quality filter, guiding the pretraining process toward generating more factual, safe, and high-quality outputs.

## Foundational Learning
**Language Model Pretraining** - The standard approach of training models on large text corpora to learn language patterns and representations. *Why needed:* Forms the baseline against which improvements are measured. *Quick check:* Understanding the standard masked language modeling or autoregressive objectives.

**Sequence Generation** - Treating language modeling as generating sequences (suffixes) from given contexts (prefixes). *Why needed:* Enables the method to generate alternative continuations beyond the original corpus. *Quick check:* Familiarity with encoder-decoder or decoder-only architectures for generation tasks.

**Reward Modeling** - Using a judge model to provide scalar rewards for generated sequences based on quality, safety, and factuality criteria. *Why needed:* Provides the supervision signal that guides the self-improving process. *Quick check:* Understanding how models can be trained to evaluate other models' outputs.

## Architecture Onboarding

**Component Map:** Corpus Data -> Judge Model -> Generator Model -> Reward Function -> Training Loop -> Improved Model

**Critical Path:** The core training loop where the generator produces suffixes, the judge evaluates them, rewards are computed, and the generator is updated based on these rewards. This loop is where the self-improvement occurs.

**Design Tradeoffs:** The method trades increased computational cost (running the judge model for every generated suffix) for improved quality metrics. The three-phase curriculum balances stability in early training with progressive exposure to more challenging supervision signals.

**Failure Signatures:** If the judge model is not sufficiently capable, the supervision signals may be poor, leading to degraded performance. Over-reliance on the judge could also cause the generator to diverge from natural language patterns if not properly balanced with original corpus data.

**First Experiments:**
1. Test the judge model's evaluation consistency on a held-out set of prefix-suffix pairs
2. Verify the dynamic balancing mechanism correctly transitions between phases during training
3. Compare generation quality of the final model against a standard pretrained baseline on a small validation set

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The three-phase curriculum and dynamic generation-reward balancing introduce significant methodological complexity that may require extensive hyperparameter tuning for replication
- Reliance on a frozen post-trained judge model raises questions about whether improvements in generation quality translate to meaningful downstream task performance gains
- The evaluation of safety and factuality improvements, while showing percentage gains, lacks clear demonstration of real-world deployment robustness and generalization across diverse linguistic contexts

## Confidence

**Generation quality improvements (86.3% win rate): High** - Supported by direct judge model evaluations with clear metrics

**Factuality improvements (36.2% relative gain): Medium** - Judge-based evaluations show improvement, but synthetic claim verification may not fully capture real-world factual accuracy

**Safety improvements (18.5% relative gain): Low** - Evaluation methodology appears less rigorous with limited discussion of adversarial testing or diverse safety scenarios

## Next Checks
1. Evaluate the pretrained models on a diverse set of downstream tasks to verify that judge-based improvements translate to practical performance gains across domains
2. Conduct ablation studies isolating the impact of each generation strategy (original suffix, rewrite, rollout) to determine which components drive the reported improvements
3. Test model behavior on adversarial prompts and out-of-distribution data to assess the robustness and generalization of the claimed safety and factuality enhancements