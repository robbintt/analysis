---
ver: rpa2
title: 'MMPB: It''s Time for Multi-Modal Personalization'
arxiv_id: '2509.22820'
source_url: https://arxiv.org/abs/2509.22820
tags:
- image
- arxiv
- concept
- personalization
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces MMPB, the first large-scale benchmark designed\
  \ to evaluate the personalization capabilities of Vision-Language Models (VLMs).\
  \ It comprises 10,000 image-query pairs across 111 personalizable concepts spanning\
  \ four categories\u2014humans, animals, objects, and characters\u2014with humans\
  \ enriched by preference-grounded queries."
---

# MMPB: It's Time for Multi-Modal Personalization

## Quick Facts
- arXiv ID: 2509.22820
- Source URL: https://arxiv.org/abs/2509.22820
- Reference count: 40
- First large-scale benchmark for evaluating VLM personalization across 10,000 image-query pairs

## Executive Summary
MMPB introduces the first large-scale benchmark designed to evaluate the personalization capabilities of Vision-Language Models (VLMs). The benchmark comprises 10,000 image-query pairs across 111 personalizable concepts spanning humans, animals, objects, and characters. It assesses personalization through three task types—Awareness, Appropriateness, and Coherency—evaluated via a three-stage protocol involving concept injection, multi-turn dialogue, and personalized querying. Results from 23 widely used VLMs reveal significant challenges in maintaining consistency over dialogue, handling user preferences, leveraging visual cues, and avoiding safety-induced evasive responses.

## Method Summary
MMPB evaluates VLM personalization through a 3-stage protocol: concept injection (text or image), multi-turn dialogue (0-100 turns), and personalized querying. The benchmark uses 111 concepts with 5 reference images and 4 granularity levels of text descriptions each. Three task types are assessed: Awareness (concept detection), Appropriateness (context-based suppression), and Coherency (context-appropriate reasoning). Evaluation is conducted on 23 models including both open- and closed-source variants, using multiple-choice VQA accuracy as the primary metric. The benchmark specifically tests cold-start personalization with minimal prior context.

## Key Results
- Text-based concept injection significantly outperforms image-based injection (63.8% vs 57.8% accuracy in 0-turn setting)
- Closed-source models exhibit high refusal rates for human-centric concepts due to safety alignment
- Performance drops sharply after 5 dialogue turns due to long-context positional bias
- Even top-ranked models on general benchmarks perform poorly on preference-grounded tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based concept injection outperforms image-based injection for personalization in VLMs
- Mechanism: Textual descriptions are processed through the language encoder and concatenated with query representations, while image-based injection requires the vision encoder to extract and maintain fine-grained visual features across context. The language pathway appears to retain concept information more robustly during multi-turn interactions, potentially because text tokens receive more direct attention from the LLM backbone.
- Core assumption: The model's attention mechanism weights text tokens more heavily than visual tokens during retrieval of injected concepts
- Evidence anchors:
  - [abstract] "Our findings indicate that most VLMs struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues"
  - [Section 5.4] "Text-based concept injection consistently outperforms image-based injection, achieving average accuracies of 63.8% vs. 57.8% in the 0-turn, and 57.9% vs. 52.6% in the 10-turn setting"
  - [corpus] MC-LLaVA paper addresses multi-concept personalization but focuses on fine-tuning approaches rather than in-context injection comparison

### Mechanism 2
- Claim: Long-context positional bias causes concepts injected mid-sequence to be "lost" during retrieval
- Mechanism: The attention distribution across long contexts exhibits a U-shaped pattern where tokens at the beginning and end receive higher attention weights, while mid-sequence tokens (where concepts are often placed in multi-turn dialogues) receive diminished attention. This creates a retrieval failure where the model cannot access concept information even though it was successfully encoded.
- Core assumption: The attention mechanism's positional encoding or bias creates uneven access to context tokens based on their position
- Evidence anchors:
  - [Section 5.5] "Regardless of the prompting methods, models often forget concepts located near the midpoint, reflecting the 'lost-in-the-middle' effect"
  - [Section 5.5] "Performance for both text- and image-based inputs deteriorates sharply after 5 turns"
  - [corpus] Corpus evidence does not directly address positional attention bias in VLM personalization; this remains underexplored in related work

### Mechanism 3
- Claim: Safety alignment in closed-source models creates systematic refusal behaviors that block personalization for human-centric concepts
- Mechanism: RLHF training and hardcoded safety filters classify identity recognition queries as privacy-sensitive, triggering refusal responses ("I cannot identify specific individuals") even when the task is legitimate personalization. The safety objective overrides the model's perceptual capability, producing evasive responses rather than incorrect ones.
- Core assumption: Safety training creates activation patterns that suppress identity-related outputs regardless of user consent or context
- Evidence anchors:
  - [Section 5.2] "All closed-source models, except Gemini, exhibit evasive responses in recognition and preference-grounded tasks... most evasive cases fall under the human category: out of 7,501 queries, up to 2,237 trigger evasive answers"
  - [Section 5.2, Table 3] Shows examples: Claude-3.5-Sonnet: "I cannot and should not identify or name specific features of the person..."
  - [corpus] No corpus papers directly address safety-personalization tradeoffs; this is identified as a research gap

## Foundational Learning

- Concept: **In-Context Learning vs. Weight Modification**
  - Why needed here: MMPB evaluates personalization through context injection (concatenating concept descriptions/images with queries) rather than fine-tuning. Understanding this distinction is critical for interpreting results—failures may indicate context utilization limits rather than fundamental personalization incapacity
  - Quick check question: When the paper reports that text-based injection outperforms image-based injection, does this mean the model cannot learn visual concepts, or that the context window mechanism handles text more effectively?

- Concept: **Deductive vs. Abductive Reasoning**
  - Why needed here: Recognition tasks (Awareness, Appropriateness) require deductive reasoning (matching visual features to injected descriptions), while preference-grounded Coherency tasks require abductive reasoning (inferring what a user would prefer given partial information). The paper finds preference tasks significantly harder, suggesting VLMs lack robust abductive capabilities
  - Quick check question: Why would a model that correctly identifies "<sks> in an image" struggle to answer "What would <sks> likely be doing in this scene?"

- Concept: **Cold-Start Evaluation Paradigm**
  - Why needed here: MMPB specifically tests personalization with minimal prior context (2 images or moderate text description) to reflect real-world scenarios where personalization must begin immediately. This contrasts with methods that require extensive pre-training on user data
  - Quick check question: How does the cold-start constraint affect what personalization capabilities can be fairly evaluated?

## Architecture Onboarding

- Component map:
  Concept Repository -> Injection Module -> Dialogue Engine -> Query Generator -> Quality Filter

- Critical path:
  1. Select concept category and specific concept from repository
  2. Choose injection modality (text vs. image) and granularity
  3. Inject concept into model context
  4. Optionally execute multi-turn dialogue (0-100 turns)
  5. Present personalized query with evaluation image
  6. Evaluate response against ground truth for task type

- Design tradeoffs:
  - **Text vs. Image Injection**: Text provides higher accuracy but may not capture fine-grained visual features; images are more natural but underutilized by current VLMs
  - **Static vs. Dynamic Concepts**: MMPB assumes static appearance/preferences; real-world personalization requires temporal adaptation
  - **Multiple-Choice vs. Open-Ended**: MCQ format enables automated evaluation but may not capture nuanced personalization quality; open-ended responses are provided for qualitative analysis

- Failure signatures:
  - **Under-personalization bias**: Models score higher on Appropriateness than Awareness, systematically rejecting personalized concepts (72/92 cases in under-personalization region)
  - **Refusal cascades**: Closed-source models return "Unknown" or identity-refusal responses for human concepts, inflating error rates regardless of actual capability
  - **Multi-entity confusion**: Awareness remains stable with multiple entities, but Coherency drops substantially, indicating reasoning breakdown under complexity
  - **Hard-negative failure**: Text-based injection shows 15-20% accuracy drops on same-species negatives vs. different-species, while image-based injection remains robust

- First 3 experiments:
  1. **Baseline text injection with moderate descriptions across all 23 models**: Establish performance distribution and identify which model families (InternVL, Qwen, LLaVA) handle personalization best. This validates your evaluation pipeline against reported benchmarks
  2. **Ablation on injection position in 10-turn dialogue**: Inject concept at turns 1, 5, and 10 to quantify "lost-in-the-middle" effect for your target model. This reveals positional attention bias severity
  3. **Preference task with CoT prompting**: Test whether chain-of-thought prompting improves abductive reasoning on Coherency tasks, comparing zero-shot vs. zero-shot-CoT vs. few-shot-CoT. This probes if preference reasoning gaps are fundamental or promptable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can safety alignment protocols be redesigned to mitigate refusal behaviors while maintaining privacy in human-centric personalization?
- Basis in paper: [explicit] Section 5.2 highlights that closed-source models exhibit "evasive responses" (refusals) due to safety filters, asking if safety and personalization can coexist.
- Why unresolved: Current safety alignment treats identity queries as privacy-sensitive, blocking the core mechanism of user identification required for personalization.
- What evidence would resolve it: Demonstrating high accuracy on MMPB human tasks without triggering standard refusal patterns (e.g., "I cannot identify...") in closed-source models.

### Open Question 2
- Question: What model architectures or training strategies can enable VLMs to utilize visual reference images as effectively as text descriptions for concept injection?
- Basis in paper: [explicit] Section 5.4 notes that "Simple textual injections... achieve comparable accuracy to the 1-image condition," calling for research to "effectively harness these visual cues."
- Why unresolved: Current VLM attention mechanisms seemingly fail to isolate and retain fine-grained visual features from reference images compared to semantic text tokens.
- What evidence would resolve it: Achieving statistically significant higher performance with multi-image injection compared to "Simple" text keywords on the Awareness task.

### Open Question 3
- Question: How can VLMs adapt to dynamic concept states, such as changing appearances or evolving preferences, rather than static definitions?
- Basis in paper: [explicit] Section 10 (Limitations) states MMPB focuses on static appearance and encourages "future work to introduce dynamic concept updates."
- Why unresolved: MMPB assumes a static state; real-world deployment requires updating concept embeddings over time without full retraining or catastrophic forgetting.
- What evidence would resolve it: Performance on a benchmark variant where reference images chronologically precede query images showing style or preference changes.

## Limitations

- Dataset and evaluation code not yet released, preventing immediate reproducibility and independent verification of reported performance gaps
- Human evaluation was limited to 10 samples per task type, potentially missing edge cases in complex personalization scenarios
- Benchmark focuses on cold-start personalization with minimal context, which may not reflect real-world personalization scenarios where models accumulate user data over time

## Confidence

- **High Confidence**: Text-based injection outperforming image-based injection (supported by quantitative comparisons across multiple models and settings)
- **Medium Confidence**: Safety-induced refusal behaviors in closed-source models (observed across all tested closed-source models, but mechanism attribution remains speculative)
- **Medium Confidence**: Long-context positional bias causing "lost-in-the-middle" effects (consistent performance drops observed, but architectural attribution requires further investigation)

## Next Checks

1. Replicate the text vs. image injection comparison using a subset of concepts (e.g., 10 concepts, 2 models) to verify the 6% performance gap before scaling to full evaluation
2. Test whether adding explicit reminder prompts at mid-sequence positions eliminates the performance drop in 10-turn dialogues, isolating whether the effect is due to positional bias or attention guidance
3. Compare refusal rates between open-source models with safety fine-tuning and closed-source models to determine if evasive responses are training-based or inference-time filtering specific