---
ver: rpa2
title: Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation
arxiv_id: '2507.19882'
source_url: https://arxiv.org/abs/2507.19882
tags:
- counterfactual
- learning
- prompt
- causal
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving causally invariant
  prompt learning in vision-language models. Existing methods struggle to capture
  robust features that generalize effectively across categories due to inadequate
  theoretical foundations.
---

# Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation

## Quick Facts
- **arXiv ID:** 2507.19882
- **Source URL:** https://arxiv.org/abs/2507.19882
- **Reference count:** 40
- **Primary result:** DiCap improves CLIP accuracy by 17.6% on seen classes and 3.87% on unseen classes for image classification.

## Executive Summary
This paper addresses the challenge of achieving causally invariant prompt learning in vision-language models. Existing methods struggle to capture robust features that generalize effectively across categories due to inadequate theoretical foundations. The proposed DiCap framework uses a diffusion-based counterfactual generation method to iteratively sample gradients from the marginal and conditional distributions of the causal model. This guides the generation of counterfactuals that satisfy the minimal sufficiency criterion, ensuring the identifiability of counterfactual outcomes while imposing strict bounds on estimation errors. A contrastive learning framework then leverages these counterfactuals to refine prompt embeddings aligned with causal features.

## Method Summary
DiCap generates counterfactual images by inverting factual images to noise using a pretrained diffusion model, then denoising guided by gradients from an anti-causal predictor toward a counterfactual label (the second-most probable class). These counterfactuals serve as "hard negatives" in a dual contrastive loss that trains soft prompts on a frozen CLIP model. The method theoretically bounds counterfactual estimation error by the diffusion model's reconstruction fidelity and ensures "minimal sufficiency" through iterative gradient sampling.

## Key Results
- On ImageNet, DiCap improves accuracy by 17.6% on seen classes and 3.87% on unseen classes compared to CLIP baseline
- Strong performance across image classification, image-text retrieval, and visual question answering tasks
- Particularly effective on unseen categories due to causal feature alignment

## Why This Works (Mechanism)

### Mechanism 1
Diffusion processes approximate the structural causal model by mapping forward diffusion (adding noise) to **Abduction** in causal reasoning, approximating exogenous variables ($U$). The reverse denoising process guided by anti-causal predictor gradients performs **Action** and **Estimation**, forcing changes only to causal features necessary to align with the counterfactual label while preserving non-causal features. Core assumption: noise in diffusion maps isomorphically to exogenous variables of data generation process. Evidence anchors: abstract mentions gradient sampling from marginal/conditional distributions; Section 3.3 links diffusion steps to Abduction/Action/Estimation phases. Break condition: noise scale $s$ too high loses non-causal info, too low fails to flip label.

### Mechanism 2
Class-similar counterfactuals serve as "hard negatives" in contrastive learning, forcing prompt embeddings to decouple causal features from spurious correlations. The method selects the counterfactual label as the class with second-highest prediction probability (e.g., "tiger" for a "cat" image). By generating a counterfactual that looks like the factual image but belongs to this similar class, and treating it as a negative sample, the contrastive loss forces the prompt to find subtle causal discriminators rather than relying on easy background cues. Core assumption: similar classes share non-causal contexts, making causal separation the only learnable signal. Evidence anchors: Section 3.5 describes "Closest as Counterfactual" strategy; Table 4 shows similarity sampling outperforms random significantly. Break condition: weak pre-trained classifier provides incorrect nearest neighbors, reducing learning signal.

### Mechanism 3
Reconstruction fidelity of the diffusion model provides a theoretical upper bound on counterfactual estimation error. The paper derives that if a diffusion model can reconstruct a factual image with error $\le \delta$, then the error of counterfactual estimation relative to the true SCM outcome is also bounded by $\delta$. This provides mathematical guarantees on the validity of generated samples. Core assumption: conditions of Theorem 3.2 hold, specifically structural function is differentiable with positive definite Jacobian. Evidence anchors: abstract claims "identifiability of counterfactual outcomes while imposing strict bounds on estimation errors"; Section 3.4 Corollary 3.4 formalizes error bound. Break condition: violation of invertibility or independence assumptions in Theorem 3.2 invalidates theoretical bound.

## Foundational Learning

- **Concept: Structural Causal Models (SCM)**
  - Why needed: The paper frames vision tasks as recovering $y \to x$ (causal) vs $n \to x$ (non-causal) relationships. You cannot understand "minimal sufficiency" or "abduction" without SCMs.
  - Quick check: Can you explain why adding noise to an image corresponds to "Abduction" in Pearl's causal framework?

- **Concept: Anti-causal Prediction**
  - Why needed: The framework relies on an "anti-causal predictor" to guide the diffusion model. Standard classifiers predict $y$ from $x$, which is anti-causal relative to data generation process $y \to x$.
  - Quick check: Why does the paper use the gradient of $p(y|x)$ to guide generation of $x$?

- **Concept: Contrastive Learning & Hard Negatives**
  - Why needed: The core training loop is a dual contrastive loss. Understanding why "hard negatives" (similar counterfactuals) are more effective than random negatives is key to method's success.
  - Quick check: How does treating a generated "tiger" image as a negative sample for a "cat" prompt improve generalization better than using a "car" as a negative?

## Architecture Onboarding

- **Component map:** Frozen Pre-trained Encoder (CLIP) -> Anti-causal Predictor -> Diffusion Module -> Prompt Learner
- **Critical path:** The quality of gradient guidance from the anti-causal predictor is the critical path. If the predictor fails to distinguish subtle causal features, the generated counterfactual will not satisfy "minimal sufficiency," and the prompt learner will converge on spurious correlations.
- **Design tradeoffs:**
  - Scale parameter ($s$): Balances counterfactual strength. Low $s$ = too similar to factual (easy negative); High $s$ = artifacts/loss of non-causal context. Optimal range: $5 < s < 20$.
  - Prompt Length: Shorter prompts (length 4) generalize better to unseen classes than longer ones (length 16), likely due to reduced overfitting to spurious textual cues.
- **Failure signatures:**
  - Performance drop on unseen classes: Indicates overfitting to training class contexts (spurious) rather than causal visual features.
  - High CLD Score: High Counterfactual Latent Distance implies generated images drifting too far from factual image's non-causal context.
- **First 3 experiments:**
  1. Run generation with varying $s$ (1, 5, 15, 30) and visualize counterfactuals to confirm "Goldilocks" zone where class changes but background remains.
  2. Compare "Closest as Counterfactual" vs. "Random" counterfactuals on small validation set to verify "hard negative" advantage (replicate Table 4).
  3. Train prompts with lengths [4, 8, 16] and plot accuracy delta between seen/unseen classes to verify regularization effect of short prompts (replicate Table 3).

## Open Questions the Paper Calls Out
- **Question 1:** How can DiCap be adapted for multi-modal prompt learning to handle complex interactions between modalities beyond current uni-modal focus?
- **Question 2:** To what extent does bias/inaccuracy of pre-trained anti-causal predictor compromise theoretical error bounds of counterfactual estimation?
- **Question 3:** Can computational cost of iterative diffusion sampling be reduced for large-scale training without violating identifiability conditions?
- **Question 4:** What specific constraints or validation frameworks are necessary for responsible deployment in sensitive domains like healthcare?

## Limitations
- Theoretical foundation relies heavily on invertibility assumptions in Theorem 3.2 that may not hold for complex natural images
- Method requires strong anti-causal predictor, but paper doesn't fully address what happens when predictor is imperfect
- Computational cost of generating counterfactuals via diffusion is significantly higher than standard prompt tuning methods

## Confidence
- **High Confidence:** Using class-similar counterfactuals as hard negatives in contrastive learning (Mechanism 2) - well-supported by ablation studies in Table 4 and aligns with established contrastive learning principles
- **Medium Confidence:** Diffusion-based counterfactual generation framework (Mechanism 1) - theoretically grounded but practical implementation details and exact noise-to-exogenous variable mapping remain somewhat abstract
- **Medium Confidence:** Error bound guarantees (Mechanism 3) - theoretical derivation appears sound but practical significance in real-world scenarios not extensively validated

## Next Checks
1. **Predictor Robustness Test:** Replace anti-causal predictor with weaker classifier and measure degradation in counterfactual quality and downstream performance to validate critical dependency on predictor strength
2. **Cross-Dataset Generalization:** Evaluate DiCap on dataset with significantly different visual contexts (e.g., medical imaging) to test whether causal feature alignment generalizes beyond natural images
3. **Scaling Analysis:** Systematically vary scale parameter $s$ across wider range (1-50) and plot counterfactual quality metrics (CLD) against downstream performance to precisely characterize optimal range and failure modes