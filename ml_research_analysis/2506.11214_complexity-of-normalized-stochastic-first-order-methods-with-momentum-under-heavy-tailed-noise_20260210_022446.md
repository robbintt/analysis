---
ver: rpa2
title: Complexity of normalized stochastic first-order methods with momentum under
  heavy-tailed noise
arxiv_id: '2506.11214'
source_url: https://arxiv.org/abs/2506.11214
tags:
- inequality
- lemma
- where
- complexity
- follows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of finding approximate stationary
  points for unconstrained optimization problems under heavy-tailed noise conditions.
  The authors propose three practical normalized stochastic first-order methods with
  different momentum variants (Polyak, multi-extrapolated, and recursive) that do
  not require explicit knowledge of problem-dependent quantities like Lipschitz constants
  or noise bounds.
---

# Complexity of normalized stochastic first-order methods with momentum under heavy-tailed noise

## Quick Facts
- arXiv ID: 2506.11214
- Source URL: https://arxiv.org/abs/2506.11214
- Reference count: 35
- Primary result: First-order oracle complexity bounds for finding approximate stochastic stationary points under heavy-tailed noise with dynamic parameter updates

## Executive Summary
This paper addresses the challenge of finding approximate stationary points for unconstrained optimization problems under heavy-tailed noise conditions. The authors propose three practical normalized stochastic first-order methods with different momentum variants (Polyak, multi-extrapolated, and recursive) that do not require explicit knowledge of problem-dependent quantities like Lipschitz constants or noise bounds. The methods employ dynamically updated algorithmic parameters to achieve convergence under weaker conditions than commonly used bounded variance and mean-squared smoothness assumptions.

The primary results include first-order oracle complexity bounds for finding approximate stochastic stationary points. When the tail exponent α is known, the methods achieve complexities of eO(ϵ−(3α−2)/(α−1)) for Polyak momentum, eO(ϵ−(p(2α−1)+α−1)/(p(α−1))) for multi-extrapolated momentum (with p being the smoothness order), and eO(ϵ−(2α−1)/(α−1)) for recursive momentum. When α is unknown, the complexities are eO(ϵ−2α/(α−1)), eO(ϵ−(3pα+α)/(2p(α−1))), and eO(ϵ−3α/(2(α−1))) respectively. These bounds either improve upon or match the best-known results in the literature. Numerical experiments on data fitting, robust regression, and multimodal contrastive learning problems demonstrate the practical effectiveness of the proposed methods compared to their unnormalized counterparts.

## Method Summary
The paper introduces three normalized stochastic first-order methods with momentum for solving unconstrained optimization problems under heavy-tailed noise. The methods employ dynamic updates for algorithmic parameters, eliminating the need for explicit knowledge of Lipschitz constants or noise bounds. The Polyak momentum variant uses standard momentum updates with normalized gradients, the multi-extrapolated momentum uses p-th order smoothness with multiple gradient evaluations, and the recursive momentum employs a recursive averaging scheme. Each method maintains adaptive step sizes and momentum parameters that are updated based on observed gradients and iterates, allowing them to handle heavy-tailed noise distributions without requiring bounded variance assumptions.

## Key Results
- First-order oracle complexity of eO(ϵ−(3α−2)/(α−1)) for Polyak momentum when tail exponent α is known
- Complexity bounds of eO(ϵ−(p(2α−1)+α−1)/(p(α−1))) for multi-extrapolated momentum with smoothness order p
- When α is unknown, complexities improve to eO(ϵ−2α/(α−1)) for Polyak momentum and similar bounds for other variants
- Numerical experiments demonstrate superior performance compared to unnormalized methods on data fitting, robust regression, and contrastive learning tasks

## Why This Works (Mechanism)
The methods work by normalizing stochastic gradients to make them scale-invariant, which is crucial under heavy-tailed noise where standard variance assumptions fail. The dynamic parameter updates allow the algorithms to adapt to problem characteristics without requiring prior knowledge of Lipschitz constants or noise bounds. By using momentum with carefully designed adaptive parameters, the methods can achieve faster convergence rates while maintaining robustness to heavy-tailed perturbations.

## Foundational Learning
- Heavy-tailed distributions: Required for modeling real-world noise in optimization; quick check: verify if data exhibits power-law tails with finite moments
- Stochastic gradient methods: Fundamental optimization framework; quick check: understand basic SGD convergence under standard assumptions
- Momentum acceleration: Technique for improving convergence speed; quick check: compare convergence rates with and without momentum
- Lipschitz continuity: Traditional smoothness assumption; quick check: verify if objective function satisfies local smoothness conditions
- Normalization techniques: Key for handling scale-invariant problems; quick check: test performance on ill-conditioned problems
- Tail exponent estimation: Critical for adapting to noise characteristics; quick check: validate α estimation accuracy on synthetic data

## Architecture Onboarding

Component Map:
Stochastic Oracle -> Gradient Normalization -> Momentum Update -> Adaptive Parameter Update -> Parameter Update

Critical Path:
The critical path follows: stochastic gradient computation → normalization → momentum application → parameter update. Each component must execute successfully for the algorithm to progress, with the adaptive parameter update being crucial for maintaining convergence under heavy-tailed noise.

Design Tradeoffs:
The main tradeoff is between computational overhead from dynamic parameter updates and convergence speed improvements. The methods sacrifice some computational efficiency per iteration for better asymptotic performance and robustness to heavy-tailed noise. Using momentum introduces additional memory requirements but significantly accelerates convergence compared to non-momentum variants.

Failure Signatures:
- Divergence when tail exponent is severely underestimated
- Slow convergence when smoothness order p is incorrectly estimated
- Numerical instability when normalization factors become too small
- Suboptimal performance on problems with light-tailed noise

First Experiments:
1. Test Polyak momentum variant on a simple quadratic with heavy-tailed noise
2. Compare multi-extrapolated momentum with different smoothness orders on a logistic regression problem
3. Validate recursive momentum on a robust regression task with Cauchy noise

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertainty about the reliability of empirically estimated smoothness order p in practice
- Potential performance degradation when tail exponent α is only approximately known
- Theoretical assumptions about distributional properties may not hold in all practical scenarios
- Limited analysis of computational overhead from dynamic parameter updates

## Confidence
- High confidence in the theoretical framework and mathematical proofs
- Medium confidence in practical parameter estimation procedures (particularly for smoothness order p)
- Medium confidence in the complexity bounds when α is unknown
- Low confidence in the robustness of results to slight violations of distributional assumptions

## Next Checks
1. Empirical validation of the proposed methods on additional benchmark problems with varying degrees of heavy-tailed noise, particularly focusing on the accuracy of smoothness order estimation p
2. Comparative analysis of convergence rates when α is estimated versus known, including sensitivity analysis to estimation errors
3. Implementation of additional normalization schemes to assess whether the proposed methods are optimal or if further improvements are possible