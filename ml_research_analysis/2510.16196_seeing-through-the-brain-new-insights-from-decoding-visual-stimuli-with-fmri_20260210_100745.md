---
ver: rpa2
title: 'Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI'
arxiv_id: '2510.16196'
source_url: https://arxiv.org/abs/2510.16196
tags:
- image
- fmri
- space
- text
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of reconstructing visual stimuli
  from fMRI signals, a central challenge in neuroscience and machine learning. The
  authors propose PRISM, a framework that bridges fMRI signals and image reconstruction
  by projecting them into a structured text space rather than a vision-based one.
---

# Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI

## Quick Facts
- **arXiv ID**: 2510.16196
- **Source URL**: https://arxiv.org/abs/2510.16196
- **Reference count**: 35
- **Key outcome**: PRISM framework achieves up to 8% reduction in perceptual loss and 17% improvement in LPIPS for fMRI-to-image reconstruction by using structured text space

## Executive Summary
This work addresses the challenge of reconstructing visual stimuli from fMRI signals, a fundamental problem at the intersection of neuroscience and machine learning. The authors propose PRISM, a novel framework that leverages the surprising finding that fMRI signals align more closely with language model text spaces than with vision-based representations. By projecting fMRI data into a structured text space and using compositional generative models that capture objects, attributes, and relationships, PRISM significantly improves reconstruction quality compared to existing methods.

## Method Summary
PRISM bridges fMRI signals and image reconstruction by projecting brain activity into a structured text space derived from language models rather than visual embeddings. The framework includes two key components: an object-centric diffusion module that generates images by composing individual objects, and an attribute-relationship search module that automatically identifies brain-aligned object attributes and relationships. This compositional approach leverages the compositional nature of visual stimuli, allowing the model to capture not just objects but their semantic relationships and attributes as encoded in brain activity.

## Key Results
- Achieves up to 8% reduction in perceptual loss compared to state-of-the-art methods
- Demonstrates up to 17% improvement in LPIPS metric for reconstruction quality
- Shows that fMRI signals align more closely with language model text spaces than vision-based or joint text-image spaces

## Why This Works (Mechanism)
The core insight is that visual perception in the brain may be more closely represented in semantic, compositional terms rather than raw visual features. By projecting fMRI signals into a structured text space, PRISM captures the brain's encoding of visual information in terms of objects, their attributes, and relationships. This semantic representation provides a more natural bridge between brain activity and image generation, as language models already encode rich compositional knowledge about visual concepts. The object-centric diffusion approach then leverages this structured understanding to generate images that better reflect the brain's internal representation of visual stimuli.

## Foundational Learning
- **fMRI signal processing**: Understanding how to extract meaningful features from noisy brain imaging data - needed to create robust representations of visual perception; quick check: verify signal preprocessing pipeline preserves temporal and spatial patterns
- **Language model embeddings**: Using transformer-based models to create semantic representations - needed to bridge brain activity with compositional visual knowledge; quick check: compare embedding similarity across different visual categories
- **Diffusion generative models**: Leveraging noise-to-image generation with compositional priors - needed to create realistic images from abstract representations; quick check: validate that individual object generation maintains coherence
- **Perceptual quality metrics**: LPIPS and perceptual loss for quantitative evaluation - needed to measure reconstruction fidelity beyond pixel-level accuracy; quick check: correlate metric improvements with human judgment
- **Compositional scene understanding**: Modeling objects, attributes, and relationships - needed to capture the semantic structure of visual stimuli; quick check: verify relationship extraction accuracy across scene complexity

## Architecture Onboarding

Component Map: fMRI signals -> Text projection module -> Attribute-relationship search -> Object-centric diffusion -> Generated images

Critical Path: The pipeline flows from raw fMRI signals through semantic projection to compositional image generation, with the attribute-relationship search module serving as the critical bridge between brain activity and image synthesis.

Design Tradeoffs: The choice to use text space over visual embeddings trades computational efficiency for better semantic alignment with brain activity. The dual-module architecture increases reconstruction quality but adds complexity and potential failure points.

Failure Signatures: Poor reconstructions may indicate issues with text projection alignment, relationship extraction failures, or diffusion model instability. Visual artifacts often correlate with missing or incorrectly inferred object relationships.

First Experiments: (1) Test text projection accuracy on held-out fMRI data from known visual stimuli, (2) Validate object generation quality independently of the full pipeline, (3) Assess relationship extraction performance on synthetic compositional scenes.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Generalizability to different brain imaging modalities or non-visual stimulus types remains untested
- Reliance on pretrained language models introduces potential biases from their training data
- Computational overhead of dual-module architecture may limit practical deployment in resource-constrained settings

## Confidence
- High confidence in technical implementation of the PRISM framework
- Medium confidence in the broader neuroscientific implications of fMRI-language model alignment
- Medium confidence in the perceptual quality improvements being practically significant
- Low confidence in long-term stability and cross-subject generalizability

## Next Checks
1. Conduct cross-subject validation studies to assess how well PRISM generalizes across different individual brain anatomies and fMRI recording sessions
2. Perform ablation studies isolating the contribution of each module (object-centric diffusion vs. attribute-relationship search) to determine their relative importance and potential redundancy
3. Evaluate reconstruction quality using additional human perceptual metrics beyond LPIPS, including user studies where participants rate reconstruction fidelity and semantic accuracy across diverse visual categories