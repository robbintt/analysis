---
ver: rpa2
title: 'LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term
  Reasoning'
arxiv_id: '2511.01448'
source_url: https://arxiv.org/abs/2511.01448
tags:
- memory
- retrieval
- answer
- licomemory
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiCoMemory introduces a lightweight hierarchical graph (CogniGraph)
  that uses entities and relations as semantic indexing layers rather than embedding
  knowledge directly, enabling efficient and real-time updating and retrieval. The
  framework employs temporally and hierarchically sensitive retrieval with integrated
  reranking to improve reasoning accuracy and coherence.
---

# LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning

## Quick Facts
- arXiv ID: 2511.01448
- Source URL: https://arxiv.org/abs/2511.01448
- Reference count: 12
- Introduces lightweight hierarchical graph architecture for real-time agentic memory with 23% accuracy improvement

## Executive Summary
LiCoMemory presents a novel approach to long-term reasoning in conversational agents through its lightweight hierarchical graph (CogniGraph) architecture. The framework uses entities and relations as semantic indexing layers rather than embedding knowledge directly, enabling efficient real-time updating and retrieval. By employing temporally and hierarchically sensitive retrieval with integrated reranking, LiCoMemory achieves significant improvements in reasoning accuracy and coherence while reducing computational overhead.

## Method Summary
LiCoMemory introduces a lightweight hierarchical graph (CogniGraph) that uses entities and relations as semantic indexing layers rather than embedding knowledge directly, enabling efficient and real-time updating and retrieval. The framework employs temporally and hierarchically sensitive retrieval with integrated reranking to improve reasoning accuracy and coherence.

## Key Results
- Up to 23% accuracy improvement over the second-best baseline on LoCoMo and LongMemEval benchmarks
- Enhanced performance in temporal reasoning and multi-session consistency
- Supports efficient real-time memory operations in conversational agents with reduced query latency and token consumption

## Why This Works (Mechanism)
LiCoMemory's effectiveness stems from its hierarchical graph structure that separates semantic indexing from knowledge storage. By using entities and relations as lightweight indexing layers, the system can rapidly traverse and retrieve relevant information without the computational overhead of full embedding comparisons. The temporally and hierarchically sensitive retrieval mechanism ensures that recent and contextually important information is prioritized, while the integrated reranking step refines results to improve reasoning accuracy.

## Foundational Learning

**Hierarchical Graph Structure**
- Why needed: Enables scalable organization of memory without overwhelming computational resources
- Quick check: Verify that graph depth remains manageable as memory grows

**Semantic Indexing with Entities and Relations**
- Why needed: Provides lightweight pointers to knowledge rather than storing full embeddings
- Quick check: Confirm indexing layer size remains proportional to conversation length, not total memory

**Temporal Sensitivity in Retrieval**
- Why needed: Prioritizes recent and contextually relevant information for reasoning
- Quick check: Test retrieval accuracy when mixing old and new conversation data

**Integrated Reranking**
- Why needed: Refines initial retrieval results to improve final reasoning quality
- Quick check: Compare reasoning accuracy with and without reranking step

## Architecture Onboarding

**Component Map**
User Input -> Entity Extraction -> Graph Indexing -> Temporal Filtering -> Retrieval Engine -> Reranking Module -> LLM Reasoning

**Critical Path**
Entity extraction and graph indexing occur in real-time during conversation, while retrieval happens on-demand for reasoning tasks. The temporal filtering layer sits between indexing and retrieval to ensure time-sensitive information is prioritized.

**Design Tradeoffs**
The lightweight indexing approach sacrifices some precision for significant gains in speed and scalability. Using entities and relations rather than full embeddings reduces memory overhead but may miss some nuanced semantic relationships that embeddings could capture.

**Failure Signatures**
Performance degradation occurs when conversations contain highly abstract concepts that don't map cleanly to entities, or when temporal dependencies span very long timeframes that exceed the indexing window.

**Three First Experiments**
1. Measure retrieval latency and accuracy with conversations of varying lengths (10, 50, 100 turns)
2. Compare reasoning accuracy with and without the temporal filtering layer
3. Test memory consumption growth rate as conversation history accumulates over extended periods

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on proprietary datasets (LoCoMo and LongMemEval) that are not publicly accessible
- Limited scalability analysis across different conversation lengths and system architectures
- Temporal reasoning improvements demonstrated only on synthetic benchmarks rather than real-world deployment scenarios

## Confidence

**Performance Improvements**: High Confidence - Accuracy improvements are documented on specific benchmarks, though dataset accessibility limits full verification.

**Cognitive Memory Architecture**: Medium Confidence - CogniGraph approach is coherently described, but implementation details and performance trade-offs lack sufficient detail for independent assessment.

**Real-time Efficiency**: Low Confidence - Latency metrics provided without comprehensive resource utilization data or scalability analysis across diverse deployment scenarios.

## Next Checks
1. Release the LoCoMo and LongMemEval benchmark datasets or create publicly accessible equivalents to enable independent reproduction of the reported accuracy improvements.

2. Conduct head-to-head runtime comparisons with HiMem and SYNAPSE on identical hardware, measuring not just query latency but also memory consumption, CPU/GPU utilization, and throughput under varying conversation lengths.

3. Implement a multi-turn deployment test with actual users over extended periods (minimum 30 days) to validate the claimed multi-session consistency and temporal reasoning capabilities in real-world conversational contexts rather than synthetic benchmarks.