---
ver: rpa2
title: State-Space Models for Tabular Prior-Data Fitted Networks
arxiv_id: '2510.14573'
source_url: https://arxiv.org/abs/2510.14573
tags:
- mamba
- hydra
- tabular
- transformer
- tabpfn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces State-Space Models (SSMs) for tabular foundation\
  \ models, specifically targeting the TabPFN architecture. The authors address the\
  \ computational inefficiency of Transformers in TabPFN, which has quadratic complexity,\
  \ by exploring Mamba and Hydra\u2014efficient SSM alternatives with linear-time\
  \ processing."
---

# State-Space Models for Tabular Prior-Data Fitted Networks

## Quick Facts
- arXiv ID: 2510.14573
- Source URL: https://arxiv.org/abs/2510.14573
- Reference count: 40
- Primary result: Introduces bidirectional State-Space Models (Hydra) for scalable TabPFN architecture

## Executive Summary
This paper explores replacing Transformers with State-Space Models (SSMs) in tabular foundation models, specifically targeting the TabPFN architecture. The authors address the computational inefficiency of Transformers, which have quadratic complexity, by proposing efficient SSM alternatives like Mamba and Hydra with linear-time processing. A key challenge addressed is that SSMs are sensitive to input order, problematic for tabular data where row order is meaningless. The proposed Hydra architecture (bidirectional SSM) combined with Repeated Context Permutations (RCPs) achieves performance close to Transformer-based TabPFN while enabling scalability to much larger datasets.

## Method Summary
The paper introduces SSM-based alternatives to Transformers for TabPFN, specifically exploring Mamba and Hydra architectures. The key innovation is Hydra, a bidirectional SSM that processes tabular data more effectively than unidirectional models. To address SSMs' sensitivity to input order, the authors propose Repeated Context Permutations (RCPs), which average predictions over shuffled contexts. The method is evaluated on 30 OpenML CC-18 datasets, comparing Hydra-based TabPFN against Transformer baselines. The experiments demonstrate that Hydra achieves comparable accuracy while enabling scalability to datasets with up to 2^17 rows, compared to the 2^15 row limit for Transformers.

## Key Results
- Hydra-based TabPFN achieves performance within 1.1% average accuracy of Transformer baselines
- SSMs enable scalability to larger datasets (up to 2^17 rows vs 2^15 for Transformers)
- Hydra outperforms unidirectional Mamba in TabPFN contexts
- RCPs reduce order sensitivity while slightly improving accuracy

## Why This Works (Mechanism)
SSM-based TabPFN works by leveraging the linear-time complexity of State-Space Models compared to Transformers' quadratic complexity. The bidirectional processing in Hydra allows the model to capture both forward and backward dependencies in tabular data, which is crucial since SSMs are inherently sensitive to input order. By averaging predictions across multiple context permutations (RCPs), the model effectively mitigates this order sensitivity while maintaining computational efficiency. This combination enables the architecture to process larger contexts without sacrificing the order-robustness required for tabular data where row order is arbitrary.

## Foundational Learning
- **State-Space Models (SSMs)**: Continuous-time dynamical systems that can be discretized for sequence processing, offering linear-time complexity vs quadratic for Transformers - needed because Transformers become computationally prohibitive for large tabular datasets
- **Bidirectional processing**: Processing sequences in both forward and backward directions to capture complete context - needed because tabular data often requires understanding relationships in all directions, not just sequential order
- **Repeated Context Permutations (RCPs)**: Averaging predictions across multiple shuffled versions of input context - needed to address SSMs' inherent sensitivity to input order in tabular data where row order is meaningless
- **Prior-Data Fitted Networks (PFNs)**: Foundation models trained on synthetic data that can be quickly adapted to new tasks - needed as the target architecture for SSM integration
- **TabPFN architecture**: Tabular-specific implementation of PFNs - needed as the specific application domain for testing SSM scalability
- **Order sensitivity in SSMs**: The property where SSM performance varies significantly based on input sequence order - needed to understand why standard SSMs are problematic for tabular data

## Architecture Onboarding

**Component Map**: TabPFN -> Context Encoder -> SSM Layer (Mamba/Hydra) -> Prediction Head -> Output

**Critical Path**: Input data → Context encoding → Bidirectional SSM processing (Hydra) → RCP-based order averaging → Final prediction

**Design Tradeoffs**: The paper trades multiple inference passes (via RCPs) for order robustness and linear-time complexity. While RCPs increase inference time linearly, they enable the use of efficient SSMs that can scale to much larger contexts. The bidirectional design of Hydra adds complexity but provides significant performance gains over unidirectional Mamba.

**Failure Signatures**: Performance degradation when using unidirectional SSMs (Mamba) on tabular data, high variance in predictions across different row orderings, inability to scale beyond context size limits imposed by quadratic complexity.

**First Experiments**: 1) Compare Hydra vs Mamba performance on tabular datasets with varying context sizes, 2) Test RCP effectiveness by comparing order-sensitive vs order-robust variants, 3) Benchmark memory usage and inference time for SSM-based vs Transformer-based TabPFN on large datasets.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can SSM-based tabular PFNs effectively scale to datasets with more than 10,000 rows while retaining predictive performance?
- Basis in paper: The conclusion identifies the most promising future direction as testing SSMs "for longer contexts (for example, >10 k rows)."
- Why unresolved: The study limited experiments to contexts of 1,000 rows (conforming to prior benchmarks) and did not evaluate the specific data regimes where SSMs have the highest theoretical advantage over Transformers.
- What evidence would resolve it: Benchmarks on large-scale tabular datasets (>10k rows) comparing the accuracy and memory footprint of Hydra-based TabPFN against FlashAttention-enabled Transformers.

### Open Question 2
- Question: Do optimal row orderings exist that enhance performance for SSM-based PFNs, and do they differ between unidirectional and bidirectional models?
- Basis in paper: The conclusion suggests that "certain row orderings of the context may enhance the performance of SSMs" and that such orderings "may differ for unidirectional and bidirectional SSMs."
- Why unresolved: The current work mitigates order sensitivity through random shuffling (RCP) rather than investigating if specific deterministic orderings could be leveraged to improve accuracy.
- What evidence would resolve it: A study evaluating model performance when context rows are sorted by specific criteria (e.g., feature distance, class labels) compared to random shuffling.

### Open Question 3
- Question: Can architectural modifications remove the need for Repeated Context Permutations (RCP) to achieve order invariance without incurring multiple inference passes?
- Basis in paper: The authors note RCP linearly increases inference time (Algorithm 1), while the conclusion calls for investigating how to "mitigate the impact of the row order."
- Why unresolved: RCP is a post-hoc inference technique that treats order sensitivity as a noise problem rather than solving the architectural limitation that causes sensitivity in SSMs.
- What evidence would resolve it: A modified SSM layer or positional encoding scheme that achieves order invariance natively in a single forward pass.

## Limitations
- Empirical validation scope is limited to 30 OpenML datasets, which may not generalize to highly heterogeneous or real-world tabular data distributions
- The claim about enabling "scalability to much larger datasets" is supported but not exhaustively tested beyond controlled experimental bounds
- Computational overhead of Repeated Context Permutations (RCPs) is mentioned but not quantified in terms of wall-clock time or memory usage
- The bidirectional architecture's robustness across diverse tabular structures (e.g., sparse vs. dense, mixed-type features) is not fully characterized

## Confidence
- **High**: Core finding that bidirectional SSMs (Hydra) outperform unidirectional Mamba in TabPFN contexts
- **Medium**: Claims about order-robustness improvements via RCPs
- **Low**: Broader scalability assertions beyond controlled experimental bounds

## Next Checks
1. Evaluate Hydra-based TabPFN on tabular datasets with varying feature distributions (e.g., high sparsity, categorical dominance) to test architectural robustness
2. Quantify the computational cost (time/memory) of RCPs in training and inference to assess practical scalability trade-offs
3. Test the model's performance on streaming or dynamically updated tabular data to validate order-robustness claims in non-static scenarios