---
ver: rpa2
title: Deep Learning for Forensic Identification of Source
arxiv_id: '2503.20994'
source_url: https://arxiv.org/abs/2503.20994
tags:
- contrastive
- cartridge
- learning
- networks
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the common-but-unknown source problem in forensics
  by comparing cartridge casings to determine if they were fired from the same firearm.
  The authors use contrastive neural networks trained on 3D topography scans of 9mm
  cartridge casings to learn similarity scores between casings.
---

# Deep Learning for Forensic Identification of Source

## Quick Facts
- arXiv ID: 2503.20994
- Source URL: https://arxiv.org/abs/2503.20994
- Authors: Cole Patten; Christopher Saunders; Michael Puthawala
- Reference count: 36
- Primary result: Contrastive neural networks achieve ROC AUC of 0.892 on cartridge casing verification, outperforming state-of-the-art CMC algorithm (0.867 AUC)

## Executive Summary
This paper addresses the forensic challenge of determining whether cartridge casings were fired from the same firearm when the source is unknown. The authors propose using contrastive neural networks trained on 3D topography scans of breech face impressions, converting images to polar coordinates with cyclic padding to achieve rotational invariance. Their approach learns similarity scores between casings and demonstrates superior performance compared to the traditional Congruent Matching Cells (CMC) algorithm, achieving a ROC AUC of 0.892 versus 0.867 on the NBIDE dataset. The work shows that contrastive learning can effectively solve forensic verification tasks while avoiding the limitations of classification-based approaches.

## Method Summary
The method uses supervised contrastive learning with residual networks to learn similarity metrics between cartridge casings. The approach processes 3D topography scans by first converting them to polar coordinates and applying cyclic padding to achieve rotational invariance, then normalizing the data. A custom ResNet architecture serves as the backbone, mapping breech face impressions to embedding spaces where similar casings cluster together. The supervised contrastive loss explicitly maximizes similarity for same-source pairs while minimizing it for different sources. The model is trained on the E3 dataset (2967 casings, 297 firearms) and evaluated on the NBIDE dataset (144 casings, 12 firearms) using ROC AUC as the primary metric.

## Key Results
- Contrastive learning approach achieved ROC AUC of 0.892 on NBIDE dataset
- Outperformed state-of-the-art CMC algorithm with ROC AUC of 0.867
- Reference Model (width=16, 79k parameters) showed optimal performance; larger models underperformed due to overfitting
- Demonstrated moderate robustness to architectural variations in ablation study
- Training required 12 hours; inference was near-instantaneous compared to 9 days for CMC

## Why This Works (Mechanism)

### Mechanism 1: Geometric Invariance via Cyclic Padding
Converting images to polar coordinates and applying cyclic padding explicitly encodes rotational invariance by structurally enforcing it through architecture rather than relying on data augmentation. This is critical because cartridge casing orientation when fired is arbitrary.

### Mechanism 2: Supervised Contrastive Embedding (Metric Learning)
Optimizing supervised contrastive loss creates an embedding space where same-source casings cluster tightly, enabling similarity scores that outperform traditional correlation algorithms through explicit "push-pull" dynamics in the feature space.

### Mechanism 3: Verification vs. Classification Paradigm
Framing the problem as verification rather than classification avoids the "exhaustive gallery" error where systems must assign casings to known guns even when the real source is unknown, allowing the network to simply indicate non-matches.

## Foundational Learning

- **Concept:** Contrastive Learning (Siamese Networks)
  - Why needed: The paper learns a similarity metric rather than class labels, requiring understanding of how networks process input pairs to produce distances
  - Quick check: How does the network handle an input pair (A, B) during training vs. inference?

- **Concept:** Forensic "Common-but-Unknown Source"
  - Why needed: Defines success criteria where standard accuracy metrics don't apply due to effectively infinite "unknown" class
  - Quick check: Why does a standard classifier fail if the suspect's gun is not in the training database?

- **Concept:** Residual Connections (ResNets)
  - Why needed: The paper uses ResNet architecture, requiring understanding of skip connections for correct implementation
  - Quick check: What happens to the gradient in a residual block with a skip connection during backpropagation?

## Architecture Onboarding

- **Component map:** 3D topography scans → 2D grayscale breech face crop → Cartesian to Polar transform → Cyclic Padding → Normalization → Custom ResNet → Projection to embedding space → Supervised Contrastive Loss

- **Critical path:** Data preprocessing (polar coordinate conversion with cyclic padding) is most critical; misalignment breaks rotational invariance. Training requires careful batch construction to ensure positive and negative pairs exist.

- **Design tradeoffs:** Model size vs. data shows "bigger is not always better" with small datasets—largest model (Width=64) performed worse than Reference Model (Width=16) due to overfitting. Training is computationally expensive (12 hours) but inference is near-instantaneous.

- **Failure signatures:** Performance collapse when adding datasets with few exemplars per class indicates the network is learning to distinguish specific images rather than general features. Overfitting manifests as decreasing training loss but unstable or dropping test ROC AUC.

- **First 3 experiments:**
  1. Replicate Reference Model (Width=16) and verify ROC AUC (~0.89) against CMC baseline (0.867) on NBIDE
  2. Train Width=8 and Width=32 models to confirm performance peaks at Width=16 for this dataset size
  3. Attempt leave-two-out cross-validation on NBIDE to validate why training on larger E3 dataset was preferred

## Open Questions the Paper Calls Out

- Can contrastive networks improve ROC AUC by utilizing both breech face and firing pin impression data simultaneously, potentially removing superfluous preprocessing steps?

- How does the ratio of parameters to training samples impact the bias-variance tradeoff and optimal stopping point for this forensic task, given the current approach trained until overfitting?

- What is the minimum number of exemplars required per firearm class to ensure effective contrastive learning without negative transfer, given the performance drop when using datasets with only 2 exemplars per gun?

- Will introduction of significantly larger training datasets confirm theoretical scaling laws that larger models (e.g., Width=64) will outperform the current Reference Model when data constraints are removed?

## Limitations

- Small test set (144 samples) constrains statistical confidence in reported ROC AUC of 0.892
- Ablation study based on only 5 runs per configuration, limiting reliability of variance estimates
- No hyperparameter sensitivity analyses reported, leaving uncertainty about robustness to training settings
- Cyclic padding assumes perfect breech face center identification; preprocessing misalignment could break rotational invariance

## Confidence

- High: Contrastive learning mechanism for creating similarity metrics is theoretically sound and well-supported
- Medium: Superiority over CMC demonstrated but limited by test set size; architectural ablation shows trends but lacks statistical power
- Low: Generalization to firearms outside training distribution asserted but not empirically validated with diverse, unseen firearms

## Next Checks

1. Conduct statistical power analysis to determine minimum test set size needed for reliable ROC AUC estimates
2. Perform leave-one-firearm-out cross-validation to assess generalization to unseen firearm models
3. Test cyclic padding robustness by intentionally introducing preprocessing misalignment and measuring performance degradation