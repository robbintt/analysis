---
ver: rpa2
title: Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings
arxiv_id: '2512.12167'
source_url: https://arxiv.org/abs/2512.12167
tags:
- positional
- context
- rope
- attention
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DroPE, a method for extending the context
  length of pretrained large language models without requiring expensive long-context
  fine-tuning. The key insight is that positional embeddings (PEs), while critical
  for efficient training, limit zero-shot generalization to longer sequences.
---

# Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings

## Quick Facts
- arXiv ID: 2512.12167
- Source URL: https://arxiv.org/abs/2512.12167
- Reference count: 40
- Primary result: Zero-shot context extension for pretrained RoPE LLMs via short recalibration phase, achieving up to 8× context length without expensive fine-tuning

## Executive Summary
This paper introduces DroPE, a method to extend the context length of pretrained large language models without requiring expensive long-context fine-tuning. The key insight is that positional embeddings (PEs), while critical for efficient training, limit zero-shot generalization to longer sequences. DroPE removes PEs after pretraining and performs a short recalibration phase at the original context length. Empirically, DroPE enables zero-shot context extension far beyond the training length, outperforming RoPE-scaling methods and specialized architectures on needle-in-a-haystack retrieval and long-context language modeling tasks. It matches the in-context performance of RoPE models and scales effectively to billion-parameter models, requiring only a tiny fraction of the original pretraining compute.

## Method Summary
DroPE is a simple yet effective method for extending the context length of pretrained large language models. The approach involves removing all positional embedding operations (specifically RoPE rotations) from a fully pretrained RoPE-based model, then performing a short "recalibration" training phase on the original pretraining data and context length. This recalibration phase may include QK-normalization for stability with higher learning rates. The resulting model can generalize to longer contexts zero-shot, potentially with tuned softmax temperature scaling at inference. DroPE avoids the computational expense of long-context fine-tuning while maintaining or improving performance on both standard and extended-context tasks.

## Key Results
- Zero-shot context extension up to 8× training length on needle-in-a-haystack retrieval tasks
- Matches or outperforms specialized architectures and RoPE-scaling methods on long-context language modeling
- Requires only 0.5-2% of original pretraining compute for recalibration
- Successfully scales from 360M to 7B parameter models while maintaining performance

## Why This Works (Mechanism)
DroPE works by removing the constraint of fixed positional embeddings that limit zero-shot extrapolation. During pretraining, positional embeddings provide explicit position information but create a hard boundary at the training context length. By dropping these embeddings and recalibrating the model at the original context length, DroPE allows the model to develop implicit positional understanding that can generalize to longer sequences. The short recalibration phase enables the model to relearn positional relationships without the constraints of fixed embeddings, effectively "unlocking" its ability to handle longer contexts.

## Foundational Learning
- **Positional Embeddings (RoPE):** Methods for encoding token positions in transformer models; why needed: provide explicit position information during training; quick check: Verify attention layers contain rotation operations
- **Zero-shot Generalization:** Model's ability to perform on tasks/datasets not seen during training; why needed: key goal of extending context without fine-tuning; quick check: Test model on sequences longer than training length
- **QK-Normalization (QKNorm):** Normalization applied to query/key projections to stabilize training; why needed: prevents instability when removing positional embeddings; quick check: Monitor gradient norms during recalibration
- **Needle-in-a-Haystack (NIAH):** Retrieval task where a small target is embedded in long context; why needed: evaluates long-context memory and retrieval capability; quick check: Verify retrieval accuracy at various context lengths
- **Temperature Scaling:** Adjusting softmax temperature at inference to optimize performance; why needed: compensates for distribution shift in longer sequences; quick check: Tune temperature parameter on validation set

## Architecture Onboarding

- **Component map:** Pretrained RoPE Model -> Remove PE operations -> Add QKNorm -> Recalibration Training -> DroPE Model
- **Critical path:**
    1. Start with a fully pretrained RoPE-based LLM checkpoint
    2. Modify the model's forward pass to disable or remove all positional embedding operations
    3. Perform a short "recalibration" training phase on the original pretraining data and context length. This phase may include QK-normalization for stability with higher learning rates
    4. The resulting model can be deployed and will generalize to longer contexts zero-shot, potentially with a tuned softmax temperature scaling
- **Design tradeoffs:** The primary tradeoff is compute vs. architectural simplicity. DroPE requires an additional, albeit short, training phase. The benefit is avoiding expensive long-context fine-tuning. Another tradeoff is performance: DroPE aims to match the base model's original performance, which it achieves, but it does not inherently improve model quality beyond enabling longer contexts
- **Failure signatures:**
    - Catastrophic forgetting: If the recalibration phase is too aggressive (high learning rate, too many steps), the model may lose its pretrained knowledge
    - Training instability: Without QK-normalization, training with higher learning rates can lead to loss spikes and divergence
    - Performance degradation: If the base model relied heavily on absolute positional cues that cannot be recovered implicitly, the DroPE model may underperform on standard benchmarks
- **First 3 experiments:**
    1. Baseline Reproduction: Take a small pretrained RoPE model (e.g., the 500M parameter model from the paper), apply the DroPE recalibration for a set number of steps (e.g., 2K steps), and compare its validation perplexity against the original RoPE checkpoint and a NoPE model trained from scratch
    2. Long-Context Evaluation: Evaluate the recalibrated DroPE model on a needle-in-a-haystack (NIAH) task at 2x, 4x, and 8x the original training context length. Compare its retrieval accuracy against the base model and the base model enhanced with RoPE-scaling methods like YaRN
    3. Ablation on Recalibration Start Point: Retrain the base model, but drop the PEs and begin recalibration at different points in the original training run (e.g., 0%, 50%, 87.5%, 100% of total steps). Measure the final validation perplexity to confirm that retaining RoPE for most of training yields the best results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can QKNorm be replaced by normalizing only queries, and would this enable faster recalibration or training without positional embeddings at larger scales?
- Basis in paper: [explicit] "We note that preliminary experiments showed that normalizing only the queries led to even faster learning and also successfully stabilized long training. We believe further exploration of this new Q-norm method could be an exciting direction for future work to train transformers without positional embeddings at even larger scales."
- Why unresolved: Only preliminary experiments were conducted; systematic comparison of query-only vs. query-key normalization was not performed
- What evidence would resolve it: Controlled experiments comparing Q-only, K-only, and QK normalization across multiple model scales with learning rate sweeps and convergence analysis

### Open Question 2
- Question: What is the maximum context extension factor DroPE can achieve before performance degrades, and is there a theoretical upper bound?
- Basis in paper: [inferred] The paper demonstrates extension up to 8× the original context but does not explore larger factors or provide theoretical bounds on extrapolation limits
- Why unresolved: Empirical evaluation stops at 8× extension; no theoretical analysis of extrapolation capacity for NoPE architectures after recalibration
- What evidence would resolve it: Systematic evaluation at 16×, 32×, and beyond; theoretical analysis relating model capacity and recalibration budget to maximum extrapolation

### Open Question 3
- Question: Can DroPE be combined with alternative positional embedding schemes (e.g., p-RoPE, Fourier PE) to further improve long-context performance?
- Basis in paper: [explicit] "These directions are complementary to ours and can be used in place of RoPE within the DroPE framework."
- Why unresolved: No experiments were conducted with non-RoPE positional embeddings before dropping
- What evidence would resolve it: Comparative experiments training with Fourier, wavelet, or hybrid positional encodings, then applying DroPE's dropping and recalibration procedure

### Open Question 4
- Question: Does DroPE scale to 70B+ parameter models while maintaining the same recalibration efficiency observed at 7B?
- Basis in paper: [inferred] The largest model tested is Llama2-7B; billion-parameter models may exhibit different optimization dynamics during recalibration
- Why unresolved: No experiments at 70B+ scale; unclear if 0.5-2% recalibration budget scales linearly
- What evidence would resolve it: DroPE recalibration experiments on 70B+ models with varying recalibration budgets, measuring both recovery of in-context performance and long-context generalization

## Limitations
- DroPE's effectiveness depends on the base model's capacity to learn implicit positional understanding during recalibration
- The method has only been validated on decoder-only transformer architectures, limiting generalizability
- Performance on tasks requiring strict positional accuracy beyond tested retrieval and language modeling tasks remains unknown

## Confidence

- **High confidence:** The empirical findings regarding performance improvements on NIAH and LongBench tasks, the successful application to multiple model sizes (360M to 7B parameters), and the computational efficiency claims (short recalibration phase vs. expensive long-context fine-tuning) are well-supported by the presented experiments and ablation studies
- **Medium confidence:** The core hypothesis that positional embeddings limit zero-shot context extension is supported but not definitively proven. While the ablation study on retention duration is strong evidence, alternative explanations (e.g., specific optimization dynamics during recalibration) cannot be ruled out without further investigation. The proposed QKNorm solution is based on preliminary findings rather than comprehensive analysis
- **Low confidence:** The generalizability of DroPE to all model families and pretraining approaches is asserted but not demonstrated. The paper does not explore potential failure modes in detail, and the method's performance on tasks requiring strict positional accuracy (beyond the tested retrieval and language modeling tasks) remains unknown

## Next Checks
1. **Architectural Generality Test:** Apply DroPE to a pretrained encoder-only transformer (e.g., BERT) and evaluate its performance on a long-document classification task. This would validate whether the method generalizes beyond decoder-only LLMs and whether it works for tasks requiring strict positional awareness
2. **Pretraining Dependency Analysis:** Conduct a more granular ablation study on the retention duration of RoPE during pretraining (e.g., at 25%, 50%, 75%, 87.5%, and 100% of steps) to precisely quantify how much positional information is necessary and at what stage of training. This would strengthen the paper's claims about the detrimental effect of PEs on zero-shot generalization
3. **Failure Mode Characterization:** Systematically test DroPE on a model known to rely heavily on absolute positional information (e.g., a model trained with fixed absolute positional embeddings) and characterize the specific failure modes. This would help define the boundaries of DroPE's applicability and provide guidance on when alternative long-context methods are preferable