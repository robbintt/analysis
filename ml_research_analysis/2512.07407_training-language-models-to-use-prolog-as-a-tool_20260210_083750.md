---
ver: rpa2
title: Training Language Models to Use Prolog as a Tool
arxiv_id: '2512.07407'
source_url: https://arxiv.org/abs/2512.07407
tags:
- reasoning
- prolog
- reward
- answer
- height
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using reinforcement learning to teach language
  models to use Prolog as an external tool for reliable reasoning. The authors fine-tune
  a 3B-parameter model using Group Relative Policy Optimization (GRPO) on a cleaned
  GSM8K-Prolog-Prover dataset, systematically varying prompt structures, reward compositions,
  and inference protocols.
---

# Training Language Models to Use Prolog as a Tool

## Quick Facts
- arXiv ID: 2512.07407
- Source URL: https://arxiv.org/abs/2512.07407
- Reference count: 40
- Primary result: A 3B-parameter model achieves 89.87% accuracy on GSM8K-Prolog-Prover tasks and 80.21% on GSM8K test set using reinforcement learning to use Prolog as a tool.

## Executive Summary
This paper demonstrates that reinforcement learning can teach small language models to use Prolog as an external tool for reliable mathematical reasoning. By fine-tuning a 3B-parameter Qwen2.5-Instruct model using Group Relative Policy Optimization (GRPO) on a cleaned GSM8K-Prolog dataset, the authors achieve state-of-the-art accuracy for small models while producing auditable symbolic reasoning traces. The approach shows that carefully designed reward structures and agentic inference protocols can compensate for smaller model size, achieving zero-shot MMLU performance comparable to few-shot results from 7B models.

## Method Summary
The authors fine-tune a 4-bit quantized Qwen2.5-3B-Instruct model using LoRA (r=64, α=64) on a cleaned GSM8K-Prolog-Prover dataset (2,500 train, 375 validation examples). Training uses GRPO with AdamW optimizer (lr=5e-6, batch=8, 1 epoch, cosine schedule, weight_decay=0.1, grad_clip=0.1). The model is evaluated using four inference protocols: Single-Try, Multiple-Try, Agentic Internal, and Agentic Independent, with temperature=0.2. Three reward suites are tested, varying from correctness-focused to structure-and-faithfulness-focused compositions. SWI-Prolog with CLP(Q) library serves as the external tool for execution and verification.

## Key Results
- Best configuration (SP-Struct prompt, Reward Suite 1) achieves 89.87% accuracy on in-distribution GSM8K-Prolog-Prover tasks and 80.21% on GSM8K test set.
- With agentic inference, the 3B model achieves zero-shot MMLU performance comparable to few-shot results from 7B models (58.13% on MMLU-Stem, 30.67% on MMLU-Pro).
- Reward Suite 3 with SP-Declare prompt produces the most faithful Prolog code (structural validity 6.9%, semantic similarity 52.6%) but at the cost of accuracy (59.73%).
- Agentic Internal inference shows the largest performance gains on out-of-distribution tasks, demonstrating the value of interactive tool use.

## Why This Works (Mechanism)
The approach works by combining reinforcement learning with tool-augmented inference, allowing the model to iteratively generate, execute, and refine Prolog code rather than producing a single static answer. GRPO's relative reward computation encourages exploration of different reasoning paths while the reward suites provide structured feedback on correctness, syntax, format, and (optionally) semantic fidelity. Agentic inference protocols enable the model to debug and correct its Prolog code through iterative interaction with the Prolog interpreter, mimicking how humans use external tools to verify and refine their reasoning.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: This RL algorithm samples multiple outputs per prompt and uses their relative rewards to update the policy. Quick check: Can you explain why GRPO samples multiple outputs for a single input before updating, and how it determines which outputs to reinforce?
- **Logic Programming & Constraint Logic Programming (CLP)**: Understanding facts, rules, queries, and how curly-brace constraints `{X = Y + Z}` differ from `is` is essential. Quick check: What is the difference between `X is 2+2` and `{X = 2+2}` in CLP(Q), and which one is preferred for symbolic reasoning in this context?
- **Tool-Augmented / Agentic Inference**: The paper distinguishes between "prolog-as-output" (generate-then-verify) and "prolog-as-a-tool" (interactive, agentic). Quick check: What is the key difference between 'Agentic Internal' and 'Agentic Independent' inference when the model encounters a persistent failure?

## Architecture Onboarding
- **Component map**: Base Model (Qwen2.5-3B-Instruct) -> LoRA Adapter -> GRPO Training Loop -> Reward Engine (Python functions) -> SWI-Prolog Tool -> Inference Protocols
- **Critical path**: Clean dataset (gsm8k-prolog-prover) → Train with GRPO, Reward Suite 1, SP-Struct prompt → Evaluate with Agentic Internal inference for generalization or Multiple-Try for max accuracy
- **Design tradeoffs**: Optimizing for accuracy (Reward Suite 1, SP-Struct) may produce 'hacky' code with poor structural validity. Optimizing for structural/semantic match (Reward Suite 2/3, SP-Declare) improves faithfulness but may lower accuracy.
- **Failure signatures**: Recursion Trap (infinite recursion detected), Reward Hacking (hard-coding answers), Scaffold Failure (well-formatted but invalid Prolog code)
- **First 3 experiments**: 1) Reproduce baseline with SP-Struct, Reward Suite 1, and Multiple-Try inference. 2) Test inference scaling on GSM8K test set and MMLU-Stem using all four protocols. 3) Ablate reward signals with SP-Declare and Reward Suite 2 to measure trade-offs.

## Open Questions the Paper Calls Out
- **Generalization to richer domains**: Can this approach generalize to probabilistic programming or multi-tool integration? The current scope is limited to single-tool mathematical reasoning.
- **Dynamic reward schedules**: Would live validation-based reward adaptation outperform the fixed curriculum used in Reward Suite 3? The paper suggests this could be beneficial.
- **Minimizing accuracy-faithfulness trade-off**: How can the systematic trade-off between high accuracy and strict structural faithfulness be reduced? Current reward suites fail to incentivize both simultaneously.

## Limitations
- The approach requires manual dataset cleaning (15 discrepancies in GSM8K-Prolog) to ensure valid training examples.
- There's a fundamental trade-off between answer accuracy and structural faithfulness of the generated Prolog code.
- Generalization to richer domains and multi-tool scenarios remains unexplored.

## Confidence
- **High**: GRPO training methodology, reward suite design, and inference protocols are well-specified and reproducible.
- **Medium**: Some GRPO-specific parameters (number of generations G, advantage normalization) are not fully specified.
- **Medium**: LoRA target modules are not explicitly listed, though standard practice suggests attention layers.

## Next Checks
1. Verify GRPO implementation details including generation count G and advantage computation method.
2. Test the impact of different LoRA target modules on final performance.
3. Implement and evaluate dynamic reward scheduling based on live validation metrics.