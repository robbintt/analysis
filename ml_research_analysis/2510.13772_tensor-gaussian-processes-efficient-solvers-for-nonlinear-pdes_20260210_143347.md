---
ver: rpa2
title: 'Tensor Gaussian Processes: Efficient Solvers for Nonlinear PDEs'
arxiv_id: '2510.13772'
source_url: https://arxiv.org/abs/2510.13772
tags:
- points
- collocation
- e-01
- nonlinear
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TGPS, a tensor Gaussian process-based solver
  for nonlinear PDEs that overcomes the scalability limitations of existing GP/kernel-based
  methods. The core idea is to decompose the solution into one-dimensional factor
  functions along each input dimension, modeled as GPs and combined via tensor decomposition
  (CP or TR), reducing the problem to learning a collection of one-dimensional GPs.
---

# Tensor Gaussian Processes: Efficient Solvers for Nonlinear PDEs

## Quick Facts
- arXiv ID: 2510.13772
- Source URL: https://arxiv.org/abs/2510.13772
- Reference count: 40
- Primary result: TGPS achieves 10⁻³ to 10⁻⁶ accuracy on nonlinear PDEs with linear scaling in dimension and collocation points

## Executive Summary
The paper introduces TGPS, a tensor Gaussian process-based solver for nonlinear PDEs that overcomes the scalability limitations of existing GP/kernel-based methods. The core idea is to decompose the solution into one-dimensional factor functions along each input dimension, modeled as GPs and combined via tensor decomposition (CP or TR), reducing the problem to learning a collection of one-dimensional GPs. This design enables linear scaling with both input dimension and number of collocation points. For nonlinear PDEs, the authors linearize nonlinear terms using partial freezing and Newton's method, then apply an alternating least squares (ALS) scheme with closed-form updates, avoiding inefficient stochastic optimization.

## Method Summary
TGPS represents the PDE solution u(x) as a tensor decomposition of one-dimensional Gaussian process factor functions. Each factor fᵢᵣ(xᵢ) is modeled as a GP with inducing points, and the full solution combines these factors via CP or Tensor-Ring format. For nonlinear PDEs, the method linearizes nonlinear terms using partial freezing (freezing terms from previous iteration) or Newton's method (first-order Taylor expansion). An alternating least squares algorithm with closed-form updates then solves for the inducing values, cycling through dimensions while keeping other factors fixed. This approach achieves linear scaling with both dimension and collocation points while maintaining accuracy comparable to or better than PINNs.

## Key Results
- TGPS achieves 10⁻³ to 10⁻⁶ relative L² errors on benchmark nonlinear PDEs
- 100-1000× faster training than PINN-style stochastic optimization
- Scales to 6D problems with 48K collocation points
- Superior accuracy and efficiency compared to PINNs and recent GP solvers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor decomposition enables linear scaling with dimension and collocation count
- Mechanism: Decomposes d-dimensional problem into d independent 1D GP learning problems, reducing complexity from O(M³) to O(M·Σᵢ(NᵢR)² + Nᵢ³R³)
- Core assumption: True solution admits low-rank tensor approximation (R ≲ (√d/ε)^(d-1)/k under Sobolev regularity)
- Evidence anchors: [abstract] "reduces the task to learning a collection of one-dimensional GPs"; [section 3.1] Complexity analysis; [corpus] Standard GP scalability issues noted
- Break condition: Exponentially large rank (R ∝ (√d/ε)^(d-1)) under only basic Sobolev regularity

### Mechanism 2
- Claim: ALS with closed-form updates eliminates stochastic optimization inefficiency
- Mechanism: Multilinear structure of solution and derivatives enables quadratic loss in each factor when others are fixed, yielding closed-form least squares solution
- Core assumption: PDE operators can be linearized while maintaining multilinear structure during ALS steps
- Evidence anchors: [abstract] "develop an alternating least squares (ALS) approach that admits closed-form updates"; [section 3.2] Derivation of multilinear structure; Figure 2 shows 100-1000× faster convergence
- Break condition: Nonlinear terms cannot be adequately linearized; iterative scheme diverges with poor initialization

### Mechanism 3
- Claim: Partial freezing and Newton linearization enable ALS for nonlinear PDEs
- Mechanism: Linearizes nonlinear terms by freezing select factors from previous iteration (partial freezing) or using first-order Taylor expansion (Newton), preserving closed-form ALS updates
- Core assumption: Frozen/linearized terms provide sufficient approximation quality for convergence
- Evidence anchors: [abstract] "use a partial freezing strategy and Newton's method"; [section 3.2] Explicit linearization formulas; Table 2 shows 10⁻⁴-10⁻⁶ errors on challenging nonlinear PDEs
- Break condition: Highly nonlinear or discontinuous terms where first-order approximation fails; multiple solution branches causing convergence to local minima

## Foundational Learning

- Concept: **Reproducing Kernel Hilbert Spaces (RKHS) and GP conditional mean**
  - Why needed here: Each factor function fᵢᵣ lives in an RKHS Gᵢ induced by kernel κᵢ; kernel interpolation form fᵢᵣ(xᵢ) = κᵢ(xᵢ,γᵢ)Kᵢ⁻¹ηᵢᵣ is the GP conditional mean
  - Quick check question: Given a squared exponential kernel κ(x,x') = exp(-|x-x'|²/2ℓ²), what is the effect of length-scale ℓ on the RKHS norm of a function with fixed inducing values?

- Concept: **CP and Tensor-Ring (TR) Decomposition**
  - Why needed here: CP: u = Σᵣ ∏ᵢ fᵢᵣ(xᵢ) with shared rank R; TR: u = Tr(F₁(x₁)F₂(x₂)...F_d(x_d)) where Fᵢ outputs Rᵢ₋₁ × Rᵢ matrices
  - Quick check question: For d=4 with CP rank R=10 versus TR rank Rᵢ=3, which has fewer total parameters? (Answer: CP has d·N·R; TR has d·N·R²)

- Concept: **PDE Stability and Fill Distance**
  - Why needed here: Convergence requires PDE stability (∥u₁-u₂∥ ≤ C·∥P(u₁)-P(u₂)∥) and bounded Voronoi aspect ratio; fill distance h = sup_x inf_{x'∈M} |x-x'| must → 0
  - Quick check question: If you double the number of collocation points in a 2D domain, by what factor does the fill distance decrease (assuming uniform sampling)? (Answer: roughly 1/√2)

## Architecture Onboarding

- Component map:
  - Factor functions fᵢᵣ -> One per dimension-rank pair, each a GP with Nᵢ inducing points
  - Inducing locations γᵢ -> Fixed grid per dimension (not trained)
  - Kernel matrices Kᵢ -> Precomputed and inverted once
  - Linearization module -> Implements partial freezing (TGPS-PF) or Newton (TGPS-NT)
  - ALS solver -> Cycles through dimensions, solving least-squares for Hᵢ given fixed Hⱼ (j≠i)

- Critical path:
  1. Initialize inducing values (e.g., small random or zero)
  2. For each ALS iteration:
     a. Linearize nonlinear terms using current solution
     b. For each dimension i: Assemble least-squares system AᵢHᵢ = bᵢ from residual + boundary losses; Solve via Cholesky: Hᵢ = (AᵢᵀAᵢ + λKᵢ⁻¹)⁻¹Aᵢᵀbᵢ
  3. Check convergence (loss change < tolerance)

- Design tradeoffs:
  - CP vs TR decomposition: CP simpler to implement; TR more expressive per parameter but requires matrix operations
  - Partial Freezing vs Newton: PF simpler (just freeze terms); Newton may converge faster but requires computing Fréchet derivatives
  - More inducing points vs higher rank: Nᵢ controls per-dimension resolution; R controls cross-dimensional interactions

- Failure signatures:
  - Loss plateaus early: initialization too far from solution; try smaller learning rate equivalent or damping
  - Divergence during ALS: check kernel length-scales; ensure Kᵢ is well-conditioned with nugget
  - Poor accuracy on high-frequency solutions: increase Nᵢ or use Matérn kernel with smaller length-scale

- First 3 experiments:
  1. Reproduce nonlinear elliptic PDE (Eq. 18) with 2400 collocation points: verify relative L² error < 10⁻⁷ (Table 1b)
  2. Ablate kernel length-scales on Burgers' equation (ν=0.02) per Table 6: confirm sensitivity and identify working range
  3. Test 6D Allen-Cahn (a=15) with 48K collocation points: compare CP vs TR decomposition (Table 8b)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit regularization techniques be designed to stabilize the alternating least squares (ALS) updates against poor initialization?
- Basis: [explicit] The authors state in Section K that the current fixed-point iterations may diverge if the starting point is poorly chosen, and they propose designing regularization in future work.
- Why unresolved: The current algorithm lacks a mechanism to guarantee convergence from arbitrary initializations, limiting robustness.
- What evidence would resolve it: A modified algorithm incorporating parameter estimates from previous iterations that empirically converges reliably across random seeds and diverse initial conditions.

### Open Question 2
- Question: Can the optimal tensor rank (number of factor functions) be determined adaptively during training?
- Basis: [inferred] The theoretical analysis provides loose bounds on rank, and the experiments rely on a manual grid search (e.g., $\{5, 10, \dots, 25\}$) to find a suitable rank.
- Why unresolved: There is currently no principled, automated method to select the rank $R$, leading to potential inefficiency (over-parameterization) or loss of accuracy (under-parameterization).
- What evidence would resolve it: An extension of the framework that dynamically adjusts rank based on error thresholds or variational bounds, demonstrated on the benchmark PDEs.

### Open Question 3
- Question: Does the required rank scale independently of the input dimension for general PDE solutions?
- Basis: [inferred] Lemma 4.1 suggests rank depends on $d$, yet the experiments treat rank as a fixed hyperparameter even as dimensions increase to 6D.
- Why unresolved: It is unclear if the linear scaling advantage holds for very high dimensions (e.g., $d > 10$) without requiring an exponential increase in rank to maintain accuracy.
- What evidence would resolve it: Theoretical bounds or empirical results showing that a fixed or slowly growing rank suffices to solve PDEs in 10+ dimensions with low error.

## Limitations

- Potential exponential rank growth (R ∝ (√d/ε)^(d-1)) under only Sobolev regularity could make storage infeasible for very high dimensions
- Linearization approach (partial freezing/Newton) is heuristic with no convergence guarantees for all nonlinearities
- Some implementation details (exact inducing value initialization, hyperparameter selection strategy) are underspecified

## Confidence

- **High confidence**: CP/TR decomposition enabling linear scaling, ALS with closed-form updates, theoretical expressivity bounds
- **Medium confidence**: Nonlinear PDE linearization effectiveness, convergence guarantees under stated conditions, rank requirements under Sobolev regularity
- **Low confidence**: Performance on extremely high-dimensional problems (d > 6), robustness to poor initialization, generalizability beyond benchmark PDE forms

## Next Checks

1. **Rank scalability test**: Systematically vary CP/TR rank (R ∈ [5,20,50,100]) on 4D Allen-Cahn (a=20) and measure accuracy/storage trade-offs to empirically determine rank requirements
2. **Initialization robustness**: Run 2D Burgers' with ν=0.001 using 5 different initialization strategies (zero, small random, pretrained PINN, etc.) and measure convergence variance
3. **Linearization failure modes**: Test TGPS-NT on a nonlinear PDE with known discontinuous solutions or multiple branches to identify conditions where first-order Taylor linearization fails