---
ver: rpa2
title: We Can't Understand AI Using our Existing Vocabulary
arxiv_id: '2502.07586'
source_url: https://arxiv.org/abs/2502.07586
tags:
- concepts
- language
- words
- neologism
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes that understanding AI requires developing new
  vocabulary terms ("neologisms") to represent machine-specific concepts. The authors
  frame interpretability as a communication problem between human and machine conceptualizations
  of the world.
---

# We Can't Understand AI Using our Existing Vocabulary

## Quick Facts
- arXiv ID: 2502.07586
- Source URL: https://arxiv.org/abs/2502.07586
- Authors: John Hewitt; Robert Geirhos; Been Kim
- Reference count: 29
- Key outcome: Learning new vocabulary tokens ("neologisms") whose embeddings capture human concepts (communicated to machines) or machine concepts (learned by humans) to improve human-AI communication and control.

## Executive Summary
This paper proposes that understanding AI requires developing new vocabulary terms ("neologisms") to represent machine-specific concepts. The authors frame interpretability as a communication problem between human and machine conceptualizations of the world. They present a method called "neologism embedding learning" that learns new token embeddings to capture human concepts (like response length and diversity) and machine concepts (like response quality). Experimental results show that these learned neologisms successfully control model behavior - enabling precise length control where baseline prompting fails, increasing response diversity by up to 99% for finding correct answers, and allowing the model to communicate its own quality preferences. The work demonstrates that creating a shared human-machine language through neologisms can improve both understanding and control of AI systems.

## Method Summary
The method learns new token embeddings (neologisms) that capture specific concepts by optimizing preference loss while keeping all model parameters frozen. A new token is added to the vocabulary with an initialized embedding, then optimized using APO-up loss on preference pairs (chosen vs. rejected responses) until a heuristic early stopping criterion is met. The approach enables both human-to-machine concept transfer (teaching models human concepts via neologisms) and machine-to-human transfer (learning what neologisms mean from model preferences). Experiments use Gemma 2B with Adafactor optimizer, learning rate 0.02, and batch size 1, training until loss drops by 0.2.

## Key Results
- Neologisms enable precise length control where baseline prompting fails
- Response diversity increases by up to 99% for finding correct answers
- Model can communicate its own quality preferences through neologisms
- Behavioral effects achieved without modifying model weights

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Only Preference Learning
Learning a new token's embedding via preference optimization creates a controllable semantic handle without modifying model weights. A new token w is added to vocabulary V with randomly-initialized or seeded embedding E_w. Using preference dataset D = {(x_i, y_chosen, y_rejected)} where x_i contains w, the method optimizes only E_w via APO-up loss while keeping all model parameters θ frozen. The embedding learns to occupy a region in representation space that, when processed by the frozen model, steers generation toward the preferred behavior.

### Mechanism 2: Abstraction-Level Matching via Preference Contrast
Preference pairs implicitly define the abstraction level of the learned concept by encoding what distinguishes "chosen" from "rejected." The contrast between y_chosen and y_rejected in training data determines what the neologism encodes. If pairs differ primarily in response length, the neologism learns a length concept; if they differ in diversity, it learns diversity. The preference loss forces E_w to capture exactly the dimensions along which chosen exceeds rejected, creating an abstraction that is neither too detailed nor too high-level.

### Mechanism 3: Bidirectional Concept Transfer via Shared Token Interface
The same method enables both human-to-machine (H→M) and machine-to-human (M→H) concept transfer by reversing who defines the preference signal. For H→M, humans construct preference pairs reflecting their concept (e.g., length constraints), and the machine learns to respond to the neologism. For M→H, the model's own judgments define preferences (e.g., self-scoring responses), and humans learn what the neologism means by observing its effects. The token serves as a bidirectional interface.

## Foundational Learning

- Concept: **Preference Optimization (DPO/APO)**
  - Why needed here: The paper uses APO-up, a variant of Direct Preference Optimization, to train neologism embeddings. Understanding how preference losses encode relative preferences is essential.
  - Quick check question: Can you explain why DPO alone might cause both chosen and rejected outputs to decrease in probability, and how APO-up addresses this?

- Concept: **Token Embeddings in Transformers**
  - Why needed here: Neologism learning modifies only the embedding vector E_w ∈ R^d for a new token. Understanding how embeddings map to representation space and how frozen layers process them is critical.
  - Quick check question: If you freeze all model weights but train a single token's embedding, what gradient path allows the embedding to influence output?

- Concept: **Abstraction Levels in Interpretability**
  - Why needed here: The paper positions neologisms between mechanistic interpretability (too detailed) and behavioral evaluation (too high-level). Recognizing appropriate abstraction is key to designing useful neologisms.
  - Quick check question: Given a concept like "response helpfulness," what level of abstraction would be too detailed vs. too high-level?

## Architecture Onboarding

- Component map:
  - Tokenizer extension: Add new token w to vocabulary V → V' = V ∪ {w}
  - Embedding matrix augmentation: Append new embedding E_w (d-dimensional) to embedding matrix E ∈ R^(d×|V|)
  - Preference dataset constructor: Pairs (x containing w, y_chosen, y_rejected) defining concept
  - APO-up loss: Preference loss with anchor term to increase chosen probability
  - Frozen model wrapper: Ensures gradients flow only to E_w

- Critical path:
  1. Define concept and construct preference pairs that isolate it
  2. Initialize E_w (paper uses embedding of semantically related word like "Ensure")
  3. Optimize E_w via APO-up until loss decreases by ~0.2 (early stopping heuristic)
  4. Evaluate by prompting with neologism and measuring target behavior

- Design tradeoffs:
  - **Initialization strategy**: Random vs. seeded from existing word—seeded may converge faster but risks entanglement with existing semantics
  - **Learning rate**: Paper uses 0.02 for most experiments (high, but only one parameter vector); 0.001 for model-preference experiments—suggests sensitivity to noise in preference signal
  - **Batch size 1 with early stopping**: Prevents overfitting but may require careful tuning of stopping criterion

- Failure signatures:
  - **Text degeneration**: If both chosen and rejected probabilities drop, may indicate DPO-style collapse—switch to APO-up
  - **No behavioral change**: E_w may be in a representation-space "dead zone"; try different initialization
  - **Overly specific neologism**: Works only on training distribution—preference pairs may be too narrow
  - **Confounded concept**: Neologism has unpredictable effects—preference pairs may encode multiple unintended differences

- First 3 experiments:
  1. **Reproduce length neologism**: Take 100 LIMA instructions, construct length-constrained pairs using a teacher model, train ensure_h on Gemma 2B. Verify responses meet length constraints where baseline prompting fails.
  2. **Ablate initialization**: Train the same neologism with (a) random init, (b) "Ensure" embedding, (c) "Make" embedding. Compare convergence speed and final control accuracy.
  3. **Test composability**: Train separate length and diversity neologisms, then prompt with both in a single instruction. Measure whether both constraints are simultaneously satisfied—tests the claim that neologisms participate in compositional language.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multiple neologisms be composed together effectively in natural language prompts?
- Basis in paper: Authors state "Neologisms enable compositionality... they should combine together with other concepts we've learned" and "Users can also use them in composition with other new words."
- Why unresolved: No experiments test combining neologisms; only single neologisms (length, diversity, good_m) are evaluated independently.
- What evidence would resolve it: Experiments showing that prompts containing multiple learned neologisms (e.g., "Give me a diverse_h long_h response") produce behavior reflecting both concepts simultaneously.

### Open Question 2
- Question: How can models be taught to autonomously decide when and where to use neologisms in generation?
- Basis in paper: "In future work, we expect to instead teach the model where and when to use neologisms."
- Why unresolved: Current method enforces neologism tokens by replacing logits with −∞ during generation, preventing models from learning self-initiated usage.
- What evidence would resolve it: Demonstrating models that correctly insert neologisms into their outputs without explicit prompting, evaluated on tasks requiring concept communication.

### Open Question 3
- Question: Do neologism embeddings transfer across model architectures and scales?
- Basis in paper: All experiments use Gemma 2B; no investigation of whether learned neologisms generalize to larger models or different architectures.
- Why unresolved: Embeddings are learned for specific token representations tied to one model's embedding space.
- What evidence would resolve it: Testing whether neologisms learned on smaller models transfer zero-shot or with minimal adaptation to larger models from the same family or different architectures.

### Open Question 4
- Question: What is the upper bound on concept complexity that neologism embedding learning can capture?
- Basis in paper: Experiments only demonstrate simple concepts (length constraints, diversity, response quality); authors position these as "proof-of-concept" without addressing scalability to abstract or multi-faceted concepts.
- Why unresolved: No experiments test whether nuanced human concepts (e.g., "safety," "helpfulness without harm") or complex machine concepts can be encoded.
- What evidence would resolve it: Systematic experiments varying concept complexity and measuring success rates, with clear metrics for what constitutes successful neologism learning.

## Limitations

- The abstraction-level matching mechanism lacks direct empirical validation beyond observed behavioral changes
- Bidirectional transfer mechanism for machine-to-human communication remains largely theoretical
- Reliance on frozen models creates constraints if concept requires distributed changes across multiple layers
- Early stopping criterion appears heuristic and may not guarantee optimal neologism quality

## Confidence

**High Confidence**: The basic experimental demonstrations (length control, diversity increase, quality communication) are reproducible and show clear behavioral effects. The mechanism of embedding-only preference learning through APO-up loss is well-grounded in established preference optimization literature.

**Medium Confidence**: The claim that neologisms create a "shared language" between humans and machines is supported by behavioral evidence but lacks deeper validation of semantic alignment. The bidirectional communication framing is conceptually sound but empirically underdeveloped, particularly for machine-to-human concept transfer.

**Low Confidence**: The theoretical framing of neologisms as occupying an ideal "sweet spot" between mechanistic and behavioral interpretability is largely philosophical. The claim that this approach fundamentally solves the communication problem in AI interpretability remains speculative without validation across diverse model architectures and concepts.

## Next Checks

1. **Semantic Alignment Validation**: Beyond measuring behavioral effects (length, diversity), conduct human evaluations to determine whether the neologisms actually capture the intended human concepts. Test whether humans can predict neologism effects and whether different annotators agree on what each neologism means.

2. **Cross-Model Generalizability**: Test whether neologisms learned on one model architecture transfer to others (e.g., does a length neologism learned on Gemma 2B work on Llama or GPT models?). This would validate whether the learned concepts capture stable semantic notions rather than model-specific quirks.

3. **Concept Compositionality and Interference**: Systematically test whether multiple neologisms can be composed without interference and whether neologisms for related concepts (e.g., "length" and "detail") remain distinct or merge into shared representation space.