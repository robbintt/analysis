---
ver: rpa2
title: 'Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language
  Models Across Decoding Temperatures'
arxiv_id: '2601.00942'
source_url: https://arxiv.org/abs/2601.00942
tags:
- sparse
- temperature
- dense
- instruction-tuned
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether sparse Mixture-of-Experts (MoE)
  language models exhibit greater reliability degradation under stochastic decoding
  compared to dense models. Experiments compare OLMoE-7B (sparse base), Mixtral-8x7B
  (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic
  arithmetic tasks across four temperature settings.
---

# Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures

## Quick Facts
- **arXiv ID**: 2601.00942
- **Source URL**: https://arxiv.org/abs/2601.00942
- **Reference count**: 6
- **Key outcome**: Instruction tuning, not architectural sparsity, determines temperature sensitivity in language models.

## Executive Summary
This study investigates whether sparse Mixture-of-Experts (MoE) language models exhibit greater reliability degradation under stochastic decoding compared to dense models. Experiments compare OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic tasks across four temperature settings. Results show that temperature sensitivity is determined by instruction tuning rather than architectural sparsity. The sparse instruction-tuned model (Mixtral) demonstrated stability comparable to the dense instruction-tuned model (Qwen), maintaining consistent accuracy across all temperatures. Only the sparse base model (OLMoE) exhibited systematic degradation as temperature increased, attributed to format non-compliance rather than architectural effects.

## Method Summary
The study compares three language models across deterministic arithmetic tasks using four decoding temperature settings. The models include OLMoE-7B (sparse base model), Mixtral-8x7B (sparse instruction-tuned model), and Qwen2.5-3B (dense instruction-tuned model). Temperature settings range from deterministic (temperature=0) to stochastic decoding environments (temperatures 0.5, 1.0, and 1.5). Performance is measured through accuracy on arithmetic tasks, with particular attention to how each model's reliability changes as randomness in decoding increases.

## Key Results
- Temperature sensitivity is determined by instruction tuning rather than architectural sparsity
- Sparse instruction-tuned model (Mixtral) showed stability comparable to dense instruction-tuned model (Qwen)
- Sparse base model (OLMoE) exhibited systematic degradation as temperature increased due to format non-compliance

## Why This Works (Mechanism)
Instruction tuning appears to provide models with the ability to maintain output format consistency even under stochastic decoding conditions. This training approach likely reinforces the importance of task-specific output structures, enabling models to resist format drift when temperature increases. The mechanism suggests that models trained with explicit format adherence instructions develop internal representations that prioritize task completion requirements over pure probabilistic sampling.

## Foundational Learning
- **Mixture-of-Experts (MoE) architecture**: Why needed - to reduce computational cost by activating only relevant parameters; Quick check - verify which experts activate for different task types
- **Temperature scaling in decoding**: Why needed - controls randomness in token selection; Quick check - observe output diversity at different temperature settings
- **Instruction tuning**: Why needed - adapts models to follow specific formats and task requirements; Quick check - measure format compliance across models
- **Sparse vs dense model comparison**: Why needed - to evaluate architectural trade-offs in reliability; Quick check - compare parameter utilization efficiency

## Architecture Onboarding
- **Component map**: Input -> Tokenizer -> Embedding layer -> (Sparse: Router network + Expert layers | Dense: Dense layers) -> Attention mechanism -> Output layer -> Logits
- **Critical path**: Token generation flow from input through decoding to final output
- **Design tradeoffs**: MoE reduces computational cost but may introduce routing complexity; instruction tuning improves format compliance but requires additional training data
- **Failure signatures**: Format non-compliance in sparse base models at higher temperatures; inconsistent routing in MoE architectures
- **First experiments**: 1) Test temperature sensitivity on non-arithmetic tasks; 2) Isolate MoE component contributions to temperature effects; 3) Compare different sparse architectures with varying instruction tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to three specific models on arithmetic tasks
- Attribution of degradation to format non-compliance requires further validation
- Alternative explanations for temperature sensitivity cannot be ruled out

## Confidence
- High confidence in finding that instruction tuning determines temperature stability
- Medium confidence in architectural claims about sparsity
- Low confidence in attribution of performance degradation solely to format non-compliance

## Next Checks
1. Test temperature stability hypothesis across diverse task types and model families
2. Conduct ablation studies on sparse base model to isolate MoE component contributions
3. Evaluate additional sparse architectures with varying instruction tuning intensities