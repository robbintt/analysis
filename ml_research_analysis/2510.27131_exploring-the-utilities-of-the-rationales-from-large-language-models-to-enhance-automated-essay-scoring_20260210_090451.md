---
ver: rpa2
title: Exploring the Utilities of the Rationales from Large Language Models to Enhance
  Automated Essay Scoring
arxiv_id: '2510.27131'
source_url: https://arxiv.org/abs/2510.27131
tags:
- scoring
- score
- essay
- ensemble
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared essay-based scoring with rationale-based scoring
  for automated essay evaluation using the ASAP Prompt 6 dataset. The findings show
  that essay-based scoring generally outperformed rationale-based scoring, achieving
  a higher QWK of 0.848 versus 0.823.
---

# Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring

## Quick Facts
- arXiv ID: 2510.27131
- Source URL: https://arxiv.org/abs/2510.27131
- Authors: Hong Jiao; Hanna Choi; Haowei Hua
- Reference count: 40
- Primary result: Essay-based scoring outperformed rationale-based scoring (QWK 0.848 vs 0.823), but rationale-based models achieved higher F1-score for underrepresented score 0 category

## Executive Summary
This study investigates whether rationales generated by large language models can enhance automated essay scoring (AES) when combined with traditional essay-based approaches. Using the ASAP Prompt 6 dataset, researchers compared seven encoder models fine-tuned on raw essays versus the same models fine-tuned on GPT-generated rationales. While essay-based scoring achieved higher overall QWK scores, rationale-based approaches demonstrated superior performance for the underrepresented score 0 category. Ensemble modeling combining both approaches further improved accuracy, reaching a QWK of 0.870.

## Method Summary
The researchers employed a comprehensive experimental design using the ASAP Prompt 6 dataset with 1,260 training essays, 180 validation essays, and 360 test essays. They generated rationales using GPT-4.1 and GPT-5, then fine-tuned seven different encoder models (BERT, DeBERTa-v3, DistilBERT, ELECTRA, RoBERTa variants) on both raw essays and the generated rationales. Seven ensemble strategies were tested, including stacking with Ridge regression, elite selection, and tiered approaches. The primary evaluation metric was Quadratic Weighted Kappa (QWK), with F1-scores analyzed for individual score categories to assess minority class performance.

## Key Results
- Essay-based scoring achieved higher overall QWK (0.848) compared to rationale-based scoring (0.823)
- Rationale-based models demonstrated superior F1-score for score 0 (0.5285) compared to essay-based models (0.0-0.1735)
- Stacking ensemble combining essay and rationale-based models achieved the highest QWK of 0.870
- ELECTRA-large fine-tuned on essays achieved the best individual model performance with QWK of 0.8495

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated rationales provide complementary scoring signals that improve minority class detection
- Mechanism: Rationales explicitly articulate evaluation criteria aligned with rubrics, creating abstracted representations that emphasize qualitative judgments over surface features. This abstraction appears to reduce the models' reliance on correlated features (e.g., essay length) that may bias predictions against underrepresented score categories.
- Core assumption: Rationales capture evaluative information distinct from raw text features, particularly for edge cases where rubric-aligned reasoning is critical.
- Evidence anchors:
  - [abstract] "rationale-based scoring led to higher scoring accuracy in terms of F1 scores for score 0 which had less representation due to class imbalance issues"
  - [section: Results] "rationale-based models yielded higher F1 score for score 0, while majority of the essay-based models did not assign score 0 to any essay"
  - [corpus] Related work (Chu et al. 2024) found similar patterns where rationale-augmented models improved analytic trait scoring, though with different encoder architectures

### Mechanism 2
- Claim: Stacking ensemble learning optimally combines essay-based and rationale-based model predictions
- Mechanism: Stacking uses a meta-learner (Ridge regression) to learn optimal linear combination weights from validation performance, automatically discovering which models provide non-redundant information. This outperforms fixed-weight methods because it captures model correlations.
- Core assumption: Essay-based and rationale-based models make different error patterns that a learned combination can exploit.
- Evidence anchors:
  - [abstract] "ensemble modeling combining essay and rationale-based models further improved accuracy, reaching a QWK of 0.870"
  - [section: Methods] "Stacking works best because it learns optimal combinations rather than using fixed rules"
  - [section: Results] "Stacking ensemble model led to the highest QWK (0.8703), the best among all trained models"
  - [corpus] Limited corpus evidence on stacking specifically for AES; most prior work uses simpler averaging or single-model approaches

### Mechanism 3
- Claim: Large encoder models (ELECTRA, DeBERTa-v3) fine-tuned on essays capture scoring-relevant features better than zero-shot LLM scoring
- Mechanism: Fine-tuning adapts pre-trained representations to the specific score distribution and rubric criteria of the target dataset, whereas zero-shot LLMs must generalize without dataset-specific optimization. Encoder-only models are particularly efficient for classification tasks.
- Core assumption: The ASAP Prompt 6 score distribution and rubric have learnable patterns that supervised fine-tuning can capture.
- Evidence anchors:
  - [section: Results] "GPT-4.1 yielded a QWK of 0.6067 while GPT-5 yielded a QWK of 0.7283" compared to fine-tuned ELECTRA-large achieving 0.8495
  - [section: Methods] "DeBERTa-v3 and RoBERTa generally outperform vanilla BERT" for classification
  - [corpus] Consistent with prior ASAP benchmarks showing fine-tuned BERT variants (QWK 0.80-0.85) outperform zero-shot LLMs (QWK 0.60-0.78)

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK)
  - Why needed here: The primary evaluation metric; measures agreement between predicted and human scores while penalizing larger disagreements more heavily than smaller ones. Essential for interpreting all results tables.
  - Quick check question: If a model consistently predicts scores 1 point higher than human raters, would QWK be lower than if it made random errors of the same magnitude? (Yes—systematic bias is penalized differently than random error)

- Concept: Class imbalance effects on F1 scores
  - Why needed here: Score 0 has only 44 essays (2.4%); standard accuracy metrics can appear high while completely missing minority classes. F1 scores per class reveal this failure mode.
  - Quick check question: Why might a model achieve high overall QWK while having F1=0 for score 0? (The model could correctly score 95%+ of essays while never predicting the rare class, and QWK's quadratic weighting may not heavily penalize this specific failure)

- Concept: Encoder-only vs decoder-only transformer architectures
  - Why needed here: The paper uses BERT-family encoder models for fine-tuning but GPT decoder models for rationale generation. Understanding this distinction clarifies why different architectures are used for different tasks.
  - Quick check question: Why might ELECTRA (encoder) be preferred for score classification while GPT (decoder) is used for rationale generation? (Encoders are optimized for producing fixed representations for classification; decoders excel at autoregressive text generation)

## Architecture Onboarding

- Component map:
Raw Essays (ASAP Prompt 6, n=1,800)
    ├─→ [Fine-tuning Path] → 7 Encoder Models (BERT, DeBERTa, DistilBERT, ELECTRA, RoBERTa variants)
    │                              └─→ Essay-based predictions
    │
    └─→ [Rationale Generation Path] → GPT-4.1 / GPT-5 with rubric prompting
                                           └─→ Generated rationales
                                                  └─→ [Fine-tuning] → Same 7 encoder models
                                                                              └─→ Rationale-based predictions

Ensemble Layer:
  21 model predictions (7 essay + 7 GPT-4.1 rationale + 7 GPT-5 rationale)
      └─→ 7 ensemble strategies (Stacking, Elite, Tiered, etc.)
              └─→ Final scores

- Critical path: Rationale generation quality → rationale token length management → encoder fine-tuning on rationales → stacking ensemble combination. The paper identifies token truncation (GPT-4.1 rationales exceeded 512 tokens for 113-189 essays) and over-succinctness (GPT-5 rationales) as quality control points.

- Design tradeoffs:
  - Single best model (ELECTRA-large on essays, QWK 0.8495) vs. full ensemble (Stacking all 21 models, QWK 0.8703): ~2% QWK gain for 3x model complexity
  - GPT-4.1 rationales (longer, more expensive at $24) vs. GPT-5 rationales (shorter, more expensive at $50): GPT-4.1 rationales performed better in ensemble, suggesting content quality outweighs cost
  - Fixed-weight vs. learned ensemble methods: Stacking adds meta-learner complexity but achieves best results

- Failure signatures:
  - Zero F1 for score 0: Indicates model never predicts minority class; check if ensemble includes rationale-based models which mitigate this
  - Large QWK gap between validation and test: Suggests overfitting in stacking meta-learner
  - Rationale token overflow: Check length distributions before fine-tuning; truncation may lose critical rubric-aligned reasoning

- First 3 experiments:
  1. Baseline replication: Fine-tune ELECTRA-large on essays only with 80/20 split; verify QWK ~0.849. This establishes the foundation before adding complexity.
  2. Rationale quality ablation: Generate rationales with GPT-4.1 at temperature 0.2, fine-tune DeBERTa-v3-large on rationales, compare F1-score-0 against essay-only baseline. Expect ~0.5 vs ~0.0 improvement for minority class.
  3. Minimal ensemble test: Combine only top-3 essay models (ELECTRA-large, DeBERTa-v3-large, RoBERTa-base) with top-3 GPT-4.1 rationale models using stacking. Target QWK >0.86 with reduced computational cost vs. full 21-model ensemble.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does explicit data augmentation for underrepresented score categories (specifically scores 0 and 1) improve the performance of rationale-based and essay-based automated scoring models?
- Basis in paper: [explicit] The authors state, "Future exploration can explicitly implement data augmentation methods to improve scoring accuracy with LMs based on essays and rationales respectively," noting the current study did not address class imbalance.
- Why unresolved: The current study identified class imbalance as a challenge, particularly for the score 0 category, but did not test data augmentation techniques to mitigate it.
- What evidence would resolve it: A comparison of QWK and F1 scores for models trained on the original dataset versus a dataset augmented using techniques like those cited by Jiao, Lnu, & Zhai (2024).

### Open Question 2
- Question: Can high-quality sentence embeddings from models specifically optimized for semantic similarity (e.g., Sentence-Transformers) outperform the general-purpose embeddings used in this study?
- Basis in paper: [explicit] The authors note that ELECTRA and DeBERTa "could be mediocre for embeddings" and suggest that "high-quality sentence embeddings from Sentence-Transformers families such as all-MiniLM and E5 can be explored in future studies."
- Why unresolved: The supervised machine learning models trained on the current embeddings performed poorly (QWK ~0.72–0.78) compared to the fine-tuned language models.
- What evidence would resolve it: Training X-gradient boosting or light gradient boosting models on embeddings from all-MiniLM or E5 and comparing their QWK scores against the current baseline.

### Open Question 3
- Question: Does removing the succinctness constraint in the prompt allow GPT-5 to generate higher-quality rationales that improve scoring accuracy?
- Basis in paper: [explicit] The authors hypothesize that GPT-5 rationale models underperformed possibly because the "current prompting to GPT-5 for rationale generation might be too strict," and state they will "experiment with other options that GPT-5 rationale can be more elaborated."
- Why unresolved: The added instructions to keep GPT-5 rationales under 512 tokens may have resulted in a loss of critical information, degrading the rationale-based model's performance.
- What evidence would resolve it: Generating new rationales with GPT-5 without the strict token/succinctness limit and comparing the QWK of models fine-tuned on these longer rationales.

### Open Question 4
- Question: Is the complementary benefit of rationale-based scoring consistent across different ASAP prompts or essay types (e.g., source-independent vs. passage-dependent)?
- Basis in paper: [inferred] The study focuses exclusively on ASAP Prompt 6, which is a passage-dependent task, and does not test the generalizability of the ensemble findings to other prompts or datasets.
- Why unresolved: It is unclear if the utility of rationales for improving accuracy in minority score categories is specific to the characteristics of Prompt 6 or a generalizable phenomenon.
- What evidence would resolve it: Replicating the ensemble methodology on other prompts in the ASAP dataset (e.g., Prompt 1 or 2) and evaluating if the combination of essay and rationale models yields similar QWK improvements.

## Limitations

- The study uses a single dataset (ASAP Prompt 6) with only 1,800 essays and severe class imbalance, limiting generalizability to other AES applications.
- GPT-4.1 rationales exceeded 512 tokens for 113-189 essays, suggesting potential information loss that wasn't analyzed for its impact on scoring accuracy.
- The stacking ensemble's meta-learner was trained on only 180 validation essays, raising concerns about overfitting not quantified in the paper.
- Cost analysis ($24 for GPT-4.1 vs $50 for GPT-5) presents the trade-off but doesn't evaluate cost-effectiveness relative to performance gains.
- The study doesn't explore alternative ensemble strategies like boosting or Bayesian model averaging that might achieve similar or better results with fewer models.

## Confidence

- **High Confidence**: Essay-based scoring outperforming rationale-based scoring for overall QWK (0.848 vs 0.823) - supported by consistent results across all seven encoder models and the baseline ELECTRA-large performance matching prior ASAP benchmarks.
- **Medium Confidence**: Rationale-based models' superior F1-score for score 0 - while the pattern is clear (F1 0.0 vs 0.5285), the small sample size (44 essays) means results could be sensitive to individual essay characteristics.
- **Medium Confidence**: Stacking ensemble superiority - the 0.8703 QWK is statistically better than individual models, but the meta-learner's reliance on a small validation set (180 essays) introduces potential overfitting risks not quantified in the paper.

## Next Checks

1. **Cross-dataset validation**: Test the best ensemble model (Stacking with QWK 0.870) on a different AES dataset (e.g., Kaggle ASAP full dataset or a different prompt) to assess generalizability beyond Prompt 6.

2. **Stacking overfitting analysis**: Compare stacking performance on validation (n=180) versus test sets across all ensemble strategies to quantify overfitting, and test simpler ensembles (top-3 models only) to establish performance bounds.

3. **Rationale quality ablation**: Systematically vary GPT-4.1 temperature (0.0, 0.2, 0.5, 1.0) and analyze the relationship between rationale coherence scores, token lengths, and resulting F1-scores for minority classes to identify optimal generation parameters.