---
ver: rpa2
title: 'ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning
  of Large Language Models'
arxiv_id: '2511.02757'
source_url: https://arxiv.org/abs/2511.02757
tags:
- conmezo
- mezo
- optimization
- gradient
- momentum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConMeZO introduces a cone-based sampling strategy to accelerate
  zeroth-order (ZO) optimization for large language model (LLM) finetuning. It improves
  upon MeZO by sampling perturbation directions within a cone centered on a momentum
  estimate, thereby reducing gradient variance and enhancing convergence speed.
---

# ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models

## Quick Facts
- arXiv ID: 2511.02757
- Source URL: https://arxiv.org/abs/2511.02757
- Authors: Lejs Deen Behric; Liang Zhang; Bingcong Li; Kiran Koshy Thekumparampil
- Reference count: 40
- Primary result: ConMeZO achieves up to 2× speedup in wall-clock time during LLM finetuning while maintaining MeZO's low memory footprint

## Executive Summary
ConMeZO introduces a cone-based sampling strategy to accelerate zeroth-order (ZO) optimization for large language model (LLM) finetuning. It improves upon MeZO by sampling perturbation directions within a cone centered on a momentum estimate, thereby reducing gradient variance and enhancing convergence speed. The method maintains MeZO's low memory footprint while achieving up to 2× speedup in wall-clock time during LLM finetuning tasks. On RoBERTa-large, ConMeZO outperforms MeZO and other baselines across multiple benchmarks. For OPT-1.3B and OPT-13B, it consistently achieves higher accuracy/F1 scores, demonstrating scalability to billion-parameter models.

## Method Summary
ConMeZO modifies the MeZO algorithm by constraining the random perturbation directions to lie within a cone around a momentum vector. The search direction is constructed as $z_t = \sqrt{d}(\cos(\theta)\hat{m}_t + \sin(\theta)u_t)$, where $\hat{m}_t$ is the normalized momentum and $u_t$ is random noise. This geometric constraint forces the search direction to lie within an angle $\theta$ of the momentum, reducing variance compared to standard ZO optimization that samples uniformly from a high-dimensional sphere. The method maintains MeZO's core mechanics while adding a momentum buffer and vectorized implementation for improved wall-clock speed.

## Key Results
- Achieves up to 2× speedup in wall-clock time during LLM finetuning compared to MeZO
- On RoBERTa-large, ConMeZO outperforms MeZO and other baselines across multiple benchmarks
- For OPT-1.3B and OPT-13B, ConMeZO consistently achieves higher accuracy/F1 scores
- Maintains MeZO's low memory footprint while providing significant performance improvements

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction via Cone Sampling
Standard ZO optimization samples uniformly from a high-dimensional sphere, which is inefficient because random vectors are nearly orthogonal to the true gradient. ConMeZO constructs the search direction $z_t = \sqrt{d}(\cos(\theta)\hat{m}_t + \sin(\theta)u_t)$, where $\hat{m}_t$ is the normalized momentum and $u_t$ is random noise. This geometric constraint forces the search direction to lie within an angle $\theta$ of the momentum. The core assumption is that the accumulated momentum vector $\hat{m}_t$ maintains a positive correlation with the true gradient $\nabla f(x_t)$. If the momentum vector becomes orthogonal or misaligned with the true gradient, the cone may systematically exclude the optimal descent direction, causing convergence to stall.

### Mechanism 2: Dimension-Dependent Speedup
Theoretical analysis shows that when momentum aligns with the gradient, the per-step optimization progress scales effectively as $O(1)$ rather than $O(1/d)$. The expected decrease includes a term proportional to $d \cos^2(\theta) \cos^2(\rho_t) \|\nabla f(x_t)\|^2$. If the alignment angle $\rho_t$ is small (good momentum) and $\theta$ is small, the $d$-dependent term dominates, effectively canceling the $1/d$ variance penalty of standard ZO. The core assumption is that the loss landscape is $\ell$-smooth, and the optimizer maintains a "momentum warm-up" phase to ensure $\rho_t$ shrinks effectively. If the cone angle $\theta$ is set too wide ($\approx \pi/2$), the method degrades to standard MeZO. If set too narrow ($\approx 0$) with poor initial momentum, the optimizer may fail to descend.

### Mechanism 3: Vectorized Memory-Time Trade-off
Utilizing a dedicated momentum buffer allows for vectorized operations that reduce wall-clock time despite a moderate increase in memory usage. Standard MeZO implementations often perturb parameters one at a time to save memory, incurring Python loop overhead. ConMeZO maintains a flattened momentum buffer, enabling fused in-place perturbations and updates. This removes the need to regenerate random tensors repeatedly inside loops. The core assumption is that the hardware has sufficient VRAM to accommodate one extra parameter-sized buffer (approx. 50-100% increase over MeZO, but still << Adam). On extremely memory-constrained devices (e.g., edge devices with barely enough memory for the model), the additional buffer causes OOM errors.

## Foundational Learning

- **Concept: Zeroth-Order Gradient Estimator (ZOGE)**
  - **Why needed here:** This is the fundamental operation ConMeZO modifies. It estimates gradients using finite differences between two forward passes ($\frac{f(x+\epsilon z) - f(x-\epsilon z)}{2\epsilon}$) without backpropagation.
  - **Quick check question:** Why does the variance of a standard random ZOGE scale linearly with model dimension $d$?

- **Concept: Momentum (Exponential Moving Average)**
  - **Why needed here:** In ConMeZO, momentum is not just a smoother; it acts as the "axis" for the search cone.
  - **Quick check question:** If the momentum parameter $\beta$ is set too low (e.g., 0.1), how does it affect the geometry of the cone sampling? (Hint: Does the cone direction persist?)

- **Concept: Concentration of Measure in High Dimensions**
  - **Why needed here:** Justifies why "cone sampling" is mathematically sound. In high dimensions, random vectors are almost always nearly orthogonal to any specific direction, so restricting sampling to a cone around a "good" direction statistically excludes "bad" directions.
  - **Quick check question:** In a 1-billion parameter space, what is the expected cosine similarity between a random perturbation and the true gradient? (Answer: $\approx 0$).

## Architecture Onboarding

- **Component map:**
  - Flattened Parameter Buffer ($x$) -> Momentum Buffer ($m$) -> Forward Oracle -> Cone Sampler -> Parameter Update

- **Critical path:**
  1. **Sample:** Generate $u_t$ and combine with normalized momentum $\hat{m}_t$ and angle $\theta$ to get $z_t$.
  2. **Perturb & Evaluate:** Run forward passes to get $f(x + \lambda z_t)$ and $f(x - \lambda z_t)$.
  3. **Estimate:** Compute ZO gradient $g_t$.
  4. **Update:** $x \leftarrow x - \eta g_t$ and $m \leftarrow \beta m + (1-\beta)g_t$.
  5. **Warm-up:** Apply the schedule for $\beta$ during the first 2k steps.

- **Design tradeoffs:**
  - **Cone Angle $\theta$:** Small $\theta$ (e.g., 1.2) = aggressive exploitation, fast convergence if aligned, high risk of stuck. Large $\theta$ (e.g., 1.5) = robust but slower.
  - **Vectorization:** Gaining speed at the cost of ~1GB extra VRAM for a 1B model.
  - **Warm-up:** Essential to prevent early noisy gradients from permanently biasing the momentum cone.

- **Failure signatures:**
  - **Immediate Divergence:** Learning rate $\eta$ is too high for the ZO scalar differences.
  - **Flat Loss (Stagnation):** $\theta$ is too narrow and momentum initialized poorly, locking updates into a barren plateau.
  - **OOM:** The momentum buffer exceeds available VRAM (fallback to standard MeZO).

- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Optimize a 1000-dim quadratic function. Compare ConMeZO vs. MeZO step-count to reach tolerance (should see ~2x speedup).
  2. **Scale Test (RoBERTa):** Finetune on SST-2. Sweep $\theta \in [1.2, 1.5]$ and plot convergence curves to verify the trade-off between early speed and final accuracy.
  3. **Memory Profile (OPT-1.3B):** Measure peak VRAM and s/iter. Verify that memory overhead is indeed ~30-50% over MeZO baseline and wall-clock speed is maintained or improved.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's robustness to highly non-convex landscapes or sudden gradient shifts is not fully explored
- The cone sampling strategy may struggle in early training phases where momentum is poorly aligned with the gradient
- The sensitivity analysis for the cone angle hyperparameter θ is limited, with optimal values potentially varying significantly across different model architectures and tasks

## Confidence

- **High Confidence:** The empirical speedup claims (2× faster convergence) and memory efficiency improvements are well-supported by the experimental results across multiple benchmarks and model sizes.
- **Medium Confidence:** The theoretical convergence guarantees match MeZO's worst-case bounds, but the claimed O(d) speedup in practice depends heavily on the assumption of good momentum alignment, which may not always hold.
- **Low Confidence:** The sensitivity analysis for the cone angle hyperparameter θ is limited. While 1.2-1.5 radians is suggested, optimal values may vary significantly across different model architectures and tasks.

## Next Checks

1. **Early Training Dynamics:** Run ConMeZO on a curriculum learning setup where the task difficulty increases over time. Monitor how quickly the momentum alignment recovers after sudden gradient changes.
2. **Adversarial Perturbation Test:** Intentionally corrupt the momentum buffer at a random iteration and measure how long it takes for the optimizer to recover versus standard MeZO.
3. **Memory-Constrained Deployment:** Test ConMeZO on a 2GB VRAM device (e.g., Jetson Nano) with OPT-125M. Verify the claimed 30-50% memory overhead and measure the impact on convergence when the momentum buffer must be offloaded to CPU.