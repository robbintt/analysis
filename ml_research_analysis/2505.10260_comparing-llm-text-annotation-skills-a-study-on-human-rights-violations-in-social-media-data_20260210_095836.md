---
ver: rpa2
title: 'Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in
  Social Media Data'
arxiv_id: '2505.10260'
source_url: https://arxiv.org/abs/2505.10260
tags:
- performance
- human
- language
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of state-of-the-art large
  language models (LLMs) for annotating multilingual social media posts to detect
  references to human rights violations. Using a dataset of Russian and Ukrainian
  posts, the research compares zero-shot and few-shot prompting in both English and
  Russian across GPT-3.5, GPT-4, LLaMA3, Mistral-7B, and Claude-2.
---

# Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data

## Quick Facts
- arXiv ID: 2505.10260
- Source URL: https://arxiv.org/abs/2505.10260
- Authors: Poli Apollinaire Nemkova; Solomon Ubani; Mark V. Albert
- Reference count: 7
- Key outcome: GPT-4.0 achieved highest performance (F1=0.84, accuracy=0.82) for detecting human rights violations in multilingual social media posts, with linguistic alignment and few-shot prompting significantly improving open-source model performance.

## Executive Summary
This study evaluates state-of-the-art large language models for annotating multilingual social media posts to detect human rights violations. Using 1,000 Russian and Ukrainian Telegram posts, the research systematically compares zero-shot and few-shot prompting across five models (GPT-3.5, GPT-4, LLaMA3, Mistral-7B, Claude-2) in both English and Russian. GPT-4.0 emerged as the top performer with 0.84 F1 score, particularly excelling in zero-shot English prompting, while open-source models showed substantial improvements with few-shot examples and language-aligned prompts. The study provides actionable guidance for deploying LLMs in sensitive, multilingual domains like human rights monitoring, highlighting critical tradeoffs between model capabilities, prompting strategies, and operational costs.

## Method Summary
The researchers curated a dataset of 1,000 Telegram posts (966 Russian, 34 Ukrainian) manually annotated for human rights violations through double-annotation with adjudicated labels. Five LLMs were evaluated: GPT-3.5, GPT-4, Claude-2 (via API) and LLaMA-3.2-1B, Mistral-7B (local inference). A 2×2 experimental design tested zero-shot vs. few-shot prompting in English vs. Russian across all models. Performance was measured using precision, recall, F1-score, and accuracy against human-annotated labels, with additional analysis on cases where human annotators disagreed to understand model behavior on ambiguous content.

## Key Results
- GPT-4.0 achieved highest overall performance (F1=0.84, accuracy=0.82) with zero-shot English prompting
- Models performed significantly better when prompts aligned with dataset language, especially open-source models
- Few-shot prompting provided substantial gains for open-source models (LLaMA3, Mistral-7B) but minimal improvement for closed-source models like GPT-4
- Models showed correlated difficulty with humans on ambiguous cases, suggesting shared sensitivity to contextual uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning prompt language with data language improves classification performance, particularly for open-source models
- Mechanism: Russian prompts for Russian/Ukrainian social media posts reduce cross-lingual reasoning overhead, allowing models to process semantic content directly rather than implicitly translating task instructions
- Core assumption: The model's multilingual representations are sufficiently robust that instruction-following in the target language activates more relevant semantic pathways than English-mediated reasoning
- Evidence anchors:
  - [abstract] "Models performed better when prompts were aligned with the dataset language, and open-source models improved significantly with few-shot examples"
  - [section 4] "Russian prompts generally outperformed English prompts across models, particularly with GPT-3.5 and GPT-4"
  - [corpus] Limited direct corpus support; related work on LLM annotation tasks does not specifically address linguistic alignment effects
- Break condition: When target language is low-resource in the model's training distribution, linguistic alignment may degrade performance relative to English prompting

### Mechanism 2
- Claim: Few-shot prompting provides larger marginal gains for open-source models than for closed-source models
- Mechanism: In-context examples establish task-specific decision boundaries that smaller or less-aligned models cannot derive from instructions alone. GPT-4's superior zero-shot suggests stronger instruction-following capabilities, while LLaMA-3 and Mistral-7B require examples to approximate similar task calibration
- Core assumption: The few-shot examples are representative of the test distribution and do not introduce distributional shift or annotation artifacts
- Evidence anchors:
  - [abstract] "open-source models improved significantly with few-shot examples"
  - [section 5.1] "Few-Shot Prompting Boosts Open-Source Model Performance: Open-source models showed substantial improvements with few-shot prompting compared to zero-shot prompting. In contrast, closed-source models like GPT-4.0 and GPT-3.5 displayed strong zero-shot capabilities, with marginal gains in few-shot settings"
  - [corpus] "Text Annotation via Inductive Coding" (arXiv:2512.00046) similarly finds LLMs require careful prompting for qualitative tasks, though does not directly compare open vs. closed-source few-shot gains
- Break condition: Few-shot examples that are noisy, unrepresentative, or contradictory can degrade performance through negative transfer

### Mechanism 3
- Claim: Models and humans exhibit correlated difficulty on ambiguous cases, suggesting shared sensitivity to contextual uncertainty rather than purely model-specific failure modes
- Mechanism: Posts with indirect or implicit references to human rights violations lack explicit lexical markers, requiring contextual inference. When annotators disagree (184/1000 cases), models also show precision drops, indicating ambiguity is a shared bottleneck
- Core assumption: Human disagreement is a meaningful signal of inherent task difficulty rather than annotator error or fatigue
- Evidence anchors:
  - [section 3.1] "Inter-annotator agreement was measured using Cohen's Kappa with value 0.63, which indicated substantial agreement between the two primary annotators"
  - [section 5.1, ablation study] "Both LLaMA-3 and GPT-4.0 exhibit significant drops in Precision on these cases, suggesting an increased likelihood of false positives when ambiguity is present"
  - [corpus] "Self-reflection in Automated Qualitative Coding" (arXiv:2601.09905) notes that zero/few-shot classifiers produce errors on complex cases, aligning with ambiguity-sensitivity findings
- Break condition: If human disagreement reflects systematic bias rather than ambiguity, model-human correlation on these cases may not indicate shared difficulty

## Foundational Learning

- Concept: Zero-shot vs. Few-shot prompting
  - Why needed here: The paper's central experimental manipulation; understanding when examples help vs. hurt is critical for deployment decisions
  - Quick check question: For a new multilingual classification task with GPT-4, would you start with zero-shot or few-shot, and why?

- Concept: Cohen's Kappa and inter-annotator agreement
  - Why needed here: The paper uses Kappa = 0.63 to contextualize task difficulty; understanding this metric helps interpret whether 84% F1 is strong or weak relative to human ceiling
  - Quick check question: If two annotators agree on 90% of labels but Kappa is 0.40, what does this indicate about class balance and agreement?

- Concept: Precision-recall tradeoffs in imbalanced classification
  - Why needed here: The dataset has 517 positive / 483 negative samples; the paper reports models optimizing recall (GPT-3.5 at 0.92) at precision cost, which matters for human rights monitoring where false negatives carry high cost
  - Quick check question: In a human rights monitoring system, would you prioritize higher recall or higher precision, and what are the operational consequences of each choice?

## Architecture Onboarding

- Component map: Dataset (1000 Telegram posts) -> Human annotation (double-annotation + adjudication) -> LLM inference (5 models × 4 prompt conditions) -> Evaluation (precision, recall, F1, accuracy)

- Critical path: 1) Dataset sampling and human annotation with Cohen's Kappa validation 2) Prompt template design for each language and shot condition 3) Model inference with fixed seeds 4) Metric computation and ablation on agreement/disagreement subsets

- Design tradeoffs:
  - GPT-4: Highest performance (F1=0.84) but highest cost; best for high-stakes applications
  - Open-source models: Lower baseline but viable with few-shot; require additional validation and lower precision suggests manual review step
  - Russian prompts: Generally better performance but require Russian-language prompt engineering capability

- Failure signatures:
  - LLaMA-3 zero-shot Russian: F1=0.26, accuracy=0.50 (near-random) — indicates linguistic alignment insufficient without few-shot for smaller models
  - Claude-2: Moderate F1 (0.58) with low recall (0.45) — suggests conservative false-negative-avoidance not achieved
  - High recall / low precision configurations (e.g., LLaMA-3 few-shot Russian: recall=0.99, precision=0.52) — over-flagging requires downstream filtering

- First 3 experiments:
  1. Replicate GPT-4 zero-shot English on a held-out subset to validate F1≈0.84 and establish baseline before prompt iteration
  2. Ablate prompt language: run identical few-shot prompts in English vs. Russian on LLaMA-3 to quantify linguistic alignment effect size
  3. Error analysis on false positives: manually review 20 cases where models flagged HRV but humans did not to identify systematic over-activation patterns (e.g., military activity mentions without explicit violation references)

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (1,000 posts) limits generalizability and statistical power
- Limited language coverage (primarily Russian with 34 Ukrainian posts) restricts multilingual findings
- Binary classification may oversimplify the nuanced nature of human rights violations
- Study does not explore multi-label or severity-level annotation that could be more informative

## Confidence

- **High Confidence**: GPT-4's superior performance (F1=0.84) with zero-shot English prompting is well-established across multiple evaluations
- **Medium Confidence**: Specific performance metrics for individual models under different prompting conditions may be sensitive to random initialization
- **Low Confidence**: Generalizability to other languages, content types, or human rights violation categories remains untested

## Next Checks

1. **Dataset Expansion Validation**: Replicate the main experiments on a 5× larger dataset (5,000 posts) from the same source to verify that model performance rankings and linguistic alignment effects remain stable with increased sample size

2. **Cross-Domain Transfer Test**: Apply the best-performing prompting strategy (GPT-4 zero-shot English) to a different human rights corpus (e.g., Arabic social media posts about refugee situations) to assess domain transferability of the 0.84 F1 benchmark

3. **Cost-Performance Tradeoff Analysis**: For each model, calculate the total cost per 1,000 annotations at their best-performing setting, then determine the marginal cost per F1 point improvement to provide concrete operational guidance for resource-constrained human rights organizations