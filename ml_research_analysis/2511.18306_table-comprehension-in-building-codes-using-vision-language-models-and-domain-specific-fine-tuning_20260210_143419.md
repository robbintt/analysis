---
ver: rpa2
title: Table Comprehension in Building Codes using Vision Language Models and Domain-Specific
  Fine-Tuning
arxiv_id: '2511.18306'
source_url: https://arxiv.org/abs/2511.18306
tags:
- input
- arxiv
- vlms
- qwen2
- building
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of extracting and interpreting
  tabular data from building codes, which is crucial for automated question-answering
  systems in construction and engineering. It proposes two methods: a direct input
  method where images of tables are fed directly to Vision Language Models (VLMs),
  and an indirect input method where images are first converted to LaTeX code before
  processing.'
---

# Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning

## Quick Facts
- **arXiv ID**: 2511.18306
- **Source URL**: https://arxiv.org/abs/2511.18306
- **Reference count**: 40
- **Key outcome**: Direct image input to VLMs outperforms LaTeX conversion for table comprehension; LoRA fine-tuning on 400 samples yields >100% relative accuracy gains for smaller models.

## Executive Summary
This paper addresses the challenge of extracting and interpreting tabular data from building codes, crucial for automated question-answering systems in construction and engineering. The authors propose two methods for table comprehension: direct image input to Vision Language Models (VLMs) and indirect input via LaTeX conversion, with the former consistently outperforming the latter. To further enhance VLM performance, they apply Low-Rank Adaptation (LoRA) fine-tuning on a domain-specific dataset, achieving significant accuracy improvements. The study demonstrates that parameter-efficient fine-tuning can substantially enhance VLMs' ability to understand complex structured data in specialized domains like building codes.

## Method Summary
The study compares direct image input versus LaTeX conversion for table comprehension in building codes. Researchers extracted table images from NBCC 2020 at 300 DPI, generated 500 QA pairs using InternVL2_5-8B, and split them into 400 training and 100 test samples. They fine-tuned VLMs (Qwen2.5-VL, Llama-3.2) using LoRA with rank=16, alpha=32, targeting visual and attention layers, and evaluated results using InternVL3-8B as a judge. The evaluation used 4-bit quantization and measured accuracy through automated comparison of model outputs against ground truth.

## Key Results
- Direct image input method consistently outperformed LaTeX conversion across all tested models.
- LoRA fine-tuning on 400 samples achieved relative accuracy gains exceeding 100% for smaller models like Qwen2.5-VL-3B-Instruct.
- Models with lower pre-trained accuracy showed more significant improvement from fine-tuning.
- The correction-to-regression ratio during fine-tuning indicated effective parameter updates without significant forgetting.

## Why This Works (Mechanism)

### Mechanism 1
Direct visual processing of tables yields higher comprehension accuracy than converting tables to intermediate structured text (LaTeX). VLMs rely on spatial visual features—such as cell alignment, borders, and relative positioning—to parse table semantics. Converting images to LaTeX abstracts this spatial data into syntax that requires multi-step reasoning, introducing cognitive load and potential conversion errors. The VLM's vision encoder is more adept at processing 2D spatial layouts natively than its language decoder is at reasoning over nested markup syntax. Break condition: If tables have extremely poor visual resolution or handwritten notes, a clean text/markdown conversion might be superior.

### Mechanism 2
Low-Rank Adaptation (LoRA) effectively aligns general-purpose VLMs to domain-specific tabular data by updating a small subset of weights. By freezing pre-trained weights and injecting trainable rank-decomposition matrices into specific layers (attention and projection layers), the model retains its general reasoning capabilities while "nudging" internal representations to recognize domain-specific patterns (e.g., building code terminology). The knowledge required for table comprehension is linearly projectable or adaptable via low-rank updates without catastrophic forgetting. Break condition: If the base model lacks sufficient foundational reasoning or vision capabilities, LoRA may fail to bridge the gap.

### Mechanism 3
Targeting visual projection and attention layers during fine-tuning is critical for cross-modal alignment in table QA tasks. The `visual_proj` layer maps image embeddings to the language space. Fine-tuning this, along with attention mechanisms (`q_proj`, `k_proj`), allows the model to better "pinpoint" specific regions of a table when processing a query, improving the linkage between visual cells and textual answers. The bottleneck in pre-trained models is the alignment of visual features to language tokens, not the raw feature extraction itself. Break condition: Aggressive fine-tuning with a small dataset (400 samples) risks overfitting to specific table layouts, leading to regression on unseen formats.

## Foundational Learning

- **Concept: Vision Language Models (VLMs)**
  - Why needed here: You must understand that these models are not just OCR engines; they map images and text into a shared embedding space to reason about both simultaneously.
  - Quick check question: Does the model process the image as a sequence of pixels or as pre-extracted text tokens? (Answer: It uses a vision encoder to create visual embeddings, combined with text embeddings).

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: This is the optimization technique used to achieve the results. You need to know why we freeze weights and add matrices (A and B) rather than retraining the whole network.
  - Quick check question: In the equation $W' = W + \Delta W$, what does $\Delta W$ represent in the context of LoRA? (Answer: The product of low-rank matrices $A \cdot B$ that are trained while $W$ is frozen).

- **Concept: Hallucination in VLMs**
  - Why needed here: The paper notes that VLMs struggle with "absurd question answering" and "object hallucination." Understanding this helps diagnose why pre-trained models failed on complex tables.
  - Quick check question: Why might a VLM invent a value not present in a table image? (Answer: It may rely on statistical correlations from training data rather than visual evidence, especially when visual cues are ambiguous).

## Architecture Onboarding

- **Component map**: PDF → `pdflumber`/`PyMuPDF` (300 DPI Image) → InternVL (QA Pair Generator) → Pre-trained VLM (Qwen2-VL) → LoRA config (Rank=16, Alpha=32) → InternVL3-8B (Evaluator).

- **Critical path**:
  1. **Dataset Curation**: The quality of the 400 training triplets (Image, Question, Answer) is the primary driver of success.
  2. **Module Selection**: You must correctly identify and target the `visual_proj` and attention modules in the PEFT config.
  3. **Evaluation**: Using an LLM-as-a-judge (InternVL3) is necessary because simple string matching fails on semantic equivalence (e.g., "2500 m²" vs "2500").

- **Design tradeoffs**:
  - **Direct vs. Indirect Input**: Direct is more accurate but requires processing the full image; Indirect (LaTeX) is text-only but brittle due to conversion errors.
  - **Model Size vs. Gain**: Smaller models (3B) showed massive *relative* gains but may still trail larger models (11B) in absolute accuracy post-fine-tuning.
  - **Correction vs. Regression**: Fine-tuning improves domain performance but introduces a small risk of "forgetting" general knowledge (regression).

- **Failure signatures**:
  - **Low Indirect Accuracy**: If LaTeX accuracy is <20%, suspect the GPT-4.1 conversion step introduced syntax errors or misaligned columns.
  - **High Regression Ratio**: If fine-tuning makes the model worse on general tasks, the LoRA rank (16) or learning rate (2e-5) may be too high.

- **First 3 experiments**:
  1. **Baseline Probe**: Run the pre-trained Qwen2.5-VL-3B on 10 tables using the Direct Input method to establish a baseline error rate.
  2. **Format Ablation**: Compare Direct Image input vs. Markdown/LaTeX input on the same 10 tables to verify the paper's finding that visual cues > text syntax.
  3. **Adapter Overfit Test**: Fine-tune with LoRA on the 400-sample set for 5 epochs vs. 20 epochs to check if the "correction-to-regression" ratio stabilizes or worsens.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does integrating domain-specific fine-tuned VLMs into a complete Multimodal Retrieval-Augmented Generation (MRAG) framework affect end-to-end performance on building code queries? The conclusion states that future work includes developing "a complete end-to-end MRAG framework incorporating the fine-tuned model at its core." This is unresolved because the current study isolates the table comprehension component and does not evaluate the model's performance when nested within a larger retrieval and generation pipeline. Evidence would require evaluation results from a full MRAG system tested on a curated set of building code queries.

- **Open Question 2**: To what extent does increasing the training dataset size beyond 400 samples improve the accuracy and stability of fine-tuning, particularly for models with high pre-trained baselines? The authors identify the dataset size as a limitation: "We utilized 400 samples for fine-tuning, which represents one of the limitations of this study." This is unresolved because it is unclear if the observed "correction-to-regression" ratios would improve with more data, or if the relative gains for smaller models would plateau. Evidence would require a comparative study scaling the training set and analyzing resulting accuracy deltas and stability ratios.

- **Open Question 3**: How does VLM performance vary across specific categories of tables (e.g., span tables vs. fire-resistance ratings) within the NBCC, and can category-specific fine-tuning yield better results? The paper notes, "NBCC contains diverse types of tables serving different purposes," and suggests "classifying these tables into categories and performing detailed analyses for each type." This is unresolved because the reported accuracy aggregates performance across all table types, potentially masking significant discrepancies in how well models handle distinct structural layouts or content densities. Evidence would require a stratified error analysis breaking down accuracy scores by table type, followed by experiments using specialized adapters for specific table categories.

## Limitations

- **Data source and quality**: The paper uses proprietary NBCC 2020 building codes, making full dataset replication impossible without access. The generation of 500 QA pairs using InternVL2_5-8B introduces potential bias based on the specific prompt used, which is not fully disclosed.
- **Evaluation method reliability**: Using InternVL3-8B as an automated judge for answer correctness may introduce evaluation bias, particularly for semantic equivalence cases where minor formatting differences exist. This could inflate or deflate accuracy metrics.
- **Fine-tuning dataset size**: The 400-sample training set is relatively small for domain adaptation, raising concerns about overfitting and generalizability to unseen table formats or building codes.

## Confidence

- **High Confidence**: The claim that direct image input outperforms LaTeX conversion is well-supported by empirical results across all tested models. The mechanism explaining why visual cues provide better information than text syntax is logically sound.
- **Medium Confidence**: The LoRA fine-tuning results showing >100% relative accuracy gains are impressive but may be partially attributable to the smaller base models having more room for improvement. The specific layer targeting (visual_proj, attention layers) is theoretically justified but not exhaustively validated across alternative configurations.
- **Low Confidence**: The claim that fine-tuning improves cross-domain generalization is weakly supported, as the paper doesn't test on tables from different building codes or domains.

## Next Checks

1. **Format Ablation Study**: Replicate the direct vs. indirect input comparison on 10 held-out tables from a different building code or regulatory document to verify the visual advantage generalizes beyond NBCC 2020.

2. **Layer Targeting Sensitivity**: Conduct an ablation study varying which layers are fine-tuned (e.g., only visual_proj vs. full LoRA config) to determine if the specific targeting strategy is critical or if simpler approaches achieve similar gains.

3. **Cross-Domain Transfer Test**: Evaluate the fine-tuned models on table comprehension tasks from completely different domains (e.g., financial tables, scientific papers) to assess whether the building code adaptation improves or degrades general table understanding capabilities.