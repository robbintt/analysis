---
ver: rpa2
title: Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems
arxiv_id: '2508.10806'
source_url: https://arxiv.org/abs/2508.10806
tags:
- user
- explanations
- users
- systems
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the accessibility gap in eXplainable AI (XAI)
  for users with vision impairments through a two-part study. First, a literature
  review of 79 studies reveals that evaluations of XAI techniques rarely include disabled
  users, with most explanations relying on visual formats.
---

# Who Benefits from AI Explanations? Towards Accessible and Interpreable Systems

## Quick Facts
- arXiv ID: 2508.10806
- Source URL: https://arxiv.org/abs/2508.10806
- Reference count: 40
- One-line primary result: Visual-only XAI explanations exclude users with vision impairments; simplified, multimodal explanations improve accessibility.

## Executive Summary
This paper addresses the accessibility gap in eXplainable AI (XAI) for users with vision impairments through a two-part study. First, a literature review of 79 studies reveals that evaluations of XAI techniques rarely include disabled users, with most explanations relying on visual formats. Second, the authors present a four-part methodological proof of concept: AI system categorization, persona definition, prototype design with XAI techniques, and expert/user assessment. The prototype was evaluated with accessibility experts from the Canadian National Institute for the Blind and users with lived experience of sight loss. Preliminary findings suggest that simplified explanations are more comprehensible for non-visual users than detailed ones, and that multimodal presentation is required for more equitable interpretability. The study highlights the need for XAI methods that go beyond visual explanations to accommodate diverse user needs.

## Method Summary
The paper presents a four-component methodological framework: AI system categorization, persona definition, prototype design with XAI techniques, and expert/user assessment. The prototype implements Random Forest regression on traffic flow prediction using the UTD19 dataset, with LIME and SHAP for explanations. The system was designed with WAI-ARIA standards for accessibility, generating both detailed (tabular) and simplified (linear) explanations. The evaluation involved accessibility experts and three users with sight loss, focusing on qualitative feedback about explanation comprehension and interface accessibility.

## Key Results
- Visual-only XAI explanations create systematic exclusion of users with vision impairments
- Simplified explanations improve comprehension for non-visual users compared to detailed ones
- Multimodal presentation (text + audio + visual) is required for equitable interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual-only XAI explanations create systematic exclusion of users with vision impairments.
- Mechanism: XAI techniques (SHAP, LIME, Grad-CAM) predominantly output visual formats—bar plots, heatmaps, summary plots—which cannot be processed by screen readers or converted to auditory/tactile modalities without additional design intervention.
- Core assumption: Users with vision impairments interact with systems primarily through non-visual channels (screen readers, braille displays).
- Evidence anchors:
  - [abstract] "evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats"
  - [Section 2, p.3] "SHAP... often visualizes them as complex bar plots or summary plots, which may not translate well to auditory or tactile formats"
  - [corpus] Weak direct corpus support; neighbor papers focus on cognitive understanding rather than disability-specific accessibility.

### Mechanism 2
- Claim: Simplified explanations may improve comprehension for non-visual users compared to detailed ones.
- Mechanism: Detailed explanations (e.g., LIME-Detailed, SHAP-Detailed) present complex tabular data and multi-feature interactions that are difficult to parse via screen readers. Simplified explanations reduce cognitive load by presenting fewer features in linear formats.
- Core assumption: Cognitive load during auditory information processing scales with data complexity differently than visual processing.
- Evidence anchors:
  - [abstract] "Preliminary findings suggest that simplified explanations are more comprehensible for non-visual users than detailed ones"
  - [Section 3.4, p.6] "All three participants agreed that while the detailed explanations provided more information, the simplified explanations offered a better overall understanding"
  - [Section 4, p.7] "this finding should be interpreted with caution, as it is based on feedback from a small sample of only three participants"
  - [corpus] No direct corpus validation; this is a preliminary finding requiring replication.

### Mechanism 3
- Claim: Multimodal presentation (text + audio + visual) is required for equitable interpretability across diverse user abilities.
- Mechanism: No single modality serves all users. Visual explanations serve sighted users; text descriptions serve screen reader users; audio feedback serves users navigating without visual attention. Multiple modalities allow users to choose or combine based on context and ability.
- Core assumption: Users have heterogeneous sensory capabilities and situational contexts that benefit from modality choice.
- Evidence anchors:
  - [abstract] "multimodal presentation is required for more equitable interpretability"
  - [Section 3.5, p.7] "A single model of explanation, typically visual (such as tables, figures, and graphs), will not be accessible to all users, even when supplemented with auditory descriptions"
  - [corpus] Neighbor paper "Beyond Technocratic XAI" supports context-dependent, situated explanation design, though not disability-specific.

## Foundational Learning

- Concept: Screen readers and assistive technology fundamentals
  - Why needed here: The paper assumes familiarity with how users with vision impairments interact with systems (JAWS, NVDA, Orca). Understanding that screen readers linearize content helps explain why tables are problematic.
  - Quick check question: Can you explain why a 5-column data table is harder to comprehend via screen reader than a 5-item bulleted list?

- Concept: XAI techniques (SHAP, LIME, feature attribution)
  - Why needed here: The prototype implements these techniques. You need to understand what they output (feature importance scores, local explanations) to evaluate accessibility transformations.
  - Quick check question: What is the difference between a SHAP summary plot and a SHAP local explanation? Which would be easier to convert to text?

- Concept: WCAG and WAI-ARIA accessibility standards
  - Why needed here: The prototype followed WAI-ARIA standards. These define how to make interactive web content accessible to assistive technologies.
  - Quick check question: What ARIA attribute would you add to a dynamically-generated chart to make it readable by screen readers?

## Architecture Onboarding

- Component map: AI System Categorization → Persona Definition → Prototype Design → Expert/User Assessment
- Critical path: The 4-component methodology is sequential. Skipping the persona definition (Section 3.2) risks building an XAI interface that doesn't match actual user interaction patterns. The expert evaluation (Section 3.4) identified critical issues (e.g., misleading "negative" labels) before user testing.
- Design tradeoffs:
  - **Simplified vs. Detailed explanations**: Simplified improves initial comprehension; detailed supports deeper analysis. The paper suggests offering both, not choosing one.
  - **Visual-first vs. multimodal design**: Building visual-first and adding accessibility later fails. Multimodal must be designed from the start.
  - **Expert vs. end-user evaluation**: Experts catch technical accessibility issues (color contrast, ARIA); users identify comprehension barriers.
- Failure signatures:
  - Tables that are "readable but meaningless"—screen reader announces values but user cannot synthesize relationships.
  - Visual explanations with after-the-fact text descriptions that fail to capture spatial relationships.
  - Detailed explanations that overwhelm users who lack domain expertise or AI literacy.
- First 3 experiments:
  1. **Replicate with larger sample**: Test simplified vs. detailed explanations with ≥20 users with vision impairments to validate preliminary findings (currently n=3).
  2. **Compare modalities**: Test the same explanation content delivered as (a) linear text, (b) hierarchical audio navigation, (c) combined text + sonification. Measure comprehension and task completion time.
  3. **Test across expertise levels**: Include both domain experts (who want detail) and lay users (who want simplicity) to map the simplified/detailed tradeoff space.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do non-visual users consistently prefer simplified explanations over detailed ones across larger and more diverse populations?
- Basis in paper: [explicit] The authors state that while preliminary findings suggest simplified explanations are more comprehensible, "this finding should be interpreted with caution, as it is based on feedback from a small sample of only three participants."
- Why unresolved: The study's user evaluation was limited to three active participants, which is insufficient to generalize preference patterns regarding explanation complexity for the entire sight-loss community.
- What evidence would resolve it: A large-scale quantitative user study with statistically significant participation from users with vision impairments, comparing comprehension and preference scores for both simplified and detailed XAI outputs.

### Open Question 2
- Question: Which specific multimodal formats most effectively convey complex XAI data, such as feature importance, to non-visual users?
- Basis in paper: [explicit] The authors conclude that "multimodal presentation is required for more equitable interpretability" and explicitly call for "further research [to] explore additional multimodal explanation formats."
- Why unresolved: The current proof of concept primarily tested LIME and SHAP outputs adapted with text-to-speech and basic sound feedback, leaving other modalities (e.g., haptic, spatial audio) unexplored.
- What evidence would resolve it: Comparative studies testing diverse non-visual modalities (e.g., tactile graphics, sonification) to measure user comprehension and cognitive load when interpreting AI decisions.

### Open Question 3
- Question: How can inclusive XAI frameworks effectively accommodate the diverse and often conflicting needs of users with different types of disabilities?
- Basis in paper: [inferred] The paper acknowledges using a single persona with total blindness as a "starting point," while noting that "future research should incorporate a broader range of personas to capture the multifaceted nature of disability."
- Why unresolved: Designing for a single persona (Caroline) may overlook the specific needs of users with low vision, cognitive disabilities, or motor impairments, limiting the generalizability of the proposed methodological framework.
- What evidence would resolve it: An evaluation of the XAI framework using multiple personas representing a spectrum of disabilities (e.g., low vision, color blindness, deafness) to identify necessary design trade-offs.

## Limitations
- The simplified vs. detailed explanation finding is based on only three participants, making it a preliminary signal rather than a robust conclusion.
- The paper does not address how to handle the tradeoff between accessibility and technical precision—simplified explanations may mislead expert users who require granular feature attribution.
- No validation exists for the long-term usability of multimodal XAI interfaces; initial comprehension does not guarantee sustained effectiveness.

## Confidence
- **High**: Visual-only XAI explanations systematically exclude users with vision impairments (supported by literature review and technical analysis of XAI outputs).
- **Medium**: Multimodal presentation improves accessibility when designed from the start (supported by expert feedback and accessibility standards).
- **Low**: Simplified explanations are universally better for non-visual users (based on n=3, requires replication).

## Next Checks
1. Replicate the simplified vs. detailed comparison with ≥20 users with vision impairments to establish statistical significance.
2. Test whether expert users with domain knowledge require detailed explanations despite accessibility challenges, mapping the expertise-accessibility tradeoff.
3. Evaluate the same XAI interface over multiple sessions (e.g., weekly for 4 weeks) to assess learning curves and sustained usability.