---
ver: rpa2
title: 'AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models'
arxiv_id: '2505.24784'
source_url: https://arxiv.org/abs/2505.24784
tags:
- axiom
- slot
- mixture
- type
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AXIOM is an object-centric active inference agent that learns to
  play games from raw pixels with high sample efficiency, outperforming deep RL baselines
  like BBF and DreamerV3 in 10k interactions without gradient-based optimization.
  It represents scenes as compositions of objects with piecewise-linear dynamics,
  expands its generative model online by growing mixture components from single events,
  and refines them via Bayesian model reduction for generalization.
---

# AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models

## Quick Facts
- arXiv ID: 2505.24784
- Source URL: https://arxiv.org/abs/2505.24784
- Reference count: 40
- Outperforms deep RL baselines on Gameworld 10k benchmark in 10k interactions without gradient-based optimization

## Executive Summary
AXIOM is an object-centric active inference agent that learns to play games from raw pixels with high sample efficiency. It represents scenes as compositions of objects with piecewise-linear dynamics and expands its generative model online by growing mixture components from single events, then refines them via Bayesian model reduction for generalization. On the Gameworld 10k benchmark, AXIOM achieves higher cumulative rewards and faster convergence than baselines, with an interpretable, compact model (0.3–1.6M parameters) and efficient inference.

## Method Summary
AXIOM uses four coupled mixture models: sMM segments pixels into object-centric latent slots using Gaussian Mixture Models; iMM clusters objects into discrete types based on color/shape; tMM models dynamics as a Switching Linear Dynamical System; and rMM predicts tMM switch states, rewards, and object interactions. Learning occurs through online variational inference without gradients, with structure expansion via stick-breaking thresholds and pruning via Bayesian Model Reduction every 500 steps. Planning uses active inference with Expected Free Energy (utility + information gain) and MPC-style sampling (512 rollouts).

## Key Results
- Achieves higher cumulative rewards than BBF and DreamerV3 baselines on Gameworld 10k benchmark
- Learns effective policies in 10k interactions without gradient-based optimization
- Demonstrates robustness to perturbations (color/shape changes) and supports uncertainty-aware planning

## Why This Works (Mechanism)

### Mechanism 1: Object-Centric State Factorization
Decomposing pixel observations into slot-based object representations accelerates learning by reducing the effective state space. The Slot Mixture Model assigns each pixel to one of K slots using categorical assignment variables and Gaussian likelihoods with position/color/shape features. Each slot maintains continuous latents (position, velocity, color, shape) and discrete latents (type, presence, motion). This factorization allows dynamics models to operate on structured object features rather than raw pixels.

### Mechanism 2: Online Structure Learning via Expansion and Pruning
Dynamically growing and pruning mixture components enables the model to match environmental complexity without over-parameterization. Each mixture module uses truncated stick-breaking priors. Upon receiving data, AXIOM computes posterior-predictive log-density for each existing component; if the best score falls below threshold τ, a new component is instantiated. Every 500 frames, Bayesian Model Reduction evaluates merge candidates by comparing expected free energy of merged vs. separate components.

### Mechanism 3: Active Inference with Information-Seeking Exploration
Augmenting utility with information gain during planning produces structured exploration that resolves model uncertainty. Planning selects policies π that minimize expected free energy G(π) = Στ [−E[log p(rτ)] + DKL(q(αrmm|Oτ,π) ∥ q(αrmm))]. The second term (information gain) scores how much a policy would reduce uncertainty about rMM parameters. Early in training, high uncertainty drives exploration; as the model converges, utility dominates.

## Foundational Learning

- **Variational Inference and Free Energy**
  - Why needed here: AXIOM uses coordinate-ascent variational inference to update posteriors over latents and parameters each frame, avoiding gradient computation.
  - Quick check question: Can you explain how minimizing variational free energy F(q) upper-bounds negative log-evidence?

- **Mixture Models with Stick-Breaking Priors**
  - Why needed here: All four modules use truncated stick-breaking (Dirichlet) priors to enable nonparametric-like growth with finite caps.
  - Quick check question: What role does the final pseudocount α0 play in controlling component creation propensity?

- **Switching Linear Dynamical Systems (SLDS/rSLDS)**
  - Why needed here: The tMM implements an SLDS with switch states s(k)t,tmm selecting linear dynamics; the rMM adds recurrence by conditioning switches on continuous features.
  - Quick check question: How does the rSLDS differ from standard SLDS in how switch states depend on continuous state?

## Architecture Onboarding

- **Component map:**
  - Image -> sMM (pixel tokens → Gaussian mixture over K slots → continuous slot latents) -> iMM (slot features → Gaussian mixture over V types → discrete type assignments) -> rMM (continuous/discrete features → mixture over M components → predicts tMM switch) -> tMM (previous slot state + switch state → L linear transition models → next slot state) -> Planning (rollout H-step policies → evaluate expected free energy → select action)

- **Critical path:**
  1. Image arrives → sMM E-step assigns pixels to slots, updates slot posteriors
  2. Gate variable G(k)t computed from presence/moving latents → filters inactive slots
  3. iMM infers type for each active slot
  4. rMM E-step computes responsibilities ρ(k)t,m → predicts tMM switch
  5. tMM applies linear transition conditioned on switch
  6. M-step updates all module parameters using accumulated sufficient statistics
  7. Every 500 steps: BMR evaluates merge candidates for rMM

- **Design tradeoffs:**
  - Expansion threshold τ: Lower values = more conservative growth (fewer components, risk underfitting); higher values = rapid growth (risk overfitting, computational cost)
  - BMR interval ∆TBMR: Frequent reduction accelerates generalization but may merge prematurely; infrequent reduction leaves redundant components
  - Information gain weight λIG: Higher values encourage exploration but may delay reward optimization; ablation shows game-specific sensitivity
  - Planning depth H and rollouts P: Deeper/more rollouts improve plan quality but increase latency (Table 2 shows 252-534ms for 64-512 rollouts)

- **Failure signatures:**
  - Slot fragmentation: Single object split across multiple slots (sMM expansion too aggressive, or color variance priors too tight)
  - Slot merging: Distinct objects assigned to same slot (α0,smm too low, or objects have similar appearance)
  - Dynamics collapse: All trajectories assigned to one tMM component (τtMM too restrictive)
  - Exploration paralysis: Agent avoids all interactions after negative reward (information gain disabled or BMR merged exploration clusters; see Cross ablation)
  - Perturbation amnesia: Performance drops after color change and doesn't recover (iMM relying too heavily on color features; mitigated by remapping trick)

- **First 3 experiments:**
  1. **Slot extraction validation:** Run sMM on single-frame images from each game; verify that K converges to the true object count and visualize pixel assignments. Check that position/color/shape latents match ground truth.
  2. **Dynamics mode discovery:** Collect trajectories with random actions; visualize tMM components by projecting latent transitions. Confirm that discovered modes (e.g., "falling," "bouncing," "stationary") align with game physics.
  3. **Ablation grid:** Run AXIOM with (a) no BMR, (b) no information gain, (c) fixed interaction distance on all 10 games. Reproduce Table 1 to validate that performance deltas match reported patterns before attempting extensions.

## Open Questions the Paper Calls Out

### Open Question 1
Can core priors about objects and their interactions be automatically discovered from data rather than engineered? The current implementation relies on hand-specified priors about object-centric dynamics; extending to complex domains like Atari or Minecraft requires methods to infer priors when generative processes are less transparent. Demonstrating learned priors that achieve comparable sample efficiency on Gameworld 10k or successfully scaling to visually complex domains without domain-specific engineering would resolve this.

### Open Question 2
What is the optimal scheduling strategy for Bayesian Model Reduction (BMR) during learning? The fixed 500-step interval helps in some games (Gold, Hunt) via spatial generalization but hurts in others (Cross) by prematurely reducing information gain and discouraging exploration. A systematic study of adaptive BMR scheduling strategies that maintain or improve performance across all environments would resolve this.

### Open Question 3
How does the information gain exploration term scale to tasks requiring hard exploration? Current results show mixed effects—information gain helps in Bounce but encourages visiting negatively-rewarding states in games like Gold; Gameworld environments deliberately avoid hard exploration challenges. Evaluation on benchmark environments with sparse rewards or deceptive local optima, comparing performance with and without the information gain objective, would resolve this.

## Limitations
- The expansion threshold and BMR schedule appear sensitive to game-specific dynamics with significant variance in performance when mis-tuned
- The slot assignment mechanism assumes objects occupy spatially coherent regions with distinct colors/shapes, potentially degrading on games with heavy occlusion or amorphous objects
- Robustness claims to perceptual perturbations rely on an ad-hoc remapping strategy without theoretical justification

## Confidence

**High:** The core architectural design (object-centric factorization + mixture-based dynamics) is internally consistent and mathematically sound.

**Medium:** The sample efficiency claims are credible given the ablation evidence, but comparisons to deep RL baselines may not generalize beyond the Gameworld benchmark.

**Low:** The robustness claims to perceptual perturbations (color/shape changes) rely on an ad-hoc remapping strategy without theoretical justification.

## Next Checks
1. Test AXIOM on a game with overlapping objects to assess slot fragmentation/merge failures
2. Run with disabled BMR on a game showing high component count to quantify pruning impact on generalization
3. Evaluate the effect of varying the expansion threshold τ on exploration behavior and final reward across all 10 games to identify sensitivity patterns