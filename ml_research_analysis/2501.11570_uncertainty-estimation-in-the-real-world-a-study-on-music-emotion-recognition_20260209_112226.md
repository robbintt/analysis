---
ver: rpa2
title: 'Uncertainty Estimation in the Real World: A Study on Music Emotion Recognition'
arxiv_id: '2501.11570'
source_url: https://arxiv.org/abs/2501.11570
tags:
- uncertainty
- data
- methods
- music
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates uncertainty estimation in music emotion
  recognition, a highly subjective task with significant annotator disagreement. The
  authors explore several methods including probabilistic loss functions (MSE, KLD,
  NLL), random seeds, and Monte Carlo dropout to estimate both the central tendencies
  and uncertainty of emotional responses.
---

# Uncertainty Estimation in the Real World: A Study on Music Emotion Recognition

## Quick Facts
- arXiv ID: 2501.11570
- Source URL: https://arxiv.org/abs/2501.11570
- Reference count: 0
- Primary result: All tested uncertainty quantification methods failed to capture inter-rater variability in music emotion recognition (R² ≪ 0 for standard deviation prediction).

## Executive Summary
This study investigates uncertainty estimation in music emotion recognition, a highly subjective task with significant annotator disagreement. The authors explore several methods including probabilistic loss functions (MSE, KLD, NLL), random seeds, and Monte Carlo dropout to estimate both the central tendencies and uncertainty of emotional responses. Using the DEAM dataset with 1744 songs and multiple raters, they find that while all methods perform similarly in predicting mean valence and arousal ratings (R² around 0.6-0.7), none effectively model the uncertainty associated with inter-rater variations. The correlation between predicted and empirical standard deviations is weak or negligible for all methods, with R² values well below zero. The study concludes that current uncertainty quantification techniques are insufficient for capturing data uncertainty in subjective tasks like music emotion recognition, highlighting the need for alternative approaches to model inter-rater variability in emotional responses.

## Method Summary
The study uses the DEAM dataset with 1744 songs, each annotated by multiple raters for valence and arousal. A frozen MusicFM-MSD foundation model extracts 1024-dimensional embeddings, followed by a 2-layer FCN with 128 hidden units each. The model predicts both mean emotional ratings (via tanh activation) and uncertainty (via negative softplus activation producing σ̂ ∈ (0,1)). Three probabilistic loss functions are tested: MSE, KLD, and NLL. Additionally, multi-seed ensembling (15 seeds) and Monte Carlo dropout are used for inference-time uncertainty estimation. The models are trained with Adam optimizer, learning rate starting at 10^-3 and reducing by 0.9 after 3 epochs without improvement, for up to 100 epochs with batch size 32.

## Key Results
- All methods achieve similar R² ≈ 0.6-0.7 for predicting mean valence and arousal ratings
- None of the methods successfully predict empirical standard deviations (R² ≪ 0)
- Multi-seed and MC dropout consistently underestimate empirical SD (predicted range ~0.00-0.10 vs. empirical 0.00-0.60)
- NLL produces narrow SD predictions with occasional large outliers; MSE/KLD produce wider ranges but near-zero correlation with empirical SD
- Negative or near-zero Spearman correlation between predicted and empirical SD across all methods

## Why This Works (Mechanism)

### Mechanism 1
Probabilistic loss functions (MSE, KLD, NLL) enable joint prediction of mean responses and associated uncertainty by treating outputs as distribution parameters rather than point estimates. The model outputs both location (μ̂) and scale (σ̂) parameters. MSE and KLD losses require ground-truth uncertainty during training, directly penalizing deviations in σ̂. NLL loss balances prediction error against uncertainty—larger σ̂ reduces penalty for mean errors, while a logarithmic term penalizes overconfident (small σ̂) predictions. Core assumption: The conditional distribution of emotional responses Y|X=x follows a Gaussian distribution with diagonal covariance, meaning valence and arousal are independent random variables given the stimulus.

### Mechanism 2
Random seed ensembling captures model uncertainty by sampling from the posterior distribution over parameters conditioned on the dataset. Training n independent models with different random initializations yields parameter samples θ̂₁, θ̂₂, ..., θ̂_n drawn i.i.d. from P_Θ|D. Aggregating predictions via mean ⟨ŷᵢ⟩ and variance s²_seed provides point estimate and uncertainty. No architectural changes required—uncertainty emerges from ensemble disagreement. Core assumption: Variation across model optima reflects meaningful uncertainty about the mapping from audio to emotion, rather than artifacts of optimization non-convexity.

### Mechanism 3
Monte Carlo dropout approximates Bayesian inference by sampling from the posterior over weights at inference time through stochastic forward passes. With dropout enabled during inference, each forward pass uses a randomly masked subset of activations. Repeating n times yields predictions {ỹ₁, ỹ₂, ..., ỹ_n} from which mean ⟨ỹᵢ⟩ and variance s²_MC are computed. Requires only a single trained model. Core assumption: The dropout mask distribution approximates a variational posterior over model weights, and variability from masked neurons reflects prediction uncertainty.

## Foundational Learning

- Concept: Aleatoric (data) vs. Epistemic (model) uncertainty
  - Why needed here: The paper explicitly distinguishes these but notes that investigated methods cannot disentangle them. Understanding this taxonomy is essential for interpreting why methods fail—data uncertainty from subjectivity is irreducible; model uncertainty could theoretically decrease with more data.
  - Quick check question: If you double the number of annotators per song and average their ratings, which type of uncertainty should decrease?

- Concept: Heteroscedastic regression
  - Why needed here: The NLL loss implements heteroscedastic regression where predicted variance σ̂²(x) varies per input. This is distinct from homoscedastic models with constant noise variance. The failure of this approach suggests input-dependent variance prediction is fundamentally harder for subjective tasks.
  - Quick check question: In Eq. 3, what happens to the loss when the model predicts very high σ̂ for a sample with large mean error?

- Concept: Foundation model freezing
  - Why needed here: The experimental setup freezes MusicFM-MSD and trains only the FCN head. This architectural choice may limit uncertainty estimation if the pretrained embeddings discard stimulus features relevant to annotator disagreement (e.g., genre familiarity cues).
  - Quick check question: What information might a foundation model trained with random masking lose that could be relevant to predicting annotator disagreement?

## Architecture Onboarding

- Component map: Audio (24kHz, 45s) → MusicFM-MSD (frozen, 1024-dim embeddings) → Channel & temporal averaging → FCN (2×128, ELU, 50% dropout) → Output heads (mean: tanh, variance: NegSoftPlus)

- Critical path: Audio preprocessing → Foundation model inference (frozen) → FCN forward pass → Loss computation → Backprop through FCN only

- Design tradeoffs:
  - Frozen vs. fine-tuned foundation model: Freezing reduces overfitting risk on small MER datasets but may lose uncertainty-relevant features. Fine-tuning could recover these but requires more data.
  - Diagonal vs. full covariance: Paper assumes diagonal Σ for numerical stability and simplicity, precluding valence-arousal correlation modeling.
  - Multi-seed vs. MC dropout: Multi-seed requires 15× training cost; MC dropout requires 15× inference passes. Both fail to capture inter-rater variability.

- Failure signatures:
  - All methods achieve R² ≈ 0.6–0.7 for mean prediction but R² ≪ 0 for standard deviation prediction
  - Multi-seed and MC dropout consistently underestimate empirical SD (predicted SD range ~0.00–0.10 vs. empirical 0.00–0.60)
  - NLL produces narrow SD predictions with occasional large outliers; MSE/KLD produce wider ranges but near-zero correlation with empirical SD
  - Negative or near-zero Spearman correlation between predicted and empirical SD across all methods

- First 3 experiments:
  1. Replicate the MSE vs. KLD vs. NLL comparison on DEAM with a single seed. Verify that mean prediction R² matches reported ~0.6 and SD prediction R² is negative.
  2. Ablate the frozen foundation model by replacing MusicFM-MSD with a trainable CNN feature extractor. Compare whether trainable features improve SD prediction correlation.
  3. Replace the Gaussian assumption with a mixture density network (MDN) outputting parameters of a Gaussian mixture model. Evaluate whether multi-modality in the output distribution better captures heterogeneous rater populations.

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative deep learning architectures (e.g., distributional prediction networks, quantile regression, or deep ensembles with diversity regularization) effectively capture inter-rater variability in subjective regression tasks like MER? The five tested methods (MSE, KLD, NLL losses; random seeds; MC dropout) all failed to predict empirical standard deviations, with R² values well below zero. What evidence would resolve it: Demonstrating a method that achieves positive R² and meaningful correlation (e.g., Spearman r > 0.3) between predicted and empirical standard deviations on the DEAM dataset or similar benchmarks.

### Open Question 2
Does the use of frozen pretrained foundation models discard uncertainty-relevant information during embedding extraction, limiting downstream uncertainty estimation? The MusicFM foundation model was frozen in all experiments; no comparison with end-to-end trained models was conducted. What evidence would resolve it: Comparing uncertainty estimation performance between frozen-pretrained and end-to-end trained models, or analyzing whether foundation model embeddings retain variance-related information.

### Open Question 3
How does dataset scale (number of annotated samples and raters per sample) affect the ability to disentangle and accurately estimate aleatoric versus epistemic uncertainty in subjective tasks? The DEAM dataset has only 1744 songs with a minimum of 10 raters, and no experiments varied data quantity systematically. What evidence would resolve it: Controlled experiments with subsampled datasets varying both sample count and raters-per-sample, measuring uncertainty estimation quality across conditions.

## Limitations
- Frozen foundation model may have limited the models' ability to learn features relevant to predicting annotator disagreement
- Gaussian assumption for emotional responses may be overly simplistic for capturing the complex, potentially multi-modal nature of human emotional perception
- Dataset size and number of annotators per song may be insufficient to reliably estimate uncertainty in highly subjective tasks

## Confidence
- High: Failure of all investigated methods to capture data uncertainty (R² ≪ 0 for SD prediction)
- Medium: Frozen foundation model limits uncertainty estimation capability
- Medium: Gaussian assumption insufficient for subjective response distributions

## Next Checks
1. Implement and test a mixture density network (MDN) to model potentially multi-modal response distributions
2. Fine-tune the foundation model (rather than freezing) to assess whether learned features improve uncertainty prediction
3. Conduct ablation studies varying the number of annotators per song to determine the minimum required for reliable uncertainty estimation