---
ver: rpa2
title: 'MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving
  General Capabilities'
arxiv_id: '2505.12043'
source_url: https://arxiv.org/abs/2505.12043
tags:
- uni00000013
- general
- uni00000011
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture of Losses (MoL), a dual-loss training
  framework for large language models that addresses catastrophic forgetting during
  domain-specific adaptation. The method applies cross-entropy loss to domain-specific
  corpora for knowledge acquisition while using Kullback-Leibler divergence with the
  base model for general corpora to preserve foundational capabilities.
---

# MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities

## Quick Facts
- arXiv ID: 2505.12043
- Source URL: https://arxiv.org/abs/2505.12043
- Reference count: 13
- Key outcome: 27.9% higher accuracy on Math-500 and 83.3% improvement on AIME25 subset using 1:1 domain-to-general corpus ratio

## Executive Summary
This paper introduces Mixture of Losses (MoL), a dual-loss training framework that addresses catastrophic forgetting during domain-specific LLM adaptation. The method applies cross-entropy loss to domain-specific corpora for knowledge acquisition while using Kullback-Leibler divergence with the base model for general corpora to preserve foundational capabilities. Experiments demonstrate that a 1:1 domain-to-general corpus ratio optimally balances training, achieving significant improvements on domain benchmarks while maintaining general language skills without extensive hyperparameter tuning.

## Method Summary
The Mixture of Losses framework routes domain-specific data through standard Cross-Entropy (CE) loss for knowledge injection while processing general data using Kullback-Leibler (KL) divergence against a frozen base model's logits. This creates a dual optimization objective where domain samples receive 99% CE weight and 1% KL regularization, while general samples receive 1% CE weight and 99% KL alignment. The framework uses LoRA (rank 64) for parameter-efficient training with a small mixing coefficient α=0.01 to stabilize the combined loss. The optimal 1:1 domain-to-general corpus ratio was empirically determined to balance domain knowledge acquisition with general capability preservation.

## Key Results
- 27.9% higher accuracy on Math-500 benchmark in non-think reasoning mode
- 83.3% improvement on challenging AIME25 subset in think mode
- Maintains general capabilities while enhancing domain expertise without extensive hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
Decoupling loss functions by data type allows the model to acquire domain knowledge while anchoring general capabilities. Domain-specific data uses CE loss for "hard" token prediction, while general data uses KL divergence against the base model as "soft" labels. Core assumption: the model can switch optimization modes based on data source. Break condition: noisy dataset metadata causing wrong loss function application.

### Mechanism 2
KL divergence on general data acts as a gradient regularizer, preventing catastrophic forgetting. The paper observes KL generates smaller gradient magnitudes than CE, creating a "negative feedback regression mechanism" that stabilizes the model. Core assumption: reduced gradient magnitude is sufficient to maintain complex reasoning capabilities. Break condition: α coefficient set too high (e.g., 0.5), diluting KL's stabilizing effect.

### Mechanism 3
A 1:1 ratio of domain-to-general corpus provides optimal equilibrium for the dual-loss system. This balanced dataset prevents domain-specific CE gradients from dominating training dynamics. Core assumption: optimal ratio determined by sample count rather than complexity or information density. Break condition: extreme data sparsity or task difficulty making 1:1 ratio inappropriate.

## Foundational Learning

- **Knowledge Distillation (KL Divergence)**: The mathematical tool used to "freeze" general capabilities by matching probability distributions rather than predicting exact tokens. Quick check: Why would matching a probability distribution preserve reasoning better than predicting the exact next token?

- **Catastrophic Forgetting**: The failure mode MoL solves—where learning new medical knowledge overwrites previously learned grammar or logic. Quick check: If you fine-tune a model exclusively on medical data, what happens to its ability to write Python code?

- **LoRA (Low-Rank Adaptation)**: The parameter-efficient fine-tuning method used in experiments (rank 64). Quick check: Does MoL require full-parameter training, or can it work with parameter-efficient fine-tuning?

## Architecture Onboarding

- **Component map**: Data Loader -> Frozen Base Model -> Trainable Model -> Loss Router -> Combiner
- **Critical path**: Pre-processing data with domain/general tags → Loading batch and identifying tag → Forward pass through Base and Trainable model → Computing specific loss → Backprop
- **Design tradeoffs**: Memory requires loading two models or caching logits; speed has KL computation overhead; generality vs. specificity shows 1:1 ratio doubles training time but preserves skills
- **Failure signatures**: Mode Collapse (domain jargon in general contexts, likely α too high); Stagnation (domain accuracy fails to improve, check if α too high)
- **First 3 experiments**: 1) Sanity Check: Train on 1:1 data using only CE loss, expect high domain accuracy but significant drops in general benchmarks. 2) Alpha Sensitivity: Run MoL with α=0.01 vs. α=0.5, verify higher α destabilizes balance. 3) Ratio Sweep: Train with Domain:General ratios of 1:0.5, 1:1, and 1:2, confirm 1:1 provides best average score.

## Open Questions the Paper Calls Out
- What specific mechanisms drive the unexpected enhancement of Instruction Following and AIME reasoning capabilities through KL divergence alignment on general corpora?
- Does the empirically validated 1:1 domain-to-general corpus ratio generalize to domains with extreme data scarcity or high distributional divergence?
- How does the Mixture of Losses framework interact with full-parameter fine-tuning compared to the Low-Rank Adaptation used in the study?

## Limitations
- Experimental validation primarily focuses on medical-domain adaptation using a single base model (Qwen3-8B)
- Framework's effectiveness across diverse domains (legal, financial, scientific) and model architectures remains untested
- Computational overhead of maintaining a frozen base model for KL divergence calculations could be prohibitive for very large models

## Confidence
- **High Confidence**: Dual-loss mechanism is mathematically sound; KL gradients are smaller than CE gradients
- **Medium Confidence**: 1:1 ratio being optimal is supported by experiments but may be dataset-specific
- **Low Confidence**: Framework works "without requiring extensive hyperparameter tuning" given specific optimal values found

## Next Checks
1. Apply MoL to a non-medical domain (e.g., legal or financial) using the same Qwen3-8B base model to validate cross-domain generalization of the 1:1 ratio and hyperparameters.

2. Implement MoL on a different base model architecture (e.g., Llama-3 or Mistral) using the same medical dataset to test architectural independence of the framework.

3. Measure GPU memory usage and training time for MoL versus standard CE training across different batch sizes and model scales (8B, 70B parameters) to quantify practical overhead.