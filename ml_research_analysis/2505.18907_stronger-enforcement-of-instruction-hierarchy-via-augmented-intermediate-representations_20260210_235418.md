---
ver: rpa2
title: Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations
arxiv_id: '2505.18907'
source_url: https://arxiv.org/abs/2505.18907
tags:
- attacks
- injection
- input
- instruction
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of prompt injection attacks, where
  malicious inputs can hijack the behavior of large language models (LLMs). The core
  method, Augmented Intermediate Representations (AIR), injects instruction hierarchy
  (IH) signals into every decoder layer of the LLM using layer-specific trainable
  embeddings, rather than only at the input layer as in prior work.
---

# Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations

## Quick Facts
- arXiv ID: 2505.18907
- Source URL: https://arxiv.org/abs/2505.18907
- Reference count: 37
- Primary result: AIR reduces gradient-based prompt injection attack success rates by 1.6× to 9.2× compared to state-of-the-art defenses while maintaining model utility

## Executive Summary
This paper addresses the critical problem of prompt injection attacks in large language models (LLMs), where malicious inputs can hijack model behavior. The proposed solution, Augmented Intermediate Representations (AIR), injects instruction hierarchy signals into every decoder layer of the LLM using layer-specific trainable embeddings, rather than only at the input layer as in prior work. Experimental results demonstrate significant improvements in robustness against gradient-based attacks while maintaining model utility with only minor degradation in some cases.

## Method Summary
AIR modifies the transformer architecture by adding trainable embedding tables to each decoder layer. These embeddings are indexed by privilege levels (P0 for system/user instructions, P1 for data segments, P2 for responses) and added to the intermediate token representations at each layer. The method uses a two-stage training approach: first standard fine-tuning on clean data, then adversarial fine-tuning with prompt injection examples. This allows the model to learn to respect instruction hierarchy throughout all processing stages rather than just at the input.

## Key Results
- AIR reduces gradient-based attack success rates by 1.6× to 9.2× compared to state-of-the-art defenses
- Robustness improvements are consistent across different model sizes (3B, 7B, 8B parameters)
- Utility degradation is minimal (0.2% to 4.2%) with SFT training, though DPO generally performs better
- The per-layer embedding approach outperforms input-only injection methods

## Why This Works (Mechanism)

### Mechanism 1: Layer-specific IH embedding injection improves signal fidelity
- **Claim**: Layer-specific IH embedding injection improves signal fidelity throughout the network
- **Mechanism**: AIR adds a trainable embedding vector to intermediate token representations at every decoder layer. Each layer has its own embedding table S_j with K entries (one per privilege level). The augmentation follows: x'_ij = x_ij + s^k_j, where s^k_j = S_j[k_i] retrieves the embedding for token i's privilege level k_i.
- **Core assumption**: The IH signal degrades or becomes indistinguishable as it propagates through layers when only injected at the input
- **Evidence**: [abstract] "our work typically inject the IH signal exclusively at the initial input layer, which we hypothesize limits its ability to effectively distinguish the privilege levels of tokens as it propagates through the different layers"
- **Break condition**: If intermediate representations already encode sufficient privilege information (e.g., through pretraining), the additional embeddings may provide marginal benefit

### Mechanism 2: Direct IH signal injection enables stronger gradient flow
- **Claim**: Direct IH signal injection to all decoder blocks enables stronger gradient flow during adversarial training
- **Mechanism**: By injecting the privilege index k_i directly to each decoder block rather than relying on implicit propagation, the training objective can more directly optimize how each layer processes tokens of different privilege levels
- **Core assumption**: The model learns layer-specific transformations that respect IH when the signal is explicitly available at each layer during both forward and backward passes
- **Evidence**: [Section 4] "AIR directly injects the IH signals (k_i) to all the decoder blocks as shown in Fig. 2c"
- **Break condition**: If the training data doesn't contain sufficient conflicting instruction examples, the model may not learn to leverage the per-layer signals effectively

### Mechanism 3: Per-layer embeddings allow layer-appropriate privilege representations
- **Claim**: Per-layer embeddings allow the model to learn layer-appropriate privilege representations
- **Mechanism**: Each decoder layer has independently trainable embeddings, allowing the network to learn how privilege information should influence processing at different depths (e.g., early layers might focus on token classification, later layers on instruction following)
- **Core assumption**: Different layers serve different functions in processing privilege information, and a single global embedding would be suboptimal
- **Evidence**: [Section 4] "AIR introduces a trainable embedding table S_j to each decoder block, consisting of K entries"
- **Break condition**: If all layers use embeddings similarly (learned embeddings converge), the per-layer design may be unnecessary overhead

## Foundational Learning

- **Concept**: Transformer decoder stack and residual connections
  - **Why needed here**: AIR adds embeddings into the residual stream at each layer. Understanding how information flows through layer norm, attention, and residual connections is essential for correct implementation.
  - **Quick check question**: At which point in a decoder block does AIR add the IH embedding—before or after attention? (Answer: After attention, augmenting the intermediate representation before the next layer.)

- **Concept**: Instruction hierarchy and privilege levels
  - **Why needed here**: The defense relies on defining privilege levels (P0 > P1 > P2) and ensuring the model respects them when instructions conflict.
  - **Quick check question**: If P0 tokens say "summarize" and P1 tokens say "ignore previous instructions," which should the model follow?

- **Concept**: Gradient-based adversarial attacks (GCG)
  - **Why needed here**: The primary threat model is white-box attacks that optimize adversarial prefixes. Understanding how gradients flow through the model helps interpret why AIR improves robustness.
  - **Quick check question**: Why might adding IH embeddings at every layer make gradient-based attacks harder to optimize?

## Architecture Onboarding

- **Component map**: Input tokens -> Privilege labeling -> AIR embedding lookup (per layer) -> Transformer layers -> Output
- **Critical path**:
  1. Define privilege levels for your application (e.g., system=0, user=1, data=2)
  2. Add embedding tables to each decoder block
  3. Initialize tables (paper uses N(0, 0.02²) for Llama, N(0, 0.1²) for Qwen)
  4. Format training data with IH labels
  5. Run two-stage training

- **Design tradeoffs**:
  - Parameter overhead: ~0.4M for 8B model (negligible)
  - Inference compute: negligible addition (table lookup + vector add per token per layer)
  - Training compute: similar to other IH defenses
  - Utility vs robustness: SFT training shows utility drops up to 4.2% on some models; DPO generally better

- **Failure signatures**:
  - High ASR on gradient attacks despite training → check embedding initialization scale (Qwen needed 5x larger)
  - Utility degradation >5% → may need DPO instead of SFT, or reduce adversarial data proportion
  - No improvement over baseline → verify IH labels are correctly assigned during data formatting

- **First 3 experiments**:
  1. Replicate AIR on a small model (Llama-3.2-3B) with the provided Alpaca dataset splits, confirming GCG ASR reduction matches reported ranges.
  2. Ablate: Compare AIR with embeddings only at input layer vs all layers to quantify the per-layer contribution.
  3. Test on out-of-distribution attacks (Completion, Escape Separation) to verify robustness generalizes beyond training distribution.

## Open Questions the Paper Calls Out

- **Open Question 1**: How effective is the AIR defense mechanism in multi-turn conversational settings and complex agentic workflows?
  - **Basis in paper**: [explicit] Appendix A explicitly identifies the evaluation of the method in multi-turn conversational settings and complex agentic workflows as a key direction for future work.
  - **Why unresolved**: The paper's evaluations are strictly confined to single-turn interactions using the AlpacaFarm and SEP datasets.
  - **What evidence would resolve it**: Empirical results from benchmarks designed for multi-turn dialogue or agentic integration (e.g., AgentBench) showing AIR's utility and robustness over extended interactions.

- **Open Question 2**: Is there a universal initialization strategy for the layer-specific embedding tables that ensures robustness across diverse model architectures without manual tuning?
  - **Basis in paper**: [inferred] Appendix B.2 notes that the standard initialization (std=0.02) yielded suboptimal results for the Qwen model, requiring a fivefold increase in standard deviation, suggesting sensitivity to the magnitude of intermediate representations.
  - **Why unresolved**: The authors did not perform exhaustive tuning due to computational constraints, leaving the generalizability of the initialization hyperparameter unresolved.
  - **What evidence would resolve it**: A systematic ablation study across various model architectures (e.g., Mistral, Gemma) identifying a correlation between hidden-state magnitudes and optimal initialization values.

- **Open Question 3**: Can AIR maintain its robustness against adaptive attackers who specifically optimize their attack to account for the augmented intermediate representations?
  - **Basis in paper**: [inferred] Appendix A acknowledges that the defense lacks formal robustness guarantees and that "advanced attacks might still succeed."
  - **Why unresolved**: The evaluation focuses on standard gradient-based attacks (GCG), but does not test adaptive attacks where the adversary is aware of the AIR mechanism and alters the loss function accordingly.
  - **What evidence would resolve it**: Evaluation against an adaptive white-box attack where the gradient computation includes the AIR parameters to attempt to bypass the hierarchy enforcement.

## Limitations

- The method's effectiveness is demonstrated primarily on chat-style instruction-following tasks and may not generalize well to domains with different privilege structures (e.g., code generation, mathematical reasoning).
- Robustness gains depend heavily on the quality and diversity of adversarial examples used in Stage 2 training, with only two attack patterns tested.
- The AIR framework requires careful implementation of privilege labeling across all training data, with potential inconsistencies significantly degrading both utility and security performance.

## Confidence

- **High Confidence**: The architectural implementation of AIR embeddings and their integration into the transformer layers is clearly specified and technically sound. The experimental results showing robustness improvements over baseline defenses are well-documented and reproducible.
- **Medium Confidence**: The claimed utility preservation is supported by experimental results, but the trade-offs between different training methods (SFT vs DPO) and their impact on utility are not fully characterized. The initialization differences between Llama and Qwen models suggest some model-specific tuning may be required.
- **Low Confidence**: The paper's explanation of why layer-wise embedding injection outperforms input-only injection relies primarily on hypothesis rather than empirical validation. The ablation studies don't isolate the contribution of the per-layer design versus the overall IH signal injection.

## Next Checks

- **Check 1**: Implement a systematic ablation where AIR embeddings are removed from individual decoder layers to quantify the contribution of each layer. This would validate whether all layers truly benefit from direct IH signal injection or if certain depths are more critical.
- **Check 2**: Fine-tune AIR on a different domain (e.g., code generation or mathematical reasoning) and evaluate both utility and robustness. This would test whether the IH framework generalizes beyond chat-style tasks.
- **Check 3**: Generate and evaluate against a broader range of prompt injection attacks (e.g., multi-step attacks, context-aware attacks) to assess whether the robustness gains hold against attacks not seen during training.