---
ver: rpa2
title: 'DualSchool: How Reliable are LLMs for Optimization Education?'
arxiv_id: '2505.21775'
source_url: https://arxiv.org/abs/2505.21775
tags:
- dual
- p2dc
- llms
- optimization
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUALSCHOOL evaluates how reliably LLMs can convert primal linear
  programs to duals and perform related tasks like verification and error correction.
  It introduces a new Canonical Graph Edit Distance (CGED) metric that handles convention
  differences in dualization, enabling robust equivalence detection beyond existing
  methods.
---

# DualSchool: How Reliable are LLMs for Optimization Education?

## Quick Facts
- arXiv ID: 2505.21775
- Source URL: https://arxiv.org/abs/2505.21775
- Authors: Michael Klamkin; Arnaud Deza; Sikai Cheng; Haoruo Zhao; Pascal Van Hentenryck
- Reference count: 40
- Primary result: Leading open models achieve only 47.8% accuracy on dualization tasks despite knowing procedures

## Executive Summary
DUALSCHOOL evaluates how reliably LLMs can convert primal linear programs to duals and perform related tasks like verification and error correction. It introduces a new Canonical Graph Edit Distance (CGED) metric that handles convention differences in dualization, enabling robust equivalence detection beyond existing methods. Experiments with leading open models show low accuracy: best models achieve only 47.8% CGED accuracy on small instances, with similarly poor performance on correction, verification, and classification tasks. Objective-value checks yield many false positives, while graph edit distance without canonicalization is too restrictive. The work highlights that despite knowing dualization procedures, LLMs struggle to execute them correctly, limiting their reliability for educational and practical use.

## Method Summary
The study evaluates LLM performance on primal-to-dual conversion (P2DC) and derivative tasks using a new Canonical Graph Edit Distance metric. It generates 1,300+ primal-dual pairs from 2D polytopes, CO relaxations, and LLM4OPT-derived benchmarks. LLMs generate gurobipy code for dual LPs, which is executed and canonicalized (slack elimination, sign normalization, constraint sense standardization) before GED comparison. The framework tests zero-shot and one-shot prompting across multiple open models, comparing CGED against objective-value and non-canonicalized GED baselines.

## Key Results
- Best open models achieve only 47.8% CGED accuracy on dualization tasks
- Objective-value checks produce many false positives, accepting structurally incorrect formulations
- LLMs can articulate dualization procedures but fail to execute them correctly due to implementation errors
- CGED effectively handles convention differences while maintaining structural correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Canonicalization preprocessing enables convention-invariant equivalence detection between dual formulations.
- Mechanism: CGED normalizes LPs by (1) eliminating slack variables via constraint substitution, (2) standardizing variable signs by flipping coefficients and bounds, and (3) converting all constraints to ≥ form before computing graph edit distance. This removes benign symmetries while preserving structural correctness.
- Core assumption: The canonicalization rules cover all valid dualization convention differences (Assumption: the paper does not prove completeness).
- Evidence anchors:
  - [abstract] "Canonical Graph Edit Distance (CGED) metric that handles convention differences in dualization, enabling robust equivalence detection beyond existing methods"
  - [Section 4.1] Describes slack variable elimination (Eq. 5), variable sign canonicalization (Eq. 6), and the five allowed transformation types
  - [corpus] Weak direct support; corpus focuses on unrelated conversion tasks (ANN-SNN, CVR prediction), not LP canonicalization
- Break condition: CGED fails for equivalence types it explicitly ignores—constraint scaling, variable substitutions, and polyhedral rotations (noted in Section 4 as intentional design choices).

### Mechanism 2
- Claim: Objective-value verification produces false positives because incorrect formulations can yield matching optimal values.
- Mechanism: Objective checks compare only the optimal value, not the formulation structure. Missing constraints, extra variables, or structural errors that don't affect the optimum are silently accepted. In P2DC specifically, returning the primal problem itself passes due to strong duality.
- Core assumption: The paper assumes objective-value similarity is necessary but insufficient for correctness (validated experimentally, not proven theoretically).
- Evidence anchors:
  - [abstract] "Objective-value checks yield many false positives"
  - [Section 4] "if the formulation omits a constraint that is not tight at the optimal value, the optimal value check will still mark the formulation correct"
  - [Table 2] OBJ accuracy consistently exceeds CGED accuracy (e.g., Phi-4: 53.7% OBJ vs 47.8% CGED on NL4OPT), demonstrating false positives
  - [corpus] No direct corpus support; corpus papers address different evaluation contexts
- Break condition: Objective-value checks may correctly identify equivalent formulations when the optimal solution is unique and all constraints are tight.

### Mechanism 3
- Claim: LLMs can articulate dualization procedures but fail to execute them correctly due to execution-level errors rather than knowledge gaps.
- Mechanism: LLMs retrieve correct high-level procedures (e.g., standard-form conversion, SOB rules) from training data but make implementation mistakes: forgetting variable bounds (gurobipy defaults to lb=0), flipping bound senses without adjusting coefficients, or omitting dual variables for bounded primals.
- Core assumption: Procedure knowledge implies execution capability (this assumption is falsified by the paper's results).
- Evidence anchors:
  - [abstract] "Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals"
  - [Appendix B.1] All evaluated models except Gemma 3-12B and Mistral 7B produced valid procedural descriptions
  - [Appendix B.3] Documents common implementation errors: gurobipy defaults, incorrect bound senses, missing dual variables
  - [corpus] Weak support; corpus neighbor "Behavior and Representation in LLMs for Combinatorial Optimization" mentions LLM limitations in optimization but not execution-vs-knowledge gaps specifically
- Break condition: This gap may narrow with fine-tuning on structured optimization tasks or chain-of-thought prompting with explicit intermediate verification steps.

## Foundational Learning

- Concept: Linear Programming Duality
  - Why needed here: The entire benchmark is built on primal-to-dual conversion. Without understanding dual variables (shadow prices), constraint correspondence, and the duality theorems, you cannot evaluate whether CGED's canonicalization rules are sound.
  - Quick check question: Given a primal maximization with ≤ constraints, what are the sign restrictions on the corresponding dual variables?

- Concept: Graph Edit Distance (GED)
  - Why needed here: CGED builds on GED as its core comparison algorithm. GED finds the minimum-cost sequence of node/edge operations to transform one graph into another, enabling permutation-invariant comparison of LP structures.
  - Quick check question: If two graphs represent the same LP with variables renamed and constraints reordered, should GED be zero? Why or why not?

- Concept: Symbolic vs. Numerical Verification
  - Why needed here: The paper contrasts CGED (symbolic, structure-preserving) with objective-value checks (numerical, structure-agnostic). Understanding this distinction is critical for choosing evaluation methods in LLM4Optimization.
  - Quick check question: An LP formulation has an extra redundant constraint. Will objective-value verification catch this error? Will CGED?

## Architecture Onboarding

- Component map: Data Generation Pipeline → Primal LP sources (2D polytopes, CO relaxations, NL4OPT/ComplexOR/EasyLP) → Dualization.jl symbolic dualizer → Ground-truth dual pairs → Evaluation Pipeline → LLM response → Parser (gurobipy execution → MPS file) → Canonicalizer (slack elimination, sign normalization) → GED comparator → CGED score → Error Injection Module → Ground-truth dual → Error type application (missing variable, flipped sense, etc.) → Corrupted dual for CORRECTION/VERIFICATION tasks

- Critical path:
  1. Parsing correctness—if the LLM's gurobipy code fails or produces malformed MPS, the instance is marked incorrect
  2. Canonicalization correctness—incorrect canonicalization produces false negatives in equivalence detection
  3. GED computation accuracy—the graph comparison must correctly identify node/edge correspondences

- Design tradeoffs:
  - CGED vs. NGED: CGED adds canonicalization to handle conventions (more permissive) vs. NGED's strict matching (more false negatives)
  - CGED vs. OBJ: CGED preserves structural information (catches subtle errors) vs. OBJ's single-number check (faster but more false positives)
  - Canonicalization scope: Paper intentionally ignores scaling and variable substitutions—appropriate for P2DC but may require extension for general LP equivalence

- Failure signatures:
  - High OBJ accuracy + low CGED accuracy = LLM producing structurally wrong duals with correct objective values (most common pattern in Table 2)
  - High NGED accuracy + low CGED accuracy = contradiction (CGED is strictly more permissive; this would indicate implementation bug)
  - Near-zero execution accuracy = parsing failures or LLM hallucinating invalid syntax

- First 3 experiments:
  1. Reproduce Table 2 baseline on a single dataset (e.g., NL4OPT) with one model (e.g., Phi-4) to validate your CGED implementation matches paper results
  2. Ablate canonicalization steps: run evaluation with only slack elimination, only sign normalization, and neither—measure impact on false positive/negative rates
  3. Test a closed-source model (GPT-4o, Claude) on symbolic dualization task (Table 5 setup) to verify the finding that even strong models struggle with generic code generation for dualization

## Open Questions the Paper Calls Out

- **Open Question 1**: Does fine-tuning with the DualSchool dataset and CGED metric significantly improve LLM reliability on optimization tasks?
  - Basis in paper: [explicit] The conclusion states future work includes "evaluating its efficacy as a fine-tuning dataset" and using the framework for "reinforcement learning with symbolic feedback."
  - Why unresolved: The current study only evaluates base model performance; it does not attempt to repair the identified deficits through training.
  - What evidence would resolve it: Performance improvements on P2DC tasks after fine-tuning models using CGED as a reward signal.

- **Open Question 2**: Can the DualSchool framework and CGED metric be effectively extended to quadratic or conic optimization formulations?
  - Basis in paper: [explicit] The conclusion explicitly lists "extending DUALSCHOOL to quadratic and conic formulations" as a future direction.
  - Why unresolved: The current methodology and Canonical Graph Edit Distance algorithm are tailored specifically for Linear Programs (LPs).
  - What evidence would resolve it: Successful application of the framework to QP/CP instances with modified canonicalization rules.

- **Open Question 3**: Do closed-source frontier models (e.g., GPT-4) significantly outperform open-source models on P2DC tasks?
  - Basis in paper: [inferred] The experiments section notes that "due to resource limitations, the experiments consider only small and medium-sized open-weight LLMs."
  - Why unresolved: It is unclear if proprietary models with larger parameter counts or different architectures overcome the reasoning deficits observed in open models.
  - What evidence would resolve it: A comparative study benchmarking leading closed-source models against the DualSchool dataset.

- **Open Question 4**: Does the "discrepancy" between reciting procedures and executing them disappear with scale?
  - Basis in paper: [inferred] The authors highlight a discrepancy where LLMs "recite the conversion procedure accurately" but "fail to consistently produce correct duals."
  - Why unresolved: It is uncertain if this is a fundamental lack of reasoning capability or a failure that resolves with increased model scale.
  - What evidence would resolve it: Analysis showing a positive correlation between model scale (parameters) and the closure of the gap between procedural knowledge and execution accuracy.

## Limitations

- CGED implementation details remain partially unspecified—canonicalization thresholds and exact slack elimination rules require inference from appendix examples
- Prompt templates use placeholders not fully specified in the paper
- The paper does not prove completeness of canonicalization rules for all valid dualization conventions

## Confidence

- **High confidence**: Dualization.jl provides reliable ground-truth duals; objective-value checks consistently produce false positives across multiple datasets and models; LLMs produce correct procedural descriptions but fail execution
- **Medium confidence**: CGED effectively handles convention differences while maintaining structural correctness; the knowledge-execution gap exists across all evaluated models regardless of size or architecture
- **Low confidence**: The 47.8% accuracy represents an absolute ceiling for LLM dualization without architectural changes; canonicalization rules cover all meaningful equivalence classes

## Next Checks

1. **Ablate canonicalization**: Run evaluation with only slack elimination, only sign normalization, and neither—measure impact on false positive/negative rates to isolate each transformation's contribution
2. **Test closed-source models**: Evaluate GPT-4o and Claude on symbolic dualization (Table 5 setup) to verify the knowledge-execution gap exists beyond open models
3. **Cross-task consistency**: Verify that models performing well on correction/verification tasks also perform well on P2DC, or identify which specific capabilities transfer between tasks