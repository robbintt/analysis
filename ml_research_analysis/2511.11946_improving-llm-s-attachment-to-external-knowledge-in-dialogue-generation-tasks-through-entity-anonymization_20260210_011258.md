---
ver: rpa2
title: Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks
  Through Entity Anonymization
arxiv_id: '2511.11946'
source_url: https://arxiv.org/abs/2511.11946
tags:
- knowledge
- anonymized
- dialogue
- llms
- anonymization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of large language models (LLMs)
  relying on their internal knowledge rather than the provided knowledge graphs during
  dialogue generation, which can lead to inappropriate responses. The authors introduce
  LLM-KAT, an evaluation procedure to measure knowledge attachment, and propose entity
  anonymization to encourage LLMs to better leverage external knowledge.
---

# Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization

## Quick Facts
- **arXiv ID:** 2511.11946
- **Source URL:** https://arxiv.org/abs/2511.11946
- **Authors:** Hadi Sheikhi; Chenyang Huang; Osmar R. Zaïane
- **Reference count:** 32
- **Primary result:** Entity anonymization improves LLM knowledge attachment to provided knowledge graphs in dialogue generation tasks, with F1 scores increasing by up to 5.04 points

## Executive Summary
This paper addresses the challenge of large language models (LLMs) relying on their internal knowledge rather than provided knowledge graphs during dialogue generation, which can lead to inappropriate responses. The authors introduce LLM-KAT, an evaluation procedure to measure knowledge attachment, and propose entity anonymization to encourage LLMs to better leverage external knowledge. By anonymizing entities in the dialogue history and knowledge graph, the approach forces LLMs to rely on the given context rather than their internal knowledge. Experiments on the OpenDialKG dataset demonstrate that anonymization improves LLMs' attachment to external knowledge, with models showing increased F1 scores in contextual alignment without significantly affecting response quality.

## Method Summary
The authors propose entity anonymization to improve LLM attachment to external knowledge graphs during dialogue generation. They anonymize entities by replacing real-world names with typed identifiers (e.g., "Person_1", "Film_A") using a strong LLM (QwQ-32B) as an expert anonymizer. The anonymization forces models to rely solely on provided knowledge graph triplets rather than internal parametric knowledge. They introduce LLM-KAT, a QA-based evaluation procedure that transforms knowledge attachment measurement into entity extraction tasks, providing more reliable metrics than traditional span-based approaches. The method is tested on OpenDialKG dataset using various LLMs (Qwen, DeepSeek, QwQ) across different scales.

## Key Results
- Anonymization significantly improves LLM knowledge attachment, with F1 scores increasing by up to 5.04 points
- Larger LLMs show greater improvements from anonymization (7B models: +1.48 F1/Turn; 32B models: +3.57 F1/Turn)
- Response quality (Naturalness/Coherence) remains stable with slight decreases (-1.36 to -3.47) for most models
- LLM-KAT evaluation identifies 79% of synthetic unanswerable cases correctly, significantly outperforming traditional span-based metrics

## Why This Works (Mechanism)

### Mechanism 1: Entity Anonymization Blocks Parametric Knowledge Access
When LLMs recognize familiar entities, they retrieve associated knowledge from pretrained weights. Anonymization breaks this pattern by creating entities that have no representation in the model's parameters, forcing the model to attend solely to the provided knowledge graph triplets for reasoning.

### Mechanism 2: LLM-KAT Evaluation Transforms Attachment Measurement into QA Task
Instead of using BERT-based span extraction (which produces high false positive rates), LLM-KAT prompts a strong LLM to extract candidate entities from responses given a knowledge triplet. The LLM can identify cases where the context is insufficient ("IS_IMPOSSIBLE"), filtering out spurious matches before computing F1 scores.

### Mechanism 3: Scale-Dependent Benefits from Anonymization
Larger models store more comprehensive entity knowledge in their parameters. When entities are recognizable, this knowledge interferes more strongly with external knowledge integration. Anonymization disproportionately benefits larger models by equalizing the playing field—all models must then rely on the same limited external context.

## Foundational Learning

- **Concept: Knowledge Graph-Based Dialogue Generation (KG-DG)**
  - **Why needed here:** The entire paper frames the problem as generating dialogue responses conditioned on retrieved knowledge graph subgraphs. Understanding that knowledge is provided as structured triplets (subject, relation, object) is essential.
  - **Quick check question:** Given the triplet `(The Matrix, directed_by, Lana Wachowski)` and dialogue history mentioning "The Matrix," what entity should appear in the response if grounded to this knowledge?

- **Concept: Parametric vs. Contextual Knowledge in LLMs**
  - **Why needed here:** The core problem is LLMs preferring parametric knowledge (learned during pretraining) over contextual knowledge (provided in the prompt). Anonymization is a technique to suppress parametric access.
  - **Quick check question:** If an LLM generates "Tom Hanks is married to Julia Roberts" but the provided knowledge graph says `(Tom Hanks, spouse, Rita Wilson)`, which knowledge source did the model prioritize?

- **Concept: Entity Anonymization for Knowledge Conflict Control**
  - **Why needed here:** The paper's intervention is anonymization—replacing real entity names with identifiers. This technique has prior roots in privacy literature and in diagnosing LLM knowledge conflicts.
  - **Quick check question:** If you anonymize "Robert Downey Jr." as "P1" in both the dialogue history and knowledge triplets, what information can the LLM no longer access about this person from its pretrained weights?

## Architecture Onboarding

- **Component map:** Raw Dialogue + Knowledge Graph -> [LLM Anonymizer] -> Anonymized Dialogue + Anonymized KG Triplets -> [Target LLM] -> Generated Response -> [LLM-KAT Evaluator] -> F1 Score -> [UniEval] -> Naturalness/Coherence Scores

- **Critical path:** Anonymization quality is the bottleneck. Use a strong LLM (QwQ-32B in paper) with in-context examples to generate consistent entity mappings and handle variants. Prompt design matters for smaller models. Evaluation must filter unanswerable cases.

- **Design tradeoffs:** Attachment vs. Naturalness (anonymization improves F1 but slightly reduces quality). Anonymizer strength vs. cost (QwQ-32B is expensive). Zero-shot vs. fine-tuning (no adaptation to domain-specific entity patterns).

- **Failure signatures:** Detachment with real entities (model generates facts not in KG). Quality collapse after anonymization (Naturalness/Coherence drops >5%). LLM-KAT false negatives (evaluator marks valid answers as "IS_IMPOSSIBLE").

- **First 3 experiments:**
  1. Baseline LLM-KAT on normal data (establishes detachment baseline)
  2. Full anonymization pipeline (expects +1.5 to +5.0 F1 improvement)
  3. Partial anonymization ablation (validates anonymization drives gains)

## Open Questions the Paper Calls Out

- **Open Question 1:** How does entity anonymization perform on more complex or domain-specific KG-DG datasets beyond OpenDialKG?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that "our experiments were conducted solely on the OpenDialKG dataset, as more complex knowledge graph-based dialogue generation datasets are not currently available in literature."

- **Open Question 2:** Can lightweight or rule-based anonymization methods achieve comparable knowledge attachment rates to heavy LLM-based anonymizers like QwQ-32B?
  - **Basis in paper:** [explicit] The authors acknowledge that the approach "relies on a heavy model (QwQ-32B) as an expert anonymizer, which is costly and requires significant computational resources."

- **Open Question 3:** To what extent does forcing attachment via anonymization impact model performance when the provided external knowledge is flawed or noisy?
  - **Basis in paper:** [inferred] The methodology relies on the assumption that the model is given a "flawlessly retrieved knowledge graph."

## Limitations

- **Dataset Generalization:** All experiments are conducted on a single dataset (OpenDialKG) with movie domain knowledge, limiting generalizability to other domains.
- **Evaluator Reliability:** LLM-KAT evaluation depends entirely on the capability of the evaluating LLM, with ~21% false positives in synthetic cases.
- **Computational Cost:** The approach relies on a heavy model (QwQ-32B) as an expert anonymizer, which is computationally expensive.

## Confidence

- **High Confidence:** The core finding that anonymization improves knowledge attachment (F1 scores increase 1.48-5.04 points) is well-supported by multiple experimental conditions.
- **Medium Confidence:** The claim that larger models benefit more from anonymization is supported but could be influenced by other factors such as prompt sensitivity differences.
- **Medium Confidence:** The observation that anonymization slightly reduces response quality is supported but the effect size varies considerably across models and conditions.

## Next Checks

1. **Cross-Domain Validation:** Test the anonymization approach on at least two additional KG-DG datasets from different domains to verify generalization beyond the movie domain.

2. **Evaluator Ablation Study:** Systematically compare LLM-KAT results using different evaluating models to establish the sensitivity of attachment measurements to evaluator choice.

3. **Partial Anonymization Optimization:** Conduct a finer-grained analysis of partial anonymization (e.g., 25%, 50%, 75%) to determine if there's an optimal balance between knowledge attachment and response quality.