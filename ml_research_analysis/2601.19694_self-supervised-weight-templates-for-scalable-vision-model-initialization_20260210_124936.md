---
ver: rpa2
title: Self-Supervised Weight Templates for Scalable Vision Model Initialization
arxiv_id: '2601.19694'
source_url: https://arxiv.org/abs/2601.19694
tags:
- weight
- initialization
- templates
- vision
- sweet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SWEET introduces a self-supervised framework for scalable vision
  model initialization using Tucker-based weight templates. Instead of pre-training
  fixed-size models, SWEET learns a shared weight template and lightweight weight
  scalers under Tucker factorization, enabling flexible adaptation to architectures
  of varying depths and widths.
---

# Self-Supervised Weight Templates for Scalable Vision Model Initialization

## Quick Facts
- arXiv ID: 2601.19694
- Source URL: https://arxiv.org/abs/2601.19694
- Authors: Yucheng Xie; Fu Feng; Ruixiao Shi; Jing Wang; Yong Rui; Xin Geng
- Reference count: 17
- Primary result: SWEET improves average image classification accuracy by 1.60% across five model sizes and shows comparable gains in detection (2.04 AP), segmentation (2.76 mIoU), and generation (2.19 FID).

## Executive Summary
SWEET introduces a self-supervised framework for scalable vision model initialization using Tucker-based weight templates. Instead of pre-training fixed-size models, SWEET learns a shared weight template and lightweight weight scalers under Tucker factorization, enabling flexible adaptation to architectures of varying depths and widths. To enhance width expansion flexibility, SWEET incorporates width-wise stochastic scaling during pre-training, which regularizes the template along width-related dimensions and promotes robust, width-invariant representations. The self-supervised pre-training objective further enhances generalizability across diverse vision tasks. Extensive experiments demonstrate SWEET's state-of-the-art performance in initializing variable-sized models across classification, detection, segmentation, and generation tasks.

## Method Summary
SWEET learns a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths. All layer weights are concatenated into a unified matrix W ∈ R^{L×P}, then reconstructed via W = G ×₁X ×₂U ×₃V, where G is a low-rank core tensor acting as a bottleneck that filters non-transferable knowledge while scalers adapt to target dimensions. The framework incorporates width-wise stochastic scaling during pre-training via dropout on scaler dimensions, preventing overfitting to fixed width and forcing low-index template dimensions to capture width-invariant representations. SWEET uses MAE self-supervised pre-training to optimize the template and scalers, producing more generalizable templates than task-specific supervision. For initialization, the template G is frozen and scalers are adapted to target dimensions through slicing or light optimization (~0.16 epoch) before standard fine-tuning.

## Key Results
- SWEET improves average image classification accuracy by 1.60% across five model sizes (L3H3, L3H6, L3H12, L6H6, L12H12)
- SWEET shows comparable gains in detection (2.04 AP), segmentation (2.76 mIoU), and generation (2.19 FID) tasks
- Width-wise stochastic scaling improves accuracy across L3H3, L3H4, L3H5 configurations (e.g., 48.63 → 49.57 for L3H3)
- SWEET outperforms task-specific pre-training (WAVE) on cross-task transfer to detection and segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tucker-based factorization enables flexible depth and width scaling by decoupling size-agnostic knowledge (template G) from size-specific modulation (scalers U, V, X).
- Mechanism: All layer weights are concatenated into unified matrix W ∈ R^{L×P}, then reconstructed via W = G ×₁X ×₂U ×₃V. The low-rank core tensor G (with r₁×r₂×r₃ ≪ L×P) acts as a bottleneck that filters non-transferable knowledge while scalers adapt to target dimensions.
- Core assumption: Visual knowledge has a low-rank structure that can be compressed without losing transferability across scales.
- Evidence anchors:
  - [abstract] "we learn a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths"
  - [Section 3.2] "G serves as a universal Weight Template encoding size-agnostic knowledge, while the lightweight Weight Scalers (X, U, V) modulate its reuse and composition"
  - [corpus] Weak direct evidence; related work on weight initialization (Starting Positions Matter) and low-rank adapters (ConsNoTrainLoRA) supports low-rank factorization principles but not Tucker specifically for vision.

### Mechanism 2
- Claim: Self-supervised MAE pre-training produces more generalizable templates than task-specific supervision.
- Mechanism: Template G and scalers are optimized to reconstruct masked image patches (75% masking ratio), forcing the template to encode fundamental visual structures rather than task-specific patterns (e.g., classification boundaries).
- Core assumption: Masked reconstruction captures transferable visual priors that generalize across classification, detection, segmentation, and generation.
- Evidence anchors:
  - [abstract] "The self-supervised pre-training objective further enhances generalizability across diverse vision tasks"
  - [Section 3.3] "This indirect optimization decouples knowledge extraction from specific parameter values, regularizes the initialization space, and promotes the learning of universal and structural visual patterns"
  - [Section 5.1.3] "WAVE, trained only on classification, shows weaker transferability to detection and segmentation, suggesting the benefit of self-supervised templates for cross-task initialization"
  - [corpus] LeJEPA and related self-supervised methods support representation generality claims but don't directly validate for weight templates.

### Mechanism 3
- Claim: Width-wise stochastic scaling (dropout on scalers during pre-training) improves adaptation to arbitrary widths.
- Mechanism: Binary masks M_U, M_V randomly drop scaler dimensions during training (Û = M_U ⊙ U), preventing overfitting to fixed width and forcing low-index template dimensions to capture width-invariant representations.
- Core assumption: Width-robust features occupy lower-index positions in the template's factorized dimensions.
- Evidence anchors:
  - [abstract] "width-wise stochastic scaling during pre-training, which regularizes the template along width-related dimensions and promotes robust, width-invariant representations"
  - [Section 5.4.2, Table 5] Ablation shows stochastic scaling improves accuracy across L3H3, L3H4, L3H5 configurations (e.g., 48.63 → 49.57 for L3H3)
  - [corpus] No direct corpus evidence for width-wise dropout on weight scalers specifically.

## Foundational Learning

- Concept: **Tucker Decomposition**
  - Why needed here: Core mathematical structure for factorizing weight tensors into interpretable components.
  - Quick check question: Can you explain why Tucker is more flexible than Kronecker for heterogeneous component sharing?

- Concept: **Masked Autoencoders (MAE)**
  - Why needed here: Self-supervised objective that drives template learning without task-specific labels.
  - Quick check question: Why does 75% masking encourage learning transferable vs. task-specific features?

- Concept: **Low-Rank Bottlenecks**
  - Why needed here: Constrains template G to compress only transferable knowledge.
  - Quick check question: How does the low-rank constraint r₁×r₂×r₃ ≪ L×P act as a filter for non-transferable information?

## Architecture Onboarding

- Component map:
  - Weight Template G (r₁×r₂×r₃) -> Weight Scalers (X, U, V) -> Unified Weight Matrix W -> MAE Decoder

- Critical path:
  1. Pre-training: Optimize G + scalers jointly under MAE loss with stochastic scaling
  2. Initialization: Freeze G, adapt scalers to target dimensions (select slices or lightly optimize ~0.16 epoch)
  3. Fine-tuning: Train target model normally without constraints

- Design tradeoffs:
  - Template rank (r₁, r₂, r₃): Higher = more expressive but less efficient transfer; lower = stronger regularization but capacity ceiling
  - Stochastic scaling dropout rate: Controls width generalization vs. width-specific knowledge
  - Unified concatenation vs. component-specific templates: Unified enables cross-component sharing but may dilute specialized patterns

- Failure signatures:
  - Small models (e.g., L12H3) showing disproportionately low gains → template rank too high or stochastic scaling insufficient
  - Detection/segmentation underperforming classification → MAE pre-training may need domain augmentation
  - Scaler optimization not converging → learning rate too high or template rank too low for target complexity

- First 3 experiments:
  1. Baseline ablation: Compare SWEET vs. random initialization vs. WAVE on single scale (L6H6) for classification to validate template quality.
  2. Width scaling test: Initialize L3H3, L6H6, L12H12 from same template; plot accuracy vs. width to verify stochastic scaling effectiveness.
  3. Cross-task transfer: Zero-shot initialize (no scaler optimization) for detection and segmentation; measure gap vs. light scaler adaptation to quantify template generality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Tucker-based unified weight template formulation effectively generalize to architectures with strong spatial inductive biases, such as Convolutional Neural Networks (CNNs) or hierarchical Vision Transformers?
- Basis in paper: [inferred] The paper evaluates SWEET exclusively on standard Vision Transformers (ViT) with global attention. The method aggregates weights into a unified matrix W (Eq. 5), but it does not discuss how this factorization interacts with the localized weight sharing and spatial hierarchies inherent in CNNs or Swin Transformers.
- Why unresolved: The Tucker decomposition and unified matrix construction assume a level of structural homogeneity present in ViTs. It is unclear if the "size-agnostic knowledge" captured by the template is compatible with the rigid, spatially-constrained kernels of CNNs.
- What evidence would resolve it: Experimental results applying SWEET to standard CNN backbones (e.g., ResNet) and hierarchical Transformers, comparing initialization performance against standard random initialization and pre-trained baselines.

### Open Question 2
- Question: Does the low-rank constraint on the weight template G impose a representational bottleneck that limits the peak performance of significantly larger target models?
- Basis in paper: [inferred] Section 3.2 mentions imposing a low-rank constraint where r_1 × r_2 × r_3 ≪ L × P to filter non-transferable knowledge. While beneficial for efficient transfer, the paper does not analyze if this "bottleneck" restricts the representational capacity required when scaling up to models much larger than the pre-training backbone.
- Why unresolved: The experiments scale models up to roughly 44M parameters (L6H12) from a ViT-Base teacher. It remains unverified if the compressed template retains sufficient information to optimally initialize models with orders of magnitude more parameters (e.g., ViT-Large or Huge).
- What evidence would resolve it: A scaling curve analysis showing the performance gap between SWEET-initialized models and fully pre-trained models as the parameter count of the target model increases substantially beyond the pre-training scale.

### Open Question 3
- Question: How sensitive is the framework to the specific configuration of the rank dimensions (r_1, r_2, r_3) in the Tucker decomposition, and can these be determined automatically?
- Basis in paper: [inferred] The paper defines the rank constraints G ∈ R^{r_1 × r_2 × r_3} but does not provide a mechanism for selecting these hyperparameters. The performance likely depends on balancing the compression ratio against the retention of transferable features.
- Why unresolved: There is no discussion of an automated search or theoretical justification for the specific rank sizes used in the experiments. An open question is whether a one-size-fits-all rank configuration exists or if it requires tuning per task/architecture.
- What evidence would resolve it: An ablation study varying the rank dimensions and analyzing the trade-off between the storage efficiency of the template and the downstream task performance, or a proposed heuristic for auto-selecting ranks.

## Limitations
- Critical implementation details remain underspecified, including exact core tensor ranks, dropout rate for width-wise stochastic scaling, and downstream fine-tuning protocols
- The unified weight concatenation approach assumes homogeneous layer structures that may not generalize to heterogeneous architectures
- MAE pre-training has not been validated specifically for weight template learning, leaving questions about optimal masking ratios

## Confidence
- **High confidence**: Tucker factorization enables flexible scaling via modular template/scaler separation (Mechanism 1)
- **Medium confidence**: Self-supervised MAE pre-training produces more generalizable templates than task-specific supervision (Mechanism 2)
- **Medium confidence**: Width-wise stochastic scaling improves adaptation to arbitrary widths (Mechanism 3)

## Next Checks
1. Ablation on core tensor ranks: Systematically vary (r₁, r₂, r₃) and measure impact on transfer quality across model sizes to validate whether the low-rank bottleneck effectively filters non-transferable knowledge without overly constraining capacity.
2. Cross-architecture transferability test: Apply SWEET templates to a non-ViT architecture (e.g., ConvNeXt or Swin) to test whether the unified weight concatenation approach generalizes beyond homogeneous ViT structures.
3. Domain generalization evaluation: Evaluate SWEET templates on non-natural image datasets (medical imaging, satellite imagery, or synthetic data) to validate whether MAE-based pre-training captures sufficiently universal visual priors or overfits to natural image statistics.