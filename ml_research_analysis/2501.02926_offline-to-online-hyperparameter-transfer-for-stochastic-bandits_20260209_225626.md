---
ver: rpa2
title: Offline-to-online hyperparameter transfer for stochastic bandits
arxiv_id: '2501.02926'
source_url: https://arxiv.org/abs/2501.02926
tags:
- learning
- algorithm
- problem
- complexity
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of transferring hyperparameters
  from offline data to online stochastic bandit problems. The authors consider a multi-task
  setting where problem instances are drawn from an unknown distribution, and the
  learner has access to historical data from similar tasks.
---

# Offline-to-online hyperparameter transfer for stochastic bandits

## Quick Facts
- arXiv ID: 2501.02926
- Source URL: https://arxiv.org/abs/2501.02926
- Reference count: 40
- This paper provides theoretical bounds and practical algorithms for transferring hyperparameters from offline data to online stochastic bandit problems.

## Executive Summary
This paper addresses the problem of hyperparameter tuning for stochastic bandits using offline multi-task data. The authors develop a framework for transferring hyperparameters from historical data to new bandit tasks, providing both theoretical guarantees and practical algorithms. The core innovation uses derandomization to analyze the piecewise constant structure of the loss function, enabling efficient hyperparameter optimization even in continuous parameter spaces.

## Method Summary
The method involves three phases: an offline phase where historical task data is collected, a tuning phase where hyperparameters are optimized using critical point computation, and an online phase where the tuned algorithm is deployed. The key technical contribution is Algorithm 3, which recursively identifies critical points in the hyperparameter space where the optimal arm changes. This enables efficient empirical risk minimization by only evaluating the loss at these critical points rather than performing grid search over the continuous parameter space.

## Key Results
- Achieves O(n log T) log QD bound for UCB hyperparameter tuning with arbitrary distributions
- Provides inter-task sample complexity of Õ(n log T/ε²) and intra-task complexity of O(nT)
- Sharpens bounds to O(log KT) for Bernoulli/categorical rewards
- Demonstrates strong empirical performance on CIFAR-10/100 for neural network hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Derandomization Exposes Piecewise Constant Structure
Derandomizing arm rewards transforms the expected loss into a piecewise constant function of the hyperparameter. For UCB algorithms, the derandomized dual function has finitely many discontinuities where arm selection changes, with each piece corresponding to a fixed sequence of arm pulls. This structure enables efficient computation of critical points.

### Mechanism 2: Rademacher Complexity Bounds via Discontinuity Count
The sample complexity scales with log(QD), not the size of the hyperparameter space, because the loss is constant on each piece. This bounded Rademacher complexity enables uniform convergence guarantees even for continuous parameter spaces, with log QD = O(n log T) for UCB.

### Mechanism 3: Critical Point Computation Makes ERM Tractable
Empirical Risk Minimization over continuous hyperparameters is made tractable by evaluating loss only at critical points. Algorithm 3 recursively finds discontinuity points by detecting when different arms become optimal, achieving O(QD) expected runtime by visiting each piece once.

## Foundational Learning

- **Rademacher Complexity and Uniform Convergence**
  - Why needed here: The sample complexity argument hinges on bounding Rademacher complexity of the loss function class.
  - Quick check question: Can you explain why Rademacher complexity of O(log m / N) implies O(1/√N) sample complexity?

- **Multi-Armed Bandits and Regret Decomposition**
  - Why needed here: Understanding how UCB's exploration parameter affects arm selection is essential to see why critical points exist.
  - Quick check question: For 2-armed UCB, derive the condition on α where the selected arm switches at time t.

- **Data-Driven Algorithm Configuration Paradigm**
  - Why needed here: This work extends the framework from Bal20, BNVW17 to stochastic bandits with inter/intra-task distinctions.
  - Quick check question: How does this setting differ from online algorithm configuration where you learn during deployment?

## Architecture Onboarding

- **Component map:** Offline Phase -> Tuning Phase -> Online Phase
- **Critical path:**
  1. Estimate Q_D empirically from pilot runs
  2. Collect offline data with T_o ≥ T per task (or use sequential policy if n > Q_D)
  3. Run Algorithm 3 to enumerate critical points
  4. Return argmin over finite set {critical points} ∪ {boundaries}
- **Design tradeoffs:**
  - More offline tasks (larger N) → better generalization but higher data cost
  - Longer offline horizons (larger T_o) → more accurate loss estimation but slower offline phase
  - Assumption: Tasks are i.i.d. from D; distribution shift invalidates guarantees
- **Failure signatures:**
  - Learned α performs worse than theory-default (α=1): Check if task distribution has shifted or offline data insufficient
  - Runtime blowup in Algorithm 3: Q_D may be underestimated; add early stopping
  - Regret doesn't improve with more N: Check if hyperparameter truly matters for this task family
- **First 3 experiments:**
  1. **Synthetic 2-arm Bernoulli with small gap**: Replicate Figure 2 setting (p₁=0.5, p₂~0.51) with N=200, T_o=20; verify learned α > 1 (high exploration needed)
  2. **Ablation on N**: Plot test regret vs. number of offline tasks (as in Figure 5); confirm 1/√N decay
  3. **Estimate Q_D on your domain**: Run Algorithm 3 on 1000 random seeds; compare empirical Q_D to theoretical O(nT) bound to validate tractability

## Open Questions the Paper Calls Out

- **Open Question 1**: How can one strategically design offline data collection policies to minimize intra-task sample complexity? The paper uses simple policies but defers optimizing this to future work.

- **Open Question 2**: Can the theoretical intra-task complexity bounds (O(nT) for general distributions, O(QD·T) otherwise) be tightened? Current bounds may be loose compared to empirical observations.

- **Open Question 3**: How do the transfer learning guarantees degrade under distribution shift between offline and online tasks? The paper assumes i.i.d. tasks but practical applications often involve non-stationarity.

## Limitations

- Theoretical bounds rely on piecewise constant structure which may not hold for non-i.i.d. reward distributions, potentially leading to exponential growth in critical points
- Empirical evaluation focuses on relatively simple bandit structures without extensively testing distribution shift scenarios
- Critical point computation assumes full counterfactual information, which may be impractical in pure bandit feedback settings

## Confidence

- **High confidence**: The derandomization mechanism and piecewise constant structure analysis
- **Medium confidence**: The Rademacher complexity bounds and sample complexity guarantees
- **Medium confidence**: The practical critical point computation and its runtime guarantees

## Next Checks

1. **Test Q_D scaling empirically**: For a 3-armed Bernoulli bandit with varying gaps, measure the actual number of critical points across multiple random seeds and compare to the O(n log T) theoretical bound.

2. **Evaluate distribution shift robustness**: Take the CIFAR-10 dataset and artificially shift the task distribution between offline and online phases. Measure how quickly the learned hyperparameter degrades.

3. **Validate importance weighting**: Implement the importance-weighted estimator for the case where offline rewards are only available for the arm that would have been played. Compare its performance to the full information assumption.