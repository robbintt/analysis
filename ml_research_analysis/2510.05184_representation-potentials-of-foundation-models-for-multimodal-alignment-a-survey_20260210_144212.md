---
ver: rpa2
title: 'Representation Potentials of Foundation Models for Multimodal Alignment: A
  Survey'
arxiv_id: '2510.05184'
source_url: https://arxiv.org/abs/2510.05184
tags:
- representations
- arxiv
- learning
- alignment
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey investigates the representation potentials of foundation
  models, defined as their capacity to capture task-specific information within a
  single modality while providing a transferable basis for alignment and unification
  across modalities. The study reviews representative foundation models in vision,
  language, speech, and multimodality, and synthesizes empirical evidence from studies
  in vision, language, speech, multimodality, and neuroscience.
---

# Representation Potentials of Foundation Models for Multimodal Alignment: A Survey

## Quick Facts
- **arXiv ID:** 2510.05184
- **Source URL:** https://arxiv.org/abs/2510.05184
- **Authors:** Jianglin Lu; Hailing Wang; Yi Xu; Yizhou Wang; Kuo Yang; Yun Fu
- **Reference count:** 37
- **Primary result:** Foundation models exhibit structural regularities and semantic consistencies in their representation spaces, enabling cross-modal transfer and alignment through scale, architectural biases, training objectives, and task diversity.

## Executive Summary
This survey investigates the representation potentials of foundation models—their capacity to capture task-specific information within a single modality while providing a transferable basis for alignment and unification across modalities. The authors review representative foundation models in vision, language, speech, and multimodality, synthesizing empirical evidence from diverse fields including neuroscience. They find that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. Key factors fostering representation potentials include scale, architectural inductive biases, training objectives, and task and instruction diversity.

## Method Summary
The survey synthesizes empirical evidence from studies in vision, language, speech, multimodality, and neuroscience to characterize foundation model representation potentials. The core methodology involves extracting representation matrices from frozen pretrained models (e.g., ViT, LLaMA, CLIP) on paired datasets, then computing similarity metrics including Centered Kernel Alignment (CKA), Canonical Correlation Analysis (CCA), and Mutual Nearest Neighbors (MNN) to assess geometric convergence. The survey also examines transfer performance by fitting linear mappings between modalities and evaluating downstream task performance. The analysis focuses on identifying conditions under which representations converge toward shared statistical models of reality regardless of modality.

## Key Results
- Foundation models exhibit structural regularities and semantic consistencies in their representation spaces across modalities
- Scale, architectural inductive biases, training objectives, and task diversity are key factors fostering representation potentials
- Representation alignment is layer-dependent, with deeper layers converging toward modality-agnostic abstractions while early layers remain modality-specific
- Self-supervised objectives encourage formation of representations that are linearly alignable across modalities

## Why This Works (Mechanism)

### Mechanism 1
Increasing model scale and data diversity correlates with convergence toward a shared statistical model of reality, regardless of modality. Large-scale pretraining on heterogeneous data forces models to compress information into robust, general-purpose abstractions. As capacity increases, the solution space for representing "world structure" shrinks, causing independently trained models (and even different modalities) to settle on geometrically similar representational geometries (the "Platonic Representation Hypothesis"). Core assumption: The underlying structure of the world exhibits consistent statistical regularities recoverable from any sufficiently rich sensory input. Break condition: Convergence may fail if training data is narrow, biased, or if the model lacks sufficient capacity.

### Mechanism 2
Self-supervised objectives (e.g., next-token prediction, contrastive learning) encourage formation of representations that are linearly alignable across modalities. These objectives optimize for semantic consistency and relational invariance rather than pixel-perfect reconstruction, forcing models to organize concepts based on meaning and creating a "semantic structure" in the latent space that is approximately isomorphic across vision and language, allowing a simple linear map to connect them. Core assumption: High-level semantic concepts share similar relational structure regardless of whether perceived visually or described textually. Break condition: Alignment degrades if training objectives incentivize modality-specific idiosyncrasies that do not map to the other modality.

### Mechanism 3
Representation alignment is layer-dependent, with deeper layers converging toward modality-agnostic abstractions while early layers remain modality-specific. Early layers process raw sensory signals (pixels vs. waveforms), which are physically distinct. As information flows through the network, layers progressively discard modality-specific details to encode high-level, task-relevant abstractions (semantics), resulting in higher similarity in middle-to-deep layers. Core assumption: Task-relevant abstractions are inherently amodal. Break condition: Highly specialized models may maintain domain-specific divergence even in deep layers.

## Foundational Learning

- **Concept: Representational Similarity Metrics (CKA/CCA)**
  - Why needed here: To quantify "alignment" qualitatively. You cannot optimize or verify alignment without a numerical definition of similarity between two high-dimensional embedding spaces.
  - Quick check question: Can you explain why CKA is preferred over simple cosine similarity when comparing two different neural network architectures? (Hint: Invariance to orthogonal transformations).

- **Concept: Latent Space Geometry**
  - Why needed here: Understanding that "alignment" is not just about individual points but the shape of the manifold. The survey posits that models converge on the shape of their representation space (structure of reality).
  - Quick check question: If two models have high CKA similarity, what does that imply about their ability to classify the same subset of data?

- **Concept: Modality-Agnostic Abstraction**
  - Why needed here: This is the theoretical goal of multimodal alignment—finding a shared "language" of representation independent of whether input was image, text, or audio.
  - Quick check question: According to the paper, at what depth in a network do you typically expect to find the highest degree of modality-agnostic abstraction?

## Architecture Onboarding

- **Component map:** Unimodal Encoders -> Representation Extractor -> Alignment Module -> Fusion Head
- **Critical path:** Extracting embeddings -> Centering data (critical for CKA/CCA) -> Computing Similarity OR Training Linear Map -> Evaluating Transfer Performance
- **Design tradeoffs:**
  - Metric Selection: Use CKA for global structural similarity (robust to scale/rotation), but use Mutual Nearest Neighbors (MNN) if preserving local semantic neighborhoods is more critical for retrieval tasks
  - Layer Selection: Aligning early layers is computationally cheaper but less effective for semantic tasks; deep layers align better but may lose fine-grained detail
- **Failure signatures:**
  - High CKA, Low Performance: The spaces share global structure but the specific dimensions relevant to your downstream task are rotated or inverted relative to each other
  - Modality Collapse: The alignment procedure forces one modality to dominate, causing the model to hallucinate visual features based on text priors
- **First 3 experiments:**
  1. Layer-wise Similarity Scan: Run CKA between a frozen ViT (e.g., DINOv2) and a frozen LLM (e.g., Llama) across all layer pairs to identify the "sweet spot" of highest alignment
  2. Linear Stitching Test: Train a single linear layer to map ViT embeddings to LLM embedding space and evaluate zero-shot classification performance on a held-out dataset
  3. Scale Ablation: Repeat Experiment 1 with smaller vs. larger variants of the same model family to observe if alignment scores increase with scale

## Open Questions the Paper Calls Out

### Open Question 1
What are the fundamental limits of convergence for representations across modalities with distinct physical properties, such as vision and language? The authors explicitly discuss "the limits of convergence across modalities," noting that distinct sensors capture complementary aspects of reality and that "full convergence to a single, identical representation... is neither achievable nor necessarily desirable." This remains unresolved because while evidence suggests structural regularities, it remains unclear where modality-specific constraints (e.g., spatial detail in vision vs. abstract relations in language) prevent formation of a fully unified representation space. Resolution would require a theoretical framework or empirical mapping that defines which semantic concepts are universally transferable and which are inextricably bound to their specific sensory modality.

### Open Question 2
How can the field establish robust, standardized evaluation metrics that distinguish between geometric similarity and functional equivalence in representation alignment? The survey highlights the "need for robust evaluation standards," pointing out that high similarity scores (e.g., CKA, CCA) do not guarantee models can decode information similarly, leading to questions about "what alignment scores truly capture." This remains unresolved because current metrics often produce ambiguous results where high scores can mask significant underlying geometric differences, complicating cross-study comparisons. Resolution would require development of a universal benchmark where metric scores correlate predictably with performance on downstream functional tasks across diverse architectures.

### Open Question 3
Do specialized foundation models (e.g., for robotics or narrow tasks) converge toward shared representations, or do they diverge into unique representational manifolds? The authors highlight "cases where domain-specific divergence may arise," specifically noting that highly specialized models or those in domains like robotics may develop unique representations that diverge from general-purpose abstractions. This remains unresolved because most evidence for alignment comes from general-purpose vision and language models; behavior of representations in specialized sensorimotor or niche domains is currently fragmented and under-explored. Resolution would require empirical studies demonstrating that models trained on narrow, specialized tasks fail to align with general foundation models via standard linear or affine mappings.

## Limitations

- Empirical evidence for linear alignment across all modality pairs is primarily derived from vision-language systems, with limited validation for speech and other modalities
- Claims about layer-dependent alignment patterns are based on existing literature rather than direct experimental validation across diverse model families
- Mechanisms proposed assume sufficient model capacity and data diversity, but exact thresholds where these conditions fail remain poorly characterized

## Confidence

- **High Confidence:** The observation that larger models with more diverse training data exhibit stronger representation convergence (Mechanism 1) is well-supported by multiple independent studies
- **Medium Confidence:** The claim that self-supervised objectives encourage linearly alignable representations (Mechanism 2) is supported by evidence but may not generalize to all training objectives or architectural choices
- **Medium Confidence:** The layer-dependent alignment hypothesis (Mechanism 3) is consistent with known deep learning principles but requires more systematic validation across different model scales and architectures

## Next Checks

1. **Cross-Modal Linear Transfer Validation:** Systematically test linear alignment across vision, language, and speech models using standardized datasets and metrics to verify if alignment holds beyond vision-language pairs

2. **Layer-Wise Convergence Analysis:** Conduct controlled experiments varying model depth, width, and architecture to precisely characterize when and how layer-dependent alignment patterns emerge

3. **Capacity Threshold Determination:** Empirically identify the minimum model capacity and data diversity requirements where representation convergence fails, establishing concrete bounds for the proposed mechanisms