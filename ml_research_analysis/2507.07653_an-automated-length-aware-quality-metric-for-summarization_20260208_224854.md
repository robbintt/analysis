---
ver: rpa2
title: An Automated Length-Aware Quality Metric for Summarization
arxiv_id: '2507.07653'
source_url: https://arxiv.org/abs/2507.07653
tags:
- summarization
- metric
- summary
- length
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NOIR, a parameter-free metric for evaluating
  summarization quality by measuring the tradeoff between semantic retention and length
  compression. NOIR uses semantic embeddings and token counts to calculate a quality
  score that increases with better semantic preservation and greater compression.
---

# An Automated Length-Aware Quality Metric for Summarization

## Quick Facts
- **arXiv ID**: 2507.07653
- **Source URL**: https://arxiv.org/abs/2507.07653
- **Reference count**: 6
- **Primary result**: Introduces NOIR metric that achieves mean score 4.55 for real summaries vs 0.35 for random pairings, with r=0.36 correlation to human judgment

## Executive Summary
This paper introduces NOIR, a parameter-free metric for evaluating summarization quality by measuring the tradeoff between semantic retention and length compression. NOIR uses semantic embeddings and token counts to calculate a quality score that increases with better semantic preservation and greater compression. Experiments show NOIR effectively distinguishes good summaries from unrelated text pairings, achieving a mean score of 4.55 compared to near-zero for random pairings. The metric correlates positively (r=0.36) with human judgments of summary quality and remains stable across different embedding models. NOIR provides an automated, objective, and efficient tool for evaluating and improving summarization algorithms without requiring reference summaries.

## Method Summary
NOIR computes a dimensionless ratio measuring the tradeoff between semantic retention and length compression. The metric takes original text and candidate summary as inputs, computes their cosine similarity using sentence embeddings, counts tokens for both texts, and applies the formula M_NOIR = ln(T_summary/T_original) / ln(D_semantic). The method requires no training and works with any embedding model that satisfies basic calibration requirements: random text pairs should have near-zero similarity and paraphrase pairs should have high similarity. The paper uses all-MiniLM-L6-v2 and all-mpnet-base-v2 embedders with tiktoken for token counting.

## Key Results
- Real summaries achieve mean NOIR score of 4.55 ± 0.06 while random text pairings score 0.35
- Human evaluation shows positive correlation (r=0.36) between NOIR scores and quality judgments
- NOIR distributions remain well-separated across different embedding models and power parameter values
- The metric correctly penalizes summaries longer than originals (negative scores) and identifies compression-only summaries with low semantic retention

## Why This Works (Mechanism)

### Mechanism 1: Logarithmic Compression-to-Retention Ratio
NOIR captures summarization quality by computing a dimensionless ratio of token compression to semantic degradation. The metric M_NOIR = ln(T_summary/T_original) / ln(D_semantic) treats summarization as an iterative process where each "step" reduces tokens by factor k and semantic similarity by factor D. Better summarizers have higher D/k ratios, making the logarithmic ratio sensitive to quality differences. The log-transform converts multiplicative degradation into an additive scale that can be measured without knowing the underlying step count.

### Mechanism 2: Cosine Similarity Approximates Multiplicative Semantic Loss
Cosine similarity between sentence embeddings serves as a valid proxy for semantic degradation that behaves multiplicatively across compression steps. In high-dimensional embedding space, random semantic "drift" directions accumulate orthogonally (√N behavior). For small angular deviations, cos(α√N) ≈ exp(-c·N) over the relevant range (similarity 0.2–1.0). This exponential approximation justifies treating cosine similarity as multiplicative degradation D.

### Mechanism 3: Quality Metric Separation via Threshold Detection
NOIR produces bimodal distributions that separate true summaries from random text pairings. Genuine summaries maintain semantic similarity despite compression (M_NOIR ≈ 4.55 ± 2.08), while random pairings have near-zero semantic similarity regardless of length ratio (M_NOIR ≈ 0.35). The 2+ σ separation enables threshold-based filtering without reference summaries.

## Foundational Learning

- **Concept: Sentence/Document Embeddings**
  - Why needed here: NOIR's entire semantic measurement depends on understanding how neural models map text to dense vectors and what cosine similarity represents.
  - Quick check question: Given two texts with cosine similarity 0.85, would you expect them to be paraphrases, related topics, or unrelated? What about 0.15?

- **Concept: Multiplicative Processes and Logarithms**
  - Why needed here: The metric assumes degradation compounds multiplicatively; the log transform is what makes this measurable without knowing iteration count.
  - Quick check question: If a summarizer retains 90% semantic similarity per halving of length, what would be the semantic similarity after three halvings? What would NOIR compute?

- **Concept: High-Dimensional Geometry (Curse/Blessing of Dimensionality)**
  - Why needed here: Understanding why random directions become orthogonal in high dimensions explains why the multiplicative approximation works.
  - Quick check question: In a 368-dimensional space, why would random "semantic drift" steps tend to be perpendicular to each other rather than aligned?

## Architecture Onboarding

- **Component map**:
  Input: (original_text, candidate_summary)
  [Tokenizer] → token counts T_original, T_summary
  [Sentence Embedder] → vectors v_original, v_summary (384-dim for MiniLM, 768-dim for mpnet)
  [Cosine Similarity] → D = v_summary · v_original
  [NOIR Calculator] → M = ln(T_summary/T_original) / ln(D)
  Output: scalar quality score

- **Critical path**: Embedding model selection is the dominant factor. Must verify: random text pairs → similarity ≈ 0 (not 0.4–0.7 as in some models), paraphrase pairs → similarity ≈ 0.9+, max token limit accommodates your texts (512 for MiniLM; truncation silently degrades results).

- **Design tradeoffs**:
  - Speed vs. quality: all-MiniLM-L6-v2 (faster, 384-dim) vs. all-mpnet-base-v2 (slower, 768-dim, modestly higher NOIR values, ρ=0.55 correlation between them)
  - Tokenizer choice: Paper uses tiktoken but notes ±20% variance across tokenizers is acceptable since ratios cancel; ensure consistency within evaluation runs
  - Power parameter p: Paper tests M = -ln(|T/T₀|)^p / ln(D); finds p=1.0 maximizes separation (Figure 10); deviating from p=1 reduces discriminability

- **Failure signatures**:
  - NOIR ≈ 0 for genuine summaries → embedding model fails calibration test (random pairs not near 0)
  - NOIR < 0 → summary is longer than original; metric correctly penalizes
  - High variance across compression bins → potential content-domain interaction; may need domain-specific thresholds
  - Embedding truncation warnings → text exceeds 512 tokens; semantic measurement incomplete

- **First 3 experiments**:
  1. Calibration test: Embed 50 random text pairs from your domain; compute cosine similarity distribution. Verify mean ≈ 0 and σ < 0.2. If failed, select different embedding model.
  2. Paraphrase test: Generate 20 paraphrases (manual or LLM-assisted); verify similarity > 0.9. Then create 20 summaries at different compression ratios (0.2–0.8); plot similarity vs. compression ratio. Expect monotonic decrease.
  3. Threshold calibration: Create evaluation set with known good/bad summaries (human-labeled or intentionally corrupted). Sweep NOIR threshold to maximize F1 separation. Paper suggests mean NOIR ≈ 4.5 for good summaries, but domain-specific thresholds may differ.

## Open Questions the Paper Calls Out

- How robust is the NOIR metric when applied to documents with long input lengths that exceed the standard token limits of current embedding models?
- Is there a mathematical operator superior to cosine similarity for measuring semantic degradation that avoids violating non-negativity constraints?
- Does the positive correlation between NOIR and human evaluation hold across larger, statistically significant datasets?
- To what extent does the content or topic of the source text influence the absolute NOIR score?

## Limitations

- The metric depends critically on embedding models maintaining near-perfect calibration (random pairs → similarity ≈ 0), a property not universal across all sentence embedders
- The multiplicative degradation assumption may not hold for all domains or summarization styles, potentially limiting generalizability
- The human evaluation sample size (15 texts) is too small to confirm correlation stability across diverse summarization styles

## Confidence

- **High confidence**: Empirical separation between summary and random pairings; human correlation results; stability across embedding models
- **Medium confidence**: Theoretical derivation from idealized summarizer model; the exponential approximation of cosine similarity; threshold detection mechanism
- **Low confidence**: Multiplicative degradation assumption across diverse domains; isotropy of embedding spaces; absence of pathological embedding behaviors in all use cases

## Next Checks

1. **Domain transferability test**: Apply NOIR to summaries from a domain with fundamentally different semantic structure (e.g., legal documents, code, poetry). Verify that the embedding model maintains the required 0-1 calibration boundaries and that the multiplicative approximation holds for this domain's semantic drift patterns.

2. **Alternative degradation models**: Generate synthetic summaries where semantic degradation follows polynomial or threshold-based patterns rather than geometric decay. Compare NOIR's discriminative power against these patterns to test whether the metric's sensitivity to quality differences is robust to the assumed degradation model.

3. **Embedding model stress test**: Systematically evaluate NOIR across 10+ sentence embedders with varying architectures (BERT variants, contrastive models, domain-specific encoders). Document which models satisfy the calibration requirements and whether the power parameter p=1.0 remains optimal across this diverse set.