---
ver: rpa2
title: Revisiting the Relation Between Robustness and Universality
arxiv_id: '2510.19427'
source_url: https://arxiv.org/abs/2510.19427
tags:
- similarity
- robustness
- representations
- robust
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits the modified universality hypothesis (Jones
  et al., 2022) that adversarially robust models are highly similar. We conduct extensive
  empirical studies using multiple similarity measures, architectures, and datasets.
---

# Revisiting the Relation Between Robustness and Universality

## Quick Facts
- arXiv ID: 2510.19427
- Source URL: https://arxiv.org/abs/2510.19427
- Reference count: 40
- Key outcome: Extensive empirical studies challenge the universality of adversarially robust models: predictions do not converge with increasing robustness, and representational similarity is not consistently higher across datasets.

## Executive Summary
This paper challenges the Modified Universality Hypothesis (Jones et al., 2022) that adversarially robust models are highly similar. Through extensive empirical studies across multiple datasets (ImageNet1k, ImageNet100, CIFAR-10), architectures (ResNet, Wide ResNet, ResNeXt, DenseNet, VGG), and similarity measures, the authors demonstrate that robust models exhibit partial rather than strict universality. While adversarial robustness constrains internal representations (making them mechanistically similar), it does not guarantee convergence in predictive behavior or overall representational similarity. The authors show that predictive differences originate in the classification layer, which can be aligned through retraining, highlighting that some aspects of robust models are constrained while others remain divergent.

## Method Summary
The authors conduct experiments using adversarially trained models with varying L2 robustness parameters (ε ∈ {0, 0.25, 0.5, 1.0, 3.0}) across multiple architectures and datasets. They measure predictive similarity using Agreement rate and JSDSim, and representational similarity using CKA, ProcrustesSim, Jaccard (k=10 neighbors), and negative RTD. To probe mechanistic similarity, they generate inverted images by optimizing seed images to match target representations. The key intervention involves freezing robust model trunks and retraining linear classifiers to test whether predictive similarity can be artificially induced. For ImageNet1k, they use pretrained checkpoints from Salman et al. (2020) and train additional models with specified adversarial training parameters.

## Key Results
- Predictive similarity does not increase with robustness: Models with different architectures trained robustly do not converge in their predictions despite using similar representations.
- Mechanistic similarity shows mixed results: Using inverted images as inputs reveals consistent positive correlation with robustness on ImageNet but not on CIFAR-10.
- Representational similarity can be misleading: Global measures like CKA may show high similarity even when models disagree on specific instances, with disagreeing instances sometimes showing higher representational similarity than agreeing ones.
- Classification layer divergence: Predictive behavior differences originate in the final classification layer, but retraining classifiers on robust representations can improve predictive similarity.

## Why This Works (Mechanism)

### Mechanism 1
Adversarial robustness constrains the internal representation mechanism (feature extraction), but leaves the final classification layer divergent. Adversarial training forces models to ignore non-robust features, pushing different architectures to rely on similar "robust" features, making their internal representations converge. However, this convergence stops at the penultimate layer; the final linear classifier remains highly dependent on initialization or minor training dynamics, leading to different predictions. Inverted images successfully isolate the specific features a model uses, providing a truer measure of "mechanistic" similarity than standard inputs. If robust training does not reduce the reliance on non-robust features (e.g., via gradient masking), mechanistic similarity would likely not increase.

### Mechanism 2
Representational similarity metrics (like CKA) can be "global" proxies that fail to capture local behavioral divergence. Measures like CKA or Procrustes average similarity over the entire dataset. They may report high global similarity even if the models disagree on specific subsets of data (low agreement) or if the local neighborhood topology of representations differs significantly. High global representational alignment implies that similar downstream mappings (classifiers) should be possible, which this paper refutes. If similarity measures were strictly localized or strictly predictive (e.g., measuring mutual information of outputs), they would not show this disconnect.

### Mechanism 3
Predictive universality can be artificially induced by retraining the classification head on frozen robust representations. Since robust representations from different models are semantically aligned, a new classifier trained on these representations learns a consistent mapping to labels. This overrides the divergent weights learned during the original end-to-end optimization. The robust representations contain sufficient signal for the classification task despite the accuracy trade-off often associated with robustness. If the robust representations were collapsed or too entangled, a linear probe would fail to recover accuracy or agreement.

## Foundational Learning

- **Concept: Adversarial Robustness (L2)**
  - **Why needed here:** The paper investigates robustness not as a security feature, but as a prior that simplifies the learned function. Understanding that high ε constrains the model to learn smoother, more human-aligned features is essential.
  - **Quick check question:** Does increasing the robustness parameter ε typically increase or decrease the model's standard accuracy on clean data?

- **Concept: Representational Similarity Metrics (CKA, Procrustes)**
  - **Why needed here:** The paper relies on distinguishing between "mechanistic" similarity (using inverted images) and "regular" similarity. One must understand that CKA measures global alignment of feature matrices, ignoring the specific readout (classifier).
  - **Quick check question:** If two models have a CKA similarity of 0.9, does that guarantee they predict the same class for a given image?

- **Concept: Image Inversion / Model Metamers**
  - **Why needed here:** This is the diagnostic tool used to reveal "mechanistic" similarity. It isolates features the model actually uses by optimizing a seed image to match a target representation.
  - **Quick check question:** Why is a seed image modified to match a target representation (inversion) better for comparing models than just using the target image directly?

## Architecture Onboarding

- **Component map:**
  - Input -> Feature Extractor (Trunk) -> Classifier (Head) -> Output
  - Similarity Suite: CKA (Global), Procrustes (Metric), Jaccard (Local), RTD (Topological)
  - Inversion Engine: Optimization loop to generate "mechanistic" inputs (inverted images)

- **Critical path:**
  1. Train/Load models with varying L2 robustness (ε)
  2. Generate inverted images to probe mechanistic similarity
  3. Measure Representational Similarity (Penultimate Layer) vs. Predictive Similarity (Softmax Outputs)
  4. Intervention: Retrain linear probes on frozen trunks to verify if the trunk is truly universal

- **Design tradeoffs:**
  - Strict Universality vs. Partial Universality: The paper argues against strict universality. Accept that while trunks converge, heads will not without explicit intervention (probes)
  - Dataset Selection: Do not assume ImageNet results transfer to CIFAR-10; the paper shows universality is dataset-dependent

- **Failure signatures:**
  - High CKA, Low Agreement: The classic signature of the "Modified Universality" failure. Indicates the trunk is similar but the head is divergent
  - High Robustness, Low Similarity: On datasets like CIFAR-10, increasing robustness does not guarantee increased representational similarity

- **First 3 experiments:**
  1. Reproduce the Disconnect: Take two robust models (ε=3) of different architectures. Compute CKA (expect high) and Agreement (expect low) on standard ImageNet validation
  2. The Probe Test: Freeze the trunks of the models from Experiment 1. Train new linear classifiers on top. Verify if Agreement increases significantly (validating the classifier as the divergence point)
  3. Mechanistic Check: Generate inverted images for a target class using Model A. Feed these images into Model B. Check if Model B recognizes the class (tests if they use the same features)

## Open Questions the Paper Calls Out

- **Open Question 1:** Why are some aspects of robust models strongly constrained by adversarial robustness while others are not?
  - The authors state in the Discussion that "the contrast points towards an interesting direction for future work: why is it that some aspects of models seem to be strongly constrained by robustness whereas others are not?"
  - The paper empirically observes "partial universality" but lacks a theoretical explanation for why robustness acts as a strong prior for features but fails to align the classification layer.
  - A theoretical framework or ablation study identifying the specific mechanics of adversarial training that isolate feature extraction layers from the classification head would resolve this.

- **Open Question 2:** Why do instances with disagreeing predictions exhibit higher representational similarity than instances with agreeing predictions?
  - The Discussion asks, "how can unintuitive results be explained like higher representational similarity for disagreeing instances compared to similarity of agreeing instances?"
  - This counter-intuitive finding suggests global similarity measures may capture statistical artifacts rather than functional alignment, but the geometric or statistical cause is not identified.
  - An analysis of the representation space geometry that explains how distinct decision boundaries can exist within nearly identical representational manifolds would resolve this.

- **Open Question 3:** How can we systematically identify which components of a neural network are universal versus non-universal?
  - The authors conclude that "identifying universal components is an interesting direction of future work" to distinguish parts suitable for general interpretability research.
  - The study shows universality is not strict (it applies to representations/mechanisms but not final predictions), but offers no method to isolate the "universal parts" a priori.
  - A diagnostic tool or metric capable of labeling specific layers or sub-networks as universal across different robust models and architectures would resolve this.

## Limitations
- Dataset Specificity: The paper shows dataset-dependent behavior in universality, but the exact mechanisms driving this difference remain unclear, particularly for CIFAR-10 where mechanistic similarity does not consistently correlate with robustness.
- Metric Interactions: The paper demonstrates that global representational measures can mask local behavioral differences, but doesn't fully characterize when or why this occurs, particularly regarding the geometric causes of the disagreeing-agreeing similarity paradox.
- Retraining Assumptions: While retraining classifiers on frozen robust representations improves predictive similarity, the paper doesn't investigate whether this transfers to out-of-distribution data or whether the retrained classifiers generalize beyond the validation set.

## Confidence
- High Confidence: The core finding that predictive behavior differences originate in the classification layer (supported by the linear probe intervention experiment)
- Medium Confidence: The claim that representational similarity measures can be misleading when used in isolation (supported by the agreement-CKA disconnect in Figure 5)
- Medium Confidence: The observation that mechanistic similarity correlates with robustness on ImageNet but not consistently on CIFAR-10 (limited by unspecified inversion optimization parameters)

## Next Checks
1. Reproduce the Disconnect: Take two robust models (ε=3) of different architectures. Compute CKA (expect high) and Agreement (expect low) on standard ImageNet validation.
2. The Probe Test: Freeze the trunks of the models from Experiment 1. Train new linear classifiers on top. Verify if Agreement increases significantly (validating the classifier as the divergence point).
3. Mechanistic Check: Generate inverted images for a target class using Model A. Feed these images into Model B. Check if Model B recognizes the class (tests if they use the same features).