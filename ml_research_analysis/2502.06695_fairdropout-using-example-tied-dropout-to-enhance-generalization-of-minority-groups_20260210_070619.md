---
ver: rpa2
title: 'FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority
  Groups'
arxiv_id: '2502.06695'
source_url: https://arxiv.org/abs/2502.06695
tags:
- spurious
- fairdropout
- group
- neurons
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairDropout, a method that improves generalization
  for minority groups in datasets with spurious correlations. The authors observe
  that models trained with empirical risk minimization (ERM) tend to memorize minority
  group examples, leading to poor worst-group accuracy.
---

# FairDropout: Using Example-Tied Dropout to Enhance Generalization of Minority Groups

## Quick Facts
- arXiv ID: 2502.06695
- Source URL: https://arxiv.org/abs/2502.06695
- Reference count: 15
- Improves worst-group accuracy on 4 of 5 benchmark datasets without requiring group labels during training

## Executive Summary
This paper introduces FairDropout, a method that improves generalization for minority groups in datasets with spurious correlations. The authors observe that models trained with empirical risk minimization (ERM) tend to memorize minority group examples, leading to poor worst-group accuracy. Building on this insight, FairDropout applies example-tied dropout to redirect memorization to specific neurons, which are then dropped during inference. This approach redistributes the "memorization burden" more fairly across examples, improving worst-group accuracy without requiring group labels during training. The method is evaluated on a benchmark suite spanning vision, language, and healthcare tasks, including CelebA, MetaShift, Waterbirds, MultiNLI, and MIMIC-CXR. FairDropout consistently outperforms ERM and state-of-the-art methods for handling spurious correlations and class imbalance, achieving the highest worst-group accuracy on four of the five datasets. Notably, FairDropout can be combined with classifier retraining techniques like DFR, further boosting performance. The results demonstrate that FairDropout effectively reduces reliance on spurious features and enhances robustness to subpopulation shifts.

## Method Summary
FairDropout introduces example-tied dropout to redistribute memorization burden across examples in datasets with spurious correlations. The method partitions neurons into generalizing (fraction p_gen) and memorizing (fraction 1-p_gen) sets. Each example samples a fixed subset of memorizing neurons (probability p_mem) during training. At inference, all memorizing neurons are dropped, forcing the model to rely on generalizing features. This approach improves worst-group accuracy without requiring group labels, as memorization is fairly distributed rather than concentrated on minority examples. The method is evaluated across vision, language, and healthcare benchmarks, consistently outperforming ERM and competing methods.

## Key Results
- Achieves highest worst-group accuracy on 4 of 5 benchmark datasets (CelebA, MetaShift, MultiNLI, MIMIC-CXR)
- Outperforms ERM by 30+ percentage points on CelebA (from ~45% to ~75% worst-group accuracy)
- Maintains or improves average accuracy while boosting worst-group performance
- Can be combined with classifier retraining (DFR) for additional gains, achieving best results on 4/5 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minority-group examples are disproportionately memorized by ERM-trained models, relying on fewer neurons that hurt generalization.
- Mechanism: ERM minimizes average loss, so majority groups dominate gradient updates. Minority examples—being atypical under the training distribution—are "memorized" via sparse, example-specific neurons rather than robust features. These neurons encode spurious correlations and flip easily when removed, indicating weak generalization.
- Core assumption: Memorization can be localized to a subset of neurons whose removal disproportionately affects minority-group predictions without harming majority performance.
- Evidence anchors:
  - [abstract] "models trained with empirical risk minimization tend to generalize well for examples from the majority groups while memorizing instances from minority groups"
  - [section 3.2, Figure 2] "minority-group examples require fewer neurons to flip their prediction" and dropping these neurons has "smaller effect on training worst-group accuracy"
  - [corpus] "Uncovering Memorization Effect in the Presence of Spurious Correlations" confirms memorization-localization parallels in spurious-correlation settings
- Break condition: If memorization cannot be localized (e.g., distributed across most neurons) or if minority examples require as many neurons as majority examples to flip predictions, the mechanism fails.

### Mechanism 2
- Claim: Example-tied dropout redirects memorization into designated "memorizing neurons" that can be dropped at inference.
- Mechanism: FairDropout partitions neurons into (1) generalizing neurons seen by all examples (fraction p_gen) and (2) memorizing neurons (fraction 1 - p_gen), from which each example samples a fixed subset (probability p_mem). During training, memorization is funneled into these allocated neurons. At test time, dropping all memorizing neurons removes spurious-feature reliance while preserving core-feature pathways.
- Core assumption: The network has sufficient capacity to separate generalizable features from memorization, and dropping memorizing neurons does not corrupt core-feature representations.
- Evidence anchors:
  - [abstract] "apply example-tied dropout as a method we term FairDropout, aimed at redirecting this memorization to specific neurons that we subsequently drop out during inference"
  - [section 3.3, Figure 4] describes neuron partitioning, uniform allocation, and test-time dropping of memorizing neurons
  - [corpus] No direct corpus papers replicate example-tied dropout; mechanism is proposed here with limited external validation
- Break condition: If memorizing neurons encode both spurious and core features, dropping them harms generalization; or if p_gen/p_mem misallocation prevents effective memorization redirection.

### Mechanism 3
- Claim: FairDropout improves worst-group accuracy without group labels by fairly distributing memorization burden across examples.
- Mechanism: By allocating each example the same number of memorizing neurons, minority examples cannot monopolize capacity for spurious-feature memorization. This "fair" redistribution forces the model to rely more on generalizable features accessible to all examples, improving minority-group test performance.
- Core assumption: Fair allocation of memorization capacity reduces reliance on spurious correlations without requiring explicit group annotation.
- Evidence anchors:
  - [abstract] "redistributes the 'memorization burden' more fairly across examples, improving worst-group accuracy without requiring group labels"
  - [section 4.2.2, Table 2] FairDropout achieves highest or near-highest worst-group accuracy on 4/5 datasets without group labels
  - [corpus] "Class-Conditional Distribution Balancing for Group Robust Classification" and "Mitigating Shortcut Learning with InterpoLated Learning" address spurious correlations via alternative mechanisms (reweighting, interpolation), but do not validate FairDropout's specific redistribution claim
- Break condition: If fairness in allocation does not translate to reduced spurious-feature reliance (e.g., memorizing neurons still capture group-specific spurious patterns), worst-group gains vanish.

## Foundational Learning

- Concept: Spurious correlations
  - Why needed here: FairDropout targets features (e.g., image background) that correlate with labels in training but not causally, causing minority-group failures. Understanding this is prerequisite to interpreting why ERM fails and why dropping memorizing neurons helps.
  - Quick check question: Can you distinguish between a "core feature" (causally related to label) and a "spurious feature" (correlated but non-causal) in a given dataset?

- Concept: Memorization localization
  - Why needed here: The method assumes memorization can be isolated to specific neurons. Without this concept, the rationale for example-tied dropout and selective dropping is opaque.
  - Quick check question: If removing a small set of neurons flips an example's prediction without affecting others, what does this suggest about how that example was learned?

- Concept: Worst-group accuracy vs. average accuracy
  - Why needed here: FairDropout optimizes for worst-group performance, not average performance. Practitioners must understand this metric to evaluate trade-offs.
  - Quick check question: Why might a model with high average accuracy still be unsafe for deployment in high-stakes domains?

## Architecture Onboarding

- Component map:
  Input -> Backbone (ResNet-50/BERT) -> FairDropout layer -> Classifier -> Output
  FairDropout splits neurons into generalizing (p_gen fraction) and memorizing (1-p_gen fraction) sets

- Critical path:
  1. Identify insertion point for FairDropout (e.g., after ResNet block 3 or before BERT classifier head)
  2. Tune hyperparameters (p_gen, p_mem, learning rate, weight decay) using worst-class accuracy on validation set
  3. Train with FairDropout in "training mode" (memorizing neurons active)
  4. Evaluate in "testing mode" (memorizing neurons dropped) to measure worst-group accuracy

- Design tradeoffs:
  - p_gen too high → insufficient memorization capacity, potentially underfitting; p_gen too low → too many memorizing neurons, risk of dropping useful features
  - p_mem controls per-example memorizing-neuron count; low values may not capture complex spurious patterns, high values may over-allocate
  - Placement in earlier layers vs. later layers affects whether spurious features (often low-level) are captured; dataset-dependent tuning required
  - Combining with DFR adds validation-set dependency but boosts performance (Table 2: FairDropout-DFR achieves best on 4/5 datasets)

- Failure signatures:
  - Worst-group accuracy unchanged from ERM → FairDropout not effectively redirecting memorization; check p_gen/p_mem settings
  - Average accuracy drops significantly → over-dropping memorizing neurons that encode core features; increase p_gen
  - High variance across runs → insufficient hyperparameter tuning or sensitive insertion point; broaden search space

- First 3 experiments:
  1. Replicate CelebA baseline: Train ResNet-50 with ERM, verify minority-group (blond men) memorization via neuron-flip analysis (Figure 2)
  2. Ablate p_gen and p_mem: Sweep values (Table 3 ranges) on CelebA; plot worst-group accuracy vs. hyperparameters to identify stable region
  3. Combine with DFR: Train FairDropout, then retrain classifier on group-balanced validation set; compare standalone FairDropout vs. FairDropout-DFR on Waterbirds (where standalone FairDropout underperforms DFR)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does dropping "memorizing neurons" harm the retention of informative, yet rare, features that are not spurious?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that "prior research has shown that memorization can sometimes be beneficial for generalization," and they note this aspect warrants further investigation.
- Why unresolved: FairDropout assumes memorization is primarily linked to spurious correlations and overfitting, but it does not distinguish between harmful memorization (spurious patterns) and beneficial memorization (long-tail/atypical valid examples).
- What evidence would resolve it: An analysis of performance on distinct subpopulations of rare but valid examples (distinct from the "minority groups" defined by spurious correlations) before and after the application of FairDropout.

### Open Question 2
- Question: Do the designated "generalizing neurons" remain free of spurious feature reliance during the training process?
- Basis in paper: [explicit] The paper states that FairDropout "implicitly assumes that generalizing neurons are less likely to memorize examples... this hypothesis requires further exploration."
- Why unresolved: The method isolates a subset of neurons for memorization, but there is no theoretical guarantee or empirical verification that the remaining "generalizing" neurons do not also learn spurious correlations independent of the dropped set.
- What evidence would resolve it: A mechanistic interpretability study (e.g., using probing classifiers) on the "generalizing" neurons to test for the presence of spurious attribute information after training.

### Open Question 3
- Question: Why does FairDropout underperform relative to Deep Feature Retraining (DFR) on synthetic datasets like Waterbirds, and does this indicate a failure to disentangle spurious background features?
- Basis in paper: [inferred] The results show FairDropout achieves 77.8% on Waterbirds compared to DFR's 89.0%. The authors suggest this is due to the transferability of ImageNet features but do not explain why FairDropout specifically struggles to improve upon ERM in this synthetic context compared to natural datasets like CelebA.
- Why unresolved: It is unclear if the failure to match DFR on Waterbirds is due to the synthetic nature of the dataset, the type of spurious feature (background vs. demographic), or a limitation of the neuron-dropping heuristic in specific architectures.
- What evidence would resolve it: A comparative analysis of the "memorizing neurons" identified in synthetic vs. natural datasets to determine if they correlate differently with the ground-truth spurious attributes.

## Limitations

- Performance on synthetic datasets (Waterbirds) lags behind DFR, suggesting potential limitations in handling certain types of spurious features
- Does not explicitly distinguish between harmful and beneficial memorization, potentially dropping neurons encoding rare but valid features
- Requires hyperparameter tuning (p_gen, p_mem) that may be dataset-specific and computationally intensive

## Confidence

- Memorization localization mechanism: Medium confidence - supported by single paper evidence but lacks external replication
- Performance claims: High confidence - consistent best or near-best results on 4/5 datasets with substantial improvements over ERM
- Group label independence: Partial confidence - tuning uses worst-class accuracy proxy, and DFR combination requires group labels

## Next Checks

1. Ablate memorizing neurons post-hoc in ERM-trained models to confirm minority-group reliance without FairDropout
2. Sweep p_gen and p_mem more granularly to verify robustness of improvements
3. Apply FairDropout to a dataset without clear spurious correlations to test whether gains are spurious-feature-specific or general