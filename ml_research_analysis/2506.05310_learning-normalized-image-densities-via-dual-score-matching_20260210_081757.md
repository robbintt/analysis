---
ver: rpa2
title: Learning normalized image densities via dual score matching
arxiv_id: '2506.05310'
source_url: https://arxiv.org/abs/2506.05310
tags:
- energy
- probability
- score
- images
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for learning normalized energy
  models from image data by combining ideas from diffusion generative models with
  dual score matching. The authors derive a joint objective that matches both space
  and time scores of the energy function, ensuring consistent energy estimates across
  noise levels.
---

# Learning normalized image densities via dual score matching

## Quick Facts
- arXiv ID: 2506.05310
- Source URL: https://arxiv.org/abs/2506.05310
- Authors: Florentin Guth; Zahra Kadkhodaie; Eero P Simoncelli
- Reference count: 40
- Key outcome: Method learns normalized energy models from image data, achieving NLL comparable to state-of-the-art diffusion models while enabling one-shot energy evaluation

## Executive Summary
This paper introduces dual score matching, a framework that combines space and time score matching to learn normalized energy models from image data. By jointly estimating the gradient of the energy with respect to both the image and the noise level, the method ensures consistent energy estimates across all noise levels and resolves mode-dependent constants that plague single score matching. The authors propose an energy network architecture based on the inner product of the input image and a score network output, which preserves the score network's denoising inductive biases while providing tractable energy values.

## Method Summary
The method learns an energy function Uθ(y,t) parameterized as ½⟨y, sθ(y,t)⟩ where sθ is a score network. Training uses a dual objective combining denoising score matching (ℓDSM) and time score matching (ℓTSM), both evaluated at different noise levels t sampled uniformly from [10⁻⁹, 10³]. The energy is normalized by anchoring to the Gaussian entropy at t_max, ensuring proper probabilistic interpretation. The score network uses a UNet architecture with homogeneous instance normalization to maintain homogeneity properties, and training requires double backpropagation to compute the time derivative of the energy.

## Key Results
- Achieves -74.4 nats/dim on ImageNet64, competitive with state-of-the-art diffusion models
- Demonstrates strong generalization: models trained on non-overlapping subsets assign nearly identical probabilities to all images
- Shows effective dimensionality varies significantly with image content and noise level, challenging assumptions about high-dimensional distributions
- One-shot energy evaluation is 12 seconds versus 3 hours 20 minutes for traditional integration over noise levels

## Why This Works (Mechanism)

### Mechanism 1: Dual Score Matching Resolves Mode-Dependent Constants
Single score matching recovers energy only up to mode-dependent additive constants; adding time score matching constrains energy across modes. The space score (∇yU) learns local geometry via denoising, while the time score (∂tU) enforces mass conservation across noise levels. Since p(y,t) has connected support through diffusion, integrating both scores along space-time paths recovers consistent relative energy levels between modes that would otherwise be isolated.

### Mechanism 2: Inner Product Architecture Preserves Denoising Inductive Biases
Computing energy as Uθ(y,t) = ½⟨y, sθ(y,t)⟩ preserves the score network's inductive biases while yielding a tractable energy. When sθ is conservative (∃φ: sθ = ∇yφ) and homogeneous (sθ(λy,t) = λsθ(y,t)), taking the gradient of the inner product recovers the original score: ∇yUθ = sθ. This avoids the squared-norm architecture's failure to preserve optimization properties.

### Mechanism 3: Gaussian Anchoring at Large Noise Provides Global Normalization
The normalization constant is fixed by anchoring to the known Gaussian distribution at large noise levels. At t = tmax, the noisy distribution p(y|tmax) ≈ N(0, tmaxI). The time score objective ensures the normalizing constant is time-invariant (mass conservation). Setting the constant to match Gaussian entropy at tmax normalizes the entire energy landscape.

## Foundational Learning

- **Score Matching & the Miyasawa-Tweedie Identity**: The framework builds on expressing the score (gradient of log-density) via the optimal denoiser: ∇yU(y,t) = Ex[(y-x)/t | y]. Quick check: Given a noisy observation y = x + √t·z with z∼N(0,I), what is the optimal MMSE denoiser and how does it relate to the score?

- **Diffusion Processes & Forward/Backward SDEs**: Understanding why p(y,t) has connected support requires seeing how Gaussian diffusion progressively smooths and merges modes. Quick check: In a variance-exploding diffusion dx_t = dw_t, how does the density evolve and why does it become Gaussian as t→∞?

- **Conservative Vector Fields & Homogeneity**: The inner product energy architecture requires sθ to be a gradient of some potential (conservative) and scale-linearly (homogeneous). Quick check: If sθ(y) is homogeneous of degree 1, show that ⟨y, sθ(y)⟩ is homogeneous of degree 2. When is a vector field conservative?

## Architecture Onboarding

- **Component map**: Input image → noisy augmentation → UNet → inner product with input → dual loss (requires double backprop for ∂tUθ)
- **Critical path**: Input image → noisy augmentation → UNet → inner product with input → dual loss (requires double backprop for ∂tUθ)
- **Design tradeoffs**: Double backprop: +4× training time, but enables one-shot energy evaluation (12s vs 3h20min for 50k images); GeLUs over ReLUs: ensures ∇yUθ is differentiable but breaks exact homogeneity; Noise range [tmin, tmax] = [10⁻⁹, 10³]: wider range better for normalization, but tmin limited by numerical precision
- **Failure signatures**: Energies not converging across training splits: increase training data or check noise level distribution; Dimensionality estimates going negative: use MSE-based formula (Eq. 11) instead of energy-based (Eq. 12); Poor denoising at low noise: architecture may be insufficiently homogeneous; check normalization layers
- **First 3 experiments**: 1) Train on 1D/2D Gaussian mixture to verify dual score matching recovers true energy; 2) Split ImageNet64 training data in half, train two models, compute correlation of log p(x) on held-out images; 3) For constant images, effective dimensionality should approach 0 as t→0; for pure noise, should approach d=4096

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What structural properties of natural image datasets cause the distribution of log-probabilities to approximate a Gumbel distribution?
- Basis in paper: [explicit] The authors state in Section 3.2 that the Gumbel fit is "novel, and calls for an explanation," noting it arises as a limit distribution for the maximum of i.i.d. variables rather than the sum.
- Why unresolved: The paper observes the empirical fit but does not derive a theoretical link between image statistics and extreme value distribution theory.
- What evidence would resolve it: A theoretical derivation showing how the "maximum" operation (implicit in the energy landscape or dataset composition) leads to Gumbel statistics.

### Open Question 2
- Question: Can the convergence of the dual score matching objective be theoretically guaranteed by proving the joint space-time distribution satisfies a Poincaré inequality?
- Basis in paper: [explicit] Section 4 and Appendix A.3 state, "It would be desirable to show that our training objective... quantitatively controls the distance between the learned energy and the data energy."
- Why unresolved: The authors currently offer no theoretical guarantees that minimizing their objective leads to a good approximation of the true energy.
- What evidence would resolve it: A formal proof establishing a Poincaré inequality for the joint distribution of images and noise levels.

### Open Question 3
- Question: What specific "undocumented geometrical regularities" in the energy landscape enable generalization despite the high local dimensionality observed?
- Basis in paper: [explicit] In Section 3.3, the authors note that the estimated dimensionalities are "too high to explain how the curse of dimensionality is avoided."
- Why unresolved: The results challenge the manifold hypothesis, leaving the mechanism by which models generalize on high-dimensional image data unexplained.
- What evidence would resolve it: Identification and mathematical characterization of the specific geometric constraints that restrict the energy landscape's effective complexity.

### Open Question 4
- Question: Can the computational cost of dual score matching be reduced to match standard diffusion training speeds without compromising energy consistency?
- Basis in paper: [explicit] Section 4 lists training time as a limitation due to "double back-propagation" and explicitly suggests future work: "We believe this can be improved... or through the use of sliced score matching."
- Why unresolved: The current implementation requires roughly 4× the training time of a standard score network.
- What evidence would resolve it: Empirical comparisons showing that an optimized or sliced score matching objective achieves similar NLL and generalization performance with training times comparable to standard diffusion models.

## Limitations
- Requires double backpropagation, increasing training time by approximately 4× compared to standard score matching
- Depends critically on the score network being conservative and homogeneous, conditions that are approximately enforced but not exactly satisfied
- Normalization procedure is sensitive to the choice of t_max and the assumption that the model satisfies the diffusion equation
- Requires substantial data (100k+ samples per split) for reliable probability estimates

## Confidence

- High: Dual score matching resolves mode-dependent constants (supported by synthetic validation and consistent generalization results)
- High: Inner product architecture preserves denoising performance (PSNR results match or exceed baseline)
- Medium: Time score ensures consistent normalization across noise levels (mechanism is sound but empirical verification is indirect)
- Low: Effective dimensionality estimates reflect meaningful image properties (interpretation depends on model assumptions)

## Next Checks
1. Test dual score matching on synthetic mixtures with disconnected modes at all noise levels to verify the mechanism for bridging isolated modes
2. Evaluate model performance when trained with t_min > 10⁻⁹ to assess numerical precision requirements
3. Compare effective dimensionality estimates using MSE-based formula versus energy-based formula on synthetic data with known dimensionality