---
ver: rpa2
title: Learning Dynamics of VLM Finetuning
arxiv_id: '2510.11978'
source_url: https://arxiv.org/abs/2510.11978
tags:
- cw-dpo
- negatives
- stage
- loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a key instability in VLM finetuning called
  the "squeezing effect," where uninformative gradients from easy negatives destabilize
  training. To address this, the authors propose Cooling-Weighted DPO (CW-DPO), a
  two-stage method that first smooths the loss landscape using constrained supervised
  finetuning with gentle negatives, then applies preference optimization with a competence-aware
  cooling weight that suppresses gradients from easy negatives.
---

# Learning Dynamics of VLM Finetuning

## Quick Facts
- arXiv ID: 2510.11978
- Source URL: https://arxiv.org/abs/2510.11978
- Reference count: 40
- One-line primary result: CW-DPO achieves 142.6 CIDEr on COCO Test, outperforming SFT and vanilla DPO with improved stability and calibration.

## Executive Summary
This paper identifies a key instability in VLM finetuning called the "squeezing effect," where uninformative gradients from easy negatives destabilize training. To address this, the authors propose Cooling-Weighted DPO (CW-DPO), a two-stage method that first smooths the loss landscape using constrained supervised finetuning with gentle negatives, then applies preference optimization with a competence-aware cooling weight that suppresses gradients from easy negatives. CW-DPO achieves state-of-the-art performance on multiple vision-language benchmarks, including COCO Test (CIDEr score of 142.6), and demonstrates improved stability, better calibration, and faster convergence compared to baselines like SFT and vanilla DPO. Ablation studies confirm that the cooling-weight mechanism is the primary driver of these gains.

## Method Summary
CW-DPO is a two-stage finetuning method for vision-language models. Stage 1 performs constrained supervised finetuning (SFT-C) that minimizes negative log-likelihood on positive samples while applying a soft penalty to prevent overconfidence on negatives. Stage 2 applies a modified DPO objective where the negative term is scaled by a cooling weight computed from the model's average token log-probability on each negative. The cooling weight suppresses uninformative gradients from easy or off-distribution samples. The method uses Qwen2.5-VL-72B as the base model, COCO Karpathy split for captioning, and GPT-4o-generated negatives.

## Key Results
- CW-DPO achieves 142.6 CIDEr on COCO Test, outperforming SFT (140.2) and vanilla DPO (138.8)
- TV divergence reduced from ~0.45 to ~0.15 and JS divergence from ~0.30 to ~0.10 compared to vanilla DPO
- Improved calibration metrics with better uncertainty estimation on held-out sets
- Faster convergence with stable entropy throughout training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cooling-weighted gradient suppression mitigates the "squeezing effect" caused by uninformative gradients from easy negatives.
- Mechanism: The cooling weight w_c = σ((ℓ̄_θ(y_l|χ) - ℓ_floor) / τ) scales the negative-sample gradient term in DPO. When the model already assigns low probability to a negative (ℓ̄_θ ≪ ℓ_floor), w_c → 0, nullifying its gradient. For hard negatives where uncertainty persists (ℓ̄_θ ≥ ℓ_floor), w_c → 1, preserving the learning signal.
- Core assumption: The average per-token log-probability is a reliable proxy for sample "easiness" and gradient informativeness.
- Evidence anchors:
  - [abstract] "Stage 2 applies a DPO objective in which the negative term is scaled by a cooling weight computed from the model's average token log-probability on each negative, suppressing uninformative gradients from easy or off-distribution samples."
  - [Section 4.3] CW-DPO reduces TV divergence from ~0.45 to ~0.15 and JS divergence from ~0.30 to ~0.10 compared to vanilla DPO, indicating more stable updates.
  - [corpus] Related work (AlphaPO, GIFT) confirms reweighting mechanisms improve alignment, but corpus does not directly validate this specific cooling-weight formula.
- Break condition: If ℓ_floor is set too high, all negatives get suppressed (underfitting). If too low, easy negatives still dominate (squeezing persists).

### Mechanism 2
- Claim: Two-stage training (smooth-before-optimize) stabilizes the belief geometry before preference alignment.
- Mechanism: Stage 1 applies constrained SFT: minimize NLL on positives while ensuring NLL on negatives stays above threshold C via soft ReLU penalty. This prevents premature probability collapse and maintains higher entropy distributions before Stage 2's contrastive learning.
- Core assumption: A smoother initial loss landscape reduces gradient noise during subsequent preference optimization.
- Evidence anchors:
  - [abstract] "Stage 1 performs supervised finetuning with gentle negatives: low-weight smoothed supervision that regularizes the base policy and curbs overconfidence."
  - [Section 4.2, Figure 3] SFT-C maintains significantly higher entropy throughout training and achieves higher CIDEr/SPICE on Top-5 generations vs. standard SFT.
  - [corpus] No direct corpus validation for this two-stage priming mechanism in VLMs.
- Break condition: If constraint C is too strict, the model never learns to distinguish negatives; if too loose, overconfidence persists.

### Mechanism 3
- Claim: Asymmetric gradient modulation (targeting only the loser term) is more stable than global loss reweighting.
- Mechanism: CW-DPO modifies only Δ_l (loser log-prob difference) with w_c, leaving Δ_w (winner) untouched. The gradient becomes: ∇L_CW-DPO = β(1-a')[(π_θ - y_w) - w_c·(π_θ - y_l)], surgically dampening the destabilizing loser component.
- Core assumption: Instability originates primarily from the loser gradient term G^l_t, not the winner term.
- Evidence anchors:
  - [Section 3.2] "This decomposition pinpoints the squeezing effect to the loser term (g_l - g^ref_l), which drives instability for easy negatives."
  - [Appendix B, Table 3] CW-DPO achieves 142.6 CIDEr vs. Focal-DPO's 140.5, with TV divergence 0.15 vs. 0.28—demonstrating that asymmetric modulation cuts distributional shift nearly in half.
  - [corpus] Related methods (SDPO, Physio-DPO) use importance sampling or energy-based reweighting but do not isolate the loser term specifically.
- Break condition: If winner and loser distributions overlap significantly, asymmetric suppression may bias learning toward incorrect preferences.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: CW-DPO modifies the standard DPO objective; understanding the baseline is essential.
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model compared to RLHF?

- Concept: Gradient influence and Neural Tangent Kernel (NTK)
  - Why needed here: The paper's theoretical analysis decomposes gradient influence into Belief Geometry, eNTK Kernel, and Loss Residual.
  - Quick check question: How does the eNTK kernel K_t = J_o J_u^⊤ capture how an update on one sample affects another?

- Concept: Entropy and calibration in language models
  - Why needed here: The "squeezing effect" is diagnosed via entropy collapse and miscalibration.
  - Quick check question: Why does low predictive entropy indicate overconfidence and reduced output diversity?

## Architecture Onboarding

- Component map:
  - Pretrained VLM -> Stage 1 (Constrained SFT) -> Stage 2 (CW-DPO) -> Finetuned VLM
  - Stage 1: Positive batch -> standard NLL; Negative batch -> soft penalty via ReLU(C - E[-log π(y-|x)]). Combined via λ weighting.
  - Stage 2: Preference pairs (y_w, y_l) -> compute cooling weight w_c from average log-prob -> scale Δ_l -> modified DPO loss.
  - Monitoring: Δlog p probes on held-out set for early stopping and curriculum signals.

- Critical path:
  1. Initialize from pretrained VLM (e.g., Qwen2.5-VL-72B).
  2. Stage 1: Train on 75% data with SFT-C (hyperparameters: C, λ).
  3. Set reference model π_ref ← π_θ after Stage 1.
  4. Stage 2: Train on 25% preference data with CW-DPO (hyperparameters: β, τ, ℓ_floor).
  5. Monitor TV/JS divergence and entropy on probe set.

- Design tradeoffs:
  - Higher C → more smoothing but risk of underfitting negatives.
  - Lower ℓ_floor → more negatives retained but increased gradient noise.
  - On-policy vs. dataset negatives: On-policy provides fresh contrast; dataset negatives maintain diversity. Paper recommends mixing.

- Failure signatures:
  - Entropy collapses rapidly → increase C or λ in Stage 1.
  - Validation metrics plateau early → ℓ_floor may be too high (over-suppression).
  - CIDEr degrades vs. SFT baseline → vanilla DPO squeezing effect; verify cooling weight is applied correctly.

- First 3 experiments:
  1. Reproduce SFT-C vs. standard SFT on a small COCO subset: plot entropy and CIDEr over training to validate Stage 1 smoothing effect.
  2. Ablate cooling weight (set w_c = 1.0) on Stage 2: compare TV/JS divergence and CIDEr to confirm gradient suppression benefit.
  3. Sensitivity sweep on ℓ_floor ∈ {-5.0, -3.0, -2.0}: verify robustness claim and identify viable range for your dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the computational overhead of Stage 1 (Constrained SFT) be reduced without compromising the loss landscape smoothing?
- **Basis in paper:** [explicit] Appendix F.2.1 states that the per-step computational cost in Stage 1 is "approximately doubled" because it requires a forward pass on both positive and negative batches ($B^+$ and $B^-$).
- **Why unresolved:** The paper accepts this overhead as a "favorable trade-off" for stability but does not explore approximations, such as using a subset of negatives or stale gradients, to mitigate the doubling of compute cost.
- **What evidence would resolve it:** Experiments showing that constraining negatives on a subset of data or using a cheaper proxy for the negative constraint maintains the "squeezing effect" mitigation while lowering the FLOP count closer to standard SFT.

### Open Question 2
- **Question:** Does the Cooling-Weighted DPO mechanism generalize effectively to pure Large Language Models (LLMs) or other modalities like video?
- **Basis in paper:** [explicit] The Conclusion states that "CW-DPO can be readily extended to broader finetuning scenarios and diverse real-world multimodal tasks," yet the experiments are restricted to image-based VLMs (specifically Qwen2.5-VL).
- **Why unresolved:** While the theoretical derivation (Section 2.2) relies on token probabilities applicable to any sequence model, the "squeezing effect" is motivated specifically by the "complex state dependencies" of the visual stream. It is unclear if the visual modality is a prerequisite for the instability CW-DPO fixes.
- **What evidence would resolve it:** Evaluations of CW-DPO on standard text-only LLM benchmarks (e.g., GSM8K or AlpacaEval) or video-language benchmarks to verify if the cooling weight mechanism yields equivalent stability gains.

### Open Question 3
- **Question:** Is the "squeezing effect" and the efficacy of the cooling weight dependent on model scale?
- **Basis in paper:** [inferred] Section 4.1 specifies that all experiments use the Qwen2.5-VL-72B backbone. The Related Work (Section A) connects the analysis to "overparameterization" (referencing Sagawa et al., 2020), leaving the behavior of smaller models unstated.
- **Why unresolved:** The stability issues caused by "uninformative gradients" might be more pronounced or qualitatively different in extremely large models compared to smaller VLMs (e.g., 7B parameters), which may regularize differently.
- **What evidence would resolve it:** A scaling curve analysis comparing the performance gap between vanilla DPO and CW-DPO across different parameter counts (e.g., 7B vs 72B) to see if the instability is exacerbated by scale.

## Limitations
- All experiments conducted on single vision-language model (Qwen2.5-VL-72B) with limited task diversity
- Theoretical analysis relies on assumptions about gradient decomposition that may not hold across all VLM architectures
- Ablation studies do not explore full hyperparameter space or interactions between Stage 1 and Stage 2 settings

## Confidence
- **High**: The diagnosis of the squeezing effect and its link to entropy collapse is well-supported by the gradient analysis in Section 3.2 and corroborated by the TV/JS divergence reduction in Section 4.3.
- **Medium**: The two-stage training procedure's benefits are demonstrated empirically but lack ablation of Stage 1's necessity or comparison to simpler initialization strategies.
- **Low**: The claim that CW-DPO's asymmetric gradient modulation is inherently more stable than global reweighting lacks direct comparison to alternatives like Focal-DPO or importance sampling in the ablation studies.

## Next Checks
1. **Cross-task Generalization**: Evaluate CW-DPO on VQA and image retrieval tasks (e.g., GQA, Flickr30k Entities) to verify that Stage 1 smoothing and Stage 2 gradient suppression generalize beyond captioning.
2. **Alternative Negative Sampling**: Replace GPT-4o negatives with beam search or human-annotated errors to test whether CW-DPO's cooling weight remains effective when negative quality varies.
3. **On-policy vs. Dataset Negatives**: Conduct an ablation study comparing mixed negative sampling (as recommended) to purely on-policy or purely dataset negatives, measuring entropy stability and CIDEr scores.