---
ver: rpa2
title: On Reconstructing Training Data From Bayesian Posteriors and Trained Models
arxiv_id: '2507.18372'
source_url: https://arxiv.org/abs/2507.18372
tags:
- data
- training
- reconstruction
- bayesian
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a general statistical framework for analyzing
  training data reconstruction (TDR) attacks. It formulates the TDR problem as approximating
  the empirical distribution of training data using weighted pseudo-data, and shows
  this is equivalent to minimizing maximum mean discrepancy (MMD) with a kernel determined
  by model gradients.
---

# On Reconstructing Training Data From Bayesian Posteriors and Trained Models

## Quick Facts
- arXiv ID: 2507.18372
- Source URL: https://arxiv.org/abs/2507.18372
- Authors: George Wynne
- Reference count: 20
- Primary result: Establishes a unified framework for analyzing training data reconstruction attacks across Bayesian and non-Bayesian models using maximum mean discrepancy

## Executive Summary
This paper develops a general statistical framework for analyzing training data reconstruction (TDR) attacks. The framework formulates TDR as approximating the empirical distribution of training data using weighted pseudo-data, which is shown to be equivalent to minimizing maximum mean discrepancy (MMD) with a kernel determined by model gradients. The work unifies the analysis of both Bayesian and non-Bayesian models under a single mathematical approach, revealing that model complexity is a double-edged sword: while improving performance, it also increases vulnerability to reconstruction attacks by making more training data features recoverable.

## Method Summary
The paper introduces a unified framework for analyzing training data reconstruction attacks by formulating the problem as approximating the empirical distribution of training data using weighted pseudo-data. This formulation is shown to be equivalent to minimizing maximum mean discrepancy (MMD) with a kernel determined by model gradients. The approach generalizes existing TDR methods to both Bayesian and non-Bayesian models, introducing the first score-matching method for Bayesian models. The framework characterizes which features of training data can be recovered based on model complexity, demonstrating that more complex models extract more features and enable more complete reconstruction. Numerical experiments on a real Bayesian linear regression model validate that while exact reconstruction is impossible, sufficient statistics like data count, means, and variances can be accurately recovered.

## Key Results
- Established a unified framework for both Bayesian and non-Bayesian models using MMD-based approach
- Demonstrated that model complexity increases reconstruction vulnerability by making more training data features recoverable
- Introduced the first score-matching method for Bayesian models that generalizes existing TDR methods
- Showed that sufficient statistics (data count, means, variances) can be accurately recovered even when exact reconstruction is impossible

## Why This Works (Mechanism)
The framework works by treating training data reconstruction as a distribution approximation problem. By modeling the reconstruction task as minimizing MMD between the true empirical distribution and a weighted pseudo-data distribution, the approach leverages the connection between kernel methods and statistical learning. The kernel used in the MMD computation is determined by the model's gradients, which encode information about how the model parameters depend on the training data. This gradient-based kernel captures the relationship between model parameters and training data, enabling the reconstruction of sufficient statistics. The score-matching formulation for Bayesian models extends this approach to posterior distributions, providing a unified treatment across different model types.

## Foundational Learning
- Maximum Mean Discrepancy (MMD): A kernel-based distance metric between probability distributions that measures the difference between feature means in reproducing kernel Hilbert spaces. Why needed: Provides the mathematical foundation for measuring distributional differences in the reconstruction framework. Quick check: Verify MMD is zero if and only if distributions are identical.

- Kernel methods in statistical learning: Techniques that use kernel functions to implicitly map data into high-dimensional feature spaces. Why needed: Enable the use of inner products in feature space without explicit mapping, crucial for the MMD formulation. Quick check: Confirm kernel is positive definite and valid for the chosen feature space.

- Score matching: A method for estimating parameters by minimizing the Fisher divergence between the model distribution and the data distribution. Why needed: Provides the theoretical basis for the Bayesian model reconstruction approach. Quick check: Verify the score function correctly computes the gradient of the log-probability.

- Model gradients and parameter-data relationship: Gradients indicate how model parameters change with respect to training data. Why needed: Serve as the kernel function in the MMD framework, encoding information about training data. Quick check: Ensure gradients are properly computed and normalized.

- Sufficient statistics: Statistics that capture all information about parameters contained in the sample. Why needed: Define the features of training data that can be recovered through reconstruction. Quick check: Verify recovered statistics match theoretical expectations for the given model.

## Architecture Onboarding

Component map:
Bayesian/Non-Bayesian model -> Model gradients -> Kernel function -> MMD computation -> Weighted pseudo-data optimization -> Training data reconstruction

Critical path:
Model gradients are computed and used to construct the kernel function, which is then used in MMD computation. The MMD is minimized through weighted pseudo-data optimization to achieve training data reconstruction. The quality of reconstruction depends on the informativeness of gradients and the appropriateness of the kernel choice.

Design tradeoffs:
The framework balances between model expressiveness and reconstruction vulnerability. More complex models provide better performance but also extract more features from training data, increasing reconstruction risk. The choice of kernel affects both reconstruction accuracy and computational complexity. Bayesian models offer uncertainty quantification but require additional computational overhead for posterior sampling.

Failure signatures:
Reconstruction fails when gradients are uninformative (e.g., saturated activations, constant features) or when the kernel cannot capture the relevant data features. Poor kernel choice leads to inaccurate MMD computation and suboptimal reconstruction. Over-regularization in complex models can reduce gradient informativeness, limiting reconstruction capability.

First experiments:
1. Validate MMD computation on synthetic distributions with known differences to verify the kernel choice and implementation
2. Test reconstruction on simple linear models where analytical solutions are available for comparison
3. Verify the relationship between model complexity and reconstruction accuracy using models of increasing parameter count

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that all models can be analyzed through gradient-based kernels may not hold for architectures where gradients are sparse or uninformative
- The framework assumes access to model gradients, which may not be feasible in black-box scenarios where only model outputs are accessible
- Limited empirical validation focused on Bayesian linear regression, which may not capture the complexity of modern deep learning architectures

## Confidence
- Theoretical claims: High - MMD-based formulation provides rigorous mathematical foundation
- Practical implications: Medium - Limited scope of experiments and assumptions about gradient access
- Empirical validation: Medium - Focused on simple Bayesian linear regression model

## Next Checks
1. Test the framework on non-linear models and deep neural networks to verify if the MMD-based approach remains effective when gradients become high-dimensional and sparse

2. Validate the theoretical relationship between model complexity and reconstruction vulnerability across different model families, including both Bayesian and non-Bayesian approaches

3. Examine the practical feasibility of gradient-based reconstruction in black-box scenarios where only model outputs are accessible, not internal gradients