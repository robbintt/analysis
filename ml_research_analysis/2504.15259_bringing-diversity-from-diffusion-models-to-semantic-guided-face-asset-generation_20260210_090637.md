---
ver: rpa2
title: Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation
arxiv_id: '2504.15259'
source_url: https://arxiv.org/abs/2504.15259
tags:
- face
- texture
- generation
- diffusion
- geometry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for generating diverse, high-quality
  3D face assets with semantic control by leveraging pre-trained diffusion models
  and adversarial training. The approach addresses the challenge of limited diversity
  in existing 3D face datasets by synthesizing a large dataset of 44,000 3D face models
  with associated attributes (age, gender, ethnicity) using a pre-trained diffusion
  model.
---

# Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation

## Quick Facts
- **arXiv ID**: 2504.15259
- **Source URL**: https://arxiv.org/abs/2504.15259
- **Reference count**: 33
- **Primary result**: Generates 44K diverse 3D face assets with semantic control via diffusion models and adversarial training, achieving 0.014s per face with high attribute control accuracy.

## Executive Summary
This paper presents a method for generating diverse, high-quality 3D face assets with semantic control by leveraging pre-trained diffusion models and adversarial training. The approach addresses the challenge of limited diversity in existing 3D face datasets by synthesizing a large dataset of 44,000 3D face models with associated attributes (age, gender, ethnicity) using a pre-trained diffusion model. A texture normalization method converts these synthetic images into clean albedo maps, and a disentangled GAN-based generator maps semantic attributes to UV-space geometry and albedo, enabling both direct sampling and continuous latent editing while preserving identity. The system produces PBR-ready assets including geometry, albedo, specular, and displacement maps, along with secondary assets like eyeballs and teeth.

## Method Summary
The method uses a two-stage pipeline: first, generate synthetic 3D face data by conditioning Stable Diffusion v1.5 with ControlNet on normal maps from scanned geometries, then normalize the resulting textures using a patch-parameter estimation network and MLP-based translation to produce clean albedo maps. Second, train a two-step GAN with an encoder E that disentangles identity from attributes (age, gender, ethnicity) via adversarial training, followed by generators G1 and G2 that map semantic codes to UV-space geometry and albedo. Asset refinement includes super-resolution, specular/displacement map generation, and attachment of secondary assets (eyes, teeth, gums).

## Key Results
- Generates 44K diverse 3D face assets with attribute distributions: 14 ethnicities, 45%/45%/10% male/female/unisex, 13 age groups (15-75)
- Attribute control accuracy: race 75.88%, gender 91.93%, age 49.90%
- Identity preservation during editing: 0.9594 (gender change) and 0.9542 (age change)
- Real-time generation speed: 0.014s per face
- Texture normalization PSNR: 24.67 vs FFHQ-UV's 17.64

## Why This Works (Mechanism)

### Mechanism 1: Texture Normalization as Domain Transfer
Converting diffusion-synthesized portraits with baked lighting into clean albedo maps enables PBR-ready assets. A patch-parameter estimation network predicts spatially-varying lighting factors θ across a 64×64 grid, then an MLP-based pixel translation network applies per-pixel color correction guided by an explicit skin color reference. The approach treats lighting removal as a domain transfer from 44K synthetic textures to 200 high-quality scanned albedos.

### Mechanism 2: Two-Step Disentanglement for Identity-Preserving Semantic Control
Separating unlabeled identity information from labeled attributes (age, gender, ethnicity) enables smooth attribute editing without identity drift. Step 1 trains encoder E to produce attribute-invariant codes by adversarially fooling a conditional code discriminator DE while reconstructing via generator G1. Step 2 freezes E and trains generator G2 with discriminator DG that receives the unlabeled code as spatial conditioning, ensuring generated samples match the identity distribution of real codes.

### Mechanism 3: Diffusion Priors as Diversity Source with Quality Anchoring
Pre-trained diffusion models provide demographic diversity unavailable in scanned datasets, while texture normalization anchors outputs to production-quality albedo standards. Stable Diffusion v1.5 + ControlNet generates 65K portraits conditioned on semantic prompts and normal maps from random scan geometries. After reconstruction, UV projection, completion, normalization, and sanity-check filtering, 44K high-quality UV texture/geometry pairs remain.

## Foundational Learning

- **Concept: UV Texture Mapping and Albedo**
  - Why needed here: The entire pipeline operates in UV space; understanding how 3D geometry maps to 2D texture coordinates and what albedo (diffuse reflectance) means is essential for grasping why normalization is necessary and how PBR assets differ from shaded images.
  - Quick check question: Given a frontal portrait, which facial regions will have missing UV data after projection, and why must albedo exclude lighting information for PBR rendering?

- **Concept: GAN Latent Space Disentanglement**
  - Why needed here: The two-step training separates identity (unlabeled) from semantics (labeled). Without understanding how adversarial training enforces distribution matching and why conditional discriminators enable control, the architecture's identity-preservation claims will be opaque.
  - Quick check question: If the encoder E successfully disentangles age from identity, what should the conditional distribution p(code | age=20) look like compared to p(code | age=50), and how does discriminator DE enforce this?

- **Concept: Physically-Based Rendering (PBR) Material Properties**
  - Why needed here: The output includes albedo, specular, and displacement maps. Understanding how these interact in a rendering pipeline clarifies why incomplete or lit textures are unusable and what the refinement stage contributes.
  - Quick check question: If a texture map has baked-in highlights (specular reflection captured in the image), what rendering artifacts occur when that texture is used as albedo under novel lighting?

## Architecture Onboarding

- **Component map**: Stable Diffusion v1.5 + ControlNet → ReFA reconstruction → UV projection + PSNR-based completion → Normalization network (patch estimator + MLP translator) → Encoder E + G1 + discriminator DE → Frozen E + G2 + discriminator DG → Super-resolution + specular/displacement translation → Secondary asset attachment

- **Critical path**: Data generation quality (especially normalization) → Generator disentanglement fidelity → Asset refinement resolution. If normalization produces albedo with residual lighting, downstream generators learn incorrect texture distributions; if disentanglement fails, editing produces identity drift regardless of refinement quality.

- **Design tradeoffs**: Patch-based normalization vs. full-image translation (patch approach limits capacity but requires less target-domain data); two-step vs. single-step conditional GAN (two-step explicitly separates identity/attributes but adds training complexity); diffusion data source vs. pure scan training (diffusion provides diversity but introduces artifact risk requiring manual filtering).

- **Failure signatures**: Lighting baked into normalized textures (check specular highlights in flat skin regions); identity drift during attribute editing (compare cosine similarity of embeddings before/after edit); texture completion mismatches (visible seams or tone discontinuities at UV boundaries); geometry-texture misalignment (facial features projected to wrong UV locations).

- **First 3 experiments**:
  1. Validate normalization on held-out scan data: Render scanned assets under novel lighting, run through normalization, compare to ground-truth albedo using PSNR. Target: match or exceed reported 24.67; if below 20, debug patch parameter estimation or increase target domain diversity.
  2. Ablate two-step training with identity metric: Train single-step conditional GAN baseline, generate 100 random pairs with identical identity code but varying attributes, compute proposed directional identity similarity. Target: two-step >0.95, single-step <0.90; if similar, discriminator conditioning may need architectural revision.
  3. Stress-test attribute control with classifier: Generate 1000 samples per attribute combination, run through independent pre-trained classifiers (age/gender/ethnicity). Target: classifier accuracy on generated samples within ±10% of test-set accuracy; if substantially higher, generator may be collapsing to mode; if lower, conditioning pathway is weak.

## Open Questions the Paper Calls Out

### Open Question 1
Can the generation of high-quality facial geometry be decoupled from the limited diversity of the scanned dataset to match the diversity achieved in texture synthesis? The paper relies on refining initial geometries from a scan dataset because existing diffusion models lack the priors for controllable, high-quality 3D geometric generation comparable to 2D textures. A generative framework that produces diverse, semantically controlled 3D face geometries from scratch would resolve this.

### Open Question 2
Can multi-view consistent texture synthesis effectively replace the nearest-neighbor blending method to improve texture completion in invisible UV regions? The current method relies on blending frontal projections with retrieved nearest neighbors, which fails if the generated portrait lacks similar labels in the scanned data. A study comparing current completion methods against multi-view synthesis would show improved identity consistency.

### Open Question 3
How can the texture normalization model be improved to maintain fidelity for rare appearances or skin tones not well-represented in the small (200-subject) scanned target domain? The normalization model functions as a domain transfer from 44K synthetic images to 200 high-quality scans; this domain gap makes it difficult to accurately normalize inputs that fall outside the skin-tone distribution of the target domain. Quantitative evaluation on rare phenotypes would demonstrate successful normalization without relying on the heuristic "skin color guide" input.

## Limitations
- Limited diversity in facial geometry due to reliance on scanned dataset rather than generative models
- Normalization accuracy constrained by small target domain (200 subjects), potentially reducing skin-tone fidelity for rare appearances
- Identity preservation metric is novel and not benchmarked against established face verification datasets

## Confidence

- **High confidence**: Attribute control accuracy (race: 75.88%, gender: 91.93%, age: 49.90%)—measured via pre-trained classifiers with clear methodology
- **Medium confidence**: Identity preservation during editing (0.9594 gender, 0.9542 age)—novel metric, but ablation shows two-step design is critical; however, external validation is absent
- **Low confidence**: Generalization of the normalization method—patch-based approach is effective within the paper's setup, but may not scale to broader skin tone distributions without more diverse target data

## Next Checks
1. Validate normalization on held-out scan data: Render scanned assets under novel lighting, run through normalization, compare to ground-truth albedo using PSNR. Target: match or exceed reported 24.67; if below 20, debug patch parameter estimation or increase target domain diversity.
2. Ablate two-step training with identity metric: Train single-step conditional GAN baseline, generate 100 random pairs with identical identity code but varying attributes, compute proposed directional identity similarity. Target: two-step >0.95, single-step <0.90; if similar, discriminator conditioning may need architectural revision.
3. Stress-test attribute control with classifier: Generate 1000 samples per attribute combination, run through independent pre-trained classifiers (age/gender/ethnicity). Target: classifier accuracy on generated samples within ±10% of test-set accuracy; if substantially higher, generator may be collapsing to mode; if lower, conditioning pathway is weak.