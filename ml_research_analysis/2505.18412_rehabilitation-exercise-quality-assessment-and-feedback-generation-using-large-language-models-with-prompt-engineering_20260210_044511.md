---
ver: rpa2
title: Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large
  Language Models with Prompt Engineering
arxiv_id: '2505.18412'
source_url: https://arxiv.org/abs/2505.18412
tags:
- rehabilitation
- exercise
- feedback
- quality
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates that pre-trained Large Language Models\
  \ (LLMs) can assess rehabilitation exercise quality and generate actionable feedback\
  \ using carefully engineered prompts and exercise-specific features extracted from\
  \ body joint data. The approach achieves high accuracy in exercise classification\
  \ across two datasets\u2014UI-PRMD and REHAB24-6\u2014with best results from three-shot\
  \ prompting and certainty elicitation."
---

# Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering

## Quick Facts
- **arXiv ID**: 2505.18412
- **Source URL**: https://arxiv.org/abs/2505.18412
- **Reference count**: 40
- **Primary result**: Pre-trained LLMs can accurately assess exercise quality and generate personalized feedback using engineered prompts and joint data features

## Executive Summary
This study explores the use of pre-trained Large Language Models (LLMs) for assessing the quality of rehabilitation exercises and generating actionable feedback. By leveraging carefully engineered prompts and exercise-specific features extracted from body joint data, the approach achieves high accuracy in exercise classification across two datasets: UI-PRMD and REHAB24-6. The method demonstrates that LLMs can provide interpretable reasoning and personalized natural language feedback without requiring fine-tuning, making it a promising tool for virtual rehabilitation platforms.

## Method Summary
The method involves extracting exercise-specific features from body joint data and using these features as input to pre-trained LLMs through carefully engineered prompts. The study employs a three-shot prompting strategy and incorporates certainty elicitation to enhance the accuracy of exercise classification. The LLMs are evaluated on their ability to classify exercises and generate feedback, with performance compared to traditional deep learning models. The approach avoids the need for fine-tuning, relying instead on the pre-trained knowledge of the LLMs.

## Key Results
- LLMs achieved high accuracy in exercise classification across UI-PRMD and REHAB24-6 datasets.
- Three-shot prompting with certainty elicitation yielded the best classification results.
- LLMs provided interpretable reasoning and personalized feedback, though slightly underperformed deep learning models on one dataset.
- Quantitative evaluation of feedback quality was limited due to the absence of labeled feedback datasets.

## Why This Works (Mechanism)
The approach leverages the pre-trained knowledge of LLMs, which are adept at understanding and generating natural language. By engineering prompts to incorporate exercise-specific features, the LLMs can effectively interpret body joint data and provide meaningful feedback. The three-shot prompting strategy and certainty elicitation enhance the model's ability to classify exercises accurately, while the interpretable reasoning offers insights into the decision-making process.

## Foundational Learning
- **Exercise-Specific Feature Extraction**: Extracting relevant features from body joint data is crucial for providing accurate input to the LLMs. Quick check: Verify feature extraction aligns with exercise requirements.
- **Prompt Engineering**: Carefully designed prompts guide LLMs to interpret data correctly. Quick check: Test prompts with sample data to ensure clarity and relevance.
- **Certainty Elicitation**: Incorporating certainty measures helps improve classification accuracy. Quick check: Validate certainty elicitation with diverse datasets.
- **Natural Language Processing**: LLMs' ability to generate human-like feedback is central to the approach. Quick check: Assess feedback quality with domain experts.
- **Deep Learning vs. LLMs**: Understanding the strengths and limitations of each model type is essential. Quick check: Compare performance metrics across models.

## Architecture Onboarding

### Component Map
Exercise Data -> Feature Extraction -> Prompt Engineering -> LLM -> Feedback Generation

### Critical Path
The critical path involves extracting features from exercise data, engineering prompts, and passing them to the LLM for classification and feedback generation. This path ensures that the input data is accurately interpreted and meaningful feedback is provided.

### Design Tradeoffs
- **No Fine-Tuning vs. Domain Specificity**: Avoiding fine-tuning reduces complexity but may limit domain-specific accuracy.
- **Interpretability vs. Performance**: LLMs offer interpretable reasoning but may slightly underperform specialized deep learning models.
- **Real-Time Efficiency**: The computational demands of LLMs in real-time settings are not fully addressed.

### Failure Signatures
- **Misclassification**: Incorrect exercise classification due to ambiguous prompts or insufficient feature extraction.
- **Feedback Inaccuracy**: Generated feedback may not align with patient needs due to lack of domain-specific fine-tuning.
- **Computational Delays**: Potential delays in real-time applications due to LLM processing time.

### First Experiments
1. Test the accuracy of feature extraction on a small sample of exercise data.
2. Evaluate the effectiveness of different prompt engineering strategies on LLM performance.
3. Compare the feedback quality generated by LLMs with expert-provided feedback.

## Open Questions the Paper Calls Out
The paper highlights several open questions, including the generalizability of the LLM approach across different rehabilitation contexts, the need for labeled feedback datasets to quantitatively evaluate feedback quality, and the computational efficiency of using LLMs in real-time rehabilitation settings.

## Limitations
- The approach relies on pre-trained models without fine-tuning, which may limit domain-specific accuracy.
- Quantitative evaluation of feedback quality is constrained by the lack of labeled feedback datasets.
- LLMs slightly underperformed deep learning models on one dataset, indicating potential limitations in handling complex movements.
- The computational efficiency of LLMs in real-time clinical applications is not fully addressed.

## Confidence
- **High confidence**: The accuracy of exercise classification using LLMs with engineered prompts, particularly with three-shot prompting and certainty elicitation, is well-supported by the experimental results.
- **Medium confidence**: The interpretability and personalized feedback capabilities of LLMs are promising but require further validation with larger, more diverse datasets and real-world clinical trials.
- **Low confidence**: The generalizability of the LLM approach across different rehabilitation contexts and patient populations remains uncertain without additional testing and domain-specific fine-tuning.

## Next Checks
1. Conduct a large-scale clinical trial to evaluate the effectiveness of LLM-generated feedback in improving patient outcomes and adherence to rehabilitation exercises.
2. Develop and curate labeled feedback datasets to enable rigorous quantitative evaluation of the quality and relevance of LLM-generated feedback.
3. Compare the computational efficiency and real-time performance of LLMs versus traditional deep learning models in virtual rehabilitation platforms to assess their practicality for clinical use.