---
ver: rpa2
title: 'Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium
  GPUs'
arxiv_id: '2503.05139'
source_url: https://arxiv.org/abs/2503.05139
tags:
- training
- data
- performance
- arxiv
- ling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report presents two open-source Mixture-of-Experts (MoE) models,
  Ling-Lite and Ling-Plus, trained to address the challenge of cost inefficiency and
  resource limitations in large-scale model training. By optimizing model architecture,
  training strategies, and evaluation frameworks, the team achieved comparable performance
  to leading industry benchmarks using lower-specification hardware, reducing computing
  costs by approximately 20%.
---

# Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs

## Quick Facts
- arXiv ID: 2503.05139
- Source URL: https://arxiv.org/abs/2503.05139
- Authors: Ling Team; Binwei Zeng; Chao Huang; Chao Zhang; Changxin Tian; Cong Chen; Dingnan Jin; Feng Yu; Feng Zhu; Feng Yuan; Fakang Wang; Gangshan Wang; Guangyao Zhai; Haitao Zhang; Huizhong Li; Jun Zhou; Jia Liu; Junpeng Fang; Junjie Ou; Jun Hu; Ji Luo; Ji Zhang; Jian Liu; Jian Sha; Jianxue Qian; Jiewei Wu; Junping Zhao; Jianguo Li; Jubao Feng; Jingchao Di; Junming Xu; Jinghua Yao; Kuan Xu; Kewei Du; Longfei Li; Lei Liang; Lu Yu; Li Tang; Lin Ju; Peng Xu; Qing Cui; Song Liu; Shicheng Li; Shun Song; Song Yan; Tengwei Cai; Tianyi Chen; Ting Guo; Ting Huang; Tao Feng; Tao Wu; Wei Wu; Xiaolu Zhang; Xueming Yang; Xin Zhao; Xiaobo Hu; Xin Lin; Yao Zhao; Yilong Wang; Yongzhen Guo; Yuanyuan Wang; Yue Yang; Yang Cao; Yuhao Fu; Yi Xiong; Yanzhe Li; Zhe Li; Zhiqiang Zhang; Ziqi Liu; Zhaoxin Huan; Zujie Wen; Zhenhang Sun; Zhuoxuan Du; Zhengyu He
- Reference count: 14
- Key outcome: Two open-source Mixture-of-Experts models (Ling-Lite: 16.8B total/2.75B active; Ling-Plus: 290B total/28.8B active) achieve comparable performance to industry benchmarks using lower-specification hardware, reducing computing costs by approximately 20%

## Executive Summary
This report presents two open-source Mixture-of-Experts (MoE) models, Ling-Lite and Ling-Plus, trained to address the challenge of cost inefficiency and resource limitations in large-scale model training. By optimizing model architecture, training strategies, and evaluation frameworks, the team achieved comparable performance to leading industry benchmarks using lower-specification hardware, reducing computing costs by approximately 20%. Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. The models demonstrate superior tool use capabilities and maintain stable performance across diverse hardware platforms. These results show that state-of-the-art 300B MoE models can be effectively trained on less powerful devices, offering a scalable and cost-effective approach to foundational model development.

## Method Summary
The Ling team developed two Mixture-of-Experts models through a multi-phase approach addressing both architectural efficiency and training optimization. The core architecture employs fine-grained experts with a shared expert to balance specialization and generalization, combined with dropless routing and load balance loss for stability. Training utilizes EDiT (Elastic Distributed Training) for heterogeneous hardware efficiency, incorporating layer-wise synchronization, time-based synchronization, and pseudo-gradient penalty. The system includes NormHead and router z-loss for numerical stability, plus stochastic routing warmup to prevent expert overload. Pre-training spans 9 trillion tokens with context extension from 4K to 16K, followed by post-training with SFT and DPO phases. The framework includes comprehensive evaluation tools and cross-platform alignment strategies to ensure consistent performance across diverse hardware configurations.

## Key Results
- Ling-Lite (16.8B total/2.75B active) and Ling-Plus (290B total/28.8B active) achieve competitive performance on MMLU-Pro, GPQA, LiveCodeBench, AIME 2024, BFCL_v2, and C-Eval benchmarks
- Training costs reduced by approximately 20% compared to premium GPU configurations while maintaining comparable performance
- Models demonstrate superior tool use capabilities and maintain stable performance across heterogeneous hardware platforms
- Successfully extended context length from 4K to 16K with RoPE adaptation (θ: 10K→600K) while preserving model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained experts with shared experts improve MoE model specialization and training efficiency
- Mechanism: Scaling expert count while proportionally reducing intermediate size maintains total capacity but promotes specialization; a dedicated shared expert processes all tokens to preserve general capabilities. The routing equation: $o'_t = o_t + E_{share}(h_t)$ where $o_t$ aggregates top-k routed experts.
- Core assumption: Expert specialization is beneficial for task diversity and does not degrade generalization when compensated by a shared expert.
- Evidence anchors:
  - [section 3.2.1] "This design promotes a higher degree of specialization among experts, allowing the model to encapsulate a wider and more diverse range of knowledge."
  - [section 3.2.1] Notes that without shared experts, "individual experts may struggle to simultaneously develop both general and specialized capabilities."
  - [corpus] Related work (DeepSeek-AI [2024b], Dai et al. [2024]) uses similar fine-grained expert designs.
- Break condition: If expert load imbalance persists despite balance loss, indicating routing collapse where few experts receive most tokens.

### Mechanism 2
- Claim: EDiT (Elastic Distributed Training) enables efficient training on heterogeneous hardware by decoupling synchronization from iteration counts.
- Mechanism: Layer-wise synchronization during forward propagation reduces single-operation data volume. Time-based synchronization allows faster nodes more local updates. Pseudo-gradient penalty uses exponential moving average to detect anomalous workers, excludes them from sync, and applies weighted averaging plus gradient clipping.
- Core assumption: Workers can progress at different speeds locally without fundamentally harming convergence; gradient noise from asynchrony can be bounded.
- Evidence anchors:
  - [section 2.2] "In an ideal environment... the speed-up ratio of EDiT would reach 66.1%."
  - [section 2.2, Figure 8] Shows speed comparisons between EDiT and baseline across accelerator counts.
  - [corpus] No direct corpus validation of EDiT-specific claims; mechanism is in-house method with limited external verification.
- Break condition: If pseudo-gradient anomaly detection is too aggressive, excluding valid gradient updates; or if straggler problem becomes systematic rather than transient.

### Mechanism 3
- Claim: NormHead, router z-loss, and stochastic routing warmup jointly stabilize MoE training by bounding softmax logits.
- Mechanism: NormHead applies L2 normalization to LM-head weights before token prediction: $h_o = W_{lm\_head} / ||W_{lm\_head}||_2 \cdot h$. Router z-loss penalizes large logits in expert routing softmax. Stochastic routing warmup interpolates learned logits with random logits during early training to prevent expert overload.
- Core assumption: Training instability in MoE models primarily stems from unbounded softmax logits in both language modeling head and expert router.
- Evidence anchors:
  - [section 3.2.3] "Our empirical experiments indicate that NormHead significantly enhances the stability of training."
  - [section 3.2.2] Stochastic routing warmup formula with interpolation parameter $\alpha = \min(i/W, 1.0)$.
  - [section 6.1] Reports "no observed instances of loss divergence, wide loss spikes, or disruptions in expert routing balance" when combining mechanisms.
  - [corpus] Zoph et al. [2022] cited for z-loss in MoE architectures; corpus validation moderate.
- Break condition: If loss spikes persist despite all mechanisms, suggesting data-optimizer interaction issues rather than architectural instability.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) Routing**
  - Why needed here: The entire architecture depends on understanding how tokens are routed to subsets of experts via learned router weights.
  - Quick check question: Can you explain why dropless routing (no token dropping) requires careful load balancing, and how top-k routing affects computational sparsity?

- Concept: **Local SGD and Asynchronous Distributed Training**
  - Why needed here: EDiT builds on Local SGD principles; understanding why multiple local updates before synchronization helps requires distributed optimization fundamentals.
  - Quick check question: Why does straggler mitigation in synchronous training typically hurt throughput, and how does Local SGD address this?

- Concept: **Softmax Numerical Stability (Z-loss, Logit Normalization)**
  - Why needed here: Both router and LM-head instability are attributed to unbounded logits; z-loss and NormHead directly address this.
  - Quick check question: What happens to softmax gradients when logits become very large, and why does z-loss (penalizing squared logits) help?

## Architecture Onboarding

- Component map: DLRover -> Megatron-LM with MoE support -> XPUTimer profiling -> EDiT training -> PCache distributed caching -> Babel cross-cluster sync -> Flood inference framework

- Critical path:
  1. Pre-training (9T tokens): Initial 4K context → Long-context 16K (150B tokens) → Annealing
  2. Post-training: SFT with synthetic data (rule-based + LLM-based quality filtering) → DPO (Vanilla + Robustness phases) → Format recovery DPO
  3. Continuous evaluation feedback to training data curation (Section 5.1.2, Figure 19)

- Design tradeoffs:
  - Fine-grained experts vs. routing overhead: More experts increase specialization but complicate load balancing.
  - EDiT time-based sync vs. convergence quality: Higher asynchrony improves throughput but may affect final loss.
  - Lower-spec hardware vs. training time: ~20% cost reduction (per Section 1.2) but longer wall-clock time.

- Failure signatures:
  - **Loss divergence**: Check softmax logits in router and LM-head; verify z-loss and NormHead are active.
  - **Expert load imbalance**: Check router token distribution; if few experts receive >50% tokens, routing collapsed.
  - **Cross-platform misalignment**: Verify matmul and communication operators match across platforms at framework level, not just operator level (Section 6.2).
  - **Wide loss spikes**: Trigger skip + retry mechanism; if persistent, reduce learning rate for affected step.

- First 3 experiments:
  1. **Validate cross-platform operator consistency**: Run small-scale training (1B tokens) on both Device A and Device D; compare loss curves. If divergence >0.1%, debug framework-level attention/router implementations per Section 6.2.
  2. **Profile routing balance during warmup**: Enable stochastic routing warmup with W=2000 steps; monitor expert activation distribution every 100 steps. Confirm no expert exceeds 2x average load.
  3. **EDiT throughput benchmark**: Run Ling-Lite on 256 accelerators with EDiT vs. baseline synchronous All-Reduce. Measure step/s and verify claimed speedup range (target: >40% improvement in heterogeneous setting).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive regularization strategies be developed for Direct Preference Optimization (DPO) that effectively distinguish between unnecessary verbosity and essential complexity in reasoning tasks?
- Basis in paper: [explicit] The authors state, "Future research could investigate adaptive strategies that balance length control with the varying complexity and requirements of tasks across different domains," noting that length-regularized DPO often penalizes the longer responses required for complex reasoning.
- Why unresolved: The paper demonstrates that standard length regularization hurts performance on math and coding tasks, but they only tested manual strategies (repeated prompts) which failed; a dynamic, adaptive solution remains unidentified.
- What evidence would resolve it: A DPO variant that reduces verbosity on simple tasks while statistically maintaining or improving accuracy on complex benchmarks (e.g., MATH, LiveCodeBench) compared to vanilla DPO.

### Open Question 2
- Question: Does the efficiency advantage of Mixture-of-Experts (MoE) models over dense models continue to scale linearly or sub-linearly beyond the compute budgets tested (approx. 1e24 FLOPs)?
- Basis in paper: [inferred] While the paper analyzes scaling laws and finds that the efficiency lever increases from 3x to 3.5x as compute grows, it does not determine if this trend saturates or continues indefinitely at extreme scales (e.g., trillion-parameter models).
- Why unresolved: The empirical analysis was limited to specific compute budgets (1e18 to 3e20 FLOPs in scaling experiments), leaving the behavior at the frontier of compute (1e25+) extrapolated but unverified.
- What evidence would resolve it: Training runs at significantly higher compute budgets showing the relationship between the MoE efficiency lever and compute budget.

### Open Question 3
- Question: Can cross-platform alignment in distributed MoE training be automated to prevent gradient accumulation errors without requiring manual, framework-level debugging?
- Basis in paper: [inferred] The authors note in "Bitter Lessons" that validating basic operations is insufficient because "minor precision errors can accumulate," requiring full alignment of forward/backward passes which is labor-intensive and prone to "significant disparities."
- Why unresolved: The current solution relies on rigorous, manual alignment of modules (Attention, MLPs) and tensor parallelism implementations across hardware, which is a bottleneck rather than a systematic fix.
- What evidence would resolve it: A verification tool or training framework that automatically detects and corrects platform-specific floating-point divergences to ensure consistent loss convergence across heterogeneous devices.

### Open Question 4
- Question: What specific data or optimizer state interactions trigger "wide" loss spikes in ultra-large MoE models, and can they be predicted rather than reactively skipped?
- Basis in paper: [inferred] The paper classifies loss spikes into narrow and wide types, stating wide spikes "persist across more steps and can significantly disrupt model stability," and that they are "typically triggered by specific interactions between the data and optimizer states."
- Why unresolved: The team relies on reactive "skip and retry" mechanisms or automatic learning rate reduction rather than a predictive understanding of the underlying data characteristics causing the instability.
- What evidence would resolve it: A diagnostic model capable of identifying "spike-inducing" data batches prior to training ingestion, thereby preventing instability without discarding valid training steps.

## Limitations
- EDiT asynchronous training method lacks comprehensive empirical validation across diverse hardware configurations beyond reported benchmarks
- Exact architectural specifications for both Ling-Lite and Ling-Plus models remain unspecified, limiting precise reproduction
- Claimed ~20% cost reduction versus premium GPUs is based on internal benchmarks without third-party verification or detailed breakdown

## Confidence
- **High Confidence**: The fundamental MoE routing mechanisms (fine-grained experts with shared expert, NormHead, z-loss, stochastic routing warmup) are well-established in literature with moderate corpus validation and demonstrated empirical stability.
- **Medium Confidence**: The EDiT asynchronous training method shows promising theoretical foundations and internal benchmark results, but lacks extensive third-party validation and detailed hyperparameter specifications.
- **Medium Confidence**: Performance benchmark results on MMLU-Pro, GPQA, and other tasks appear reasonable given the model scale, though exact parameter counts would strengthen these claims.
- **Low Confidence**: Cost reduction claims (~20% vs premium GPUs) are difficult to verify without detailed breakdowns of hardware costs, energy consumption, and total training time across different setups.

## Next Checks
1. **Operator Consistency Verification**: Run a small-scale (1B token) training run on both target hardware platforms (Device A and Device D), comparing loss curves and final metrics. Any divergence >0.1% indicates framework-level implementation issues requiring debugging of attention, router, or communication operators.

2. **Routing Balance Monitoring**: Implement real-time expert activation distribution monitoring during stochastic routing warmup (first 2000 steps). Verify no expert receives >2x the average token load, confirming the fine-grained expert design with shared expert effectively prevents routing collapse.

3. **EDiT Performance Benchmark**: Conduct controlled experiments comparing EDiT against baseline synchronous training across 128, 256, and 512 accelerators. Measure step/second throughput and final validation loss, targeting >40% speedup improvement in heterogeneous settings while maintaining convergence quality.