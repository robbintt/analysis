---
ver: rpa2
title: On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization
arxiv_id: '2511.11362'
source_url: https://arxiv.org/abs/2511.11362
tags:
- mezo
- memory
- fine-tuning
- arxiv
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes memory-efficient on-device fine-tuning for
  large language models (LLMs) using zeroth-order optimization (MeZO) versus conventional
  backpropagation (BP). BP requires storing intermediate activations and gradients,
  severely limiting model size on memory-constrained devices.
---

# On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization

## Quick Facts
- arXiv ID: 2511.11362
- Source URL: https://arxiv.org/abs/2511.11362
- Authors: Prabodh Katti; Sangwoo Park; Bipin Rajendran; Osvaldo Simeone
- Reference count: 0
- Primary result: MeZO enables at least 2× larger models than BP under fixed memory constraints, with potential savings up to 25× for long contexts.

## Executive Summary
This paper analyzes memory-efficient on-device fine-tuning for large language models using zeroth-order optimization (MeZO) versus conventional backpropagation (BP). BP requires storing intermediate activations and gradients, severely limiting model size on memory-constrained devices. MeZO avoids this overhead by estimating gradients via forward passes alone, enabling deployment of larger models within the same memory budget. Theoretical analysis shows MeZO can accommodate at least 2× larger models than BP, with potential savings up to 25× for long contexts.

## Method Summary
The paper proposes MeZO (Memory-efficient Zeroth-Order Optimization) for on-device fine-tuning of large language models. Instead of backpropagation, MeZO uses simultaneous perturbation stochastic approximation (SPSA) to estimate gradients through forward passes only. The method perturbs model weights with random noise and computes finite-difference gradient estimates via two forward passes per iteration. Under fixed memory constraints, MeZO enables deployment of 2×-25× larger models compared to BP, depending on context length. Experiments on the BoolQ task validate these findings, showing MeZO with Llama2-7B or Llama2-13B achieves higher accuracy than BP with GPT2-medium under equivalent ~17GB memory constraints.

## Key Results
- Under fixed memory budgets, MeZO enables deployment of 2×–25× larger models than BP
- MeZO with Llama2-7B or Llama2-13B achieves ~82% accuracy vs <75% for BP with GPT2-medium after ~2 hours
- Theoretical analysis shows MeZO savings range from 2× to 25× depending on context length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MeZO estimates gradients using only forward passes, eliminating activation storage overhead.
- Mechanism: The method applies simultaneous perturbation stochastic approximation (SPSA), perturbing weights with random noise ε, then computing finite-difference gradient estimates via two forward passes: ∇L ≈ [L(θ+εz) − L(θ−εz)] / (2ε) · z. No backward pass is required.
- Core assumption: The loss landscape for pre-trained LLMs has low effective dimensionality, making random perturbation directions informative enough for descent.
- Evidence anchors:
  - [abstract] "MeZO alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states."
  - [section 1] "MeZO approximates gradients using only forward passes... strong pre-training substantially reduces the effective dimensionality of the parameter space."
  - [corpus] ConMeZO paper confirms "ZO methods often suffer from slow convergence due to high-variance stochastic gradient estimators" — validates variance challenge.
- Break condition: If the loss landscape has high effective curvature or dimensionality, gradient estimates become too noisy for meaningful descent.

### Mechanism 2
- Claim: Under fixed memory budgets, MeZO enables deployment of 2×–25× larger models than BP.
- Mechanism: BP memory = weights + gradients + activations + optimizer states. MeZO memory ≈ weights + minimal activation buffer (L'/L factor). The savings scale with context length N since activations grow as O(BLND) while weights grow as O(LD²).
- Core assumption: Implementation can achieve low L'/L (activation buffering ratio); the paper uses L'/L = 0.15–0.41 in experiments.
- Evidence anchors:
  - [abstract] "Theoretical analysis shows MeZO can accommodate at least 2× larger models than BP, with potential savings up to 25× for long contexts."
  - [section 2.4] "For N=32768, the memory savings afforded by MeZO range from 2× to up to 25×... with activation checkpointing, the advantage reduces to approximately 5×."
  - [corpus] FZOO paper notes "backward pass increases memory usage to more than 10 times the inference level" — corroborates BP overhead magnitude.
- Break condition: If implementation requires high L'/L due to tensor buffering policies, memory advantage degrades toward BP levels.

### Mechanism 3
- Claim: Larger model capacity under MeZO compensates for slower convergence, achieving higher asymptotic accuracy.
- Mechanism: Under identical ~17GB memory constraint, BP is limited to GPT2-medium (~350M params) while MeZO fits Llama2-7B or Llama2-13B. The stronger pre-trained model's capacity outweighs ZO's slower optimization.
- Core assumption: Sufficient wall-clock time is available for MeZO's longer convergence trajectory.
- Evidence anchors:
  - [abstract] "After ~2 hours, MeZO reaches ~82% accuracy versus <75% for BP."
  - [section 3] "Fig. 4 confirms that MeZO has a slower fine-tuning convergence rate in terms of wall-clock time. However, thanks to the more capable model afforded by its more efficient memory usage, MeZO can eventually achieve a higher validation accuracy."
  - [corpus] TeZO paper addresses "slow convergence due to the curse of dimensionality" — active research area for improvement.
- Break condition: If time budget is severely constrained (<30 min in paper's setup), BP with smaller model may be preferred.

## Foundational Learning

- Concept: **Zeroth-Order Optimization (SPSA)**
  - Why needed here: Core algorithm enabling gradient-free updates. Must understand perturbation sampling, finite-difference estimation, and variance-scaling tradeoffs.
  - Quick check question: Can you explain why two forward passes (θ+εz, θ−εz) yield a gradient estimate along direction z?

- Concept: **Transformer Memory Components**
  - Why needed here: Breaking down where memory goes — weights (12bLD²), activations (quadratic in N), gradients, optimizer states — enables principled comparison.
  - Quick check question: Why do activations scale quadratically with context length N while weights do not?

- Concept: **Activation Checkpointing**
  - Why needed here: Baseline memory-reduction technique for BP; understanding its √L storage tradeoff contextualizes MeZO's relative advantage.
  - Quick check question: What is the recomputation cost of checkpointing every √L layers versus storing all activations?

## Architecture Onboarding

- Component map:
```
[Model Weights: 12bLD² bytes] ← Both BP and MeZO store
[Gradients: 12bLD² bytes] ← BP only
[Optimizer States: variable] ← BP only (Adam ≈ 2× weights)
[Activations: BLND × factor] ← BP stores all; MeZO stores L'/L fraction
[Perturbation Buffer: O(D)] ← MeZO only, for noise vector z
```

- Critical path:
  1. Profile target device memory budget
  2. Solve Eq. (2) and (3) for max model size under BP vs MeZO
  3. If MeZO model size > 2× BP, proceed; otherwise checkpointing may suffice
  4. Tune L'/L via memory allocator instrumentation
  5. Grid-search learning rate (MeZO typically needs 10–100× smaller LR than BP)

- Design tradeoffs:
  - **L'/L ratio**: Lower = less memory but requires more careful memory management; paper used 0.15–0.41
  - **Perturbation count**: Paper fixed at 5; more perturbations reduce variance but linearly increase runtime
  - **Learning rate scale**: MeZO LR = [5e-7, 5e-8, 5e-9] vs BP LR = [5e-4, 5e-5, 5e-6]

- Failure signatures:
  - Memory still exceeds budget → L'/L too high; profile tensor allocations
  - Accuracy plateaus early → Learning rate may be too low; perturbation scale ε may be mismatched to loss curvature
  - Convergence too slow for time budget → Consider hybrid approaches (LoHo from references) or Sparse MeZO

- First 3 experiments:
  1. Reproduce memory analysis: Measure actual peak memory for GPT2-medium BP vs Llama2-7B MeZO on identical hardware; compare to theoretical predictions from Eq. (2–3).
  2. Learning rate sensitivity sweep: Run MeZO with Llama2-7B on BoolQ with LR ∈ {1e-6, 5e-7, 1e-7, 5e-8, 1e-8}; plot convergence curves at 15, 30, 60, 120 minutes.
  3. L'/L ablation: Instrument memory allocator to enforce different L'/L values (0.05, 0.15, 0.25, 0.41); measure accuracy impact at fixed wall-clock time.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can MeZO-based fine-tuning be specialized for neuromorphic systems and other architectures with dynamic activation sparsity?
- Basis in paper: [explicit] The conclusion states: "More research is required to specialize the main conclusions to neuromorphic systems and other models with dynamic sparsity at the level of activations."
- Why unresolved: The current analysis assumes dense transformer architectures without dynamic sparsity patterns, which may alter the memory-accuracy trade-offs.
- What evidence would resolve it: Empirical evaluation of MeZO on neuromorphic hardware or sparse activation models, comparing memory savings and convergence behavior.

### Open Question 2
- Question: How does the performance gap between MeZO and BP vary across diverse downstream tasks beyond binary classification (BoolQ)?
- Basis in paper: [inferred] Experiments are limited to a single dataset (BoolQ) with binary question answering; generalization to other task types is unexplored.
- Why unresolved: Different tasks may have different sensitivity to model scale versus optimization quality, potentially changing the MeZO vs. BP trade-off.
- What evidence would resolve it: Benchmarking across multiple tasks (e.g., generation, multi-class classification, reasoning) under equivalent memory constraints.

### Open Question 3
- Question: What are optimal memory allocation strategies for minimizing L′ (buffered activation layers) in practical MeZO implementations?
- Basis in paper: [inferred] The L′/L parameter is implementation-specific; experiments use ad-hoc values (0.15 and 0.41) without systematic optimization.
- Why unresolved: Memory management policies depend on hardware-specific tensor buffering behaviors not characterized in this work.
- What evidence would resolve it: Systematic study of L′ values across different memory allocators and hardware platforms, measuring actual peak memory usage.

### Open Question 4
- Question: Can hybrid approaches combining MeZO with parameter-efficient fine-tuning (e.g., LoRA, AdaZeta) further improve the memory-accuracy frontier for on-device adaptation?
- Basis in paper: [inferred] The paper mentions extensions like AdaZeta and Sparse MeZO but does not evaluate them under the unified memory-constrained on-device setting.
- Why unresolved: Combining MeZO's activation-free property with low-rank adaptations could yield different optimal operating points.
- What evidence would resolve it: Comparative experiments measuring memory, convergence time, and accuracy for MeZO+PEFT combinations under fixed memory budgets.

## Limitations
- Cross-task generalization: Results are based on a single dataset (BoolQ) and may not generalize to other task types
- Noise scale sensitivity: The paper uses fixed perturbation magnitude without exploring adaptive scaling or warm-up schedules
- Hardware-specific gains: The memory savings ratios (L'/L = 0.15-0.41) were achieved on specific hardware and may vary across architectures

## Confidence
- **High confidence**: Memory savings theory (2×-25×) and BP activation overhead magnitude. The mathematical bounds and literature on transformer memory are well-established.
- **Medium confidence**: Task-level accuracy gains (82% vs 75%) because they are based on a single dataset and may not generalize.
- **Low confidence**: The claim that pre-trained LLMs inherently have low effective dimensionality is cited but not empirically validated in this paper.

## Next Checks
1. **Cross-task ablation**: Repeat the memory-constrained fine-tuning on GLUE benchmark tasks (SST-2, MNLI, QQP) to verify if MeZO consistently achieves higher accuracy than BP within the same memory budget.
2. **Adaptive ε tuning**: Implement an adaptive perturbation magnitude schedule (e.g., ε_t = ε_0 / √t) and measure if convergence improves while maintaining the memory advantage.
3. **L'/L stress test**: Systematically vary the activation buffering ratio L'/L (0.05, 0.10, 0.20, 0.41) on a memory-constrained device and measure the trade-off between peak memory usage and final task accuracy.