---
ver: rpa2
title: Large Language Models and Synthetic Data for Monitoring Dataset Mentions in
  Research Papers
arxiv_id: '2502.10263'
source_url: https://arxiv.org/abs/2502.10263
tags:
- data
- dataset
- mentions
- research
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning framework that automates
  dataset mention detection in research papers by leveraging large language models,
  synthetic data generation, and a two-stage fine-tuning approach. The method addresses
  the scarcity of labeled training data through a weakly supervised learning pipeline
  that extracts, validates, and refines dataset mentions using LLM-based zero-shot
  extraction, quality assessment, and autonomous reasoning.
---

# Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers

## Quick Facts
- arXiv ID: 2502.10263
- Source URL: https://arxiv.org/abs/2502.10263
- Authors: Aivin V. Solatorio; Rafael Macalaba; James Liounis
- Reference count: 40
- Primary result: Two-stage fine-tuning approach achieves 71.43 Fβ-score on dataset mention detection

## Executive Summary
This paper presents a machine learning framework that automates dataset mention detection in research papers by leveraging large language models, synthetic data generation, and a two-stage fine-tuning approach. The method addresses the scarcity of labeled training data through a weakly supervised learning pipeline that extracts, validates, and refines dataset mentions using LLM-based zero-shot extraction, quality assessment, and autonomous reasoning. The Phi-3.5-mini model is pre-fine-tuned on synthetic data and further fine-tuned on manually annotated subsets, while a ModernBERT classifier filters candidate passages to reduce computational overhead. Evaluated on a held-out annotated sample, the fine-tuned model achieves a 71.43 Fβ-score, outperforming NuExtract-v1.5 and GLiNER-large-v2.1.

## Method Summary
The framework uses a three-stage pipeline: (1) GPT-4o-mini extracts dataset mentions via zero-shot prompting, (2) an LLM-as-a-Judge filters obvious errors, and (3) a reasoning agent applies structured self-evaluation to catch subtler misclassifications. A ModernBERT classifier filters passages before LLM extraction to reduce computational cost. The Phi-3.5-mini model is pre-fine-tuned on synthetic data (21,408 mentions) for 10 epochs, then fine-tuned on manually annotated data for 20 epochs using 16-rank LoRA. The approach is evaluated on a test set of 20 annotated pages, achieving 71.43 Fβ-score.

## Key Results
- Two-stage fine-tuning (synthetic + curated) achieves 71.43 Fβ-score, outperforming synthetic-only (61.76) and curated-only (57.58) approaches
- Multi-stage LLM refinement pipeline filters 42% of initial 37,225 mentions, retaining 21,408 high-quality dataset mentions
- ModernBERT classifier achieves 100% precision and recall in filtering passages for LLM extraction

## Why This Works (Mechanism)

### Mechanism 1
Pre-fine-tuning on synthetic data followed by curated fine-tuning improves extraction performance over either stage alone. Synthetic data exposes the model to diverse dataset mention patterns, establishing broad representations. Curated fine-tuning then refines decision boundaries using human-verified labels, reducing false positives while retaining generalization.

### Mechanism 2
Multi-stage LLM refinement (extraction → judgment → reasoning) reduces false positives in synthetic training data. Zero-shot extraction captures broad candidates but includes noise (organizations, reports misclassified as datasets). LLM-as-a-Judge filters obvious errors. Reasoning agent applies structured self-evaluation with devil's-advocate review to catch subtler misclassifications.

### Mechanism 3
Encoder-based filtering before LLM extraction reduces computational cost while maintaining recall. ModernBERT classifies passages as likely containing dataset mentions. Only positive passages proceed to extraction, reducing LLM inference calls. ModernBERT's 2048-token context window captures mentions in longer passages that 512-token BERT misses.

## Foundational Learning

- **Weakly Supervised Learning**: The pipeline generates training labels via LLMs rather than human annotation, requiring understanding of how noisy labels propagate through fine-tuning. Quick check: Can you explain why synthetic-only training (61.76 Fβ) underperformed two-stage training (71.43 Fβ)?

- **LoRA (Low-Rank Adaptation)**: Fine-tuning uses 16-rank LoRA, not full parameter updates. Understanding this is critical for reproducing training and diagnosing underfitting. Quick check: If validation loss plateaus but training loss continues dropping during pre-fine-tuning, is this a LoRA capacity issue or a data quality issue?

- **Jaccard Similarity for Partial Matching**: The Fβ metric uses Jaccard similarity with 0.5 threshold, allowing partial matches. This affects how you interpret precision/recall tradeoffs. Quick check: A prediction of "World Bank Development Indicators" vs. ground truth "World Development Indicators" achieves what Jaccard score? Would it count as a match?

## Architecture Onboarding

- **Component map:**
Research Papers (PDFs) → Page-level text extraction → ModernBERT Classifier → Phi-3.5-mini Extractor → Structured output

Training pipeline (offline):
Raw papers → GPT-4o-mini Zero-shot Extractor → GPT-4o-mini LLM-as-Judge → GPT-4o-mini Reasoning Agent → Synthetic dataset → Phi-3.5-mini Pre-fine-tuning → Manual annotation → Phi-3.5-mini Fine-tuning

- **Critical path:** Pre-fine-tuning quality determines whether curated fine-tuning can effectively refine the model. If synthetic data has high noise, subsequent stages cannot recover performance.

- **Design tradeoffs:**
  - ModernBERT context window (2048 tokens) vs. inference speed—larger context improves recall but increases latency.
  - Synthetic data volume vs. noise—more data improves coverage but requires stronger filtering.
  - Test set size (n=20) vs. evaluation reliability—small test set limits statistical confidence in reported gains.

- **Failure signatures:**
  - Low recall in extraction: Check ModernBERT classifier false negative rate on held-out data.
  - High false positive rate: Review reasoning agent filtering; may need stricter classification criteria.
  - Overfitting during fine-tuning: Check if validation loss diverges from training loss; reduce learning rate or increase effective batch size.

- **First 3 experiments:**
  1. **Ablation by data source:** Train models on synthetic-only, curated-only, and two-stage configurations. Confirm 9.67-point Fβ improvement is reproducible with different random seeds and data splits.
  2. **Classifier recall audit:** Manually review 200 negative classifications from ModernBERT to measure false negative rate. Target: <5% miss rate on true dataset mentions.
  3. **Synthetic data quality assessment:** Sample 100 examples from each stage (zero-shot, post-judge, post-reasoning) and manually compute precision. Confirm reasoning agent improves precision without excessive recall loss.

## Open Questions the Paper Calls Out

### Open Question 1
Does pre-fine-tuning on synthetic data exhibit diminishing returns as the volume of synthetic data increases, and if so, at what scale? The paper uses a fixed synthetic dataset size (21,408 mentions) without systematically varying the volume to test the relationship between synthetic data quantity and marginal performance gains.

### Open Question 2
How well does the framework generalize to research domains beyond climate change literature? The current study evaluates only on climate-related papers from two sources, leaving domain transfer untested.

### Open Question 3
How reliable are the reported results given the very small test set (n=20)? Small test sets yield high variance in performance estimates, making it unclear whether observed differences between models reflect true capability gaps or sampling noise.

### Open Question 4
To what extent does topic-specific fine-tuning improve performance on specialized domains compared to the general climate-focused model? No experiments were conducted with domain-specific annotations to quantify the marginal benefit of specialization over the general model.

## Limitations

- Evaluation relies on a very small test set (n=20), limiting statistical confidence in reported performance improvements.
- Synthetic data generation pipeline's quality is difficult to verify without access to intermediate outputs or comprehensive human validation.
- Two-stage fine-tuning approach assumes synthetic data adequately represents target domain patterns, but this assumption is not externally validated.

## Confidence

- **High Confidence**: The two-stage training approach demonstrably outperforms either stage alone (71.43 Fβ vs 61.76 and 57.58). The ModernBERT classifier shows perfect precision and recall on the held-out sample.
- **Medium Confidence**: The multi-stage LLM refinement pipeline reduces synthetic data noise by filtering 42% of mentions, but the absolute quality of retained mentions is uncertain without broader validation.
- **Low Confidence**: Claims about synthetic data addressing scarcity and improving generalization are based on single dataset evaluation without cross-domain validation or comparison to alternative weakly supervised approaches.

## Next Checks

1. **Classifier Recall Audit**: Manually review 200 passages classified as negative by ModernBERT to measure false negative rate. Target: <5% miss rate on true dataset mentions.

2. **Synthetic Data Quality Validation**: Sample 100 examples from each LLM refinement stage (zero-shot, post-judge, post-reasoning) and manually compute precision. Confirm reasoning agent improves precision without excessive recall loss.

3. **Test Set Size Impact**: Evaluate the trained model on an expanded test set (n≥100) to establish statistical significance of performance improvements and identify potential overfitting to the small original test set.