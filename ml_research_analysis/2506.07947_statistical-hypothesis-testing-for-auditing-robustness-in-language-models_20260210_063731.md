---
ver: rpa2
title: Statistical Hypothesis Testing for Auditing Robustness in Language Models
arxiv_id: '2506.07947'
source_url: https://arxiv.org/abs/2506.07947
tags:
- language
- outputs
- testing
- perturbation
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Distribution-based perturbation analysis (DBPA) is a model-agnostic
  statistical framework for quantifying how input perturbations affect language model
  outputs. The method addresses the challenge of comparing entire output distributions
  despite computational intractability and semantic interpretability issues.
---

# Statistical Hypothesis Testing for Auditing Robustness in Language Models

## Quick Facts
- arXiv ID: 2506.07947
- Source URL: https://arxiv.org/abs/2506.07947
- Authors: Paulius Rauba; Qiyao Wei; Mihaela van der Schaar
- Reference count: 33
- Primary result: DBPA provides p-values, effect sizes, and multiple testing control for quantifying LLM output changes under input perturbations

## Executive Summary
This paper introduces Distribution-based Perturbation Analysis (DBPA), a statistical framework for quantifying how input perturbations affect language model outputs. The method addresses the fundamental challenge of comparing entire output distributions without enumerating the exponential output space. By constructing empirical null and alternative distributions via Monte Carlo sampling and pairwise similarity measures, DBPA enables frequentist hypothesis testing without restrictive distributional assumptions. The framework provides interpretable p-values, scalar effect sizes, and supports multiple testing with controlled error rates, demonstrating effectiveness across persona instruction and healthcare scenario case studies.

## Method Summary
DBPA constructs empirical output distributions through Monte Carlo sampling (k samples per condition) and reduces high-dimensional text comparisons to scalar similarity scores via embedding-based cosine similarity. The method computes Jensen-Shannon divergence between within-condition and cross-condition similarity distributions to quantify distributional shifts. A permutation test over pooled outputs yields valid frequentist p-values under exchangeability assumptions, requiring far fewer model queries than direct resampling approaches. The framework supports multiple testing with Benjamini-Hochberg FDR control and provides interpretable effect sizes as scalar measures of distributional change.

## Key Results
- Quantified response changes under persona instructions with effect sizes ranging 0.14-0.41
- Measured true/false positive rates in healthcare scenarios with AUC values 0.43-0.65
- Evaluated alignment between different language models while controlling stochastic variability
- Successfully captured statistically significant distributional shifts with controlled error rates

## Why This Works (Mechanism)

### Mechanism 1
Monte Carlo sampling provides tractable approximation of LLM output distributions without enumerating the exponential output space. Instead of computing full probability mass functions over all possible outputs, the method draws k i.i.d. samples from each condition, constructing empirical distributions. Variance scales as O(1/k), making statistical power controllable via sample size. Core assumption: k samples sufficiently characterize the output distribution for detecting semantic shifts. Evidence: [abstract] and [section 2.4] describe the sampling approach and variance scaling. Break condition: Sample size too small for effect size of interest, leading to insufficient power.

### Mechanism 2
Pairwise semantic similarity distributions enable interpretable comparison of output distributions by reducing to scalar similarity scores. Given k samples from each condition, construct P0 (intrinsic variability) and P1 (cross-distribution similarities) using cosine similarity on embeddings. This sidesteps high dimensionality and excess semantic information. Core assumption: Embedding function is deterministic, stable, and semantic-preserving. Evidence: [section 3.2] and [section 3] describe the reduction approach. Break condition: Embedding model fails to preserve semantic equivalence.

### Mechanism 3
Permutation testing over pooled outputs yields valid frequentist p-values under exchangeability assumption with fewer model queries than direct resampling. Under H0: Dx = Dx', the pooled vector is exchangeable, so permutation test reuses the same 2k generated outputs to deliver valid p-values. Core assumption: Exchangeability holds under the null hypothesis. Evidence: [section 3.1] and [section 3.2] describe the permutation approach. Break condition: Exchangeability violated due to different dependency structures.

## Foundational Learning

- Concept: **Permutation Testing and Exchangeability**
  - Why needed here: The entire inference framework rests on permutation-based p-values; without understanding exchangeability, you cannot diagnose when the test is invalid.
  - Quick check question: If you permuted similarity scores directly rather than raw outputs before computing similarities, would exchangeability still hold?

- Concept: **Jensen-Shannon Divergence**
  - Why needed here: JSD is the default discrepancy measure; understanding its properties (symmetry, boundedness [0,1], sensitivity to shape differences) is essential for interpreting effect sizes.
  - Quick check question: Why would JSD be preferred over KL divergence for comparing empirical similarity distributions?

- Concept: **Monte Carlo Variance and Statistical Power**
  - Why needed here: Sample size k directly controls estimator variance O(1/k) and test power; practitioners must trade off computational cost against detection sensitivity.
  - Quick check question: If you need to detect an effect size ω = 0.1 at α = 0.05 with 80% power, approximately how does the required k scale?

## Architecture Onboarding

- Component map:
  Response Sampler -> Embedding Encoder -> Similarity Computer -> Distribution Comparator -> Permutation Engine

- Critical path:
  1. Define perturbation Δx and generate x' = Δx(x)
  2. Sample k responses from S(x) and k from S(x') — dominant API cost
  3. Embed all 2k responses
  4. Compute P0 (k(k-1)/2 similarities) and P1 (k² similarities)
  5. Compute Tobs = JSD(P0 || P1)
  6. Run B permutations to build null distribution of T
  7. Compute p-value and interpret

- Design tradeoffs:
  - Larger k: Higher power, lower variance, but 2x cost per additional sample
  - More permutations B: More precise p-values, but linear cost increase
  - Embedding model choice: ada-002 vs. cheaper alternatives—affects semantic sensitivity
  - Distance measure ω: JSD vs. Wasserstein vs. Energy distance—sensitive to different distributional properties

- Failure signatures:
  - P-values cluster near 0 or 1 with insufficient variability: k too small
  - Inconsistent results across embedding models: Embedding not semantic-preserving for your domain
  - High false positive rate on control perturbations: Exchangeability assumption violated
  - Effect sizes near 0 but significant p-values: Overpowered test detecting trivial differences

- First 3 experiments:
  1. Sanity check with null perturbation: Apply identity perturbation (x' = x); verify p-values uniformly distributed and effect sizes near zero.
  2. Power analysis: For known semantic perturbations (e.g., persona shift), sweep k ∈ {10, 25, 50, 100} and plot p-value stability to determine minimum viable sample size.
  3. Embedding sensitivity test: Run same perturbation analysis with 2-3 different embedding models (ada-002, text-embedding-3-small, open-source alternative); verify conclusions robust to embedding choice.

## Open Questions the Paper Calls Out

### Open Question 1
What are the rigorous selection guidelines for key design choices, such as distance metrics and embedding functions, to ensure generalizability? The authors state in the Limitations section that "Key design choices—such as distance metrics, embedding functions, and their effect on generalization—would benefit from deeper analysis and clearer selection guidelines that should be established in future work." This remains unresolved because while the paper demonstrates that results are sensitive to these choices, it provides no theoretical or heuristic framework for selecting the optimal configuration. A comparative study or theoretical framework linking specific properties of embeddings and metrics to DBPA's statistical power and error rates across tasks would resolve this.

### Open Question 2
How can the statistical inferences provided by DBPA be translated into practical strategies for enhancing model robustness and alignment? The Discussion section notes that "translating these findings into practical strategies for enhancing model robustness and aligning outputs with human preferences remains a significant challenge." The current work focuses on auditing (detecting significant shifts) but does not define a mechanism to mitigate unwanted sensitivities or enforce stability based on the test results. A demonstration of a closed-loop system where DBPA results are used as a reward signal or constraint to regularize model training or decoding would resolve this.

### Open Question 3
How can the number of Monte Carlo samples (k) be adaptively determined to optimize the trade-off between statistical power and computational cost? Section 2.4 states that "The number of MC samples k can be treated as a hyperparameter and it should be adapted based on the problem setup," implying the lack of an automated or principled selection method. The paper relies on manual selection of k, leaving the automation of this efficiency-power balance as an open methodological detail. An algorithm that dynamically adjusts k based on real-time estimates of effect size or variance during the sampling process would resolve this.

### Open Question 4
Does the DBPA framework maintain its reliability and statistical power in high-stakes domains outside of the healthcare and persona case studies presented? The authors explicitly request that "Future work should add stronger empirical evidence and test the method in other domains." The paper validates the method primarily on medical recommendations and role-playing prompts; behavior in legal, financial, or code-generation contexts remains unverified. Successful application of the framework in diverse domains (e.g., legal reasoning or code synthesis) demonstrating controlled Type I errors and high true positive rates would resolve this.

## Limitations

- Computational expense requires 2k LLM queries per hypothesis test plus B permutations, making large-scale auditing costly
- Effectiveness hinges on semantic embedding space faithfully capturing human-relevant differences between outputs
- Assumes exchangeability under the null hypothesis, which may not hold for models with memory or stateful generation

## Confidence

- **High Confidence**: The core statistical framework (Monte Carlo sampling, permutation testing, JSD effect size) is well-established and mathematical derivations are sound
- **Medium Confidence**: Empirical results demonstrating real-world utility across different perturbation types are compelling but based on limited case studies
- **Low Confidence**: The assertion that DBPA is "model-agnostic" beyond demonstrated cases with GPT-4, GPT-3.5, and Llama-2-70B requires broader validation

## Next Checks

1. Power Analysis Validation: Systematically sweep k values for perturbations of known effect sizes (e.g., identity vs. large persona shifts) and plot power curves to establish minimum viable sample sizes for detecting small, medium, and large effects

2. Cross-Modeling Generalization: Apply DBPA to three additional LLM architectures (including open-source models with different training regimes) and compare effect size distributions to test the claimed model-agnostic property

3. Semantic Fidelity Stress Test: Create adversarial output pairs that are semantically identical but syntactically different, and vice versa, then measure how well DBPA's embedding-based similarity captures human judgments of equivalence versus surface-level changes