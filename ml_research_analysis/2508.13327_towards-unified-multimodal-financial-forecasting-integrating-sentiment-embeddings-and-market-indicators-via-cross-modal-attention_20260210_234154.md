---
ver: rpa2
title: 'Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings
  and Market Indicators via Cross-Modal Attention'
arxiv_id: '2508.13327'
source_url: https://arxiv.org/abs/2508.13327
tags:
- financial
- numerical
- data
- sentiment
- market
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents STONK, a multimodal framework that combines\
  \ numerical market indicators with sentiment-enriched news embeddings using feature\
  \ concatenation and cross-modal attention. The model outperforms numeric-only baselines\
  \ in stock-movement prediction, achieving classification accuracy of 0.65\u2013\
  0.68 and F1 scores of 0.72\u20130.73."
---

# Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention

## Quick Facts
- arXiv ID: 2508.13327
- Source URL: https://arxiv.org/abs/2508.13327
- Reference count: 15
- Key outcome: STONK framework achieves 0.65-0.68 accuracy and 0.72-0.73 F1 scores, with Sharpe ratios up to 3.15 and profit factors of 2.03 in multimodal stock prediction

## Executive Summary
This study introduces STONK, a multimodal framework that combines numerical market indicators with sentiment-enriched news embeddings for stock movement prediction. The model employs feature concatenation and cross-modal attention mechanisms to integrate textual and numerical data sources. Domain-adapted encoders (MiniLM, DeBERTa) are fine-tuned to improve performance, demonstrating that multimodal approaches outperform numeric-only baselines in financial forecasting tasks.

## Method Summary
STONK integrates sentiment embeddings from news articles with traditional market indicators through a cross-modal attention mechanism. The framework uses domain-adapted encoders fine-tuned on financial text data to capture nuanced market sentiment. Numerical indicators and textual embeddings are concatenated and processed through attention layers that learn inter-modal relationships. The model is trained on historical stock data with labeled movements, evaluating both classification metrics and financial performance indicators like Sharpe ratio and profit factor.

## Key Results
- Classification accuracy of 0.65-0.68 and F1 scores of 0.72-0.73
- Sharpe ratios up to 3.15 and profit factors reaching 2.03
- Domain-adapted encoders (MiniLM, DeBERTa) improve performance over numeric-only baselines

## Why This Works (Mechanism)
The cross-modal attention mechanism effectively captures interactions between market indicators and sentiment signals, allowing the model to weigh the relative importance of textual and numerical features for different prediction contexts. By fine-tuning domain-specific encoders on financial text, the framework better captures industry-specific terminology and market-relevant sentiment patterns. The integration of multiple data modalities provides complementary information that single-modality approaches miss, leading to more robust predictions across varying market conditions.

## Foundational Learning
- Cross-modal attention: Why needed - to learn relationships between textual and numerical features; Quick check - verify attention weights sum to reasonable distributions across modalities
- Domain adaptation: Why needed - financial text contains specialized terminology requiring specific training; Quick check - compare performance with generic vs. domain-specific embeddings
- Multimodal integration: Why needed - market movements influenced by both quantitative metrics and qualitative sentiment; Quick check - ablate individual modalities to measure performance degradation

## Architecture Onboarding

**Component Map**: News Articles -> Sentiment Encoder -> Sentiment Embeddings -> Cross-Modal Attention -> Numerical Indicators -> Feature Concatenation -> Prediction Layer

**Critical Path**: The cross-modal attention layer serves as the critical integration point where sentiment embeddings and numerical indicators interact to produce final predictions. Domain-adapted encoders must be pre-trained before fine-tuning on financial data.

**Design Tradeoffs**: The framework trades computational complexity for improved accuracy through multimodal integration. Using pre-trained domain-adapted encoders reduces training time but requires careful fine-tuning to avoid catastrophic forgetting. The attention mechanism adds interpretability but increases model complexity.

**Failure Signatures**: Poor performance on out-of-sample data suggests overfitting to specific market conditions. Degradation in attention-based integration may indicate insufficient training data or poorly aligned feature spaces between modalities.

**3 First Experiments**:
1. Ablation study removing sentiment embeddings to measure contribution of textual data
2. Test different attention mechanisms (dot-product vs. additive) for cross-modal integration
3. Evaluate performance with different encoder architectures (BERT vs. DeBERTa) on sentiment extraction

## Open Questions the Paper Calls Out
None

## Limitations
- High financial metrics (Sharpe ratio up to 3.15, profit factor 2.03) may be sensitive to specific market conditions and may not generalize
- Cross-modal attention effectiveness demonstrated but specific contributions of textual vs. numerical features remain unclear
- Real-world trading scenarios including transaction costs and market impact are not evaluated

## Confidence
- Primary claims: Medium
- Methodology soundness: Medium
- Generalizability across markets: Low
- Real-world applicability: Medium

## Next Checks
1. Test model performance across multiple economic cycles and market regimes to assess robustness beyond current evaluation period
2. Conduct out-of-sample validation using data from different geographic markets and asset classes to verify generalizability
3. Implement comprehensive backtesting framework including realistic transaction costs, slippage, and market impact to evaluate practical trading viability