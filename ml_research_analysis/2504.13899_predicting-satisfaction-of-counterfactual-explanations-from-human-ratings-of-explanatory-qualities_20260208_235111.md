---
ver: rpa2
title: Predicting Satisfaction of Counterfactual Explanations from Human Ratings of
  Explanatory Qualities
arxiv_id: '2504.13899'
source_url: https://arxiv.org/abs/2504.13899
tags:
- satisfaction
- explanations
- metrics
- counterfactual
- overall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different explanatory qualities of
  counterfactual explanations predict user satisfaction. Using a dataset of 30 counterfactual
  scenarios rated by 206 participants across seven metrics (feasibility, coherence,
  complexity, understandability, completeness, fairness, trust) plus overall satisfaction,
  the authors model satisfaction as a function of these metrics.
---

# Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities

## Quick Facts
- arXiv ID: 2504.13899
- Source URL: https://arxiv.org/abs/2504.13899
- Reference count: 30
- Primary result: Feasibility and trust are the strongest predictors of satisfaction with counterfactual explanations

## Executive Summary
This study investigates how different explanatory qualities of counterfactual explanations predict user satisfaction. Using a dataset of 30 counterfactual scenarios rated by 206 participants across seven metrics (feasibility, coherence, complexity, understandability, completeness, fairness, trust) plus overall satisfaction, the authors model satisfaction as a function of these metrics. The analysis reveals that feasibility and trust are the strongest predictors of satisfaction, with completeness also playing a meaningful role. Even without feasibility and trust, other metrics explain 58% of satisfaction variance, highlighting their importance. Complexity appears largely independent of satisfaction, suggesting that more detailed explanations don't necessarily reduce satisfaction. Strong correlations among metrics imply a latent structure in how users judge quality, and demographic background significantly shapes evaluation patterns. These findings suggest that XAI systems should prioritize generating feasible, trustworthy counterfactuals while considering domain-specific needs and user expertise when designing explanations.

## Method Summary
The study uses the CounterEval dataset containing 30 counterfactual scenarios rated by 196 participants across seven explanatory quality metrics (feasibility, consistency, completeness, trust, understandability, fairness, complexity) on 1-6 Likert scales. Satisfaction is modeled both as a continuous variable (regression) and binned into low/medium/high classes (classification). Two split strategies are employed: random 80/20 splits with 5-fold CV, and scenario-based splits (24 train/6 test scenarios). Models include OLS regression, DecisionTree, and RandomForest, with SHAP used for feature importance. SMOTE-TOMEK resampling addresses class imbalance in classification tasks.

## Key Results
- Feasibility and trust are the strongest predictors of satisfaction, with coefficients β ≈ 0.36 each (p < 0.001)
- 58% of satisfaction variance remains explainable even when excluding feasibility and trust
- Factor analysis reveals three latent factors explaining 65.9% of variance, with feasibility, consistency, trust, completeness, and fairness loading together
- Domain expertise (ML or medical background) significantly shapes how users evaluate explanatory qualities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feasibility and trust are the strongest predictors of user satisfaction with counterfactual explanations.
- Mechanism: Users evaluate explanations primarily through two questions: "Can I act on this?" (feasibility) and "Will it work?" (trust). These judgments dominate the satisfaction response because counterfactuals are inherently action-oriented.
- Core assumption: Satisfaction reflects users' practical assessment of explanation utility rather than abstract quality judgments.
- Evidence anchors:
  - [abstract] "feasibility (the actionability of suggested changes) and trust (the belief that the changes would lead to the desired outcome) consistently stand out as the strongest predictors of user satisfaction"
  - [section] OLS regression: Feasibility β ≈ 0.358, Trust β ≈ 0.362, both p < 0.001; removing them drops R² from 0.757 to 0.580
  - [corpus] Related work on actionable recourse (VanNostrand et al., 2024) corroborates feasibility's role, but corpus lacks direct replication of this specific predictive relationship.
- Break condition: If users lack agency to implement changes (e.g., purely diagnostic contexts), feasibility may decouple from satisfaction.

### Mechanism 2
- Claim: Explanatory metrics cluster into latent factors that users evaluate holistically rather than independently.
- Mechanism: Rather than assessing each quality dimension separately, users form an integrated "good explanation" impression. Factor analysis reveals three latent factors explaining 65.9% of variance, with feasibility, consistency, trust, completeness, and fairness loading together.
- Core assumption: Users cannot or do not cognitively isolate individual metric dimensions when forming satisfaction judgments.
- Evidence anchors:
  - [abstract] "Strong correlations among metrics imply a latent structure in how users judge quality"
  - [section] Factor analysis: Factor 1 (40.5% variance) loads feasibility (0.78), consistency (0.77), trust (0.79), completeness (0.63), fairness (0.69); complexity loads weakly (0.03)
  - [corpus] No direct corpus evidence on latent factor structure in XAI evaluation.
- Break condition: If metrics are presented sequentially with forced independent rating, latent structure may weaken.

### Mechanism 3
- Claim: Domain expertise (ML or medical background) significantly shifts how users weight explanatory qualities.
- Mechanism: Experts apply different evaluation criteria based on professional norms—ML experts may scrutinize model reasoning more critically, while medical professionals require stronger justification before trusting outputs.
- Core assumption: Expertise changes both evaluation patterns and thresholds for satisfaction.
- Evidence anchors:
  - [abstract] "demographic background significantly shapes ranking patterns"
  - [section] Chi-square tests: ML Experience (p = 0.0299) and Medical Background (p = 0.0162) differ significantly across bi-clusters; Age, Education not significant
  - [corpus] Ghassemi et al. (cited in paper) discuss expert caution in medical AI; corpus lacks direct replication of expertise-satisfaction linkage.
- Break condition: Expertise effects may diminish with standardized training or domain-agnostic explanation formats.

## Foundational Learning

- Concept: **Counterfactual Explanations**
  - Why needed here: The entire evaluation framework concerns how users judge "what if" style explanations that suggest minimal input changes to achieve different outcomes.
  - Quick check question: Can you distinguish a counterfactual explanation ("If X changed, outcome would be Y") from a feature-importance explanation ("X contributed 30% to the decision")?

- Concept: **Proxy Metrics vs. Human Evaluation in XAI**
  - Why needed here: The paper critiques automated metrics (sparsity, proximity) as failing to capture human preferences, motivating the multi-metric human evaluation approach.
  - Quick check question: Why might a sparse counterfactual (changing one feature) receive low user satisfaction despite scoring well on algorithmic metrics?

- Concept: **Regression vs. Classification Modeling**
  - Why needed here: The paper models satisfaction both continuously (R² metrics) and categorically (low/medium/high classes), with different model performance patterns.
  - Quick check question: Why would a Decision Tree overfit in scenario-based splits but perform well in random splits?

## Architecture Onboarding

- Component map:
  Data layer (CounterEval dataset) -> Feature layer (7 explanatory metrics) -> Model layer (OLS, DecisionTree, RandomForest) -> Evaluation layer (random vs scenario-based splits)

- Critical path:
  1. Load ratings cube → 2. Aggregate or preserve individual-level data → 3. Train satisfaction predictors → 4. Compare random vs. scenario-based generalization → 5. Extract feature importance (coefficients, SHAP)

- Design tradeoffs:
  - Linear models offer interpretability (β coefficients) but may miss interactions; trees capture nonlinearity but overfit unseen scenarios
  - Including demographic features could improve personalization but reduces generalizability
  - Scenario-based splits test true generalization but yield higher variance (RMSE 0.891 ± 0.071 vs. 0.890 ± 0.017)

- Failure signatures:
  - R² drops >15% between random and scenario-based splits → model overfitting to scenario-specific patterns
  - High feature correlation (>0.7) with low individual β significance → multicollinearity masking true importance
  - Complexity showing near-zero importance across all models → metric may be measuring orthogonal construct

- First 3 experiments:
  1. **Baseline replication**: Replicate OLS regression on the 7-metric feature set, verify R² ≈ 0.75 and feasibility/trust coefficients ≈ 0.36.
  2. **Ablation study**: Systematically remove top predictors (feasibility, then trust) and measure R² degradation to confirm 0.58 residual explanatory power.
  3. **Scenario generalization test**: Train on 24 scenarios, test on held-out 6; if Decision Tree R² drops below 0.60, investigate which scenario types cause failure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do the predictive models of satisfaction generalize to specialized domains (e.g., medical diagnostics, financial lending) and to expert user cohorts such as clinicians or ML engineers?
- Basis in paper: [explicit] The authors state: "Future research could validate these findings in other domains (e.g., medical, financial) and with specialized user cohorts (e.g., clinicians, ML engineers)."
- Why unresolved: The study used only 30 general scenarios rated by a mixed pool of 206 participants; domain-specific constraints and expert mental models may alter which metrics drive satisfaction.
- What evidence would resolve it: Replicate the modeling approach with domain-specific counterfactual scenarios and expert participants, then compare coefficient stability and predictive accuracy to the current findings.

### Open Question 2
- Question: Under what conditions does fairness become a decisive predictor of satisfaction, and can its effect be disentangled from correlated metrics like feasibility and trust?
- Basis in paper: [explicit] The authors note: "Fairness did not emerge as driver of satisfaction in this dataset... fairness's impact may have been absorbed by correlated metrics" and suggest "Future work should examine user groups that are more sensitive to these specific metrics."
- Why unresolved: The dataset did not systematically vary fairness independently of other qualities, creating multicollinearity that masks fairness's unique contribution.
- What evidence would resolve it: Design controlled experiments where fairness is orthogonally manipulated while holding other metrics constant, across scenarios with varying discrimination-sensitivity.

### Open Question 3
- Question: Can incorporating user demographic and expertise variables (e.g., ML experience, medical background) as predictive features improve satisfaction modeling beyond using only explanatory metrics?
- Basis in paper: [inferred] The paper found that "Machine Learning Experience and Medical Background emerged as the most relevant demographic factors, both showing statistically significant differences across the clusters (p = 0.0299 and p = 0.0162, respectively)," yet explicitly notes: "demographic variables were not used as predictive features."
- Why unresolved: While demographic influences on evaluation patterns were demonstrated, their predictive utility for modeling satisfaction remains untested.
- What evidence would resolve it: Train models with and without demographic features, comparing predictive performance and examining whether different user subgroups require different metric weightings.

## Limitations
- Cross-sectional design captures immediate reactions rather than sustained utility of counterfactual explanations
- Single controlled experiment limits generalizability to real-world deployment contexts
- Factor analysis suggests latent structure but doesn't establish causation between metrics and satisfaction

## Confidence
- **High**: Feasibility and trust as top predictors (strongest evidence: OLS coefficients p < 0.001, consistent across models)
- **Medium**: Latent factor structure interpretation (supported by factor analysis but lacks external validation)
- **Medium**: Demographic expertise effects (statistically significant but effect sizes not quantified)

## Next Checks
1. Replicate the scenario-based split experiment with different train/test scenario partitions to verify the 15% R² drop isn't due to specific scenario selection
2. Conduct longitudinal study measuring satisfaction decay over time for explanations rated highly on feasibility vs. trust
3. Test whether the latent factor structure holds when controlling for scenario difficulty and participant expertise simultaneously