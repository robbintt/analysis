---
ver: rpa2
title: 'Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop
  Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures'
arxiv_id: '2601.09293'
source_url: https://arxiv.org/abs/2601.09293
tags:
- machine
- scheduling
- policy
- agent
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel framework for solving Dynamic Job Shop
  Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic
  job arrivals and unexpected machine breakdowns. The approach uses Coloured Timed
  Petri Nets to represent the scheduling environment, combined with Maskable Proximal
  Policy Optimization (MPPO) to enable dynamic decision-making while restricting the
  agent to feasible actions at each decision point.
---

# Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures

## Quick Facts
- **arXiv ID:** 2601.09293
- **Source URL:** https://arxiv.org/abs/2601.09293
- **Reference count:** 40
- **Primary result:** Novel MPPO framework with action masking consistently outperforms traditional heuristics on dynamic JSSP benchmarks under stochastic arrivals and machine failures.

## Executive Summary
This paper introduces a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, combining Coloured Timed Petri Nets (CTPN) with Maskable Proximal Policy Optimization (MPPO). The approach addresses the challenges of stochastic job arrivals and unexpected machine breakdowns by using action masking to restrict the agent to feasible actions at each decision point. Random job arrivals are modeled using a Gamma distribution to capture complex temporal patterns, while machine failures are modeled using a Weibull distribution to represent age-dependent degradation. Extensive experiments on dynamic JSSP benchmarks demonstrate that the method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization, highlighting the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies.

## Method Summary
The framework uses CTPN to represent the scheduling environment, where places represent job/machine states and transitions represent possible actions. The reinforcement learning agent observes the Petri net marking (token distribution) and uses Maskable PPO to select which enabled transition to fire. Two action-masking strategies are explored: gradient-free masking that assigns -∞ to invalid action logits, and gradient-based masking that adds a penalty loss for invalid actions. The environment incorporates stochastic elements through Gamma-distributed job arrivals and Weibull-distributed machine failures, with the agent trained to minimize makespan using sparse rewards (final makespan penalty only). The implementation uses Stable-Baselines3 for the RL component and PetriRL for the CTPN environment.

## Key Results
- The MPPO framework with action masking consistently outperforms 12 dispatching heuristics on dynamic JSSP benchmarks
- Gradient-free masking shows superior stability compared to gradient-based masking in experiments
- The framework demonstrates resilience to uncertainty, maintaining performance under stochastic arrivals and machine failures
- Training time scales significantly with problem size, from 5×10⁵ steps for small instances to 2×10⁷ steps for 100×20 instances

## Why This Works (Mechanism)

### Mechanism 1: Constraint-Guided Exploration via Logit Masking
The framework utilizes Petri Net guard functions to generate a dynamic Boolean mask, applied to the policy network's output logits by setting invalid action logits to negative infinity before the softmax operation. This mathematically forces the probability of invalid actions to zero, ensuring the agent wastes no gradient steps exploring physically impossible states. The core assumption is that action validity can be strictly determined by the current system state and fixed structural rules. Evidence shows training without masking results in unstable, noisy rewards compared to the smooth convergence of the masked agent.

### Mechanism 2: Decoupled State Representation (Hybrid CTPN-RL)
The CTPN handles the "physics" of the factory—concurrency, precedence, and resource locking—via token flow, while the RL agent observes only the "marking" and chooses which enabled transition to fire. This reduces the learning burden: the agent learns what to prioritize rather than how the machinery moves. The core assumption is that the Petri Net model accurately captures all relevant system dynamics so the agent can trust the environment's state transitions. The framework demonstrates this separation through distinct "Jobs Management Block" and "Machines Block" in the net structure.

### Mechanism 3: Stochastic Generalization via Parametric Distributions
The framework samples machine downtimes from a Weibull distribution (capturing age-dependent wear-out) and job arrivals from a Gamma distribution (capturing burstiness/clustering). This forces the agent to learn robust heuristics that survive non-stationary conditions, rather than overfitting to a deterministic schedule. The core assumption is that the shape parameters chosen for training closely approximate the target deployment environment. Performance degradation with random arrivals validates that the agent is reacting to increased uncertainty.

## Foundational Learning

- **Concept: Markov Decision Processes (MDP) & The Credit Assignment Problem**
  - **Why needed here:** The paper uses sparse rewards (penalty only at the end of the episode), making it essential to understand how PPO propagates this final reward back through the sequence of firing decisions.
  - **Quick check question:** If the agent receives a reward of -150 at step 500, how does it determine which of the 500 transitions were responsible?

- **Concept: Petri Net Primitives (Places, Transitions, Tokens)**
  - **Why needed here:** The state observation is a "marking" (vector of token counts), requiring interpretation of the input vector as "Job 1 is in Place P3 waiting for Machine 2" to debug the agent's behavior.
  - **Quick check question:** In a Coloured Petri Net, what prevents a token representing a "Steel Job" from entering a transition guarded by `color == 'Aluminum'`?

- **Concept: Proximal Policy Optimization (PPO) Clipping**
  - **Why needed here:** The paper selects PPO for stability, and understanding the clipping objective explains why training curves show stable reward rises without catastrophic collapses.
  - **Quick check question:** Why does PPO restrict the size of policy updates (clipping) during training, and what would happen to the schedule policy if it didn't?

## Architecture Onboarding

- **Component map:** Petri Net Definition -> Observation Encoding -> Mask Integration -> Agent Training
- **Critical path:** 1) Petri Net Definition: Mapping factory floor to Places and Transitions, 2) Observation Encoding: Serializing token state into vector, 3) Mask Integration: Ensuring gradient flow bypasses masked outputs
- **Design tradeoffs:** Gradient-free masking is safer for strict constraints, while gradient-based might help the agent "learn" constraints but risks instability. Sparse rewards avoid reward hacking but demand more training samples.
- **Failure signatures:** Deadlock (agent stops taking actions, implying Petri Net logic error), Masking Bypass (agent attempts invalid action, suggesting mask logic lag), Poor Generalization (performance degrades on larger instances, suggesting network capacity issues)
- **First 3 experiments:** 1) Sanity Check (Static): Run agent on deterministic 3×3 JSSP with no breakdowns to validate learning pipeline, 2) Ablation on Masking: Train with and without masking to quantify structural prior value, 3) Perturbation Test: Introduce single machine breakdown to observe dynamic rerouting capability

## Open Questions the Paper Calls Out
- **Can incorporating prior distribution characteristics of random events into the agent's observations improve anticipatory scheduling strategies?** The current framework relies on reactive observations and does not explicitly inform the agent of underlying probabilistic parameters governing future disruptions.
- **Is it possible to construct intermediate reward functions that accelerate credit assignment without inducing "reward hacking"?** The authors avoided frequent feedback because "reward shaping carries the risk of reward hacking," but this leaves the question of safe hybrid reward signals unresolved.
- **Can the proposed PetriRL policy generalize to larger industrial-scale instances without the retraining overhead shown in Figure 7?** The experiments train distinct agents for specific benchmark sizes, leaving transferability across different scales untested.

## Limitations
- Network architecture and exact hyperparameter values are not fully specified, making exact reproduction challenging
- Claims about superiority of Weibull/Gamma distributions over other uncertainty models lack direct comparative evidence
- Performance degradation with random arrivals suggests learned policy may not fully generalize to highly volatile conditions
- Study focuses on small to medium-sized instances (up to 100×20), leaving scalability to larger problems untested

## Confidence
- **High Confidence:** Core mechanism of action masking improving sample efficiency and convergence stability is well-supported by experimental evidence
- **Medium Confidence:** Hybrid CTPN-RL architecture improving learning by separating structural constraints from behavioral optimization is plausible but primarily supported by absence of reported deadlocks
- **Medium Confidence:** Training under Weibull/Gamma uncertainty inducing better generalization is supported by performance under test conditions but lacks direct comparison to alternative uncertainty distributions

## Next Checks
1. **Architecture Ablation:** Train and compare three variants (CTPN-RL with gradient-free masking, Pure NN-RL without CTPN, NN-RL with CTPN state but no masking) to isolate contribution of each component
2. **Uncertainty Distribution Sensitivity:** Train agents under uniform/random breakdowns/arrivals and compare generalization performance to Weibull/Gamma-trained agents on same test set
3. **Scalability Test:** Extend experimental scope to larger Taillard instances (e.g., 200×20 or 100×50) to evaluate framework's scalability limits and performance degradation with state-space complexity