---
ver: rpa2
title: 'FastEagle: Cascaded Drafting for Accelerating Speculative Decoding'
arxiv_id: '2509.20416'
source_url: https://arxiv.org/abs/2509.20416
tags:
- arxiv
- draft
- fasteagle
- eagle-3
- cascaded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# FastEagle: Cascaded Drafting for Accelerating Speculative Decoding

## Quick Facts
- arXiv ID: 2509.20416
- Source URL: https://arxiv.org/abs/2509.20416
- Reference count: 0
- Primary result: FastEagle achieves up to 1.87x throughput speedup on LLaMA-based models using cascaded drafting

## Executive Summary
FastEagle introduces a novel cascaded drafting approach for speculative decoding that achieves higher throughput than existing methods by generating multiple drafts in parallel and selecting the best candidate through sequential acceptance. The method builds upon EAGLE-3's draft-then-verify paradigm but introduces a critical innovation: using a smaller model to generate drafts for progressively larger models in a cascade, reducing computational overhead while maintaining quality. This approach addresses the fundamental trade-off in speculative decoding between draft speed and acceptance rate, demonstrating consistent improvements across various model sizes and benchmark datasets.

## Method Summary
FastEagle implements a cascaded drafting architecture where multiple smaller drafter models generate token sequences in parallel, which are then evaluated by a verifier model that accepts the first draft meeting quality criteria. Unlike traditional speculative decoding that uses a single draft model, FastEagle's cascade structure allows for rapid initial drafts from smaller models while progressively refining quality through deeper cascades. The sequential acceptance criterion evaluates drafts in order of increasing model size/complexity, terminating early when an acceptable draft is found. This design reduces computational overhead compared to running a single large draft model while maintaining or improving acceptance rates through the multi-stage quality control mechanism.

## Key Results
- Achieves up to 1.87x throughput improvement over baseline autoregressive decoding
- Outperforms EAGLE-3 in speed-quality trade-off across tested model sizes
- Demonstrates consistent performance gains across Vicuna and DeepSeek-Distill benchmarks

## Why This Works (Mechanism)
FastEagle works by leveraging the complementary strengths of multiple drafter models at different scales. The cascaded approach exploits the fact that smaller models can generate initial drafts very quickly, while larger models provide higher quality verification. By structuring the drafting process as a cascade rather than parallel independent attempts, FastEagle reduces redundant computation and focuses verification resources on the most promising candidates. The sequential acceptance criterion creates an efficient early-exit mechanism that terminates the drafting process as soon as acceptable quality is achieved, avoiding unnecessary computation from additional draft attempts.

## Foundational Learning
- Speculative decoding fundamentals: why needed - to accelerate LLM inference by generating tokens in parallel; quick check - understand draft-then-verify paradigm
- Multi-stage acceptance criteria: why needed - to balance speed and quality in cascaded systems; quick check - analyze sequential vs parallel acceptance strategies
- Cascade architecture design: why needed - to optimize resource allocation across draft models; quick check - examine depth vs width trade-offs in cascades
- Feature alignment in cascaded systems: why needed - to ensure consistency across draft model outputs; quick check - verify alignment metrics between cascade stages
- KV cache management in batching: why needed - to handle memory constraints at high batch sizes; quick check - monitor memory usage across different batch configurations

## Architecture Onboarding
- Component map: Input -> Cascade of Drafters (small → large) -> Sequential Verifier -> Output
- Critical path: Draft generation → Sequential acceptance evaluation → Token emission
- Design tradeoffs: Speed vs quality (smaller drafts faster but less accurate vs larger drafts slower but more reliable)
- Failure signatures: Acceptance rate degradation at deeper cascade levels, memory bottlenecks at high batch sizes, feature misalignment between cascade stages
- Three first experiments: 1) Benchmark throughput vs batch size on Vicuna model, 2) Ablation study varying cascade depth, 3) Memory profiling at different concurrency levels

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the memory footprint of the cascaded architecture be optimized to sustain speedups at batch sizes larger than 32?
- Basis in paper: [inferred] The authors note that in vLLM tests, FastEagle peaks at batch size 32 while EAGLE-3 peaks at 56, attributing the degradation to memory pressure from "additional KV cache states."
- Why unresolved: The current implementation becomes memory-bound at high concurrency, limiting its utility in high-throughput serving scenarios compared to autoregressive baselines.
- What evidence would resolve it: A modified implementation demonstrating sustained or superior throughput relative to EAGLE-3 at batch sizes > 56.

### Open Question 2
- Question: Can the training objective be refined to fully stabilize acceptance rates at deeper cascade levels to match EAGLE-3?
- Basis in paper: [inferred] Fig. 3 and the associated text highlight that while FastEagle is effective, it suffers a "mild depth-wise decline" in accept rate (0.81 to 0.74) whereas EAGLE-3 remains "most stable overall."
- Why unresolved: Error accumulation in feed-forward cascades appears intrinsically harder to control than in autoregressive loops, despite the introduction of multi-level supervision.
- What evidence would resolve it: Ablation results showing a flattened or increasing acceptance rate curve at draft depths 5-7, matching the stability of autoregressive drafting.

### Open Question 3
- Question: Does the single-pass cascaded approach generalize effectively to Mixture-of-Experts (MoE) architectures?
- Basis in paper: [inferred] Experiments are strictly limited to dense LLaMA-based architectures (Vicuna, DeepSeek-Distill), leaving the interaction with sparse expert routing unexplored.
- Why unresolved: MoE models utilize sparse activation patterns that may disrupt the feature alignment assumptions relied upon for the drafter's loss function.
- What evidence would resolve it: Benchmark results applying FastEagle to a sparse MoE model (e.g., DeepSeek-V3 or Mixtral) with analysis on feature alignment loss convergence.

## Limitations
- Memory bottlenecks at high batch sizes limit scalability compared to autoregressive baselines
- Depth-wise decline in acceptance rates suggests instability in deeper cascade configurations
- Limited evaluation scope restricted to dense LLaMA-based architectures without MoE testing
- Theoretical convergence guarantees for multi-stage drafting remain incomplete

## Confidence
- Core acceleration claims: High (consistent experimental improvements demonstrated)
- Architectural innovations: Medium (novel approach but limited comparative analysis)
- Theoretical framework: Low (several assumptions lack rigorous mathematical justification)

## Next Checks
1. Test cascaded drafting approach on diverse multilingual tasks to assess cross-lingual generalization
2. Conduct ablation studies varying number of drafting stages to determine optimal cascade depth for different model sizes
3. Implement formal convergence analysis with provable guarantees for sequential acceptance criterion under various distributional assumptions