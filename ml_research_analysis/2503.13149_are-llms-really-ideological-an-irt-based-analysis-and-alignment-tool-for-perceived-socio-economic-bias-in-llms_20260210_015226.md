---
ver: rpa2
title: Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for
  Perceived Socio-Economic Bias in LLMs
arxiv_id: '2503.13149'
source_url: https://arxiv.org/abs/2503.13149
tags:
- bias
- llms
- should
- political
- ideological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an Item Response Theory (IRT)-based framework
  for detecting and quantifying socioeconomic bias in large language models (LLMs)
  without relying on subjective human judgments. Unlike traditional methods, IRT accounts
  for item difficulty and response behavior, improving ideological bias estimation.
---

# Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs

## Quick Facts
- arXiv ID: 2503.13149
- Source URL: https://arxiv.org/abs/2503.13149
- Reference count: 40
- Primary result: IRT-based framework detects socio-economic bias in LLMs with R² = 0.896 for bias estimation and R² = 0.864 for response avoidance detection

## Executive Summary
This study introduces an Item Response Theory (IRT)-based framework for detecting and quantifying socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional methods, IRT accounts for item difficulty and response behavior, improving ideological bias estimation. We fine-tune two LLM families (Meta LLaMa 3.2-1B-Instruct and ChatGPT 3.5) to represent distinct ideological positions and apply a two-stage approach: (1) modeling response avoidance and (2) estimating perceived bias in answered responses. Results show that off-the-shelf LLMs often avoid ideological engagement rather than exhibit bias, with baseline ChatGPT refusing answers 92.55% of the time.

## Method Summary
The study uses a 105-item test inventory based on Everett's 2013 SECS framework covering economic and social conservatism/liberalism. Two-stage IRT scoring is applied: Stage 1 uses a 2PL model for PNA probability detection, and Stage 2 employs GPCM for ordinal bias estimation. Open-ended responses are generated from 6 models (baseline and fine-tuned variants) and classified to a 5-point scale using Mistral-Small:24b. The IRT models are fitted in R using the `mirt` package, with left-leaning items recoded before GPCM fitting.

## Key Results
- IRT framework achieved R² = 0.864 for response avoidance detection
- IRT framework achieved R² = 0.896 for bias estimation
- Baseline ChatGPT refused answers 92.55% of the time, indicating PNA behavior rather than bias

## Why This Works (Mechanism)
The IRT-based approach works by treating ideological bias detection as a psychometric measurement problem, where item difficulty and respondent ability are simultaneously estimated. The two-stage model first identifies response avoidance patterns (PNA behavior) using a 2PL IRT model, then estimates ideological position on answered items using GPCM. This separation is crucial because PNA behavior itself may indicate bias (avoiding engagement) rather than the content of responses. The framework distinguishes between genuine ideological positions and strategic avoidance, providing more nuanced bias measurement than simple classification accuracy.

## Foundational Learning
- **Item Response Theory (IRT)**: Psychometric framework for modeling item difficulty and respondent ability; needed to handle varying item characteristics and response patterns
- **Graded Response Model (GRM) / Generalized Partial Credit Model (GPCM)**: IRT models for ordinal responses; needed for 4-tier bias estimation (SA/A/D/SD)
- **Two-Parameter Logistic (2PL) Model**: IRT model for binary outcomes; needed for PNA probability estimation
- **Response Avoidance Classification**: Mapping open-ended responses to PNA vs. engaged categories; needed to separate avoidance from ideological content
- **Factor Analysis for Construct Validity**: Validating economic/social dimension separation; needed to ensure items measure intended constructs
- **Latent Trait Estimation (θ)**: Unidimensional ideological positioning; needed to create comparable bias scores across models

## Architecture Onboarding
- **Component Map**: Item Generation -> Response Generation -> Classification -> IRT Stage 1 (2PL) -> IRT Stage 2 (GPCM) -> θ Score Output
- **Critical Path**: The classification step is the critical path bottleneck, as errors propagate to both IRT stages and directly affect bias estimates
- **Design Tradeoffs**: Using LLM classification trades human annotation cost for potential classification errors; IRT provides statistical rigor but requires sufficient response variation
- **Failure Signatures**: Non-convergence warnings in mirt indicate sparse responses or poor item separation; high PNA rates (>90%) suggest task framing issues
- **First Experiments**: (1) Validate classification accuracy on 50-100 manual annotations; (2) Test prompt variations to reduce baseline PNA rates; (3) Verify IRT convergence with synthetic data before applying to real responses

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's validity depends on the accuracy of LLM-based response classification, which is not quantified in reported metrics
- Fine-tuning datasets remain unavailable due to ethics policy restrictions, preventing independent verification
- Findings about PNA behavior suggest off-the-shelf models may be unsuitable for ideological assessment without careful prompt engineering

## Confidence
- **High confidence**: IRT methodology and mathematical framework; statistical model performance metrics (R² values); reproducibility of two-stage approach
- **Medium confidence**: Interpretation of θ scores as meaningful ideological positioning; generalizability to other LLM architectures; stability across different classification models
- **Low confidence**: Absolute magnitude of ideological bias estimates; claim about models avoiding engagement rather than exhibiting bias; comparative ranking without confidence intervals

## Next Checks
1. Manually validate response classifications for 50-100 items across models to establish baseline accuracy
2. Systematically test different prompt formulations with baseline models to establish PNA rate sensitivity
3. Apply the framework to a parallel inventory covering socio-economic dimensions relevant to a non-US context