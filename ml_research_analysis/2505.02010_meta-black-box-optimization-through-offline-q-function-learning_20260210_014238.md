---
ver: rpa2
title: Meta-Black-Box-Optimization through Offline Q-function Learning
arxiv_id: '2505.02010'
source_url: https://arxiv.org/abs/2505.02010
tags:
- learning
- optimization
- offline
- algorithm
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Q-Mamba, an offline reinforcement learning
  framework for meta-black-box optimization (MetaBBO). The key idea is to decompose
  the high-dimensional action space of algorithm configuration into sequential Q-functions
  for each hyperparameter, enabling effective learning via a Mamba-based architecture.
---

# Meta-Black-Box-Optimization through Offline Q-function Learning

## Quick Facts
- arXiv ID: 2505.02010
- Source URL: https://arxiv.org/abs/2505.02010
- Reference count: 40
- Q-Mamba achieves competitive or superior optimization performance compared to online MetaBBO baselines while reducing training time by more than half.

## Executive Summary
This paper introduces Q-Mamba, an offline reinforcement learning framework for meta-black-box optimization (MetaBBO). The key idea is to decompose the high-dimensional action space of algorithm configuration into sequential Q-functions for each hyperparameter, enabling effective learning via a Mamba-based architecture. Q-Mamba achieves competitive or superior optimization performance compared to online MetaBBO baselines while reducing training time by more than half. It demonstrates strong zero-shot generalization to realistic neuroevolution tasks. Ablation studies confirm the importance of the conservative Q-loss and balanced exploration-exploitation data collection.

## Method Summary
Q-Mamba uses offline RL to learn a meta-level policy that configures hyperparameters of black-box optimization algorithms. The method decomposes the joint configuration space into sequential per-hyperparameter Q-functions learned autoregressively using a Mamba architecture. It trains on an E&E dataset (50% expert trajectories from pre-trained MetaBBO baselines, 50% random exploration) using a compositional Q-loss with conservative regularization. The approach achieves strong performance on BBOB benchmarks and demonstrates zero-shot generalization to neuroevolution tasks, with training times more than halved compared to online methods.

## Key Results
- Achieves comparable or better optimization performance than online MetaBBO baselines (RLPSO, LDE, GLEET)
- Reduces training time by more than 50% compared to online methods (13h vs 16h)
- Demonstrates zero-shot generalization to realistic neuroevolution tasks with continuous control
- Ablation studies confirm the importance of conservative Q-loss and balanced E&E data collection

## Why This Works (Mechanism)

### Mechanism 1: Q-function Decomposition for High-Dimensional Action Spaces
Decomposing the joint algorithm configuration space into sequential per-hyperparameter Q-functions reduces learning difficulty in massive action spaces. Instead of learning Q(s, a₁:K) over M^K possible configurations, the method learns K separate Q-functions autoregressively: Q(aᵢ|s, a₁:ᵢ₋₁). Each dimension's Q-value is predicted conditioned on previous decisions, following Eq. (2) in the paper. The Bellman backup propagates reward information from the final action dimension backward through inverse maximization.

### Mechanism 2: Conservative Q-learning with E&E Dataset
Combining exploitation data from trained MetaBBO baselines with exploration data from random policies enables stable offline learning while mitigating distribution shift. The compositional Q-loss (Eq. 5) adds three components: (1) TD-error Bellman backup for selected actions, (2) weighted loss on final action dimension to anchor Q-value accuracy, and (3) conservative regularization pushing out-of-distribution action Q-values toward zero. The E&E dataset (μ=0.5) provides both high-quality trajectories and diverse state-action coverage.

### Mechanism 3: Mamba Architecture for Long-Sequence Q-learning
Mamba's selective state space model provides superior long-sequence modeling for autoregressive Q-function prediction compared to Transformer architectures. Unlike Transformers with fixed attention patterns, Mamba parameterizes state transition matrices B and C as functions of input x(t), enabling selective memory/forgot of historical states. For Q-Mamba's sequences (T=500 optimization steps × K hyperparameters), this allows adaptive focus on relevant historical optimization states. Hardware-aware parallel scan (PrefixSum) enables training efficiency comparable to FlashAttention.

## Foundational Learning

- **Offline RL Distribution Shift**
  - Why needed here: Q-Mamba learns from fixed datasets without environment interaction. Distribution shift occurs when the learned policy queries state-action pairs outside the dataset support, causing Q-value overestimation and policy degradation.
  - Quick check question: Can you explain why maximizing Q-values on out-of-distribution actions is problematic when those Q-values were never trained on real outcomes?

- **Bellman Backup and Q-learning Consistency**
  - Why needed here: The decomposed Q-function maintains equivalence to joint Q-learning through carefully designed Bellman updates (Eq. 2). Understanding this ensures the decomposition doesn't introduce optimization bias.
  - Quick check question: How does the inverse maximization operation in Eq. (2) maintain consistency with the standard Bellman backup in Eq. (1)?

- **State Space Models (SSMs) and Selective Memory**
  - Why needed here: Mamba extends classical SSMs by making dynamics input-dependent. Understanding h(t) = Ah(t-1) + B(x(t))x(t) and how B, C become functions of input explains why Mamba handles variable-length dependencies better than RNNs.
  - Quick check question: Why does making B and C input-dependent allow the model to "selectively remember or forget" compared to fixed-parameter SSMs?

## Architecture Onboarding

- **Component map:**
  [Optimization State s_t (9-dim)] + [Action Token a_{i-1} (5-bit)] -> [Embedding Layer] -> [Mamba Block with hidden state h_{i-1}] -> [Q-value Head (Linear + LeakyReLU)] -> Q^t_i ∈ R^M (M=16 bins) -> [argmax] -> a^t_i -> [Tokenize] -> (loop to next hyperparameter i+1) After K hyperparameters: Execute algorithm A -> Get s_{t+1}

- **Critical path:**
  1. **Dataset construction**: Pre-train MetaBBO baselines (RLPSO, LDE, GLEET) on target algorithm/problem distribution -> collect μ·D expert trajectories + (1-μ)·D random trajectories
  2. **Tokenization**: Discretize continuous hyperparameters into M=16 bins using uniform binning; encode as 5-bit binary (00000-01111) plus start token (11111)
  3. **Training loop**: Sample trajectory τ from dataset -> compute compositional Q-loss (Eq. 5) -> AdamW update (lr=5e-3)
  4. **Inference**: Given new optimization problem, run autoregressive Q-prediction at each generation to configure algorithm

- **Design tradeoffs:**
  - **M=16 bins**: Coarser than M=64/128 but trains faster; BBO algorithms show low sensitivity to small hyperparameter changes
  - **μ=0.5**: Balanced E&E; higher μ risks overfitting to suboptimal expert behaviors, lower μ lacks quality demonstrations
  - **β=10, λ=1**: Aggressive weighting on final action dimension anchors Q-accuracy; conservative regularization prevents OOD overestimation but may underestimate some valid actions
  - **Mamba vs Transformer**: Mamba offers ~20% training speedup and selective memory; Transformer may be more interpretable via attention visualization

- **Failure signatures:**
  - **Training divergence with λ=0**: Without conservative loss, Q-values explode on unseen actions -> add/increase λ
  - **High variance across runs with μ=1**: Pure expert data lacks coverage -> add random exploration data
  - **Poor generalization to new problem distributions**: Training problems may not cover test landscape features -> expand training set diversity
  - **Slow convergence on high-K algorithms (K>10)**: Sequence length K×T challenges memory -> consider gradient checkpointing or reduce T during training

- **First 3 experiments:**
  1. **Reproduce Table 1 baseline comparison**: Train Q-Mamba on Alg0 (K=3) with BBOB training set (16 functions); verify training time ~13h and test performance on 8 held-out functions. This validates the full pipeline.
  2. **Ablate conservative loss (Table 2)**: Train variants with λ∈{0,1,10} and β∈{1,10} on Alg1 (K=10); confirm λ=1, β=10 performs best. This validates the offline RL stabilization mechanism.
  3. **Test zero-shot neuroevolution (Figure 2)**: Take trained Q-Mamba from experiment 1, apply to Mujoco continuous control with population=10, generations=50; compare against RLPSO/LDE/GLEET. This validates out-of-distribution generalization claim.

## Open Questions the Paper Calls Out

- **How can an algorithm feature extraction mechanism be designed to enable Q-Mamba to co-train on diverse low-level algorithms without requiring re-training?**
  - Basis: The Conclusion states Q-Mamba currently requires re-training for new algorithms and identifies co-training via feature extraction as an "important future work."

- **Does the Q-Mamba architecture (decomposed Q-learning with Mamba) generalize effectively to standard offline RL benchmarks outside the MetaBBO domain?**
  - Basis: Section 5.2 notes the observation of superiority is "limited within MetaBBO domain" and explicitly calls for "further validation tests... on other RL tasks."

- **Does the uniform discretization of continuous hyperparameters into 16 bins limit performance for optimization algorithms highly sensitive to fine-grained parameter changes?**
  - Basis: Appendix E.1 indicates 16 bins were chosen because EAs have "low sensitivity on slight value changes," implying the method might fail for algorithms requiring higher precision.

## Limitations
- Framework relies heavily on dataset quality, with conservative Q-learning unable to recover from insufficient exploration coverage
- Autoregressive decomposition assumes weak hyperparameter dependencies, which may not hold for tightly coupled algorithm parameters
- Performance on complex, high-dimensional neuroevolution tasks remains partially validated

## Confidence
- **High confidence**: Offline learning efficiency claims (training time reduction >50%), ablation study results for Q-loss components, autoregressive decomposition mathematical proof
- **Medium confidence**: Zero-shot generalization to neuroevolution (based on limited task set), Mamba architecture benefits (no direct Transformer comparison in MetaBBO context)
- **Low confidence**: Scalability to algorithms with K>20 hyperparameters, robustness across diverse optimization landscapes beyond BBOB and Mujoco

## Next Checks
1. **Dataset coverage analysis**: Quantify state-action space coverage in E&E dataset and measure correlation with downstream performance to validate conservative Q-learning assumptions
2. **Hyperparameter coupling stress test**: Systematically evaluate Q-Mamba on algorithms with known parameter interdependencies (e.g., mutation rate + population size) to test decomposition limits
3. **Extended zero-shot evaluation**: Test on diverse optimization domains (discrete combinatorial, multi-objective, constrained problems) beyond continuous BBOB/Mujoco benchmarks to assess true generalization capability