---
ver: rpa2
title: Observation Noise and Initialization in Wide Neural Networks
arxiv_id: '2502.01556'
source_url: https://arxiv.org/abs/2502.01556
tags:
- network
- neural
- gradient
- training
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the Neural Tangent Kernel (NTK) framework to
  incorporate observation noise and arbitrary prior means. The authors introduce a
  regularizer in the training loss that corresponds to adding observation noise in
  the NTK-Gaussian Process (GP) posterior, addressing the limitation of standard NTK-GP
  assuming noiseless targets.
---

# Observation Noise and Initialization in Wide Neural Networks
## Quick Facts
- arXiv ID: 2502.01556
- Source URL: https://arxiv.org/abs/2502.01556
- Reference count: 40
- The paper extends NTK-GP framework to handle observation noise and arbitrary prior means

## Executive Summary
This paper addresses two fundamental limitations in the Neural Tangent Kernel (NTK) framework: its inability to handle noisy observations and its restriction to zero-mean prior functions. The authors introduce a regularization approach that enables NTK-GP to work with real-world noisy data by incorporating observation noise through an additional term in the training loss. Additionally, they propose a "shifted network" approach that allows the incorporation of arbitrary prior mean functions without requiring ensembles or kernel matrix inversion, significantly improving computational efficiency.

The theoretical contributions are validated empirically, demonstrating that wide neural networks trained with these modifications converge to their linearized counterparts and achieve better data efficiency in transfer learning scenarios. The work bridges the gap between idealized NTK assumptions and practical machine learning applications where data is noisy and prior knowledge is valuable.

## Method Summary
The authors extend the NTK framework in two key ways. First, they introduce observation noise by adding a regularization term to the training loss, which corresponds to modifying the NTK-GP posterior to handle noisy targets. This regularization preserves the linearization property of wide neural networks during training. Second, they propose a "shifted network" approach where the network architecture is modified to include a prior mean function directly in the first layer, allowing arbitrary prior means to be incorporated through a single training run. This eliminates the need for computationally expensive kernel matrix inversion or ensemble methods. The theoretical analysis shows that these modifications maintain the convergence properties of NTK as network width increases.

## Key Results
- Theoretical proof that observation noise regularization preserves linearization accuracy in wide networks
- Shifted network approach enables arbitrary prior means without kernel matrix inversion or ensembles
- Empirical validation shows improved data efficiency in transfer learning with learned prior means

## Why This Works (Mechanism)
The proposed approach works by modifying the NTK framework's assumptions about data and prior knowledge. By adding observation noise regularization, the method accounts for real-world data imperfections that standard NTK-GP cannot handle. The shifted network architecture effectively embeds prior knowledge directly into the network structure, allowing the model to leverage this information during training. As network width increases, the linearization approximation becomes increasingly accurate, ensuring that the theoretical guarantees hold in the infinite-width limit. This combination of theoretical rigor and practical modifications enables NTK-based methods to be applied to more realistic scenarios.

## Foundational Learning
**Neural Tangent Kernel (NTK)**: A kernel that describes the training dynamics of infinitely wide neural networks. Needed to understand the theoretical framework being extended. Quick check: Verify that NTK is defined as the inner product of gradients with respect to parameters.

**Gaussian Process (GP) Regression**: A probabilistic framework for regression that provides uncertainty estimates. Required to understand how NTK relates to GP posteriors. Quick check: Confirm that GP predictions are made by conditioning on observed data.

**Wide Neural Network Limit**: The theoretical regime where network width approaches infinity. Essential for understanding when NTK approximations become exact. Quick check: Verify that in the infinite-width limit, neural networks become linear in their parameters.

**Kernel Methods**: Algorithms that use kernel functions to enable linear methods in high-dimensional spaces. Needed to understand NTK as a kernel method. Quick check: Confirm that kernel methods can represent non-linear relationships through the kernel trick.

**Prior Mean Functions**: The expected output values before observing data in Bayesian inference. Required to understand how prior knowledge is incorporated. Quick check: Verify that prior means shift the predictions of GP models.

## Architecture Onboarding
**Component Map**: Input -> Wide Neural Network -> NTK Kernel -> Regularized Loss -> Linearized Model -> Prediction

**Critical Path**: Data preprocessing → Network initialization → NTK computation → Regularization addition → Training optimization → Linearization verification

**Design Tradeoffs**: The regularization approach trades off some theoretical purity for practical applicability to noisy data. The shifted network method trades architectural complexity for computational efficiency in handling prior means.

**Failure Signatures**: Poor performance with very wide networks may indicate breakdown of linearization assumptions. Failure to improve with prior means suggests incorrect prior specification or insufficient network capacity.

**First Experiments**:
1. Train a wide network with the observation noise regularizer on synthetic noisy data and verify linearization accuracy
2. Implement the shifted network approach with a simple prior mean (e.g., constant function) and compare against standard NTK-GP
3. Test transfer learning with learned prior means on a few-shot learning benchmark

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Empirical scaling behavior in extremely wide networks (>10,000 hidden units) remains unverified
- Linearization accuracy throughout training with non-smooth activation functions requires further validation
- Shifted network approach may face practical limitations with complex, computationally expensive prior mean functions

## Confidence
**High confidence**: The theoretical extension of NTK-GP to include observation noise is mathematically sound and the regularization interpretation is correct.

**Medium confidence**: The linearization accuracy claims hold for the tested architectures and widths, but may not generalize to all network types.

**Medium confidence**: The shifted network approach works as described for simple prior means, but scalability to complex priors needs validation.

## Next Checks
1. Test the regularization approach on extremely wide networks (>10,000 hidden units) to verify the claimed scaling properties hold in the infinite-width limit
2. Evaluate the shifted network method with complex, non-linear prior mean functions that require significant computation to evaluate
3. Verify linearization accuracy throughout training for networks with non-smooth activations (ReLU, Leaky ReLU) and compare against smooth activations (Tanh, Softplus)