---
ver: rpa2
title: The Three Regimes of Offline-to-Online Reinforcement Learning
arxiv_id: '2510.01460'
source_url: https://arxiv.org/abs/2510.01460
tags:
- offline
- fine-tuning
- environment
- data
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a stability-plasticity principle to explain
  inconsistent empirical behavior in offline-to-online reinforcement learning. The
  key insight is that fine-tuning performance depends on balancing preservation of
  prior knowledge (stability) and adaptation to new data (plasticity), with the relative
  strength of pretrained policy versus offline dataset determining which stability
  source to prioritize.
---

# The Three Regimes of Offline-to-Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01460
- Source URL: https://arxiv.org/abs/2510.01460
- Authors: Lu Li; Tianwei Ni; Yihao Sun; Pierre-Luc Bacon
- Reference count: 40
- One-line primary result: Offline-to-online RL fine-tuning performance depends on whether the pretrained policy or dataset provides superior prior knowledge, requiring distinct stability strategies for each regime.

## Executive Summary
This paper proposes a stability-plasticity principle to explain inconsistent empirical behavior in offline-to-online reinforcement learning. The key insight is that fine-tuning performance depends on balancing preservation of prior knowledge (stability) and adaptation to new data (plasticity), with the relative strength of pretrained policy versus offline dataset determining which stability source to prioritize. The authors identify three regimes based on this relationship: Superior (pretrained policy outperforms dataset), Comparable (similar performance), and Inferior (dataset outperforms policy). Each regime requires distinct stability properties. A large-scale empirical study across 63 settings shows that results align with predictions in 45 cases, with only 3 opposite mismatches, supporting the framework's utility for guiding design choices in offline-to-online RL.

## Method Summary
The method involves offline pretraining using CalQL, ReBRAC, or BC+FQE on D4RL datasets, followed by fine-tuning with SAC or TD3 for 500k steps. The key innovation is classifying each setting into one of three regimes based on the relative performance of the pretrained policy versus the dataset behavior policy using a TOST statistical test with margin δ=0.05. Six fine-tuning variants are tested: baseline, warmup (K=5000), offline RL regularization, offline data replay (α=0.5), reset, and combined offline RL+data. The framework predicts which strategy should work best in each regime based on stability-plasticity trade-offs.

## Key Results
- Large-scale empirical study across 63 settings shows framework predictions align with results in 45 cases
- Only 3 opposite mismatches observed, demonstrating framework utility for guiding offline-to-online RL design
- Parameter reset significantly improves performance in Inferior regime by recovering plasticity lost during suboptimal offline pretraining
- Q-function divergence occurs on offline distribution when offline data is excluded in Inferior regime, but is mitigated by data replay

## Why This Works (Mechanism)

### Mechanism 1: Regime-Dependent Stability Anchoring
The fine-tuning strategy must anchor stability to the superior source of prior knowledge (either pretrained policy π₀ or offline dataset D), as stability and plasticity are inherently in trade-off. In Superior regime, anchor to π₀ prevents forgetting superior behaviors; in Inferior regime, anchor to D retains superior behavioral patterns from the dataset.

### Mechanism 2: Plasticity Recovery via Parameter Reset
In Inferior regime specifically, resetting network parameters while keeping data improves final performance by recovering plasticity lost during potentially suboptimal offline pretraining. Offline pretraining on difficult data can reduce network plasticity through primacy bias, and reset allows fresh optimization dynamics.

### Mechanism 3: Q-Function Divergence Mitigation
In Inferior regime, excluding offline data during fine-tuning causes Q-function divergence on the offline distribution, whereas including it anchors the value estimation. When π₀ is poor, fine-tuning without D forces extrapolation on sparse online exploration, leading to high TD loss and Q-value explosion on offline data.

## Foundational Learning

- **Concept: Offline-to-Online RL Transition**
  - Why needed here: The entire paper is structured around this transition phase. Understanding the distinct goals of offline pretraining (stability/regularization) vs. online fine-tuning (plasticity/optimization) is prerequisite to understanding why the "regime" matters.
  - Quick check question: Does the current setup require preserving the behavior policy's constraints (offline) or maximizing exploration (online)?

- **Concept: Stability-Plasticity Dilemma**
  - Why needed here: This is the theoretical lens used to explain the inconsistent results. One must grasp that increasing stability (e.g., strong regularization) often reduces plasticity (ability to learn new/better behaviors).
  - Quick check question: If we aggressively constrain the policy to the offline dataset, will it be able to surpass the performance of that dataset?

- **Concept: Distribution Shift**
  - Why needed here: A core challenge identified in the "Inferior" regime mechanism. The divergence of Q-functions occurs because the online distribution shifts away from the offline data support, leading to erroneous value estimates if not corrected.
  - Quick check question: Is the online agent visiting states not covered by the offline dataset, and if so, how is the critic estimating values there?

## Architecture Onboarding

- **Component map:** Pretrained Policy π₀ + Offline Dataset D → Regime Classifier (TOST test comparing J(π₀) vs J(π_D)) → Strategy Selector (Superior: Online Warmup, Inferior: Data Replay+Reset) → SAC/TD3 Training Loop

- **Critical path:**
  1. Evaluate J(π₀): Run pretrained policy in evaluation environment
  2. Evaluate J(π_D): Calculate average return of offline dataset trajectories
  3. Determine Regime: Apply TOST with margin δ=0.05 to gap between J(π₀) and J(π_D)
  4. Configure Fine-tuning: Select π₀-centric or D-centric modules based on regime

- **Design tradeoffs:**
  - Raw Return vs. Dense Proxy: Using raw returns for regime classification is robust but struggles with sparse rewards; dense proxies require domain knowledge
  - Warmup vs. Reset: Warmup preserves π₀ (high stability, lower plasticity); Reset destroys π₀ (low stability, high plasticity)

- **Failure signatures:**
  - Stagnation: Performance hovers near J(π_D) and fails to improve (likely Superior regime using Inferior strategies)
  - Catastrophic Collapse: Performance drops to near zero (likely Inferior regime using Superior strategies or Reset in Superior regime)
  - Opposite Mismatch: Framework predicts π₀-centric better but D-centric wins (occurs in ~5% of cases)

- **First 3 experiments:**
  1. Regime Audit: For your dataset and pretrained model, compute J(π₀) and J(π_D) to confirm which regime you're in
  2. Strategy Ablation: Run Online Data Warmup (π₀-centric) and Data Replay (D-centric) to verify which aligns with regime prediction
  3. Boundary Test: If in Inferior regime, test Parameter Reset variant as high-risk, high-reward intervention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to incorporate dataset characteristics, such as coverage, which currently remain difficult to capture?
- Basis in paper: The conclusion states that "other dataset characteristics, such as coverage, play an important role but remain difficult to capture consistently. Extending the framework to incorporate such dimensions is an important direction for future work."
- Why unresolved: The current taxonomy relies solely on performance returns (J(π₀) vs J(π_D)), ignoring the structural quality or state-action coverage of the data, which likely influences the plasticity required for fine-tuning.
- What evidence would resolve it: An expanded model that utilizes dataset coverage metrics to refine regime predictions or explain the "adjacent mismatches" observed in the empirical study.

### Open Question 2
- Question: Can the discrete three-regime taxonomy be reformulated as a continuous spectrum to better handle boundary cases and varying stability-plasticity needs?
- Basis in paper: The authors acknowledge that "discretizing behavior into three regimes is a simplification" and that "real systems often lie along a continuum."
- Why unresolved: The current rigid classification using t-tests with a margin might force settings into ill-fitting categories, potentially leading to suboptimal design choices for boundary cases.
- What evidence would resolve it: A continuous function mapping the relative performance gap to an optimal stability coefficient that outperforms the current categorical method in selecting fine-tuning strategies.

### Open Question 3
- Question: Are there robust design principles that can mitigate the high sensitivity to hyperparameters and implementation details observed in the Comparable Regime?
- Basis in paper: Section 5.4 notes that in the Comparable Regime, "outcomes become highly sensitive to hyperparameters, initialization, and other implementation details," causing significant variance despite the framework predicting similar performance.
- Why unresolved: While the framework predicts π₀-centric and D-centric methods should theoretically yield comparable results, the observed fragility of this balance makes reliable method selection difficult in practice.
- What evidence would resolve it: A unified fine-tuning algorithm or regularization technique that reduces variance and consistently achieves the theoretical performance upper bound in the Comparable Regime without extensive hyperparameter tuning.

## Limitations
- The framework assumes the relative performance gap between pretrained policy and dataset behavior policy reliably predicts fine-tuning dynamics, but this may not hold when datasets have limited exploration coverage
- Empirical validation covers 63 settings but only shows 45 cases aligning with predictions, leaving room for domain-specific factors not captured by regime classification
- The use of statistical tests (TOST) to classify regimes introduces uncertainty, as small performance differences may not meaningfully predict future learning behavior

## Confidence
- High confidence: The existence of regime-dependent stability requirements and the general superiority of anchoring to the better source of prior knowledge
- Medium confidence: The specific mechanisms for Q-function divergence mitigation and plasticity recovery through parameter reset
- Medium confidence: The statistical significance of regime classification through TOST testing

## Next Checks
1. **Domain Transfer Test**: Apply the framework to a new domain (e.g., Atari or continuous control with different dynamics) to verify regime classification generalizes beyond D4RL benchmarks

2. **Dataset Quality Ablation**: Systematically vary dataset quality (e.g., by subsampling expert trajectories) to test whether regime classification remains predictive as the dataset-policy performance gap changes

3. **Policy Architecture Sensitivity**: Test whether different policy architectures (e.g., MLP vs. transformer) affect the validity of regime classification and the effectiveness of regime-specific strategies