---
ver: rpa2
title: EVaR-Optimal Arm Identification in Bandits
arxiv_id: '2510.04728'
source_url: https://arxiv.org/abs/2510.04728
tags:
- evar
- lemma
- compact
- bound
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes the first optimal algorithm for best-arm\
  \ identification under the Entropic Value-at-Risk (EVaR) criterion in multi-armed\
  \ bandits. The authors propose a \u03B4-correct Track-and-Stop algorithm that matches\
  \ an information-theoretic lower bound on sample complexity."
---

# EVaR-Optimal Arm Identification in Bandits

## Quick Facts
- arXiv ID: 2510.04728
- Source URL: https://arxiv.org/abs/2510.04728
- Reference count: 40
- This paper establishes the first optimal algorithm for best-arm identification under the Entropic Value-at-Risk (EVaR) criterion in multi-armed bandits.

## Executive Summary
This paper addresses the problem of best-arm identification in multi-armed bandits under the Entropic Value-at-Risk (EVaR) risk measure. The authors propose a δ-correct Track-and-Stop algorithm that achieves asymptotically optimal sample complexity. The key technical innovation lies in characterizing the EVaR-constrained KL projections KL^U_inf and KL^L_inf, which enable both the algorithm implementation and the information-theoretic lower bound derivation. Through C-tracking of oracle sampling proportions and a generalized likelihood ratio test for stopping, the algorithm identifies the arm with minimum EVaR while maintaining δ-correctness.

## Method Summary
The algorithm follows the Track-and-Stop framework, using C-tracking to maintain sampling proportions that converge to oracle values t*(ν) maximizing the characteristic information term. At each round, it computes empirical distributions, solves for oracle allocations using the KL projections, and pulls arms according to the tracking rule. The stopping rule employs a generalized likelihood ratio test with threshold β(n,δ) derived from mixture supermartingales. The final recommendation is the arm with minimum empirical EVaR at stopping time.

## Key Results
- First optimal algorithm for EVaR-based best-arm identification with proven asymptotic optimality
- Characterization of information-theoretic lower bound through KL^U_inf and KL^L_inf functionals
- Derivation of tractable dual formulations for both convex (KL^U_inf) and non-convex (KL^L_inf) projection problems
- Proven δ-correctness with sample complexity matching the lower bound T(μ)log(1/δ)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Track-and-Stop algorithm with C-tracking achieves asymptotically optimal sample complexity for EVaR-based best arm identification by converging to oracle sampling proportions.
- **Mechanism:** The algorithm computes oracle proportions t*(ν) that maximize the characteristic information term T(ν), then uses C-tracking to ensure sampling frequencies converge to these proportions. The generalized likelihood ratio test stops when the statistic crosses a threshold derived from mixture supermartingales.
- **Core assumption:** The correspondence ν → t*(ν) is upper hemicontinuous and convex, ensuring empirical proportions converge to optimal allocations as estimates improve (Lemma 10).
- **Evidence anchors:**
  - [Abstract] "We propose a δ-correct, Track-and-Stop based algorithm and derive a corresponding lower bound on the expected sample complexity, which we prove is asymptotically matched."
  - [Section 4] "For sampling rule we use C-Tracking Garivier and Kaufmann (2016), we use generalized likelihood ratio test (GLRT) for stopping time and then the algorithm outputs the arm with the minimum EVaR of the corresponding empirical distribution."
  - [Corpus] Track-and-Stop framework appears in related work including "Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima," suggesting this extends a validated approach to EVaR.
- **Break condition:** Failure occurs if t*(ν) lacks uniqueness or if convergence to oracle proportions fails due to discontinuity in the objective or numerical instability in computing KL projections.

### Mechanism 2
- **Claim:** The KL^U_inf and KL^L_inf functionals characterize the information-theoretic complexity of distinguishing arms under EVaR constraints and enable tractable computation through dual representations.
- **Mechanism:** Two distinct projection problems: KL^U_inf finds the nearest distribution with EVaR ≥ threshold (convex, solvable via semi-infinite programming dual), while KL^L_inf finds the nearest with EVaR ≤ threshold (non-convex, tractable via outer infimum over scalar z with inner convex dual).
- **Core assumption:** The dual formulations preserve optimality and feasible sets remain compact under weak convergence, ensuring minima are attained (Corollary 4, Lemma 17).
- **Evidence anchors:**
  - [Abstract] "The implementation of our algorithm and the characterization of the lower bound both require solving a complex convex optimization problem and a related, simpler non-convex one."
  - [Section 3.1] "KLU_inf projection... renders the associated convex optimization a semi-infinite program... To obtain a tractable formulation, we instead adopt the (equivalent) robust KL-ball definition of EVaR in (3)."
  - [Corpus] Limited direct corpus evidence on EVaR-specific KL projections; this appears as a novel contribution extending prior CVaR work.
- **Break condition:** Computation fails if the dual optimization doesn't converge, the primal-dual gap remains large, or the outer z-search in KL^L_inf misses the global optimum.

### Mechanism 3
- **Claim:** Mixture supermartingales constructed from dual representations provide anytime-valid confidence bounds enabling δ-correct stopping without prespecified sample sizes.
- **Mechanism:** The algorithm constructs test supermartingales by integrating over priors on dual parameters (λ₁, λ₃ for upper; γ for lower projections). Ville's inequality bounds the probability that log-likelihood ratios exceed thresholds, yielding the stopping rule β(n,δ) = log((K-1)/δ) + 3log(n+1) + 2.
- **Core assumption:** The dual integrands are exp-concave on compact domains, allowing application of the exp-concave aggregation inequality (Lemma F.1 of Agrawal et al. 2021).
- **Evidence anchors:**
  - [Section C.1] "By writing both KL functionals through the EVaR duals and constructing mixture-supermartingale argument we deliver a time-uniform deviation inequality that leads to the threshold β(n, δ) above."
  - [Proposition 25] "For any two arms i ≠ j... Pr(∃n ≥ 1 : Nᵢ(n)KL^U_inf + Nⱼ(n)KL^L_inf ≥ x + h(n)) ≤ e^{-x}"
  - [Corpus] The mixture supermartingale approach aligns with anytime-valid inference methods referenced in related BAI literature.
- **Break condition:** The bound fails if the supermartingale property is violated due to numerical errors in computing dual factors or if the prior doesn't have full support on the feasible set.

## Foundational Learning

- **Concept: Entropic Value-at-Risk (EVaR) and dual representations**
  - **Why needed here:** EVaR is the core risk measure. Understanding its MGF form EVaRα(X) = inf_{z>0} [log E[e^{zX}] + ρ]/z and distributionally robust form EVaRα(X) = sup_{Q: KL(Q||P)≤ρ} E_Q[X] is essential for deriving KL projections.
  - **Quick check question:** Explain why EVaR is the tightest coherent upper bound on CVaR and how ρ = -log(1-α) controls the ambiguity set radius.

- **Concept: Track-and-Stop framework for fixed-confidence BAI**
  - **Why needed here:** This is the algorithmic backbone. Understanding how C-tracking enforces sampling proportions and GLRT stopping works is prerequisite.
  - **Quick check question:** Given oracle proportions t* maximizing T(ν), how does C-tracking ensure N_k(n)/n → t*_k while maintaining forced exploration?

- **Concept: KL-divergence projections under constraints**
  - **Why needed here:** KL^U_inf and KL^L_inf are the key mathematical objects. Understanding their asymmetry and computational properties is critical.
  - **Quick check question:** For KL^L_inf(η, ν) = inf_{κ: EVaR(κ)≤ν} KL(η||κ), why is this non-convex (consider the constraint set structure) and how does the outer infimum over z > 0 restore tractability?

## Architecture Onboarding

- **Component map:**
  1. Empirical distribution estimator - Maintains ̂μ_a(n) for each arm
  2. EVaR computer - Solves inf_{z>0} [log E[e^{zX}] + ρ]/z
  3. Oracle proportion solver - Maximizes Equation 5 using KL projections
  4. KL projection modules - KL^U_inf (convex dual) and KL^L_inf (non-convex outer loop over z)
  5. C-tracking allocator - Enforces sampling proportions with forced exploration
  6. GLRT statistic computer - Computes Z_i(n) using KL projections and arm counts
  7. Stopping rule - Checks Z_i(n) ≥ β(n,δ)
  8. Recommendation rule - Returns argmin_a EVaR(̂μ_a)

- **Critical path:**
  Pull arm → update empirical distribution → compute EVaR → solve oracle proportions via KL projections → C-tracking selects next arm → compute GLRT statistic → check stopping threshold → recommend arm with minimum empirical EVaR

- **Design tradeoffs:**
  1. **KL^L_inf z-search precision** - Finer grid improves optimality but increases computation; Assumption: authors use bounded interval search
  2. **Forced exploration frequency** - More exploration stabilizes estimates but slows convergence to oracle proportions
  3. **Prior selection for mixtures** - Uniform priors suffice asymptotically but finite-sample performance may vary
  4. **Numerical tolerance in positivity constraints** - Dual factors require strict positivity; boundary handling critical

- **Failure signatures:**
  1. **Non-convergence of sampling proportions** - N_k(n)/n oscillates rather than stabilizing
  2. **Premature stopping** - GLRT threshold crossed incorrectly, error rate exceeds δ
  3. **EVaR boundary issues** - When true EVaR near 0 or 1, z-optimization hits boundaries causing instability
  4. **Supermartingale property violation** - Empirical error rate > δ indicates dual factor computation errors

- **First 3 experiments:**
  1. **Synthetic validation on known instances** - K=5 arms with Beta mixtures, test δ ∈ {0.1, 0.01, 0.001}, measure empirical error rate (< δ required), sample complexity vs. T(ν)log(1/δ), and proportion convergence
  2. **Ablation on KL projection accuracy** - Vary z-grid precision for KL^L_inf solver, measure impact on computation time and sample complexity on instances with similar EVaR values
  3. **Comparison with CVaR-based BAI** - Implement Agrawal et al. (2021) CVaR algorithm, compare on heavy-tailed distributions and finance scenarios to quantify EVaR's tighter bounds vs. computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a unified algorithmic framework be developed that generalizes best-arm identification across the family of coherent risk measures (e.g., including both CVaR and EVaR)?
- **Basis in paper:** [explicit] In the conclusion, the authors state: "Two promising directions emerge... developing a unified framework that treats coherent risks under a single universal constraint class (via common duality and KL-projection primitives)."
- **Why unresolved:** The current work derives specific KL-projection functionals ($KL^U_{inf}$ and $KL^L_{inf}$) tailored strictly to the entropic geometry of EVaR; it is unclear if these primitives map generally to other coherent risks like CVaR.
- **What evidence would resolve it:** A generalized lower bound and an algorithm template that accepts a generic coherent risk measure as an input and specializes automatically to EVaR or CVaR constraints.

### Open Question 2
- **Question:** What is the non-asymptotic computational cost and empirical performance of solving the non-convex $KL^L_{inf}$ projection at every round of the algorithm?
- **Basis in paper:** [inferred] The abstract notes the implementation "require[s] solving a complex convex optimization problem and a related, simpler non-convex one," and Section 3.1 describes $KL^L_{inf}$ as "intrinsically nonconvex."
- **Why unresolved:** The paper proves asymptotic optimality ($\lim_{\delta \to 0}$) but does not analyze the finite-time computational burden of the "one-dimensional search" required for the non-convex projection, nor the impact of approximation errors on $\delta$-correctness.
- **What evidence would resolve it:** An empirical evaluation of the algorithm's runtime scaling with $K$ and $n$, or a finite-sample analysis bounding the error introduced by approximating the KL projections.

### Open Question 3
- **Question:** Can the EVaR-optimal identification strategy be extended to the fixed-budget setting where the sample count is fixed rather than the confidence level?
- **Basis in paper:** [inferred] The introduction and problem setup restrict the scope to "fixed-confidence best arm identification," while the conclusion suggests the "principled pathway" provided could apply to "risk-aware decision problems" more broadly.
- **Why unresolved:** The Track-and-Stop algorithm relies on a stopping rule based on the Generalized Likelihood Ratio (GLRT) crossing a threshold, a mechanism specific to the fixed-confidence setting.
- **What evidence would resolve it:** Derivation of a lower bound on the error probability for a fixed horizon $T$ and an allocation algorithm (e.g., using similar KL projections) that minimizes this error.

## Limitations

- The numerical optimization for KL^L_inf is non-convex and computationally intensive, with no guarantees on finding global optima in finite time
- The algorithm's performance degrades when true EVaR values are near the boundary (0 or 1), causing instability in z-optimization
- The C-tracking convergence depends on upper hemicontinuity of t*(ν), which may fail for instances with nearly identical EVaR values

## Confidence

- **High confidence**: The information-theoretic lower bound characterization via KL projections is sound given the established Track-and-Stop framework. The δ-correctness guarantee follows from standard supermartingale arguments.
- **Medium confidence**: The asymptotic optimality proof relies on empirical proportions converging to oracle values, but finite-sample behavior and computational tractability of the projections require further validation.
- **Low confidence**: The numerical implementation details for KL^L_inf optimization are underspecified, and the handling of boundary cases (EVaR near 0 or 1) may introduce instability.

## Next Checks

1. **Numerical stability verification**: Implement KL^L_inf solver and systematically test on synthetic instances with known solutions, measuring primal-dual gaps and z-search convergence rates.
2. **Finite-sample error rate analysis**: Run extensive simulations across multiple problem instances to empirically measure δ-correctness, particularly focusing on scenarios where EVaR values are close or distributions have heavy tails.
3. **Comparative complexity assessment**: Benchmark against CVaR-based BAI algorithms on finance-relevant distributions to quantify the tradeoff between EVaR's tighter risk bounds and increased computational overhead.