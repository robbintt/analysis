---
ver: rpa2
title: 'LLM DNA: Tracing Model Evolution via Functional Representations'
arxiv_id: '2509.24496'
source_url: https://arxiv.org/abs/2509.24496
tags:
- only
- decoder
- apache-2
- unknown
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM DNA, a mathematical framework for representing
  large language models (LLMs) as low-dimensional vectors that capture functional
  behavior and evolutionary relationships. The method defines DNA via a bi-Lipschitz
  mapping from the LLM function space, ensuring inheritance (small model changes preserve
  DNA) and genetic determinism (similar DNA implies similar behavior).
---

# LLM DNA: Tracing Model Evolution via Functional Representations

## Quick Facts
- **arXiv ID:** 2509.24496
- **Source URL:** https://arxiv.org/abs/2509.24496
- **Reference count:** 40
- **Primary result:** Introduces LLM DNA, a low-dimensional vector representation capturing functional similarity between LLMs, enabling evolutionary relationship detection and phylogenetic tree construction

## Executive Summary
This paper proposes LLM DNA, a mathematical framework that represents large language models as low-dimensional vectors capturing their functional behavior and evolutionary relationships. The method leverages random Gaussian projections and semantic embeddings to compress the high-dimensional functional space of LLMs into stable, comparable DNA vectors. The framework is theoretically grounded using the Johnson-Lindenstrauss lemma and validated through extensive experiments on 305 diverse LLMs, demonstrating superior performance in model relationship detection and routing tasks compared to existing methods.

## Method Summary
The method extracts DNA by first generating textual responses from LLMs using 600 prompts sampled across six datasets, then encoding these responses into semantic vectors using a sentence-embedding model, concatenating all embeddings into a high-dimensional functional representation, and finally projecting this representation to a low-dimensional DNA vector using a pre-computed Gaussian random matrix. The approach is training-free and relies on the Johnson-Lindenstrauss lemma to preserve pairwise distances between models in the compressed space, ensuring that small functional changes result in small DNA changes (inheritance) and similar DNAs imply similar behavior (genetic determinism).

## Key Results
- Proved existence of LLM DNA using Johnson-Lindenstrauss lemma with bi-Lipschitz mapping properties
- Outperformed existing model representation methods on routing tasks with AUC of 0.957
- Constructed phylogenetic tree from 305 diverse LLMs revealing architectural shifts and evolutionary speeds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Random Gaussian projection preserves relative functional distances between LLMs in low-dimensional space
- **Mechanism:** Uses Johnson-Lindenstrauss lemma to embed high-dimensional concatenated response embeddings into DNA space while approximately preserving pairwise distances
- **Core assumption:** LLM functional space behaves as metric space where JL lemma bounds apply, with sufficient target dimension L
- **Evidence anchors:** Abstract mentions JL lemma proof; Corollary 4.2 states DNA obtained from random linear projection with high probability
- **Break condition:** If target dimension L is too small relative to number of models K, distortion increases and bi-Lipschitz property fails

### Mechanism 2
- **Claim:** Semantic embeddings of textual responses act as cross-architectural functional proxy
- **Mechanism:** Queries models with prompts, generates text, embeds text using sentence-embedding model to abstract LLM to semantic input-output behavior
- **Core assumption:** Sentence-embedding model captures semantic equivalence effectively and correlates with functional similarity
- **Evidence anchors:** Section 5.1 argues string-based comparisons inadequate and proposes semantic vectors to capture meaning
- **Break condition:** If embedding model has blind spots or biases mapping functionally distinct responses to similar vectors

### Mechanism 3
- **Claim:** Stochastic sampling of input prompts approximates true functional distance
- **Mechanism:** Uses Monte Carlo approach sampling t inputs from distribution μ to estimate Hilbert distance, with concentration bounds ensuring convergence
- **Core assumption:** Sample size t large enough for concentration inequalities, and sampled distribution μ is representative
- **Evidence anchors:** Definition 5.1 introduces Stochastic Functional Distance; Lemma 5.2 proves concentration bound
- **Break condition:** If sample size t too small or prompt distribution narrow, empirical variance destabilizes DNA

## Foundational Learning

- **Concept:** **Johnson-Lindenstrauss Lemma**
  - **Why needed here:** This mathematical result justifies compressing high-dimensional functional data into low-dimensional DNA vector without losing relative relationships between models
  - **Quick check question:** If you have 1,000 LLMs, does the JL lemma guarantee you can represent them in 10 dimensions while preserving pairwise distances?

- **Concept:** **Bi-Lipschitz Continuity**
  - **Why needed here:** This property defines stability of DNA, ensuring small model changes result in small DNA changes (inheritance) and similar DNAs imply similar models (genetic determinism)
  - **Quick check question:** If a mapping is Lipschitz but not bi-Lipschitz, which property of LLM DNA (inheritance or genetic determinism) might fail?

- **Concept:** **Hilbert Spaces**
  - **Why needed here:** Models space of all LLM functions as Hilbert space to define rigorous distance metric, allowing application of geometric tools like JL lemma
  - **Quick check question:** Why is defining inner product (and thus norm/distance) on functional space necessary for constructing phylogenetic tree?

## Architecture Onboarding

- **Component map:** Input Sampler -> Target LLM -> Sentence Embedder -> Concatenation Layer -> Random Projector
- **Critical path:** Sequential pipeline: Prompting -> Response Generation -> Embedding -> Projection. Most computationally expensive step is generating responses from Target LLMs, especially for large models
- **Design tradeoffs:**
  - Sample size (t): Larger t improves distance approximation but increases inference cost linearly; paper uses t=600
  - DNA dimension (L): Higher L reduces distortion but increases storage and comparison overhead; paper defaults to L=128
  - Embedding Model: More powerful embedder captures nuance better but adds latency; paper uses specific 8B parameter embedder
- **Failure signatures:**
  - High variance in DNA: Small changes in prompts cause large shifts, suggesting sample size t too small
  - Conflation of distinct models: Distinct architectures map to similar DNAs, suggesting embedding model lacks resolution or projection dimension L too low
  - Temporal inconsistency: Phylogenetic tree contradicts known release dates without explanation, suggesting stochastic approximation failed or functional behavior dominates
- **First 3 experiments:**
  1. **Sanity Check (Inheritance):** Take base model (Llama-2-7b) and known fine-tune (Llama-2-7b-chat), extract DNAs, verify Euclidean distance small compared to random model (Mistral)
  2. **Stability Test (Mantel):** Extract DNAs for subset of models using two disjoint datasets (SQuAD vs HellaSwag), compute correlation between resulting distance matrices to verify independence from specific input set
  3. **Scaling Check (Phylogeny):** Extract DNAs for known family tree (Gemma → Gemma 2 → Gemma 3), construct tree using Neighbor-Joining, verify matches chronological release order

## Open Questions the Paper Calls Out

- **Question:** Can LLM DNA effectively detect transfer of security risks such as backdoors during model fine-tuning or distillation?
  - **Basis:** Introduction lists "safety auditing—tracking how security risks such as backdoors are transferred" as key motivation, yet experiments focus solely on model provenance and routing accuracy
  - **Why unresolved:** Paper establishes DNA captures functional similarity but does not validate if specific malicious functional traits (backdoors) are preserved as distinct markers during evolutionary processes
  - **What evidence would resolve it:** Experiments applying DNA framework to models with known implanted backdoors to verify if backdoor presence correlates with specific DNA features or distances

- **Question:** How can validity of undocumented evolutionary relationships identified by LLM DNA be systematically verified?
  - **Basis:** Authors note DNA comparisons "uncover previously undocumented relationships" and rely on manual inspection of model cards to explain false positives
  - **Why unresolved:** Manual verification is unscalable and subjective; paper lacks automated ground-truthing mechanism for novel connections it discovers
  - **What evidence would resolve it:** Scalable validation method such as automated weight correlation analysis or historical commit tracking that confirms relationships predicted by DNA proximity

## Limitations

- Biological analogy risks may mislead as genomic DNA has well-understood mutation mechanism and single universal encoding while LLM DNA is mathematical abstraction that could conflate functionally distinct models
- Framework critically depends on choice of sentence embedding model with no ablation studies testing robustness to alternative embedders
- 600-prompt sample may not capture full functional space of diverse LLMs optimized for different tasks, potentially leading to misleading DNA distances

## Confidence

- **High Confidence:** Theoretical existence proof using Johnson-Lindenstrauss lemma is mathematically sound; bi-Lipschitz property derivation follows standard functional analysis; routing task results (AUC 0.957) are reproducible
- **Medium Confidence:** Phylogenetic tree construction and relationship detection results are internally consistent but rely heavily on quality of semantic embeddings and representativeness of prompt samples
- **Low Confidence:** Cross-architectural comparison capability claim is weakest as it assumes semantic embeddings can fully capture functional equivalence across models with different tokenizers, architectures, and training objectives

## Next Checks

1. **Embedding Model Ablation:** Repeat DNA extraction pipeline using alternative sentence embedding models (OpenAI embeddings, Cohere embeddings) and compare resulting phylogenetic trees and routing performance to test framework robustness

2. **Prompt Distribution Sensitivity:** Vary prompt sampling strategy using different dataset combinations (only reasoning vs only conversational datasets) and measure correlation between resulting DNA distance matrices using Mantel tests to validate 600-prompt set representativeness

3. **Known Relationship Ground Truth:** Extract DNAs for curated set of models with documented fine-tuning lineages (Alpaca, Vicuna, Dolly) and compare DNA distances against known parent-child relationships to compute precision and recall for detecting documented relationships