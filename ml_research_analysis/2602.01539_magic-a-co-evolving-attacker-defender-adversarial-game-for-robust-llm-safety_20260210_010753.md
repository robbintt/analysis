---
ver: rpa2
title: 'MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety'
arxiv_id: '2602.01539'
source_url: https://arxiv.org/abs/2602.01539
tags:
- attacker
- defender
- safety
- adversarial
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAGIC is a multi-turn multi-agent reinforcement learning framework
  that formulates LLM safety alignment as an adversarial asymmetric game between an
  attacker and a defender. The attacker learns to iteratively rewrite harmful queries
  into deceptive prompts, while the defender simultaneously optimizes to recognize
  and refuse such inputs, triggering co-evolution.
---

# MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety

## Quick Facts
- **arXiv ID**: 2602.01539
- **Source URL**: https://arxiv.org/abs/2602.01539
- **Reference count**: 40
- **Primary result**: MAGIC significantly improves defense success rates across single-turn and multi-turn benchmarks while preserving model helpfulness through co-evolutionary attacker-defender RL.

## Executive Summary
MAGIC formulates LLM safety alignment as an adversarial asymmetric game between an attacker and a defender, using multi-turn multi-agent reinforcement learning. The attacker learns to rewrite harmful queries into deceptive prompts while the defender optimizes to recognize and refuse such inputs, triggering co-evolution. To address data scarcity, MAGIC constructs an Attack Pool Benchmark enriched with Chain-of-Thought completions across 20 rewriting strategies. Theoretical analysis shows the game equilibrium provides stronger pointwise safety guarantees than previous methods. Extensive experiments demonstrate MAGIC significantly improves defense success rates while preserving model helpfulness.

## Method Summary
MAGIC implements a two-phase training process: Phase 1 involves SFT warm-up on an Attack Pool benchmark (440 vanilla + 8,800 adversarial prompts with 20 mutation strategies) enriched with Chain-of-Thought reasoning. Phase 2 uses alternating GRPO optimization where the defender generates multiple responses per attack and the attacker generates multiple attacks per seed. The framework maintains separate policy networks for attacker and defender to avoid gradient conflicts, using Qwen3Guard as a binary reward model for harmfulness judgments and refusal detection.

## Key Results
- MAGIC achieves significantly lower attack success rates compared to baseline methods across both single-turn and multi-turn benchmarks
- The co-evolutionary training process discovers novel combinatorial attack strategies beyond static red-teaming datasets
- Theoretical analysis proves Subgame Perfect Nash Equilibrium provides stronger pointwise safety guarantees than expectation-based guarantees

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decoupled attacker-defender optimization reduces gradient conflicts inherent in shared-parameter self-play.
- **Mechanism**: MAGIC maintains separate policy networks π_A (attacker) and π_D (defender), alternating optimization via GRPO while freezing one agent during the other's update.
- **Core assumption**: Gradient interference is the primary cause of instability in symmetric self-play approaches.
- **Evidence anchors**: Section 4.1 describes alternating optimization; Fig. 1 right shows gradient conflicts in shared-parameter setups; Self-RedTeam uses shared parameters.
- **Break condition**: If attacker and defender are re-coupled (shared weights), gradient conflicts should re-emerge, degrading convergence stability.

### Mechanism 2
- **Claim**: Co-evolution drives discovery of long-tail vulnerabilities beyond static red-teaming datasets.
- **Mechanism**: The SFT-initialized attacker explores novel attack strategies via RL rewards, while the defender adapts to each emerging distribution.
- **Core assumption**: The attack space contains exploitable structure that RL can navigate, and the attacker's initial reasoning capability is sufficient to bootstrap exploration.
- **Evidence anchors**: Section 5.3, Fig. 3 heatmap shows ASR dynamics across training iterations; Table 16 demonstrates novel combinatorial strategies.
- **Break condition**: If attacker initialization lacks CoT reasoning (MAGIC-base ablation), exploration degrades and defender improvement stalls.

### Mechanism 3
- **Claim**: Subgame Perfect Nash Equilibrium (SPNE) provides pointwise safety guarantees stronger than normal-form Nash equilibrium.
- **Mechanism**: SPNE requires the defender to respond optimally to every possible attacker prompt y_A (pointwise), not just the expected distribution.
- **Core assumption**: The sequential game structure accurately reflects real-world attack dynamics, and a safe rejection action y_ref exists for all inputs.
- **Evidence anchors**: Definition 3.1 and Theorem 3.2 (Appendix A proof); Section 3 explicitly contrasts with Liu et al. (2025) which guarantees safety only in expectation.
- **Break condition**: If the action space is infinite or compactness assumptions fail, the existence of optimal pure strategies is not guaranteed.

## Foundational Learning

- **Concept: Sequential game theory and equilibrium concepts (SPNE vs Nash)**
  - Why needed here: Understanding why MAGIC formulates the problem as a sequential game rather than simultaneous-move normal-form game is critical to grasping the theoretical safety guarantees.
  - Quick check question: Why does SPNE provide pointwise guarantees while normal-form Nash only guarantees safety in expectation?

- **Concept: Policy gradient methods and GRPO (Group Relative Policy Optimization)**
  - Why needed here: The core training loop uses GRPO for both attacker and defender optimization; understanding advantage estimation from group statistics is essential for debugging reward signals.
  - Quick check question: How does GRPO differ from standard PPO in baseline estimation, and why does this matter for LLM policy updates?

- **Concept: Chain-of-Thought reasoning for adversarial prompt generation**
  - Why needed here: The attacker's ability to discover novel strategies depends on CoT-enriched SFT initialization; the format reward enforces hidden reasoning.
  - Quick check question: What is the role of the icas...icas format tags, and why is CoT only applied to the attacker?

## Architecture Onboarding

- **Component map**: Attacker π_A (SFT-initialized LLM with CoT) -> Defender π_D (Base LLM via RL) -> Reward Model (Qwen3Guard) -> GRPO Trainer (alternating updates)

- **Critical path**: Phase 1: SFT warm-up on CoT-enriched prompts → Attacker gains initial reasoning. Phase 2: Defender optimization (freeze attacker, sample G responses, compute safety rewards) → Attacker optimization (freeze defender, sample G attacks, compute attack success via defender response). Repeat for K rounds.

- **Design tradeoffs**: Safety vs helpfulness (over-refusal vs under-refusal); Compute vs coverage (attacker-only single rollout vs multi-query methods); Judge model sensitivity (GPT-4o dense scale vs Qwen3Guard binary).

- **Failure signatures**: Over-refusal (defender refuses adversarial benign prompts); Gradient conflicts (shared-parameter self-play shows slower convergence); Attacker collapse (base model attacker refuses to rewrite harmful queries).

- **First 3 experiments**:
  1. Reproduce ablation (Table 5/12): Compare No-Game, Defender-only, MAGIC-base, MAGIC-sft on a 100-sample subset to verify safety-helpfulness tradeoff
  2. Cross-evaluation heatmap (Fig. 3): Train for 50 steps, checkpoint attacker/defender every 10 steps, measure ASR on held-out HarmBench to confirm co-evolution dynamics
  3. Judge model sensitivity: Evaluate MAGIC defender on HarmBench using both Qwen3Guard and GPT-4o to quantify reward model bias

## Open Questions the Paper Calls Out

- Can the MAGIC framework be effectively adapted for multimodal LLMs or agentic tool-use scenarios?
- To what extent does the choice of the specific reward model (Qwen3Guard) bias the defender's learned safety boundaries?
- How does MAGIC scale regarding computational efficiency and convergence stability for models significantly larger than 14B parameters?

## Limitations

- The SPNE safety guarantee relies on compactness of the action space and existence of optimal pure strategies, which may not hold in real-world deployment with infinite continuous spaces
- The entire training pipeline depends on Qwen3Guard's binary safety judgments without ablation or sensitivity analysis of different judge models
- Experiments only validate against fixed HarmBench and WildJailBreak patterns, not novel attack strategies that exploit temporal reasoning

## Confidence

- **High Confidence**: The decoupled optimization architecture and its gradient conflict reduction are well-supported by empirical comparisons with Self-RedTeam
- **Medium Confidence**: The co-evolution discovery of long-tail vulnerabilities is supported by heatmap dynamics, but novelty quantification needs more rigorous analysis
- **Low Confidence**: The SPNE safety guarantee provides strong theoretical motivation, but practical strength versus expectation-based guarantees requires validation on adversarial edge cases

## Next Checks

1. **Judge Model Sensitivity**: Evaluate MAGIC defender on HarmBench using both Qwen3Guard and GPT-4o reward models. Measure differences in ASR and refusal rates to quantify reward model bias impact on learned equilibrium.

2. **Edge Case Safety Validation**: Construct adversarial prompts that exploit known limitations of binary classifiers (e.g., obfuscated harmful content, multi-turn jailbreak patterns). Test whether SPNE guarantees hold when the action space boundaries are stressed.

3. **Long-tail Attack Generalization**: Create a benchmark of 100 novel attack strategies not present in the Attack Pool or HarmBench. Evaluate MAGIC defender's ASR against these unseen patterns to verify true generalization beyond memorization.