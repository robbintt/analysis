---
ver: rpa2
title: 'Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions'
arxiv_id: '2503.23250'
source_url: https://arxiv.org/abs/2503.23250
tags:
- prompt
- user
- permissions
- actions
- encrypted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses security threats like prompt injection attacks
  on LLM-integrated applications that could lead to unauthorized API actions or data
  misuse. The proposed method, Encrypted Prompt, appends a permission-encrypted prompt
  to each user input, which specifies current permissions for LLM actions.
---

# Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions

## Quick Facts
- arXiv ID: 2503.23250
- Source URL: https://arxiv.org/abs/2503.23250
- Reference count: 12
- Proposed method appends permission-encrypted prompts to user inputs, verifying permissions before executing any LLM-generated actions

## Executive Summary
This paper addresses security threats from prompt injection attacks on LLM-integrated applications that could lead to unauthorized API actions or data misuse. The proposed Encrypted Prompt method appends permission metadata to each user input, which specifies current permissions for LLM actions. Before executing any API calls, the server verifies whether these actions are within the permitted scope defined in the encrypted prompt. This approach ensures that only authorized actions within current permissions can be executed, effectively mitigating risks from adversarial prompts and unauthorized API misuse. The method is flexible, allowing developers to define permissions based on user status, device state, and application needs, and can be implemented at the software level without requiring model retraining.

## Method Summary
The Encrypted Prompt approach works by concatenating an encrypted permission token to each user prompt before sending it to the LLM. This token contains permission specifications and a public key for verification. When the server receives the combined input, it validates the key pair to ensure the permissions haven't been tampered with. The LLM then processes the user prompt and generates potential actions. Before executing any API calls, the server checks whether these actions fall within the permission scope specified in the encrypted token. If permissions are insufficient or tampered with, the actions are blocked. This creates a runtime enforcement layer that validates LLM-generated actions against predefined authorization boundaries without modifying the underlying model.

## Key Results
- Proposed Encrypted Prompt method prevents unauthorized API actions by verifying permissions before execution
- Approach allows flexible permission definitions based on user status, device state, and application needs
- Implementation possible at software level without requiring model retraining or kernel modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Appending permission metadata to user inputs enables runtime enforcement of authorization boundaries for LLM-generated actions
- Mechanism: An Encrypted Prompt containing delimiters, permissions, and a public key is concatenated to each user prompt. Before executing any API call generated by the LLM, the server checks the action against the permission scope in the Encrypted Prompt. Actions exceeding permissions are blocked or trigger additional verification
- Core assumption: The enforcement layer correctly interprets the permission schema and cannot be bypassed by the LLM output manipulating the verification logic
- Evidence anchors: [abstract] "These permissions are verified before executing any actions (such as API calls) generated by the LLM." [section 3] "The core principle is that only actions generated by the LLM within the scope of its current permissions are allowed to be executed"
- Break condition: If the permission-to-API mapping is incomplete or the enforcement layer has privilege escalation vulnerabilities, authorized-but-harmful actions may still execute

### Mechanism 2
- Claim: Cryptographic signatures prevent tampering with permission metadata during transmission or server-side processing
- Mechanism: The Encrypted Prompt includes a public key; the server holds the corresponding private key. Before processing, the server validates the key pair match. Mismatches indicate tampering or stale permissions, triggering rejection or re-verification
- Core assumption: The private key remains secure on the server and the cryptographic implementation has no vulnerabilities
- Evidence anchors: [section 1] "For public/private key verification to prevent permission from being modified, RSA or other methods can be used." [section 4] "The public key in the encrypted prompt is compared with the private key on the server to ensure that permissions have not been altered"
- Break condition: If the key pair is compromised or the signature verification logic is bypassed, permissions can be forged

### Mechanism 3
- Claim: Software-level implementation allows flexible, application-specific permission policies without model modifications
- Mechanism: Permissions are defined at the application layer rather than OS kernel, enabling per-application rules, dynamic adjustment based on user/device state, and integration with existing LLM pipelines without retraining
- Core assumption: Developers correctly define permission boundaries and the application runtime enforces them consistently
- Evidence anchors: [section 1] "Encrypted prompt could be implemented in the software (application) level, allowing easier implementation." [section 4] "Permissions can be implemented at the application layer rather than the system layer"
- Break condition: If application logic has bugs or the permission definitions don't cover all API surfaces, gaps emerge

## Foundational Learning

- Concept: **Public-Key Cryptography (RSA/ECDSA)**
  - Why needed here: Understanding how asymmetric encryption ensures the Encrypted Prompt's integrity via signature verification
  - Quick check question: Can you explain why the private key must never leave the server in this architecture?

- Concept: **Access Control Models (RBAC/ABAC)**
  - Why needed here: The permission schema in the paper maps to established access control patterns (levels, boolean flags, graphs)
  - Quick check question: How would you represent "read-only access to cloud storage but no email sending" in a boolean permission vector?

- Concept: **Prompt Injection Attack Vectors**
  - Why needed here: Understanding direct vs. indirect injection scenarios clarifies why the defense targets action execution rather than LLM behavior
  - Quick check question: In the "malicious online content" scenario, why can't the LLM itself prevent the attack?

## Architecture Onboarding

- Component map: Client device -> Server (middleware, LLM, permission store) -> APIs
- Critical path: 1) Permission determination on client (based on auth state, device context) 2) Encrypted Prompt assembly and transmission 3) Server-side key verification (fail-fast if mismatch) 4) LLM inference and action generation 5) Permission enforcement before any API execution
- Design tradeoffs: Flexibility vs. complexity (permission schemas can be simple or complex), Client-side trust (compromised client could inject valid but overly permissive tokens), Latency (cryptographic verification adds minimal overhead)
- Failure signatures: Key mismatch errors (indicates tampering or stale sessions), Permission denied for legitimate actions (suggests permission schema misconfiguration), Authorized but harmful actions (defense gap)
- First 3 experiments: 1) Implement a minimal prototype with 3 permission levels and 5 mock APIs; test with benign and injection prompts 2) Simulate a key tampering attack by modifying the Encrypted Prompt in transit; verify rejection 3) Benchmark latency impact of RSA-2048 signature verification on a 1000-request batch

## Open Questions the Paper Calls Out
1. What specific data structures (e.g., integer levels, boolean sets, or graph structures) offer the optimal balance between flexibility and security when defining permission rules for different LLM-integrated scenarios? The paper proposes the Encrypted Prompt mechanism but leaves the actual logic and structure of permission definitions entirely to the developer.

2. How can the Encrypted Prompt framework be extended to detect and mitigate adversarial attacks that trigger actions strictly within the user's authorized permission scope? The current method functions as a binary gatekeeper based on permissions but lacks the semantic capability to distinguish between legitimate and malicious use of permitted actions.

3. What are the most effective system response strategies (e.g., silent rejection, user re-authentication, or decoy responses) when an LLM generates an action that exceeds the encrypted permissions? The paper defines the verification step but does not explore the security implications or user experience trade-offs of subsequent reactions to blocked actions.

## Limitations
- The method cannot safeguard against "authorized" actions resulting from various attacks - if the LLM generates a legitimate action within permissions but for malicious purposes, the framework cannot distinguish this
- Permission schemas are left undefined, requiring developers to determine appropriate structures without guidance on optimal designs
- Effectiveness against real-world attack patterns remains unverified without empirical validation or benchmark datasets

## Confidence

**High confidence**: The cryptographic verification mechanism using public/private key pairs is well-established and the paper correctly identifies this as the core integrity protection for permission metadata. The architectural separation of permission enforcement from model behavior is sound.

**Medium confidence**: The general approach of appending permission metadata to prompts and verifying before action execution is valid, but the practical effectiveness depends heavily on implementation details not provided. The flexibility of permission schemas is theoretically correct but introduces significant complexity in real deployments.

**Low confidence**: Claims about effectiveness against prompt injection attacks lack empirical support. Without concrete attack scenarios, evaluation methodology, or benchmark results, it's impossible to assess whether this approach actually prevents the stated threats or merely adds overhead.

## Next Checks

1. **Permission Schema Implementation**: Design and implement a concrete permission format for a realistic API surface (e.g., 10-15 common cloud service APIs). Test whether the schema can express necessary restrictions without being overly permissive or restrictive, and verify that the serialization/deserialization pipeline is secure against injection.

2. **Attack Surface Analysis**: Construct a suite of prompt injection scenarios including direct injection, indirect injection via external content, and multi-step attacks. Systematically test whether the Encrypted Prompt mechanism blocks all malicious actions while preserving legitimate functionality, measuring both false positive and false negative rates.

3. **Key Management Security**: Implement the full key generation, distribution, and verification pipeline. Test for vulnerabilities including replay attacks (reusing valid Encrypted Prompts), forgery attempts (modifying permissions in valid tokens), and man-in-the-middle scenarios. Measure the cryptographic overhead and assess whether the approach remains practical at scale.