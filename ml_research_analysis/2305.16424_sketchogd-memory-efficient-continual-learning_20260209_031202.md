---
ver: rpa2
title: 'SketchOGD: Memory-Efficient Continual Learning'
arxiv_id: '2305.16424'
source_url: https://arxiv.org/abs/2305.16424
tags:
- sketching
- learning
- tasks
- sketchogd
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of orthogonal gradient
  descent (OGD) for continual learning, where storing all past gradients becomes prohibitive.
  The authors propose SketchOGD, which uses matrix sketching to compress gradients
  into a fixed-size sketch, enabling online updates without advance knowledge of task
  count.
---

# SketchOGD: Memory-Efficient Continual Learning

## Quick Facts
- arXiv ID: 2305.16424
- Source URL: https://arxiv.org/abs/2305.16424
- Reference count: 40
- One-line primary result: Proposes SketchOGD, using matrix sketching to compress gradients for memory-efficient continual learning, outperforming baselines under fixed memory budgets.

## Executive Summary
This paper addresses the memory inefficiency of orthogonal gradient descent (OGD) for continual learning, where storing all past gradients becomes prohibitive. The authors propose SketchOGD, which uses matrix sketching to compress gradients into a fixed-size sketch, enabling online updates without advance knowledge of task count. Three variants are introduced, each using different sketching methods (direct, symmetric, and hybrid), balancing memory and approximation quality. Theoretical analysis provides bounds on reconstruction error tied to spectral decay, showing tighter guarantees for sharper decay. Experiments on Rotated/Permuted MNIST, Split MNIST, and Split CIFAR show SketchOGD outperforms existing memory-efficient OGD variants under fixed memory budgets, with performance depending on task-specific spectral characteristics. SketchOGD offers a scalable, theoretically grounded solution for long-horizon continual learning.

## Method Summary
SketchOGD compresses model gradients into a fixed-size matrix sketch using random projections, enabling memory-efficient continual learning without prior knowledge of task count. For each new gradient encountered during training, it updates a sketch matrix online using one of three methods: direct sketching of gradients, sketching of the symmetric gradient product, or a hybrid symmetric approach. An orthonormal basis is extracted from the sketch before each new task, and subsequent weight updates are projected orthogonal to this basis to preserve performance on prior tasks. The approach maintains a user-specified memory budget regardless of the number of tasks, with theoretical bounds on approximation error tied to the spectral decay of the gradient matrix.

## Key Results
- SketchOGD-1 (direct sketching) outperforms baselines in Rotated MNIST with k=1200, while SketchOGD-2 (symmetric sketching) achieves similar results with k=600, demonstrating memory-efficiency.
- Theoretical error bounds are tighter for SketchOGD-2,3 when gradient singular values decay sharply, matching empirical observations where these methods excel on tasks with pronounced spectral decay.
- Performance varies by task: SketchOGD-2 outperforms SketchOGD-1 on Rotated MNIST (sharper decay) but underperforms on Permuted MNIST (slower decay), highlighting the importance of spectral characteristics.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fixed-size matrix sketching can approximate the gradient subspace needed for orthogonal projection, enabling memory-efficient continual learning.
- **Mechanism:** The algorithm maintains a sketch (Y, W) of the gradient matrix G using random projection matrices (Ω, Ψ). For each new gradient g, the sketch is updated online via linear operations (Y ← Y + gω^T for Method 1; Y ← Y + g(g^TΩ) for Method 2). An orthonormal basis B is extracted from the sketch (via orth Y or QR decomposition), and weight updates are projected orthogonal to B to preserve performance on prior data.
- **Core assumption:** The gradient matrix G has low effective rank or sharply decaying singular values, so a fixed-size sketch captures its range with acceptable reconstruction error.
- **Evidence anchors:**
  - [abstract] "SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fixed, user-determined size."
  - [section II-C] "A sketch (Ω,Ψ, Y, W) of a matrix A∈R m×n is an approximation formed by first drawing two i.i.d. standard normal matrices..."
  - [corpus] No direct corpus papers validate this specific sketching approach for continual learning; related work (COLA, Continual Learning via Sparse Memory Finetuning) focuses on different memory-efficient strategies.
- **Break condition:** The reconstruction error E_G(B) = ∥(I - BB^T)G∥_F^2 grows large when the gradient spectrum is flat or the sketch size k is too small relative to the effective rank of G, causing the sketch to miss critical directions.

### Mechanism 2
- **Claim:** Sketching GG^T (rather than G directly) provides tighter approximation bounds when singular values decay sharply.
- **Mechanism:** Methods 2 and 3 sketch the symmetric matrix GG^T, which has the same range as G but allows symmetric approximation (Proposition II.3). The quadratic scaling of magnitudes in GG^T emphasizes larger gradients, making the sketch more sensitive to important directions. Theoretical bounds (Theorem IV.3) show error scales with Tr(Σ_2^2) Tr(Σ_1^{-1}) + Tr(Σ^2), which is tighter than Method 1's bound when Tr(Σ_2^2) Tr(Σ_1^{-1}) ≤ Tr(Σ^2).
- **Core assumption:** The singular values of G decay sufficiently fast (e.g., step decay or exponential decay) so that Σ_1 (top γ singular values) dominates.
- **Evidence anchors:**
  - [section IV-B] "Remark IV.4: Note that this is a tighter bound than Theorem IV.1 when Tr(Σ_2^2) Tr(Σ_1^{-1}) ≤ Tr(Σ^2). The smaller the lower singular values are, and the larger the higher singular values are, the more advantageous this bound becomes."
  - [section VII] "SketchOGD-2 outperforms SketchOGD-1 in Rotated MNIST despite its higher memory cost. This aligns with the tighter theoretical error bounds for SketchOGD-2,3 under sharper spectral gradient decay (Figure 2)..."
  - [corpus] No corpus papers examine spectral decay in this context.
- **Break condition:** If singular values decay slowly (e.g., near-flat spectrum), Tr(Σ_2^2) Tr(Σ_1^{-1}) may exceed Tr(Σ^2), and Method 2/3's advantage disappears; Method 1 may be more favorable.

### Mechanism 3
- **Claim:** Orthogonal projection of weight updates preserves prior task outputs by preventing changes along stored gradient directions.
- **Mechanism:** During training on task τ, each update Δw is projected: w ← w + (I - BB^T)Δw, where B is the orthonormal basis from the sketch of previous gradients. This ensures updates lie orthogonal to range(G_{1:τ-1}), locally preserving predictions on prior data (since output changes are bounded by g^T Δw for stored gradients g).
- **Core assumption:** The model is locally linear around the current weights (first-order Taylor approximation holds), and the stored gradients span the relevant directions for prior tasks.
- **Evidence anchors:**
  - [section II-A] "OGD stores the gradients ∇_w f(x_i, w)... and future weight updates are projected onto the orthogonal subspace (range G)^⊥."
  - [section V] "Remark V.2: A small reconstruction error implies that G_{τ_S} is nearly contained in the sketched subspace, thereby leading to smaller catastrophic forgetting."
  - [corpus] Related papers (e.g., CIP-Net, COLA) use different mechanisms (prototype-based or adapter retrieval) rather than orthogonal projection.
- **Break condition:** If the sketch reconstruction error E_G(B) is large, critical gradient directions are missed, and subsequent updates may interfere with prior task outputs, leading to catastrophic forgetting.

## Foundational Learning

- **Concept:** Orthogonal projection and subspaces
  - Why needed here: Understanding how (I - BB^T)Δw projects updates orthogonal to the span of B is central to SketchOGD's anti-forgetting mechanism.
  - Quick check question: If B has 3 orthonormal columns, what is the rank of (I - BB^T) in R^10?

- **Concept:** Singular value decomposition (SVD) and spectral decay
  - Why needed here: The theoretical bounds and method selection depend on the decay rate of singular values of the gradient matrix G.
  - Quick check question: If Σ has diagonal entries [10, 5, 1, 0.1, 0.01], what is Tr(Σ^2) and why does it matter for sketching?

- **Concept:** Matrix sketching via random projections
  - Why needed here: The core memory compression technique uses random Gaussian matrices to form low-rank approximations.
  - Quick check question: Why does Y = AΩ (with Ω ∈ R^{n×k}) provide information about the range of A?

## Architecture Onboarding

- **Component map:**
  Sketch state (Y, optionally W, Ω, Ψ) -> Basis extraction (QR decomposition) -> Projection operator (I - BB^T) -> Update loop (online sketch update, weight projection)

- **Critical path:**
  1. Initialize sketch (Y=0, draw random Ω/Ψ).
  2. For each task ≥ 2: extract B from sketch.
  3. For each training step: compute Δw from base optimizer, project with (I - BB^T), update weights.
  4. Periodically (e.g., on random subset of training points): compute gradient g of correct logit, update sketch.

- **Design tradeoffs:**
  - Method 1 (sketch G): Lowest memory (pk), simpler, may suffice for slow spectral decay.
  - Method 2 (sketch GG^T): More memory (2pk), tighter bounds for sharp decay.
  - Method 3 (symmetric sketch GG^T): Highest memory (≥4pk), best reconstruction (Theorem IV.4), but may be overkill if spectral decay is moderate.
  - Larger k improves approximation but increases memory and computation (O(pk^2) for basis extraction).

- **Failure signatures:**
  1. Rapid accuracy drop on early tasks → sketch missing critical gradient directions (k too small or spectral decay slower than expected).
  2. Memory overflow → forgot to fix k/l parameters; sketch size growing with tasks (should be constant).
  3. Numerical instability in basis extraction → Ω_1 not full rank (rare with Gaussian draws, but possible if γ > k-2).

- **First 3 experiments:**
  1. Replicate Rotated MNIST with k=1200 for Method 1 and k=600 for Method 2; compare task-wise accuracy curves against Figure 4.
  2. Ablate sketch size: vary k from 200 to 1500 on Permuted MNIST; plot final average accuracy vs. memory to verify tradeoff.
  3. Test on a task sequence with artificially flattened gradient spectrum (e.g., by normalizing gradients); verify Method 1 outperforms Method 2/3 as per Remark IV.5.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does intentionally scaling gradients by powers of their magnitudes prior to compression improve SketchOGD performance?
  - Basis in paper: [explicit] The authors state in the Future Work section, "we may test whether scaling the gradients by powers of their magnitudes before compression may influence results for the better."
  - Why unresolved: SketchOGD-2 and -3 currently benefit incidentally from quadratic magnitude scaling via GG^T, but the effect of explicit, varied scaling powers remains untested.
  - What evidence would resolve it: Empirical results from experiments where gradients are scaled by specific exponents before sketching, compared against the current baseline performance.

- **Open Question 2**
  - Question: How do distinct types of singular value decay profiles (e.g., linear vs. step decay) differentially impact the error bounds for Sketching Method 1 versus Method 2?
  - Basis in paper: [explicit] The authors note they "would like to investigate the way different types of singular value decay would affect these two bounds differently."
  - Why unresolved: While the paper establishes that sharper decay favors Method 2/3, the specific theoretical implications of different decay shapes on the bound gap (Tr(Σ^2) vs Tr(Σ_2^2)Tr(Σ_1^{-1})) are not fully characterized.
  - What evidence would resolve it: A theoretical derivation or simulation showing the bound behavior for various synthetic spectral decay distributions.

- **Open Question 3**
  - Question: How does SketchOGD perform on modern architectures (e.g., ResNets, Transformers) and large-scale datasets like ImageNet?
  - Basis in paper: [explicit] The authors identify "extending the empirical study to larger-scale datasets (e.g., CORe50, ImageNet) and modern architectures (e.g., ResNets, Transformers)" as an important avenue for future research.
  - Why unresolved: The paper's experiments are limited to fully connected networks and LeNet on MNIST/CIFAR; scalability to deeper or attention-based models is unproven.
  - What evidence would resolve it: Benchmarking SketchOGD against baselines on standard large-scale benchmarks using ResNet or Vision Transformer architectures.

## Limitations

- The paper's claims hinge on assumptions about gradient spectral decay that are not validated empirically across diverse tasks, with Method 1 often performing competitively despite weaker theoretical guarantees.
- Optimal sketch size and method choice appear task-dependent without clear selection criteria, and the paper lacks ablation studies on sketch size k to quantify the memory-accuracy tradeoff.
- The choice of random projection matrices (Gaussian) versus potentially more efficient sketching methods (like CountSketch) is not explored, leaving open questions about implementation efficiency.

## Confidence

- **High confidence**: The orthogonal projection mechanism (Mechanism 3) is well-established in OGD literature and the implementation details are clearly specified.
- **Medium confidence**: The memory-efficiency claims are supported by experiments, but the optimal sketch size and method choice appear task-dependent without clear selection criteria.
- **Low confidence**: The theoretical advantages of Methods 2/3 over Method 1 are not consistently reflected in empirical results, suggesting the spectral decay assumptions may not hold in practice or the bounds are loose.

## Next Checks

1. **Spectral Decay Analysis**: Compute and visualize the singular value decay of gradient matrices across all tasks in the experimental datasets to empirically validate the assumptions underlying Method 2/3's theoretical advantages.
2. **Memory-Accuracy Pareto Frontier**: Systematically vary k from 100 to 2000 for each method on Split CIFAR-100, plotting final average accuracy against memory usage to quantify the true memory-efficiency tradeoff.
3. **Sketching Method Comparison**: Implement CountSketch or other structured random projections and compare against Gaussian sketching on Rotated MNIST to test whether the choice of sketching method impacts performance beyond just memory constraints.