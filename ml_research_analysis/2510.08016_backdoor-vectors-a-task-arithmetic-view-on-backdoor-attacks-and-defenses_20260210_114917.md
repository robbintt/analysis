---
ver: rpa2
title: 'Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses'
arxiv_id: '2510.08016'
source_url: https://arxiv.org/abs/2510.08016
tags:
- backdoor
- merging
- task
- attack
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work frames backdoor attacks in model merging as task arithmetic
  by introducing Backdoor Vectors (BVs), which are the difference between backdoored
  and clean fine-tuned models. BVs provide a simple, unified framework to understand,
  measure, and improve backdoor attacks.
---

# Backdoor Vectors: a Task Arithmetic View on Backdoor Attacks and Defenses

## Quick Facts
- **arXiv ID:** 2510.08016
- **Source URL:** https://arxiv.org/abs/2510.08016
- **Reference count:** 40
- **Primary result:** Introduces Backdoor Vectors (BVs) and Sparse Backdoor Vectors (SBVs) for stronger backdoor attacks, plus Injection BV Subtraction (IBVS) defense that reduces ASR with minimal accuracy loss

## Executive Summary
This paper frames backdoor attacks within the model merging paradigm as task arithmetic by introducing Backdoor Vectors (BVs), which are the element-wise differences between backdoored and clean fine-tuned models. BVs provide a simple, unified framework to understand, measure, and improve backdoor attacks through vector arithmetic operations. The authors propose Sparse Backdoor Vectors (SBVs) that merge multiple BVs to create stronger, more resilient attacks, and introduce Injection BV Subtraction (IBVS), a lightweight defense that mitigates unknown backdoor attacks by subtracting a fixed BV. Experiments on ViT-B-32, ViT-L-14, and ConvNeXt show that SBVs significantly improve ASR in single-task and multi-task settings, while IBVS reduces ASR with minimal accuracy loss.

## Method Summary
The core method introduces Backdoor Vectors (BVs) as the difference between weights of backdoored and clean fine-tuned models, enabling backdoor injection and mitigation through simple vector arithmetic. SBVs enhance attack strength by merging multiple BVs using sign-consistent sparsification, which filters out non-essential noise and amplifies core malicious signals. The IBVS defense exploits the observation that inherent triggers share structure with simpler injected triggers, allowing a fixed BV from a simple trigger to suppress more complex attacks. The framework operates within model merging contexts where multiple fine-tuned models are combined, and demonstrates effectiveness across both single-task (multiple models for same task) and multi-task (models for different tasks) scenarios.

## Key Results
- SBVs achieve ASR of 99.99% in single-task merging scenarios, outperforming state-of-the-art methods
- IBVS defense reduces ASR from 81.66% to 0.01% on ImageNet with only 0.09% drop in clean accuracy
- SBVs maintain high ASR (up to 99.55%) even in multi-task merging settings with up to 10 tasks
- The defense is effective against unknown backdoor attacks without requiring knowledge of the specific attack

## Why This Works (Mechanism)

### Mechanism 1: BV Linear Separability
Backdoor functionality is linearly separable in weight space and can be captured by the difference between a poisoned and clean fine-tuned model. The BV framework assumes that backdoor behavior manifests as a consistent direction in weight space that can be isolated through subtraction.

### Mechanism 2: SBV Sign-Consistent Merging
Multiple BVs from different backdoor instances share a core weight-space signature. By enforcing sign consistency across these vectors, the sparsification process amplifies the shared malicious signal while filtering out noise, creating a more robust attack that resists dilution during model merging.

### Mechanism 3: Inherent Trigger Vulnerability
Inherent triggers exploit adversarial vulnerabilities in the pre-trained base model, creating a generalizable structure that overlaps with simpler injected triggers. This shared structure enables the IBVS defense to subtract a BV from a simple trigger to suppress more complex attacks.

## Foundational Learning

**Task Arithmetic / Task Vectors**: Understanding that model capabilities can be treated as vectors in weight space is essential for grasping how backdoors can be added, subtracted, or merged. *Quick check*: If you add a task vector for "sentiment analysis" to a pre-trained language model, what capability should the resulting model gain?

**Model Merging / Weight-Space Operations**: The threat model specifically involves combining multiple fine-tuned models through weight averaging. This differs from standard fine-tuning scenarios. *Quick check*: In single-task merging, multiple models are fine-tuned for the same task but with different random seeds. How does this differ from multi-task merging?

**Backdoor Attacks & Attack Success Rate (ASR)**: Understanding that backdoors create hidden behaviors triggered by specific patterns, and that ASR measures the effectiveness of these attacks, is critical for evaluating the framework. *Quick check*: A model has 95% accuracy on clean data. What would a high ASR signify when evaluating that same model on data containing the backdoor trigger?

## Architecture Onboarding

**Component map**: Backdoor Vector Generator -> Sparse BV Combiner (Algorithm 1) -> Sparse Mask Generator (Algorithm 2) -> Injection BV Subtraction Module

**Critical path**: The SBV Combiner is the core attack component whose output must preserve malicious signal while being sparse enough to avoid dilution. The IBVS Module is the critical defense component that relies on overlap between defense BV and unknown backdoor.

**Design tradeoffs**: 
- SBV sparsification threshold: Increasing BVs improves ASR but requires more training; too sparse masks lose signal
- IBVS subtraction coefficient: Higher values reduce ASR more but incur larger clean accuracy drops
- IBVS trigger choice: Simple fixed triggers must have sufficient overlap with complex attacks to be effective

**Failure signatures**:
- Attack Failure: ASR drops to baseline levels after merging, indicating overly aggressive sparsification or inconsistent constituent BVs
- Defense Failure: IBVS causes significant clean accuracy drops without meaningful ASR reduction, indicating interference with normal task knowledge

**First 3 experiments**:
1. Compute BV from clean vs backdoored models and verify backdoor transfer to different clean model
2. Generate 5 BVs with different seeds, create SBV, merge with 9 clean vectors, measure ASR vs naive average
3. Apply IBVS using white-square trigger BV to merged model and measure ASR reduction vs undefended baseline

## Open Questions the Paper Calls Out
- Does the BV framework generalize to large language models and non-vision architectures? (Explicitly stated as limitation)
- Why does a simple injected trigger BV effectively defend against unknown inherent trigger attacks? (Mechanism unclear)
- Does SBV attack strength plateau beyond k=5 backdoor vectors? (Limited exploration beyond k=5)

## Limitations
- Framework focuses on CLIP-like vision models; applicability to other architectures remains unexplored
- Assumes backdoor functionality is linearly separable in weight space, which may not hold for all attack types
- IBVS defense effectiveness relies on assumption that inherent and injected triggers share generalizable structure

## Confidence
- BV linear separability assumption: Medium confidence
- SBV sign consistency effectiveness: Medium confidence  
- IBVS defense mechanism: Low confidence
- Generalization to other architectures: Low confidence

## Next Checks
1. Test BV arithmetic on backdoors created via non-trigger-based methods (e.g., dataset poisoning without visual patterns) to verify linearity assumptions
2. Evaluate SBV resilience against fine-tuning-based defenses by applying a few steps of unlearning to merged models and measuring ASR degradation
3. Compare IBVS against baseline that subtracts random Gaussian noise matched to BV statistics to confirm defense exploits specific backdoor structure