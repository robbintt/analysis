---
ver: rpa2
title: 'SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich
  Industrial Assembly Tasks'
arxiv_id: '2509.13949'
source_url: https://arxiv.org/abs/2509.13949
tags:
- learning
- reinforcement
- share
- human
- assembly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHaRe-RL is a structured, interactive reinforcement learning framework
  for contact-rich industrial assembly. It combines manipulation primitives, human
  demonstrations and online corrections, and per-axis adaptive force limits to enable
  safe, sample-efficient learning in real-world tasks.
---

# SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks

## Quick Facts
- arXiv ID: 2509.13949
- Source URL: https://arxiv.org/abs/2509.13949
- Reference count: 40
- 95% success rate, 5.4 s cycle time for 0.2–0.4 mm clearance Harting connector insertion within 3 h of real-world interaction

## Executive Summary
SHaRe-RL is a structured, interactive reinforcement learning framework for contact-rich industrial assembly. It combines manipulation primitives, human demonstrations and online corrections, and per-axis adaptive force limits to enable safe, sample-efficient learning in real-world tasks. Experiments on Harting connector insertion show 95% success rate and 5.4 s cycle time within 3 h of real-world interaction—outperforming both HG-DAgger (80%, 7.4 s) and a skilled human demonstrator (6.5 s). The adaptive safety mechanism provably bounds contact forces, and ablation studies confirm the necessity of each component for achieving reliable, high-precision assembly.

## Method Summary
SHaRe-RL decomposes contact-rich assembly into scripted manipulation primitives (Approach, Insert, Press, Release, Retreat) with one learned adaptive primitive (InsertAMP). A human-in-the-loop RLPD (off-policy SAC variant) trains on equal minibatches from demonstration and on-policy buffers, with online corrections via SpaceMouse. An adaptive per-axis force limit $F_{lim,t} = \alpha(|F_{meas,t}|)F_{max}$ contracts exponentially upon contact, provably bounding forces regardless of policy output. Sparse binary success rewards and vision-based perception complete the architecture.

## Key Results
- 95% success rate and 5.4 s cycle time on 0.2–0.4 mm clearance Harting connector insertion
- Outperforms HG-DAgger (80% success, 7.4 s) and skilled human demonstrator (6.5 s)
- Achieves results within 3 h of real-world interaction using 20 demonstrations and online corrections

## Why This Works (Mechanism)

### Mechanism 1: Search Space Reduction via Manipulation Primitive Nets
Structuring tasks into manipulation primitives (MPs) reduces effective exploration complexity, enabling sample-efficient learning in high-dimensional spaces. The system decomposes long-horizon assembly into a sequence of MPs, learning only specific parameters within "Adaptive MPs" while coarse movements are scripted. Core assumption: task can be cleanly decomposed into distinct phases where only a subset of degrees of freedom require adaptive control. Evidence: ablation without priors fails to align despite dense rewards due to large state space.

### Mechanism 2: Human-Guided Density Estimation
Combining offline demonstrations with online corrections likely accelerates convergence by biasing replay buffer toward high-reward states difficult to reach via random exploration. The learner samples mini-batches equally from demonstration and on-policy buffers. When policy fails or hesitates, human operator intervenes via SpaceMouse, injecting successful state-action trajectories. Core assumption: reward signal is sufficiently sparse that random exploration fails to discover successful states within practical time limits. Evidence: SHaRe (no-interventions) plateaus at ~40% success.

### Mechanism 3: Stable Contact Dynamics via Adaptive Force Limits
A deterministic, adaptive force limit theoretically bounds contact forces during exploration, preventing hardware damage and stabilizing policy update distribution. The system uses recurrence $F_{lim,t} = \alpha(|F_{meas,t}|)F_{max}$ where α decays exponentially upon contact, creating contracting system converging to safe equilibrium force. Core assumption: contact environment is sufficiently rigid to validate force measurements and impedance controller responds quickly to limit changes. Evidence: convergence to $F^*_{lim}$ guaranteed when recurrence is locally stable.

## Foundational Learning

- **Concept: Task Frame Formalism (TFF)**
  - Why needed: Architecture relies on defining constraint frames to switch between position and force control
  - Quick check: Can you define a coordinate frame where z-axis is normal to surface, and explain why you would choose force control for z and position control for x/y?

- **Concept: Off-Policy RL with Demonstrations (RLPD)**
  - Why needed: Method uses specific SAC variant designed to handle mixed data sources without offline pre-training
  - Quick check: What is primary risk when training off-policy RL agent on replay buffer containing only expert data versus mix of exploration data?

- **Concept: Impedance Control**
  - Why needed: Robot must behave like spring-mass-damper system to be safe; adaptive limits modify wrench commands but underlying behavior governed by impedance settings
  - Quick check: If you command high velocity toward wall with high stiffness, what happens to contact force compared to same velocity with low stiffness?

## Architecture Onboarding

- **Component map:** Actor Process (robot, 10 Hz) -> MP-Net Engine (state machine) -> Safety Layer (α(F) recurrence) -> Learner Process (GPU) -> Demo/Policy Buffers -> RLPD (SAC)
- **Critical path:** Setting up MP-Net structure; if stop conditions or frames are wrong, policy will train on wrong phase of task
- **Design tradeoffs:** Static vs Adaptive Limits (static increases cycle time while adaptive allows aggressive free-space motion but requires tuning decay θ); Dense vs Sparse Rewards (dense failed due to flat landscapes in rotation)
- **Failure signatures:** High Force Transients (force limit recurrence doesn't shrink fast enough, causing "ringing" or overshoot in Fmeas); Hesitation (demos insufficient or reward scaling wrong, policy outputs near-zero velocities)
- **First 3 experiments:**
  1. System Identification: Validate adaptive force limit in isolation; command step input into wall and verify Fmeas converges to calculated F*lim without physical damage
  2. Primitive Validation: Run MP-Net with zero-policy or random actions inside AMP; verify safety layer keeps robot safe and transitions trigger correctly
  3. Overfit Test: Collect 20 demos and train without online interaction; verify behavioral cloning baseline works moderately well

## Open Questions the Paper Calls Out
- How does sample efficiency and intervention burden scale when applied to longer-horizon assembly sequences with multiple adaptive primitives?
- Can learned policies generalize zero-shot to novel connector geometries or tighter tolerances, or is domain-specific retraining always required?
- Can reliance on human teleoperation be reduced or replaced by model-based exploration or self-supervised rewards without sacrificing sample efficiency?

## Limitations
- Results limited to single connector type with specific tolerances; generalization to other geometries unproven
- Heavy reliance on human demonstrations and interventions may not scale to longer-horizon tasks
- Theoretical stability guarantees assume rigid-body contact, which may not hold for compliant materials

## Confidence
- High confidence: Adaptive force limit mechanism is theoretically sound and experimentally validated for preventing contact damage
- Medium confidence: Human-guided RLPD approach effectively bootstraps learning from demonstrations, but long-term reliance on human interventions is unclear
- Medium confidence: MP-Net structuring demonstrably reduces sample complexity for this specific assembly task, but applicability to less structured contact-rich tasks is uncertain

## Next Checks
1. **Generalization Test**: Evaluate SHaRe-RL on different connector geometry or distinct contact-rich assembly task to assess robustness of learned policies and MP-Net decomposition
2. **Ablation of Human Intervention Frequency**: Systematically vary frequency and quality of human interventions during training to quantify minimum human guidance required
3. **Real-World Stress Test**: Deploy trained policy in production-like environment with variable lighting, part wear, and operator variability to test practical limits of vision-based perception and adaptive safety system