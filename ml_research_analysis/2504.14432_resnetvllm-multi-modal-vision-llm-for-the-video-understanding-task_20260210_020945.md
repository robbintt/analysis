---
ver: rpa2
title: ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task
arxiv_id: '2504.14432'
source_url: https://arxiv.org/abs/2504.14432
tags:
- video
- understanding
- resnetvllm
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ResNetVLLM introduces a zero-shot video understanding framework
  that combines a ResNet-based visual encoder with a Large Language Model, avoiding
  reliance on pre-trained video models by using a non-pretrained ResNet for visual
  feature extraction. This design allows joint learning of visual and semantic representations
  within a unified architecture.
---

# ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task

## Quick Facts
- arXiv ID: 2504.14432
- Source URL: https://arxiv.org/abs/2504.14432
- Reference count: 40
- State-of-the-art zero-shot video understanding performance on multiple benchmarks

## Executive Summary
ResNetVLLM introduces a novel zero-shot video understanding framework that combines a randomly initialized ResNet visual encoder with a Large Language Model. The model avoids reliance on pre-trained video models by learning visual features from scratch within the language modeling loop. This approach achieves state-of-the-art performance in zero-shot video understanding across multiple benchmarks, scoring 3.55/5 on Video-based Text Generation and achieving strong results on video question answering tasks.

## Method Summary
The model employs a two-phase training strategy: a Warm-Up phase where only the ResNet visual encoder is trained for 150 epochs to map video frames to LLM-compatible embeddings, followed by a Joint-Training phase where the entire architecture is fine-tuned together. The system processes videos by sampling 100 frames, extracting 2D ResNet features, flattening them, and feeding them as visual tokens into an LLM alongside text prompts. The training uses SGD for the warm-up phase and AdamW for joint fine-tuning, with Bayesian optimization for hyperparameter tuning.

## Key Results
- Achieves 3.55/5 on Video-based Text Generation benchmark
- Attains 78.3% accuracy on MSVD-QA video question answering
- Scores 63.5% on MSRVTT-QA, 59.9% on TGIF-QA, and 54.8% on ActivityNet-QA

## Why This Works (Mechanism)

### Mechanism 1
Training a visual encoder from scratch within the language modeling loop produces visual embeddings more semantically aligned with the LLM's internal representations than frozen, pre-trained features. The ResNet encoder is initialized randomly and updated via backpropagation from the language modeling loss, forcing visual features to evolve specifically to minimize text generation error rather than relying on generic feature extractors.

### Mechanism 2
Decoupling training into a Warm-Up phase (visual encoder only) followed by Joint-Training prevents unstable gradients from a randomly initialized visual encoder from destroying pre-trained LLM weights. The first stage trains the ResNet to map frames to the LLM's input space without updating the LLM, while the second stage fine-tunes the entire network with AdamW.

### Mechanism 3
Utilizing 2D ResNet with flattened global average pooling features allows the model to process video as a sequence of frame-level tokens, relying on the LLM's self-attention to model temporal relationships rather than explicit 3D convolutions. This treats video frames as independent images after feature extraction and flattens them into a token sequence for temporal reasoning.

## Foundational Learning

- **Vision-Language Alignment (Projection Layers)**: Why needed - The model must map ResNet feature vectors into the LLM's embedding space. Quick check - How does the model ensure that a visual vector representing a "dog" is mathematically close to the token embedding for "dog"?

- **Zero-Shot Generalization**: Why needed - The paper claims "zero-shot" performance by avoiding pre-trained video models. Quick check - Why does avoiding pre-trained visual weights arguably make this a "purer" zero-shot transfer learning experiment?

- **Tokenization of Visual Data**: Why needed - LLMs process text tokens, so video frames must be converted into a format the LLM accepts. Quick check - How does flattening ResNet features turn a spatial grid into a sequence the Transformer can process?

## Architecture Onboarding

- **Component map**: Video → Frame Sampling (100 frames) → ResNet (2D) → Global Average Pooling → Flatten → Visual Tokens → Projection Layer → LLM → Output Text

- **Critical path**: The ResNet-to-LLM Projection is the bottleneck. If this layer does not effectively compress spatial information into semantic tokens during the Warm-Up phase, the LLM receives noise.

- **Design tradeoffs**: 2D vs. 3D - Using 2D ResNet is computationally cheaper but sacrifices native motion feature extraction. Vanilla vs. Pretrained - Initializing ResNet randomly avoids "concept leakage" but requires more training data to converge.

- **Failure signatures**: Hallucination - describes objects not present in the video. Static Bias - describes the scene accurately but fails to identify actions. Training Instability - loss diverges early if Warm-Up phase is skipped.

- **First 3 experiments**: 1) Ablate Warm-Up - train without the 150-epoch ResNet-only phase. 2) Visual Encoder Comparison - swap random ResNet for frozen CLIP visual encoder. 3) Temporal Sampling Sensitivity - reduce frame count from 100 to 16 or 8.

## Open Questions the Paper Calls Out

None

## Limitations

- Architectural details remain underspecified, particularly how visual tokens are integrated into the LLM's token stream
- Lack of comparison to traditional video models trained on equivalent video-text pairs makes it difficult to assess practical superiority
- Model's ability to handle temporal reasoning appears entirely delegated to the LLM, raising questions about performance on videos requiring fine-grained motion understanding

## Confidence

- **High confidence**: The training methodology is technically sound and represents a valid approach to multimodal model training
- **Medium confidence**: The core hypothesis that training visual encoders from scratch produces better aligned features is plausible
- **Low confidence**: The claim that this represents a superior "zero-shot" approach is not well-supported without direct comparisons to traditional video models

## Next Checks

1. **Temporal Resolution Sensitivity**: Systematically vary frame sampling rate (8, 16, 32, 64, 100 frames) to determine minimum temporal resolution required for maintaining performance across different video understanding tasks

2. **Pre-trained vs. Random Visual Encoder Comparison**: Replace randomly initialized ResNet with frozen CLIP visual encoder while keeping all other architecture and training details constant to test whether "zero-shot purity" provides actual performance benefits

3. **Cross-dataset Generalization Test**: Evaluate the model on video-text pairs from datasets not seen during training to rigorously test true zero-shot generalization capabilities