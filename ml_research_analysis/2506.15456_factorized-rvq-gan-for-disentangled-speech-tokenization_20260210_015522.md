---
ver: rpa2
title: Factorized RVQ-GAN For Disentangled Speech Tokenization
arxiv_id: '2506.15456'
source_url: https://arxiv.org/abs/2506.15456
tags:
- speech
- latexit
- phonetic
- layer
- sha1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Hierarchical Audio Codec (HAC), a neural\
  \ speech codec that factorizes its bottleneck into three linguistic levels\u2014\
  acoustic, phonetic, and lexical\u2014within a single model. HAC leverages knowledge\
  \ distillation from HuBERT for phoneme-level structure and from LaBSE for lexical\
  \ cues, enabling it to produce disentangled token sets."
---

# Factorized RVQ-GAN For Disentangled Speech Tokenization

## Quick Facts
- arXiv ID: 2506.15456
- Source URL: https://arxiv.org/abs/2506.15456
- Reference count: 0
- This paper introduces HAC, a neural speech codec that factorizes its bottleneck into three linguistic levels—acoustic, phonetic, and lexical—within a single model, enabling disentangled token sets aligned with phonemes and word-level semantics.

## Executive Summary
This paper introduces Hierarchical Audio Codec (HAC), a neural speech codec that factorizes its bottleneck into three linguistic levels—acoustic, phonetic, and lexical—within a single model. HAC leverages knowledge distillation from HuBERT for phoneme-level structure and from LaBSE for lexical cues, enabling it to produce disentangled token sets. Experiments on English and multilingual data show that HAC's factorized bottleneck yields disentangled token sets: one aligns with phonemes, while another captures word-level semantics. HAC outperforms single-level baselines in both disentanglement and reconstruction quality, with multilingual models achieving the highest average Phoneme Normalized Mutual Information (PNMI) of 0.52 across eight languages, and token sets exhibiting strong word detection F1 scores (up to 0.8 for top-performing models).

## Method Summary
HAC factorizes its speech representation bottleneck into three hierarchical levels—acoustic, phonetic, and lexical—using knowledge distillation from HuBERT and LaBSE. The model jointly learns these disentangled representations, allowing for structured speech tokenization. During training, HAC is optimized to reconstruct speech while maintaining the linguistic structure of each level. The resulting token sets are evaluated for their alignment with phonemes and words, and their impact on reconstruction quality is compared against single-level baselines. The method is tested on both English and multilingual datasets.

## Key Results
- HAC achieves the highest average PNMI of 0.52 across eight languages, demonstrating strong phoneme-level disentanglement.
- Token sets from HAC exhibit strong word detection F1 scores, reaching up to 0.8 for top-performing models.
- HAC outperforms single-level baselines in both disentanglement and reconstruction quality, showing the effectiveness of factorized representations.

## Why This Works (Mechanism)
HAC works by explicitly factorizing the speech representation into hierarchical linguistic levels, each capturing different aspects of the signal. By leveraging knowledge distillation from HuBERT and LaBSE, the model is guided to align its internal representations with established phoneme and lexical structures. This hierarchical factorization allows HAC to produce token sets that are both linguistically meaningful and useful for downstream tasks like speech reconstruction and word detection.

## Foundational Learning
- **Phoneme Normalization**: Why needed - to enable fair comparison of phoneme-level representations across languages; Quick check - PNMI scores above 0.5 indicate meaningful alignment.
- **Knowledge Distillation**: Why needed - to transfer linguistic structure from pretrained models to HAC; Quick check - token sets align with HuBERT and LaBSE outputs.
- **Factorized Bottleneck**: Why needed - to separate acoustic, phonetic, and lexical information; Quick check - each token set specializes in its intended level.
- **Multilingual Training**: Why needed - to ensure generalizability across diverse linguistic structures; Quick check - consistent PNMI and F1 scores across languages.
- **Speech Reconstruction**: Why needed - to maintain audio quality while factorizing representations; Quick check - low reconstruction error compared to baselines.
- **Word Detection F1**: Why needed - to quantify lexical-level token set quality; Quick check - F1 scores approaching 0.8 indicate strong lexical alignment.

## Architecture Onboarding

**Component Map**
Encoder -> Factorized Bottleneck (Acoustic/Phonetic/Lexical) -> Decoder

**Critical Path**
The critical path for HAC's performance is the factorized bottleneck, where the three linguistic levels are learned and disentangled. The encoder maps raw audio to this bottleneck, and the decoder reconstructs speech from the factorized representations. Knowledge distillation from HuBERT and LaBSE guides the phonetic and lexical levels, respectively.

**Design Tradeoffs**
- Factorizing the bottleneck improves linguistic disentanglement but may increase model complexity.
- Using pretrained models for distillation improves alignment with linguistic structure but introduces dependency on external models.
- Multilingual training enhances generalization but requires more diverse training data.

**Failure Signatures**
- Poor PNMI or F1 scores indicate insufficient disentanglement at the phoneme or lexical level.
- High reconstruction error suggests the factorized representations lose too much acoustic information.
- Inconsistent results across languages may indicate limited multilingual generalization.

**First Experiments**
1. Train HAC on a single language and evaluate PNMI and word detection F1 to confirm basic disentanglement.
2. Compare HAC's reconstruction quality to a single-level baseline under identical conditions.
3. Conduct an ablation study removing HuBERT and LaBSE distillation to quantify their impact on token quality.

## Open Questions the Paper Calls Out
None.

## Limitations
- The robustness of HAC's disentanglement across diverse linguistic structures beyond the tested languages is uncertain.
- Scalability to languages with very different phonotactic or morphological characteristics is unclear.
- The reliance on pretrained models for distillation raises questions about the meaningfulness of factorized representations without external signals.
- The study does not thoroughly evaluate HAC's performance under extremely low bitrates, where reconstruction artifacts might compromise token interpretability.

## Confidence
- **High**: The core claim that HAC produces disentangled linguistic representations is supported by quantitative evidence (PNMI scores, word detection F1) and explicit architectural design.
- **Medium**: The claim of superior reconstruction quality compared to single-level baselines is plausible but not exhaustively compared to all relevant prior codecs.
- **Low to Medium**: The claim of multilingual generalization is promising but based on a limited set of eight languages with no in-depth cross-linguistic error analysis.

## Next Checks
1. Evaluate HAC on languages with non-phonemic orthographies or rich morphology to assess robustness of factorized representations.
2. Conduct ablation studies removing HuBERT and LaBSE distillation to quantify their impact on token disentanglement and codec performance.
3. Test HAC under aggressive bitrate constraints to measure the trade-off between compression efficiency and linguistic token interpretability.