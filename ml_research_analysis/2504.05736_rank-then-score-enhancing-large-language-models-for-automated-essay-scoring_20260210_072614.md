---
ver: rpa2
title: 'Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring'
arxiv_id: '2504.05736'
source_url: https://arxiv.org/abs/2504.05736
tags:
- essay
- scoring
- essays
- score
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Rank-Then-Score (RTS), a novel fine-tuning
  framework for enhancing large language models (LLMs) in automated essay scoring
  (AES) tasks. RTS consists of two models: a Ranker and a Scorer.'
---

# Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring

## Quick Facts
- **arXiv ID:** 2504.05736
- **Source URL:** https://arxiv.org/abs/2504.05736
- **Reference count:** 40
- **Primary result:** RTS framework improves QWK by 1.9% on HSK and 1.7–1.1% on ASAP compared to direct prompting

## Executive Summary
This paper introduces Rank-Then-Score (RTS), a novel fine-tuning framework for enhancing large language models (LLMs) in automated essay scoring (AES) tasks. RTS consists of two models: a Ranker and a Scorer. The Ranker uses pairwise ranking with feature-enriched data to identify a candidate score set for a target essay, while the Scorer predicts the final score based on the candidate set and essay content. Experiments on Chinese (HSK) and English (ASAP) datasets demonstrate that RTS consistently outperforms the direct prompting method, achieving improvements of 1.9% on HSK and 1.7–1.1% on ASAP in terms of average Quadratic Weighted Kappa (QWK).

## Method Summary
RTS employs a two-stage fine-tuning approach: a Ranker model that performs pairwise ranking with feature-enriched data to narrow the score interval, followed by a Scorer model that predicts the final score from the candidate set. The framework leverages smaller models for ranking and larger models for scoring, using Binary Search Tree traversal with multi-validation to stabilize candidate set generation. Linguistic, structural, and semantic features are concatenated with essay text for the Ranker, while the Scorer conditions on both essay content and the candidate score set.

## Key Results
- RTS consistently outperforms direct prompting method across both HSK (1.9% QWK improvement) and ASAP (1.7–1.1% QWK improvement) datasets
- On HSK dataset, RTS surpasses other methods using smaller models, highlighting cross-lingual adaptability
- Feature enrichment improves Ranker accuracy by 3.1% but degrades Scorer performance when added there
- Calibration with 15% accuracy degradation in training data improves Scorer robustness

## Why This Works (Mechanism)

### Mechanism 1
Decomposing essay scoring into ranking-then-scoring subtasks improves LLM performance by aligning with next-token prediction training. The Ranker narrows the score interval via pairwise comparisons, producing a candidate score set. The Scorer then conditions on this reduced search space rather than predicting from the full range. This reduces cognitive load and leverages LLMs' comparative reasoning strength.

### Mechanism 2
Feature enrichment at the Ranker (not Scorer) improves discrimination accuracy. Linguistic, structural, and semantic features (e.g., Type-Token Ratio, syntactic tree height, readability indices) are concatenated with essay text. LibSVM selects top 10 features via F-score ranking. These guide pairwise comparisons without overwhelming the Scorer's generative process.

### Mechanism 3
Binary Search Tree (BST)-style traversal with multi-validation stabilizes candidate set generation. Reference essays are arranged by score. Starting from median, the Ranker compares target essay vs. reference, branching left/right based on result. Each comparison uses 4 validations (order-swapped pairs × 2 reference essays) with voting logic to reduce position bias.

## Foundational Learning

**Concept: Quadratic Weighted Kappa (QWK)**
- Why needed here: Primary evaluation metric; measures agreement between predicted and ground-truth scores while penalizing larger disagreements more heavily
- Quick check question: If a model predicts score 3 when ground truth is 4, does QWK penalize more than predicting 2 when ground truth is 4?

**Concept: Pairwise Ranking Loss vs. Regression Loss**
- Why needed here: RTS uses ranking for the Ranker (classification: which essay is better) and regression for the Scorer (predict numeric score). Understanding this distinction explains why two models are needed
- Quick check question: Why can't a single model optimize both pairwise ranking and pointwise scoring simultaneously without performance tradeoffs?

**Concept: Position Bias in LLM Comparisons**
- Why needed here: Multi-validation (swapping essay order in prompt) mitigates LLM tendency to favor first-presented option
- Quick check question: If C(e1, e2) = 1 but C(e2, e1) also = 1, what does this indicate about the model's comparison behavior?

## Architecture Onboarding

**Component map:**
Feature Extractor -> Ranker (smaller LLM) -> BST traversal with multi-validation -> Candidate score set -> Scorer (larger LLM) -> Final score

**Critical path:**
1. Extract features → concatenate with essay text
2. Ranker performs BST traversal → outputs candidate score set (interval or discrete scores)
3. Scorer receives (essay, candidate set) → predicts final score
4. Training: Ranker learns pairwise classification; Scorer learns conditioned regression with calibrated candidate accuracy

**Design tradeoffs:**
- Two-model overhead: Inference requires 2 forward passes; smaller Ranker (1.5B) mitigates cost
- Feature placement: Features help Ranker (+3.1% accuracy) but hurt Scorer when added there (ceiling drops)
- Reference essay selection: Random selection works, but quality variance may cause instability (unexplored)
- Candidate set calibration: 100% accuracy in training causes over-reliance; 15% noise improves robustness

**Failure signatures:**
- Prompt-specific feature mismatch: Prompt 2 HSK showed 6.4% accuracy drop with features—some prompts may not benefit from selected features
- Degraded candidate set: If Ranker accuracy <80%, Scorer conditioning may underperform vanilla
- Score range edge cases: BST may misroute when target score falls between adjacent reference scores (mitigated by additional leaf nodes)

**First 3 experiments:**
1. **Baseline replication:** Implement Vanilla fine-tuning on your target dataset to establish QWK baseline; confirm gap vs. RTS
2. **Feature ablation by prompt:** Rank features per prompt using F-score; test whether top-10 features improve Ranker accuracy consistently or if some prompts require custom feature sets
3. **Candidate accuracy sweep:** Vary calibration level (0%, 10%, 15%, 20%, 25%) for Scorer training data; plot QWK vs. calibration to identify optimal noise level for your dataset

## Open Questions the Paper Calls Out

**Open Question 1**
How does the strategy for selecting reference essays impact the robustness and accuracy of the Rank-Then-Score framework? The authors state that while random selection achieves satisfactory results, exploring different selection methods is necessary. A comparative analysis of RTS performance using clustering or stratified sampling versus random baseline would resolve this.

**Open Question 2**
Can the Rank-Then-Score pipeline maintain its performance improvements when applied to significantly smaller model architectures (e.g., sub-1B parameters)? The paper suggests room for optimization in model size but doesn't test sub-1B parameter models. Experiments applying RTS to smaller model families would measure the performance gap against larger implementations.

**Open Question 3**
Why does the inclusion of hand-crafted features degrade the Ranker's accuracy on specific prompts, such as HSK Prompt 2? The paper observes a significant accuracy drop (87.1% to 80.7%) on Prompt 2 but doesn't analyze which specific feature types conflict with certain essay topics. A fine-grained feature importance analysis per prompt would identify problematic features.

## Limitations
- Fine-tuning hyperparameters (learning rate, batch size, epochs) are not specified, creating reproducibility challenges
- The exact mechanism for applying 15% accuracy reduction to Scorer training data remains ambiguous
- Reference essay selection methodology could impact BST traversal stability, though quality variance effects are unexplored

## Confidence

**High Confidence:** The two-model architecture and core evaluation methodology (QWK on HSK/ASAP) are clearly specified

**Medium Confidence:** The feature enrichment mechanism and BST traversal approach are well-documented, but their implementation details require interpretation

**Low Confidence:** The calibration procedure for reducing candidate set accuracy and the handling of BST edge cases need clarification

## Next Checks
1. Implement a controlled feature ablation study to determine whether the top-10 feature selection consistently improves Ranker performance across all prompts, or if prompt-specific feature selection is required
2. Conduct a systematic sweep of candidate set accuracy calibration levels (0%, 10%, 15%, 20%, 25%) during Scorer training to empirically identify the optimal noise level for your specific dataset
3. Test Ranker performance with different reference essay selection strategies (random vs. quality-filtered) to quantify the impact of reference quality variance on BST traversal stability