---
ver: rpa2
title: MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image
  Retrieval
arxiv_id: '2505.19707'
source_url: https://arxiv.org/abs/2505.19707
tags:
- image
- retrieval
- text
- target
- composed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MLLM-Guided VLM Fine-Tuning with Joint Inference
  (MVFT-JI), a novel approach for zero-shot composed image retrieval (ZS-CIR). Unlike
  prior methods that rely on adapters generating pseudo-text tokens, MVFT-JI leverages
  a multimodal large language model (MLLM) to synthesize training data from unlabeled
  images for two complementary tasks: target text retrieval and text-to-image retrieval.'
---

# MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image Retrieval

## Quick Facts
- arXiv ID: 2505.19707
- Source URL: https://arxiv.org/abs/2505.19707
- Authors: Rong-Cheng Tu; Zhao Jin; Jingyi Liao; Xiao Luo; Yingjie Wang; Li Shen; Dacheng Tao
- Reference count: 40
- Key outcome: Achieves up to 11% absolute gains in R@1 on CIRR dataset compared to state-of-the-art ZS-CIR baselines

## Executive Summary
This paper introduces MVFT-JI, a novel approach for zero-shot composed image retrieval (ZS-CIR) that leverages a multimodal large language model (MLLM) to synthesize training data from unlabeled images. Unlike prior methods using adapters to generate pseudo-text tokens, MVFT-JI fine-tunes a vision-language model (VLM) on two complementary tasks: target text retrieval and text-to-image retrieval. During inference, it combines the VLM's compositional alignment with the MLLM's reasoning capabilities. Extensive experiments on FashionIQ, CIRCO, and CIRR benchmarks demonstrate significant performance improvements over state-of-the-art baselines.

## Method Summary
MVFT-JI uses MiniCPM-V-2_6 MLLM to generate synthetic training triplets (reference image, modification text, target text) and caption pairs from 10K unlabeled ImageNet images. A BLIP2 VLM (ViT-L/14 with Q-Former) is fine-tuned on these data using InfoNCE losses for both tasks. The Q-Former extracts aligned features while the image encoder remains frozen. At inference, retrieval scores combine VLM-computed similarity between composed queries and target images with MLLM-generated target text similarity, using a weighted average. Training uses AdamW with learning rate 1e-5 and decay every 10 epochs.

## Key Results
- Achieves up to 11% absolute gains in R@1 on CIRR dataset
- Outperforms state-of-the-art ZS-CIR baselines across all three benchmarks
- Ablation study confirms importance of both VLM and MLLM components
- Demonstrates effectiveness of joint optimization of complementary retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Task Decomposition
The method decomposes composed retrieval into two sub-problems: retrieving target text given query ($P(t_i|x_i, m_i)$) and retrieving image given text ($P(x_t|t_i)$). By maximizing these conditional probabilities via contrastive learning, the VLM inherently learns composition logic without explicit triplet supervision. This works if a latent textual description captures the semantic intent and the target image is conditionally independent given this text.

### Mechanism 2: Semantic Transference via Synthetic Supervision
The MLLM generates modification and target texts for unlabeled images, creating synthetic supervision. The VLM learns to align fused (image + modification) representations with generated target text representations, forcing it to resolve compositional semantics in latent space. This transfers MLLM reasoning capabilities into the VLM's alignment space, assuming the generated descriptions are accurate proxies for target semantics.

### Mechanism 3: Inference-Time Reasoning Fusion
The method combines direct VLM similarity scores with MLLM-generated target text similarity at inference time using a weighted average. This dual-view verification balances the VLM's learned structural alignment with the MLLM's high-level semantic reasoning. The approach assumes MLLM reasoning complements VLM alignment without systematic conflict, as validated by ablation studies.

## Foundational Learning

- **Concept: Composed Image Retrieval (CIR)** - Core task where query is multimodal (reference image + modifying text). The goal is finding a modified image, not the original. Quick check: If given red car image + "make it blue," should retrieve blue cars, not red cars or "blue car" text.

- **Concept: Q-Former / BLIP-2 Architecture** - Q-Former bridges frozen image encoder and text model using learned query tokens to extract aligned features. Quick check: During fine-tuning, are image encoder weights updated or only Q-Former? (Answer: Image encoder frozen, only Q-Former/Query tokens learned).

- **Concept: InfoNCE / Contrastive Loss** - Training objectives defined as InfoNCE losses that maximize similarity of positive pairs while minimizing similarity with negative pairs. Quick check: If positive pair similarity is low but softmax denominator is high, what happens to loss? (Answer: Loss will be high, driving optimizer to increase positive pair similarity relative to negatives).

## Architecture Onboarding

- **Component map:** Data Engine (MLLM + Unlabeled Images) → Triplets & Pairs → VLM (Frozen Image Encoder + Q-Former + Text Encoder) → Inference Runtime (User Query → VLM Branch + MLLM Branch → Score Fusion)

- **Critical path:** 
  1. Prompt Engineering (High Risk): MLLM prompt quality dictates synthetic data quality
  2. Q-Former Optimization: Central learning step to fuse visual and textual modalities
  3. Inference Fusion: Weighted average of similarities from composed query and MLLM-generated target text

- **Design tradeoffs:**
  - Fine-tunes VLM (gains accuracy) vs. training-free baselines (no training phase)
  - MLLM dependency adds computational overhead and latency
  - Prompt sensitivity couples system to MLLM's prompt adherence

- **Failure signatures:**
  - Hallucination Drift: MLLM describes impossible-to-render objects, VLM learns to non-existent semantic clusters
  - Modality Gap: VLM fails to fuse image and text, simply returns reference image

- **First 3 experiments:**
  1. Verify Data Synthesis: Run MLLM on 10 samples, manually check if Target Text logically follows Modification Text applied to Reference Image
  2. Overfit Single Batch: Train VLM on single batch, verify Lt and Lc drop near zero
  3. Ablation Reproduction: Implement inference pipeline, reproduce w/o MLLM similarity vs. full fusion comparison

## Open Questions the Paper Calls Out
None

## Limitations
- MLLM-generated synthetic data quality depends on prompt engineering and model capabilities
- Computational overhead from MLLM inference at both training and test time
- Limited evaluation to three specific benchmark datasets, generalization to other domains unclear

## Confidence

**High Confidence:**
- Theoretical decomposition of composed retrieval problem is mathematically sound
- Inference-time fusion strategy provides measurable performance gains
- Overall framework architecture is coherent and implementable

**Medium Confidence:**
- MLLM-generated synthetic data adequately captures semantic space for CIR learning
- Performance improvements depend on dataset characteristics and prompt quality
- Scalability to different domains beyond evaluated benchmarks

**Low Confidence:**
- Long-term stability and generalization to unseen data distributions
- Robustness to MLLM model version or prompt engineering variations
- Computational efficiency claims regarding MLLM inference overhead

## Next Checks

1. **Synthetic Data Quality Validation**: Generate and manually inspect 50-100 synthetic triplets from MLLM using provided prompts. Verify target text logically follows from modification text applied to reference image.

2. **Component Ablation Analysis**: Systematically remove each major component (MLLM data generation, VLM fine-tuning, inference-time fusion) and measure performance degradation on FashionIQ.

3. **Cross-Dataset Generalization Test**: Train model on FashionIQ data and evaluate on CIRCO and CIRR without fine-tuning to test true zero-shot capabilities.