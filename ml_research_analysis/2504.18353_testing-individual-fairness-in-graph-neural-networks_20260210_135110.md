---
ver: rpa2
title: Testing Individual Fairness in Graph Neural Networks
arxiv_id: '2504.18353'
source_url: https://arxiv.org/abs/2504.18353
tags:
- fairness
- testing
- individual
- gnns
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This PhD project develops a testing framework to assess and ensure
  individual fairness in Graph Neural Networks (GNNs), addressing the challenge that
  biases in graph-structured data can propagate through GNNs' message-passing mechanisms,
  leading to discriminatory outcomes. The research involves a systematic literature
  review to create a taxonomy of individual fairness approaches, followed by adapting
  existing fairness testing and mitigation techniques specifically for GNNs by generating
  natural individual discriminatory instances while preserving graph topology.
---

# Testing Individual Fairness in Graph Neural Networks

## Quick Facts
- arXiv ID: 2504.18353
- Source URL: https://arxiv.org/abs/2504.18353
- Reference count: 40
- Primary result: Develops framework to test and ensure individual fairness in Graph Neural Networks (GNNs) through structure-preserving perturbations and novel adequacy metrics

## Executive Summary
This PhD project addresses the critical challenge of ensuring individual fairness in Graph Neural Networks (GNNs) by developing a systematic testing framework. The research recognizes that biases in graph-structured data propagate through GNNs' message-passing mechanisms, leading to discriminatory outcomes that standard fairness testing methods fail to detect. By adapting existing fairness testing and mitigation techniques specifically for GNNs, the framework generates natural individual discriminatory instances while preserving graph topology, enabling the detection and correction of structural biases.

The project involves three key phases: conducting a systematic literature review to create a taxonomy of individual fairness approaches, adapting these techniques for GNN-specific challenges, and evaluating the framework through industrial case studies with Deloitte's Trustworthy AI team. The framework bridges the gap between individual fairness testing and the non-IID nature of graph data by extending IID-based methods to handle structural dependencies through topology-preserving perturbations and novel test adequacy criteria.

## Method Summary
The framework generates Individual Discriminatory Instances (IDIs) through structure-preserving perturbations that maintain node degrees and neighborhood distributions while modifying sensitive attributes. This involves adapting gradient-guided adversarial sampling or GANs to graph data by adding regularization terms that constrain perturbations to preserve topological consistency. The method trains a GNN on datasets like German Credit (converted to graphs) using standard architectures (GCN/GAT), then systematically generates test cases by optimizing perturbations under topological constraints. Success is measured by the number of detected discriminatory instances and a proposed "fairness neuron coverage" metric that tracks activation of fairness-relevant neurons in GNN layers.

## Key Results
- Demonstrates standard IID fairness testing fails to detect structural bias in GNNs
- Proposes structure-preserving perturbations as solution for generating natural IDIs in graph data
- Develops novel test adequacy criterion extending fairness neuron coverage to graph models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Graph topology exhibits homophily regarding sensitive attributes, causing GNN message-passing to propagate and amplify localized biases
- **Mechanism:** GNNs update target node representations by aggregating neighbor features; if neighbors share sensitive attributes (clustering), aggregation embeds structural correlation into latent representations
- **Core assumption:** Biases are structurally encoded in graph connections (non-IID data), not just in node features
- **Evidence anchors:** Abstract states biases "propagate through GNNs' message-passing mechanisms"; section 1.2 confirms nodes with similar sensitive attributes cluster, reinforcing disparities

### Mechanism 2
- **Claim:** Fairness testing for GNNs requires generating IDIs via structure-preserving perturbations rather than independent feature mutations
- **Mechanism:** Standard IID testing perturbs features in isolation; GNN testing must maintain relational context by perturbing sensitive attributes while preserving topological constraints like node degree and neighborhood distribution
- **Core assumption:** Valid "natural" test instance must adhere strictly to original graph's degree distribution and neighborhood semantics
- **Evidence anchors:** Section 2 states generating IDIs for GNNs requires preserving both node features and structural dependencies induced by graph topology

### Mechanism 3
- **Claim:** Fairness testing completeness in GNNs relies on "fairness neuron coverage" metric adapted for graph layers
- **Mechanism:** Deep network testing uses coverage to estimate thoroughness; tracking activation of fairness-relevant neurons quantifies how well test cases explore model's "bias potential"
- **Core assumption:** Specific neurons in GNN layers are causally responsible for fairness violations, and activating them correlates with effective testing
- **Evidence anchors:** Section 2 proposes GNN-specific adequacy metric extending fairness neuron coverage to measure test case activation of fairness-relevant components

## Foundational Learning

- **Concept: Non-IID vs. IID Data Distribution**
  - **Why needed here:** Framework's central thesis is standard fairness testing fails because it assumes data points are independent; in graphs, a person is defined by their connections
  - **Quick check question:** If you change one node's feature in a graph, does it affect prediction of its neighbors? (Answer: Yes, in GNNs)

- **Concept: Message Passing (Aggregation)**
  - **Why needed here:** This is vector for bias propagation; understanding GNN layer functions by averaging/summing neighbor information explains why "topology preservation" is critical for testing
  - **Quick check question:** How does GNN update node's representation at layer L+1? (Answer: By aggregating its own features at L with neighbors' features at L)

- **Concept: Individual Fairness (Similarity Principle)**
  - **Why needed here:** Specific definition of fairness being tested ("similar individuals, similar outcomes"), distinct from group fairness (demographic parity)
  - **Quick check question:** Does individual fairness require equal outcomes for groups, or consistent outcomes for similar cases? (Answer: Consistent outcomes for similar cases)

## Architecture Onboarding

- **Component map:** Input: Graph Data G=(V, E) + Sensitive Attributes -> Taxonomy Module: Rules defining similarity and discrimination -> IDI Generator: Adapts techniques to output structure-preserving perturbations -> Test Oracle: Determines if prediction changes -> Adequacy Evaluator: Calculates "Fairness Neuron Coverage" -> Mitigation Loop: Retrains model using generated IDIs

- **Critical path:** IDI Generator; if generated test cases fail to preserve graph topology (structure-preserving perturbation), test inputs are invalid (unnatural), rendering fairness evaluation meaningless

- **Design tradeoffs:**
  - Naturalness vs. Severity: Highly perturbed inputs may find more bugs but risk being "unnatural" (out of distribution)
  - Topology vs. Feature Bias: Framework must disentangle if prediction changed due to node's new feature or implicit change in structural context

- **Failure signatures:**
  - Low Coverage, High Disparity: Adequacy metric (neuron coverage) is low, indicating test suite insufficient
  - Unnatural IDIs: Generated test cases violate semantic constraints (e.g., structurally impossible profile)
  - Performance Collapse: Mitigation via retraining significantly drops prediction accuracy

- **First 3 experiments:**
  1. Baseline IID Failure: Apply standard tabular fairness testing (ignoring edges) to GNN to demonstrate it misses structural bias
  2. Topology Constraint Validation: Generate IDIs with and without "structure-preserving" constraint; measure "naturalness" score (distribution distance) of resulting nodes
  3. Adequacy Correlation: Test if higher "fairness neuron coverage" correlates with reduction in measured individual discrimination on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can structure-preserving perturbations generate natural individual discriminatory instances in non-IID graph data?
- Basis in paper: [explicit] Authors ask (SQ2) how to adapt techniques to "generate natural individual discriminatory instances" while handling "non-IID nature of graph data" through "structure-preserving perturbations"
- Why unresolved: Existing IID methods treat instances independently, failing to account for message-passing mechanisms and neighborhood dependencies in GNNs
- What evidence would resolve it: Methodology that perturbs sensitive attributes without disrupting node degree or neighborhood distributions, validated by statistical similarity to original graph

### Open Question 2
- Question: What test adequacy criteria are required to ensure complete fairness evaluation in Graph Neural Networks?
- Basis in paper: [explicit] Phase 2 states need for "novel test adequacy criterion" based on extending "fairness neuron coverage" to graph-based models to ensure tests are "complete and representative"
- Why unresolved: Current adequacy metrics designed for tabular or image data don't map to structural components (edges, layers) specific to GNNs
- What evidence would resolve it: New metric (e.g., layer-wise fairness neuron coverage) that correlates high coverage with higher detection rates of discriminatory instances

### Open Question 3
- Question: Can fairness testing framework for GNNs effectively detect bias in industrial Graph-based Large Language Models?
- Basis in paper: [explicit] Paper asks (RQ2) "To what extent can proposed framework ensure fairness in industrial applications," specifically targeting graph-based LLMs in collaboration with Deloitte
- Why unresolved: Framework is theoretical/in-progress, and complexity of hallucinations combined with structural bias in industrial LLMs presents unique challenge
- What evidence would resolve it: Successful detection and mitigation of bias in Deloitte's enterprise knowledge graphs without degrading LLM's reasoning or query-answering performance

## Limitations
- Framework's effectiveness contingent on ability to generate truly "natural" test instances while preserving graph topology, which remains open research challenge
- Proposed "fairness neuron coverage" metric lacks validation from existing literature and may not adequately capture distributed fairness properties in GNNs
- Reliance on homophily for bias propagation may not hold in heterophilic graphs, limiting framework's applicability

## Confidence
- High confidence: Problem statement regarding structural bias propagation in GNNs is well-established in literature; need for topology-preserving perturbations is theoretically sound
- Medium confidence: Adaptation of fairness testing techniques to GNNs through structure-preserving perturbations is methodologically plausible but lacks detailed implementation specifications
- Low confidence: Proposed "fairness neuron coverage" metric as test adequacy criterion has no direct supporting evidence in corpus and may not effectively measure fairness testing completeness

## Next Checks
1. **Structural Dependency Validation:** Measure prediction change when perturbing node's sensitive attribute versus perturbing neighbors' sensitive attributes to quantify structural amplification effect
2. **Naturalness Score Correlation:** Test whether "naturalness" of generated IDIs (measured by discriminator accuracy) correlates with ability to reveal actual model biases in downstream fairness metrics
3. **Coverage-Adequacy Experiment:** Systematically vary "fairness neuron coverage" threshold during testing and measure impact on number of detected discriminatory instances and false positive rates