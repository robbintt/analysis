---
ver: rpa2
title: 'FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using
  Information Gain'
arxiv_id: '2505.14826'
source_url: https://arxiv.org/abs/2505.14826
tags:
- sentences
- fishersft
- information
- fine-tuning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FisherSFT, a method to improve the statistical
  efficiency of supervised fine-tuning (SFT) for large language models (LLMs) by selecting
  an informative subset of training examples. The core idea is to select sentences
  that maximize information gain, measured by the Hessian of the log-likelihood of
  the LLM, approximated efficiently using linearized multinomial logistic regression
  models.
---

# FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain

## Quick Facts
- arXiv ID: 2505.14826
- Source URL: https://arxiv.org/abs/2505.14826
- Reference count: 40
- Primary result: FisherSFT selects informative subsets for SFT, outperforming uniform sampling on synthetic and Shakespeare datasets.

## Executive Summary
FisherSFT is a method to improve the statistical efficiency of supervised fine-tuning (SFT) for large language models (LLMs) by selecting a subset of training examples that maximize information gain. It uses a Hessian-based information gain measure, approximated via linearized multinomial logistic regression models, to greedily select sentences that maximize a lower bound on the log determinant of the design matrix. The method aims to reduce prediction error while maintaining computational efficiency, achieving theoretical guarantees of O(dL/√n) error rate with n selected sentences.

## Method Summary
FisherSFT selects a subset of training examples for SFT by maximizing information gain, measured through the Hessian of the log-likelihood of the LLM. To make this computationally tractable, the method approximates the LLM's behavior using linearized multinomial logistic regression models and greedily selects sentences to maximize a lower bound on the log determinant of the design matrix. This approach ensures both computational efficiency and theoretical analyzability, with prediction error decreasing at rate O(dL/√n) where n is the number of chosen sentences.

## Key Results
- FisherSFT outperforms uniform sampling and density-based methods on synthetic data and the Shakespeare dataset.
- Achieves lower prediction errors and generates more coherent text during GPT-2 fine-tuning on Shakespeare.
- Demonstrates improved statistical efficiency in data selection for SFT.

## Why This Works (Mechanism)
FisherSFT works by identifying and selecting the most informative training examples for fine-tuning. The core insight is that not all training data contributes equally to model learning; by focusing on high-information examples, the method achieves better performance with fewer samples. The Hessian-based information gain measure captures how much each example reduces uncertainty in model parameters, while the linearization approximation makes this computation tractable for large LLMs.

## Foundational Learning
- **Hessian of log-likelihood**: Measures curvature of loss surface; needed to quantify information gain per example. Quick check: verify Hessian captures parameter uncertainty reduction.
- **Multinomial logistic regression linearization**: Approximates LLM behavior; needed for tractable Hessian computation. Quick check: compare linearized vs. true LLM gradients on small dataset.
- **Log-determinant maximization**: Ensures diverse, high-information subset selection; needed for theoretical guarantees. Quick check: confirm greedy algorithm increases log-det bound.
- **Greedy subset selection**: Computationally efficient approximation; needed for scalability. Quick check: test if greedy vs. exhaustive selection yields similar results on small data.
- **O(dL/√n) error rate**: Theoretical performance guarantee; needed to bound prediction error. Quick check: verify error scaling empirically as n increases.
- **Design matrix properties**: Underlie information gain computation; needed for subset selection quality. Quick check: inspect condition number of selected subsets.

## Architecture Onboarding
- **Component map**: LLM -> Log-likelihood -> Hessian -> Information Gain -> Subset Selection -> Fine-tuning
- **Critical path**: Data selection (Hessian computation + greedy algorithm) → SFT training
- **Design tradeoffs**: Accuracy vs. computational cost (linearization), subset quality vs. scalability (greedy selection)
- **Failure signatures**: Poor Hessian approximation leading to suboptimal selection, greedy algorithm missing globally optimal subsets
- **First experiments**: 1) Compare FisherSFT-selected vs. random subsets on synthetic data, 2) Benchmark against uniform sampling on Shakespeare, 3) Measure Hessian approximation error vs. true LLM gradients

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on practical approximations (linearization, Hessian estimation) whose validity is uncertain.
- Computational overhead of information gain metrics may be prohibitive for very large datasets or models.
- Limited empirical validation: only tested on synthetic data and one real-world dataset (Shakespeare).
- Does not address scalability to diverse NLP tasks, languages, or comparison with recent fine-tuning methods (e.g., LoRA, prefix tuning).

## Confidence
- Theoretical guarantees hold under idealized assumptions: Medium
- Hessian approximation captures LLM behavior: Medium
- Method generalizes beyond tested datasets/tasks: Low
- Computational efficiency is practical for large-scale use: Low

## Next Checks
1. Validate O(dL/√n) error scaling empirically as n increases on multiple datasets.
2. Test robustness of Hessian-based selection to noise and distribution shifts.
3. Benchmark against recent fine-tuning methods (e.g., LoRA, prefix tuning) on diverse NLP tasks.