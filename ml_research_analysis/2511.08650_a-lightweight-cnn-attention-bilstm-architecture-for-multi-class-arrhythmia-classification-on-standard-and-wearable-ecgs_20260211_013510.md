---
ver: rpa2
title: A Lightweight CNN-Attention-BiLSTM Architecture for Multi-Class Arrhythmia
  Classification on Standard and Wearable ECGs
arxiv_id: '2511.08650'
source_url: https://arxiv.org/abs/2511.08650
tags:
- classification
- attention
- arrhythmia
- lead
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurate and efficient multi-class
  arrhythmia classification from ECG signals using deep learning. A lightweight hybrid
  model combining 1D CNN, attention mechanism, and BiLSTM was developed to extract
  local features, focus on diagnostically relevant segments, and capture long-term
  temporal dependencies.
---

# A Lightweight CNN-Attention-BiLSTM Architecture for Multi-Class Arrhythmia Classification on Standard and Wearable ECGs

## Quick Facts
- arXiv ID: 2511.08650
- Source URL: https://arxiv.org/abs/2511.08650
- Reference count: 18
- Primary result: 0.86 average F1-score on 12-lead ECGs with 0.945M parameters

## Executive Summary
This paper presents a lightweight hybrid deep learning model for multi-class arrhythmia classification from ECG signals. The architecture combines 1D CNN, attention mechanism, and BiLSTM to extract local features, focus on diagnostically relevant segments, and capture long-term temporal dependencies. Evaluated on the CPSC 2018 dataset for both 12-lead and single-lead ECGs, the model achieves strong performance while maintaining a small parameter count suitable for wearable deployment.

## Method Summary
The model processes 60-second ECG segments (15,000 samples) through three stacked CNN blocks that extract hierarchical morphological features. An attention mechanism then selectively weights diagnostically relevant time steps before two BiLSTM layers capture bidirectional temporal dependencies. The architecture outputs multi-class predictions for 9 arrhythmia types. Training uses class-weighted loss to address severe class imbalance, with Adam optimization and early stopping on 80/10/10 splits of the CPSC 2018 dataset.

## Key Results
- Average F1-score of 0.86 on 12-lead ECGs, outperforming ResNet-18 and TI-CNN baselines
- Single-lead (Lead-I) configuration achieves 0.78 F1 while reducing parameters to 0.52M
- Real-time inference capability on Raspberry Pi with 174ms latency and 3.66MB model size
- Ablation study confirms attention and BiLSTM layers significantly improve performance

## Why This Works (Mechanism)

### Mechanism 1: Attention-Guided Temporal Feature Refinement
The attention mechanism improves classification by selectively weighting diagnostically relevant ECG segments. A Conv1D layer with sigmoid activation produces an attention map that is multiplied element-wise with CNN features, adaptively amplifying informative time steps while suppressing noise. This works because arrhythmia-specific patterns often occupy localized temporal regions rather than distributing uniformly across the signal.

### Mechanism 2: Bidirectional Long-Range Temporal Dependency Capture
BiLSTM layers improve rhythm classification by modeling temporal relationships in both forward and backward directions across the ECG sequence. This captures patterns that span multiple cardiac cycles, such as irregular RR intervals, allowing the model to contextualize each time step using both past and future information. This is essential for identifying rhythm-related abnormalities that extend beyond local waveform morphology.

### Mechanism 3: Hierarchical Local Feature Extraction via Stacked CNN Blocks
Sequential convolutional blocks extract progressively abstract morphological features from raw ECG signals. Three blocks, each with two Conv1D layers plus BatchNorm, ReLU, MaxPool, and Dropout, reduce temporal dimensionality while increasing channel depth. This hierarchical structure captures QRS morphology, P-waves, and T-waves at different scales, establishing a strong morphological foundation for classification.

## Foundational Learning

- **Class-weighted loss for imbalanced datasets**: Needed because CPSC 2018 has severe class imbalance (STE: 185 samples vs RBBB: 1675 samples). Quick check: Can you explain why inverse frequency weighting might still fail if minority class samples have high variance?

- **Attention mechanism fundamentals**: Needed to understand how sigmoid-gated element-wise multiplication selectively amplifies features. Quick check: What would happen if the attention Conv1D used ReLU instead of sigmoid activation?

- **Bidirectional RNN semantics**: Needed because BiLSTM doubles representational capacity by processing sequences in both directions but introduces latency during inference. Quick check: Why might BiLSTM be preferable for offline ECG analysis but problematic for real-time streaming?

## Architecture Onboarding

- **Component map**: Input (C×15000) → [CNN Block 1] → [CNN Block 2] → [CNN Block 3] → [Attention: Conv1D(sigmoid) × element-wise] → [BiLSTM 1] → [BiLSTM 2] → [Global Average Pooling] → [Dense] → [Softmax]

- **Critical path**: The attention map generation (Conv1D_att) directly gates information flow to BiLSTM. If this layer fails to produce meaningful weights, BiLSTM receives undifferentiated features.

- **Design tradeoffs**: Accuracy vs. parameters (full model: 0.945M params, 0.86 F1 vs CNN-only: 0.52M params, 0.81 F1); single-lead vs 12-lead (F1 drops from 0.86 to 0.78 but enables wearable deployment); edge deployment (174ms inference latency, 3.66MB model size).

- **Failure signatures**: STE class consistently underperforms (F1=0.58 on 12-lead, 0.27 on single-lead) likely due to low sample count and subtle ST-segment changes; single-lead SNR drops to F1=0.57 (vs 0.809 on 12-lead) suggesting Lead-I may lack spatial information needed for normal rhythm confirmation.

- **First 3 experiments**:
  1. Reproduce ablation: Train CNN-only, CNN+Attention, CNN+BiLSTM, and full model to verify reported F1 progression (0.81 → 0.84 → 0.85 → 0.86).
  2. Class-wise error analysis: Generate confusion matrices for STE and SNR on single-lead to identify systematic misclassifications.
  3. Latency profiling: Measure inference time per component (CNN, Attention, BiLSTM) on target edge hardware to identify bottleneck.

## Open Questions the Paper Calls Out
- **Generalization to external datasets**: How effectively does the model generalize to external ECG datasets with different sampling rates or demographic profiles? The study relies exclusively on CPSC 2018 without testing domain adaptation on other benchmarks like MIT-BIH or PTB-XL.

- **Real-time streaming adaptation**: Can the architecture be adapted to process real-time streaming data without fixed 60-second segmentation? The current methodology forces inputs into a static 60-second window, introducing latency and misalignment with continuous monitoring requirements.

- **Model optimization for microcontrollers**: To what extent can model optimization techniques (e.g., quantization) reduce the memory footprint for deployment on microcontrollers smaller than a Raspberry Pi? While Raspberry Pi deployment is successful, the paper doesn't explore if the 3.66MB model fits stringent constraints of specialized wearable microcontrollers.

- **Improving STE classification in single-lead**: Can STE classification be improved in single-lead configurations without compromising lightweight nature? Single-lead F1 drops from 0.58 to 0.27, indicating a specific failure mode for wearable applications where STE detection is critical.

## Limitations
- Hyperparameter opacity: Critical CNN and BiLSTM parameters are not fully specified, making exact reproduction challenging without codebase access.
- Class imbalance mitigation: The exact weighting formula and additional sampling strategies remain unclear, particularly affecting STE performance (F1=0.27 on single-lead).
- Edge deployment validation: Single-lead results on wearable devices lack real-world validation data; reported inference latency assumes optimal conditions.

## Confidence
- **High confidence**: CNN-BiLSTM architecture design, attention mechanism implementation, and general F1-score ordering (CNN < CNN+Attention < CNN+BiLSTM < Full Model).
- **Medium confidence**: Exact parameter counts (0.945M total), specific F1 values per class, and TFLite deployment metrics, as these depend on unspecified hyperparameters.
- **Low confidence**: STE classification performance due to extreme class imbalance (185 samples) and lack of ablation on minority class handling strategies.

## Next Checks
1. Reproduce ablation study: Train all four model variants to verify the reported F1 progression and parameter efficiency.
2. STE-specific analysis: Conduct targeted experiments on STE classification using oversampling or data augmentation to assess whether poor performance stems from architecture limitations or data scarcity.
3. Real-time streaming test: Implement latency profiling on actual wearable hardware with variable ECG sampling rates to validate the 174ms inference claim under realistic conditions.