---
ver: rpa2
title: 'The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent
  Space Transitions in Multilingual LLMs'
arxiv_id: '2509.17030'
source_url: https://arxiv.org/abs/2509.17030
tags:
- layer
- neurons
- principal
- component
- semantics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the lack of understanding of how multilingual
  large language models (LLMs) perform semantic processing across languages. It proposes
  the Transfer Neurons Hypothesis, which posits that specific neurons in the MLP module
  are responsible for transferring representations between language-specific latent
  spaces and a shared semantic latent space.
---

# The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs

## Quick Facts
- arXiv ID: 2509.17030
- Source URL: https://arxiv.org/abs/2509.17030
- Authors: Hinata Tezuka; Naoya Inoue
- Reference count: 40
- Primary result: Identifies specific MLP neurons that transfer representations between language-specific and shared semantic latent spaces in multilingual LLMs

## Executive Summary
This paper proposes the Transfer Neurons Hypothesis, which identifies specific neurons in the MLP module responsible for transferring representations between language-specific latent spaces and a shared semantic latent space in multilingual LLMs. Through systematic experiments, the authors demonstrate that deactivating these neurons significantly degrades multilingual reasoning performance, validating their critical role. The study reveals a functional stratification where early-layer neurons (Type-1) align inputs to shared space while late-layer neurons (Type-2) diverge them back to output languages.

## Method Summary
The method involves identifying transfer neurons by computing their contribution to moving hidden states toward target centroids in the latent space. For each language pair, centroids are computed for shared (English-centric) and language-specific spaces per layer. Neurons are scored based on how much their output vectors reduce distance to these centroids. The top-1k neurons per type (Type-1: layers 1-20, Type-2: layers 21-32) are then deactivated to validate their importance through performance degradation on multilingual QA tasks. The approach uses PCA visualization, Mutual k-NN alignment, and centroid-based scoring to systematically identify and validate transfer neurons.

## Key Results
- Deactivating top transfer neurons reduces F1 scores by up to 0.62 on multilingual knowledge QA tasks
- Type-2 neurons (shared to language-specific) are more language-specific than Type-1 neurons (input to shared)
- Transfer neurons show distinct layer distributions with Type-1 in early/middle layers and Type-2 in final layers
- Random neuron deactivation shows minimal performance impact compared to transfer neuron ablation

## Why This Works (Mechanism)

### Mechanism 1: Vector-Space Translation via MLP Residual Injection
- Claim: The model shifts internal representations between language-specific and shared semantic spaces by adding specific vector offsets to the residual stream.
- Mechanism: The authors leverage the "neuronic view" of MLPs, where the module acts as a key-value store. They hypothesize that specific neurons, when activated, emit value vectors ($v_i$) that displace the hidden state ($h$) toward the centroid of a target latent space (e.g., shifting a Japanese input toward the English-centric shared space).
- Core assumption: The transition between latent spaces is primarily driven by linear vector additions from MLP outputs, and distinct latent spaces form separable clusters with computable centroids.
- Evidence anchors:
  - [abstract] Proposes that "certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space."
  - [section 4.1] Hypothesizes that "specific activations and their corresponding value vectors... are responsible for shifting internal representations."
  - [corpus] Related work like "Cross-Lingual Generalization and Compression" supports the existence of shared neurons, though the specific "vector injection" mechanism for transfer is unique to this hypothesis.
- Break condition: If the geometry of the latent space is highly non-linear or if the Self-Attention mechanism, rather than the MLP, is the primary driver of these spatial shifts, this vector addition model would fail to capture the full transfer dynamics.

### Mechanism 2: Functional Stratification of Transfer Neurons (Type-1 vs. Type-2)
- Claim: Transfer neurons are functionally specialized by depth: early layers (Type-1) align inputs to a shared space, while late layers (Type-2) diverge them back to output languages.
- Mechanism: The model architecture implicitly divides labor. Type-1 neurons (layers 1-20) are scored based on their ability to move hidden states toward the "shared" (English) centroid. Type-2 neurons (layers 21-32) are scored on moving states toward language-specific centroids. This creates a "convergence-then-divergence" processing flow.
- Core assumption: The "shared semantic latent space" is static and resides predominantly in the middle layers, and language-specific processing occurs only at the fringes.
- Evidence anchors:
  - [section 4.2] Explicitly defines Type-1 (input $\to$ shared) and Type-2 (shared $\to$ output) neurons.
  - [section 5.1] Shows distinct distribution patterns where Type-1 neurons appear in early/middle layers and Type-2 cluster in final layers.
  - [corpus] "Language Arithmetics" also identifies language-specific neurons, aligning with the functional stratification observed here.
- Break condition: If Type-1 neurons were found to be critical for final output generation, or if distinct "shared" and "specific" spaces do not emerge in the middle layers, the stratification hypothesis would weaken.

### Mechanism 3: Centroid-Proximity Scoring for Neuron Identification
- Claim: Transfer neurons can be identified by calculating the extent to which their output vector reduces the distance between a hidden state and a target latent space centroid.
- Mechanism: For a hidden state $h$, the method computes a "neuron score" (Eq. 9). It compares the distance from $h$ to the target centroid against the distance from $h + \text{neuron output}$ to the centroid. Neurons that consistently pull the state closer to the target centroid are ranked as transfer neurons.
- Core assumption: The geometric center (centroid) of a set of hidden states (e.g., all English sentences) accurately represents the "ideal" position for that latent space.
- Evidence anchors:
  - [section 4.3] Details the scoring methodology using centroids ($C^l_{shared}$ and $C^l_{L2}$) and cosine similarity.
  - [section 5.2] Validates the method by showing that deactivating top-scored neurons disrupts representation similarity.
  - [corpus] Evidence for centroid-based control is weak in the provided corpus neighbors, which focus more on neuron existence than the geometric scoring method.
- Break condition: If the "centroid" is a poor proxy for the manifold of a language (e.g., highly multimodal distributions), this scoring method would fail to identify the correct neurons.

## Foundational Learning
- Concept: Residual Stream & MLP "Value" Vectors
  - Why needed here: To understand the claim that neurons "transfer" representations by adding vectors. You must visualize the hidden state not as a static object, but as a point being pushed by MLP outputs.
  - Quick check question: How does the output of an MLP layer ($\alpha v$) modify the hidden state in a Transformer?
- Concept: Latent Space Geometry (PCA & Centroids)
  - Why needed here: The entire identification mechanism relies on representing languages as clusters in a vector space and measuring distances to their centers.
  - Quick check question: If you plotted hidden states for English vs. Japanese, would you expect them to overlap or separate in the early layers according to this paper?
- Concept: Causal Intervention (Ablation)
  - Why needed here: The paper validates its hypothesis not just by observing neurons, but by deactivating them (setting $\alpha=0$) and observing the collapse of reasoning ability.
  - Quick check question: What is the expected outcome on multilingual QA performance if the Type-1 transfer neurons are ablated?

## Architecture Onboarding
- Component map:
  - MLP Layers: Contains the Transfer Neurons (rows in the Down-Projection matrix)
  - Residual Stream: The vector space where the "transfer" occurs via addition
  - Layer Indices: Critical split at Layer 20 (Type-1 vs. Type-2 boundary)
  - Centroids ($C_{shared}$, $C_{L2}$): Reference points used for neuron scoring
- Critical path:
  1. Collect parallel sentences (L1, L2) and extract hidden states for each layer
  2. Compute the "Shared" (English) and "Language-Specific" centroids for each layer
  3. Score neurons by their contribution to reducing distance to these centroids
  4. Deactivate top-k neurons to validate the causal link to performance degradation
- Design tradeoffs:
  - **Centroid Stability vs. Non-linearity**: Using a fixed centroid simplifies scoring but may fail if the manifold is twisted
  - **English as "Shared"**: The paper assumes English space = Shared space. This is efficient but may bias the "shared" representation toward English syntax/semantics
- Failure signatures:
  - **Ineffective Transfer**: Deactivating neurons yields no change in representation similarity (indicating identification failed)
  - **Language Drift**: Deactivating Type-2 neurons causes the model to output a different language (e.g., English) than the input language (e.g., Japanese), confirming the transfer role but breaking the user experience
- First 3 experiments:
  1. **Spatial Visualization**: Reproduce the PCA plots (Fig 2) on a small subset of languages (e.g., English/Japanese) to confirm the convergence/divergence phenomenon in your target model
  2. **Centroid Scoring**: Implement the scoring function (Eq. 9) to identify the top-100 Type-1 neurons for a specific language and verify they reside in the early layers
  3. **Zero-Shot Ablation**: Deactivate the identified top-100 neurons and measure the drop in F1 score on a multilingual QA task to confirm their criticality

## Open Questions the Paper Calls Out
None

## Limitations
- The study only validates findings on decoder-only models and does not test encoder-decoder architectures or larger models where MLP-to-attention ratios may differ
- The assumption of English as the "shared" semantic space introduces potential Western-centrism bias that could affect transfer patterns for non-European languages
- The method assumes latent spaces form compact, spherical clusters - an assumption that may not hold for morphologically rich languages or languages with different syntactic structures

## Confidence
- **High confidence** in experimental methodology for neuron identification and ablation
- **Medium confidence** in geometric scoring method's sensitivity to data quality and representation variance
- **Low confidence** in universal applicability across model families and the causal relationship between identified neurons and transfer mechanism

## Next Checks
1. **Architectural Generalization Test**: Apply the transfer neuron identification method to an encoder-decoder model (e.g., mBERT or mT5) and compare the layer distribution and functional roles of identified neurons against the decoder-only case.

2. **Language Family Diversity Validation**: Repeat the centroid scoring and ablation experiments with language pairs from different families (e.g., Japanese-Korean, Arabic-Hebrew, Swahili-English) to test whether the transfer mechanism generalizes beyond Indo-European languages.

3. **Intervention Granularity Analysis**: Instead of full neuron deactivation, implement partial activation modulation (scaling αᵢ values) to create a fine-grained performance response curve, distinguishing between neurons that are necessary versus those that are merely sufficient for transfer.