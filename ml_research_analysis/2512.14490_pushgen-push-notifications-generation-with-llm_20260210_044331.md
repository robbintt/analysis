---
ver: rpa2
title: 'PushGen: Push Notifications Generation with LLM'
arxiv_id: '2512.14490'
source_url: https://arxiv.org/abs/2512.14490
tags:
- push
- content
- reward
- notifications
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PushGen introduces a two-stage framework for automated push notification
  generation using LLMs. It employs category-controllable fine-tuning to ensure diverse,
  style-consistent content and a pairwise reward model trained on A/B test data to
  select high-CTR notifications.
---

# PushGen: Push Notifications Generation with LLM
arXiv ID: 2512.14490
Source URL: https://arxiv.org/abs/2512.14490
Reference count: 17
Key outcome: +14.08% CTR improvement on machine-generated content, +4.026% in public domain, and +0.067% DAU gain

## Executive Summary
PushGen is a two-stage LLM-based framework for automated push notification generation that achieves significant performance improvements in real-world deployment. The system combines category-controllable fine-tuning to ensure diverse, style-consistent content with a pairwise reward model trained on A/B test data to select high-CTR notifications. Deployed at scale, PushGen achieved 85.2% replacement rate of human-written notifications while delivering substantial CTR improvements across different domains.

## Method Summary
PushGen employs a two-stage framework for push notification generation. First, it uses category-controllable fine-tuning to train LLMs on historical notification data, ensuring generated content matches different notification categories with appropriate style and diversity. Second, it implements a pairwise reward model trained on large-scale A/B test data to select the most effective notifications from multiple candidates. The system was deployed in production for three weeks, during which it generated notifications for over 100 million users and demonstrated significant improvements in click-through rates compared to human-written content.

## Key Results
- +14.08% CTR improvement on machine-generated push notifications
- +4.026% CTR improvement in public domain testing
- +0.067% daily active user (DAU) gain with 85.2% of notifications replaced by model-generated content

## Why This Works (Mechanism)
The two-stage approach addresses key challenges in automated notification generation: content diversity through category-controllable fine-tuning and effectiveness optimization through reward model selection. By training on historical A/B test data, the system learns platform-specific user preferences and optimizes for engagement metrics rather than just relevance.

## Foundational Learning
- **Category-controllable fine-tuning**: Needed to ensure generated content matches different notification categories with appropriate tone and style. Quick check: verify generated content diversity across categories matches human-written benchmarks.
- **Pairwise reward modeling**: Required to select the most effective notifications from multiple candidates based on learned user preferences. Quick check: validate reward model predictions against historical A/B test outcomes.
- **A/B test data utilization**: Essential for training reward models on real-world engagement patterns. Quick check: confirm model performance correlates with historical A/B test results.
- **Large-scale deployment monitoring**: Critical for measuring real-world impact beyond controlled experiments. Quick check: track CTR changes across different user segments over time.

## Architecture Onboarding
**Component map**: User context → Category controller → LLM generator → Multiple candidates → Reward model → Selected notification → Delivery system

**Critical path**: The core workflow flows from user context through category determination to LLM generation, producing multiple notification candidates that are evaluated by the reward model to select the final notification for delivery.

**Design tradeoffs**: The system balances content diversity (through category-specific fine-tuning) against optimization efficiency (through reward model selection), while maintaining fallback options with human-written content to ensure quality control.

**Failure signatures**: Poor CTR performance indicates issues with either the fine-tuning process (inappropriate content style) or reward model training (misaligned optimization objectives). Sudden performance drops may signal data drift or changes in user preferences.

**3 first experiments**:
1. Compare CTR performance across different notification categories to validate fine-tuning effectiveness
2. Test reward model selection accuracy against human judgment on notification pairs
3. Measure content diversity metrics (n-gram overlap, semantic similarity) between generated and human-written notifications

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation including long-term user fatigue from LLM-generated content, generalization across different app categories and user demographics, and the optimal balance between human and machine-generated content.

## Limitations
- Results may not generalize beyond the TikTok ecosystem to different app categories or user demographics
- Three-week deployment period may not capture long-term effects including novelty decay or user fatigue
- Platform-specific reward model may capture preferences that don't transfer to other contexts

## Confidence
- Technical claims: Medium - well-described methodology but incomplete implementation details
- Business impact claims: Medium-High - substantial CTR improvements with statistical significance
- Generalization claims: Low - limited to single platform and short deployment period

## Next Checks
1. Conduct multi-platform deployment studies across different app categories and user demographics to test generalizability of the CTR improvements
2. Implement longer-term monitoring (3-6 months) to assess sustainability of performance gains and detect any negative effects from content repetition or user fatigue
3. Perform ablation studies to isolate the individual contributions of the fine-tuning stage versus the reward model selection stage to the observed performance improvements