---
ver: rpa2
title: 'KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network
  for an Accurate Ice Sheet Emulator'
arxiv_id: '2510.24926'
source_url: https://arxiv.org/abs/2510.24926
tags:
- loss
- feature
- curation
- re-weight
- emulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces KAN-GCN, a hybrid emulator that combines a
  Kolmogorov-Arnold Network (KAN) with graph convolutional networks (GCNs) for fast
  and accurate ice sheet modeling. The KAN acts as a feature-wise calibrator, performing
  per-feature nonlinear encoding via learnable 1D warps, while the GCN handles spatial
  message passing on the mesh.
---

# KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator

## Quick Facts
- arXiv ID: 2510.24926
- Source URL: https://arxiv.org/abs/2510.24926
- Reference count: 24
- Key outcome: KAN-GCN consistently matches or exceeds accuracy of pure GCN and MLP-GCN baselines for ice sheet emulation, with improved inference throughput on coarser meshes by replacing edge-wise message passing with node-wise transforms.

## Executive Summary
This paper introduces KAN-GCN, a hybrid emulator combining Kolmogorov-Arnold Networks (KANs) with Graph Convolutional Networks (GCNs) for fast and accurate ice sheet modeling. The KAN front end applies learnable one-dimensional warps to each input feature independently, improving feature conditioning and nonlinear encoding without increasing message-passing depth. Tested on 36 melting-rate simulations across three mesh sizes for Pine Island Glacier, Antarctica, KAN-GCN demonstrates superior or comparable accuracy to baseline models while offering computational advantages on coarser resolutions. The approach is particularly effective for nonlinear targets like ice velocity, where KAN's adaptive univariate bases capture feature-specific responses more effectively than standard GCNs or MLPs.

## Method Summary
The method employs a KAN encoder as a front-end feature calibrator, followed by a GCN stack for spatial message passing. The KAN layer uses Gaussian RBFs (FastKAN) to apply learnable 1D warps to each input feature independently, transforming the 6 node features (melt rate, SMB, time, Vx, Vy, thickness) into a calibrated representation. This is followed by 2-5 GCN layers with LeakyReLU activation for spatial aggregation, ending with a linear head that predicts changes in velocity and thickness. The model is trained on residual targets (ΔVx, ΔVy, ΔH) using weighted MSE loss, with Adam optimizer and cosine annealing scheduler. Training uses 500 epochs on 25,812 graphs generated from 36 melting-rate simulations across three mesh sizes (2km, 5km, 10km), normalized to [-1,1].

## Key Results
- KAN-GCN consistently matches or exceeds accuracy of pure GCN and MLP-GCN baselines across all mesh sizes
- The hybrid approach is particularly effective for nonlinear velocity predictions, improving feature conditioning before spatial aggregation
- On coarser meshes (10km), KAN-GCN achieves inference speedup by replacing edge-wise message passing with node-wise transforms, though this advantage diminishes on finer meshes

## Why This Works (Mechanism)

### Mechanism 1
Placing a Kolmogorov-Arnold Network (KAN) as a front-end feature calibrator improves accuracy for highly nonlinear targets like ice velocity by isolating feature-wise transformations before spatial aggregation. The KAN layer applies learnable 1D univariate warps to each input feature independently, capturing complex non-linear relationships between physical drivers and the target state without increasing spatial message-passing depth. This mechanism assumes primary nonlinearities in ice sheet dynamics are feature-dependent and can be decoupled from spatial processes handled by GCN.

### Mechanism 2
Replacing a standard GCN layer with a node-wise KAN layer offers a favorable accuracy-vs-efficiency trade-off on coarser meshes. Standard GCNs perform edge-wise message passing with significant overhead on sparse graphs. The KAN layer performs a dense node-wise transform that is computationally cheaper on coarse meshes due to fewer edges, effectively replacing edge-heavy operations with node-heavy ones. This advantage diminishes on very fine meshes where KAN's per-node basis evaluations incur higher costs.

### Mechanism 3
The hybrid architecture relies on a "depth threshold" where KAN is beneficial only if sufficient GCN layers remain for spatial aggregation. The KAN replaces the first GCN layer, and in shallow networks (2 layers), this replacement reduces message-passing depth too aggressively, hurting performance. In deeper networks (3-5 layers), remaining GCN layers provide ample spatial context, allowing KAN's feature calibration to improve results without starving the model of spatial connectivity.

## Foundational Learning

- **Concept: Kolmogorov-Arnold Networks (KANs)**
  - Why needed here: KANs use learnable activation functions (splines/RBFs) on edges instead of fixed activations on nodes, allowing adaptive feature-wise transformations crucial for capturing nonlinear ice physics.
  - Quick check question: How does the FastKAN implementation in this paper approximate the B-spline to improve efficiency?

- **Concept: Graph Convolutional Networks (GCNs) on Unstructured Meshes**
  - Why needed here: GCNs handle spatial physics through spectral approximations that enable convolutions on irregular graphs like ice sheet meshes, with layer depth translating to spatial diffusion.
  - Quick check question: In Eq. 5, what does the matrix $\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$ represent in terms of neighbor information flow?

- **Concept: Residual Learning for Physics Emulation**
  - Why needed here: Reformulating the task to predict Δ (change in velocity/thickness) rather than absolute values aligns with finite-difference solvers and improves conditioning by keeping targets near zero-mean.
  - Quick check question: Why might predicting the change (Δv) be numerically more stable for a neural network than predicting the absolute state (vt) directly?

## Architecture Onboarding

- **Component map:** Input (6 features) -> KAN Layer (FastKAN with Gaussian RBFs) -> GCN Stack (2-5 layers with LeakyReLU) -> Linear Head (3 outputs) -> Post-process (add Δ to previous state)
- **Critical path:** The transition from the KAN feature encoder to the first GCN layer is critical; if KAN output dimension doesn't match GCN hidden dimension or if KAN warps are ill-conditioned, spatial layers will receive garbage signals.
- **Design tradeoffs:**
  - Depth vs. Calibration: Use ≥3 layers for KAN-GCN; shallow networks (2 layers) should stick to pure GCN
  - Mesh Resolution: Choose KAN for coarse surrogate models but benchmark carefully for fine high-fidelity runs
  - B-Spline vs. RBF: Paper uses FastKAN (Gaussian RBFs) for GPU throughput, potentially trading fine-grained curve fidelity
- **Failure signatures:**
  - Oversmoothing: 5-layer KAN may underperform MLP on coarse meshes due to regularization effects
  - Underfitting (2-layer): KAN-GCN underperforms GCN baseline due to loss of second message-passing hop
  - Inference Slowdown: KAN adds latency on extremely dense meshes where edge-savings don't dominate
- **First 3 experiments:**
  1. Baseline Validation: Implement 3-layer Pure GCN and 3-layer KAN-GCN on 10km mesh dataset, verify KAN-GCN achieves lower RMSE on velocity
  2. Ablation on Depth: Run both architectures across 2, 3, 4, and 5 layers, confirm performance dip at 2 layers for KAN-GCN
  3. Inference Timing: Measure wall-clock time for full 20-year transient simulation on coarsest vs. finest mesh to verify "coarse-speedup / fine-overhead" trade-off

## Open Questions the Paper Calls Out

- **Open Question 1:** Can KAN-GCN generalize to other ice sheets like those in Greenland without significant re-tuning of feature-wise calibrations? The study is restricted to Pine Island Glacier, and it's unknown if learned nonlinear encodings transfer to domains with different bedrock topographies or climate forcing regimes.
- **Open Question 2:** What specific regularization techniques can mitigate performance degradation in deep KAN-GCN models on coarse meshes? The paper notes 5-layer KAN underperforms MLP on 10km mesh but doesn't propose methods like dropout or spline penalties to allow higher-capacity KAN to succeed.
- **Open Question 3:** Does KAN-GCN maintain numerical stability and bounded error accumulation over multi-century transient simulations? Training uses 20-year simulations, but sea-level projections require century-scale modeling where error accumulation is critical and high one-step accuracy doesn't guarantee long-term stability.

## Limitations
- Computational speedup on coarse meshes (10km) is offset by overhead on finer meshes (2km), making the efficiency claim mesh-dependent rather than universal
- Absence of explicit hyperparameter details (RBF centers, bandwidth, loss weights) leaves reproducibility uncertain
- Paper lacks direct comparisons to alternative hybrid architectures (e.g., attention-GCN or MLP-GCN), limiting claims of KAN's unique contribution

## Confidence
- **High:** Accuracy improvements on velocity with ≥3-layer KAN-GCN stacks
- **Medium:** Mesh-dependent inference speedup mechanism
- **Low:** Generalization to other physics problems or mesh types without re-tuning

## Next Checks
1. Reproduce the 2-layer underperformance: Train a 2-layer KAN-GCN and GCN baseline on the coarsest mesh, confirm the accuracy drop when KAN replaces the first GCN layer
2. Verify mesh speedup reversal: Benchmark inference time on the finest mesh (2km) to confirm KAN adds latency when edge-savings no longer dominate
3. Test KAN capacity sensitivity: Vary the number of RBF centers in the FastKAN layer (e.g., 8, 16, 32) and measure accuracy trade-offs to determine optimal expressiveness