---
ver: rpa2
title: On Fairness of Unified Multimodal Large Language Model for Image Generation
arxiv_id: '2502.03429'
source_url: https://arxiv.org/abs/2502.03429
tags:
- image
- bias
- generation
- demographic
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines bias in unified multimodal large language models
  (U-MLLMs) for image generation, specifically targeting gender and race disparities.
  The authors benchmark several state-of-the-art U-MLLMs and find significant demographic
  biases, with Janus-Pro exhibiting the worst gender bias (0.90) compared to Stable
  Diffusion (0.67).
---

# On Fairness of Unified Multimodal Large Language Model for Image Generation

## Quick Facts
- **arXiv ID**: 2502.03429
- **Source URL**: https://arxiv.org/abs/2502.03429
- **Reference count**: 35
- **Primary result**: Debiasing U-MLLMs reduces gender bias by 71.91% while maintaining image quality

## Executive Summary
This paper examines bias in unified multimodal large language models (U-MLLMs) for image generation, specifically targeting gender and race disparities. The authors benchmark several state-of-the-art U-MLLMs and find significant demographic biases, with Janus-Pro exhibiting the worst gender bias (0.90) compared to Stable Diffusion (0.67). Through component analysis, they locate the bias primarily in the language model rather than the vision encoder. To address this, they propose a two-stage approach: first finetuning the model on balanced synthetic data generated by a diffusion model, then applying a novel balanced preference loss that minimizes preference differences across demographic groups. Their method reduces gender bias in VILA-U by 71.91% (from 0.89 to 0.25) while improving image quality metrics, and achieves intersectional bias reduction to 0.15.

## Method Summary
The authors develop a two-stage debiasing approach for U-MLLMs. First, they generate balanced synthetic data using a diffusion model to create images representing diverse demographic groups. Second, they apply a balanced preference loss that explicitly minimizes preference differences across demographic groups during finetuning. The method includes a novel Component-wise Group Distribution Alignment (CGDA) technique that focuses on specific bias patterns in generated images. The approach is tested on three U-MLLMs: Janus-Pro, LLaVA-Next, and VILA-U, with comprehensive evaluation across multiple bias metrics including Demographic Pairwise Bias (DPB), individual fairness (IF), and intersectional fairness (IF*).

## Key Results
- Gender bias reduction of 71.91% in VILA-U (from 0.89 to 0.25) while improving image quality metrics
- Intersectional bias reduction to 0.15, demonstrating effectiveness across multiple demographic dimensions
- Language model identified as primary bias source, not vision encoder
- Proposed method outperforms existing debiasing techniques across all benchmarked U-MLLMs

## Why This Works (Mechanism)
The two-stage approach works by first establishing a balanced training distribution through synthetic data generation, then explicitly optimizing for group-level fairness through the balanced preference loss. By targeting the language model component specifically, the method addresses the root cause of bias propagation in U-MLLMs. The balanced preference loss directly minimizes preference differences across demographic groups, ensuring the model doesn't favor any particular group during generation. The component-wise approach allows for precise control over bias patterns while maintaining semantic fidelity of the generated images.

## Foundational Learning

**U-MLLM Architecture**: Unified multimodal models that combine vision and language understanding in a single framework. Needed to understand how biases propagate through the generation pipeline. Quick check: Verify model architecture matches described components.

**Demographic Bias Metrics**: Statistical measures including DPB, IF, and IF* that quantify bias across demographic groups. Needed to evaluate fairness improvements. Quick check: Ensure metrics capture both individual and group-level biases.

**Diffusion Model Generation**: Technique for creating synthetic training data with controlled demographic distributions. Needed to establish balanced training data. Quick check: Verify generated images maintain quality while representing diverse groups.

**Preference Learning**: Optimization approach that learns from human or model preferences rather than explicit labels. Needed for the balanced preference loss. Quick check: Confirm preference differences are properly minimized across groups.

## Architecture Onboarding

**Component Map**: User Prompt -> Language Model -> Vision Encoder -> Image Generation

**Critical Path**: The language model serves as the critical path for bias propagation, as it interprets prompts and influences the generation process before the vision encoder processes visual information.

**Design Tradeoffs**: The approach trades some computational overhead for bias reduction, requiring additional finetuning stages and synthetic data generation. However, this investment yields significant fairness improvements without sacrificing image quality.

**Failure Signatures**: Models may exhibit residual bias in intersectional scenarios or when prompted with ambiguous demographic references. Performance degradation may occur if synthetic data generation doesn't adequately represent real-world distributions.

**First Experiments**:
1. Benchmark existing U-MLLMs on standard bias metrics to establish baseline performance
2. Test component-wise analysis to isolate bias sources between language model and vision encoder
3. Validate synthetic data generation quality and demographic representation before finetuning

## Open Questions the Paper Calls Out
The authors note that their work focuses primarily on overt demographic categories (gender and race) and may miss subtler forms of bias. They also highlight the need for longitudinal studies to evaluate whether debiased models maintain their reduced bias over time and with continued use. The reliance on synthetic data generation raises questions about how well results generalize to real-world applications.

## Limitations
- Reliance on synthetic data generation may not fully capture real-world demographic distributions
- Focus on overt demographic categories may miss subtler forms of bias
- Evaluation metrics, while comprehensive, may not capture all relevant fairness dimensions
- Lack of full open-source access to some target models limits reproducibility

## Confidence

**High confidence** in the experimental methodology and component analysis identifying the language model as the primary source of bias

**Medium confidence** in the effectiveness of the two-stage debiasing approach, as results are based on specific synthetic data and evaluation conditions

**Medium confidence** in the generalizability of findings to other U-MLLMs and real-world applications, given the limited model diversity in the study

## Next Checks
1. Replicate the debiasing experiments using real-world image datasets rather than synthetic data to verify robustness across data types
2. Test the proposed method on additional U-MLLMs not included in the original study to assess generalizability
3. Conduct longitudinal studies to evaluate whether debiased models maintain their reduced bias over time and with continued use