---
ver: rpa2
title: 'MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question
  Answering'
arxiv_id: '2511.16283'
source_url: https://arxiv.org/abs/2511.16283
tags:
- query
- retrieval
- https
- arxiv
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of existing retrieval-augmented
  generation (RAG) systems that are typically single-intent oriented, leading to incomplete
  evidence coverage for complex scientific questions with multiple correlated intents.
  To tackle this, the authors introduce the Multi-Intent Scientific Question Answering
  (MuISQA) benchmark and propose an intent-aware retrieval framework.
---

# MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering

## Quick Facts
- arXiv ID: 2511.16283
- Source URL: https://arxiv.org/abs/2511.16283
- Authors: Zhiyuan Li; Haisheng Yu; Guangchuan Guo; Nan Zhou; Jiajun Zhang
- Reference count: 40
- Primary result: Introduces intent-aware retrieval framework that improves evidence coverage and answer generation for multi-intent scientific questions

## Executive Summary
This paper addresses the limitation of single-intent retrieval-augmented generation systems for complex scientific questions with multiple correlated intents. The authors propose a multi-intent approach that decomposes questions into intent-specific queries using LLM-generated hypotheses, retrieves supporting passages for each intent, and aggregates results via Reciprocal Rank Fusion to improve coverage while reducing redundancy. They introduce the MuISQA benchmark and demonstrate substantial improvements in retrieval accuracy and answer quality compared to conventional approaches.

## Method Summary
The proposed framework tackles multi-intent scientific question answering by first using LLMs to hypothesize potential answers and decompose them into intent-specific queries. For each identified intent, the system retrieves relevant passages from scientific literature. Retrieved fragments are then aggregated and re-ranked using Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while minimizing redundancy. This intent-aware approach contrasts with traditional single-query RAG systems by explicitly handling the complexity of questions that contain multiple, interrelated information needs.

## Key Results
- Significant improvements in retrieval accuracy on MuISQA benchmark compared to single-intent RAG baselines
- Enhanced evidence coverage across diverse intents while reducing redundancy in retrieved passages
- Better answer generation performance for complex scientific questions with multiple correlated intents
- Demonstrated effectiveness on both MuISQA and general RAG datasets

## Why This Works (Mechanism)
The framework works by explicitly addressing the multi-intent nature of complex scientific questions through a decomposition-retrieval-aggregation pipeline. By generating hypothesized answers first, the system can identify distinct information needs within a single question. The subsequent multi-query retrieval ensures each intent receives dedicated evidence coverage, while RRF effectively balances and ranks results from multiple retrieval streams, preventing any single intent from dominating the final answer generation.

## Foundational Learning
- Retrieval-Augmented Generation (RAG): Combines information retrieval with text generation to ground responses in external evidence; needed for accessing up-to-date scientific knowledge; quick check: can the system retrieve relevant passages from a test corpus
- Intent Decomposition: Breaking complex questions into constituent information needs; essential for handling multi-intent queries; quick check: does the decomposition correctly identify all major intents in sample questions
- Reciprocal Rank Fusion (RRF): Combines rankings from multiple retrieval systems; provides balanced aggregation across different intents; quick check: does RRF improve coverage compared to single-query ranking
- LLM Hypothesis Generation: Using large language models to generate potential answers that reveal underlying intents; enables automatic intent discovery; quick check: are generated hypotheses coherent and relevant to the original question

## Architecture Onboarding
Component Map: LLM Hypothesis Generation -> Intent Decomposition -> Multi-Query Retrieval -> RRF Aggregation -> Answer Generation

Critical Path: The core workflow follows hypothesis generation to intent identification, then parallel retrieval for each intent, followed by RRF-based aggregation before final answer synthesis.

Design Tradeoffs: Balances computational overhead of multiple retrieval queries against improved coverage; trades simplicity of single-query approaches for explicit handling of multi-intent complexity; relies on LLM quality for accurate intent identification.

Failure Signatures: Poor intent decomposition leading to incomplete evidence coverage; redundant retrievals overwhelming the aggregation step; RRF imbalance favoring certain intents over others.

Three First Experiments:
1. Compare retrieval accuracy with and without LLM-based hypothesis generation
2. Test RRF effectiveness by varying the number of intents and retrieval streams
3. Evaluate coverage improvement by measuring intent-specific evidence completeness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Scalability concerns for questions with numerous interrelated intents requiring extensive decomposition
- Potential hallucination risks when LLMs generate hypothesized answers that may influence intent identification
- Reliance on human-annotated MuISQA data that may not capture full diversity of real-world scientific queries
- Computational efficiency and runtime overhead not extensively evaluated across different hardware configurations

## Confidence
- Core retrieval accuracy improvements: Medium
- Answer generation quality gains: Medium
- Scalability to extremely complex questions: Low
- Computational efficiency claims: Low

## Next Checks
1. Conduct ablation studies comparing the proposed intent-aware retrieval framework against simpler multi-query RAG baselines that do not use LLM-generated hypotheses
2. Evaluate the system's performance on longer, more complex scientific questions with more than three distinct intents to test scalability limits
3. Measure computational efficiency and runtime overhead compared to standard single-intent RAG approaches across different hardware configurations