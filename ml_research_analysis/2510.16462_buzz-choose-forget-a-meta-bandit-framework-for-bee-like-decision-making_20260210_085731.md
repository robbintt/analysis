---
ver: rpa2
title: 'Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making'
arxiv_id: '2510.16462'
source_url: https://arxiv.org/abs/2510.16462
tags:
- figure
- learning
- dataset
- policy
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MAYA, a sequential imitation learning framework
  for modeling heterogeneous decision-making strategies in honeybees. The key challenge
  is to predict individual bee behavior under varying memory constraints and environmental
  conditions, particularly when expert policies shift over time and are not always
  optimal.
---

# Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making

## Quick Facts
- arXiv ID: 2510.16462
- Source URL: https://arxiv.org/abs/2510.16462
- Reference count: 40
- Bees' heterogeneous decision-making strategies can be modeled using a sliding-window meta-bandit framework with Wasserstein distance

## Executive Summary
MAYA is a sequential imitation learning framework that models heterogeneous decision-making strategies in honeybees by combining multi-armed bandit policies with windowed regret similarity evaluation. The framework addresses the challenge of predicting individual bee behavior under varying memory constraints and environmental conditions, particularly when expert policies shift over time and are not always optimal. Experiments on 80 tracked bees across five datasets demonstrate that a memory window of τ=7 consistently yields the best fit, with MAYA-Wass achieving superior performance over traditional IRL baselines and GLM models.

## Method Summary
MAYA models bee decision policies as a mixture of contextual multi-armed bandit strategies, using a sliding window τ to focus on recent behavior. At each trial, the framework computes similarity between the bee's observed regret and each candidate policy's simulated regret using Wasserstein, KL divergence, or Dynamic Time Warping metrics. The policy with minimum distance is selected, allowing the model to switch between strategies as the bee's behavior evolves. The framework is evaluated on 5 datasets with 80 bees total, spanning France/Australia and cold/moderate/hot weather conditions, using binary choice tasks in Y-maze experiments.

## Key Results
- τ=7 memory window consistently yields best performance across all datasets
- MAYA-Wass (Wasserstein distance) achieves highest performance outperforming IRL baselines
- Low-regret bees align predominantly with LinUCB-like choices; high-regret bees align more with Epsilon greedy
- Generated trajectories preserve behavioral archetypes with ClusterAcc > 85% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Constraining observation history to a sliding window τ improves imitation of agents with limited biological memory. At each trial t ≥ τ, MAYA evaluates regret trajectories only over the most recent τ observations, matching the recency effect observed in bee cognition. Core assumption: Bees make decisions based on a bounded memory horizon; their policy at time t depends primarily on the last τ trials. Break condition: If τ is set too small (< 5), alignment becomes unstable especially for slow learners.

### Mechanism 2
Wasserstein distance outperforms KL divergence and DTW for aligning bee regret trajectories to bandit policies. Wasserstein captures geometric relationships between reward distributions while remaining robust to small probability mass differences. Core assumption: Regret trajectories can be meaningfully compared as distributions over action outcomes rather than pure time series. Break condition: If regret distributions become highly multimodal or sparse, Wasserstein may require additional regularization.

### Mechanism 3
Maintaining an ensemble of diverse MAB policies enables faithful imitation of heterogeneous biological learners. At each time step, MAYA computes similarity between the bee's observed regret and each candidate policy's simulated regret, then selects the policy with minimum distance. This allows the model to switch between strategies as the bee's behavior evolves. Core assumption: Bee behavior can be approximated as a mixture of canonical bandit strategies. Break condition: If the candidate policy set lacks a policy close to the bee's true strategy, alignment degrades uniformly across similarity metrics.

## Foundational Learning

- **Concept: Multi-Armed Bandit Regret**
  - Why needed here: MAYA evaluates bee behavior through cumulative regret, not reward maximization
  - Quick check: If a bee chooses correctly 70% of the time in a task where the optimal policy achieves 95%, what is the instantaneous regret on a wrong choice? (Answer: 1)

- **Concept: Imitation Learning vs. Inverse Reinforcement Learning**
  - Why needed here: The paper explicitly contrasts MAYA with IRL baselines that optimize for the best policy rather than faithful imitation
  - Quick check: Why does Behavioral Cloning fail to capture bee trajectories? (Answer: BC generalizes poorly and doesn't model the sequential decision process)

- **Concept: Trajectory Similarity Measures**
  - Why needed here: MAYA's core operation is computing distance between regret trajectories
  - Quick check: When would DTW be preferable to Wasserstein? (Answer: When temporal alignment matters more than distributional similarity)

## Architecture Onboarding

- **Component map:**
  Input -> Candidate Policies -> Similarity Module -> Output Policy
  Bee regret trajectory and context -> {UCB, EpsilonGreedy, LinUCB, Uniform} -> Wasserstein/KL/DTW -> Learned policy π_θ

- **Critical path:** The similarity computation δ(π_bee, π_i, τ, t) at line 24 of Algorithm 1 determines policy selection. Errors propagate if τ is misconfigured, candidate policies don't span the behavioral space, or similarity metric fails to distinguish similar-but-distinct trajectories.

- **Design tradeoffs:**
  - τ selection: Lower τ (5-7) adapts faster to policy shifts but risks noise; higher τ (10+) is more stable but lags behind strategy changes
  - Similarity metric: Wasserstein is most robust across conditions; KL is faster to compute but sensitive to normalization
  - Candidate policy set: More policies increase coverage but raise computational cost and tie-breaking frequency

- **Failure signatures:**
  - MSE plateaus or increases after τ=7: Indicates τ is too large, model is over-smoothing
  - High variance in alignment proportions across runs: Candidate policies may be too similar
  - ClusterAcc < 70%: Generated trajectories don't preserve behavioral archetypes
  - All MAYA variants converge to same performance: Similarity metrics not distinguishing policies

- **First 3 experiments:**
  1. τ sweep on single dataset: Run MAYA-Wass with τ ∈ {3, 5, 7, 10, 20, T} on Dataset 2, plot MSE/MAE vs τ
  2. Per-bee alignment analysis: For 16 bees in Dataset 1, compute alignment proportions and correlate with cumulative regret
  3. Similarity metric ablation: Run all three MAYA variants on all five datasets with τ=7, compute ClusterAcc using DBA clustering

## Open Questions the Paper Calls Out

### Open Question 1
How effective is MAYA when deployed in large-scale ecological simulations for environmental governance? The authors state future work will deploy MAYA in large-scale ecological simulations to assess its predictive value for ecological management decisions. Current study validates on individual bee trajectories in controlled Y-maze experiments.

### Open Question 2
Can the framework be generalized to non-binary decision-making scenarios or continuous action spaces? The methodology is strictly limited to contextual binary foraging tasks (Y-maze, Left/Right choices), while natural pollinator behavior often involves multi-alternative or continuous spatial navigation.

### Open Question 3
Does the optimal memory window (τ) shift significantly under environmental stressors other than temperature? The paper notes weather influences optimal τ, with cold weather requiring shorter windows, suggesting the memory parameter is sensitive to environmental conditions.

## Limitations
- Generalizability to natural foraging contexts remains untested beyond controlled laboratory experiments
- Biological validity of the memory window assumption (τ=7) relies on experimental observation rather than direct cognitive measurement
- Candidate policy set may not fully span the behavioral strategy space of bees in complex environments

## Confidence
- **High Confidence (95%+):** MAYA's superior performance over IRL baselines and GLM models in controlled experimental settings
- **Medium Confidence (80-95%):** Wasserstein distance as optimal similarity metric; behavioral interpretations linking policy alignment to bee regret levels
- **Low Confidence (60-80%):** Biological validity of memory window assumption; generalizability to natural foraging environments

## Next Checks
1. Test MAYA on bee foraging data from natural environments with varying floral distributions to assess whether τ=7 remains optimal
2. Systematically vary τ from 3-20 trials and measure performance degradation curves to quantify sensitivity to memory horizon assumptions
3. Incorporate additional candidate policies (e.g., Thompson sampling) to test whether MAYA's alignment patterns change or improve