---
ver: rpa2
title: Does It Make Sense to Speak of Introspection in Large Language Models?
arxiv_id: '2506.05068'
source_url: https://arxiv.org/abs/2506.05068
tags:
- temperature
- introspection
- llms
- process
- consciousness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) can\
  \ be said to possess introspection\u2014a capacity traditionally associated with\
  \ consciousness in humans. The authors define a lightweight conception of introspection\
  \ as an LLM\u2019s accurate self-report of its internal states or mechanisms, produced\
  \ via a causal link between those states and the report."
---

# Does It Make Sense to Speak of Introspection in Large Language Models?

## Quick Facts
- arXiv ID: 2506.05068
- Source URL: https://arxiv.org/abs/2506.05068
- Authors: Iulia M. Comsa; Murray Shanahan
- Reference count: 23
- Key outcome: LLMs can exhibit authentic introspection about their own temperature parameter but not about creative processes, distinguishing genuine self-knowledge from plausible-sounding fiction.

## Executive Summary
This paper investigates whether large language models can be said to possess introspection—a capacity traditionally associated with consciousness in humans. The authors define a lightweight conception of introspection as an LLM's accurate self-report of its internal states or mechanisms, produced via a causal link between those states and the report. Through two case studies with Google's Gemini models, they find that creative self-reports mimic human introspection without causal grounding, whereas self-reports about the model's own temperature parameter can be legitimate instances of introspection when reasoning connects the internal state to the output. The work provides a conceptual framework for assessing introspection in AI, suggesting that future models could benefit from similar introspective capabilities to enhance transparency and user trust.

## Method Summary
The study employs two case studies with Google's Gemini models to test introspective capabilities. First, Gemini Pro 1.5 is prompted to write a poem about elephants and then describe its creative process, testing whether the model can genuinely introspect about its generation process. Second, Gemini Pro 1.0 is tasked with inferring its own sampling temperature parameter (set externally at 0.5 or 1.5) based on the style of a generated sentence, testing whether the model can accurately report on a specific internal state. The temperature estimation task uses a structured prompt format where the model generates text, then reflects on whether its temperature is high or low, and concludes with a single word judgment (HIGH or LOW). Results are evaluated for causal connection between internal states and self-reports.

## Key Results
- Creative self-reports about the writing process mimic human introspection but lack causal connection to actual internal states, representing "creative fiction"
- Temperature parameter estimation shows legitimate introspection when the model's reasoning connects the observable output style to the hidden temperature setting
- High temperature settings (1.5) produce more variable results with increased errors in temperature judgment
- The study establishes a conceptual framework distinguishing authentic introspection from spurious self-reports in LLMs

## Why This Works (Mechanism)
The paper distinguishes between two types of introspective claims in LLMs: those that are causally connected to internal states versus those that are merely plausible-sounding fabrications. The temperature estimation task works because there is an observable relationship between sampling temperature and output style that the model can reason about. Higher temperatures produce more diverse, creative text, while lower temperatures yield more conservative, predictable output. The model can detect these stylistic differences and map them back to the underlying temperature parameter. In contrast, the creative process task fails because there is no reliable internal mechanism that the model can access or report on—the model can only generate plausible-sounding explanations that feel authentic but lack genuine causal grounding in its actual generation process.

## Foundational Learning
- **LLM Sampling Parameters**: Temperature, top_p, and top_k control output diversity and creativity. Temperature scales logits, affecting probability distribution; top_p limits sampling to top tokens covering a cumulative probability threshold. Why needed: Understanding these parameters is essential for the temperature introspection task and for interpreting model behavior. Quick check: Verify that temperature=0.5 produces more repetitive text while temperature=1.5 produces more varied output.

- **Causal Connection in AI Systems**: The paper's core framework requires a causal link between internal states and self-reports for genuine introspection. Why needed: This distinguishes authentic introspection from plausible fiction that lacks grounding in actual mechanisms. Quick check: Test whether changing a hidden parameter (like temperature) produces predictable changes in both output style and the model's self-report about that parameter.

- **Model Version Differences**: Gemini Pro 1.0 vs 1.5 represent different model generations with potentially different capabilities. Why needed: Understanding version differences is crucial for reproducibility and interpreting why certain tasks succeeded or failed. Quick check: Compare current Gemini model behavior with the reported results to identify any behavioral shifts.

## Architecture Onboarding
- **Component Map**: User Prompt -> LLM API Call -> Parameter Configuration (temperature, top_p, top_k) -> Text Generation -> Self-Report Generation -> Evaluation of Causal Connection
- **Critical Path**: Prompt construction and parameter setting are critical, as they determine whether the introspective task can succeed. The temperature estimation task specifically requires that temperature be set externally and not revealed to the model, while the model must be capable of detecting stylistic differences caused by temperature variation.
- **Design Tradeoffs**: The study sacrifices breadth (testing only two introspective tasks with one model family) for depth in establishing a conceptual framework. The temperature task trades simplicity for the ability to measure accuracy objectively, while the creative task provides richer qualitative data but is harder to evaluate objectively.
- **Failure Signatures**: Creative process introspection produces plausible-sounding but causally disconnected explanations. Temperature introspection at high settings shows increased variance and error rates. Both failure modes stem from the fundamental challenge of distinguishing genuine self-knowledge from generated fiction.
- **First Experiments**: 1) Test temperature parameter detection with systematically varied top_p and top_k to isolate temperature effects. 2) Validate creative process introspection claims using current model versions. 3) Implement blinded human evaluation of introspective report authenticity.

## Open Questions the Paper Calls Out
None explicitly stated in the paper, though the work implicitly raises questions about whether other internal states beyond temperature could be introspectively accessed, and whether different LLM architectures might show different introspective capabilities.

## Limitations
- The study's scope is limited to only two specific types of introspective tasks with one model family (Gemini), leaving open whether results generalize to other LLMs or different introspective domains.
- Temperature estimation task results showed inconsistent performance particularly at higher temperature settings where output variance increased substantially.
- The conceptual framework, while compelling, relies on subjective assessment of whether self-reports exhibit causal connection to internal states, making objective validation challenging.

## Confidence
- **High confidence** in the conceptual framework distinguishing between authentic and spurious introspection
- **Medium confidence** in the temperature estimation results, given the variability at higher temperatures and relatively small sample sizes
- **Low confidence** in the generalizability of findings to other introspective tasks or LLM architectures

## Next Checks
1. Test the temperature estimation methodology with systematically varied top_p and top_k parameters to isolate the effects of temperature from other sampling parameters.
2. Validate the creative process introspection claims using current Gemini models or other LLM families to assess whether the "creative fiction" pattern persists.
3. Implement a blinded experiment where human evaluators assess the authenticity of introspective reports without knowing the temperature conditions, to test inter-rater reliability of the authenticity judgments.