---
ver: rpa2
title: Towards a perturbation-based explanation for medical AI as differentiable programs
arxiv_id: '2502.14001'
source_url: https://arxiv.org/abs/2502.14001
tags:
- matrix
- learning
- input
- jacobian
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for objective explainability of AI
  models in medical applications, where black-box models are problematic. The authors
  propose a perturbation-based explanation (PBX) approach to measure how stably a
  model responds to small input perturbations, aiming to provide local, model-agnostic
  interpretability without requiring additional data.
---

# Towards a perturbation-based explanation for medical AI as differentiable programs
## Quick Facts
- arXiv ID: 2502.14001
- Source URL: https://arxiv.org/abs/2502.14001
- Authors: Takeshi Abe; Yoshiyuki Asai
- Reference count: 11
- The paper presents a perturbation-based explanation (PBX) approach using Jacobian matrix computation for interpretable medical AI models

## Executive Summary
This paper addresses the critical need for explainable AI in medical applications where black-box models pose significant risks. The authors propose a perturbation-based explanation method that measures how stably AI models respond to small input perturbations, providing local interpretability without requiring additional data. By establishing that the Jacobian matrix of deep learning models can be computed through a single forward pass algorithm, the approach offers a computationally efficient way to understand which input features most influence each output dimension.

The method is theoretically grounded in differential calculus and provides model-agnostic interpretability at the instance level. Unlike global explanation methods, this approach reveals the local sensitivity of model outputs to input perturbations, making it particularly valuable for medical decision-making where understanding individual predictions is crucial. The computational efficiency and single-pass Jacobian computation distinguish this method from traditional finite difference approaches.

## Method Summary
The core contribution is a perturbation-based explanation (PBX) approach that leverages the Jacobian matrix of deep learning models to measure output sensitivity to input perturbations. The authors establish a sufficient condition for Jacobian computability: if each layer's activation function has a computable Jacobian at any point, then the full model's Jacobian can be calculated. They present Algorithm 1, which computes the exact Jacobian matrix in a single forward pass, making it more efficient than finite difference methods. The approach provides local, model-agnostic interpretability by revealing which input features most influence each output dimension through sensitivity analysis.

## Key Results
- The Jacobian matrix of standard deep learning models can be computed using a single forward-pass algorithm
- The method provides a new type of explainability measure complementary to existing methods by showing which input features most influence each output dimension
- The approach is model-agnostic, local (calculated per instance), and computationally efficient
- A sufficient condition for Jacobian computability is established: if each layer's activation function has a computable Jacobian at any point, the full model's Jacobian can be calculated

## Why This Works (Mechanism)
The method works by leveraging the mathematical properties of differentiable functions and the chain rule. When a deep learning model is composed of differentiable layers, the overall Jacobian matrix can be computed through recursive application of the chain rule. The single-pass algorithm efficiently computes this matrix by propagating both the input through the network and the partial derivatives simultaneously. This reveals the local sensitivity of each output dimension to perturbations in each input feature, providing a quantitative measure of feature importance for individual predictions.

## Foundational Learning
- Jacobian matrix computation: Essential for understanding how output changes with respect to input perturbations; quick check: verify the matrix dimensions match (output_dim Ã— input_dim)
- Chain rule application: Critical for composing layer-wise derivatives into the full model Jacobian; quick check: ensure proper multiplication order in composite functions
- Forward-mode differentiation: The algorithm's efficiency relies on computing derivatives during the forward pass; quick check: confirm single-pass computation without backpropagation
- Differentiable activation functions: Required for the Jacobian to exist at evaluation points; quick check: verify activation function smoothness at specific input values
- Local sensitivity analysis: The interpretation framework depends on understanding point-wise feature influence; quick check: test with small perturbations around evaluation point
- Model-agnostic interpretability: The method's generality depends on differentiable components; quick check: apply to models with different architectures and activation functions

## Architecture Onboarding
Component map: Input -> Model Layers (with activation functions) -> Output, with parallel Jacobian computation path
Critical path: Forward pass through model layers while simultaneously computing partial derivatives using chain rule
Design tradeoffs: Single-pass efficiency vs. requirement for differentiable activation functions at evaluation points
Failure signatures: Non-differentiable activation functions (ReLU at zero), vanishing gradients, or numerical instability in derivative computation
First experiments:
1. Apply the Jacobian computation to a simple linear model and verify against analytical derivatives
2. Test the algorithm on a convolutional neural network for image classification with ReLU activations
3. Evaluate the method on a recurrent neural network for time series prediction with sigmoid/tanh activations

## Open Questions the Paper Calls Out
None

## Limitations
- The requirement for differentiable activation functions at evaluation points may exclude important models using ReLU or other non-smooth activations
- The method's focus on local explanations means it cannot capture global model behavior or interactions across different input regions
- While computational efficiency is claimed, no empirical runtime comparisons against existing methods are provided
- The perturbation-based explanation framework has not been validated on actual medical datasets or compared against established interpretability methods in clinical settings

## Confidence
High: The mathematical foundation for Jacobian computability and the forward-pass algorithm are sound, given the stated assumptions about activation functions. The theoretical framework appears robust and well-defined.

Medium: The claim that this provides meaningful interpretability for medical AI applications lacks empirical validation. The practical utility and clinical relevance remain to be demonstrated through experiments with real medical data.

Low: The assertion that this method is "model-agnostic" is questionable, as it requires differentiable activation functions and may not apply to models with discrete components or non-differentiable operations.

## Next Checks
1. Test the Jacobian computation algorithm on a medical imaging dataset (e.g., chest X-rays or retinal scans) and compare the resulting explanations against ground truth clinical annotations.

2. Benchmark computational efficiency by measuring runtime against finite difference methods and existing gradient-based explanation techniques across models of varying sizes.

3. Evaluate the method's behavior on models with ReLU activations by testing whether the approach still provides meaningful explanations when applied to sub-regions where the activation is differentiable.