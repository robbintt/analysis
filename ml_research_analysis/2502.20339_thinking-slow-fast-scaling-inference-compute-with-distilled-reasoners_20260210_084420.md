---
ver: rpa2
title: 'Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners'
arxiv_id: '2502.20339'
source_url: https://arxiv.org/abs/2502.20339
tags:
- distilled
- time
- https
- inference
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether lower-complexity models can outperform
  Transformer-based models in reasoning tasks by leveraging faster generation speeds.
  The authors distill pure Mamba and hybrid Mamba-in-Llama models from Llama teacher
  models to enable efficient inference for test-time compute scaling.
---

# Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners

## Quick Facts
- arXiv ID: 2502.20339
- Source URL: https://arxiv.org/abs/2502.20339
- Authors: Daniele Paliotta; Junxiong Wang; Matteo Pagliardini; Kevin Y. Li; Aviv Bick; J. Zico Kolter; Albert Gu; François Fleuret; Tri Dao
- Reference count: 35
- Lower-complexity models can outperform Transformers in reasoning tasks through faster generation speeds

## Executive Summary
This paper investigates whether subquadratic architectures can match or exceed Transformer-based models in reasoning tasks by leveraging faster generation speeds. The authors develop distilled Mamba and hybrid Mamba-in-Llama models from Llama teacher models, demonstrating that these architectures can achieve strong performance on mathematical reasoning benchmarks while generating tokens 2.8-4.2× faster. The approach enables efficient test-time compute scaling, where faster generation allows more completions within fixed time budgets, improving coverage and accuracy.

The key insight is that reasoning performance can be enhanced not just by increasing model size, but by optimizing inference speed to generate more diverse reasoning paths. The distilled models outperform Llama-1B on coverage and accuracy metrics while being significantly faster, and larger distilled models at 3B scale surpass smaller Transformers. This demonstrates a new paradigm for scaling inference compute in reasoning applications beyond traditional model size increases.

## Method Summary
The authors employ knowledge distillation to transfer reasoning capabilities from Transformer-based Llama models to faster subquadratic architectures. They create both pure Mamba models and hybrid Mamba-in-Llama architectures, distilling them using only 8 billion tokens of reasoning-focused training data. The distillation process leverages the teacher models' strengths while capitalizing on the faster generation speeds of Mamba architectures. The approach focuses on mathematical reasoning tasks (MATH and GSM8K) and evaluates performance under fixed time budgets, where the faster token generation enables more completion attempts and better coverage of the solution space.

## Key Results
- Distilled models generate tokens 2.8-4.2× faster than Transformer counterparts
- Under fixed time budgets, faster generation enables better coverage and accuracy through more completion attempts
- Pure and hybrid Mamba models outperform Llama-1B on coverage and accuracy while being faster
- Larger distilled models (3B scale) outperform smaller Transformer models on reasoning benchmarks

## Why This Works (Mechanism)
The core mechanism relies on test-time compute scaling through faster inference. Traditional reasoning approaches focus on increasing model size or computational budget per token, but this work demonstrates that generating tokens more quickly enables a different scaling strategy. By producing completions faster, the model can attempt more reasoning paths within the same time constraint, effectively exploring a larger solution space. The subquadratic Mamba architecture enables this speed advantage while maintaining reasoning capabilities through knowledge distillation from stronger Transformer teachers.

## Foundational Learning
- **Knowledge Distillation**: Transferring knowledge from larger teacher models to smaller student models - needed to leverage strong reasoning capabilities while benefiting from faster architectures; quick check: verify teacher-student performance gap
- **Subquadratic Architectures**: Mamba and similar models with computational complexity better than O(n²) - needed to achieve the 2.8-4.2× speedup in token generation; quick check: confirm generation speed measurements
- **Test-time Compute Scaling**: Using inference time budgets to generate multiple solutions - needed to exploit the speed advantage for better coverage; quick check: validate coverage improvements under time constraints
- **Hybrid Model Design**: Combining Mamba with Transformer components (Mamba-in-Llama) - needed to balance speed with reasoning quality; quick check: compare pure vs hybrid performance
- **Reasoning Chain Generation**: Producing multiple solution paths for complex problems - needed to demonstrate the benefits of faster inference; quick check: measure solution diversity metrics
- **Fixed Budget Evaluation**: Assessing performance under time constraints rather than token limits - needed to capture the practical benefits of faster generation; quick check: verify time-based evaluation methodology

## Architecture Onboarding

**Component Map:**
Teacher Llama -> Distillation Process -> Student Mamba/Pure Hybrid -> Inference Engine

**Critical Path:**
Token Generation Speed -> Number of Completions -> Coverage of Solution Space -> Final Accuracy

**Design Tradeoffs:**
Speed vs. Reasoning Quality: Mamba architectures provide 2.8-4.2× speedup but may sacrifice some reasoning depth compared to Transformers
Pure vs. Hybrid: Pure Mamba offers maximum speed while hybrid Mamba-in-Llama balances speed with retained Transformer capabilities
Model Size vs. Efficiency: Larger distilled models (3B) outperform smaller Transformers despite being the same size class

**Failure Signatures:**
Poor performance on non-mathematical reasoning tasks (limited evaluation scope)
Degradation in long-context scenarios not captured by MATH/GSM8K benchmarks
Potential instability in hybrid architectures under extreme generation speeds

**First Experiments:**
1. Compare pure Mamba vs hybrid Mamba-in-Llama on same reasoning benchmarks to isolate speed vs quality tradeoffs
2. Test performance scaling with varying time budgets to validate test-time compute scaling claims
3. Evaluate on commonsense reasoning and logical inference tasks beyond mathematical domains

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Training data scope limited to only 8 billion tokens of reasoning-focused content
- Evaluation restricted to mathematical reasoning benchmarks (MATH and GSM8K)
- No detailed analysis of memory usage patterns or long-context performance
- Hybrid architecture complexity may affect practical deployment considerations

## Confidence
- **High confidence**: Observed speed improvements (2.8-4.2×) and benchmark performance on MATH/GSM8K
- **Medium confidence**: Generalization to broader reasoning tasks beyond mathematical problems
- **Medium confidence**: Long-term stability and robustness of distilled Mamba models in production settings
- **Low confidence**: Scalability claims beyond the 3B parameter range tested

## Next Checks
1. Evaluate distilled models on diverse reasoning benchmarks including commonsense reasoning, logical inference, and multi-step planning tasks
2. Conduct comprehensive memory and computational efficiency analysis across varying sequence lengths
3. Test model performance with extended reasoning chains and longer time horizons to validate test-time compute scaling under demanding scenarios