---
ver: rpa2
title: Evaluation of GPT-based large language generative AI models as study aids for
  the national licensure examination for registered dietitians in Japan
arxiv_id: '2508.10011'
source_url: https://arxiv.org/abs/2508.10011
tags:
- correct
- chatgpt
- answer
- nutrition
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated the potential of large language models (LLMs)
  as study aids for the Japanese national licensure examination for registered dietitians.
  Questions from the 2023 exam were used as prompts for ChatGPT (GPT-3.5) and three
  Bing models (Precise, Creative, Balanced, based on GPT-4).
---

# Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan

## Quick Facts
- **arXiv ID**: 2508.10011
- **Source URL**: https://arxiv.org/abs/2508.10011
- **Reference count**: 18
- **Key result**: Bing-Precise and Bing-Creative models exceeded the 60% passing threshold on the Japanese registered dietitian exam, while ChatGPT and Bing-Balanced did not.

## Executive Summary
This study evaluated the potential of large language models (LLMs) as study aids for the Japanese national licensure examination for registered dietitians. The researchers used 200 questions from the 2023 exam (188 after excluding those with figures/graphs) as prompts for ChatGPT (GPT-3.5) and three Bing models (Precise, Creative, Balanced, all based on GPT-4). Each question was entered in ten independent sessions to assess accuracy, consistency, and response time. Bing-Precise (66.2%) and Bing-Creative (61.4%) exceeded the passing threshold of 60%, while Bing-Balanced (43.3%) and ChatGPT (42.8%) did not. The study found significant inconsistencies in model responses across repeated trials, raising concerns about their reliability as study aids.

## Method Summary
The researchers collected 200 questions from the 2023 Japanese national registered dietitian exam, excluding 12 with figures/graphs. They tested ChatGPT (GPT-3.5) and three Bing models (GPT-4-based: Precise, Balanced, Creative) on 188 multiple-choice questions. Each question was input in 10 independent sessions with cleared context between attempts. For accuracy, they calculated correct answer rates and compared them to the 60% passing threshold. For consistency, they used Shannon's entropy and Precision Focus Index on a subset of 30 questions (3 random per subject field). Response times were manually measured. They also tested prompt engineering by explicitly providing correct answers and explanations.

## Key Results
- Bing-Precise (66.2%) and Bing-Creative (61.4%) exceeded the 60% passing threshold; Bing-Balanced (43.3%) and ChatGPT (42.8%) did not.
- All models showed inconsistent responses across repeated trials, with none achieving consistent correct answers.
- Bing models generally outperformed others across subject fields except Nutrition Education, where all models underperformed.

## Why This Works (Mechanism)
Large language models can process and generate text-based responses to exam questions by leveraging their pre-trained knowledge. The models' performance depends on their underlying architecture (GPT-3.5 vs GPT-4), training data composition, and response generation parameters. The study demonstrates that more advanced models (GPT-4-based) can achieve passing scores on specialized professional exams when properly prompted, though their consistency remains problematic.

## Foundational Learning
- **LLM temperature parameter**: Controls randomness in response generation; why needed because it affects consistency; quick check: observe variance in repeated responses with different temperature settings
- **Pre-training data bias**: Models perform better on subjects well-represented in training data; why needed because nutrition content is underrepresented; quick check: analyze subject-wise performance gaps
- **Response entropy measurement**: Quantifies response variability across trials; why needed to assess consistency reliability; quick check: calculate Shannon entropy for repeated model responses

## Architecture Onboarding
- **Component map**: Exam questions -> LLM prompt input -> Response generation -> Answer parsing -> Accuracy/consistency evaluation
- **Critical path**: Question input → Model response → Answer classification → Performance metric calculation
- **Design tradeoffs**: General-purpose vs specialized models (accuracy vs consistency), web interface vs API (accessibility vs parameter control)
- **Failure signatures**: Inconsistent answers across trials, poor performance on underrepresented subjects, time-varying results due to model updates
- **First experiments**:
  1. Run 10 trials of the same question with temperature set to 0 to test consistency improvement
  2. Test prompt engineering with chain-of-thought reasoning on 30 random questions
  3. Evaluate a nutrition-specialized LLM on the same exam questions for comparison

## Open Questions the Paper Calls Out
### Open Question 1
Does utilizing an API-based LLM environment with a near-zero temperature parameter significantly improve the consistency and reproducibility of answers for dietitian licensure exam preparation? The authors recommend using API-based solutions with low temperature settings to address inconsistent responses, but the current study used standard web interfaces with undisclosed temperature parameters.

### Open Question 2
Can targeted fine-tuning of general-purpose LLMs with specialized nutrition and dietetics resources bridge the accuracy gap between dietitian and medical licensure exam performance? The authors attribute lower performance to pre-training data biases and suggest intensive fine-tuning with nutrition-focused resources.

### Open Question 3
To what extent do newer multimodal models (e.g., GPT-4o) improve accuracy on questions containing visual data, such as figures and graphs? The study excluded visual questions, while noting future studies should evaluate multimodal models that may handle such data better.

## Limitations
- Single year's exam from one national licensing system limits generalizability
- Inconsistent model performance across repeated trials suggests sensitivity to prompt phrasing and model state
- Manual response-time measurements introduce potential measurement error

## Confidence
- **High Confidence**: Bing-Precise and Bing-Creative exceeded the 60% passing threshold; none of the models achieved consistent correct answers across repeated trials
- **Medium Confidence**: Subject-specific performance patterns; minimal impact of prompt engineering except when correct answers were explicitly provided
- **Low Confidence**: Stability of findings over time given rapid LLM evolution; practical utility as reliable study aids

## Next Checks
1. Replicate the experiment with current versions of GPT-4 and GPT-3.5 to assess whether performance patterns persist as models are updated
2. Test the effect of different prompt engineering strategies (including role-based prompts and chain-of-thought reasoning) on both accuracy and consistency across multiple trials
3. Evaluate performance on multiple years of exam questions to assess whether observed subject-specific weaknesses represent systematic gaps or random variation