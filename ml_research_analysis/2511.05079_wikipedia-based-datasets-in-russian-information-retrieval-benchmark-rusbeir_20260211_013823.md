---
ver: rpa2
title: Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR
arxiv_id: '2511.05079'
source_url: https://arxiv.org/abs/2511.05079
tags:
- datasets
- retrieval
- wikipedia
- russian
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Wikipedia Interesting Facts series of\
  \ datasets for Russian information retrieval, constructed from the \u201CDid you\
  \ know...\u201D section of Russian Wikipedia. The datasets leverage interesting\
  \ facts and their referenced Wikipedia articles, annotated at the sentence level\
  \ with graded relevance, to support tasks like fact-checking and retrieval-augmented\
  \ generation."
---

# Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR

## Quick Facts
- arXiv ID: 2511.05079
- Source URL: https://arxiv.org/abs/2511.05079
- Reference count: 28
- Primary result: Wikipedia Interesting Facts datasets for Russian IR, showing BM25 outperforms neural models on full-document retrieval while neural models excel on shorter texts, with consistent improvements from reranking

## Executive Summary
This paper introduces Wikipedia-based datasets for Russian information retrieval benchmark RusBEIR, constructed from the "Did you know..." section of Russian Wikipedia. The datasets feature interesting facts paired with referenced Wikipedia articles, annotated at the sentence level with graded relevance scores. The study evaluates both lexical (BM25) and neural retrieval models across varying document lengths and demonstrates that combining retrieval with neural reranking consistently improves performance. The datasets and code are publicly available.

## Method Summary
The method involves extracting interesting facts from Russian Wikipedia's "Did you know..." section, retrieving referenced articles, and creating sentence-level relevance annotations. Four dataset variants are constructed: full articles, paragraphs, sentences, and sliding windows of 2-6 sentences. BM25 baseline uses Elasticsearch with Russian-optimized preprocessing (lemmatization via PyMorphy3, stopword removal). Neural models include multilingual and Russian-specific bi-encoders (BGE-M3, FRIDA, USER-BGE-M3) and a cross-encoder reranker (bge-reranker-v2-m3). Evaluation uses NDCG@10 on 5,433 queries across the datasets.

## Key Results
- BM25 achieves 74.95 NDCG@10 on full articles while FRIDA (neural) achieves 60.54, but FRIDA achieves 20.79 vs BM25's 13.58 on sentences
- Neural reranking improves all single models by approximately 9 percentage points on average
- Russian-specific models (FRIDA, USER-BGE-M3) consistently outperform multilingual models across all datasets
- BGE-M3 with max-length 2048 achieves optimal balance of performance and efficiency

## Why This Works (Mechanism)

### Mechanism 1: Document Length Differentially Impacts Retrieval Paradigms
BM25 operates on sparse term frequency signals that accumulate evidence across long documents, while neural bi-encoders compress documents into fixed-dimension vectors, losing granularity beyond their max token limit. When documents exceed this limit, truncation discards potentially relevant information, creating a representational bottleneck. The relevance signal in long documents is distributed across the text rather than concentrated in the first N tokens.

### Mechanism 2: Neural Reranking Recovers Lost Precision from First-Stage Retrieval
Bi-encoders encode queries and documents separately, computing similarity via cosine distance—a process that loses query-document interaction signals. Cross-encoders process query-document pairs jointly, allowing deep attention across all token pairs, capturing fine-grained semantic alignment. This two-stage design trades inference cost for precision gains under the assumption that the top-k retrieved documents contain the relevant answer.

### Mechanism 3: Language-Specific Fine-Tuning Compensates for Morphological Complexity
Russian has rich inflectional morphology (6 cases, 3 genders, complex verb aspects). Multilingual models distribute capacity across 100+ languages, diluting Russian-specific representations. Language-specific fine-tuning concentrates model capacity on Russian morphological patterns and lexical semantics, improving embedding quality for morphologically complex tokens.

## Foundational Learning

- Concept: Sparse vs. Dense Retrieval Paradigms
  - Why needed here: The entire paper hinges on comparing BM25 (sparse, term-based) against neural bi-encoders (dense, embedding-based); understanding this distinction is prerequisite to interpreting results.
  - Quick check question: Given a query "interesting facts about Moscow," would BM25 retrieve documents containing exact term matches, semantic paraphrases like "curious trivia regarding the Russian capital," or both? Explain which paradigm handles which case better.

- Concept: Cross-Encoder vs. Bi-Encoder Architectures
  - Why needed here: The reranking mechanism depends entirely on cross-encoders' joint query-document processing; without this understanding, the two-stage retrieval improvement appears magical.
  - Quick check question: If a bi-encoder produces a query embedding q and document embedding d, and a cross-encoder processes the same query-document pair, which model can attend to interactions between query token i and document token j? Why does this matter for relevance scoring?

- Concept: Tokenization and Context Window Limits
  - Why needed here: The paper explicitly investigates max-length effects (Table 6) and neural model degradation on long documents; this requires understanding how tokenizers truncate text and why 512 vs. 8192 token limits create different performance profiles.
  - Quick check question: A BGE-M3 model with max-length=512 processes a 2000-word Russian article. What information is lost, and why might this hurt retrieval quality more for fact-checking queries than for broad topic queries?

## Architecture Onboarding

- Component map:
Raw Query + Corpus Documents (sentences/paragraphs/articles) -> Preprocessing Layer -> BM25 Index -> Sparse Retrieval -> Top-k candidates
                                      |
                                      +-> Neural Bi-Encoder -> Dense Retrieval -> Top-k candidates
                                      |
                                      v
Candidate Pool -> Top-k from either or both retrievers -> Cross-Encoder Reranker -> Final Ranking -> NDCG@10 evaluation

- Critical path:
1. Dataset construction: Extract facts from Wikipedia "Did you know..." section, retrieve referenced articles, annotate sentence-level relevance (scores 0/1/2)
2. Corpus segmentation: Generate wikifacts-{articles, para, sents, window_2-6} variants
3. First-stage retrieval: Run BM25 and neural bi-encoders independently
4. Reranking: Apply bge-reranker-v2-m3 to top-100 candidates from each first-stage retriever
5. Evaluation: Compute NDCG@10 using graded relevance judgments

- Design tradeoffs:
- Max-length 2048 vs. 8192: Paper finds 2048 optimal—longer contexts add 0.3-0.5 NDCG gain but 4-8x inference cost (batch size drops from 64 to 8)
- BM25 vs. dense first-stage: BM25 provides better recall for long documents with no inference cost; dense retrievers better for semantic matching on short texts but require GPU inference
- Russian-specific vs. multilingual: Russian models (FRIDA, USER-BGE-M3) win on Russian benchmarks but won't transfer to cross-lingual tasks

- Failure signatures:
- Neural retriever returns irrelevant results on long documents: Check if documents exceed max-length; inspect truncation position; consider sliding-window chunking
- Reranking doesn't improve over first-stage: Verify candidate pool size (need top-50 to 100, not top-10); check if cross-encoder is loaded correctly with correct max-length
- BM25 underperforms on Russian text: Confirm lemmatization is applied (PyMorphy3), not stemming; verify stopword list includes Russian-specific terms

- First 3 experiments:
1. Reproduce Table 4 on wikifacts-sents vs. wikifacts-articles with BM25 and FRIDA; plot NDCG@10 vs. average document length to validate length-dependent performance crossover
2. Ablate reranker impact: For FRIDA retriever, vary reranker candidate pool size (10, 25, 50, 100) and measure NDCG@10 to identify minimum viable candidate set
3. Test max-length hypothesis: Run BGE-M3 on wikifacts-articles with max-length ∈ {512, 1024, 2048, 4096}; measure both NDCG@10 and inference latency per query to validate 2048 efficiency claim

## Open Questions the Paper Calls Out
- To what extent can Large Language Models (LLMs) replace human annotators in generating graded relevance judgments for the "Did you know..." facts?
- Can the dataset construction methodology based on Wikipedia's "Did you know..." section be successfully adapted for low-resource languages where Wikipedia structure is less standardized?
- Does the superior performance of lexical models on full-document retrieval stem primarily from neural embedding dilution or truncation issues?

## Limitations
- The morphological complexity hypothesis remains speculative without isolating morphological processing as the causal mechanism
- Reranking improvement relies on the assumption that top-k candidates contain relevant documents without testing recall
- Russian-specific model superiority lacks ablation studies isolating morphological benefits from other confounding factors

## Confidence
- **High confidence**: The document-length effect on retrieval paradigms (BM25 vs. neural) is well-supported by quantitative results across multiple dataset variants and max-length experiments
- **Medium confidence**: The reranking improvement mechanism is validated empirically but relies on the assumption that top-k candidates contain relevant documents—a condition not tested with recall analysis
- **Medium confidence**: Language-specific fine-tuning superiority is demonstrated on the RusBEIR benchmark but lacks ablation studies isolating morphological processing benefits from other confounding factors

## Next Checks
1. Perform recall@k analysis for first-stage retrievers to quantify the assumption that reranking only reorders relevant documents rather than recovering missing ones
2. Conduct ablation experiments comparing PyMorphy3 lemmatization vs. alternative Russian morphological analyzers to isolate their contribution to BM25 and neural model performance
3. Test cross-lingual transfer by evaluating the Russian-specific models (FRIDA, USER-BGE-M3) on multilingual datasets to quantify the trade-off between specialization and generalization