---
ver: rpa2
title: 'Number Representations in LLMs: A Computational Parallel to Human Perception'
arxiv_id: '2502.16147'
source_url: https://arxiv.org/abs/2502.16147
tags:
- component
- layer
- birth
- population
- index
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) encode numerical values in a structured,
  non-uniform manner, exhibiting sublinear scaling reminiscent of human logarithmic
  mental number line perception. This study investigates how LLMs represent numerical
  magnitudes by analyzing hidden state embeddings across model layers using dimensionality
  reduction techniques (PCA and PLS) and geometric regression.
---

# Number Representations in LLMs: A Computational Parallel to Human Perception

## Quick Facts
- arXiv ID: 2502.16147
- Source URL: https://arxiv.org/abs/2502.16147
- Reference count: 40
- Key outcome: LLMs encode numerical values with sublinear scaling, mirroring human logarithmic number line perception

## Executive Summary
This study investigates how large language models represent numerical magnitudes by analyzing hidden state embeddings across model layers. Using dimensionality reduction techniques and geometric regression, researchers found that LLMs encode numbers in a structured, non-uniform manner with systematic compression - smaller values have higher resolution while larger numbers are compressed. The findings reveal that numerical embeddings maintain order preservation but exhibit distances between consecutive values that decrease as magnitudes increase, suggesting LLMs process numbers with cognitive-like patterns similar to human logarithmic mental number line perception.

## Method Summary
The researchers analyzed hidden state embeddings from LLMs using PCA and PLS dimensionality reduction techniques combined with geometric regression to examine numerical representations. They conducted experiments with contextualized numbers, letters, and real-world tasks including celebrity birth years and population sizes. The study systematically examined how numerical values are encoded across different model layers, comparing the strength of sublinear scaling patterns captured by different analytical approaches to understand the geometric structure of numerical representations.

## Key Results
- LLMs exhibit sublinear scaling in numerical representations, with distances between consecutive values decreasing as magnitudes increase
- PCA consistently captures stronger sublinearity than PLS, suggesting linear probes may obscure underlying geometric structures
- Numerical embeddings maintain order preservation while showing systematic compression patterns similar to human logarithmic number line perception

## Why This Works (Mechanism)
The sublinear scaling observed in LLM numerical representations likely emerges from the interaction between tokenization schemes and the model's attention mechanisms. When processing numbers, transformers must balance precision for small values against efficiency for large values, naturally leading to compressed representations at higher magnitudes. This compression pattern mirrors human cognitive limitations where we can discriminate between small numbers more precisely than large ones, suggesting a computational efficiency principle that applies across both biological and artificial neural systems.

## Foundational Learning
- **Dimensionality reduction (PCA/PLS)**: Needed to visualize high-dimensional embeddings in interpretable 2D/3D spaces; quick check: compare explained variance ratios
- **Geometric regression**: Required to quantify scaling relationships in embedding spaces; quick check: validate linearity assumptions with residual analysis
- **Logarithmic number line**: Human cognitive framework for understanding numerical perception; quick check: compare LLM compression ratios to Weber's law predictions
- **Transformer attention mechanisms**: Core architectural component that shapes numerical representations; quick check: ablate attention heads to isolate effects
- **Tokenization schemes**: Critical factor affecting how numbers are processed; quick check: compare results across different tokenizers

## Architecture Onboarding

Component Map: Input numbers -> Tokenizer -> Embedding Layer -> Attention Layers -> Hidden States -> PCA/PLS Analysis -> Geometric Regression

Critical Path: The study focuses on how numerical information flows from raw tokens through embedding layers into hidden states, where geometric properties emerge. The critical analysis occurs at the hidden state level where dimensionality reduction and regression techniques extract scaling patterns.

Design Tradeoffs: Models must balance precision for small numbers against efficiency for large numbers, leading to the observed compression. This reflects a fundamental tradeoff between representational capacity and computational efficiency that appears across both biological and artificial systems.

Failure Signatures: Linear probes (PLS) may miss or underestimate geometric structures present in the data. Models with different tokenization schemes may show varying degrees of sublinear scaling. The pattern may not generalize across all architectural variants.

First Experiments:
1. Compare sublinear scaling patterns across different LLM architectures (GPT, BERT, T5 variants)
2. Test how tokenization schemes affect numerical representation compression
3. Validate findings with arithmetic reasoning tasks to check ecological relevance

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize across different LLM architectures and tokenization schemes
- Hidden state embeddings may not fully capture complete numerical reasoning capabilities
- Dimensionality reduction techniques may introduce artifacts or oversimplify complex representations
- Study does not address how numerical representations interact with other semantic information in real-world applications

## Confidence

High confidence:
- LLMs exhibit sublinear scaling in numerical representations across multiple experiments

Medium confidence:
- The pattern parallels human logarithmic mental number line perception based on structural similarities

Low confidence:
- PCA consistently captures stronger sublinearity than PLS may be sensitive to specific model configurations

## Next Checks
1. Test the sublinear scaling hypothesis across a broader range of LLM architectures (e.g., different transformer variants) and tokenization schemes to assess generalizability
2. Conduct ablation studies to isolate the contribution of specific layers or attention mechanisms to the observed numerical compression patterns
3. Validate the ecological relevance of the findings by evaluating numerical reasoning performance in more complex, real-world tasks (e.g., multi-step arithmetic problems or numerical analogies)