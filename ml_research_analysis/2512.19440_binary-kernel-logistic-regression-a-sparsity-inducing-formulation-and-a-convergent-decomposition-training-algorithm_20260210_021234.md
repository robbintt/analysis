---
ver: rpa2
title: 'Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent
  decomposition training algorithm'
arxiv_id: '2512.19440'
source_url: https://arxiv.org/abs/2512.19440
tags:
- data
- formulation
- points
- sparsity
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a sparsity-inducing formulation for training
  binary kernel logistic regression (KLR) that improves the trade-off between model
  sparsity and testing accuracy compared to existing approaches. The authors extend
  the KLR training formulation by introducing a slack variable that adjusts the contribution
  of easy-to-classify data points, thereby promoting sparsity in the dual variables.
---

# Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm

## Quick Facts
- arXiv ID: 2512.19440
- Source URL: https://arxiv.org/abs/2512.19440
- Authors: Antonio Consolo; Andrea Manno; Edoardo Amaldi
- Reference count: 40
- The paper proposes a sparsity-inducing formulation for training binary kernel logistic regression that improves the trade-off between model sparsity and testing accuracy

## Executive Summary
This paper addresses the challenge of achieving sparsity in binary kernel logistic regression while maintaining competitive classification accuracy. The authors introduce a novel sparsity-inducing formulation that extends the standard KLR training problem by incorporating a slack variable, which adjusts the contribution of easy-to-classify data points and promotes sparsity in the dual variables. To efficiently solve the resulting optimization problem, they develop a Sequential Minimal Optimization (SMO)-type decomposition algorithm that exploits second-order information and establish its global convergence properties.

## Method Summary
The authors propose a sparsity-inducing formulation for binary kernel logistic regression by introducing a slack variable that allows easy-to-classify data points to have minimal impact on the decision boundary. This formulation is solved using a decomposition algorithm based on Sequential Minimal Optimization principles, which exploits second-order information through working set selection. The algorithm iteratively updates a subset of dual variables while keeping others fixed, ensuring global convergence to the optimal solution. The approach effectively balances the trade-off between model sparsity and classification accuracy.

## Key Results
- Achieved 92.5% average testing accuracy with 42.2% sparsity across 12 benchmark datasets
- Compared to SVM: 90.5% accuracy with 26.8% sparsity (proposed method has better sparsity-accuracy trade-off)
- Second-order working set selection reduced CPU training time by approximately 18% compared to first-order variant

## Why This Works (Mechanism)
The proposed sparsity-inducing formulation works by introducing a slack variable that reduces the influence of easy-to-classify data points on the optimization objective. This mechanism effectively identifies and downweights redundant or less informative samples during training, leading to a sparser model without sacrificing classification performance. The slack variable acts as a threshold that determines when a data point should contribute significantly to the decision boundary, with easy examples being assigned smaller weights in the dual formulation.

## Foundational Learning
- Kernel methods: These transform data into higher-dimensional spaces to make nonlinear patterns linearly separable. Why needed: KLR uses kernels to handle nonlinear decision boundaries efficiently. Quick check: Verify kernel matrix computation and positive semi-definiteness.
- Sequential Minimal Optimization (SMO): An optimization algorithm that breaks large problems into smaller subproblems. Why needed: Enables efficient solution of the dual optimization problem. Quick check: Confirm working set selection and dual variable update rules.
- Convex optimization: The study of minimizing convex functions over convex sets. Why needed: Ensures global convergence of the decomposition algorithm. Quick check: Verify objective function convexity and constraint qualifications.
- Regularization: Adding penalty terms to prevent overfitting. Why needed: Controls model complexity and generalization. Quick check: Confirm regularization parameter effects on sparsity and accuracy.

## Architecture Onboarding

Component map: Input data -> Kernel transformation -> Slack variable formulation -> SMO decomposition -> Sparse dual variables -> Prediction function

Critical path: The decomposition algorithm iteratively selects working sets of dual variables, solves the subproblem for these variables while fixing others, and updates the solution until convergence. This process directly determines the final sparse model and prediction accuracy.

Design tradeoffs: The authors balance between model sparsity and classification accuracy through the slack variable parameter. A larger slack promotes more sparsity but may reduce accuracy, while a smaller slack maintains accuracy at the cost of denser models. The second-order working set selection improves convergence speed but increases computational complexity per iteration.

Failure signatures: The method may fail to achieve significant sparsity if the slack parameter is set too conservatively, or it may sacrifice accuracy if the slack is too aggressive. Convergence issues could arise if the kernel matrix is ill-conditioned or if numerical precision is insufficient for the second-order updates.

First experiments:
1. Validate convergence on a simple synthetic dataset with known solution
2. Test sparsity-accuracy trade-off by varying the slack parameter on a small benchmark
3. Compare training time between first-order and second-order working set selection variants

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the study.

## Limitations
- The sparsity-inducing formulation introduces an additional hyperparameter that requires careful tuning
- Experimental evaluation is limited to 12 benchmark datasets, which may not represent diverse real-world scenarios
- The convergence analysis assumes certain conditions that may not always hold in practice

## Confidence

High confidence in:
- The theoretical framework of the sparsity-inducing formulation and its convergence properties
- The experimental results demonstrating improved sparsity-accuracy trade-off compared to existing methods

Medium confidence in:
- The practical effectiveness of the second-order working set selection procedure in reducing training time
- The generalizability of the findings to datasets beyond those tested

Low confidence in:
- The robustness of the method under various data distributions and noise levels
- The scalability to extremely large-scale problems

## Next Checks

1. Evaluate the method's performance on a broader range of real-world datasets, particularly those with varying sizes, feature dimensions, and noise characteristics.

2. Conduct an ablation study to quantify the individual contributions of the slack variable and the second-order working set selection to overall performance improvements.

3. Test the method's robustness by systematically introducing different levels and types of noise to the training data and measuring the impact on accuracy and sparsity.