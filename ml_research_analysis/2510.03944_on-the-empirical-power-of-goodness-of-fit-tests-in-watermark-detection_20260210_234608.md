---
ver: rpa2
title: On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection
arxiv_id: '2510.03944'
source_url: https://arxiv.org/abs/2510.03944
tags:
- tests
- watermark
- detection
- text
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates goodness-of-fit (GoF) tests
  for watermark detection in LLM-generated text. The key insight is that watermark
  detection can be formulated as a hypothesis testing problem where pivotal statistics
  follow a known distribution under human-written text, making GoF tests a natural
  tool.
---

# On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection
## Quick Facts
- arXiv ID: 2510.03944
- Source URL: https://arxiv.org/abs/2510.03944
- Reference count: 40
- Key outcome: Goodness-of-fit tests consistently outperform baseline methods for watermark detection across various conditions, with unique advantages at low temperatures due to text repetition

## Executive Summary
This paper presents a comprehensive evaluation of goodness-of-fit (GoF) tests for detecting watermarks in LLM-generated text. The study formulates watermark detection as a hypothesis testing problem where pivotal statistics follow known distributions under human-written text, making GoF tests a natural tool. Eight different GoF tests are systematically evaluated across three popular watermarking schemes, three open-source LLMs, two datasets, and multiple temperature and editing conditions. The results demonstrate that GoF tests provide consistent advantages over baseline methods, with particular robustness to text editing and strong performance at low temperatures where text repetition occurs.

## Method Summary
The authors evaluate eight goodness-of-fit tests for watermark detection across three watermarking schemes (MOSS, PENCIL, and SynQuake) and three open-source LLMs (Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Mistral-7B-Instruct). They test across two datasets (in-house generated and Perplexity questions) at three temperatures (0.1, 0.7, 1.5) with four editing conditions (no edit, substitution, insertion, deletion, and information-rich edits). The study also includes a case study on text repetition at low temperatures and an analysis of computational efficiency. All experiments follow a consistent methodology of computing pivotal statistics under the null hypothesis of human-written text and comparing against theoretical null distributions.

## Key Results
- GoF tests consistently outperform baseline watermark detection methods across all tested temperatures and text lengths
- The tests demonstrate robustness to both common edits (substitution, insertion, deletion) and information-rich edits (title, citation, and domain changes)
- Text repetition at low temperatures provides a unique advantage to GoF tests, enabling strong detection performance even in challenging settings

## Why This Works (Mechanism)
Watermark detection can be formulated as a hypothesis testing problem where the null hypothesis assumes human-written text. Under this assumption, pivotal statistics computed from the text follow known theoretical distributions. GoF tests are designed to determine whether observed data matches these theoretical distributions. When watermarked text violates the assumptions of the null hypothesis, the GoF tests can detect this deviation. The effectiveness stems from the fact that watermarking schemes introduce statistical patterns that differ from naturally occurring human text, which GoF tests can identify through their distributional assumptions.

## Foundational Learning
**Hypothesis Testing** - Framework for making decisions based on data by comparing against a null hypothesis. Needed to formulate watermark detection as a statistical problem. Quick check: Verify that the null hypothesis correctly represents human-written text behavior.

**Pivotal Statistics** - Statistics whose distribution does not depend on unknown parameters under the null hypothesis. Needed to enable comparison against theoretical distributions. Quick check: Confirm that computed statistics are truly pivotal under the assumed null.

**Goodness-of-Fit Tests** - Statistical tests that determine whether observed data follows a specified theoretical distribution. Needed as the core detection mechanism. Quick check: Validate that the chosen GoF tests are appropriate for the distribution of pivotal statistics.

**Watermarking Schemes** - Methods for embedding detectable patterns in LLM-generated text. Needed to understand what statistical patterns need to be detected. Quick check: Characterize the statistical properties introduced by each watermarking scheme.

**Text Repetition Analysis** - Study of token frequency patterns at low temperatures. Needed to explain the unique advantage of GoF tests at low temperatures. Quick check: Quantify the deviation from uniform distribution in token repetition.

## Architecture Onboarding
**Component Map**: Text -> Pivotal Statistics Computation -> GoF Test -> Detection Decision
**Critical Path**: Input text → Compute pivotal statistic → Apply GoF test → Compare to threshold → Detection output
**Design Tradeoffs**: GoF tests offer strong theoretical guarantees but require accurate distributional assumptions; simpler baselines may be faster but less robust
**Failure Signatures**: False positives occur when human text violates distributional assumptions; false negatives occur when watermarking patterns are too subtle
**First Experiments**: 1) Test GoF performance on clean vs. watermarked text at single temperature, 2) Compare GoF vs. baseline methods on edited text, 3) Analyze computational time across different GoF tests

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on English text, limiting generalizability to other languages
- Performance under adversarial attacks or sophisticated text modifications remains unexplored
- Real-world deployment considerations with mixed human-AI content streams require further investigation

## Confidence
- High confidence: GoF tests outperform baselines across tested conditions
- Medium confidence: Robustness to common and information-rich edits
- Medium confidence: Unique advantage at low temperatures due to text repetition

## Next Checks
1. Test GoF watermark detection performance on multilingual datasets to assess cross-language applicability
2. Evaluate resistance to adversarial attacks designed to evade GoF-based detection
3. Implement and benchmark GoF-based watermark detection in real-world scenarios with mixed human-AI generated content streams