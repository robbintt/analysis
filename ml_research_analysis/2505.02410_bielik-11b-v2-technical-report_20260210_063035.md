---
ver: rpa2
title: Bielik 11B v2 Technical Report
arxiv_id: '2505.02410'
source_url: https://arxiv.org/abs/2505.02410
tags:
- bielik
- polish
- language
- bielik-11b-v2
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This technical report introduces Bielik 11B v2, a state-of-the-art
  Polish language model built on Mistral 7B v0.2 architecture and scaled to 11B parameters
  using depth up-scaling. The model incorporates two key technical innovations: Weighted
  Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction
  types by assigning quality-based weights to training examples, and Adaptive Learning
  Rate, which dynamically adjusts based on context length.'
---

# Bielik 11B v2 Technical Report

## Quick Facts
- arXiv ID: 2505.02410
- Source URL: https://arxiv.org/abs/2505.02410
- Reference count: 27
- Primary result: Polish language model achieving 65.71 on Open PL LLM Leaderboard and 8.56 on Polish MT-Bench

## Executive Summary
Bielik 11B v2 represents a significant advancement in Polish language modeling, leveraging Mistral 7B v0.2 architecture scaled to 11B parameters through depth up-scaling. The model introduces two key technical innovations: Weighted Instruction Cross-Entropy Loss for optimized learning across diverse instruction types, and Adaptive Learning Rate that dynamically adjusts based on context length. Trained on a comprehensive 198 billion token corpus, Bielik 11B v2 demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities, outperforming many larger models on tasks ranging from linguistic understanding to complex reasoning.

## Method Summary
The Bielik 11B v2 model builds upon the Mistral 7B v0.2 architecture through depth up-scaling to 11B parameters. The training process incorporates two innovative techniques: Weighted Instruction Cross-Entropy Loss, which assigns quality-based weights to training examples across diverse instruction types, and Adaptive Learning Rate, which dynamically adjusts based on context length. The model was trained on a comprehensive corpus of 198 billion tokens, optimizing for both Polish language proficiency and cross-lingual capabilities. This approach enables the model to achieve strong performance while maintaining computational efficiency compared to larger models.

## Key Results
- Achieved 65.71 score on Open PL LLM Leaderboard, establishing new benchmarks for Polish language models
- Scored 8.56 on Polish MT-Bench, outperforming specialized Polish language models
- Demonstrated superior performance compared to models with 2-6× more parameters across multiple tasks

## Why This Works (Mechanism)
The model's success stems from its architectural foundation and innovative training techniques. By scaling Mistral 7B to 11B parameters through depth up-scaling, the model achieves greater representational capacity while maintaining efficiency. The Weighted Instruction Cross-Entropy Loss optimizes learning by assigning appropriate weights to diverse instruction types based on quality, ensuring robust performance across varied tasks. The Adaptive Learning Rate mechanism allows for more efficient training by dynamically adjusting to context length, enabling better optimization of the learning process.

## Foundational Learning
- **Depth up-scaling**: Increases model capacity by adding layers to existing architecture - needed for enhanced representational power while maintaining efficiency
- **Weighted Cross-Entropy Loss**: Assigns different weights to training examples based on quality - needed to optimize learning across diverse instruction types
- **Adaptive Learning Rate**: Dynamically adjusts learning rate based on context length - needed for efficient training across varying input sizes
- **Cross-lingual optimization**: Training approach that balances language-specific and multilingual capabilities - needed for strong performance in both Polish and cross-lingual tasks

## Architecture Onboarding

**Component Map**: Tokenizer -> Embedding Layer -> Transformer Blocks (11B params) -> Weighted Loss Function -> Adaptive Learning Rate Scheduler

**Critical Path**: Input tokens → Tokenizer → Embedding → Transformer Layers → Output Layer → Weighted Cross-Entropy Loss

**Design Tradeoffs**: 
- Depth up-scaling vs parameter efficiency (11B vs larger models)
- Weighted loss vs uniform training (quality optimization vs simplicity)
- Adaptive learning rate vs fixed scheduling (efficiency vs stability)

**Failure Signatures**:
- Performance degradation on Polish-specific tasks indicates inadequate weighting in instruction loss
- Context length handling issues suggest adaptive learning rate misconfiguration
- Cross-lingual capability gaps may indicate imbalanced training data distribution

**First Experiments**:
1. Evaluate weighted vs uniform loss performance on Polish linguistic tasks
2. Test adaptive learning rate effectiveness across varying context lengths
3. Benchmark cross-lingual performance against specialized models

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization of performance improvements to real-world applications remains unclear
- Potential biases in training data not comprehensively addressed
- Computational efficiency claims relative to larger models need independent verification

## Confidence
- **High Confidence**: Architectural modifications and training methodology are well-documented and technically sound
- **Medium Confidence**: Benchmark performance metrics on Polish-specific tasks, though standardized evaluation protocols are lacking
- **Low Confidence**: Claims of outperforming models with 2-6× more parameters across all tasks due to limited comparative analysis

## Next Checks
1. Independent replication of Open PL LLM Leaderboard and Polish MT-Bench results using standardized protocols
2. Analysis of model performance across different Polish dialects and informal language registers
3. Comprehensive bias and fairness audit of the training corpus