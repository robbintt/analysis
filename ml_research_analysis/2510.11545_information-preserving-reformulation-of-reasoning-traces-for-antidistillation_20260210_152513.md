---
ver: rpa2
title: Information-Preserving Reformulation of Reasoning Traces for Antidistillation
arxiv_id: '2510.11545'
source_url: https://arxiv.org/abs/2510.11545
tags:
- reasoning
- traces
- part
- original
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes PART, an information-preserving antidistillation
  reformulation method for reasoning traces in large language models. The core idea
  is to disrupt unauthorized knowledge distillation while retaining human readability
  by modifying reasoning traces at two levels: removing self-talk behaviors at the
  token level and reordering sub-conclusions before their reasoning processes at the
  structural level.'
---

# Information-Preserving Reformulation of Reasoning Traces for Antidistillation

## Quick Facts
- arXiv ID: 2510.11545
- Source URL: https://arxiv.org/abs/2510.11545
- Authors: Jiayu Ding; Lei Cui; Li Dong; Nanning Zheng; Furu Wei
- Reference count: 16
- Primary result: PART degrades distilled student performance by up to 13.5% while preserving human readability

## Executive Summary
This paper introduces PART, a method for reformulating reasoning traces in large language models to prevent unauthorized knowledge distillation while maintaining human readability. The approach operates at two levels: removing self-talk behaviors (like "Hmm" or "Wait") at the token level and reordering sub-conclusions before their reasoning processes at the structural level. A small auxiliary model fine-tuned on paired original-reformulated data performs these transformations with minimal overhead. Experiments demonstrate that PART consistently degrades the performance of distilled student models across various sizes (up to 32B parameters) and tasks, with a 13.5% performance drop observed on AIME 2024, while maintaining high lexical and semantic similarity to original traces.

## Method Summary
PART works by reformulating reasoning traces through two key transformations. First, it removes self-talk tokens (e.g., "Hmm," "Wait") that generate disproportionately large gradients during supervised fine-tuning, disrupting the distillation process. Second, it reorders sub-conclusions to appear before their supporting reasoning processes, breaking the sequential chain-of-thought structure that LLMs rely on for generation. A small auxiliary model (Qwen2.5-1.5B-Instruct) is fine-tuned to perform these reformulations with minimal overhead. The reformulated traces are then served alongside unchanged final answers, preserving human utility while degrading student model performance when used for distillation training.

## Key Results
- PART degrades distilled student performance by up to 13.5% on AIME 2024 benchmark
- Reformulated traces maintain 91% lexical match ratio at threshold 0.7
- Semantic similarity between original and reformulated traces is 0.950 cosine
- Auxiliary model adds only ~4% computational overhead with <1% additional parameters

## Why This Works (Mechanism)

### Mechanism 1: Gradient Disruption via Self-Talk Removal
Removing self-talk behaviors disrupts distillation by eliminating tokens that generate disproportionately large gradients during supervised fine-tuning. Self-talk tokens are consistently low-probability for student models, resulting in large parameter updates. By removing them, the training signal that would otherwise drive significant parameter updates is attenuated.

### Mechanism 2: Structural Perturbation via Sub-Conclusion Reordering
Reordering reasoning traces to place sub-conclusions before their reasoning processes weakens distillation effectiveness. While humans can understand reasoning with a conclusion-before-process structure, LLMs rely on sequential chain-of-thought structures to generate correct conclusions due to limited single-step computation capacity. This reordering breaks the sequential dependency patterns that supervised fine-tuning exploits.

### Mechanism 3: Information-Preserving Transformation via Auxiliary Model
A small auxiliary model (Qwen2.5-1.5B-Instruct) fine-tuned on paired original-reformulated data performs the reformulation efficiently. This specialized model applies the two-step transformation (remove self-talk, reorder sub-conclusions) with minimal latency while ensuring the output remains informative for human readers.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) Gradient Dynamics**
  - Why needed here: Understanding that SFT updates model parameters based on prediction error is crucial. Tokens the student model is unlikely to predict (low probability) generate large gradients, which PART exploits by removing specific high-gradient tokens.
  - Quick check question: If a student model already predicts a token with 99% probability, will removing that token from the training data significantly change its parameters during SFT?

- **Chain-of-Thought (CoT) in Reasoning Models**
  - Why needed here: Reasoning models like DeepSeek-R1 rely on generating extended step-by-step traces to improve final answer quality. PART exploits the sequential dependency inherent in how LLMs generate these traces, contrasting it with how humans consume them.
  - Quick check question: Why does forcing a model to generate its final answer before its reasoning steps typically degrade its performance on complex tasks?

- **Knowledge Distillation Attack Surface**
  - Why needed here: Understanding the attack surface is fundamental to understanding the defense. The attack surface in this context is the set of reasoning traces exposed via an API that attackers use as high-quality training data.
  - Quick check question: A model provider's API exposes only the final answer, not the reasoning trace. Is this API vulnerable to the same type of reasoning distillation attack described in the paper?

## Architecture Onboarding

- **Component map:** User Query -> Teacher Model (generates raw_trace + answer) -> Reformulation Module (processes raw_trace into reformulated_trace) -> Serving Layer (outputs reformulated_trace + answer) -> End User
- **Critical path:** The primary data flow is: `User Query` -> `Teacher Model` (generates `raw_trace` + `answer`) -> `Reformulation Module` (processes `raw_trace` into `reformulated_trace`) -> `Serving Layer` (outputs `reformulated_trace` + `answer`) -> `End User`. The security goal is achieved on this path.
- **Design tradeoffs:**
  - Defense Strength vs. Information Preservation: More aggressive reformulation would be a stronger defense but would provide less value to the user
  - Reformulation Quality vs. Overhead: Using larger models for reformulation would improve quality but add significant latency and cost
  - Detectability: Reformulated traces have a distinctive statistical signature (low self-talk frequency), which is beneficial for identifying misuse but could theoretically be used by attackers to reverse-engineer the defense
- **Failure signatures:**
  1. Information Loss: Reformulated traces are judged by users to be unhelpful, vague, or missing critical steps
  2. Insufficient Distillation Degradation: Attacker's student model trained on reformulated traces achieves performance equivalent to one trained on original traces
  3. Operational Failure: Auxiliary reformulation model is too slow or fails to correctly apply transformations
- **First 3 experiments:**
  1. Distill a student model (e.g., Qwen2.5-7B) on a dataset of original reasoning traces from a teacher model, then separately on reformulated versions of the same dataset. Evaluate both students on benchmarks and compare performance drops.
  2. Generate a test set of original and PART-reformulated traces, compute semantic similarity using embedding models, and conduct human evaluation where annotators blind to the source rate informativeness of both versions.
  3. Deploy the auxiliary reformulation model in staging environment, measure added latency (p50, p95, p99) and compute cost per request compared to serving raw output, validating overhead is within claimed ~4% target.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the work:

1. How does PART perform against reinforcement learning-based distillation strategies that optimize for reward signals rather than next-token likelihood?

2. Can an adaptive adversary reverse the reformulation to restore distillation effectiveness by training models to handle reordered traces or reconstruct missing self-talk tokens?

3. Does the structural perturbation remain effective as student model scale increases beyond 32B parameters, given that larger models may have greater context processing capabilities?

## Limitations

- The defense assumes self-talk tokens are universally low-probability across all student model architectures and sizes, but this may vary significantly between model families or training regimes.
- The structural reordering defense assumes human comprehension is robust to non-sequential presentation of reasoning, but this may not hold for highly complex mathematical or logical chains where order is semantically critical.
- The information preservation metrics rely on automatic lexical and semantic similarity measures, which may not capture subtle degradations in reasoning quality that humans could detect.

## Confidence

- **Antidistillation effectiveness (High confidence):** Empirical results show consistent performance degradation across multiple student model sizes and diverse benchmarks with substantial magnitude effects.
- **Information preservation (Medium confidence):** High automated similarity metrics are reported, but human evaluation covers limited sample sizes and may not fully capture reasoning quality degradation.
- **Auxiliary model efficiency (High confidence):** Specific computational overhead and parameter count claims are measurable and can be independently validated.
- **Defense mechanism universality (Low confidence):** The paper does not test PART against adaptive attackers or explore whether the defense works equally well across different reasoning model architectures.

## Next Checks

1. **Adaptive Attacker Scenario:** Train a student model on reformulated traces but include an additional fine-tuning stage where the model is exposed to both original and reformulated traces simultaneously. Measure whether this "adaptive" student can recover the performance lost to PART.

2. **Cross-Architecture Generalization:** Apply PART to reasoning traces from different model families (e.g., OpenAI's o-series, Claude's reasoning models) and test whether student models distilled from these reformulated traces show similar performance degradation.

3. **Human Comprehension Stress Test:** Conduct a controlled study where human subjects must solve complex reasoning problems after studying either original or PART-reformulated traces. Measure solution accuracy, time-to-solution, and subjective comprehension ratings to determine if there are edge cases where information preservation breaks down.