---
ver: rpa2
title: 'ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly'
arxiv_id: '2509.02949'
source_url: https://arxiv.org/abs/2509.02949
tags:
- question
- task
- step
- attach
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMQA-Assembly, a new multimodal QA dataset
  focused on assembly tasks. The dataset includes 391 QA pairs that require understanding
  human activity recordings and instruction manuals in an online-style manner.
---

# ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly

## Quick Facts
- **arXiv ID**: 2509.02949
- **Source URL**: https://arxiv.org/abs/2509.02949
- **Reference count**: 40
- **Primary result**: New multimodal QA dataset with 391 pairs focusing on assembly tasks, showing proprietary models perform only on par with text-only models

## Executive Summary
This paper introduces ProMQA-Assembly, a new multimodal QA dataset focused on assembly tasks. The dataset includes 391 QA pairs that require understanding human activity recordings and instruction manuals in an online-style manner. To develop the dataset, the authors used a semi-automated QA annotation approach where LLMs generate candidates and humans verify them. They also created instruction task graphs for 78 toys in Assembly101. The benchmarking experiments showed that even strong proprietary models perform only on par with text-only models, indicating significant room for improvement in multimodal models' vision understanding and reasoning over multiple modalities. The dataset and benchmarking results can contribute to the development of procedural-activity assistants.

## Method Summary
The ProMQA-Assembly dataset was developed using a semi-automated QA annotation approach where LLMs generate candidate questions and answers which are then verified by human annotators. The dataset consists of 391 QA pairs requiring understanding of human activity recordings and instruction manuals in an online-style manner. Additionally, the authors created instruction task graphs for 78 toys from the Assembly101 dataset to support procedural reasoning tasks. The multimodal nature combines visual and textual information, requiring models to reason across both modalities to answer procedural questions about assembly tasks.

## Key Results
- ProMQA-Assembly dataset contains 391 QA pairs focused on assembly tasks
- Proprietary multimodal models perform only on par with text-only models on the dataset
- Significant room for improvement exists in multimodal models' vision understanding and reasoning capabilities
- Dataset includes instruction task graphs for 78 toys from Assembly101

## Why This Works (Mechanism)
The dataset works by creating a challenging multimodal reasoning task that requires understanding both visual demonstrations of assembly activities and corresponding textual instructions. The semi-automated annotation process ensures high-quality QA pairs while maintaining scalability. The combination of human activity recordings with instruction manuals creates a realistic procedural task scenario that demands reasoning across multiple modalities. The dataset's focus on toys provides controlled experimental conditions while still requiring complex multimodal understanding.

## Foundational Learning
- **Multimodal QA**: Understanding questions requiring information from multiple input types (why needed: assembly tasks inherently combine visual and textual information; quick check: verify QA pairs require both modalities for correct answers)
- **Procedural Reasoning**: Understanding sequences of actions and their dependencies (why needed: assembly tasks follow specific step sequences; quick check: task graphs correctly represent procedural dependencies)
- **Vision-Language Integration**: Combining visual and textual information for coherent understanding (why needed: models must reason across both modalities; quick check: models cannot answer questions using only one modality)
- **Semi-automated Annotation**: Using LLMs for candidate generation followed by human verification (why needed: ensures scalability while maintaining quality; quick check: human verification catches LLM errors)
- **Online-style Learning**: Processing information sequentially as it becomes available (why needed: mimics real-time assembly scenarios; quick check: models can handle temporal dependencies)

## Architecture Onboarding

**Component Map**: Video Processing -> Visual Feature Extraction -> Text Processing -> Instruction Parsing -> Multimodal Fusion -> QA Answering

**Critical Path**: Visual input → Feature extraction → Multimodal fusion → Answer generation. The bottleneck is typically multimodal fusion, where models must effectively combine visual and textual information.

**Design Tradeoffs**: 
- Small dataset size vs. annotation quality (prioritized quality over quantity)
- Toy assembly focus vs. real-world applicability (prioritized controlled experimental conditions)
- Semi-automated vs. fully manual annotation (prioritized scalability while maintaining quality)

**Failure Signatures**: 
- Models relying too heavily on either visual or textual information
- Inability to track temporal dependencies in assembly sequences
- Failure to ground visual observations in textual instructions
- Overfitting to toy-specific assembly patterns

**3 First Experiments**:
1. Evaluate baseline text-only models on ProMQA-Assembly to establish multimodal advantage threshold
2. Test models with visual inputs only to assess vision understanding capabilities
3. Analyze attention patterns in multimodal models to understand modality integration

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 391 QA pairs is relatively small, potentially limiting generalizability across diverse assembly tasks
- Semi-automated annotation process relying on LLM generation may introduce biases in question-answer pair selection
- Focus on toys in Assembly101 provides controlled conditions but may not translate to complex industrial assembly tasks
- Benchmarking results based on limited model comparisons may not reflect performance across all multimodal architectures

## Confidence

**Dataset construction methodology**: High - The semi-automated approach with human verification is well-documented and reproducible

**Performance gap findings**: Medium - Results are based on limited model comparisons and may not generalize to all multimodal architectures

**Applicability to real-world assembly**: Low - Current dataset focuses on toys, limiting direct industrial relevance

## Next Checks

1. Expand dataset size and diversity by including non-toy assembly tasks and varying complexity levels
2. Conduct cross-validation with multiple LLM models and human annotators to assess annotation consistency
3. Test model performance on real-time assembly scenarios with varying lighting, camera angles, and environmental conditions