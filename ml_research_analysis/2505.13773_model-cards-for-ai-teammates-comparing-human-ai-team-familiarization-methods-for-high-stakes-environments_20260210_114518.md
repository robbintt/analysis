---
ver: rpa2
title: 'Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods
  for High-Stakes Environments'
arxiv_id: '2505.13773'
source_url: https://arxiv.org/abs/2505.13773
tags:
- agent
- participants
- team
- human
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared three methods of familiarizing humans with
  an AI teammate in a high-stakes ISR environment: reading documentation, in-situ
  training, or no familiarization. Participants in documentation and in-situ conditions
  developed more sophisticated team strategies, with documentation users showing faster
  strategy adoption but risk-averse behavior that limited high scores.'
---

# Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments

## Quick Facts
- arXiv ID: 2505.13773
- Source URL: https://arxiv.org/abs/2505.13773
- Reference count: 25
- Participants familiarized through documentation adopted strategies faster but showed risk-averse behavior limiting high scores, while in-situ training enabled experimentation with higher outcome variance.

## Executive Summary
This study compared three methods of familiarizing humans with an AI teammate in a high-stakes ISR environment: reading documentation, in-situ training, or no familiarization. Participants in documentation and in-situ conditions developed more sophisticated team strategies, with documentation users showing faster strategy adoption but risk-averse behavior that limited high scores. In-situ participants were more willing to experiment with control modes but had weaker understanding of AI internal processes. The most valuable information about the agent included decision-making algorithms and relative strengths/weaknesses compared to humans. Documentation-based familiarization led to more consistent but lower-scoring performance, while in-situ training resulted in higher variability with potential for top scores. The study recommends combining AI documentation, structured in-situ training, and exploratory interaction for effective human-AI team familiarization.

## Method Summary
The study used a custom Pygame ISR environment where human and AI agents collaboratively identify targets and weapons. Sixty participants (ages 18-39, minimum 1 hr/week gaming experience) were randomly assigned to three between-subjects conditions: documentation (6-page model card), in-situ training (training round with agent), or control (minimal familiarization). All completed 4 rounds × 4 minutes with SAGAT pauses for situation awareness assessment. Performance metrics included score (target IDs, weapon IDs, HP/time bonuses, destruction penalties), control mode usage, risk distribution, NASA TLX workload, trust Likert scale, and AI understanding quiz. Exit interviews provided qualitative insights on strategy development.

## Key Results
- Documentation users adopted convergent strategies fastest but showed risk-averse behavior preventing high scores
- In-situ participants experimented more with control modes but had weaker mental models of AI internal processes
- Knowledge of AI decision-making and relative strengths/weaknesses enabled effective task allocation strategies
- Control group used "search-and-destroy" strategy with low scores due to failure to leverage AI capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Documentation-based familiarization accelerates strategy adoption but induces risk-averse behavior that limits peak performance.
- **Mechanism:** Explicit knowledge transfer through model cards creates accurate mental models of AI internal processes, enabling rapid strategy formation. However, this same explicit knowledge appears to anchor users to documented behaviors, reducing willingness to explore alternative control modes or riskier strategies.
- **Core assumption:** The risk aversion stems from documentation content itself rather than individual participant characteristics.
- **Evidence anchors:**
  - [abstract] "Documentation-based familiarization led to the fastest adoption of these strategies, but also biased participants towards risk-averse behavior that prevented high scores."
  - [Section V-A] "8 Documentation participants also reported needing 1-2 rounds before they could operationalize the model card's information."
  - [Section VI-A] "documentation-based familiarization caused the fastest adoption of the convergent strategy, it also led to an over-fixation on this strategy, which prevented many participants from reaching higher scores."
  - [corpus] Related work on human-AI team perception (arxiv:2501.15332) suggests human-like AI teammates can degrade performance—documentation may amplify anthropomorphic expectations that don't match actual AI behavior.
- **Break condition:** If documentation were designed to explicitly encourage experimentation (e.g., "try Auto mode in low-risk scenarios"), risk aversion might decrease.

### Mechanism 2
- **Claim:** In-situ training enables behavioral inference and experimentation, producing higher outcome variance with potential for top scores.
- **Mechanism:** Direct interaction during a no-stakes training round allows users to observe AI behavior patterns, experiment with control modes, and develop tacit knowledge. This experiential learning produces less accurate mental models of internal processes but greater practical fluency.
- **Core assumption:** The training round's low-risk nature is critical; if stakes were introduced, exploration would decrease.
- **Evidence anchors:**
  - [abstract] "Participants familiarized through direct interaction were able to infer much of the same information through observation, and were more willing to take risks and experiment with different control modes, but reported weaker understanding of the agent's internal processes."
  - [Section V-A] "8 In-situ participants reported that Auto mode was effective (compared to 2 in other groups). Many In-situ participants used the training round to experiment with Auto mode."
  - [Section VI-A] "exploratory interaction may be a crucial element of human-AI familiarization, allowing the human to probe the agent's behavior and develop their own interaction style in a low-threat setting."
  - [corpus] Weak direct corpus support for in-situ training specifically; neighbor papers focus on AI perception and team dynamics rather than training methods.
- **Break condition:** Breaks if the AI's behavior is not consistently observable (e.g., highly stochastic policies where single interactions mislead).

### Mechanism 3
- **Claim:** Knowledge of AI decision-making processes and relative strengths/weaknesses enables sophisticated task allocation strategies.
- **Mechanism:** Understanding what the AI does well (e.g., resilience to weapon damage) and how it prioritizes (e.g., target selection heuristics) allows humans to delegate appropriately and coordinate spatially. This mirrors human-human team specialization.
- **Core assumption:** The AI has stable, exploitable strengths/weaknesses; if capabilities change dynamically or are opaque, this mechanism weakens.
- **Evidence anchors:**
  - [abstract] "The most valuable information about the agent included details of its decision-making algorithms and its relative strengths and weaknesses compared to humans."
  - [Section V-A] "knowledge of the agent's strengths and weaknesses (specifically, that it could avoid weapon damage) allowed participants to allocate team tasks effectively."
  - [Section VII] "This mirrors human-human teams – teams often specialize along what each member is good at."
  - [corpus] arxiv:2503.15516 finds measurable AI properties affect human teammate preferences, supporting the importance of capability transparency.
- **Break condition:** Breaks if documentation is overly technical (participants found heuristic formulas "too complex to be useful during gameplay").

## Foundational Learning

- **Concept: Mental Model Accuracy vs. Practical Fluency**
  - **Why needed here:** The study reveals a tradeoff between understanding AI internals (documentation) and practical interaction skills (in-situ). Engineers designing onboarding must decide which to prioritize—or how to combine both.
  - **Quick check question:** Can you explain why a user might perform well with an AI teammate despite failing a quiz about its decision-making?

- **Concept: Risk Asymmetry in Human-AI Teams**
  - **Why needed here:** The ISR task created asymmetric risk (AI more resilient than human), driving strategy. Real systems often have similar asymmetries that familiarization must surface.
  - **Quick check question:** In your system, what risks can the AI absorb that humans cannot, and vice versa?

- **Concept: Control Granularity and Autonomy Levels**
  - **Why needed here:** Three control modes (Auto, Priorities, Override) produced different outcomes. Understanding when users switch modes—and why—is essential for interface design.
  - **Quick check question:** What triggers a user in your system to shift from supervisory to manual control?

## Architecture Onboarding

- **Component map:** Documentation (model cards) -> In-situ training module -> Interaction layer with control modes -> Observability layer (AI behavior transparency) -> Feedback layer (performance metrics + SAGAT-style queries)

- **Critical path:**
  1. Pre-deployment: Create model card with decision-making algorithms + strengths/weaknesses (avoid overly technical formulas)
  2. Onboarding Phase 1: Documentation review (6 pages in study; adjust for complexity)
  3. Onboarding Phase 2: Structured in-situ training with no-stakes scenarios
  4. Onboarding Phase 3: Exploratory interaction period before operational use
  5. Ongoing: Monitor control mode usage patterns and risk distribution

- **Design tradeoffs:**
  - **Documentation depth vs. usability:** Participants ignored complex heuristic formulas. Favor qualitative descriptions over mathematical precision.
  - **Training safety vs. realism:** No-risk training encourages experimentation but may not transfer to high-stakes conditions. Consider graduated stress exposure.
  - **Control flexibility vs. cognitive load:** 7 participants over-used waypoint micromanagement and neglected their own tasks. Constrain control options or add guardrails.

- **Failure signatures:**
  - **Micromanagement spiral:** Excessive Override mode usage, human aircraft taking damage, high frustration scores (correlated in Section V-E)
  - **Black-box attribution:** Users describing AI in generic terms without behavioral specifics (4 Control participants)
  - **Risk delegation without understanding:** Assigning all risky tasks to AI without verifying resilience claims (led to consistent ~1300 point scores, missing time bonuses)

- **First 3 experiments:**
  1. **A/B test familiarization sequences:** Compare (documentation→in-situ) vs. (in-situ→documentation) ordering. Hypothesis: Starting with in-situ may reduce documentation-induced risk aversion while preserving mental model accuracy.
  2. **Control mode instrumentation:** Log mode switches with context (map state, HP, time remaining). Identify patterns that predict high vs. low performers to create adaptive guidance.
  3. **Model card simplification study:** Test abbreviated model cards (2 pages, strengths/weaknesses only) vs. full technical documentation. Measure strategy adoption speed, risk behavior, and quiz scores.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Would a hybrid familiarization approach combining documentation, in-situ training, and exploratory interaction outperform single-method approaches in both speed of strategy adoption and score potential?
- **Basis in paper:** [explicit] The authors conclude by recommending "a human-AI team familiarization method that combines AI documentation, structured in-situ training, and exploratory interaction," but this hybrid approach was not tested in the study.
- **Why unresolved:** The study only compared three distinct conditions (documentation, in-situ, and control) in isolation, not in combination.
- **What evidence would resolve it:** A follow-up between-subjects study adding a fourth condition where participants receive documentation AND in-situ training, comparing performance, strategy adoption speed, and score variance against the three original conditions.

### Open Question 2
- **Question:** Does the micromanagement behavior observed in some participants correlate with specific personality traits, and is this behavior prevalent among professionals in actual high-stakes domains?
- **Basis in paper:** [explicit] The authors state they are "interested in the root cause of this micromanagement" and that "Future work may explore whether a correlation exists with certain personality traits, and whether these traits are common in people who choose careers in high-stakes domains."
- **Why unresolved:** The study identified micromanagement behavior but did not collect personality data or recruit domain experts.
- **What evidence would resolve it:** A study with personality assessments (e.g., Big Five, need for control scales) and recruitment of actual ISR/military operators.

### Open Question 3
- **Question:** How should familiarization materials be adapted based on users' prior AI/ML expertise to address both expert preconceptions and non-expert misconceptions?
- **Basis in paper:** [inferred] The paper found that AI/ML experts incorrectly applied preconceptions while non-experts held common misconceptions, and states "System designers should address both types of misconceptions during the training process."
- **Why unresolved:** The study used identical documentation for all participants and did not test adapted materials.
- **What evidence would resolve it:** A study comparing performance of AI/ML experts and non-experts given either standard or expertise-tailored documentation.

## Limitations
- Controlled environment and brief task duration limit generalizability to real-world high-stakes settings
- Model card effectiveness depends heavily on specific content and structure not fully detailed in paper
- Between-subjects design with n=60 participants may not capture individual learning trajectories or long-term adaptation effects

## Confidence
- **High Confidence:** Documentation accelerates strategy adoption but induces risk aversion; in-situ training enables experimentation but produces weaker mental models; knowledge of AI strengths/weaknesses enables effective task allocation
- **Medium Confidence:** The optimal familiarization sequence combines documentation, in-situ training, and exploratory interaction; AI/ML expertise interferes with accurate AI understanding; risk asymmetry drives strategy differences
- **Low Confidence:** Documentation content can be generalized across domains; in-situ training transfers to high-stakes conditions; specific control mode usage patterns predict performance

## Next Checks
1. **Generalizability Test:** Replicate the study with different AI teammate capabilities (e.g., different risk profiles, different control granularity) to verify if documentation-induced risk aversion persists across domains.
2. **Longitudinal Adaptation Study:** Extend familiarization to multiple sessions over days/weeks to measure whether initial familiarization method effects persist or evolve with continued interaction.
3. **Context Transfer Experiment:** Test whether in-situ training in low-stakes scenarios transfers to high-stakes operational conditions, or if stress exposure requires graduated familiarization protocols.