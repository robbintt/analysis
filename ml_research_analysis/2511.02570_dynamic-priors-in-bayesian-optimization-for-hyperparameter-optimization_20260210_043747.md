---
ver: rpa2
title: Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization
arxiv_id: '2511.02570'
source_url: https://arxiv.org/abs/2511.02570
tags:
- priors
- regret
- optimization
- prior
- dynabo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DynaBO, a Bayesian optimization framework
  that allows users to dynamically inject prior knowledge during hyperparameter optimization.
  Unlike prior work, which restricts user input to initialization, DynaBO enables
  repeated interventions at runtime, combining prior-weighted acquisition functions
  with a safeguard to detect and reject misleading priors.
---

# Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization

## Quick Facts
- arXiv ID: 2511.02570
- Source URL: https://arxiv.org/abs/2511.02570
- Reference count: 40
- Key outcome: DynaBO enables dynamic prior injection during HPO, outperforming state-of-the-art methods across diverse benchmarks.

## Executive Summary
This paper introduces DynaBO, a Bayesian optimization framework that allows users to dynamically inject prior knowledge during hyperparameter optimization. Unlike prior work that restricts user input to initialization, DynaBO enables repeated interventions at runtime, combining prior-weighted acquisition functions with a safeguard to detect and reject misleading priors. The method generalizes πBO by stacking multiple decaying priors while preserving theoretical convergence guarantees. Empirically, DynaBO outperforms state-of-the-art competitors across diverse HPO benchmarks—including deep learning models—for all prior types, from expert to adversarial.

## Method Summary
DynaBO extends Bayesian optimization by allowing dynamic injection of prior knowledge during the optimization process. The core innovation is a time-decaying weighted acquisition function that combines expected improvement with multiple user-provided priors. Each prior is represented as a Normal distribution over specific hyperparameters and is applied at runtime with decreasing influence. A rejection mechanism safeguards against misleading priors by comparing the acquisition function values of configurations sampled from the prior region against those from the incumbent region. The framework is implemented using SMAC3 with a Random Forest surrogate model and is tested on tabular benchmarks (YAHPO, PD1) across various configuration spaces.

## Key Results
- DynaBO consistently outperforms vanilla BO and πBO across all prior types (Expert, Random, Worst, Adversarial) on multiple benchmarks
- The rejection mechanism successfully identifies and discards misleading priors without significant performance loss
- DynaBO maintains theoretical convergence guarantees while providing practical performance improvements in real-world scenarios
- Performance gains are particularly notable for deep learning models including ResNet and Transformer architectures

## Why This Works (Mechanism)
DynaBO works by leveraging user-provided priors as soft constraints that guide the search toward promising regions of the configuration space. The key insight is that priors can be made dynamic—injected and updated during optimization—rather than static initializations. By decaying the influence of priors over time and stacking multiple priors, the method balances exploration with exploitation while remaining robust to potentially incorrect user inputs through its rejection mechanism.

## Foundational Learning
- **Dynamic prior injection**: Allows users to provide guidance during optimization rather than just at initialization, making the process more interactive and responsive to emerging insights.
- **Time-decaying prior weights**: Prevents premature convergence to suboptimal regions by gradually reducing prior influence as optimization progresses and more data becomes available.
- **Prior rejection safeguards**: Detects and discards misleading priors by comparing acquisition function values, ensuring the optimization remains robust to incorrect user input.

## Architecture Onboarding
**Component Map:** User Prior -> Rejection Mechanism -> Prior-Weighted Acquisition -> SMAC3 Optimizer -> Surrogate Model -> Function Evaluation

**Critical Path:** User injects prior → Rejection mechanism evaluates prior → Prior-weighted acquisition function computed → SMAC3 selects next configuration → Function evaluated → Surrogate updated

**Design Tradeoffs:** The framework trades computational overhead (evaluating rejection mechanism, maintaining multiple priors) for improved convergence and robustness. The rejection mechanism adds a small computational cost but prevents potentially catastrophic performance degradation from misleading priors.

**Failure Signatures:** Performance matching vanilla BO indicates over-rejection of priors; numerical instability in acquisition function suggests improper prior clipping; poor convergence may indicate insufficient prior influence or incorrect timing of prior injection.

**First Experiments:**
1. Implement SMAC3 with Random Forest surrogate and verify basic BO functionality
2. Add the dynamic acquisition function with single prior and test numerical stability
3. Implement rejection mechanism and test on synthetic prior scenarios

## Open Questions the Paper Calls Out
- How can DynaBO be effectively extended to multi-fidelity optimization scenarios?
- How does the optimization performance change when priors are sourced from real human users rather than synthetic heuristics?
- Can Large Language Models (LLMs) effectively automate the generation of dynamic priors to support low-code user interfaces?

## Limitations
- Prior construction methodology is not fully specified, making exact replication difficult
- Proprietary PD1 benchmark limits independent verification of some results
- Implementation details in SMAC3 affect acquisition function optimization but are not fully documented

## Confidence
- High Confidence: Core algorithmic framework and theoretical guarantees
- Medium Confidence: Empirical methodology and benchmark selection
- Low Confidence: Exact reproduction of prior distributions and their effects

## Next Checks
1. Implement and test the prior clipping mechanism $[10^{-12}, 1]$ across all benchmarks
2. Systematically vary the rejection threshold $\tau$ from -0.05 to -0.25 to characterize robustness trade-offs
3. Conduct ablation studies testing prior injection at different iteration intervals (early vs. late)