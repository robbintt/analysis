---
ver: rpa2
title: Discovering and Learning Probabilistic Models of Black-Box AI Capabilities
arxiv_id: '2512.16733'
source_url: https://arxiv.org/abs/2512.16733
tags:
- capability
- state
- learning
- agent
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PCML, a framework for learning interpretable
  probabilistic models of black-box AI (BBAI) capabilities. PCML discovers capabilities
  by actively querying a BBAI with distinguishing policies generated via Monte Carlo
  Tree Search, aiming to expose disagreements between optimistic and pessimistic capability
  models.
---

# Discovering and Learning Probabilistic Models of Black-Box AI Capabilities

## Quick Facts
- arXiv ID: 2512.16733
- Source URL: https://arxiv.org/abs/2512.16733
- Authors: Daniel Bramblett; Rushang Karia; Adrian Ciotinga; Ruthvick Suresh; Pulkit Verma; YooJung Choi; Siddharth Srivastava
- Reference count: 17
- Primary result: PCML framework learns interpretable probabilistic models of BBAI capabilities via active query synthesis, achieving 60% lower variational distance than random exploration on MiniGrid and revealing surprising limitations

## Executive Summary
This paper introduces PCML, a framework for discovering and learning interpretable probabilistic models of black-box AI capabilities. PCML actively queries a BBAI by generating distinguishing policies via Monte Carlo Tree Search to expose disagreements between optimistic and pessimistic capability models. The learned models describe capabilities, their conditions, and probabilistic outcomes, enabling safer usage and design of AI systems.

## Method Summary
PCML learns probabilistic PDDL-style capability models by maintaining pessimistic and optimistic bounds that partition the state space based on observed effects. MCTS synthesizes queries as policies in a distinguishing MDP where states are distribution pairs and rewards are distance metrics. The framework iteratively executes queries, updates transition datasets, and refines model bounds until convergence or timeout. Two variants exist: PCML-E (exact distributions) and PCML-S (sample-based for efficiency).

## Key Results
- PCML achieves significantly lower variational distance than random exploration across diverse domains
- 60% lower VD on MiniGrid, 20% lower on SayCan compared to random baseline
- Reveals surprising limitations and side-effects of BBAIs while enabling safer usage and design

## Why This Works (Mechanism)

### Mechanism 1
Maintaining pessimistic and optimistic model bounds reduces uncertainty by identifying states where models disagree, enabling targeted exploration. States are partitioned by observed effects (η_c(s,D)). Pessimistic conditions accept only observed states via disjunction (pcond(S_φ) = ∨_{s∈S_φ} ℓ(s)); optimistic conditions generalize by negating other partitions. Disagreement between these bounds indicates missing data. Core assumption: The true agent model M* is expressible in the given predicate-object abstraction vocabulary.

### Mechanism 2
Framing query synthesis as a distinguishing MDP enables MCTS to systematically find capability sequences that expose model disagreements. Define MDP P† with state ⟨ρ_ψ, ρ_ω⟩ (distribution pairs), actions as capabilities, and reward as distribution distance. MCTS with UCT explores sequences where pessimistic and optimistic predictions diverge, generating queries that maximize information gain per execution. Core assumption: Distribution distance metrics (TV or symmetric difference) correlate with actual information gain about missing transitions.

### Mechanism 3
Sample-based MCTS (PCML-S) using symmetric-difference rewards achieves better computational efficiency than exact distribution tracking while preserving query quality. Instead of propagating full distributions, track only states in the intersection of support sets; reward = 1 when state enters symmetric difference (immediately distinguishable). This avoids exponential state-space materialization. Core assumption: States in symmetric difference are diagnostic and sufficient for distinguishing models without tracking exact probability masses.

## Foundational Learning

- **Probabilistic PDDL with conditional effects**
  - Why needed here: Output representation for learned capabilities; must understand preconditions, probabilistic outcomes, and disjunctive conditions
  - Quick check question: Given capability c with condition (A ∨ B) → {0.3: X, 0.7: Y}, what transitions does M |= predict from state {A, ¬B}?

- **Monte Carlo Tree Search with UCT**
  - Why needed here: Query synthesis core; must understand selection (UCT formula), expansion, rollout, and backpropagation
  - Quick check question: Why does UCT balance exploration (κ√(ln N(s)/N(s,c))) vs. exploitation (Q(s,c))?

- **Total Variation Distance**
  - Why needed here: Primary metric for model disagreement; used in distinguishing MDP reward and convergence evaluation
  - Quick check question: For two distributions P and Q over states {s1, s2, s3}, how is δ_TV(P, Q) computed?

## Architecture Onboarding

- Component map: Simulator S_E → Random Walk → Capability Discovery → Dataset D → Model Construction (pess/opt) ←→ Query Synthesis (MCTS) → State Distribution Updates → Executed Query

- Critical path: Dataset D → partition construction → BDD conditions → distinguishing MDP → MCTS policy extraction → query execution → D update. If any partition is empty or MCTS finds no distinguishing policy, falls back to random capability sequence.

- Design tradeoffs:
  - PCML-E vs PCML-S: Exact distributions (accurate but O(|S|) memory) vs. sample-based (efficient but may miss probability-mass disagreements)
  - Pessimistic vs optimistic output: Pessimistic is safer (sound wrt observed data) but may under-generalize; optimistic generalizes but may hallucinate capabilities
  - Query length Θ: Longer sequences reach more states but compound stochasticity

- Failure signatures:
  - Models never converge (Mpess ≢ Mopt after budget): Abstraction insufficient, or agent non-stationary
  - Random baseline outperforms: High stochasticity makes planned queries uninformative; check agent success rates
  - MCTS timeout: State space too large; reduce horizon or increase pruning

- First 3 experiments:
  1. Run PCML on a deterministic domain (e.g., Blocksworld) with known ground-truth PDDL; verify Mpess ≡ Mopt ≡ M* within budget.
  2. Inject controlled stochasticity (e.g., 20% action failure rate); measure VD convergence rate vs. random baseline over 10 runs.
  3. Test abstraction mismatch: use predicates that cannot express true preconditions; verify that Mpess and Mopt do not converge (confirming Theorem 3 assumption check).

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework distinguish between an agent's behavioral preferences (e.g., tie-breaking) and genuine structural constraints? The current method conflates policy preferences with capability limits, which may reduce the generalizability of the learned models. An algorithm extension that successfully decouples internal preferences from hard constraints in an agent with known policy biases would resolve this.

### Open Question 2
How can relevant intents be dynamically acquired through user interaction? The current evaluation assumes a fixed set of single-literal intents, ignoring context-specific user objectives. An interactive protocol where users define high-level goals, and the system efficiently narrows the hypothesis space to relevant capabilities would resolve this.

### Open Question 3
Can the framework be extended to handle non-stationary agents with evolving capabilities? The convergence proofs rely on the assumption that the agent's capability distribution is fixed, which may not hold for online learning agents. Empirical results showing PCML maintaining model accuracy while an agent undergoes fine-tuning or policy updates would resolve this.

## Limitations
- Abstraction Dependence: PCML's convergence critically depends on the expressiveness of the provided abstraction function
- Stochasticity Sensitivity: Low agent success rates (e.g., 6% on SayCan) diminish single-query information gain
- Computational Scalability: PCML-E's exact distribution tracking leads to exponential memory requirements in state-space size

## Confidence
- **High Confidence**: The theoretical framework (soundness, completeness, convergence) is mathematically rigorous and the experimental methodology is reproducible
- **Medium Confidence**: The empirical results showing PCML's superiority over random exploration are convincing across multiple domains
- **Low Confidence**: The practical utility of learned models for safety-critical applications depends heavily on the quality and expressiveness of the abstraction function

## Next Checks
1. **Abstraction Expressiveness Test**: Systematically evaluate PCML on domains where the abstraction is deliberately made insufficient. Verify that pessimistic and optimistic models fail to converge, confirming the framework correctly identifies abstraction limitations.

2. **Stochasticity Stress Test**: Evaluate PCML-S on a controlled environment where agent success rates vary from 10% to 90%. Measure convergence rates and variational distance reduction across this spectrum to quantify performance degradation in high-stochasticity regimes.

3. **Computational Scaling Analysis**: Benchmark both PCML-E and PCML-S on environments with progressively larger state spaces. Track memory usage, query synthesis time, and whether PCML-S maintains comparable query quality to PCML-E as state space grows.