---
ver: rpa2
title: 'Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation'
arxiv_id: '2504.06225'
source_url: https://arxiv.org/abs/2504.06225
tags:
- encoder-decoder
- gemma
- arxiv
- llms
- decoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to adapt pretrained decoder-only
  language models (LLMs) into encoder-decoder models. The authors argue that encoder-decoder
  architectures offer a better quality-efficiency trade-off due to their separate
  encoder and decoder modules, which allow for richer contextual representations and
  more flexible model sizing.
---

# Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation

## Quick Facts
- arXiv ID: 2504.06225
- Source URL: https://arxiv.org/abs/2504.06225
- Reference count: 22
- Primary result: Adapting Gemma 2 decoder-only models to encoder-decoder architecture yields better quality-efficiency trade-offs, with encoder-decoder models outperforming decoder-only counterparts on instruction tuning and SuperGLUE while maintaining similar inference latency.

## Executive Summary
This paper introduces a method to adapt pretrained decoder-only language models (LLMs) into encoder-decoder models. The authors argue that encoder-decoder architectures offer a better quality-efficiency trade-off due to their separate encoder and decoder modules, which allow for richer contextual representations and more flexible model sizing. They adapt Gemma 2 (2B and 9B) and several smaller models by reusing parameters from the decoder-only models, initializing the encoder from the decoder and cross-attention from either the self-attention weights (for balanced models) or random initialization with a warmup phase (for unbalanced models). Two pretraining objectives are compared: prefix language modeling with knowledge distillation and UL2. The adapted models are evaluated on multiple benchmarks including pretraining tasks, instruction tuning, and SuperGLUE. Results show that encoder-decoder models achieve comparable or slightly better pretraining performance but significantly better instruction-tuning performance than their decoder-only counterparts. For example, the 2B-2B model outperforms Gemma 2 2B by about 7% after instruction tuning. The adaptation method also allows for pairing large encoders with small decoders (e.g., 9B-2B), providing better quality with similar inference latency. Encoder-decoder models consistently perform better on SuperGLUE, indicating higher-quality contextual representations. The study concludes that adapting decoder-only models to encoder-decoder is more compute-efficient and effective than training from scratch, and offers a promising direction for developing high-performance LLMs with improved quality-efficiency trade-offs.

## Method Summary
The authors adapt pretrained decoder-only LLMs (specifically Gemma 2 2B and 9B) to encoder-decoder architectures by reusing and remapping parameters. The encoder is initialized from the decoder-only model with causal attention switched to bidirectional, while the decoder inherits FFN and self-attention weights and adds a cross-attention layer. For balanced configurations (equal encoder/decoder sizes), cross-attention is initialized from self-attention weights; for unbalanced configurations, cross-attention is randomly initialized with a warmup phase. Two pretraining objectives are explored: PrefixLM with knowledge distillation and UL2. The adapted models are then fine-tuned using the same instruction-tuning recipe as the original Gemma 2 models. The approach enables flexible sizing of encoder and decoder modules to optimize the quality-efficiency trade-off for different tasks.

## Key Results
- Encoder-decoder models achieve comparable or slightly better pretraining performance than decoder-only counterparts.
- Encoder-decoder models significantly outperform decoder-only models on instruction tuning benchmarks (e.g., 2B-2B outperforms Gemma 2 2B by ~7%).
- Unbalanced encoder-decoder configurations (e.g., 9B-2B) provide better quality with similar inference latency to small decoder-only models.
- Encoder-decoder models consistently outperform decoder-only models on SuperGLUE, indicating higher-quality contextual representations.
- Adaptation is more compute-efficient than training encoder-decoder models from scratch while preserving learned capabilities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting decoder-only models to encoder-decoder architectures preserves learned capabilities while enabling architectural efficiencies.
- Mechanism: Parameter transfer from pretrained decoder-only models initializes the encoder-decoder weights, providing a warm start. Specifically, the encoder inherits all decoder-only weights with causal attention switched to bidirectional. The decoder inherits FFN and self-attention weights, while cross-attention is initialized from self-attention (balanced) or trained via warmup (unbalanced).
- Core assumption: Pretrained decoder-only weights contain generalizable linguistic knowledge that transfers across architectures when appropriately mapped.
- Evidence anchors:
  - [abstract] "adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches"
  - [Section 3.2] Detailed initialization strategy: encoder fully initialized, decoder FFN/self-attn initialized, cross-attention handled separately.
  - [corpus] Weak direct corpus support; T5Gemma 2 follows a similar adaptation recipe, providing external validation of the approach.
- Break condition: If the source decoder-only model is undertrained or has poor representations, adaptation will transfer those deficiencies.

### Mechanism 2
- Claim: Bidirectional self-attention in the encoder improves contextual representation and downstream task performance compared to causal attention.
- Mechanism: Unlike decoder-only models that process tokens sequentially (causal mask), the encoder can attend to the entire input simultaneously. This allows the model to build representations that integrate context from both past and future tokens within the input sequence.
- Core assumption: Tasks involving input understanding (e.g., classification, comprehension) benefit from full contextualization available via bidirectional attention.
- Evidence anchors:
  - [Section 6] Ablation showing causal attention in the encoder underperforms bidirectional by 4.1 PT and 4.7 IT points at 2B-2B.
  - [Section 5/Table 2b] Consistent SuperGLUE improvements across all scales, attributed in discussion to "bidirectional self-attention."
  - [corpus] "Causal Reasoning Favors Encoders" (arXiv:2512.10561) supports encoder benefits for complex reasoning.
- Break condition: If tasks are purely generative with no input-context dependency, bidirectional encoding may not provide advantages.

### Mechanism 3
- Claim: Encoder-decoder models with a small decoder and large encoder (unbalanced) improve the quality-efficiency trade-off for input-heavy tasks.
- Mechanism: Inference cost in encoder-decoder models scales with decoder size and output length. By keeping the decoder small (e.g., 2B) while maintaining a larger encoder (e.g., 9B), the model retains strong input understanding but reduces the cost of autoregressive generation.
- Core assumption: Many tasks (e.g., summarization) require deep input comprehension but limited generative complexity.
- Evidence anchors:
  - [Section 1] Mentions "high flexibility in changing the encoder and decoder size (e.g., a large encoder paired with a small decoder)."
  - [Section 5/Figure 4] 9B-2B shows similar latency to Gemma 2B but significantly outperforms 2B-2B.
  - [corpus] Limited direct corpus evidence for this specific unbalanced encoder-decoder trade-off.
- Break condition: If the task requires generating long, complex outputs, a small decoder may become a bottleneck.

## Foundational Learning

- **Encoder-Decoder Architecture**:
  - Why needed here: The entire paper is about adapting to this architecture. You must understand the separation of encoder (input processing) and decoder (output generation) and the role of cross-attention.
  - Quick check question: In a standard Transformer encoder-decoder, which component's output does the cross-attention mechanism in the decoder attend to?

- **Causal vs. Bidirectional Attention**:
  - Why needed here: The switch from causal (decoder-only) to bidirectional (encoder) is a core modification claimed to improve representations.
  - Quick check question: Can a token at position 5 attend to a token at position 10 in a bidirectional attention layer? What about in a causal one?

- **Prefix Language Modeling (PrefixLM) vs. UL2**:
  - Why needed here: These are the two primary pretraining objectives compared for the adaptation. Understanding their differences (generative vs. denoising) is key to interpreting results.
  - Quick check question: Which objective, PrefixLM or UL2, is described as being more aligned with generative tasks and benefiting from knowledge distillation?

## Architecture Onboarding

- **Component map**:
  1. **Encoder**: Identical architecture to decoder-only source (e.g., Gemma 2), but self-attention is **bidirectional**. Weights initialized from source model.
  2. **Decoder**: FFN and self-attention initialized from source. Self-attention remains **causal**. Includes a new **cross-attention** layer attending to encoder outputs.
  3. **Cross-Attention**: New layer connecting encoder and decoder. Initialization depends on model configuration (from self-attention if balanced, random+warmup if unbalanced).

- **Critical path**: For an engineer, the critical path is the adaptation process:
  1. Take pretrained decoder-only checkpoint.
  2. Split into Encoder and Decoder components, copying weights.
  3. Modify Encoder attention mask to be bidirectional.
  4. Initialize Decoder cross-attention (method depends on encoder/decoder size match).
  5. Train/adapt with chosen objective (PrefixLM or UL2).

- **Design tradeoffs**:
  - **Balanced (e.g., 2B-2B) vs. Unbalanced (e.g., 9B-2B)**: Balanced allows full weight transfer (faster convergence). Unbalanced allows quality-efficiency tuning but requires cross-attention warmup and converges slower.
  - **PrefixLM vs. UL2**: PrefixLM (with distillation) better for generative IT performance. UL2 better for encoder representations (SuperGLUE). Paper notes combining them is non-trivial.
  - **GQA vs. MHA in Encoder**: Expanding to MHA possible but showed mixed results (better PT, worse IT in 2B-2B test).

- **Failure signatures**:
  - **Cross-attention warmup issues in unbalanced models**: Performance drop if warmup steps are incorrect (paper found 1K better than 0 or 5K).
  - **Slow convergence**: Unbalanced models (e.g., 9B-2B) converge slower than balanced because cross-attention is trained from scratch.
  - **Objective mismatch**: Using UL2 when generative IT performance is the primary goal may underperform compared to PrefixLM.

- **First 3 experiments**:
  1. **Reproduce the Balanced Adaptation**: Adapt a small decoder-only model (e.g., from the mT5-sized suite) to encoder-decoder using PrefixLM. Verify rapid convergence and improved IT/SuperGLUE scores over the baseline decoder-only model.
  2. **Ablate Bidirectional Attention**: Using the setup from Experiment 1, run an ablation keeping the encoder attention causal. Measure the drop in performance (should be significant, ~4 PT/IT points).
  3. **Test Unbalanced Configuration**: Create a model with a larger encoder and smaller decoder. Experiment with the cross-attention warmup schedule (e.g., 0, 1K, 5K steps) on a subset of data to identify the optimal setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can jointly optimizing PrefixLM and UL2 objectives capture the strengths of both generative and representation learning?
- Basis in paper: [explicit] The authors state, "Another direction is to jointly optimize the model on PrefixLM and UL2, which we leave for future work."
- Why unresolved: Checkpoint merging failed, and sequential optimization yielded mixed results (e.g., switching from UL2 to PrefixLM hurt PT performance).
- What evidence would resolve it: Successful training runs utilizing a multi-task objective that outperforms single-objective baselines on both PT/IT and SuperGLUE benchmarks.

### Open Question 2
- Question: Is effective adaptation possible when initializing the encoder and decoder from different pretrained decoder-only model families?
- Basis in paper: [explicit] The authors note, "In theory, we can also adapt decoder-only models from different families, such as pairing LLaMA models with QWen models."
- Why unresolved: All experiments were restricted to the Gemma 2 family (2B and 9B) and mT5-sized models pretrained from scratch.
- What evidence would resolve it: Experiments showing stable convergence and competitive performance for hybrid architectures (e.g., LLaMA encoder with Qwen decoder).

### Open Question 3
- Question: Does the adaptation method retain its efficiency and quality advantages when applied to Mixture-of-Experts (MoE) models?
- Basis in paper: [explicit] The Future Work section lists "testing the combination of dense and MoE LLMs" as a specific interest.
- Why unresolved: The study focused exclusively on dense transformer architectures, leaving the interaction between cross-attention and sparse expert routing unexplored.
- What evidence would resolve it: Comparative analysis of dense-to-dense vs. MoE-to-MoE adaptation regarding training stability, inference latency, and downstream task performance.

## Limitations
- The adaptation method is demonstrated only on Gemma 2 (2B and 9B) and smaller models, limiting generalizability to other decoder-only architectures.
- The paper does not provide a principled method for combining PrefixLM and UL2 objectives, leaving a gap for unified optimization frameworks.
- The analysis of unbalanced configurations is limited to one example (9B-2B), with optimal encoder/decoder ratios for other scale pairs unexplored.
- Claims about compute efficiency are asserted but not directly validated with absolute FLOP or wall-clock comparisons to training from scratch.

## Confidence
- **High Confidence**: The core adaptation method (parameter transfer, initialization strategy, bidirectional attention benefit) is well-supported by ablation studies and consistent results across multiple scales.
- **Medium Confidence**: The quality-efficiency trade-off improvement from unbalanced encoder-decoder models is demonstrated for the 9B-2B case, but generalizability to other configurations and tasks is less certain.
- **Low Confidence**: The claim that adaptation is more compute-efficient than training from scratch is asserted but not empirically validated with direct comparisons.

## Next Checks
1. **Cross-Architecture Transfer**: Apply the adaptation method to a decoder-only model with a significantly different architecture (e.g., LLaMA, Mistral) or pretraining objective (e.g., DPO-tuned). Measure the success rate of adaptation and the performance gap compared to adapting Gemma 2, isolating the effect of architectural/objective mismatch.

2. **Objective Ablation and Combination**: Design a unified pretraining objective that interpolates between PrefixLM and UL2 (e.g., weighted sum of losses, or a curriculum that starts with one and transitions to the other). Compare the adapted model's performance on both IT and SuperGLUE benchmarks to the individual PrefixLM and UL2 baselines, and assess if a single model can achieve the best of both worlds.

3. **Unbalanced Configuration Sweep**: Systematically vary the encoder/decoder size ratios (e.g., 3B-1B, 6B-2B, 9B-3B) and evaluate on a diverse set of tasks with different input-output length characteristics (e.g., summarization, classification, long-form generation). Identify the optimal ratio for each task type and the point at which the small decoder becomes a bottleneck, mapping the quality-efficiency Pareto frontier.