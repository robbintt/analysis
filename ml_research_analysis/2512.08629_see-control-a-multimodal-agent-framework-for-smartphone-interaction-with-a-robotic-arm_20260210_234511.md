---
ver: rpa2
title: 'See-Control: A Multimodal Agent Framework for Smartphone Interaction with
  a Robotic Arm'
arxiv_id: '2512.08629'
source_url: https://arxiv.org/abs/2512.08629
tags:
- search
- task
- agent
- tasks
- smartphone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces See-Control, a framework that enables embodied
  smartphone operation via a robotic arm, addressing the limitations of Android Debug
  Bridge (ADB)-dependent methods. The core method involves using a multimodal large
  language model (MLLM) to generate robotic control commands based on visual perception
  of the smartphone screen, without requiring ADB or system backend access.
---

# See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm

## Quick Facts
- **arXiv ID**: 2512.08629
- **Source URL**: https://arxiv.org/abs/2512.08629
- **Reference count**: 40
- **Primary result**: Introduces a framework enabling embodied smartphone operation via a robotic arm using only visual input, without requiring ADB access.

## Executive Summary
This paper presents See-Control, a novel framework that enables multimodal large language models (MLLMs) to control smartphones through a robotic arm using only visual perception. The system addresses limitations of traditional ADB-dependent methods by generating robotic control commands based on visual understanding of the smartphone screen. The framework includes a benchmark of 155 tasks, an MLLM-based embodied agent, and a richly annotated dataset of operation episodes. User studies show strong acceptance for robotic smartphone automation, highlighting its potential for assistive human-computer interaction and in-home automation.

## Method Summary
The See-Control framework operates by capturing real-time screenshots of a smartphone via screen mirroring, processing these images through a visual perception module that uses Grounding DINO for icon detection and OCR for text detection, and then feeding the marked images into an MLLM (GPT-4o) to generate robotic control commands. The system maintains a history of actions and observations to enable error recovery through a POMDP-based approach. Actions are mapped to discrete tokens like Tap(x,y) or Swipe(direction) and executed by a 3-axis robotic arm. The framework successfully handles simple tasks but faces challenges with complex, cross-app scenarios due to error accumulation and latency issues.

## Key Results
- The agent demonstrates proficiency in simple, single-app tasks with high success rates
- User studies show strong acceptance for robotic smartphone automation in assistive applications
- The framework successfully performs platform-agnostic navigation gestures without requiring system-level access
- The dataset supports training MLLMs with enhanced GUI understanding and Vision-Language-Action models

## Why This Works (Mechanism)

### Mechanism 1: Visual Grounding via Hierarchical Perception
If the system accurately identifies interactive GUI elements (icons/text) from pixels alone, then it can generate robotic actions without system-level access (ADB). The agent uses a two-stage visual pipeline where Grounding DINO proposes candidate regions for icons, which are then annotated with visual marks and fed into the MLLM for semantic mapping to specific coordinates.

### Mechanism 2: Platform-Agnostic Physical Gestures
If the robotic arm mimics human gestures rather than executing system commands, then it can perform "Back" or "Exit" operations across different operating systems. Instead of ADB commands, the system uses physical swipe gestures that the OS interprets as navigation commands.

### Mechanism 3: POMDP-Based History Utilization
If the agent maintains a history of actions and observations, then it can self-correct errors in multi-step tasks where the environment state is only partially observable. The POMDP setup enables the agent to identify errors and revert to previous steps when necessary.

## Foundational Learning

- **Visual Grounding (Icon/Text Detection)**
  - Why needed: Translates natural language instructions into pixel coordinates without backend data access
  - Quick check: Explain the difference between OCR and Object Detection when identifying a "Submit" button

- **Partially Observable Markov Decision Process (POMDP)**
  - Why needed: The agent only sees screenshots, not internal phone state, requiring inference for decision-making
  - Quick check: In this context, what represents the Observation and what represents the State?

- **Discretized Action Space**
  - Why needed: Maps continuous robotic motion to tractable LLM inputs using discrete tokens
  - Quick check: Why is mapping a "Back" action to a specific gesture considered a "discretization" of robotic behavior?

## Architecture Onboarding

- **Component map**: PC/Server -> Visual Perception Module (Grounding DINO + OCR) -> Agent Core (MLLM with history) -> Robot Arm Interface -> Phone (environment)
- **Critical path**: The Vision-to-Control latency loop: capture screen → upload image to MLLM → process reasoning → send command to robot → execute physical motion
- **Design tradeoffs**: Visual prompting vs. direct detection (accuracy vs. latency), low-DoF arm vs. dexterous hand (safety vs. capability)
- **Failure signatures**: Drift in gesture navigation, visual occlusion, keyboard layout variation
- **First 3 experiments**:
  1. Calibration Run: Execute Tap on known coordinates and Swipe gestures to verify coordinate mapping
  2. Visual Module Isolation: Test Grounding DINO + OCR pipeline on static screenshots for detection precision
  3. Simple Task Loop: Run "Calculator: 1+1" task to verify full perception-reasoning-action loop functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating dexterous end-effectors with multi-touch capabilities significantly improve Success Rates on complex tasks currently limited by the single-contact constraint?
- Basis: Authors identify limitation of "simplified action space, limited to single-touch operations" and propose investigating dexterous end-effectors
- Why unresolved: Current prototype uses low-DoF arm restricted to single contact point
- Evidence needed: Comparative study evaluating multi-fingered robotic agent vs. baseline on challenging task set

### Open Question 2
- Question: Does end-to-end Vision-Language-Action (VLA) training effectively mitigate execution errors caused by MLLM latency in dynamic environments?
- Basis: Paper notes "latency inherent to MLLMs persists" and proposes "tighter perception–action loops via end-to-end VLA training"
- Why unresolved: Current system relies on discrete steps creating invocation delays unsuitable for time-sensitive interactions
- Evidence needed: Empirical results showing reduced latency and higher success in dynamic app categories using trained VLA model

### Open Question 3
- Question: What decision logic should govern a hybrid system that dynamically switches between software-bridge (ADB) and embodied (ESO) pathways?
- Basis: Paper states "ESO agents offer a complementary path, rather than a replacement" and anticipates systems choosing between pathways
- Why unresolved: Framework establishes embodied route feasibility but doesn't propose pathway selection mechanism
- Evidence needed: Architecture defining meta-controller that evaluates context and selects appropriate control method

## Limitations

- Limited to single-touch interactions, excluding multi-touch gestures like pinch-to-zoom
- Error accumulation in long task sequences degrades performance significantly
- Visual grounding may struggle with custom or context-dependent interface elements
- Reliance on visual observation creates challenges with non-standard UI elements and gesture controls

## Confidence

**High Confidence**:
- Basic visual grounding and action execution pipeline works reliably for simple tasks
- POMDP-based history mechanism enables basic error recovery in short sequences
- Platform-agnostic gesture navigation successfully handles "Back" and "Exit" operations

**Medium Confidence**:
- Visual prompting approach improves accuracy but adds latency
- Discretized action space successfully maps complex robotic movements
- User acceptance findings may not reflect real-world deployment scenarios

**Low Confidence**:
- Scalability to truly complex, cross-app workflows remains unproven
- Exact prompts and few-shot examples for MLLM not specified
- Robustness under varying environmental conditions not fully characterized

## Next Checks

1. **Error Propagation Analysis**: Systematically measure Success Rate degradation as task complexity increases beyond current benchmark, specifically testing cross-app workflows involving 5+ steps to quantify error accumulation limits.

2. **Visual Grounding Robustness**: Evaluate Grounding DINO + OCR pipeline's performance on custom app interfaces with non-standard icons, animated elements, and context-dependent visual cues to assess generalization beyond provided dataset.

3. **Latency Impact Study**: Measure end-to-end latency of vision-to-control loop and correlate with Success Rate to determine maximum UI animation speed system can reliably handle without acting on stale data.