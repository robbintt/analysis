---
ver: rpa2
title: Optimization, Generalization and Differential Privacy Bounds for Gradient Descent
  on Kolmogorov-Arnold Networks
arxiv_id: '2601.22409'
source_url: https://arxiv.org/abs/2601.22409
tags:
- theorem
- then
- training
- holds
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the optimization, generalization, and differential
  privacy (DP) properties of gradient descent (GD) training for Kolmogorov-Arnold
  Networks (KANs). The authors derive bounds for both standard GD and its DP variant
  (DP-GD) on two-layer KANs with B-spline edge functions.
---

# Optimization, Generalization and Differential Privacy Bounds for Gradient Descent on Kolmogorov-Arnakov Networks

## Quick Facts
- arXiv ID: 2601.22409
- Source URL: https://arxiv.org/abs/2601.22409
- Reference count: 40
- This paper analyzes optimization, generalization, and differential privacy properties of gradient descent training for two-layer Kolmogorov-Arnakov Networks (KANs) with B-spline edge functions.

## Executive Summary
This paper provides theoretical analysis of gradient descent (GD) and differentially private GD (DP-GD) for training two-layer KANs with B-spline edge functions. The authors derive bounds showing that polylogarithmic network width suffices for GD to achieve O(1/T) optimization rate and O(1/n) generalization rate under NTK separability assumptions. For DP-GD, they establish that polylogarithmic width is both sufficient and necessary to match classical convex Lipschitz lower bounds. The analysis reveals implicit regularization effects where GD solutions remain close to initialization, providing practical guidance for width and training duration selection.

## Method Summary
The method analyzes full-batch gradient descent on two-layer KANs with B-spline basis functions. For non-private GD, the analysis uses a double induction argument to bound cumulative training loss and distances from initialization/reference points, achieving O(1/T) optimization rate with polylogarithmic width under NTK separability. Generalization bounds follow from on-average argument stability, showing O(1/n) rates when training loss is small. For DP-GD, Gaussian noise is added to gradients with variance calibrated to trajectory-wise sensitivity, controlled via projection onto bounded domains. The analysis establishes both sufficiency and necessity of polylogarithmic width for achieving optimal privacy-utility tradeoffs.

## Key Results
- Polylogarithmic network width suffices for GD to achieve O(1/T) optimization rate under NTK separability
- Polylogarithmic width is both sufficient and necessary for DP-GD to achieve O(√d/(nε)) utility matching convex lower bounds
- Implicit regularization causes GD solutions to remain close to initialization, enabling fast O(1/n) generalization rates
- Diminishing returns occur beyond critical width values, with accuracy plateauing for both training and test performance

## Why This Works (Mechanism)

### Mechanism 1: Reference-Point Optimization with Double Induction
- Claim: Gradient descent on two-layer KANs achieves O(1/T) optimization rate with polylogarithmic width under NTK separability.
- Mechanism: The curvature of the loss landscape depends on parameter deviation from initialization. A double induction simultaneously bounds (1) cumulative training loss and (2) distances from both initialization and a reference point, ensuring iterates remain in a region where Hessian eigenvalues are uniformly controlled.
- Core assumption: NTK separability with margin γ > 0 (Assumption 4.8)—weaker than Gram-matrix positive-definiteness.
- Evidence anchors:
  - [abstract]: "polylogarithmic network width suffices for GD to achieve an optimization rate of order 1/T"
  - [Section 4.2, Theorem 4.9]: Explicit rate bound with γ-dependent constants
  - [corpus]: Related work on lazy training regimes (arXiv:2510.21245) supports convergence in overparameterized settings
- Break condition: Width m scales polynomially rather than polylogarithmically; NTK separability margin γ → 0.

### Mechanism 2: Generalization via On-Average Argument Stability
- Claim: Population risk achieves fast O(1/n) rate when training loss is small, revealing implicit regularization toward initialization-nearby solutions.
- Mechanism: The generalization gap is bounded by cumulative training loss along the GD trajectory multiplied by a complexity term involving distance from initialization. Small training loss → small generalization gap.
- Core assumption: Reference point Θ* exists near initialization with low training loss (realizability via NTK separability).
- Evidence anchors:
  - [abstract]: "generalization rate of order 1/n"
  - [Section 4.3, Theorem 4.11]: L(Θ(T)) bounded by (Λ²_Θ*/n + 1/ηT)·C(Θ*)
  - [corpus]: Algorithmic stability analysis for SGD (arXiv:2502.00885) provides precedent for stability-based generalization
- Break condition: ηT ≪ n (insufficient training); width too small to satisfy realizability.

### Mechanism 3: Differential Privacy via Width-Constrained DP-GD
- Claim: Polylogarithmic width is both sufficient AND necessary for DP-GD to achieve O(√d/(nε)) utility—matching convex Lipschitz lower bounds.
- Mechanism: DP-GD adds Gaussian noise scaled to gradient sensitivity. Wider networks amplify noise through increased parameter dimension, but some width is needed for learning. The "admissible width regime" balances these effects.
- Core assumption: Trajectory-wise sensitivity control via projection onto bounded domains; curvature bounded by ρ̄.
- Evidence anchors:
  - [abstract]: "polylogarithmic width is not only sufficient but also necessary under differential privacy"
  - [Section 4.4, Theorem 4.16]: Explicit utility bound eO(√d/(γ⁴nε))
  - [corpus]: Privacy-utility dynamics of noisy SGD (arXiv:2510.16687) shows similar tradeoffs in high dimensions
- Break condition: m ≫ polylog(n) (excessive width amplifies noise); T too large (noise accumulates).

## Foundational Learning

- Concept: **Neural Tangent Kernel (NTK)**
  - Why needed here: The NTK-separability assumption (margin γ) replaces stronger Gram-matrix conditions. Understanding NTK as the kernel governing training dynamics in the lazy regime is essential.
  - Quick check question: Can you explain why NTK separability with margin γ is weaker than requiring λ_min(G_∞) > 0?

- Concept: **Algorithmic Stability**
  - Why needed here: Generalization bounds derive from on-average argument stability—the sensitivity of outputs to single-data-point perturbations—not from uniform convergence.
  - Quick check question: How does on-average argument stability differ from uniform stability, and why does it suffice for generalization bounds?

- Concept: **Differential Privacy (Rényi DP)**
  - Why needed here: The privacy analysis uses Rényi DP composition, then converts to (ε,δ)-DP. Understanding sensitivity calibration is critical for the noise variance formulas.
  - Quick check question: Why does projection-based sensitivity control (rather than gradient clipping) simplify the DP-GD analysis for KANs?

## Architecture Onboarding

- Component map:
  - Input layer: d-dimensional input with 1/√d normalization
  - Hidden layer: m units, each computing σ(1/√d · Σ_i Σ_k a_{i,j,k} b_k(x_i))
  - Output layer: 1/√m · Σ_j Σ_k c_{j,k} b_k(x_{1,j})
  - Parameter vector: Θ = (a, c) ∈ R^{mp(d+1)} with a ∈ R^{mdp}, c ∈ R^{mp}
  - B-spline basis: p basis functions {b_k} with bounded derivatives (Assumption 4.1)

- Critical path:
  1. Initialize: a^{(0)} ~ N(0, I_{mdp}), c^{(0)} ~ N(0, I_{mp})
  2. Forward pass: Compute hidden units x_{1,j} = σ(1/√d · A·h(x)), then output f_Θ(x)
  3. Gradient step: Θ^{(k+1)} = Θ^{(k)} - η∇L_S(Θ^{(k)}) [+ Gaussian noise for DP-GD]
  4. [DP-GD only] Project onto Ω_a × Ω_c = B(a^{(0)}, R₁) × B(c^{(0)}, R₂)

- Design tradeoffs:
  - **Width m**: Polylog(n) suffices for non-private GD; excessive width yields diminishing returns. For DP-GD, both too-small and too-large widths hurt utility.
  - **Iterations T**: For GD, T ≳ n needed for generalization. For DP-GD, T ≍ nε/√d (early stopping critical to limit noise accumulation).
  - **Projection radii R₁, R₂**: Must be large enough to contain reference point, small enough to control sensitivity. Paper uses R ≍ τ ≍ (log(T) + √log(n/δ))/γ.

- Failure signatures:
  - Training loss not decreasing: Check if m ≳ polylog(n, T, 1/γ, 1/δ); verify NTK separability holds empirically
  - Test accuracy plateauing early: Expected behavior per Theorem 4.12—increasing T beyond T_crit provides minimal generalization gain
  - DP-GD utility degrading with width: Width exceeds admissible regime; reduce m or increase nε
  - DP-GD utility degrading with iterations: Noise accumulation dominates; implement early stopping at T ≍ nε/√d

- First 3 experiments:
  1. **Width sweep for GD**: Train two-layer KAN with m ∈ {4, 8, 16, 32, 64, 128, 256} on synthetic logistic data (n=20K, d=10). Plot training and test accuracy vs. m. Expect plateau beyond m_crit ≈ 24 (Figure 2a validation).
  2. **Iteration sweep for generalization**: Fix m=32, vary T ∈ {64, 128, 256, 512, 1024}. Monitor training vs. test accuracy gap. Expect training accuracy to continue improving while test accuracy saturates near T_crit ≈ 256.
  3. **DP-GD width-iteration grid**: For ε=2, δ=1/n, sweep m ∈ {4, 8, 16, 32, 64} and T ∈ {8, 16, 32, 64, 128}. Plot private utility (test accuracy). Expect peak at moderate m and T, with degradation at extremes—validating the admissible width regime (Figure 2b).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimization, generalization, and differential privacy guarantees derived for two-layer KANs be extended to deep architectures with more than two layers?
- **Basis in paper:** [explicit] Section 7 explicitly lists "extending the analysis beyond two-layer KANs" as a limitation and direction for future work.
- **Why unresolved:** The theoretical framework relies on the specific spectral properties and NTK dynamics of the two-layer model, which may differ fundamentally in deep hierarchical structures.
- **What evidence would resolve it:** A theoretical proof establishing convergence and generalization bounds for L-layer KANs, or empirical evidence demonstrating that the identified width/iteration scaling laws hold for deeper networks.

### Open Question 2
- **Question:** Is it possible to establish similar theoretical bounds for KANs utilizing non-smooth activation functions, such as ReLU?
- **Basis in paper:** [explicit] Section 7 identifies the need for "relaxing the smoothness assumption on $\sigma$ to cover non-smooth activations such as ReLU."
- **Why unresolved:** The current analysis (Assumption 4.1) requires the activation function $\sigma$ to have bounded first and second derivatives, a condition violated by ReLU.
- **What evidence would resolve it:** A modified theoretical framework using tools for non-smooth analysis (e.g., subgradients) that yields comparable utility and privacy bounds for ReLU-based KANs.

### Open Question 3
- **Question:** Do the optimization and privacy-utility tradeoffs hold for Stochastic Gradient Descent (SGD) as they do for full-batch Gradient Descent?
- **Basis in paper:** [explicit] Section 7 lists "generalizing our guarantees from GD to SGD" as a limitation to be addressed in future work.
- **Why unresolved:** The current proofs rely on the stability of full-batch GD, whereas SGD introduces variance from mini-batch sampling which complicates trajectory and sensitivity analysis.
- **What evidence would resolve it:** Convergence and generalization bounds for KANs trained with SGD that explicitly account for batch size and gradient noise variance.

## Limitations

- The analysis is limited to two-layer KANs, with no theoretical guarantees established for deeper architectures
- The NTK separability assumption requires margin γ > 0, but empirical methods to verify this property on real datasets are not provided
- The DP-GD width necessity result relies on worst-case constructions that may not reflect practical performance limitations
- The analysis requires smooth activation functions with bounded derivatives, excluding non-smooth activations like ReLU

## Confidence

- **High Confidence**: The polylogarithmic width sufficiency for optimization (O(1/T)) and generalization (O(1/n)) rates in non-private GD is well-supported by the double induction argument and stability analysis
- **Medium Confidence**: The width necessity result for DP-GD matches theoretical lower bounds but may be conservative in practice; the admissible width regime prediction needs more empirical validation across diverse datasets
- **Low Confidence**: The exact critical thresholds (e.g., T_crit ≈ 256 for generalization saturation) may be dataset-dependent and require calibration for different problem domains

## Next Checks

1. **NTK Separability Testing**: Develop empirical diagnostics to verify NTK separability with margin γ on real datasets, measuring how γ varies with network width and architecture choices
2. **Width Regime Mapping**: Systematically map the admissible width regime for DP-GD across different privacy budgets (ε ∈ {0.1, 0.5, 1, 2, 5}) and dataset scales (n ∈ {10K, 100K, 1M})
3. **Early Stopping Validation**: Empirically determine optimal stopping criteria for DP-GD that balance noise accumulation against privacy-utility tradeoffs, comparing fixed-iteration approaches against validation-set-based stopping