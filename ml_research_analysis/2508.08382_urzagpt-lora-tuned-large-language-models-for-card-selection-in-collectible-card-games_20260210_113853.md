---
ver: rpa2
title: 'UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible
  Card Games'
arxiv_id: '2508.08382'
source_url: https://arxiv.org/abs/2508.08382
tags:
- card
- cards
- drafting
- games
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces UrzaGPT, a LoRA-tuned large language model
  for drafting in Magic: The Gathering. The key innovation is framing drafting as
  a language modeling task, where the model predicts the next card to select given
  the current deck and pack contents.'
---

# UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games

## Quick Facts
- **arXiv ID:** 2508.08382
- **Source URL:** https://arxiv.org/abs/2508.08382
- **Reference count:** 11
- **Primary result:** Llama-3-8B reaches 66.2% accuracy after LoRA fine-tuning on Magic: The Gathering drafting data

## Executive Summary
This paper introduces UrzaGPT, a LoRA-tuned large language model for drafting in Magic: The Gathering. The key innovation is framing drafting decisions as a language modeling task, where the model predicts the next card to select given the current deck and pack contents. Starting from an open-weight LLM, the authors use LoRA fine-tuning on human drafting data. They find that small LLMs (7-8B parameters) perform poorly out-of-the-box, while GPT-4o achieves 43% accuracy in zero-shot setting. After fine-tuning with LoRA for 10,000 steps, Llama-3-8B reaches 66.2% accuracy, approaching state-of-the-art domain-specific models at 68%. The results demonstrate that LLMs can be effectively adapted for drafting tasks with relatively small amounts of domain-specific training data.

## Method Summary
The authors transform Magic: The Gathering drafting data into a language modeling task by serializing the partial game state (current deck list + available pack) into structured prompts. They fine-tune Llama-3-8B and Mistral-7B-Instruct using LoRA with rank=8, alpha=16, dropout=0.05, and max sequence length=512. The model is trained for 10,000 steps with batch size 32 (8 per device x 4 accumulation) across 3 GPUs, achieving 66.2% accuracy on a held-out test set of 10K samples from the NEO dataset.

## Key Results
- Base Llama-3-8B achieves only 38% accuracy in zero-shot setting with full card text
- GPT-4o achieves 43% accuracy in zero-shot setting with name-only prompts
- LoRA fine-tuning improves Llama-3-8B to 66.2% accuracy after 10,000 steps
- Color adherence increases from 60% to 66% with LoRA fine-tuning
- Name-only prompts outperform full card text (43% vs 38% for GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1: Semantic State-to-Action Mapping via Next-Token Prediction
Drafting decisions can be re-framed as a conditional language modeling task where the optimal card is the most probable next token given the game state. The architecture serializes the partial observability of the game (current deck list + available pack) into a structured prompt. The model leverages pre-trained semantic understanding of card interactions to predict the "missing" token (the card name) that best coheres with the existing latent strategy in the prompt.

### Mechanism 2: Efficient Domain Adaptation via Subspace Optimization (LoRA)
Low-Rank Adaptation (LoRA) allows a generalist model to learn domain-specific preferences (human drafting heuristics) without catastrophic forgetting or full retraining. By freezing the pre-trained weights and injecting trainable low-rank matrices, the optimization process projects gradient updates into a lower-dimensional subspace, constraining the model to adapt its existing knowledge to the "human preference" distribution rather than learning the game from scratch.

### Mechanism 3: Context Dilution & Knowledge Retrieval
Model performance relies more on activating internal "compressed" knowledge of a card than on processing explicit text descriptions provided in context. Providing full card text increases token count and potentially introduces noise ("diluting the prompt"). The model performs better when prompted with concise identifiers (names) that trigger high-dimensional embeddings learned during pre-training, rather than forcing it to reason over raw text rules in a limited context window.

## Foundational Learning

**Concept: Low-Rank Adaptation (LoRA)**
- **Why needed here:** To fine-tune large models (7-8B parameters) on consumer-grade hardware (3x A100s) and achieve rapid domain shift without the cost of full parameter updates.
- **Quick check question:** If you increase the rank ($r$) of your LoRA adapter, do you expect the model to adhere better to complex constraints like card colors, or just memorize the training data faster? (Answer: Better constraint adherence, as per Figure 5).

**Concept: Context Window & "Lost in the Middle"**
- **Why needed here:** The paper identifies that adding full card text harms performance, likely due to retrieval difficulties in long contexts. Understanding context limitations is vital for prompt engineering here.
- **Quick check question:** Why might a model fail to select a card from the middle of a 15-card pack if the prompt includes the full text of every card?

**Concept: Behavioral Cloning (Imitation Learning)**
- **Why needed here:** UrzaGPT is trained on human logs, not game outcomes. It predicts what a human *would* do, not necessarily the mathematically optimal move.
- **Quick check question:** If the training data consists of average players rather than experts, what type of bias is the model likely to learn?

## Architecture Onboarding

**Component map:** 17lands.com logs → Data Ingestor → Prompt Serializer → Base Model (Llama-3-8B) → LoRA Adapter Layers → Output Parser → Card Selection

**Critical path:** The **Prompt Serializer** is the single point of failure for reasoning. If the serialization order changes or the format deviates from the fine-tuning distribution, the model's ability to "read" the board state degrades immediately.

**Design tradeoffs:**
- **Name vs. Full Text:** Names are token-efficient and rely on internal knowledge (better performance); Full Text provides info for unseen cards but causes context dilution (worse performance).
- **Rank ($r$) Selection:** Low rank ($r=4$) is faster but may lack capacity for complex strategy (color adherence); High rank ($r=32$) improves accuracy but increases compute. The paper suggests a sweet spot around $r=16$.

**Failure signatures:**
- **Hallucination:** Model picks a card not in the pack (Illegal Action).
- **Color Espionage:** Model consistently picks cards outside the current deck's color identity, indicating a failure to grasp deck cohesion constraints.
- **Format Drift:** Model outputs reasoning text instead of just the card name.

**First 3 experiments:**
1. **Zero-Shot Constraint Check:** Test base Llama-3-8B with "Name Only" vs. "Full Text" on a held-out set of 100 samples to verify the "Context Dilution" hypothesis on your specific inference setup.
2. **LoRA Rank Sweep:** Fine-tune for 1,000 steps with ranks $r \in \{4, 8, 16\}$. Measure "Color Accuracy" (adherence to deck colors) rather than just pick accuracy to verify strategic reasoning.
3. **Tokenization Boundary Test:** Verify the Output Parser can handle edge cases (e.g., cards with split names or punctuation) to ensure "Illegal Action" rates are due to model intelligence, not software bugs.

## Open Questions the Paper Calls Out

**Open Question 1:** Can a drafting model fine-tuned on one game expansion generalize effectively to new expansions with different card pools?
- **Basis in paper:** The authors ask, "Can models trained on one set adapt to others with similar mechanics? Are their representations reusable across expansions?"
- **Why unresolved:** The study restricted training and evaluation to the single "NEO" dataset to compare with prior work, leaving cross-set transfer learning unexplored.
- **What evidence would resolve it:** Fine-tuning UrzaGPT on Set A and evaluating its prediction accuracy on Set B without further training, compared to a baseline retrained on Set B.

**Open Question 2:** Does full parameter fine-tuning provide significant performance gains over LoRA for the drafting task?
- **Basis in paper:** Section 6 asks, "Would full fine-tuning exceed the PEFT setting?" and calls for more work to investigate how far performance can be pushed with more trainable parameters.
- **Why unresolved:** The authors exclusively utilized Low-Rank Adaptation (LoRA) to manage computational resources and did not benchmark against full fine-tuning.
- **What evidence would resolve it:** A comparison of drafting accuracy between a fully fine-tuned Llama-3-8B model and the LoRA-adapted version using the same dataset.

**Open Question 3:** Does high prediction accuracy in card selection correlate with higher win rates in simulated gameplay?
- **Basis in paper:** The authors acknowledge that predicting human picks is a noisy proxy for quality and that "accuracy... fails to capture the quality of full drafts."
- **Why unresolved:** The paper evaluates performance solely on prediction accuracy against human data because fast simulators for the full game are not available at scale.
- **What evidence would resolve it:** Implementing the model in a gameplay simulator to measure the win rates of completed decks against agents using other drafting methods.

## Limitations

**Data Representativeness:** The evaluation relies on a single Magic: The Gathering set (NEO), raising concerns about generalizability across different card sets, mechanics, and drafting formats.

**Model Selection Bias:** Results show significant variation between base model sizes, with 7-8B models performing poorly in zero-shot settings while GPT-4o achieves 43% accuracy.

**Strategic Evaluation Gap:** While the paper measures accuracy and color adherence, it doesn't evaluate actual drafting quality or long-term game outcomes.

## Confidence

**High Confidence (90%+):** The core finding that LoRA fine-tuning improves draft selection accuracy from ~60% to ~66% is well-supported by the experimental results.

**Medium Confidence (70-90%):** The claim that name-only prompts outperform full card text due to context dilution is plausible but not conclusively proven.

**Low Confidence (50-70%):** The assertion that base LLM knowledge of card interactions drives performance relies on implicit assumptions about pre-training data composition.

## Next Checks

1. **Cross-Set Generalization Test:** Evaluate UrzaGPT on drafting data from at least two additional Magic: The Gathering sets (e.g., one older set and one newer set) to measure performance degradation and identify whether the model learns transferable drafting heuristics or set-specific patterns.

2. **Strategic Quality Assessment:** Implement a simulation pipeline that uses UrzaGPT's draft picks to construct complete decks, then measure their performance against human-drafted decks in actual gameplay. This would validate whether high accuracy correlates with drafting quality.

3. **Prompt Format Ablation Study:** Systematically test alternative prompt structures beyond "Name Only" vs "Full Text" - including card ordering, explicit color identity cues, or semantic descriptions - to isolate the exact mechanisms driving the observed performance differences and identify potential optimizations.