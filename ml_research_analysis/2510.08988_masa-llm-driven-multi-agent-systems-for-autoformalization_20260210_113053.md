---
ver: rpa2
title: 'MASA: LLM-Driven Multi-Agent Systems for Autoformalization'
arxiv_id: '2510.08988'
source_url: https://arxiv.org/abs/2510.08988
tags:
- agent
- formal
- language
- autoformalization
- formalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MASA, a modular framework for building multi-agent
  systems for autoformalization, which converts natural language mathematical statements
  into formal representations using collaborative agents. The framework emphasizes
  modularity, flexibility, and extensibility, enabling seamless integration of new
  agents and tools.
---

# MASA: LLM-Driven Multi-Agent Systems for Autoformalization

## Quick Facts
- **arXiv ID**: 2510.08988
- **Source URL**: https://arxiv.org/abs/2510.08988
- **Reference count**: 16
- **Primary result**: Iterative self-refinement system achieves 35.25% (Qwen2.5-7B) and 61.89% (GPT-4.1-mini) success rates for formalizations that are both syntactically correct and semantically aligned

## Executive Summary
This paper introduces MASA, a modular framework for building multi-agent systems for autoformalization that converts natural language mathematical statements into formal representations. The framework emphasizes modularity, flexibility, and extensibility, enabling seamless integration of new agents and tools. MASA supports specialized agents for autoformalization, critique, and refinement, along with integration with large language models and theorem provers. The paper demonstrates MASA's effectiveness through use cases on real-world mathematical definitions and experiments on formal mathematics datasets.

## Method Summary
The MASA framework employs a multi-agent pipeline where an AutoformalizationAgent generates initial code, which is then verified by HardCritiqueAgent (theorem prover for syntax) and SoftCritiqueAgent (LLM judge for semantics). If errors are detected, FormalRefinementAgent or InformalRefinementAgent corrects them iteratively. The system uses LLMs (GPT-4.1-mini, Qwen2.5-7B) for generation/judgment and theorem provers (Isabelle, Lean4) for syntax verification. The iterative self-refinement system loops until pass or maximum iterations, achieving both syntactic correctness and semantic alignment.

## Key Results
- Iterative self-refinement achieves 61.89% success rate with GPT-4.1-mini and 35.25% with Qwen2.5-7B for formalizations that are both syntactically correct and semantically aligned
- Hard critique mechanism improves pass rates significantly, with G4M few-shot pass rate jumping from 76.23% to 86.48% when combined with Hard Critique and Refinement
- Targeting "Alignment Faithfulness" increases semantic alignment scores, with Qwen improving from 54.51% to 73.77% in AF scores

## Why This Works (Mechanism)

### Mechanism 1: Iterative Error Correction via Critique-Refinement Loops
Multi-agent systems improve accuracy by iteratively correcting formalization errors based on specific feedback rather than relying on single-pass generation. The system separates generation from verification, allowing recovery from initial syntactic or semantic failures. Core assumption: backend LLM can interpret error messages and execute corrections without drifting from original intent.

### Mechanism 2: Syntactic Grounding via Theorem Provers (Hard Critique)
Offloading syntax verification to external theorem provers provides ground-truth signals that reduce formalization syntax errors. The HardCritiqueAgent interfaces with Isabelle/Lean4, extracting precise error details like "Undefined type name" or "Inner syntax error" to act as deterministic guardrails against LLM hallucinations.

### Mechanism 3: Semantic Alignment via LLM-as-Judge (Soft Critique)
LLMs serve as proxy evaluators for semantic alignment where formal verification is impossible or ambiguous. The SoftCritiqueAgent prompts an LLM to judge if formal code matches natural language description based on specific "aspect description" like "are all conditions properly encoded?"

## Foundational Learning

- **Concept**: Interactive Theorem Provers (ITPs) - Lean4/Isabelle
  - **Why needed here**: Hard Critique mechanism relies entirely on understanding compiler output. You cannot debug agents without understanding why "real" is undefined in Lean/Isabelle syntax.
  - **Quick check question**: Given a Lean4 error "unknown identifier 'Nat'", what agent and component would you use to attempt a fix?

- **Concept**: Chain-of-Thought / Critique-Revise Prompting
  - **Why needed here**: MASA architecture operationalizes this pattern. Understanding how to structure prompts for "Critique" vs. "Refinement" roles is essential for configuring agents.
  - **Quick check question**: How does prompt structure differ between agent asked to "Formalize" versus "Refine based on error"?

- **Concept**: Agent Orchestration / State Machines
  - **Why needed here**: System is a state machine (Algorithm 3). You need to understand how to pass state between agents without losing context.
  - **Quick check question**: In Algorithm 3, what happens to loop if HardCritique returns True but SoftCritique returns False?

## Architecture Onboarding

- **Component map**: AutoformalizationAgent -> HardCritiqueAgent -> FormalRefinementAgent OR SoftCritiqueAgent -> InformalRefinementAgent -> Back to AutoformalizationAgent
- **Critical path**: 1) Input Natural Language Statement 2) AutoformalizationAgent generates initial code 3) HardCritiqueAgent checks syntax via prover 4) If syntax fails -> FormalRefinementAgent 5) If syntax passes -> SoftCritiqueAgent checks alignment 6) If semantic fails -> InformalRefinementAgent 7) Loop until pass or max iterations
- **Design tradeoffs**: Modularity vs. Latency - modularity creates overhead but improves debuggability; Model Strength - stronger models handle refinement loops better, weaker models struggle to recover from errors
- **Failure signatures**: Import Loops - system repeatedly fails to find correct import; Syntax-Semantic Drift - FormalRefinementAgent fixes syntax by deleting semantic content; Fluctuation - model guessing rather than converging
- **First 3 experiments**: 1) Replicate "Softmax" Use Case to watch error resolution sequence 2) Stress Test Critique with malformed Lean4 code to verify error parser 3) Ablation on Iterations comparing 1 vs 3 iterations on miniF2F

## Open Questions the Paper Calls Out

### Open Question 1
Does integrating a central intelligence agent to dynamically distribute tasks improve autoformalization performance compared to current fixed sequential pipeline? The paper states the proposed system "lacks a central intelligence agent to distribute and control the different agents" and uses hardcoded sequential workflows rather than dynamic orchestration.

### Open Question 2
Can "fine-grained criteria" or symbolic proxies provide more reliable semantic evaluation than current high-level LLM-as-a-Judge approach? The paper notes semantic evaluation is "limited to high-level judges" and calls for "evaluations involving judges with more fine-grained criteria."

### Open Question 3
To what extent does optimization for syntactic correctness during refinement cause semantic divergence from ground-truth formalization? The paper observes that while formal refinement improves pass rates, "some metric scores decrease... as optimizing for syntactic correctness does not always align with the ground-truth formalization."

## Limitations

- Semantic evaluation relies heavily on LLM-as-Judge methods rather than formal ground truth verification, introducing uncertainty about true quality of alignments
- Performance gap between model strengths reveals that weaker models struggle to recover from errors, creating reliability issues in constrained model deployment scenarios
- Modularity creates overhead and practical deployment considerations are not fully explored, with acknowledged tradeoff between modularity and latency

## Confidence

- **High Confidence**: Iterative error correction mechanism is well-supported by experimental evidence, particularly Pass Rate improvements when Hard Critique and Refinement are active
- **Medium Confidence**: Semantic alignment claims via LLM-as-Judge are supported by proxy metrics but lack formal ground truth validation
- **Medium Confidence**: Modularity and extensibility claims are demonstrated through framework design but performance overhead is not quantitatively measured

## Next Checks

1. **Formal Semantic Verification**: Implement small-scale formal verification check for semantic alignment on subset of miniF2F problems where ground truth proofs exist. Compare LLM-as-Judge scores against actual theorem prover verification.

2. **Cross-Model Refinement Analysis**: Design experiment where strong model (GPT-4.1-mini) performs refinement on code generated by weak model (Qwen2.5-7B), and vice versa, to validate hybrid strategy generalization.

3. **Dependency Resolution Stress Test**: Create test suite of formalization problems requiring complex library imports and missing dependencies. Measure system's ability to resolve these dependencies through ImportRetrievalAgent and track for import loops or successful convergence.