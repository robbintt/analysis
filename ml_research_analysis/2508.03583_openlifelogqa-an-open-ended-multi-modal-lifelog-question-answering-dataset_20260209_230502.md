---
ver: rpa2
title: 'OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset'
arxiv_id: '2508.03583'
source_url: https://arxiv.org/abs/2508.03583
tags:
- lifelog
- dataset
- data
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenLifelogQA, a novel open-ended multi-modal
  question-answering dataset for lifelog data. The dataset comprises 14,187 question-answer
  pairs based on 27,705 event descriptions extracted from an 18-month lifelog dataset
  containing 725,606 images and metadata.
---

# OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset

## Quick Facts
- arXiv ID: 2508.03583
- Source URL: https://arxiv.org/abs/2508.03583
- Reference count: 29
- Key outcome: OpenLifelogQA dataset with 14,187 open-ended QA pairs on lifelog data; baseline LLaVA-NeXT-Interleave achieves 89.7% BERTScore, 25.87% ROUGE-L, and 3.9665 LLM Score.

## Executive Summary
This paper introduces OpenLifelogQA, a novel open-ended multi-modal question-answering dataset for lifelog data. The dataset comprises 14,187 question-answer pairs based on 27,705 event descriptions extracted from an 18-month lifelog dataset containing 725,606 images and metadata. The questions cover three types: atomic, temporal, and aggregation, with a focus on practical, real-world applications. A baseline experiment using the LLaVA-NeXT-Interleave model achieved competitive performance with an average BERT Score of 89.7%, ROUGE-L of 25.87%, and LLM Score of 3.9665. The dataset is released under a Creative Commons license to support research into lifelog technologies and personal chat-based assistants.

## Method Summary
OpenLifelogQA is constructed from 18 months of lifelog data (725,606 images, metadata) from a single user. The data is segmented into 27,705 events with human-annotated descriptions, timestamps, and representative images. QA pairs are generated by volunteers and GPT-4o, then filtered using LLaMa 3.1 8B for quality (score ≥3). The baseline uses LLaVA-NeXT-Interleave (7B) with zero-shot inference, providing question + ground-truth event contexts + images. Performance is evaluated with BERTScore, ROUGE-L, and GPT-4o-based LLM Score.

## Key Results
- Dataset contains 14,187 QA pairs covering atomic, temporal, and aggregation question types.
- Baseline LLaVA-NeXT-Interleave achieves 89.7% BERTScore, 25.87% ROUGE-L, and 3.9665 LLM Score.
- Aggregation questions show lowest performance (LLM Score 3.34), suggesting difficulty with multi-event reasoning.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Modal Context Grounding for Answer Generation
- Claim: Providing interleaved images and textual context descriptions enables the multi-modal LLM to ground answers in both visual and temporal evidence.
- Mechanism: The baseline system supplies the question, ground-truth event descriptions (time, location, activity), and associated images to LLaVA-NeXT-Interleave, which processes them jointly to generate open-ended answers.
- Core assumption: The visual encoder (SigLIP) and language model (Qwen base) can jointly reason across image patches and temporal metadata when presented in an interleaved format.
- Evidence anchors:
  - [abstract]: "A baseline experiment is reported for this dataset with competitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665 LLM Score from LLaVA-NeXT-Interleave 7B model."
  - [section 5.1]: "We provide the question, groundtruth contexts, and the images of these contexts... to the model and ask for an answer."
  - [corpus]: Weak corpus evidence—no direct comparisons to image-only or text-only baselines in retrieved papers.
- Break condition: If retrieval fails to provide correct context events, the QA model will answer from irrelevant or missing information.

### Mechanism 2: Event Segmentation as Structured Retrieval Units
- Claim: Segmenting continuous lifelog data into discrete events with start times and representative images creates tractable retrieval targets and reduces answer-space complexity.
- Mechanism: Human annotators identify event boundaries (e.g., "working on laptop" from 12:23:42 to next event start), generating first-person descriptions that serve as grounded contexts for QA pairs.
- Core assumption: Events can be reliably identified by human reviewers from PoV image sequences and metadata, and these boundaries align with natural question scopes.
- Evidence anchors:
  - [section 3.2]: "An event is a sequence of images illustrating a single activity of the lifelogger... Each event is associated with a start time and a representative image at that time."
  - [section 4.1]: "The average number of events for each QA pair is 1.66 events."
  - [corpus]: LSC-ADL paper confirms activity-level annotations capture temporal relationships in lifelogs.
- Break condition: Ambiguous event boundaries (e.g., overlapping or rapidly switching activities) may cause incorrect context retrieval.

### Mechanism 3: Hybrid Human-LLM Annotation with LLM-Based Quality Filtering
- Claim: Combining human annotators with GPT-4o generation, followed by automated accuracy scoring, enables scalable QA construction while maintaining answer correctness.
- Mechanism: Volunteers generate 10 QA pairs per day; GPT-4o generates 20. Humans edit LLM outputs. LLaMa 3.1 8B Instruct rates each answer 1–5; pairs scoring <3 are corrected or discarded.
- Core assumption: LLaMa 3.1 8B can reliably judge answer accuracy against ground-truth contexts, and human editors catch residual errors.
- Evidence anchors:
  - [section 3.2]: "We asked volunteers and GPT-4o to generate the event ID... along with the QA."
  - [section 3.2]: "We use a LLaMa 3.1 8B Instruct to rate on a scale of 1 to 5... if the score is lower than 3, we ask the volunteers to correct."
  - [corpus]: No corpus papers validate this specific hybrid annotation approach for lifelog QA.
- Break condition: Systematic LLM biases or domain-specific errors may pass the filter if the grading model shares similar blind spots.

## Foundational Learning

- Concept: Multi-Modal Large Language Models (MLLMs)
  - Why needed here: The baseline uses LLaVA-NeXT-Interleave, which processes interleaved images and text. Understanding vision encoders, projection layers, and multi-image attention is essential.
  - Quick check question: Can you explain how SigLIP visual features are aligned with Qwen's embedding space for multi-image reasoning?

- Concept: Retrieval-Augmented Generation (RAG) for QA
  - Why needed here: The paper provides ground-truth contexts in experiments but explicitly notes real-world deployment requires retrieval. Understanding dense retrieval, reranking, and context window management is critical.
  - Quick check question: How would you retrieve relevant events from 27,705 candidates given a natural language question?

- Concept: Open-Ended QA Evaluation Metrics
  - Why needed here: The paper uses three evaluation approaches (ROUGE-L, BERTScore, LLM Score), each measuring different aspects. The 0.5B model paradox—higher lexical scores but lower LLM Score—shows why multiple metrics matter.
  - Quick check question: Why might a semantically similar answer ("I spent 17 minutes") receive a low LLM Score when the ground truth says "23 minutes"?

## Architecture Onboarding

- Component map: Data layer (725,606 PoV images + metadata) -> Event segmentation layer (27,705 events with timestamps and images) -> QA construction layer (14,187 QA pairs) -> Inference pipeline (Question -> (Retrieval) -> Context events + images -> LLaVA-NeXT-Interleave -> Answer) -> Evaluation layer (ROUGE-L, BERTScore, LLM Score)

- Critical path: 1. Retrieve correct events given question (currently bypassed with ground truth) 2. Interleave retrieved images with textual context descriptions 3. Generate answer via multi-modal reasoning 4. Evaluate factual accuracy, not just lexical overlap

- Design tradeoffs: Single lifelogger data (privacy simpler, but limited generalization) vs. multi-user (better coverage, higher privacy risk); Open-ended answers (natural, informative) vs. closed/extractive (easier evaluation); Human-only annotation (high quality, costly) vs. LLM-assisted (scalable, quality control needed); Ground-truth context in experiments (isolates QA capability) vs. end-to-end retrieval+QA (realistic, compounds errors)

- Failure signatures: High BERTScore + low LLM Score: Lexical/semantic overlap without factual accuracy (e.g., "17 minutes" vs. "23 minutes" contextually similar but wrong); Aggregation question failures (LLM Score 3.34 vs. 4.26 for atomic): Calculation/counting errors across multiple events; Temporal reasoning drift: Incorrect before/after relationships when events are close in time

- First 3 experiments: 1. Implement a retrieval component (e.g., CLIP-based dense retrieval on event descriptions) and measure retrieval recall@k before QA evaluation. 2. Fine-tune LLaVA-NeXT-Interleave on the training split (11,159 QA pairs) to improve answer style consistency and aggregation accuracy. 3. Ablate image inputs (text-only context) vs. full multi-modal to quantify visual grounding contribution across question types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can temporal reasoning and chain-of-thought methods improve the performance of multi-modal models on complex aggregation questions in lifelog datasets?
- Basis in paper: [explicit] The conclusion states, "In future work, we aim to explore temporal reasoning and chain-of-thought methods to better tackle complex aggregation questions."
- Why unresolved: The baseline results (Table 6) show that aggregation questions have the lowest LLM Score (3.3362) compared to atomic and temporal types, indicating that current models struggle with the calculation and synthesis required for these questions.
- What evidence would resolve it: An experiment comparing the baseline model's performance on aggregation questions against a version fine-tuned or prompted with explicit chain-of-thought reasoning steps.

### Open Question 2
- Question: How does model performance degrade when the QA system must retrieve relevant contexts from the entire lifelog corpus rather than utilizing provided ground-truth contexts?
- Basis in paper: [explicit] Section 5.1 notes that the baseline experiment provided ground-truth contexts, but "In real-world applications, the contexts are not always available, and there is a need to retrieve the correct contexts for the QA model."
- Why unresolved: The current baseline isolates the answering capability by bypassing the retrieval step. It is unclear if the model can maintain its performance (e.g., 3.9665 LLM Score) when it must first locate the correct events among 18 months of data.
- What evidence would resolve it: Results from an end-to-end pipeline evaluation where the model must retrieve relevant images/metadata from the full dataset before generating an answer.

### Open Question 3
- Question: Are standard semantic similarity metrics (BERTScore, ROUGE-L) reliable for evaluating lifelog QA, given they may favor lexical overlap over factual correctness?
- Basis in paper: [inferred] Section 5.2 highlights a discrepancy where the 0.5B model achieved higher BERTScore (90.63%) than the 7B model but a lower LLM Score, as it generated lexically similar but factually incorrect answers (e.g., "25 minutes" vs. "5 minutes").
- Why unresolved: The paper suggests that high lexical similarity can mask calculation errors in aggregation questions, creating a false positive in evaluation.
- What evidence would resolve it: A correlation analysis between human evaluation scores and automatic metrics specifically on questions requiring numerical or temporal reasoning.

## Limitations
- Ground-truth context assumption: Results use provided contexts, not real retrieval, so end-to-end performance may degrade.
- Single-user bias: All data from one lifelogger over 18 months limits generalization across different habits and demographics.
- Evaluation metric opacity: LLM Score uses GPT-4o judging but exact rubric is unspecified, affecting reproducibility.

## Confidence
- High confidence: Dataset construction methodology (event segmentation, multi-modal grounding, hybrid annotation) is well-specified and reproducible given dataset access.
- Medium confidence: Baseline performance metrics are reliable for the experimental setup (ground-truth contexts provided), but may not generalize to retrieval-augmented deployment.
- Low confidence: Comparative advantage over existing lifelog QA approaches is unclear due to limited direct baseline comparisons in literature.

## Next Checks
1. Implement a retrieval-augmented pipeline (e.g., CLIP-based dense retrieval on event descriptions) and measure end-to-end performance degradation from the ground-truth context baseline.
2. Conduct cross-validation across different lifeloggers or augment the dataset with synthetic variations to assess generalization limits.
3. Compare the 0.5B and 7B model performance on the aggregation question subset to understand if model scale affects multi-event reasoning accuracy.