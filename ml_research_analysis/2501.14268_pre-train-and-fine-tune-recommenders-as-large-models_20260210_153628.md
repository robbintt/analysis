---
ver: rpa2
title: 'Pre-train and Fine-tune: Recommenders as Large Models'
arxiv_id: '2501.14268'
source_url: https://arxiv.org/abs/2501.14268
tags:
- information
- fine-tuning
- knowledge
- learning
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes treating recommenders as large pre-trained
  models and fine-tuning them with an Information-Aware Adaptive Kernel (IAK) technique
  to address multi-domain learning challenges in recommendation systems. The authors
  first provide a theoretical explanation of fine-tuning from the information bottleneck
  perspective, decomposing it into knowledge compression and knowledge matching phases.
---

# Pre-train and Fine-tune: Recommenders as Large Models

## Quick Facts
- arXiv ID: 2501.14268
- Source URL: https://arxiv.org/abs/2501.14268
- Reference count: 40
- Primary result: OLR+IAK achieves +1.09% CTR-AUC and +0.70% CTCVR-AUC improvements over SOTA baselines in multi-domain recommendation

## Executive Summary
This paper proposes treating recommenders as large pre-trained models and fine-tuning them with an Information-Aware Adaptive Kernel (IAK) technique to address multi-domain learning challenges in recommendation systems. The authors first provide a theoretical explanation of fine-tuning from the information bottleneck perspective, decomposing it into knowledge compression and knowledge matching phases. IAK is designed as an encoder-decoder structure that compresses general business knowledge and learns domain-specific knowledge through Gaussian approximation. Extensive offline experiments show that OLR+IAK achieves significant improvements over state-of-the-art baselines, with average CTR-AUC increases of +1.09% and CTCVR-AUC increases of +0.70% across multiple domains. The method has been deployed on a billion-scale online food platform, yielding considerable business profits with improvements in order rate and net GMV.

## Method Summary
The method involves pre-training a large multi-task recommender (OLR) on general data, then fine-tuning with domain-specific IAK modules. IAK uses an encoder-decoder MLP structure where the encoder learns compressed representations of pre-trained knowledge relevant to the target domain, and the decoder combines these with domain-specific patterns. Training optimizes a composite objective combining cross-entropy loss with KL divergence regularization that constrains parameter deviation from initialization. The frozen OLR serves multiple domains through parallel execution of domain-specific IAKs, with outputs selected by domain ID during inference.

## Key Results
- OLR+IAK achieves average +1.09% CTR-AUC and +0.70% CTCVR-AUC improvements over state-of-the-art baselines across multiple domains
- The method successfully addresses the pseudo cold-start problem by using one week of domain-specific data for fine-tuning
- IAK demonstrates model-agnostic applicability, improving MMoE, ESMM, and other baseline architectures
- Online deployment on a billion-scale food platform yielded significant business profits with improved order rates and net GMV

## Why This Works (Mechanism)

### Mechanism 1: Information Bottleneck-guided Fine-tuning
The paper formalizes fine-tuning through an information bottleneck objective: Ĝ = arg min -I(Ĝ;T) + βI(Ĝ;G), where the model learns to compress general business knowledge (G) while retaining information relevant to the downstream task (T). During training, the IAK module's encoder learns to extract domain-relevant features from frozen OLR representations. This works because the distribution shift between general training data and specific domains can be captured by compressing away irrelevant knowledge rather than learning entirely new representations.

### Mechanism 2: Two-Phase Knowledge Adaptation
Effective fine-tuning decomposes into sequential knowledge compression (filtering task-irrelevant knowledge) and knowledge matching (learning domain-specific patterns). The IAK encoder-decoder structure explicitly approximates both phases: (1) the encoder projects OLR representations into a compressed bottleneck space via Gaussian-constrained optimization, and (2) the decoder combines compressed knowledge with domain-specific learned transformations to produce final predictions. This two-phase approach allows the model to efficiently adapt to new domains without retraining the entire architecture.

### Mechanism 3: Plug-and-Play Domain Adaptation
A single frozen pre-trained recommender can serve multiple domains by training lightweight domain-specific adapters (IAKs) that modify intermediate representations. The OLR remains frozen while IAK modules (one per domain) are trained on domain-specific data. During inference, all IAKs execute in parallel, and the appropriate output is selected via domain ID. This avoids modifying the core model architecture or retraining on historical data, making it highly efficient for industrial deployment.

## Foundational Learning

- **Concept**: Information Bottleneck Theory
  - Why needed here: The entire theoretical framework rests on understanding how neural networks balance compression (reducing representation complexity) against prediction accuracy (preserving task-relevant information). The mutual information formulation I(X;Y) and the trade-off parameter β are central to deriving the IAK training objective.
  - Quick check question: If you increase β in equation (5), does the learned representation become more or less compressed? What does this imply about the domain-specific knowledge retained?

- **Concept**: Multi-Domain Learning vs. Transfer Learning
  - Why needed here: The paper positions fine-tuning as superior to traditional multi-domain learning for industrial settings because it avoids retraining the entire model when adding new domains. Understanding this distinction clarifies why the authors prioritize architectural non-invasiveness and data efficiency (one week of domain-specific data suffices).
  - Quick check question: Under what conditions would multi-domain learning (e.g., STAR topology with shared and specific parameters) outperform the proposed fine-tuning approach? Consider training data availability and domain relatedness.

- **Concept**: Gaussian Approximation for Variational Inference
  - Why needed here: The IAK training objective D_KL(w || w₀) relies on assuming the parameter distribution after training can be approximated as Gaussian N(μ,σ). This transforms an intractable functional optimization problem into tractable gradient descent on mean and variance parameters.
  - Quick check question: Why does the paper initialize w₀ ~ N(0,I) rather than using the pre-trained OLR weights? How does this initialization relate to the "knowledge compression" phase?

## Architecture Onboarding

- **Component map**:
```
Input Features → [Frozen OLR Embedding + Sequence Modules + MMoE + Debias] → Logits
                                                              ↓
                                                    [IAK Encoder: d=50]
                                                              ↓
                                                    [IAK Decoder]
                                                              ↓
                                              Domain-Specific Prediction
                                      
Training: OLR frozen, IAK trained on domain-specific data (1 week recommended)
Inference: All IAKs run in parallel → Select output by domain ID
```

- **Critical path**:
  1. **Pre-trained OLR Availability**: Requires existing industrial-scale recommender (~1B parameters, trained on hundreds of billions of samples) that has learned general business knowledge
  2. **Domain Data Collection**: Extract domain-specific subset (e.g., Region4 users, Time2 mealtime) covering at least one week to avoid pseudo cold-start issues
  3. **IAK Training**: Initialize encoder with N(0,I), train with modified objective H(Ŷ|Y) + D_KL(w || w₀), use dynamic batch-aware learning rate when training multiple IAKs together
  4. **Parallel Deployment**: Deploy all trained IAKs simultaneously; inference selects appropriate output via domain ID indicator function
  5. **Monitoring**: Track domain-specific CTR-AUC/CTCVR-AUC; watch for pseudo cold-start (users absent from fine-tuning data)

- **Design tradeoffs**:
  - **Bottleneck dimension (d_e)**: Larger values (50-100) improve performance with diminishing returns; smaller values (10-30) reduce inference cost but may lose domain-specific nuances. Paper recommends d_e=50 based on sensitivity analysis.
  - **Fine-tuning data span**: Longer spans reduce pseudo cold-start risk but increase training cost; paper finds one week balances performance and efficiency (Figure 3a/3b show plateau after ~7 days).
  - **Sample isolation vs. weighted mixing**: Completely isolating domain samples ignores cross-domain item correlations; weighted mixing (e.g., 70% target domain + 30% related domain) improves performance (Figure 3c) but complicates data pipeline.
  - **Single vs. multiple IAK training**: Training IAKs one-by-one is simpler but slower; joint training requires dynamic batch-aware learning rates (Equation 21) to handle sample imbalance across domains.

- **Failure signatures**:
  - **CTR-AUC drops below ZS-OLR baseline**: Indicates excessive compression (β too high) or insufficient domain data (increase training span)
  - **High variance across training runs**: Check batch composition—domains with few samples per batch need learning rate adjustment via Equation 21
  - **Specific user segments underperform**: Investigate pseudo cold-start—users absent from fine-tuning data cannot benefit from domain-specific IAK; consider extending time span or using weighted mixing with related domains
  - **Cross-topic scenarios (e.g., Region4 + Time2) degrade**: Verify cross-topic IAKs trained jointly; single-topic IAKs may not capture topic interactions (Table 4 shows +0.3-0.6% gains with explicit cross-topic training)

- **First 3 experiments**:
  1. **Baseline Ablation**: Train OLR+IAK on a single high-variance domain (e.g., Region4 with +1.3% CTR-AUC gain in Table 2). Compare against: (a) ZS-OLR, (b) OLR with simple MLP adapter (no information bottleneck), (c) full OLR retraining on domain data. This validates the information bottleneck contribution versus naive fine-tuning and establishes training efficiency.
  
  2. **Bottleneck Sensitivity Sweep**: Train IAKs with d_e ∈ {10, 30, 50, 80, 100} on 3 diverse domains (one each from multi-scene, multi-period, multi-region). Plot CTR-AUC vs. d_e to identify saturation point. Verify if saturation point differs across domain types (e.g., multi-scene may need larger capacity due to behavioral differences).
  
  3. **Pseudo Cold-Start Characterization**: On a target domain, train IAKs with varying time spans (1 day, 3 days, 7 days, 14 days). Measure: (a) overall AUC, (b) AUC specifically for users with <5 interactions in fine-tuning data, (c) AUC for users with >50 interactions. This quantifies the pseudo cold-start issue and validates the one-week recommendation.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal method for mixing training samples from different domains across disparate topics (e.g., mixing region data with time data) to maximize knowledge transfer?
  - Basis in paper: The conclusion states, "Future work may include exploring a better method to mix different domains in different topics."
  - Why unresolved: While the paper demonstrates the effectiveness of cross-topic testing (Section 7.4), it relies on standard domain-specific fine-tuning. The authors explicitly identify the need for a more sophisticated mixing strategy.
  - What evidence would resolve it: Development of a dynamic weighting or attention-based mechanism for multi-topic batch training that consistently outperforms the current strategy of training tasks separately or using fixed weights.

- **Open Question 2**: How can the "pseudo cold start" issue be resolved without simply increasing the time span of the fine-tuning dataset?
  - Basis in paper: Section 6 reports the "pseudo cold start issue" where a user in the pre-trained model is absent from the fine-tuning domain data, causing the IAK to lack specific knowledge for that user.
  - Why unresolved: The exploration in Section 7.6 suggests that increasing the time span of data helps, but the authors imply this is a trade-off rather than a solution, as it increases training costs.
  - What evidence would resolve it: A modification to the IAK architecture or a meta-learning approach that allows the kernel to generalize to users missing from the downstream domain data without requiring larger datasets.

- **Open Question 3**: Is the KL-divergence between trained and initial parameter distributions a universally valid proxy for information compression in recommendation fine-tuning?
  - Basis in paper: Section 5.3 relies on "Assumption 1" (positive correlation between parameter distance and information) to derive the Gaussian approximation for the information bottleneck.
  - Why unresolved: The paper treats this assumption as a simplification to make the objective function solvable. It remains unproven whether parameter drift is always semantically meaningful or merely noise in the context of user interests.
  - What evidence would resolve it: A theoretical analysis or empirical study correlating the KL-divergence of the kernel parameters with explicit mutual information estimates between the input features and the latent representations.

## Limitations
- The evaluation relies heavily on offline AUC metrics, which may not fully capture online performance in dynamic user environments
- The proprietary OLR architecture remains undisclosed, limiting reproducibility of the primary results
- The analysis of when fine-tuning outperforms multi-domain learning (versus traditional shared-specific parameter approaches) remains theoretical rather than empirically validated

## Confidence
- **High confidence**: The information bottleneck theoretical framework and its application to fine-tuning decomposition (knowledge compression + knowledge matching) is well-formulated and mathematically rigorous
- **Medium confidence**: The IAK architecture and training procedure are clearly specified, but the optimal β hyperparameter remains unspecified, requiring empirical tuning for different domains
- **Low confidence**: The claim that one week of domain-specific data is sufficient for effective fine-tuning across all scenarios requires more extensive validation, particularly for domains with sparse user interactions or rapidly changing preferences

## Next Checks
1. Conduct ablation studies comparing IAK fine-tuning against STAR topology multi-domain learning under varying data availability conditions to empirically validate the theoretical advantages
2. Implement online A/B testing of the fine-tuned recommender in a controlled environment to measure business impact (order rate, GMV) versus offline AUC improvements
3. Systematically investigate the pseudo cold-start problem by analyzing performance degradation across user activity levels and testing alternative data mixing strategies beyond the proposed 70/30 weighted sampling