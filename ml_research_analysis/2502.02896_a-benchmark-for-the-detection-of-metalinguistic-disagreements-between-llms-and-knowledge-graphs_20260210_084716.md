---
ver: rpa2
title: A Benchmark for the Detection of Metalinguistic Disagreements between LLMs
  and Knowledge Graphs
arxiv_id: '2502.02896'
source_url: https://arxiv.org/abs/2502.02896
tags:
- metalinguistic
- knowledge
- arxiv
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a benchmark for detecting metalinguistic disagreements
  between large language models (LLMs) and knowledge graphs (KGs). The authors hypothesize
  that errors in LLM-based fact extraction from KGs can stem from disagreements about
  word meaning rather than factual errors.
---

# A Benchmark for the Detection of Metalinguistic Disagreements between LLMs and Knowledge Graphs

## Quick Facts
- arXiv ID: 2502.02896
- Source URL: https://arxiv.org/abs/2502.02896
- Reference count: 40
- Key outcome: Proposed benchmark for detecting metalinguistic disagreements between LLMs and KGs using two-stage zero-shot chain-of-thought classification; initial results show FNR 0.104-0.504 and MDR 0.04-0.264 across 9 LLMs on 250 T-REx triples

## Executive Summary
This paper addresses the challenge of distinguishing metalinguistic disagreements (disagreements about word meaning) from factual errors in LLM-KG interactions. Using a two-stage zero-shot chain-of-thought classification pipeline, the authors detect when LLMs disagree with KG triples due to predicate interpretation differences rather than factual errors. Testing on 250 T-REx triples across 9 LLMs revealed significant metalinguistic disagreement rates (0.04-0.264), with examples showing divergent interpretations of predicates like "followed by" versus "replaced by." The work establishes a foundation for benchmarking metalinguistic disagreement detection and proposes human annotation as the next critical step for validation.

## Method Summary
The method employs a two-stage zero-shot chain-of-thought classification pipeline. First, a truth-value classifier evaluates KG triples with context and generates rationales for negative classifications. Second, an LLM-as-a-judge (gpt-4o-2024-05-13) analyzes these rationales to classify whether disagreements stem from metalinguistic interpretation differences. The approach was tested on 250 triples sampled from the T-REx dataset (100 Wikipedia abstracts with aligned Wikidata triples), measuring false negative rates and metalinguistic disagreement rates across 9 different LLMs.

## Key Results
- False negative rates ranged from 0.104 to 0.504 across 9 tested LLMs
- Metalinguistic disagreement rates ranged from 0.04 to 0.264
- Predicate interpretation differences were the primary source of metalinguistic disagreements (e.g., "followed by" vs. "replaced by")
- The two-stage detection pipeline successfully identified interpretable rationales for metalinguistic disagreements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage zero-shot chain-of-thought classification pipeline can detect whether LLM-KG disagreements stem from semantic interpretation differences rather than factual errors.
- Mechanism: The first classifier evaluates triple truth given context and generates a rationale. The second classifier (LLM-as-a-judge) analyzes that rationale to classify disagreement type. This separates "I disagree with the fact" from "I interpret this predicate differently."
- Core assumption: LLMs can reliably articulate their interpretive reasoning about predicate and concept meanings through chain-of-thought generation, and another LLM can accurately classify this reasoning.
- Evidence anchors:
  - [abstract] "Using the T-REx dataset, they tested 9 LLMs on 250 knowledge triples and found false negative rates ranging from 0.104 to 0.504, with metalinguistic disagreement rates of 0.04 to 0.264"
  - [section] "each result was then processed by a second zero-shot chain-of-thought classifier (using gpt-4o-2024-05-13) acting as an LLM-as-a-judge, to classify whether the truth-value-assigning classifier's rationale indicated a metalinguistic disagreement"
  - [corpus] Limited direct corpus support for this specific two-stage metalinguistic detection; SHROOM uses similar LLM-as-judge methodology for hallucination detection
- Break condition: If LLM-as-a-judge cannot reliably distinguish metalinguistic from factual disagreements (authors explicitly acknowledge this limitation), or if detected "disagreements" are artifacts of LLM processing rather than genuine semantic differences.

### Mechanism 2
- Claim: Metalinguistic disagreements manifest primarily through divergent predicate interpretations, detectable when LLM rationales propose alternative predicates or apply narrower/wider definitions than KG schema intends.
- Mechanism: LLMs trained on natural language may interpret predicates like "followed by" differently than formal KG definitions. When rejecting triples, they may suggest alternative predicates (e.g., "replaced by") or stricter criteria (e.g., "primary ingredient only"), revealing semantic rather than factual disagreement.
- Core assumption: Predicate interpretation variability between natural language usage patterns and formal ontology definitions is systematic and detectable through rationale analysis.
- Evidence anchors:
  - [abstract] "Examples showed LLMs disagreed on predicate meanings like 'followed by' versus 'replaced by.'"
  - [section] Table 2: LLM rationale states "The predicate 'followed by' implies that Mary II succeeded James II directly, which is not the case" and "'made from material' is not an appropriate relation when the object is not a primary ingredient"
  - [corpus] Related work on KG-LLM alignment (Enrich-on-Graph, MultiHal) addresses semantic gaps but frames these as hallucination issues rather than metalinguistic disagreement per se
- Break condition: If predicate disagreement rationales are incoherent, or if disagreements reduce to random errors rather than systematic interpretation differences.

### Mechanism 3
- Claim: Human-validated benchmarks with inter-annotator agreement metrics can establish ground truth for metalinguistic vs. factual disagreement classification.
- Mechanism: Crowdsourced human annotators review KG-derived statements with context, indicating disagreement type. This creates gold standard data for evaluating automated detection systems, following SHROOM benchmark methodology.
- Core assumption: Humans can reliably distinguish metalinguistic from factual disagreements, and this distinction generalizes to LLM evaluation contexts.
- Evidence anchors:
  - [abstract] "The proposed benchmark aims to distinguish metalinguistic from factual disagreements through human annotation and evaluation across multiple KGs, addressing limitations of the initial study"
  - [section] "Human annotators will be presented with a summary of a Wikipedia page and a statement generated from the Wikidata knowledge graph triple... the annotator must indicate if they disagree with the statement, and if so, whether they disagree on the factuality of the statement or the meaning of any of the terms"
  - [corpus] SHROOM hallucination benchmark provides precedent for crowdsourced human annotation methodology in related detection tasks
- Break condition: If inter-annotator agreement is low for the metalinguistic/factual distinction, or if the distinction proves too context-dependent for reliable human judgment.

## Foundational Learning

- Concept: Metalinguistic vs. Factual Disagreement
  - Why needed here: This is the paper's core distinction. Factual disagreements concern what is true in the world (e.g., "Is Sarah taller than John?"). Metalinguistic disagreements concern word meaning (e.g., "What height counts as 'tall'?"). Without this distinction, all LLM-KG mismatches appear as hallucinations or factual errors.
  - Quick check question: An LLM rejects "chocolate made from material sugar" — is it denying sugar is in chocolate, or arguing about what "made from material" should mean?

- Concept: KG Predicate Semantics
  - Why needed here: The paper demonstrates that predicates are the primary site of metalinguistic disagreement. KG predicates like P156 ("followed by") have formal definitions that may conflict with natural language usage patterns in LLM training data.
  - Quick check question: If Wikidata defines "followed by" as requiring immediate succession, but an LLM's training data uses "followed by" more loosely, what happens when evaluating a historical succession triple?

- Concept: Two-Component Semantics
  - Why needed here: The paper situates its contribution within debates about LLM meaning and belief representation. Two-component semantics (truth conditions + aboutness/topic) provides theoretical framing for whether LLMs can generate meaningful statements or have beliefs relative to KGs.
  - Quick check question: If an LLM's "disagreement" with a KG triple is metalinguistic rather than factual, what does that imply about whether the LLM "believes" the triple?

## Architecture Onboarding

- Component map: KG-text alignment dataset (T-REx) -> Stage 1 Classifier (truth-value) -> Stage 2 Classifier (LLM-as-a-judge) -> Aggregation Layer (FNR/MDR) -> Validation Layer (human annotation) -> Evaluation Layer (inter-annotator agreement)

- Critical path:
  1. Sample triples from KG-text alignment dataset with stratification across predicate types
  2. For each triple, execute Stage 1 classifier with context to generate truth value + rationale
  3. For negative classifications, execute Stage 2 classifier to detect metalinguistic disagreement
  4. Compute aggregate statistics (FNR, MDR) by LLM, predicate type, domain
  5. (Proposed) Collect human annotations on subset to validate Stage 2 classifier accuracy

- Design tradeoffs:
  - LLM-as-a-judge vs. human annotation: speed/cost vs. reliability (authors acknowledge LLM judges may conflate error types)
  - Sample size vs. coverage: 250 triples sufficient for proof-of-concept; larger scale needed for robust conclusions
  - Single vs. multi-KG evaluation: Wikidata-only limits generalizability; multi-KG proposed
  - Zero-shot vs. few-shot: Zero-shot used for generality; few-shot may improve rationale quality but introduces prompt engineering dependencies

- Failure signatures:
  - High false positive MDR: LLM-as-a-judge flags hallucinations or context misinterpretations as metalinguistic disagreements
  - Low inter-annotator agreement: Humans cannot reliably make the metalinguistic/factual distinction
  - KG-specific artifacts: Results don't transfer across KGs with different schema conventions
  - Predicate-type clustering: Metalinguistic disagreements concentrate in specific predicate types (temporal, gradable) — may indicate boundary conditions

- First 3 experiments:
  1. Human validation study: Annotate 100-150 triples from the initial experiment to measure LLM-as-a-judge precision/recall for metalinguistic detection
  2. Cross-KG evaluation: Replicate experiment on DBpedia and ConceptNet subsets to test whether MDR varies by knowledge graph schema design
  3. Predicate type analysis: Construct targeted probe sets for ambiguous predicates, temporal relations, and gradable adjectives to characterize which predicate classes are most susceptible to metalinguistic disagreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can metalinguistic disagreement be reliably distinguished from other error types such as hallucinations and context misinterpretations in LLM outputs?
- Basis in paper: [explicit] The authors list as a key limitation that "What's interpreted as metalinguistic disagreement could potentially be other types of errors or inconsistencies in LLM outputs, such as hallucinations or context misinterpretations."
- Why unresolved: The current study uses LLM-as-a-judge without human validation, making it impossible to determine if detected disagreements are genuine metalinguistic disputes or artifacts of other error types.
- What evidence would resolve it: A benchmark with human-annotated examples that clearly separate metalinguistic disagreements from factual disagreements, hallucinations, and context misinterpretations.

### Open Question 2
- Question: To what extent do LLM-as-a-judge evaluations align with human judgments for detecting metalinguistic disagreement?
- Basis in paper: [explicit] The authors state "The detection of metalinguistic disagreement relies on using an LLM-as-a-judge, which may not be a reliable substitute for human judgment" and cite research questioning this reliability.
- Why unresolved: No human validation was performed in the initial study; the authors explicitly call for "Human review at scale" to validate whether LLMs' classifications align with human judgments.
- What evidence would resolve it: Inter-annotator agreement metrics comparing LLM-as-a-judge classifications against human expert annotations across a substantial dataset.

### Open Question 3
- Question: Are metalinguistic disagreements more prevalent in certain knowledge domains or with specific predicate types (temporal, gradable, ambiguous)?
- Basis in paper: [inferred] The authors specify that their proposed benchmark should use "triples from different knowledge graphs spanning multiple knowledge domains" to "test if metalinguistic disagreements are more prevalent in certain areas," but this analysis was not conducted.
- Why unresolved: The initial experiment used only Wikidata triples from Wikipedia abstracts without systematic variation across domains or predicate types.
- What evidence would resolve it: A stratified benchmark sampling across multiple KGs (e.g., DBpedia, YAGO, Freebase) with analysis of disagreement rates by domain and predicate characteristics.

### Open Question 4
- Question: What is the actual prevalence of metalinguistic disagreement between LLMs and KGs at scale?
- Basis in paper: [explicit] The authors identify "Limited sample size" as a significant shortcoming, noting "The experiment uses a relatively small sample of 250 triples. A larger-scale study is needed to draw more robust conclusions."
- Why unresolved: Current estimates (MDR 0.04–0.264 across 9 LLMs) come from a small sample and may not generalize to broader KG-LLM interactions.
- What evidence would resolve it: Scaling the evaluation to thousands of triples across multiple LLMs and KGs with human-validated annotations to establish population-level prevalence rates.

## Limitations
- Reliance on LLM-as-a-judge without human validation for metalinguistic disagreement detection
- Limited sample size (250 triples) from a single knowledge graph (Wikidata)
- Potential conflation of metalinguistic disagreements with other error types like hallucinations

## Confidence

- **High confidence**: The basic premise that metalinguistic disagreements exist between LLMs and KGs, and that these differ from factual errors (supported by observable predicate interpretation differences in Table 2)
- **Medium confidence**: The two-stage detection methodology works as intended for identifying metalinguistic disagreements, pending human validation results
- **Low confidence**: The reported FNR and MDR ranges accurately characterize the true distribution of metalinguistic disagreements across LLMs and predicates, due to limited sample size and single-KG evaluation

## Next Checks
1. **Human annotation validation**: Conduct human evaluation on 100-150 triples to measure inter-annotator agreement and assess LLM-as-a-judge accuracy for metalinguistic detection
2. **Cross-KG replication**: Replicate the benchmark on DBpedia and ConceptNet to test whether metalinguistic disagreement patterns vary with knowledge graph schema design and predicate conventions
3. **Predicate-type clustering analysis**: Construct targeted probe sets for ambiguous predicates (temporal relations, gradable adjectives) to characterize which predicate classes are most susceptible to metalinguistic disagreement