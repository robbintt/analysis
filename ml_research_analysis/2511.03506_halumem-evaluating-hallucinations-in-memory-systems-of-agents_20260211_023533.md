---
ver: rpa2
title: 'HaluMem: Evaluating Hallucinations in Memory Systems of Agents'
arxiv_id: '2511.03506'
source_url: https://arxiv.org/abs/2511.03506
tags:
- memory
- memories
- information
- question
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces HaluMem, the first operation-level benchmark\
  \ for evaluating hallucinations in memory systems. It defines three tasks\u2014\
  memory extraction, updating, and question answering\u2014and constructs user-centric\
  \ datasets (HaluMem-Medium and HaluMem-Long) containing ~15k memory points and ~3.5k\
  \ multi-type questions."
---

# HaluMem: Evaluating Hallucinations in Memory Systems of Agents

## Quick Facts
- arXiv ID: 2511.03506
- Source URL: https://arxiv.org/abs/2511.03506
- Reference count: 40
- Primary result: First operation-level benchmark for evaluating hallucinations in memory systems, revealing critical bottlenecks in extraction and updating that propagate to question answering

## Executive Summary
This paper introduces HaluMem, the first operation-level benchmark for evaluating hallucinations in memory systems. It defines three tasks—memory extraction, updating, and question answering—and constructs user-centric datasets (HaluMem-Medium and HaluMem-Long) containing ~15k memory points and ~3.5k multi-type questions. Evaluation reveals that memory systems tend to generate and accumulate hallucinations during extraction and updating, which propagate to question answering. Results show that MemOS achieves the highest memory extraction F1 score (~79.7%), while most systems exhibit low QA accuracy (<70%) and high omission rates (>50%) on long-context data, highlighting critical challenges in long-term memory reliability and update consistency.

## Method Summary
HaluMem evaluates memory systems through three sequential operations: memory extraction (parsing dialogue into memories), memory updating (modifying existing memories), and memory question answering (retrieving and reasoning with memories). The benchmark uses GPT-4o for automated scoring with specific prompt templates, retrieving 10 most relevant memories for update verification and 20 for QA. Two datasets are provided: HaluMem-Medium with 1.5k average turns and 1M token contexts, and HaluMem-Long with 2.6k average turns and 1M token contexts including adversarial distractors. Systems are evaluated on extraction F1, recall, precision, false memory resistance, updating accuracy, and QA performance across basic fact recall, multi-hop inference, and dynamic preference tracking tasks.

## Key Results
- MemOS achieves highest memory extraction F1 score (~79.7%) but low False Memory Resistance (28.85% FMR)
- Most systems show QA accuracy below 70% with omission rates exceeding 50% on long-context data
- Performance degrades significantly on HaluMem-Long, with Mem0 dropping from 42.91% to 3.23% recall
- All systems struggle with multi-hop inference and dynamic preference tracking questions
- High recall extraction strategies correlate with poor resistance to adversarial distractors

## Why This Works (Mechanism)

### Mechanism 1: Operation-Level Error Localization
The HaluMem framework decouples the memory pipeline into extraction, updating, and QA stages, comparing system outputs against stage-specific gold standards rather than just final answers. This isolates "Amnesia" (omission in extraction) from "Fabrication" (hallucination in generation). The core assumption is that hallucinations observed in QA stages are causally linked to specific failures in upstream memory operations rather than purely model reasoning errors.

### Mechanism 2: The Extraction Bottleneck
Performance in Memory QA appears upper-bounded by the recall and accuracy of the initial Memory Extraction stage. High omission rates in extraction propagate directly as "unknown" answers in QA. The assumption is that retrieval mechanisms are functional and primary failures stem from absence of memories in the database rather than retrieval inability. Systems with low extraction recall show drastically reduced QA accuracy, implying retrieval failure often stems from initial storage failure.

### Mechanism 3: Adversarial Distractor Filtering
High-recall extraction strategies may degrade system reliability by failing to filter "distractor memories" (unconfirmed information introduced by AI in dialogue history). The "False Memory Resistance" metric measures this. Systems prioritizing high coverage showed lower FMR, suggesting a tradeoff between coverage and precision against noise. The assumption is that information not explicitly confirmed or corrected by users should not be hardened into long-term memory.

## Foundational Learning

- **Concept: Memory Lifecycle (CRUD vs. E/U/Q)**
  - Why needed: Standard databases use Create/Read/Update/Delete. HaluMem re-frames this for LLMs as Extraction (Create), Updating (Modify), and QA (Read/Reason). Understanding this distinction is vital because "Extraction" implies an inference step (what is worth remembering?), which is the primary source of hallucination.
  - Quick check: If a user says "I used to love coffee but now I hate it," does the system perform an *Extraction* of a new fact or an *Update* of an old one, and how does that distinction affect the evaluation?

- **Concept: Hallucination Taxonomy (Omission vs. Fabrication)**
  - Why needed: The paper emphasizes "Omission" (forgetting) as a critical, often overlooked hallucination type in long-term memory, distinct from "Fabrication" (making things up).
  - Quick check: In a long dialogue (1M tokens), why might a system achieve high "Precision" but fail to answer user questions correctly, and which metric in the paper captures this failure?

- **Concept: Context Scale vs. Memory Density**
  - Why needed: HaluMem-Long uses "irrelevant dialogues" to extend context to 1M tokens, simulating "sparse" memory events in a sea of noise.
  - Quick check: Why did systems like Mem0 and Mem0-Graph see a sharp decline in HaluMem-Long compared to HaluMem-Medium, and what does this imply about their ability to distinguish high-value memories from noise?

## Architecture Onboarding

- **Component map:** Multi-turn Dialogue (D) -> Memory System -> Memory Extractor (E) -> Memory Updater (U) -> Retriever (R) -> LLM Generator (A) -> Evaluation Layer
- **Critical path:** Memory Extraction (E) stage. The paper explicitly links downstream failures to the inability to successfully extract and persist initial memory points. If E fails, the memory is lost forever.
- **Design tradeoffs:**
  - Recall vs. FMR: Systems that aggressively extract information (High Recall) tend to absorb adversarial noise (Low False Memory Resistance)
  - Graph vs. Plaintext: Graph-based systems offer better reasoning but may suffer from synchronization issues and higher management complexity
  - Manageability vs. Capacity: RAG offers high manageability (low hallucination) but lower capacity for complex reasoning compared to GraphRAG
- **Failure signatures:**
  - "Amnesia" Drift: QA accuracy degrades significantly on HaluMem-Long (>50% omission rates) because the system fails to recall specific preferences in a sea of irrelevant text
  - "Stagnant Update": System correctly extracts new fact but fails to link to existing memory, resulting in duplicate or conflicting entries
  - "Distractor" Injection: System persists information mentioned by AI assistant that user never confirmed
- **First 3 experiments:**
  1. Run system on 10 single-turn dialogues from HaluMem-Medium containing "distractor" statements. Verify if system extracts user's statement while ignoring AI's unconfirmed suggestion
  2. Ingest a HaluMem-Long profile (1M tokens). Query a specific preference change mentioned early in context. Check if system returns current state (Update success) or old state (Update failure)
  3. Calculate "Memory Recall" for your system. If Recall < 60%, diagnose whether extraction prompt is too conservative or vector database retrieval is failing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can memory systems balance high recall in extraction with strong resistance to false memories when processing long-context, distractor-heavy dialogues?
- Basis: Results show systems like MemOS achieve high recall (81.9%) but low FMR (28.85%) on long contexts, while conservative systems have high FMR but miss information
- Why unresolved: Current systems struggle to filter irrelevant content while maintaining comprehensive memory coverage
- What evidence would resolve it: A system achieving >80% Weighted Recall *and* >80% FMR on the HaluMem-Long benchmark

### Open Question 2
- Question: What mechanisms can ensure stable linkage between memory extraction and updating to reduce high omission rates in long-term state tracking?
- Basis: Authors conclude that "current systems face a clear bottleneck in memory updating: the extraction and updating stages lack stable linkage, resulting in low accuracy and high omission rates"
- Why unresolved: Updates often fail because relevant base information was never successfully extracted
- What evidence would resolve it: Significant reduction in "Memory Updating Omission Rate" (currently >50% for most systems) on HaluMem dataset

### Open Question 3
- Question: How can memory systems be optimized to handle complex reasoning tasks, such as multi-hop inference and dynamic preference tracking?
- Basis: Figure 5 shows all evaluated systems perform poorly on "Multi-hop Inference" and "Generalization & Application" questions compared to basic recall tasks
- Why unresolved: Current retrieval and storage methods fail to model complex relationships or temporal logic required for inference
- What evidence would resolve it: Accuracy improvements on "Multi-hop Inference" and "Dynamic Update" question categories to match "Basic Fact Recall" performance levels

## Limitations

- Evaluation dependency on GPT-4o creates potential bias in hallucination detection without addressing cross-model consistency
- Memory system configuration variance (graph vs. plaintext, retrieval limits) makes it unclear whether performance differences stem from architectural advantages or implementation choices
- Adversarial content realism may not reflect real-world interaction patterns, potentially overstating false memory resistance problems

## Confidence

- Operation-Level Error Localization: **High** - Systematic evidence clearly links upstream failures to downstream degradation
- Extraction Bottleneck Theory: **High** - Strong correlation evidence with clear causal explanation through omission propagation
- Adversarial Distractor Filtering: **Medium** - Well-defined FMR metric but uncertain real-world prevalence of such scenarios

## Next Checks

1. Run HaluMem evaluation using multiple LLM judges (GPT-4, Claude, Gemini) to verify cross-model consistency in hallucination detection
2. Deploy tested memory systems in actual multi-turn user interactions over extended periods, then evaluate same memory points to determine if controlled adversarial scenarios match real hallucination patterns
3. Systematically vary ratio of memory-relevant to irrelevant content in long-context datasets to quantify how memory density affects extraction recall and QA accuracy, separating context scaling effects from signal-to-noise ratio impacts