---
ver: rpa2
title: 'Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource
  Languages: Do Linguistic Differences Matter?'
arxiv_id: '2511.22482'
source_url: https://arxiv.org/abs/2511.22482
tags:
- languages
- guarani
- mbya
- nheengatu
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates performance differences in fine-tuned translators
  for two Brazilian Indigenous languages, Guarani Mbya and Nheengatu, despite their
  linguistic similarity. Using the NLLB-200 pre-trained model, translators were fine-tuned
  with comparable datasets (3K vs 7K pairs) in both translation directions.
---

# Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?

## Quick Facts
- **arXiv ID**: 2511.22482
- **Source URL**: https://arxiv.org/abs/2511.22482
- **Reference count**: 24
- **Primary result**: Fine-tuned translators show significant performance differences between linguistically similar ultra-low-resource languages, with Nheengatu outperforming Guarani Mbya by 26.7 BLEU points

## Executive Summary
This study investigates why two Brazilian Indigenous languages with similar linguistic features show vastly different performance when fine-tuned for machine translation. Using the NLLB-200 pre-trained model, translators for Guarani Mbya and Nheengatu were fine-tuned with comparable datasets (3K vs 7K pairs) in both translation directions. Results revealed Nheengatu achieved significantly higher SacreBLEU scores (66.9%) compared to Guarani Mbya (40.2%), despite similar data availability. Through systematic experimentation, the researchers ruled out pre-trained model limitations, model size variations (600M vs 3B parameters), and training data size as causes. The study suggests that structural linguistic differences, such as word order and morphological complexity, may significantly impact fine-tuning success for ultra-low-resource languages.

## Method Summary
The study fine-tuned NLLB-200 pre-trained models on Guarani Mbya and Nheengatu translation pairs to and from English. Experiments tested different model sizes (600M and 3B parameters) and training data sizes (3K vs 7K pairs). Performance was measured using SacreBLEU scores across multiple configurations. Additional experiments included training with downsampled Nheengatu data (500 pairs) and using different pre-trained models (M2M-124) to isolate factors affecting performance differences.

## Key Results
- Nheengatu achieved 66.9% SacreBLEU score compared to Guarani Mbya's 40.2%
- Performance gap persisted across different model sizes (600M vs 3B parameters)
- Results were consistent across both translation directions (to/from English)
- Only extreme downsampling (to 500 pairs) reduced Nheengatu's performance to match Guarani Mbya's

## Why This Works (Mechanism)
The performance differences between Guarani Mbya and Nheengatu fine-tuned translators cannot be explained by data quantity, model size, or pre-trained model choice. The authors hypothesize that linguistic structural differences—such as word order patterns, morphological complexity, and syntactic structures—may be responsible for the observed performance gap. When Nheengatu's training data was downsampled to only 500 pairs, its performance dropped to match Guarani Mbya's level, suggesting that data quantity plays a role but cannot fully explain the initial gap.

## Foundational Learning

**Machine Translation Fine-tuning**
*Why needed*: Understanding how pre-trained models adapt to specific language pairs
*Quick check*: Verify model converges on validation set and achieves reasonable baseline performance

**BLEU Score Evaluation**
*Why needed*: Standard metric for measuring translation quality
*Quick check*: Confirm consistent tokenization and preprocessing across all experiments

**Morphological Complexity Analysis**
*Why needed*: Different morphological systems affect model learning patterns
*Quick check*: Compare agglutinative vs. isolating features between target languages

**Word Order Patterns**
*Why needed*: Syntactic differences impact translation difficulty
*Quick check*: Identify subject-object-verb vs. subject-verb-object structures in source languages

## Architecture Onboarding

**Component Map**: Pre-trained NLLB-200 -> Fine-tuning Dataset -> Training Process -> Evaluation Metrics

**Critical Path**: Pre-trained model initialization → Dataset loading and preprocessing → Gradient-based fine-tuning → SacreBLEU evaluation

**Design Tradeoffs**: Using larger models (3B vs 600M) provides marginal improvements but increases computational cost; choosing between translation directions affects resource availability

**Failure Signatures**: 
- Low BLEU scores across all experiments suggest pre-trained model limitations
- Inconsistent performance between translation directions indicates data quality issues
- Performance improvement with larger models suggests capacity constraints

**3 First Experiments**:
1. Fine-tune NLLB-200 with 3K pairs for Guarani Mbya and Nheengatu in both directions
2. Repeat with 7K pairs for Nheengatu to test data size effects
3. Test with downsampled Nheengatu data (500 pairs) to verify data quantity hypothesis

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments only examined two languages, limiting generalizability
- Analysis of linguistic features was correlative rather than causative
- Only one translation direction pair (to/from English) was tested
- The study cannot definitively prove linguistic factors are responsible

## Confidence
- **Medium** confidence in the core finding that Nheengatu outperforms Guarani Mbya, given consistent experimental results across multiple configurations
- **Low** confidence in the linguistic differences explanation, as this remains speculative without direct linguistic feature analysis

## Next Checks
1. Conduct systematic linguistic feature analysis comparing Guarani Mbya and Nheengatu (morphological complexity, word order patterns, syntactic structures) to identify specific differences
2. Expand experiments to additional language pairs with varying linguistic properties while maintaining similar data sizes
3. Test with higher-resource datasets for both languages to determine if the performance gap persists at larger scales