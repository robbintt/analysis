---
ver: rpa2
title: Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M
arxiv_id: '2505.10212'
source_url: https://arxiv.org/abs/2505.10212
tags:
- memorization
- recommendation
- llms
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) have
  memorized the MovieLens-1M dataset during training, a critical issue for the reliability
  of recommender systems research. The authors define dataset memorization as the
  ability to retrieve item attributes, user profiles, and user-item interactions via
  prompting.
---

# Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M

## Quick Facts
- arXiv ID: 2505.10212
- Source URL: https://arxiv.org/abs/2505.10212
- Reference count: 40
- Key outcome: All tested LLMs show varying degrees of MovieLens-1M dataset memorization, with larger models achieving higher coverage and inflated recommendation metrics, raising concerns about overoptimistic evaluations.

## Executive Summary
This study investigates whether Large Language Models have memorized the MovieLens-1M dataset during training, which could invalidate recommender system evaluations. The authors operationalize memorization as the ability to retrieve exact item attributes, user profiles, and user-item interactions through prompting. Using coverage-based metrics, they evaluate multiple GPT and Llama models across three dataset aspects. Results show that all models exhibit memorization, with GPT-4o retrieving 80.76% of items and 9.37% of interactions, while smaller models show much lower coverage. The study finds that recommendation performance correlates with memorization extent, suggesting current LLM-based recommender systems may be leveraging memorized data rather than generalizing.

## Method Summary
The authors define dataset memorization as the ability to retrieve item attributes, user profiles, and user-item interactions via prompting. They evaluate memorization coverage by prompting LLMs with dataset identifiers using few-shot templates. For items, they use "MovieID::Title" pairs; for users, "UserID::Gender::Age::Occupation::Zip-code"; and for interactions, "UserID::MovieID" pairs. They measure coverage as the fraction of exact matches retrieved. The study also evaluates recommendation performance using leave-n-out splitting and compares results across model families to assess correlation between memorization and inflated metrics. Popularity bias is examined by stratifying items into top/bottom 20% by interaction frequency.

## Key Results
- GPT-4o achieves 80.76% item coverage, 9.37% interaction coverage, and HR@10 of 0.2796, while GPT-4o mini shows 8.47%, 1.53%, and 0.0316 respectively
- Llama-3.1 405B exhibits 12.9% mean memorization rate compared to Llama-3.1 8B at 5.82%, corresponding to a 54.23% decrease in nDCG and 47.36% decrease in HR
- Popular items show higher retrieval rates: GPT-4o retrieves 89.06% of top-20% popular items versus 63.97% of bottom-20% items

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting can extract memorized dataset content from LLMs at measurable rates. The paper operationalizes memorization as the ability of an LLM to return exact attribute values when prompted with dataset identifiers. By framing the LLM as the dataset itself, the prompt elicits stored associations between IDs and attributes formed during pre-training. The few-shot prompt format successfully activates relevant parametric knowledge, though failure to retrieve does not prove absence of memorization. Current extraction rates may be lower bounds since prompt optimization was left for future work.

### Mechanism 2
Larger model scale correlates with higher memorization coverage and inflated recommendation metrics. Greater parameter count increases storage capacity and retention of training data patterns. The paper shows that within model families, larger variants achieve both higher item/user/interaction coverage and superior Hit Rate and nDCG scores—suggesting performance gains may partially reflect memorized test data rather than genuine generalization. This correlation implies causation, though other factors like architectural improvements could contribute alongside memorization.

### Mechanism 3
Popularity bias in training data is preserved and amplified through memorization. Items appearing more frequently in the original dataset (and likely in crawled web text) are overrepresented in LLM training corpora, creating stronger weight associations. The paper shows higher retrieval rates for top-20% popular items versus bottom-20% items across all tested models. This observed retrieval asymmetry reflects training data distribution rather than prompt design favoring well-known movies, though evaluation explicitly controlling for item name frequency in general web corpora would provide clearer attribution.

## Foundational Learning

- **Concept: Memorization vs. Generalization in Evaluation**
  - Why needed here: The paper's central concern is that high benchmark performance may reflect retrieved training data rather than learned transferable patterns
  - Quick check question: If an LLM achieves 90% accuracy on a test set, how would you determine whether this reflects genuine reasoning or memorized examples?

- **Concept: Coverage-Based Memorization Metrics**
  - Why needed here: The paper defines memorization quantitatively as the fraction of dataset entries exactly recoverable via prompting
  - Quick check question: Why is exact match used rather than semantic similarity for assessing item attribute retrieval?

- **Concept: Popularity Bias in Recommender Systems**
  - Why needed here: Understanding why popular items are over-recommended helps interpret the paper's finding that LLMs inherit and potentially amplify this bias
  - Quick check question: How does popularity bias differ from cold-start problems in recommendation evaluation?

## Architecture Onboarding

- **Component map:** MovieLens-1M dataset files -> Few-shot prompt templates -> LLM inference with temperature=0 -> Response parsing -> Exact match verification -> Coverage aggregation -> Recommendation evaluation with leave-n-out split -> Metric computation

- **Critical path:** Prompt design -> LLM query -> Response parsing -> Exact match verification -> Coverage aggregation. For recommendation tasks: User history formatting -> Zero-shot recommendation prompt -> Top-50 output parsing -> Metric computation against held-out interactions.

- **Design tradeoffs:** The authors chose few-shot prompting over zero-shot and CoT after pilot testing, trading prompt engineering effort for higher extraction rates. Temperature=0 ensures determinism but may underestimate what could be extracted with sampling. The paper acknowledges automatic prompt engineering could improve results but was left for future work.

- **Failure signatures:** (1) Models returning "Unknown" or hallucinated attributes indicate low memorization or prompt mismatch. (2) GPT-4o mini showing 8.47% item coverage vs. GPT-4o's 80.76% suggests architectural/size constraints. (3) Llama-3.3 70B underperforming Llama-3.1 variants on user coverage (5.84% vs. ~15%) may indicate training data differences between versions.

- **First 3 experiments:**
  1. Replicate item coverage baseline: Query your target LLM with the Figure 1 prompt template on a 100-item sample from MovieLens-1M to establish retrieval rate before full evaluation.
  2. Popularity-stratified extraction test: Divide items into top/bottom 20% by interaction count and compare retrieval rates to confirm bias preservation in your model.
  3. Recommendation-memorization correlation check: Run the Figure 3 prompt on users with known high vs. low interaction memorization and compare HR@10 to validate whether performance tracks memorization in your setup.

## Open Questions the Paper Calls Out
The paper identifies prompt engineering as a critical area for future work, noting that current coverage rates may be lower bounds since few-shot prompting was selected through preliminary testing rather than exhaustive optimization. The study also calls for systematic evaluation across multiple recommendation datasets to assess generalizability of the memorization findings beyond MovieLens-1M.

## Limitations
- Memorization measurements depend heavily on prompt engineering effectiveness, which was not systematically optimized
- Study only examines one dataset (MovieLens-1M), limiting generalizability to other domains or recommendation datasets
- Correlation between memorization and recommendation performance does not establish causation—architectural improvements could independently drive both

## Confidence

- **High confidence**: Larger models exhibit higher memorization coverage rates is robust across multiple model families tested
- **Medium confidence**: Correlation between memorization extent and inflated recommendation metrics is well-supported within this dataset but may not generalize to all evaluation settings
- **Medium confidence**: Popularity bias finding is consistently observed but may reflect training data distribution rather than inherent memorization mechanisms

## Next Checks

1. **Prompt optimization study**: Systematically test alternative prompt formats (zero-shot, CoT, adversarial prompts) on the same dataset to establish whether current coverage rates are near-optimal or substantial underestimates of memorization capacity.

2. **Cross-dataset generalization**: Replicate the memorization measurement methodology on at least two additional recommendation datasets (e.g., Netflix Prize, Amazon product reviews) to test whether scale-memorization relationships hold across domains.

3. **Attribution experiment**: Design an evaluation that distinguishes between memorization of specific MovieLens instances versus generalization from broader web-scale training data by testing model performance on semantically similar but distinct user-item pairs not present in MovieLens-1M.