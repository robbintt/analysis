---
ver: rpa2
title: 'SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models'
arxiv_id: '2506.11120'
source_url: https://arxiv.org/abs/2506.11120
tags:
- pruning
- performance
- loss
- methods
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SDMPrune, a novel self-distillation MLP pruning
  method for compressing large language models (LLMs) with minimal performance degradation.
  The method addresses the limitations of existing gradient-based pruning approaches
  that rely on one-hot labels and miss potential predictions from other words, which
  is critical for preserving the generative capability of the original model.
---

# SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models

## Quick Facts
- **arXiv ID**: 2506.11120
- **Source URL**: https://arxiv.org/abs/2506.11120
- **Reference count**: 40
- **Primary result**: SDMPrune achieves 20-40% MLP parameter reduction with minimal performance degradation on 1B-scale LLMs

## Executive Summary
SDMPrune introduces a novel self-distillation approach for compressing large language models by selectively pruning MLP modules while preserving attention layers. The method addresses limitations in standard gradient-based pruning that relies on one-hot labels and misses the full probability distribution of predictions. By incorporating self-distillation loss during the pruning phase, SDMPrune obtains more accurate gradient information for importance scoring, leading to significantly better performance than existing pruning methods on zero-shot benchmarks.

## Method Summary
SDMPrune employs a two-stage pruning process that first uses one-hot cross-entropy loss for a cold-start phase, then switches to self-distillation loss (KL divergence between original and pruned models) for the main pruning stage. The method focuses exclusively on MLP modules, which contain more than 5× the parameters of attention modules in LLaMA3.2-1.2B models. Importance scores are calculated using first-order Taylor expansion of the loss with respect to weights. After pruning, models are fine-tuned using LoRA on the LaMini-instruction dataset. The approach requires loading both teacher and student models during Stage 2, effectively doubling memory usage during pruning.

## Key Results
- Outperforms existing pruning methods on zero-shot benchmarks while achieving 20-40% MLP parameter reduction
- Maintains competitive performance among 1B-scale open-source LLMs after pruning and LoRA fine-tuning
- Demonstrates that MLP-only pruning yields superior results compared to joint pruning of attention and MLP modules
- Achieves better perplexity and accuracy metrics compared to standard one-hot label pruning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-distillation loss during pruning yields more accurate gradient information for importance scoring compared to one-hot label losses.
- **Mechanism**: Standard pruning with one-hot labels ignores probability distributions of non-target tokens. Self-distillation loss (KL divergence) preserves "soft" predictions, retaining generative capabilities that hard targets miss.
- **Core assumption**: The original model's predictive capability is encoded in the full probability distribution over vocabulary, not just the ground-truth token.
- **Evidence anchors**: Abstract notes one-hot labels "ignore potential predictions on other words," Section 4.2 defines the interpolated loss, and corpus supports component-specific optimization.
- **Break condition**: If temperature T is too high or α=0, mechanism degrades to standard one-hot pruning.

### Mechanism 2
- **Claim**: Selectively pruning MLP modules while preserving Attention maximizes parameter reduction with minimal performance degradation.
- **Mechanism**: MLPs in LLaMA have significantly lower average importance scores than attention modules but constitute over 5× the parameter count. Removing low-importance MLP neurons reduces computational load with less functional disruption.
- **Core assumption**: First-order Taylor expansion importance scores accurately reflect neuron contribution to final output.
- **Evidence anchors**: Abstract states predictions are "less sensitive to MLP modules," Section 5.4.1 shows MLP-only pruning superiority, and corpus supports architectural split.
- **Break condition**: Aggressive pruning of both MLPs and attention layers causes performance degradation that fine-tuning cannot recover.

### Mechanism 3
- **Claim**: Two-stage pruning (cold-start followed by self-distillation) is necessary to operationalize intra-pruning distillation.
- **Mechanism**: Self-distillation requires teacher-student gap. Stage 1 cold-start uses one-hot labels for low-ratio prune, creating divergence. Stage 2 uses original model as teacher for high-ratio pruning with full soft distribution.
- **Core assumption**: Initial cold-start pruning doesn't irreversibly destroy critical semantic capabilities before distillation begins.
- **Evidence anchors**: Section 4.1 explains cold-start necessity, Algorithm 1 separates hard and distillation loops, and corpus discusses integrated retraining/recovery.
- **Break condition**: If cold-start ratio is too aggressive, student diverges too far from teacher manifold, preventing effective knowledge transfer.

## Foundational Learning

- **Concept: Knowledge Distillation (Soft Labels)**
  - **Why needed here**: Core innovation shifts from hard binary targets to soft probability distributions to guide pruning decisions.
  - **Quick check question**: If you set temperature T=0 in distillation loss, what happens to gradient signals from non-target classes? (Answer: They vanish).

- **Concept: First-Order Taylor Expansion**
  - **Why needed here**: Estimates neuron importance by approximating loss change if neuron were removed, without actual forward passes for every removal.
  - **Quick check question**: Why is importance score defined as |gradient · weight| rather than just weight magnitude? (Answer: Captures loss sensitivity to that weight, not just size).

- **Concept: Structural vs. Unstructured Pruning**
  - **Why needed here**: SDMPrune removes entire neurons (rows/columns) rather than individual sparse weights. Requires understanding physical reshaping of weight matrices.
  - **Quick check question**: Why does removing an MLP hidden layer neuron require zeroing rows/columns in three different weight matrices? (Answer: Neuron acts as bottleneck connecting input to output projection).

## Architecture Onboarding

- **Component map**: C4 Calibration Dataset -> LLaMA Transformer (Attention + MLP blocks) -> MLP Block (Up-proj, Gate-proj, Down-proj) -> Importance Scores -> Pruning Masks -> LoRA Fine-tuning -> Evaluation

- **Critical path**:
  1. Calibration: Pass data through model to compute gradients
  2. Importance Calculation: Compute I_i for all MLP hidden neurons using Taylor expansion
  3. Ranking: Sort neurons by I_i
  4. Masking: Zero out rows/cols corresponding to bottom k% neurons
  5. Loop: Repeat for defined steps/epochs

- **Design tradeoffs**:
  - Memory vs. Accuracy: Requires loading original teacher model alongside student during Stage 2, doubling GPU memory usage
  - Speed vs. Recovery: Pruning only MLPs preserves context handling but offers lower compression limits than full-model pruning

- **Failure signatures**:
  - Catastrophic Forgetting: If α in L_dis is too low, model overfits to calibration labels and loses general generative ability
  - Structural Mismatch: If pruning W_u doesn't align with W_d, matrix multiplication fails due to dimension mismatch
  - Stalling: If cold-start ratio is too low, student equals teacher and distillation loss gradient becomes zero

- **First 3 experiments**:
  1. Baseline Verification: Replicate MLP-only vs Attn+MLP ablation on smaller model (100M params) to verify MLP pruning sensitivity
  2. Hyperparameter Scan: Grid search on Temperature T and Loss Ratio α to find sweet spot between ground truth vs distribution focus
  3. Recovery Test: Prune to 40% and attempt LoRA vs Full Parameter recovery to confirm rapid LoRA fine-tuning claim

## Open Questions the Paper Calls Out
- **Lightweight distillation implementation**: The authors aim to explore lightweight implementation of distillation to reduce memory usage and computational overhead from teacher model-assisted pruning.
- **Higher compression rates**: Authors seek to further enhance pruning performance at higher compression rates beyond the current 40% limit where performance degrades.
- **Larger architecture generalization**: The method's effectiveness on significantly larger models (70B+ parameters) or Mixture-of-Experts architectures remains untested.

## Limitations
- Memory overhead from maintaining teacher and student models during Stage 2 limits scalability to smaller GPU setups
- Performance degradation at extreme compression rates (>40%) where the method currently shows sharp drops in accuracy
- Empirical reliance on LLaMA-specific observations without theoretical framework for generalizing to other LLM architectures

## Confidence

- **High Confidence**: Self-distillation loss mechanism is technically sound and supported by ablation studies showing superior performance vs. one-hot baselines; Taylor expansion importance scoring is standard and reproducible
- **Medium Confidence**: Claim of competitive performance among 1B-scale LLMs is supported by benchmarks but lacks comparison to state-of-the-art full-model pruning methods; MLP-only pruning superiority may not generalize to all architectures
- **Low Confidence**: Assertion that MLP predictions are less sensitive lacks theoretical justification; method risks failure on architectures where MLPs play larger reasoning roles

## Next Checks
1. **Architectural Generalization Test**: Apply SDMPrune to non-LLaMA architecture (e.g., Gemma or Mistral) and compare MLP vs. Attention sensitivity to validate broader LLM property
2. **Cold-Start Sensitivity Analysis**: Sweep cold-start pruning ratio k' from 5% to 25% to measure impact on final perplexity and zero-shot accuracy, quantifying two-stage process robustness
3. **Memory Overhead Benchmark**: Measure GPU memory usage during Stage 2 pruning (teacher + student) vs. full-model pruning with knowledge distillation to validate "efficiency" claim under resource constraints