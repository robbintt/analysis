---
ver: rpa2
title: 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?'
arxiv_id: '2502.00674'
source_url: https://arxiv.org/abs/2502.00674
tags:
- self-moa
- performance
- arxiv
- diversity
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether ensembling different Large Language
  Models (LLMs) is beneficial compared to repeatedly sampling from a single top-performing
  model. The authors introduce Self-MoA, which aggregates multiple outputs from the
  same high-performing LLM, and compare it to standard Mixed-MoA, which mixes outputs
  from different models.
---

# Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?

## Quick Facts
- arXiv ID: 2502.00674
- Source URL: https://arxiv.org/abs/2502.00674
- Authors: Wenzhe Li; Yong Lin; Mengzhou Xia; Chi Jin
- Reference count: 21
- Primary result: Self-MoA achieves 6.6% improvement over Mixed-MoA by aggregating multiple outputs from a single top-performing model

## Executive Summary
This paper challenges the conventional wisdom that mixing outputs from different Large Language Models improves performance. Through systematic experiments on AlpacaEval 2.0 and reasoning benchmarks, the authors demonstrate that aggregating multiple outputs from a single high-performing model (Self-MoA) significantly outperforms mixing outputs from multiple different models (Mixed-MoA). The key insight is that MoA performance is highly sensitive to the quality of proposer models, and including weaker models in Mixed-MoA actually reduces average quality. The authors introduce Self-MoA-Seq, a sequential aggregation method that scales to larger numbers of samples while respecting context window constraints.

## Method Summary
The study compares two MoA variants: Self-MoA, which generates 6 samples from a single high-performing LLM (WizardLM-2-8x22B) at temperature 0.7 and aggregates them using Qwen1.5-110B-Chat at temperature 0; and Mixed-MoA, which samples one output each from 6 different LLMs (including weaker models like Gemma 2-9B) and aggregates them. The sequential variant (Self-MoA-Seq) uses a sliding window approach to handle more than 6 samples. Evaluation is conducted on AlpacaEval 2.0 for instruction following, and MMLU-redux, CRUX, and MATH for reasoning tasks, measuring Length-Controlled Win Rate and accuracy.

## Key Results
- Self-MoA achieves 6.6% improvement over Mixed-MoA on AlpacaEval 2.0
- MoA performance shows stronger positive correlation with quality (α) than diversity (β)
- Mixing different LLMs often lowers average quality, hurting performance
- Self-MoA-Seq effectively scales to 30 samples without significant degradation

## Why This Works (Mechanism)

### Mechanism 1: Quality-Diversity Trade-off
If a single high-performing model is available, aggregating its own outputs (Self-MoA) is likely to outperform mixing outputs from multiple lower-quality models (Mixed-MoA). Standard MoA seeks diversity, often including weaker models, which lowers the average quality of the proposer set. Because aggregator performance is statistically more sensitive to quality than diversity, the "quality drag" from weaker models hurts Mixed-MoA more than the reduced diversity hurts Self-MoA. Self-MoA maintains high average quality while relying on stochastic decoding (temperature > 0) to generate sufficient "in-model" diversity.

### Mechanism 2: Sequential Sliding Window Aggregation (Self-MoA-Seq)
Sequential aggregation allows scaling the number of aggregated samples beyond the aggregator's context window limit without significant performance degradation. Instead of feeding N candidates simultaneously, Self-MoA-Seq processes candidates in a sliding window (e.g., size 6), synthesizing the "best so far" with a small batch of new candidates iteratively. This bypasses the hard token limit of the aggregator model.

### Mechanism 3: Generative Synthesis vs. Selection
Synthesizing multiple responses via an LLM aggregator yields better instruction-following performance than selecting the single most consistent answer. Aggregation prompts the model to combine strengths from multiple inputs into a superior hybrid output, while selection discards potentially useful minority information. The paper shows Self-MoA improves over the baseline, while Universal Self-Consistency performs worse than MoA counterparts.

## Foundational Learning

- **Concept: In-Model Diversity**
  - Why needed here: To understand why sampling repeatedly from one model creates enough difference to be useful, replacing the need for multiple different models.
  - Quick check question: Why does increasing temperature generally improve Self-MoA performance up to a point?

- **Concept: The Aggregator Role (Synthesis)**
  - Why needed here: Self-MoA relies on the aggregator not just to vote, but to write a new answer. Understanding this is key to selecting the right aggregator model.
  - Quick check question: In Self-MoA, should the aggregator be a different model or the same model as the proposer? (Hint: See Table 3, "TaskBest" as aggregator).

- **Concept: Context Window Constraints**
  - Why needed here: This is the primary motivation for the "Sequential" variant proposed in the paper.
  - Quick check question: Why does aggregating 30 full-length responses fail in standard Self-MoA, and how does Self-MoA-Seq solve this?

## Architecture Onboarding

- **Component map:** Prompt -> Proposer (Generate N samples at Temp 0.7) -> Collect N outputs -> (Standard) Feed all N to Aggregator OR (Sequential) Feed window of 3 current + 3 new to Aggregator -> Aggregator -> Final Answer

- **Critical path:**
  1. Prompt -> Proposer (Generate N samples at Temp 0.7)
  2. Collect N outputs
  3. (Standard) Feed all N to Aggregator OR (Sequential) Feed window of 3 current + 3 new to Aggregator
  4. Aggregator -> Final Answer

- **Design tradeoffs:**
  - Cost vs. Quality: Self-MoA requires N inferences per query. Increasing N scales cost linearly but gains diminish.
  - Diversity vs. Quality: Mixing models (Mixed-MoA) adds hardware complexity for potentially worse performance if added models are weaker.
  - Context Length: Standard Self-MoA is capped by aggregator's context window; Self-MoA-Seq adds latency to bypass this.

- **Failure signatures:**
  - Identical Outputs: If outputs are identical, the temperature is too low or the model is overfitted; Self-MoA provides no lift.
  - Context Overflow: Aggregator crashes or truncates inputs early. (Switch to Self-MoA-Seq).
  - Aggregator Bias: The aggregator ignores the proposed inputs and answers from its own weights (common if proposer outputs are low quality or confusing).

- **First 3 experiments:**
  1. **Sanity Check:** Run Self-MoA (6 samples) vs. Single-Pass on AlpacaEval 2.0 using a strong base model (e.g., Qwen2-7B) to verify the ~3-6% lift claimed.
  2. **Temperature Sweep:** Vary proposer temperature (0.0, 0.5, 0.7, 1.0) for Self-MoA to measure the sensitivity of "in-model diversity."
  3. **Scale Test:** Implement Self-MoA-Seq with 30 samples and compare against Standard Self-MoA with 6 samples to determine if added compute justifies accuracy gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical scaling limit of Self-MoA performance relative to the number of repeated samples?
- Basis in paper: Section 5 explicitly asks about scaling with repeated samples and notes that diversity plateaus and adding samples can have negative effects.
- Why unresolved: Experiments were limited to 30 samples with non-monotonic performance improvements, leaving the compute-optimal ceiling undefined.
- What evidence would resolve it: A theoretical analysis or empirical study of Self-MoA performance with significantly larger sample sizes (>100) across diverse benchmarks.

### Open Question 2
- Question: To what extent does the capability gap between the aggregator and the proposer constrain MoA performance?
- Basis in paper: Section 4.1 notes inconsistent linear regression trends for MATH because the aggregator (Qwen2-7B) was "relatively weak" compared to the strongest proposer.
- Why unresolved: The study did not isolate the aggregator's intelligence as an independent variable to determine if a weaker aggregator bottlenecks stronger proposers.
- What evidence would resolve it: Experiments using proposers of fixed quality while systematically varying the capability of the aggregator model.

### Open Question 3
- Question: Can a dynamic routing mechanism identify instances where Mixed-MoA outperforms Self-MoA in real-time?
- Basis in paper: Section 4.2 identifies specific scenarios (mixture tasks) where Mixed-MoA slightly outperforms Self-MoA (by 0.17%-0.35%) due to specialized model diversity.
- Why unresolved: The paper determines the "what" but not the "how" for deployment, as identifying these instances requires prior knowledge of sample origin.
- What evidence would resolve it: A study testing a classifier that predicts the optimal MoA strategy (Self vs. Mixed) based on input prompt features before aggregation.

## Limitations

- The paper does not provide the exact aggregator prompt template, making faithful reproduction challenging.
- The quality-diversity trade-off relationship may not generalize across all task types beyond instruction-following and mathematical reasoning.
- The optimal temperature range for different model families and task domains remains unexplored.

## Confidence

- **High Confidence (Mechanism 1 - Quality-Diversity Trade-off):** Empirical results showing 6.6% improvement and statistical analysis demonstrating quality's stronger correlation with performance than diversity.
- **Medium Confidence (Mechanism 2 - Self-MoA-Seq):** Sequential approach is logically sound but lacks extensive validation across different window sizes and iteration counts.
- **Medium Confidence (Mechanism 3 - Synthesis vs. Selection):** Comparison with Universal Self-Consistency shows synthesis outperforming selection, but evaluation is limited to one baseline.

## Next Checks

1. **Temperature Sensitivity Analysis:** Systematically vary proposer temperature from 0.0 to 1.5 in increments of 0.2 while measuring both Self-MoA performance and "in-model diversity" using semantic similarity metrics.

2. **Aggregator Model Ablation:** Test different aggregator models including: (a) same model as proposer, (b) larger model, (c) smaller model, and (d) non-LLM aggregator to determine impact on performance.

3. **Sequential Window Size Optimization:** Implement Self-MoA-Seq with varying window sizes (2, 4, 6, 8) and measure both performance and latency while tracking quality across sequential steps to detect drift effects.