---
ver: rpa2
title: 'ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction'
arxiv_id: '2511.20937'
source_url: https://arxiv.org/abs/2511.20937
tags:
- world
- forward
- inverse
- modeling
- gpt-5
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ENACT, a benchmark to evaluate embodied
  cognition in vision-language models (VLMs) by framing it as world modeling from
  egocentric interaction in a visual question answering (VQA) format. Grounded in
  a partially observable Markov decision process (POMDP), ENACT tests two complementary
  tasks: forward world modeling (reordering observations given actions) and inverse
  world modeling (reordering actions given observations).'
---

# ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction

## Quick Facts
- arXiv ID: 2511.20937
- Source URL: https://arxiv.org/abs/2511.20937
- Reference count: 40
- Models struggle with embodied cognition tasks, with accuracy dropping sharply as interaction horizons lengthen

## Executive Summary
This paper introduces ENACT, a benchmark to evaluate embodied cognition in vision-language models (VLMs) by framing it as world modeling from egocentric interaction in a visual question answering (VQA) format. Grounded in a partially observable Markov decision process (POMDP), ENACT tests two complementary tasks: forward world modeling (reordering observations given actions) and inverse world modeling (reordering actions given observations). The benchmark comprises 8,972 QA pairs derived from robotics simulation, focusing on long-horizon home-scale activities. Experiments reveal that state-of-the-art VLMs lag significantly behind human performance, with accuracy dropping sharply as interaction horizons lengthen.

## Method Summary
ENACT evaluates VLMs on embodied cognition through world modeling via egocentric interaction. Two sequence reordering VQA tasks are defined: forward world modeling (reorder shuffled future observations given ordered actions) and inverse world modeling (reorder shuffled actions given ordered observations). The benchmark uses BEHAVIOR simulator to generate 8,972 QA pairs from 29 long-horizon activities, with each QA containing egocentric RGB images, symbolic scene graphs with 11 predicate classes, and actions as visible scene graph differences. Task accuracy (exact sequence match) and pairwise accuracy (correctly ordered adjacent pairs) are computed using an online semantic verifier that accepts predictions where visible state changes are subsets of ground truth.

## Key Results
- VLMs consistently perform better on inverse world modeling than forward world modeling tasks
- Performance degrades sharply with increasing interaction horizons, approaching 0% accuracy at L≥8 for most models
- VLMs exhibit anthropocentric biases including right-handed action preferences and degradation with non-human-like camera configurations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequence reordering of observations given actions (forward world modeling) serves as a proxy for evaluating action-conditioned visual reasoning and temporal abstraction.
- **Mechanism:** The task forces models to simulate environmental dynamics mentally—matching each action to its visual consequence—without conflating evaluation with photorealistic video generation. Correct ordering requires tracking object state changes, affordances, and physical consequences across multiple steps.
- **Core assumption:** If a model can correctly order shuffled observations given actions, it implicitly possesses action-effect reasoning and embodied awareness.
- **Evidence anchors:** [abstract] "solving these tasks implicitly demands capabilities central to embodied cognition—affordance recognition, action–effect reasoning, embodied awareness, and interactive, long-horizon memory"
- **Break condition:** If models solve reordering via surface-level visual similarity rather than action-grounded reasoning, the mechanism would not measure embodied cognition.

### Mechanism 2
- **Claim:** Inverse world modeling (ordering actions given observations) is consistently easier for VLMs than forward world modeling, revealing asymmetric capabilities in retrospective textual reasoning versus prospective visual simulation.
- **Mechanism:** Given ordered visual state transitions, models must map each change to a textual action description. This retrospection leverages stronger language priors—models are better at describing what changed than predicting what will change from symbolic actions.
- **Core assumption:** The performance gap between inverse and forward tasks reflects genuine differences in cognitive demand, not task artifact.
- **Evidence anchors:** [section 3.1] "Models consistently perform better on inverse task than forward one... This asymmetry suggests that models handle retrospective textual reasoning better than the prospective visual simulation required for forward planning."
- **Break condition:** If inverse advantage stems purely from shorter or simpler action descriptions rather than cognitive asymmetry, the mechanism would be confounded.

### Mechanism 3
- **Claim:** VLMs exhibit anthropocentric biases—right-handed preference and sensitivity to human-like camera intrinsics—transferred from training on human-centric visual data.
- **Mechanism:** Training data overrepresents right-handed actions and human-eye-like viewpoints. When evaluated on non-standard embodiments (fisheye lenses, elevated cameras, left-handed predicates), models fail to generalize.
- **Core assumption:** Performance degradation under non-human configurations reflects training data bias rather than task difficulty inherent to those configurations.
- **Evidence anchors:** [section 3.4] "VLMs exhibit strong right-handed bias... 9.38% of true left-hand changes were incorrectly identified as right-hand changes, whereas only 4.67% of right-hand changes were misattributed to the left."
- **Break condition:** If degradation were due to insufficient training on any diverse camera data (not specifically non-human), the mechanism would be misattributed.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: ENACT formalizes embodied interaction as a POMDP where agents must reason under partial observability from egocentric views.
  - Quick check question: Given a sequence of egocentric observations and actions, can you identify what information remains unobservable at each step?

- **Concept: Scene Graphs as Symbolic State Representations**
  - Why needed here: Actions are defined as scene graph differences (predicate changes like Add Open(fridge)), not low-level motor commands. Understanding this abstraction is essential for interpreting the action space.
  - Quick check question: If a scene graph contains {OnTop(cup, table), Inside(milk, fridge)}, what predicates change when the robot grasps the cup?

- **Concept: World Models in Embodied AI**
  - Why needed here: The benchmark evaluates whether VLMs have internalized world models—action-conditioned predictors of environmental dynamics—through the forward/inverse reordering lens.
  - Quick check question: What is the difference between a forward world model (predicting states from actions) and an inverse world model (inferring actions from states)?

## Architecture Onboarding

- **Component map:** BEHAVIOR simulator -> Segmentation module -> Key-frame trajectory sampler (KFTS) -> QA generator -> Online verifier

- **Critical path:** Raw trajectory from simulator -> Scene graph extraction per frame -> Key-frame segmentation by state-change detection -> Valid trajectory sampling via DAG path counting -> QA pair generation with permutation-based labels -> VLM evaluation with semantic verifier

- **Design tradeoffs:**
  - **Symbolic actions vs. continuous motor commands:** Chosen symbolic (scene graph deltas) for scalability and interpretability; sacrifices low-level dynamics fidelity.
  - **Reordering vs. generation:** Chose reordering to isolate reasoning from visual synthesis quality; cannot evaluate generative world models directly.
  - **Simulation vs. real-world:** Primary data from BEHAVIOR simulator; real-world validation (960 QAs) shows consistent trends with minimal sim-to-real gap.

- **Failure signatures:**
  - **Hallucination (43.9% forward, 41.8% inverse):** Predicting state changes that did not occur—indicates reliance on textual priors over visual grounding.
  - **Omission (37.1% forward, 41.8% inverse):** Failing to predict visible state changes—indicates poor object persistence tracking under partial observability.
  - **Right-hand mixing (9.38% left→right vs 4.67% right→left):** Incorrectly attributing left-hand actions to right hand—indicates anthropocentric bias.
  - **Long-horizon collapse:** Task accuracy approaches 0% at L≥8 for most models—indicates limited interactive spatial memory.

- **First 3 experiments:**
  1. **Baseline evaluation on ENACT:** Run GPT-5 mini or InternVL3.5-241B on the 8,972 QA pairs, reporting pairwise accuracy across step lengths L∈{3,...,10} to confirm the forward/inverse gap and horizon degradation.
  2. **Ablation on action representation:** Test natural language vs. symbolic predicates vs. emoji encodings on a 2,304-QA subset to verify that inverse advantage persists across encodings.
  3. **Anthropocentric bias probe:** Evaluate on camera configuration variants (fisheye, aperture 80, high viewpoint) to measure performance degradation and confirm sensitivity to non-human intrinsics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can finetuning VLMs on the ENACT dataset improve their embodied world modeling abilities?
- **Basis in paper:** [explicit] The authors state in the Limitations section, "we do not explore finetuning VLM in this work, but we expect our automatic and scalable dataset can also bring benefits."
- **Why unresolved:** The study focuses exclusively on the evaluation of pre-trained models; the potential of the dataset as a training resource for improving embodied cognition remains untested.
- **What evidence would resolve it:** Finetuning representative VLMs on the ENACT dataset and measuring performance improvements on both the benchmark and downstream real-world robotics tasks.

### Open Question 2
- **Question:** How can video generative models be fairly evaluated for physical consistency within this framework?
- **Basis in paper:** [explicit] The paper excludes video generation models due to "frequent physical inconsistency of generated rollouts and the difficulty of designing fair evaluation metrics."
- **Why unresolved:** The current benchmark relies on symbolic ordering of discrete frames, which does not map easily to the continuous, often physically implausible outputs of video models.
- **What evidence would resolve it:** Developing automated metrics that can validate physical consistency in continuous video rollouts without relying on expensive human studies.

### Open Question 3
- **Question:** Do the identified biases (e.g., right-handedness, human-like FOV) generalize across a wider range of model architectures?
- **Basis in paper:** [explicit] The authors acknowledge that "in-depth ablation experiments were necessarily focused on a representative subset of models" due to computational costs.
- **Why unresolved:** It is unclear if the anthropocentric biases and degradation trends observed in frontier models (like GPT-5) persist in smaller or structurally different architectures.
- **What evidence would resolve it:** Scaling the evaluation to include a more diverse set of VLMs, particularly smaller open-weight models, to verify the universality of the embodied biases.

## Limitations
- The benchmark's reliance on symbolic scene graph predicates limits its ability to evaluate low-level physical reasoning and motor control aspects of embodied cognition.
- Performance degradation under non-human camera configurations could stem from either training data bias or insufficient exposure to diverse viewpoints during pre-training.
- The symbolic action space (11 predicates) may not comprehensively represent all types of embodied interactions needed for real-world generalization.

## Confidence
- **High confidence:** The benchmark construction methodology (POMDP formalization, KFTS sampling, semantic verifier) is well-specified and reproducible. The observed performance gap between inverse and forward tasks is robust across multiple model families and action encodings.
- **Medium confidence:** The interpretation of anthropocentric bias findings as evidence of training data bias is plausible but not definitively proven; alternative explanations exist.
- **Low confidence:** Claims about what specific cognitive capabilities are measured by the reordering tasks (beyond sequence ordering) are largely inferential.

## Next Checks
1. **Alternative action encoding experiment:** Test the same VLMs on the ENACT benchmark using continuous motor command representations instead of symbolic scene graph predicates to isolate whether symbolic abstraction drives the inverse/forward asymmetry.
2. **Sim-to-real generalization study:** Evaluate VLMs on the real-world subset of 960 ENACT QA pairs using a held-out test set, comparing performance degradation patterns to the simulation-only results to validate the benchmark's ecological validity.
3. **Training data bias isolation:** Train a VLM from scratch on diverse camera configurations and evaluate on ENACT to determine whether the observed biases are remediable through targeted data augmentation versus requiring architectural changes.