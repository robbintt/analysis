---
ver: rpa2
title: Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering
arxiv_id: '2511.01090'
source_url: https://arxiv.org/abs/2511.01090
tags:
- data
- romanian
- educational
- filtering
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-dimensional filtering approach to
  improve Romanian pretraining data quality for Large Language Models (LLMs). The
  method involves training a lightweight multitask classifier on LLM-annotated Romanian
  texts, which predicts educational value, topic, format, and reader education level.
---

# Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering

## Quick Facts
- arXiv ID: 2511.01090
- Source URL: https://arxiv.org/abs/2511.01090
- Authors: Vlad Negoita; Mihai Masala; Traian Rebedea
- Reference count: 16
- Primary result: Multi-dimensional filtering of Romanian pretraining data significantly improves downstream LLM performance across Ro-MMLU, Ro-ARC, and Ro-HellaSwag benchmarks.

## Executive Summary
This paper addresses the challenge of improving Romanian pretraining data quality for Large Language Models (LLMs) by introducing a multi-dimensional filtering approach. The method uses a lightweight multitask classifier to predict educational value, topic, format, and reader education level on LLM-annotated Romanian texts. By applying this classifier to the FineWeb2 corpus with a threshold of 3.5 for educational value, the researchers created FineWeb2-Edu-Ro, a high-quality dataset. Continual pretraining of a Llama-2-7B model on this filtered dataset showed significant performance improvements across multiple benchmarks compared to unfiltered data and another filtering method (JQL).

## Method Summary
The approach involves training a lightweight multitask classifier on LLM-annotated Romanian texts to predict educational value, topic, format, and reader education level. The researchers used Gemma-3-12B-it with Romanian chain-of-thought prompts to annotate 1M+ FineWeb2 Romanian samples, then distilled these annotations into a RoBERT-base encoder with 4 prediction heads. The filtered dataset (FineWeb2-Edu-Ro) was created by applying the classifier to the FineWeb2 corpus with a threshold of 3.5 for educational value. A Llama-2-7B model was continually pretrained on this filtered dataset and evaluated against unfiltered baselines and JQL filtering on Ro-MMLU, Ro-ARC, and Ro-HellaSwag benchmarks.

## Key Results
- Significant performance improvements across Ro-MMLU, Ro-ARC, and Ro-HellaSwag benchmarks when pretraining on FineWeb2-Edu-Ro versus unfiltered data
- Notable topic distribution differences between Romanian and English data, with Romanian texts showing higher proportions of Finance & Business, Health, and Politics content
- The multi-dimensional filtering approach outperformed another filtering method (JQL) on Romanian downstream tasks

## Why This Works (Mechanism)
The multi-dimensional filtering approach works by creating a high-quality pretraining dataset that better represents educational content across multiple dimensions. By using LLM annotation followed by distillation into a lightweight classifier, the method scales effectively to large corpora while maintaining quality control. The educational value threshold ensures that only content meeting a minimum quality standard is included, while the topic, format, and education level predictions enable diverse and appropriately targeted pretraining data.

## Foundational Learning

- **Multitask Learning**: Training a single model to perform multiple related tasks simultaneously, improving generalization and efficiency. Needed because it enables comprehensive data filtering across educational value, topic, format, and education level in one pass. Quick check: Verify that the multitask classifier achieves reasonable performance on all four prediction tasks (RMSE ~0.96 for educational value, ~0.69 accuracy for topic).

- **Chain-of-Thought Prompting**: A prompting technique that encourages LLMs to generate intermediate reasoning steps before providing final answers. Needed to improve the quality and consistency of LLM-generated annotations for the training data. Quick check: Compare annotation quality with and without CoT prompting on a small validation set.

- **Continual Pretraining**: The process of further training a pretrained LLM on domain-specific or higher-quality data to adapt its capabilities. Needed to leverage the improved FineWeb2-Edu-Ro dataset for better downstream performance on Romanian tasks. Quick check: Monitor validation loss during continual pretraining to ensure effective learning without catastrophic forgetting.

## Architecture Onboarding

- **Component Map**: FineWeb2 Romanian corpus -> Gemma-3-12B-it annotation (with Ro CoT prompts) -> RoBERT-base multitask classifier (4 heads) -> Educational value filtering (threshold 3.5) -> FineWeb2-Edu-Ro dataset -> Llama-2-7B continual pretraining -> Downstream evaluation

- **Critical Path**: Gemma-3-12B-it annotation -> RoBERT-base multitask classifier training -> Educational value filtering -> Llama-2-7B continual pretraining

- **Design Tradeoffs**: The choice between annotation quality and scale (using LLM vs human annotation), model complexity (multitask vs single-task classifiers), and filtering strictness (educational value threshold) all impact the final dataset quality and downstream performance.

- **Failure Signatures**: Poor annotation quality leading to noisy training labels, over-filtering or under-filtering from suboptimal threshold, and catastrophic forgetting during continual pretraining are potential failure modes that can be diagnosed through validation metrics and downstream performance.

- **First Experiments**:
  1. Train the multitask classifier on the annotated data and evaluate its performance on the held-out validation set.
  2. Apply the classifier to the FineWeb2 Romanian corpus with different educational value thresholds and analyze the resulting dataset characteristics.
  3. Continually pretrain Llama-2-7B on FineWeb2-Edu-Ro and evaluate on Ro-MMLU, Ro-ARC, and Ro-HellaSwag to establish baseline performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-generated annotations without extensive human validation at scale
- Limited ablation of filtering thresholds and their impact on downstream task performance
- Absence of a controlled comparison using the same corpus with different filtering methods applied identically

## Confidence

| Claim | Confidence |
|-------|------------|
| Significant downstream performance gains with FineWeb2-Edu-Ro | High |
| Multi-dimensional filtering approach outperforms JQL | Medium |
| Romanian data has inherently different topic distribution than English | Low |

## Next Checks
1. **Annotator Agreement Audit**: Re-annotate a random subset of 200 documents (100 from the original 100-sample set, 100 newly sampled) using both Gemma-3-12B-it and human raters to measure label stability and calibration of the educational value scores.

2. **Threshold Sensitivity Analysis**: Retrain the downstream Llama-2-7B model using FineWeb2-Edu-Ro subsets created with varying educational value thresholds (3.0, 3.5, 4.0) and compare Ro-MMLU/ARC/HellaSwag performance to identify the optimal tradeoff between data quantity and quality.

3. **Cross-Lingual Corpus Comparison**: Apply the same FineWeb2-Edu-Ro filtering pipeline to the English FineWeb2 subset and directly compare the resulting dataset characteristics (token count, educational value distribution, topic balance) and downstream model performance to isolate language-specific effects from filtering effects.