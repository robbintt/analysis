---
ver: rpa2
title: 'WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data
  and Scalable Reinforcement Learning'
arxiv_id: '2509.13305'
source_url: https://arxiv.org/abs/2509.13305
tags:
- tool
- search
- arxiv
- customers
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WebSailor-V2 addresses the gap between open-source and proprietary
  deep research agents by introducing a complete post-training pipeline featuring
  enhanced synthetic data construction and scalable reinforcement learning. The method
  uses SailorFog-QA-V2, which builds a densely connected knowledge graph with diverse
  uncertainty definitions beyond simple obfuscation, and employs a dual-environment
  RL framework combining a high-fidelity simulator and a robust real-world setup integrated
  within a data-policy feedback loop.
---

# WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.13305
- Source URL: https://arxiv.org/abs/2509.13305
- Reference count: 11
- Qwen3-30B-A3B trained on WebSailor-V2 achieves 35.3 on BrowseComp-EN, 44.1 on BrowseComp-ZH, and 30.6 on HLE, outperforming all existing open-source agents and surpassing the 671B DeepSeek-V3.1

## Executive Summary
WebSailor-V2 introduces a comprehensive post-training pipeline that significantly narrows the performance gap between open-source and proprietary deep research agents. The system leverages enhanced synthetic data construction through SailorFog-QA-V2, which builds densely connected knowledge graphs with diverse uncertainty definitions, and employs a dual-environment reinforcement learning framework combining high-fidelity simulation with real-world integration. When trained on Qwen3-30B-A3B, WebSailor-V2 achieves state-of-the-art performance among open-source agents and competitive results with leading proprietary systems, demonstrating that sophisticated synthetic data and scalable RL can bridge the chasm to proprietary agent capabilities.

## Method Summary
WebSailor-V2 addresses the gap between open-source and proprietary deep research agents through a complete post-training pipeline featuring enhanced synthetic data construction and scalable reinforcement learning. The method uses SailorFog-QA-V2, which builds a densely connected knowledge graph with diverse uncertainty definitions beyond simple obfuscation, and employs a dual-environment RL framework combining a high-fidelity simulator and a robust real-world setup integrated within a data-policy feedback loop. Training on Qwen3-30B-A3B, WebSailor-V2 achieves state-of-the-art performance among open-source agents and competitive results with leading proprietary systems.

## Key Results
- WebSailor-V2 achieves 35.3 on BrowseComp-EN, 44.1 on BrowseComp-ZH, and 30.6 on HLE
- Outperforms all existing open-source agents on benchmark tasks
- Surpasses the 671B parameter DeepSeek-V3.1, demonstrating competitive performance with proprietary systems

## Why This Works (Mechanism)
WebSailor-V2's effectiveness stems from its dual-pronged approach: sophisticated synthetic data generation and scalable reinforcement learning. The SailorFog-QA-V2 synthetic data construction method creates a densely connected knowledge graph that captures diverse uncertainty patterns beyond simple obfuscation, providing rich training signals for the agent. The dual-environment RL framework combines a high-fidelity simulator for safe exploration and rapid iteration with a real-world environment for robust deployment capabilities, integrated within a data-policy feedback loop that continuously refines both the synthetic data and the agent's policies.

## Foundational Learning
- **Synthetic Data Construction**: Why needed: Traditional obfuscation methods limit training diversity; quick check: Verify knowledge graph density metrics
- **Knowledge Graph Density**: Why needed: Ensures comprehensive coverage of uncertainty scenarios; quick check: Measure node connectivity and path diversity
- **Dual-Environment RL**: Why needed: Balances safe exploration with real-world robustness; quick check: Compare policy performance across simulator and real environments
- **Data-Policy Feedback Loop**: Why needed: Enables continuous improvement of both data quality and agent behavior; quick check: Track performance trends over training iterations

## Architecture Onboarding

**Component Map:**
SailorFog-QA-V2 (Knowledge Graph Construction) -> Dual-Environment RL Framework (Simulator + Real-World) -> Data-Policy Feedback Loop -> Trained Agent

**Critical Path:**
Knowledge graph construction → Synthetic data generation → RL policy training → Real-world evaluation → Feedback integration

**Design Tradeoffs:**
- High-fidelity simulation vs. computational cost
- Synthetic data diversity vs. generation complexity
- Real-world robustness vs. training stability

**Failure Signatures:**
- Knowledge graph sparsity leading to poor uncertainty handling
- Simulator-reality gap causing policy degradation
- Feedback loop instability resulting in oscillating performance

**First 3 Experiments:**
1. Test synthetic data generation with varying graph densities and uncertainty types
2. Evaluate RL policy performance across simulator and real-world environments
3. Measure the impact of feedback loop frequency on convergence and stability

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark comparisons rely on standardized test sets with potential synthetic data contamination
- DeepSeek-V3.1 comparison conditions and hardware differences not fully specified
- Proprietary systems' architectures and training methodologies remain opaque, complicating direct attribution

## Confidence
- **High Confidence**: Technical methodology for synthetic data construction and dual-environment RL framework is well-described and reproducible
- **Medium Confidence**: Benchmark results against open-source agents are reliable, but comparisons to proprietary systems require cautious interpretation
- **Medium Confidence**: Claim of surpassing DeepSeek-V3.1 needs verification under controlled, comparable conditions

## Next Checks
1. Conduct ablation studies removing synthetic data components to quantify their specific contribution to performance gains
2. Test WebSailor-V2 on proprietary benchmarks (if accessible) or on newly constructed, uncontaminated test sets to verify generalization claims
3. Implement cross-platform evaluation using identical hardware and software configurations when comparing against both open-source and proprietary baselines