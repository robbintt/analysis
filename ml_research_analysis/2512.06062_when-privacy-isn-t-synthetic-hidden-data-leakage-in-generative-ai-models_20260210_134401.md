---
ver: rpa2
title: 'When Privacy Isn''t Synthetic: Hidden Data Leakage in Generative AI Models'
arxiv_id: '2512.06062'
source_url: https://arxiv.org/abs/2512.06062
tags:
- data
- synthetic
- real
- training
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hidden privacy leakage in synthetic
  data generated by modern generative models, even under strict black-box access.
  The core idea is that synthetic data can leak information about training data through
  distributional overlap, detectable by analyzing cluster structures in synthetic
  samples.
---

# When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models

## Quick Facts
- arXiv ID: 2512.06062
- Source URL: https://arxiv.org/abs/2512.06062
- Reference count: 40
- This paper demonstrates that synthetic data from non-private generators can leak training data structure through cluster-level distributional overlap, detectable via the proposed Cluster-Medoid Leakage Attack (CMLA).

## Executive Summary
This paper reveals a hidden privacy vulnerability in synthetic data generation: even without direct memorization, non-private generative models can leak training data information through distributional overlap detectable at the cluster level. The authors propose CMLA, a model-agnostic attack that samples synthetic data, clusters it, extracts medoids, and measures proximity to real data points. Experiments show significant leakage in standard generators (ASR up to 0.6, coverage up to 48%) while privacy-preserving models maintain much lower overlap. The findings demonstrate that high-fidelity synthetic data can unintentionally expose training data structure, and CMLA provides a practical audit tool for privacy risk assessment.

## Method Summary
The Cluster-Medoid Leakage Attack (CMLA) works by repeatedly sampling synthetic data from a black-box generator, encoding mixed-type attributes (one-hot categorical, standardized continuous), and clustering the synthetic samples using HDBSCAN. Cluster medoids are extracted as representatives of dense synthetic regions, and their proximity to real data points is measured using nearest-neighbor distances. Attack Success Rate (ASR) and Coverage metrics quantify bidirectional overlap at various thresholds. The method is model-agnostic and requires only black-box access to the generator, making it applicable for auditing synthetic data generators across domains.

## Key Results
- Non-private generators (CTGAN, TVAE, TabDDPM, GReaT) show high ASR (up to 0.6) and coverage (up to 48%) at small thresholds, indicating significant structural leakage
- Privacy-preserving models (PATE-GAN, PrivBayes) maintain much lower overlap, with ASR remaining below 0.10 and coverage around 1-5%
- GReaT on Adult dataset shows exact training point reproduction with d_min = 0.0000 and median = 0.0000
- The attack works effectively without requiring white-box access to the generator architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic data clusters align with high-density regions of training data, enabling membership inference without direct memorization.
- **Mechanism**: Generative models approximate the true data distribution P to produce realistic samples. When they learn local structure faithfully, synthetic samples form tight clusters whose geometry mirrors training data neighborhoods. Cluster medoids act as "structural echoes" of original data modes.
- **Core assumption**: The generator has learned distributional structure closely enough that synthetic modes correspond to real modes.
- **Evidence anchors**:
  - [abstract] "synthetic data can leak information about the underlying training samples through structural overlap in the data manifold"
  - [Section III-C] "clusters in the synthetic data tend to align with high-density regions of the real training data"
  - [corpus] Related work on MIA dynamics confirms models encode membership information during training.

### Mechanism 2
- **Claim**: Medoids extracted from synthetic clusters serve as proxies for identifying training record proximity.
- **Mechanism**: Unsupervised clustering identifies dense synthetic regions. The medoid—most centrally located actual sample—summarizes each cluster. Because medoids are real synthetic points, they represent concrete generator outputs that can be measured against real data distance.
- **Core assumption**: Clustering parameters correctly identify meaningful dense regions rather than noise.
- **Evidence anchors**:
  - [Section III-D] "Each medoid can be viewed as a 'snapshot' of a synthetic mode: if that mode corresponds to a true region in the training data, the medoid will lie close to one or more real records."
  - [Table I] GReaT on Adult shows min d_min of 0.0000 and median of 0.0000, demonstrating medoids at exact or near-exact training point locations.

### Mechanism 3
- **Claim**: Attack Success Rate (ASR) and Coverage metrics quantify distributional overlap in a model-agnostic manner.
- **Mechanism**: ASR(τ) measures what fraction of medoids fall within threshold τ of any real record. Coverage(τ) measures what fraction of real records have a medoid within τ. Together they capture bidirectional proximity without requiring knowledge of model architecture.
- **Core assumption**: The distance metric δ and threshold τ are semantically meaningful for the data domain.
- **Evidence anchors**:
  - [Section III-E] "ASR(τ) answers the question: 'What fraction of the generator's mode representatives fall suspiciously close to the training data?'"
  - [Figure 3/4 descriptions] Non-private models show early ASR rises at τ≈0.1 (ASR 0.45-0.60) while PATE-GAN and PrivBayes remain below 0.10.

## Foundational Learning

- **Concept**: Differential Privacy (DP) noise calibration
  - **Why needed here**: Understanding why PATE-GAN and PrivBayes show reduced leakage requires grasping how DP bounds individual influence on outputs through calibrated noise.
  - **Quick check question**: Can you explain why adding noise proportional to sensitivity reduces the ability to detect whether a specific record influenced model outputs?

- **Concept**: Density-based clustering (DBSCAN/HDBSCAN)
  - **Why needed here**: The attack relies on identifying dense synthetic regions without specifying cluster count a priori—HDBSCAN automatically determines K based on density reachability.
  - **Quick check question**: How does HDBSCAN distinguish between core points, border points, and noise, and why does this matter for identifying meaningful synthetic modes?

- **Concept**: Membership Inference Attacks (MIA)
  - **Why needed here**: CMLA extends MIA logic to structural/geometric leakage rather than confidence-based inference; understanding traditional MIA helps contextualize this work's contribution.
  - **Quick check question**: What is the fundamental difference between confidence-based MIA (exploiting generalization gaps) and the structural overlap approach proposed here?

## Architecture Onboarding

- **Component map**: Generator G (black-box) → Sample M synthetic records → Encode Ψ(·) → Cluster (HDBSCAN) → Extract K medoids → Compute d_min to real data → Report ASR(τ), Coverage(τ)

- **Critical path**: Encoding quality determines clustering quality determines medoid meaningfulness determines metric validity. The shared representation Ψ must preserve semantic relationships across mixed-type attributes.

- **Design tradeoffs**:
  - More samples M → better distribution approximation but higher compute
  - Tighter clustering → more medoids, finer-grained analysis but risk of overfitting to sampling noise
  - Stricter τ thresholds → more conservative privacy assessment but may miss meaningful leakage
  - Dimensionality reduction → clearer cluster structure but potential distance distortion

- **Failure signatures**:
  - ASR curves flat at zero until very large τ: generator may be producing uniformly distributed or heavily noised outputs
  - Coverage extremely low across all models: encoding may be collapsing semantic distinctions
  - Large gap between mean and median d_min (heavy tails): leakage concentrated in few clusters, not systematic

- **First 3 experiments**:
  1. Replicate Figure 3 on a new dataset to validate that ASR curves match expected privacy/fidelity tradeoff patterns before auditing proprietary generators.
  2. Ablate the encoding function: compare raw one-hot+standardized vs. PCA-reduced vs. UMAP embeddings to assess sensitivity of clustering quality and resulting ASR/Coverage metrics to representation choices.
  3. Test break conditions: train a generator with increasing DP noise levels (ε = ∞, 10, 1, 0.1) and verify that ASR(τ=0.1) decreases monotonically as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can proximity-based privacy audits be effectively extended to conditional generation settings where synthetic data is produced subject to constraints or prompts?
- **Basis in paper**: [explicit] Conclusion explicitly calls for "extending proximity audits to conditional generation" to establish standardized privacy evaluations.
- **Why unresolved**: The current CMLA framework evaluates unconditional generation; conditional generators may leak information differently, as conditioning could expose finer-grained distributional structure.
- **What evidence would resolve it**: Experiments applying CMLA to conditional generators, measuring whether ASR/coverage increase when conditioning reveals sensitive subgroups.

### Open Question 2
- **Question**: What defense mechanisms specifically address distributional neighborhood inference rather than just sample-level memorization?
- **Basis in paper**: [explicit] Abstract calls for "stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone."
- **Why unresolved**: Current DP defenses reduce leakage but are not designed to explicitly prevent cluster-structure overlap; the relationship between DP noise injection and geometric overlap remains underexplored.
- **What evidence would resolve it**: Novel regularization techniques or training objectives that explicitly penalize synthetic-real cluster alignment, evaluated via CMLA metrics.

### Open Question 3
- **Question**: How sensitive are CMLA results to the choice of encoding function Ψ, distance metric δ, and clustering algorithm?
- **Basis in paper**: [inferred] Methodology section describes using one-hot encoding, standardization, and DBSCAN/HDBSCAN, but does not evaluate robustness to alternative choices.
- **Why unresolved**: Different encodings or metrics could significantly alter distance distributions, cluster detection, and thus ASR/coverage measurements, potentially over- or under-estimating leakage.
- **What evidence would resolve it**: Ablation experiments varying encodings, distance metrics, and clustering methods to quantify metric sensitivity.

### Open Question 4
- **Question**: Does CMLA generalize effectively to non-tabular data modalities such as images, text, and time series?
- **Basis in paper**: [explicit] Section III-H states the framework "can be extended to non-tabular domains—such as images, text, or time series—by choosing appropriate encoders and similarity metrics."
- **Why unresolved**: The paper evaluates only tabular benchmarks; high-dimensional and unstructured data may exhibit different cluster structures, manifold geometries, and privacy-utility trade-offs that affect attack efficacy.
- **What evidence would resolve it**: Experiments applying CMLA to image, text, and time-series generators using modality-appropriate encoders.

## Limitations
- The attack's effectiveness depends on generator quality—undertrained or heavily regularized models may not produce meaningful clusters
- Clustering hyperparameters are not fully specified, potentially affecting reproducibility
- The method may produce false positives if synthetic data naturally clusters similarly to real data due to domain constraints
- Current evaluation is limited to tabular data; extension to other modalities requires careful adaptation

## Confidence

- **High Confidence**: The existence of leakage in non-private generators (GReaT showing d_min = 0.0000 confirms exact training point reproduction); the basic mechanism of clustering synthetic data to detect distributional overlap; the contrast between private and non-private generators' performance.
- **Medium Confidence**: The generalizability of ASR/Coverage metrics as comprehensive privacy indicators; the sufficiency of small τ thresholds for detecting meaningful privacy risk; the claim that this method works "without requiring white-box access."
- **Low Confidence**: Performance under different encoding schemes (the impact of PCA/UMAP is mentioned but not characterized); the attack's effectiveness against generators with sophisticated privacy mechanisms beyond DP; the robustness to adversarial generator designs that deliberately obscure cluster structure.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary HDBSCAN parameters (min_cluster_size, min_samples) across multiple orders of magnitude and measure impact on ASR/Coverage—this will reveal whether results are robust or parameter-dependent.

2. **Cross-Dataset Generalization**: Apply CMLA to a completely different domain (e.g., image datasets using GANs) to test whether structural clustering leakage extends beyond tabular data as claimed.

3. **Adversarial Generator Test**: Design a generator that deliberately avoids mode clustering (e.g., uniform distribution within support bounds) and verify that CMLA correctly reports minimal leakage, confirming the method doesn't produce false positives.