---
ver: rpa2
title: 'Reimagining Anomalies: What If Anomalies Were Normal?'
arxiv_id: '2402.14469'
source_url: https://arxiv.org/abs/2402.14469
tags:
- normal
- anomaly
- each
- anomalies
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for generating counterfactual
  explanations (CEs) for image anomaly detection models. The core idea is to train
  a generator to produce multiple alternative modifications of each anomaly, each
  capturing a different concept of anomalousness, while ensuring these modifications
  are perceived as normal by the anomaly detector.
---

# Reimagining Anomalies: What If Anomalies Were Normal?

## Quick Facts
- arXiv ID: 2402.14469
- Source URL: https://arxiv.org/abs/2402.14469
- Reference count: 40
- Primary result: Introduces a method to generate counterfactual explanations for image anomaly detection models that are perceived as normal by the detector

## Executive Summary
This paper presents a novel approach to generating counterfactual explanations for image anomaly detection models. The core innovation is training a generator to create multiple alternative modifications of anomalous inputs, each capturing a different concept of anomalousness while ensuring these modifications appear normal to the detector. Unlike traditional feature-attribution methods, this approach answers what changes would make an anomaly appear normal, providing semantic explanations that reveal detector biases and limitations.

The method is evaluated across multiple datasets (MNIST, Colored-MNIST, CIFAR-10, GTSDB, and ImageNet-Neighbors) and three state-of-the-art anomaly detection models. Results show that generated counterfactuals successfully appear normal to detectors (AuROC close to 50% on most datasets), maintain realism (FID scores similar to anomalies), and capture multiple disentangled concepts (concept classifier accuracy >90% in most cases).

## Method Summary
The method trains a conditional generator to produce counterfactual explanations by minimizing an anomaly score loss while satisfying additional constraints. The generator takes an anomalous image and target parameters, outputting modified versions that reduce the anomaly score. The approach combines three key mechanisms: anomaly score minimization to make outputs appear normal, concept disentanglement to capture multiple modes of normality, and cycle-consistency to ensure minimal editing. The framework is implemented using both GAN and diffusion-based approaches, with the GAN variant using a Wide ResNet encoder-decoder architecture.

## Key Results
- Generated counterfactuals achieve AuROC close to 50% on most datasets, indicating they appear normal to detectors
- FID scores show counterfactuals maintain similar realism to original anomalies
- Concept classifier accuracy exceeds 90% in most cases, demonstrating successful disentanglement
- Method successfully reveals detector biases, exposing cases where supervised classifiers overfit to specific anomaly characteristics

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Optimization via Anomaly Score Minimization
The generator modifies anomalous inputs to align with the detector's distribution of "normal" data by minimizing a targeted anomaly score loss. This effectively "fools" the detector into classifying the output as normal. The core assumption is that the pre-trained anomaly detector is differentiable so the generator can receive meaningful learning signals.

### Mechanism 2: Concept Disentanglement for Multi-Modal Explanations
The architecture separates distinct semantic factors that trigger an anomaly flag by conditioning the generator on categorical concept indices. A concept classifier is trained concurrently to identify which concept is present in the generated image, forcing the generator to map different latent directions to different semantic modes of normality.

### Mechanism 3: Cycle-Consistency for Minimal Editing
The counterfactuals remain recognizable as modifications of the original anomaly rather than arbitrary normal samples by enforcing a reconstruction constraint. This creates a closed loop that penalizes drastic, unnecessary deviations, ensuring the explanation focuses only on the anomalous features.

## Foundational Learning

- **Concept: Generative Adversarial Networks (GANs)**
  - Why needed here: The implementation uses a GAN framework to ensure generated counterfactuals look realistic (indistinguishable from real data)
  - Quick check question: How does the Discriminator guide the Generator to produce realistic images vs. the Anomaly Detector guiding it to produce normal images?

- **Concept: One-Class Classification (e.g., DSVDD)**
  - Why needed here: Understanding these detectors map normal data to a tight cluster helps explain why "shrinking" distance to the center defines becoming "normal"
  - Quick check question: If a DSVDD model considers "cats" normal, what does it mean for an image to be "normal" in the latent space?

- **Concept: Latent Space Interpolation/Traversal**
  - Why needed here: The method effectively traverses latent space from an "anomalous" region to a "normal" one
  - Quick check question: Does the generator traverse a continuous path or jump to a discrete normal prototype?

## Architecture Onboarding

- **Component map:** Anomalous Image $x$ -> Conditional Generator $G$ -> Counterfactual $\bar{x}$ -> Anomaly Detector $\phi$ + Discriminator $D$ + Concept Classifier $R$
- **Critical path:** Input Anomaly $x$ is fed to $G$ alongside target score $\alpha=0$ and concept $k$. $G$ outputs $\bar{x}$ (Counterfactual). $\phi(\bar{x})$ provides "Normality Loss". $D(\bar{x})$ provides "Realism Loss". $R(\bar{x})$ provides "Disentanglement Loss".
- **Design tradeoffs:** GANs are faster but harder to train stable disentanglement vs. Diffusion models that handle high-resolution data better but are computationally expensive. Balancing $\lambda_\phi$ (make it normal) vs $\lambda_{rec}$ (don't change it too much).
- **Failure signatures:** Identity Collapse (outputs unchanged anomaly), Semantic Hallucination (generates generic normal image), Concept Collapse (all concepts produce same output)
- **First 3 experiments:** Lambda Sweep on MNIST to find optimal balance, Ablation on Concepts (K=1 vs K=3) to test disentanglement, Bias Test on a detector trained only on blue anomalies

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the ideal number of categorical concepts ($K$) be determined automatically rather than being set manually as a hyperparameter?
- **Open Question 2:** How can the generator optimization be modified to prevent "over-correction" where the model makes unnecessary changes to multiple attributes instead of the minimal modification?
- **Open Question 3:** Can unsupervised prompt learning or prompt engineering be effectively integrated into the diffusion-based branch to enhance semantic explainability?
- **Open Question 4:** Can techniques from adversarial examples be leveraged to stabilize the generator and improve counterfactual quality?

## Limitations

- The method assumes the anomaly detector is fixed and differentiable, but real-world detectors may have non-smooth decision boundaries or suffer from overfitting
- Claims of concept disentanglement lack rigorous statistical validation of independence between concepts
- Computational feasibility for real-time applications is not thoroughly addressed, particularly for diffusion-based approaches

## Confidence

- **High Confidence:** The mechanism for generating normal-looking counterfactuals (anomaly score minimization) is well-supported by the optimization framework and evaluation metrics
- **Medium Confidence:** The concept disentanglement claims are supported by qualitative examples but lack rigorous statistical validation of independence between concepts
- **Medium Confidence:** The cycle-consistency mechanism for minimal editing is theoretically sound but the paper doesn't thoroughly analyze trade-offs between minimal changes and semantic relevance

## Next Checks

1. **Detector Sensitivity Analysis:** Test the method on detectors with known biases or non-smooth decision boundaries to assess robustness of counterfactual generation across different detector architectures
2. **Concept Independence Validation:** Perform quantitative analysis (e.g., correlation analysis, intervention studies) to verify that generated concepts are truly disentangled rather than merely orthogonal
3. **Computational Efficiency Benchmarking:** Measure inference time and memory requirements for both GAN-based and diffusion-based generators across different image resolutions to establish practical deployment constraints