---
ver: rpa2
title: 'Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling'
arxiv_id: '2601.15664'
source_url: https://arxiv.org/abs/2601.15664
tags:
- object
- composition
- arxiv
- image
- multi-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Skywork UniPic 3.0, a unified multimodal framework
  that integrates single-image editing and multi-image composition. The key innovation
  is formulating multi-image composition as a sequence modeling problem, where the
  target image latent is concatenated with reference image latents to form a unified
  sequence.
---

# Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling

## Quick Facts
- arXiv ID: 2601.15664
- Source URL: https://arxiv.org/abs/2601.15664
- Authors: Hongyang Wei; Hongbo Liu; Zidong Wang; Yi Peng; Baixin Xu; Size Wu; Xuying Zhang; Xianglong He; Zexiang Liu; Peiyu Wang; Xuchen Song; Yangguang Li; Yang Liu; Yahui Zhou
- Reference count: 40
- Primary result: Achieves state-of-the-art single-image editing and surpasses Nano-Banana and Seedream 4.0 on MultiCom-Bench for multi-image composition

## Executive Summary
Skywork UniPic 3.0 presents a unified framework that integrates single-image editing and multi-image composition through a sequence modeling approach. The key innovation is concatenating target image latents with reference image latents to form a unified sequence, enabling end-to-end training for both tasks. The authors introduce a comprehensive data curation pipeline focused on Human-Object Interaction scenarios, collecting 215K high-quality training samples. The framework achieves significant acceleration through trajectory mapping and distribution matching, reducing inference steps from typical diffusion settings to just 8 steps while maintaining quality.

## Method Summary
UniPic 3.0 formulates multi-image composition as a sequence modeling problem where the target image latent is concatenated with reference image latents to form a unified sequence. The model is trained end-to-end using a large-scale curated dataset specifically designed for multi-image composition tasks. To accelerate inference, the authors integrate trajectory mapping and distribution matching techniques in the post-training stage, enabling high-fidelity sample generation in only 8 sampling steps (12.5x speedup). The framework supports an arbitrary number (1-6) of input images and achieves state-of-the-art performance on both single-image editing and multi-image composition benchmarks.

## Key Results
- Achieves state-of-the-art performance on single-image editing benchmarks
- Outperforms Nano-Banana and Seedream 4.0 on the newly proposed MultiCom-Bench for multi-image composition
- Enables 12.5x speedup through 8-step inference while maintaining high fidelity
- Demonstrates effectiveness across 1-6 input images with unified sequence modeling approach

## Why This Works (Mechanism)
The sequence modeling approach effectively treats multi-image composition as a conditional generation task where reference images provide context for generating the target image. By concatenating latents into a unified sequence, the model can learn cross-image relationships and spatial coherence through attention mechanisms. The focused data curation on Human-Object Interaction scenarios provides rich semantic relationships that improve compositional quality. The trajectory mapping and distribution matching techniques enable faster sampling by learning the compressed diffusion trajectory while preserving the target distribution.

## Foundational Learning
- **Sequence modeling for image composition**: Required to understand how concatenating latents enables multi-image reasoning through attention mechanisms
- **Diffusion sampling acceleration**: Needed to grasp trajectory mapping and distribution matching techniques that enable 8-step inference
- **Human-Object Interaction data curation**: Important for understanding why the dataset focuses on HOI scenarios and how this affects model capabilities
- **Latent space manipulation**: Critical for understanding how reference and target images are represented and combined in the unified framework
- **Attention-based cross-image reasoning**: Key to understanding how the model learns relationships between multiple input images
- **Conditional image generation**: Fundamental for understanding how reference images guide the generation of target compositions

## Architecture Onboarding
**Component map**: Image encoder → Latent concatenation → Sequence modeling transformer → Diffusion UNet → Image decoder

**Critical path**: Reference images → Encoding → Latent concatenation → Sequence attention → Conditional generation → Target image output

**Design tradeoffs**: Unified sequence modeling simplifies architecture but may face scalability limits with increasing input count; HOI-focused curation improves specific scenarios but may limit generalization; 8-step inference accelerates generation but requires careful trajectory learning

**Failure signatures**: Poor spatial coherence in complex compositions; artifacts when reference images have conflicting contexts; quality degradation with more than 4-5 input images

**First experiments**: 1) Test composition quality with varying numbers of input images (1-6) to identify scalability limits, 2) Evaluate inference quality-speed tradeoff by varying sampling steps from 4 to 16, 3) Assess generalization by testing on non-HOI composition scenarios not represented in training data

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the sequence modeling paradigm effectively scale beyond 6 input images without architectural modifications?
- Basis in paper: [explicit] The paper states the model "supports an arbitrary (1~6) number" of input images but does not explain the upper bound constraint.
- Why unresolved: The paper provides no analysis of sequence length scaling behavior or memory/compute constraints that determine the 6-image limit.
- What evidence would resolve it: Ablation experiments varying input image count from 1 to 10+ images, measuring quality degradation, memory usage, and attention pattern effectiveness.

### Open Question 2
- Question: Does the HOI-centric data curation bias the model against non-HOI composition scenarios (e.g., object-object composition, background-only fusion)?
- Basis in paper: [explicit] The authors "identify Human-Object Interaction (HOI) as the most sought-after category" and "construct a high-quality dataset... with a focus on challenging HOI scenarios."
- Why unresolved: MultiCom-Bench also targets HOI scenarios specifically, providing no evaluation of non-HOI composition quality.
- What evidence would resolve it: Benchmarking on a diverse multi-image composition dataset with balanced non-HOI categories, comparing against general-purpose baselines.

### Open Question 3
- Question: What biases or artifacts from the closed-source teacher models (Nano-Banana, Seedream 4.0) propagate into UniPic 3.0 through the synthesis pipeline?
- Basis in paper: [inferred] The data synthesis stage relies entirely on Nano-Banana (for 2-3 images) and Seedream 4.0 (for 4-6 images) to generate training targets, with no analysis of inherited limitations.
- Why unresolved: Using proprietary models as data generators creates a dependency chain where teacher model biases become training data biases.
- What evidence would resolve it: Systematic error analysis comparing UniPic 3.0 failures to known failure modes of Nano-Banana and Seedream 4.0; training with alternative synthetic data sources.

## Limitations
- MultiCom-Bench evaluation protocol lacks public availability and peer review
- Performance comparisons may use different evaluation conditions across methods
- Unified sequence modeling approach may face scalability challenges with many input images
- HOI-focused data curation may limit generalization to non-interaction composition types

## Confidence
- **High confidence**: Technical description of unified sequence modeling formulation and trajectory mapping integration
- **Medium confidence**: Claims about data quality and curation process
- **Medium confidence**: Single-image editing performance claims due to potential evaluation protocol differences

## Next Checks
1. Replicate the MultiCom-Bench evaluation protocol independently to verify claimed performance advantages over Nano-Banana and Seedream 4.0
2. Conduct ablation studies on sampling steps to quantify quality-speed tradeoff of 8-step inference approach
3. Test model generalization on held-out composition types not represented in training data