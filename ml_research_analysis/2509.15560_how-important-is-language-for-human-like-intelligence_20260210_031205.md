---
ver: rpa2
title: How important is language for human-like intelligence?
arxiv_id: '2509.15560'
source_url: https://arxiv.org/abs/2509.15560
tags:
- language
- https
- human
- arxiv
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that language is not just a communication tool\
  \ but a key driver of human-like intelligence, both in biological and artificial\
  \ systems. Large language models (LLMs) trained on natural language develop sophisticated\
  \ abilities\u2014including language understanding, pragmatic inference, and systematic\
  \ reasoning\u2014by learning a compressed, culturally evolved model of the world."
---

# How important is language for human-like intelligence?

## Quick Facts
- arXiv ID: 2509.15560
- Source URL: https://arxiv.org/abs/2509.15560
- Authors: Gary Lupyan; Hunter Gentry; Martin Zettersten
- Reference count: 0
- Primary result: Language is a key driver of human-like intelligence in both biological and artificial systems, providing essential scaffolding for abstract thought and complex problem-solving.

## Executive Summary
The paper argues that language is not merely a communication tool but a fundamental driver of human-like intelligence. Through synthesis of cognitive science evidence and analysis of large language models, the authors demonstrate that language profoundly shapes abstract reasoning, theory of mind, and systematic thinking. They show that individuals without typical language input struggle with cognitive tasks, while language-impaired individuals show deficits in tasks thought to be "nonverbal." These findings challenge the view that linguistic and nonlinguistic cognition are neurally or functionally distinct.

## Method Summary
This theoretical work synthesizes evidence from cognitive science and AI to argue for language's transformative role in intelligence. It references LLMs trained on large text corpora via next-token prediction and cites prior cognitive studies on homesigners, aphasia patients, and category learning experiments. The paper presents no original experiments but proposes testable claims about language's causal role in abstract reasoning that could be validated through controlled ablation studies comparing linguistic and nonlinguistic models on human-like cognitive benchmarks.

## Key Results
- Language profoundly influences human cognition, with individuals lacking typical language input showing deficits in abstract reasoning and theory of mind
- Large language models trained on natural language develop sophisticated abilities including language understanding, pragmatic inference, and systematic reasoning
- Experimental manipulations confirm that language causally enhances learning, categorization, and attention in both humans and AI

## Why This Works (Mechanism)
The paper argues that language provides a compressed, culturally evolved model of the world that serves as essential scaffolding for abstract thought. LLMs learn to predict language by internalizing the latent structure that produced it, effectively learning a compressed model of the world. This process forces models to develop representations of causality, object permanence, and abstract concepts that are embedded in linguistic patterns. The culturally evolved nature of language means it contains distilled human knowledge accumulated over generations, making it an efficient substrate for learning complex world models.

## Foundational Learning
- **Self-Supervised Learning (Next-Token Prediction)**: This core training paradigm forces models to learn world structure as a byproduct of minimizing prediction error on text. Quick check: Can you explain why predicting the next word in a diverse corpus might require learning about causality or object permanence?
- **Compression**: The paper frames intelligence as learning efficient, compressed representations of the world's structure. Quick check: How does compressing a large dataset force a model to learn its underlying patterns rather than just memorizing it?
- **Abstraction**: Language provides pre-built abstractions (e.g., "democracy", "gravity") that are easier to learn than to reinvent. Quick check: What is the difference between a concrete concept (e.g., "red") and an abstract one (e.g., "justice"), and why might the latter rely more on language?

## Architecture Onboarding
- **Component map**: Learner System (general-purpose neural network) -> Language Corpus (compressed, culturally-evolved world knowledge) -> Training Objective (self-supervised prediction) -> Emergence of general reasoning capabilities
- **Critical path**: Exposure to large-scale, diverse natural language corpus → Application of self-supervised prediction objective → Internalization of compressed world model and abstractions → Emergence of general reasoning and task capabilities
- **Design tradeoffs**: Tradeoff between data efficiency and generality. LLMs are less data-efficient than humans but achieve generality by learning from collective intelligence embedded in language. Systems trained on less language may be more efficient but lack human-like abstract reasoning.
- **Failure signatures**: (1) Fluency without understanding, (2) Inability to generalize systematic compositionality, (3) Poor performance on downstream tasks requiring abstract inference
- **First 3 experiments**:
  1. Train identical models on natural text vs. shuffled/synthetic text with matched statistics, test on abstract reasoning tasks. Prediction: Only natural text model succeeds.
  2. Fine-tune pre-trained model on "nonverbal" reasoning task, introduce secondary linguistic task during inference. Prediction: Primary task performance degrades.
  3. Train model and test ability to categorize objects along dimensions implied by context but not explicitly labeled. Prediction: Categorical distinctions will emerge and sharpen with training.

## Open Questions the Paper Calls Out
- Can artificial networks trained solely on nonlinguistic data achieve human-like reasoning, or is language a necessary training substrate? Prediction: Nonlinguistic AI will struggle on uniquely human intelligence tasks.
- Do neural dissociations between language and thought in adults preclude language from playing a causal role in the development of those thoughts? Challenge: Adult neural modularity may mask developmental dependencies.
- Does learning to predict language force a system to learn a "compressed model of the world," or merely surface-level statistical correlations? Challenge: Distinguishing understanding from exploiting statistical shortcuts.

## Limitations
- The paper demonstrates correlations between language exposure and cognitive abilities but faces difficulty establishing definitive causation
- While LLMs show remarkable language capabilities, it remains unclear whether they truly "understand" language in a human-like way or merely simulate understanding
- The comparison between human language acquisition and LLM training involves significant differences in learning mechanisms, embodiment, and developmental trajectories

## Confidence
- **High Confidence**: Language plays a significant role in human abstract reasoning and theory of mind development; LLMs trained on natural language develop sophisticated linguistic and reasoning capabilities; Language provides cognitive scaffolding that enhances learning and categorization
- **Medium Confidence**: Language is essential (rather than merely facilitative) for human-like intelligence; The mechanisms by which LLMs learn world structure through language are analogous to human language-mediated cognition; Abstract concepts rely fundamentally on linguistic mediation
- **Low Confidence**: LLMs possess genuine theory of mind capabilities comparable to humans; Language is the primary driver of intelligence rather than one important component among many; The specific neural mechanisms of language-cognition interaction in humans directly parallel LLM architecture

## Next Checks
1. **Controlled Ablation Study**: Train identical transformer architectures on matched corpora with varying linguistic properties (natural text vs. shuffled text vs. synthetic language-like text) and systematically measure performance differences on abstract reasoning tasks, theory of mind tests, and systematic composition tasks.
2. **Neurocognitive Mapping**: Conduct neuroimaging studies comparing human language-cognition interactions with computational analogs in LLMs, analyzing attention patterns during linguistic vs. non-linguistic reasoning.
3. **Cross-Modal Transfer Experiments**: Design experiments where LLMs must transfer knowledge from linguistic to non-linguistic domains and vice versa, measuring transfer efficiency and fidelity compared to human cross-modal learning patterns.