---
ver: rpa2
title: Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?
arxiv_id: '2509.25696'
source_url: https://arxiv.org/abs/2509.25696
tags:
- labels
- time-series
- pseudo
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to train time-series question answering
  (TSQA) models using pseudo labels generated by vision-language models (VLMs), addressing
  the challenge of limited labeled data in time-series domains. The approach converts
  time-series signals into plot images, generates pseudo labels using a VLM through
  natural language interaction, and trains a TSQA model to predict these labels.
---

# Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?

## Quick Facts
- arXiv ID: 2509.25696
- Source URL: https://arxiv.org/abs/2509.25696
- Authors: Takuya Fujimura; Kota Dohi; Natsuo Yamashita; Yohei Kawaguchi
- Reference count: 0
- Primary result: TSQA model trained on VLM pseudo-labels achieves 93.12% accuracy, outperforming the VLM baseline of 80.20%

## Executive Summary
This paper presents a method to train time-series question answering (TSQA) models using pseudo labels generated by vision-language models (VLMs), addressing the challenge of limited labeled data in time-series domains. The approach converts time-series signals into plot images, generates pseudo labels using a VLM through natural language interaction, and trains a TSQA model to predict these labels. Despite potential noise in VLM-generated labels, the method leverages the inherent robustness of deep neural networks to noisy labels, enabling effective training. Experiments on the SUSHI dataset demonstrate that TSQA models trained with pseudo labels achieve 93.12% accuracy, surpassing the VLM baseline performance of 80.20%.

## Method Summary
The method converts time-series signals (2,048 points each) into plot images (8×4 inches, 100 dpi) and queries GPT-4o to generate pseudo labels for multiple-choice questions. A TSQA model consisting of a frozen Mistral-7B-Instruct-v0.1 LLM and a trainable 3-layer Informer encoder is trained using cross-entropy loss on these pseudo labels. The approach leverages DNN robustness to noisy labels through gradient cancellation of random errors and early-learning dominance of correct patterns. The model successfully mitigates most random VLM misclassifications while inheriting systematic errors where the VLM consistently mislabels similar signal patterns.

## Key Results
- TSQA models trained with pseudo labels achieve 93.12% accuracy on SUSHI test set
- Outperforms GPT-4o baseline (80.20% accuracy) despite being trained on its noisy predictions
- Maintains high performance with significant label noise (47.20% accuracy with 40% correct labels)
- Requires fewer than 1000 training samples to achieve strong performance
- Successfully mitigates most VLM misclassifications while inheriting systematic errors in cases of consistent mislabeling

## Why This Works (Mechanism)

### Mechanism 1: Gradient Cancellation from Random Label Noise
DNNs can be trained effectively even with incorrect training labels, provided the errors are not systematically patterned. During mini-batch training, gradient contributions from randomly distributed incorrect labels cancel each other out, while gradients from correct labels accumulate and dominate weight updates. This mechanism fails when VLM errors become systematic across similar signal patterns.

### Mechanism 2: Early-Learning Dominance of Correct Patterns
Neural networks prioritize learning simpler, generalizable patterns from correct labels before eventually memorizing complex noise patterns. During early training phases, gradients from the majority of correct labels dominate learning; overfitting to noisy labels only occurs after correct-label gradients have been exhausted. Training must be stopped before this transition to prevent memorization.

### Mechanism 3: Systematic Error Inheritance Through Feature-Label Correlations
When VLM errors correlate consistently with specific signal features, the TSQA model inherits these systematic misclassifications. If the VLM consistently assigns incorrect labels to signals sharing certain visual or structural characteristics, the student model learns this spurious feature-label correlation as a valid pattern because it appears consistently across training samples.

## Foundational Learning

- **Concept: Pseudo-labeling and self-training**
  - Why needed: The entire methodology depends on using VLM predictions as proxies for ground-truth labels; understanding this paradigm is essential for grasping why imperfect labels can still train effective models.
  - Quick check: Can you explain why pseudo-labels from an 80% accurate VLM can train a 93% accurate student model, rather than capping student performance at the VLM's accuracy?

- **Concept: Label noise robustness in DNNs**
  - Why needed: The paper's core hypothesis relies on DNNs being inherently robust to noisy labels; understanding the conditions under which this holds is critical for applying the method.
  - Quick check: What is the difference between random label noise and systematic label noise, and which type does gradient cancellation effectively mitigate?

- **Concept: Time-series representation via visual encoders vs. specialized encoders**
  - Why needed: The paper converts time-series to plot images for VLMs but uses a specialized encoder (Informer) for the student model; understanding this representational tradeoff clarifies why each architecture is chosen.
  - Quick check: Why might a specialized time-series encoder outperform a VLM that sees the same signal as a plot image?

## Architecture Onboarding

- **Component map:**
  - Time-series signals (2,048 points) → Plot images (8×4 inches, 100 dpi) → GPT-4o → Pseudo-labels
  - Raw time-series signals → 3-layer Informer encoder → 4,096-dim embeddings → Frozen Mistral-7B-Instruct-v0.1 → Answer predictions

- **Critical path:**
  1. Convert raw time-series signals to plot images
  2. Query GPT-4o with images + multiple-choice questions to generate pseudo-labels
  3. Train Informer encoder (with frozen Mistral backbone) to predict pseudo-labels from raw signals
  4. Early-stop based on validation accuracy to prevent overfitting to noise

- **Design tradeoffs:**
  - Frozen LLM vs. full fine-tuning: Freezing reduces compute and preserves general language capabilities but may limit domain adaptation
  - Image-based VLM vs. raw-signal encoder: VLM provides zero-shot capability via visual reasoning; encoder is more parameter-efficient and directly processes temporal dynamics
  - Synthetic dataset (SUSHI) vs. real-world data: Synthetic data enables controlled analysis but may not reflect complexity of noisy real-world time-series

- **Failure signatures:**
  - Systematic error propagation: If VLM consistently mislabels certain patterns, student inherits this error—visible in confusion matrix clustering
  - Insufficient data: With <100 samples or very high noise ratios (>60% incorrect), correct-label gradients cannot dominate
  - Over-training: Extended epochs beyond early-learning phase cause memorization of noisy labels—monitor training vs. test accuracy divergence

- **First 3 experiments:**
  1. Establish VLM baseline quality: Query GPT-4o on training set to measure pseudo-label accuracy (target: >75% for viable training)
  2. Train TSQA with pseudo-labels and compare to VLM: If student does not exceed VLM accuracy, check for systematic error patterns via confusion matrix analysis
  3. Stress-test noise tolerance: Progressively corrupt pseudo-labels to identify the minimum correct-label ratio where training fails

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important research directions regarding systematic error detection, generative task extension, and real-world data generalization.

## Limitations
- Reliance on synthetic SUSHI dataset that may not capture real-world time-series complexity and noise characteristics
- Inheritance of systematic VLM errors when specific signal patterns are consistently misclassified
- Limited evaluation to multiple-choice tasks, leaving generative time-series QA performance untested

## Confidence
- **High Confidence:** Gradient cancellation mechanism for random label noise and empirical observation that TSQA-PL outperforms GPT-4o (93.12% vs 80.20%)
- **Medium Confidence:** Early-learning dominance hypothesis explaining correct label dominance before noise memorization
- **Medium Confidence:** Systematic error inheritance mechanism demonstrated through UMAP visualization but not quantitatively measured

## Next Checks
1. Apply TSQA-PL method to a naturally occurring time-series dataset with ground-truth labels to verify synthetic data generalization
2. Implement and test a confidence threshold or ensemble method to identify and filter systematic VLM errors before they propagate to the student model
3. Train TSQA-PL model on SUSHI data, then fine-tune on a small labeled dataset from a different domain to assess cross-domain generalization capabilities