---
ver: rpa2
title: 'Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs'
arxiv_id: '2506.00846'
source_url: https://arxiv.org/abs/2506.00846
tags:
- attention
- distribution
- limit
- neural
- infinite-width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper rigorously analyzes the infinite-width limit of a single
  attention layer in neural networks. The key challenge addressed is that previous
  asymptotic theories based on Gaussian approximations (like neural network Gaussian
  processes or Tensor Programs) fail to capture attention layers except under special
  regimes like infinitely many heads or tailored scaling schemes.
---

# Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs

## Quick Facts
- **arXiv ID:** 2506.00846
- **Source URL:** https://arxiv.org/abs/2506.00846
- **Reference count:** 40
- **One-line primary result:** A single attention layer converges to a hierarchical Gaussian distribution (Gaussian conditional on dot-product scores) under standard 1/âˆšð‘› scaling, departing fundamentally from previous Gaussian approximations.

## Executive Summary
This paper rigorously characterizes the infinite-width limit of a single attention layer, solving a fundamental challenge in neural network theory. Previous asymptotic analyses failed for attention layers except under special regimes (infinite heads or 1/ð‘› scaling), but this work derives the exact limiting distribution using an extended Tensor Programs framework. The key insight is that attention output is Gaussian conditional on random similarity scores, making the overall distribution hierarchical and non-Gaussian. Numerical experiments confirm the theory accurately describes finite-head attention at practical widths, laying groundwork for a unified theory of deep Transformer architectures.

## Method Summary
The authors extend the Tensor Programs (Netsor) framework to handle the product of two independent matrix multiplication variables (dot-products) in attention layers. They define a new class of scalar variables for dot-product scores and prove these converge to multivariate Gaussian vectors independent of the vector variables. The theoretical limit is constructed by sampling two independent Gaussian objectsâ€”value vectors and score variablesâ€”and combining them via softmax. The finite-width experiments use standard multi-head attention with i.i.d. Gaussian initialization and 1/âˆšð‘› scaling, comparing empirical distributions to the theoretical limit via kernel density estimation and KL divergence.

## Key Results
- A single attention layer with finite heads converges to a hierarchical Gaussian distribution under standard 1/âˆšð‘› scaling
- The output is Gaussian conditional on random similarity scores, making the overall distribution fundamentally non-Gaussian
- KL divergence between finite-width empirical distributions and the theoretical limit decreases consistently as width increases
- The 1/âˆšð‘› scaling regime preserves the attention signal, while 1/ð‘› scaling trivializes the problem (scores â†’ 0)

## Why This Works (Mechanism)

### Mechanism 1
The output of a single attention layer converges to a hierarchical Gaussian distribution, which is non-Gaussian overall but Gaussian when conditioned on the random similarity scores. The attention output depends on two sources of randomness: the vector variables (e.g., values $V$) and the scalar dot-product scores (e.g., $QK^\top$). In the infinite-width limit, the dot-product scores $\mathring{p}$ converge to a Gaussian vector that is statistically independent of the vector variables $Z$. Because the output variance depends on these random scores (via Softmax), the resulting distribution is a mixture of Gaussians rather than a single Gaussian.

### Mechanism 2
The standard Tensor Programs framework can be extended to rigorously handle the product of two independent MatMul variables (dot-products) without resorting to infinite-head approximations. The authors extend the "Netsor" program by defining a new class of scalar variables $p_i = (g_{i,1})^\top g_{i,2} / \sqrt{n}$. They prove these scalars converge to a multivariate Gaussian vector independent of the basis vectors. This allows the Master Theorem to treat the Softmax operation as a non-linearity acting on this external random variable.

### Mechanism 3
The theoretical infinite-width distribution accurately approximates finite-width attention layers at practical widths (e.g., $n=256$) with finite heads. Although the theory strictly applies as $n \to \infty$, the variance of the dot-product scores stabilizes relatively quickly. Empirical results show that the KL divergence between the finite-width empirical distribution and the theoretical limit decreases consistently as width increases.

## Foundational Learning

### Concept: Tensor Programs (Netsor)
**Why needed here:** This is the mathematical engine used to prove the limit. You need to understand how it decomposes a neural network into "MatMul" and "Nonlin" operations to see why the dot-product is a special case.
**Quick check question:** Can you distinguish between a variable generated by a matrix multiplication (MatMul) and one generated by a coordinate-wise nonlinearity (Nonlin) in the Netsor framework?

### Concept: Scaling Regimes ($1/\sqrt{n}$ vs $1/n$)
**Why needed here:** The paper's main breakthrough is solving the $1/\sqrt{n}$ regime. Understanding why $1/n$ scaling trivializes the problem (scores $\to 0$) is essential to valuing the contribution.
**Quick check question:** If you scale the dot-product score by $1/n$ instead of $1/\sqrt{n}$, what value does the Softmax input converge to as width increases?

### Concept: Conditional Distribution
**Why needed here:** The core result is that $Z$ is Gaussian *conditional* on $\mathring{p}$. If you treat them as jointly Gaussian without this conditioning, you will misinterpret the output distribution.
**Quick check question:** If the variance of an output $Z$ depends on a random variable $R$, is the marginal distribution of $Z$ necessarily Gaussian?

## Architecture Onboarding

### Component map:
Input vectors $x \in \mathbb{R}^n$ -> MatMul (generates $Q$, $K$, $V$) -> Dot-Product Layer (computes $p = QK^\top / \sqrt{n}$) -> Softmax (attention weights) -> Output (weighted sum of $V$)

### Critical path:
The analysis tracks the joint convergence of the **Value vectors** ($Z_{\tilde{v}}$) and the **Dot-product scores** ($\mathring{p}$). The interaction happens at the final aggregation step $y = \text{Softmax}(p) \cdot V$.

### Design tradeoffs:
- **Theory vs. Practice:** The "infinite-head" regime simplifies math to a Gaussian but is unrealistic (real models use finite heads). The "finite-head" regime preserves non-Gaussianity but requires complex hierarchical analysis.
- **Scaling:** $1/n$ scaling ensures Gaussianity but kills the attention signal (theoretical convenience vs. functional preservation).

### Failure signatures:
- **Gaussian Assumption:** If you assume the attention output is Gaussian for finite heads, you will underestimate the variance and miscalculate the probability of extreme activations (tail risk).
- **Low-Rank Bottleneck:** If head dimension is small relative to width, the standard $1/\sqrt{n}$ scaling might not capture the variance correctly without the adjustments mentioned in Appendix B.2.

### First 3 experiments:
1. **Replicate KL Decay:** Implement a single attention layer with width $n \in \{16, 64, 256, 1024\}$. Plot the KL divergence between the empirical output distribution and the theoretical limit $Z_y$ to verify Figure 1(b).
2. **Scaling Ablation:** Run the same layer with $1/n$ scaling on the dot-product. Observe that the output collapses to a standard Gaussian, confirming the necessity of the $1/\sqrt{n}$ assumption for the non-Gaussian result.
3. **Head-Count Sensitivity:** Fix width $n=256$ and vary the number of heads $H$. Plot the distribution shape to confirm that as $H$ increases, the distribution approaches the "infinite-head" Gaussian limit, validating the link to prior work.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the non-Gaussian limiting distribution persist and stabilize when attention layers are stacked to form deep Transformer architectures?
- Basis in paper: [explicit] The authors state their findings "lay the groundwork for developing a unified theory of deep Transformer architectures" and predict non-Gaussianity will influence signal propagation across layers.
- Why unresolved: The current analysis is rigorously restricted to a single attention layer.
- What evidence would resolve it: A theoretical proof extending to multi-layer networks, showing hierarchical distribution evolution.

### Open Question 2
- Question: Can the Tensor Programs framework be modified to support growing sequence lengths alongside increasing width?
- Basis in paper: [explicit] The conclusion lists the assumption of constant sequence length as a limitation and identifies "extending the framework to handle growing sequence lengths" as future work.
- Why unresolved: The current setup treats spatial dimensions as fixed constants independent of width $n$.
- What evidence would resolve it: A derivation allowing sequence length to scale with width without losing convergence guarantees.

### Open Question 3
- Question: How does the derived non-Gaussianity specifically affect the optimization landscape and training convergence of Transformers?
- Basis in paper: [explicit] The authors argue that non-Gaussianity "may lead to more irregular curvature in the optimization landscape" and necessitates a framework distinct from Gaussian-based approaches.
- Why unresolved: The paper characterizes the initialization distribution but does not derive the training dynamics.
- What evidence would resolve it: Analysis of gradient descent dynamics accounting for the hierarchical structure.

## Limitations
- Analysis is strictly limited to a single attention layer with i.i.d. Gaussian initialization and constant sequence length
- Does not address multi-layer Transformers, learned parameters (relative position encodings), or real-world data distributions
- Empirical validation relies on synthetic data and a specific clipping activation function

## Confidence

### Confidence Labels
- **High Confidence:** The characterization of the hierarchical Gaussian limit distribution (Theorem 3.1) and the convergence mechanism via Tensor Programs (Mechanism 1 and 2)
- **Medium Confidence:** The practical relevance of the limit distribution for finite-width networks (Mechanism 3)
- **Low Confidence:** The claim that this work lays the "groundwork" for a unified theory of deep Transformers

## Next Checks
1. **Activation Robustness:** Reproduce the KL convergence experiment with alternative activation functions (ReLU, GeLU, Softplus) to test if the theoretical limit requires the specific clipping function used in the paper.
2. **Attention Variants:** Apply the analysis framework to attention mechanisms with learnable key/value projections or relative positional encodings to identify what breaks in the current theory.
3. **Depth Extension:** Extend the Tensor Programs analysis to a two-layer attention network to empirically test whether the single-layer limit provides a good approximation for the first layer in a deeper model.