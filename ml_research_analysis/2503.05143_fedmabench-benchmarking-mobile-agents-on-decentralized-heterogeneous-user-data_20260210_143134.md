---
ver: rpa2
title: 'FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User
  Data'
arxiv_id: '2503.05143'
source_url: https://arxiv.org/abs/2503.05143
tags:
- mobile
- data
- agents
- training
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FedMABench is the first comprehensive benchmark for federated
  learning of mobile agents on decentralized user data, addressing the lack of standardized
  evaluation frameworks in this area. It introduces six datasets with 30+ subsets
  spanning over 800 apps across five categories, emphasizing three types of heterogeneity:
  app category distribution, specific app preferences, and two-level sample counts
  (episodes and steps).'
---

# FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data

## Quick Facts
- **arXiv ID:** 2503.05143
- **Source URL:** https://arxiv.org/abs/2503.05143
- **Reference count:** 40
- **Primary result:** First comprehensive benchmark for federated learning of mobile agents on decentralized user data

## Executive Summary
FedMABench addresses the critical gap in standardized evaluation frameworks for federated learning of mobile agents using decentralized user data. The benchmark introduces six datasets spanning over 800 apps across five categories, with 30+ subsets designed to capture three types of heterogeneity: app category distribution, specific app preferences, and two-level sample counts. By integrating eight federated algorithms with 10+ base models, FedMABench provides a research-friendly framework that bridges theoretical FL research and practical mobile agent applications. The benchmark demonstrates that federated algorithms consistently outperform local training while revealing nuanced insights about heterogeneity impacts and inter-app correlations.

## Method Summary
FedMABench introduces a comprehensive benchmarking framework specifically designed for evaluating federated learning of mobile agents on decentralized user data. The methodology centers on six carefully curated datasets containing over 800 apps across five categories, organized into 30+ subsets to capture different heterogeneity patterns. The framework implements eight federated learning algorithms and supports integration with 10+ base models for mobile agents. The evaluation protocol systematically examines three types of heterogeneity: distribution across app categories, individual app preferences, and sample count variations at both episode and step levels. This structured approach enables researchers to assess algorithm performance under realistic mobile agent deployment scenarios while maintaining user privacy through decentralized training.

## Key Results
- Federated algorithms consistently outperform local training across all tested scenarios and heterogeneity conditions
- Heterogeneity negatively impacts performance, with specific app differences proving more fundamental than category differences
- Apps from distinct categories can exhibit correlations during training, suggesting potential for cross-category knowledge transfer

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic simulation of mobile agent deployment environments through multi-level heterogeneity modeling. By capturing both categorical and specific app preferences alongside episode/step-level sample count variations, FedMABench creates a comprehensive evaluation landscape that reflects real-world user behavior patterns. The integration of multiple federated algorithms with diverse base models enables robust performance comparisons under various privacy-preserving training conditions, while the decentralized data structure maintains the privacy guarantees essential for mobile agent applications.

## Foundational Learning
1. **Federated Learning Fundamentals** - Understanding how distributed training across heterogeneous data sources differs from centralized approaches. Why needed: Mobile agents require privacy-preserving training on user-specific data. Quick check: Can you explain the difference between FedAvg and local training convergence?
2. **Mobile Agent Architecture** - Knowledge of reinforcement learning agents adapted for mobile applications. Why needed: The benchmark evaluates specific agent performance characteristics. Quick check: What distinguishes mobile agents from traditional RL agents in terms of input/output constraints?
3. **Heterogeneity Types** - Recognition of categorical, preferential, and sample-count heterogeneity in user data. Why needed: These variations directly impact algorithm performance and benchmark validity. Quick check: Can you identify which heterogeneity type would most affect a navigation app's training?

## Architecture Onboarding
**Component Map:** Data Generator -> Heterogeneity Injector -> FL Trainer -> Performance Evaluator -> Results Aggregator
**Critical Path:** Data preparation and heterogeneity injection must complete before federated training begins; training results flow directly to evaluation modules
**Design Tradeoffs:** The benchmark prioritizes comprehensive heterogeneity modeling over computational efficiency, accepting longer training times for more realistic evaluation scenarios
**Failure Signatures:** Algorithm performance degradation under high heterogeneity, convergence issues with specific app subsets, and evaluation inconsistencies across different base models
**First Experiments:** 1) Run FedAvg baseline on homogeneous data subset, 2) Test algorithm performance across all five app categories, 3) Evaluate impact of two-level sample count heterogeneity on convergence rates

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited diversity of base models tested (10+) compared to the broader landscape of mobile agent architectures
- Generalizability to non-Android platforms and other mobile domains remains unverified
- Two-level sample count structure may introduce variability not fully characterized in current evaluations

## Confidence
- Federated algorithms outperform local training: **High**
- Heterogeneity negatively impacts performance: **Medium to High**
- Specific app differences more fundamental than category differences: **Medium**
- Apps from distinct categories can exhibit correlations: **Medium** (requires further validation)

## Next Checks
1. Test FedMABench algorithms on iOS applications and compare performance consistency with Android results to validate cross-platform applicability
2. Evaluate additional base models beyond the current 10+, particularly focusing on transformer-based architectures and their performance under federated settings
3. Conduct extended training runs to assess the stability of federated vs. local training performance claims over time, especially under varying heterogeneity conditions