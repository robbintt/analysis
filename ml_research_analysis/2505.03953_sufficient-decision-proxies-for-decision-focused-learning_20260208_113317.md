---
ver: rpa2
title: Sufficient Decision Proxies for Decision-Focused Learning
arxiv_id: '2505.03953'
source_url: https://arxiv.org/abs/2505.03953
tags:
- decision
- problem
- proxy
- problems
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates when single-scenario decision proxies\
  \ in decision-focused learning (DFL) are suboptimal, showing that dominated optimal\
  \ decisions can occur in continuous and discrete problems. The authors present two\
  \ alternative proxies\u2014scenario-based and quadratic\u2014that guarantee optimal\
  \ decision-making under sufficient conditions."
---

# Sufficient Decision Proxies for Decision-Focused Learning

## Quick Facts
- arXiv ID: 2505.03953
- Source URL: https://arxiv.org/abs/2505.03953
- Reference count: 18
- Key outcome: Decision-focused learning with deterministic proxies fails when optimal decisions are scenario-wise dominated; quadratic and scenario-based proxies provide guaranteed optimality under sufficient conditions.

## Executive Summary
This paper identifies a fundamental limitation in decision-focused learning (DFL): deterministic proxies fail when the true optimal decision is dominated in every possible realization of uncertain parameters. The authors present two alternative proxies—scenario-based (predicting multiple scenarios) and quadratic (predicting decisions directly)—that guarantee optimal decision-making under sufficient conditions. Experiments on portfolio optimization, weighted-set multi-cover, and probabilistic traveling salesperson problems demonstrate that both alternatives outperform the standard deterministic proxy, with the quadratic proxy often performing best. The work provides theoretical justification for proxy selection and practical alternatives when single-scenario approximations fail.

## Method Summary
The authors investigate three decision proxies for DFL: deterministic (single scenario prediction), scenario-based (predict n scenarios and optimize SAA objective), and quadratic (predict decision vector and solve projection onto feasible set). The deterministic proxy fails when optimal decisions are scenario-wise dominated. The scenario-based proxy approximates the expected value function via SAA, while the quadratic proxy guarantees surjectivity by design. Training uses neural networks with cvxpylayers for differentiable convex optimization in portfolio problems, and score function gradient estimation for discrete problems (WSMC, PTSP). Experiments use standard supervised learning splits with mean absolute regret as the metric.

## Key Results
- Deterministic proxies fail to learn in portfolio optimization when optimal decisions are dominated, achieving zero gradients
- Scenario-based proxies improve with more scenarios (n=2,8,16), with best performance at higher n
- Quadratic proxy consistently matches or outperforms other methods across all three problem domains
- Runtime efficiency: quadratic proxy faster than scenario-based for large problems due to avoiding multiple solver calls
- Theoretical guarantee: quadratic proxy is surjective by design, eliminating the dominance limitation

## Why This Works (Mechanism)

### Mechanism 1: Decision Dominance Limits Expressiveness
Standard deterministic proxies fail when the true optimal decision is strictly dominated in every possible realization of uncertain parameters, yet optimal in expectation. In a deterministic proxy $x_D(\hat{c}) = \arg\min f(\hat{c}, x)$, the solver seeks a decision $x$ that minimizes cost for a single predicted scenario $\hat{c}$. If the true stochastic optimal decision lies in the interior of the feasible set while the deterministic objective is strictly monotonic, the proxy will only select boundary solutions. The mapping $x_D(\cdot)$ is non-surjective, making the true optimum unreachable.

### Mechanism 2: Sample Average Approximation (SAA) Restores Optimality
Predicting multiple scenarios ($n > 1$) instead of a point estimate allows the proxy to approximate the expected value function sufficiently to recover non-dominated decisions. By defining a proxy $x_{S_n}(\hat{c}_1, \dots, \hat{c}_n) = \arg\min \sum f(\hat{c}_i, x)$, the learner optimizes over a landscape closer to the true stochastic objective $E[f(c, x)]$. Even with small $n$ (e.g., 2), the optimizer is no longer forced to select a solution optimal for a single scenario but can find interior solutions that are robust across the predicted scenarios.

### Mechanism 3: Surjective Quadratic Projection
Replacing the problem-specific objective $f(c, x)$ with a quadratic projection task guarantees the proxy is surjective, meaning any feasible decision $x \in X$ can theoretically be reached. The "Quadratic Proxy" $x_Q(\hat{\xi}) = \arg\min ||\hat{\xi} - x||^2_2$ ignores the uncertain parameters $c$ during the optimization step. Instead, the neural network predicts a target $\hat{\xi}$ in the decision space, and the proxy projects this target onto the feasible set $X$. Since the projection is surjective (any $x$ can be reached by predicting $\hat{\xi}=x$), the "dominance" limitation of deterministic proxies is removed.

## Foundational Learning

- **Concept: Decision-Focused Learning (DFL) vs. Prediction-Focused Learning (PFL)**
  - Why needed: The paper critiques the standard "predict-and-optimize" approach where errors in predicting $c$ propagate to $x$. Understanding DFL is necessary to grasp why the authors optimize the policy $\pi = x \circ \phi_\theta$ rather than the prediction $\phi_\theta$.
  - Quick check: Does the loss function minimize Mean Squared Error (PFL) or Decision Regret (DFL)?

- **Concept: Value of the Stochastic Solution (VSS)**
  - Why needed: The paper extends VSS to the contextual setting ($V_{PFL} - V_{DFL}$). This quantifies exactly how much value is lost by using a deterministic proxy when the true problem requires stochastic optimization.
  - Quick check: If $V^* = V_{DFL}$, is the deterministic proxy sufficient? (Answer: Yes).

- **Concept: Surjective Functions**
  - Why needed: The core theoretical contribution hinges on whether the proxy mapping $x: \Xi \to X$ is surjective. If not, the image of the proxy is a strict subset of feasible decisions, potentially excluding the true optimum.
  - Quick check: For a function $f: A \to B$, does every element in $B$ have a pre-image in $A$?

## Architecture Onboarding

- **Component map:** Context Encoder -> Predictor $\phi_\theta$ -> Decision Proxy (Solver) -> Loss (Decision Regret)
- **Critical path:** The gradient flow from the Solver output back to the Predictor. For Portfolio (Convex): Use Differentiable Convex Layers (implicit differentiation). For WSMC/PTSP (Discrete): Use Score Function Gradient Estimation (REINFORCE) because the solver is non-differentiable.
- **Design tradeoffs:**
  - Deterministic Proxy: Simplest, but fails on "dominated" problems; suffers from zero-gradients
  - Scenario-based Proxy: Better gradient signal; complexity scales linearly with $n$
  - Quadratic Proxy: Fastest inference (projection often faster than solving original objective); bypasses parameter prediction entirely
- **Failure signatures:**
  - Zero-gradients: The deterministic proxy returns boundary solutions which often have zero gradients
  - High Variance: Score function estimators in the Scenario proxy may show high variance learning curves
- **First 3 experiments:**
  1. Sanity Check (Linear): Run a simple linear objective problem. Verify that all proxies converge (Jensen's equality holds).
  2. Continuous Stress Test: Implement the Kelly Criterion Portfolio example. Verify that deterministic proxy flatlines while quadratic and scenario proxies learn interior optimal solution.
  3. Combinatorial Scaling: Test the Weighted-Set Multi-Cover (WSMC) problem. Compare runtime. Verify that quadratic proxy runtime remains stable while scenario proxy runtime increases with $n$.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does the structure of the true objective function remain relevant in a DFL setting when using the quadratic proxy? Given the strong performance of the quadratic proxy (which predicts decisions directly in the decision space), "it opens the question of what the relevance is of the true objective in a DFL-setting."
- **Open Question 2:** Does the dimensionality of the prediction space influence the complexity of learning a high-quality policy? Specifically regarding differences between $\dim(X)$ and $\dim(C)$, "What is more of an open question is whether the prediction space influences the complexity of learning a high-quality policy."
- **Open Question 3:** How do the proposed sufficient decision proxies perform on problems with dependent uncertain variables? The Discussion section lists "problems with dependent uncertain variables" as a specific avenue for future work.

## Limitations

- The theoretical claims rely heavily on the assumption of "scenario-wise dominance" (strict monotonicity of $f$ over compact $X$), which may not hold in many practical problems
- The quadratic proxy's effectiveness depends on the projection step remaining computationally tractable for complex constraint sets
- The score function gradient estimator's performance is sensitive to hyperparameter choices (sigma initialization, sample count) not fully specified in the paper

## Confidence

- **High Confidence:** The failure mode of deterministic proxies on dominated decisions (Section 3) is theoretically sound and empirically demonstrated in the Portfolio problem
- **Medium Confidence:** The SAA approximation quality improves with scenario count (Section 4.1), though the specific learning curves show high variance in discrete problems
- **Medium Confidence:** The quadratic proxy's surjectivity guarantees (Theorem 7) are mathematically valid, but its practical performance depends heavily on problem structure

## Next Checks

1. **Surjectivity Test:** Construct a synthetic convex problem where the true optimal decision is strictly dominated, verify that deterministic proxy fails while quadratic proxy succeeds
2. **Gradient Variance Analysis:** For WSMC/PTSP, measure the variance of score function gradients across training runs and compare to deterministic proxy's zero gradients
3. **Computational Complexity Scaling:** Measure projection time for quadratic proxy versus solver time for deterministic proxy across increasing problem sizes to validate the claimed efficiency advantage