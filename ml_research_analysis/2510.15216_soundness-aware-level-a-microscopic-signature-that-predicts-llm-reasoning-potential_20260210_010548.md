---
ver: rpa2
title: 'Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning
  Potential'
arxiv_id: '2510.15216'
source_url: https://arxiv.org/abs/2510.15216
tags:
- reasoning
- rules
- features
- arxiv
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method to predict a pre-trained language\
  \ model\u2019s reasoning potential using a microscopic metric based on its internal\
  \ logic rules. The authors formalize reasoning as chains of Horn clauses (if-then\
  \ rules) extracted from features learned by cross-layer sparse autoencoders."
---

# Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential

## Quick Facts
- arXiv ID: 2510.15216
- Source URL: https://arxiv.org/abs/2510.15216
- Reference count: 40
- This paper proposes SAL (Soundness-Aware Level), a microscopic metric that predicts post-RLVR reasoning performance from pre-trained LLMs by analyzing internal logic rule distributions.

## Executive Summary
This paper introduces SAL (Soundness-Aware Level), a zero-label microscopic metric that predicts a pre-trained language model's reasoning potential by analyzing its internal logic rules. The method uses cross-layer sparse autoencoders to extract interpretable features, formalizes reasoning as Horn clauses between these features, categorizes rules by semantic soundness (strict, plausible, or noisy), and measures how well the model separates these distributions. Experiments across diverse model families and scales show SAL strongly correlates with post-RLVR reasoning performance (R²=0.87) and varies by model family, offering a practical tool for selecting and designing stronger base models.

## Method Summary
The method extracts features from LLM latent space using cross-layer sparse autoencoders (SAEs), then estimates transition probabilities between features to form Horn clauses (P→Q rules). An LLM judge categorizes each rule by semantic soundness level (Strict/Plausible/Noisy), and SAL computes the Jensen-Shannon Divergence between probability distributions across these soundness categories. SAL is computed from internal statistics without labels and shows strong correlation with post-RLVR reasoning performance via an empirical law ε = exp(-α·s^β).

## Key Results
- SAL strongly correlates with post-RLVR reasoning performance (R²=0.87) across diverse model families and scales
- SAL increases monotonically with model scale but shows family-specific variations at comparable sizes
- High-potential models (SAL ~0.20) show distinct modes in rule distributions, while low-potential models (SAL ~0.06) show collapsed distributions
- The empirical law ε = exp(-α·s^β) predicts error rates from SAL with high accuracy (R²=0.985 for interpolation)

## Why This Works (Mechanism)

### Mechanism 1
Sparse autoencoders decompose LLM residual stream activations into interpretable semantic features. A cross-layer SAE is trained to reconstruct each layer's hidden state using a small subset of features from current and preceding layers, with sparsity penalty forcing discovery of semantically coherent features. Each feature is labeled by prompting a high-capability LLM to summarize maximally activating text spans. Core assumption: LLM representations can be decomposed into sparse, approximately monosemantic features corresponding to human-interpretable concepts.

### Mechanism 2
The probability distribution over learned logic rules reveals internal knowledge organization quality. For each Horn clause P→Q, the method estimates p(Q|P) by counting co-occurrences across thousands of inputs, then categorizes rules by semantic soundness using an LLM judge. High-potential models assign high probabilities to strict rules and low probabilities to noisy rules, creating distinct distribution modes. Core assumption: Feature co-occurrence frequency reflects learned rule strength rather than spurious corpus correlations.

### Mechanism 3
The Jensen-Shannon Divergence between probability distributions of rules across soundness levels predicts post-RLVR reasoning performance. For each soundness category, the method constructs a histogram over rule probabilities, and SAL measures how well the model separates these distributions. High-potential models show SAL ~0.20 with distinct modes; low-potential models show SAL ~0.06 with collapsed distributions. Core assumption: The ability to distinguish sound from unsound knowledge at the feature-rule level is causally related to reasoning potential.

## Foundational Learning

- **Horn Clauses (Logic Programming)**: Rules of the form α_c1 ∧ α_c2 → α_c3 used to formalize LLM reasoning steps. Quick check: Given features "square root symbol" and "number 4", what would a Horn clause representation of "√4 = 2" look like?

- **Sparse Autoencoders for Mechanistic Interpretability**: Tools for decomposing uninterpretable hidden states into human-readable features. Quick check: Why might a high sparsity penalty cause features to become polysemantic or dead?

- **Jensen-Shannon Divergence**: Symmetric, bounded measure for comparing probability distributions. Quick check: If two distributions ρ_Strict and ρ_Noise are identical, what would JSD return? What does this mean for SAL?

## Architecture Onboarding

- **Component map**: Data preparation (128K math questions) -> SAE training (C=32768 features, L=8 layers) -> Feature interpretation (DeepSeek-R1 labels) -> Rule extraction (co-occurrence counting) -> Soundness labeling (LLM judge) -> SAL computation (JSD)

- **Critical path**: SAE quality → interpretable features → meaningful rules → reliable soundness labels → stable SAL. If SAE reconstruction MSE > 0.80 or dead rate > 10%, downstream steps degrade.

- **Design tradeoffs**: Higher feature count → more granular features but higher compute and potential dead features (paper uses 2^15 = 32768). Layer selection captures cross-layer propagation but may miss layer-specific patterns. 1-2 premise limitation due to O(C^3) combinations. DeepSeek-R1 provides scalable annotation but only 56.6% human agreement.

- **Failure signatures**: High SAE dead rate (>10%) → insufficient feature coverage. Collapsed rule distributions (all soundness levels similar) → SAL ~0.05, model likely low-potential. "Strict" rules with low probabilities (<0.5) → SAE features may capture only syntactic patterns. JSD dominated by Strict vs. Plausible → model may lack clear noise discrimination.

- **First 3 experiments**: 1) Train SAE on small model, verify normalized MSE < 0.80 and dead rate < 5%, visualize top features. 2) Extract rules, inspect 20 highest p(Q|P) rules to confirm semantic patterns. 3) Compute SAL for 3-4 models of varying capability, correlate with downstream accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
Does improving a model's soundness-awareness during pre-training causally improve its post-RLVR reasoning performance? The paper establishes correlation (R²=0.87) but cannot distinguish whether high SAL causes better reasoning or is merely a byproduct of other factors. Intervention experiments modifying pre-training to increase SAL and measuring resulting post-RLVR performance changes would resolve this.

### Open Question 2
Does SAL reliably predict reasoning potential in domains beyond mathematical problem-solving? All experiments use mathematical corpora and benchmarks, but the feature extraction and rule categorization pipeline may be domain-specific. Experiments applying SAL to pre-trained models evaluated on non-mathematical reasoning benchmarks after RLVR would resolve this.

### Open Question 3
What determines the significant variation in SAL across model families at comparable scales? The paper notes model family plays an equally critical role but does not disentangle whether differences arise from architecture, pre-training data composition, optimization procedures, or other factors. Ablation studies varying these factors while controlling for scale would resolve this.

### Open Question 4
How does SAL behave at model scales beyond 14B parameters, and does it continue to saturate? Section 3.4 observes SAL approaches saturation beyond 14B, suggesting additional parameters may refine existing rule clusters rather than create new separations. Computing SAL for larger models (70B, 500B scale) and plotting the full scaling curve would resolve this.

## Limitations
- Reliance on DeepSeek-R1 for feature labeling and soundness annotation introduces uncertainty given only 56.6% human agreement
- Cross-layer SAE architecture's applicability to non-uniform model depths is unclear
- 1-2 premise limitation on Horn clauses may miss deeper reasoning patterns
- Empirical law correlating SAL with post-RLVR performance is based on specific model family and math reasoning domain

## Confidence

- **High confidence**: SAL correlates with post-RLVR reasoning performance across tested models; SAL increases with model scale; SAL varies across families at fixed scale
- **Medium confidence**: The mechanism by which SAL predicts reasoning potential (via internal distribution separation) is sound, but the causal relationship remains to be proven
- **Low confidence**: The generalizability of SAL to non-mathematical reasoning domains and its robustness to SAE architectural variations

## Next Checks

1. **Generalization Test**: Apply SAL to pre-trained models trained on different domains (e.g., code, general web text) and validate correlation with post-RLVR performance on domain-specific benchmarks

2. **SAE Architecture Robustness**: Train SAL using standard per-layer SAEs instead of cross-layer SAEs and compare correlation strength with post-RLVR performance

3. **Causal Intervention Test**: Perform an intervention where SAL is explicitly optimized during pre-training (e.g., via auxiliary loss encouraging distribution separation) and measure the resulting post-RLVR reasoning performance gains