---
ver: rpa2
title: 'Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings'
arxiv_id: '2602.01757'
source_url: https://arxiv.org/abs/2602.01757
tags:
- inversion
- embedding
- victim
- zero2t
- algen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Zero2Text introduces a novel training-free embedding inversion
  attack that overcomes the key limitation of existing methods: poor cross-domain
  generalization. By combining LLM-based token generation with dynamic online optimization,
  it reconstructs text from unknown target embeddings without relying on any auxiliary
  datasets or leaked text-vector pairs.'
---

# Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings

## Quick Facts
- arXiv ID: 2602.01757
- Source URL: https://arxiv.org/abs/2602.01757
- Reference count: 11
- Primary result: Training-free cross-domain embedding inversion achieving 1.8× higher ROUGE-L and 6.4× higher BLEU-2 on MS MARCO

## Executive Summary
Zero2Text introduces a novel training-free embedding inversion attack that overcomes the key limitation of existing methods: poor cross-domain generalization. By combining LLM-based token generation with dynamic online optimization, it reconstructs text from unknown target embeddings without relying on any auxiliary datasets or leaked text-vector pairs. The method leverages recursive online alignment via ridge regression to map generated text embeddings to the victim model's space, while maintaining diversity and accuracy through confidence-aware scoring.

## Method Summary
Zero2Text is a training-free embedding inversion attack that uses LLM-based token generation combined with dynamic online optimization to reconstruct text from unknown target embeddings. The method employs recursive online alignment via ridge regression to map generated text embeddings to the victim model's space, with confidence-aware scoring to maintain diversity and accuracy. It requires zero offline queries, consumes 4× fewer tokens during online queries, and maintains strong performance across varying model sizes.

## Key Results
- Achieves 1.8× higher ROUGE-L and 6.4× higher BLEU-2 scores on MS MARCO against OpenAI's embedding model
- Requires zero offline queries and consumes 4× fewer tokens during online queries compared to baselines
- Maintains strong performance across varying model sizes and domains
- Effective against standard defenses like differential privacy

## Why This Works (Mechanism)

### Mechanism 1: Recursive Online Alignment via Ridge Regression
Instance-specific projection matrices trained on-the-fly outperform static alignment methods for cross-domain inversion. At each iteration, the attacker queries the victim embedder with candidate sentences, obtaining ground-truth embeddings. A projection matrix W_t is computed via closed-form ridge regression to map local embeddings to the victim's space, updating incrementally as more query-response pairs accumulate. The method assumes the victim embedder's output space is approximately linearly mappable from the attacker's local embedder space for short sequences.

### Mechanism 2: Confidence-Aware Hybrid Scoring
Dynamically weighting LLM linguistic priors against embedding similarity based on projection reliability prevents early divergence and improves late-stage precision. A scoring function combines z-normalized LLM logits and cosine similarity, with a confidence term that ensures the model relies more on embedding similarity as the projection matrix becomes more accurate. The method assumes LLM logits provide grammatically plausible continuations even when the projection matrix is unreliable.

### Mechanism 3: Query Budget Decay with Exponential Gating
Sending fewer queries over time (controlled by decay factor γ) maintains reconstruction quality while reducing token consumption. The attacker sends K_A × γ^(t-1) candidate sentences to the victim API, where γ < 1. Early iterations explore broadly; later iterations exploit the refined projection matrix without redundant queries. The method assumes the projection matrix stabilizes sufficiently after early iterations.

## Foundational Learning

- **Ridge Regression for Online Mapping**: Understanding regularized least-squares solutions is essential for the core alignment mechanism. The bias-variance trade-off controlled by λ and the computational advantage of closed-form updates over iterative optimization are critical. Quick check: Given a small set of local-victim embedding pairs, explain why ridge regression (with λ > 0) is preferred over ordinary least squares for computing W_t.

- **Beam Search with Diversity Constraints**: Token generation uses beam search but adds a pairwise cosine similarity threshold to prevent mode collapse. Understanding how diversity filters interact with beam scoring is essential. Quick check: If all top-K tokens by probability have embeddings with cosine similarity > T_hw, what happens to the candidate set and how might this affect reconstruction?

- **Differential Privacy Noise Mechanisms**: The defense evaluation tests against LDP mechanisms. Understanding how noise injection degrades embedding similarity helps interpret robustness results. Quick check: For a given privacy budget ϵ/d, explain why smaller values increase reconstruction difficulty and how this relates to the noise scale in the Laplace mechanism.

## Architecture Onboarding

- **Component map**: LLM Generator (Qwen3-0.6B) -> Local Embedder (all-mpnet-base-v2) -> Projection Module -> Scoring Module -> Query Scheduler -> Victim API -> Projection Module update -> Beam Buffer

- **Critical path**: Target embedding e_v → LLM generates candidates → Local embedder encodes candidates → Projection module maps to victim space → Scoring module ranks candidates → Query scheduler selects subset for API → Victim API returns ground-truth embeddings → Projection module updates W_t → Beam buffer retains top sequences → Repeat until EOS or max length

- **Design tradeoffs**: Higher K_A improves projection accuracy but increases token cost; higher beam size explores more sequences but compounds computational cost; lower γ delays query reduction, improving long-sequence reconstruction at higher cost; higher T_hw enforces more diversity but may exclude grammatically preferred tokens

- **Failure signatures**: Early divergence produces fluent but semantically unrelated text; mode collapse causes all beam candidates to converge to near-identical continuations; projection instability leads to oscillating or large-norm matrices; API throttling occurs when query budget is exceeded

- **First 3 experiments**:
  1. Baseline alignment comparison: Run Zero2Text vs. ALGEN on 50 MS MARCO samples using OpenAI (3-small); log ROUGE-L, BLEU-2, and total query count
  2. Ablation on confidence weighting: Disable conf_t and compare reconstruction quality on PubMed to assess sensitivity to domain-specific linguistic priors
  3. Defense robustness test: Apply LapMech with ϵ/d ∈ {0.5, 1.0, 2.0} to OpenAI embeddings before inversion; plot reconstruction metric degradation

## Open Questions the Paper Calls Out
None

## Limitations
- The online ridge regression alignment depends heavily on the quality and diversity of initial query-response pairs, with no empirical validation of sensitivity to initial query set composition
- The diversity-aware filtering mechanism could inadvertently remove high-probability tokens that happen to be semantically similar, potentially degrading linguistic coherence early in the sequence
- The exponential query decay schedule is fixed across all domains and target lengths, though the optimal decay rate may vary significantly between short-form queries and long-form documents

## Confidence
- High confidence: Cross-domain performance claims (ROUGE-L ~1.8×, BLEU-2 ~6.4× improvements on MS MARCO vs. ALGEN) - supported by direct experimental comparison
- Medium confidence: Training-free/zero-query advantage claims - token counts show clear reduction but computational trade-offs relative to offline training are not fully quantified
- Medium confidence: Defense robustness claims - differential privacy effectiveness is demonstrated but limited to Laplace and PurMech mechanisms

## Next Checks
1. **Projection sensitivity analysis**: Systematically vary initial query count (3×K_A, 5×K_A, 10×K_A) and monitor reconstruction quality degradation across 50 MS MARCO samples; plot reconstruction metrics against query set size

2. **Diversity filter ablation**: Run Zero2Text with T_hw disabled (set to 1.0) and enabled (0.9) on 30 long-form PubMed samples (>50 tokens); measure whether removing the filter improves ROUGE-L at the cost of increased mode collapse

3. **Adaptive decay scheduling**: Implement a length-aware decay schedule where γ is computed as a function of target sequence length; evaluate on a mixed-length corpus to determine if domain-specific decay improves overall reconstruction efficiency