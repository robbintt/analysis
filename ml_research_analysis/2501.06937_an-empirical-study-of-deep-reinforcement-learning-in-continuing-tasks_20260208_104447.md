---
ver: rpa2
title: An Empirical Study of Deep Reinforcement Learning in Continuing Tasks
arxiv_id: '2501.06937'
source_url: https://arxiv.org/abs/2501.06937
tags:
- steps
- reward
- resets
- algorithms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides an empirical study of several deep reinforcement
  learning algorithms in continuing tasks, where the agent-environment interaction
  is ongoing and cannot be broken down into episodes. The study uses a suite of testbeds
  based on Mujoco and Atari environments, exploring different reset scenarios including
  no resets, predefined resets, and agent-controlled resets.
---

# An Empirical Study of Deep Reinforcement Learning in Continuing Tasks

## Quick Facts
- **arXiv ID:** 2501.06937
- **Source URL:** https://arxiv.org/abs/2501.06937
- **Reference count:** 40
- **Primary result:** Deep RL algorithms perform significantly worse in continuing tasks without resets compared to those with predefined resets; predefined resets help by limiting state space and enabling escape from suboptimal states.

## Executive Summary
This paper provides an empirical study of deep reinforcement learning algorithms in continuing tasks, where agent-environment interaction is ongoing and cannot be broken down into episodes. The study evaluates DDPG, TD3, SAC, PPO, and DQN on Mujoco and Atari environments under different reset scenarios. Key findings include that predefined resets significantly improve performance by limiting the effective state space and enabling exploration from high-value regions, while agent-controlled resets often degrade performance due to noise sensitivity in the augmented action dimension. The study also demonstrates that large discount factors and shared reward offsets cause instability, and evaluates a temporal-difference-based reward centering method that improves performance across a broad range of algorithms and tasks.

## Method Summary
The paper modifies standard Gymnasium environments to remove time-based truncation and implement three reset scenarios: no resets, predefined resets with cost -10, and agent-controlled resets via augmented action dimension. It evaluates five deep RL algorithms (DDPG, TD3, SAC, PPO, DQN) on 15 continuous control tasks (Mujoco-based) and 6 discrete tasks (Atari). The primary metric is average-reward rate (rewards per step) evaluated over the last 10,000 steps across 10 independent runs. A key methodological contribution is the TD-based reward centering method that maintains a scalar estimate of the average reward rate to stabilize learning with large discount factors or reward offsets.

## Key Results
- Tested algorithms perform significantly worse in tasks without resets compared to those with predefined resets
- Predefined resets help by limiting the effective state space and allowing agents to escape suboptimal states
- All algorithms perform poorly with large discount factors (0.999) or shared reward offsets without reward centering
- Agent-controlled resets often degrade performance due to noise sensitivity in the augmented action dimension
- TD-based reward centering consistently improves performance across algorithms and tasks

## Why This Works (Mechanism)

### Mechanism 1: Resets as State-Space Regularization
Predefined resets improve sample efficiency and final performance in continuing tasks by limiting the effective state space distribution and forcing exploration from high-value regions. In continuing tasks without resets, agents often drift into vast, low-reward regions of the state space or get stuck in suboptimal local optima. Stochastic resets truncate the reachable state space, preventing the agent from wasting capacity learning to navigate irrelevant "tail" states. They also act as a forced exploration mechanism, periodically injecting the agent back into the initial state distribution. The environment must be weakly communicating for this to work.

### Mechanism 2: Reward Centering via Offset Decoupling
TD-based reward centering mitigates instability caused by large discount factors or large constant reward offsets by offloading the estimation of the average reward rate from the value network to a scalar. Value functions in continuing tasks contain a large state-independent offset. Neural networks struggle to approximate this constant bias alongside relative state values. Reward centering subtracts a running estimate of the reward rate from the reward signal, reducing the magnitude of the values the network must approximate and effectively decoupling the "bias" (average reward) from the "shape" (differential value).

### Mechanism 3: Agent-Controlled Reset Sensitivity
Granting the agent control over resets often degrades performance compared to predefined resets due to noise sensitivity in the augmented action dimension. Adding a reset action increases the dimensionality of the action space. Standard exploration noise applied uniformly is often poorly calibrated for the reset dimension. Small noise in a probability output can trigger excessive, suboptimal resets, or conversely, the agent may fail to learn the "reset" action as an escape mechanism, treating it as noise.

## Foundational Learning

- **Concept: Continuing vs. Episodic Tasks**
  - **Why needed here:** The paper fundamentally shifts the evaluation from "maximizing return until reset" to "maximizing reward rate over infinite horizons."
  - **Quick check question:** Does your environment reset automatically after T steps, or does the agent have to run indefinitely?

- **Concept: Weakly Communicating MDPs**
  - **Why needed here:** The paper's results rely on the property that the agent can recover from any state. If the environment has "absorbing" failure states, no RL algorithm can succeed without a reset mechanism.
  - **Quick check question:** If the agent falls over, can it theoretically stand back up, or is it stuck forever?

- **Concept: Laurent Series Decomposition of Value**
  - **Why needed here:** To understand why reward centering works, one must grasp that value functions have a constant term (the offset) and a differential term.
  - **Quick check question:** Why does a large constant reward (+100 per step) break standard Q-learning approximations? (Hint: It blows up the target magnitude).

## Architecture Onboarding

- **Component map:** Environment Wrapper -> Agent -> Reward Centering Module
- **Critical path:** 1) Implement the Continuing Wrapper (ensure `done` is only triggered by state-based failures, not time), 2) Implement the TD-based Reward Centering logic inside the critic update step, 3) Calibrate Exploration Noise specifically for the reset action dimension if using agent-controlled resets
- **Design tradeoffs:**
  - Reset Cost: Low cost = agent learns risky, high-reward behavior with frequent resets. High cost = agent learns conservative, stable behavior.
  - Discount Factor: While 0.99 is standard, this paper shows 0.999 fails without reward centering
- **Failure signatures:**
  - Silent Divergence: Losses look stable, but the average reward rate drifts to random-policy levels
  - Offset Collapse: Q-values explode to infinity or become constant across states due to large reward offsets
- **First 3 experiments:**
  1. Run SAC on HalfCheetah with no resets. Observe if the agent gets stuck in a fallen state
  2. Add random resets (p=0.001). Confirm performance recovers to narrow the root cause to state-space exploration
  3. Run with Î³=0.999 and large reward offset (+100). Compare standard SAC vs. SAC + Reward Centering to verify stability

## Open Questions the Paper Calls Out
- How can the reset cost parameter be optimally selected or adapted in continuing tasks where it functions as both a problem and solution parameter?
- What hyperparameter configurations are optimal for deep reinforcement learning algorithms when applied specifically to continuing tasks?
- Do deep reinforcement learning algorithms designed for the average-reward criterion face the same challenges as discounted algorithms in continuing tasks?
- How do offline reinforcement learning algorithms perform in continuing tasks compared to the online algorithms evaluated in this study?

## Limitations
- The effectiveness of predefined resets depends critically on the weakly communicating assumption; environments with true absorbing failure states are not tested
- The reward centering method assumes a single recurrent class in the Markov chain, which may not hold in multimodal or complex tasks
- The agent-controlled reset mechanism shows inconsistent performance, suggesting that the exploration strategy for the reset dimension is not yet optimized

## Confidence
- **High:** The core finding that predefined resets significantly improve performance in continuing tasks is well-supported by experimental evidence across multiple algorithms and environments
- **Medium:** The reward centering method shows consistent improvements, but the optimal hyperparameter settings vary across tasks
- **Medium:** The agent-controlled reset mechanism's poor performance is consistently observed, but the paper does not provide a complete solution for optimizing the exploration strategy

## Next Checks
1. Test the algorithms on environments with true absorbing states to verify that the weakly communicating assumption is necessary for the proposed methods to work
2. Implement a systematic exploration strategy for the reset dimension in agent-controlled reset scenarios and compare performance against the baseline
3. Extend the reward centering method to handle environments with multiple recurrent classes by maintaining separate offset estimates for different regions of the state space