---
ver: rpa2
title: 'LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of
  LLMs with Structural Counterfactuals'
arxiv_id: '2601.10700'
source_url: https://arxiv.org/abs/2601.10700
tags:
- concept
- causal
- concepts
- text
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LIBERTy, a causal framework for benchmarking
  concept-based explanations of large language models using structural counterfactuals.
  LIBERTy generates counterfactuals by explicitly defining structured causal models
  over concepts and using LLMs to instantiate them as text, eliminating reliance on
  human-written proxies.
---

# LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals

## Quick Facts
- arXiv ID: 2601.10700
- Source URL: https://arxiv.org/abs/2601.10700
- Reference count: 40
- Primary result: LIBERTy benchmark shows fine-tuned matching methods outperform LLM-generated counterfactuals, but substantial headroom remains for concept-based explanation methods.

## Executive Summary
LIBERTy introduces a causal framework for benchmarking concept-based explanations of large language models using structural counterfactuals. The framework explicitly defines Structured Causal Models (SCMs) over concepts and uses LLMs to instantiate them as text, providing a silver-standard reference for evaluation. Three synthetic datasets (disease detection, CV screening, workplace violence prediction) are introduced alongside a new metric, order-faithfulness, which evaluates whether explanation methods preserve the relative importance ranking of concept effects. Experiments with five models and eight explanation methods reveal that fine-tuned matching approaches perform best, but significant performance gaps remain, calling for novel, theory-grounded explanation methods.

## Method Summary
LIBERTy generates counterfactuals by explicitly defining SCMs over concepts and using LLMs to instantiate them as text. The framework follows Pearl's three-step counterfactual procedure: abduction (fixing exogenous variables), action (intervening on target concepts), and prediction (propagating through the SCM). For evaluation, it introduces two metrics: Error Distance (ED) measuring absolute deviation from ground-truth Individual Causal Concept Effects (ICaCE), and Order-Faithfulness (OF) measuring ordinal alignment of concept importance rankings. The benchmark evaluates eight explanation methods across five models on three synthetic datasets, revealing that fine-tuned matching methods (FT Match) outperform LLM-generated counterfactuals and concept erasure approaches.

## Key Results
- Fine-tuned matching methods achieve the lowest estimation error (ED ≈ 0.19) and highest order-faithfulness (OF ≈ 0.86) across all three datasets.
- LLM-generated counterfactual explanations fail to match the performance of fine-tuned matching, with ED ≈ 0.48 and OF ≈ 0.58.
- Proprietary LLMs (GPT-4o, Llama) show near-zero sensitivity to demographic interventions, likely due to post-training alignment, making faithful explanation impossible.
- Substantial headroom remains for improvement, with current methods showing significant performance gaps compared to ground-truth effects.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural counterfactuals generated via explicit SCMs provide a silver-standard reference for evaluating concept-based explanations.
- Mechanism: A user-defined causal graph specifies concept relationships (confounders, mediators, colliders). Exogenous noise variables (personas, templates) and endogenous concepts are sampled. GPT-4o instantiates the final text. For counterfactuals, Pearl's three-step procedure (Abduction → Action → Prediction) propagates an intervention through the SCM while holding exogenous variables fixed, then regenerates text with deterministic decoding (temperature=0).
- Core assumption: The explicitly defined SCM approximates a valid data-generating process; deterministic LLM decoding preserves structural counterfactual semantics.
- Evidence anchors:
  - [abstract] "LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs)... interventions on a concept propagate through the SCM until an LLM generates the counterfactual."
  - [Section 4, Counterfactual Generation] "We follow Pearl's three-step counterfactual procedure... (1) Abduction: fix the exogenous variables... (2) Action: intervene on a target concept, and (3) Prediction: propagate..."
  - [corpus] Neighbor paper "Canonical Representations of Markovian Structural Causal Models" addresses formal SCM counterfactual frameworks; weak direct validation for this specific SCM→LLM pipeline.
- Break condition: If the SCM does not reflect concept relationships as they actually appear in text, or if deterministic decoding produces incoherent text, structural counterfactuals may diverge from any meaningful ground truth.

### Mechanism 2
- Claim: The order-faithfulness metric robustly evaluates explanation methods by testing whether they preserve the relative importance ranking of concept effects.
- Mechanism: For two concept changes, compute both the reference ICaCE difference vector and the explanation difference vector. Order-faithfulness measures the proportion of vector entries where signs agree. This decouples evaluation from arbitrary importance score scales and focuses on ordinal alignment.
- Core assumption: Faithful explanations must rank concepts consistently with their true causal effects, even if absolute magnitudes differ.
- Evidence anchors:
  - [abstract] "new evaluation metric, order-faithfulness"
  - [Section 3.2, Box 3.1 Def 4] Formal definition: "OF(...)=sign(ICaCE_diff; M_diff)" measuring proportion of agreeing entries.
  - [corpus] Neighbor paper "A Framework for Causal Concept-based Model Explanations" discusses causal faithfulness requirements but does not propose this specific metric.
- Break condition: If explanation methods produce importance scores on fundamentally incompatible scales or if ICaCE vectors have mixed signs with no clear ordering, the metric may yield ambiguous results.

### Mechanism 3
- Claim: Fine-tuned matching methods (FT Match) outperform LLM-generated counterfactuals because they retrieve candidates from the true DGP distribution.
- Mechanism: FT Match fine-tunes an encoder (DeBERTa) on the target task, then retrieves nearest neighbors in embedding space among texts with the desired concept value. Unlike LLM counterfactual generation—which mimics human editing heuristics—matching selects real samples from the data distribution, implicitly respecting the underlying SCM structure.
- Core assumption: The embedding space of a task-fine-tuned encoder organizes texts in a way that preserves concept semantics relevant to the explained model.
- Evidence anchors:
  - [Section 7.1] "FT Match... achieves the lowest estimation error and emerges as the most faithful method. Its advantage likely stems from the model learning task-specific representations..."
  - [Appendix A.3] "LLM-generated counterfactuals fail in this setting because their edits reflect heuristic assumptions, rather than the actual underlying mechanism."
  - [corpus] Neighbor paper "Multi-Domain Explainability of Preferences" also finds concept-based matching effective for explanation tasks.
- Break condition: If the concept change is rare in the data pool (sparse matches), or if fine-tuned embeddings do not separate the target concept, retrieval quality degrades.

## Foundational Learning

- Concept: **Structural Causal Models (SCMs)**
  - Why needed here: LIBERTy's entire evaluation framework depends on understanding how endogenous variables (concepts) relate through structural equations, and how interventions propagate.
  - Quick check question: Given a causal graph A→B→Y, what happens to B when you intervene on A? What about when you observe A changes naturally?

- Concept: **Counterfactual Reasoning (Pearl's 3-Step)**
  - Why needed here: Generating structural counterfactuals requires abduction (fixing exogenous noise), action (do-intervention), and prediction (propagation)—not just "what if" text editing.
  - Quick check question: In Pearl's framework, what is the difference between an interventional query P(Y|do(X=x)) and a counterfactual query about what Y would have been had X been different?

- Concept: **Individual Treatment Effect (ITE) vs Average Treatment Effect (ATE)**
  - Why needed here: ICaCE (individual) and CaCE (population-level) are direct analogues; local explanations should estimate ICaCE, global explanations should estimate CaCE.
  - Quick check question: Why might an explanation method correctly estimate average effects but fail on individual-level predictions?

## Architecture Onboarding

- Component map:
  - SCM Definitions -> Text Generator -> Explained Models -> Explanation Methods -> Evaluation Pipeline
  - SCM Definitions: Three domain-specific causal graphs (Workplace Violence, Disease Detection, CV Screening) with structural equations for each concept
  - Exogenous Grounding: Persona strings (informal facts) and templates (narrative structures) sampled to ensure text diversity while maintaining counterfactual validity
  - Text Generator: GPT-4o (temperature=0) instantiates concept values as natural language
  - Explained Models: Five models (DeBERTa, T5, Qwen, Llama, GPT-4o) trained/prompted to predict the target Y from text
  - Explanation Methods: Eight methods across four families (Counterfactual Generation, Matching, Concept Erasure, Concept Attributions)
  - Evaluation Pipeline: Computes ICaCE/CaCE reference effects, compares against explanation scores using Error Distance and Order-Faithfulness

- Critical path:
  1. Define or adapt an SCM for your domain (concept nodes, edges, structural equations)
  2. Generate grounding texts (personas/templates) and sample concept values
  3. Use GPT-4o (temp=0) to generate factual texts
  4. Apply Pearl's procedure to generate structural counterfactuals for each test intervention
  5. Train explained models on generated data (excluding counterfactuals)
  6. Train/run explanation methods on explained models
  7. Evaluate explanation faithfulness against reference ICaCE/CaCE

- Design tradeoffs:
  - **Synthetic vs Real Data**: Synthetic SCMs enable controlled interventions but may not reflect real-world concept relationships. Human validation (Appendix B) confirms coherence but not real-world grounding
  - **Deterministic Decoding**: Required for structural counterfactuals but produces less diverse, more templated text. Exogenous grounding (personas/templates) mitigates but does not eliminate this
  - **LLM-as-Generator vs LLM-as-Explainer**: GPT-4o generates the data; other LLMs are explained. This separation is intentional but may introduce artifacts if the generator's concept instantiation differs from human expression patterns

- Failure signatures:
  - **Low concept classifier accuracy** (Appendix E.1): If concept value extraction fails, matching methods degrade sharply
  - **High ICaCE variance across templates**: Indicates deterministic decoding may still produce inconsistent concept instantiations
  - **Explanation methods agreeing with each other but disagreeing with reference ICaCE**: Suggests methods share a systematic bias (e.g., all focusing on lexical features rather than semantics)
  - **Zero-shot LLMs showing near-zero sensitivity to demographic interventions**: Expected for aligned models (Section 7.3), not a framework failure

- First 3 experiments:
  1. **Reproduce baseline results**: Run FT Match and CF Gen on the CV Screening dataset with DeBERTa-v3 as the explained model. Verify ED ≈ 0.19 (FT Match) vs ≈ 0.48 (CF Gen) and OF ≈ 0.86 vs ≈ 0.58 per Table 15
  2. **Ablate deterministic decoding**: Generate counterfactuals with temperature > 0 and measure ICaCE variance across multiple samples. Compare ED/OF against the temperature=0 baseline to quantify the structural counterfactual assumption's impact
  3. **Test a new SCM**: Define a simplified 4-concept graph (similar to CEBaB) for a new domain, generate 500 examples, and compare FT Match vs LLM-generated counterfactuals. This validates whether the framework generalizes beyond the three provided datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel, theory-grounded explanation methods be developed to close the substantial performance gap (Error Distance $\approx 0.3$) identified in the LIBERTy benchmark?
- Basis in paper: [explicit] The authors conclude that "substantial headroom for improvement" remains and explicitly call for the development of "smaller, theory-grounded, causal-inspired explainability approaches" rather than relying on general LLMs.
- Why unresolved: Current state-of-the-art methods (fine-tuned matching) still show significant error, and LLM-generated counterfactual explanations fail against structural benchmarks, leaving the optimal architectural approach undefined.
- What evidence would resolve it: A new explanation method achieving an Error Distance and Order-Faithfulness score significantly surpassing the current "FT Match" baseline across all three datasets.

### Open Question 2
- Question: How can causal learning techniques be modified to ensure fine-tuned models capture the data-generating process (DGP) rather than failing to reflect the ground-truth causal structure?
- Basis in paper: [explicit] The analysis reveals that "vanilla fine-tuning may fail to capture the causal structure of the data," leading to a discrepancy between model sensitivity and true effects, which the authors highlight as an opportunity for "unique learning methods."
- Why unresolved: Standard fine-tuning optimizes for prediction accuracy but does not inherently enforce alignment with the specific structural causal equations defined in the DGP.
- What evidence would resolve it: A training methodology that results in fine-tuned models exhibiting concept sensitivity scores that statistically match the ground-truth effects defined in the SCMs without degrading predictive performance.

### Open Question 3
- Question: To what extent do explanation faithfulness evaluations conducted on synthetic, LLM-generated structural counterfactuals transfer to models operating on human-written text?
- Basis in paper: [inferred] The paper acknowledges the limitation that "texts are synthetic rather than human-written" and that there may be "mismatches between how the LLM instantiates concepts and how humans would naturally express them."
- Why unresolved: While the framework ensures structural validity, the linguistic patterns of GPT-4o generated text differ from natural human text, potentially biasing the evaluation of explanation methods.
- What evidence would resolve it: A correlation analysis showing that explanation method rankings on LIBERTy datasets correspond strongly with rankings on equivalent human-curated counterfactual datasets (e.g., CEBaB).

### Open Question 4
- Question: Does the observed low sensitivity of proprietary LLMs (like GPT-4o) to demographic interventions indicate successful alignment mitigation or a failure to reason over the causal structure?
- Basis in paper: [inferred] The authors find that proprietary LLMs show "markedly reduced sensitivity to demographic concepts," which they hypothesize is due to "post-training mitigation." However, this insensitivity also means the model fails to track the ground-truth effects established in the DGP.
- Why unresolved: It is unclear if the model is ignoring the concept as intended by safety alignment or if it is simply failing to process the intervention causally.
- What evidence would resolve it: A study comparing sensitivity scores across various alignment techniques to disentangle "safety via blindness" from "safety via equitable reasoning."

## Limitations

- **Synthetic Data Dependency**: The framework relies on synthetic data generated via SCMs, which may not reflect real-world concept relationships and linguistic patterns
- **Deterministic Decoding Constraint**: Structural counterfactuals require temperature=0, producing less natural text that may not capture the full complexity of concept interactions
- **Proprietary Model Sensitivity**: Zero-shot LLMs show near-zero sensitivity to demographic interventions due to alignment, making faithful explanation impossible for these models

## Confidence

- **High**: The LIBERTy framework design (SCM + Pearl's procedure + evaluation metrics) is internally consistent and technically sound
- **Medium**: The claim that FT Match outperforms LLM-generated counterfactuals is well-supported by error metrics, but the mechanism (distributional vs heuristic editing) needs further validation
- **Medium**: The assertion that zero-shot LLMs are less sensitive to demographic interventions is supported by results but may conflate alignment effects with model architecture differences

## Next Checks

1. **Cross-Domain SCM Transfer**: Apply the workplace violence SCM to a different industry dataset and measure whether FT Match still outperforms LLM-generated counterfactuals
2. **Deterministic vs Stochastic Counterfactuals**: Generate counterfactuals with temperature=0.5 and measure the trade-off between text naturalness and ICaCE variance
3. **Concept Classifier Ablation**: Systematically degrade concept classifier accuracy (via simulated noise) and measure the impact on matching method performance across all three datasets