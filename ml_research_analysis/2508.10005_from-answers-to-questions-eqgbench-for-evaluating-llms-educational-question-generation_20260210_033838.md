---
ver: rpa2
title: 'From Answers to Questions: EQGBench for Evaluating LLMs'' Educational Question
  Generation'
arxiv_id: '2508.10005'
source_url: https://arxiv.org/abs/2508.10005
tags:
- question
- evaluation
- generation
- knowledge
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EQGBench, a comprehensive evaluation benchmark
  for assessing large language models' (LLMs) capabilities in educational question
  generation. The benchmark includes a high-quality dataset of 900 structured evaluation
  samples across mathematics, physics, and chemistry, designed to simulate real-world
  educational scenarios.
---

# From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation

## Quick Facts
- **arXiv ID**: 2508.10005
- **Source URL**: https://arxiv.org/abs/2508.10005
- **Reference count**: 8
- **Primary result**: Comprehensive benchmark for evaluating LLMs' educational question generation capabilities across mathematics, physics, and chemistry

## Executive Summary
This paper introduces EQGBench, a novel evaluation benchmark designed to assess large language models' capabilities in educational question generation. The benchmark addresses the growing need for automated assessment tools as educational applications of LLMs expand. It provides a structured framework for evaluating how well LLMs can transform answers into pedagogically sound questions across three core STEM subjects.

The benchmark consists of 900 high-quality evaluation samples with a five-dimensional evaluation framework covering knowledge point alignment, question type alignment, question item quality, solution explanation quality, and competence-oriented guidance. Through systematic evaluation of 46 mainstream LLMs, the study reveals that while top-performing models demonstrate strong foundational capabilities, they struggle with generating questions that exhibit deep pedagogical intent, particularly in mathematics. The automated evaluation pipeline showed strong reliability and consistency when validated against expert teacher assessments.

## Method Summary
The researchers constructed EQGBench through a multi-stage process involving domain expert collaboration, systematic data collection, and rigorous quality control. They curated 900 structured evaluation samples across mathematics, physics, and chemistry, ensuring each sample includes answers and associated pedagogical metadata. The five-dimensional evaluation framework was developed through expert consultation to capture essential aspects of educational question quality. For evaluation, they implemented an automated pipeline that scores generated questions across all dimensions, with validation against human expert assessments to ensure reliability.

## Key Results
- EQGBench provides 900 structured evaluation samples across mathematics, physics, and chemistry
- Top-performing LLMs show strong foundational question generation capabilities but struggle with deep pedagogical intent
- The automated evaluation pipeline demonstrated high reliability when validated against expert teacher assessments
- Systematic evaluation of 46 mainstream LLMs revealed performance gaps, particularly in mathematics question generation

## Why This Works (Mechanism)
The benchmark works by providing a structured framework that captures the complexity of educational question generation beyond simple text generation. The five-dimensional evaluation ensures comprehensive assessment of both technical and pedagogical quality. The automated pipeline leverages consistent evaluation criteria while maintaining alignment with human pedagogical standards through expert validation. This systematic approach enables scalable and reliable assessment of LLM capabilities in educational contexts.

## Foundational Learning

### Educational Question Generation Concepts
- **Why needed**: Understanding how questions should be structured to achieve specific learning objectives
- **Quick check**: Can identify differences between factual recall questions and higher-order thinking questions

### Pedagogical Quality Assessment
- **Why needed**: Evaluating whether generated questions support effective learning and skill development
- **Quick check**: Can distinguish between surface-level and deep pedagogical features in questions

### STEM Domain Knowledge
- **Why needed**: Ensuring questions are technically accurate and appropriately scoped for subject matter
- **Quick check**: Can verify subject-specific terminology and conceptual accuracy

### Automated Evaluation Metrics
- **Why needed**: Enabling scalable assessment of large numbers of generated questions
- **Quick check**: Can implement scoring systems that align with human judgment

## Architecture Onboarding

### Component Map
- Domain experts -> Dataset curation -> 900 structured samples
- Five-dimensional framework design -> Evaluation criteria definition
- LLMs -> Question generation -> Generated questions
- Automated pipeline -> Quality scoring -> Performance metrics

### Critical Path
1. Expert consultation and framework development
2. Dataset curation and quality assurance
3. LLM evaluation and scoring
4. Performance analysis and benchmarking

### Design Tradeoffs
- **Breadth vs Depth**: Three STEM subjects provide focused evaluation but limit generalizability
- **Automation vs Human Judgment**: Automated pipeline enables scalability but may miss nuanced pedagogical qualities
- **Sample Size vs Diversity**: 900 questions ensure quality but may not capture full educational scenario diversity

### Failure Signatures
- Over-reliance on factual recall questions without higher-order thinking
- Technical inaccuracies in subject-specific content
- Misalignment between question intent and learning objectives
- Poor solution explanation quality

### 3 First Experiments
1. Test benchmark on additional LLM architectures not included in original evaluation
2. Apply framework to different educational domains (e.g., humanities)
3. Conduct longitudinal study to track model improvements over time

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on three STEM subjects, limiting generalizability to other educational domains
- Sample size of 900 questions may not fully capture the diversity of real-world educational scenarios
- Automated evaluation metrics may not fully capture nuanced pedagogical qualities that human experts would prioritize

## Confidence

**High confidence**: Benchmark construction methodology and dataset quality are well-documented and methodologically sound

**Medium confidence**: Automated evaluation pipeline's reliability scores, while promising, need broader validation across different educational contexts

**Medium confidence**: Comparative analysis of 46 LLMs is robust, but performance gaps identified may shift as models continue to evolve rapidly

## Next Checks

1. Conduct cross-validation with a larger, more diverse teacher panel to verify automated evaluation metrics' alignment with human pedagogical judgment

2. Test benchmark applicability across additional subjects (e.g., humanities, social sciences) to assess domain generalizability

3. Implement longitudinal studies to track how question generation capabilities evolve as newer LLMs are released, establishing the benchmark's temporal stability