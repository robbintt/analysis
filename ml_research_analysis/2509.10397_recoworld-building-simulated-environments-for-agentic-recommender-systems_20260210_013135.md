---
ver: rpa2
title: 'RecoWorld: Building Simulated Environments for Agentic Recommender Systems'
arxiv_id: '2509.10397'
source_url: https://arxiv.org/abs/2509.10397
tags:
- user
- arxiv
- recommender
- agentic
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RecoWorld presents a simulated environment framework for training
  and evaluating agentic recommender systems. It uses a dual-view architecture where
  a simulated user and an agentic recommender engage in multi-turn interactions aimed
  at maximizing user retention.
---

# RecoWorld: Building Simulated Environments for Agentic Recommender Systems

## Quick Facts
- arXiv ID: 2509.10397
- Source URL: https://arxiv.org/abs/2509.10397
- Reference count: 40
- Key outcome: Simulated environment framework for training agentic recommender systems via multi-turn user–system interactions, optimizing retention through instruction-following feedback loops

## Executive Summary
RecoWorld introduces a simulated environment framework for training and evaluating agentic recommender systems. It employs a dual-view architecture where simulated users and recommender agents engage in multi-turn interactions, with users generating instructions when disengaged and recommenders adapting accordingly. The framework supports diverse content representations (text, multimodal, semantic IDs) and enables RL-based policy optimization using trajectory-level retention metrics. This marks an important step toward collaborative recommender systems where "user instructs, recommender responds," enabling safe testing of recommendation strategies without impacting real users.

## Method Summary
RecoWorld models user–recommender interaction as a Markov Decision Process, where states represent user mindsets, actions are recommendation policies, and rewards are session-level retention metrics. The dual-view architecture features an LLM-based user simulator that processes recommended items through a three-step loop (reason → act → update mindset), generating natural language instructions when disengagement is sensed. The agentic recommender adapts its output using these instructions and reasoning traces. The environment supports three content representation modes (text, multimodal, semantic IDs) and can be trained using RL algorithms like PPO or DPO on trajectory-level rewards extracted from session statistics.

## Key Results
- Framework enables safe testing of agentic recommender strategies without real user exposure
- Supports multi-turn instruction-following interactions to optimize long-term retention
- Demonstrates viable RL training environment using trajectory-level reward signals
- Provides blueprint for future collaborative recommender systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-turn instruction-following interactions between simulated users and recommender agents can improve long-term retention metrics.
- **Mechanism:** The dual-view architecture creates a feedback loop where (1) the user simulator reviews recommended items, (2) upon sensing disengagement risk, generates reflective instructions (e.g., "show me more interesting content"), (3) the agentic recommender adapts its recommendations using these instructions plus reasoning traces, and (4) the cycle repeats until the user exits. Reward signals are extracted from trajectory-level statistics (time spent, clicks, turn count) rather than single-item metrics.
- **Core assumption:** LLM-based simulators can produce instruction signals that meaningfully approximate real user feedback disengagement patterns.
- **Evidence anchors:**
  - [abstract] "The user simulator reviews recommended items, updates its mindset, and when sensing potential user disengagement, generates reflective instructions."
  - [section 2] "The sequence of recommended items and the user's actions form the interaction trajectory for the session... We use trajectory-level interaction metrics such as total time spent as reward signals."
  - [corpus] Related work (AgentRecBench, UserRL) confirms RL training in simulated environments is a promising but still-experimental approach; FMR scores 0.457–0.658 suggest moderate but not definitive validation.
- **Break condition:** If simulated user instructions do not correlate with real user behavior signals, the feedback loop optimizes for artifacts rather than retention.

### Mechanism 2
- **Claim:** Explicit and implicit instruction signals can jointly guide adaptive recommendation policies.
- **Mechanism:** The agentic recommender processes both explicit natural-language instructions and implicit behavioral signals (e.g., watch duration patterns, skip rates). A reasoning-intensive retrieval module decomposes user instructions into subtasks, delegating them to retrieval and ranking modules. The agent maintains memory of user behavior to inform future interactions.
- **Core assumption:** The recommender can accurately align ambiguous user instructions with underlying intents (e.g., distinguishing situational vs. long-term interests).
- **Evidence anchors:**
  - [section 2] "User preferences are not conveyed solely through explicit instructions; they are also conveyed implicitly through behavior."
  - [section 4] "We envision a reasoning-intensive retrieval model powered by LLMs to infer user intents, refine ambiguous queries, and enhance the system's ability to understand user instructions."
  - [corpus] CTRL-Rec demonstrates natural language control of recommenders but notes alignment challenges with vague requests.
- **Break condition:** If instruction alignment is unreliable, the system may misinterpret user intent, leading to irrelevant or frustrating recommendations.

### Mechanism 3
- **Claim:** Semantic ID and multimodal content representations enable scalable, semantically meaningful simulation of user-item interactions.
- **Mechanism:** RecoWorld supports three content representations: (1) text-based modeling using metadata and descriptions, (2) multimodal modeling with audio/video/image tokens via MLLMs, and (3) semantic ID modeling where items are encoded into compact semantic identifiers. These representations allow the simulator to process diverse content types while maintaining semantic coherence for preference tracking.
- **Core assumption:** Semantic IDs or multimodal embeddings sufficiently capture the features users respond to when engaging with content.
- **Evidence anchors:**
  - [section 3.1] "Items with similar semantics share similar IDs, with the sequence from the first to the last character encoding progressively finer-grained semantic information."
  - [section 3.1] "Compared with text-only LLMs, multimodal models provide richer representational capacity for multimodal content, enabling more faithful input understanding."
  - [corpus] No direct comparative validation in corpus; this is proposed architecture, not established mechanism.
- **Break condition:** If content representations fail to capture key engagement signals (e.g., mood, sarcasm, audio features in text-only mode), simulation fidelity degrades.

## Foundational Learning

- **Concept: Markov Decision Processes for Recommender Systems**
  - Why needed here: RecoWorld models user–system interaction as an MDP where states are user mindsets, actions are recommendation policies, and rewards are retention metrics. Understanding state transitions and policy optimization is essential for implementing the RL training loop.
  - Quick check question: Can you explain how the state transition $p_\theta(s_{t+1}|s_t, a_t)$ differs in RecoWorld from a traditional recommendation bandit setup?

- **Concept: Instruction-Following vs. Conversational Recommendation**
  - Why needed here: The paper distinguishes InFoRec (direct command response) from CRS (proactive suggestion during dialogue). This distinction shapes how agents should interpret and act on user feedback.
  - Quick check question: If a user says "I'm bored," should an InFoRec system (a) suggest popular content proactively, or (b) ask clarifying questions to refine the instruction? Why?

- **Concept: Dynamic Memory and Evolving Preferences**
  - Why needed here: User simulators must maintain interaction-wise and session-wise memory, with a temporal update function for preference state. Without this, simulators cannot model realistic preference shifts or fatigue.
  - Quick check question: How would you implement the engagement memory threshold $\delta$ to filter noisy interactions while retaining informative behavioral signals?

## Architecture Onboarding

- **Component map:**
  User Simulator (LLM-based with context, engagement history, action space) -> Multi-turn Interaction Loop (item processing with think→act→update mindset) -> Agentic Recommender (perception→reasoning→action modules with memory) -> Reward System (trajectory metrics extraction and LLM-based evaluation)

- **Critical path:**
  1. Initialize user profile and session context
  2. Recommender generates initial item list
  3. User simulator processes items one-by-one: think → act → update mindset
  4. If disengagement detected, simulator generates instruction; recommender adapts list
  5. Repeat until user exits without instruction
  6. Compute trajectory rewards; update recommender policy via RL (PPO/DPO)

- **Design tradeoffs:**
  - Text-only vs. multimodal vs. semantic ID: Text is flexible but may miss nuance; multimodal is expressive but computationally heavy; semantic ID is compact but requires continual pre-training
  - Explicit vs. implicit instructions: Explicit enables direct control but requires alignment; implicit is abundant but noisier
  - Single-agent vs. multi-agent simulation: Single-agent is simpler; multi-agent captures social dynamics but increases complexity

- **Failure signatures:**
  - Simulator generates instructions that are too vague or inconsistent with user profile
  - Recommender fails to align instructions with actionable retrieval/ranking changes
  - Reward signals plateau despite policy updates—suggesting simulator–real user gap
  - Semantic ID representations become stale as new content lacks proper encodings

- **First 3 experiments:**
  1. **Baseline sanity check:** Run user simulator on a held-out dataset with known user actions; compare simulated vs. ground-truth interaction sequences using NDCG/Recall to validate behavioral fidelity
  2. **Instruction-following ablation:** Compare recommender performance with instructions enabled vs. disabled; measure session length and click-through rate to quantify instruction impact
  3. **Content representation comparison:** Run identical simulation with text-only, multimodal, and semantic ID settings; compare reward trajectories and simulator reasoning quality to identify representation bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurately does the RecoWorld user simulator replicate real human behavior distributions compared to human annotators in multi-turn interactions?
- Basis in paper: [explicit] The authors state, "Our paper does not present experimental results; instead, we outline evaluation designs... We compare session-level interaction statistics between the simulated user and the human annotator as a sanity check."
- Why unresolved: The paper presents a framework and blueprint but lacks quantitative results validating the simulator's fidelity against ground-truth human data.
- What evidence would resolve it: Statistical correlation scores (e.g., KL-divergence) between simulated trajectories and human annotator behaviors in identical recommendation scenarios.

### Open Question 2
- Question: Which content representation architecture (text-based, multimodal, or semantic ID) optimally balances reasoning capability, simulation fidelity, and computational efficiency?
- Basis in paper: [inferred] Section 3.1 presents three modeling alternatives with distinct trade-offs: text lacks nuance, multimodal models impose "significant context and efficiency challenges," and semantic IDs require "periodic retraining."
- Why unresolved: The paper describes these as interchangeable options depending on the backbone (e.g., Llama4 vs. Qwen3-Omni) but does not provide comparative analysis on which yields better agent training outcomes.
- What evidence would resolve it: A benchmark comparison of agent performance (e.g., retention rates) when trained exclusively on simulators using each representation type.

### Open Question 3
- Question: How can agentic recommenders reliably disambiguate vague or open-ended user instructions (e.g., "I'm bored") into actionable retrieval strategies?
- Basis in paper: [explicit] The authors note, "Users may provide vague or open-ended feedback... which poses a greater challenge than responding to specific requests... We envision a reasoning-intensive retrieval model... to infer user intents."
- Why unresolved: While the paper identifies the problem and proposes a reasoning-based solution, it stops short of detailing the mechanism for mapping abstract instructions to concrete configuration changes.
- What evidence would resolve it: Evaluation of an intent-alignment module showing high success rates in converting ambiguous natural language feedback into relevant recommendation list updates.

## Limitations

- The framework's validity depends on LLM-based simulators accurately modeling real user behavior and disengagement patterns without direct validation against real user data
- Semantic ID approach requires continual updates as new content is introduced, and its effectiveness compared to multimodal representations remains unvalidated
- Reward design based on trajectory-level statistics may not capture fine-grained user satisfaction signals, potentially leading to suboptimal long-term engagement strategies

## Confidence

- **High confidence:** The dual-view architecture design and multi-turn interaction loop are well-specified and implementable
- **Medium confidence:** The theoretical framework for instruction-following and reasoning-intensive retrieval is sound, but practical implementation challenges remain
- **Low confidence:** The effectiveness of semantic ID representations and the simulator's ability to generate realistic user instructions that generalize to real users

## Next Checks

1. Validate simulator behavioral fidelity by comparing simulated interaction sequences against ground-truth user data from existing RecSys datasets, measuring alignment using metrics like NDCG and recall
2. Implement an ablation study comparing instruction-following performance with and without user instructions, measuring changes in session length, click-through rate, and user satisfaction scores
3. Conduct a content representation comparison experiment running identical simulations across text-only, multimodal, and semantic ID settings to identify representation bottlenecks and their impact on simulator reasoning quality and reward trajectories