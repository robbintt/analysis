---
ver: rpa2
title: Canonical Autoregressive Generation
arxiv_id: '2506.06446'
source_url: https://arxiv.org/abs/2506.06446
tags:
- token
- tokenization
- canonical
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can generate the same output string with
  different tokenizations, leading to inconsistent pricing under pay-per-token models.
  This is particularly prevalent for non-English languages.
---

# Canonical Autoregressive Generation

## Quick Facts
- arXiv ID: 2506.06446
- Source URL: https://arxiv.org/abs/2506.06446
- Reference count: 40
- Large language models can generate identical strings with different tokenizations, leading to inconsistent pricing under pay-per-token models.

## Executive Summary
Large language models exhibit tokenization multiplicity, where identical output strings can be generated with different tokenizations, leading to arbitrary price variations under pay-per-token pricing models. This is particularly prevalent for non-English languages. The authors propose canonical generation, a method that restricts models to only generate canonical tokenizations. They introduce an efficient sampling algorithm based on the Gumbel-Max trick that enforces this constraint while maintaining comparable generation quality and runtime efficiency.

## Method Summary
Canonical generation modifies standard autoregressive sampling by enforcing that only canonical tokenizations are generated. The method uses Gumbel-Max sampling to efficiently sample from the constrained distribution: at each step, Gumbel noise is added to log-probabilities, tokens are sorted by perturbed scores, and the first token producing a canonical sequence is selected. This avoids the computational cost of checking canonicity for low-probability tokens while provably generating token sequences closer to the true training distribution.

## Key Results
- Canonical generation provably reduces KL-divergence between generated and training distributions compared to standard sampling
- Quality scores (xCOMET) and accuracy remain comparable to standard generation across translation and math problem-solving tasks
- Tokenization multiplicity can cause up to 15% price variation for identical outputs under pay-per-token pricing models

## Why This Works (Mechanism)

### Mechanism 1
Standard BPE, Wordpiece, and Unigram tokenizers are "non-recovering," meaning if a partial token sequence is non-canonical, it cannot be extended into a canonical sequence. The underlying merge rules (BPE), likelihood maximization (Unigram), or greedy matching (Wordpiece) enforce strict ordering. Once a sub-optimal merge or token choice is made locally, the global optimum (canonical representation) becomes unreachable without backtracking. This property is proven for these tokenizers in Theorem 2 and Appendix B.

### Mechanism 2
Canonical sampling reduces the KL-divergence between the generated token distribution and the training distribution by masking non-canonical tokens at each step and redistributing their probability mass proportionally to remaining canonical tokens. Since the training distribution assigns zero probability to non-canonical sequences, removing these paths brings the inference distribution closer to the training distribution. Theorem 3 formally proves KL(p, d̃) < KL(p, d) under the assumption that non-canonical paths exist.

### Mechanism 3
The Gumbel-Max trick allows for efficient sampling from the constrained "canonical" distribution without calculating normalization constants for the entire vocabulary. By adding Gumbel noise to log-probabilities and sorting, the algorithm simulates sampling from the full distribution, then iterates through ranked candidates and selects the first token resulting in a canonical sequence. This avoids checking canonicity for low-probability tokens and terminates early when high-probability tokens are canonical.

## Foundational Learning

- **Tokenization Multiplicity**
  - Why needed here: This is the core problem the paper solves. It explains why identical output strings can have different costs (token counts) and different internal representations.
  - Quick check question: If a model generates "apple" as ["ap", "ple"] vs ["apple"], do these map to the same string? Which one is likely "canonical"?

- **Autoregressive Sampling**
  - Why needed here: The paper modifies the standard next-token prediction loop. Understanding that generation is sequential (s_{t+1} ← s_t + t) is vital to seeing why a "non-recovering" tokenizer error is fatal to the sequence.
  - Quick check question: In standard sampling, does the choice of token at step t constrain the available tokens at step t+1 strictly by syntax, or by probability?

- **KL-Divergence**
  - Why needed here: This metric quantifies the "distance" between the model's behavior and the training data. The paper's theoretical contribution relies on proving this distance decreases.
  - Quick check question: If distribution A has support where distribution B has zero probability, does that contribute to the KL-divergence?

## Architecture Onboarding

- **Component map:**
  - Logit Processor -> Sorter -> Canonicity Checker -> Selector

- **Critical path:**
  1. Model outputs logits
  2. Add Gumbel noise
  3. Sort vocabulary
  4. (Loop) Iterate through top tokens → Check Canonicity
  5. Return token; append to context

- **Design tradeoffs:**
  - Latency vs. Fairness: The canonicity check adds latency (encoding/decoding check). Algorithm 1 minimizes this, but it is strictly slower than greedy sampling.
  - Latency vs. Language: Non-English languages may trigger more "non-canonical" high-probability tokens, causing deeper search loops.

- **Failure signatures:**
  - High Latency: The search loop iterates deep into the vocabulary (low probability tokens) because the model is "hallucinating" non-canonical tokens.
  - Runtime Error: If no canonical token exists, the loop might exhaust the vocabulary (theoretical edge case).

- **First 3 experiments:**
  1. Baseline Measurement: Generate 100 samples for a fixed multilingual prompt using standard sampling. Measure the standard deviation of token counts for identical output strings to verify "price variation."
  2. A/B Latency Test: Implement Algorithm 1 vs. naive rejection sampling. Measure the average number of is_canonical calls per generated token to validate the efficiency claim.
  3. Distribution Drift: Compare the quality (e.g., BLEU/COMET scores) of Canonical vs. Standard generation on a translation task. Verify if the "performance is comparable" claim holds.

## Open Questions the Paper Calls Out

- **What specific properties must a tokenizer satisfy to ensure that subsequences of canonical token sequences are also canonical (the non-recovering property)?**
  - The authors state this would be interesting to better understand, noting they currently only prove this for BPE, Unigram, and Wordpiece. A formal theoretical characterization of tokenizer properties that guarantees non-recoverability would resolve this.

- **Can global strategies for redistributing probability mass from non-canonical tokens outperform the proposed local, proportional redistribution in generation quality?**
  - The authors suggest investigating global strategies beyond next-token sampling, as their local redistribution method occasionally restricts the sampling space to low probability paths.

- **Do specialized multilingual practices, such as language-specific fine-tuning or distinct tokenizers, reduce the prevalence of tokenization multiplicity in non-English languages?**
  - The paper identifies tokenization multiplicity as particularly prevalent in non-English languages but asks if commonly used practices to improve multilingual language generation may reduce this prevalence.

## Limitations

- The efficiency claims for Gumbel-Max sampling assume high-probability tokens are more likely to be canonical, which may not hold uniformly across all languages and domains
- Performance degradation occurs for Turkish and Swahili, indicating the approach may not generalize well to all non-English languages
- The evaluation focuses primarily on translation and math problem solving tasks, without extensive testing across diverse NLP tasks

## Confidence

**High Confidence:** The theoretical framework is sound and proofs are mathematically rigorous. The problem of tokenization multiplicity is well-established.

**Medium Confidence:** Efficiency claims are supported by theoretical arguments and limited empirical evidence, but comprehensive runtime benchmarks are missing.

**Low Confidence:** Generalizability across diverse languages and tasks is not thoroughly established, with evaluation limited to six languages and three task types.

## Next Checks

1. **Runtime Benchmarking Across Languages and Hardware:** Implement canonical generation across multiple hardware configurations and measure actual latency overhead for different languages, particularly focusing on Turkish and Swahili where performance degradation was observed.

2. **Cross-Task Quality Evaluation:** Test canonical generation across diverse NLP tasks including summarization, question answering, code generation, and creative writing using multiple quality metrics.

3. **Tokenizer Robustness Analysis:** Test the non-recovering property and canonical generation performance on alternative tokenizer architectures including SentencePiece variants and newer tokenizer designs.