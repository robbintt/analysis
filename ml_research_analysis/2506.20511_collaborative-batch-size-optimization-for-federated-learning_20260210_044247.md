---
ver: rpa2
title: Collaborative Batch Size Optimization for Federated Learning
arxiv_id: '2506.20511'
source_url: https://arxiv.org/abs/2506.20511
tags:
- batch
- size
- learning
- training
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing batch sizes in
  federated learning (FL) to improve training efficiency while avoiding hardware failures.
  In FL, participants train models locally with fixed hardware constraints, but without
  information exchange, improper batch size configurations can hinder performance
  or cause training failures.
---

# Collaborative Batch Size Optimization for Federated Learning

## Quick Facts
- arXiv ID: 2506.20511
- Source URL: https://arxiv.org/abs/2506.20511
- Reference count: 21
- Authors: Arno Geimer; Karthick Panner Selvam; Beltran Fiz Pontiveros
- One-line primary result: Random search-based method determines optimal shared batch size in 3 rounds, achieving 15.68× speedup on MNIST while maintaining 96.09% accuracy

## Executive Summary
This paper addresses the challenge of optimizing batch sizes in federated learning (FL) where participants have fixed hardware constraints but no information exchange. The authors propose a randomized binary search method (RASBA) that automatically determines the maximum achievable shared batch size across all participants within the first few training rounds. By having clients sample random batch sizes in parallel and adjusting bounds based on training success or failure, the method efficiently converges to an optimal configuration without manual tuning. Experiments demonstrate significant computational time savings while maintaining competitive accuracy on standard FL benchmarks.

## Method Summary
The proposed method employs a randomized binary search approach to find the optimal shared batch size across heterogeneous clients in federated learning. Initially, minimum and maximum batch size bounds are set, then clients sample random batch sizes within these bounds in parallel during training rounds. Successful training increases the minimum bound while failures decrease the maximum bound, iteratively narrowing the search space. An error handling mechanism ensures training continuity by having a fraction of clients use the minimum batch size during the search phase. The method converges to the optimal batch size within 3 rounds, significantly reducing computation time while maintaining model accuracy close to the theoretical optimum.

## Key Results
- RASBA finds optimal shared batch size within 3 training rounds
- Achieves 15.68× speedup on MNIST with batch size 256 compared to smaller fixed batch sizes
- Maintains 96.09% accuracy on MNIST and 73.42% on CIFAR-10 with minimal computation time loss
- Successfully prevents hardware failures during batch size optimization phase

## Why This Works (Mechanism)
The method works by leveraging parallel sampling and adaptive bound adjustment to efficiently search the batch size space. Clients simultaneously test different batch sizes, providing rapid feedback about the feasibility of various configurations. The binary search mechanism ensures logarithmic convergence time by systematically eliminating infeasible batch sizes. The error handling component maintains training stability by ensuring a minimum performance floor during the search phase. This combination allows the system to quickly identify the maximum feasible batch size that all participants can support while preventing training failures that would otherwise halt the federated learning process.

## Foundational Learning
- Federated Learning Fundamentals: Why needed - Understanding the decentralized nature and constraints of FL; Quick check - Can you explain how FL differs from centralized training?
- Batch Size Impact on Training: Why needed - Batch size affects both training stability and convergence speed; Quick check - What happens when batch size exceeds GPU memory?
- Randomized Search Algorithms: Why needed - The method uses randomization to parallelize the search process; Quick check - How does randomized search differ from exhaustive search?
- Binary Search Principles: Why needed - The convergence mechanism relies on binary search logic; Quick check - Can you trace the bound updates through multiple rounds?
- Error Handling in Distributed Systems: Why needed - Maintaining training continuity during optimization is critical; Quick check - What are common failure modes in FL and how are they typically handled?

## Architecture Onboarding

**Component Map:** Server -> Client Pool -> Local Training -> Feedback Loop -> Server Update

**Critical Path:** Server initialization → Batch size sampling → Parallel local training → Success/failure reporting → Bound adjustment → Repeat until convergence

**Design Tradeoffs:** 
- Parallel sampling vs sequential search: Parallelism accelerates convergence but increases resource usage
- Aggressive vs conservative bound adjustment: Faster convergence vs stability during search
- Error handling overhead vs training continuity: Small accuracy cost for preventing complete failures

**Failure Signatures:** 
- Persistent training failures across multiple rounds indicate insufficient minimum bound
- No bound movement suggests all sampled batch sizes are equally successful or failing
- Slow convergence may indicate poor initial bound selection or highly heterogeneous client capabilities

**First Experiments:**
1. Test convergence speed with different initial bound ranges on homogeneous hardware
2. Measure accuracy degradation when error handling is disabled
3. Evaluate performance with varying fractions of clients using minimum batch size during search

## Open Questions the Paper Calls Out
The authors acknowledge that their evaluation focuses on homogeneous hardware settings and suggest future work on heterogeneous federations with multiple hardware tiers. They note that real-world federated learning deployments involve participants with highly heterogeneous computational resources and network conditions, which could affect the method's performance. The study does not investigate the impact of varying local epoch counts or different model architectures beyond MobileNet-v3 and ResNet18.

## Limitations
- Evaluation limited to small-scale benchmarks (MNIST and CIFAR-10) that may not represent real-world complexity
- Homogeneous hardware settings don't capture highly heterogeneous computational resources in real deployments
- Error handling mechanism may introduce bias by having clients train with minimum batch sizes during search
- Limited investigation of impact on different model architectures beyond two specific examples

## Confidence

**Batch size determination method effectiveness:** High
**Performance improvements on tested datasets:** Medium
**Generalization to heterogeneous real-world deployments:** Low
**Impact of error handling on final model quality:** Medium

## Next Checks
1. Test the method on heterogeneous federations with at least three distinct hardware tiers and evaluate performance degradation compared to the homogeneous case
2. Conduct ablation studies to quantify the impact of the error handling mechanism on final model accuracy and convergence speed
3. Evaluate the approach on larger-scale datasets (e.g., ImageNet) and more diverse model architectures (e.g., transformers, language models) to assess scalability