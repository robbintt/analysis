---
ver: rpa2
title: Towards Minimal Fine-Tuning of VLMs
arxiv_id: '2512.19219'
source_url: https://arxiv.org/abs/2512.19219
tags:
- head
- heads
- image-lora
- selection
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Image-LoRA is a parameter-efficient fine-tuning method for vision-language
  models that restricts adaptation to visual tokens and a subset of attention heads.
  The approach applies low-rank adaptation only to the value path of attention layers
  within the visual-token span, reducing adapter-only training FLOPs proportionally
  to the visual-token fraction.
---

# Towards Minimal Fine-Tuning of VLMs

## Quick Facts
- arXiv ID: 2512.19219
- Source URL: https://arxiv.org/abs/2512.19219
- Reference count: 40
- Image-LoRA matches or approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs

## Executive Summary
Image-LoRA is a parameter-efficient fine-tuning method for vision-language models that restricts adaptation to visual tokens and a subset of attention heads. The approach applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs proportionally to the visual-token fraction. Head selection is performed using influence scores estimated with a rank-1 Image-LoRA, and updates are stabilized via selection-size normalization. Across screen-centric grounding and referring benchmarks with varying text-to-image ratios, Image-LoRA matched or closely approached standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method preserved pure-text reasoning performance, as demonstrated on GSM8K.

## Method Summary
Image-LoRA modifies standard LoRA by applying a diagonal mask matrix that restricts adaptation to visual-token spans only. For each self-attention head, it modifies only the value projection weights, applying changes only to visual tokens. Head selection is performed using a first-order Taylor approximation with squared gradient norms computed via a rank-1 probe. The method uses a shared low-rank matrix across layers with per-head rank-1 matrices, and applies normalization based on the number of selected heads per layer.

## Key Results
- Matched standard LoRA accuracy on ScreenSpot-Pro (37.35% vs 37.82%) while using fewer trainable parameters
- Preserved pure-text reasoning performance on GSM8K (25.55% vs 25.55%)
- Outperformed random head selection on multiple grounding benchmarks
- Reduced adapter-only training FLOPs proportionally to visual-token fraction

## Why This Works (Mechanism)

### Mechanism 1: Visual-Token Only Adaptation
Restricting low-rank adaptation to visual-token spans preserves pure-text reasoning while enabling visual grounding. Image-LoRA applies a diagonal mask matrix where entries are 1 only for visual token indices, ensuring the adapter modifies value representations for visual tokens only. For text-only inputs, the adapter is never activated, guaranteeing zero interference with language pathways.

### Mechanism 2: Value-Only Projection Selectivity
Adapting only the Value projection is sufficient for visual grounding tasks. In self-attention, modifying value representations for visual tokens changes "what content is provided" without altering the attention distribution. This is more targeted than modifying keys (which changes softmax distribution) or queries/outputs (which don't affect text token outputs for causal masks).

### Mechanism 3: Influence-Guided Head Selection
A first-order Taylor approximation using squared gradient norm identifies a critical subset of attention heads. Instead of expensive ablation, it estimates importance using gradients from a rank-1 Image-LoRA, prioritizing heads with high gradient norms. This proxy approximates how much updating a head could reduce loss.

## Foundational Learning

**Concept: Transformer Attention Mechanism (Query, Key, Value)**
- Why needed: The method is built on selectively adapting the Value projection and understanding how attention weights route information
- Quick check: If you modify a token's Key vector, does it change the *content* that token contributes, or *which* tokens attend to it?

**Concept: Low-Rank Adaptation (LoRA)**
- Why needed: Image-LoRA modifies standard LoRA with masking, head-specific matrices, and normalization
- Quick check: For a 1024×1024 weight matrix with rank r=8, what are the dimensions of A and B?

**Concept: VLM Tokenization**
- Why needed: The method hinges on distinguishing "visual tokens" from "text tokens"
- Quick check: How does a VLM typically handle a 224×224 image differently from text before transformer layers?

## Architecture Onboarding

**Component map:**
Image-LoRA Adapter -> Head Selector -> Selection-Size Normalizer -> VLM

**Critical path:**
1. Run head selection on probe dataset to determine H_chosen
2. Insert Image-LoRA adapters (shared A, zero-initialized B^(h))
3. Forward: Apply ΔW_V only for tokens t ∈ I_v in selected heads
4. Backward: Gradients flow only through selected heads/visual tokens

**Design tradeoffs:**
- Efficiency vs. Generality: Visual-only restriction guarantees text-preservation but may limit deep text-visual interaction
- Head Budget vs. Accuracy: Too few heads drops accuracy; default is ~1 head/layer
- Batching Complexity: Varying I_v per sample complicates efficient batching

**Failure signatures:**
- Catastrophic Forgetting: If I_v is accidentally set to all tokens
- Degraded Grounding: If head selection fails on unrepresentative probe sets
- Instability: If normalization is omitted, layers with many selected heads over-power updates

**First 3 experiments:**
1. **Smoke Test:** On a small VLM, verify adapter activates only on visual tokens and text-token gradients are zero
2. **A/B Head Selection:** Compare implementation vs. random selection on a tiny dataset; confirm non-uniform influence scores
3. **End-to-End Sanity Check:** Fine-tune on RefCOCO comparing Base, Std-LoRA, and Image-LoRA; verify GSM8K preservation

## Open Questions the Paper Calls Out

**Open Question 1:** How does Image-LoRA reshape attention patterns across heads and layers compared to Standard LoRA, and how do these dynamics change during training? The paper focuses on efficiency and accuracy outcomes but lacks a mechanistic interpretability study of resulting attention maps or their evolution.

**Open Question 2:** How do the importance and optimality of selected attention heads evolve over the course of training, particularly in web-scale settings? The current method uses static head selection based on a one-shot influence score, but the paper asks whether different subsets may be optimal at different training stages.

**Open Question 3:** Can alternative head-selection criteria lead to substantially different accuracy-efficiency trade-offs compared to the proposed rank-1 gradient norm method? The study isolates a specific first-order approximation but doesn't benchmark against alternative methods like second-order information or learning the mask.

**Open Question 4:** What kernel-level and hardware-level optimizations are required to make the dynamic visual-span indexing of Image-LoRA competitive in throughput with Standard LoRA during inference? The paper identifies implementation efficiency as a limitation and admits current implementation lacks necessary optimizations.

## Limitations

- Generalization across VLM architectures with different token handling and cross-modal attention designs
- Head selection sensitivity to probe dataset and initialization
- Limited analysis of cross-attention layers and complex multimodal reasoning
- Practical efficiency gains not fully validated in full training setup

## Confidence

**High Confidence:** Visual-token only adaptation preserves pure-text reasoning (GSM8K results), Value-only adaptation is sufficient for grounding tasks (ScreenSpot-Pro ablation), Influence-guided head selection outperforms random selection (Table 2)

**Medium Confidence:** The decoupling assumption between visual and text reasoning, One head per layer is sufficient for most tasks, Gradient norms at initialization correlate with long-term head utility

**Low Confidence:** Generalizability to VLMs with different architectural choices, Impact on cross-attention and complex multimodal reasoning, Practical efficiency gains in full training setup

## Next Checks

1. **Cross-Architecture Validation:** Test Image-LoRA on VLMs with different architectural designs and tasks requiring complex visual-text interaction. Measure both performance and whether visual-text decoupling holds.

2. **Cross-Attention Analysis:** Extend the method to explicitly analyze and adapt cross-attention layers. Compare performance when adapting only self-attention vs. including cross-attention.

3. **Probe Dataset Sensitivity:** Systematically vary the probe dataset and initialization for head selection. Test whether the same heads are consistently selected across different datasets and whether performance degrades significantly with different selections.