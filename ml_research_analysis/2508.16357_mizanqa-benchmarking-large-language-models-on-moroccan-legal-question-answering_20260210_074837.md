---
ver: rpa2
title: 'MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering'
arxiv_id: '2508.16357'
source_url: https://arxiv.org/abs/2508.16357
tags:
- alefisolated
- laminitial
- legal
- aleffinal
- yehmedial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MizanQA introduces a novel Arabic legal question-answering benchmark
  focused on Moroccan law, addressing the challenge of evaluating large language models
  in specialized, low-resource legal domains. The dataset comprises over 1,700 multiple-choice
  questions reflecting the linguistic and legal complexity of Moroccan law, including
  multi-answer formats and culturally specific terminology.
---

# MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering

## Quick Facts
- **arXiv ID:** 2508.16357
- **Source URL:** https://arxiv.org/abs/2508.16357
- **Reference count:** 4
- **Primary result:** Novel Arabic legal QA benchmark with 1,776 Moroccan law questions; leading LLMs achieve only 58% accuracy

## Executive Summary
MizanQA introduces the first Arabic legal question-answering benchmark focused on Moroccan law, addressing the critical gap in evaluating large language models on specialized, low-resource legal domains. The dataset comprises over 1,700 multiple-choice questions reflecting the linguistic and legal complexity of Moroccan law, including multi-answer formats and culturally specific terminology. The benchmark is designed to test both legal reasoning and cultural competency in Arabic contexts, where existing datasets fail to capture the nuanced requirements of Moroccan legal systems.

Experiments with leading multilingual and Arabic-focused LLMs reveal substantial performance gaps, with top models achieving only up to 58% accuracy on the benchmark. The authors propose new evaluation metrics to handle multiple correct answers and assess confidence calibration, finding that even state-of-the-art models struggle with the legal reasoning and cultural specificity required. These results highlight the need for domain-specific, culturally grounded LLM development for legal reasoning in Arabic contexts.

## Method Summary
The MizanQA benchmark comprises 1,776 Arabic multiple-choice questions across 14 legal categories, designed to evaluate LLMs on Moroccan legal knowledge. Questions feature 2-12 answer options with 1-10 possible correct answers per question, reflecting the complexity of real legal scenarios. The benchmark is available at https://huggingface.co/datasets/adlbh/MizanQA-v0. Zero-shot prompting via APIs was used to evaluate seven models: Allam-2 (7b), Gemini-1.5-flash, Gemini-2.0-flash, Llama-3.3 (70b), Llama-4-maverick (17b), and Llama-4-scout (17b), using Groq for non-Gemini models and Gemini API for Google models.

Four evaluation metrics were proposed: Strict Accuracy (ACC), F1-like variants (α=1,2), Partial Match Penalized Accuracy (PMPA with β=1,0.5), and Expected Calibration Error (ECEopt, ECEset). The prompt format required models to output confidence scores and selected options in the format `[("Confidence Score", "Option 1"), ...]`. The evaluation framework handles the multi-answer nature of questions and assesses both accuracy and confidence calibration.

## Key Results
- Leading LLMs achieve only 58% accuracy on MizanQA, demonstrating significant performance gaps in Moroccan legal reasoning
- Multi-answer questions pose substantial challenges, with models frequently failing to identify all correct options
- Confidence calibration metrics reveal systematic miscalibration across all evaluated models
- Cultural and linguistic specificity of Moroccan law creates barriers that general-purpose LLMs cannot overcome

## Why This Works (Mechanism)
None

## Foundational Learning
- **Arabic legal terminology:** Understanding Moroccan legal vocabulary is essential for interpreting questions correctly; quick check: verify all Arabic terms have accurate legal meanings in context
- **Multi-answer MCQ evaluation:** Standard accuracy metrics fail when multiple answers are correct; quick check: implement PMPA and verify it handles partial credit appropriately
- **Confidence calibration:** Models' probability estimates must be evaluated for reliability; quick check: compute ECE with different bin counts to verify stability
- **Zero-shot prompting for specialized domains:** No fine-tuning is used, testing general LLM capabilities; quick check: ensure prompts are domain-appropriate and clear
- **Cultural competency in NLP:** Models must understand culturally-specific legal concepts; quick check: validate that cultural references are correctly interpreted
- **Legal reasoning evaluation:** Assessing logical deduction in legal contexts requires specialized metrics; quick check: verify that legal reasoning steps are captured in evaluation

## Architecture Onboarding

**Component Map:**
Dataset (1,776 questions) -> API Models (7 LLMs) -> Prompt Processing -> Output Parsing -> Metric Calculation (4 metrics) -> Results Analysis

**Critical Path:**
Dataset loading → API calls with prompts → Output parsing → Metric computation → Results aggregation

**Design Tradeoffs:**
- Zero-shot approach tests general capabilities but may underperform fine-tuned models
- Multiple metrics capture different aspects but increase complexity
- Arabic-specific content limits broader applicability but ensures domain authenticity

**Failure Signatures:**
- Models outputting single options when multiple correct answers exist
- Malformed output parsing due to format variations
- Confidence scores outside expected [0,1] range
- API rate limiting affecting complete dataset evaluation

**First Experiments:**
1. Test parsing robustness with 100 sample outputs across all models
2. Verify ECE calculation stability with different bin counts (5, 10, 20)
3. Assess impact of temperature variation (0.0, 0.3, 0.7) on multi-option selection

## Open Questions the Paper Calls Out
None

## Limitations
- No specified generation hyperparameters (temperature, top_p, max_tokens) affecting reproducibility
- ECE calculation depends on unspecified number of bins (M), introducing potential variance
- Confidence score normalization procedure unclear (prompt uses 1-100, equations use [0,1])
- Arabic-specific terminology may create translation challenges for non-Moroccan implementers

## Confidence
- **High confidence:** Dataset construction (1,776 questions, 14 categories), evaluation metric definitions, API selection for each model, overall experimental design and research question validity
- **Medium confidence:** Model performance trends and relative rankings, multi-answer handling challenges, ECE and PMPA metric implementations
- **Low confidence:** Absolute metric values (especially confidence scores and ECE), exact numerical performance comparisons between models, calibration curve interpretations

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically test different temperature settings (0.0, 0.3, 0.7) and document impact on multi-option selection rates and confidence distributions across all models
2. **ECE Bin Sensitivity:** Reproduce ECE calculations using 5, 10, and 20 bins; compare variance in calibration error scores to establish stability of reported values
3. **Output Format Robustness:** Implement comprehensive regex parsing with detailed logging of parse failures and format deviations; verify that all model outputs can be correctly mapped to the required `[("Confidence Score", "Option"), ...]` structure across all 1,776 questions