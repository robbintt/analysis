---
ver: rpa2
title: 'UserBench: An Interactive Gym Environment for User-Centric Agents'
arxiv_id: '2507.22034'
source_url: https://arxiv.org/abs/2507.22034
tags:
- user
- preferences
- search
- preference
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "UserBench is a new benchmark for evaluating large language models\
  \ (LLMs) as collaborative agents in user-centric tasks. It focuses on agents\u2019\
  \ ability to uncover and act on evolving, implicit user preferences during multi-turn\
  \ interactions, such as travel planning."
---

# UserBench: An Interactive Gym Environment for User-Centric Agents

## Quick Facts
- arXiv ID: 2507.22034
- Source URL: https://arxiv.org/abs/2507.22034
- Reference count: 34
- Primary result: New benchmark evaluating LLMs as collaborative agents in user-centric tasks with preference elicitation and alignment

## Executive Summary
UserBench introduces a benchmark for evaluating large language models as collaborative agents in user-centric tasks where user preferences are implicit and evolve through interaction. The benchmark simulates users with underspecified goals that gradually reveal preferences over multi-turn conversations, requiring agents to ask clarifying questions and make informed decisions. Experiments with 12 models show that while agents excel at tool use and search functionality, they struggle significantly with preference elicitation and alignment, achieving only 20% full alignment and eliciting less than 30% of preferences actively.

## Method Summary
UserBench provides a gym environment for training and evaluating user-centric agents through simulated interactions where users have underspecified goals that unfold over multiple turns. The benchmark focuses on travel planning tasks where agents must use tool-augmented search capabilities and customizable settings to navigate user preferences. Agents interact with simulated users who gradually reveal their preferences, requiring the agent to balance between gathering information through questions and making decisions that align with revealed preferences. The evaluation measures both preference elicitation (asking clarifying questions) and preference alignment (making decisions that match user preferences).

## Key Results
- Agents achieve only 20% full alignment with user preferences in simulated interactions
- Agents elicit less than 30% of user preferences actively through questioning
- Performance drops by 40% in single-choice settings, highlighting challenges in optimal decision-making
- While proficient in tool use and search, agents struggle to effectively collaborate with users

## Why This Works (Mechanism)
UserBench works by creating a controlled simulation environment that captures the core challenge of user-agent collaboration: discovering and acting on implicit, evolving preferences through multi-turn interaction. The mechanism exposes the gap between technical capability (tool use, search) and collaborative intelligence (understanding user intent, asking the right questions).

## Foundational Learning
- **Preference Elicitation**: Agents must learn to ask clarifying questions to uncover user preferences - needed because users often have implicit or unarticulated needs; quick check: count of clarifying questions per interaction
- **Preference Alignment**: Agents must map discovered preferences to concrete decisions - needed because understanding preferences is insufficient without actionable decisions; quick check: decision accuracy vs ground truth preferences
- **Multi-turn Dialogue Management**: Agents must balance information gathering with decision-making over conversation turns - needed because premature decisions or excessive questioning both harm outcomes; quick check: turn count vs alignment rate
- **Tool-Augmented Search**: Agents must integrate external tools for information gathering - needed because user preferences often require real-world data lookup; quick check: tool usage frequency and relevance
- **Simulated User Modeling**: The benchmark uses predefined user response patterns - needed to create reproducible evaluation conditions; quick check: consistency of user responses across runs

## Architecture Onboarding

Component Map: User Agent -> Dialogue Manager -> Tool-Augmented Search -> Decision Engine -> User Simulator

Critical Path: Dialogue Manager (clarifying questions) -> Tool-Augmented Search (preference discovery) -> Decision Engine (preference alignment) -> User Simulator (feedback)

Design Tradeoffs: Balancing question asking vs decision making; depth of preference elicitation vs conversation length; tool usage vs direct inference

Failure Signatures: Low preference elicitation rate (agents don't ask enough questions); poor preference alignment (agents make wrong decisions despite having information); excessive tool usage (agents over-rely on external search)

First Experiments: 1) Baseline with no clarifying questions; 2) Maximum question asking strategy; 3) Tool-only approach without preference tracking

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on simulated users rather than real human interactions, potentially missing real-world complexity
- Focuses primarily on travel planning domain, limiting generalizability to other user-centric tasks
- Assumes access to specific external tools and APIs that may not reflect real-world deployment constraints

## Confidence

The claim that UserBench exposes a gap between task execution and effective user collaboration has High confidence, supported by quantitative results showing performance drops and low alignment rates. The assertion that agents struggle to elicit and align with user intent also has High confidence, given experimental data showing less than 30% active preference elicitation. The observation that performance drops 40% in single-choice settings has Medium confidence, as this result depends on specific task design and may vary with different evaluation scenarios.

## Next Checks
1. Conduct a user study with real human participants to validate whether simulation-based results translate to actual user-agent interactions, particularly measuring preference elicitation and alignment accuracy in real-world settings.

2. Test the benchmark across multiple domains beyond travel planning (e.g., healthcare decisions, financial planning, education) to assess generalizability of observed performance gaps and alignment challenges.

3. Evaluate whether providing additional training data focused on preference elicitation and clarification questions improves the 20% full alignment rate, and determine if this represents a fundamental limitation of current models or a training data deficiency.