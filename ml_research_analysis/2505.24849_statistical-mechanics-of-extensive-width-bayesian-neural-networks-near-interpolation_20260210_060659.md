---
ver: rpa2
title: Statistical mechanics of extensive-width Bayesian neural networks near interpolation
arxiv_id: '2505.24849'
source_url: https://arxiv.org/abs/2505.24849
tags:
- gaussian
- neural
- networks
- error
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work provides a theoretical framework for analysing the generalisation\
  \ performance of two-layer Bayesian neural networks in the extensive-width regime\
  \ near interpolation. The key method combines replica theory from statistical physics\
  \ with spherical/Harish-Chandra\u2013Itzykson\u2013Zuber integrals to derive tractable\
  \ predictions for the Bayes-optimal mean-square generalisation error."
---

# Statistical mechanics of extensive-width Bayesian neural networks near interpolation

## Quick Facts
- arXiv ID: 2505.24849
- Source URL: https://arxiv.org/abs/2505.24849
- Authors: Jean Barbier; Francesco Camilli; Minh-Toan Nguyen; Mauro Pastore; Rudy Skerk
- Reference count: 40
- Primary result: Derives Bayes-optimal generalisation error for two-layer Bayesian neural networks in extensive-width regime near interpolation using replica theory and spherical integrals

## Executive Summary
This work develops a statistical mechanics framework to analyse the generalisation performance of two-layer Bayesian neural networks when the hidden layer width scales as k = Θ(d²) with input dimension d. The theory combines replica methods with spherical/Harish-Chandra–Itzykson–Zuber integrals to derive tractable predictions for the Bayes-optimal mean-square generalisation error. The analysis reveals a rich phenomenology with specialisation transitions where student weights suddenly align with teacher weights once sufficient data becomes available.

The framework handles both homogeneous readouts (single readout per neuron) and non-homogeneous readouts (multiple readouts), showing that homogeneous readouts exhibit a single specialisation transition while non-homogeneous readouts display an infinite sequence of transitions with features having larger readouts learned first. The theory predicts that for Gaussian priors, the pre-specialisation phase is universal—independent of the weight prior—while for non-Gaussian priors, the posterior remains rotationally invariant until specialisation occurs.

## Method Summary
The method uses replica theory to analyse the posterior distribution over network weights given training data. The key insight is that when k = Θ(d²), the second-order tensor S₂ = Wᵀ(v)W/√k becomes the dominant order parameter, while higher-order tensors can be approximated as effective noise. The free entropy is computed using spherical integrals (HCIZ) to handle the tensor constraints, yielding a replica-symmetric potential that can be extremised to obtain order parameters and generalisation error. The theory is validated through extensive numerical experiments using Hamiltonian Monte Carlo, ADAM, and a GAMP-RIE algorithm for efficient prediction.

## Key Results
- Specialisation transitions occur when α exceeds critical thresholds αsp, with student weights suddenly aligning with teacher weights
- For homogeneous readouts: single specialisation transition when model learns to align weights with teacher
- For non-homogeneous readouts: infinite sequence of transitions with features having larger readouts learned first
- Universality in pre-specialisation phase: posterior depends only on second-order statistics regardless of weight prior
- GAMP-RIE algorithm matches universal branch performance but cannot find specialisation phase

## Why This Works (Mechanism)

### Mechanism 1: Order Parameter Condensation and Specialization Transition
Learning progresses through a sharp phase transition where student weights suddenly align with teacher weights once the sample rate α exceeds a critical threshold αsp. At low α, the posterior is dominated by an "effectively rotationally invariant" phase where only the second-order tensor S₀ᵀ(v)W₀ is learned, not W₀ itself. The prior over inner weights becomes irrelevant (universality). Beyond αsp, the symmetry breaks and Θ(k) hidden neurons align with the teacher's. Core assumption: The joint post-activations (λᵃ) across replicas are approximately Gaussian (Gaussian equivalence principle).

### Mechanism 2: Hierarchical Tensor Learning via Hermite Decomposition
The activation's Hermite expansion determines which statistical structures are learnable before vs. after specialization. The covariance Kᵃᵇ = Σₗ≥1 (μ²ₗ/ℓ!)Qᵃᵇₗ decomposes learning into tensor orders. With n = Θ(d²) data: order-0 (scalar mean) and order-1 (S₁ = Wᵀv/√k) tensors are perfectly recovered; order-2 tensor (S₂ = Wᵀ(v)W/√k) is the threshold challenge; order-ℓ≥3 tensors only contribute after specialization (otherwise appear as effective noise g(1)). Core assumption: Higher-order overlaps Qᵃᵇₗ for ℓ≥3 can be reduced to functions of the weight overlap Qᵂ(v) via diagonal dominance approximation.

### Mechanism 3: Moment-Matching Measure Relaxation
The high-dimensional entropic term becomes tractable by replacing the true constrained measure P(S₂ᵃ|Qᵂ) with a tilted generalized Wishart distribution. Instead of sampling weights W directly, the theory tracks second-order tensors S₂ᵃ = Wᵃᵀ(v)Wᵃ/√k. The simplified measure ẼP captures: (i) Wishart structure, (ii) replica coupling via exponential tilt with multiplier τ, (iii) moment-matching to enforce E[Tr S₂ᵃS₂ᵇ/d²] = Eᵥ∼Pᵥ[v²(Qᵂᵃᵇ(v))²] + γv̄². This enables exact treatment via HCIZ integrals. Core assumption: The prior Pᵂ over inner weights only enters the entropy through Qᵂ at leading order (universality in the universal phase).

## Foundational Learning

- **Bayes-Optimal Inference and the Nishimori Identity**
  - Why needed here: The entire theoretical framework relies on the student having correct priors (Pᵂ = P⁰ᵂ, Pₒᵤₜ = P⁰ₒᵤₜ), which gives the Nishimori identity: teacher-student overlaps equal student-student overlaps at equilibrium
  - Quick check question: If you change the student's prior to a Gaussian when the teacher uses binary weights, does the theory still apply? (Answer: No—the Nishimori identity breaks and replica symmetry may fail.)

- **Hermite Polynomial Expansions**
  - Why needed here: Activation functions are expanded σ(x) = Σₗ μₗ/ℓ! Heₗ(x) to separate contributions to the covariance Kᵃᵇ. The coefficients μₗ determine which tensor orders matter
  - Quick check question: For ReLU, which Hermite coefficients are nonzero? (Answer: μ₀ = 1/√(2π), μ₁ = 1/2, μ₂ = 1/√(2π), μ₄ = -1/√(2π) — infinitely many nonzero.)

- **Replica Symmetric Ansatz**
  - Why needed here: The replica trick requires taking s → 0⁺ limit of an s-replica system. RS ansatz assumes all off-diagonal overlaps equal (Qᵃᵇ = q for a≠b), which is rigorously justified for Bayes-optimal inference
  - Quick check question: What does replica symmetry breaking indicate in this context? (Answer: It would indicate algorithmic hardness or multiple separated phases in the posterior.)

## Architecture Onboarding

- Component map:
  ```
  Teacher Network: W₀ ∈ ℝᵏˣᵈ, v ∈ ℝᵏ → labels y via Pₒᵤₜ(·|λ₀)
  Student Network: Same architecture, learns posterior P(W|D)
  
  Theoretical Pipeline:
  [Hermite expansion] → [Covariance K structure] → [Replica free entropy fRS] → [Saddle point equations (76)] → [Generalization error εₒₚₜ]
  
  Key Order Parameters:
  - q₂: Student-teacher alignment on S₂ = Wᵀ(v)W/√k
  - Qᵂ(v): Per-readout-value weight overlap
  - τ: Moment-matching Lagrange multiplier (linked to matrix denoising MMSE)
  ```

- Critical path:
  1. Discretize readout distribution Pᵥ into bins {v}
  2. Solve saddle point equations (76) numerically (extremize fRS from Result 2.1)
  3. Extract Qᵂ*(v) to determine phase (universal if Qᵂ* ≈ 0; specialized if Qᵂ* > 0)
  4. Compute generalization error via Eq. (79): εₒₚₜ - Δ = (μ₂²/2)(r₂ - q₂*) + g(1) - Eᵥ[v²g(Qᵂ*(v))]

- Design tradeoffs:
  - **HCIZ integral evaluation** (ι(·) function): Numerically challenging for small γ; provides unified treatment of both phases but has ~1% error in some regimes
  - **GAMP-RIE vs. MCMC**: GAMP-RIE is polynomial-time but provably cannot find specialization; HMC can find it but may require exponentially many steps
  - **Discrete vs. continuous Pᵥ**: Continuous readouts give infinite sequence of transitions but require binning approximation; discrete readouts give finite transitions but sharper phenomenology

- Failure signatures:
  - Qᵂ(v) predicted > 0 but numerical methods yield Qᵂ ≈ 0: Likely RS potential is too flat (occurs near γ ≳ 1 for quadratic activation)
  - Training loss plateaus at ~2× εᵤₙᵢ: Algorithm trapped in glassy phase (see Fig. 10)
  - HMC q₂ overlaps stabilize ~1% below theory prediction: May indicate need for refined order parameters (see Appendix E.2)

- First 3 experiments:
  1. **Validation sweep**: Implement the saddle point solver (code in repository); reproduce Figure 1 curves for ReLU with homogeneous readouts, verifying the αsp transition location matches theory
  2. **Universality test**: Run MCMC for both Gaussian and Rademacher inner weights with identical other parameters; confirm identical εₒₚₜ for α < αsp but divergence for α > αsp
  3. **Hardness probe**: Train with ADAM on ReLU teacher (α = 5.0, γ = 0.5, Δ = 10⁻⁴); measure gradient steps to cross threshold ε* between εᵤₙᵢ and εₒₚₜ; verify exponential scaling in d (as in Appendix I Figure 8)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a polynomial-time algorithm that can achieve the Bayes-optimal generalisation performance in the specialisation phase, or does a statistical-to-computational gap fundamentally prevent efficient learning?
- Basis in paper: "It would thus be interesting to settle whether GAMP-RIE has the best prediction performance achievable by a polynomial-time learner when n = Θ(d²) for such targets."
- Why unresolved: GAMP-RIE matches only the universal branch; algorithms that reach specialisation (ADAM, HMC) appear to require exponentially long training time with increasing dimension for some readout distributions
- What evidence would resolve it: Either a provably polynomial-time algorithm achieving Bayes-optimal error in the specialisation phase, or a rigorous lower bound proof showing exponential complexity is necessary

### Open Question 2
- Question: What determines whether the specialisation transition is computationally hard to reach, and how does the readout weight distribution affect this hardness?
- Basis in paper: "For the case of the continuous distribution of readouts Pv = N(0,1), our numerical results are inconclusive on hardness, and deserve numerical investigation at a larger scale."
- Why unresolved: Empirical results show exponential-time scaling for homogeneous and Rademacher readouts, but the evidence is inconclusive for Gaussian readouts
- What evidence would resolve it: Large-scale simulations across different readout distributions systematically comparing training time scaling, potentially combined with theoretical analysis of the loss landscape structure

### Open Question 3
- Question: Can a unified asymptotically exact theory be derived for the free entropy and generalisation error without requiring major breakthroughs in multi-matrix model theory?
- Basis in paper: "We believe that obtaining asymptotically exact formulas... will require some major breakthrough in the field of multi-matrix models."
- Why unresolved: The current approach blends spin glass techniques and spherical integrals with approximations; the replica-symmetric formulas, while numerically accurate, are not proved exact
- What evidence would resolve it: Rigorous proof that the replica-symmetric formulas are asymptotically exact, or a refined mathematical framework that yields provably exact expressions

## Limitations

- The theoretical framework assumes perfect knowledge of the teacher's prior and correct specification of P_W and P_out, which may not hold in practical scenarios
- Numerical validation shows excellent agreement with HMC and ADAM in most regimes, but discrepancies appear near phase transitions, particularly for quadratic activations with γ ≳ 1 where the RS potential becomes too flat
- The current approach requires approximations that, while numerically accurate, are not rigorously proved to be exact

## Confidence

- **High confidence**: The existence of specialisation transitions, the universality of the pre-specialisation phase for different weight priors, and the overall phase diagram structure
- **Medium confidence**: Quantitative predictions for the transition location α_sp and the exact generalization error values, particularly for γ ≳ 1
- **Low confidence**: The performance of GAMP-RIE in finding the specialisation branch, as this remains computationally challenging even for the algorithm

## Next Checks

1. **Transition sharpness verification**: Systematically vary d, γ, and Δ to map out the parameter space where the specialisation transition becomes sharp vs. rounded, particularly focusing on the regime γ ≳ 1 where current theory shows limitations

2. **Algorithmic performance characterization**: Compare multiple optimization algorithms (SGD, Adam, HMC, GAMP-RIE) across the full phase diagram to identify which algorithms can access the specialisation branch and quantify the computational overhead required

3. **Higher-order correlation analysis**: Implement the extended order parameter framework from Appendix E.2 to track Q_2(v,v') correlations and test whether this resolves the remaining ~1% discrepancies between theory and HMC simulations