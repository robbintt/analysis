---
ver: rpa2
title: Success and Cost Elicit Convention Formation for Efficient Communication
arxiv_id: '2510.24023'
source_url: https://arxiv.org/abs/2510.24023
tags:
- games
- speaker
- listener
- success
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to train large multimodal models to
  form linguistic conventions that enable efficient communication with people. The
  approach uses simulated reference games between models and preference optimization
  based on both communicative success and cost, without requiring additional human-produced
  data.
---

# Success and Cost Elicit Convention Formation for Efficient Communication

## Quick Facts
- arXiv ID: 2510.24023
- Source URL: https://arxiv.org/abs/2510.24023
- Authors: Saujas Vaduguru; Yilun Hua; Yoav Artzi; Daniel Fried
- Reference count: 40
- Primary result: Training multimodal models with success and cost optimization enables efficient convention formation for communication with humans

## Executive Summary
This paper presents a method to train large multimodal models to form linguistic conventions that enable efficient communication with people. The approach uses simulated reference games between models and preference optimization based on both communicative success and cost, without requiring additional human-produced data. Experiments with human listeners in repeated reference games over COCO photographs and tangram images show that the trained model reduces message length by up to 41% while increasing communicative success by 15% over the interaction. Human listeners also respond 35-52% faster to utterances from the convention-forming model compared to baselines. Crucially, the paper demonstrates that training based on success or cost alone is insufficient - both components are necessary to elicit efficient convention formation.

## Method Summary
The method trains speaker models through simulated reference games where a speaker generates utterances describing target images and a listener guesses which image was described. Preference pairs are constructed favoring the shortest successful utterance over longer or unsuccessful alternatives. The training uses IPO-style optimization with LoRA adapters on base models (Gemma-3-12B or Pixtral-12B). The key innovation is optimizing jointly for communicative success and utterance cost, which induces convention formation. The approach requires no additional human-produced data, instead generating preference signals through self-play between pretrained speaker and listener models. In-context history enables models to build and reuse conventions across repetitions, reducing word novelty rates while maintaining communicative effectiveness.

## Key Results
- Models reduce message length by up to 41% while increasing communicative success by 15% over interactions
- Human listeners respond 35-52% faster to utterances from convention-forming models
- Success-only or cost-only training is insufficient; both components are necessary for efficient convention formation
- Word novelty rates decrease over repetitions as models reuse conventions from prior utterances

## Why This Works (Mechanism)

### Mechanism 1: Joint Success-Cost Optimization Drives Convention Emergence
Optimizing simultaneously for communicative success and utterance cost induces convention formation; either objective alone is insufficient. Preference pairs are constructed favoring the shortest successful utterance over longer or unsuccessful alternatives, creating selection pressure toward abbreviated forms that retain discriminative power within the interaction context.

### Mechanism 2: Simulated Self-Play Generates Training Signal Without Human Data
Simulated reference games between pretrained speaker and listener models produce sufficient preference signal for DPO-style optimization. A context of k images is sampled; the speaker generates n candidate utterances per trial; the listener guesses each; pairs are constructed from outcomes. The simulation provides coverage of interaction trajectories that would be expensive to collect from humans.

### Mechanism 3: In-Context History Enables Convention Reuse Across Repetitions
Conditioning on prior trials allows models to build and reuse conventions, reducing word novelty and message length over repetitions. The model receives the full trial history as context. Through preference training, it learns that reusing words from prior successful descriptions is preferred, leading to decreasing word novelty ratios.

## Foundational Learning

- **Repeated Reference Games**: Why needed - the entire training and evaluation framework depends on understanding how repeated interactions with the same images enable convention formation. Quick check - Can you explain why the same image set is reused across multiple trials and what behavioral changes are expected?

- **Direct Preference Optimization (DPO/IPO)**: Why needed - the method uses preference pairs rather than scalar rewards; understanding pairwise optimization is essential for implementing the training loop. Quick check - How does constructing preference pairs differ from defining a scalar reward, and what are the optimization implications?

- **Word Novelty Ratio (WNR)**: Why needed - WNR is the primary metric for detecting convention formation; interpreting it correctly is critical for evaluation. Quick check - Given two utterances describing the same image, how would you compute WNR, and what does a decreasing trend indicate?

## Architecture Onboarding

- **Component map**: Base speaker model (Gemma-3-12B or Pixtral-12B pretrained) -> Base listener model (finetuned for shuffled image presentation) -> Simulation engine (Algorithm 1) -> Preference pair constructor (SUCCESS+COST criterion) -> IPO trainer with LoRA adapters

- **Critical path**: Pretrained model -> Vision encoder adaptation (tangrams only) -> Simulate 400-500 games -> Construct preference pairs -> IPO training (3 epochs, Î²=0.3) -> Evaluate with human or simulated listeners

- **Design tradeoffs**: (1) Greedy vs. nucleus decoding - greedy maximizes accuracy on COCO but causes degenerate repetition on tangrams; (2) Context size vs. compute - larger histories improve convention reuse but increase inference cost; (3) Temperature for image context sampling - lower temperatures yield harder, more similar image sets

- **Failure signatures**: (1) SUCCESS-only models show high accuracy but increasing message length (no efficiency gain); (2) COST-only models collapse to near-chance accuracy; (3) Greedy decoding on tangrams causes identical descriptions across all images

- **First 3 experiments**:
  1. Validate simulation quality: Run simulated games with base models, measure listener accuracy and message length distributions before any preference training to confirm baseline behavior.
  2. Ablate utility components: Train three speaker variants (SUCCESS+COST, SUCCESS-only, COST-only) and compare WNR trajectories and final accuracy on a held-out validation set.
  3. Test human transfer: Deploy SUCCESS+COST speaker with human listeners on a small pilot (n=10 games), measure response time reduction and subjective ratings vs. baseline.

## Open Questions the Paper Calls Out

- **Context Window Limitations**: Extending to more complex domains would involve managing much longer contexts that may not fit in the (effective) context window. The current method relies on providing the full sequence of previous trials, which is feasible for short games but fails for extended interactions.

- **Safety and Deception**: A model that communicates its intent as efficiently as possible may do so by withholding important information, hence acting deceptively. The current study uses benign reference games with objective ground truths, whereas real-world applications require balancing efficiency against safety and truthfulness.

- **Complex Dialogue Tasks**: The method is demonstrated only in the setting of repeated reference games. It's unclear if simple notions of "success" (correct identification) and "cost" (word count) transfer to tasks requiring multi-step reasoning or collaborative planning like navigation or negotiation.

## Limitations

- The method relies entirely on simulated preference data between pretrained models, with uncertain fidelity compared to human-collected data
- Results are shown only on controlled datasets (COCO photographs and tangram images) with consistent visual characteristics
- The approach assumes base models have sufficient visual-language grounding to produce meaningful candidate utterances and listener judgments

## Confidence

- **High Confidence**: Joint SUCCESS+COST optimization mechanism works better than either component alone; in-context history mechanism enables convention reuse; method achieves measurable efficiency gains with human listeners
- **Medium Confidence**: Simulated self-play generates sufficient training signal; convention formation generalizes to new referents; approach scales to more complex communication tasks
- **Low Confidence**: Utility function perfectly captures human trade-offs between brevity and clarity; method will work equally well with different base model architectures; convention formation process is robust to variations in simulation parameters

## Next Checks

1. **Simulation Fidelity Validation**: Run parallel experiments using simulated preferences vs. human-collected preference data (n=50 games) to quantify the gap between simulation and reality. Measure agreement rates between simulated and human preferences, and assess whether model trained on simulated data transfers equally well to human listeners.

2. **Base Model Robustness Test**: Repeat the entire pipeline with alternative base models (e.g., Llama-3-8B, Claude-3-Sonnet) to determine whether the convention formation mechanism is model-dependent or generalizes across architectures. Compare WNR trajectories, final accuracy, and human response times across model variants.

3. **Cross-Domain Transfer Experiment**: Apply the trained models to a significantly different visual domain (e.g., medical imaging, satellite imagery, or complex 3D scenes) to test generalization beyond controlled datasets. Measure whether conventions formed on COCO/tangrams transfer to new visual categories and whether the same preference optimization strategy remains effective.