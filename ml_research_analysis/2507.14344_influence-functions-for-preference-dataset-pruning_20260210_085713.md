---
ver: rpa2
title: Influence Functions for Preference Dataset Pruning
arxiv_id: '2507.14344'
source_url: https://arxiv.org/abs/2507.14344
tags:
- influence
- training
- examples
- data
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies influence functions to detect and remove harmful
  examples from a human preference dataset used to train reward models. The method
  uses conjugate-gradient approximated inverse Hessians in the LoRA parameter space
  to estimate the influence of each training example on validation performance.
---

# Influence Functions for Preference Dataset Pruning

## Quick Facts
- **arXiv ID**: 2507.14344
- **Source URL**: https://arxiv.org/abs/2507.14344
- **Reference count**: 6
- **Primary result**: Removing the most harmful 10% of examples from human preference data improves pairwise accuracy by 1.5% using influence-based filtering

## Executive Summary
This paper introduces an influence function-based approach for pruning harmful examples from human preference datasets used to train reward models. The method leverages conjugate-gradient approximated inverse Hessians in LoRA parameter space to estimate how each training example influences validation performance. By removing the most harmful 10% of examples, the approach achieves a 1.5% improvement in pairwise accuracy compared to training on the full dataset. The authors also compare influence functions against gradient similarity, finding that while gradient similarity better identifies helpful examples, influence functions are superior at detecting harmful ones, highlighting the importance of local curvature information.

## Method Summary
The approach uses influence functions to identify and remove harmful examples from preference datasets. The method computes the influence of each training example on validation loss by approximating the inverse Hessian using conjugate gradient methods in the LoRA parameter space. This allows the model to estimate how removing each example would affect overall performance. The authors then filter out the most harmful examples (identified as those whose removal would most improve validation performance) and retrain the reward model on the pruned dataset. They compare this approach against gradient similarity-based filtering and evaluate performance using pairwise accuracy on validation data.

## Key Results
- Removing the most harmful 10% of examples improves pairwise accuracy by 1.5% compared to full dataset training
- Influence functions are more effective than gradient similarity at identifying harmful examples
- Gradient similarity outperforms influence functions at identifying helpful examples, suggesting local curvature is important for detecting harmful outliers
- The computational cost of influence function estimation remains a significant limitation

## Why This Works (Mechanism)
Influence functions provide a principled way to estimate how individual training examples affect model performance by approximating the effect of removing each example. The method captures second-order information (curvature) through the inverse Hessian, which first-order methods like gradient similarity cannot capture. This local curvature information is crucial for distinguishing between examples that appear similar in gradient space but have fundamentally different effects on model behaviorâ€”particularly for identifying harmful outliers that might otherwise be mistaken for useful training data.

## Foundational Learning
- **Influence Functions**: A statistical technique for estimating the effect of individual data points on model predictions by approximating the change in parameters when removing that point
  - *Why needed*: To quantify how each training example affects validation performance
  - *Quick check*: Verify that influence scores correlate with expected changes in validation loss when examples are removed
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that modifies model weights through low-rank updates rather than full parameter updates
  - *Why needed*: Reduces computational cost of influence function computation by working in a lower-dimensional parameter space
  - *Quick check*: Confirm that LoRA updates preserve model performance while reducing parameter count
- **Conjugate Gradient Method**: An iterative algorithm for solving systems of linear equations, used here to approximate the inverse Hessian
  - *Why needed*: Computing the full inverse Hessian is computationally prohibitive for large models
  - *Quick check*: Verify convergence of conjugate gradient approximation within reasonable iteration counts
- **Reward Modeling**: Training models to predict human preferences between pairs of responses
  - *Why needed*: The target application domain where dataset quality directly impacts alignment
  - *Quick check*: Ensure pairwise accuracy metric appropriately captures model performance on preference data
- **Pairwise Accuracy**: The percentage of preference comparisons where the model correctly ranks the preferred response higher
  - *Why needed*: Standard evaluation metric for reward models that directly measures alignment with human preferences
  - *Quick check*: Validate that accuracy improvements correspond to better qualitative preferences

## Architecture Onboarding

**Component Map**: Human Preference Data -> Influence Function Computation -> Example Scoring -> Pruning (remove harmful 10%) -> Reward Model Training -> Pairwise Accuracy Evaluation

**Critical Path**: The computational bottleneck is the influence function estimation, which requires multiple passes through the training data and iterative conjugate gradient computation for each example to approximate the inverse Hessian-vector product.

**Design Tradeoffs**: The method trades computational efficiency for improved data quality. While influence functions provide more accurate identification of harmful examples than first-order methods, they require substantially more computation. The use of LoRA helps mitigate this cost but doesn't eliminate it.

**Failure Signatures**: 
- Computational cost becoming prohibitive for larger datasets
- Influence scores failing to correlate with actual performance improvements
- Over-pruning that removes useful examples along with harmful ones
- Diminishing returns where the most harmful examples have minimal impact on overall performance

**3 First Experiments**:
1. Compare influence function pruning against random pruning at various removal rates (5%, 10%, 20%) to establish baseline improvement
2. Test gradient similarity filtering as a faster alternative and compare its effectiveness at identifying both helpful and harmful examples
3. Evaluate the impact of pruning on different types of preference examples (easy vs. hard comparisons) to understand which examples benefit most from removal

## Open Questions the Paper Calls Out
None

## Limitations
- Computational expense of influence function estimation remains a significant bottleneck
- 1.5% improvement in pairwise accuracy, while meaningful, may not justify additional computational overhead
- Method effectiveness appears context-dependent, being most valuable for detecting harmful outliers rather than general dataset optimization
- Results are limited to human preference datasets and reward model architectures

## Confidence
- **Influence function effectiveness at detecting harmful examples**: Medium
- **Computational cost-benefit trade-off**: Low
- **Generalizability beyond preference data**: Low

## Next Checks
1. Benchmark influence function pruning against simpler, cheaper heuristics (e.g., loss-based filtering) across multiple dataset scales to quantify the cost-accuracy trade-off
2. Test the approach on non-preference datasets (e.g., classification or regression tasks) to evaluate generalizability beyond reward modeling
3. Implement parallel or distributed Hessian approximation to measure potential speedups and assess whether computational costs can be reduced to practical levels