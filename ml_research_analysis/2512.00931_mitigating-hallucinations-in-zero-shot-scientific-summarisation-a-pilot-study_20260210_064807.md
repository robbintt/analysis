---
ver: rpa2
title: 'Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study'
arxiv_id: '2512.00931'
source_url: https://arxiv.org/abs/2512.00931
tags:
- prompt
- were
- abstract
- alignment
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates prompt engineering methods to reduce context
  inconsistency hallucinations in zero-shot LLM summarisation of scientific texts.
  Seven prompt methods were tested across six LLMs using eight yeast biotechnology
  abstracts: a baseline prompt, two levels of increasing instruction complexity (PE-1,
  PE-2), and two levels of context repetition (CR-K1, CR-K2) and random addition (RA-K1,
  RA-K2) of abstract sentences.'
---

# Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study

## Quick Facts
- arXiv ID: 2512.00931
- Source URL: https://arxiv.org/abs/2512.00931
- Authors: Imane Jaaouine; Ross D. King
- Reference count: 40
- Key outcome: Context repetition and random addition significantly improve lexical alignment in LLM summarisation, while increased instruction complexity reduces semantic alignment.

## Executive Summary
This pilot study investigates prompt engineering methods to reduce context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts. Seven prompt methods were tested across six LLMs using eight yeast biotechnology abstracts. Statistical analysis showed that context repetition and random addition significantly improved lexical alignment with abstracts, while increased instruction complexity reduced semantic alignment. The findings demonstrate prompt engineering as a practical tool for mitigating hallucinations, though results may be limited by the small dataset and lack of human evaluation.

## Method Summary
The study tested seven prompt methods on six LLMs using eight yeast biotechnology abstracts. Methods included a baseline prompt, two levels of increasing instruction complexity (PE-1, PE-2), and two levels of context repetition (CR-K1, CR-K2) and random addition (RA-K1, RA-K2) of abstract sentences. Summaries were evaluated using ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity. Statistical analysis employed Wilcoxon signed-rank tests with Bonferroni-Holm correction and BCa bootstrap confidence intervals on 336 generated summaries.

## Key Results
- Context repetition (CR-K1/K2) and random addition (RA-K1/K2) significantly improved lexical alignment with abstracts
- Increased instruction complexity (PE-1) reduced semantic alignment across multiple metrics
- Random addition achieved similar lexical improvements to context repetition, suggesting quantity of repetition matters more than semantic relevance
- Context repetition weakly improved alignment with repeated key sentences (partial support for H4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context repetition of key sentences improves lexical alignment between LLM outputs and source text.
- Mechanism: Repeating semantically relevant sentences increases token frequency in the input prompt, which biases the LLM's next-token prediction toward lexically overlapping content from the repeated sentences, reducing context inconsistency hallucinations.
- Core assumption: Repetition strengthens attention weights on specific content during inference.
- Evidence anchors:
  - [abstract] "Statistical analysis... showed that context repetition... significantly improved lexical alignment with abstracts (H1, H2 supported)."
  - [section 5.4] "CR-K2 achieved higher significance than CR-K1 across all three ROUGE scores... indicating a relationship between the number of key sentences repeated, K, and lexical alignment."
  - [corpus] Limited direct support; related work on adaptive repetition addresses position bias in ranking, not summarization.
- Break condition: Effect may plateau or reverse if repeated content exceeds useful signal threshold, causing overfitting to repeated phrases.

### Mechanism 2
- Claim: Random sentence addition improves lexical alignment similarly to context repetition, suggesting quantity of repetition matters more than semantic relevance for lexical metrics.
- Mechanism: Adding any sentences from the abstract (even randomly selected ones) increases overlap probability between prompt and desired output, as ROUGE metrics reward exact n-gram matches regardless of semantic importance.
- Core assumption: Lexical alignment improvements reflect surface-level overlap rather than deeper comprehension.
- Evidence anchors:
  - [abstract] "CR and RA significantly improve the lexical alignment... H1, H2 supported."
  - [section 5.4] "Random addition prompt methods significantly impacted both lexical and semantic alignment with the abstract text and repeated key sentences... RA-K2 achieved more positive performance effects."
  - [corpus] No direct corpus support for random addition effects in summarization; neighbor papers focus on semantic relevance in retrieval, not random augmentation.
- Break condition: Random addition may improve lexical scores without improving factual accuracy or coherence of summaries.

### Mechanism 3
- Claim: Increased instruction complexity (PE-1) reduces semantic alignment, possibly by over-constraining the model's generation space.
- Mechanism: Explicit instructions about summary style and structure may prime the model toward generic abstract patterns from training data rather than adapting to the specific scientific content, increasing divergence from source semantics.
- Core assumption: Instruction complexity shifts attention from content to format, disrupting semantic grounding.
- Evidence anchors:
  - [abstract] "Increased instruction complexity reduced semantic alignment (H3 not supported)."
  - [section 5.4] "PE-1 led to significantly reduced semantic alignment across BERTScore, cosine similarity and METEOR score... suggests that increased prompt instruction discouraged the LLM from paraphrasing and semantically varying summaries."
  - [corpus] Neighbor paper on prompt compression suggests lengthy prompts increase costs, but doesn't address semantic degradation.
- Break condition: Higher complexity instructions might improve alignment for well-structured, familiar text types where training data matches the task format.

## Foundational Learning

- Concept: Context inconsistency hallucinations vs. factuality hallucinations
  - Why needed here: The paper specifically targets faithfulness issues (output misaligned with input context), not factual errors about world knowledge. Understanding this distinction is critical for selecting appropriate mitigation strategies.
  - Quick check question: If an LLM summary contradicts the provided abstract but is factually correct about general biology, which hallucination type is this?

- Concept: Lexical vs. semantic alignment metrics
  - Why needed here: ROUGE captures exact word overlap, while BERTScore and METEOR capture meaning. A method that improves ROUGE but harms BERTScore may produce different-sounding summaries that miss the original meaning.
  - Quick check question: Why might random sentence addition improve ROUGE-2 but not necessarily improve summary quality?

- Concept: Zero-shot prompting and knowledge boundaries
  - Why needed here: The study assumes scientific abstracts fall outside LLM pre-training data, forcing reliance on prompt context. This assumption motivates testing whether prompt manipulation can substitute for domain knowledge.
  - Quick check question: How would results differ if abstracts came from Wikipedia instead of recent arXiv papers?

## Architecture Onboarding

- Component map:
  - Abstract text → sentence segmentation (NLTK Punkt tokenizer) → key sentence extraction (all-MiniLM-L6-v2 embeddings + FAISS L2 distance) → prompt assembly (7 variants) → 6 instruction-tuned LLMs via Hugging Face Inference API (default sampling, max 300 tokens) → evaluation (6 metrics: ROUGE-1/2/L, BERTScore, METEOR, cosine similarity) → BCa bootstrap CIs + Wilcoxon signed-rank tests with Bonferroni-Holm correction

- Critical path:
  1. Sentence extraction quality directly affects CR/RA conditions—poor semantic similarity computation propagates errors.
  2. Prompt construction must ensure K repeated sentences don't exceed token limits.
  3. Statistical significance requires both tests to agree; relying on only one inflates false positives.

- Design tradeoffs:
  - **Small dataset (8 abstracts)**: Enables deep analysis but limits generalizability across scientific domains.
  - **Automatic metrics only**: Scalable but no human validation of hallucination reduction or summary usefulness.
  - **Zero-shot only**: Clean test of prompting effects but doesn't reflect real-world fine-tuned deployments.
  - **K ∈ {1, 2}**: Practical for short abstracts but may not scale to longer documents.

- Failure signatures:
  - PE-1/PE-2 showing no improvement or degradation signals that instruction elaboration isn't a universal solution.
  - CR and RA performing similarly suggests semantic relevance of repeated content may not be the active ingredient for lexical metrics.
  - High variance in RA-K2 results indicates sensitivity to which random sentences are selected.

- First 3 experiments:
  1. **Domain transfer test**: Apply the same 7 prompt methods to abstracts from computational linguistics or materials science to assess whether findings generalize beyond yeast biotechnology.
  2. **Human evaluation layer**: Collect expert judgments on factual accuracy and hallucination presence for a subset of summaries to validate whether improved metrics correlate with actual hallucination reduction.
  3. **K scaling experiment**: Test K ∈ {3, 5, 7} on longer texts (introduction sections, not just abstracts) to identify optimal repetition ratio and potential break points where repetition becomes counterproductive.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the semantic relevance of repeated sentences significantly impact summary alignment, or is the improvement primarily driven by increased prompt length?
- Basis in paper: [explicit] The authors note that Random Addition (RA) performed surprisingly well compared to Context Repetition (CR) and suggest "future work could study the impact of semantic relevance of repeated sentences on summary alignment."
- Why unresolved: The study found that repeating random sentences often yielded results comparable to or better than repeating key sentences, leaving the specific contribution of semantic relevance versus mere token repetition unclear.
- What evidence would resolve it: Ablation studies controlling strictly for prompt length while varying the semantic density of the repeated content to isolate the variable responsible for reduced hallucinations.

### Open Question 2
- Question: How does the scalability of context repetition and random addition methods change with higher values of K (number of repeated sentences) and increased prompt length?
- Basis in paper: [explicit] The conclusion explicitly calls for "researching the relationships between context repetition and random addition performance effects, prompt length, and the number K of key sentences."
- Why unresolved: The experiment was limited to K∈ {1, 2}, providing insufficient data to determine if the observed improvements in lexical alignment persist, plateau, or degrade as the ratio of repeated text increases.
- What evidence would resolve it: Extending the experimental framework to test a wider range of K values (e.g., K up to 5 or 10) to map performance curves relative to the total abstract length.

### Open Question 3
- Question: Do the prompt engineering mitigation strategies identified in this study transfer effectively to fine-tuned LLMs, or are they specific to zero-shot settings?
- Basis in paper: [inferred] The limitations section states that "Zero-shot prompting was used, indicating that the results may vary for fine-tuned LLMs."
- Why unresolved: The study relied on instruction-tuned models using only pre-training data; models explicitly fine-tuned on scientific corpora may process context repetition differently, potentially rendering the "random addition" technique ineffective or detrimental.
- What evidence would resolve it: Replicating the seven prompt conditions using domain-specific fine-tuned models (e.g., scientific LLMs) and comparing the statistical significance of alignment improvements against the zero-shot baselines.

## Limitations

- Small dataset of 8 abstracts from a single domain (yeast biotechnology) limits generalizability
- Exclusive use of automatic metrics without human evaluation cannot confirm actual hallucination reduction
- K ∈ {1, 2} constraint may not capture optimal repetition ratios for different text lengths
- Random seed for RA prompt generation and default sampling parameters for LLMs remain unspecified

## Confidence

- **High Confidence**: Context repetition (CR-K1/K2) significantly improves lexical alignment - multiple statistical tests converge on this finding
- **Medium Confidence**: Random addition (RA-K1/K2) achieves similar lexical improvements to context repetition - statistically significant but mechanism unclear
- **Low Confidence**: Increased instruction complexity (PE-1) reduces semantic alignment - mechanism is speculative and may be task-dependent

## Next Checks

1. **Domain Transfer Test**: Apply the 7 prompt methods to abstracts from computational linguistics or materials science to assess generalizability beyond yeast biotechnology.
2. **Human Evaluation Layer**: Collect expert judgments on factual accuracy and hallucination presence for a subset of summaries to validate whether improved metrics correlate with actual hallucination reduction.
3. **K Scaling Experiment**: Test K ∈ {3, 5, 7} on longer texts (introduction sections, not just abstracts) to identify optimal repetition ratio and potential break points where repetition becomes counterproductive.