---
ver: rpa2
title: GNN Explanations that do not Explain and How to find Them
arxiv_id: '2601.20815'
source_url: https://arxiv.org/abs/2601.20815
tags:
- explanations
- explanation
- section
- class
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Self-explainable Graph Neural Networks (SE-GNNs) provide explanations
  during inference by coupling an explanation extractor that identifies explanatory
  subgraphs with a classifier that uses these subgraphs to generate predictions. However,
  this work identifies a critical failure: explanations can be completely unrelated
  to how SE-GNNs infer labels.'
---

# GNN Explanations that do not Explain and How to find Them

## Quick Facts
- arXiv ID: 2601.20815
- Source URL: https://arxiv.org/abs/2601.20815
- Authors: Steve Azzolin; Stefano Teso; Bruno Lepri; Andrea Passerini; Sagar Malhotra
- Reference count: 40
- Self-explainable GNNs can achieve optimal accuracy while outputting explanations unrelated to their decision process

## Executive Summary
Self-explainable Graph Neural Networks (SE-GNNs) provide explanations during inference by coupling an explanation extractor that identifies explanatory subgraphs with a classifier that uses these subgraphs to generate predictions. However, this work identifies a critical failure: explanations can be completely unrelated to how SE-GNNs infer labels. Specifically, under mild assumptions, SE-GNNs can achieve optimal true risk while producing degenerate explanations that encode the predicted label without revealing the actual decision-making process. This failure can be maliciously exploited to conceal the use of sensitive attributes and can also emerge naturally during training.

The study demonstrates that a malicious attacker can successfully train SE-GNNs to output arbitrary unfaithful explanations while maintaining high predictive accuracy. Moreover, most existing faithfulness metrics fail to detect these degenerate explanations. To address this, the authors introduce a novel faithfulness metric, EST (Extension Sufficiency Test), which reliably identifies unfaithful explanations by evaluating all possible supergraphs of the explanation within the input graph. Experiments show that EST consistently rejects degenerate explanations while recognizing faithful ones, whereas popular metrics often fail. The results underscore the need for reliable auditing of SE-GNN explanations and provide practitioners with a more robust tool to assess explanation trustworthiness.

## Method Summary
The method involves training SE-GNNs with a malicious objective that forces the explanation extractor to highlight arbitrary nodes while maintaining high predictive accuracy. The attack uses a combined loss function: classification loss plus a binary cross-entropy term that pushes relevance scores to match designated explanations. The EST metric then evaluates faithfulness by sampling supergraphs of the explanation and checking if predictions remain stable when extended with complementary features. The approach is validated across multiple SE-GNN architectures (GSAT, DIR, SMGNN) and datasets (RBGV, MNISTsp, MUTAG, SST2P).

## Key Results
- SE-GNNs can achieve optimal true risk while outputting explanations completely unrelated to their actual decision-making process
- Malicious attacks can train SE-GNNs to output arbitrary unfaithful explanations while maintaining high predictive accuracy (F1 ≥92% on designated explanations)
- EST metric reliably identifies degenerate explanations where popular metrics (Fid-, Suf, CF) fail to detect them

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SE-GNNs can achieve optimal true risk while outputting explanations completely unrelated to their actual decision-making process.
- Mechanism: The explanation extractor `e` selects an anchor set `Z = {z_y}_{y∈Y}` (recurrent patterns appearing in all graphs, thus non-discriminative). The classifier `g` then maps these anchor nodes directly to labels. Since anchor nodes have no class-discriminative power, the model must use other features to achieve high accuracy, but these are hidden from the explanation.
- Core assumption: Hard explanation extractor outputting scores in `{r,1}` or `{0,1}`, deterministic ground truth labeling function, and non-empty explanation `|R|>0`.
- Evidence anchors:
  - [abstract] "under mild assumptions, SE-GNNs can achieve optimal true risk while producing degenerate explanations that encode the predicted label without revealing the actual decision-making process."
  - [Section 3, Theorem 1] Proves optimal true-risk is achievable with anchor set explanations for GSAT, LRI, CAL, GMT-lin, and SMGNN.
  - [corpus] Related work "Beyond Topological Self-Explainable GNNs" formalizes similar failure modes as Minimal Explanations.
- Break condition: If explanation size is constrained too tightly (e.g., DIR's top-K), models may still encode labels in small degenerate subgraphs rather than avoiding them.

### Mechanism 2
- Claim: An attacker can deliberately train SE-GNNs to output arbitrary unfaithful explanations while maintaining predictive accuracy.
- Mechanism: Add a binary cross-entropy loss `L_expl` that forces explanation relevance scores to match designated malicious explanations. The combined loss `L_clf + L_expl` trains the model to predict correctly while highlighting only attacker-specified (task-irrelevant) nodes.
- Core assumption: Attacker has full control over model training; designated explanations exist in all samples (anchor set property).
- Evidence anchors:
  - [Section 4, Eq. 3] Explicit formulation of attack objective combining classification loss with explanation supervision.
  - [Table 2] Attack achieves F1 ≥92% on designated explanations across GSAT, DIR, SMGNN on RBGV, MNISTsp, MUTAG, SST2P.
  - [corpus] Weak corpus evidence for this specific attack mechanism; related work focuses on post-hoc explanation manipulation rather than training-time attacks.
- Break condition: Attack may fail on out-of-distribution samples (SMGNN on SST2P test set dropped to F1 ≈59% due to OOD graphs).

### Mechanism 3
- Claim: EST (Extension Sufficiency Test) reliably identifies unfaithful explanations by testing prediction stability across all possible supergraphs of the explanation.
- Mechanism: EST computes `max_{R⊆G'⊆G} d(g(e(G)), g(e(G')))` where `d` is a distance measure. If extending the explanation with complementary features changes the prediction, the original explanation was insufficient (omitted relevant information). This catches degenerate explanations because they cannot preserve predictions when extended with task-relevant features.
- Core assumption: Access to the trained SE-GNN for perturbation evaluation; budget of supergraphs to sample is sufficient.
- Evidence anchors:
  - [Section 5.2, Definition 1] Formal EST definition.
  - [Table 3] EST achieves highest rejection ratios (≥50%) for degenerate explanations where other metrics fail (Fid-, Suf, CF often ≈0%).
  - [corpus] "Predicting Satisfaction of Counterfactual Explanations" discusses similar perturbation-based evaluation but for tabular data.
- Break condition: EST may conservatively reject faithful explanations if supergraphs create OOD samples; cannot detect redundant explanations that include irrelevant features alongside relevant ones.

## Foundational Learning

- Concept: **Self-Explainable GNN Architecture (SE-GNN)**
  - Why needed here: Understanding `f(G) = g(e(G))` is essential to see how explanation extractor `e` and classifier `g` can be decoupled, enabling the degenerate behavior.
  - Quick check question: Can you explain why a high-accuracy SE-GNN cannot rely solely on anchor set nodes for predictions?

- Concept: **Sufficiency vs Necessity in Faithfulness Metrics**
  - Why needed here: EST tests sufficiency (prediction stable when complement perturbed); understanding this distinction clarifies why necessity metrics fail to catch degenerate explanations (the classifier `g` always responds to explanation removal regardless of faithfulness).
  - Quick check question: Why does a necessity metric mark the unfaithful explanation `R=u_violet` as "necessary" in Example 1?

- Concept: **Anchor Sets and Label Encoding**
  - Why needed here: The core theoretical insight—explanations can encode labels via pattern selection rather than revealing decision logic. Understanding this is critical for interpreting Theorem 1 and the attack mechanism.
  - Quick check question: Given a dataset where every graph contains a blue node, why is this node unsuitable for explaining class predictions?

## Architecture Onboarding

- Component map:
  - Input graph G -> Explanation extractor e -> Relevance scores p_u -> Thresholded subgraph R -> Classifier g -> Prediction y
  - Input graph G -> EST perturbation sampling -> Supergraphs G' -> Classifier g -> Distance comparison

- Critical path:
  1. Input graph G enters explanation extractor `e`
  2. Relevance scores computed, thresholded to extract R
  3. R passed to classifier `g` for prediction
  4. For faithfulness auditing: EST perturbs complement, checks prediction stability

- Design tradeoffs:
  - EST is intentionally conservative: prioritizes catching unfaithful explanations over avoiding false positives
  - Attack requires full training control; mitigation requires access to model for EST evaluation
  - Sparsity regularization can inadvertently encourage degenerate explanations by forcing label encoding into small subgraphs

- Failure signatures:
  - High accuracy + explanations highlighting non-discriminative features (e.g., punctuation, background pixels, anchor nodes)
  - Faithfulness metrics (Fid-, Fid+, Suf, CF) showing ≈0% rejection despite clearly irrelevant explanations
  - Same explanation appearing for different classes across random seeds

- First 3 experiments:
  1. **Reproduce degenerate explanation**: Train GSAT on RBGV dataset with natural settings; verify if green/violet nodes are highlighted with high EST rejection ratio.
  2. **Validate attack**: Apply malicious training (Eq. 3) on MNISTsp targeting background pixels; measure F1 score on designated explanations and EST rejection ratio.
  3. **Compare metrics**: On attacked models, compute rejection ratios for Fid-, Fid+, Suf, RFid-, and EST; confirm EST uniquely catches degenerate cases where others fail.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can faithfulness metrics be developed to detect unfaithful explanations that contain redundancy (both relevant and irrelevant features), which the current Extension Sufficiency Test (EST) cannot identify?
- Basis: [explicit] Section 8 states EST "cannot identify unfaithful behaviors arising from redundancy, i.e., explanations containing both truly relevant elements and irrelevant ones."
- Why unresolved: The proposed EST metric focuses on insufficiency (omission of features), but identifying the inclusion of irrelevant features alongside relevant ones remains a blind spot for sufficiency-based metrics.
- What evidence would resolve it: A metric capable of distinguishing a minimal sufficient explanation from a sufficient but redundant one would resolve this.

### Open Question 2
- Question: Do Prime Implicant explanations admit degenerate cases where they pass robustness checks but still fail to faithfully represent the model's reasoning?
- Basis: [explicit] Section F.3 notes it is "an interesting direction for future work is to investigate whether Prime Implicant explanations themselves can still admit degenerate cases."
- Why unresolved: While Prime Implicants are formally robust to complement perturbations, it is theoretically possible they could still align with spurious patterns or mislead users.
- What evidence would resolve it: A formal analysis or counter-example showing a Prime Implicant that satisfies EST but fails to align with ground-truth causal features would resolve this.

### Open Question 3
- Question: How can SE-GNN architectures be modified to inherently prevent degenerate explanations without relying on post-hoc auditing?
- Basis: [explicit] Section 6 notes that avoiding degeneracy via stochastic optimization "remains beyond the practitioner's control, motivating further investigation into more robust SE-GNNs in future work."
- Why unresolved: The paper identifies a failure case where degenerate explanations yield optimal true risk, but offers no architectural constraints to prevent the model from encoding the label into the explanation itself.
- What evidence would resolve it: The development of a model architecture where the conditions of Theorem 1 cannot be satisfied would resolve this.

## Limitations

- EST metric may produce false rejections on faithful explanations when supergraphs create out-of-distribution samples
- Attack mechanism's effectiveness degrades on out-of-distribution test data (SMGNN on SST2P dropped to F1 ≈59%)
- EST cannot detect unfaithful explanations that contain both relevant and irrelevant features (redundancy blind spot)

## Confidence

- **High Confidence**: The theoretical existence of degenerate explanations (Theorem 1) and the EST metric's ability to detect them under controlled conditions
- **Medium Confidence**: The attack mechanism's generalizability across different datasets and model architectures
- **Low Confidence**: EST's performance on real-world datasets without anchor set properties and its false positive rate on faithful explanations

## Next Checks

1. Test EST on naturally trained SE-GNNs on datasets without anchor set properties to assess false positive rates
2. Investigate whether adversarial training with EST as a regularizer can prevent degenerate explanations
3. Evaluate EST's computational efficiency on large-scale graphs (b=50 supergraph samples may be prohibitive for industrial applications)