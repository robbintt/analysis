---
ver: rpa2
title: Deceptive Sequential Decision-Making via Regularized Policy Optimization
arxiv_id: '2501.18803'
source_url: https://arxiv.org/abs/2501.18803
tags:
- deception
- problem
- adversary
- agent
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adversaries inferring sensitive
  information from observing autonomous systems by introducing deceptive sequential
  decision-making strategies. The authors model autonomous systems as Markov decision
  processes and adversaries using inverse reinforcement learning to recover reward
  functions.
---

# Deceptive Sequential Decision-Making via Regularized Policy Optimization

## Quick Facts
- **arXiv ID:** 2501.18803
- **Source URL:** https://arxiv.org/abs/2501.18803
- **Reference count:** 40
- **Key outcome:** Three deception strategies (diversionary, targeted, equivocal) successfully misled adversaries in network defense simulations while maintaining at least 98% of optimal performance.

## Executive Summary
This paper addresses the problem of adversaries inferring sensitive information from observing autonomous systems by introducing deceptive sequential decision-making strategies. The authors model autonomous systems as Markov decision processes and adversaries using inverse reinforcement learning to recover reward functions. To counter these efforts, they present three regularization strategies for policy synthesis that manipulate occupancy measures to create false reward inferences. The approach provides a tunable deception-performance tradeoff with provable bounds on revenue loss.

## Method Summary
The authors formulate deceptive policy synthesis as regularized Markov decision process optimization problems. They introduce three deception types: diversionary deception maximizes distance from optimal occupancy measures, targeted deception minimizes distance to specified target occupancy measures, and equivocal deception balances occupancy measures between goal and decoy states. Each deception type adds a regularization term to the policy optimization objective with weight β, creating a controllable tradeoff between deception strength and performance loss. The approach is validated through numerical simulations of a network defense problem.

## Key Results
- Diversionary deception successfully led adversaries to any false conclusion about reward functions while maintaining >98% optimal performance
- Targeted deception directed adversaries to specific false reward inferences with bounded performance degradation
- Equivocal deception created equal likelihood of real and false rewards explaining agent behavior
- Analytical bounds on revenue loss increase linearly with deception parameter β, enabling controlled tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Occupancy measure manipulation steers adversary IRL algorithms toward false reward inferences.
- **Mechanism:** The system policy π determines occupancy measures x^π(s,a) (expected state-action visitation frequencies). Since IRL algorithms estimate reward functions from observed trajectory feature expectations μ_E = (1/N)∑_τ_j ∑_t φ(s_t^j, a_t^j), and these feature expectations are directly computed from occupancy measures, manipulating x(s,a) propagates to manipulated μ_E, causing incorrect reward recovery.
- **Core assumption:** The adversary uses IRL methods (apprenticeship learning, MaxEnt IRL, or Deep IRL) that rely on feature expectations derived from observed trajectories.
- **Evidence anchors:**
  - [abstract] "We model autonomous systems as Markov decision processes, with adversaries using inverse reinforcement learning to recover reward functions."
  - [Section II-B, Remark 2] "Since the occupancy measures {x(s,a)}_{x∈S,a∈A} are the expectations of the state-action visitations in the observed trajectories, they directly influence these observed trajectories, and thus the feature expectations."
- **Break condition:** If adversary uses model-free methods bypassing reward estimation, or has ground-truth priors resistant to feature manipulation, deception effectiveness degrades.

### Mechanism 2
- **Claim:** Regularization-based policy optimization creates a tunable deception-performance tradeoff with provable bounds.
- **Mechanism:** Each deception type adds a regularization term to the objective function with weight β > 0. Diversionary deception maximizes r(s,a)x(s,a) + β(x(s,a) - x^*(s,a))^2 (maximizing distance from optimal). Targeted deception minimizes distance to target occupancy measures. Equivocal deception enforces ∑_s∈S_goal ∑_a x(s,a) ≈ ∑_s∈S_decoy ∑_a x(s,a). Strong convexity/concavity ensures unique, tractable solutions.
- **Core assumption:** The optimization problems remain feasible with deception constraints; β correctly balances deception strength against performance loss.
- **Evidence anchors:**
  - [abstract] "We show how each form of deception can be implemented in policy optimization problems and analytically bound the loss in total accumulated reward induced by deception."
  - [Section III-A, Theorem 1] "Theorem 1 reveals that the bound on the revenue loss increases linearly with the deception parameter β, which enables the design of β based on the maximum allowable revenue loss."
- **Break condition:** If β is too large, performance loss exceeds acceptable thresholds. If problem constraints (e.g., goal state visitation requirements) conflict with deception objectives, infeasibility may occur.

### Mechanism 3
- **Claim:** Decoy agents with manipulated policies create ambiguous or misleading adversary inferences in multi-agent settings.
- **Mechanism:** In the moving target defense (MTD) formulation, real agents and decoy agents operate together. Decoy agents have reduced rewards (1% of real agents). The adversary computes likelihood L_i = exp(D_i/λ)/∑ exp(D_i/λ) where D_i = ∑_y_i∈S_i w(y_i)R̃_i(y_i) measures estimated importance. Deceptive policies ensure real and decoy agents produce similar D_i values (equivocal) or decoy agents produce higher D_i values (targeted/diversionary).
- **Core assumption:** Adversary cannot distinguish agents a priori and must infer "real" agents solely from behavioral observations via IRL.
- **Evidence anchors:**
  - [Section IV, Numerical Results] "After estimating the agents' rewards using IRL, the adversary infers the real agent(s) as the agent(s) that have higher reward in state 'N' and lower reward in state 'B'."
  - [Section IV-A1, Table II] With β = 0.4, adversary's utility drops from 100 to 0, confirming successful diversionary deception.
- **Break condition:** If adversary has side-channel information about agent identities, or if decoy agent rewards are trivially distinguishable, deception fails.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** The paper models autonomous systems as MDPs (S, A, r, T, α) and uses occupancy measures derived from MDP structure as the primary policy representation. Understanding state spaces, transition dynamics, and reward functions is essential for interpreting the deception formulations.
  - **Quick check question:** Can you explain why occupancy measures satisfy the constraint ∑_a x(j,a) - γ∑_{s,a} T(s,a,j)x(s,a) = α(j)?

- **Concept: Inverse Reinforcement Learning (IRL)**
  - **Why needed here:** The adversary model assumes IRL is used to recover reward functions from observed trajectories. Understanding how apprenticeship learning, MaxEnt IRL, and Deep IRL extract feature expectations is critical for understanding attack surfaces the deception exploits.
  - **Quick check question:** How does feature expectation μ_E = (1/N)∑_{τ_j}∑_t φ(s_t^j, a_t^j) relate to the reward function learned by IRL?

- **Concept: Convex Optimization and Regularization**
  - **Why needed here:** All three deception types formulate policy synthesis as modified convex optimization problems. Understanding strong convexity/concavity, feasibility conditions, and regularization tradeoffs is necessary for implementing and tuning the approach.
  - **Quick check question:** Why does adding β(x - x*)^2 to a linear objective preserve convexity, and what happens to the solution as β → ∞?

## Architecture Onboarding

- **Component map:**
  Environment (MDP/MMDP) -> Policy Synthesis Module -> Deceptive Policy π_d (occupancy measures) -> Agent Execution (generates trajectories) -> Adversary IRL Module (AL / MaxEnt / Deep IRL) -> Adversary Inference (real vs. decoy agent identification)

- **Critical path:**
  1. Define MDP with goal states S_goal and decoy states S_decoy
  2. Solve base Optimization Problem 3 to obtain x* and optimal revenue R*
  3. Select deception type and formulate corresponding regularized problem
  4. Choose β based on allowable performance loss using Theorem bounds
  5. Solve regularized problem, extract policy via π(a|s) = x(s,a)/∑_{a'}x(s,a')
  6. Deploy policy and generate deceptive trajectories

- **Design tradeoffs:**
  - **β selection:** Higher β → stronger deception but larger performance loss. Theorem bounds are worst-case; empirical loss is typically much smaller.
  - **Target occupancy x^tar design (targeted deception):** Larger max_{s,a} x^tar(s,a) worsens worst-case performance bound. Designs closer to x* improve performance.
  - **Goal/decoy state selection (equivocal):** If optimal occupancy on S_goal and S_decoy is naturally similar, equivocal deception induces near-zero performance loss. Otherwise, β must compensate.
  - **Decoy agent count:** More decoy agents increase ambiguity for adversary but expand state space (|S| = |S_i|^{n+m}).

- **Failure signatures:**
  - **Infeasibility (equivocal):** Equality constraint ∑_{S_goal}x(s,a) = ∑_{S_decoy}x(s,a) may be unsatisfiable. Use Optimization Problem 7's soft constraint instead.
  - **Insufficient deception:** If adversary correctly identifies real agent, increase β. Verify adversary's L_i values in simulation.
  - **Excessive performance loss:** If revenue drops below threshold, decrease β or redesign target occupancy measures.
  - **IRL algorithm sensitivity:** Different IRL methods may respond differently. Test against multiple algorithms (AL, MaxEnt, Deep IRL).

- **First 3 experiments:**
  1. **Baseline validation:** Implement single-agent MDP with diversionary deception. Sweep β ∈ {0.1, 0.2, ..., 1.0}. Plot adversary utility vs. revenue loss. Verify Theorem 1 bound holds empirically.
  2. **Targeted deception with controlled x^tar:** Design two target occupancy measures—one close to x*, one far. Compare revenue loss and deception success. Validate Theorem 2 prediction that closer x^tar yields better performance.
  3. **Multi-agent equivocal deception:** Set up 5-agent MTD problem (1 real, 4 decoy). Test equivocal deception with β ∈ {10, 50, 100, 200}. Measure adversary inference distribution across agents and verify real-decoy likelihood convergence.

## Open Questions the Paper Calls Out
None

## Limitations
- The deception effectiveness depends on adversaries using IRL algorithms; if adversaries use alternative inference methods, the deception may fail
- The approach assumes perfect synchronization and no communication constraints between agents in multi-agent settings
- Theoretical bounds on performance loss are worst-case scenarios that may significantly overestimate actual degradation in practice

## Confidence
- **High confidence:** The policy optimization formulations are mathematically sound and the occupancy measure manipulation mechanism is well-established in RL literature. The numerical results showing successful deception in the network defense scenario are convincing.
- **Medium confidence:** The analytical bounds on performance loss are valid but represent worst-case scenarios that may not reflect practical behavior. The adversarial IRL model is reasonable but simplified compared to real-world adversaries who might use more sophisticated inference techniques.
- **Low confidence:** The robustness of deception strategies against adversaries who combine IRL with other inference methods (like model-based prediction or side-channel information) is unclear. The paper doesn't address how deception effectiveness scales with problem complexity or state space size.

## Next Checks
1. **IRL Algorithm Sensitivity:** Test deception strategies against multiple IRL algorithms (AL, MaxEnt, Deep IRL) and combinations thereof. Measure how deception effectiveness varies across algorithms and whether certain algorithms are more resistant to the proposed regularization approaches.

2. **Robustness to Prior Knowledge:** Evaluate deception performance when adversaries have partial prior knowledge about the system (e.g., known goal states, transition dynamics, or reward structure). Determine how much prior information degrades deception effectiveness and what compensation mechanisms might be needed.

3. **Scalability and Computational Complexity:** Implement the deception strategies on larger MDPs (10-100x state space size) and measure computational overhead. Assess whether the optimization problems remain tractable and whether deception effectiveness scales proportionally with problem complexity.