---
ver: rpa2
title: Devanagari Digit Recognition using Quantum Machine Learning
arxiv_id: '2506.09069'
source_url: https://arxiv.org/abs/2506.09069
tags:
- quantum
- devanagari
- hybrid
- classical
- digit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first application of hybrid quantum-classical
  machine learning to Devanagari handwritten digit recognition, combining a convolutional
  neural network for feature extraction with a 10-qubit variational quantum circuit
  for classification. The model achieves 99.80% test accuracy and 0.2893 test loss
  on the DHCD dataset, surpassing classical baselines with significantly fewer parameters
  (2.34 million total, only 100 in the quantum component).
---

# Devanagari Digit Recognition using Quantum Machine Learning

## Quick Facts
- arXiv ID: 2506.09069
- Source URL: https://arxiv.org/abs/2506.09069
- Authors: Sahaj Raj Malla
- Reference count: 17
- Primary result: First application of hybrid quantum-classical ML to Devanagari digit recognition, achieving 99.80% test accuracy with only 100 trainable quantum parameters

## Executive Summary
This study presents the first application of hybrid quantum-classical machine learning to Devanagari handwritten digit recognition. The approach combines a 4-layer convolutional neural network for feature extraction with a 10-qubit variational quantum circuit for classification. The model achieves 99.80% test accuracy on the DHCD dataset while using significantly fewer parameters than classical baselines (2.34 million total, only 100 in the quantum component). This demonstrates that quantum-enhanced architectures can improve recognition accuracy for complex scripts while maintaining efficiency.

## Method Summary
The hybrid architecture uses a classical CNN to extract 2048-dimensional features from 28×28 grayscale images, followed by a linear layer projecting to 1024 dimensions for amplitude embedding into a 10-qubit quantum circuit. The quantum layer applies 5 variational layers with RY/RZ rotations and ring-topology CNOT entanglement, measuring 55 observables (10 single-Z + 45 pairwise ZZ correlations). A final linear layer maps these to 10 output classes. Training uses AdamW optimizer, label smoothing, gradient clipping, and data augmentation including rotation, affine transforms, and elastic distortions.

## Key Results
- Achieves 99.80% test accuracy and 0.2893 test loss on DHCD digit subset
- Outperforms classical CNN baseline (99.03%) by 0.77% accuracy
- Uses only 100 trainable parameters in the quantum component versus millions in classical equivalents
- Sets new benchmark for regional script recognition while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Feature Division with Amplitude Embedding
The architecture achieves high accuracy by delegating spatial feature extraction to classical CNNs while using quantum circuits for compact classification-level feature transformation. A 4-layer CNN extracts 2048-dimensional features from 28×28 images. A linear layer projects this to 1024 dimensions (matching 2¹⁰ for 10 qubits), normalized via L2 norm for amplitude embedding. The quantum state preparation encodes all 1024 feature values as probability amplitudes across computational basis states.

### Mechanism 2: Entanglement-Enhanced Feature Discrimination
The ring-topology CNOT entanglement pattern may enable the circuit to capture correlations between features that classical linear classifiers would miss. Each of 5 variational layers applies RY/RZ rotations per qubit followed by CNOT gates in a ring (0→1→2→...→9→0). Measurements include both single-qubit Pauli-Z expectations and all 45 pairwise ZZ correlations, producing a 55-dimensional quantum feature vector.

### Mechanism 3: Parameter Efficiency via Quantum Compression
The quantum layer achieves competitive performance with dramatically fewer trainable parameters than equivalent classical classification heads. The VQC uses only 100 trainable parameters (10 qubits × 2 rotation angles × 5 layers), compared to millions in the CNN backbone. The final linear layer (55→10) adds only 560 parameters. This compression is enabled by quantum state space representing 2¹⁰ amplitudes with only 100 rotation parameters.

## Foundational Learning

- **Amplitude Embedding (Quantum State Preparation)**: Why needed here: The paper encodes 1024 classical features as quantum state amplitudes. Without understanding this, you cannot debug data-to-quantum pipeline issues. Quick check question: Given a normalized 4-element vector [0.5, 0.5, 0.5, 0.5], what 2-qubit quantum state does this represent?

- **Variational Quantum Circuits (VQCs)**: Why needed here: The entire quantum classification layer is a VQC trained via gradient descent. Understanding parameterized gates and measurement is essential for modification. Quick check question: How many trainable parameters exist in a 3-qubit VQC with 4 layers using RY and RZ rotations per qubit?

- **Hybrid Gradient Flow**: Why needed here: The model requires backpropagation through quantum circuits (via parameter-shift rule or similar). This affects training stability and optimizer choice. Quick check question: Why might gradient clipping (used in this paper with max norm 1.0) be particularly important for hybrid quantum-classical training?

## Architecture Onboarding

- **Component map**: Input (28×28 grayscale) -> CNN Backbone (4 conv layers, 2048 output) -> Classical Adapter (Linear 2048→1024, L2 Normalize) -> Quantum Layer (10-qubit amplitude embedding → 5 variational layers → 55 Pauli measurements) -> Classification Head (Linear 55→10) -> Loss (Label-smoothed cross-entropy)

- **Critical path**: Feature extraction correctness (CNN output dimensions match 2048) → Normalization stability (prevent division by zero with ε=10⁻⁸) → Quantum measurement validity (expectation values in [-1, 1]) → Gradient flow through quantum-classical boundary

- **Design tradeoffs**: Qubit count vs. input fidelity (10 qubits = 1024 amplitudes, forcing dimensionality reduction from 2048 CNN features) → Circuit depth vs. trainability (5 layers provide expressivity but risk barren plateaus) → Measurement strategy (55 observables captures correlations but may include redundant features)

- **Failure signatures**: NaN loss during training → check L2 normalization epsilon; Accuracy plateaus below 95% → verify amplitude embedding isn't truncating features; High variance across runs → quantum parameters initialized randomly; Validation >> test accuracy → overfitting to augmented training distribution

- **First 3 experiments**:
  1. **Baseline replication**: Train the exact architecture on DHCD digits with reported hyperparameters (lr=10⁻³, batch=32, no dropout, α=0.05, depth=5). Target: ≥99.5% test accuracy.
  2. **Ablation on quantum component**: Replace VQC with classical linear layer (1024→55→10) matching parameter count. Compare accuracy to quantify quantum contribution (paper reports 99.03% vs 99.80%).
  3. **Qubit scaling study**: Train 4, 6, 8, 10-qubit variants and plot accuracy vs. qubit count. Verify the paper's reported trend (99.47%→99.80%) and identify potential diminishing returns.

## Open Questions the Paper Calls Out

- **Question**: How does the hybrid model perform when deployed on real noisy quantum hardware compared to classical simulation? Basis: All quantum operations were simulated using PennyLane. Real-world deployment on noisy quantum hardware may affect stability and accuracy. Future work explicitly includes "Deployment on quantum hardware (e.g., IBM Q, Rigetti) to assess noise robustness."

- **Question**: Can the hybrid quantum-classical architecture scale to the full Devanagari character set (46 classes including consonants) while maintaining performance? Basis: The study focuses exclusively on Devanagari digits. Extending to the full character set (including 36 consonants) is a natural next step but requires more computational and data resources.

- **Question**: Does increasing qubit count and circuit depth beyond 10 qubits yield meaningful performance gains or encounter diminishing returns? Basis: The current architecture is constrained to 10 qubits. Exploring deeper or wider circuits with more qubits could improve expressive power, but will depend on hardware availability.

## Limitations

- The quantum advantage contribution cannot be fully isolated from architectural changes and random initialization effects
- Simulation-based validation doesn't account for noise, decoherence, and measurement errors present in real quantum hardware
- Limited dataset diversity (17,000 training images) may not represent the full complexity of handwriting styles in real-world applications

## Confidence

- **High**: CNN feature extraction effectiveness, basic hybrid architecture feasibility, parameter efficiency claims
- **Medium**: Quantum contribution to accuracy gains, generalization to larger script recognition problems
- **Low**: Hardware deployment feasibility, scalability to more complex character sets

## Next Checks

1. **Entanglement ablation**: Replace VQC's ZZ measurements with classical correlation features while maintaining parameter count. Compare accuracy to isolate quantum contribution.
2. **Hardware validation**: Run the 10-qubit circuit on real quantum hardware (e.g., IBM Quantum) and compare accuracy, latency, and resource usage to simulation.
3. **Script scalability**: Test the hybrid architecture on a more complex script (e.g., Chinese characters or Indic scripts with conjuncts) to evaluate generalizability beyond Devanagari digits.