---
ver: rpa2
title: On the locality bias and results in the Long Range Arena
arxiv_id: '2501.14850'
source_url: https://arxiv.org/abs/2501.14850
tags:
- transformer
- performance
- task
- long
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the nature of tasks in the Long Range Arena
  (LRA) benchmark and their implications for long-range dependency modeling. The authors
  find that the tasks in LRA are primarily positional and local, which favors architectures
  like SSMs and MEGA that incorporate inductive biases for locality.
---

# On the locality bias and results in the Long Range Arena

## Quick Facts
- arXiv ID: 2501.14850
- Source URL: https://arxiv.org/abs/2501.14850
- Reference count: 26
- One-line primary result: Small convolutional models with limited receptive fields can achieve near-SOTA results on LRA tasks, questioning the benchmark's validity for assessing true long-range dependency modeling.

## Executive Summary
This paper investigates the nature of tasks in the Long Range Arena (LRA) benchmark and their implications for long-range dependency modeling. The authors find that the tasks in LRA are primarily positional and local, which favors architectures like SSMs and MEGA that incorporate inductive biases for locality. Through empirical analysis, they demonstrate that small convolutional models with limited receptive fields can achieve near state-of-the-art results, questioning the validity of LRA as a true long-range dependency benchmark. The authors show that with appropriate training techniques and data augmentation, Transformers can achieve competitive performance, matching or exceeding specialized architectures. Their results suggest that the design constraints in SSMs primarily serve to add data efficiency rather than enable superior long-range modeling capabilities.

## Method Summary
The authors evaluate multiple architectures on LRA benchmark tasks including CIFAR10 (1024 pixels as 1D), Pathfinder (32×32), Text Classification (byte-level, 4000 tokens), Text Retrieval (ACL Anthology, 4000 bytes), and ListOps (nested math operations, 2000 tokens). They implement Transformers with rotary embeddings, convolutional models with various kernel sizes (5, 7, 11, 21, 31, 61), and compare against specialized architectures like MEGA. Key training techniques include multi-task denoising (30% token masking, 1/3 random tokens) for text tasks, AutoAugment-derived augmentation for CIFAR10, and training on all three Pathfinder difficulty sets (67 epochs vs 200). The authors conduct extensive ablation studies to isolate the effects of architectural choices versus training strategies.

## Key Results
- Small convolutional models with kernel size 61 achieve ~83% on CIFAR (vs 90% SOTA) and ~99% on Text Retrieval, demonstrating that global context is often unnecessary
- Transformers with rotary embeddings + training techniques match or exceed SOTA (avg 85.69%) when baseline results were ~54%
- The design choices behind SSMs (kernel decay, sublinear parameterization) primarily add data efficiency rather than enable superior long-range modeling capabilities

## Why This Works (Mechanism)

### Mechanism 1: Locality Bias in Benchmark Tasks
The high performance of specialized architectures on LRA likely stems from the benchmark's reliance on short-range dependencies rather than true long-range modeling requirements. Convolutional models with small, bounded kernels (receptive field ~30 tokens) aggregate information effectively because the signal in LRA tasks is locally concentrated. Architectures with built-in decay (SSMs, MEGA) effectively act as efficient local aggregators.

### Mechanism 2: SSM Constraints as Inductive Bias (Not Capability)
The specific mathematical constraints of State Space Models (SSMs)—such as kernel decay and sublinear parameterization—function primarily as data-efficient regularizers rather than enablers of superior modeling capacity. SSMs restrict the hypothesis space to smooth, decaying interactions, making them data-efficient on small datasets like LRA but technically less expressive than unconstrained long convolutions.

### Mechanism 3: Transformer Inefficiency as a Data Problem
The Transformer's historical failure on LRA is attributable to a mismatch between its high capacity/expressiveness and the low data/regime of the benchmark, rather than an inability to model long contexts. Transformers lack strong inductive biases for locality, and in small data regimes they overfit or learn spurious correlations. Injecting positional biases (Rotary Embeddings) and regularization (Augmentation/Denoising) bridges this gap.

## Foundational Learning

- **Concept: Receptive Field vs. Effective Context**
  - **Why needed here:** To understand why a model with a limit of 30 tokens interaction range can solve "Long Range" tasks.
  - **Quick check question:** Can a 6-layer network with kernel size 5 connect two pixels 100 steps apart? (Answer: No, max range is $6 \times 2 = 12$).

- **Concept: Rotary Positional Embeddings (RoPE)**
  - **Why needed here:** The paper identifies RoPE as a key mechanism for injecting "time-decay" and relative position biases into Transformers, mimicking SSMs.
  - **Quick check question:** Does RoPE add absolute position vectors to embeddings or rotate query/key vectors? (Answer: Rotates vectors based on relative distance).

- **Concept: Inductive Bias in Sequence Modeling**
  - **Why needed here:** To distinguish between an architecture's *efficiency* (learning fast with little data due to decay/locality assumptions) vs. its *capability* (ultimate performance ceiling).
  - **Quick check question:** Does a "time-decay" kernel assumption help more with local pattern extraction or global state tracking? (Answer: Local pattern extraction).

## Architecture Onboarding

- **Component map:** Input Sequence -> Core Architecture (Transformer with RoPE OR SSM OR Convolution) -> Regularizer (Multitask Denoising OR Geometric Augmentation) -> Classification Output

- **Critical path:**
  1. Implement Rotary Embeddings (RoPE) for the Transformer backbone
  2. Implement the "Small Convolution" baseline (Table 1) to establish the locality floor for any new task
  3. Add the specific data augmentation strategy (e.g., AutoAugment for CIFAR, Operand Shuffling for ListOps)

- **Design tradeoffs:**
  - **SSM/MEGA:** High data efficiency, fast inference. Tradeoff: Hardcoded locality bias may fail on tasks requiring true global state tracking
  - **Vanilla Transformer:** High expressiveness, theoretically global. Tradeoff: Computationally expensive and requires massive data/augmentation to converge on LRA
  - **gMLP (Unrestricted):** Maximum convolution expressiveness. Tradeoff: $O(L^2)$ parameterization without specialized efficiency tricks

- **Failure signatures:**
  - Transformer Random Guessing: If training techniques are removed, the model may settle on random heuristics (e.g., ListOps accuracy ~17% based on majority class)
  - Overfitting on Text: Without multitask denoising, byte-level models overfit to local n-grams rather than semantic meaning

- **First 3 experiments:**
  1. **Locality Baseline:** Train a 6-layer Conv net with kernel size 11 on your target LRA task. If accuracy > 80% of SOTA, the task is likely local, not global
  2. **RoPE Ablation:** Train a Transformer with Learned Positional Embeddings vs. RoPE. Expect a significant divergence in ListOps and Pathfinder tasks
  3. **Augmentation Impact:** Train on Pathfinder using only "hard" examples vs. "easy + intermediate" sets. Observe if the model learns faster with the varied curriculum

## Open Questions the Paper Calls Out

### Open Question 1
How can the Long Range Arena benchmark be redesigned to genuinely assess long-range dependencies rather than rewarding positional and local inductive biases? The authors conclude that their "findings point to a significant re-evaluation of both model architectures and the LRA benchmark" and they "leave the design of such a benchmark for future work." This remains unresolved because the paper demonstrates that current LRA tasks can be solved with small receptive fields (e.g., kernel size 5 for text), suggesting they fail to test true long-range capability.

### Open Question 2
To what extent do local patterns account for the accuracy improvements observed in the ListOps task? In Appendix B, regarding the ListOps task, the authors state: "The extent to which local patterns account for the increase in accuracy to 63% escapes our analysis and still needs to be studied empirically." While the authors identify global distributional biases, the specific mechanism by which local patterns improve performance from baseline to SOTA remains unverified.

### Open Question 3
How can training strategies be standardized to decouple the evaluation of learning efficiency from long-range modeling capabilities? The authors note that "Standardizing such techniques for this or future benchmarks is interesting because it decouples the evaluation of learning efficiency from that of long-range modeling capabilities." This remains unresolved because the study shows that Transformers match SOTA results when using specific training techniques, conflating the model's raw architectural capacity with its data efficiency.

## Limitations

- The findings are based on a specific set of five LRA benchmark tasks, which may not generalize to all long-range dependency tasks
- The paper does not test these findings on alternative long-range benchmarks or real-world applications where long-range dependencies are more pronounced
- The computational efficiency trade-offs between different architectures are not quantified in depth

## Confidence

**High Confidence Claims:**
- The locality bias in LRA tasks is well-supported by empirical evidence showing that small convolutional models achieve near-SOTA results
- The effectiveness of training techniques (rotary embeddings, augmentation, denoising) for Transformers on LRA is convincingly demonstrated
- The characterization of LRA as primarily a local rather than long-range benchmark is strongly supported

**Medium Confidence Claims:**
- The assertion that SSM constraints function primarily as inductive bias rather than enabling superior modeling capacity is supported but could benefit from testing on non-LRA tasks
- The claim that SSM design choices add efficiency rather than capability is reasonable but requires validation beyond the LRA benchmark

**Low Confidence Claims:**
- The broader implication that all specialized architectures for long-range modeling may be unnecessary is an overreach from the LRA-specific findings
- The call for LRA redesign, while logical, lacks specific proposals for what alternative benchmarks would better capture long-range dependencies

## Next Checks

1. **Cross-Benchmark Validation**: Test the locality hypothesis by applying the small convolutional models to other long-range benchmarks like LongBench or PG-19. If these models maintain high performance across diverse long-range tasks, the locality bias claim strengthens; if performance drops significantly, it suggests LRA is indeed unrepresentative.

2. **Architectural Efficiency Comparison**: Quantify the total computational cost (FLOPs during training and inference) of achieving competitive LRA performance using Transformers with full training techniques versus specialized architectures like SSMs or MEGA. This would validate whether the "no benefit" claim holds when considering efficiency, not just accuracy.

3. **True Long-Range Task Creation**: Design a synthetic task where accuracy depends critically on information from distant positions (e.g., requiring aggregation of tokens separated by >1000 positions). Test whether the locality-biased architectures fail on this task while Transformers succeed, or whether even Transformers struggle, indicating that genuine long-range modeling remains challenging regardless of architecture.