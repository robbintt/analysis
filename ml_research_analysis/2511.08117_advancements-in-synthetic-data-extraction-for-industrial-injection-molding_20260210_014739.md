---
ver: rpa2
title: Advancements in synthetic data extraction for industrial injection molding
arxiv_id: '2511.08117'
source_url: https://arxiv.org/abs/2511.08117
tags:
- data
- synthetic
- training
- injection
- molding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of insufficient data for machine
  learning in industrial injection molding by exploring synthetic data generation.
  The approach involves simulating injection molding processes using CAD software
  and a production simulator to generate synthetic time series data, which is then
  labeled using an existing classifier.
---

# Advancements in synthetic data extraction for industrial injection molding

## Quick Facts
- arXiv ID: 2511.08117
- Source URL: https://arxiv.org/abs/2511.08117
- Reference count: 16
- Synthetic data improves injection molding quality classification while reducing manual data collection needs

## Executive Summary
This paper addresses the challenge of insufficient training data for machine learning in industrial injection molding by developing a synthetic data generation approach. The method combines CAD software with a production simulator to create synthetic time series data representing injection molding cycles, which are then labeled using an existing classifier. An LSTM network is trained on datasets containing varying proportions of synthetic data (0-30%), showing that validation accuracy remains stable while training accuracy decreases slightly. The approach demonstrates improved robustness to perturbations and better F1 scores, suggesting synthetic data can effectively augment real datasets and reduce the need for extensive manual labor, machine usage, and material waste.

## Method Summary
The approach generates synthetic time series data by simulating injection molding processes using CAD software and a production simulator. These synthetic cycles are labeled using an existing classifier based on CAD fill study visualizations. The synthetic data is combined with real sensor data (34 features sampled at 10ms intervals from 275 real cycles, augmented to 1100 via 4x downsampling) in proportions ranging from 0% to 30% synthetic. An LSTM network with three layers (100 units each) and dropout regularization is trained using Adam optimizer (learning rate 0.0001175) for 50 epochs. The model follows a unidirectional many-to-one architecture, mapping variable-length time series to binary quality labels.

## Key Results
- Training accuracy decreases from 92.8% to 85.5% as synthetic data proportion increases from 0% to 30%
- Validation accuracy remains stable, averaging 92.3% with 30% synthetic data versus 93.6% without synthetic data
- Model robustness to perturbations improves with synthetic data inclusion
- F1 score and AUC-ROC metrics show enhancement with synthetic data augmentation

## Why This Works (Mechanism)

### Mechanism 1: Class Imbalance Compensation Through Targeted Synthetic Generation
Deliberately over-generating defective samples in simulation counteracts the natural scarcity of defects in real production data. Real industrial datasets are heavily imbalanced because companies avoid deliberate production waste, while the simulation pipeline generates synthetic data with 60% "not good" cycles versus 40% good (inverted from real data's 56.5%/43.5%).

### Mechanism 2: Temporal Feature Learning via LSTM on Process Time Series
LSTM architectures capture sequential dependencies in injection molding cycle data that map to quality outcomes. The 34 input features form time series where early process deviations propagate through the cycle, with LSTM's memory cells retaining information across hundreds of timesteps.

### Mechanism 3: Regularization Through Synthetic Noise Injection
Adding synthetic data acts as implicit regularization, reducing overfitting while maintaining validation performance. Training accuracy drops significantly with increased synthetic data, but validation accuracy only decreases by 1.3 percentage points, preventing memorization of real training data.

## Foundational Learning

- **Concept: LSTM sequence-to-one classification** - Why needed: The model maps variable-length time series (injection cycles) to binary quality labels. You must understand how hidden states evolve across timesteps and how dropout between LSTM layers prevents overfitting. Quick check: Can you explain why the first two LSTM layers have `return_sequences=True` but the third doesn't?

- **Concept: Injection molding process physics** - Why needed: Feature engineering and labeling require understanding that parameters like "holding pressure level," "changeover point," and "residual compound cushion" directly affect fill completeness. Quick check: Why would a fast injection speed combined with low holding pressure cause the "not good" outcome shown in Figure 2 (right panel)?

- **Concept: Train/validation split with synthetic data contamination** - Why needed: The 33% validation split contains ONLY real data—synthetic data is never mixed into validation. This prevents leakage where simulator artifacts artificially inflate metrics. Quick check: If you accidentally included synthetic samples in validation, what specific artifact might cause misleadingly high accuracy?

## Architecture Onboarding

- **Component map:**
  ```
  Real Sensor Data (34 features @ 10ms) ─┐
                                        ├─→ Augmentation (4x) ─→ Training Pool
  CAD + Production Simulator ─→ Synthetic Time Series ─→ Manual Labeling ─┘
                                                                    │
                                                          LSTM (3 layers)
                                                          100 units each
                                                          Dropout: 0.16/0.16/0.28
                                                                    │
                                                          Dense(1, sigmoid)
                                                                    │
                                                          Binary Quality Label
  ```

- **Critical path:**
  1. Parameter selection in production simulator (injection speed, changeover, holding pressure)
  2. CAD fill study visual validation for labeling
  3. Manual label assignment based on expert assessment
  4. Time series concatenation with real data in specified proportions
  5. LSTM training with Adam optimizer (lr=0.0001175), MSE loss, 50 epochs

- **Design tradeoffs:**
  - Manual vs. automated labeling: Current approach uses "expertise and knowledge of parameters" + CAD output, but authors flag this as "increased time and effort"—future work targets automated classifier
  - Unidirectional vs. bidirectional LSTM: Many-to-one unidirectional chosen; bidirectional could capture backward dependencies but doubles computation and may overfit on small dataset
  - 30% synthetic ceiling: Authors stopped at 30% but suggest future work to "increase synthetic fraction as much as possible"—current limit may reflect diminishing returns or instability at higher ratios

- **Failure signatures:**
  - Training accuracy diverges significantly from validation accuracy → overfitting to real data memorization
  - Validation loss curves show high variance across runs → synthetic data introducing instability rather than robustness
  - F1 score drops dramatically for minority class → synthetic defects not matching real defect distribution

- **First 3 experiments:**
  1. **Baseline replication:** Train on 0% synthetic data, verify ~93.6% validation accuracy with stated hyperparameters. If significantly different, check data augmentation (4x via 10ms offset shifting) is implemented correctly.
  2. **Synthetic fraction sweep:** Train at 5%, 10%, 15%, 20%, 25%, 30% synthetic. Plot training vs. validation accuracy gap. Expect training accuracy to decline while validation stabilizes—flag if validation degrades >3 percentage points.
  3. **Class distribution audit:** Compare confusion matrices at 0% vs. 30% synthetic. Verify that "not good" recall improves (or at least doesn't degrade) when synthetic data with 60% defect rate is added.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Generative Adversarial Networks (GANs) generate synthetic injection molding data that captures noise and uncertainties of real-world data more effectively than the current CAD-simulation approach?
- Basis in paper: The authors explicitly state as future work: "we will try to use Generative Adversarial Networks (GANs) to generate more synthetic data that also take into account the noise and uncertainties of real-world data."
- Why unresolved: The current approach relies on CAD simulation and a production simulator, which may not fully capture the noise and uncertainties inherent in real production environments.
- What evidence would resolve it: Comparative experiments between GAN-generated and simulation-generated synthetic data, measuring how closely each matches real data characteristics and their respective impacts on model performance.

### Open Question 2
- Question: What is the maximum proportion of synthetic data that can be incorporated into training datasets before model performance significantly degrades?
- Basis in paper: The authors state: "In the next studies, we will try to increase the synthetic fraction in our data as much as possible." They only tested up to 30% synthetic data.
- Why unresolved: The study only evaluated synthetic data proportions up to 30%, leaving unknown the upper limits of synthetic data incorporation before performance degradation.
- What evidence would resolve it: Systematic experiments with progressively higher proportions of synthetic data (40%, 50%, 60%, etc.) to identify the threshold where validation accuracy and F1 scores begin to significantly decline.

### Open Question 3
- Question: Can an automated classifier accurately label synthetic injection molding data to eliminate the need for manual labeling?
- Basis in paper: The authors mention: "For a less complex label process to evaluate the production cycles, we will try to train an independent classifier in the future that can label the synthetic data." They also note in the Evaluation section that "the labeling of the injection molding cycles had to be done manually, as our automated method for this crucial step had not yet been worked out."
- Why unresolved: Manual labeling increases time and effort, reducing the efficiency gains of synthetic data generation. An automated approach would streamline the pipeline but hasn't been developed or tested.
- What evidence would resolve it: Implementation and validation of an automated labeling system, comparing its accuracy and consistency against manual expert labeling across diverse synthetic injection molding cycles.

## Limitations

- Absence of validation protocol for synthetic data quality—assumes CAD fill patterns adequately represent real defect physics without quantitative comparison
- 30% synthetic ceiling appears arbitrary with no analysis of optimal ratios or performance degradation beyond this threshold
- Critical implementation details remain unspecified, including exact 34 feature definitions, normalization procedures, and how quadrupling augmentation affects temporal dependencies

## Confidence

**High Confidence:**
- Training accuracy decreases while validation accuracy remains stable when synthetic data is added
- LSTM architecture successfully performs binary classification on injection molding time series
- Manual labeling using CAD fill studies provides adequate ground truth

**Medium Confidence:**
- 30% synthetic data ratio represents optimal balance between augmentation and overfitting
- Synthetic data improves model robustness to perturbations
- Inverted class distribution (60% defective synthetic) effectively compensates for real data imbalance

**Low Confidence:**
- Synthetic data generation process reliably captures sufficient real-world defect variation

## Next Checks

1. **Distribution Shift Analysis**: Quantitatively compare synthetic vs. real feature distributions (mean, std, KL divergence) to verify synthetic data falls within real data support without systematic bias
2. **Cross-Validation Stability**: Run 5-fold cross-validation with 0% and 30% synthetic data to assess variance in validation accuracy (reported as single average: 92.3% ± ?)
3. **Error Analysis**: Examine confusion matrices for both conditions, particularly focusing on false positive/negative rates for the minority class to verify synthetic defects improve minority class recall