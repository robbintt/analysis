---
ver: rpa2
title: 'Textual Bayes: Quantifying Uncertainty in LLM-Based Systems'
arxiv_id: '2506.10060'
source_url: https://arxiv.org/abs/2506.10060
tags:
- bayesian
- mhlp
- uncertainty
- prompt
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Textual Bayes, a method for quantifying uncertainty
  in LLM-based systems by treating prompts as textual parameters in a Bayesian model.
  It addresses the challenge of accurately quantifying uncertainty in black-box LLMs,
  which is critical for high-stakes applications.
---

# Textual Bayes: Quantifying Uncertainty in LLM-Based Systems

## Quick Facts
- arXiv ID: 2506.10060
- Source URL: https://arxiv.org/abs/2506.10060
- Reference count: 40
- Primary result: Introduces MHLP, an MCMC algorithm using LLM-based proposals to quantify uncertainty over prompts and improve calibration on AIME, SimpleQA, and QASPER benchmarks

## Executive Summary
Textual Bayes addresses the challenge of uncertainty quantification in black-box LLM-based systems by treating prompts as textual parameters in a Bayesian framework. The core innovation is Metropolis-Hastings through LLM Proposals (MHLP), which samples from posterior distributions over prompts using LLM-based proposals, enabling principled uncertainty quantification over both prompts and downstream predictions. Empirically, Textual Bayes improves both predictive accuracy and uncertainty quantification on benchmarks like AIME, SimpleQA, and QASPER, achieving better calibration (lower ECE and SECE) and higher accuracy than baselines. The method is a turnkey modification applicable to closed-source models and opens avenues for integrating Bayesian methods into LLM-based systems.

## Method Summary
Textual Bayes treats prompts as parameters in a Bayesian model, using Metropolis-Hastings through LLM Proposals (MHLP) to sample from posterior distributions over prompts. The method specifies priors via free-form textual constraints, generates proposals using prompt optimization techniques like TextGrad, and estimates likelihoods via mini-batch approximations with surrogate models when needed. For new inputs, predictive uncertainty is computed by aggregating outputs from multiple sampled prompts, with confidence scores derived from semantic clustering of responses.

## Key Results
- MHLP improves accuracy and calibration vs. baselines on AIME, SimpleQA, and QASPER benchmarks
- Achieves lower Expected Calibration Error (ECE) and Semantic Expected Calibration Error (SECE) than CoT, TextGrad, and perturbation baselines
- Outperforms methods in factuality tasks, demonstrating superior calibration of confidence scores
- Shows effectiveness on both free-form generation (QASPER) and structured reasoning tasks (AIME)

## Why This Works (Mechanism)

### Mechanism 1: LLM-Based Proposal Distribution for Textual MCMC
MHLP generates semantically coherent prompt proposals using prompt optimization methods as the proposal distribution q(θ′ | θ). The optimizer proposes prompts that satisfy prior constraints and improve likelihood on training data, with the MH accept/reject step correcting for any bias in this proposal. Core assumption: prompt optimization methods can generate proposals close enough to high-posterior-density regions to make MCMC tractable.

### Mechanism 2: Textual Prior Specification via Free-Form Constraints
Human priors over prompts are expressed as natural language constraints and sampled via LLM generation. Practitioners write constraint strings describing desired prompt properties, and an LLM generates θ_i ~ p(θ_i) by conditioning on these constraints. Core assumption: LLMs can faithfully sample from the implicit distribution defined by textual constraints.

### Mechanism 3: Posterior Predictive via Monte Carlo Integration
Uncertainty over prompts propagates to calibrated uncertainty over outputs through Monte Carlo averaging. Sample m prompts from posterior via MHLP, then compute p(y_new | x_new, D) ≈ (1/m) Σ p(y_new | x_new, θ^(j)). Downstream uncertainty follows from this distribution. Core assumption: posterior samples are representative of p(θ | D); approximations do not severely distort the distribution.

## Foundational Learning

- **Metropolis-Hastings Algorithm**
  - Why needed here: MHLP is a specialized MH variant; understanding the accept/reject step, proposal distributions, and convergence diagnostics is prerequisite.
  - Quick check question: Given proposal q(θ′ | θ) and unnormalized density g(θ), derive the MH acceptance probability γ.

- **Bayesian Inference Fundamentals**
  - Why needed here: The framework relies on Bayes' rule, posterior vs. likelihood distinction, and the cold posterior effect.
  - Quick check question: Explain why the posterior predictive p(y_new | x_new, D) marginalizes over θ rather than using a point estimate.

- **LLM-Based Systems as Statistical Models**
  - Why needed here: Viewing prompts as parameters θ in p(y | x, θ) is the core conceptual shift; understanding LBS(x; θ) as a random function is essential.
  - Quick check question: In a chain-of-thought system, what sources of stochasticity contribute to p(y | x, θ) beyond token-level sampling?

## Architecture Onboarding

- **Component map:** Prior specification (text constraints) -> Prior sampling (LLM generation) -> MHLP sampler (proposal via TextGrad -> likelihood estimation -> accept/reject) -> Posterior predictive (Monte Carlo integration)

- **Critical path:** Getting likelihood estimation right is the bottleneck. If p(y | x, θ) is misestimated (e.g., surrogate model mismatch, batching noise), MHLP accepts/rejects incorrectly, and posterior samples are biased.

- **Design tradeoffs:**
  - **Temperature τ:** Cold posteriors (τ < 1) may improve accuracy but deviate from true Bayesian posterior
  - **Batch size b:** Smaller batches increase noise but reduce cost; paper uses b = 1
  - **Chain length T, burn-in d, thinning h:** Longer chains improve mixing but multiply LLM API costs
  - **Surrogate model choice:** Using weaker open-source model for likelihood may skew accept/reject decisions

- **Failure signatures:**
  - **Low acceptance rate:** Proposal distribution misaligned with posterior; check optimizer objectives and constraint satisfaction
  - **Degenerate posterior:** All samples cluster around one prompt; insufficient mixing or over-aggressive tempering
  - **Miscalibrated confidence:** SECE/ECE high despite MHLP; check likelihood estimation quality and surrogate model alignment
  - **Divergent prompts:** Proposals become incoherent; add stricter prior constraints or cap optimizer steps

- **First 3 experiments:**
  1. **Sanity check MHLP on synthetic task:** Use a dataset where ground-truth optimal prompt is known; verify MHLP recovers high-likelihood prompts and acceptance rate is reasonable (>10%)
  2. **Ablate proposal method:** Compare TextGrad vs. simpler proposers (e.g., random paraphrase) on acceptance rate and downstream ECE; quantify contribution of optimization-based proposals
  3. **Stress-test likelihood surrogate:** Run MHLP with GPT-4o as primary but Llama-3.1-Nemotron-70B as surrogate; compare posterior samples vs. using GPT-4o log-probs directly to measure surrogate-induced bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational cost of MHLP be reduced to near-baseline levels while preserving posterior quality?
- Basis in paper: "MCMC is costly; despite consuming similar inference compute to leading baselines, Textual Bayes requires a one-time expensive application of MHLP. This cost might be addressed, for example, by further engineering the underlying prompt optimization method or training a small language model specifically for the task of generating proposals."
- Why unresolved: MCMC's sequential nature fundamentally requires many iterations; current proposal generation relies on expensive LLM calls.
- What evidence would resolve it: Demonstration of a proposal distribution that achieves comparable effective sample size in ≤10× fewer LLM calls.

### Open Question 2
- Question: How do the approximations (mini-batching, surrogate models, Monte Carlo) affect convergence to the true posterior over prompts?
- Basis in paper: "Our method involves several approximations which will inevitably cause samples to deviate from the true posterior."
- Why unresolved: No theoretical analysis is provided bounding the approximation error; empirical evaluation only measures downstream performance, not posterior fidelity.
- What evidence would resolve it: Theoretical bounds on KL divergence between approximate and true posteriors, or empirical validation using synthetic settings where the true posterior is known.

### Open Question 3
- Question: Can MHLP be effectively adapted to alignment or safety objectives beyond Bayesian inference?
- Basis in paper: "We could use MHLP to modulate the outputs of LLM-based systems in accordance with unnormalized functions quantifying objectives such as alignment or safety."
- Why unresolved: The conformal factuality experiment shows promise, but alignment objectives involve complex, potentially uncalibrated reward signals.
- What evidence would resolve it: Successful application of MHLP with alignment reward models demonstrating improved safety metrics without performance degradation.

### Open Question 4
- Question: Does the assumption of independent textual priors significantly impact posterior quality in multi-prompt systems?
- Basis in paper: "For simplicity, we construct our prior p(θ) = ∏p(θi) by assuming that all textual variables are independent, but this setup can be easily generalized."
- Why unresolved: Multi-prompt agentic systems often have semantically related prompts where dependencies matter; no experiments test multi-prompt settings.
- What evidence would resolve it: Comparison of independent vs. joint priors on a multi-prompt LLM pipeline, measuring calibration and accuracy differences.

## Limitations

- Prior specification via free-form textual constraints lacks empirical validation and may not accurately reflect practitioner beliefs
- Proposal distribution quality critically affects MHLP performance, but the paper doesn't provide direct evidence that proposals adequately explore the posterior space
- Likelihood estimation using mini-batches and surrogate models introduces significant approximation error that may affect posterior quality and downstream calibration

## Confidence

- **High Confidence**: Core MCMC framework and mathematical derivations are sound; empirical results showing improved calibration are reproducible
- **Medium Confidence**: Factuality calibration improvements depend on surrogate likelihood quality and correlation assumptions
- **Low Confidence**: Effectiveness of free-form textual priors and LLM-based sampling from implicit distributions remain unproven