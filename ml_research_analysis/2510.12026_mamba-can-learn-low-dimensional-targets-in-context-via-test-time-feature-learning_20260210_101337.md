---
ver: rpa2
title: Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning
arxiv_id: '2510.12026'
source_url: https://arxiv.org/abs/2510.12026
tags:
- mamba
- learning
- in-context
- have
- logd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes Mamba's in-context learning (ICL) capability
  through a theoretical lens, focusing on tasks defined by low-dimensional nonlinear
  target functions. The authors introduce a framework combining input embeddings,
  a simplified Mamba architecture, and a two-stage gradient-based pretraining algorithm.
---

# Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning

## Quick Facts
- arXiv ID: 2510.12026
- Source URL: https://arxiv.org/abs/2510.12026
- Reference count: 40
- Mamba achieves competitive performance with Transformers on low-dimensional nonlinear tasks while maintaining computational efficiency

## Executive Summary
This paper provides a theoretical analysis of Mamba's in-context learning (ICL) capabilities, focusing on tasks defined by low-dimensional nonlinear target functions. The authors develop a framework that demonstrates Mamba can perform test-time feature learning, extracting relevant task features directly from context examples. Through rigorous mathematical proofs and experimental validation, the paper establishes that Mamba achieves performance comparable to nonlinear Transformers while maintaining the computational efficiency benefits of selective state spaces.

## Method Summary
The authors introduce a theoretical framework combining input embeddings, a simplified Mamba architecture, and a two-stage gradient-based pretraining algorithm. The framework models in-context learning as a feature learning problem where the model must extract relevant task features from context examples. The pretraining stage uses gradient-based optimization to learn parameters that enable effective feature extraction during test-time inference. The analysis focuses on single-index model tasks and establishes sample complexity bounds that improve upon linear Transformers while matching nonlinear Transformers.

## Key Results
- Mamba can perform test-time feature learning, extracting relevant task features from context examples
- Sample complexity bounds improve upon linear Transformers and match nonlinear Transformers
- The generative exponent (rather than information exponent) determines pretraining sample complexity
- Mamba achieves competitive performance with Transformers on single-index model tasks while outperforming kernel methods

## Why This Works (Mechanism)
Mamba's nonlinear gating mechanism enables effective feature extraction during in-context learning. The selective state space design allows the model to learn task-specific features from context examples, overcoming limitations of purely linear recurrent updates. The two-stage pretraining process ensures the model develops the capacity for test-time adaptation without requiring additional gradient steps during inference.

## Foundational Learning

**Gradient-based pretraining** - Needed to establish initial parameter values that enable effective test-time feature learning; quick check: verify pretraining loss decreases over iterations

**Single-index model theory** - Required for analyzing low-dimensional nonlinear target functions; quick check: confirm input data follows the assumed statistical properties

**Sample complexity analysis** - Essential for understanding the relationship between training data size and generalization; quick check: validate theoretical bounds against empirical performance

## Architecture Onboarding

**Component map:** Input embeddings -> Simplified Mamba layer -> Selective state space mechanism -> Feature extraction module -> Output prediction

**Critical path:** Context examples are processed through the Mamba layer, where the selective state space mechanism performs nonlinear feature extraction. The extracted features are then used to make predictions on new inputs.

**Design tradeoffs:** Mamba trades some modeling capacity compared to full Transformers for significant computational efficiency gains. The simplified architecture focuses on core mechanisms while maintaining theoretical tractability.

**Failure signatures:** Poor performance on high-dimensional tasks, failure to extract relevant features from context when input distributions deviate significantly from assumptions, and suboptimal scaling with sequence length.

**First experiments:**
1. Test Mamba's performance on synthetic single-index model tasks with varying dimensionalities
2. Compare sample complexity against linear and nonlinear Transformer baselines
3. Evaluate feature extraction quality through visualization of learned representations

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the extension of theoretical analysis to more complex tasks beyond single-index models, the behavior of more advanced Mamba architectures, and the robustness of findings under different input distributions and noise conditions.

## Limitations
- Theoretical analysis focuses specifically on single-index model tasks
- Simplified Mamba architecture may not capture full complexity of real-world implementations
- Assumptions about input distributions may not hold in practical applications
- Experimental validation is limited in scope and may not generalize to all in-context learning scenarios

## Confidence
- High confidence in theoretical framework and mathematical proofs
- Medium confidence in experimental validation due to limited scope
- Medium confidence in generalizability of findings to more complex tasks

## Next Checks
1. Extend experiments to more diverse and complex in-context learning tasks beyond single-index models
2. Test the theoretical framework with more advanced Mamba architectures and larger-scale implementations
3. Investigate the robustness of the findings under different input distributions and noise conditions