---
ver: rpa2
title: Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning
  Models
arxiv_id: '2602.02136'
source_url: https://arxiv.org/abs/2602.02136
tags:
- safety
- reasoning
- alignment
- distribution
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety tax problem in large reasoning
  models (LRMs), where safety alignment degrades general reasoning ability. The authors
  identify that existing safety alignment datasets exhibit distributional gaps with
  target LRMs, hypothesizing this gap causes safety tax.
---

# Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models

## Quick Facts
- arXiv ID: 2602.02136
- Source URL: https://arxiv.org/abs/2602.02136
- Authors: Yingsha Xie; Tiansheng Huang; Enneng Yang; Rui Min; Wenjie Lu; Xiaochun Cao; Naiqiang Tan; Li Shen
- Reference count: 16
- Primary result: DGR achieves +30.2% improvement in average reasoning accuracy while maintaining safety performance compared to Vanilla SFT

## Executive Summary
This paper addresses the "safety tax" problem in large reasoning models (LRMs), where safety alignment degrades general reasoning ability. The authors identify that existing safety alignment datasets exhibit distributional gaps with target LRMs, hypothesizing this gap causes safety tax. Their solution, Distribution-Grounded Refinement (DGR), transforms out-of-distribution safety reasoning data to align with the target model's distribution through a two-stage pipeline: (1) prompting the target model to rephrase reasoning traces and answers, and (2) quality control filtering to remove overthinking and meta-thinking samples. Experimental results show DGR effectively mitigates safety tax while maintaining safety performance.

## Method Summary
DGR is a two-stage pipeline that transforms external safety datasets to align with a target LRM's distribution. First, the target model rephrases reasoning traces and answers from external safety data using templates that preserve safety semantics while adapting to the model's native distribution. Second, quality control filtering removes samples exhibiting "overthinking" (excessive token generation without terminal tags) and "meta-thinking" (instructional self-commentary instead of substantive responses). The refined dataset is then used for standard supervised fine-tuning. The approach assumes that distributional alignment reduces the safety tax by preventing the model from drifting away from its original reasoning patterns during safety alignment.

## Key Results
- DGR achieves +30.2% improvement in average reasoning accuracy compared to Vanilla SFT on DirectRefusal benchmark
- The degree of reasoning degradation correlates with the extent of distribution shift between safety datasets and target models
- Safety alignment functions primarily as a knowledge activation mechanism, with as few as 10 samples sufficient to activate effective refusal behaviors

## Why This Works (Mechanism)

### Mechanism 1
Distribution shift between external safety data and the target model's native distribution drives reasoning capability degradation (safety tax). When fine-tuning on out-of-distribution (OOD) safety reasoning traces distilled from external LRMs, the target model's output distribution drifts away from its original reasoning patterns, disrupting learned reasoning circuits and causing accuracy drops on benchmarks.

### Mechanism 2
Prompting the target model to rephrase external safety data aligns the training distribution with its native reasoning style, reducing safety tax. DGR uses the target model to rewrite reasoning traces and answers from external datasets, transforming OOD data into in-distribution samples while preserving safety semantics.

### Mechanism 3
Quality control filtering removes samples that introduce noise or induce undesirable behaviors during fine-tuning. Two failure modes are filtered: (1) "overthinking" samples where models generate unbounded reasoning without terminal tags (>5,000 tokens), and (2) "meta-thinking" samples where models produce instructional self-commentary instead of substantive responses.

## Foundational Learning

- **Safety Tax / Alignment Tax**: The core problem this paper addresses—understanding why safety alignment degrades reasoning helps contextualize DGR's value proposition. Quick check: Can you explain why adding safety training data might hurt a model's math performance?

- **Chain-of-Thought (CoT) Reasoning in LRMs**: DGR operates on CoT traces; understanding the `think` / `answer` structure is essential for data processing. Quick check: What is the difference between the reasoning trace and the final answer in an LRM output?

- **Distribution Shift in Fine-tuning**: The central hypothesis relies on measuring and mitigating distribution shift; familiarity with metrics (BLEU, embedding similarity) is prerequisite. Quick check: If fine-tuning on dataset A causes test accuracy on task B to drop, what are two possible causes?

## Architecture Onboarding

- **Component map**: External safety dataset -> Target model rephrasing -> Quality control filtering -> Refined dataset -> SFT training
- **Critical path**: Load external safety dataset -> Generate rephrased CoT and answer using target model -> Apply quality filters (token limit + keyword detection) -> Fallback to original on failure -> Train target model on refined dataset via SFT
- **Design tradeoffs**: Rephrasing adds inference cost but reduces distribution shift (+30.2% avg reasoning accuracy); aggressive filtering ensures purity but may reduce dataset size; prompt template choice affects results but shows robustness across variants
- **Failure signatures**: Reasoning collapse (MATH500/GPQA accuracy drops >30% from baseline); over-refusal on benign inputs (high safety but low "Not Overrefusal" on XSTest); generation loops (infinite CoT without termination)
- **First 3 experiments**: (1) Baseline comparison: Vanilla SFT vs. DGR SFT on DirectRefusal with s1.1-7B; (2) Scaling analysis: Vary vanilla safety data quantity and DGR ratio; (3) Few-shot safety activation: Fine-tune on M=10 samples only and compare safety performance

## Open Questions the Paper Calls Out

### Open Question 1
Can DGR be effectively integrated into reinforcement learning–based safety alignment pipelines (e.g., PPO/GRPO-style methods)? The authors state in Limitations that they do not explore integrating DGR into RL-based safety alignment pipelines, either as a data-refinement stage or an auxiliary objective.

### Open Question 2
Does the relationship between distribution shift and reasoning degradation hold consistently for models at 70B+ parameter scales? The authors note that more comprehensive evaluations on LRMs with larger parameter scales (e.g., 70B and beyond) are necessary to fully assess the impact of distribution shifts across different model capacities.

### Open Question 3
How can safety alignment achieve nuanced refusal behavior that avoids over-refusal while maintaining robust safety? The authors conclude that achieving nuanced safety by decoupling templates from harmfulness remains a direction worthy of exploration.

## Limitations
- The safety tax mechanism is empirically observed but not causally isolated—controlled experiments varying only distributional properties are absent
- The DGR approach assumes the target model's rephrasing faithfully preserves safety semantics while changing only distributional properties; safety dilution through rephrasing is possible but not systematically tested
- The meta-thinking filter relies on keyword detection which may not generalize to all model behaviors

## Confidence
- **High confidence**: The safety tax problem exists in LRMs and the basic DGR pipeline improves reasoning while maintaining safety in controlled experiments
- **Medium confidence**: The distributional gap is the primary mechanism of safety tax (strong correlation evidence but causation not proven via intervention), and the knowledge activation hypothesis (10 samples sufficient)
- **Low confidence**: The generality of DGR across different model families and safety datasets (only tested on s1.1-7B and three specific datasets), and the long-term stability of DGR-aligned models

## Next Checks
1. **Controlled distribution shift experiment**: Create synthetic safety datasets with varying degrees of distribution shift and measure reasoning degradation in isolation from other variables
2. **Safety semantic preservation test**: Blind human evaluation comparing original vs. DGR-rephrased safety responses to verify no degradation in refusal quality or strength
3. **Component ablation study**: Evaluate DGR performance with individual components disabled to quantify each stage's contribution to final performance gains