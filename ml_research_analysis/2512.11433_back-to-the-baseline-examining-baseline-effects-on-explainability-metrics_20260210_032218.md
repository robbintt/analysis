---
ver: rpa2
title: 'Back to the Baseline: Examining Baseline Effects on Explainability Metrics'
arxiv_id: '2512.11433'
source_url: https://arxiv.org/abs/2512.11433
tags:
- baseline
- baselines
- information
- methods
- deletion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the instability of faithfulness metrics for
  attribution methods in XAI, focusing on the Insertion and Deletion metrics. The
  core problem is that the choice of baseline function, used to replace important
  pixels in the input image, significantly impacts the ranking of attribution methods.
---

# Back to the Baseline: Examining Baseline Effects on Explainability Metrics

## Quick Facts
- arXiv ID: 2512.11433
- Source URL: https://arxiv.org/abs/2512.11433
- Reference count: 40
- This work addresses the instability of faithfulness metrics for attribution methods in XAI, focusing on the Insertion and Deletion metrics

## Executive Summary
This paper addresses a critical instability in faithfulness metrics for attribution methods in Explainable AI (XAI). The core problem is that the choice of baseline function, used to replace important pixels in the input image, significantly impacts the ranking of attribution methods. Different baselines favor different attribution methods, leading to unreliable comparisons and making it difficult to determine which attribution method is truly most faithful. The authors propose two key desiderata for a good baseline: efficient information removal and maintaining the data distribution, and introduce a novel model-dependent baseline generated using feature visualization techniques to address these issues.

## Method Summary
The authors identify that existing baselines fail to satisfy both efficient information removal and maintaining the data distribution (low Out-Of-Distribution score), creating a trade-off between these two criteria. To overcome this limitation, they propose a novel model-dependent baseline generated using feature visualization techniques. This baseline is optimized to produce zero activations in the penultimate layer of the model, effectively removing information while remaining within the data distribution. The proposed approach aims to provide a better trade-off between information removal and OOD score compared to existing baselines, leading to more reliable and stable rankings of attribution methods.

## Key Results
- The choice of baseline function significantly impacts the ranking of attribution methods
- Current baselines fail to satisfy both efficient information removal and maintaining the data distribution
- The proposed model-dependent baseline provides a better trade-off between information removal and OOD score compared to existing baselines
- This leads to more reliable and stable rankings of attribution methods, improving the evaluation of their faithfulness

## Why This Works (Mechanism)
The proposed model-dependent baseline works by optimizing for zero activations in the penultimate layer of the model through feature visualization techniques. This approach directly targets the information removal criterion while simultaneously constraining the generated baseline to remain within the data distribution, thus addressing the fundamental trade-off that plagues existing baselines. By being model-dependent, it adapts to the specific characteristics of each model being explained, rather than using generic baselines that may systematically favor certain attribution methods over others.

## Foundational Learning
- **Insertion and Deletion metrics**: These are faithfulness metrics that measure how well attribution maps identify important regions by either inserting or deleting pixels based on attribution scores and measuring the change in model output. Understanding these metrics is crucial because the paper shows their instability depends on baseline choice.
- **Out-Of-Distribution (OOD) score**: A measure of how much a generated baseline deviates from the natural data distribution. This is important because baselines that are too far from the data distribution can unfairly penalize or favor certain attribution methods.
- **Feature visualization**: A technique used to generate inputs that maximize specific neural network activations. This is needed to create the model-dependent baseline that optimizes for zero activations in the penultimate layer.

## Architecture Onboarding

**Component Map**: Input Image -> Attribution Method -> Attribution Map -> Baseline Replacement -> Modified Image -> Model Prediction -> Performance Metric (Insertion/Deletion)

**Critical Path**: The critical path flows from the input image through the attribution method to generate an attribution map, which is then used with the baseline to create modified images. These modified images are fed back into the model, and the change in prediction score is measured by the insertion or deletion metric. The baseline choice affects every step after attribution map generation.

**Design Tradeoffs**: The main tradeoff is between information removal (how effectively the baseline removes important information) and distributional fidelity (how close the baseline is to natural data). Aggressive baselines remove more information but may be OOD, while conservative baselines maintain distribution but remove less information. The proposed model-dependent baseline attempts to optimize both simultaneously.

**Failure Signatures**: If a baseline systematically ranks certain attribution methods higher regardless of their actual faithfulness, this indicates baseline bias. If the insertion and deletion metrics give contradictory rankings, this suggests instability in the evaluation framework. If a baseline produces images that look unnatural to humans, this indicates high OOD scores.

**First Experiments**:
1. Compare the proposed model-dependent baseline against standard baselines (zero, blur, mean image) on a simple CNN architecture to verify improved stability
2. Measure OOD scores for different baselines to quantify the distributional shift problem
3. Test whether the insertion and deletion metrics agree more frequently when using the proposed baseline

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses primarily on standard image classification models, leaving open questions about the approach's applicability to transformers or other architectures
- The feature visualization optimization approach may introduce its own biases or limitations in representing "efficient information removal"
- The analysis doesn't fully address potential interactions between baseline choice and specific insertion/deletion metric implementations

## Confidence
- High confidence in the core observation that baseline choice significantly impacts attribution method rankings
- Medium confidence in the proposed solution's generalizability beyond the tested model classes
- Medium confidence in the feature visualization optimization producing truly optimal baselines

## Next Checks
1. Test the model-dependent baseline across diverse model architectures (transformers, vision-language models) to verify generalization
2. Conduct ablation studies on the feature visualization optimization parameters to ensure robustness
3. Evaluate the interaction between baseline choice and different implementations of insertion/deletion metrics to isolate pure baseline effects