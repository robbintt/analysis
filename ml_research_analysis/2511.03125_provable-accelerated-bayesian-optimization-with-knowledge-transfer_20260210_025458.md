---
ver: rpa2
title: Provable Accelerated Bayesian Optimization with Knowledge Transfer
arxiv_id: '2511.03125'
source_url: https://arxiv.org/abs/2511.03125
tags:
- deltabo
- source
- regret
- kernel
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of accelerating Bayesian optimization
  (BO) through knowledge transfer from related source tasks. The authors propose DeltaBO,
  a novel algorithm that models the target function as the sum of a source function
  and a difference function, each belonging to different reproducing kernel Hilbert
  spaces (RKHSs).
---

# Provable Accelerated Bayesian Optimization with Knowledge Transfer

## Quick Facts
- **arXiv ID:** 2511.03125
- **Source URL:** https://arxiv.org/abs/2511.03125
- **Authors:** Haitao Lin; Boxin Zhao; Mladen Kolar; Chong Liu
- **Reference count:** 40
- **Primary result:** DeltaBO achieves O(√T(T/N + γδ)) regret bound, outperforming baselines on hyperparameter tuning tasks.

## Executive Summary
This paper addresses the challenge of accelerating Bayesian optimization (BO) through knowledge transfer from related source tasks. The authors propose DeltaBO, which models the target function as the sum of a source function and a difference function, each belonging to different reproducing kernel Hilbert spaces (RKHSs). By leveraging the source function's posterior distribution, DeltaBO interprets each target evaluation as a biased observation of the difference function, enabling more efficient optimization. The algorithm achieves a regret bound of O(√T(T/N + γδ)), explicitly demonstrating how more source data reduces target regret.

Empirically, DeltaBO outperforms baseline methods on both real-world hyperparameter tuning tasks (Gradient Boosting and Multi-Layer Perceptron) and synthetic functions (Gaussian, Bohachevsky, and assumption-satisfied settings). The algorithm achieves lower cumulative and average regret across all settings, with particular improvements when the source and target functions are sufficiently similar. The theoretical analysis provides a principled framework for understanding when and how knowledge transfer can accelerate BO.

## Method Summary
DeltaBO operates by decomposing the target function f into a source function g and a difference function δ, where f = g + δ. The algorithm first computes the posterior distribution of g from the source dataset using a Matérn kernel (ν=5/2). During target optimization, DeltaBO selects queries using a GP-UCB acquisition function that combines the posterior means and variances of both g and δ. Crucially, when updating the posterior of δ, the algorithm uses inflated noise that accounts for the uncertainty in the source function estimate, specifically σ²_g,N(x_t) + σ². This inflation ensures theoretical guarantees while allowing the algorithm to efficiently explore the difference function.

The key innovation is the use of independent kernels for g and δ, allowing flexibility in modeling different aspects of the target function. The algorithm maintains separate GP posteriors for both components, with the acquisition function balancing exploration of the difference function against exploitation of the source knowledge. Each iteration involves computing the residual y_t - μ_g,N(x_t) and updating δ's posterior with the inflated noise, effectively treating each target evaluation as a noisy observation of δ centered around zero.

## Key Results
- DeltaBO achieves O(√T(T/N + γδ)) regret bound, showing explicit dependence on source data size N
- Outperforms GP-UCB, Env-GP, and Diff-GP baselines on UCI Breast Cancer hyperparameter tuning (GBoost and MLP)
- Demonstrates superior performance on synthetic functions including Gaussian, Bohachevsky, and assumption-satisfied settings
- Regret decreases as source-target similarity increases, with strongest improvements when γδ ≪ γf,T

## Why This Works (Mechanism)
DeltaBO works by leveraging the structural assumption that target functions can be decomposed into a source component and a difference component. This decomposition allows the algorithm to interpret target evaluations as noisy observations of the difference function, with noise inflated by the uncertainty in the source estimate. The GP-UCB acquisition function then balances exploration of the difference function against exploitation of the source knowledge, with the theoretical regret bound explicitly capturing how more source data reduces target regret. The independence assumption between g and δ enables clean decomposition of the posterior variance, while the inflated noise term ensures valid confidence bounds.

## Foundational Learning
- **Reproducing Kernel Hilbert Spaces (RKHS):** Mathematical framework for representing functions as elements of a Hilbert space, enabling kernel-based function approximation and GP modeling. Why needed: Provides the theoretical foundation for representing source and difference functions and computing their posteriors.
- **Gaussian Process (GP) Posteriors:** Distribution over functions that updates with observations, providing both mean and variance predictions. Why needed: Enables uncertainty quantification for both source and difference functions during optimization.
- **GP-UCB Acquisition:** Sequential selection strategy that balances exploration and exploitation using confidence bounds. Why needed: Provides the query selection mechanism that leverages both source knowledge and difference function uncertainty.
- **Information Gain (γ):** Measure of how much information observations provide about the underlying function. Why needed: Appears in the regret bound and characterizes the complexity of the difference function.
- **Confidence Bounds in BO:** Statistical guarantees that ensure the true function remains within predicted bounds with high probability. Why needed: Critical for proving no-regret guarantees and ensuring safe exploration.
- **Knowledge Transfer in BO:** Using information from related tasks to accelerate optimization of a target task. Why needed: The core problem setting that DeltaBO addresses through the additive decomposition.

## Architecture Onboarding

**Component Map:** Source GP posterior → Acquisition function → Target query → Residual computation → Difference GP posterior → Next iteration

**Critical Path:** Source data → g posterior → GP-UCB selection → Target evaluation → Residual update → δ posterior → Output

**Design Tradeoffs:** Independent kernels provide flexibility but require careful kernel selection; inflated noise ensures theoretical guarantees but may slow convergence if source uncertainty is large; decomposition assumes additive structure which may not hold for all task pairs.

**Failure Signatures:** Poor performance when source and target are dissimilar (γδ large); numerical instability in kernel matrix inversion with many source points; suboptimal exploration if inflated noise is too conservative.

**First Experiments:**
1. Verify posterior variance computation for δ uses σ²_g,N(x_t) + σ² noise inflation
2. Test different kernel combinations (k_g ≠ k_δ) to validate independence assumption
3. Compare performance with theoretical β_t schedule vs. fixed β_t=0.2

## Open Questions the Paper Calls Out

**Open Question 1:** What is the provable lower bound on the cumulative regret for the DeltaBO algorithm?
The paper currently only establishes an upper bound of $\tilde{\mathcal{O}}(\sqrt{T(T/N + \gamma_\delta)})$ but does not determine if this rate is theoretically optimal. A formal proof establishing a matching lower bound or analysis showing a gap between the best achievable lower bound and the proposed upper bound would resolve this.

**Open Question 2:** Under what theoretical conditions does DeltaBO suffer from negative transfer?
The analysis assumes γδ ≪ γf,T (source and target are similar), but does not characterize the performance degradation if the source task is unrelated or the additive assumption is violated. A modified regret bound that explicitly accounts for the divergence between source and target tasks, or a theoretical condition where DeltaBO performs worse than standard GP-UCB, would address this gap.

**Open Question 3:** Can the theoretical guarantees be maintained if the independence assumption between the source function g and difference function δ is relaxed?
Assumption 1 requires g and δ to be drawn independently from GPs, but the derivation of the posterior variance and confidence bounds relies on this independence. A generalization that admits a covariance structure between g and δ, or an empirical study showing performance sensitivity to correlations, would provide insight into this limitation.

## Limitations

- The additive decomposition assumption may not hold for all source-target task pairs, potentially limiting applicability
- Performance degrades when source and target functions are dissimilar (large γδ), though the paper doesn't quantify this cost
- The theoretical analysis relies on independence between g and δ, which may be restrictive in practice
- Implementation details such as initial observation selection and β_t scheduling are underspecified

## Confidence

- **High confidence:** The core algorithmic framework of DeltaBO (modeling target as sum of source and difference functions, using inflated noise for residual updates) is clearly specified and theoretically justified.
- **Medium confidence:** The empirical results showing DeltaBO outperforming baselines are reproducible given the specified hyperparameters, though exact replication may vary due to unspecified implementation details.
- **Low confidence:** The theoretical regret analysis extending to kernel classes beyond Matérn requires careful verification, as the proofs may have assumptions not fully stated in the main text.

## Next Checks

1. Verify the posterior variance computation for δ uses the inflated noise term σ²_g,N(x_t) + σ² rather than just σ², as this is critical for maintaining theoretical guarantees.

2. Implement the exact β_t schedule from theory (β_t = 2log(|D|t²π²/6ρ)) and compare performance against the fixed β_t=0.2 used in experiments to understand the practical impact.

3. Test DeltaBO with different kernel choices for g and δ (k_g ≠ k_δ) to validate the claim that independent kernels are allowed and to assess sensitivity to kernel mismatch.