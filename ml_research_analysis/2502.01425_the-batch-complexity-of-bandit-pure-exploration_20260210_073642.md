---
ver: rpa2
title: The Batch Complexity of Bandit Pure Exploration
arxiv_id: '2502.01425'
source_url: https://arxiv.org/abs/2502.01425
tags:
- algorithm
- complexity
- sample
- then
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pure exploration in multi-armed
  bandits with a fixed-confidence setting, where the goal is to identify the correct
  answer to a query about the arm distributions while minimizing the number of samples
  used. The authors focus on batched algorithms, which can only change their sampling
  behavior a limited number of times, between batches of observations.
---

# The Batch Complexity of Bandit Pure Exploration

## Quick Facts
- arXiv ID: 2502.01425
- Source URL: https://arxiv.org/abs/2502.01425
- Authors: Adrienne Tuynman; Rémy Degenne
- Reference count: 40
- Primary result: Introduces PET algorithm achieving near-optimal batch complexity for batched pure exploration in multi-armed bandits

## Executive Summary
This paper addresses the problem of pure exploration in multi-armed bandits with a fixed-confidence setting, where the goal is to identify the correct answer to a query about the arm distributions while minimizing the number of samples used. The authors focus on batched algorithms, which can only change their sampling behavior a limited number of times, between batches of observations. They provide an instance-dependent lower bound on the number of batches used by any sample efficient algorithm for pure exploration tasks. Additionally, they introduce a general batched algorithm, Phased Explore then Track (PET), and prove upper bounds on its expected sample complexity and batch complexity. The results are illustrated on best-arm identification and thresholding bandits. The paper shows that the batch complexity of PET is close to the lower bound under mild conditions, and its sample complexity is close to optimal in the high confidence regime. The authors also demonstrate the practical performance of PET through experiments on the best-arm identification setting, where it outperforms state-of-the-art algorithms in terms of sample complexity.

## Method Summary
The paper introduces the Phased Explore then Track (PET) algorithm for batched pure exploration in multi-armed bandits. PET operates in phases with uniform exploration until a stopping criterion is met, then tracks promising arms. The algorithm uses confidence regions and an Explore-then-Track strategy to balance exploration and exploitation while respecting batch constraints. PET's design leverages insights from the Track-and-Stop algorithm but adapts it to the batched setting through careful phase-based exploration and adaptive tracking. The method includes a Generalized Likelihood Ratio (GLR) stopping rule and uses specific parameters for confidence region radii and exploration weights that depend on the phase and problem instance.

## Key Results
- PET achieves batch complexity O(ln T*(μ)/T₀ + 5) where T*(μ) is the optimal sample complexity
- PET's sample complexity approaches the optimal T*(μ)log(1/δ) in the high confidence regime
- PET outperforms state-of-the-art algorithms in sample complexity on tested best-arm identification instances
- The paper provides instance-dependent lower bounds on batch complexity for pure exploration tasks

## Why This Works (Mechanism)
PET works by intelligently balancing exploration and exploitation within the constraints of batched updates. The algorithm uses phase-based uniform exploration to gather initial information about all arms, then identifies promising arms and focuses sampling on them while maintaining exploration of less promising arms to avoid missing the optimal solution. The GLR stopping rule ensures statistical validity while the adaptive tracking phase allows efficient concentration of samples on the most promising arms. The careful parameter tuning for confidence regions and exploration weights enables PET to approach the theoretical lower bounds on both sample and batch complexity.

## Foundational Learning

**Confidence Regions** - Statistical sets containing the true parameter with high probability; needed to quantify uncertainty about arm distributions and determine when to stop sampling. Quick check: Verify regions shrink at the correct rate (O(1/√n)) as samples accumulate.

**GLR Stopping Rule** - Uses generalized likelihood ratio statistics to determine when sufficient evidence has been gathered; needed for valid hypothesis testing in sequential settings. Quick check: Confirm GLR statistic correctly identifies when alternative hypotheses are supported.

**Explore-then-Track Strategy** - Combines initial broad exploration with focused tracking of promising arms; needed to balance information gathering with efficient exploitation in batched settings. Quick check: Ensure exploration phase sufficiently identifies promising arms before tracking begins.

## Architecture Onboarding

**Component Map:** Bandit environment -> PET algorithm (Phases: Explore -> Track) -> GLR stopping rule -> Output (arm identification)

**Critical Path:** Sample collection -> Update arm statistics -> Check stopping criterion -> Output result

**Design Tradeoffs:** Batch frequency vs. adaptivity - more frequent batches allow better adaptation but increase communication/computation overhead. Uniform exploration vs. targeted sampling - broader initial exploration ensures no optimal arm is missed but delays focused sampling.

**Failure Signatures:** Algorithm never stops (incorrect confidence region or stopping threshold), excessive sample complexity (poor identification of promising arms), incorrect arm identification (confidence regions too narrow).

**3 First Experiments:** 1) 2-arm Gaussian BAI with large gap (μ₁=1, μ₂=0.5) to verify basic functionality, 2) 10-arm Gaussian BAI with small gap (best=1, others uniform[0.6,0.9]) to test discrimination, 3) Thresholding bandit with τ=0.6 and arms at 0.5, 0.6 to verify TBP performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis focuses on specific problem instances (BAI, TBP) and may not generalize to all pure exploration tasks
- Performance guarantees depend on specific problem structure and confidence parameters
- Experiments are limited to Gaussian arms with specific configurations, limiting broader applicability claims

## Confidence

**Theoretical Claims:** High - The mathematical derivations follow established bandit theory and provide rigorous proofs for the main results.

**Algorithm Implementation:** Medium - While the algorithm description is clear, implementation details for numerical solvers and specific parameter choices require careful attention.

**Empirical Validation:** Medium - Experiments demonstrate PET's performance on specific instances but limited scope prevents broad generalization claims.

## Next Checks

1. Verify numerical solver for w*(B̂_r) matches the closed-form solution for BAI from Lemma A.9 and implements the general optimization correctly

2. Test PET implementation against known analytical solutions for simple instances (e.g., 2-arm BAI with large gap)

3. Compare PET's empirical batch complexity against the theoretical bound O(ln T*(μ)/T₀ + 5) across multiple problem instances