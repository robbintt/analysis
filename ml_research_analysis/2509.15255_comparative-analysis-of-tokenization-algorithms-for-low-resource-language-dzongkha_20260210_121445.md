---
ver: rpa2
title: Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha
arxiv_id: '2509.15255'
source_url: https://arxiv.org/abs/2509.15255
tags:
- tokenizers
- dzongkha
- tokenizer
- sentencepiece
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated three tokenization algorithms\u2014Byte-Pair\
  \ Encoding (BPE), WordPiece, and SentencePiece (Unigram)\u2014for their suitability\
  \ for Dzongkha, a low-resource language spoken by approximately 700,000 people in\
  \ Bhutan. The research aimed to address the lack of efficient tokenizers for Dzongkha,\
  \ which hinders downstream NLP tasks such as phishing detection and translation."
---

# Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha

## Quick Facts
- arXiv ID: 2509.15255
- Source URL: https://arxiv.org/abs/2509.15255
- Authors: Tandin Wangchuk; Tad Gonsalves
- Reference count: 31
- Primary result: SentencePiece (Unigram) outperforms BPE and WordPiece on Dzongkha tokenization efficiency metrics

## Executive Summary
This study evaluates three tokenization algorithms—Byte-Pair Encoding (BPE), WordPiece, and SentencePiece (Unigram)—for their suitability for Dzongkha, a low-resource language spoken by approximately 700,000 people in Bhutan. The research addresses the lack of efficient tokenizers for Dzongkha, which hinders downstream NLP tasks such as phishing detection and translation. The algorithms were trained on a 27-million-character corpus and assessed using metrics including Normalized Sequence Length (NSL), Subword Fertility, Proportion of Continued Words (PCW), and execution time. SentencePiece outperformed the other algorithms across all metrics: it achieved the lowest NSL values (0.0594, 0.1162, and 0.1105 relative to baseline tokenizers), subword fertility (0.09), and PCW (0.09), while also demonstrating the fastest execution time (131 ms per loop). These results indicate that SentencePiece is the most effective tokenizer for Dzongkha, enabling more accurate and efficient NLP applications. The findings underscore the importance of tailored approaches for low-resource languages and lay the groundwork for developing Dzongkha Large Language Models.

## Method Summary
The study trained BPE, WordPiece, and SentencePiece tokenizers from scratch using a 27-million-character Dzongkha corpus provided by the Dzongkha Development Commission. Vocabulary sizes of 10,000 and 30,000 tokens were tested for each algorithm. WordPiece and BPE were implemented using the HuggingFace Tokenizers library, while SentencePiece was trained using its Python wrapper. The trained models were evaluated on a preprocessed dataset of 180,000 manually segregated Dzongkha words, with metrics including Normalized Sequence Length (relative to GPT-2, Llama3, and TikToken baselines), Subword Fertility, Proportion of Continued Words, and execution time measured across multiple runs.

## Key Results
- SentencePiece achieved the lowest NSL values (0.0594, 0.1162, and 0.1105 relative to baseline tokenizers)
- SentencePiece demonstrated the lowest subword fertility (0.09) and PCW (0.09)
- SentencePiece showed the fastest execution time (131 ms per loop)
- All three algorithms showed improved efficiency at 30,000 vocabulary size compared to 10,000

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SentencePiece (Unigram) produces more efficient tokenization for Dzongkha than frequency-based merge algorithms.
- **Mechanism:** The Unigram language model estimates probability distributions over subword units and prunes low-probability tokens iteratively, rather than greedily merging frequent pairs. This probabilistic approach may better capture Dzongkha's subword statistics compared to BPE's frequency-only heuristics.
- **Core assumption:** Dzongkha's "linguistic complexity" (unspecified in paper) benefits from probabilistic vocabulary selection over greedy merge rules.
- **Evidence anchors:**
  - [abstract] "SentencePiece (Unigram) were evaluated for their suitability for Dzongkha"
  - [section 5] "SentencePiece has a value lower than 1.0" for subword fertility, indicating fewer tokens per word
  - [corpus] Related work "Reducing Tokenization Premiums for Low-Resource Languages" documents systematic tokenization inefficiencies in low-resource languages
- **Break condition:** If Dzongkha has highly agglutinative morphology requiring explicit morpheme boundaries, BPE's merge rules might capture structure better than probabilistic pruning.

### Mechanism 2
- **Claim:** Direct raw corpus training (without pre-tokenization) improves low-resource language tokenization.
- **Mechanism:** SentencePiece trains directly on raw text, treating input as a continuous stream. This avoids propagation of errors from language-specific pre-tokenizers designed for high-resource languages like English.
- **Core assumption:** Pre-tokenization tools for Dzongkha are either unavailable or poorly designed, introducing noise.
- **Evidence anchors:**
  - [section 2.3] "facilitating direct training on a raw corpus... language-independent, fast, and lightweight"
  - [section 4] Dataset was pre-processed to extract words, but this was evaluation data, not training data
  - [corpus] Weak direct evidence; corpus papers don't address pre-tokenization effects
- **Break condition:** If a high-quality Dzongkha word segmenter exists, pre-tokenization-based methods (WordPiece, BPE) could match or exceed SentencePiece.

### Mechanism 3
- **Claim:** Lower Normalized Sequence Length (NSL) indicates potential downstream efficiency gains.
- **Mechanism:** NSL measures compression ratio relative to baseline tokenizers. Lower NSL means fewer tokens per input, reducing sequence length for downstream models—potentially improving inference speed and reducing compute costs.
- **Core assumption:** Token efficiency translates to downstream task improvement (not evaluated in this paper).
- **Evidence anchors:**
  - [section 3] NSL defined as "compression ratio of a tokenizer relative to a baseline tokenizer"
  - [section 5] SentencePiece achieved NSL of 0.0594 vs GPT-2, 0.1162 vs Llama3, 0.1105 vs TikToken
  - [corpus] "Reducing Tokenization Premiums for Low-Resource Languages" links tokenization overhead to increased API costs and degraded performance
- **Break condition:** If overly aggressive compression merges semantically distinct units, downstream model understanding could degrade despite efficiency gains.

## Foundational Learning

- **Concept: Subword Tokenization**
  - **Why needed here:** All three algorithms (BPE, WordPiece, SentencePiece) operate at subword level. Understanding why subwords matter for low-resource languages is prerequisite to interpreting results.
  - **Quick check question:** Why does word-level tokenization fail for low-resource languages with large vocabularies?

- **Concept: Vocabulary Size Trade-offs**
  - **Why needed here:** The study tested 10,000 and 30,000 vocabulary sizes. This directly affects NSL, fertility, and OOV rates.
  - **Quick check question:** What happens to subword fertility when vocabulary size decreases from 30,000 to 10,000?

- **Concept: Tokenization Premium**
  - **Why needed here:** The paper's NSL metric is essentially measuring tokenization premium—the overhead cost of encoding non-English text with English-centric tokenizers.
  - **Quick check question:** If English has NSL=1.0 by definition, what does NSL=0.0594 imply about Dzongkha tokenization efficiency?

## Architecture Onboarding

- **Component map:**
  Raw Dzongkha Corpus (27M chars) -> Tokenizer Training (BPE / WordPiece / SentencePiece) -> Vocabulary File (10K or 30K tokens) -> Evaluation Dataset (180K words) -> Metrics: NSL, Subword Fertility, PCW, Execution Time

- **Critical path:** Corpus quality -> Vocabulary construction -> Tokenization efficiency metrics. The paper uses DDC-provided corpus; quality of this corpus directly affects generalizability.

- **Design tradeoffs:**
  - SentencePiece: Best metrics but slowest training (2+ min vs 43s for BPE at 10K vocab)
  - BPE: Fast training, moderate performance
  - WordPiece: Fast training, worst PCW (0.27) indicating more word fragmentation

- **Failure signatures:**
  - High subword fertility (>1.0): Over-segmentation, vocabulary insufficient
  - High PCW: Words frequently split, may lose semantic units
  - NSL >1.0: Tokenizer produces longer sequences than baseline (inefficient)

- **First 3 experiments:**
  1. **Reproduce on held-out Dzongkha data:** Split DDC corpus 80/20, train on 80%, evaluate metrics on 20% to test generalization.
  2. **Downstream task probe:** Train a simple classifier (e.g., sentiment or topic) using each tokenizer; measure if NSL/fertility gains transfer to task accuracy.
  3. **Vocabulary size ablation:** Test 5K, 15K, 20K, 25K vocabularies to find inflection points where metrics degrade significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the choice of SentencePiece versus BPE or WordPiece impact accuracy on downstream Dzongkha tasks such as translation, sentiment analysis, and phishing detection?
- **Basis in paper:** [explicit] The Discussion section states, "future research should extend evaluation to downstream application tasks such as translation, classification, sentiment analysis and text generation."
- **Why unresolved:** The current study only evaluated intrinsic metrics (NSL, Fertility, PCW) rather than extrinsic task performance.
- **What evidence would resolve it:** Benchmarking results (e.g., BLEU scores, F1-scores) from Dzongkha models utilizing these different tokenizers on standard NLP tasks.

### Open Question 2
- **Question:** Does the optimized SentencePiece tokenizer effectively improve the training convergence and final performance of a dedicated Dzongkha Large Language Model?
- **Basis in paper:** [explicit] The Conclusion notes that this work paves the way for "building Dzongkha Large Language Models," implying the integration of this tokenizer into an LLM is the necessary next step.
- **Why unresolved:** The paper evaluates the tokenizer in isolation and does not test it within the architecture of an actual language model.
- **What evidence would resolve it:** Comparative training loss curves and downstream task performance of LLMs initialized with the candidate tokenizers.

### Open Question 3
- **Question:** Is SentencePiece still the optimal algorithm when trained on significantly larger or noisier datasets compared to the 27-million-character manually segregated corpus used in this study?
- **Basis in paper:** [inferred] The experimental setup relied on a specific, "manually segregated" dataset of 180,000 words, which may not represent the noise and diversity found in web-scale data.
- **Why unresolved:** The relative performance of tokenization algorithms can shift when moving from clean, constrained corpora to massive, uncurated datasets.
- **What evidence would resolve it:** Re-evaluation of the efficiency metrics using tokenizers trained on a larger, web-scraped Dzongkha corpus.

## Limitations

- **Data Access Barrier:** The study relies on a proprietary 27-million-character corpus from the Dzongkha Development Commission with no public availability, preventing independent verification.
- **Downstream Validation Gap:** While SentencePiece shows superior tokenization efficiency, the paper does not empirically validate whether these improvements translate to better performance on actual NLP tasks.
- **Linguistic Analysis Deficiency:** The paper attributes SentencePiece's performance to Dzongkha's "linguistic complexity" without specifying which morphological features benefit from the Unigram approach.

## Confidence

**High Confidence (8-10/10)**
- SentencePiece outperforms BPE and WordPiece on the specific Dzongkha corpus and evaluation dataset used in this study
- The measured differences in NSL, subword fertility, and PCW are statistically significant and reproducible given access to the same data
- The methodology for computing the four evaluation metrics is clearly specified and implementable

**Medium Confidence (5-7/10)**
- SentencePiece's superiority generalizes to other Dzongkha datasets beyond the DDC corpus
- The Unigram approach will outperform other tokenizers for other low-resource languages with similar linguistic properties
- Lower tokenization metrics (NSL, fertility) will translate to measurable improvements in downstream task performance

**Low Confidence (1-4/10)**
- The specific mechanisms by which SentencePiece captures Dzongkha's linguistic structure
- The optimal vocabulary size for Dzongkha tokenization beyond the tested values
- Whether the efficiency gains justify the computational cost difference in training time

## Next Checks

1. **Reproducibility Test with Held-out Data**
   Split the DDC corpus into 80% training and 20% testing partitions. Train all three tokenizers on the 80% portion and compute the same four metrics on the held-out 20%. This validates whether the reported performance differences generalize beyond the evaluation dataset and are not artifacts of data leakage or overfitting to the test set.

2. **Downstream Task Performance Validation**
   Implement a simple binary classification task (e.g., phishing detection as mentioned in the paper's motivation, or sentiment analysis) using each tokenizer's vocabulary. Train identical model architectures with each tokenizer and compare both accuracy and computational efficiency (inference time, memory usage). This directly tests whether the tokenization efficiency gains translate to practical benefits.

3. **Cross-linguistic Generalization Study**
   Apply the same experimental protocol to at least two other low-resource languages with different morphological typologies (e.g., morphologically rich like Tamil, and isolating like Vietnamese). Compare whether SentencePiece consistently outperforms BPE and WordPiece across languages, or if the Dzongkha-specific results reflect language-specific properties rather than universal low-resource tokenization advantages.