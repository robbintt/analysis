---
ver: rpa2
title: Trillion 7B Technical Report
arxiv_id: '2504.15431'
source_url: https://arxiv.org/abs/2504.15431
tags:
- arxiv
- https
- language
- korean
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trillion-7B, a Korean-centric multilingual
  language model that achieves exceptional token efficiency through a novel Cross-lingual
  Document Attention (XLDA) mechanism. XLDA enables effective knowledge transfer from
  English to target languages like Korean and Japanese by allowing cross-lingual attention
  across strategically packed document sequences, rather than blocking attention at
  language boundaries.
---

# Trillion 7B Technical Report

## Quick Facts
- arXiv ID: 2504.15431
- Source URL: https://arxiv.org/abs/2504.15431
- Reference count: 28
- Primary result: Achieves competitive multilingual performance with only 10% multilingual data through Cross-lingual Document Attention mechanism

## Executive Summary
Trillion-7B is a Korean-centric multilingual language model that achieves exceptional token efficiency through a novel Cross-lingual Document Attention (XLDA) mechanism. The model demonstrates strong cross-lingual consistency and effective generalization to vision tasks while requiring only 59.4K H100 GPU hours ($148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages show robust multilingual performance and establish a new efficiency frontier for multilingual models.

## Method Summary
Trillion-7B employs Cross-lingual Document Attention (XLDA) to enable effective knowledge transfer from English to target languages like Korean and Japanese. This mechanism allows cross-lingual attention across strategically packed document sequences rather than blocking attention at language boundaries. Combined with optimized data mixtures, language-specific filtering, and a tailored tokenizer, the model achieves competitive multilingual performance while dedicating only 10% of its 2T training tokens to multilingual data.

## Key Results
- Achieves competitive multilingual performance with only 10% multilingual data allocation
- Requires just 59.4K H100 GPU hours ($148K) for full training
- Demonstrates strong cross-lingual consistency and effective vision task generalization across 27 benchmarks in four languages

## Why This Works (Mechanism)
The Cross-lingual Document Attention mechanism enables effective knowledge transfer by allowing cross-lingual attention across strategically packed document sequences rather than blocking attention at language boundaries. This approach facilitates information flow between languages during training, improving multilingual understanding while maintaining computational efficiency. The mechanism works particularly well for East Asian languages where structural similarities enable effective cross-lingual transfer.

## Foundational Learning
1. Cross-lingual attention mechanisms
   - Why needed: Enables knowledge transfer between languages during training
   - Quick check: Verify attention patterns show cross-lingual connections

2. Document-level attention strategies
   - Why needed: Determines how information flows across multilingual contexts
   - Quick check: Confirm document boundaries are respected while enabling cross-lingual connections

3. Token efficiency optimization
   - Why needed: Reduces computational costs while maintaining performance
   - Quick check: Compare token usage against baseline multilingual models

## Architecture Onboarding

**Component Map:**
Tokenizer -> Cross-lingual Document Attention -> Language-specific layers -> Vision adapter

**Critical Path:**
Input tokens → Tokenizer → XLDA mechanism → Language-specific processing → Output generation

**Design Tradeoffs:**
- Cross-lingual attention vs. computational overhead
- Korean-centric focus vs. broader multilingual generalization
- Vision task adaptation vs. core language model performance

**Failure Signatures:**
- Degraded performance on low-resource languages
- Inconsistent cross-lingual translations
- Vision task failures with non-East Asian visual content

**First Experiments:**
1. Evaluate cross-lingual consistency between Korean and Japanese
2. Test token efficiency against standard multilingual models
3. Assess vision task performance on diverse visual domains

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on East Asian languages, limiting generalizability to other language families
- Cross-lingual attention mechanisms may have computational overhead not captured in efficiency metrics
- Vision task generalization claims lack detailed training and evaluation protocols

## Confidence
- High confidence: Token efficiency improvements and computational cost claims ($148K, 59.4K H100 GPU hours)
- Medium confidence: Cross-lingual consistency results across 27 benchmarks
- Low confidence: Vision task generalization claims due to limited detail

## Next Checks
1. Evaluate cross-lingual transfer capabilities on European and African language pairs
2. Conduct ablation studies comparing XLDA against standard cross-lingual training approaches
3. Test zero-shot vision-language capabilities across diverse visual domains