---
ver: rpa2
title: Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)
arxiv_id: '2508.04894'
source_url: https://arxiv.org/abs/2508.04894
tags:
- graph
- attacks
- node
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores adversarial vulnerabilities in graph-aware\
  \ large language models (LLMs) for node classification, an area dominated by Graph\
  \ Neural Networks (GNNs). The authors investigate two representative graph-aware\
  \ LLM approaches\u2014LLAGA (node templates + linear projector) and GRAPH PROMPTER\
  \ (GNN + linear projector)\u2014using existing adversarial attacks from GNNs, including\
  \ NETTACK and METAATTACK, as well as new node sequence template injection attacks."
---

# Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2508.04894
- Source URL: https://arxiv.org/abs/2508.04894
- Reference count: 40
- Primary result: Graph-aware LLMs are highly vulnerable to both structural and feature perturbation attacks, with feature attacks often more effective than structural ones.

## Executive Summary
This work investigates adversarial vulnerabilities in graph-aware large language models (LLMs) for node classification, focusing on two representative approaches: LLAGA (node templates + linear projector) and GRAPH PROMPTER (GNN + linear projector). The authors evaluate existing adversarial attacks from GNNs alongside new node sequence template injection attacks. They find that feature perturbation attacks (homoglyph and reordering) are surprisingly effective, often surpassing structural attacks in causing accuracy degradation. The paper also introduces GALGUARD, an end-to-end defense combining LLM-based feature correction with adapted GNN structural defenses, which significantly improves robustness.

## Method Summary
The study evaluates two graph-aware LLM approaches: LLAGA, which uses fixed node sequence templates with placeholder injection, and GRAPH PROMPTER, which employs a GNN encoder. The authors apply poisoning and evasion attacks including NETTACK, METAATTACK, and new template injection attacks. Feature perturbations use homoglyph and reordering attacks. A unified attack combining structural and feature perturbations is also tested. The proposed defense, GALGUARD, integrates an LLM-based feature corrector (GPT-4 Turbo), graph purification, and adapted GNNGuard.

## Key Results
- Structural attacks (NETTACK, METAATTACK) achieve up to 51% accuracy drop on LLAGA
- Feature perturbation attacks (homoglyph, reordering) achieve up to 84% accuracy drop, often outperforming structural attacks
- Unified attacks combining structural and feature perturbations achieve up to 87% accuracy reduction
- GALGUARD defense significantly improves robustness across all attack types and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Node sequence template encoding in graph-aware LLMs creates an exploitable attack surface for placeholder injection.
- Mechanism: LLAGA constructs fixed-shape computational trees for each node, sampling k neighbors per level. When a node has fewer than k neighbors, placeholders are inserted to maintain structure. Attackers can inject non-adjacent or high-degree "supernodes" into these placeholder positions through edge manipulation, corrupting the target node's representation and degrading the projection layer's learning.
- Core assumption: The fixed-template design prioritizes structural consistency over robustness to edge perturbations.
- Evidence anchors:
  - [abstract]: "a new attack surface in LLAGA, where malicious nodes can be injected as placeholders into the node sequence template, significantly degrading performance"
  - [section 6]: "The inclusion of uninformative nodes in the node sequence corrupts the representation of the target node, leading to a poorly trained projection layer"
  - [corpus]: Weak direct support; corpus papers focus on node injection broadly but not placeholder-specific mechanisms
- Break condition: If graph encoding abandons fixed-shape templates or validates neighbor informativeness before inclusion, this attack surface closes.

### Mechanism 2
- Claim: Imperceptible feature perturbations are more effective than structural attacks on graph-aware LLMs.
- Mechanism: Graph-aware LLMs process textual node features through their language model backbone. Homoglyph attacks replace characters with visually identical Unicode variants; reordering attacks use direction-control characters to alter rendering sequences. Both preserve visual appearance but corrupt the encoded semantics. The LLM's reliance on textual features over structural information makes these attacks highly effective—evasion attacks via reordering achieved up to 73% accuracy reduction on Cora.
- Core assumption: The LLM backbone's text processing is the primary bottleneck for model integrity, not the graph encoder.
- Evidence anchors:
  - [abstract]: "imperceptible feature perturbation attacks are highly effective, often outperforming structural attacks"
  - [section 7]: "Reordering attacks are generally more potent than Homoglyph attacks... feature perturbation attacks (specifically Reordering) outperform structural attacks (METAATTACK) by a large margin"
  - [corpus]: "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks" corroborates text-level vulnerabilities
- Break condition: If input sanitization (OCR or LLM-based correction) normalizes text before encoding, feature attacks lose potency.

### Mechanism 3
- Claim: GRAPHPROMPTER's GNN-based encoder provides greater robustness than LLAGA's template-based encoding.
- Mechanism: GRAPHPROMPTER uses a GNN encoder (specifically GAT) to encode graph structure, preserving unperturbed textual node features during structural attacks. The textual embeddings remain unchanged even when the adjacency matrix is modified, allowing clean features to augment the perturbed structure. This dual-channel design distributes vulnerability rather than concentrating it.
- Core assumption: Robustness gains come from architectural decoupling of feature and structure processing.
- Evidence anchors:
  - [section 5.2]: "GRAPH PROMPTER's textual node embeddings remain unchanged during structural perturbations, as only the N component is affected"
  - [abstract]: "the GNN encoder used in GRAPHPROMPTER demonstrates greater robustness"
  - [corpus]: "Backdoor or Manipulation? Graph Mixture of Experts Can Defend Against Various Graph Adversarial Attacks" suggests multi-component architectures improve defense
- Break condition: If attackers combine feature and structural perturbations (unified attack), this robustness advantage diminishes but does not disappear.

## Foundational Learning

- Concept: **Poisoning vs. Evasion Attacks**
  - Why needed here: The paper evaluates both attack types across models; understanding the distinction is critical for interpreting results (e.g., evasion attacks caused up to 51% degradation vs. max 11% for poisoning on structural attacks).
  - Quick check question: Can you explain why evasion attacks are more effective than poisoning attacks for graph-aware LLMs in this study?

- Concept: **Frozen LLM with Trainable Projector**
  - Why needed here: Both LLAGA and GRAPHPROMPTER keep the LLM frozen and only train a lightweight projector (2-layer MLP). This design choice affects which components are vulnerable to different attack types.
  - Quick check question: What component would an attacker need to influence to affect the LLM's graph understanding in these architectures?

- Concept: **Message-Passing in GNNs**
  - Why needed here: GRAPHPROMPTER's robustness derives from its GAT-based message passing; GNNGuard defense adapts attention mechanisms for robustness.
  - Quick check question: How does message-passing relate to the vulnerability of GNN-based encoders to edge perturbations?

## Architecture Onboarding

- Component map: Graph (nodes with text features + adjacency) -> Graph Encoder (LLAGA template/Laplacian or GRAPHPROMPTER GNN) -> Projector (2-layer MLP) -> LLM Backbone (Vicuna-7B/Llama2-7B) -> Node Classification
- Critical path:
  1. Input graph (nodes with text features + adjacency)
  2. Graph encoder produces embeddings
  3. Projector maps embeddings to LLM-compatible tokens
  4. LLM performs node classification
  5. (Defense path) Feature corrector sanitizes text → purified graph → robustness training
- Design tradeoffs:
  - Template vs. GNN encoding: LLAGA's template is simpler but creates placeholder vulnerability; GRAPHPROMPTER's GNN is more robust but computationally heavier
  - Frozen vs. fine-tuned LLM: Frozen LLMs reduce cost but limit adaptation; the paper shows base LLM choice (Vicuna vs. Llama2) has minimal impact on robustness
  - Defense integration: GALGUARD's feature corrector (GPT-4 Turbo) adds inference cost and latency
- Failure signatures:
  - Placeholder injection attack: Sudden accuracy drops (e.g., 0.89→0.30 on Cora evasion) with localized neighborhood disruption
  - Feature perturbation attack: Performance degradation without visible graph structure changes
  - Unified attack: Near-complete failure (up to 87% accuracy reduction) indicating both channels compromised
- First 3 experiments:
  1. Baseline robustness check: Run LLAGA and GRAPHPROMPTER on clean Cora/CiteSeer/PubMed, then apply METAATTACK structural perturbations (10% edges) to establish vulnerability bounds
  2. Feature perturbation validation: Apply homoglyph and reordering attacks at 1%, 5%, 10% perturbation levels to quantify feature sensitivity and compare against structural attack effectiveness
  3. GALGUARD component ablation: Test feature corrector alone vs. graph purification alone vs. full GALGUARD to identify which defense component contributes most to robustness recovery

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset scope: Experiments focus on citation networks (Cora, CiteSeer, PubMed) with moderate graph sizes
- Base LLM dependency: Findings might differ with larger models or different architectures
- Defense computational overhead: GALGUARD introduces significant inference latency and cost

## Confidence
- High Confidence: The relative effectiveness of structural vs. feature attacks on graph-aware LLMs
- Medium Confidence: The specific mechanism of placeholder injection attacks on LLAGA's template encoding
- Medium Confidence: The end-to-end effectiveness of GALGUARD defense across all attack types and datasets

## Next Checks
1. Dataset generalization test: Replicate key experiments on larger graphs (e.g., Reddit, Amazon co-purchase) and non-citation network types
2. Defense overhead quantification: Measure inference latency and computational cost of GALGUARD components and compare against baseline performance drop
3. Hyperparameter sensitivity analysis: Systematically vary the k-neighborhood size in LLAGA's template encoding and attention head configurations in GRAPHPROMPTER