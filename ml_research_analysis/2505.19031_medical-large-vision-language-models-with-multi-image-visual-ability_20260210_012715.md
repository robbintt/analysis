---
ver: rpa2
title: Medical Large Vision Language Models with Multi-Image Visual Ability
arxiv_id: '2505.19031'
source_url: https://arxiv.org/abs/2505.19031
tags:
- multi-image
- dataset
- medical
- med-mim
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limited ability of existing medical large
  vision-language models to process multi-image clinical scenarios, which require
  sophisticated visual understanding such as temporal reasoning and cross-modal analysis.
  To bridge this gap, the authors introduce the Med-MIM instruction dataset, comprising
  83.2K medical multi-image QA pairs across four visual ability types (temporal understanding,
  reasoning, comparison, co-reference).
---

# Medical Large Vision Language Models with Multi-Image Visual Ability

## Quick Facts
- arXiv ID: 2505.19031
- Source URL: https://arxiv.org/abs/2505.19031
- Authors: Xikai Yang; Juzheng Miao; Yuchen Yuan; Jiaze Wang; Qi Dou; Jinpeng Li; Pheng-Ann Heng
- Reference count: 29
- Primary result: Introduces Med-MIM dataset and fine-tuned models (Med-Mantis, MIM-LLaVA-Med) that significantly outperform existing LVLMs on multi-image medical understanding tasks across temporal, reasoning, comparison, and co-reference abilities.

## Executive Summary
This work addresses the limited ability of existing medical large vision-language models to process multi-image clinical scenarios, which require sophisticated visual understanding such as temporal reasoning and cross-modal analysis. To bridge this gap, the authors introduce the Med-MIM instruction dataset, comprising 83.2K medical multi-image QA pairs across four visual ability types (temporal understanding, reasoning, comparison, co-reference). They fine-tune Mantis and LLaVA-Med using this dataset, producing MIM-LLaVA-Med and Med-Mantis models optimized for multi-image analysis. A comprehensive Med-MIM benchmark is also developed to evaluate multi-image understanding capabilities. Experimental results show that both models significantly outperform existing state-of-the-art LVLMs on held-in and held-out subsets of the benchmark, demonstrating the effectiveness of the Med-MIM instruction dataset in enhancing medical LVLMs' multi-image comprehension.

## Method Summary
The authors create the Med-MIM instruction dataset with 83.2K multi-image QA pairs spanning four visual ability types, combining inherent data from MS-CXR-T, EMBED, LUMIERE with composed data from LLaVA-Med VQA. They fine-tune existing LVLMs (LLaVA-Med-7B and Mantis-8B) using NLL loss with interleaved image-text formatting "(image {id}: <Image> embeddings </Image>)" for 3 epochs on 4× NVIDIA A40 GPUs. The models are evaluated on a Med-MIM benchmark (2,968 closed + 256 open) and held-out datasets MIM-RAD and MIM-ODIR (each 300 closed + 300 open), measuring close-ended accuracy and open-ended metrics (BERT-recall, BLEU, ROUGE-L).

## Key Results
- Med-Mantis achieves 63.82% accuracy on held-in benchmark compared to 48.42% for base Mantis
- Both models show significant improvements on held-out benchmarks (MIM-RAD, MIM-ODIR) across all four visual ability types
- Co-reference ability (spatial grounding) achieves highest performance (~80%) while reasoning ability shows lowest scores (~30%)
- Ablation study demonstrates that removing any visual ability subset causes noticeable performance drops on corresponding test sets

## Why This Works (Mechanism)

### Mechanism 1
Structured multi-image instruction tuning enables LVLMs to acquire differentiated visual abilities (temporal, reasoning, comparison, co-reference). The Med-MIM dataset organizes 83.2K QA pairs into four distinct visual ability categories, each targeting specific cognitive operations. Fine-tuning on these structured categories allows models to learn category-specific patterns rather than treating all multi-image inputs uniformly. Core assumption: Models can transfer learned visual ability patterns within categories to unseen medical images of similar structure. Evidence: Ablation study shows removing a visual ability subset causes "noticeable drop in performance on the corresponding testing set."

### Mechanism 2
Interleaved image-text formatting with positional encoding enables cross-image attention for multi-image reasoning. The model restructures input as "(image {id}: <Image> embeddings </Image>)" interleaved with text, allowing the language model's attention mechanism to attend across image representations. The NLL loss trains the model to generate tokens conditioned on the full interleaved sequence, implicitly learning cross-image relationships. Core assumption: Base LVLM architecture has sufficient attention capacity to bind information across 2-3 images without modifications. Evidence: The interleaved formatting is explicitly specified and attention-based cross-image reasoning is a standard transformer capability.

### Mechanism 3
Composed multi-image data augmentation transfers single-image medical knowledge to multi-image contexts. The composed Med-MIM dataset takes existing single-image QA pairs from LLaVA-Med VQA and appends location-specific suffixes ("In the first image...", "In the second image..."), creating synthetic multi-image training examples. This leverages pre-existing medical knowledge while teaching multi-image grounding. Core assumption: Single-image medical concepts are transferable to multi-image contexts via simple location prefixes without requiring new medical knowledge. Evidence: Fig.4(b) shows composed dataset "significantly improves closed-type QA performance for both models."

## Foundational Learning

- **Multi-image attention in transformer-based LVLMs**: The paper assumes readers understand how attention mechanisms can bind information across multiple image embeddings in a sequence. Quick check: Can you explain why an interleaved image-text format enables cross-image reasoning in a decoder-only transformer?

- **Instruction tuning for visual abilities**: The core contribution is structuring instruction data by visual ability type; understanding instruction tuning is prerequisite. Quick check: How does supervised fine-tuning on structured QA pairs differ from pretraining on raw image-text pairs?

- **Held-in vs. held-out benchmark evaluation**: The paper evaluates on both held-in (derived from training distribution) and held-out (MIM-RAD, MIM-ODIR) benchmarks to assess generalization. Quick check: Why is performance on held-out benchmarks critical for claiming zero-shot multi-image capability?

## Architecture Onboarding

- **Component map**: Data preparation -> GPT-4o QA generation -> Fine-tuning with NLL loss -> Evaluation on held-in and held-out benchmarks
- **Critical path**: 1) Data preparation (inherent datasets + composed datasets) 2) GPT-4o QA generation with human quality control 3) Fine-tuning with NLL loss on interleaved sequences 4) Evaluation on held-in benchmark + held-out (MIM-RAD, MIM-ODIR)
- **Design tradeoffs**: Capped sequence length at 3 images for computational feasibility; may limit applicability to longer longitudinal studies. Used GPT-4o for QA generation; quality depends on prompt engineering and human review. Built on existing models rather than training from scratch; inherits their limitations.
- **Failure signatures**: Model confuses image ordering in temporal tasks → check co-reference training quality. Performance drops on held-out benchmarks → possible overfitting to held-in distribution. Hallucinated medical details → composed dataset may not provide sufficient medical grounding.
- **First 3 experiments**: 1) Reproduce held-in benchmark results on one visual ability using released Med-MIM dataset to validate fine-tuning pipeline 2) Ablate composed vs. inherent data: train two variants and evaluate on MIM-RAD to isolate contribution of composed augmentation 3) Extend sequence length test: evaluate on 4-image inputs to characterize attention degradation boundary

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the methodology raises several important unresolved issues regarding the limitations of GPT-4o-generated data, the architectural constraints of 3-image caps, and the evaluation completeness across different medical modalities.

## Limitations

- **Dataset Quality and Generalization**: The Med-MIM dataset relies heavily on GPT-4o for QA generation with human quality control as the primary validation step. The composed dataset may not capture full complexity of genuine multi-image medical reasoning scenarios.
- **Architectural Constraints**: Models are capped at processing only 3 images per input sequence, significantly limiting applicability to longitudinal medical studies that often involve 4+ visits.
- **Evaluation Completeness**: Lacks ablation studies isolating contribution of each visual ability category and comparison with more recent multi-image LVLM architectures.

## Confidence

- **High Confidence**: Structured instruction tuning mechanism is well-supported by ablation results showing performance drops when visual ability subsets are removed.
- **Medium Confidence**: Composed dataset augmentation strategy shows measurable improvements but lacks direct validation against truly novel multi-image medical scenarios.
- **Low Confidence**: Claims about zero-shot generalization to held-out benchmarks cannot be fully verified without access to specific test images and detailed error analysis.

## Next Checks

1. **Ablation Study on Visual Ability Categories**: Train four model variants, each fine-tuned on only one of the four visual ability categories, then evaluate all variants on held-out MIM-RAD and MIM-ODIR benchmarks to reveal whether visual ability taxonomy captures distinct competencies.

2. **Extended Sequence Length Analysis**: Systematically evaluate Med-Mantis and MIM-LLaVA-Med on 4-image, 5-image, and 6-image inputs to characterize attention degradation curves, measuring performance on temporal understanding tasks specifically.

3. **Cross-Domain Transfer Test**: Take Med-MIM models and evaluate them on multi-image radiology QA datasets from non-chest domains (e.g., brain CT, musculoskeletal X-rays) without additional fine-tuning to test whether learned multi-image reasoning patterns transfer beyond training domain distribution.