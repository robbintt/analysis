---
ver: rpa2
title: Different types of syntactic agreement recruit the same units within large
  language models
arxiv_id: '2512.03676'
source_url: https://arxiv.org/abs/2512.03676
tags:
- agreement
- units
- overlap
- syntactic
- phenomena
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how syntactic knowledge is represented
  within large language models (LLMs) by examining whether different syntactic phenomena
  recruit shared or distinct model components. Using a functional localization approach
  inspired by cognitive neuroscience, the authors identify units most responsive to
  67 English syntactic phenomena across seven open-weight models.
---

# Different types of syntactic agreement recruit the same units within large language models

## Quick Facts
- arXiv ID: 2512.03676
- Source URL: https://arxiv.org/abs/2512.03676
- Reference count: 40
- Different types of syntactic agreement (subject-verb, anaphor, determiner-noun) recruit overlapping units in LLMs

## Executive Summary
This study investigates how syntactic knowledge is represented within large language models (LLMs) by examining whether different syntactic phenomena recruit shared or distinct model components. Using a functional localization approach inspired by cognitive neuroscience, the authors identify units most responsive to 67 English syntactic phenomena across seven open-weight models. They find that these units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting agreement constitutes a meaningful functional category for LLMs. This pattern replicates in Russian and Chinese, and in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. The findings reveal that syntactic agreement—a critical marker of syntactic dependencies—constitutes a meaningful category within LLMs' representational spaces.

## Method Summary
The authors use functional localization to identify LLM units most responsive to 67 English syntactic phenomena from the BLiMP benchmark. For each phenomenon, they extract residual stream activations from grammatical and ungrammatical minimal pairs, apply last-token pooling, and compute absolute activation magnitudes. Welch t-tests rank units by discriminability between conditions, with top-1% selected as responsive. Cross-validation confirms units generalize across sentence instances (62.99%-95.45% overlap). Causal importance is established through zero-ablations showing 7.61% average accuracy drop versus 1.68% for random units. Agreement categories (subject-verb, anaphor, determiner-noun) are analyzed for unit overlap, with cross-lingual patterns examined in Russian, Chinese, and 57 languages using lang2vec syntactic similarity measures.

## Key Results
- Agreement phenomena (DET-N: 68.91%, S-V: 40.29%, anaphor: 23.10%) show substantially higher within-category unit overlap than cross-category overlaps (5.72%-8.23%)
- Cross-lingual subject-verb agreement unit overlap correlates with syntactic similarity (Spearman 0.460, p<10⁻⁸³) across 1,586 language pairs
- Agreement units primarily recruited in MLP modules (local dependencies) versus attention modules for long-distance phenomena like NPI licensing

## Why This Works (Mechanism)

### Mechanism 1: Functional Localization via Activation Magnitude Contrast
Units responsive to specific syntactic phenomena are identified by measuring activation differences between grammatical and ungrammatical sentence pairs. For each phenomenon, Welch t-statistics on absolute activation magnitudes across grammatical vs. ungrammatical conditions (last-token pooling) select top-1% units with highest discriminability. These units generalize across sentence instances (62.99%-95.45% 2-fold overlap) and their ablation reduces grammaticality judgment accuracy (avg. 7.61% drop vs. 1.68% random). The method assumes activation magnitude differences reflect functional specialization rather than correlated confounds. Controls using pseudo-minimal pairs show only 1.03% overlap, mitigating template artifact concerns.

### Mechanism 2: Agreement as a Functional Category with Shared Neural Substrate
Syntactic agreement phenomena draw on overlapping model units, distinguishing them from other syntactic categories. Within-category unit overlaps for agreement phenomena (DET-N: 68.91%, S-V: 40.29%, anaphor: 23.10%) substantially exceed cross-category overlaps (5.72%-8.23%). Agreement categories also show higher inter-category overlap with each other (15.26%-23.17%) than with non-agreement phenomena. This suggests shared computational substrate rather than independent implementations that happen to co-localize. The pattern holds across languages and is consistent with agreement requiring computation of syntactic features that apply across different dependency types.

### Mechanism 3: Cross-linguistic Unit Overlap Scales with Syntactic Similarity
Agreement-related units are partially shared across languages, with overlap proportional to syntactic similarity. Cross-lingual overlap between English and Russian agreement units (9.99%) exceeds English-Chinese (2.22%). Across 57 languages, Spearman correlation between syntactic similarity (lang2vec) and subject-verb agreement unit overlap is 0.460 (p<10⁻⁸³). This suggests shared units reflect common syntactic representations rather than incidental overlap in high-dimensional activation space. The relationship holds across diverse language families, indicating structural similarity drives representational similarity in agreement processing.

## Foundational Learning

- **Concept: Minimal Pair Paradigm**
  - Why needed here: The entire localization method depends on contrasting grammatical vs. ungrammatical sentences that differ in exactly one syntactic feature. Understanding this controls-vs-treatment design is essential.
  - Quick check question: Given sentences "The dogs bark" (grammatical) and "The dogs barks" (ungrammatical), what syntactic phenomenon does this minimal pair test?

- **Concept: Welch t-test for Unit Selection**
  - Why needed here: The localization algorithm ranks units by t-statistic, not raw activation. This controls for variance differences across units.
  - Quick check question: Why might a unit with large mean activation difference but high variance be less informative than one with moderate difference but low variance?

- **Concept: Zero/Mean Ablation as Causal Intervention**
  - Why needed here: Correlational identification (high t-statistic) does not prove causal role. Ablation tests whether units are necessary for task performance.
  - Quick check question: If ablating top-1% units causes 7% accuracy drop but ablating random 1% causes 1.5% drop, what does the 5.5% difference represent?

## Architecture Onboarding

- **Component map:** Residual stream (post-layer outputs excluding embeddings) -> MLP modules (local dependencies, agreement phenomena) -> Attention modules (long-distance dependencies, NPI licensing) -> Layer distribution (agreement: final layers; ellipsis/islands/NPI: initial layers)

- **Critical path:** 1) Load model + tokenizer -> extract activations via hooks 2) Process minimal pairs -> last-token pooling -> compute |activation| per unit 3) Welch t-test per unit -> rank by t-statistic -> select top-k% 4) Cross-validate (k-fold) -> verify generalization 5) Ablate identified units -> measure accuracy drop

- **Design tradeoffs:** Top-1% threshold captures most relevant units but may miss distributed representations; last-token pooling captures full-sentence context but conflates position information; zero vs. mean ablation highly correlated (R>0.98) except GPT-2 (0.541) due to non-zero mean activations

- **Failure signatures:** Low cross-validation overlap (<10%): units not generalizing—check for data leakage or template artifacts; ablation effect <2× random: localization failed or task doesn't require identified units; DeepSeek shows only 0.76% ablation effect (inconsistent across phenomena)

- **First 3 experiments:** 1) Replicate Figure 1 on single phenomenon: Split 1000 pairs into 2 folds, localize independently, measure overlap. Target: >60% overlap 2) Ablation sanity check: Zero-ablate top-1% units on held-out set. Compare to random ablation. Target: >2× performance drop 3) Cross-phenomenon overlap test: Compute pairwise Jaccard similarity for agreement vs. non-agreement phenomena. Target: Agreement-within > agreement-cross

## Open Questions the Paper Calls Out
- Do the identified syntax-responsive units generalize to sentences containing ambiguities, long-distance dependencies, or infrequent structures? The current analysis relies on the BLiMP benchmark, which focuses on minimal grammatical/ungrammatical pairs, leaving the model's processing of valid but complex syntactic structures understudied.

- Are there distributed syntactic representations that are overlooked by the top-n% unit localization method? The study prioritizes sparse, high-magnitude units in the standard activation basis, potentially failing to capture syntactic information encoded in superposed or low-magnitude dimensions.

- What is the specific causal mechanism by which shared agreement units support performance across different agreement types? While the paper demonstrates that ablating shared units drops performance, it does not fully isolate the circuit-level mechanisms to confirm if these units implement a single general algorithm or distinct, interleaved sub-circuits.

## Limitations
- The functional localization method may overlook distributed syntactic representations by focusing only on top-1% units
- The causal role evidence from ablation may underestimate distributed representations since only sparse units are tested
- The cross-linguistic analysis does not report whether random unit subsets show comparable similarity correlations, leaving open the possibility of high-dimensional space artifacts

## Confidence

- **High Confidence**: Cross-validation consistency of unit localization (62.99%-95.45% overlap) and ablation effects showing 2-5× greater accuracy drops than random (robust across 67 phenomena and 7 models except DeepSeek)
- **Medium Confidence**: Agreement categories recruiting overlapping units constitutes a meaningful functional category, supported by consistent 2-4× higher within-category overlaps but lacking direct mechanistic explanation of why agreement phenomena share substrate
- **Medium Confidence**: Cross-linguistic unit overlap scaling with syntactic similarity (Spearman 0.460, p<10⁻⁸³) is statistically robust but the control for random unit subsets is unreported

## Next Checks
1. Compute Spearman correlation between syntactic similarity and overlap for random unit subsets (same size as agreement units) across the 57-language dataset to test whether observed correlation exceeds chance

2. Ablate progressively larger unit sets (top-5%, top-10%) for agreement phenomena to determine if performance drops saturate at 1% or continue increasing, revealing extent of distributed vs. localized representations

3. For English-Russian language pair, localize agreement units independently in each language, then test whether ablating English-identified units in Russian model causes performance drops, directly validating cross-linguistic representational sharing