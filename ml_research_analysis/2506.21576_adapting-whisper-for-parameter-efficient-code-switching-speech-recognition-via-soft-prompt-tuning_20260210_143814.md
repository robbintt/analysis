---
ver: rpa2
title: Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition
  via Soft Prompt Tuning
arxiv_id: '2506.21576'
source_url: https://arxiv.org/abs/2506.21576
tags:
- prompt
- whisper
- tuning
- soft
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the use of Soft Prompt Tuning (SPT) for\
  \ adapting Whisper to code-switching (CS) speech recognition, focusing on parameter\
  \ efficiency and mitigating catastrophic forgetting. The authors evaluate multiple\
  \ SPT variants\u2014including Deep Prompt Tuning, Residual Prompt Tuning, and Language\
  \ Prompt Tuning\u2014and introduce SPT4ASR, a combined approach."
---

# Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning

## Quick Facts
- arXiv ID: 2506.21576
- Source URL: https://arxiv.org/abs/2506.21576
- Reference count: 0
- Key outcome: SPT4ASR achieves significant MER reductions (e.g., 13.12% on devman of SEAME for Whisper-small) while using far fewer parameters than full fine-tuning and preserving monolingual performance.

## Executive Summary
This paper investigates Soft Prompt Tuning (SPT) for adapting Whisper to code-switching (CS) speech recognition, focusing on parameter efficiency and mitigating catastrophic forgetting. The authors evaluate multiple SPT variants—including Deep Prompt Tuning, Residual Prompt Tuning, and Language Prompt Tuning—and introduce SPT4ASR, a combined approach. Experiments on SEAME and ASRU2019 datasets show that SPT4ASR achieves significant MER reductions while using far fewer parameters than full fine-tuning. SPT-based methods also preserve performance on monolingual tasks, demonstrating their effectiveness in reducing catastrophic forgetting compared to traditional fine-tuning approaches.

## Method Summary
The paper adapts Whisper for code-switching ASR using soft prompt tuning, where learnable embedding tensors are added to the input or intermediate layers of the frozen model. The authors evaluate multiple SPT variants: Vanilla SPT (learnable prompts at input), Deep Prompt Tuning (DPT, prompts at intermediate layers), Residual Prompt Tuning (ResPT, prompts refined via shared MLP), and Language Prompt Tuning (LPT, language ID-aware prompts). They combine these into SPT4ASR. Training uses 10 epochs, batch size 8, AdamW optimizer (lr=1e-3 for SPT/LoRA, 1e-6 for FFT), and greedy decoding. Experiments use SEAME and ASRU2019 datasets, measuring Mix Error Rate (MER) for CS tasks and CER/WER for monolingual preservation.

## Key Results
- SPT4ASR achieves the lowest MER on SEAME and ASRU2019 datasets among SPT variants
- On SEAME devman, Whisper-small with SPT4ASR achieves 13.12% MER reduction versus zero-shot Whisper
- SPT-based methods preserve monolingual performance, demonstrating reduced catastrophic forgetting compared to full fine-tuning
- SPT4ASR uses only 3.74M trainable parameters versus millions for full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Deep Prompt Tuning (DPT) Enhances Adaptation Granularity
- **Claim**: Inserting learnable soft prompts at intermediate layers allows more effective adaptation to code-switching speech than input-level prompts alone
- **Mechanism**: DPT concatenates prompt embeddings to the output of each transformer block, influencing attention and feed-forward operations throughout the network. This bypasses the vanishing influence of static, input-only prompts on deeper layers, allowing the model to inject code-switching-specific signals at multiple stages of processing
- **Core assumption**: The model's ability to adapt to code-switching is bottlenecked by the limited capacity of input-level prompts to influence deep representations
- **Evidence anchors**: "...deep prompt tuning is the most effective SPT approach..." (abstract); "DPT (S4) achieves the most significant improvement..." (Section 4.2.1)
- **Break condition**: If code-switching adaptation requires primarily acoustic-feature level changes rather than higher-level linguistic representations, deep prompts may provide diminishing returns

### Mechanism 2: Freezing Model Weights Mitigates Catastrophic Forgetting
- **Claim**: By freezing pre-trained Whisper parameters and only training soft prompt parameters, the model retains monolingual capabilities while adapting to code-switching
- **Mechanism**: Original model parameters remain unchanged; adaptation is steered entirely by learned soft prompt embeddings. Since original weights for monolingual tasks are not overwritten, prior knowledge is preserved. The small number of trainable parameters also acts as a regularizer
- **Core assumption**: Pre-trained Whisper contains sufficient general knowledge that can be "re-programmed" for code-switching via prompts without modifying core representations
- **Evidence anchors**: "...SPT-based methods also preserve performance on monolingual tasks..." (abstract); "Table 4 shows that while FFT-based models exhibited significant performance degradation, both SPT4ASR and LoRA maintained performance close to baseline..." (Section 4.3)
- **Break condition**: If pre-trained representations are fundamentally incompatible with code-switching, prompt tuning alone will be insufficient regardless of its effect on forgetting

### Mechanism 3: Reparameterized Prompts Accelerate Convergence
- **Claim**: Using a shared MLP with residual connections to generate/refine soft prompts (ResPT) leads to more efficient training than directly optimizing raw prompt embeddings
- **Mechanism**: A small neural network takes a raw prompt embedding as input and outputs a refined prompt. This reparameterization acts as an optimizer in parameter space, smoothing the loss landscape and making effective prompts easier to find. The residual connection ensures original prompt information is retained, aiding stability
- **Core assumption**: The loss surface for directly optimizing raw prompt embeddings is complex and hard to navigate, making convergence slow and sensitive to initialization
- **Evidence anchors**: "ResPT's key advantage is refining soft prompts quickly without requiring extensive pre-training" (Section 2.4.2)
- **Break condition**: If the reparameterization MLP introduces too many additional parameters or the residual connection dilutes the prompt signal too much, performance could degrade

## Foundational Learning

- **Concept**: Soft Prompt Tuning (SPT)
  - **Why needed here**: This is the core adaptation method. Unlike fine-tuning, it does not update model weights. Understanding that SPT adds learnable tensors to the input (or intermediate layers) that are optimized via backpropagation is essential
  - **Quick check question**: How does SPT differ from "hard" prompt engineering (text-based instructions) and from full fine-tuning? (Answer: SPT uses continuous, learnable vectors, not discrete text tokens, and does not update the main model's weights)

- **Concept**: Code-Switching (CS)
  - **Why needed here**: The target problem. It involves intra-sentential language alternation. Standard monolingual ASR models struggle with this because of language confusion and mismatched language models
  - **Quick check question**: What makes code-switching ASR harder than bilingual ASR (where each utterance is in a single language)? (Answer: The need to handle dynamic language transitions within a single utterance and the lack of robust intra-sentential code-switching training data)

- **Concept**: Catastrophic Forgetting
  - **Why needed here**: The primary motivation for using SPT over full fine-tuning. It is the phenomenon where a neural network forgets previously learned information upon learning new information
  - **Quick check question**: In the context of this paper, how would catastrophic forgetting manifest? (Answer: A significant drop in accuracy on the original monolingual datasets, like LibriSpeech or AISHELL-1, after the model is adapted for code-switching)

## Architecture Onboarding

- **Component map**:
  - Speech Spectrogram → Whisper Frozen Transformer Encoder-Decoder → Text Output
  - Soft Prompts (learnable embedding tensors) → Concatenated to input or intermediate layers
  - Optional: Shared MLP (ResPT) → Refines soft prompts
  - Optional: Language Encoder (LPT) → Encodes language ID information into prompts
  - SPT4ASR → Combined architecture integrating DPT, ResPT, and LPT

- **Critical path**:
  1. **Input Processing**: A sequence of acoustic features `X` is extracted from the speech
  2. **Prompt Injection**: 
     - (Vanilla/ResPT/LPT): Soft prompts `P` are concatenated with `X` -> `[P, X]`
     - (DPT): Deep prompts are injected at each transformer block's output
  3. **Forward Pass**: The modified input passes through the *frozen* Whisper model
  4. **Loss Calculation**: The model's output is compared to the ground truth transcript
  5. **Backward Pass**: Gradients are computed *only* for the soft prompt parameters (`P` or the weights of the MLP/Language Encoder). The Whisper backbone receives no updates

- **Design tradeoffs**:
  - **Vanilla SPT**: Simplest, fewest parameters (~0.20M). Weakest performance
  - **Deep Prompt Tuning (DPT)**: Strongest individual performance (per paper). More parameters (~1.39M). More complex to implement (requires modifying model internals)
  - **SPT4ASR**: Best overall performance. ~3.74M trainable parameters (comparable to LoRA). More complex design, potential for overfitting on small datasets
  - **Full Fine-Tuning (FFT) with SPT**: Best performance of all, but abandons parameter efficiency and requires training the entire model, risking catastrophic forgetting

- **Failure signatures**:
  - **No Improvement / High MER**: Soft prompts may not be influencing the model. Check if they are correctly concatenated and set as trainable. Prompt length may be too short
  - **Catastrophic Forgetting**: If the validation loss on original languages increases dramatically during code-switching training, ensure the Whisper backbone is *actually* frozen. Check that `requires_grad` is False for all backbone parameters
  - **Convergence Issues**: If using ResPT, the MLP may be poorly initialized. If using DPT, learning rate may need adjustment for the deeper prompts

- **First 3 experiments**:
  1. **Baseline Establishment**: Run zero-shot inference using the pre-trained Whisper model on the target code-switching dataset (e.g., SEAME devman). This sets the lower bound for adaptation. Measure MER
  2. **Vanilla SPT Ablation**: Implement Vanilla SPT. Experiment with prompt lengths (e.g., 16, 32, 64, 128) and positions (Encoder-only, Decoder-only, Both). Identify the most effective basic setup
  3. **Advanced SPT Comparison**: Implement DPT and the SPT4ASR combination. Compare their MER on the dev set against the best Vanilla SPT result and a LoRA baseline. This validates the paper's core findings for your specific setup

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can soft prompt tuning (SPT) methods be optimized to consistently outperform Low-Rank Adaptation (LoRA) in both parameter efficiency and error rates for code-switching ASR?
- **Basis in paper**: [explicit] The authors state in Section 4.2.2 that LoRA "still outperforms SPT4ASR... whether in the small or medium model" in terms of performance and efficiency, despite SPT's improvements
- **Why unresolved**: The paper establishes SPT as a viable alternative but acknowledges it currently offers a trade-off rather than a decisive advantage over the dominant PEFT method (LoRA)
- **What evidence would resolve it**: Experiments demonstrating SPT variants achieving lower Mix Error Rates (MER) than LoRA with equal or fewer trainable parameters on standard CS datasets

### Open Question 2
- **Question**: What mechanisms drive the superior performance of combining Full Fine-Tuning (FFT) with soft prompts ("Whole model SPT") over standard FFT?
- **Basis in paper**: [explicit] Section 4.2.2 notes that "integrating soft prompts with whole model full tuning resulted in significantly better performance... compared to standard FFT methods," a result described as surprising
- **Why unresolved**: While the empirical result is presented, the paper does not provide a theoretical explanation for why adding prompt tokens to FFT improves optimization compared to FFT alone
- **What evidence would resolve it**: Ablation studies analyzing the loss landscape or gradient dynamics of FFT with and without soft prompts to identify the specific contribution of the prompt tokens

### Open Question 3
- **Question**: Does the SPT4ASR framework generalize to code-switching scenarios involving languages with different scripts or lower resource availability than Mandarin-English?
- **Basis in paper**: [inferred] The experiments are restricted to Mandarin-English (SEAME, ASRU2019), limiting the claims of "low-resource transfer" to this specific, relatively high-resource pair
- **Why unresolved**: The effectiveness of language prompt tuning (LPT) and other variants may vary depending on the linguistic distance between the languages or the quality of Whisper's pre-trained embeddings for those specific languages
- **What evidence would resolve it**: Evaluation of the SPT4ASR method on diverse code-switching datasets involving low-resource languages or those with non-Latin scripts not specifically targeted in the pre-training data

## Limitations

- **Limited generalization across code-switching datasets**: Results are based on only two Mandarin-English datasets (SEAME, ASRU2019), with no validation on other language pairs or more diverse code-switching corpora
- **Lack of monolingual baseline for Whisper-small/medium**: The paper does not provide MER/WER numbers for Whisper-small/medium on the monolingual datasets used for catastrophic forgetting evaluation, making it difficult to quantify relative degradation
- **Sparse implementation details for SPT variants**: Critical architectural details like exact DPT injection points, ResPT MLP architecture, and SPT4ASR combination method are not fully specified

## Confidence

- **High Confidence**: The core claim that SPT-based methods achieve lower MER on code-switching tasks than other parameter-efficient methods and significantly better than zero-shot Whisper
- **Medium Confidence**: The claim that SPT-based methods mitigate catastrophic forgetting compared to full fine-tuning, though quantification is weakened by lack of Whisper baseline on monolingual datasets
- **Low Confidence**: The paper's discussion of specific mechanisms by which each SPT variant works, which is largely theoretical without direct empirical validation through ablation studies

## Next Checks

1. **Monolingual Baseline Establishment**: Re-run the Whisper-small/medium models on the monolingual datasets (AISHELL-1, LibriSpeech, SPREDS-U1) used for catastrophic forgetting evaluation to provide a true baseline for measuring performance degradation after code-switching adaptation

2. **Ablation Study of SPT4ASR Components**: Conduct systematic ablation studies on the SPT4ASR combination, training and evaluating models with each component (DPT, ResPT, LPT) individually and in different combinations on SEAME to clarify which elements are most critical for performance gains

3. **Cross-Dataset Generalization Test**: Evaluate the best-performing SPT4ASR model (trained on SEAME) on a different code-switching dataset, such as CS-Dialogue corpus or Mandarin-English subset of Common Voice, to test generalization to new speakers, acoustic conditions, and switching patterns