---
ver: rpa2
title: An Agentic AI Framework for Training General Practitioner Student Skills
arxiv_id: '2512.18440'
source_url: https://arxiv.org/abs/2512.18440
tags:
- feedback
- medical
- agent
- patient
- personality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We present an agentic AI framework for training general practitioner
  (GP) student skills that addresses limitations in current virtual simulated patients
  (VSPs), including medical accuracy, consistent roleplaying, scenario generation,
  and educationally structured feedback. The framework integrates three agent roles:
  a generator agent that produces evidence-based vignettes using large language models
  (LLMs) and retrieval augmentation, a conversational agent that simulates patient
  dialogue with controlled response generation and personality customization via Big
  Five traits, and a critic agent that provides standards-based automated feedback
  on communication (using the Master Interview Rating Scale) and clinical reasoning.'
---

# An Agentic AI Framework for Training General Practitioner Student Skills

## Quick Facts
- arXiv ID: 2512.18440
- Source URL: https://arxiv.org/abs/2512.18440
- Reference count: 40
- Presented an agentic AI framework with three roles (generator, conversational, critic) for GP student training that showed realistic dialogue, appropriate difficulty, and useful feedback in evaluation with 14 medical students

## Executive Summary
This paper presents an agentic AI framework designed to address limitations in current virtual simulated patient (VSP) training for general practitioner students. The framework integrates three specialized agent roles: a generator agent for evidence-based scenario creation, a conversational agent for patient simulation with personality customization, and a critic agent for standards-based feedback on communication and clinical reasoning. Evaluated with 14 medical students in a spoken consultation setting, the system demonstrated realistic and vignette-faithful dialogue, appropriate difficulty calibration, stable personality signals, and highly useful feedback. The study supports the architectural pattern of separating scenario control, interaction control, and standards-based assessment for building dependable and pedagogically valuable VSP training tools.

## Method Summary
The framework employs a three-agent architecture where a generator agent creates medical vignettes using LLMs with retrieval augmentation for evidence-based content, a conversational agent simulates patient dialogue with controlled response generation and Big Five personality trait customization, and a critic agent provides automated feedback based on communication standards (Master Interview Rating Scale) and clinical reasoning. The system was evaluated through interactive spoken consultations with 14 medical students, assessing dialogue realism, vignette fidelity, difficulty calibration, personality stability, and feedback quality. The evaluation focused on both technical performance metrics and educational value perceptions from student participants.

## Key Results
- Demonstrated realistic and vignette-faithful dialogue generation in spoken consultation setting
- Showed appropriate difficulty calibration and stable personality signals across interactions
- Generated highly useful, example-rich feedback that students found valuable for learning

## Why This Works (Mechanism)
The framework's effectiveness stems from the specialized division of labor among three agent types, each optimized for distinct functions: scenario generation, patient interaction, and feedback delivery. This separation enables focused optimization of each component while maintaining coherent overall system behavior. The generator agent's use of retrieval augmentation ensures medical accuracy by grounding scenarios in evidence-based content. The conversational agent's personality customization via Big Five traits allows for diverse patient representations, while its controlled response generation maintains consistency with medical scenarios. The critic agent's standards-based approach using established scales like the Master Interview Rating Scale provides pedagogically sound, actionable feedback that aligns with professional training requirements.

## Foundational Learning
- **Large Language Models with Retrieval Augmentation**: Why needed - to ensure medical scenarios are grounded in current evidence and clinical guidelines; Quick check - verify generated vignettes match established medical literature and contain no contradictions
- **Big Five Personality Trait Integration**: Why needed - to create diverse, realistic patient personalities that challenge different communication approaches; Quick check - test personality consistency across multiple interactions with the same patient profile
- **Master Interview Rating Scale**: Why needed - provides standardized, validated framework for assessing communication skills in medical education; Quick check - confirm feedback aligns with professional standards and identifies specific improvement areas
- **Three-Agent Architecture**: Why needed - separates concerns for better optimization, fault isolation, and specialized performance; Quick check - verify each agent maintains functionality independently and integrates smoothly
- **Evidence-Based Scenario Generation**: Why needed - ensures medical accuracy and educational relevance of training scenarios; Quick check - validate clinical scenarios against current medical guidelines and textbooks
- **Controlled Response Generation**: Why needed - maintains consistency with medical scenarios while allowing natural conversation flow; Quick check - test for medically accurate responses that don't contradict scenario parameters

## Architecture Onboarding

Component Map: Generator Agent -> Conversational Agent -> Critic Agent

Critical Path: Medical scenario generation → Patient dialogue simulation → Feedback delivery → Student skill development

Design Tradeoffs:
- Separated agent roles for specialized optimization versus integrated monolithic system for simplicity
- Evidence-based generation with retrieval augmentation versus faster but potentially less accurate direct LLM generation
- Standardized feedback scales versus customized but potentially inconsistent assessment approaches
- Spoken interaction interface versus text-based for more realistic clinical simulation

Failure Signatures:
- Generator agent producing medically inaccurate or inconsistent scenarios
- Conversational agent responses that deviate from personality profiles or medical accuracy
- Critic agent providing feedback that doesn't align with established medical communication standards
- Integration failures causing delays or inconsistencies between scenario, dialogue, and feedback components

3 First Experiments:
1. Test generator agent's ability to create diverse, medically accurate vignettes across different clinical scenarios
2. Validate conversational agent's personality consistency and medical accuracy across multiple patient interactions
3. Evaluate critic agent's feedback quality and alignment with Master Interview Rating Scale standards

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 14 medical students limits generalizability and statistical power of findings
- Controlled spoken consultation setting may not fully represent real clinical environments or capture complete complexity of patient interactions
- Single evaluation round without longitudinal assessment of learning outcomes or transfer to actual clinical settings
- Lack of comparative analysis against existing VSP training approaches to establish relative effectiveness

## Confidence
- **High confidence**: Technical implementation of three-agent architecture demonstrates sound engineering principles
- **Medium confidence**: Observed performance in controlled testing (realistic dialogue, appropriate difficulty, stable personality, useful feedback) is credible but needs broader validation
- **Low confidence**: Claims about framework's effectiveness for actual GP skill development and superiority over existing VSP approaches lack robust comparative evidence

## Next Checks
1. Conduct larger-scale study (n > 50) with randomized controlled design comparing agentic framework against both traditional VSP training and standard virtual patient systems
2. Implement longitudinal assessment tracking student performance across multiple sessions and measuring skill retention and transfer to real clinical environments over 3-6 months
3. Perform external validation with clinical educators and standardized patients to assess framework's alignment with professional standards and identify potential gaps in medical accuracy or educational value