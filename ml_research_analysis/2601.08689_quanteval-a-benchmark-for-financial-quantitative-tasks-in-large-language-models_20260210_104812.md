---
ver: rpa2
title: 'QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language
  Models'
arxiv_id: '2601.08689'
source_url: https://arxiv.org/abs/2601.08689
tags:
- strategy
- quantitative
- reasoning
- coding
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces QuantEval, a benchmark designed to evaluate
  large language models on three essential dimensions of quantitative finance: knowledge-based
  question answering, quantitative mathematical reasoning, and quantitative strategy
  coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting
  framework that executes model-generated strategies and evaluates them using financial
  performance metrics, enabling a more realistic assessment of quantitative coding
  ability.'
---

# QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models

## Quick Facts
- arXiv ID: 2601.08689
- Source URL: https://arxiv.org/abs/2601.08689
- Reference count: 40
- Primary result: Large language models show substantial performance gaps relative to human experts on financial quantitative tasks, particularly in reasoning and strategy coding

## Executive Summary
QuantEval introduces a comprehensive benchmark to evaluate large language models on three core dimensions of quantitative finance: knowledge-based question answering, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling realistic assessment of quantitative coding ability. The authors evaluate 13 state-of-the-art open-source and proprietary LLMs, observing significant performance gaps relative to human experts, particularly in reasoning and strategy coding. To address these gaps, they conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements in reasoning and strategy execution. The full deterministic backtesting configuration is released to ensure strict reproducibility.

## Method Summary
QuantEval benchmarks LLMs across three financial quantitative tasks: knowledge-based QA (660 samples), quantitative mathematical reasoning (855 samples), and quantitative strategy coding (60 samples). The coding evaluation uses a CTA-style backtesting framework that executes generated strategies and measures performance via executable rate and metric mean absolute errors (return, drawdown, Sharpe, return/drawdown ratio). The authors evaluate 13 open-source and proprietary models using chain-of-thought prompting. To improve performance, they conduct supervised fine-tuning on 57K financial samples followed by GRPO reinforcement learning with format and accuracy rewards on DianJin-R1-7B, showing consistent improvements in reasoning and strategy execution.

## Key Results
- Open-source models fail early in strategy coding evaluation, with most failures due to interface mismatch and framework misuse
- Chain-of-thought prompting improves quantitative reasoning accuracy but can degrade knowledge-based recall
- Supervised fine-tuning followed by reinforcement learning improves reasoning performance from 42.0% to 48.4% on reasoning tasks
- Proprietary models show higher executability in strategy coding but still face runtime failures and logic violations

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought (CoT) prompting improves quantitative reasoning accuracy but can degrade knowledge-based recall. CoT decomposes multi-step computations into explicit intermediate steps, reducing arithmetic errors. For factual recall, however, verbose reasoning chains may introduce hallucinated assumptions that override correct priors.

### Mechanism 2
Execution-based backtesting evaluation surfaces strategy coding failures invisible to syntax-only checks. The CTA framework runs generated strategies on historical data with fixed constraints. Failures are categorized as syntax errors, interface mismatches, framework/API misuse, runtime failures, and logic violations. Most open-source models fail at interface compliance before runtime.

### Mechanism 3
Domain-aligned supervised fine-tuning followed by reinforcement learning with format and accuracy rewards improves quantitative reasoning modestly. SFT on 57K financial samples provides cold-start domain familiarity. GRPO RL then optimizes a reward combining structured output format and correctness. Training curves show early-phase improvement with later stabilization.

## Foundational Learning

- Concept: CTA (Commodity Trading Advisor) backtesting framework
  - Why needed here: Strategy coding dimension requires understanding how strategies are instantiated, executed, and evaluated under realistic constraints
  - Quick check question: Given a strategy that generates 100% annualized return with 50% max drawdown on backtest data, what additional checks would you perform before trusting the result?

- Concept: Financial performance metrics (Sharpe ratio, maximum drawdown, return-to-drawdown ratio)
  - Why needed here: Evaluation relies on MAE between model-generated strategies and expert references on these metrics
  - Quick check question: A strategy has annualized return 15%, volatility 10%, risk-free rate 3%. Compute the Sharpe ratio.

- Concept: Chain-of-Thought prompting vs. direct prompting
  - Why needed here: Benchmark explicitly compares w/ CoT vs. w/o CoT
  - Quick check question: For a multi-step option pricing problem requiring volatility estimation followed by Black-Scholes calculation, which prompting approach would you expect to help more, and why might it fail?

## Architecture Onboarding

- Component map: Raw sources (textbooks, market data, strategy repositories) -> cleaning -> structured datasets per task type -> expert annotation + multi-agent pipeline (Summarizer -> Extractor -> Generator -> Validator) -> automated dedup/leakage scanning -> expert review -> final benchmark release -> backtesting harness (fixed configuration) -> strategy execution -> metric computation

- Critical path: Strategy coding evaluation is the bottleneck. A strategy must (1) compile, (2) match CTA class/function interfaces, (3) execute without runtime errors, (4) respect risk constraints, (5) produce metrics within acceptable MAE of expert reference. Failures cascade earlyâ€”most open-source models fail at step 2.

- Design tradeoffs: Small strategy coding set (60 samples) limits statistical power but reflects high expert-validation cost. Fixed backtest configuration ensures reproducibility but may not generalize to alternative cost models or asset universes. CoT-only evaluation for coding was chosen because direct prompting yielded near-zero executability, trading off comparability for non-trivial signal.

- Failure signatures: Interface mismatch (missing required class signatures), runtime failures (shape mismatches, NaN propagation), logic violations (code executes but violates task constraints), knowledge QA CoT degradation (verbose reasoning introduces unsupported assumptions).

- First 3 experiments:
  1. Run a baseline open-source model on the strategy coding subset with and without CoT. Log exact failure categories (syntax, interface, runtime, logic).
  2. Fine-tune a small model on the 57K domain-aligned corpus and measure improvement on the reasoning subset before/after.
  3. Evaluate a proprietary model on strategy coding with a perturbed backtest configuration. Measure metric sensitivity to configuration changes.

## Open Questions the Paper Calls Out

- Can LLM performance on strategy coding be significantly improved by scaling the evaluation dataset beyond 60 instances? (Basis: limitations section notes small coding set due to expert validation cost)
- How sensitive are model rankings and strategy metrics to variations in the backtesting configuration? (Basis: limitations section notes different configurations may affect reported metrics)
- Do the observed financial reasoning capabilities transfer effectively to multilingual financial markets? (Basis: limitations section highlights current focus on English-only instances)

## Limitations

- QuantEval contains only 60 strategy coding instances due to high expert validation cost, limiting statistical power
- The benchmark currently focuses on English-only instances, limiting multilingual market applicability
- Different backtesting configurations (transaction costs, slippage, asset universe) may affect reported metrics

## Confidence

- High Confidence: Benchmark design and evaluation methodology are well-specified and internally consistent; early-failure cascade observation is robustly supported
- Medium Confidence: Performance gaps between open-source and proprietary models are plausible but require independent replication; CoT prompting effects are consistent with execution logs
- Low Confidence: Generalizability of SFT+GRPO training improvements beyond DianJin-R1-7B model and specific corpus; training specifications are incomplete

## Next Checks

1. Reproduce execution failure analysis: Run strategy coding evaluation on a publicly available open-source model and categorize failures to verify the claimed early-failure cascade
2. Validate CoT prompting effects: Design controlled experiment comparing CoT vs. direct prompting on reasoning tasks to replicate reported trade-offs
3. Test configuration sensitivity: Evaluate a proprietary model on strategy coding with perturbed backtest parameters to assess robustness of execution-based evaluation framework