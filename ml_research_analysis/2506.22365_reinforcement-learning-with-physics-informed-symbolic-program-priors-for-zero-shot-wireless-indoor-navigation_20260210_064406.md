---
ver: rpa2
title: Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot
  Wireless Indoor Navigation
arxiv_id: '2506.22365'
source_url: https://arxiv.org/abs/2506.22365
tags:
- learning
- policy
- neural
- reinforcement
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PiPRL, a hierarchical neuro-symbolic framework
  for wireless indoor navigation using physics-informed symbolic programs as inductive
  biases in reinforcement learning. It addresses the challenge of zero-shot generalization
  in complex indoor environments where mmWave signals propagate through multiple paths.
---

# Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation

## Quick Facts
- arXiv ID: 2506.22365
- Source URL: https://arxiv.org/abs/2506.22365
- Reference count: 36
- Proposed PiPRL framework reduces training time by 26% compared to non-physics-based RL baselines

## Executive Summary
PiPRL introduces a hierarchical neuro-symbolic framework that leverages physics-informed symbolic programs as inductive biases for zero-shot wireless indoor navigation. The system addresses the challenge of navigating complex indoor environments where mmWave signals propagate through multiple paths by encoding physics priors into symbolic programs written in a domain-specific language called RLang. These programs guide reinforcement learning alongside neural perception modules for SLAM and signal processing, achieving better navigation efficiency across line-of-sight and non-line-of-sight scenarios in unseen testing environments.

## Method Summary
The PiPRL framework combines symbolic programs encoding physics priors with neural components through a hierarchical architecture. Physics priors such as signal strength monotonicity and angle-of-arrival navigation are encoded as symbolic programs in RLang, which serve as inductive biases during reinforcement learning. The framework integrates these symbolic programs with neural perception modules (for SLAM and signal processing) and visual control components. The symbolic programs guide the RL process while neural networks handle perception tasks, creating a neuro-symbolic system that achieves zero-shot generalization across different indoor environments.

## Key Results
- PiPRL reduces training time by over 26% compared to non-physics-based RL baselines
- Consistently achieves better navigation efficiency across LOS, 1-NLOS, and 2+-NLOS scenarios
- Outperforms both purely symbolic and neural approaches in unseen testing environments

## Why This Works (Mechanism)
The effectiveness stems from combining physics-informed symbolic priors with neural learning, where the symbolic programs encode domain knowledge about mmWave signal propagation that would be difficult for pure neural networks to learn from data alone. This neuro-symbolic approach leverages the interpretability and structured reasoning of symbolic programs while maintaining the adaptability of neural networks for perception tasks.

## Foundational Learning
- **mmWave signal propagation**: Understanding how millimeter-wave signals reflect, diffract, and propagate through indoor environments is crucial for encoding appropriate physics priors. Quick check: Verify that signal models account for multipath effects in typical indoor scenarios.
- **Reinforcement learning with inductive biases**: The framework demonstrates how domain-specific knowledge can be encoded as priors to improve sample efficiency and generalization. Quick check: Compare learning curves with and without physics priors to quantify efficiency gains.
- **Neuro-symbolic integration**: Combining symbolic reasoning with neural perception requires careful architectural design to ensure complementary strengths. Quick check: Evaluate performance degradation when either component is removed.

## Architecture Onboarding

**Component Map:**
Symbolic Programs (RLang) -> RL Agent -> Neural Perception (SLAM/Signal) -> Visual Control -> Environment

**Critical Path:**
Physics priors encoded in RLang guide the RL agent's policy learning, which is informed by neural perception outputs and executed through visual control commands in the environment.

**Design Tradeoffs:**
The framework trades implementation complexity for improved generalization and training efficiency. The specialized RLang requires domain expertise to design but provides strong inductive biases that pure neural approaches lack.

**Failure Signatures:**
Performance degradation may occur when physics priors encoded in symbolic programs don't match real-world conditions, or when neural perception modules fail to accurately process environmental signals.

**First Experiments:**
1. Ablation study comparing PiPRL against pure neural RL baseline without physics priors
2. Cross-environment generalization test where model trained on one building navigates in another
3. Performance comparison across varying levels of signal obstruction (LOS vs NLOS scenarios)

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to controlled indoor environments, potentially not capturing real-world deployment complexities
- RLang appears specialized for navigation tasks, raising questions about generalizability to other robotics applications
- Lacks comparison with state-of-the-art model-based RL approaches that might achieve similar efficiency gains

## Confidence
- **High confidence**: Physics-informed symbolic priors improve navigation efficiency in controlled indoor scenarios
- **Medium confidence**: The hierarchical neuro-symbolic architecture provides meaningful generalization across LOS and NLOS conditions
- **Low confidence**: The scalability of RLang to diverse robotic tasks and environmental complexities

## Next Checks
1. Evaluate PiPRL in multi-story buildings with dynamic obstacles and varying human traffic to test robustness in realistic deployment scenarios
2. Compare performance against state-of-the-art model-based RL approaches that use different inductive biases for navigation tasks
3. Test the framework's generalization to different frequency bands (sub-6GHz) and outdoor environments to assess cross-domain applicability