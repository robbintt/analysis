---
ver: rpa2
title: 'Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation
  Learning'
arxiv_id: '2507.02915'
source_url: https://arxiv.org/abs/2507.02915
tags:
- audio
- masked
- online
- available
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce Audio-JEPA, a self-supervised audio representation
  learning model based on the Joint-Embedding Predictive Architecture (JEPA) paradigm.
  Audio-JEPA adapts I-JEPA to audio by predicting latent representations of masked
  spectrogram patches using a Vision Transformer backbone, trained with random masking
  on mel-spectrograms from 10-second, 32kHz AudioSet clips.
---

# Audio-JEPA: Joint-Embedding Predictive Architecture for Audio Representation Learning

## Quick Facts
- **arXiv ID**: 2507.02915
- **Source URL**: https://arxiv.org/abs/2507.02915
- **Reference count**: 40
- **Primary result**: Audio-JEPA achieves comparable performance to wav2vec 2.0 and data2vec on X-ARES using less than one-fifth of the training data and without hyperparameter tuning.

## Executive Summary
Audio-JEPA introduces a self-supervised audio representation learning model based on the Joint-Embedding Predictive Architecture (JEPA) paradigm. The model adapts I-JEPA to audio by predicting latent representations of masked spectrogram patches using a Vision Transformer backbone. Trained on mel-spectrograms from 10-second, 32kHz AudioSet clips with random masking, Audio-JEPA demonstrates that masked latent prediction is effective for general-purpose audio representation learning. The approach achieves strong performance on the X-ARES evaluation suite, particularly excelling in k-nearest-neighbor evaluation on music and environmental sound tasks, though it shows weaker results on speech-related benchmarks under linear probing.

## Method Summary
Audio-JEPA operates by encoding 10-second, 32kHz AudioSet clips into mel-spectrograms (128 mel bands, 256 time bins), which are then divided into 16×16 non-overlapping patches. A Vision Transformer encoder processes these patches to produce embeddings, while a predictor network generates predictions for randomly masked patches. The model is trained using L2 loss between predicted and target embeddings, with a target encoder updated via exponential moving average (EMA). Random masking of 40-60% of patches is applied per batch. The architecture uses standard ViT configurations with 768-dim embeddings, 12 layers, and 12 heads. Training employs AdamW optimization with warmup and cosine annealing, running for 100k steps on 4× V100 GPUs.

## Key Results
- Achieves comparable performance to wav2vec 2.0 and data2vec on X-ARES suite using <20% of training data
- Outperforms baselines in k-nearest-neighbor evaluation on multiple music and environmental sound tasks
- Underperforms on speech-related benchmarks under linear probing, ranking last on several speech datasets
- Demonstrates effectiveness of masked latent prediction for audio representation learning without hyperparameter tuning

## Why This Works (Mechanism)
Audio-JETA leverages the JEPA paradigm by predicting latent representations rather than raw audio signals, which enables more abstract and semantically meaningful representations. The masked prediction task forces the model to learn contextual relationships between audio patches, creating representations that capture global audio structure. Using a ViT encoder allows the model to process spectrogram patches in a way that naturally handles variable-length audio while maintaining spatial relationships. The random masking strategy proves more effective than block masking for audio, suggesting that the model benefits from learning to predict isolated contextual patches rather than contiguous blocks.

## Foundational Learning
**JEPA/Masked Latent Prediction**: The core mechanism of predicting masked latent representations rather than raw signals enables learning of more abstract, semantically meaningful features. Why needed: Raw signal prediction is computationally expensive and may not capture high-level semantic structure. Quick check: Verify that latent prediction loss produces stable training curves and reasonable embedding statistics.

**AudioSet Preprocessing**: Converting audio to mel-spectrograms and patching them for transformer input. Why needed: Transformers require structured input; spectrograms provide frequency-temporal structure while being computationally efficient. Quick check: Confirm mel-spectrogram dimensions match expected patch sizes and normalization is appropriate.

**Target Encoder with EMA**: Using a slowly updated target encoder prevents collapse and stabilizes training. Why needed: Without EMA, the model can easily learn degenerate solutions where all predictions are constant. Quick check: Monitor embedding norms and variance during training to detect collapse.

## Architecture Onboarding

**Component Map**: Audio clips → Mel-spectrogram → Patch extraction → ViT encoder → Latent embeddings → Predictor → Predicted embeddings → L2 loss (masked patches only) → EMA-updated target encoder

**Critical Path**: The training loop centers on masked latent prediction: input spectrogram → patch masking → encoder forward pass → predictor forward pass → L2 loss computation → parameter updates. The EMA target encoder update runs asynchronously to the main training step.

**Design Tradeoffs**: The choice of random masking over block masking prioritizes learning contextual relationships over spatial continuity. Using a ViT rather than a CNN-based architecture provides better handling of global context but at higher computational cost. The decision to evaluate with frozen embeddings prioritizes efficiency over potentially better fine-tuned performance.

**Failure Signatures**: Representation collapse manifests as constant or near-zero embedding norms. Poor kNN performance suggests embeddings lack semantic structure. Underperformance on speech tasks with linear probing indicates embeddings may not be linearly separable for speech-specific features.

**First Experiments**:
1. Train for 10k steps with random masking to verify basic functionality and check for collapse
2. Evaluate frozen embeddings on a subset of X-ARES tasks using kNN to assess semantic structure
3. Compare random versus block masking strategies on a single task to confirm the paper's findings

## Open Questions the Paper Calls Out

**Open Question 1**: Would attention-pooling heads substantially improve linear-probe performance for Audio-JEPA, and by how much?
The authors suggest that replacing the single-frame MLP used in linear-probe evaluation with attention-pooling blocks could yield fairer comparisons and narrow the current gap. This is particularly relevant given that Audio-JEPA ranks last on roughly half of X-ARES datasets under linear probing, and the L2 training objective does not guarantee linearly separable embeddings.

**Open Question 2**: Can modern audio transformer backbones (e.g., ConvFormer, CAFormer) and alternative positional encodings improve Audio-JEPA's modeling of long-range temporal dependencies?
The paper proposes that swapping the vanilla ViT for recent audio transformers and testing rotary or conditional sine-cosine encodings should improve modeling of long-range temporal cues in 10-second audio clips across speech, music, and environmental sounds.

**Open Question 3**: What are the optimal hyperparameters (mask ratio, EMA decay, optimizer settings) for Audio-JEPA, and how much performance headroom remains?
The authors acknowledge that a systematic sweep of mask ratio, EMA decay, and optimizer settings could uncover additional performance headroom, as the current configuration was trained with off-the-shelf hyperparameters without tuning.

**Open Question 4**: Can incorporating speech-specialized pre-training data or augmentations improve Audio-JEPA's performance on fine-grained speech discrimination tasks?
Audio-JEPA is weakest on tasks requiring fine-grained speech discrimination (e.g., speaker verification and keyword spotting), suggesting that specialized data could improve these cases. The model underperforms substantially on ASV2015, VoxCeleb1, Fluent Speech Commands, and Speech Commands V1.

## Limitations
- Underperforms on speech-related benchmarks under linear probing, ranking last on several speech datasets
- Limited hyperparameter tuning may leave significant performance headroom unexplored
- Evaluation relies on frozen target encoder embeddings rather than fine-tuning, potentially underestimating practical utility
- The EMA decay schedule and exact mel-spectrogram parameters are underspecified, affecting reproducibility

## Confidence
- **High confidence**: Core architectural contribution of adapting JEPA to audio is clearly demonstrated with consistent results across multiple baselines and tasks
- **Medium confidence**: Data efficiency claims are valid but may be affected by limited hyperparameter tuning
- **Medium confidence**: Task-specific performance variations are clearly reported, though the underlying reasons for speech task underperformance require further investigation

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary EMA decay rate, masking ratio, and learning rate schedule to identify optimal configurations and determine if current performance is near the ceiling
2. **Cross-modal evaluation**: Test Audio-JEPA embeddings on multimodal AudioSet tasks (e.g., audio-video retrieval) to assess generality beyond single-modality benchmarks
3. **Fine-tuning versus frozen evaluation**: Compare linear probing and kNN results against full fine-tuning performance across all X-ARES tasks to understand practical deployment trade-offs