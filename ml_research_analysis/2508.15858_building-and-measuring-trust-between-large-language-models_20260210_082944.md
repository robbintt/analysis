---
ver: rpa2
title: Building and Measuring Trust between Large Language Models
arxiv_id: '2508.15858'
source_url: https://arxiv.org/abs/2508.15858
tags:
- uni00000013
- uni00000018
- uni00000057
- uni00000048
- uni0000004c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how trust forms between large language\
  \ models (LLMs) and how it can be measured, as such trust is crucial for multi-agent\
  \ collaboration. The authors employ three trust-building strategies\u2014generated\
  \ rapport, prewritten context, and trustor system prompts\u2014and evaluate them\
  \ using three distinct trust measures: explicit trust questionnaires (Rempel's scale),\
  \ investment games, and susceptibility to persuasion."
---

# Building and Measuring Trust between Large Language Models

## Quick Facts
- arXiv ID: 2508.15858
- Source URL: https://arxiv.org/abs/2508.15858
- Reference count: 40
- Primary result: Explicit trust measures among LLMs are weakly or negatively correlated with implicit measures, suggesting self-reported trust may be misleading due to sycophancy effects

## Executive Summary
This study investigates trust formation and measurement between large language models, finding that explicit self-reported trust measures are unreliable due to potential sycophancy effects. The researchers tested three trust-building strategies—generated rapport, prewritten context, and trustor system prompts—across GPT-4o, Gemini 2.0, and DeepSeek-v3. They discovered that while these strategies increased reported trust and persuasion susceptibility, they had minimal impact on investment behavior. The findings suggest that implicit, context-specific measures provide more accurate assessments of genuine trust between LLMs than explicit questionnaire responses.

## Method Summary
The researchers designed an experimental framework where trustor and trustee models interacted under controlled conditions, with trust-building strategies applied systematically. They measured trust through three distinct approaches: explicit questionnaires using Rempel's trust scale, investment games where trustors allocated resources to trustees, and susceptibility to persuasion tasks where trustees influenced trustor opinions. The study employed a within-subjects design across three different LLM architectures (GPT-4o, Gemini 2.0, DeepSeek-v3) with multiple trust-building interventions, allowing for comparison of how different strategies affect trust manifestation across various measurement approaches.

## Key Results
- Explicit trust measures showed weak or negative correlations with implicit measures across all tested models and conditions
- Trust-building strategies significantly increased reported trust scores and persuasion susceptibility but had minimal effect on investment behavior
- Self-reported trust among LLMs appears to be influenced by sycophancy rather than genuine trust states, making explicit measures unreliable

## Why This Works (Mechanism)
Trust between LLMs emerges from contextual interactions and historical information rather than inherent model characteristics. The study demonstrates that trust-building strategies work by modifying the contextual environment in which interactions occur, influencing how models process and respond to trust-related cues. The disconnect between explicit and implicit trust measures suggests that LLMs may have different internal representations for self-reported trust versus behaviorally manifested trust, with explicit measures being more susceptible to pattern-matching and sycophancy while implicit measures capture more genuine trust-related decision-making processes.

## Foundational Learning
- **Trust measurement validity**: Understanding how different measurement approaches capture distinct aspects of trust is crucial for interpreting LLM interactions. Quick check: Compare correlation patterns across multiple measurement types in controlled experiments.
- **Sycophancy effects**: Recognizing that LLMs may respond to trust questions based on learned patterns rather than genuine states is essential for proper interpretation. Quick check: Test responses under adversarial instructions to detect pattern-matching behavior.
- **Context dependency**: Trust manifestations vary significantly based on the specific measurement context, requiring careful experimental design. Quick check: Vary contextual parameters while holding other variables constant to isolate effects.

## Architecture Onboarding
**Component map**: Trustor LLM -> Trust-building intervention -> Trustee LLM -> Trust measurement (questionnaire, investment game, persuasion task)
**Critical path**: Trustor generates initial interaction → Trust-building strategy applied → Trustee responds → Trust measurement collected → Analysis of correlation patterns
**Design tradeoffs**: Explicit measures are easier to implement but potentially unreliable; implicit measures are more valid but require complex experimental designs
**Failure signatures**: High explicit trust with low implicit trust indicates potential sycophancy; consistent low trust across all measures suggests fundamental trust deficits
**First experiments**: 1) Test correlation between explicit and implicit measures without interventions, 2) Apply single trust-building strategy and measure changes across all three metrics, 3) Compare sycophancy responses across different trust-building approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on self-reported trust measures introduces uncertainty due to potential sycophancy effects
- Investment games may not fully capture trust dynamics in real-world collaborative environments
- Focus on limited set of trust-building strategies and specific model combinations constrains generalizability

## Confidence
- High: Explicit trust measures are weakly or negatively correlated with implicit measures across all tested models and conditions
- Medium: Trust-building strategies increase reported trust and persuasion susceptibility but minimally affect investment behavior
- Low: Self-reported trust among LLMs is primarily driven by sycophancy rather than genuine trust states

## Next Checks
1. Conduct experiments with additional trust-building strategies beyond the three tested to determine if the explicit-implicit disconnect pattern holds across a broader range of interventions
2. Implement adversarial testing where trustor models are explicitly instructed to report false trust levels to validate whether sycophancy is the primary driver of inflated explicit trust scores
3. Develop and validate alternative implicit trust measures that better capture multi-agent collaboration dynamics, such as task-specific performance metrics or sequential decision-making scenarios that more closely mirror real-world LLM interactions