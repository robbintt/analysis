---
ver: rpa2
title: 'From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions'
arxiv_id: '2511.11789'
source_url: https://arxiv.org/abs/2511.11789
tags:
- persona
- personas
- agents
- task
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether assigning personas to LLM-based
  agents introduces biases in multi-agent interactions, focusing on social traits
  like trustworthiness and insistence. Through controlled experiments in collaborative
  problem-solving and persuasion tasks, we find that personas from historically advantaged
  groups (e.g., men and White individuals) are perceived as less trustworthy and demonstrate
  less insistence, while agents exhibit significant in-group favoritism.
---

# From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions

## Quick Facts
- arXiv ID: 2511.11789
- Source URL: https://arxiv.org/abs/2511.11789
- Reference count: 22
- Key outcome: Personas from historically advantaged groups (men, White) perceived as less trustworthy and showing less insistence in LLM multi-agent interactions, with persistent in-group favoritism across tasks and models.

## Executive Summary
This study investigates how assigning demographic personas to LLM-based agents affects multi-agent interactions, focusing on social traits like trustworthiness and insistence. Through controlled experiments in collaborative problem-solving and persuasion tasks, the research reveals that personas from historically advantaged groups (men and White individuals) are perceived as less trustworthy and demonstrate less insistence, while agents exhibit significant in-group favoritism. These biases persist across various LLMs, group sizes, and interaction rounds, highlighting an urgent need for mitigation to ensure fairness and reliability in multi-agent systems.

## Method Summary
The study employs a three-stage experimental framework to analyze persona-induced bias in LLM multi-agent systems. First, dyadic interactions with one persona-assigned agent and one default agent are tested in both collaborative problem-solving (using GPQA questions) and persuasion tasks (using PMIYC/Perspectrum claims). Second, dyadic interactions expand to two persona-assigned agents. Finally, multi-agent scenarios (3-6 agents) with up to five interaction rounds are examined. The research uses GPT-4o, Gemini-1.5-Pro, and DeepSeek-V3 models with personas based on gender (woman, man, trans woman, trans man, non-binary) and race (White, Black, Asian, Hispanic). Key metrics include trustworthiness, insistence, conformity rates, win rates, and persuasion effectiveness.

## Key Results
- Personas from historically advantaged groups (men, White) show lower trustworthiness and insistence scores
- In-group favoritism is consistently observed across all experimental conditions
- Bias effects persist across different LLM models, group sizes, and interaction rounds
- Gender-based biases are more pronounced than race-based biases in multi-agent settings

## Why This Works (Mechanism)
Persona assignment influences agent behavior through pre-defined social identity templates that shape response patterns and interaction styles. The mechanism operates by priming LLMs with demographic characteristics that trigger culturally embedded associations and behavioral expectations. When agents interact, these primed identities create systematic variations in trust signaling, assertiveness levels, and conformity tendencies that align with real-world social biases. The multi-agent environment amplifies these effects through social influence dynamics, where individual agent biases compound into group-level patterns of favoritism and discrimination.

## Foundational Learning
- **Persona priming**: Why needed - establishes baseline identity framework for agent behavior; Quick check - verify persona descriptions are consistently applied across all model calls
- **Trustworthiness metrics**: Why needed - quantifies perceived reliability differences between persona groups; Quick check - confirm conformity calculations match defined formulas
- **Insistence measurement**: Why needed - captures assertiveness variations across demographic groups; Quick check - validate that 1-P(conformity) correctly represents insistence
- **In-group favoritism detection**: Why needed - identifies preferential treatment within same-demographic groups; Quick check - ensure group composition is correctly tracked in multi-agent interactions
- **Multi-round interaction dynamics**: Why needed - reveals how biases evolve over extended exchanges; Quick check - verify conversation history is properly maintained across rounds

## Architecture Onboarding

**Component Map:**
GPQA/Persuasion Dataset -> Initial Response Generation -> Persona Assignment -> Multi-Agent Interaction Orchestration -> Bias Metric Computation

**Critical Path:**
Dataset filtering → Initial response generation → Persona assignment → Interaction execution → Metric calculation

**Design Tradeoffs:**
- Temperature=0 for reproducibility vs. natural language variability
- Single-round vs. multi-round interactions for bias stability
- Limited persona categories vs. comprehensive demographic representation

**Failure Signatures:**
- Non-deterministic outputs despite temperature=0 settings
- Majority influence confounding persona-specific effects
- Incomplete filtering of gender/race-related content in datasets

**3 First Experiments:**
1. Run initial response generation for all three models on 10 GPQA questions to verify correct/incorrect distribution
2. Execute single dyadic CPS interaction with woman persona vs. default agent to test basic metric computation
3. Conduct 3-agent persuasion task with mixed personas to validate multi-agent orchestration

## Open Questions the Paper Calls Out
None

## Limitations
- Exact filtering thresholds for GPQA questions and persuasion claims remain unspecified
- Prompt engineering details for persona assignment and response generation are not fully documented
- Multi-agent orchestration mechanics for N>2 agents are unclear, particularly message passing and conversation history formatting

## Confidence

**High Confidence:**
- Experimental methodology structure and metric definitions are clearly specified
- Core finding of reduced trustworthiness and insistence for advantaged group personas is robust

**Medium Confidence:**
- Specific quantitative bias magnitudes may vary with implementation details
- Effect sizes are likely stable but absolute values could differ

**Low Confidence:**
- Multi-agent orchestration mechanics could significantly affect measured outcomes
- Exact bias magnitudes depend on unspecified implementation details

## Next Checks

1. **Cross-model validation:** Replicate the study using only publicly available LLMs (e.g., Llama-3, Mixtral) to verify whether observed persona-induced biases persist across different model architectures.

2. **Bias detection audit:** Implement systematic checks for gender/race-related content in filtered datasets to ensure the 854 persuasion claims are genuinely neutral on these dimensions.

3. **Sensitivity analysis:** Test the stability of conformity metrics by varying conversation history length and message formatting in multi-agent settings to establish bounds on measurement sensitivity.