---
ver: rpa2
title: KL-Regularized Reinforcement Learning is Designed to Mode Collapse
arxiv_id: '2510.20817'
source_url: https://arxiv.org/abs/2510.20817
tags:
- reward
- arxiv
- distribution
- solution
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes mode collapse in KL-regularized reinforcement
  learning (RL) for large language models (LLMs). The authors show that commonly used
  RL objectives often have unimodal optimal solutions by construction, explaining
  observed diversity collapse during post-training.
---

# KL-Regularized Reinforcement Learning is Designed to Mode Collapse

## Quick Facts
- arXiv ID: 2510.20817
- Source URL: https://arxiv.org/abs/2510.20817
- Reference count: 40
- This paper shows that KL-regularized RL objectives often have unimodal optimal solutions by construction, explaining mode collapse in LLM post-training, and proposes MARA to fix it.

## Executive Summary
This paper analyzes mode collapse in KL-regularized reinforcement learning for large language models, demonstrating that standard RL objectives with equal rewards preserve reference policy probability ratios, preventing lower-support solutions from being promoted. The authors prove that mode coverage depends primarily on regularization strength and relative reward/reference probability scales, not on reverse vs. forward KL choice as commonly believed. To address this, they propose Mode Anchored Reward Augmentation (MARA), a simple method that constructs a target distribution with uniform high mass over all high-quality modes by minimally modifying rewards, improving both quality and diversity on LLM tasks and chemical language models for drug discovery.

## Method Summary
The paper introduces Mode Anchored Reward Augmentation (MARA) as a simple two-line pseudocode method to promote mode diversity in KL-regularized RL. MARA works by augmenting rewards for high-quality samples: r̄ = R(z) + β(log π_ref(z) − log π_ref(y)), where z is the anchor (highest reference probability among samples with R(y) ≥ τ). This equalizes probabilities in the target distribution for all high-quality samples, promoting mode coverage. The method requires no external diversity signals and works with both reverse and forward KL regularization.

## Key Results
- MARA improves both quality and diversity on LLM tasks (creative QA) and chemical language models for drug discovery, outperforming vanilla RL
- Mode coverage depends primarily on regularization strength β and relative reward/reference probability scales, not on reverse vs. forward KL choice
- Standard RL with equal rewards never promotes lower-support solutions, preserving reference policy probability ratios exactly

## Why This Works (Mechanism)

### Mechanism 1: Target Distribution Construction via KL-Regularized Objective
The KL-regularized RL objective implicitly defines a target distribution Gβ whose shape determines mode coverage, independent of optimization quality. For reverse-KL, the optimal solution is Gβ(y) ∝ πref(y)exp(R(y)/β). The probability ratio between any two samples is fixed by the objective design: log(Gβ(y1)/Gβ(y2)) = log(πref(y1)/πref(y2)) + (R(y1)-R(y2))/β.

### Mechanism 2: Equal-Reward Ratio Preservation
When multiple correct solutions receive equal rewards, KL-regularized RL preserves their relative probabilities from the reference policy exactly, never promoting lower-support samples. If R(y1) = R(y2), then Gβ(y1)/Gβ(y2) = πref(y1)/πref(y2), independent of β.

### Mechanism 3: MARA's Anchor-Based Reward Modification
MARA augments rewards for high-quality samples as R̄(y) = R(z) + β(log πref(z) - log πref(y)) to equalize their probabilities in the target distribution. Substituting R̄ into Gβ yields identical unnormalized density for all samples above threshold: πref(z)exp(R(z)/β).

## Foundational Learning

- Concept: **KL Divergence Asymmetry**
  - Why needed here: The paper's central thesis is that common intuitions about reverse vs. forward KL "mode-seeking" vs. "mass-covering" behavior do not transfer to KL-regularized RL
  - Quick check question: Given two Gaussians, explain why reverse KL prefers placing mass on one mode while forward KL spreads mass across all modes. Now explain why this breaks when the approximating family is flexible (e.g., independent categoricals).

- Concept: **Variational Inference and Target Distributions**
  - Why needed here: The paper analyzes RL objectives through the lens of VI—maximizing Jβ is equivalent to minimizing DKL(πθ || Gβ) toward an implicit target Gβ
  - Quick check question: Derive why maximizing expected reward minus KL penalty produces a target distribution proportional to πref(y)exp(R(y)/β).

- Concept: **Mode Coverage vs. Mode Seeking**
  - Why needed here: Understanding when and why standard objectives produce unimodal vs. multimodal solutions is prerequisite to diagnosing diversity collapse
  - Quick check question: Given two correct answers with equal reward but πref(y1) = 10× πref(y2), predict their final ratio under standard KL-regularized RL. What happens if you reduce β by 10×?

## Architecture Onboarding

- Component map:
Policy πθ → Sample Trajectories → Reward Function R → KL Penalty to πref → Gradient Update
                                                ↓
                                        Threshold Check: R(yi) ≥ τ?
                                           ↓ Yes           ↓ No
                                  [Select Anchor z]    [Keep Original]
                                           ↓
                                  [Augment: R̄(yi) = R(z) + β(log πref(z) - log πref(yi))]
                                           ↓
                                        [Gradient Update with Augmented Rewards]

- Critical path:
  1. Batch sampling from current policy
  2. Compute original rewards R(yi) for all samples
  3. Identify threshold τ (can be percentile-based per batch)
  4. Select anchor z = argmax πref(yi) among samples where R(yi) ≥ τ
  5. Apply reward augmentation only to above-threshold samples
  6. Proceed with standard KL-regularized gradient computation

- Design tradeoffs:
  - Threshold τ: Lower values include more samples in augmentation (more diversity but potential quality dilution); higher values are more selective
  - Anchor selection: Using highest-support sample as anchor maximizes the correction effect; alternative anchors may be appropriate when prior support is unreliable
  - Per-batch vs. global threshold: Per-batch adapts to reward distribution but may introduce training instability

- Failure signatures:
  - All outputs still collapse to single mode despite MARA → Check if threshold τ is too high (excluding valid modes) or if reward function has hidden biases
  - Quality degradation → Threshold too low, including near-correct samples that shouldn't be treated equally
  - Training instability with forward KL → Expected; forward KL gradient doesn't match standard intuitions

- First 3 experiments:
  1. **Verifiable 1-2 Task Reproduction**: Train LM to output either "1" or "2" with equal reward. Confirm vanilla RL collapses to higher-probability answer; confirm MARA maintains uniform distribution. Use entropy of valid answers as diversity metric.
  2. **β Sensitivity Analysis**: On a synthetic task with known high-reward modes having different reference supports, sweep β from 0.001 to 1.0 and plot when preference flips between modes. Verify Equation 10 prediction holds empirically.
  3. **Ablation: Anchor Selection Strategy**: Compare using highest-support anchor vs. random anchor vs. lowest-support anchor. Hypothesis: lowest-support anchor will over-correct and hurt quality; highest-support anchor should balance quality and diversity best.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical properties of the forward KL regularized gradient, and can improved gradient estimators be derived that better leverage its structure?
- Basis in paper: Conclusion states "a deeper analysis into the properties of the forward KL regularized gradient" and "better gradients that optimizes the MARA objective" as future directions
- Why unresolved: The paper proves that forward KL regularized RL does not yield a standard forward KL gradient (Remark 3.4), but does not develop alternative gradient estimators tailored to this structure
- What evidence would resolve it: Convergence rate analysis of forward KL gradients; new gradient estimators with provable variance reduction or faster convergence compared to current MARA implementation

### Open Question 2
- Question: Can a theoretically grounded method for selecting the reward threshold τ be developed when reward function ranges are unknown?
- Basis in paper: The paper relies on heuristics ("taking an upper percentile of sampled rewards") for threshold selection in non-verifiable tasks, with no theoretical justification
- Why unresolved: Performance of MARA depends on τ, but no principled approach for automatic or adaptive threshold selection is provided
- What evidence would resolve it: A method for setting τ with provable diversity-quality tradeoff guarantees; ablations showing sensitivity to different τ selection strategies

### Open Question 3
- Question: What is the optimal anchor selection strategy z beyond "highest reference probability among high-reward samples," and how does anchor choice affect the target distribution shape?
- Basis in paper: Algorithm 1 selects z = arg max πref(y) s.t. R(y) ≥ τ, but other anchor choices (e.g., median support, learned anchors) are unexplored
- Why unresolved: The paper provides one anchoring heuristic without comparing alternatives or analyzing how anchor properties influence final policy behavior
- What evidence would resolve it: Comparative study of different anchor selection strategies on downstream task performance; theoretical characterization of how anchor choice affects the resulting target distribution

## Limitations

- The analysis assumes reward functions can be meaningfully thresholded to identify high-quality modes, which may not hold for all tasks where quality exists on a continuous spectrum
- MARA's performance across diverse reward structures remains largely untested, with limited validation of forward KL cases compared to reverse KL
- The anchor selection mechanism (highest-support sample) may not generalize well to cases where reference policy support is unreliable or when high-support samples are not representative of desired modes

## Confidence

- **High confidence**: The theoretical analysis of mode collapse in standard KL-regularized RL is mathematically rigorous and the proof that equal rewards preserve reference probabilities is sound
- **Medium confidence**: The proposed MARA method shows promising empirical results across three domains, but sample sizes and task diversity are limited
- **Medium confidence**: Claims about reverse vs. forward KL having identical mode coverage behavior under certain conditions are theoretically interesting but lack comprehensive empirical validation

## Next Checks

1. **Cross-domain robustness test**: Apply MARA to a task with continuous-valued rewards (e.g., regression or ranking) where mode identification is not binary, to validate threshold-based augmentation generalizes beyond verifiable tasks
2. **Forward KL gradient stability analysis**: Systematically compare training stability and convergence between reverse and forward KL implementations of MARA across multiple random seeds and learning rates, documenting any divergence or instability patterns
3. **Anchor sensitivity ablation**: Test MARA with alternative anchor selection strategies (random anchor, lowest-support anchor, centroid-based) on the same task to quantify how anchor choice affects quality-diversity tradeoff and confirm the paper's recommendation of highest-support anchor