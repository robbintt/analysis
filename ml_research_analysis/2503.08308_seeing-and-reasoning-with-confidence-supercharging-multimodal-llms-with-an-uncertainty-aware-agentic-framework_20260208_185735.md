---
ver: rpa2
title: 'Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an
  Uncertainty-Aware Agentic Framework'
arxiv_id: '2503.08308'
source_url: https://arxiv.org/abs/2503.08308
tags:
- mllm
- srice
- reasoning
- uncertainty
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SRICE, a training-free multimodal reasoning
  framework that addresses two key challenges in multimodal large language models
  (MLLMs): the need for costly data annotation and fine-tuning, and unreliable tool
  use when incorporating external vision models. SRICE uses conformal prediction-based
  uncertainty quantification to calibrate external vision model outputs and select
  the most reliable tool based on MLLM output uncertainty.'
---

# Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework

## Quick Facts
- arXiv ID: 2503.08308
- Source URL: https://arxiv.org/abs/2503.08308
- Reference count: 40
- Key outcome: SRICE achieves 4.6% average improvement over base MLLM across five datasets using training-free uncertainty-aware framework

## Executive Summary
This paper introduces SRICE, a training-free multimodal reasoning framework that addresses two key challenges in multimodal large language models (MLLMs): the need for costly data annotation and fine-tuning, and unreliable tool use when incorporating external vision models. SRICE uses conformal prediction-based uncertainty quantification to calibrate external vision model outputs and select the most reliable tool based on MLLM output uncertainty. The framework enables MLLMs to autonomously select regions of interest through multi-stage interactions with external tools, improving multimodal reasoning without additional training. Experimental results show that SRICE achieves an average improvement of 4.6% over the base MLLM across five datasets (VQA2, VizWiz, Flickr30K, GQA, and MMBench), with performance even surpassing some fine-tuning-based methods.

## Method Summary
SRICE is a two-stage training-free pipeline that enhances MLLMs with external vision tools through uncertainty-aware agentic reasoning. The first stage involves tool inference with conformal prediction (CP) calibration, where external vision models (segmentation and detection) process the input image and their outputs are calibrated using nonconformity scores computed on a held-out set (COCO-2017). The calibrated outputs are then fed to the MLLM, which autonomously selects relevant regions of interest (RoI) by mapping text-based object descriptions to visual importance. The second stage performs final answer generation via chain-of-thought reasoning on the original image plus cropped RoI regions, with uncertainty quantification based on prediction set size from top-p sampling. The framework selects the final answer from the pathway with the lowest uncertainty score, enabling coarse-to-fine reasoning without any fine-tuning.

## Key Results
- SRICE achieves 4.6% average improvement over base MLLM across five datasets
- Performance surpasses some fine-tuning-based methods despite being training-free
- Ablation study validates that conformal calibration significantly outperforms heuristic uncertainty methods
- Demonstrates effectiveness on challenging datasets including VizWiz (low-quality images) and GQA (complex reasoning)

## Why This Works (Mechanism)

### Mechanism 1: Conformal Calibration of Tool Outputs
SRICE uses conformal prediction to calibrate external vision tools by computing nonconformity score thresholds from a held-out calibration set. For object detection, this expands bounding boxes to guarantee the true object is covered with probability $1-\alpha$. The calibration ensures reliable visual information is fed to the MLLM, reducing hallucinations caused by tool noise. The core assumption is that the calibration dataset is exchangeable with test data for finite-sample coverage guarantees.

### Mechanism 2: Uncertainty-Guided Pathway Selection
The framework runs parallel reasoning paths using different tools and selects the final answer based on a prediction set size uncertainty score. It estimates uncertainty by calculating the average number of tokens required to exceed cumulative probability mass $p$ during top-$p$ sampling. A smaller prediction set indicates the model is more confident and concentrated. The system outputs the answer from the pathway with the lower uncertainty score, leveraging the correlation between token probability concentration and answer correctness.

### Mechanism 3: Agentic Region-of-Interest Extraction
SRICE enables the MLLM to autonomously select RoIs based on calibrated tool data, creating a coarse-to-fine reasoning process. Instead of fine-tuning for specific regions, the MLLM analyzes tool outputs (text descriptions of bounding boxes/masks) and returns relevant object IDs. The system then crops these regions and feeds them back into the MLLM for final reasoning. This approach alleviates the high costs of data collection and fine-tuning while maintaining reasoning effectiveness.

## Foundational Learning

- **Concept: Conformal Prediction (CP)**
  - Why needed: CP is the mathematical engine used to calibrate tools; understanding nonconformity scores and quantiles is essential
  - Quick check: Given $\alpha=0.1$, what is the guaranteed minimum probability that the true label is included in the prediction set?

- **Concept: Top-$p$ (Nucleus) Sampling**
  - Why needed: This is the basis for the uncertainty score; the paper exploits top-$p$ structure to measure confidence
  - Quick check: If a model is uncertain, will the number of tokens required to reach probability mass $p=0.9$ be higher or lower than when it is confident?

- **Concept: Vision-Language Grounding**
  - Why needed: The MLLM must interpret text-based bounding boxes to identify objects; understanding text-to-visual mapping is crucial for debugging ROI selection
  - Quick check: How does an MLLM typically represent a bounding box provided in a text prompt internally?

## Architecture Onboarding

- **Component map:** Input Image -> External Tools (SEEM/YOLOv11) -> CP Calibration Layer -> MLLM ROI Selection -> ROI Extraction -> MLLM CoT Reasoning -> Uncertainty Estimator -> Final Answer Selection

- **Critical path:** 1) Input Image → Tool Inference → CP Calibration → MLLM selects IDs → Image Cropping; 2) [Original Image, Cropped Image, Question] → MLLM CoT Reasoning → Uncertainty Score → Select Final Answer

- **Design tradeoffs:** Latency vs. Accuracy (requires multiple tool calls and two MLLM inference passes); Generality vs. Precision (CP guarantees coverage but can result in larger, looser bounding boxes)

- **Failure signatures:** High uncertainty scores on both paths (out-of-domain questions or poor image quality); Disagreement between Tool and MLLM (tool detects object but MLLM doesn't select it)

- **First 3 experiments:** 1) Visualize Calibration: Run detection tool with/without CP to verify appropriate box expansion; 2) Uncertainty Threshold Sensitivity: Sweep $p$ parameter to observe pathway selection changes; 3) Ablation on Tool Choice: Compare performance using only segmentation vs. only detection tools

## Open Questions the Paper Calls Out

- Can the SRICE framework be effectively extended to temporal data, such as video, or other modalities like audio? The current implementation focuses exclusively on static images.

- Can the "quasi-conformal" uncertainty estimation for the MLLM's textual output be replaced by a method with strict statistical coverage guarantees? The current method uses top-p sampling without coverage guarantees.

- How does the framework perform under significant distribution shift between the calibration set and the test environment? The reliance on COCO-2017 calibration raises questions about extreme domain shifts.

## Limitations

- The training-free nature may limit applicability to complex reasoning tasks that benefit from fine-tuning
- Reliance on held-out calibration data (COCO-2017) raises performance concerns under significant domain shift
- The framework requires multiple tool calls and two separate MLLM inference passes, roughly doubling inference time compared to zero-shot

## Confidence

- Core claims about SRICE's effectiveness: **Medium confidence** (4.6% improvement well-documented but training-free approach may have limitations)
- Uncertainty-guided pathway selection mechanism: **High confidence** (theoretical foundation and ablation results are strong)
- Agentic ROI extraction approach: **Medium confidence** (framework works but context preservation concerns not fully addressed)

## Next Checks

1. **Calibration Robustness Test**: Evaluate SRICE's performance when calibration data distribution significantly differs from test data (e.g., medical imaging calibrated on COCO). Measure how prediction set size and answer accuracy degrade with increasing domain shift.

2. **Uncertainty Score Validation**: Create a dataset of cases where the model is confident but incorrect (hallucinations). Test whether the uncertainty score fails to detect these errors and investigate alternative confidence metrics.

3. **Context Preservation Analysis**: Systematically evaluate scenarios where cropped regions lose critical contextual information. Compare SRICE's performance against a variant that preserves full image context alongside cropped regions to quantify the tradeoff between focused reasoning and contextual completeness.