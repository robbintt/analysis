---
ver: rpa2
title: 'Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break
  Large Models'
arxiv_id: '2506.00457'
source_url: https://arxiv.org/abs/2506.00457
tags:
- forecasting
- noise
- llms
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as effective zero-shot time-series forecasters. Experiments on real-world
  and synthetic datasets reveal that LLM-based forecasters are highly sensitive to
  noise, leading to significant performance degradation compared to state-of-the-art
  domain-specific models and even simple single-shot linear models.
---

# Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models

## Quick Facts
- **arXiv ID**: 2506.00457
- **Source URL**: https://arxiv.org/abs/2506.00457
- **Reference count**: 40
- **Primary result**: LLMs show high sensitivity to noise in time-series forecasting, performing worse than both domain-specific models and simple single-shot linear models.

## Executive Summary
This paper investigates whether large language models can effectively serve as zero-shot time-series forecasters. Through extensive experiments on real-world and synthetic datasets, the authors demonstrate that LLMs are highly sensitive to noise, with even small perturbations causing significant performance degradation. The study compares LLM-based forecasters against state-of-the-art domain-specific models and simple linear models, finding that despite exploring strategies like increasing input length and applying noise filtering, LLMs fail to achieve viable performance. The paper concludes that LLMs are not suitable for zero-shot time-series forecasting and suggests focusing on fine-tuning LLMs for numerical sequence processing instead.

## Method Summary
The study evaluates LLM-based time-series forecasting using zero-shot and few-shot prompting approaches on GPT-3.5/4/4o (via API) and LLaMA 2/3 variants (locally). Three benchmark datasets are used: Monash (8 time series from LLMTime), Function (mathematical sequences), and Informer (ETTm2, Exchange Rate, Electricity, Traffic, Weather). The paper compares LLM performance against domain-specific models (TimeMixer, iTransformer, PatchTST, TimesNet, DLinear, RLinear) and single-shot linear models (DLinear-S, RLinear-S) trained on individual input sequences. Evaluation metrics include MAE and MSE for accuracy, plus total computation cost (training + inference time). The study specifically tests noise sensitivity using Gaussian, constant, missing, and frequency-based noise types.

## Key Results
- LLM-based forecasters are highly sensitive to noise, with even small perturbations (e.g., Gaussian noise with std=0.001) causing significant performance degradation.
- Linear models trained with L1/L2 loss functions show inherent robustness to noise types, outperforming LLMs even in zero-shot settings.
- Increasing input context length provides only marginal improvements for LLMs in distinguishing signal from noise.
- Noise filtering methods (Gaussian smoothing, EMA) result in only minimal improvements for LLM performance.
- LLM inference time exceeds the combined training and inference time of linear models, making them less cost-effective.

## Why This Works (Mechanism)

### Mechanism 1: Tokenization-Induced Noise Amplification
LLMs process numerical sequences as text, where minor perturbations can drastically alter token sequences. A small noise value can change tokenization completely (e.g., from ["0", ".", "99"] to ["0", ".", "99", "1"]), amplifying distortions in representation rather than reflecting actual numerical differences.

### Mechanism 2: Lack of Robust Optimization Objective
Linear models trained with standard L1/L2 loss functions have mathematical properties that provide resilience to certain noise distributions. LLMs, lacking this specific numerical optimization during pre-training for this task, fail to filter out such noise.

### Mechanism 3: Context Length vs. Noise Discrimination
A longer input sequence provides more data points, but for LLMs, a longer noisy sequence may just be a longer sequence of confusing tokens, failing to confer a statistical advantage in distinguishing signal from noise.

## Foundational Learning

**Time-Series Forecasting Evaluation Metrics**
- Why needed here: To understand the quantitative evidence (MAE, MSE) presented to justify the superiority of linear models over LLMs.
- Quick check question: If Model A has an MAE of 0.2 and Model B has an MAE of 0.5 on the same dataset, which model is more accurate?

**Zero-Shot vs. Domain-Specific Models**
- Why needed here: The paper's core comparison is between models that require training on specific data versus those that do not.
- Quick check question: Which model type (zero-shot or domain-specific) typically requires a separate training phase before it can make predictions on a new dataset?

**Tokenization in LLMs**
- Why needed here: This is a proposed root cause for the performance degradation of LLMs on numerical data.
- Quick check question: How might a tokenizer break the number `3.14159` into smaller chunks (tokens) for an LLM to process?

## Architecture Onboarding

**Component map:**
Prompt-based LLM Forecaster: (Input Time-Series) -> [Text Serialization] -> [Prompt Template] -> [LLM] -> [Text Deserialization] -> (Forecast)
Domain-Specific Model: (Input Time-Series) -> [Model Weights (trained on data)] -> (Forecast)
Single-Shot Linear Model: (Input Time-Series) -> [Trivial Training (on input only)] -> [Model Weights] -> (Forecast)

**Critical path:** The critical path for an LLM forecaster is the serialization of the numerical series into text tokens, where "noise amplification" mechanism is suspected to originate, followed by the LLM's forward pass and parsing the text output back into numbers.

**Design tradeoffs:**
- LLM: High inference cost and latency vs. no training cost. High susceptibility to input noise.
- Domain-Specific: High training cost vs. low inference cost. Lower noise susceptibility.
- Linear Model: Extremely low training cost and low inference cost vs. limited expressive power. High noise robustness.

**Failure signatures:**
- LLM Forecaster: Performance collapses on datasets with minimal noise. Output may include non-numerical tokens or fail to follow specified format. Inference time exceeds combined training and inference time of linear model.

**First 3 experiments:**
1. Reproduce Noise Sensitivity: Take clean dataset, add small Gaussian noise, compare MAE of LLM-based forecaster against simple linear baseline.
2. Compare Tokenization Strategies: Experiment with different numerical representations in prompts to observe susceptibility to noise amplification effect.
3. Benchmark Inference Time: Measure end-to-end wall-clock time for LLM to generate forecast and compare against time required to train and infer with single-shot linear model.

## Open Questions the Paper Calls Out

### Open Question 1
How do specific tokenization schemes and embedding representations contribute to the high sensitivity of LLMs to noise in time-series forecasting? The paper notes that "the exact mechanisms behind this issue, such as the impact of tokenization and encoding, deserve deeper investigation" but doesn't analyze internal representation mechanics that cause this fragility.

### Open Question 2
Can fine-tuning LLMs on numerical sequences successfully close the performance gap with domain-specific models and mitigate noise sensitivity? The study was restricted to zero-shot and few-shot prompting; the effectiveness of fine-tuning specifically for noise robustness was proposed but not tested.

### Open Question 3
Does the integration of auxiliary textual or multimodal domain context improve the robustness of LLM forecasters against noise? The experiments relied solely on numerical inputs, potentially underutilizing the LLM's reasoning capabilities which might be triggered by textual context to distinguish signal from noise.

## Limitations

- The core claim about tokenization being the primary bottleneck is inferred from observed behavior rather than proven through ablation studies.
- Noise filtering experiments showed minimal improvement, but implementation details for these filters are not fully specified.
- Single-shot linear models may not represent the full potential of simple statistical approaches for time-series forecasting.

## Confidence

- **High Confidence**: Empirical observation that LLM-based forecasters degrade significantly more than domain-specific or linear models when noise is added.
- **Medium Confidence**: Conclusion that LLMs are not viable for zero-shot time-series forecasting in practice.
- **Low Confidence**: Specific mechanism attributing LLM noise sensitivity primarily to tokenization-induced noise amplification.

## Next Checks

1. **Tokenization Ablation Study**: Systematically test different numerical representations in prompts across the same noisy datasets to directly measure impact of tokenization choices on LLM forecasting performance.

2. **Advanced Noise Filtering Implementation**: Implement and evaluate more sophisticated noise reduction techniques (e.g., Kalman filtering, wavelet denoising) on LLM input sequences to determine if better preprocessing could substantially improve LLM robustness.

3. **Extended Linear Model Benchmarking**: Compare LLM performance against a broader range of statistical forecasting methods (ARIMA, exponential smoothing, Prophet) in both clean and noisy conditions to establish whether single-shot linear model is appropriate baseline.