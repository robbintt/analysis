---
ver: rpa2
title: 'One-shot Federated Learning Methods: A Practical Guide'
arxiv_id: '2502.09104'
source_url: https://arxiv.org/abs/2502.09104
tags:
- learning
- methods
- data
- federated
- one-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of one-shot federated
  learning (OFL), a distributed machine learning paradigm that limits client-server
  communication to a single round to address privacy and communication overhead issues
  in traditional federated learning. The paper identifies two major challenges in
  OFL: data heterogeneity (non-iid data across clients) and model heterogeneity (different
  model architectures or resources among clients).'
---

# One-shot Federated Learning Methods: A Practical Guide

## Quick Facts
- arXiv ID: 2502.09104
- Source URL: https://arxiv.org/abs/2502.09104
- Reference count: 2
- This paper provides a comprehensive survey of one-shot federated learning (OFL), a distributed machine learning paradigm that limits client-server communication to a single round to address privacy and communication overhead issues in traditional federated learning.

## Executive Summary
This paper provides a comprehensive survey of one-shot federated learning (OFL), a distributed machine learning paradigm that limits client-server communication to a single round to address privacy and communication overhead issues in traditional federated learning. The paper identifies two major challenges in OFL: data heterogeneity (non-iid data across clients) and model heterogeneity (different model architectures or resources among clients). The authors propose a novel taxonomy categorizing current OFL techniques into four main groups: Parameter Learning, Knowledge Distillation, Generative Models, and Ensemble Methods.

## Method Summary
The survey analyzes OFL methods across four categories: Parameter Learning (clustering, regularizers, Bayesian methods, prototype extraction), Knowledge Distillation (data/model distillation), Generative Models (GANs, VAEs, Stable Diffusion for synthetic data), and Ensemble Methods (static/adaptive). The paper discusses hybrid approaches combining multiple techniques to enhance performance and privacy, with most high-performing methods integrating 2-3 techniques. Key findings include the potential of prototype learning within parameter learning, the superiority of generative models over knowledge distillation methods, and the effectiveness of adaptive ensemble methods.

## Key Results
- Generative models outperform knowledge distillation for capturing local data distributions in OFL
- Adaptive ensemble methods outperform static ensembling by dynamically weighting client contributions
- Prototype learning enables class-level feature extraction that integrates well with other OFL techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative models outperform knowledge distillation for capturing local data distributions in OFL.
- Mechanism: Synthetic data generation (via GANs, VAEs, Stable Diffusion) approximates local dataset statistics without exposing raw data. The server trains on generated samples that reflect client data distributions, mitigating out-of-distribution (OOD) issues inherent to single-round communication.
- Core assumption: The generative model can sufficiently capture local data statistics from client model parameters or summaries to produce useful synthetic training data.
- Evidence anchors:
  - [abstract] "identifies that combining multiple techniques... yields better results... the superiority of generative models over knowledge distillation"
  - [section 3.3] "generative models can address the inefficiencies associated with knowledge distillation methods, often leading to superior performance"
  - [corpus] Neighbor paper "One-Shot Federated Learning with Classifier-Free Diffusion Models" explores diffusion-based approaches, consistent with the generative direction but no direct comparative evidence cited.
- Break condition: If local data has highly complex or multimodal distributions that cannot be captured by the generator architecture; if generator training data or priors introduce bias misaligned with target domain.

### Mechanism 2
- Claim: Adaptive ensemble methods outperform static ensembling by dynamically weighting client contributions based on data or model characteristics.
- Mechanism: Instead of uniform averaging, adaptive methods (e.g., mixture-of-experts, similarity-based weighting) adjust aggregation weights per-sample or per-client. This accounts for data heterogeneity where different clients contribute varying value across input regions.
- Core assumption: There exists measurable signal (feature similarity, prediction confidence, validation performance proxy) that correlates with each client's value for different inputs.
- Evidence anchors:
  - [section 3.4] "adaptive ensemble methods typically yield better results" with FENS and IntactOFL as examples
  - [section 4.1] "IntactOFL significantly improves accuracy using an MoE approach... Its accuracy surpasses both Dense and Co-boosting"
  - [corpus] Limited comparative benchmarks in corpus; claims are paper-internal.
- Break condition: If no reliable proxy exists for per-client quality; if computational overhead of adaptive routing exceeds practical constraints; if client models are too diverse for meaningful comparison.

### Mechanism 3
- Claim: Prototype learning enables class-level feature extraction that integrates well with other OFL techniques.
- Mechanism: Prototypes represent class centroids in feature space. By computing and sharing prototypes rather than full parameters, clients transmit compressed, semantically meaningful summaries that can be aggregated and combined with generative or ensemble methods.
- Core assumption: Classes are sufficiently separable in feature space that prototype representations capture essential discriminative information.
- Evidence anchors:
  - [section 3.1] "prototype learning holds the most promise for integration with others"
  - [section 4.1] "employing prototype learning or conducting more granular analyses of local models or class features in OFL holds significant promise"
  - [corpus] No direct corpus validation; this is a synthesis position from the survey.
- Break condition: If intra-class variance is high relative to inter-class distance; if feature extractors diverge significantly across clients making prototype spaces non-aligned.

## Foundational Learning

- Concept: **Federated Learning (FL) Fundamentals**
  - Why needed here: OFL is a constrained variant of FL; understanding FedAvg, data heterogeneity (non-IID), and model heterogeneity is prerequisite.
  - Quick check question: Can you explain why FedAvg fails under extreme label skew without multiple rounds?

- Concept: **Knowledge Distillation**
  - Why needed here: Distillation is a core OFL technique for compressing ensemble knowledge and enabling model heterogeneity.
  - Quick check question: What information does a student model learn from teacher logits vs. hard labels?

- Concept: **Generative Models (GANs, VAEs, Diffusion)**
  - Why needed here: Understanding trade-offs between these architectures informs synthetic data generation choices in OFL.
  - Quick check question: Why might a VAE be preferred over a GAN for capturing multi-modal data distributions?

## Architecture Onboarding

- Component map:
  - Parameter Learning → Clustering, regularizers, Bayesian methods, prototype extraction
  - Knowledge Distillation → Data distillation (DOSFL, FedD3), model distillation (often hybrid)
  - Generative Models → Embedding generation, GANs, VAEs, Stable Diffusion, model inversion
  - Ensemble Methods → Static (averaging) vs. adaptive (MoE, similarity-weighted)
  - Hybrid → Most high-performing methods combine 2-3 techniques (e.g., FedSD2C: data distillation + VAE; IntactOFL: GAN + MoE ensemble)

- Critical path:
  1. Define heterogeneity constraints (data: quantity/feature/label skew; model: architecture differences)
  2. Choose base technique based on constraints (parameter learning cannot handle model heterogeneity)
  3. If accuracy is priority, combine: prototype learning (local) + generative model (server-side synthetic data) + adaptive ensemble
  4. If privacy is paramount, prefer distillation-based or generative approaches over raw parameter sharing

- Design tradeoffs:
  - Parameter learning: Theoretical convergence guarantees, but limited to homogeneous models; privacy concerns from parameter exposure
  - Knowledge distillation: Better privacy, supports model heterogeneity, but information loss during compression
  - Generative models: Highest accuracy potential per paper, but computationally expensive; quality depends on generator capacity
  - Static vs. adaptive ensemble: Static is simpler; adaptive requires validation signal but handles heterogeneity better

- Failure signatures:
  - OOD degradation: Model fails on classes unseen in certain clients (label skew) → indicates need for generative augmentation or prototype sharing
  - Model mismatch errors: Aggregation fails when architectures differ → switch from parameter learning to distillation/generative
  - Privacy leakage from labels: Methods requiring label transmission (XorMixFL, FedCAVE) violate data-free goals → use data-free alternatives

- First 3 experiments:
  1. Baseline comparison: Implement FedAvg-style averaging vs. static ensemble vs. adaptive ensemble on CIFAR-10 with Dirichlet label skew (α=0.1) to quantify heterogeneity impact.
  2. Generative model ablation: Compare synthetic data quality from GAN vs. VAE vs. pre-trained diffusion (Stable Diffusion) on a held-out validation set; measure downstream global model accuracy.
  3. Hybrid technique integration: Combine prototype extraction at clients with server-side generative training; compare against single-technique baselines to validate the paper's claim that "aggregating more techniques yields better results."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can OFL methods achieve competitive accuracy without relying on any additional public datasets, foundation models, or label information transmission?
- Basis in paper: [explicit] The authors identify "Data-free Requirement" as a key future direction, noting that methods like One-shot FL, FedKT, FedKD depend on additional public datasets, while XorMixFL and FedCAVE require transmitting local labels, posing privacy risks.
- Why unresolved: Current state-of-the-art OFL methods (FedPFT, FedBiP, FedDISC) still leverage foundation models trained on public data. The trade-off between being truly data-free and maintaining accuracy remains unexplored.
- What evidence would resolve it: Development of an OFL method that matches or exceeds current baselines on standard benchmarks (CIFAR, Tiny ImageNet) without using any external data, labels, or pre-trained foundation models.

### Open Question 2
- Question: How can OFL methods be effectively scaled to train or fine-tune Large Language Models across geo-distributed clients?
- Basis in paper: [explicit] The authors explicitly state: "most current experimental approaches for OFL methods primarily focus on models like LeNet, VGGNet, and ResNet" and call for exploring "OFL methods to enable more practical training of LLMs with different parties."
- Why unresolved: LLMs present unique challenges including enormous parameter sizes (making even single-round communication costly), heterogeneous model architectures across clients, and the need for knowledge distillation between heterogeneous LLMs.
- What evidence would resolve it: Demonstrating OFL-based LLM training or merging on benchmarks like LLaMA or similar, with evaluation on standard NLP tasks showing competitive performance against multi-round FL baselines.

### Open Question 3
- Question: Can adaptive ensemble methods be developed that dynamically adjust weights at the per-sample level rather than just per-client or per-model level?
- Basis in paper: [inferred] The paper notes that adaptive ensemble methods like FENS and Co-boosting "still struggle to adjust according to specific data samples. Due to data heterogeneity, the disparity between data samples across different clients can be significant, necessitating varied weight distributions for different samples."
- Why unresolved: Current adaptive methods (FENS, IntactOFL with MoE) adjust ensemble weights globally or per-client, but the extreme data heterogeneity in OFL scenarios suggests per-sample adaptation could improve accuracy.
- What evidence would resolve it: An adaptive ensemble OFL method that computes instance-specific weights and demonstrates statistically significant accuracy improvements over client-level adaptive methods on non-IID benchmarks.

### Open Question 4
- Question: What are the theoretical guarantees for privacy-accuracy trade-offs when combining generative models with ensemble methods in OFL?
- Basis in paper: [inferred] The paper discusses that hybrid methods combining generative models with ensembles (like FedOV, IntactOFL) show promise, and notes privacy concerns with parameter sharing, but lacks formal analysis of privacy guarantees for these combined approaches.
- Why unresolved: Hybrid methods introduce multiple attack surfaces (model inversion on generators, membership inference on ensembles), and the privacy implications of combining techniques are not well understood.
- What evidence would resolve it: Formal privacy analysis (differential privacy bounds or similar) for hybrid OFL methods, coupled with empirical privacy attack evaluations showing the trade-off between privacy budget and model accuracy.

## Limitations
- The survey's claims about method superiority rely primarily on internal comparisons within individual papers rather than direct head-to-head benchmarking across the full taxonomy.
- Key methodological details (hyperparameters, exact data partitioning, computational budgets) are not consistently specified across surveyed methods, creating barriers to faithful reproduction.
- The analysis of future directions is largely aspirational without quantitative projections of scalability challenges for large language models or data-free requirements.

## Confidence
- **High Confidence**: The taxonomy structure (four main categories) and identification of fundamental challenges (data heterogeneity, model heterogeneity) are well-supported by the FL literature and consistent across multiple sources.
- **Medium Confidence**: Claims about generative models outperforming knowledge distillation and adaptive ensemble methods yielding better results are based on paper-internal comparisons rather than comprehensive cross-method benchmarks.
- **Low Confidence**: Specific predictions about future scalability challenges and the effectiveness of integrating multiple techniques lack quantitative evidence or systematic evaluation.

## Next Checks
1. Implement a controlled benchmark comparing FedAvg, static ensemble, adaptive ensemble, and a generative-based approach (GAN + distillation) on CIFAR-10 with Dirichlet label skew to empirically validate the relative performance claims.
2. Conduct ablation studies on generative model architectures (GAN vs. VAE vs. diffusion) to measure the claimed superiority of generative approaches in capturing local data distributions.
3. Test the integration hypothesis by implementing a hybrid method combining prototype learning, generative model training, and adaptive ensemble aggregation, comparing against single-technique baselines to verify that "aggregating more techniques yields better results."