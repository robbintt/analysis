---
ver: rpa2
title: Analyzing Cost-Sensitive Surrogate Losses via $\mathcal{H}$-calibration
arxiv_id: '2502.19522'
source_url: https://arxiv.org/abs/2502.19522
tags:
- loss
- cost-sensitive
- cross-entropy
- embeddings
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes whether cost-sensitive surrogate losses or
  cost-agnostic ones (e.g., cross-entropy) should be used to train machine learning
  models for cost-sensitive classification tasks. Through the lens of H-calibration,
  the authors show that cost-sensitive surrogates can strictly outperform their cost-agnostic
  counterparts when learning small models under common distributional assumptions.
---

# Analyzing Cost-Sensitive Surrogate Losses via $\mathcal{H}$-calibration

## Quick Facts
- arXiv ID: 2502.19522
- Source URL: https://arxiv.org/abs/2502.19522
- Reference count: 40
- Key outcome: Cost-sensitive surrogates outperform cost-agnostic ones for small models under H-calibration

## Executive Summary
This paper investigates whether cost-sensitive surrogate losses should be used instead of cost-agnostic losses (like cross-entropy) for training models on cost-sensitive classification tasks. Through the lens of H-calibration, the authors prove that cost-sensitive surrogates from the Embeddings framework are H-consistent under P-minimizability assumptions, while standard cost-agnostic surrogates combined with post-processing are not H-consistent. Empirical results on three UCI datasets confirm that cost-sensitive surrogates consistently achieve lower cost-sensitive loss compared to cost-agnostic surrogates, even with post-processing.

## Method Summary
The study compares five loss functions across four datasets: Cross-Entropy (CE), CE with post-processing thresholding, Scaled CE, Embeddings loss, and Embeddings with softmax parameterization. Linear models are trained via gradient descent on 500-sample subsets from each dataset, with model selection on validation loss. The primary metric is Cost-Sensitive Loss (CSL), computed as the element-wise product of the confusion matrix and cost matrix. Experiments are repeated across 100 random seeds to report meanÂ±SEM.

## Key Results
- Embeddings loss is H-calibrated and H-consistent for cost-sensitive classification under P-minimizability
- Cross-entropy combined with thresholding is not H-consistent for cost-sensitive tasks with restricted hypothesis classes
- On three UCI datasets, cost-sensitive surrogates consistently outperform cost-agnostic surrogates in terms of CSL
- The performance gap narrows for larger model classes but remains significant for small models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Post-processing (thresholding) a model trained on cost-agnostic loss (e.g., cross-entropy) is insufficient for cost-sensitive tasks when the model class is restricted (e.g., linear models).
- **Mechanism:** In a linear model, optimizing a cost-agnostic loss yields a decision boundary with a specific orientation (slope). Post-processing via thresholding ($\tau$) shifts the bias (intercept) of this boundary but cannot rotate it. If the optimal cost-sensitive boundary requires a different slope than the optimal cost-agnostic boundary, thresholding cannot bridge the geometric gap.
- **Core assumption:** The optimal decision boundaries for the cost-agnostic task and the cost-sensitive task are not parallel (i.e., they require different model weights, not just different biases).
- **Evidence anchors:**
  - [Section 3]: "Because the optimal classifiers $h_{ag}(\cdot)$ and $h_{aw}(\cdot)$ have different slopes, thresholding can't recover the non-vertical slope of $h_{cs}$."
  - [Theorem 6]: Proves $(L_{ag}, \text{sign}(\cdot + \tau))$ is not $\mathcal{H}$-consistent for any threshold $\tau$.

### Mechanism 2
- **Claim:** Cost-sensitive "Embedding" losses guarantee that minimizing the surrogate loss corresponds to minimizing the target cost within a specific hypothesis class $\mathcal{H}$ ($\mathcal{H}$-consistency).
- **Mechanism:** The Embeddings framework maps the discrete cost matrix to a convex polyhedral surrogate loss. This construction ensures that the link function $\psi$ mapping surrogate predictions back to discrete decisions preserves optimality. Specifically, if a surrogate prediction is $\epsilon$-optimal, the linked decision is $\delta$-optimal for the target cost.
- **Core assumption:** The learning problem is $\mathcal{P}$-minimizable (the Bayes optimal classifier exists within the chosen hypothesis class $\mathcal{H}$).
- **Evidence anchors:**
  - [Theorem 9]: "Embeddings are $\mathcal{H}$-calibrated... [implies] a non-zero error in the target loss $\ell$ transfers to a non-zero error in the cost-sensitive Embeddings loss $L$."
  - [Corollary 10]: Confirms Embeddings are $\mathcal{H}$-consistent under $\mathcal{P}$-minimizability.

### Mechanism 3
- **Claim:** Standard proper scoring rules (like Cross-Entropy) fail to be $\mathcal{H}$-consistent for cost-sensitive targets when $\mathcal{H}$ is restricted because they target the conditional distribution rather than the cost-weighted decision.
- **Mechanism:** Proper scoring rules incentivize the model to predict $\Pr[Y|X]$. In a restricted class (e.g., linear), the model might approximate this probability well in a global sense (lowest entropy), but the resulting decision boundary may misclassify high-cost errors because the loss function is insensitive to the specific cost asymmetry during the gradient update phase.
- **Core assumption:** The model class $\mathcal{H}$ is not fully expressive (i.e., $\mathcal{H} \neq \mathcal{H}_{all}$), meaning it cannot perfectly fit $\Pr[Y|X]$ for all $x$, forcing a trade-off where cost-agnostic losses prioritize the "average" error over "high-cost" errors.
- **Evidence anchors:**
  - [Section 1]: Notes that while proper scoring rules imply $\mathcal{H}$-calibration, the implication fails when the model class does not contain the conditional distribution predictor.

## Foundational Learning

- **Concept: $\mathcal{H}$-Consistency**
  - **Why needed here:** This is the central theoretical lens of the paper. Unlike Bayes consistency (which assumes infinite data/model capacity), $\mathcal{H}$-consistency asks: "If I minimize this loss using *only* linear models, do I get the best possible linear model for my actual task?"
  - **Quick check question:** Does a low Cross-Entropy loss with a linear model guarantee the lowest possible False Negative Rate for that linear model class? (Answer: No, per this paper).

- **Concept: Surrogate Loss & Link Functions**
  - **Why needed here:** We cannot directly optimize discrete cost matrices (NP-hard). We need a continuous proxy (surrogate) to run gradient descent, and a rule (link function) to convert the continuous output back to a discrete decision.
  - **Quick check question:** If you train a model to output a score $s \in \mathbb{R}$, how do you map $s$ to "Class A" or "Class B"? (e.g., $\psi(s) = \text{sign}(s - \tau)$).

- **Concept: $\mathcal{P}$-Minimizability**
  - **Why needed here:** The theoretical guarantees rely on the assumption that the "perfect" model is actually within the set of models you are allowed to train. If you search for a straight line to separate data that requires a curve, consistency guarantees may break.
  - **Quick check question:** Is there a linear function that perfectly minimizes the expected cost on your validation data?

## Architecture Onboarding

- **Component map:** Input -> Linear Model -> Loss Layer (Embeddings or CE) -> Link Function (sign or weighted argmax) -> Output
- **Critical path:** The design of the Loss Layer. You must instantiate the **Embedding** loss using the specific cost matrix of your application.
- **Design tradeoffs:**
  - Cross-Entropy vs. Embeddings: Cross-entropy is smooth and well-behaved for optimization but ignores costs during training. Embeddings are cost-aware and theoretically sound for small models but are polyhedral (piecewise linear), which may behave differently during optimization (non-smooth gradients).
  - Post-processing: Thresholding is cheap but geometrically limited. Retraining with Embeddings is computationally more expensive but closes the performance gap.
- **Failure signatures:**
  - Symptom: Validation loss decreases, but target cost-sensitive loss (CSL) plateaus or increases.
  - Diagnosis: Likely using a cost-agnostic loss (CE) with a model that is too small to separate the cost boundaries purely via post-processing thresholding.
- **First 3 experiments:**
  1. **Baseline:** Train a linear model on Cross-Entropy, perform a grid search on thresholds $\tau$ on the validation set to minimize Cost-Sensitive Loss (CSL).
  2. **Intervention:** Train a linear model using the **Embeddings** loss constructed from the same cost matrix. Use the prescribed link function.
  3. **Comparison:** Compare the CSL of both approaches. If the Embedding model outperforms the "CE + Threshold Search" model, the paper's mechanism is verified for your dataset.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the $\mathcal{H}$-consistency guarantees for the Embeddings framework be extended to smooth cost-sensitive surrogates?
- **Open Question 2:** Is the assumption of $\mathcal{H}$-minimizability a necessary condition for a surrogate to be $\mathcal{H}$-consistent?
- **Open Question 3:** Why do cost-agnostic surrogates (Cross-Entropy) outperform the Embeddings surrogate on large-scale datasets like the Diabetes dataset?
- **Open Question 4:** What are the formal connections between the Embeddings framework and the predict-then-optimize literature?

## Limitations

- The theoretical analysis assumes P-minimizability, which may not hold for all datasets and model classes
- The practical impact of polyhedral Embedding losses on optimization stability is not thoroughly explored
- The claim that this effect is primarily significant for "small models" lacks systematic study across a spectrum of model complexities
- The specific preprocessing steps and hyperparameters for the empirical evaluation are not fully detailed

## Confidence

- **High Confidence:** The core theoretical framework of H-calibration and the formal proof that Embeddings are H-consistent under P-minimizability
- **Medium Confidence:** The empirical demonstration on three UCI datasets showing Embeddings outperform CE+thresholding
- **Low Confidence:** The claim that this effect is primarily significant for "small models" is supported by the examples but lacks systematic study

## Next Checks

1. **Geometric Verification:** For a chosen UCI dataset, plot the decision boundaries of models trained with CE and Embeddings. Visually confirm that the CE boundary cannot be rotated to match the Embeddings boundary through simple thresholding.

2. **Model Capacity Sweep:** Repeat the main experiment (Table 1) with progressively larger model classes (e.g., linear -> 1-hidden layer -> 2-hidden layer). Verify if the performance gap between Embeddings and CE+thresholding narrows as model capacity increases.

3. **Optimization Dynamics:** Profile the training process for both loss types. Compare the smoothness of the loss landscape, the variance of gradients, and the convergence rate to understand the practical trade-offs of using polyhedral Embedding losses.