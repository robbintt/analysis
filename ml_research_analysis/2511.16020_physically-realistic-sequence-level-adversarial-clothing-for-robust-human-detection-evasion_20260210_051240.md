---
ver: rpa2
title: Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection
  Evasion
arxiv_id: '2511.16020'
source_url: https://arxiv.org/abs/2511.16020
tags:
- physical
- adversarial
- across
- seqasr
- garment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a physically realistic, sequence-level adversarial
  clothing framework that generates printable adversarial textures for shirts, trousers,
  and hats to achieve robust human-detection evasion. The core innovation lies in
  optimizing garment textures at the sequence level using a physically-based pipeline
  that simulates motion, multi-viewpoint rendering, cloth dynamics, and illumination
  variation, rather than frame-by-frame optimization.
---

# Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion

## Quick Facts
- **arXiv ID**: 2511.16020
- **Source URL**: https://arxiv.org/abs/2511.16020
- **Reference count**: 40
- **Primary result**: Achieves 94.7% SeqASR on YOLOv3 with physically realizable adversarial clothing

## Executive Summary
This paper presents a novel framework for generating adversarial clothing that effectively evades human detection in video sequences. The key innovation lies in optimizing garment textures at the sequence level rather than frame-by-frame, incorporating physically-based simulation of cloth dynamics, motion, and illumination variation. The method uses a compact parameterization based on dual-domain K-means clustering and ICC color locking to ensure the generated textures are printer-safe and physically realizable. Extensive experiments demonstrate superior performance compared to state-of-the-art methods, with strong results in both digital simulations and physical garment testing using sublimation printing.

## Method Summary
The framework optimizes adversarial textures through a sequence-level approach that simulates realistic garment behavior including motion, multi-viewpoint rendering, cloth dynamics, and illumination variation. A dual-domain K-means clustering with ICC color locking creates compact, printer-safe texture representations. The method employs expectation-over-transformation with temporal weighting to ensure consistent suppression across video sequences while maintaining physical plausibility through realistic garment simulation. Control-point representation enables efficient, differentiable texture generation suitable for adversarial optimization.

## Key Results
- Sequence-level attack success rate (SeqASR) of 94.7% on YOLOv3 in digital settings
- Conditional value-at-risk (CVaR) of 22.0 and non-detection rate (NDR) of 73.6%
- Physical garments retain strong performance with SeqASR of 86.2% after sublimation printing
- High cross-model transferability and stability across diverse camera elevations and garment materials

## Why This Works (Mechanism)
The approach succeeds by addressing the fundamental limitations of frame-by-frame adversarial attacks on clothing. By optimizing at the sequence level, the method accounts for temporal consistency and physical dynamics that single-frame approaches cannot capture. The physically-based simulation pipeline ensures that generated patterns remain effective under real-world conditions including cloth deformation, lighting changes, and viewpoint variation. The compact parameterization using dual-domain K-means and ICC color locking bridges the gap between digital optimization and physical realizability, ensuring generated patterns can be accurately reproduced through printing while maintaining their adversarial properties.

## Foundational Learning

**Expectation-over-transformation (EOT)**: A technique that optimizes adversarial examples against a distribution of transformations rather than fixed inputs. Needed because physical-world attacks must account for variability in real conditions. Quick check: Verify that optimization accounts for multiple simulated viewpoints and lighting conditions simultaneously.

**Physically-based rendering**: Simulation of light interaction with materials to create realistic images. Needed to ensure generated patterns appear natural and maintain adversarial properties under varying illumination. Quick check: Confirm rendering pipeline accurately models cloth reflectance properties and subsurface scattering.

**Differentiable cloth simulation**: Methods that make cloth physics simulations differentiable for integration with gradient-based optimization. Needed to enable end-to-end training where texture changes affect garment deformation. Quick check: Verify gradient flow through the cloth simulation module without numerical instability.

**Dual-domain K-means clustering**: A texture compression technique that operates in both spatial and frequency domains. Needed to create compact representations suitable for printing while preserving adversarial features. Quick check: Ensure compressed textures retain sufficient detail for effective adversarial perturbations.

**ICC color management**: International Color Consortium profiles for consistent color reproduction across devices. Needed to ensure printed patterns match digital optimization targets. Quick check: Validate color accuracy between digital patterns and physical prints using standardized color charts.

## Architecture Onboarding

**Component map**: Motion simulator -> Multi-view renderer -> Cloth dynamics engine -> Illumination variation module -> Texture optimization -> Dual-domain K-means compression -> ICC color locking

**Critical path**: The optimization pipeline must maintain differentiability from texture parameters through all simulation stages to enable gradient-based learning. The most critical components are the differentiable cloth simulation and multi-view rendering, as errors in these propagate through the entire optimization process.

**Design tradeoffs**: The method trades computational complexity for physical realism, requiring expensive simulations but producing more robust attacks. The compact parameterization reduces attack strength slightly compared to unconstrained optimization but ensures physical realizability. Temporal weighting in EOT increases training time but significantly improves sequence-level consistency.

**Failure signatures**: Patterns that work well in simulation but fail physically typically have fine details that don't survive the printing process. Temporal inconsistency manifests as frames where the subject becomes briefly visible despite overall attack success. Illumination-dependent failures occur when patterns rely on specific lighting conditions that don't hold in deployment.

**First experiments**:
1. Validate gradient flow through cloth simulation by comparing analytical and numerical gradients
2. Test pattern compression fidelity by comparing detection evasion rates before and after dual-domain K-means
3. Evaluate physical print accuracy by measuring color deviation between digital targets and printed samples

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but implicit areas for future work include extending the approach to multi-person scenarios, evaluating performance against more diverse object detection architectures including transformer-based models, and testing under more extreme environmental conditions beyond those explored in the current study.

## Limitations

- Evaluation limited to YOLOv3 architecture, though some transferability testing performed
- Physical experiments conducted under controlled conditions with specific camera placements
- Generalizability to uncontrolled real-world scenarios with varying environmental conditions remains to be validated
- Computational cost of sequence-level optimization may limit real-time applications

## Confidence

**High confidence in**: The technical implementation of the sequence-level optimization framework and its core components (motion simulation, multi-view rendering, cloth dynamics). The reported performance metrics in digital settings appear robust and well-documented.

**Medium confidence in**: The physical realizability claims, as the experiments were conducted under controlled conditions. The cross-model transferability results, given the limited diversity of tested models.

**Low confidence in**: Long-term robustness claims and performance under extreme environmental variations not tested in the paper.

## Next Checks

1. Test the adversarial clothing performance under diverse weather conditions (rain, fog, varying temperatures) and lighting scenarios to assess real-world robustness.

2. Evaluate the transferability and effectiveness against a broader range of object detection architectures beyond YOLOv3, including transformer-based detectors and one-stage versus two-stage architectures.

3. Conduct extended field tests with varying observer perspectives and distances, including scenarios where the subject is partially occluded or in motion relative to multiple observers simultaneously.