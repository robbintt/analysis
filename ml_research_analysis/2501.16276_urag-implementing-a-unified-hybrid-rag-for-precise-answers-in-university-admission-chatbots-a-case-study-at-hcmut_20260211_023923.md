---
ver: rpa2
title: 'URAG: Implementing a Unified Hybrid RAG for Precise Answers in University
  Admission Chatbots -- A Case Study at HCMUT'
arxiv_id: '2501.16276'
source_url: https://arxiv.org/abs/2501.16276
tags:
- urag
- university
- admission
- chatbots
- tier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents URAG, a hybrid framework for improving university
  admission chatbots by integrating rule-based and RAG approaches. URAG uses a two-tier
  architecture where Tier 1 handles common questions through an enriched FAQ system,
  while Tier 2 retrieves and generates responses from augmented documents.
---

# URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT

## Quick Facts
- arXiv ID: 2501.16276
- Source URL: https://arxiv.org/abs/2501.16276
- Reference count: 40
- Primary result: Achieved 62.8% accuracy, outperforming GPT-4o (54.4%) and Claude 3.5 Sonnet (50.2%)

## Executive Summary
URAG presents a hybrid two-tier RAG framework for university admission chatbots that combines rule-based FAQ retrieval with document-based generation. The system uses a lightweight Vietnamese LLM (ura-hcmut/ura-llama-7b) and achieves 62.8% accuracy on 500 evaluation questions, outperforming commercial models. The architecture prioritizes verified FAQ answers through a high similarity threshold (tFAQ=0.9) before falling back to document retrieval and generation, with a disclaimer for unhandled cases.

## Method Summary
URAG implements a two-tier retrieval architecture where Tier 1 matches queries against an enriched FAQ database using cosine similarity (tFAQ=0.9), while Tier 2 retrieves and generates responses from augmented documents (tDoc=0.8). The framework includes URAG-D for semantic chunking with context-aware rewriting and URAG-F for FAQ enrichment through paraphrase generation. A lightweight 7B-parameter Vietnamese LLM serves as the generator, with evaluation showing superior performance to commercial models on factual university admission queries.

## Key Results
- URAG achieved 62.8% accuracy on 500 evaluation questions
- Outperformed GPT-4o (54.4%) and Claude 3.5 Sonnet (50.2%) on the same dataset
- Balanced distribution between Tier 1 and Tier 2 responses with minimal fallback cases
- Successfully handled nearly all queries through the first two tiers

## Why This Works (Mechanism)

### Mechanism 1: Two-Tier Retrieval Hierarchy
A cascading FAQ-then-document retrieval architecture improves accuracy for high-stakes queries by prioritizing verified Q-A pairs over generated responses. Tier 1 matches queries against an enriched FAQ database using cosine similarity threshold (tFAQ=0.9). Only if no match exceeds threshold does the system proceed to Tier 2, which retrieves document chunks and generates responses via LLM. A fallback tier with disclaimer handles remaining cases.

### Mechanism 2: URAG-D Semantic Chunking with Context-Aware Rewriting
Document retrieval improves when chunks are semantically segmented and rewritten with global document context rather than using fixed-size splits. The approach extracts the general context from original documents, guiding the rewriting process of each chunk to maintain logical coherence. Semantic chunking adaptively determines chunk boundaries between sentences based on embedding similarity, with succinct summaries prepended to each chunk.

### Mechanism 3: URAG-F FAQ Enrichment via Paraphrase Generation
Expanding FAQ databases through LLM-generated paraphrases improves Tier 1 recall, particularly for linguistically diverse languages like Vietnamese. The approach generates new Q-A pairs from augmented document corpus, paraphrases each Q-A pair into multiple variations, and tags paraphrases to link back to root Q-A pair. Only question embeddings are indexed for retrieval.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: URAG is fundamentally a RAG variant; understanding the baseline retriever-generator pipeline is prerequisite to understanding Tier 2 and the augmentation mechanisms.
  - Quick check: Can you explain why naive RAG can still produce hallucinations despite retrieving documents?

- **Concept: Embedding Similarity Search with Thresholds**
  - Why needed: Both tiers rely on cosine similarity against thresholds (tFAQ=0.9, tDoc=0.8) to decide whether to accept a match or cascade to the next tier.
  - Quick check: What happens to system behavior if tFAQ is set too low vs. too high?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed: URAG-D and URAG-F both use LLMs with CoT prompting during the preparation phase to generate reliable chunk rewrites and FAQ paraphrases.
  - Quick check: Why might CoT be more critical during offline preparation than during online inference?

## Architecture Onboarding

- **Component map:** Preparation Phase: URAG-D (Semantic Chunker → Context Extractor → Chunk Rewriter → Summarizer) produces augmented document corpus D; URAG-F (Q-A Generator → Paraphraser) produces enriched FAQ F → Inference Phase: Query → Embedding Model (vietnamese-embedding) → Tier 1 (FAQ Index search, tFAQ=0.9, k=20) → [if no match] Tier 2 (Document Index search, tDoc=0.8, K=2) → Prompt Template → LLM Generator (ura-llama-7b) → Response (with disclaimer if fallback) → Storage: Single Vector Database with separate FAQ Index and Document Index

- **Critical path:** Query embedding → Tier 1 similarity search → (miss) → Tier 2 similarity search → (miss) → Fallback generation. Accuracy degrades along this path; slight decline in accuracy as the system progressed through the tiers.

- **Design tradeoffs:** Lightweight LLM (7B params) vs. commercial models: Lower cost and data privacy, but weaker reasoning. High tFAQ (0.9) vs. lower threshold: Prioritizes precision for critical FAQ matches but may cascade more queries to Tier 2. Paraphrase volume (k=20): Higher recall but increased index size and potential noise.

- **Failure signatures:** High fallback rate: Indicates both FAQ and document corpus lack coverage for query distribution. Correct FAQ match but wrong answer: Likely paraphrase semantic drift. Tier 2 responses with hallucinations: Lightweight LLM lacks reasoning capability for complex queries; check if retrieved documents actually contain relevant information. Latency spikes: Tier 2 requires LLM generation; monitor for complex queries requiring max_new_tokens.

- **First 3 experiments:** 1) Ablation by tier: Disable Tier 1 (set tFAQ=1.0) and measure accuracy drop to quantify FAQ contribution vs. document retrieval. 2) Threshold sensitivity: Vary tFAQ from 0.80 to 0.95 and plot accuracy vs. Tier 1/Tier 2 distribution to find optimal balance for your query distribution. 3) Chunking comparison: Replace semantic chunking with fixed-size chunking in URAG-D and measure retrieval precision (MRR) on held-out queries to validate the claimed improvement.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can integrating State-of-the-Art Hybrid Search techniques significantly improve retrieval efficiency over the current Cosine Similarity method without incurring prohibitive operational costs? The authors state in Section 5 that future research should focus on advanced retrieval strategies, such as integrating SOTA Hybrid Search techniques to replace the current Cosine Similarity with Threshold method.

- **Open Question 2:** To what extent does fine-tuning the lightweight generator LLM on domain-specific admission datasets enhance accuracy for Reasoning-type queries compared to the baseline? Section 5 suggests fine-tuning the LLM on domain-specific datasets related to university admissions could further enhance the relevance and accuracy of responses.

- **Open Question 3:** How robust is the URAG framework against error propagation when the LLM used in the URAG-D and URAG-F mechanisms produces hallucinations during the document preparation phase? The methodology relies on an LLM to rewrite chunks and generate FAQ pairs, but the evaluation focuses on inference-time accuracy while assuming the "Augmented Documents" and "Enriched FAQ" created during the preparation phase are error-free.

## Limitations

- Framework tested exclusively on Vietnamese university admission queries using a single institutional dataset
- Lightweight 7B-parameter LLM may not generalize to domains requiring complex reasoning
- Semantic chunking and paraphrase mechanisms lack external validation across different languages or domains

## Confidence

- **High Confidence:** The two-tier architecture's core principle (FAQ-first, then document retrieval) is well-supported by empirical results showing 62.8% accuracy and balanced distribution between tiers.
- **Medium Confidence:** The effectiveness of URAG-D's semantic chunking with context rewriting and URAG-F's paraphrase expansion is inferred from performance improvements but lacks direct ablation studies or external validation.
- **Low Confidence:** The generalizability of the framework to other domains, languages, or institutional contexts remains speculative.

## Next Checks

1. **Ablation Study on Mechanisms:** Conduct controlled experiments disabling URAG-D and URAG-F individually to quantify their marginal contributions to accuracy, validating whether semantic chunking and paraphrase expansion are essential rather than incidental to performance gains.

2. **Cross-Domain Generalization:** Deploy the URAG framework on a different institutional domain (e.g., healthcare patient FAQs or technical support) using the same architecture but new corpora, testing whether the two-tier approach and lightweight LLM configuration transfer beyond university admissions.

3. **Robustness Testing:** Systematically generate adversarial queries that are linguistically similar to FAQ entries but semantically unrelated, and measure false positive rates to validate whether the high threshold (tFAQ=0.9) effectively prevents incorrect FAQ matches while maintaining coverage.