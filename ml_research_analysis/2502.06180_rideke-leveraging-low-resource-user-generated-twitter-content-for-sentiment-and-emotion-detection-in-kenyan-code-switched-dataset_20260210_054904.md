---
ver: rpa2
title: 'RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment
  and Emotion Detection in Kenyan Code-Switched Dataset'
arxiv_id: '2502.06180'
source_url: https://arxiv.org/abs/2502.06180
tags:
- sentiment
- supervised
- emotion
- semi-supervised
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RideKE, a sentiment and emotion analysis
  dataset for Kenyan-accented English code-switched with Swahili and Sheng. The dataset
  contains over 29,000 tweets from ride-hailing services like Uber, Bolt, and Little
  Cab, with sentiment labels (positive, negative, neutral) and emotion labels (frustration,
  happy, angry, sad, empathy, fear, love, surprise).
---

# RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset

## Quick Facts
- **arXiv ID**: 2502.06180
- **Source URL**: https://arxiv.org/abs/2502.06180
- **Reference count**: 40
- **Primary result**: XLM-R achieved 69.2% accuracy and 66.1% F1 for sentiment analysis; DistilBERT led emotion detection with 59.8% accuracy and 31% F1.

## Executive Summary
RideKE introduces a novel dataset for sentiment and emotion analysis in Kenyan-accented English code-switched with Swahili and Sheng, collected from over 29,000 tweets related to ride-hailing services (Uber, Bolt, Little Cab). The dataset is annotated for sentiment (positive, negative, neutral) and eight emotion categories (frustration, happy, angry, sad, empathy, fear, love, surprise). Four transformer-based models—DistilBERT, mBERT, XLM-R, and AfriBERTa—were evaluated using both supervised and semi-supervised learning. XLM-R performed best on sentiment, while DistilBERT excelled at emotion detection. All models showed a strong bias toward predicting neutral sentiment, highlighting class imbalance challenges in low-resource code-switched settings.

## Method Summary
The RideKE dataset was constructed from tweets collected via snscrape (Jan 2017–Apr 2023) using ride-hailing service hashtags. Tweets were split into three sets: Set A (1,189 labeled: 553 human + 636 ChatGPT), Set B (2,000 human-labeled for testing), and Set C (27,090 unlabeled for semi-supervised learning). Models were fine-tuned using supervised learning (10 epochs, lr=1e-5, Adam optimizer) and semi-supervised learning (4 epochs, pseudo-labeling at 75th percentile confidence threshold). Preprocessing included lowercasing, contraction expansion, repeated character reduction, and punctuation removal. Combined cross-entropy loss was used for both sentiment and emotion tasks.

## Key Results
- XLM-R-base achieved the highest sentiment classification accuracy (69.2%) and F1 score (66.1%) under supervised learning.
- DistilBERT led emotion detection with accuracy 59.8% and F1 31%, but all models exhibited strong bias toward predicting neutral sentiment.
- AfriBERTa, despite being an Africa-centric model, showed the lowest performance, underperforming general multilingual models like XLM-R.
- Semi-supervised learning improved sentiment F1 for XLM-R and mBERT but degraded performance for DistilBERT, highlighting mixed outcomes across architectures.

## Why This Works (Mechanism)
The approach leverages low-resource, user-generated code-switched text from ride-hailing contexts, using fine-tuned transformer models to detect sentiment and emotion. Pseudo-labeling in semi-supervised learning helps expand training data, but model-specific thresholds and noisy pseudo-labels limit effectiveness. The strong neutral bias reflects class imbalance and challenges in distinguishing subtle sentiment and emotion cues in code-switched language.

## Foundational Learning
- **Code-switching in NLP**: Mixing languages (e.g., English, Swahili, Sheng) increases lexical and syntactic complexity; quick check: verify model tokenization supports all three languages.
- **Pseudo-labeling in SSL**: Confidence-based pseudo-labeling boosts training data but risks propagating errors; quick check: inspect pseudo-label distribution for class balance.
- **Sentiment and emotion classification**: Multi-task learning requires balancing objectives; quick check: ensure equal weighting in combined loss function.

## Architecture Onboarding
- **Component map**: Data collection (Twitter) -> Preprocessing (cleaning, tokenizing) -> Supervised training (10 epochs) -> Semi-supervised refinement (4 epochs, pseudo-labeling) -> Evaluation (Set B).
- **Critical path**: Fine-tune transformer on Set A, generate pseudo-labels on Set C, retrain on combined data, evaluate on Set B.
- **Design tradeoffs**: General multilingual models (XLM-R) outperform Africa-specific ones (AfriBERTa) on code-switched data, suggesting domain relevance is less critical than broad multilingual pretraining.
- **Failure signatures**: Neutral sentiment bias indicates class imbalance; SSL may degrade performance if pseudo-labels are noisy.
- **First experiments**:
  1. Re-run supervised fine-tuning on Set A, evaluate on Set B, confirm sentiment accuracy ~69%.
  2. Generate pseudo-labels on Set C using 75th percentile threshold, retrain, and compare F1 scores.
  3. Apply class weighting to mitigate neutral bias, measure impact on emotion F1.

## Open Questions the Paper Calls Out
- **Few-shot learning potential**: Can few-shot techniques outperform the current fine-tuning approach? The authors aim to explore this but did not benchmark it; evidence would require F1 comparisons against the XLM-R baseline.
- **AfriBERTa underperformance**: Why does the Africa-centric AfriBERTa underperform on Kenyan code-switched data? The authors suggest further analysis of attention mechanisms or domain-specific re-training is needed.
- **Alternative SSL strategies**: Can other semi-supervised methods (e.g., consistency regularization, contrastive learning) overcome the mixed outcomes of current pseudo-labeling? The authors propose testing these for consistent improvements.

## Limitations
- Strong neutral sentiment bias across all models, suggesting class imbalance is not adequately addressed.
- Semi-supervised learning yields mixed outcomes, improving some models but degrading others.
- AfriBERTa's underperformance raises questions about the effectiveness of Africa-centric pretraining for code-switched dialects.

## Confidence
- **High Confidence**: Dataset creation process, data sources, and experimental setup are clearly specified and reproducible.
- **Medium Confidence**: Semi-supervised protocol is described, but implementation details and pseudo-label robustness are not fully transparent.
- **Low Confidence**: Missing validation strategy for model selection and absence of explicit class weighting limit trust in reported metrics.

## Next Checks
1. Implement class weighting or oversampling to mitigate neutral sentiment bias; compare resulting sentiment and emotion F1 scores to baseline.
2. Vary the pseudo-labeling confidence threshold (e.g., 60th, 75th, 85th percentile) in semi-supervised learning; measure impact on sentiment and emotion detection.
3. Cross-validate model selection by reporting performance on both the validation split (from Set A) and the held-out Set B; analyze whether early stopping or hyperparameter tuning significantly alters reported results.