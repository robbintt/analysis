---
ver: rpa2
title: Continual Knowledge Adaptation for Reinforcement Learning
arxiv_id: '2510.19314'
source_url: https://arxiv.org/abs/2510.19314
tags:
- uni00000013
- uni00000011
- knowledge
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Continual Knowledge Adaptation for Reinforcement
  Learning (CKA-RL), a method designed to address catastrophic forgetting and improve
  knowledge transfer in continual reinforcement learning. The approach introduces
  task-specific knowledge vectors that capture adaptations for each learned task,
  combined with a dynamic adaptation strategy that leverages historical knowledge
  when learning new tasks.
---

# Continual Knowledge Adaptation for Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.19314
- Source URL: https://arxiv.org/abs/2510.19314
- Reference count: 40
- Primary result: Outperforms state-of-the-art methods with 4.20% overall performance improvement and 8.02% forward transfer boost

## Executive Summary
This paper introduces Continual Knowledge Adaptation for Reinforcement Learning (CKA-RL), a method designed to address catastrophic forgetting and improve knowledge transfer in continual reinforcement learning scenarios. The approach introduces task-specific knowledge vectors that capture adaptations for each learned task, combined with a dynamic adaptation strategy that leverages historical knowledge when learning new tasks. To manage scalability, an Adaptive Knowledge Merging mechanism merges similar knowledge vectors when the pool exceeds a maximum size. Experiments on three benchmarks (Meta-World, SpaceInvaders, and Freeway) demonstrate that CKA-RL outperforms state-of-the-art methods while maintaining constant memory usage and inference latency regardless of the number of tasks.

## Method Summary
CKA-RL decomposes policy parameters into a frozen base model (θ_base) and task-specific knowledge vectors (v_k), minimizing cross-task interference through orthogonal parameter updates. When learning a new task, only the knowledge vector and learnable adaptation coefficients are updated while keeping the base model static. The method employs dynamic knowledge adaptation using softmax-weighted historical vectors to accelerate learning on new tasks. To maintain bounded memory growth, CKA-RL implements similarity-based memory compression that merges high-similarity vectors when exceeding a maximum pool size. The approach is evaluated using SAC for Meta-World and PPO for Atari environments, with knowledge vectors initialized to zero and merged based on cosine similarity when the pool exceeds predefined limits.

## Key Results
- Achieves 4.20% improvement in overall performance compared to state-of-the-art methods
- Demonstrates 8.02% boost in forward transfer efficiency
- Maintains constant memory usage and inference latency regardless of task count through adaptive knowledge merging

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Task Decomposition
- Decomposes policy into frozen base model (θ_base) and task-specific knowledge vectors (v_k)
- Isolates parameter updates by training only the vector for new tasks while keeping base static
- Evidence: Knowledge vectors exhibit near-orthogonal structure (cosine similarity ∈ [-0.24, 0.12])
- Core assumption: Task adaptations can be represented as low-rank perturbations in orthogonal subspaces
- Break condition: If tasks require modifying shared core features in θ_base, performance will plateau

### Mechanism 2: Dynamic Knowledge Adaptation (Soft Gating)
- Dynamically weights historical vectors using learnable parameters (β_k) passed through softmax
- Allows selective transfer of relevant skills to new tasks through composition of vector pool
- Evidence: Shows higher initial rewards and faster convergence compared to baselines
- Core assumption: New tasks share structural similarities with subsets of previous tasks
- Break condition: If sequential tasks are random or adversarial, adaptation might introduce noise

### Mechanism 3: Similarity-Based Memory Compression
- Merges high-similarity vectors to maintain bounded memory growth with bounded performance degradation
- Uses cosine similarity to identify most similar vector pairs for averaging
- Evidence: Memory usage plateaus after task 5 while performance remains stable
- Core assumption: High cosine similarity indicates functional redundancy rather than destructive interference
- Break condition: If distinct tasks produce similar vectors, merging could erase critical distinct skills

## Foundational Learning

- **Catastrophic Forgetting vs. Plasticity**
  - Why needed: Core problem CKA-RL solves is retaining old skills while learning new ones
  - Quick check: Why does freezing θ_base prevent catastrophic forgetting, and what is the trade-off?

- **Parameter-Efficient Fine-Tuning (PEFT) / Task Arithmetic**
  - Why needed: Paper leverages concept that model edits can be treated as vectors
  - Quick check: How does θ_k = θ_base + v_k mathematically alter the policy's output distribution?

- **Cosine Similarity in Weight Space**
  - Why needed: Memory management system relies entirely on measuring alignment between vectors
  - Quick check: If two knowledge vectors have cosine similarity of -1, what would merging them theoretically achieve?

## Architecture Onboarding

- **Component map:**
  1. Base Network (θ_base): Shared, frozen backbone (trained on Task 1)
  2. Knowledge Vector Pool (V): Dynamic list of low-rank tensors (v_1, ..., v_k)
  3. Adapter Head (α, v_current): Lightweight learnable parameters generating active policy

- **Critical path:**
  1. Initialize: Train θ_1 on Task 1 → set θ_base = θ_1
  2. Loop (Task k):
     - Construct: θ_k = θ_base + Σ α_j v_j + v_k
     - Train: Update α (gating) and v_k (new skill) via RL (PPO/SAC). Freeze θ_base
     - Store: Add v_k to pool V
     - Merge: If |V| > K_max, find max(S_ij), average v_i, v_j

- **Design tradeoffs:**
  - K_max Value: Lower saves memory but forces aggressive merging, risking skill loss
  - Initialization: v_k initialized to 0 to ensure starting from "adapted historical average"

- **Failure signatures:**
  - Stagnant Rewards: Poor θ_base and zero-initialized vectors may struggle to bootstrap
  - Runaway Memory: Flawed similarity metric could cause destructive merges
  - Negative Transfer: α_k concentrating on conflicting vectors degrades performance

- **First 3 experiments:**
  1. Orthogonality Verification: Train on 2 conflicting tasks, plot cosine similarity of v_1 and v_2
  2. Ablation on Initialization: Compare v_k ~ N(0,1) vs. v_k = 0 to validate zero initialization claim
  3. Stress Test Merging: Run 20 tasks with K_max=3, monitor performance on Task 1 after Task 20

## Open Questions the Paper Calls Out

- **Complex-Environment Evaluation**: Can CKA-RL maintain efficiency and robustness in complex, real-world environments like outdoor vision-language navigation? (Appendix G.1 explicitly lists this as a limitation)
- **Large-Scale Architecture Generalization**: Is CKA-RL effective and stable when applied to large-scale architectures during RLHF phase of LLM training? (Appendix G.1 identifies this as future direction)
- **Adaptive Pool Size**: Can the Adaptive Knowledge Merging mechanism be modified to dynamically determine optimal pool size rather than relying on fixed hyperparameter? (Section 5.3 shows sensitivity to manually selected K_max)

## Limitations
- Only validated on small-scale networks, effectiveness on large-scale architectures (like LLMs) remains unverified
- Assumes sequential tasks share structural similarities, which may not hold for random or adversarial task distributions
- Relies on cosine similarity as proxy for functional redundancy, which may not capture true relationship between task adaptations

## Confidence
- Mechanism 1 (Orthogonal decomposition): Medium - theoretical motivation sound but empirical validation limited to cosine metrics
- Mechanism 2 (Dynamic adaptation): Medium - demonstrated effectiveness on benchmarks but untested on non-i.i.d. task distributions
- Mechanism 3 (Memory compression): Medium - theoretical error bounds exist but practical impact not thoroughly characterized
- Overall performance claims: High - consistent improvements across three distinct benchmarks with appropriate statistical comparisons

## Next Checks
1. **Task Distribution Sensitivity**: Test on curriculum from conflicting to complementary tasks to measure forward transfer under interference
2. **Vector Pool Size Stress Test**: Systematically vary K_max from 2 to 20 on Meta-World to identify merging-induced forgetting point
3. **Alternative Similarity Metrics**: Replace cosine similarity with reward correlation or behavioral cloning divergence to compare knowledge retention approaches