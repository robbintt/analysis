---
ver: rpa2
title: 'Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs'
arxiv_id: '2510.08596'
source_url: https://arxiv.org/abs/2510.08596
tags:
- confidence
- score
- metrics
- metric
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of bias in traditional reference-free
  metrics like self-perplexity, which favor safe, predictable text and are poorly
  suited for evaluating creative or novel outputs from modern LLMs. To address this,
  the author proposes the Confidence Score (CS), a metric that analyzes the shape
  of the model's output probability distribution to quantify its certainty, rather
  than just the probability of the chosen token.
---

# Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs

## Quick Facts
- arXiv ID: 2510.08596
- Source URL: https://arxiv.org/abs/2510.08596
- Reference count: 4
- The Confidence Score (CS) mitigates bias in traditional metrics against creative LLM outputs while maintaining ability to discriminate task difficulty.

## Executive Summary
Traditional reference-free metrics like self-perplexity penalize low-probability tokens regardless of certainty, leading to bias against creative or novel outputs from modern LLMs. This paper introduces the Confidence Score (CS), which evaluates model certainty by analyzing the shape of the output probability distribution rather than just the probability of the chosen token. CS multiplies the probability of the selected token by the standard deviation of the top-N probabilities, rewarding both high likelihood and clear decisiveness. Experiments demonstrate that while traditional metrics prefer safe responses 100% of the time on creative prompts, CS does so only 19% of the time, showing statistically significant improvement in creativity evaluation.

## Method Summary
The method extracts top-10 log probabilities per token from the LLM API, converts them to probabilities, and computes a token-level confidence score as P(chosen token) × σ(top-N probabilities), with N=3 and population standard deviation. Two aggregation methods are used: Average CS (mean across tokens) and Worst-Case CS (minimum). The metric is evaluated on four prompt sets: 100 creative prompts, and 30 each for easy, medium, and hard tasks. Experiments compare CS against self-perplexity and fluency score, measuring preference rates for high-temperature responses and ability to discriminate task difficulty through bootstrapped 95% confidence intervals.

## Key Results
- CS prefers novel responses in 19% of cases on creative prompts, versus 0% for traditional metrics
- CS effectively discriminates between easy, medium, and hard tasks with non-overlapping 95% confidence intervals
- Worst-Case CS acts as a "weakest link" detector, particularly effective at flagging potential factual errors
- Statistical significance confirmed via Wilcoxon signed-rank test (p<0.001) and bootstrapped intervals

## Why This Works (Mechanism)

### Mechanism 1
Analyzing the shape of the output probability distribution captures model certainty better than the probability of the chosen token alone. The Confidence Score (CS) multiplies two components for each generated token: the probability of the chosen token (P_chosen) and the standard deviation of the top-N probabilities (σ_topn). A high score requires both high likelihood for the selected token and a large gap between it and alternatives. This works because a model is truly confident when its top choice is a clear outlier relative to near competitors, not just when it has moderately high probability.

### Mechanism 2
Aggregating token-level confidence via average and worst-case methods provides complementary signals for sequence-level quality. Average CS captures holistic confidence across the full sequence, while Worst-Case CS takes the minimum token-level score, acting as a "weakest link" detector for localized uncertainty that may indicate errors or inconsistencies. This works because hallucinations and logical failures manifest as localized drops in model confidence, making the worst-case metric effective at flagging potential factual errors.

### Mechanism 3
CS reduces bias against high-temperature creative outputs while retaining ability to discriminate task difficulty. Perplexity penalizes lower-probability tokens regardless of decisiveness; CS rewards decisiveness even at moderate probabilities. This allows creative (higher-temperature) responses to score well when the model is confident in its choices, distinguishing between genuinely uncertain creative output and creative output generated with high internal certainty.

## Foundational Learning

- **Perplexity and self-perplexity as reference-free evaluation metrics**: Understanding what perplexity measures (exponentiated average negative log-probability) is prerequisite to grasping why it biases toward safe, high-frequency outputs. Quick check: Why would a model generating "the the the" have low perplexity but poor quality?

- **Probability distributions over vocabulary at each decoding step**: CS operates on the shape of these distributions (top-N probabilities and their spread), not just the argmax. Quick check: If top-3 probabilities are [0.5, 0.3, 0.2] vs [0.5, 0.25, 0.25], which has higher standard deviation and what does that imply about confidence?

- **Statistical significance via confidence intervals and bootstrapping**: The paper's claims rest on non-overlapping 95% CIs and bootstrapped difference intervals; readers must understand what this implies about evidence strength. Quick check: If two conditions have 95% CIs of [0.33, 0.45] and [0.41, 0.52], can you claim a significant difference?

## Architecture Onboarding

- **Component map**: Log-probability extraction -> Token-level CS computation -> Sequence aggregation -> Comparison layer

- **Critical path**: Accurate log-probability extraction → correct top-N selection → proper σ calculation (population, not sample) → consistent aggregation. Errors in logprob conversion or σ computation propagate directly to invalid scores.

- **Design tradeoffs**: n=3 captures local competition vs larger n dilutes signal with low-probability tail; Average CS smooths over outliers vs Worst-Case CS flags single bad tokens but is noisier; Single-generation CS is cheaper than multi-sample methods but provides no variance estimate.

- **Failure signatures**: CS uniformly high (check if σ computation uses sample instead of population SD); CS fails to distinguish task difficulty (verify prompt categorization and logprob variation); CS shows no creativity preference (ensure temperature is actually varying and high-temp outputs aren't uniformly uncertain).

- **First 3 experiments**: 1) Reproduce Table 1 on a small held-out set of creative prompts to validate pipeline correctness; 2) Ablate n ∈ {2, 3, 5, 10} to test sensitivity of creativity preference rate to top-N selection; 3) Compare CS against Self-Check GPT on a hallucination detection task to test whether Worst-Case CS correlates with multi-sample consistency.

## Open Questions the Paper Calls Out
- **Correlation with human judgments**: The paper identifies the lack of "large-scale correlation study between CS and direct human judgments" as a key limitation. This remains unresolved because current experiments use temperature settings as a proxy for creativity rather than measuring actual human perception of novelty or quality.

- **Cross-model consistency**: The analysis relies on a single model (gpt-4o-mini), and generalizability to different LLM architectures and sizes is part of "ongoing work." This is unresolved because different architectures may exhibit different probability distribution shapes, impacting the reliability of using standard deviation as a proxy for certainty.

- **Integration into decoding algorithms**: While CS has been shown to distinguish task difficulty post-hoc, it is unknown if it can function as an effective steering mechanism during generation to prevent low-confidence output. This remains unresolved because the paper states authors are "exploring applications beyond evaluation" by integrating CS into decoding algorithms.

## Limitations
- **Limited validation scope**: The CS metric shows promise for creative and difficulty-discriminative evaluation, but its robustness across diverse domains (e.g., code, structured reasoning, factual QA) remains untested.

- **Subjective task categorization**: Difficulty labels for Easy/Medium/Hard prompts are based on human annotator consensus, but the paper does not report inter-annotator agreement or validation procedures.

- **Modest creativity preference improvement**: The 19% preference rate for high-temperature responses is statistically significant, but the absolute number is modest and it's unclear whether this improvement is sufficient for practical deployment.

## Confidence
- **High confidence**: The core mechanism of CS (combining token probability with distributional certainty via σ) is clearly defined and mathematically sound. The experimental design (Wilcoxon tests, bootstrapped CIs) is rigorous and reproducible.
- **Medium confidence**: The claim that CS "mitigates creativity bias" is supported by statistically significant results, but the absolute improvement (19% vs 0%) is modest and domain-specific.
- **Low confidence**: The generalizability of CS to other evaluation scenarios (e.g., code generation, multi-turn dialogue) is not addressed, and the paper does not compare CS against alternative metrics like Self-Check GPT or human evaluation.

## Next Checks
1. **Ablation study on top-N selection**: Systematically vary n ∈ {2, 3, 5, 10} in the CS formula to determine sensitivity to the number of top probabilities used for σ calculation.

2. **Cross-domain evaluation**: Apply CS to a held-out dataset of prompts from different domains (e.g., code completion, factual QA, multi-turn dialogue) and report whether it retains its ability to discriminate difficulty and reduce creativity bias.

3. **Comparison against alternatives**: Benchmark CS against Self-Check GPT (multi-sample consistency) and human evaluation on a subset of creative prompts to assess relative performance and failure modes.