---
ver: rpa2
title: Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition
arxiv_id: '2509.10729'
source_url: https://arxiv.org/abs/2509.10729
tags:
- audio
- activity
- time
- step
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated using large language models (LLMs) for late
  multimodal sensor fusion to classify activities from audio and motion time series
  data. A curated subset of the Ego4D dataset was used, containing 12 diverse daily
  activities such as cooking, cleaning, and playing sports.
---

# Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition

## Quick Facts
- **arXiv ID**: 2509.10729
- **Source URL**: https://arxiv.org/abs/2509.10729
- **Reference count**: 40
- **Primary result**: LLM-based late multimodal fusion achieves above-chance zero- and one-shot classification for activity recognition from audio and motion data

## Executive Summary
This paper investigates the use of large language models (LLMs) for late multimodal sensor fusion in activity recognition tasks. The authors leverage a curated subset of the Ego4D dataset containing 12 diverse daily activities and combine predictions from audio (MS CLAP, VGGish) and motion (IMU) models as input to LLMs. Through prompting strategies that include both closed-set classification and open-ended reasoning, the approach demonstrates that LLMs can effectively fuse multimodal sensor information to classify activities without requiring aligned training data. Audio captions emerge as the most informative modality, with synthetic context further enhancing performance.

## Method Summary
The study employs a late fusion approach where modality-specific models process audio and motion time series data separately before feeding their predictions to LLMs for final activity classification. The Ego4D dataset is curated to include 12 daily activities such as cooking, cleaning, and sports. Audio inputs are processed using MS CLAP and VGGish models, while motion data from IMU sensors is analyzed using dedicated models. The LLM receives these per-modality predictions through carefully designed prompts that can handle both zero- and one-shot classification scenarios. The approach is evaluated on fixed-length 3-second time windows, with performance measured using F1-scores.

## Key Results
- Zero- and one-shot classification achieved F1-scores significantly above chance levels
- Audio captions provided the most informative modality for activity recognition
- Adding synthetic context to prompts further improved classification performance
- Open-ended activity classification also performed above chance, demonstrating LLM reasoning capabilities

## Why This Works (Mechanism)
The approach leverages LLMs' strong reasoning capabilities and ability to integrate heterogeneous information sources. By treating modality-specific model outputs as textual descriptions or features, LLMs can perform natural language reasoning over multimodal inputs without requiring end-to-end training on aligned sensor data. The success stems from LLMs' ability to understand context, draw inferences from partial information, and combine insights from different modalities through prompting strategies.

## Foundational Learning
- **Multimodal sensor fusion**: Combining information from multiple sensor types (audio, motion) is essential for robust activity recognition as different modalities capture complementary aspects of activities
  - *Why needed*: Single modalities often lack sufficient discriminative power for complex activity classification
  - *Quick check*: Verify that each modality captures distinct features not present in others

- **Zero- and one-shot learning**: Enabling classification with minimal or no training examples per class is crucial for real-world deployment where collecting labeled data is expensive
  - *Why needed*: Traditional supervised learning requires extensive labeled datasets that may not be available for all activities
  - *Quick check*: Ensure LLM prompts provide sufficient context for generalization to unseen activities

- **Late vs. early fusion**: Processing modalities separately before combining allows leveraging specialized models while avoiding alignment issues
  - *Why needed*: Early fusion often requires synchronized data streams and complex alignment procedures
  - *Quick check*: Confirm that separate processing doesn't lose critical temporal or cross-modal relationships

## Architecture Onboarding
- **Component map**: IMU sensors -> Motion model -> Text features -> LLM; Audio sensors -> MS CLAP/VGGish models -> Text features -> LLM; LLM performs final classification
- **Critical path**: Sensor data collection → Modality-specific processing → LLM prompting → Activity classification
- **Design tradeoffs**: Fixed 3-second windows vs. variable-length processing; late fusion simplicity vs. potential loss of cross-modal interactions; zero-shot flexibility vs. potential accuracy gains from fine-tuning
- **Failure signatures**: Poor performance on activities with similar audio profiles; degradation when motion patterns are ambiguous; timing misalignment causing modality confusion
- **Three first experiments**: 1) Ablation study removing audio captions to test modality importance, 2) Varying time window lengths to optimize temporal resolution, 3) Testing different prompt structures to maximize LLM reasoning effectiveness

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- Evaluation conducted on a curated subset of Ego4D rather than full dataset, potentially limiting generalizability
- Only 12 activities tested, which may not represent the full complexity of real-world scenarios
- Fixed 3-second time windows may not capture all activity types optimally and could miss important temporal dependencies

## Confidence
- **High confidence**: LLMs can perform zero- and one-shot classification from multimodal sensor data using late fusion approaches
- **Medium confidence**: Audio captions are the most informative modality, though this could benefit from more extensive ablation studies
- **Medium confidence**: Synthetic context improves performance, but the magnitude and consistency require further validation

## Next Checks
1. Evaluate on larger, more diverse activity sets and different datasets to assess generalizability beyond the curated Ego4D subset
2. Conduct extensive ablation studies varying time window lengths and testing different fusion strategies to optimize performance
3. Implement and test in real-time or near-real-time settings to evaluate computational efficiency and practical deployment considerations