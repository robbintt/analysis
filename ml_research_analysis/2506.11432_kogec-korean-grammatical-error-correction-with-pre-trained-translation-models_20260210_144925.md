---
ver: rpa2
title: 'KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models'
arxiv_id: '2506.11432'
source_url: https://arxiv.org/abs/2506.11432
tags:
- korean
- error
- should
- written
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research introduces KoGEC, a Korean Grammatical Error Correction
  system using pre-trained translation models. We fine-tuned NLLB (No Language Left
  Behind) models for Korean GEC, comparing their performance against large language
  models like GPT-4 and HCX-3.
---

# KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models

## Quick Facts
- arXiv ID: 2506.11432
- Source URL: https://arxiv.org/abs/2506.11432
- Authors: Taeeun Kim; Semin Jeong; Youngsook Song
- Reference count: 7
- Korean GEC system using fine-tuned NLLB translation models outperforms GPT-4o and HCX-3

## Executive Summary
This research introduces KoGEC, a Korean Grammatical Error Correction system that repurposes pre-trained translation models for grammatical error correction tasks. The authors fine-tuned NLLB models using special language tokens to distinguish between original and corrected Korean sentences, achieving superior performance compared to large language models like GPT-4o and HCX-3. The system was trained and tested on social media conversation datasets, demonstrating that compact, task-specific models can compete with larger general-purpose LLMs in specialized NLP tasks.

## Method Summary
The KoGEC system fine-tunes NLLB (No Language Left Behind) translation models for Korean grammatical error correction by treating the task as a translation problem from erroneous to correct Korean. A special token `<cor_Hang>` was added to signal the model to produce grammatically corrected output, while `<kor_Hang>` identifies the source language. The models were trained on 520K sentence pairs from NIKL and NIA social media conversation corpora, with evaluation using BLEU scores and an "LLM as judge" method that classifies error types with reference-guided grading using GPT-4o.

## Key Results
- Fine-tuned NLLB models (KoGEC) outperformed GPT-4o and HCX-3 in Korean GEC tasks
- KoGEC demonstrated more balanced error correction across 11 error types, while larger LLMs over-focused on punctuation errors
- The 3.3B parameter NLLB model achieved good performance, while the 600M parameter version struggled
- Chrome extension developed to make KoGEC accessible to users

## Why This Works (Mechanism)

### Mechanism 1: Translation-to-Correction Mapping via Special Tokens
The NLLB model's multilingual translation capability is repurposed by introducing a special token `<cor_Hang>` that signals the model to produce grammatically corrected output, distinguishing it from the source language token `<kor_Hang>`. This creates a clear input-output mapping where the model learns to "translate" grammatically incorrect sentences to correct ones.

### Mechanism 2: Specialized Fine-tuning Outperforms General LLM Scaling
Task-specific fine-tuning on compact translation models can outperform larger general-purpose LLMs for specialized grammatical error correction. While general LLMs like GPT-4o and HCX-3 have broader capabilities, their tendency to paraphrase rather than correct conservatively, combined with potential overfitting to punctuation patterns, limits their GEC performance.

### Mechanism 3: Reference-Guided LLM Evaluation Reduces Bias
Providing an LLM judge with reference solutions during evaluation improves classification accuracy of error types by reducing self-enhancement bias. When GPT-4o classifies error types in model outputs, providing the correct reference sentence allows for direct comparison rather than relying solely on the model's internal grammar knowledge.

## Foundational Learning

- **Transformer-based Sequence-to-Sequence Translation**: Understanding how NLLB's encoder-decoder architecture maps source sequences to target sequences is fundamental to comprehending why translation models can be repurposed for GEC.
  - Quick check: Can you explain how an encoder-decoder transformer differs from a decoder-only architecture like GPT in handling input-output pairs?

- **BLEU Score and Its Limitations**: The paper relies heavily on BLEU for evaluation, but BLEU measures n-gram overlap, not grammatical correctness specifically. Understanding its limitations is crucial for interpreting results.
  - Quick check: Why might a high BLEU score not correlate with accurate grammatical error correction in all cases?

- **Special Tokens and Language Embeddings**: The key innovation involves adding `<cor_Hang>` as a special token. Understanding how language tokens condition model behavior is essential for replication.
  - Quick check: How do special tokens like `<cor_Hang>` technically influence the model's output generation during inference?

## Architecture Onboarding

- **Component map**: NLLB-200 base model -> Special token vocabulary (`<kor_Hang>`, `<cor_Hang>`) -> Training data pipeline (NIKLI + NIA corpora) -> Fine-tuning configuration -> Evaluation layer (BLEU + LLM-as-judge) -> Error taxonomy (11 categories)

- **Critical path**: Data preprocessing (Unicode normalization critical) -> Special token addition to tokenizer and model embedding layer -> Fine-tuning with language token prefixes -> BLEU evaluation with proper Unicode normalization -> LLM-as-judge classification using reference-guided prompts

- **Design tradeoffs**: Model size vs. performance (600M struggled, 3.3B required), Vocabulary expansion (attempted but led to overfitting), Zero-shot vs. few-shot prompting (minimal improvement), Native vs. learner error focus (chose native speaker errors)

- **Failure signatures**: Overfitting on vocabulary-expanded model (training loss decreased but BLEU scores dropped), Unicode mismatch (Hangul Compatibility Jamo vs. Jamo causing BLEU score inflation), Unbalanced error correction (large LLMs over-focus on punctuation), Paraphrasing instead of correction (general LLMs change style)

- **First 3 experiments**:
  1. Baseline comparison: Run inference on test set with GPT-4o and HCX-3 using zero-shot prompting, compute BLEU scores against references
  2. Token ablation: Fine-tune NLLB-200-3.3B without the `<cor_Hang>` special token to measure contribution of explicit correction signaling
  3. Error type distribution analysis: Use LLM-as-judge pipeline with reference-guided grading on outputs from all models to replicate error type distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the NLLB fine-tuning approach used for Korean GEC be effectively transferred to structurally similar East Asian languages like Japanese and Chinese?
- Basis in paper: The authors identify extending the approach to Japanese and Chinese as a primary focus for future research to analyze linguistic commonalities.
- Why unresolved: While structurally related, the distinct writing systems and error patterns of Japanese and Chinese have not yet been tested with the KoGEC methodology.
- What evidence would resolve it: Successful application of the fine-tuning pipeline on Japanese and Chinese datasets, yielding performance metrics comparable to or better than current LLMs.

### Open Question 2
- Question: Can alternative model architectures like Gemma outperform NLLB by overcoming the limitations of NLLB's token vocabulary?
- Basis in paper: The authors propose exploring Google's Gemma model specifically to address the limited token vocabulary found in NLLB.
- Why unresolved: The study establishes NLLB's limitations regarding token coverage but has not yet tested if Gemma's architecture offers a solution for Korean GEC.
- What evidence would resolve it: Comparative benchmarks showing Gemma-based models achieving higher BLEU scores and better handling of unknown tokens than the current KoGEC models.

### Open Question 3
- Question: How can token vocabulary expansion be implemented without causing the overfitting and performance degradation observed in this study?
- Basis in paper: The authors attempted to expand the token vocabulary but found it decreased performance, noting the issue requires further investigation.
- Why unresolved: It is unclear if the failure was due to the specific expansion technique used or a fundamental limitation of adding untrained weights to the model.
- What evidence would resolve it: A modified fine-tuning strategy for vocabulary expansion that eliminates unknown tokens while maintaining or improving the BLEU score.

## Limitations

- The evaluation methodology relies heavily on BLEU scores and LLM-as-judge, both of which have known limitations for GEC tasks
- The vocabulary expansion experiment showed decreased performance but lacks detailed analysis of why this occurred
- The choice to focus on native speaker errors rather than learner errors limits generalizability to real-world use cases

## Confidence

**High confidence**: The claim that fine-tuned NLLB models outperform GPT-4o and HCX-3 in Korean GEC tasks, directly supported by BLEU score comparisons and error type distributions.

**Medium confidence**: The mechanism claim that special tokens enable translation-to-correction mapping, though the paper doesn't provide ablation studies to confirm this is optimal.

**Medium confidence**: The claim about KoGEC having a more balanced error correction profile, which relies on LLM-as-judge classification that hasn't been independently validated.

## Next Checks

1. **Error Classification Validation**: Run the LLM-as-judge error classification pipeline on a small subset of outputs (e.g., 100 sentences) and have human annotators independently classify the same errors. Compare agreement rates to quantify the reliability of the automatic evaluation.

2. **Cross-Lingual Generalization Test**: Apply the KoGEC model to grammatically incorrect sentences from another language (e.g., Japanese or English) that were translated to Korean. This would test whether the model learned language-agnostic grammatical patterns or Korean-specific corrections.

3. **Zero-Shot Transfer Evaluation**: Use the KoGEC model to correct grammatically incorrect sentences from Korean learners (using an available learner corpus) without any fine-tuning on learner data. This would test the model's robustness to different error patterns beyond native speaker mistakes.