---
ver: rpa2
title: 'The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction
  to Complex Moral Dilemmas'
arxiv_id: '2505.18154'
source_url: https://arxiv.org/abs/2505.18154
tags:
- moral
- value
- dilemmas
- llms
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multi-step Moral Dilemmas (MMDs) dataset,
  comprising 3,302 five-stage ethical scenarios designed to evaluate how LLMs adapt
  moral reasoning under escalating complexity. Unlike single-step assessments, MMDs
  progressively intensify value conflicts (e.g., care vs.
---

# The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas

## Quick Facts
- arXiv ID: 2505.18154
- Source URL: https://arxiv.org/abs/2505.18154
- Reference count: 27
- Key outcome: Introduces MMDs dataset showing LLMs exhibit shifting, non-transitive value preferences under escalating moral complexity

## Executive Summary
This paper presents the Multi-step Moral Dilemmas (MMDs) dataset, comprising 3,302 five-stage ethical scenarios designed to evaluate how LLMs adapt moral reasoning as dilemmas progressively intensify. Unlike single-step assessments, MMDs captures dynamic value prioritization patterns by escalating conflicts (e.g., care vs. fairness → care vs. coercion) across sequential steps. Evaluation of nine LLMs reveals significant shifts in value preferences, with models recalibrating judgments based on scenario complexity. While care remains a stable priority, fairness often supersedes it in certain contexts, highlighting context-dependent ethical reasoning. The findings suggest LLMs rely on local heuristics rather than stable principles, advocating for dynamic, context-aware evaluation paradigms.

## Method Summary
The methodology involves generating 3,302 five-step dilemmas using GPT-4o from Moral Stories norms, mapping each action to moral dimensions using 3-LLM consensus (GPT-4o-mini, DeepSeek-V3, GLM-4-Plus) with human adjudication for disagreements, and evaluating models under three context modes: no context, full context, and causal context. Causal context presents dilemmas sequentially while feeding back the model's prior choices, creating path-dependent reasoning. Value preferences are quantified via pairwise win rates across moral dimensions, with transitivity analysis revealing whether models exhibit coherent hierarchies or context-sensitive heuristics.

## Key Results
- LLMs show significant preference shifts across MMD steps, with care consistently prioritized but fairness often surpassing it in certain contexts
- Causal context uniquely enabled dynamic value adaptation while maintaining long-term coherence, closely approximating human moral development patterns
- Non-transitive preference cycles (e.g., care>sanctity, sanctity>fairness, yet care≈fairness) indicate models generate value preferences through context-driven statistical imitation rather than stable moral principles

## Why This Works (Mechanism)

### Mechanism 1: Progressive Value Conflict Intensification
- Claim: Escalating moral complexity across sequential steps reveals value prioritization patterns invisible to single-shot evaluations
- Mechanism: Each step introduces new value tensions while retaining prior context, forcing models to reconcile earlier decisions with emerging imperatives
- Core assumption: LLMs encode latent value preferences that emerge only under cumulative pressure
- Evidence anchors: [abstract] "progressively intensify value conflicts"; [Section 3.1] "This design captures how LLMs recalibrate value preferences when faced with escalating trade-offs"
- Break condition: If models produce identical preference rankings across all five steps regardless of context changes, the mechanism fails to reveal dynamics

### Mechanism 2: Causal Context Enables Path-Dependent Reasoning
- Claim: Sequential presentation with decision history produces more human-like moral adaptation than isolated or simultaneous presentation
- Mechanism: Causal context provides prior steps and model's own historical choices as input, creating temporal dependencies where each decision influences subsequent reasoning
- Core assumption: Moral reasoning in humans is inherently path-dependent; LLMs trained on human text may exhibit similar patterns when given appropriate context structure
- Evidence anchors: [Section 3.3] "causal context uniquely enabled dynamic value adaptation"; [Section 3.3] Full context "locking models into single-principle frameworks"
- Break condition: If causal context produces the same preference distributions as no-context evaluation, the mechanism provides no advantage

### Mechanism 3: Transitivity Analysis Detects Statistical Imitation
- Claim: Non-transitive preference cycles indicate models lack stable axiological hierarchies, relying instead on context-sensitive heuristics
- Mechanism: Pairwise win-rate comparisons across value dimensions reveal cycles that violate logical transitivity—a signature of pattern-matching rather than principled reasoning
- Core assumption: Coherent moral reasoning requires transitive preference structures; their absence implies surface-level statistical behavior
- Evidence anchors: [Section 4.2] "LLMs do not rely on stable moral principles, but rather generate value preferences through context-driven statistical imitation"; [Table 4] Shows specific non-transitive triads
- Break condition: If all models demonstrate transitive preference hierarchies, the claim of statistical imitation weakens

## Foundational Learning

- Concept: Moral Foundations Theory (MFT) — six dimensions: care, fairness, loyalty, authority, sanctity, liberty
  - Why needed here: MFT provides the value taxonomy for mapping actions to moral dimensions and comparing preferences
  - Quick check question: Can you name three MFT dimensions and their definitions?

- Concept: Preference transitivity — if A>B and B>C, then A>C
  - Why needed here: Central to evaluating whether LLMs have coherent value systems or merely local heuristics
  - Quick check question: Why does non-transitivity matter for moral reasoning consistency?

- Concept: Path-dependency in decision-making
  - Why needed here: Distinguishes causal context evaluation from static assessments; explains why order matters
  - Quick check question: How does path-dependency differ from context-dependency?

## Architecture Onboarding

- Component map: Dilemma Generator (GPT-4o) -> Value Mapper (3-LLM consensus + human adjudication) -> Evaluation Engine (3 context modes) -> Analysis Layer (preference scores, transitivity metrics)

- Critical path: Generate dilemmas → Map values (consensus) → Run models in causal-context mode → Compute pairwise win rates → Detect non-transitive cycles

- Design tradeoffs:
  - 5-step fixed length captures escalation but limits branching complexity
  - Western-centric moral frameworks (MFT, Schwartz) may underrepresent collectivist ethics (acknowledged limitation)
  - Binary choices (A/B) force trade-offs but oversimplify nuanced positions

- Failure signatures:
  - High refusal rates (>10%) indicate models dodging moral judgments
  - Identical preference scores across all steps suggest context not being processed
  - Perfect transitivity across all value triads may indicate mode collapse to single principle

- First 3 experiments:
  1. Replicate on your target model using provided dataset; compare preference trajectories to baseline models
  2. Ablate context modes: run same model under no-context vs. causal-context to quantify path-dependency effect
  3. Test non-transitivity: sample 50 value triads, compute pairwise win rates, identify cycles specific to your model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the integration of non-Western moral frameworks (e.g., Confucianism or Ubuntu) alter the value prioritization and transitivity hierarchies observed in LLMs?
- Basis in paper: [explicit] The authors note in the Limitations section that relying on MFT and Schwartz's theory "privilege Western-centric moral constructs" and may "underrepresent collectivist ethics"
- Why unresolved: The current MMDs dataset is constructed exclusively using Western-centric theoretical frameworks, potentially biasing the observed value conflicts
- What evidence would resolve it: Evaluation results from a dataset annotated with culture-specific dimensions in collaboration with local ethicists

### Open Question 2
- Question: Do LLMs maintain coherent moral reasoning in non-linear conflicts that involve de-escalation or branching narratives, rather than the linearly intensifying structure of MMDs?
- Basis in paper: [explicit] The authors state that real-world conflicts often involve "nonlinear escalation" which the "current step-wise design cannot model," suggesting a gap in the methodology
- Why unresolved: The MMDs framework forces a linear, five-stage escalation structure that may not reflect the dynamics of real-world negotiations or de-escalations
- What evidence would resolve it: Development and evaluation of "hybrid approaches combining branching narratives with generative adversarial scenarios" as suggested by the authors

### Open Question 3
- Question: Can specific training interventions mitigate the "non-transitive" and "shifting" moral preferences in LLMs to encourage reliance on stable principles rather than local heuristics?
- Basis in paper: [inferred] The paper concludes that LLMs use "context-driven statistical imitation" rather than "stable moral principles," implying a need for methods to enforce global consistency
- Why unresolved: The paper identifies the symptom (intransitivity and local heuristics) but does not investigate training techniques to correct this instability
- What evidence would resolve it: Experiments measuring transitivity rates on MMDs after applying consistency-regularization or principle-based reasoning training objectives

## Limitations
- Cultural bias: Both MFT and Schwartz frameworks originate from Western moral philosophy, potentially misrepresenting ethical priorities in non-Western contexts
- Dataset provenance: Exact generation hyperparameters (temperature, top-p) for GPT-4o dataset creation remain unspecified
- Value mapping reliability: 40 cases requiring manual resolution represent a non-trivial error source that could bias value preference measurements

## Confidence
- High Confidence: Value preference shifts under escalating complexity are empirically observed across multiple models; causal context advantage is clearly demonstrated
- Medium Confidence: Claims about LLMs relying on "statistical imitation" rather than stable principles are supported but require more direct behavioral evidence
- Low Confidence: Generalizability of specific preference trajectories to real-world ethical decision-making remains uncertain due to dataset constraints

## Next Checks
1. **Replication with target model**: Run causal-context evaluation on your specific LLM using the provided dataset; compare preference trajectory stability to baseline models (DeepSeek, GPT-4o, etc.)
2. **Cultural framework expansion**: Apply the MMD methodology using non-Western ethical frameworks (e.g., Confucian, Ubuntu) to assess whether preference shifts remain consistent across cultural contexts
3. **Real-world transfer test**: Map MMD-identified preference patterns to actual ethical decisions in applied domains (healthcare triage, autonomous vehicle scenarios) to evaluate practical alignment