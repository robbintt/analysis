---
ver: rpa2
title: 'From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning
  via Energy-Guided Diffusion Stratification'
arxiv_id: '2511.03828'
source_url: https://arxiv.org/abs/2511.03828
tags:
- offline
- online
- learning
- steps
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes StratDiff, a method for improving offline-to-online
  reinforcement learning by leveraging energy-guided diffusion modeling. The key idea
  is to use a diffusion model to approximate the offline behavior policy and compute
  the KL divergence between generated and actual actions to stratify samples into
  offline-like and online-like subsets.
---

# From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification

## Quick Facts
- arXiv ID: 2511.03828
- Source URL: https://arxiv.org/abs/2511.03828
- Reference count: 40
- Key outcome: StratDiff consistently improves offline-to-online RL performance on D4RL benchmarks by using energy-guided diffusion to stratify samples for adaptive learning

## Executive Summary
This paper introduces StratDiff, a method that bridges the gap between offline pretraining and online fine-tuning in reinforcement learning. The core innovation is using a diffusion model to approximate the offline behavior policy, then generating energy-guided actions and stratifying samples based on KL divergence alignment. This enables adaptive learning where conservative offline objectives are applied to behavior-consistent samples while exploratory online strategies are used for novel samples. Evaluated on D4RL with Cal-QL and IQL, StratDiff achieves higher normalized scores and more stable training compared to baselines.

## Method Summary
StratDiff operates in three phases: (1) pretrain a conditional diffusion model on offline data to capture the behavior policy, (2) train an energy network via contrastive learning using the pretrained Q-function, and (3) during online fine-tuning, generate energy-guided actions, compute KL-based alignment scores, stratify samples into offline-like and online-like subsets, and apply conservative offline objectives to the former and standard TD losses to the latter. The method uses a U-Net-style diffusion architecture with dense connections, a 4-layer MLP energy network, and fixed stratification ratio of 0.5.

## Key Results
- Consistently outperforms Cal-QL and IQL baselines on D4RL MuJoCo locomotion and AntMaze tasks
- Ablation studies confirm energy guidance is essential for performance (degrades without it)
- Performance improves with higher Unconstrained Trajectory Deviation (UTD) ratios
- Shows more stable training curves compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Energy-Guided Diffusion for Behavioral Alignment
StratDiff uses a Q-function as an energy guide to steer diffusion sampling toward actions that are both behaviorally consistent with the offline dataset and high-value. The score function decomposes as ∇log π_t(a_t|s) = ∇log μ_t(a_t|s) + ∇E_t(s,a_t), where the first term captures behavioral similarity from the pretrained diffusion model and the second term injects value-awareness via the learned energy network. This yields generated actions â that approximate the energy-guided policy. Core assumption: The diffusion model accurately captures the offline behavior policy distribution, and the Q-function provides meaningful energy signals.

### Mechanism 2: KL-Based Stratification for Sample Classification
KL divergence between the energy-guided generated action and the actual sampled action provides a proxy for measuring alignment with offline behavior. For each sample (s_i, a_i), compute score(s_i, a_i) = D_KL(a_i || π_0(·|s_i)) ≈ D_KL(a_i || â_i). Lower scores indicate offline-like behavior; higher scores indicate online-like exploration. The batch is sorted and split at ratio ρ. Core assumption: The KL divergence computed against a single generated sample meaningfully approximates distributional alignment.

### Mechanism 3: Stratified Update Rules for Conservative vs. Exploratory Learning
Applying different objectives to offline-like vs. online-like samples improves stability during fine-tuning by matching update conservatism to sample origin. For Cal-QL, offline-like samples retain the CQL regularizer R while online-like samples use pure TD loss. For IQL, offline-like samples use τ-expectile loss while online-like samples use τ=0.99 and max-Q targets. Core assumption: Offline-like samples benefit from pessimism while online-like samples can safely exploit higher Q-values without destabilization.

## Foundational Learning

- **Diffusion Models and Score-Based Generative Modeling**: Used to model the offline behavior policy and generate actions via iterative denoising with guidance. Why needed here: Enables learning and sampling from the behavioral distribution. Quick check question: Can you explain how classifier guidance modifies the score function ∇_x log p(x|y) in terms of the unconditional score and a classifier gradient?

- **Energy-Based Models and Contrastive Learning**: The energy function E_t is trained via contrastive learning to approximate intermediate energy gradients for guided sampling. Why needed here: Provides the guidance signal for directing diffusion toward high-value actions. Quick check question: How does the self-normalized contrastive loss in Eq. 7 relate to learning an energy function that distinguishes high-reward from low-reward actions?

- **Offline RL Challenges (Distribution Shift, Q-Overestimation)**: The stratification mechanism explicitly addresses distributional shift by identifying samples that deviate from offline behavior. Why needed here: Standard off-policy RL methods fail when trained purely on offline data without environment interaction due to distribution shift and Q-overestimation.

## Architecture Onboarding

- **Component map**: Base RL Algorithm (Cal-QL/IQL) -> Diffusion Behavior Model (U-Net-style) -> Energy Network (4-layer MLP) -> Stratification Module -> Stratified Loss Application
- **Critical path**: 1) Pretrain diffusion model on offline dataset, 2) Train base RL algorithm on offline data, 3) During online fine-tuning: generate guided actions → compute KL scores → stratify → apply stratified losses, 4) Continuously update energy network via contrastive learning
- **Design tradeoffs**: Guidance scale s is task-dependent (1.0–10.0); stratification ratio ρ is fixed at 0.5; UTD ratio affects performance gains; temperature β is fixed at 3.0
- **Failure signatures**: High variance in online-like samples (unstable energy guidance), stagnant performance on medium-replay datasets (poor diffusion model), exchange counts not decreasing over training (poorly discriminative stratification boundary)
- **First 3 experiments**: 1) Reproduce Cal-QL + StratDiff on halfcheetah-medium-replay, 2) Ablate energy guidance and measure performance drop, 3) Test sensitivity to stratification ratio ρ ∈ {0.3, 0.5, 0.7}

## Open Questions the Paper Calls Out

- **Can StratDiff maintain effectiveness when the offline dataset is composed of trajectories from multiple heterogeneous policies with varying optimality levels?**: The method assumes the diffusion model can approximate a coherent behavior policy μ, but multi-modal or conflicting behavioral data could lead to inaccurate KL divergence scores. Requires evaluation on specific D4RL "mixed" datasets.

- **Is there a theoretical or adaptive basis for setting the stratification ratio ρ, rather than fixing it at a static value (e.g., 0.5)?**: A fixed ratio assumes the proportion of "offline-like" to "online-like" samples remains constant or requires manual tuning. Requires comparison against an adaptive ρ schedule based on running statistics.

- **How can the sensitivity to the guidance scale be reduced to eliminate the need for extensive per-dataset hyperparameter tuning?**: The method currently relies on empirical settings from prior work or manual tuning to balance behavioral consistency and action quality. Requires automated mechanism for adjusting guidance scale.

## Limitations
- Diffusion model may fail to capture consistent behavioral pattern when dataset contains trajectories from multiple policies
- Stratification ratio ρ is fixed at 0.5 and may require adjustment for different environments
- Energy-guided sampling process is sensitive to guidance scale, requiring task-specific tuning

## Confidence

**High confidence**: Method implementation details are well-specified with clear mathematical formulations for the diffusion model, energy network, and stratification mechanism.

**Medium confidence**: The exact computation of KL divergence between point estimates is not fully specified, requiring assumptions about distributional approximation.

**Low confidence**: The U-Net diffusion architecture for vector-valued policies lacks detailed specifications on layer configurations and attention mechanisms.

## Next Checks
1. Implement and verify the KL-based stratification by computing alignment scores between generated and actual actions using a fixed-variance Gaussian approximation
2. Run ablation study removing energy guidance to confirm performance degradation as shown in Figure 4
3. Test sensitivity to stratification ratio ρ by comparing performance across ρ ∈ {0.3, 0.5, 0.7} on at least two locomotion tasks