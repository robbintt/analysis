---
ver: rpa2
title: 'Mano: Restriking Manifold Optimization for LLM Training'
arxiv_id: '2601.23000'
source_url: https://arxiv.org/abs/2601.23000
tags:
- manifold
- mano
- optimization
- muon
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mano, a novel optimizer for training large
  language models (LLMs) that combines manifold optimization with momentum-based updates.
  Unlike AdamW, which uses diagonal curvature estimates, and Muon, which applies global
  spectral normalization but loses curvature information, Mano projects momentum onto
  the tangent space of model parameters and constrains updates on a rotational Oblique
  manifold.
---

# Mano: Restriking Manifold Optimization for LLM Training

## Quick Facts
- arXiv ID: 2601.23000
- Source URL: https://arxiv.org/abs/2601.23000
- Reference count: 40
- Key outcome: Combines manifold optimization with momentum updates to outperform AdamW and Muon in LLM training with less memory and faster convergence

## Executive Summary
Mano introduces a novel optimizer for LLM training that projects momentum onto the tangent space of model parameters while constraining updates on a rotational Oblique manifold. Unlike AdamW's diagonal curvature estimates or Muon's global spectral normalization that loses curvature information, Mano maintains geometric regularization while preserving more structural information. Extensive experiments on LLaMA and Qwen3 models demonstrate that Mano consistently achieves better perplexity and faster convergence while using less memory and computational resources.

## Method Summary
Mano builds on SGD-M structure by projecting the momentum buffer onto the tangent space of parameters and normalizing along alternating dimensions (column/row) on an Oblique manifold. The method uses soft manifold constraints applied only to the update direction, not the parameters themselves, preserving model expressivity. Core operations include tangent projection to remove parameter-aligned components from momentum, alternating normalization between dimensions, and fixed rescaling to match AdamW's update range. The optimizer is applied selectively to 2D weight matrices while using AdamW for other parameter types.

## Key Results
- Consistently outperforms AdamW and Muon in test perplexity across LLaMA-350M, LLaMA-1.3B, and Qwen3-1.7B models
- Achieves up to 1.75× faster convergence in wall-clock time compared to Muon
- Uses 2× less memory than AdamW by maintaining only one momentum buffer
- 10–50× faster normalization operations than Muon's global spectral normalization
- Maintains lower gradient variance and higher signal-to-noise ratio during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting momentum onto the tangent space produces more stable gradient updates with lower variance than spectral normalization methods
- Mechanism: The tangent projection `vt = Mt − θ̂t ⊙ ⟨Mt, θ̂t⟩k` removes the component of momentum parallel to the parameter vector, retaining only the orthogonal (tangential) component
- Core assumption: The gradient-parameter alignment angle maintains a non-zero tangential component (sin(φ) ≥ γ > 0)
- Evidence anchors: Abstract states "projecting the momentum onto the tangent space of model parameters"; Section 5.3 shows Mano maintains lower gradient variance than Muon

### Mechanism 2
- Claim: Alternating column-wise and row-wise normalization captures bidirectional structural information in parameter matrices
- Mechanism: At odd steps, normalize along columns; at even steps, normalize along rows, preventing assumptions about dominant directions
- Core assumption: Neither column nor row structure exclusively dominates LLM parameter matrices
- Evidence anchors: Abstract mentions "constraining it on a rotational Oblique manifold"; Section 4.2 ablation shows static column-only normalization degrades at 1.3B scale

### Mechanism 3
- Claim: Soft manifold constraints applied only to update direction preserve model expressivity while benefiting from geometric regularization
- Mechanism: Parameters remain in Euclidean space with standard weight decay; only momentum is projected and normalized
- Core assumption: Constraining the optimization trajectory, not the solution space, is sufficient to capture geometric benefits
- Evidence anchors: Abstract highlights "significantly outperforms AdamW and Muon even with less memory consumption"; Section 4.1 explains this differs from traditional manifold optimization

## Foundational Learning

- Concept: Riemannian manifolds and tangent spaces
  - Why needed here: Mano operates by projecting onto tangent spaces of the Oblique manifold
  - Quick check question: Can you explain why projecting a vector onto a tangent space removes its component parallel to the manifold surface?

- Concept: Oblique manifold (set of matrices with unit-norm columns/rows)
  - Why needed here: The core normalization operation `A ⊘ ||A||_{2,k}` enforces unit-norm constraints
  - Quick check question: For a 4×8 matrix, what is the result of column-wise normalization versus row-wise normalization?

- Concept: Momentum-based optimization (SGD-M, Adam)
  - Why needed here: Mano builds on SGD-M structure; the momentum buffer Mt is what gets projected and normalized
  - Quick check question: What does the momentum coefficient μ=0.95 imply about how much history versus current gradient influences the update?

## Architecture Onboarding

- Component map: Compute gradient → Update momentum buffer → Alternate k dimension → Normalize parameters → Project to tangent space → Normalize tangent momentum → Apply scaled update with weight decay

- Critical path:
  1. Compute gradient → accumulate into momentum buffer
  2. Normalize parameters along dimension k → get θ̂t
  3. Project momentum onto tangent space: `vt = Mt − θ̂t ⊙ ⟨Mt, θ̂t⟩k`
  4. Normalize tangent momentum: `v̂t = vt ⊘ ||vt||_{2,k}`
  5. Apply scaled update with weight decay

- Design tradeoffs:
  - Memory vs AdamW: Halves memory (1 buffer vs 2 moments) but loses per-parameter adaptive learning rates
  - Compute vs Muon: 10–50× faster normalization but potentially less aggressive spectral whitening
  - Rotating vs static manifold: Rotation improves scaling but adds conceptual complexity

- Failure signatures:
  - Early-training loss higher than Muon/AdamW (expected behavior; convergence accelerates later)
  - Complete non-convergence suggests incorrect rotation logic or normalization dimension
  - Gradient norms collapsing to zero may indicate excessive parameter-gradient alignment

- First 3 experiments:
  1. Implement Mano on LLaMA-130M for 1000 steps; verify loss decreases and gradient SNR is positive
  2. Run static column-only vs rotational manifold on LLaMA-350M for 5000 steps; expect rotation to show clearer advantage at larger scales
  3. Time the normalization operations for a single forward-backward pass at 1B, 7B parameter scales

## Open Questions the Paper Calls Out

- **Open Question 1**: Does Mano maintain its convergence advantages at scales beyond 1.7B parameters, and how does the relative performance gap evolve as model size increases?
  - Basis: Paper expects to expand experiments to bigger LLMs to understand loss descent patterns
  - Why unresolved: Computational constraints limited experiments to LLaMA-1.3B and Qwen3-1.7B
  - What evidence would resolve it: Pretraining experiments on 7B, 13B, and larger models with consistent hyperparameters

- **Open Question 2**: What mechanisms drive Mano's accelerated late-stage convergence?
  - Basis: Paper observes Mano exhibits slower initial convergence but faster later loss reduction
  - Why unresolved: Only hypothesizes improved local minima escape properties without causal verification
  - What evidence would resolve it: Controlled experiments isolating the tangent projection, manifold normalization, and rotation components

- **Open Question 3**: Does the theoretical convergence guarantee extend to the full Mano optimizer with momentum and rotational manifold scheme?
  - Basis: Current proof excludes momentum and assumes static Oblique manifold
  - Why unresolved: Mathematical proof requires simplified assumptions; momentum introduces complex dynamics
  - What evidence would resolve it: Extended convergence proof incorporating momentum coefficient μ and alternating normalization

## Limitations

- Data distribution sensitivity: Reported improvements rely on specific tokenization and preprocessing pipelines, limiting generalizability to multilingual or non-English corpora
- Architecture dependency: Selective application to 2D weight matrices while using AdamW elsewhere raises questions about whether improvements stem from the optimization method itself or architectural choices
- Scaling extrapolation: Improvement trajectory from 350M to 1.3B may not extend linearly to trillion-parameter models, with the alternating normalization assumption lacking extreme-scale validation

## Confidence

- **High confidence**: Memory and computational complexity claims (2× less memory than AdamW, 10–50× faster than Muon) are directly measurable and confirmed in Table 3; lower gradient variance and higher SNR metrics are statistically robust
- **Medium confidence**: Convergence speed improvements and perplexity gains are well-documented but may be dataset-dependent; comparison with RSGD-M supports the soft-constraint mechanism but lacks comprehensive ablation studies
- **Low confidence**: Theoretical grounding for alternating column/row normalization outperforming static normalization is primarily empirical; claim that neither dimension exclusively dominates lacks architectural analysis validation

## Next Checks

1. **Cross-architecture validation**: Implement Mano on a non-Transformer architecture (e.g., RWKV or Mamba) to verify rotational manifold benefits are not specific to attention-based models; compare perplexity and convergence against AdamW at 1B parameter scale

2. **Dimensional ablation study**: Systematically test static column-only, static row-only, and rotational manifolds on LLaMA-350M with controlled weight initialization; measure not just perplexity but also gradient norm distributions and alignment angles to quantify the mechanism behind rotation benefits

3. **Scale extrapolation experiment**: Train LLaMA-7B and LLaMA-13B models (if computational budget allows) using identical hyperparameters to the 1.3B experiments; plot perplexity vs. parameter count for Mano, AdamW, and Muon to verify the improvement trend continues rather than saturating or reversing