---
ver: rpa2
title: 'MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question
  Answering'
arxiv_id: '2505.24040'
source_url: https://arxiv.org/abs/2505.24040
tags:
- relevance
- high
- physician
- familiarity
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedPAIR introduces a dataset of 1,300 medical QA pairs with sentence-level
  relevance annotations from 36 physician trainees. It compares human expert relevance
  judgments with those of LLMs using both self-reporting and ContextCite attribution.
---

# MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering

## Quick Facts
- arXiv ID: 2505.24040
- Source URL: https://arxiv.org/abs/2505.24040
- Reference count: 40
- Primary result: Dataset of 1,300 medical QA pairs with sentence-level relevance annotations from 36 physician trainees, revealing LLM-physician relevance disagreements

## Executive Summary
MedPAIR introduces a benchmark dataset of 1,300 medical question-answering pairs with sentence-level relevance annotations from physician trainees to evaluate alignment between human expert judgment and large language model (LLM) reasoning. The study systematically compares how physicians and LLMs identify relevant versus irrelevant information in medical contexts, using both self-reported relevance scores and ContextCite attribution methods. Results demonstrate that LLMs frequently disagree with physicians on information relevance, and that removing physician-labeled irrelevant content improves LLM accuracy by 5-25 percentage points. The benchmark enables fine-grained analysis of LLM reasoning patterns and clinical trustworthiness.

## Method Summary
The MedPAIR dataset was constructed by collecting medical QA pairs from USMLE and clinical case sources, then having 36 physician trainees annotate sentence-level relevance. Each sentence was labeled as relevant, irrelevant, or mixed based on its contribution to answering the question. Two LLM relevance assessment methods were employed: self-reporting where models assigned relevance scores to each sentence, and ContextCite which used counterfactual reasoning to determine attribution. Model performance was evaluated by comparing LLM relevance judgments against physician annotations and measuring accuracy improvements when irrelevant sentences were removed. The study tested multiple models including GPT-4o, Qwen-72B, and open-source alternatives across different prompt templates.

## Key Results
- LLMs frequently disagree with physicians on sentence-level relevance, with systematic differences in what constitutes relevant information
- Removing physician-labeled irrelevant sentences improves LLM accuracy by 5-25 percentage points across tested models
- GPT-4o outperforms open-source models, while Qwen-72B shows highest sensitivity to irrelevant content removal
- Self-reported relevance scores and ContextCite attribution methods yield consistent patterns of disagreement with physician judgments

## Why This Works (Mechanism)
The study works by creating a controlled environment where the relevance of each sentence can be independently assessed, allowing for precise measurement of alignment between human and machine reasoning. By using sentence-level annotations rather than document-level judgments, the methodology captures granular differences in how information is evaluated. The counterfactual reasoning approach in ContextCite provides an alternative validation method that helps confirm whether observed disagreements reflect genuine differences in information processing rather than artifacts of the annotation process.

## Foundational Learning
- **Sentence-level relevance annotation**: Physicians label individual sentences as relevant/irrelevant to enable fine-grained analysis of information importance
  - *Why needed*: Document-level judgments mask specific points of disagreement between humans and AI
  - *Quick check*: Verify inter-rater reliability among physician annotators
- **Counterfactual attribution reasoning**: ContextCite method tests whether removing sentences changes model outputs
  - *Why needed*: Provides objective measure of information importance beyond subjective relevance scores
  - *Quick check*: Compare results across multiple counterfactual scenarios
- **Controlled information removal experiments**: Systematically evaluate performance changes when irrelevant content is removed
  - *Why needed*: Quantifies the impact of information alignment on model accuracy
  - *Quick check*: Test removal of relevant vs irrelevant sentences to validate methodology

## Architecture Onboarding

**Component Map**: Question/Answer pairs -> Sentence segmentation -> Physician relevance annotation -> LLM relevance assessment -> Performance evaluation

**Critical Path**: Medical QA pairs → Sentence-level annotation → Relevance comparison → Accuracy measurement with/without irrelevant sentences

**Design Tradeoffs**: Fine-grained sentence-level analysis enables precise measurement but may miss holistic reasoning patterns; self-reporting vs ContextCite provides complementary validation but increases complexity

**Failure Signatures**: LLM-physician relevance disagreements concentrated in specific medical domains; performance improvements vary significantly across models; context length limitations affect relevance assessment

**First Experiments**: 1) Test inter-rater reliability among physician annotators; 2) Compare performance across medical specialties; 3) Evaluate different prompt templates for consistency

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Dataset size of 1,300 QA pairs may not capture full diversity of medical reasoning patterns
- Use of physician trainees rather than practicing clinicians could affect annotation quality
- Sentence-level approach may not reflect holistic clinical reasoning where context integration is critical
- Results may not generalize beyond English-language American medical contexts

## Confidence

**High confidence**: LLMs systematically disagree with physician relevance judgments is well-supported by dataset and methodology

**Medium confidence**: Quantitative performance improvements (5-25 percentage points) are reliable within experimental framework but may not generalize to all clinical scenarios

**Medium confidence**: Relative model ranking (GPT-4o vs Qwen-72B) is methodologically sound but may shift with different evaluation criteria

## Next Checks

1. Conduct inter-rater reliability analysis among physician annotators to establish consistency of relevance judgments
2. Expand dataset to include diverse medical specialties, languages, and healthcare contexts to test generalizability
3. Implement real-world clinical workflow testing to validate whether performance improvements translate to actual clinical decision-making environments