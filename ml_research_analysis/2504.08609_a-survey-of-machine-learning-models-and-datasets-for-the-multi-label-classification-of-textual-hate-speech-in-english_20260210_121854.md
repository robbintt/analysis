---
ver: rpa2
title: A Survey of Machine Learning Models and Datasets for the Multi-label Classification
  of Textual Hate Speech in English
arxiv_id: '2504.08609'
source_url: https://arxiv.org/abs/2504.08609
tags:
- hate
- speech
- multi-label
- classification
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically analyzes the emerging research landscape
  of textual multi-label hate speech classification in English. Analyzing 46 publications,
  we identify significant heterogeneity in datasets and models, with inconsistent
  evaluation metrics and a preference for BERT and RNN architectures.
---

# A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English

## Quick Facts
- **arXiv ID:** 2504.08609
- **Source URL:** https://arxiv.org/abs/2504.08609
- **Reference count:** 40
- **Primary result:** No single state-of-the-art model exists due to heterogeneity in datasets and inconsistent evaluation metrics

## Executive Summary
This systematic survey analyzes 46 publications on multi-label textual hate speech classification in English, revealing significant fragmentation in datasets, evaluation metrics, and model architectures. The research landscape shows heavy reliance on BERT and RNN architectures, with the Toxic Comment Classification Challenge as the most frequently used benchmark. Critical methodological issues include severe class imbalance, small and sparse datasets, and inconsistent label definitions across studies. The authors conclude that no definitive state-of-the-art model can be identified due to these inconsistencies, and provide nine recommendations for advancing the field, including transparent reporting, consolidated label sets, and exploration of large foundation models.

## Method Summary
The authors conducted a systematic mapping study following Kitchenham and Charters guidelines, analyzing 46 publications from Scopus and Web of Science databases. They identified key trends in datasets (Toxic Comment Classification Challenge most common), model architectures (BERT and RNN dominant), and evaluation metrics (ROC-AUC and F1-Micro standard). The survey synthesizes findings across multiple studies to characterize the current state of multi-label hate speech classification, highlighting methodological inconsistencies and open research challenges. The analysis focuses on English-language text classification where multiple hate speech subtypes can co-occur.

## Key Results
- **No state-of-the-art model exists** due to significant heterogeneity in datasets and inconsistent evaluation metrics
- **BERT and RNN architectures dominate** the field, with direct multi-label approaches using sigmoid activation functions
- **Critical open issues include** imbalanced training data, reliance on crowdsourcing, small and sparse datasets, and missing methodological alignment

## Why This Works (Mechanism)
Multi-label hate speech classification works by training models to predict multiple overlapping categories simultaneously using sigmoid activation functions and binary cross-entropy loss. The mechanism leverages pre-trained language models like BERT, fine-tuned on labeled datasets where each text sample can belong to multiple hate speech subtypes. This approach captures the complex, overlapping nature of hate speech by allowing the model to identify multiple forms of toxicity within a single text instance.

## Foundational Learning
**Multi-label Classification (why needed: text can contain multiple hate speech types simultaneously)**
- Quick check: Verify output layer uses sigmoid, not softmax

**Binary Cross-Entropy Loss (why needed: handles independent label probabilities)**
- Quick check: Confirm loss function treats each label independently

**ROC-AUC (why needed: robust metric for imbalanced multi-label classification)**
- Quick check: Ensure macro-averaging is used across labels

## Architecture Onboarding

**Component Map:** Text -> Tokenizer -> BERT/RoBERTa -> Linear Layer (N outputs) -> Sigmoid -> Binary Cross-Entropy Loss

**Critical Path:** Tokenization → Transformer Encoding → Label Prediction → BCE Loss → Backpropagation

**Design Tradeoffs:** 
- Sigmoid vs Softmax: Sigmoid allows overlapping labels, Softmax forces exclusivity
- Macro vs Micro averaging: Macro treats all labels equally, Micro weights by label frequency
- Pre-trained vs From-scratch: Pre-trained offers better performance with less data

**Failure Signatures:**
- Softmax outputs summing to 1 (incorrect for multi-label)
- High accuracy but low F1 indicating majority class prediction
- Metric inconsistency across studies preventing comparison

**3 First Experiments:**
1. Implement baseline BERT model on TCCC using sigmoid outputs and BCE loss
2. Test different classification thresholds (0.3-0.7) and measure F1 score impact
3. Compare macro vs micro averaging performance on imbalanced dataset

## Open Questions the Paper Calls Out

**Open Question 1:** How do large foundation models (e.g., GPT-4, Llama 3) perform on multi-label hate speech tasks compared to current BERT/RNN architectures?
- Basis: Authors recommend utilizing large foundation models because none were identified in surveyed works
- Unresolved: Zero-shot and few-shot capabilities of instruction-trained models unexplored for this task
- Resolution: Empirical benchmarks comparing foundation models against current architectures

**Open Question 2:** Can multi-label classification models be effectively adapted to align with criminal legal frameworks for English text?
- Basis: No datasets or models currently align with legal frameworks for English, unlike German research
- Unresolved: Existing datasets prioritize platform policy concepts over legal distinctions
- Resolution: Dataset annotated according to criminal statutes with classifier evaluation

**Open Question 3:** Does a consolidated benchmark with standardized metrics allow for identification of a definitive state-of-the-art model?
- Basis: Authors could not determine state-of-the-art model due to heterogeneity in datasets and metrics
- Unresolved: Fragmentation of evaluation metrics prevents meaningful performance comparison
- Resolution: Standardized comparative study evaluating diverse architectures on same datasets

## Limitations
- **No standardized benchmarks exist** - label definitions, thresholds, and metrics vary widely across studies
- **Severe class imbalance** affects model performance and makes cross-study comparison difficult
- **Small and sparse datasets** limit model generalizability and reliable performance estimation

## Confidence
- **High confidence:** Identification of methodological inconsistencies and evaluation metric preferences
- **Medium confidence:** Characterization of dataset limitations and class imbalance as critical issues
- **Medium confidence:** Recommendation that no state-of-the-art model exists due to field heterogeneity

## Next Checks
1. **Reproduce baseline model** using TCCC with BERT-base-uncased, Sigmoid output layer, and BCE loss, evaluating with ROC-AUC (Macro) and F1 (Micro/Macro)
2. **Test threshold sensitivity** by systematically varying classification thresholds (0.3-0.7) and measuring impact on F1 scores
3. **Analyze class imbalance effects** by training on both full TCCC dataset and balanced subset, measuring performance metric changes