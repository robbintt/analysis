---
ver: rpa2
title: Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for
  Intelligent Vehicles
arxiv_id: '2602.01452'
source_url: https://arxiv.org/abs/2602.01452
tags:
- object
- figure
- performance
- driver
- yolov13
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic, cross-paradigm evaluation of
  methods for identifying the semantic object at a driver''s point of gaze. It compares
  five methods across three paradigms: direct object detection (YOLOv13), segmentation-assisted
  classification (SAM2 paired with EfficientNetV2 vs YOLOv13), and query-based Vision-Language
  Models (VLMs, Qwen2.5-VL-7b vs Qwen2.5-VL-32b).'
---

# Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles

## Quick Facts
- arXiv ID: 2602.01452
- Source URL: https://arxiv.org/abs/2602.01452
- Reference count: 40
- Five methods across three paradigms evaluated on gaze-point object identification; YOLOv13 and large VLM (Qwen2.5-VL-32b) achieved Macro F1-Scores over 0.84

## Executive Summary
This paper systematically evaluates five methods across three paradigms for identifying semantic objects at a driver's point of gaze in intelligent vehicles. The paradigms include direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 vs YOLOv13), and query-based Vision-Language Models (VLMs, Qwen2.5-VL-7b vs Qwen2.5-VL-32b). Results show that direct object detection and large VLMs significantly outperform segmentation-assisted approaches, achieving Macro F1-Scores over 0.84. The large VLM demonstrated superior robustness for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions, while segmentation-assisted methods suffered from a "part-versus-whole" semantic gap that led to large recall failures.

## Method Summary
The study compares five pre-trained methods on a custom benchmark derived from BDD100K with 400 images across four scenarios (Clear Daytime, Clear Night, Rainy Daytime, Rainy Night) and 1,355 annotated gaze point-label pairs across five classes. YOLOv13 uses bounding box containment checks with confidence threshold 0.4. SAM2 generates masks from gaze points, which are then classified by EfficientNetV2 or YOLOv13. VLMs use structured prompts constraining outputs to five classes plus "Unsure." All methods map ImageNet-1K labels to target classes via Table 1, with -1 indicating no valid prediction treated as false negative. Evaluation uses Macro F1-Score, Accuracy, Macro-Precision, and Macro-Recall.

## Key Results
- YOLOv13 achieved 0.87 Macro F1-Score with 36.1 FPS, significantly outperforming segmentation-assisted methods
- Qwen2.5-VL-32b achieved 0.85 Macro F1-Score with 0.07 FPS, demonstrating superior small-object recall (0.82 for traffic lights vs 0.57 for YOLOv13)
- SAM2-based approaches achieved <0.35 Macro F1-Score due to part-versus-whole semantic gaps causing high -1 predictions
- VLM performance was particularly robust in adverse conditions, with superior nighttime traffic light detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Holistic scene analysis outperforms staged, region-isolated pipelines for gaze-point object identification.
- **Mechanism:** Both YOLOv13 and Qwen2.5-VL-32b process the full image to inform predictions, leveraging global context rather than isolated crops. This enables semantic disambiguation when gaze lands on object parts rather than whole objects.
- **Core assumption:** Drivers semantically attend to whole entities, not component parts, when fixating on object sub-regions.
- **Evidence anchors:** Abstract mentions "richer contextual understanding" trade-off; Section 5 states both methods "analyze the entire image to inform their predictions."

### Mechanism 2
- **Claim:** Large-scale VLMs leverage implicit world knowledge to detect small, contextually-cued objects under degraded visual conditions.
- **Mechanism:** The 32B-parameter VLM achieves higher recall on traffic lights (0.82 vs. 0.57 for YOLOv13) by inferring that small, colored lights at intersections are traffic signals, even when local features are obscured.
- **Core assumption:** Pre-training on diverse image-text pairs encodes traffic scene priors that transfer to gaze-point queries.
- **Evidence anchors:** Abstract highlights "superior robustness for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions."

### Mechanism 3
- **Claim:** Segmentation-first pipelines fail when prompt points fall on object parts, producing confident but semantically-misaligned masks.
- **Mechanism:** SAM2 generates high-confidence segmentations, but when gaze lands on a car component (wheel, taillight), the downstream classifier receives an isolated part with no whole-object context, causing low classifier confidence and frequent background predictions.
- **Core assumption:** Classifiers trained on whole-object images cannot reliably infer parent categories from parts without contextual cues.
- **Evidence anchors:** Abstract notes "segmentation-assisted paradigm suffers from a 'part-versus-whole' semantic gap that led to large failure in recall."

## Foundational Learning

- **Concept: Macro-averaged metrics for class-imbalanced evaluation**
  - **Why needed here:** Dataset has 429 Car instances vs. 75 Bus instances; overall accuracy masks per-class failures.
  - **Quick check question:** If a model achieves 90% accuracy but misses 80% of rare classes, which metric reveals this?

- **Concept: Bounding box containment vs. pixel-level segmentation for spatial queries**
  - **Why needed here:** Gaze points may fall near box boundaries where pixels belong to background; segmentation aims to resolve this but introduces semantic drift.
  - **Quick check question:** Given a gaze point on a car's side mirror, what does a bounding box method return vs. a segmentation-based method?

- **Concept: VLM query grounding and constrained output spaces**
  - **Why needed here:** VLMs require structured prompts to map open-ended visual reasoning to fixed class vocabularies; unconstrained outputs introduce variability.
  - **Quick check question:** Why does the prompt in Figure 5 include an "Unsure" option rather than forcing a class label?

## Architecture Onboarding

- **Component map:** Gaze coordinate → spatial resolution method (box check / mask / VLM grounding) → semantic label assignment → safety decision logic
- **Critical path:** Gaze coordinate → spatial resolution method → semantic label assignment → safety decision logic (e.g., warning suppression)
- **Design tradeoffs:**
  - YOLOv13: 36.1 FPS, 1.53 GB VRAM, 0.87 Macro F1; trades small-object recall for real-time feasibility
  - Qwen2.5-VL-32b: 0.07 FPS, 44.97 GB VRAM, 0.85 Macro F1; trades latency for night/small-object robustness
  - SAM2-based: ~10 FPS, 9-12 GB VRAM, <0.35 Macro F1; unsuitable for production without architectural redesign
- **Failure signatures:**
  - High -1 predictions + low recall (SAM2 pipelines): indicates part-versus-whole semantic gap
  - Low confidence scores on classifier stage despite high segmentation confidence: confirms mask-classifier mismatch
  - Night performance drop in recall (all models): indicates feature degradation under low-light conditions
- **First 3 experiments:**
  1. Reproduce YOLOv13 baseline on benchmark; validate containment threshold (0.4) sensitivity.
  2. Test SAM2 with multi-point prompts (centroid + gaze point) to reduce part-segmentation failures.
  3. Evaluate Qwen2.5-VL-7b vs. 32b on traffic light subset under Rainy Night to quantify scale-dependent robustness; log inference time per frame.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the noise inherent in real eye-tracking hardware alter the comparative performance of detection versus VLM paradigms?
  - **Basis in paper:** The authors note the dataset relies on "simulated gaze points" and call for future work to "validate these findings using real-world gaze data."
  - **Why unresolved:** Real eye-trackers introduce jitter and calibration errors that may affect the geometric precision required by YOLO differently than the contextual reasoning of VLMs.
  - **What evidence would resolve it:** Evaluation of the compared methods on a dataset captured with noisy, real-world eye-tracking hardware.

- **Open Question 2:** Can video-based VLMs utilize temporal context to improve robustness in adverse environmental conditions?
  - **Basis in paper:** The analysis is "frame-based," and the authors suggest extending "VLM-based approaches to video inputs to leverage temporal information."
  - **Why unresolved:** Temporal dynamics may resolve single-frame ambiguities (e.g., object occlusion in rain) that currently limit accuracy in static images.
  - **What evidence would resolve it:** A comparative study of frame-based versus video-based VLM performance on continuous driving footage.

- **Open Question 3:** Can a hybrid architecture effectively balance the real-time efficiency of YOLO with the semantic robustness of large VLMs?
  - **Basis in paper:** The conclusion recommends "investigating hybrid architectures that balance efficiency and robustness" to bridge the identified performance-efficiency trade-off.
  - **Why unresolved:** It is unknown if the complementary strengths of the top two paradigms can be integrated into a single system without introducing prohibitive latency.
  - **What evidence would resolve it:** Development and latency measurement of a cascaded model (e.g., YOLO for common cases, VLM for uncertainty) on the benchmark.

## Limitations

- The study relies on manually annotated gaze points rather than real eye-tracking hardware, limiting ecological validity
- The dataset size (1,355 gaze-point annotations across 400 images) may not capture rare edge cases or natural attention patterns
- VLM evaluation depends heavily on specific prompt engineering choices that weren't exhaustively explored
- The segmentation-assisted paradigm's poor performance may reflect specific architectural choices rather than fundamental limitations

## Confidence

- **High Confidence:** YOLOv13 outperforming segmentation-assisted methods; VLM superiority for small objects in degraded conditions; Macro F1 as appropriate metric for class imbalance
- **Medium Confidence:** Claims about holistic vs staged processing advantages; VLM's implicit world knowledge mechanism; segmentation's "part-versus-whole" failure mode
- **Low Confidence:** Exact performance thresholds across different hardware configurations; generalizability beyond the five target classes; impact of different prompt engineering strategies on VLM performance

## Next Checks

1. Test YOLOv13 with varying confidence thresholds (0.3-0.6) to establish robustness boundaries and verify the chosen 0.4 threshold is optimal
2. Implement multi-point segmentation prompts (gaze point + centroid) for SAM2 to determine if part-versus-whole failures can be mitigated without abandoning the segmentation approach
3. Conduct ablation study on VLM prompt structure — remove "Unsure" option, test alternative phrasing, and measure impact on both performance and safety-relevant uncertainty quantification