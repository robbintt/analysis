---
ver: rpa2
title: Reducing Variability of Multiple Instance Learning Methods for Digital Pathology
arxiv_id: '2507.00292'
source_url: https://arxiv.org/abs/2507.00292
tags:
- learning
- methods
- slide
- variability
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the issue of high performance variability in
  Multiple Instance Learning (MIL) methods for whole slide image (WSI) classification
  in digital pathology, which can reach up to 10-15 AUC points across different runs.
  This variability, caused by factors like weight initialization, batch shuffling,
  and learning rate, makes it difficult to reliably compare different MIL methods.
---

# Reducing Variability of Multiple Instance Learning Methods for Digital Pathology

## Quick Facts
- **arXiv ID**: 2507.00292
- **Source URL**: https://arxiv.org/abs/2507.00292
- **Reference count**: 30
- **Primary result**: Proposed multi-fidelity model fusion reduces MIL variability by up to 50% on WSI classification tasks

## Executive Summary
This paper addresses the high performance variability (10-15 AUC points) in Multiple Instance Learning methods for whole slide image classification in digital pathology. The variability stems from random initialization, batch shuffling, and learning rate choices, making it difficult to reliably compare different MIL methods. The authors propose a Multi-Fidelity, Model Fusion strategy that trains multiple models briefly, selects the most promising ones based on validation scores, and averages their weights to produce a more stable final model.

The method achieves better results than baseline and learning rate-tuned methods across all metrics and datasets, with significantly reduced standard deviation. The best-performing methods achieved a maximum AUC of 95.7 points for Camelyon16 and 88.2 points for BRACS. The approach can be applied to any existing MIL model to reduce performance variability, simplify hyperparameter tuning, and improve reproducibility while maintaining computational efficiency.

## Method Summary
The method involves training M identical models (typically 10) with the same initialization seed but different shuffle seeds for K epochs (typically 5). The top T models (typically 3) are selected based on validation AUC scores, and their weights are aggregated using either uniform averaging (Soup) or TIES-Merging (which averages only weights with matching signs and sets small conflicting values to zero). The merged model is then continued for full training (typically 100 epochs). This approach filters out poorly converging optimization trajectories early while maintaining single-forward-pass inference efficiency.

## Key Results
- The proposed methods achieved better results than baseline and learning rate tuned methods based on all metrics across all MILs and on both datasets
- The proposed methods obtained more stable results across all MIL methods and datasets, having the smallest standard deviation
- The best-performing methods achieved a maximum AUC of 95.7 points for Camelyon16 and 88.2 points for BRACS

## Why This Works (Mechanism)

### Mechanism 1: Multi-Fidelity Early Selection Filters Unpromising Optimization Trajectories
Training multiple models briefly and selecting top performers on validation AUC reliably identifies configurations that will converge to better final performance. Early validation scores serve as a proxy for final model quality, allowing rejection of initialization/shuffle combinations that fall into poor local minima early.

### Mechanism 2: Weight Averaging Smooths Optimization Noise from Initialization and Data Ordering
Averaging weights from multiple partially-trained models reduces variance caused by random initialization and batch shuffling without increasing inference cost. Each model's weights represent a noisy estimate of an optimal solution, and averaging cancels uncorrelated noise while reinforcing consistent signal.

### Mechanism 3: TIES-Merging Prevents Destructive Weight Cancellation
TIES-Merging outperforms uniform averaging when selected models have weight updates with conflicting signs by preserving only dominant directional changes. It computes weight deltas from initialization, trims small values, identifies dominant sign per parameter, and averages only values agreeing with dominant sign.

## Foundational Learning

- **Multiple Instance Learning (MIL) formulation**: Understanding that a slide = bag of patches with weak supervision is prerequisite to grasping why variability matters and how averaging helps. Quick check: Given a bag with 1000 patches and one positive label, what can you infer about individual patch labels?

- **Weight-space vs. prediction-space ensembling**: The paper's core innovation is applying model averaging (weight-space) to MIL instead of traditional ensembling (prediction-space). Distinguishing these explains why inference cost stays constant. Quick check: Why does averaging model weights before inference require only one forward pass, while averaging predictions requires N forward passes?

- **Multi-fidelity hyperparameter optimization**: The "train briefly, select, then continue" strategy borrows from multi-fidelity HPO literature. Understanding that low-fidelity evaluations (few epochs) proxy for high-fidelity (full training) explains the efficiency claim. Quick check: What is the tradeoff between fidelity (training epochs) and selection accuracy in early stopping?

## Architecture Onboarding

- **Component map**: WSI → Patch Extraction → Feature Encoder f_φ (frozen, pretrained) → MIL Aggregator g_θg (variable) → Classifier c_θc (variable) → Prediction

- **Critical path**: 1. Initialize M identical models with same seed 2. Train each for K epochs with different shuffle seeds 3. Rank by validation AUC, select top T 4. Merge weights (Soup: uniform average; TIES: sign-filtered average) 5. Continue training merged model to completion

- **Design tradeoffs**: M (models): Higher M increases selection quality but linearly increases pre-selection compute; K (epochs): Higher K improves selection fidelity but reduces efficiency gains; T (merge count): Paper finds T=3–5 optimal; Soup vs TIES: TIES more robust to sign conflicts but slightly more complex

- **Failure signatures**: High variance persists after merging → Check if K is too small for meaningful differentiation; Merged model underperforms best single model → T may be too high, including weak candidates; TIES underperforms Soup → Models may have largely consistent signs

- **First 3 experiments**: 1. Baseline variability check: Train single MIL model with 10 different init+shuffle seeds for 100 epochs each; 2. Soup3 reproduction: With M=10, K=5, T=3, implement uniform weight averaging; 3. Ablation on T: Fix M=10, K=5, vary T ∈ {1, 2, 3, 5, 10}

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed model fusion strategy effectively reduce variability when the feature extractor is fine-tuned end-to-end, rather than kept frozen? The authors explicitly state they ignore the variability of the encoder, considering it already pre-trained and frozen. This limits applicability to transfer learning scenarios where unfreezing the encoder would drastically increase parameter space.

### Open Question 2
Can this variability reduction method be successfully adapted to MIL tasks involving continuous targets, such as survival analysis or regression? The validation is restricted to classification tasks using AUC, but the method relies on "validation scores" which are less standardized for censored survival data.

### Open Question 3
Is there a risk of prematurely discarding optimal models by relying on validation scores after only K=5 epochs? The method assumes early performance predicts final performance, but deep learning optimization curves can fluctuate and different initialization seeds might lead to slow convergence ("late bloomers") that would be filtered out.

## Limitations
- The paper does not specify exact learning rates, hidden dimensions, or MIL-specific architecture parameters for each method
- Exact validation/test split indices for Camelyon16 and BRACS datasets are not provided
- The choice between Soup vs TIES merging for different MIL architectures appears somewhat arbitrary without clear justification

## Confidence

- **High Confidence**: The core mechanism of using early validation scores to select promising models (Multi-Fidelity selection) is well-supported by experimental results showing reduced variability across all tested MIL methods
- **Medium Confidence**: The superiority of TIES-Merging over uniform averaging is demonstrated but shows mixed results across different MIL methods
- **Low Confidence**: The optimal values for M (models), K (epochs), and T (merge count) are not thoroughly explored beyond the specific values used

## Next Checks

1. **Baseline Variability Quantification**: Train single MIL model (e.g., ABMIL) with 10 different init+shuffle seeds for 100 epochs each. Compute min/max/mean/std of test AUC to independently verify the claimed 10-15 AUC point variability range.

2. **TIES vs Uniform Averaging Ablation**: For each MIL method, train models using both TIES-Merging and uniform averaging with identical configurations (M=10, K=5, T=3). Compare not just STD reduction but also final AUC performance.

3. **Multi-Fidelity Selection Correlation**: Measure correlation between early validation AUC (at K=5 epochs) and final test AUC across all 10 models before selection. Verify that the selection mechanism actually identifies models that will perform well.