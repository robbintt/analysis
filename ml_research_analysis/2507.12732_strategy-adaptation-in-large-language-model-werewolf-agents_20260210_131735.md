---
ver: rpa2
title: Strategy Adaptation in Large Language Model Werewolf Agents
arxiv_id: '2507.12732'
source_url: https://arxiv.org/abs/2507.12732
tags:
- strategy
- role
- werewolf
- agents
- support
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a strategy adaptation method for LLM-based Werewolf
  agents that dynamically switches between Support and Attack strategies based on
  conversation context and role estimation. While prior approaches use fixed strategies,
  this method allows agents to adapt their behavior by explicitly estimating other
  players' roles and selecting optimal strategies.
---

# Strategy Adaptation in Large Language Model Werewolf Agents

## Quick Facts
- arXiv ID: 2507.12732
- Source URL: https://arxiv.org/abs/2507.12732
- Authors: Fuya Nakamori; Yin Jou Huang; Fei Cheng
- Reference count: 10
- Key outcome: Proposed strategy adaptation method improves Werewolf agent win rates from 0.57 to 0.60 through dynamic switching between Support and Attack strategies based on conversation context and role estimation.

## Executive Summary
This paper introduces a strategy adaptation method for LLM-based Werewolf agents that dynamically switches between Support and Attack strategies based on conversation context and estimated roles of other players. Unlike prior approaches using fixed strategies, this method enables agents to adapt their behavior by explicitly estimating other players' roles and selecting optimal strategies. Experiments using GPT-4o-mini and Gemini-2.0-flash demonstrate that the adaptation method improves win rates for Werewolf agents while showing mixed results for Villagers, with ablation studies revealing that role estimation significantly impacts performance.

## Method Summary
The method implements LLM-based Werewolf agents that estimate roles of other players using a 5-point scoring system (0-4) and dynamically switch between Support and Attack strategies at three decision points per round. Role estimation outputs per-role likelihood scores for each player, which feed into strategy selection logic that chooses between explicit Support (reducing suspicion through agreement) and Attack (amplifying accusations) strategies. The system uses natural language prompts rather than learned policies, with separate prompt templates for each agent type and strategy combination.

## Key Results
- Werewolf agents achieve 0.60 win rate with strategy adaptation compared to 0.57 with implicit methods
- Villager agents show mixed performance, benefiting from adaptation alone (0.80 win rate) but suffering when role estimation is added (0.45 win rate)
- Role estimation significantly impacts Werewolf performance but introduces noise that hurts Villager decision-making
- The method demonstrates improved effectiveness for agents who can leverage shared knowledge of ally identities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic strategy switching between Support and Attack modes improves agent performance over fixed or implicit strategies, particularly for Werewolf agents.
- **Mechanism:** The system evaluates game context at three decision points per round and selects appropriate strategy via prompted criteria. Support strategy reduces suspicion through agreement; Attack strategy amplifies accusations against opponents.
- **Core assumption:** LLMs can reliably assess situational context and follow conditional strategy-selection prompts without external fine-tuning.
- **Evidence anchors:** [abstract] "Experiments using GPT-4o-mini and Gemini-2.0-flash show that the adaptation method improves win rates for Werewolf agents, achieving 0.60 win rate compared to 0.57 for implicit methods." [section 5.1] "In both GPT and Gemini experiments, the proposed Adaptation method yielded the highest win rate for the Werewolf side."

### Mechanism 2
- **Claim:** Explicit role estimation enables strategy alignment with estimated allies and targeted opposition against estimated enemies.
- **Mechanism:** Agents output 5-point likelihood scores for each possible role (werewolf, villager, seer, doctor) for every other player, which feed into strategy selection—high werewolf likelihood triggers Attack; high ally likelihood triggers Support.
- **Core assumption:** LLMs can infer roles from limited conversational cues with sufficient accuracy to guide decisions.
- **Evidence anchors:** [section 3.2] "Each agent i is required to estimate the roles of other agents j... assigns a 5-point scale rating." [section 5.2] "For Werewolves, the highest win rate occurred when using the proposal method, indicating that role estimation had a significant impact."

### Mechanism 3
- **Claim:** Werewolf agents benefit disproportionately from strategy adaptation because they possess shared knowledge of ally identities.
- **Mechanism:** Werewolves begin the game knowing each other, enabling coordinated Support/Attack decisions. Villagers lack this information and must estimate blindly, increasing cognitive load and error potential.
- **Core assumption:** Information asymmetry (Werewolves know allies; Villagers do not) creates a structural advantage that strategy adaptation amplifies.
- **Evidence anchors:** [section 5.1] "One key reason is that Werewolf agents share knowledge of each other's identities, enabling them to synchronize their actions more effectively through strategic adaptation." [section 5.3] "When the Attack strategy is selected under adaptation, the increase in Est_j is significantly lower"—Werewolves avoid suspicion more effectively.

## Foundational Learning

- **Concept: Social Deduction Games (SDGs)**
  - **Why needed here:** Werewolf is a multi-agent incomplete-information game requiring deception, persuasion, and belief tracking. Understanding this domain is prerequisite to interpreting why strategy adaptation matters.
  - **Quick check question:** Can you explain why Villagers and Werewolves have fundamentally different information states at game start?

- **Concept: Prompt-Based Strategy Encoding**
  - **Why needed here:** The method encodes strategies as natural language prompts rather than learned policies. This requires understanding how instruction-following LLMs translate textual guidance into behavioral patterns.
  - **Quick check question:** How does a Support strategy prompt differ from an Attack strategy prompt in terms of the behavioral goals they specify?

- **Concept: Role Estimation as Classification Under Uncertainty**
  - **Why needed here:** The 5-point scoring system for role estimation is a form of probabilistic classification. Understanding calibration and accuracy tradeoffs is essential for diagnosing failure modes.
  - **Quick check question:** If an agent assigns a score of 4 to "werewolf" for a player who is actually the seer, what downstream error does this cause in strategy selection?

## Architecture Onboarding

- **Component map:** Role Estimation Module -> Strategy Selection Module -> Base Strategy Prompts -> Game Loop Interface
- **Critical path:**
  1. Implement role estimation prompt (see Table 5 in appendix)
  2. Implement adaptation criteria prompts (Tables 10, 11)
  3. Wire estimation output into strategy selection logic
  4. Pass selected strategy prompt to LLM for action generation
  5. Log Est_j scores for debugging

- **Design tradeoffs:**
  - **Explicit vs. implicit strategy:** Explicit estimation enables interpretability but introduces error propagation if estimates are wrong.
  - **Fixed vs. adaptive:** Adaptation adds complexity and prompt overhead; fixed strategies are simpler but less responsive.
  - **Shared prompts across roles:** Using the same Support/Attack structure for Villagers and Werewolves reduces prompt engineering but may miss role-specific nuances.

- **Failure signatures:**
  - Werewolf Est_j rising rapidly despite Attack strategy (indicates Attack prompt not effectively deflecting suspicion)
  - Villager win rate dropping below implicit baseline (suggests role estimation noise is hurting more than helping)
  - Agents selecting Support when under heavy suspicion (strategy selection criteria not triggering correctly)

- **First 3 experiments:**
  1. **Baseline replication:** Run implicit strategy agents against each other to confirm baseline win rates match reported values (~0.57 Werewolf, ~0.43 Villager).
  2. **Ablation: Adaptation-only:** Disable role estimation; use only strategy adaptation based on conversation context. Compare Villager win rates.
  3. **Estimation accuracy analysis:** Log all role estimation outputs; compute per-role accuracy and calibration. Identify which roles are hardest to estimate and correlate errors with win rate drops.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would strategy adaptation agents maintain their performance advantage against human players, who may employ more unpredictable or meta-strategies? Basis: [explicit] "Due to time and cost constraints, we did not conduct trials with human participants, and thus have not verified the agent's effectiveness in games against human players."

- **Open Question 2:** Can reinforcement learning optimize the timing and context of strategy switches more effectively than the current rule-based prompt criteria? Basis: [explicit] The conclusion states: "This complexity indicates room for further refinement of Adaptation; for example, by employing reinforcement learning to optimize the timing and context of strategy switches."

- **Open Question 3:** Would role-specific prompt customization for each Villager type (seer, doctor, basic villager) improve adaptation effectiveness and overcome the current performance disparity? Basis: [explicit] The conclusion suggests "customizing prompts for each Villager role to better address their unique decision-making challenges" as a refinement direction.

- **Open Question 4:** Can the role estimation component be improved to benefit rather than harm Villager agents, given that current estimation accuracy (Est ∼0.30 for Werewolves) appears insufficient for Villagers? Basis: [inferred] The ablation study showed that "for Villagers, agents with only adaptation achieved the highest win rate, while agents incorporating role estimation performed worse," and the authors note "room for improvement in the prompts used for role estimation."

## Limitations
- Role estimation accuracy is reported but not benchmarked against human performance or simpler baselines
- Results are limited to specific LLM models (GPT-4o-mini and Gemini-2.0-flash) and may not generalize to open-weight alternatives
- The disproportionate benefit for Werewolves may reflect structural game mechanics rather than superior strategy adaptation

## Confidence
- **High Confidence:** Dynamic strategy switching improves Werewolf win rates over fixed strategies (0.60 vs 0.57) - directly supported by experimental results
- **Medium Confidence:** Role estimation accuracy meaningfully impacts performance - supported by ablation studies but without detailed error analysis
- **Low Confidence:** Werewolf advantage stems primarily from shared knowledge of ally identities - claimed but not experimentally validated through counterbalancing

## Next Checks
1. **Error Analysis:** Log and analyze per-role estimation errors to identify which role classifications are most problematic and how they correlate with win rate changes
2. **Cross-Model Validation:** Test strategy adaptation with open-weight models (Llama-3.3-70B, Qwen2.5-72B) to verify results aren't model-specific artifacts
3. **Information Asymmetry Test:** Run experiments where Villagers are given partial ally information to isolate whether the Werewolf advantage comes from strategy adaptation or inherent information asymmetry