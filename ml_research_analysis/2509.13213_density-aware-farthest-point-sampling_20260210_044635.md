---
ver: rpa2
title: Density-Aware Farthest Point Sampling
arxiv_id: '2509.13213'
source_url: https://arxiv.org/abs/2509.13213
tags:
- uni00000013
- data
- uni00000011
- da-fps
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting training data for
  machine learning regression models when labeled data is limited due to computational
  constraints or high labeling costs. The authors focus on passive and model-agnostic
  sampling methods that rely only on data feature representations.
---

# Density-Aware Farthest Point Sampling
## Quick Facts
- **arXiv ID:** 2509.13213
- **Source URL:** https://arxiv.org/abs/2509.13213
- **Reference count:** 40
- **Primary result:** DA-FPS reduces MAE by up to 50% vs random/FPS on molecular datasets for training sets >5% of data

## Executive Summary
This paper introduces Density-Aware Farthest Point Sampling (DA-FPS), a novel passive sampling strategy for selecting training data in machine learning regression when labeled data is scarce. The method addresses the challenge of model-agnostic training set selection by minimizing a weighted fill distance, which the authors show is linearly related to expected prediction error for Lipschitz continuous regression models. DA-FPS iteratively selects points at maximal weighted distance from already chosen points, using kNN density estimation to compute weights that account for local data density.

The approach is evaluated on three molecular datasets (QM7, QM8, QM9) and a ZINC subset using two regression models (KRR and FNN). Results demonstrate that DA-FPS significantly outperforms random sampling, standard FPS, and other baselines, particularly for larger training set sizes. The method achieves up to 50% reduction in mean absolute prediction error while maintaining computational efficiency suitable for large-scale applications.

## Method Summary
DA-FPS is a passive, model-agnostic sampling strategy that minimizes a weighted fill distance to optimize training set selection for regression tasks. The method computes kNN-based density estimates and uses these to calculate weights for each data point, then greedily selects points that maximize weighted distance from already chosen points. A key theoretical contribution is deriving an upper bound for expected prediction error that depends linearly on this weighted fill distance. The authors prove that DA-FPS provides approximate minimizers for this distance estimation. Implementation uses cKDTree for efficient kNN queries and includes an initial uniform-weight phase for stability. The method is designed to be computationally efficient while improving over standard FPS by accounting for local data density variations.

## Key Results
- DA-FPS reduces MAE by up to 50% compared to random sampling and standard FPS on molecular datasets
- Performance improvements are most pronounced for larger training set sizes (>5% of available data)
- DA-FPS consistently outperforms k-medoids++, facility location, and other baseline methods across all tested datasets
- The method maintains computational efficiency suitable for large-scale applications while providing significant accuracy gains

## Why This Works (Mechanism)
The core mechanism relies on minimizing a weighted fill distance that accounts for local data density variations. Standard FPS selects points based on Euclidean distance alone, which can lead to oversampling in dense regions and undersampling in sparse regions. DA-FPS addresses this by weighting distances inversely to local density, effectively allocating more training points to underrepresented regions of the feature space. The theoretical analysis shows that this weighted fill distance has a linear relationship with expected prediction error for Lipschitz continuous regression models, providing a principled basis for the sampling strategy. The kNN density estimation provides a data-driven way to estimate local density without requiring additional assumptions about the data distribution.

## Foundational Learning
- **kNN density estimation:** Used to estimate local data density for weight computation; needed to identify underrepresented regions of feature space; quick check: verify weights are bounded in [1, k] range
- **Weighted fill distance:** Generalization of standard fill distance that accounts for local density; needed as the optimization target for sampling; quick check: confirm distance metric properly normalizes by density weights
- **Passive vs active learning:** DA-FPS is passive (no model queries); needed for scenarios where labeling is expensive; quick check: ensure method doesn't require model predictions during sampling
- **Lipschitz continuity:** Assumption on regression models for theoretical error bounds; needed to establish relationship between weighted fill distance and prediction error; quick check: verify regression models satisfy Lipschitz conditions
- **Greedy optimization:** DA-FPS uses greedy selection; needed for computational tractability; quick check: confirm greedy choices align with overall objective
- **k-medoids++ initialization:** Used in some baselines; needed for fair comparison with established methods; quick check: verify initialization doesn't dominate final performance

## Architecture Onboarding
**Component Map:** Data preprocessing -> kNN density estimation -> weight computation -> greedy selection -> model training -> evaluation
**Critical Path:** The core algorithm flows from density estimation through weighted distance computation to greedy point selection, with preprocessing and evaluation as bookends
**Design Tradeoffs:** kNN parameter k vs computational cost; uniform-weight initialization phase length vs early convergence; adaptive neighborhood radius ε_X vs robustness to density estimation errors
**Failure Signatures:** Poor performance indicates degenerate density estimates (weights too uniform or too extreme), incorrect weight computation, or inappropriate kNN parameters for dataset size
**First Experiments:** 1) Verify kNN density estimates produce reasonable weights (range and distribution); 2) Test DA-FPS on small synthetic dataset with known density structure; 3) Compare DA-FPS weighted distances to standard FPS distances on sample data

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends on careful tuning of kNN parameters (k, u, ε_X) which are not fully specified
- Theoretical guarantees rely on Lipschitz continuity assumption which may not hold for all regression models
- Computational complexity scales with dataset size, though still tractable for moderate datasets
- Method effectiveness decreases for very small training set sizes (<5% of data)

## Confidence
- **Claimed performance improvements:** Medium - results are well-documented but depend on unspecified hyperparameters and seeds
- **Theoretical analysis:** High - derivations are sound and properly bounded
- **Reproducibility:** Medium - key implementation details missing but method is well-defined
- **Generalizability:** Medium - tested primarily on molecular datasets with specific feature representations

## Next Checks
1) Fix all random seeds and re-run experiments to assess variance and ensure consistent results
2) Test sensitivity to kNN parameter k across different dataset sizes to identify optimal settings
3) Compare DA-FPS performance with and without the adaptive neighborhood radius modification to isolate its contribution