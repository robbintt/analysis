---
ver: rpa2
title: 'Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation
  of Action Duration and Completion through Perfect Times'
arxiv_id: '2506.00928'
source_url: https://arxiv.org/abs/2506.00928
tags:
- video
- conj
- adjunct
- person
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Perfect Times dataset, a multilingual
  benchmark (English, Italian, Russian, Japanese) for evaluating video-language models'
  (VLMs) ability to reason about action duration and completion. The dataset pairs
  videos with multiple-choice questions designed to probe temporal and aspectual understanding
  through complex sentence structures that incorporate perfectivity and telicity markers.
---

# Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times

## Quick Facts
- **arXiv ID:** 2506.00928
- **Source URL:** https://arxiv.org/abs/2506.00928
- **Reference count:** 40
- **Primary result:** VLMs underperform humans (93.36% accuracy) on temporal reasoning benchmark, achieving only 25.92-46.99% accuracy across languages.

## Executive Summary
This study introduces the Perfect Times dataset, a multilingual benchmark designed to evaluate video-language models' ability to reason about action duration and completion through complex sentence structures incorporating perfectivity and telicity markers. Despite achieving high performance on standard benchmarks, state-of-the-art VLMs consistently underperform human annotators when tested on Perfect Times, with accuracy rates ranging from 25.92% to 46.99% across English, Italian, Russian, and Japanese. The results reveal that models struggle to leverage grammatical aspect and visual cues in tandem, often ignoring linguistic markers of completion in favor of telic answers, highlighting a critical gap in VLMs' multimodal temporal reasoning capabilities.

## Method Summary
The Perfect Times dataset pairs videos with multiple-choice questions designed to probe temporal and aspectual understanding. Videos were collected from YouTube with manual annotation to ensure clarity of action boundaries. Questions were constructed using 12 templates that systematically vary temporal relations (precedence, succession, simultaneity) and grammatical aspects (perfective vs. imperfective). Distractor types were designed to test different failure modes: linguistic sensitivity to aspect, temporal ordering, and visual grounding. The dataset was evaluated across four languages using both commercially available models (GPT-4o, Gemini-2.0-flash-lite) and open-source models (InternVL2-8B, Qwen2-VL-7B, Qwen2-VL-72B, LLaVA-NeXT-Video), with results compared against human annotators achieving 93.36% accuracy.

## Key Results
- VLMs achieved 25.92-46.99% accuracy on Perfect Times, significantly below human performance of 93.36%
- All models systematically preferred telic (completed) answers over durative (ongoing) ones when incorrect
- Russian data yielded highest performance due to lexical encoding of perfectivity, while Japanese performance was lowest due to reliance on visual context
- LLaVA-NeXT-Video showed selection bias, consistently favoring option a0
- Error analysis revealed specific failure modes: linguistic shortcut failures, temporal ordering failures, and visual grounding failures

## Why This Works (Mechanism)

### Mechanism 1: Linguistic-Visual Misalignment in Temporal Integration
VLMs fail to synchronize grammatical aspect markers with visual temporal boundaries, treating language and vision as separate streams rather than integrated temporal reasoning systems. The models process visual frames independently while language processing operates on abstract token representations, neither explicitly encoding temporal endpoints needed for perfectivity judgments.

### Mechanism 2: Perfectivity/Telicity Bias in Model Predictions
VLMs systematically prefer completed (telic) answers over ongoing (durative) ones, reflecting a causal reasoning shortcut rather than genuine aspectual understanding. Training data contains prevalent causal connections between sequential actions, causing models to apply this statistical regularity as a heuristic.

### Mechanism 3: Language-Specific Encoding Effects
Temporal reasoning performance varies with how each language encodes perfectivity—lexical encoding (Russian) aids models, while ambiguity requiring visual disambiguation (Japanese) hinders them. Models exploit surface linguistic markers when available but lack robust visual-temporal grounding mechanisms.

## Foundational Learning

- **Concept: Allen's Interval Algebra**
  - **Why needed here:** The benchmark's 12 templates systematically cover all temporal relations between action pairs. Understanding interval relationships is essential for interpreting template difficulty.
  - **Quick check question:** Given two actions A and B where A ends before B starts, which Allen relation describes this?

- **Concept: Grammatical Aspect (Perfectivity & Telicity)**
  - **Why needed here:** The core evaluation tests whether models understand that "What did the person do?" vs "What was the person doing?" require different visual evidence.
  - **Quick check question:** Does "to put something somewhere" vs "to hold something" differ in inherent completion?

- **Concept: Multimodal Fusion Architectures**
  - **Why needed here:** VLMs use visual encoders, LLMs, and projection interfaces. Knowing where temporal information is lost helps diagnose failures.
  - **Quick check question:** If frames are sampled at 1 FPS from a 30-second video, what temporal resolution can the model potentially capture?

## Architecture Onboarding

- **Component map:** Video Input → Frame Sampler → Visual Encoder → Projection Layer → LLM Backbone; Text Input → Tokenizer → LLM Backbone → Output Generation
- **Critical path:** The projection layer is where temporal information is most likely lost. Visual encoders output per-frame embeddings; if projection doesn't preserve temporal ordering, the LLM receives an unordered "bag of visual features."
- **Design tradeoffs:**
  - Frame sampling rate: Higher FPS preserves temporal detail but increases compute
  - Native video vs. frame sequences: Native processing may capture motion features; frame lists lose inter-frame dynamics
  - Language coverage: Multilingual LLM backbones don't guarantee multilingual temporal reasoning
- **Failure signatures:**
  - Distractor Type 1 errors: Model ignores grammatical aspect
  - Distractor Type 2 errors: Model confuses action order
  - Distractor Type 3 errors: Model selects out-of-context action
  - Selection bias: LLaVA-NeXT-Video favors option a0
- **First 3 experiments:**
  1. Ablate distractor types to isolate whether failure is linguistic, temporal, or visual
  2. Vary frame sampling (1 FPS vs. 5 FPS vs. keyframe-only) to determine temporal resolution needs
  3. Cross-lingual transfer: Fine-tune on Russian and test on Japanese to see if lexical aspect markers transfer

## Open Questions the Paper Calls Out

### Open Question 1
How can VLM architectures be modified to effectively disambiguate ambiguous temporal conjunctions (e.g., "when") by integrating visual cues rather than relying on statistical bias? The authors note models struggle to disambiguate "when" compared to explicit markers like "while," indicating failure to align temporal boundaries with verb aspect.

### Open Question 2
Does improved performance on semi-synthetic, template-based benchmarks like Perfect Times translate to improved temporal reasoning in natural, spontaneous speech? The template-based approach may not fully reflect variability found in natural language.

### Open Question 3
What specific architectural or training modifications are required to correct VLMs' systematic preference for telic (completed) answers over durative (ongoing) ones? While the paper identifies this bias, it doesn't determine if it stems from training data imbalances or attention mechanisms.

## Limitations
- Template-based approach may not fully capture variability in natural language
- Frame sampling strategy (1 FPS or 3-second intervals) could miss critical sub-second temporal boundaries
- Cross-linguistic analysis assumes equivalent difficulty across languages despite cultural differences
- Model snapshot represents current capabilities but architectural differences complicate isolating failure sources

## Confidence

- **High Confidence (90%+):** Models consistently underperform humans across all languages; distractor type analysis reveals specific failure modes
- **Medium Confidence (70-80%):** Proposed mechanisms explaining failures are supported by ablation results and error patterns
- **Low Confidence (50-60%):** Claims about why certain languages are harder (Russian vs. Japanese encoding strategies) are suggestive but not definitively proven

## Next Checks

1. **Temporal Boundary Annotation Ablation:** Re-run benchmark with models receiving explicit start/end timestamps alongside video frames. If performance on perfectivity judgments improves significantly (>20% accuracy gain), this validates temporal boundary loss during projection as primary failure mode.

2. **Aspect-Explicit Fine-Tuning Transfer:** Fine-tune models on Russian subset with aspect labels, then evaluate on Japanese. If Japanese performance improves proportionally to Russian gains, this confirms learning lexical aspect markers transfers to visual-temporal reasoning.

3. **Frame Rate Sensitivity Analysis:** Test models using 0.5 FPS, 1 FPS, 5 FPS, and keyframe-only sampling. Plot accuracy curves against temporal resolution to determine minimum frame rate needed for reliable aspect judgments. If accuracy plateaus at 2-3 FPS, current sampling rates are adequate.