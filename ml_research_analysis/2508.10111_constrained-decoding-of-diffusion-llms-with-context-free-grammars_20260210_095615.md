---
ver: rpa2
title: Constrained Decoding of Diffusion LLMs with Context-Free Grammars
arxiv_id: '2508.10111'
source_url: https://arxiv.org/abs/2508.10111
tags:
- language
- json
- output
- constrained
- infilling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first constrained decoding method for
  diffusion language models (DLMs) that enforces context-free grammar (CFG) constraints.
  The key insight is reducing constrained infilling to deciding whether the intersection
  of a CFG and a regular language (describing all possible completions) is non-empty.
---

# Constrained Decoding of Diffusion LLMs with Context-Free Grammars

## Quick Facts
- arXiv ID: 2508.10111
- Source URL: https://arxiv.org/abs/2508.10111
- Reference count: 40
- Primary result: First CFG-constrained decoding for diffusion LLMs achieving up to 99.7% syntactic correctness on C++ code completion

## Executive Summary
This paper introduces the first constrained decoding method for diffusion language models that enforces context-free grammar constraints. The key insight is reducing constrained infilling to deciding whether the intersection of a CFG and a regular language (describing all possible completions) is non-empty. An efficient algorithm leverages the structure of this intersection language, avoiding explicit construction and pruning non-generating symbols. Experiments on C++ code completion, JSON schema extraction, and SMILES molecule generation show significant improvements in syntactic correctness (up to 99.7%) and functional correctness (up to 7%) across multiple models while maintaining practical inference time overhead.

## Method Summary
The method constructs a DFA representing all possible completions of partial output, then checks if the intersection with the target CFG is non-empty using an implicit bottom-up search algorithm. The approach handles lexemes spanning infilling regions and applies CFG normalization and size reduction heuristics. The algorithm avoids cubic blowup by implicitly enumerating only generating symbols in the intersection grammar, and includes a robust lexer for partial tokens.

## Key Results
- 99.7% syntactic correctness on C++ FIM tasks (vs 44.5% unconstrained)
- 97.4% syntactic correctness on JSON Schema extraction (vs 71.8% unconstrained)
- 5% improvement in functional correctness on C++ FIM tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining diffusion LLMs reduces to deciding if a CFG intersection with a regular language is non-empty
- Mechanism: Given partial output x with infilling regions, the method constructs a regular language Cx describing all possible completions, then checks if L ∩ Cx ≠ ∅ using CFG-DFA intersection properties. This transforms the constrained decoding problem into a well-studied formal language problem.
- Core assumption: The language of all possible completions of a partial output forms a regular language that can be efficiently represented as a DFA.
- Evidence anchors:
  - [abstract]: "reducing constrained infilling to deciding whether the intersection of a CFG and a regular language...is non-empty"
  - [Section 3.2]: "The infilling problem is answered positively if and only if the intersection L∩ = L ∩ Cx is not empty"
  - [corpus]: DINGO (arXiv:2505.23061) handles regular languages for diffusion LLMs, supporting the tractability of constrained approaches
- Break condition: If the intersection language grows exponentially or the DFA becomes intractably large, the approach becomes impractical.

### Mechanism 2
- Claim: Implicit enumeration of intersection grammar avoids cubic blowup in explicit construction
- Mechanism: Rather than materializing all |V||Q|² nonterminals and |P||Q|³ productions, the algorithm searches only generating symbols on-demand. It leverages the structure that all intersection symbols have form p⃗Aq and productions derive directly from original CFG rules.
- Core assumption: Most nonterminals in the full intersection grammar are non-generating (cannot produce terminal strings).
- Evidence anchors:
  - [abstract]: "efficient algorithm...leveraging the structure of this intersection language, avoiding explicit construction"
  - [Section 3.2]: "many of the nonterminals and productions in the intersection grammar are not generating...we therefore adapt an efficient bottom-up search"
  - [corpus]: Weak direct evidence; related work on grammar optimization (NatGI) suggests grammar reduction is an active concern
- Break condition: If the intersection grammar has many generating symbols or deep derivations, implicit search may still be slow.

### Mechanism 3
- Claim: Lexing with partial-match handling bridges character-level grammar and token-level generation
- Mechanism: The method extracts multiple possible lexeme sequences from partial outputs, accounting for tokens that span infilling regions. It builds a union NFA accepting all valid lexeme sequences simultaneously, avoiding combinatorial explosion.
- Core assumption: LLM tokens can be mapped to grammar terminals with manageable ambiguity.
- Evidence anchors:
  - [Section C]: "handling lexemes spanning infilling regions...the lexing process is not as straightforward because lexemes can span over the infilling regions"
  - [Section C, Algorithm 3]: Explicit handling of prefixes, suffixes, and cross-region lexemes
  - [corpus]: No direct corpus evidence on lexing specifically
- Break condition: If token-vocabulary mismatches cause exponential lexeme possibilities, the NFA construction fails.

## Foundational Learning

- Concept: **Context-free grammars (CFGs) vs regular languages**
  - Why needed here: The method relies on the closure property that CFG ∩ Regular Language = CFG. Understanding this hierarchy explains why the approach works for CFGs but not more expressive grammars.
  - Quick check question: Given CFG S → aSb | ε and regular language a*b*, what is their intersection?

- Concept: **Diffusion LLM generation paradigm**
  - Why needed here: Unlike autoregressive LLMs, diffusion LLMs iteratively insert tokens at arbitrary mask positions. This requires constrained decoding that handles out-of-order generation, not just left-to-right completion.
  - Quick check question: How does token insertion order in diffusion LLMs differ from autoregressive generation?

- Concept: **DFA-CFG intersection construction**
  - Why needed here: The core algorithm builds a new CFG whose language equals the intersection. The construction creates nonterminals p⃗Aq tracking both grammar symbol A and DFA states p, q.
  - Quick check question: If the original CFG has n nonterminals and the DFA has m states, what is the worst-case size of the intersection grammar?

## Architecture Onboarding

- Component map:
  1. Lexer -> NFA/DFA Builder -> Grammar Preprocessor -> Intersection Engine -> Completion Sampler

- Critical path:
  1. Receive model proposal → 2. Lex to lexeme sequences → 3. Build/updated DFA → 4. Check intersection non-emptiness → 5. Accept/reject proposal
  The intersection check (step 4) is the bottleneck; Rust implementation is recommended.

- Design tradeoffs:
  - **Overapproximation vs precision**: Allowing arbitrary tokens in infilling regions may accept proposals that hit token limits later. Exact token-counting is theoretically possible but makes the regular language state-space explode.
  - **Syntax vs functional correctness**: The method guarantees syntax but functional correctness depends on the base model; constraints can slightly improve functional scores by eliminating syntax dead-ends.
  - **Rejection sampling vs token masking**: The paper uses rejection (accept/reject proposals); alternative is token masking (block invalid tokens upfront), which may be faster but requires different integration.

- Failure signatures:
  1. **Timeout with partial output**: Model exhausts rejection budget (100 proposals) or token limit (256) before completion. Mitigation: Fall back to sampling from intersection language.
  2. **High overhead on small models**: Constrained decoding overhead is higher on smaller models (320% on 1.3B vs 20% on 33B) because base inference is faster, making constraint checking relatively expensive.
  3. **Remaining syntax errors in Con− mode**: Overapproximation allows proposals that seem completable but exceed practical token limits. Mitigation: Use Con mode with intersection-language sampling.

- First 3 experiments:
  1. **Baseline validation**: Run unconstrained generation on C++ FIM task with your target DLM. Measure syntactic correctness rate. This establishes the gap the constraint system must fill.
  2. **Ablate grammar complexity**: Test with a minimal CFG (e.g., balanced parentheses only) vs full C++ grammar. Measure overhead difference to understand where optimization efforts should focus.
  3. **Stress test lexing**: Create inputs with many ambiguous token boundaries (e.g., "123 456" where digits could merge). Verify the union NFA construction doesn't explode and measure the number of lexeme sequences considered.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the method enforce strict token limits without exponentially increasing the size of the intersection language?
- Basis in paper: [explicit] Appendix E notes that modeling the remaining number of tokens increases the regular language size drastically, rendering the method too expensive for practical application.
- Why unresolved: The current approach overapproximates by allowing arbitrary token counts in infilling regions, which causes syntax errors when the model runs out of generation capacity (e.g., mask tokens).
- What evidence would resolve it: An algorithm that incorporates token counting into the regular language construction while maintaining practical inference speeds.

### Open Question 2
- Question: Can incremental parsing techniques significantly reduce the observed 67%–205% runtime overhead in multi-region infilling?
- Basis in paper: [inferred] The Discussion suggests that leveraging incremental parsing (reusing previous intersection results) could decrease the worst-case and practical overhead of the constraining method.
- Why unresolved: The current implementation re-computes significant portions of the intersection check for each token, leading to high latency relative to unconstrained decoding.
- What evidence would resolve it: A comparative benchmark showing reduced latency per token when reusing parse states from previous diffusion steps.

### Open Question 3
- Question: Can the constrained decoding framework be effectively extended to context-sensitive language features?
- Basis in paper: [inferred] Appendix E identifies handling more powerful language classes (like context-sensitive languages) as an interesting future direction, potentially via integration with type systems.
- Why unresolved: The current theoretical reduction relies on context-free grammars; context-sensitive features fall outside this scope and require different decision procedures.
- What evidence would resolve it: A successful application of the method to semantic constraints (e.g., variable scopes) or type systems using "typed holes."

## Limitations
- Scalability uncertainty for very large grammars beyond tested examples
- Implementation sensitivity particularly in lexer handling of partial tokens
- Effectiveness on non-DLM architectures remains unclear

## Confidence
**High Confidence**: The formal language reduction claim (CFG ∩ Regular Language) and the basic intersection algorithm structure. These follow established formal language theory and are mathematically sound. The experimental results showing dramatic improvements in syntactic correctness (99.7% for C++, 97.4% for JSON) provide strong empirical validation.

**Medium Confidence**: The practical implementation details, particularly the implicit enumeration optimization and lexer handling of partial tokens. While the algorithm design is sound, implementation choices significantly impact performance and correctness. The reported inference overhead (median 100%, max 320%) suggests implementation sensitivity.

**Low Confidence**: Generalization to arbitrary CFGs beyond the tested examples, and performance on larger-scale grammars or different model families. The paper tests specific, relatively constrained grammars and diffusion models, leaving questions about broader applicability.

## Next Checks
1. **Scalability Stress Test**: Implement the constrained decoding system and test it on progressively larger grammars (starting from simple balanced parentheses, then arithmetic expressions, then subsets of Python/JavaScript). Measure the intersection check time as grammar size increases and identify the practical scalability limits.

2. **Tokenization Edge Cases**: Create adversarial test cases with ambiguous token boundaries, particularly where tokens span infilling regions or where multiple tokenization schemes could apply. Verify that the lexer correctly handles all cases and that the union NFA construction remains tractable.

3. **Alternative Architecture Validation**: Adapt the constrained decoding approach for a non-DLM architecture (such as MaskGIT or an encoder-decoder model). Evaluate whether the CFG-constrained generation still provides similar syntactic correctness improvements, or if architecture-specific modifications are needed.