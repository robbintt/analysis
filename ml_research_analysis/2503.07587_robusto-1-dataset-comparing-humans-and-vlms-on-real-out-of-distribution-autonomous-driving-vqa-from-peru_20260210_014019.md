---
ver: rpa2
title: 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous
  Driving VQA from Peru'
arxiv_id: '2503.07587'
source_url: https://arxiv.org/abs/2503.07587
tags:
- questions
- each
- driving
- vlms
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the Robusto-1 dataset to evaluate the cognitive
  alignment of vision-language models (VLMs) and humans on out-of-distribution driving
  scenarios from Peru. Using Visual Question Answering (VQA) and Representational
  Similarity Analysis (RSA), the researchers compared human and machine responses
  to 105 questions across 7 video clips.
---

# Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution Autonomous Driving VQA from Peru

## Quick Facts
- **arXiv ID**: 2503.07587
- **Source URL**: https://arxiv.org/abs/2503.07587
- **Reference count**: 40
- **Primary result**: VLMs show high internal similarity but diverge from human responses on counterfactual driving scenarios in Peru

## Executive Summary
This study introduces the Robusto-1 dataset to evaluate cognitive alignment between vision-language models (VLMs) and humans on out-of-distribution autonomous driving scenarios from Peru. Using Visual Question Answering (VQA) and Representational Similarity Analysis (RSA), the researchers compared human and machine responses to 105 questions across 7 video clips. The dataset specifically targets real-world driving scenarios that challenge current VLM capabilities, particularly in counterfactual and hypothetical reasoning situations.

The findings reveal that while VLMs demonstrate high internal consistency across different question types, they significantly diverge from human responses, especially on counterfactual and hypothetical questions. Cross-system analysis shows only mild alignment between VLMs and humans, with better alignment in multiple-choice questions but notable differences in open-ended and complex reasoning tasks. These results highlight the limitations of current VLMs in handling nuanced, real-world driving scenarios and emphasize the need for further research into their cognitive alignment with human drivers.

## Method Summary
The study employed a comprehensive methodology combining Visual Question Answering with Representational Similarity Analysis to compare human and VLM performance on autonomous driving tasks. Researchers curated 105 questions across 7 video clips capturing Peruvian driving scenarios, designed to test various cognitive capabilities including counterfactual reasoning and hypothetical situations. The dataset was specifically constructed to represent out-of-distribution conditions that challenge current VLM capabilities. Human responses were collected and compared against VLM outputs using RSA to quantify similarity patterns. The analysis examined both within-system consistency (comparing VLMs to themselves across question types) and cross-system alignment (comparing VLMs to human responses). Multiple question formats including multiple-choice and open-ended responses were utilized to assess different aspects of cognitive alignment.

## Key Results
- VLMs exhibit high internal similarity regardless of question type, showing consistent response patterns across different cognitive demands
- Humans diverge significantly from VLMs on counterfactual and hypothetical questions, demonstrating distinct reasoning patterns
- Cross-system analysis reveals mild alignment between VLMs and humans in multiple-choice questions, but notable differences in open-ended and complex reasoning tasks

## Why This Works (Mechanism)
The study's effectiveness stems from its use of Representational Similarity Analysis (RSA) to quantify cognitive alignment between human and machine reasoning patterns. By comparing response structures rather than just final answers, RSA captures the underlying similarity in how VLMs and humans process visual information and generate responses. The out-of-distribution nature of Peruvian driving scenarios creates a challenging test environment that exposes limitations in current VLM training approaches. The combination of multiple question formats (multiple-choice and open-ended) allows for nuanced assessment of different cognitive capabilities, while the focus on counterfactual and hypothetical scenarios specifically targets higher-order reasoning abilities that distinguish human cognition from current VLM performance.

## Foundational Learning
- **Representational Similarity Analysis (RSA)**: A method for quantifying similarity between response patterns; needed to compare cognitive processes beyond surface-level answers; quick check: compare correlation matrices of human vs VLM responses
- **Visual Question Answering (VQA)**: Task requiring models to answer questions about visual content; needed as the primary evaluation framework for driving scenarios; quick check: ensure questions cover diverse visual elements and reasoning types
- **Out-of-distribution testing**: Evaluating models on data different from training distribution; needed to assess real-world generalization; quick check: verify Peruvian scenarios differ systematically from typical VLM training data
- **Counterfactual reasoning**: Ability to reason about hypothetical scenarios; needed to test advanced cognitive capabilities; quick check: include questions asking "what if" scenarios
- **Cross-cultural cognitive assessment**: Evaluating models across different cultural contexts; needed to ensure global applicability; quick check: compare performance across different geographic driving datasets

## Architecture Onboarding
**Component Map**: Video Input -> Question Processing -> Visual Feature Extraction -> Reasoning Module -> Response Generation -> RSA Comparison
**Critical Path**: The most critical path is Video Input -> Visual Feature Extraction -> Reasoning Module, as these components directly impact the model's ability to understand and reason about driving scenarios
**Design Tradeoffs**: The study prioritizes real-world complexity over controlled laboratory conditions, sacrificing some experimental control for ecological validity. This tradeoff enables assessment of true VLM capabilities but introduces more variability in responses.
**Failure Signatures**: VLMs typically fail on counterfactual reasoning, show high internal consistency but low alignment with humans, and struggle with open-ended questions requiring nuanced judgment. These failures manifest as systematic patterns in RSA analysis rather than random errors.
**First Experiments**:
1. Test VLM performance on standard driving datasets before applying Robusto-1 to establish baseline capabilities
2. Conduct ablation studies removing specific visual features to identify critical components for driving scenario understanding
3. Compare multiple VLM architectures on Robusto-1 to identify which approaches show better cognitive alignment with humans

## Open Questions the Paper Calls Out
None

## Limitations
- Sample size constraints with only 105 questions across 7 video clips may limit generalizability to broader driving scenarios
- Cross-cultural validity concerns as results are based on Peruvian driving conditions which may not translate to other cultural or regulatory environments
- Model selection bias since the study doesn't specify which VLMs were tested, potentially limiting reproducibility and understanding of architectural differences

## Confidence
- **High Confidence**: VLMs show high internal similarity regardless of question type is well-supported by RSA methodology
- **Medium Confidence**: Divergence between human and VLM responses on counterfactual questions is observed but may benefit from additional contextual validation
- **Medium Confidence**: Mild alignment between VLMs and humans in multiple-choice questions is supported but requires larger-scale testing for stronger validation

## Next Checks
1. Expand the dataset to include a more diverse range of driving scenarios across multiple countries and cultures to test generalizability
2. Test a broader range of VLM architectures and sizes to identify which models show better cognitive alignment with human drivers
3. Conduct longitudinal studies to assess how VLM performance evolves as models are trained on increasingly diverse datasets