---
ver: rpa2
title: Probabilistic Joint Recovery Method for CO$_2$ Plume Monitoring
arxiv_id: '2501.18761'
source_url: https://arxiv.org/abs/2501.18761
tags:
- surveys
- time-lapse
- pjrm
- uncertainty
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of monitoring CO\u2082 plumes\
  \ in Carbon Capture and Storage (CCS) projects, where accurate imaging is critical\
  \ for risk assessment but existing methods lack uncertainty quantification. The\
  \ authors propose the Probabilistic Joint Recovery Method (pJRM), which extends\
  \ the Joint Recovery Method by computing posterior distributions across multiple\
  \ time-lapse surveys using a shared generative model."
---

# Probabilistic Joint Recovery Method for CO$_2$ Plume Monitoring

## Quick Facts
- arXiv ID: 2501.18761
- Source URL: https://arxiv.org/abs/2501.18761
- Reference count: 9
- Key outcome: pJRM provides improved time-lapse imaging and principled uncertainty quantification for CCS monitoring by jointly inverting multiple surveys through a shared generative model

## Executive Summary
This paper addresses the critical need for accurate CO₂ plume monitoring in Carbon Capture and Storage projects, where traditional methods lack uncertainty quantification. The authors propose the Probabilistic Joint Recovery Method (pJRM), which extends the Joint Recovery Method by computing posterior distributions across multiple time-lapse surveys using a shared generative model. The method jointly inverts N seismic surveys by encoding each with a distinct latent distribution while sharing a common generative model to extract common features, providing both improved reconstruction and uncertainty estimates.

The synthetic case study demonstrates that pJRM significantly outperforms Probabilistic Independent Recovery Method (pIRM) in capturing CO₂ plume structure with lower uncertainty. Results show that increasing the number of surveys further improves reconstruction quality, with uncertainty maps and error comparisons demonstrating decreasing uncertainty trends as more surveys are incorporated. The method includes a computationally efficient weak formulation that reduces expensive forward operator evaluations, making it a more reliable tool for risk assessment in subsurface carbon storage projects.

## Method Summary
pJRM jointly inverts multiple time-lapse seismic surveys by encoding each survey with a distinct latent distribution while sharing a common generative model. The method uses variational inference to learn approximate posteriors for each survey's subsurface image, then decodes these through a shared generator to produce samples from the posterior distribution. The time-lapse difference between surveys reveals the CO₂ plume evolution, while standard deviation across samples provides uncertainty maps. A weak formulation decouples forward operator evaluations from network updates, dramatically reducing computational cost.

## Key Results
- pJRM significantly outperforms pIRM in reconstruction quality for both two and six survey cases
- Uncertainty maps show decreasing uncertainty trends as more surveys are incorporated
- Correlation between uncertainty and reconstruction error validates the probabilistic framework
- Weak formulation reduces forward operator evaluations from ~100,000 to ~200 while maintaining solution quality

## Why This Works (Mechanism)

### Mechanism 1: Shared Generative Model for Cross-Survey Feature Extraction
The Shared Generative Model (SGM) learns common background features present across all N surveys (unchanged reservoir regions), while survey-specific latent distributions encode the time-varying CO₂ plume dynamics. This decomposition allows the model to separate signal (plume evolution) from shared structure (static geology). Core assumption: large regions of the reservoir remain unchanged between surveys; only the CO₂ plume region exhibits temporal variation.

### Mechanism 2: Probabilistic Latent Encoding for Uncertainty Quantification
Each survey's latent code zᵢ ~ N(0,I) is transformed via a learned Gaussian Mixture Model q_φᵢ, then decoded through G_θ to produce posterior samples. Standard deviation across samples provides pixel-wise uncertainty maps. Core assumption: the learned latent space captures the full posterior distribution; GMM encoding is sufficiently expressive for the true posterior.

### Mechanism 3: Weak Formulation for Computationally Efficient Inversion
An outer loop (~200 iterations) computes gradients involving expensive forward operators Aᵢ. An inner loop (~100,000 iterations) updates network parameters θ and φ without re-evaluating Aᵢ. This amortizes forward operator cost across many network updates. Core assumption: the solution xᵢ evolves slowly enough that cached gradient information remains useful across inner-loop iterations.

## Foundational Learning

- **Variational Inference with Amortized Posterior Encoding**: pJRM learns q_φ (the approximate posterior) to sample from the distribution of possible plume configurations given seismic data. Without this, you cannot quantify uncertainty. Quick check: Can you explain why sampling z ~ N(0,I) and transforming through q_φ produces samples from an approximate posterior rather than a prior?

- **Seismic Forward Modeling (Post-Stack Convolution)**: Understanding Aᵢx = y is essential—the forward operator maps subsurface acoustic properties to measured seismic data. The inversion problem solves for x given y and A. Quick check: Given that A is ill-posed (many x values produce similar y), why does the shared generative model help constrain the solution?

- **Time-Lapse Seismic Monitoring**: The fundamental premise is that differences between surveys reveal CO₂ plume migration. Understanding baseline vs. monitor surveys clarifies why joint inversion makes sense. Quick check: Why would independently inverting each survey produce higher uncertainty than jointly inverting all surveys together?

## Architecture Onboarding

- **Component map**: Input seismic surveys {y₁, y₂, ..., y_N} -> N Gaussian Mixture Models q_φᵢ -> Shared Generator G_θ -> N posterior distributions over subsurface images

- **Critical path**: 
  1. Initialize G_θ (untrained network)
  2. Outer loop: Compute gradients gᵢ = ∇_xᵢ [data misfit + regularization]
  3. Update xᵢ ← xᵢ - τgᵢ
  4. Inner loop: Update (φ, θ) to minimize ||x_j - G_θ(q_φⱼ(zⱼ))||²
  5. Sample posteriors and compute time-lapse difference + uncertainty maps

- **Design tradeoffs**: 
  - More surveys → better shared features but higher memory/coordination cost
  - Weak formulation → fewer forward evaluations but potential gradient staleness
  - Linear vs. nonlinear forward operator → current linear proof-of-concept is faster; FWI extension needed for field deployment
  - GMM expressiveness vs. overfitting risk in latent encoding

- **Failure signatures**:
  - High uncertainty overlapping plume region → pIRM behavior (check if SGM is actually sharing features)
  - No convergence in outer loop → forward operator may be poorly conditioned; consider preconditioning
  - Uncertainty maps uncorrelated with error → latent space may not capture true posterior structure

- **First 3 experiments**:
  1. Replicate synthetic case with N=2 surveys using provided Pylops forward operator; verify reconstruction matches Figure 2c qualitatively
  2. Ablation: Replace shared G_θ with independent generators per survey; confirm performance degrades toward pIRM baseline
  3. Scale test: Increase to N=6 surveys; verify uncertainty decreases and correlates with reconstruction error per Figure 3 trend

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations

- **Linear Forward Operator Constraint**: Current proof-of-concept uses linear post-stack convolution; FWI extension requires significant additional validation
- **Shared-Structure Assumption**: Method assumes large unchanged reservoir regions across surveys; overburden changes or complex plume geometries could violate this assumption
- **Computational Scaling**: While weak formulation reduces forward operator evaluations, inner-loop parameter updates (~100,000 iterations) remain computationally intensive for field-scale problems

## Confidence

- **pJRM outperforms pIRM in reconstruction quality**: High confidence (supported by quantitative error metrics and uncertainty maps)
- **Uncertainty decreases with more surveys**: High confidence (demonstrated correlation in synthetic case)
- **Weak formulation provides computational efficiency**: Medium confidence (theoretical justification strong, but real-world scaling untested)
- **Method generalizes to FWI**: Low confidence (architecture designed for extension, but not validated)

## Next Checks

1. **Field Data Validation**: Apply pJRM to real field survey data with known CO₂ plume locations to verify performance on noisy, complex data with acquisition variability

2. **Acquisition Sensitivity Analysis**: Systematically vary survey parameters (geometry, frequency content, noise levels) to quantify method robustness to realistic monitoring constraints

3. **Computational Scaling Benchmark**: Test method on progressively larger 3D problems to identify practical limits and optimize inner-loop iterations for field deployment