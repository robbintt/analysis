---
ver: rpa2
title: 'Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large
  Language Model-based Query Expansion'
arxiv_id: '2506.04760'
source_url: https://arxiv.org/abs/2506.04760
tags:
- query
- retrieval
- sparse
- exp4fuse
- retrievers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Exp4Fuse, a rank fusion framework that uses\
  \ zero-shot LLM-based query expansion to enhance sparse retrieval performance, particularly\
  \ for learned sparse retrievers. The framework operates by generating two ranked\
  \ document lists\u2014one from the original query and one from an LLM-augmented\
  \ query\u2014and fusing them using a modified reciprocal rank fusion method."
---

# Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion

## Quick Facts
- arXiv ID: 2506.04760
- Source URL: https://arxiv.org/abs/2506.04760
- Authors: Lingyuan Liu; Mengxiang Zhang
- Reference count: 40
- Exp4Fuse is a rank fusion framework using zero-shot LLM-based query expansion to enhance sparse retrieval performance, particularly for learned sparse retrievers.

## Executive Summary
This paper introduces Exp4Fuse, a rank fusion framework that uses zero-shot LLM-based query expansion to enhance sparse retrieval performance, particularly for learned sparse retrievers. The framework operates by generating two ranked document lists—one from the original query and one from an LLM-augmented query—and fusing them using a modified reciprocal rank fusion method. Experiments on three MS MARCO-related datasets and seven low-resource datasets demonstrate that Exp4Fuse outperforms existing LLM-based query expansion methods in enhancing sparse retrievers. When combined with advanced sparse retrievers, Exp4Fuse achieves state-of-the-art results on several benchmarks, highlighting its superior performance and effectiveness in improving query expansion for sparse retrieval.

## Method Summary
Exp4Fuse enhances sparse retrieval by fusing results from two parallel routes: an original query route and an LLM-augmented route. The LLM generates a "hypothetical document" based on the original query, which is then concatenated with the repeated original query (weighted by λ) to form an expanded query. Both the original and expanded queries are passed through a sparse retriever to generate ranked document lists, which are then fused using a modified reciprocal rank fusion method that accounts for document occurrence in both lists.

## Key Results
- Exp4Fuse outperforms existing LLM-based query expansion methods in enhancing sparse retrievers on MS MARCO-related and low-resource datasets
- The framework achieves state-of-the-art results when combined with advanced sparse retrievers
- Learned sparse retrievers benefit more from Exp4Fuse's indirect fusion approach compared to basic sparse retrievers like BM25

## Why This Works (Mechanism)

### Mechanism 1: Robustness via Dual-Route Fusion
- **Claim:** Fusing an original query route with an LLM-augmented route improves sparse retrieval robustness by mitigating the risk of noisy generation.
- **Mechanism:** The framework generates two independent ranked lists ($I_{oq}$ and $I_{eq}$). The original route preserves the high precision of the sparse retriever on the exact user intent. The augmented route introduces semantic expansion. By using a modified Reciprocal Rank Fusion (RRF), the system prioritizes documents that appear in both lists while allowing strong candidates from either single list to surface. This prevents the "lexical mismatch" or "unreliable text" issues of LLMs from single-handedly degrading retrieval quality.
- **Core assumption:** The errors in the LLM-generated hypothetical document (irrelevant terms) do not perfectly overlap with the relevant documents retrieved by the original query; the fusion algorithm can successfully down-weight noise while up-weighting shared relevance.
- **Evidence anchors:**
  - [abstract]: "Exp4Fuse operates by simultaneously considering two retrieval routes... fuses them using a modified reciprocal rank fusion method."
  - [section 5 (Ablation)]: "Using zero-shot LLM-based QE directly... can sometimes negatively impact performance... highlighting the limitations of directly using zero-shot LLM-based QE."
  - [corpus]: The neighbor paper "LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries" confirms that generation errors are a primary failure mode in single-route QE, validating the need for a fusion safety net.
- **Break condition:** If the LLM generates a hypothetical document that is semantically contradictory to the original query, the fusion weights ($w_i$) may be insufficient to prevent ranking degradation compared to the original query alone.

### Mechanism 2: Frequency Balancing via Query Weighting
- **Claim:** Repeating the original query in the augmented route ($\lambda$ weighting) restores the term frequency balance between the short user query and the long LLM document.
- **Mechanism:** Sparse retrievers (like BM25) rely heavily on term frequency (TF). A short query appended with a long LLM document results in query terms having low relative frequency compared to the generated text. By repeating the original query $\lambda$ times (default $\lambda=5$), the mechanism artificially boosts the TF weight of the user's specific keywords, ensuring they drive the retrieval logic rather than being drowned out by the generative text.
- **Core assumption:** The sparse retriever treats the concatenated input as a single "bag of words" where term frequency dictates importance, and the optimal balance is a linear function of repetition.
- **Evidence anchors:**
  - [section 3.1]: "A simple combination... might not be effective for sparse retrieval because it could imbalance the influence... We enhance the query by repeating the original query $\lambda$ times."
  - [section 4.1]: "Unless specified otherwise, Exp4Fuse uses $\lambda = 5$."
  - [corpus]: No direct corpus evidence contradicts this, but standard sparse retrieval theory (TF-IDF) supports the need for frequency normalization.
- **Break condition:** If the query is already long or the LLM document is extremely short, a fixed $\lambda=5$ might over-weight the query, potentially re-introducing the vocabulary mismatch problem the expansion was meant to solve.

### Mechanism 3: Adaptive Rank Aggregation
- **Claim:** Modifying the RRF score to include an adaptive weight ($w_i$) and an occurrence count ($n$) better captures document relevance than standard fusion.
- **Mechanism:** Standard RRF relies purely on rank position ($1/(k+r)$). The modified formula ($F_{Rscore}$) adds a multiplier based on $n$ (presence in both lists). This amplifies the score of documents that are retrieved via both the exact keywords (Route 1) and the semantic expansion (Route 2), treating them as high-confidence signals.
- **Core assumption:** A document appearing in both the exact-match list and the semantic-match list is significantly more relevant than a document appearing at a similar rank in only one list.
- **Evidence anchors:**
  - [section 3.2]: "This adjustment ensures that documents retrieved by both routes are more likely to be included in the final ranked list."
  - [section 5]: Ablation studies show that fusing "Original + Hypothetical" yields higher nDCG@10 (e.g., 77.6 vs 73.1) than either route individually, validating the synergy of the adaptive fusion.
- **Break condition:** If the two retrieval routes are highly correlated (retrieving the exact same documents), the $n$ factor provides diminishing returns, and the overhead of the second route becomes waste.

## Foundational Learning

- **Concept: Sparse vs. Learned Sparse Retrieval**
  - **Why needed here:** The paper claims Exp4Fuse specifically benefits "learned sparse retrievers" (like SPLADE) more than basic ones (BM25). You must understand that learned sparse retrievers map terms to contextualized weights, whereas BM25 uses raw term frequencies.
  - **Quick check question:** How does SPLADE differ from BM25 in representing a document's vocabulary?

- **Concept: Reciprocal Rank Fusion (RRF)**
  - **Why needed here:** This is the mathematical glue of the paper. Understanding RRF is necessary to see why the authors modified it (adding $w_i$ and $n$) to handle the specific nature of LLM noise.
  - **Quick check question:** Why does RRF use the reciprocal of the rank ($1/k$) rather than the raw rank or score?

- **Concept: Query Expansion (QE)**
  - **Why needed here:** The core premise is that LLMs generate "hypothetical documents" to solve the "vocabulary mismatch problem." You need to distinguish between *pseudo-relevance feedback* (using top results) and *LLM-based expansion* (using generation).
  - **Quick check question:** What is the primary risk of using an LLM to generate a document for query expansion (hint: see "Limitations")?

## Architecture Onboarding

- **Component map:**
  1. **Input:** User Query ($q_o$).
  2. **Splitter:** Parallel execution paths.
  3. **Path A (Original):** $q_o \rightarrow$ Sparse Retriever $\rightarrow$ Ranked List $I_{oq}$.
  4. **Path B (Expansion):** $q_o \rightarrow$ LLM Prompter $\rightarrow$ Hypothetical Doc ($r_q$) $\rightarrow$ Concatenator ($q_o \times \lambda + r_q$) $\rightarrow$ Sparse Retriever $\rightarrow$ Ranked List $I_{eq}$.
  5. **Fusion Engine:** Modified RRF Aggregator ($I_{oq}, I_{eq} \rightarrow I_{fq}$).

- **Critical path:** The **LLM Prompter** and **Concatenator**. If the prompt generates off-topic text or $\lambda$ is misconfigured, the Sparse Retriever in Path B will produce a noisy list that degrades the final fusion.

- **Design tradeoffs:**
  - *Latency vs. Quality:* The system requires 1 LLM call and 2 Sparse Retrieval passes per query. This is cheaper than dense retrieval but slower than single-pass BM25.
  - *Zero-shot vs. Few-shot:* The authors chose "zero-shot" (simple prompts) to minimize cost and complexity, trading off the potential peak performance of complex "Chain-of-Thought" prompting used in baselines like LameR.

- **Failure signatures:**
  - *Hallucination Dominance:* If the LLM invents specific entities (e.g., dates, names) not in the corpus, Path B retrieves empty or irrelevant results.
  - *Correlation:* If Path B retrieves identical results to Path A, the fusion adds computational cost with zero gain.
  - *Degradation:* If the original sparse retriever is already "perfect" (SOTA on the specific dataset), adding noisy expansion via Path B can sometimes hurt metrics like MRR unless the fusion weights are tuned heavily toward the original list.

- **First 3 experiments:**
  1. **Route Ablation:** Run retrieval using only Path A, only Path B, and then the Fusion. Confirm that Fusion > Max(A, B).
  2. **Lambda ($\lambda$) Sensitivity:** Sweep $\lambda$ (e.g., 1 to 10) on a validation set to find the optimal balance between query weight and expansion text weight.
  3. **Retriever Compatibility:** Test Exp4Fuse on a basic BM25 vs. a learned retriever (SPLADE) to verify the paper's claim that learned retrievers benefit more from the specific "indirect" fusion approach.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks explicit discussion of how LLMs' propensity for hallucination and invented details might contaminate the hypothetical document and degrade sparse retriever output. While the dual-route fusion offers some protection, no error analysis quantifies this risk or evaluates hallucination detection methods.
- The choice of $\lambda=5$ is presented without rigorous justification beyond empirical tuning. It is unclear whether this fixed value is optimal across diverse query lengths, domains, or retriever types.
- The framework's effectiveness is primarily demonstrated on MS MARCO and similar datasets. There is no validation on truly out-of-domain or multilingual corpora, leaving open questions about robustness to linguistic and topical diversity.

## Confidence
- **High:** The dual-route fusion architecture and modified RRF formulation are well-motivated and empirically validated on multiple datasets.
- **Medium:** The claim that learned sparse retrievers benefit more than BM25 is supported but could be more thoroughly explained.
- **Low:** The fixed lambda hyperparameter and the handling of LLM hallucinations lack robust theoretical or empirical grounding.

## Next Checks
1. Conduct a systematic error analysis isolating cases where the LLM-generated hypothetical document introduces hallucinated facts or irrelevant terms, and measure the impact on the fused ranking.
2. Perform a cross-domain evaluation using datasets from different domains (e.g., legal, biomedical, multilingual) to test the framework's generalization and robustness to vocabulary mismatch.
3. Experiment with adaptive or query-dependent lambda values, or integrate lightweight hallucination detection to dynamically adjust or filter the LLM-generated expansion before fusion.