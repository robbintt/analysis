---
ver: rpa2
title: 'RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly
  on Robotic Platforms'
arxiv_id: '2509.06714'
source_url: https://arxiv.org/abs/2509.06714
tags:
- inference
- delays
- control
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of deploying reinforcement learning
  directly on real robots, where two main constraints exist: limited training data
  due to costly real-world data collection, and real-time execution constraints due
  to inference delays in model-based methods. The authors propose a general framework
  to handle inference delays using a d-step MPC approach that provides sequences of
  actions to avoid execution gaps.'
---

# RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms

## Quick Facts
- arXiv ID: 2509.06714
- Source URL: https://arxiv.org/abs/2509.06714
- Authors: Zakariae El Asri; Ibrahim Laiche; Clément Rambour; Olivier Sigaud; Nicolas Thome
- Reference count: 19
- Primary result: RT-HCP achieves successful performance in 60k training steps (20 minutes) on FURUTA pendulum, outperforming baselines requiring 100k-160k steps

## Executive Summary
This paper addresses the dual challenges of deploying reinforcement learning directly on real robots: limited training data due to costly real-world data collection and real-time execution constraints due to inference delays in model-based methods. The authors propose RT-HCP, a Real-Time Hybrid Control with Physics-informed model that combines model-based RL, model-free RL, and prior dynamics knowledge. Their approach uses a d-step MPC framework that provides sequences of actions to avoid execution gaps when inference time exceeds the system's sampling period. Experiments on a FURUTA pendulum demonstrate that RT-HCP outperforms other methods, reaching successful performance in 60k training steps (20 minutes) compared to 100k-160k for baselines, while maintaining better stability and control performance under real-time constraints.

## Method Summary
RT-HCP addresses real-time RL constraints through a d-step MPC framework that outputs sequences of actions when inference time exceeds the sampling period. The method combines a physics-informed dynamics model (Euler-Lagrange prior plus learned residual) with hybrid planning that uses both CEM and learned policy/Q-function components. The system executes buffered actions while computing new ones, avoiding execution gaps. RT-HCP learns from both real transitions and imagined rollouts, achieving sample efficiency through the physics prior while maintaining real-time performance through reduced inference time via hybrid planning.

## Key Results
- Achieves successful performance in 60k training steps (20 minutes) on FURUTA pendulum
- Outperforms RT-TDMPC (100k steps) and TD-MPC (160k steps) baselines
- Reduces inference time from 47ms to 36ms through hybrid planning
- Maintains better stability and control performance under real-time constraints

## Why This Works (Mechanism)

### Mechanism 1: d-step MPC with State-Action Augmentation
Planning sequences of actions instead of single actions allows real-time control when inference time exceeds the system's sampling period. When inference requires d timesteps, the controller outputs d actions. The system executes buffered actions while computing new ones, avoiding execution gaps. The state is augmented with the d−1 missed states and pending actions, restoring the Markov property by giving the agent complete information about system dynamics between decision points.

### Mechanism 2: Hybrid Planning Reduces Inference Time
Incorporating a learned policy and Q-function into CEM planning reduces required iterations and population size, thereby reducing inference delay. The learned policy provides informative action candidates (50 of 500 samples from π), while the Q-function enables shorter planning horizons by estimating long-term returns beyond the immediate reward horizon. This reduces both the CEM population size and iterations needed for convergence.

### Mechanism 3: Physics-Informed Model Reduces Compounding Errors
A hybrid model combining analytical physics priors with learned residuals provides more accurate trajectory predictions than purely data-driven models. The analytical model (Euler-Lagrange equations) captures known pendulum dynamics across the entire state space. A residual neural network compensates for unmodeled effects (friction, encoder cable forces). This reduces prediction errors during d-step open-loop execution when replanning is delayed.

## Foundational Learning

- **Concept: Model Predictive Control (MPC) with Cross-Entropy Method (CEM)**
  - **Why needed here**: RT-HCP uses CEM-based MPC as its planning backbone. Understanding how CEM samples and optimizes action sequences is essential for tuning population size and iterations.
  - **Quick check question**: Can you explain why CEM uses elite samples to update its sampling distribution, and how this differs from gradient-based optimization?

- **Concept: Markov Decision Processes and State Augmentation**
  - **Why needed here**: The delay-MDP framework augments states to restore Markov property under inference delays. Understanding why this is necessary requires grasping partial observability from missed states.
  - **Quick check question**: If an agent only observes every 3rd state due to inference delay, what information is lost, and how does augmenting the state help?

- **Concept: Residual Learning with Physics Priors**
  - **Why needed here**: RT-HCP combines analytical models with neural networks. Understanding when to trust physics vs. learned corrections is critical for debugging.
  - **Quick check question**: If your physics model predicts pendulum acceleration but ignores friction, what would you expect the residual network to learn?

## Architecture Onboarding

- **Component map**: Physics Prior -> Residual Network -> Hybrid Planner (CEM + Policy/Q) -> Action Buffer -> Actor-Critic -> Data Buffers (D_real, D_im)

- **Critical path**: Measure inference time → Set H_p ensuring T_i < H_p·Δt → Set H_e_min = int(T_i/Δt) + 1 → Execute d-step MPC with d=H_e

- **Design tradeoffs**:
  - Longer H_p: Better planning but higher inference time → larger d → more open-loop error accumulation
  - More CEM iterations: Better action optimization but higher inference delay
  - Larger residual network: Better unmodeled compensation but slower inference
  - More imagined data: Better sample efficiency but risk of model bias

- **Failure signatures**:
  - Execution gaps / jerky motion: H_e too small for actual inference time; measure T_i empirically
  - Divergent predictions: Physics prior parameters wrong or residual network overfitting; validate on held-out trajectories
  - Slow learning: Policy providing poor CEM candidates early; consider pretraining or warm-starting
  - Instability after swing-up: Q-function inaccurate at upright state; examine value estimates near goal

- **First 3 experiments**:
  1. Characterize inference delay: Measure T_i on target hardware for H_p ∈ {3, 5, 10, 15} with CEM parameters; verify sublinear growth assumption
  2. Validate physics prior accuracy: Compare analytical model predictions vs. ground truth trajectories; identify dominant unmodeled effects
  3. Ablate hybrid components: Test (a) physics-only vs. physics+residual, (b) with vs. without policy-guided CEM, (c) with vs. without Q-function terminal value; isolate contribution of each component

## Open Questions the Paper Calls Out

- **Open Question 1**: How can RT-HCP be extended to handle high-dimensional visual observations while maintaining real-time performance and sample efficiency?
  - **Basis in paper**: The authors state "In the near future, we intend to extend our work to the case where the input of the controller is an image, leveraging recent progress in vision-based RL."
  - **Why unresolved**: The current approach assumes direct state access; visual inputs would require learning state representations, potentially increasing both inference time and sample complexity under real-time constraints.
  - **What evidence would resolve it**: Experimental validation of RT-HCP with image inputs on robotic platforms, demonstrating both sample efficiency and acceptable inference delays.

- **Open Question 2**: Will RT-HCP maintain its sample efficiency and real-time performance advantages on more complex, higher-dimensional robotic systems?
  - **Basis in paper**: The paper validates only on a FURUTA pendulum, described as "simple but high frequency" with a 4-dimensional state space.
  - **Why unresolved**: Complex systems may exhibit longer inference times, more intricate dynamics harder to capture with physics-informed models, and fundamentally different delay characteristics.
  - **What evidence would resolve it**: Evaluation on complex platforms (e.g., humanoid robots, dexterous manipulation) with higher-dimensional state and action spaces.

## Limitations

- Physical pendulum parameters and reward function remain unspecified, requiring careful system identification before faithful reproduction
- No ablation of inference delay vs sample efficiency effects; the 60k step advantage could be partially attributed to better data efficiency from physics prior rather than delay handling alone
- Only tested on a single pendulum system; generalization to higher-dimensional or more complex robots is unproven

## Confidence

- **High**: The d-step MPC mechanism for handling inference delays (basic algorithmic structure is sound)
- **Medium**: The physics-informed model improving prediction accuracy (empirically supported but mechanism partially assumed)
- **Medium**: The hybrid planning reducing inference time (supported by timing tables but relative contribution unclear)

## Next Checks

1. Conduct controlled ablation: run RT-HCP with H_e=1 (no delay handling) vs d-step MPC to isolate delay-handling benefit
2. Test on a second platform (e.g., cartpole or 2-link arm) to assess generalization beyond FURUTA pendulum
3. Measure sensitivity to physics prior accuracy by perturbing parameters and quantifying performance degradation