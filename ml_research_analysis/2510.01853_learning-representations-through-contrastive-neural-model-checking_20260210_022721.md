---
ver: rpa2
title: Learning Representations Through Contrastive Neural Model Checking
arxiv_id: '2510.01853'
source_url: https://arxiv.org/abs/2510.01853
tags:
- learning
- checking
- representations
- circuit
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CNML, a contrastive learning framework for
  learning aligned representations of LTL specifications and AIGER circuits. The key
  innovation is using the model checking task as a contrastive learning objective,
  where circuits and specifications that satisfy each other are brought closer in
  embedding space.
---

# Learning Representations Through Contrastive Neural Model Checking

## Quick Facts
- **arXiv ID:** 2510.01853
- **Source URL:** https://arxiv.org/abs/2510.01853
- **Reference count:** 40
- **Primary result:** CNML framework achieves 27.3% Recall@1% and 40.3% Recall@10% on cross-modal retrieval, outperforming baselines on both synthetic and downstream model checking tasks.

## Executive Summary
This paper introduces CNML, a contrastive learning framework for learning aligned representations of LTL specifications and AIGER circuits. The key innovation is using the model checking task as a contrastive learning objective, where circuits and specifications that satisfy each other are brought closer in embedding space. CNML employs a self-supervised approach with separate encoders for each modality, trained on synthetic data generated through reactive synthesis. The model significantly outperforms both algorithmic and neural baselines on cross-modal and intra-modal retrieval tasks, achieving high Recall@1% and Recall@10% scores. Importantly, CNML demonstrates generalization capabilities, successfully transferring learned representations to downstream tasks and generalizing from simple formulas to complex multi-guarantee specifications.

## Method Summary
CNML uses dual CodeBERT encoders (one for LTL specifications, one for AIGER circuits) with 1024-dim projection heads. The model is trained on 295,665 synthetic circuit-specification pairs generated via reactive synthesis. The contrastive objective pulls together embeddings of satisfying pairs while pushing apart non-satisfying pairs using symmetric cross-entropy loss over N×N similarity matrices. Training employs AdamW with LR 2e-4, 12k warmup steps, and linear decay for 80-100 epochs on 8 A100 GPUs. A representation similarity regularization term (λ=0.25) prevents catastrophic forgetting. The batching algorithm minimizes false negatives (targeting ~4% rate) by avoiding duplicate circuits/specs within batches.

## Key Results
- CNML achieves 27.3% Recall@1% and 40.3% Recall@10% on cross-modal retrieval (N=100), significantly outperforming baselines (9.8% and 21.3% respectively)
- On downstream model checking, CNML reaches 84.5% accuracy vs 83.0% for CodeBERT baseline
- CNML demonstrates compositional generalization, achieving 81.6% Recall@10% on multi-guarantee formulas despite training only on single-guarantee specifications
- Intra-modal retrieval shows similar gains: 33.2% R@1% (vs 12.7% baseline) for circuit-to-circuit retrieval

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Alignment via Model Checking Signal
The model checking relationship (satisfy/violate) serves as a contrastive objective that induces aligned embeddings for cross-modal retrieval. The model constructs N circuit-specification pairs known to be positive (circuit satisfies specification), computes embeddings for all, and calculates cosine similarity for all N×N pairings. Diagonal pairs (known positives) are pulled together while off-diagonal pairs (implicitly coded negatives) are pushed apart via symmetric cross-entropy loss, creating a shared latent space where satisfying pairs cluster together.

### Mechanism 2: Self-Supervised Data Generation via Reactive Synthesis
Reactive synthesis enables scalable generation of verifiable circuit-specification pairs without manual labeling. The Strix LTL synthesis tool automatically generates circuits that inherently satisfy generated LTL specifications, creating 295,665 positive pairs. Formula augmentation (shuffling assumption order, enforcing uniform wire counts) prevents syntactic overfitting and forces the model to learn semantic rather than syntactic patterns.

### Mechanism 3: Compositionality via Formula Splitting
Training on single-guarantee formulas induces compositional representations that generalize to multi-guarantee specifications. The authors exploit the logical property that if a circuit satisfies a conjunction of guarantees, it satisfies each individually. By training CNML-simple on split formulas (one guarantee each) and testing on multi-guarantee formulas, they demonstrate compositional generalization.

## Foundational Learning

- **Concept: Linear Temporal Logic (LTL)**
  - Why needed here: LTL is the specification language CNML must encode. Understanding temporal operators (next, until, always) and the assume-guarantee format is essential for interpreting what the model learns.
  - Quick check question: Can you explain why `(G i0) -> (G (¬i1 -> X o1))` means "whenever i1 does not hold, in the next step o1 should be true, as long as i0 holds"?

- **Concept: Contrastive Learning Objectives**
  - Why needed here: The core training signal. Understanding how InfoNCE-style losses create embedding spaces where semantically similar items cluster is critical for debugging retrieval failures.
  - Quick check question: In a batch of N=128 circuit-specification pairs, how many implicit negative pairs does the contrastive objective use?

- **Concept: Model Checking as Binary Relation**
  - Why needed here: The paper reframes model checking (traditionally an algorithmic verification procedure) as a binary relationship between circuits and specifications, enabling it as a training signal.
  - Quick check question: What are the three possible outcomes of a traditional model checking algorithm, and which outcome does CNML's contrastive objective implicitly leverage?

## Architecture Onboarding

- **Component map:** Eφ (CodeBERT for LTL specs) -> Projection head (768→1024) -> L2 normalize -> Embedding space; Ec (CodeBERT for AIGER circuits) -> Projection head (768→1024) -> L2 normalize -> Same embedding space

- **Critical path:** Data prep: Generate LTL spec → synthesize circuit via Strix → augment (shuffle assumptions, pad wires). Batch construction: Greedy algorithm ensures no duplicate circuits/specs per batch, minimizes false negatives. Forward pass: Each encoder processes its modality independently → pool → project → L2 normalize. Loss computation: Compute N×N cosine similarity matrix → apply symmetric cross-entropy + regularization.

- **Design tradeoffs:** Bi-encoder vs. Cross-encoder: Separate encoders enable efficient similarity search but cannot model fine-grained cross-modal token interactions. Implicit vs. Explicit Negatives: Implicit negatives avoid expensive model checking for all combinations but introduce ~4% false negative noise. Projection dimension: 1024 chosen empirically; paper notes diminishing returns above this.

- **Failure signatures:** Collapsed embeddings: If positive/negative similarity distributions overlap significantly, retrieval degrades. Monitor distribution separation. Syntactic shortcuts: If model learns wire-counting or formula-length heuristics, it fails on augmented data. Catastrophic forgetting: High learning rates cause BERT-based models to forget pre-trained knowledge. Use LR=2e-4 and λ=0.25 regularization.

- **First 3 experiments:** Reproduce retrieval baseline: Train CNML-base on cnml-base, evaluate on N=100 and N=1000 retrieval. Target: R@1% ≥ 27% (N=100), ≥ 40% (N=1000). Ablate projection dimension: Train variants with dims {512, 768, 1024, 1280}. Measure retrieval MRR and downstream accuracy. Test compositional generalization: Train on cnml-split (single-guarantee), evaluate on cnml-base (multi-guarantee). Expect performance drop but should outperform CodeBERT baseline.

## Open Questions the Paper Calls Out
- The paper identifies "synthesis" as a promising research direction for future work combining formal methods and deep learning, suggesting the learned embeddings could guide or accelerate synthesis tools.

## Limitations
- The effectiveness of implicit negative pairs (~4% false negative rate) remains unvalidated beyond the controlled synthetic dataset.
- The generalizability of learned representations to industrial-scale designs and specifications is untested.
- The paper doesn't address scalability to larger formula classes beyond LTL, such as STL or quantified extensions.

## Confidence
- **High confidence:** The experimental results on synthetic datasets demonstrating superior retrieval performance (R@1% 27.3% vs. 9.8% baseline on N=100). The compositional generalization mechanism has strong theoretical grounding in LTL semantics.
- **Medium confidence:** The downstream model checking accuracy improvements (0.845 vs. 0.830 for CodeBERT baseline). The cross-modal retrieval performance is measured but lacks comparison against more recent neural retrieval methods.
- **Low confidence:** Claims about real-world applicability to industrial verification tasks. The transfer learning results are promising but limited to a single downstream task with a relatively small test set.

## Next Checks
1. Evaluate CNML on real-world verification benchmarks (e.g., SV-COMP) to assess performance on industrial-scale specifications and circuits.
2. Test the robustness of implicit negatives by deliberately introducing known false negatives and measuring retrieval degradation.
3. Compare against modern neural retrieval baselines (e.g., Sentence-BERT, CLIP variants) using the same synthetic dataset to benchmark relative performance.