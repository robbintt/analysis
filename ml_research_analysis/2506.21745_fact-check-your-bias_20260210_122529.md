---
ver: rpa2
title: (Fact) Check Your Bias
arxiv_id: '2506.21745'
source_url: https://arxiv.org/abs/2506.21745
tags:
- bias
- evidence
- gaetz
- claim
- company
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how biases in large language models (LLMs)
  affect fact-checking outcomes in the HerO system, baseline for the FEVER-25 workshop.
  The study examines two key hypotheses: (1) whether LLM parametric knowledge contains
  inherent biases, and (2) whether these biases propagate through the fact-checking
  pipeline.'
---

# (Fact) Check Your Bias

## Quick Facts
- **arXiv ID:** 2506.21745
- **Source URL:** https://arxiv.org/abs/2506.21745
- **Authors:** Eivind Morris Bakke; Nora Winger Heggelund
- **Reference count:** 13
- **Primary result:** Biased prompts in hypothetical document generation shift ~50% of retrieved evidence, yet final fact-checking verdicts remain stable (0-4% label distribution shifts).

## Executive Summary
This paper investigates how biases in large language models (LLMs) affect fact-checking outcomes in the HerO system, baseline for the FEVER-25 workshop. The study examines two key hypotheses: (1) whether LLM parametric knowledge contains inherent biases, and (2) whether these biases propagate through the fact-checking pipeline. In the first experiment, Llama 3.1 directly classifies claims using only its parametric knowledge, defaulting to "Not Enough Evidence" for nearly half of claims. In the second experiment, intentionally biased hypothetical fact-checking documents are generated, resulting in approximately 50% unique evidence retrieved across bias conditions. Despite significant retrieval differences, final verdict predictions remain surprisingly stable across prompting strategies, with only 0-4% label distribution shifts. The study also uncovers asymmetric safeguarding behavior where the model refuses to generate supportive content for potentially harmful claims while readily producing critical content, potentially creating an inherent negative bias in evidence collection.

## Method Summary
The study uses the HerO fact-checking system with Llama 3.1 8B-Instruct, modified from the original 70B model. Experiment 1 involves direct claim classification using only parametric knowledge (bypassing retrieval). Experiment 2 generates intentionally biased hypothetical fact-checking documents using three prompting strategies (positive/support, negative/refute, objective/balanced), then uses these as queries for BM25 retrieval of top 5,000 documents. The system processes evidence through question generation and verification stages to produce final verdicts. Experiments use the Averitec dataset with train-train and train-reference splits for development and evaluation.

## Key Results
- Direct parametric prediction defaults to "Not Enough Evidence" for ~47% of claims
- Biased hypothetical document generation yields ~50% unique evidence retrieved across conditions
- Final verdict distributions remain stable (0-4% shifts) despite significant retrieval differences
- Asymmetric safeguarding causes 214 refusals (5.3% document rate) in positive condition vs 0.1% in others
- Only 31.2% agreement between parametric-only and retrieved-knowledge verdicts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Directional prompts in hypothetical document generation systematically influence which evidence gets retrieved.
- **Mechanism:** Prompts bias the semantic content of generated fact-checking passages → these passages serve as queries for BM25 retrieval → retrieval surface area shifts toward documents matching the generated perspective.
- **Core assumption:** Retrieval systems return documents semantically similar to query documents.
- **Evidence anchors:**
  - [abstract] "prompts significantly influence retrieval outcomes, with approximately 50% of retrieved evidence being unique to each perspective"
  - [section 4.3] "Jaccard similarity scores ranging from 0.42 to 0.56, indicating that different biases led to approximately half of the retrieved documents being unique"
  - [corpus] Limited direct evidence; related work on retrieval-augmented fact-checking (FlashCheck, SUCEA) addresses retrieval efficiency but not bias propagation specifically.
- **Break condition:** If retrieval uses claim-only queries without hypothetical document intermediation, this mechanism bypasses.

### Mechanism 2
- **Claim:** Asymmetric model safeguarding creates inherent negative bias in evidence collection.
- **Mechanism:** Safety training causes refusal when asked to generate supportive content for sensitive/harmful claims → critical/refuting content generates without similar friction → retrieved evidence skews negative for controversial topics.
- **Core assumption:** Refusals occur more frequently for supportive framing than critical framing of identical claims.
- **Evidence anchors:**
  - [abstract] "the model sometimes refuses to generate supporting documents for claims it believes to be false, creating an inherent negative bias"
  - [section 4.3, Table 4] "positive condition exhibited 214 total refusals (5.3% document refusal rate) affecting 94 of 500 claims (18.8%)" vs "0.1%" for other conditions
  - [corpus] No direct corpus evidence on asymmetric safeguarding in fact-checking contexts.
- **Break condition:** If safety filters apply symmetrically or are disabled, this bias source equalizes.

### Mechanism 3
- **Claim:** Final verdict predictions demonstrate robustness to retrieval variation due to verification component design.
- **Mechanism:** Verification stage synthesizes question-answer pairs from retrieved evidence → focuses on key evidentiary signals rather than document volume → dampens upstream retrieval bias.
- **Core assumption:** The verification LLM prioritizes salient evidence over exhaustive document consideration.
- **Evidence anchors:**
  - [abstract] "Despite significant retrieval differences, final verdict predictions remain surprisingly stable across prompting strategies, with only 0-4% label distribution shifts"
  - [section 5] "verification components appear remarkably robust to variations in retrieved evidence"
  - [corpus] Related work on interpretable fact-checking (InFi-Check) addresses fine-grained verification but does not examine robustness to retrieval bias.
- **Break condition:** If verification relies heavily on document quantity or ranking position, robustness may degrade.

## Foundational Learning

- **Concept:** Parametric vs. retrieved knowledge in LLMs
  - **Why needed here:** The paper isolates parametric knowledge (Experiment 1) from retrieved knowledge (Experiment 2) to diagnose bias sources.
  - **Quick check question:** Can you explain why a model might refuse to verify a claim using only parametric knowledge but succeed with retrieval?

- **Concept:** Hypothetical Document Embedding (HyDE) for retrieval
  - **Why needed here:** HerO uses LLM-generated hypothetical fact-checking documents as retrieval queries rather than raw claims.
  - **Quick check question:** How does generating a hypothetical document before retrieval differ from direct claim-to-document matching?

- **Concept:** Label distribution stability vs. instance-level agreement
  - **Why needed here:** The paper shows stable aggregate distributions despite substantial instance-level retrieval differences.
  - **Quick check question:** Why might aggregate label percentages remain stable even when individual predictions change?

## Architecture Onboarding

- **Component map:** Claim → Hypothetical document → Retrieval (top 5,000) → Reranking → Question generation → Veracity prediction
- **Critical path:** Claim → Hypothetical document → Retrieval (top 5,000) → Reranking → Question generation → Veracity prediction. Bias injection point is the hypothetical document prompt.
- **Design tradeoffs:**
  - 8B model vs. 70B (original HerO): Faster, cheaper, but potentially different bias patterns
  - Top 5,000 vs. 10,000 retrieval: Reduced compute, may miss edge-case evidence
  - Single-perspective vs. multi-perspective document generation: Efficiency vs. bias mitigation
- **Failure signatures:**
  - High "Not Enough Evidence" rate with parametric-only prediction (~47%) indicates model caution
  - Refusal spikes in positive-bias condition (18.8% of claims affected) signal safeguard activation
  - Low agreement between parametric and retrieved verdicts (31.2%) suggests knowledge conflicts
- **First 3 experiments:**
  1. **Baseline replication:** Run the adapted HerO pipeline on the Averitec training set, comparing verdict distributions to paper's baseline (Supported: 28.4%, Refuted: 66.8%, NEE: 2.4%, Conflicting: 2.4%)
  2. **Refusal rate audit:** Apply the automated refusal detection pattern to all generated documents across bias conditions, verifying the 47-fold asymmetry reported
  3. **Retrieval overlap analysis:** Compute Jaccard similarity between document sets from positive/negative/objective conditions on a 50-claim sample to confirm ~50% uniqueness finding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed bias propagation patterns and asymmetric safeguarding behaviors generalize to larger Llama model sizes (70B) and other model families?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they "utilized Llama 3.1 8B models rather than the 70B variants" and note that "further research is needed to explore if these biases are reproduced also with larger models."
- **Why unresolved:** Computational resource constraints limited experiments to the smaller 8B model, leaving unknown whether larger models exhibit similar caution biases, refusal patterns, and verdict stability.
- **What evidence would resolve it:** Running the same experimental pipeline with Llama 3.1 70B and comparing refusal rates, label distributions, and retrieval overlap metrics against the 8B results.

### Open Question 2
- **Question:** What mechanisms cause final verdict predictions to remain stable (0-4% shift) despite approximately 50% unique documents retrieved across bias conditions?
- **Basis in paper:** [inferred] The authors report this "surprising" finding and hypothesize that "the verification component effectively focuses on key evidence pieces" or "retrieved documents, though different, are so similar semantically," but do not empirically test these explanations.
- **Why unresolved:** The paper identifies the phenomenon but does not isolate which mechanism—evidence prioritization, semantic equivalence, or another factor—explains the robustness.
- **What evidence would resolve it:** Ablation studies varying the number and relevance rank of retrieved documents, coupled with semantic similarity analysis of unique vs. shared documents and their contribution to final verdicts.

### Open Question 3
- **Question:** To what extent does asymmetric safeguarding (47× higher refusal rates for supportive vs. critical prompts on controversial claims) systematically skew fact-checking outcomes toward "Refuted" verdicts?
- **Basis in paper:** [explicit] The authors "further hypothesize that the asymmetric safeguarding behavior creates an inherent negative bias in the system" and note it "may systematically disadvantage certain perspectives," but do not quantify downstream effects.
- **Why unresolved:** The paper documents the refusal asymmetry but cannot disentangle its impact from other factors affecting the positive-bias condition's retrieval and verdict distribution.
- **What evidence would resolve it:** A controlled experiment comparing verdict distributions when refusals are replaced with neutral fallback documents versus when they are excluded entirely, isolating the causal effect of safeguarding on final outcomes.

## Limitations
- Reliance on synthetic hypothetical documents rather than real-world fact-checking evidence may limit generalizability
- Pattern-matching methodology for detecting refusals provides only indirect evidence of asymmetric safeguarding
- 8B model used differs substantially from original 70B model, potentially affecting bias manifestation
- Limited analysis of instance-level prediction changes despite stable aggregate distributions

## Confidence
**High Confidence** in retrieval bias propagation claims (50% unique evidence, stable verdict distributions) due to clear statistical evidence and reproducible experimental design.
**Medium Confidence** in asymmetric safeguarding findings (18.8% vs 0.1% refusal rates) due to pattern-matching methodology limitations.
**Medium Confidence** in verification component robustness claims based on aggregate stability metrics but limited instance-level analysis.

## Next Checks
1. **Refusal Rate Verification:** Apply systematic pattern-matching to all generated documents across bias conditions to independently verify the 47-fold asymmetry in refusal rates (18.8% vs 0.1%) and identify which specific claim types trigger safety filters.

2. **Retrieval System Isolation:** Repeat the experiment with direct claim-to-document retrieval (bypassing hypothetical document generation) to determine whether bias propagation depends on the HyDE-FC intermediation step or would occur in simpler retrieval architectures.

3. **Model Size Effect Analysis:** Replicate key experiments using the original 70B model to assess whether bias patterns scale with model size and whether verification robustness holds across different model capacities.