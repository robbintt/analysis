---
ver: rpa2
title: Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative
  Language Models
arxiv_id: '2505.01539'
source_url: https://arxiv.org/abs/2505.01539
tags:
- prompts
- arguments
- attack
- graphs
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a parameterized benchmark framework for\
  \ evaluating generative language models\u2019 reasoning capabilities using dynamically\
  \ generated argumentation-based tasks. By creating attack graphs of varying complexity\
  \ and translating them into natural language prompts about witness testimony, the\
  \ authors assess models\u2019 ability to reason through layered arguments."
---

# Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models

## Quick Facts
- **arXiv ID**: 2505.01539
- **Source URL**: https://arxiv.org/abs/2505.01539
- **Reference count**: 16
- **Key outcome**: State-of-the-art models struggle with structured reasoning tasks, especially with increased complexity and multiple attack paths

## Executive Summary
This study introduces a parameterized benchmark framework for evaluating generative language models' reasoning capabilities using dynamically generated argumentation-based tasks. By creating attack graphs of varying complexity and translating them into natural language prompts about witness testimony, the authors assess models' ability to reason through layered arguments. Testing seven commercial models, including GPT-4o, Claude, Gemini, and o1-preview, on linear and non-linear attack graph prompts, the study reveals that even state-of-the-art models struggle with reasoning tasks, especially when prompts involve multiple attack paths or require rejecting main arguments. Performance drops significantly with increased complexity, and models often exhibit inconsistent reasoning patterns. The findings highlight the brittleness of current generative models' reasoning abilities, underscoring the need for robust evaluation methods in legal and evidence-based domains.

## Method Summary
The method generates attack graphs with varying complexity (1-25 arguments for linear, 1-15 for non-linear) and translates them into natural language prompts using a randomized ontology of 474 names and 90 statements. The framework creates witness testimony scenarios where arguments attack each other in directed graphs. Ground truth is determined by formal argumentation semantics: in linear graphs, the main argument is accepted if the path length is odd; in non-linear graphs, it's accepted only if all attacking paths have even length. Seven commercial models (GPT-4o, Claude, Gemini, o1-preview) are queried with these prompts, and responses are evaluated using Matthew's Correlation Coefficient and accuracy metrics to assess reasoning performance across different complexity levels and graph structures.

## Key Results
- All models show significant performance degradation as graph complexity increases, with accuracy approaching 50% baseline levels
- Models exhibit a strong "yes" bias, accepting the main argument even when valid attackers should cause rejection
- Non-linear graphs with multiple attack paths are particularly challenging, with models often tracking only single paths while ignoring parallel attacks

## Why This Works (Mechanism)

### Mechanism 1: Parameterized Complexity Scaling
Systematically increasing graph parameters (number of arguments n and attack paths k) exposes the finite reasoning horizon and working memory limits of generative models. The framework generates attack graphs where the solution requires tracking dependencies across a chain or tree of arguments. As n increases, the model must maintain a coherent state of the "accepted" vs. "rejected" sets (e.g., A1 is rejected if n is even, accepted if n is odd). Performance degradation correlates with the depth of the dependency chain.

### Mechanism 2: Order-Agnostic Logic Validation
Shuffling the presentation order of arguments in the natural language prompt disrupts sequential heuristics (recency/primacy bias), forcing the model to construct an internal dependency graph rather than relying on textual linearity. Models often presuppose the first argument encountered. By shuffling arguments, the benchmark tests if the model can resolve references into a structured graph regardless of input sequence.

### Mechanism 3: Ontological Decoupling of Form and Content
Mapping abstract attack graphs to varied natural language "skins" via an ontology ensures the model processes the logical structure of the conflict rather than relying on statistical correlations of specific tokens. The framework separates the formal logic from the semantic content by randomizing the content, minimizing the risk that the model leverages pre-trained priors about specific names or scenarios.

## Foundational Learning

- **Abstract Argumentation Frameworks (Dung Frameworks)**: The theoretical engine of the benchmark. You cannot debug the "ground truth" or the model's errors without understanding the rules of attack (e.g., "an argument is accepted if all its attackers are rejected").
  - Quick check: In a linear chain A ← B ← C, is argument A accepted or rejected? (Answer: Accepted, because C attacks B, defending A).

- **Data Contamination & Memorization**: The paper explicitly positions its dynamic generation as a counter-measure to static benchmarks. Understanding this explains why the prompts must be procedurally generated rather than curated.
  - Quick check: Why would a model solving a static legal benchmark perfectly not guarantee it can reason? (Answer: It may have memorized the question-answer pairs during training).

- **Label Imbalance & Evaluation Metrics**: The benchmark often produces skewed label distributions (e.g., in non-linear graphs, "no" answers can vastly outnumber "yes"). Relying on accuracy alone is misleading; Matthews Correlation Coefficient (MCC) is required.
  - Quick check: If a model always answers "no" in a dataset where 90% of answers are "no", is 90% accuracy a good metric? (Answer: No, use MCC or F1).

## Architecture Onboarding

- **Component map**: Graph Generator -> Ontology Instantiator -> NL Renderer -> Solver -> Evaluator
- **Critical path**: The Solver logic is the single point of failure for validity. If the translation from graph to ground truth is wrong, the entire evaluation is invalid. Ensure the implementation of "odd length = accepted" (linear) and "all even paths = accepted" (non-linear) matches the formal definitions.
- **Design tradeoffs**: Expressiveness vs. Solvability (complex non-linear graphs better test reasoning but exponentially increase prompt length and state-tracking difficulty); Cost vs. Coverage (testing reasoning models like o1-preview is expensive).
- **Failure signatures**: "Yes" Bias (model accepts main argument despite valid attackers); Single-Path Dependency (model tracks only longest path or last-read path, ignoring branching attackers); Hallucinated Constraints (model invents rules not present in prompt instructions).
- **First 3 experiments**: Baseline Linearity (sweep of n=2 to n=10 linear graphs to establish working memory degradation curve); Branching Stress Test (fix n=5, vary partitions to see if model handles multiple short attackers better than one long chain); Shuffle Robustness (take failed non-linear prompts, shuffle argument order, re-test to check if failure was structural or order-dependent).

## Open Questions the Paper Calls Out
None

## Limitations
- The specific semantic content (names, statements) could still introduce subtle biases not fully characterized
- Cost-constrained sampling may limit statistical power for comparisons between models
- Key parameters like temperature, max tokens, and system prompts were not specified, making exact replication difficult

## Confidence
- **High Confidence**: Core finding that state-of-the-art models struggle with structured reasoning tasks, evidenced by consistent performance degradation across multiple models and graph complexities
- **Medium Confidence**: Specific failure modes (yes-bias, single-path dependency, order sensitivity) are well-documented but may vary with different prompt formulations
- **Low Confidence**: Relative performance rankings between specific models may be unstable due to sampling limitations and API configuration uncertainties

## Next Checks
1. **Ground Truth Validation**: Implement a formal verification suite that cross-checks the solver logic against known small graphs (n≤5) using independent reasoning
2. **Parameter Sensitivity Analysis**: Systematically vary API parameters (temperature, max tokens) to determine if performance differences are methodologically driven or parameter-dependent
3. **Ontology Neutralization Test**: Create multiple distinct ontologies with different semantic domains to verify that content randomization effectively decouples logical reasoning from content-specific biases