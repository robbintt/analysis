---
ver: rpa2
title: 'PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through
  Adaptive KV Cache Pruning'
arxiv_id: '2510.19183'
source_url: https://arxiv.org/abs/2510.19183
tags:
- visual
- attention
- tokens
- hallucinations
- prunehal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the connection between visual attention
  insufficiency and hallucinations in multimodal large language models (MLLMs). It
  proposes that redundant visual tokens disperse the model's attention, leaving critical
  visual cues under-attended, which leads to hallucinated outputs.
---

# PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning

## Quick Facts
- **arXiv ID**: 2510.19183
- **Source URL**: https://arxiv.org/abs/2510.19183
- **Reference count**: 30
- **Primary result**: Adaptive KV cache pruning reduces hallucinations by up to 25% on LLaVA-v1.5-7B and improves GPT-4V-assisted correctness from 6.04 to 6.98

## Executive Summary
This paper addresses visual hallucinations in multimodal large language models (MLLMs) by proposing that redundant visual tokens disperse attention away from critical visual cues. The authors introduce PruneHal, a training-free framework that adaptively prunes the KV cache to retain the most attended visual tokens while eliminating redundant ones. The method dynamically balances preserving crucial visual information with eliminating distractions, guided by historical visual attention distributions. Experiments across four mainstream MLLMs demonstrate consistent hallucination reduction without introducing significant computational overhead.

## Method Summary
PruneHal operates by analyzing historical visual attention distributions during decoding to identify and prune redundant visual tokens from the KV cache. The framework maintains a balance between retaining critical visual information and removing attention-dispersing tokens. By focusing the model's attention on the most relevant visual cues, PruneHal aims to reduce hallucinated outputs. The approach is training-free and integrates seamlessly with existing decoding strategies, requiring minimal modifications to standard MLLM pipelines.

## Key Results
- Reduces CHAIR S hallucination rates by up to 25% under greedy decoding on LLaVA-v1.5-7B
- Improves GPT-4V-assisted correctness scores from 6.04 to 6.98
- Demonstrates consistent improvements across four mainstream MLLMs: LLaVA-v1.5-7B/13B, InstructBLIP-7B, and Qwen-VL-7B
- Shows virtually no computational overhead during inference

## Why This Works (Mechanism)
The mechanism exploits the observation that redundant visual tokens in the KV cache disperse the model's attention, leaving critical visual cues under-attended. By adaptively pruning these redundant tokens based on historical attention patterns, PruneHal ensures that the most relevant visual information receives appropriate attention during decoding. This focused attention distribution reduces the likelihood of the model generating hallucinated content by maintaining stronger connections to actual visual evidence.

## Foundational Learning

**Visual Attention Distribution**: The statistical patterns of how MLLMs allocate attention across visual tokens. Why needed: Understanding attention dispersion is crucial for identifying redundant tokens. Quick check: Compare attention weight distributions with and without pruning.

**KV Cache Pruning**: The process of selectively removing token representations from the key-value cache during decoding. Why needed: Reduces memory footprint and computational load while potentially improving attention focus. Quick check: Measure memory usage before and after pruning.

**Multimodal Attention Mechanisms**: How MLLMs integrate visual and textual information through cross-attention layers. Why needed: The interaction between modalities determines hallucination susceptibility. Quick check: Analyze cross-attention patterns on hallucinated vs non-hallucinated outputs.

**Greedy Decoding**: A deterministic decoding strategy that selects the highest probability token at each step. Why needed: Provides consistent evaluation conditions for hallucination metrics. Quick check: Compare hallucination rates across different decoding strategies.

**Attention History Tracking**: Recording attention weights across decoding steps to identify patterns. Why needed: Enables identification of consistently under-attended or redundant tokens. Quick check: Visualize attention history heatmaps.

## Architecture Onboarding

**Component Map**: Input Image -> Vision Encoder -> Visual Tokens -> MLLM Encoder-Decoder -> Text Output
                    ↓
              PruneHal Attention Analyzer -> KV Cache Pruning Module

**Critical Path**: Image encoding → Cross-attention → KV cache analysis → Pruning decision → Attention refinement → Text generation

**Design Tradeoffs**: Memory efficiency vs. information preservation; computational overhead vs. hallucination reduction; aggressive pruning vs. maintaining factual accuracy

**Failure Signatures**: Over-pruning leading to loss of critical visual information; under-pruning resulting in continued attention dispersion; computational overhead exceeding claimed "virtual zero" impact

**First Experiments**: 1) Ablation study removing identified redundant tokens to measure individual impact on hallucination rates. 2) Evaluation under diverse decoding strategies (top-k, nucleus sampling) to assess generalizability. 3) Testing on domain-specific MLLMs to determine effectiveness beyond general-purpose models.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on greedy decoding with limited evaluation of sampling-based strategies
- Does not investigate trade-offs between hallucination reduction and factual accuracy preservation
- Effectiveness on domain-specific MLLMs (medical, scientific) remains untested

## Confidence

**High Confidence**: Empirical improvements in hallucination reduction metrics across multiple benchmarks and model architectures
**Medium Confidence**: Proposed mechanism linking redundant visual tokens to attention insufficiency and hallucinations
**Medium Confidence**: Assertion that PruneHal introduces "virtually no computational overhead"

## Next Checks
1. Conduct ablation studies removing specific visual tokens identified as redundant to quantify their individual contribution to hallucination reduction
2. Evaluate PruneHal's performance under diverse decoding strategies (top-k, nucleus sampling) and temperature settings to assess generalizability
3. Test the method on specialized MLLMs trained on domain-specific visual data to determine if the pruning approach generalizes beyond general-purpose models