---
ver: rpa2
title: Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment
  Analysis in Short Texts
arxiv_id: '2509.04982'
source_url: https://arxiv.org/abs/2509.04982
tags:
- data
- classification
- sentiment
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates small Transformer-based models (BERT and
  RoBERTa with <1B parameters) for multi-label sentiment classification in short texts,
  addressing challenges like class imbalance, data scarcity, and limited context.
  Three strategies were tested: continued domain-specific pre-training, generative
  data augmentation (GDA), and classification head variations.'
---

# Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts

## Quick Facts
- arXiv ID: 2509.04982
- Source URL: https://arxiv.org/abs/2509.04982
- Reference count: 26
- Primary result: BERT/RoBERTa-base models with <1B parameters achieve 0.77 F1 score on multi-label sentiment classification of short texts

## Executive Summary
This study evaluates small Transformer-based models (BERT and RoBERTa with <1B parameters) for multi-label sentiment classification in short texts, addressing challenges like class imbalance, data scarcity, and limited context. Three strategies were tested: continued domain-specific pre-training, generative data augmentation (GDA), and classification head variations. Results show that moderate GDA improves model performance, while excessive pre-training can introduce noise, and classification head changes yield minimal gains. The best model achieved 0.77 F1 score, outperforming human evaluators. SHAP analysis revealed interpretable feature contributions, with tokens like "police" strongly predicting "Fear."

## Method Summary
The study fine-tunes BERT and RoBERTa base/large models for multi-label sentiment classification using the SemEval 2025 Task 11 dataset (2,768 examples, ~15 words average). Three optimization strategies were explored: continued domain-specific pre-training (50 epochs, MLM with 15% masking), generative data augmentation using GPT-4o-mini to generate up to 11,684 synthetic samples, and classification head variations. Models were trained for 30 epochs with lr=2e-5, weight_decay=0.01, AdamW optimizer, and batch_size=32. Preprocessing included lowercase conversion and removal of hyperlinks and punctuation. Evaluation metrics included F1 Score (primary), Macro F1 Score, accuracy, and Cohen's Kappa.

## Key Results
- Best model achieved 0.77 F1 score, outperforming human evaluators (0.74 F1)
- Moderate generative data augmentation improved performance, while excessive augmentation degraded results
- Continued domain-specific pre-training showed mixed results, with potential to introduce noise
- Classification head variations yielded minimal performance improvements
- SHAP analysis revealed interpretable feature contributions, with specific tokens strongly predicting sentiment labels

## Why This Works (Mechanism)
The effectiveness stems from addressing the unique challenges of short-text multi-label sentiment analysis through targeted optimization strategies. Small models are particularly suitable for this task due to their efficiency and ability to capture nuanced patterns in limited context. Generative data augmentation helps mitigate class imbalance by creating synthetic examples for underrepresented labels, while continued pre-training can adapt models to domain-specific language patterns. The combination of these strategies with careful hyperparameter tuning enables small models to achieve competitive performance despite their limited parameter count.

## Foundational Learning
- **Multi-label classification**: Why needed - Sentiment can involve multiple emotions simultaneously; Quick check - Each example has 1-5 labels
- **Class imbalance handling**: Why needed - Fear class dominates (58.2%) while others are underrepresented; Quick check - Compare per-class precision/recall
- **Data augmentation techniques**: Why needed - Limited training data (2,768 examples) requires synthetic expansion; Quick check - Validate synthetic samples maintain semantic coherence
- **Transformer fine-tuning**: Why needed - Adapting pre-trained models to specific sentiment classification task; Quick check - Monitor validation loss for overfitting
- **SHAP analysis**: Why needed - Understanding model decision-making for interpretability; Quick check - Identify top contributing tokens per label

## Architecture Onboarding

**Component Map:** Data -> Preprocessing -> Model (BERT/RoBERTa) -> Classification Head -> Training -> Evaluation

**Critical Path:** SemEval dataset → lowercase + URL/punctuation removal → RoBERTa-large → 768-dim FC + sigmoid → 30 epochs fine-tuning → F1 evaluation

**Design Tradeoffs:** Small models (<1B params) vs. larger models for efficiency vs. performance; moderate vs. excessive augmentation for generalization vs. overfitting; continued pre-training vs. direct fine-tuning for domain adaptation vs. noise introduction

**Failure Signatures:** Overfitting during pre-training (train loss ↓, eval loss ↑); performance degradation with excessive augmentation; high false positives on dominant "Fear" class due to imbalance

**Exactly 3 First Experiments:**
1. Fine-tune RoBERTa-large baseline with 30 epochs, lr=2e-5, evaluate F1
2. Apply moderate GDA (33-66% augmentation) and compare performance
3. Test continued pre-training for 50 epochs and measure impact on final F1

## Open Questions the Paper Calls Out
None

## Limitations
- Exact train/val/test split indices and class-balancing details unspecified
- Single-run evaluation without confidence intervals or multiple random seeds
- GPT-4o-mini fine-tuning configuration and synthetic data validation process unclear
- Human evaluation based on only 100 samples may lack statistical power

## Confidence

**High confidence:** Moderate GDA improves performance while excessive pre-training introduces noise; classification head changes yield minimal gains

**Medium confidence:** Specific F1 score of 0.77 and human evaluator comparison (0.77 vs 0.74 F1) due to single-run evaluation and small sample size

**Low confidence:** Exact optimization of augmentation ratios and performance differences between pre-training strategies cannot be independently verified

## Next Checks

1. Replicate with multiple random seeds (5+ runs) to establish confidence intervals and verify result stability
2. Implement full GPT-4o-mini fine-tuning pipeline and validate synthetic data quality through human or automated evaluation
3. Conduct systematic ablation studies varying preprocessing steps and augmentation ratios to verify individual component impacts