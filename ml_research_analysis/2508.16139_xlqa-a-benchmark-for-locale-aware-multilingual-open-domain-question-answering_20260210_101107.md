---
ver: rpa2
title: 'XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering'
arxiv_id: '2508.16139'
source_url: https://arxiv.org/abs/2508.16139
tags:
- question
- questions
- multilingual
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of culturally aware evaluation in
  multilingual open-domain question answering (ODQA) benchmarks. It argues that existing
  benchmarks assume locale-invariant answers across languages, which fails to capture
  the cultural and regional variations that affect question understanding and correct
  responses.
---

# XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering

## Quick Facts
- arXiv ID: 2508.16139
- Source URL: https://arxiv.org/abs/2508.16139
- Authors: Keon-Woo Roh; Yeong-Joon Ju; Seong-Whan Lee
- Reference count: 8
- Key outcome: Introduces XLQA, a benchmark for evaluating locale-sensitive multilingual ODQA, revealing that current multilingual LLMs struggle with culturally nuanced queries, achieving average EM of 26.02% and F1 of 36.33% across non-English languages.

## Executive Summary
This paper addresses a critical gap in multilingual open-domain question answering (ODQA) benchmarks by highlighting their assumption of locale-invariant answers across languages. The authors introduce XLQA, a benchmark specifically designed to evaluate locale-sensitive multilingual ODQA, containing 3,000 English seed questions expanded to eight languages with human-verified annotations. The benchmark construction pipeline combines multilingual question generation, locale-aware answer generation via LLM with retrieval-augmented generation, and human verification. Evaluation of five state-of-the-art multilingual LLMs on XLQA reveals notable failures on locale-sensitive questions, with performance gaps between English and other languages due to a lack of locale-grounding knowledge.

## Method Summary
XLQA is constructed using a scalable pipeline that begins with 3,000 English seed questions, which are expanded to eight languages using multilingual question generation techniques. Locale-aware answer generation is performed using LLMs with retrieval-augmented generation, followed by human verification to ensure accuracy and cultural appropriateness. The benchmark distinguishes between locale-invariant and locale-sensitive cases through human-verified annotations. Five state-of-the-art multilingual LLMs are evaluated on XLQA, revealing significant performance gaps on locale-sensitive questions compared to locale-invariant ones.

## Key Results
- State-of-the-art multilingual LLMs show notable failures on locale-sensitive questions in XLQA.
- Performance gaps exist between English and non-English languages due to lack of locale-grounding knowledge.
- Average exact match scores of 26.02% and F1 scores of 36.33% across non-English languages highlight current model limitations in handling culturally nuanced queries.

## Why This Works (Mechanism)
The benchmark works by explicitly separating locale-invariant from locale-sensitive questions, forcing models to demonstrate cultural and regional awareness rather than relying on universal answers. The human verification step ensures that generated answers accurately reflect cultural nuances, while the retrieval-augmented generation provides contextually relevant information for locale-specific queries.

## Foundational Learning
- **Multilingual Question Generation**: Needed to expand English seed questions across eight languages; quick check: verify semantic equivalence across translations.
- **Locale-aware Answer Generation**: Required to capture cultural and regional variations in answers; quick check: compare answers across different locales for the same question.
- **Human Verification**: Essential to validate cultural appropriateness and accuracy of generated answers; quick check: inter-annotator agreement rates.

## Architecture Onboarding
**Component Map:** English seed questions -> Multilingual generation -> LLM with RAG -> Human verification -> XLQA benchmark
**Critical Path:** Question expansion → Locale-aware answer generation → Human verification → Model evaluation
**Design Tradeoffs:** LLM automation speeds construction but may introduce bias; human verification ensures quality but limits scalability
**Failure Signatures:** Models perform well on locale-invariant questions but fail on locale-sensitive ones; English outperforms other languages consistently
**First 3 Experiments:**
1. Evaluate models on locale-invariant subset only to establish baseline performance
2. Test models on locale-sensitive subset to measure cultural awareness
3. Compare performance across language pairs to identify specific locale gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Geographic and cultural coverage limited to eight languages, potentially missing nuanced regional variations within language groups.
- Reliance on LLM-based answer generation may propagate subtle biases from training data into benchmark questions and answers.
- Definition of "locale-sensitivity" is somewhat subjective, and cultural interpretation may vary across individual annotators.

## Confidence
- **High confidence**: Core observation that existing multilingual ODQA benchmarks assume locale-invariant answers is well-supported.
- **Medium confidence**: Claim that performance gaps stem from lack of locale-grounding knowledge requires further investigation to separate from general linguistic competence differences.
- **Medium confidence**: Scalability claims for benchmark construction pipeline are reasonable but human verification effort is not fully detailed.

## Next Checks
1. Conduct cross-cultural validation with annotators from diverse geographic regions within the same language to assess consistency in locale-sensitivity annotations.
2. Perform ablation studies on LLM answer generation step by comparing human-generated answers directly with LLM-generated ones.
3. Test whether fine-tuning models on culturally diverse subsets of XLQA improves performance on locale-sensitive questions more than general multilingual training.