---
ver: rpa2
title: 'Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment'
arxiv_id: '2505.21548'
source_url: https://arxiv.org/abs/2505.21548
tags:
- cultural
- indian
- indic
- india
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates six Indic and six global LLMs on cultural alignment
  using human-grounded benchmarks and a user study. Across four tasks grounded in
  nationally representative surveys and community-authored QA datasets, Indic models
  did not align better with Indian values or practices than global models.
---

# Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment

## Quick Facts
- **arXiv ID:** 2505.21548
- **Source URL:** https://arxiv.org/abs/2505.21548
- **Reference count:** 40
- **Primary result:** Indic models do not align better with Indian values than global models; a US respondent was often a closer proxy for Indian cultural values than any Indic model.

## Executive Summary
This paper evaluates six Indic and six global LLMs on cultural alignment using human-grounded benchmarks and a user study. Across four tasks grounded in nationally representative surveys and community-authored QA datasets, Indic models did not align better with Indian values or practices than global models. In fact, a U.S. respondent was often a closer proxy for Indian cultural values than any Indic model. The user study with 115 Indian participants showed that writing suggestions from both global and Indic models introduced Westernized or exoticized writing. Prompting strategies and regional fine-tuning failed to recover alignment and sometimes degraded existing knowledge. The authors attribute these results to a scarcity of culturally grounded data, especially for pretraining, and advocate for native, community-authored corpora and thick×wide evaluations.

## Method Summary
The study evaluates 12 LLMs (6 Indic, 6 global) across four tasks: World Values Survey (WVS) questions, GlobalOpinionQA for divergent opinions, CulturalBench for cultural practices, and NormAd for normative reasoning. Human evaluations use a nationally representative sample of 15 Indian and 15 US respondents. A user study with 115 Indian participants tests writing suggestions against Westernized vs. authentic Indian norms. Models are tested with zero-shot, few-shot, and chain-of-thought prompting, and with regional fine-tuning on culturally aligned data.

## Key Results
- Indic models cluster closer to US respondents than Indian respondents on the Inglehart-Welzel cultural map across all four evaluation tasks
- A US respondent was often a closer proxy for Indian cultural values than any Indic model
- Regional fine-tuning on culturally aligned data sometimes degraded model performance rather than improving cultural alignment
- Writing suggestions from both global and Indic models in the user study introduced Westernized or exoticized elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regional fine-tuning is insufficient to overcome the Western-centric cultural priors established during base model pretraining
- Mechanism: Web-scale pretraining data creates a strong statistical prior favoring Western values; "thin" fine-tuning cannot shift this deep-seated value orientation
- Core assumption: Pretraining data for global base models is overwhelmingly Western, and fine-tuning datasets are too small or linguistically focused to counterbalance this
- Evidence anchors: [abstract]: "We attribute this to scarce culturally grounded data, especially for pretraining"; [section]: "Web-scale corpora used for pretraining are overwhelmingly produced by Western internet users"

### Mechanism 2
- Claim: Linguistic fluency (surface form) is learned independently of cultural competence (values and practices)
- Mechanism: Models optimize for grammatical correctness without encoding socio-cultural norms, resulting in "fluent but foreign" outputs
- Core assumption: Cultural alignment requires more than translating instruction datasets; it requires "native, community-authored corpora"
- Evidence anchors: [abstract]: "It remains unclear whether they reflect local values and practices or merely speak local languages"

### Mechanism 3
- Claim: Cultural reasoning in regional models is brittle and fails to generalize when explicit norms are not provided
- Mechanism: Models may answer explicit questions but struggle to infer norms from implicit context; regional training does not inject necessary "thick" cultural context
- Core assumption: Cultural competence involves implicit reasoning, not just retrieving facts
- Evidence anchors: [abstract]: "Prompting and regional fine-tuning fail to recover alignment and can even degrade existing knowledge"

## Foundational Learning

- **The Inglehart-Welzel (IW) Cultural Map**: Primary coordinate system used to visualize model values; essential for interpreting model alignment results
- **Cultural alignment vs. linguistic fluency**: Models can generate grammatically correct text in a language while remaining culturally alien in the assumptions they encode
- **Thick vs. thin cultural data**: "Thin" fine-tuning on translated regional languages cannot offset the cultural priors from Western pretraining data

## Architecture Onboarding

### Component Map
Pretraining Data -> Base Model -> Fine-tuning Data -> Aligned Model

### Critical Path
Pretraining data (Western-centric) -> Base model (global values) -> Fine-tuning (insufficient cultural depth) -> Misaligned outputs

### Design Tradeoffs
- Pretraining scale vs. cultural balance: Larger pretraining improves performance but entrenches Western priors
- Translation vs. native content: Translated instruction data cannot substitute for community-authored cultural corpora

### Failure Signatures
- Models cluster with US values despite being "Indic" models
- Fine-tuning degrades rather than improves cultural reasoning
- User suggestions introduce Westernized elements

### Three First Experiments
1. Train a model from scratch on culturally balanced pretraining data and compare alignment
2. Fine-tune with explicit mapping between linguistic context and cultural value outcomes
3. Test with multiple Indian respondents to establish confidence intervals around human cultural positions

## Open Questions the Paper Calls Out
- Which approach to pluralism—overton, steerable, or distributional—most effectively captures cultural diversity in LLMs?
- How much culturally grounded pretraining data is needed to achieve meaningful cultural alignment?
- Do these findings generalize to other non-Western regions with different cultural characteristics from India?
- How can cultural evaluation move beyond treating nations as homogeneous units to capture subnational cultural variation?

## Limitations
- Assumes WVS and GlobalOpinionQA questions accurately capture cultural values rather than other demographic factors
- Single US respondent used as proxy may not represent US cultural values reliably
- Indian respondent's views assumed to represent authentic Indian cultural values without validation
- Small user study sample (115 participants) may not represent full diversity of Indian perspectives

## Confidence
- **High confidence**: Indic models do not show better cultural alignment with Indian values than global models
- **Medium confidence**: Pretraining data scarcity is the primary driver of misalignment
- **Medium confidence**: Linguistic fluency is decoupled from cultural competence
- **Low confidence**: Single US respondent is a reliable proxy for global cultural values

## Next Checks
1. Replicate evaluation using multiple respondents from both India and US to establish confidence intervals
2. Analyze pretraining corpora of both global and Indic models to quantify cultural distribution of training data
3. Conduct controlled experiment where Indic model is fine-tuned on dataset explicitly designed to map linguistic context to cultural value outcomes