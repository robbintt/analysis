---
ver: rpa2
title: 'CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM
  Era'
arxiv_id: '2503.12329'
source_url: https://arxiv.org/abs/2503.12329
tags:
- image
- human
- captioning
- detailed
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CapArena, the first large-scale human evaluation
  platform for detailed image captioning in the era of Vision-Language Models (VLMs).
  The study addresses the challenge of benchmarking VLMs on detailed captioning tasks,
  which have been overlooked in favor of other multimodal tasks like VQA and reasoning.
---

# CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era

## Quick Facts
- arXiv ID: 2503.12329
- Source URL: https://arxiv.org/abs/2503.12329
- Reference count: 27
- Primary result: Introduces CapArena, the first large-scale human evaluation platform for detailed image captioning, achieving 94.3% correlation with human rankings via VLM-as-a-Judge at $4 per test

## Executive Summary
This paper introduces CapArena, the first large-scale human evaluation platform for detailed image captioning in the era of Vision-Language Models (VLMs). The study addresses the challenge of benchmarking VLMs on detailed captioning tasks, which have been overlooked in favor of other multimodal tasks like VQA and reasoning. The authors developed CapArena with over 6000 pairwise caption battles, where human annotators compared captions generated by 14 advanced VLMs and human-written descriptions. The results show that top models like GPT-4o achieve or surpass human-level performance in detailed captioning, while most open-source models lag behind, with InternVL2-26B as a notable exception. The study also evaluates traditional and recent captioning metrics, revealing that while some metrics like METEOR show decent caption-level agreement with humans, they suffer from systematic biases that affect model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Based on these insights, the authors release CapArena-Auto, an automated benchmark for detailed captioning that achieves 94.3% correlation with human rankings at a low cost of $4 per test.

## Method Summary
CapArena uses a pairwise battle methodology where human annotators compare captions generated by 14 VLMs against three baseline models (GPT-4o, CogVLM-19B, MiniCPM-8B) on 600 DOCCI dataset images. The pairwise preferences are aggregated using the Bradley-Terry model to produce ELO rankings. The authors also developed CapArena-Auto, which uses GPT-4o as a VLM-as-a-Judge with structured guidelines to automatically evaluate caption pairs. This automated approach achieves 94.3% correlation with human rankings at a fraction of the cost ($4 per test versus expensive human annotation).

## Key Results
- GPT-4o and Gemini-1.5-Pro achieve human-level or superior performance in detailed captioning
- Traditional metrics like BLEU and CIDEr show poor alignment with human preferences, while VLM-as-a-Judge demonstrates robust discernment
- InternVL2-26B emerges as the best open-source model, surpassing many proprietary alternatives
- CapArena-Auto achieves 94.3% correlation with human rankings at $4 per test

## Why This Works (Mechanism)

### Mechanism 1: VLM-as-a-Judge for Detailed Caption Evaluation
- Claim: Using GPT-4o as an evaluator achieves higher alignment with human preferences than traditional metrics for detailed captioning
- Mechanism: The VLM evaluator follows structured guidelines (precision, informativeness, hallucination penalties) to compare two captions against an image, then outputs a reasoned judgment
- Core assumption: VLMs possess sufficient fine-grained visual perception and reasoning to discriminate subtle quality differences in long captions
- Evidence anchors: GPT-4o evaluator achieves 0.628 caption-level agreement and 0.943 Spearman model-level correlation, outperforming METEOR, CLIPScore, and other metrics
- Break condition: Fails when evaluating Level 3/4 closely-matched caption pairs (agreement drops to 0.557–0.560 vs. human 0.620)

### Mechanism 2: Pairwise Comparison Reduces Annotation Subjectivity
- Claim: Head-to-head caption battles yield higher inter-annotator consistency than absolute scoring
- Mechanism: Annotators judge which of two captions is better rather than assigning absolute scores, reducing cognitive load
- Core assumption: Relative comparison is inherently more reliable than absolute judgment for subjective tasks
- Evidence anchors: Inter-annotator agreement reaches 0.782 across 400 validation samples; authors originally tried scoring system but found low consistency
- Break condition: Degrades when caption pairs are nearly equivalent in quality, leading to more ties and reduced discrimination

### Mechanism 3: Reference Captions Reduce Evaluator Uncertainty
- Claim: Providing human-written reference captions improves VLM-as-a-Judge agreement with human rankings
- Mechanism: References help the evaluator disambiguate uncertain image details (e.g., object identity, spatial relations) by providing a ground-truth anchor
- Core assumption: References are accurate and representative
- Evidence anchors: Reference descriptions help the evaluator clarify uncertain image details, improving model-level agreement (Spearman 0.943 → 0.946)
- Break condition: If references contain hallucinations or errors, they may mislead the evaluator rather than help

## Foundational Learning

- **Vision-Language Models (VLMs)**: Why needed here: The entire benchmark evaluates VLM captioning capabilities; understanding that VLMs combine visual encoders with LLMs for multimodal tasks is prerequisite
- **Bradley-Terry Model & ELO Ratings**: Why needed here: CapArena aggregates pairwise battle outcomes into model rankings using this statistical framework
- **Captioning Metrics Landscape (BLEU, METEOR, CIDEr, CLIPScore)**: Why needed here: The paper's central critique is that these metrics fail for detailed captions; understanding their design clarifies why

## Architecture Onboarding

- **Component map**: Image pool -> Caption generators (14 VLMs) -> Pairwise battle engine -> Ranking aggregator (Bradley-Terry) -> VLM-as-a-Judge module
- **Critical path**: 1. Select image from curated pool; 2. Generate captions from target model and baseline; 3. VLM-judge compares caption pair against image (+ optional reference); 4. Aggregate wins/losses/ties across 600 samples; 5. Compute final score and rank
- **Design tradeoffs**: Pairwise vs. scoring (higher consistency vs. inability to capture absolute quality); 3 baselines vs. 1 (reduces single-baseline bias but triples evaluation cost); VLM-judge vs. human (94.3% correlation at $4/test vs. human annotation expense)
- **Failure signatures**: Systematic bias toward longer captions; tie inflation reducing ranking discriminability; hallucination propagation from references; domain mismatch beyond everyday-life images
- **First 3 experiments**: 1. Baseline correlation check: Run CapArena-Auto on 3-5 VLMs, compare Spearman correlation against human rankings; target ≥0.90; 2. Ablate reference captions: Evaluate same model set with and without references; quantify agreement delta; 3. Stress-test on Level 4 pairs: Curate 50 caption pairs from similarly-ranked models; measure if VLM-judge approaches human inter-annotator agreement

## Open Questions the Paper Calls Out
- How can we improve automated evaluators to match human performance on hard-to-distinguish caption pairs (Level 3/4 battles)?
- How does detailed captioning performance generalize to specialized domains such as medical imagery or artwork?
- Why does strong performance on general multimodal benchmarks (e.g., MMMU) not correlate with superior detailed captioning ability?

## Limitations
- The benchmark relies on everyday-life images from DOCCI, not representing specialized domains like medical or artwork
- Evaluating hard-to-distinguish caption pairs remains challenging for both human annotators and automated judges
- The methodology requires generating captions for multiple baselines, tripling evaluation costs

## Confidence
- **High Confidence**: Human evaluation results showing GPT-4o and Gemini-1.5-Pro achieving human-level performance; systematic bias analysis of traditional metrics
- **Medium Confidence**: VLM-as-a-Judge correlation metrics and CapArena-Auto benchmark reliability; pairwise comparison methodology effectiveness
- **Low Confidence**: Generalization to non-DOCCI domains (medical, satellite, artwork); long-term stability of VLM-as-a-Judge as VLMs evolve

## Next Checks
1. Cross-domain validation: Test CapArena-Auto on 100 images from medical imaging, satellite imagery, and artwork datasets to verify domain robustness
2. Temporal stability test: Rerun VLM-as-a-Judge evaluation after 6 months to measure consistency as VLMs receive updates
3. Hallucination propagation analysis: Conduct a systematic study on how reference caption errors affect VLM-judge evaluations across 200 image-caption pairs