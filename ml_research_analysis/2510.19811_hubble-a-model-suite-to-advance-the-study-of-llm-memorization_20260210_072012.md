---
ver: rpa2
title: 'Hubble: a Model Suite to Advance the Study of LLM Memorization'
arxiv_id: '2510.19811'
source_url: https://arxiv.org/abs/2510.19811
tags:
- data
- memorization
- training
- urlhttps
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Hubble suite provides eight open-source large language models
  designed to study memorization risks across copyright, privacy, and test set contamination
  domains. The models are trained with controlled insertion of perturbed data at varying
  frequencies and training phases to measure memorization under different conditions.
---

# Hubble: a Model Suite to Advance the Study of LLM Memorization

## Quick Facts
- **arXiv ID:** 2510.19811
- **Source URL:** https://arxiv.org/abs/2510.19811
- **Reference count:** 40
- **Key outcome:** Eight open-source models with controlled sensitive data insertion to study memorization across copyright, privacy, and test set contamination domains.

## Executive Summary
Hubble provides eight open-source language models specifically designed to study memorization risks in LLMs. The models incorporate controlled insertions of sensitive data (copyrighted text, personally identifiable information, and test sets) at varying frequencies and training phases. Through systematic experiments across different model scales and corpus sizes, Hubble demonstrates that memorization risks are determined by the relative frequency of sensitive data in the training corpus, that larger models memorize more readily than smaller ones, and that sensitive data without continued exposure can be forgotten. The suite establishes benchmarks for membership inference and machine unlearning while providing publicly available models, data, and evaluation code.

## Method Summary
Hubble consists of eight transformer-based models trained with controlled insertions of sensitive data (copyright, PII, test sets) at varying duplication levels and training phases. The models use Llama 3 architecture with OLMo tokenizer and are trained on DCLM-filtered web text with perturbations spliced in at document boundaries. Duplication levels range from 0 to 256 copies, with perturbations inserted either early (0-25% of training) or late (75-100%). Memorization is measured through loss, accuracy, and generative metrics across perturbed vs unperturbed samples. The suite includes 1B and 8B models trained on 100B and 500B token corpora to study scaling effects.

## Key Results
- Increasing training corpus size from 100B to 500B tokens reduces memorization by diluting sensitive data relative frequency
- Larger models (8B vs 1B) memorize at lower duplication thresholds, showing greater capacity for verbatim retention
- Early placement of sensitive data (first 25% of training) results in reduced final memorization compared to late placement
- Current unlearning methods fail to target specific data instances without degrading performance on semantically similar content

## Why This Works (Mechanism)

### Mechanism 1: Dilution via Corpus Scaling
- **Why needed:** Explains why larger corpora reduce memorization risks at equivalent duplication levels
- **Quick check:** Would this effect hold if sensitive data were concentrated in specific domains rather than uniformly distributed?

### Mechanism 2: Forgetting Through Early Placement
- **Why needed:** Demonstrates how lack of reinforcement causes early-learned information to decay
- **Quick check:** What conditions would cause early-learned information to persist despite lack of reinforcement?

### Mechanism 3: Duplication-Threshold Memorization
- **Why needed:** Shows how repetition strengthens neural pathways and how model capacity affects memorization thresholds
- **Quick check:** Would paraphrased variants have the same duplication threshold effects as verbatim repeats?

## Foundational Learning

- **Counterfactual memorization:** What a model would not know had it not seen specific data. Needed because Hubble's randomized duplication enables causal estimation by comparing identical data at different exposure levels. Quick check: Can you explain why random assignment of duplication levels enables causal inference about memorization?

- **Membership Inference Attacks (MIA):** Determining whether a given sample was in training data. Needed because Hubble's lack of spurious temporal/length confounds makes it a benchmark for evaluating MIA methods. Quick check: Why do temporal cutoffs (as in WIKIMIA) create spurious features that trivialize MIA?

- **Gradient interference and catastrophic forgetting:** How subsequent gradient updates overwrite prior learning. Needed because understanding early placement forgetting requires knowing how optimization pressure shifts. Quick check: What conditions would cause early-learned information to persist despite lack of reinforcement?

## Architecture Onboarding

- **Component map:** DCLM corpus -> TokenSmith splicing -> GPT-NeoX training -> DeepSpeed ZeRO-1 optimization -> Hubble models

- **Critical path:** 1) Decontaminate base corpus against perturbation data 2) Tokenize with OLMo tokenizer → binary format 3) Randomly assign perturbations to duplication bins 4) Splice perturbations into training sequences surrounded by EOS tokens 5) Train with GPT-NeoX/Megatron + DeepSpeed ZeRO-1

- **Design tradeoffs:** Corpus size (100B vs 500B): Dilution benefit vs compute cost; 500B requires ~5x GPU hours. Model scale (1B vs 8B): Detection sensitivity vs resource requirements; 8B memorizes at lower duplicates but costs ~7x more. Perturbation timing: Early placement reduces memorization but may affect downstream task transfer.

- **Failure signatures:** High variance in memorization metrics at low duplication → insufficient samples per bin. Spurious correlations between duplication and text length → improper stratification. Test set contamination detection fails → decontamination pipeline gaps.

- **First 3 experiments:** 1) Replicate dilution effect: Train 1B on 100B vs 500B with identical perturbations, compare loss on duplicated vs unduplicated samples. 2) Validate timing effects: Insert perturbations in first vs last quarter of training, measure final memorization on held-out duplicates. 3) Probe PII-type variation: Attack YAGO biographies with different auxiliary information levels (full-prefix vs name-only), confirm that more context yields higher success rates.

## Open Questions the Paper Calls Out

- **Interpretability methods:** How can interpretability methods leverage Hubble's controlled perturbations to isolate the specific mechanisms of memorization versus generalization? Prior work struggles to disentangle whether a sentence is memorized because it is simple or because it was repeated; controlled insertions allow for the isolation of these factors.

- **Robust memorization metrics:** What robust metrics can effectively measure memorization for legal and policy contexts, such as copyright disputes? Current metrics (e.g., loss vs. k-eidetic) yield conflicting interpretations of "memorization," creating ambiguity for regulatory standards and fair use determination.

- **Quantization effects:** Can model quantization generally reduce memorization risks while maintaining general capabilities? While quantization is known to affect model performance, its specific efficacy as a trade-off strategy for reducing verbatim extraction without destroying utility is not yet established.

- **Precise unlearning:** Can unlearning methods be developed to target specific data instances (e.g., copyrighted text) without degrading performance on semantically similar "keep" data? Current unlearning techniques lack the precision to distinguish between the specific target data and its semantic neighbors, leading to excessive collateral damage to model utility.

## Limitations

- Scaling generalizability: The 100B→500B corpus scaling effect remains untested beyond the current study and may not hold for sensitive data concentrated in specific linguistic contexts.
- Temporal ordering robustness: The forgetting mechanism assumes monotonic decay, but doesn't explore scenarios where early data forms foundational representations that subsequent learning builds upon.
- Metric sensitivity differences: The study reveals significant variation in memorization detection thresholds across metrics but doesn't fully characterize which metric best captures practical memorization risks.

## Confidence

- **High Confidence:** The fundamental observation that larger models memorize at lower duplication levels is well-supported by multiple experiments and aligns with broader literature on model capacity effects on memorization.
- **Medium Confidence:** The corpus scaling dilution effect shows consistent patterns within the tested ranges, but generalization to arbitrary corpus sizes and compositions requires additional validation.
- **Medium Confidence:** Timing effects demonstrate clear patterns in the controlled setting, though real-world applicability depends on unknown factors about how foundation models interleave diverse training materials.

## Next Checks

1. **Corpus composition sensitivity:** Test whether the dilution effect holds when sensitive data is concentrated in specific domains or time periods rather than uniformly distributed, using synthetic corpora with controlled clustering of sensitive sequences.

2. **Scaffolded learning effects:** Design experiments where early-placed perturbations serve as foundational concepts for later training materials, measuring whether scaffolded early data resists forgetting more effectively than isolated early insertions.

3. **Metric calibration study:** Conduct controlled experiments varying duplication levels and measuring correlation between different memorization metrics (loss, accuracy, word recall) and actual information extraction success rates by adversarial attackers.