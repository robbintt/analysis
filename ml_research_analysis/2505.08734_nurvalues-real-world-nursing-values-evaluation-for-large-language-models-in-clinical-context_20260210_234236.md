---
ver: rpa2
title: 'NurValues: Real-World Nursing Values Evaluation for Large Language Models
  in Clinical Context'
arxiv_id: '2505.08734'
source_url: https://arxiv.org/abs/2505.08734
tags:
- llms
- nursing
- nurse
- value
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces NurValues, the first real-world benchmark
  for evaluating large language models'' alignment with nursing values. The benchmark
  consists of 1,100 real-world nursing behavior instances collected through a five-month
  longitudinal field study across three hospitals, covering five core value dimensions:
  Altruism, Human Dignity, Integrity, Justice, and Professionalism.'
---

# NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context

## Quick Facts
- arXiv ID: 2505.08734
- Source URL: https://arxiv.org/abs/2505.08734
- Reference count: 40
- This paper introduces NurValues, the first real-world benchmark for evaluating large language models' alignment with nursing values.

## Executive Summary
This paper introduces NurValues, the first real-world benchmark for evaluating large language models' alignment with nursing values. The benchmark consists of 1,100 real-world nursing behavior instances collected through a five-month longitudinal field study across three hospitals, covering five core value dimensions: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The dataset is augmented with LLM-generated counterfactuals to create an Easy-Level set (2,200 samples) and further transformed into dialogue-based formats with contextual cues to form a Hard-Level set (2,200 samples). Evaluation of 23 state-of-the-art LLMs reveals that DeepSeek-V3 achieves the highest performance on the Easy-Level dataset (94.55%), while Claude 3.5 Sonnet leads on the Hard-Level dataset (89.43%), significantly outperforming medical LLMs. Justice is consistently the most challenging value dimension for LLMs to assess. The study demonstrates that in-context learning strategies significantly improve alignment, and the benchmark effectively distinguishes model capabilities under both standard and complex ethical contexts.

## Method Summary
The study collected 1,100 real-world nursing behavior instances from three Chinese hospitals over five months, annotated by clinical nurses across five core value dimensions. The dataset was augmented with LLM-generated counterfactuals to create an Easy-Level set (2,200 samples) and transformed into adversarial dialogue-based formats with misleading cues to form a Hard-Level set (2,200 samples). The benchmark evaluates 23 LLMs using zero-shot I/O prompting, Chain-of-Thought, Self-Consistency, and K-Shot strategies, measuring binary classification accuracy for value alignment across both difficulty levels.

## Key Results
- DeepSeek-V3 achieves highest performance on Easy-Level dataset (94.55%)
- Claude 3.5 Sonnet leads on Hard-Level dataset (89.43%), outperforming medical LLMs
- Justice is consistently the most challenging value dimension for LLMs to assess
- In-context learning strategies, particularly Chain-of-Thought, significantly improve alignment performance
- General-purpose LLMs outperform medical LLMs on nursing value reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial dialogue transformation reduces model performance by introducing narrative bias and misleading signals.
- **Mechanism:** The system converts straightforward "Easy-Level" cases into "Hard-Level" multi-turn dialogues. These dialogues embed "contextual cues and subtle misleading signals" (e.g., persuasion, traps, deception). This increases the cognitive load on the LLM, forcing it to disentangle factual events from the narrator's subjective framing (positional bias, emotional embellishments) before making an ethical judgment.
- **Core assumption:** Models struggle to separate factual clinical behavior from the rhetorical framing of a dialogue when the framing is designed to be manipulative.
- **Evidence anchors:**
  - [abstract] "Hard-Level dataset... embeds contextual cues and subtle misleading signals... increase adversarial complexity."
  - [PAGE 3] "dialogue format introduces subtle misleading signals and contextual noise to obscure moral clarity."

### Mechanism 2
- **Claim:** General-purpose alignment (RLHF) currently outperforms domain-specific fine-tuning for nursing value reasoning.
- **Mechanism:** Medical LLMs undergo Supervised Fine-Tuning (SFT) on clinical knowledge (diagnoses, Q&A), which optimizes for factual accuracy but may not reinforce ethical prioritization. General LLMs, benefiting from broader human feedback (RLHF) across diverse reasoning tasks, appear better equipped to handle the ambiguity and value conflicts present in the Hard-Level dataset.
- **Core assumption:** Domain-specific fine-tuning data used for medical LLMs lacks sufficient coverage of ethical nuances or value-based conflicts compared to general alignment corpora.
- **Evidence anchors:**
  - [PAGE 6] "General LLMs consistently outperform medical LLMs... suggests that domain-specific fine-tuning improves clinical Q&A but not ethical reasoning."
  - [PAGE 7] "domain-specific fine-tuning is insufficient for improving moral reasoning."

### Mechanism 3
- **Claim:** In-Context Learning (ICL) strategies, particularly Chain-of-Thought (CoT), mitigate the degradation caused by adversarial complexity.
- **Mechanism:** CoT prompting forces the model to explicitly reason through the scenario step-by-step rather than relying on pattern matching. In complex dialogues, this externalizes the process of filtering out "misleading signals," allowing the model to arrive at a more robust judgment. K-Shot examples provide immediate reference points for specific value dimensions (e.g., Justice vs. Professionalism).
- **Core assumption:** The model's internal representation of values is adequate, but the retrieval or activation of these values is disrupted by the adversarial dialogue structure; CoT restores this activation.
- **Evidence anchors:**
  - [PAGE 9] "CoT achieves the best overall results... DeepSeek-V3 improves... by 23.03."
  - [PAGE 2] "in-context learning strategies significantly improve alignment."

## Foundational Learning

- **Concept:** **Counterfactual Data Augmentation**
  - **Why needed here:** Real-world nursing data is often imbalanced (e.g., most behaviors are Professionalism, few are Justice). To create a fair benchmark, the authors use LLMs (o1) to generate "value-flipped" versions of cases (e.g., turning an altruistic act into a selfish one) to balance the dataset.
  - **Quick check question:** How does generating a counterfactual case (e.g., changing injection sites) help evaluate a model's ability to distinguish between *Integrity* and *Professionalism*?

- **Concept:** **The "Suspicious Bias" / False Negative Gap**
  - **Why needed here:** The paper identifies a specific failure mode where models (especially on the Hard-Level set) exhibit high False Negative rates. This means they often judge aligned behaviors as violations, likely due to safety training making them overly cautious or "suspicious" of adversarial-sounding text.
  - **Quick check question:** Why would a safety-aligned model tend to label a nurse's compliant action as a "violation" when that action is described in a dialogue with "traps"?

- **Concept:** **The Value-Action Gap in Clinical Context**
  - **Why needed here:** While the paper focuses on evaluation, understanding *alignment* requires distinguishing between a model stating a value and applying it. The benchmark tests application by presenting scenarios where the "correct" action conflicts with superficial cues or patient demands.
  - **Quick check question:** In the "Hard-Level" dialogues, does the model need to prioritize the *narrator's* emotional state or the *nurse's* adherence to the clinical code of ethics?

## Architecture Onboarding

- **Component map:** Field Study Module (5 months, 3 hospitals) -> Raw Instances (976) -> Annotation Pipeline (5 nurses, Fleiss' Îº=0.73) -> Augmentation Engine (o1 counterfactuals) -> Easy-Level Dataset (2,200) -> Adversarial Generator (o1 dialogue transformation) -> Hard-Level Dataset (2,200) -> Evaluation Harness (23 LLMs, I/O, CoT, K-Shot)
- **Critical path:** The **Adversarial Generator** is the core differentiator. It transforms the benchmark from a simple classification task into a reasoning robustness test. Without the dialogue transformation and misleading signal injection, model performance saturates (>90% accuracy on Easy-Level).
- **Design tradeoffs:**
  - **Real vs. Synthetic:** The base data is authentic (real hospitals), ensuring ecological validity. However, the *adversarial complexity* is synthetic (generated by o1). This risks testing the generator's style rather than universal reasoning, though the paper validates robustness by testing generation via DeepSeek-V3 and Claude as well.
  - **Coverage:** "Justice" is underrepresented in real-world data (only 74 samples), creating a statistical weakness in the benchmark despite its importance.
- **Failure signatures:**
  - **Performance Collapse:** DeepSeek-V3 drops from 94.55% (Easy) to 42.95% (Hard), indicating brittleness to narrative complexity.
  - **The "Medical Paradox":** Medical LLMs (e.g., Llama3-Med42) often perform worse than their base models on Hard-Level tasks, suggesting domain fine-tuning degrades general reasoning.
- **First 3 experiments:**
  1. **Baselining:** Run standard zero-shot inference on the Easy-Level set to establish the "knowledge" baseline. If a model fails here (e.g., <80%), it lacks basic nursing knowledge, making Hard-Level testing redundant.
  2. **Robustness Stress Test:** Evaluate the model on the Hard-Level set. Analyze the confusion matrix specifically for **False Negatives** (misclassifying aligned behavior as violations) to detect "safety over-alignment."
  3. **ICL Ablation:** Apply Chain-of-Thought (CoT) prompting to the Hard-Level set. If performance recovers significantly (e.g., >15% jump), the model has the capacity but lacked the retrieval triggers. If it doesn't, the model lacks the internal value representations entirely.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does LLM performance on nursing value alignment change when evaluated on data collected from diverse cultural backgrounds and healthcare systems?
  - **Basis in paper:** [explicit] The "Future Work" section states that the current data collection limited to mainland China represents a "regional and cultural bias" and outlines plans to "collaborate with institutions from diverse cultural backgrounds."
  - **Why unresolved:** The current study is restricted to three hospitals in China, limiting the generalizability of the findings to international nursing standards and ethical norms.
  - **What evidence would resolve it:** Evaluation results from a NurValues 2.0 dataset that includes cross-cultural cases, showing whether the ranking of models or the difficulty of specific value dimensions shifts significantly.

- **Open Question 2:** Can introducing more fine-grained nursing value dimensions reveal new or different failure modes in LLMs that are masked by the current five-dimension framework?
  - **Basis in paper:** [explicit] The "Future Work" section notes the limitation of "limited coverage of value dimensions" and proposes expanding the dataset to "present more fine-grained nursing value dimensions."
  - **Why unresolved:** The current five dimensions (Altruism, Human Dignity, Integrity, Justice, Professionalism) may aggregate distinct ethical nuances, potentially hiding specific areas where LLMs struggle or hallucinate.
  - **What evidence would resolve it:** An updated benchmark with sub-categories (e.g., distinguishing between resource allocation and non-discrimination within Justice) and a comparative analysis of error rates.

- **Open Question 3:** What specific alignment strategies beyond domain-specific fine-tuning are required to make medical LLMs match or exceed the ethical reasoning capabilities of general-purpose LLMs?
  - **Basis in paper:** [inferred] The analysis of medical LLMs in Section G concludes that "capability-driven fine-tuning is insufficient for improving moral reasoning" and that "dedicated alignment strategies are required."
  - **Why unresolved:** The paper demonstrates that medical LLMs underperform general models on the Hard-Level dataset but does not propose or test a method to correct this specific deficit.
  - **What evidence would resolve it:** A follow-up study where medical LLMs are trained using proposed alignment techniques (e.g., value-specific RLHF) and subsequently outperform their base models on the Hard-Level task.

## Limitations

- Dataset authenticity concerns due to heavy reliance on synthetic adversarial generation for Hard-Level complexity
- Geographic and cultural specificity limits generalizability beyond Chinese healthcare contexts
- Evaluation framework sensitivity to prompt engineering may affect observed performance differences between general and medical LLMs

## Confidence

**High Confidence:** The core claim that general LLMs outperform medical LLMs on nursing value reasoning tasks is well-supported by empirical results across multiple models.

**Medium Confidence:** The mechanism explaining why dialogue transformation degrades performance (narrative bias, misleading signals) is plausible but requires further validation.

**Low Confidence:** The assertion that domain-specific fine-tuning inherently degrades ethical reasoning capabilities is an overgeneralization.

## Next Checks

1. **Cross-Cultural Validation:** Test the benchmark with nursing scenarios from multiple cultural contexts (Western, Middle Eastern, Southeast Asian) to assess whether performance gaps persist across different healthcare value systems.

2. **Generator Independence Test:** Evaluate model performance using Hard-Level datasets generated by different LLM generators (o1, Claude, DeepSeek-V3) to determine if observed difficulty patterns are consistent or generator-specific artifacts.

3. **Medical LLM Fine-tuning Experiment:** Take a base model that performs well on the benchmark, apply domain-specific nursing knowledge fine-tuning using the same training corpora as medical LLMs, and re-evaluate to isolate whether fine-tuning methodology or training data composition drives the performance gap.