---
ver: rpa2
title: Cascading Bandits With Feedback
arxiv_id: '2511.10938'
source_url: https://arxiv.org/abs/2511.10938
tags:
- regret
- logt
- arms
- algorithm
- cascade
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes cascading bandits with feedback for edge inference,\
  \ where each arm corresponds to an ML model with an associated accuracy and error\
  \ probability. The study examines four decision-making policies\u2014Explore-then-Commit,\
  \ Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling\u2014\
  providing sharp theoretical regret guarantees for each."
---

# Cascading Bandits With Feedback

## Quick Facts
- arXiv ID: 2511.10938
- Source URL: https://arxiv.org/abs/2511.10938
- Reference count: 40
- Primary result: Adaptive policies (LCB, Thompson Sampling) achieve O(1) regret vs O(log T) for commit-based policies in cascading bandits with feedback

## Executive Summary
This paper analyzes cascading bandits with feedback for edge inference, where each arm corresponds to an ML model with an associated accuracy and error probability. The study examines four decision-making policies—Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling—providing sharp theoretical regret guarantees for each. The key finding is that policies which commit to a fixed ordering after exploration incur O(logT) regret, while adaptive policies LCB and Thompson Sampling achieve constant O(1) regret. Simulations validate these theoretical results, demonstrating that Thompson Sampling and LCB outperform Explore-then-Commit and Action Elimination in terms of regret minimization. The results highlight the crucial role of adaptivity for efficient edge inference under uncertainty.

## Method Summary
The paper proposes a cascade bandit framework for edge inference where K ML models (arms) are ordered sequentially, with each model having an accuracy μᵢ and error probability pᵢ. At each round, the cascade traverses the ordered arms until one outputs 1, then receives user feedback. The optimal static ordering sorts arms by increasing error probability pᵢ, independent of their accuracy means μᵢ. Four algorithms are analyzed: Explore-then-Commit with oracle gap knowledge, Action Elimination with confidence intervals, LCB with descending lower confidence bounds, and Thompson Sampling with Beta posteriors. Theoretical regret analysis shows commit-based policies have O(log T) regret while adaptive policies achieve O(1) regret. Simulations with K=5 arms over T∈{10³, 10⁴, 5×10⁴} rounds validate these findings.

## Key Results
- Commit-based policies (Explore-then-Commit, Action Elimination) incur Ω(log T) regret due to inability to self-correct after exploration
- Adaptive policies (LCB, Thompson Sampling) achieve O(1) regret by continuously updating arm orderings
- The optimal static ordering sorts arms by increasing error probability pᵢ, independent of accuracy means μᵢ
- Thompson Sampling and LCB significantly outperform commit-based policies in empirical regret minimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In the cascade bandit setting, static commitment algorithms (Explore-then-Commit, Action Elimination) are fundamentally limited to Ω(log T) regret.
- Mechanism: Commitment algorithms lock in an arm ordering after exploration. If the exploration phase produces an incorrect ordering (with probability bounded away from zero), the algorithm cannot self-correct, leading to persistent regret. The lower bound arises from the fundamental tradeoff between exploration length and the probability of committing to a suboptimal ordering (Lemma 4 in the Appendix proves α₁f(t) + α₂e^{-α₃f(t)}(t - f(t)) = Ω(log t) for any choice of f(t)).
- Core assumption: The arms' error probabilities are stationary (stochastic setting). The learner has no prior knowledge of the suboptimality gaps Δᵢ.
- Evidence anchors:
  - [abstract] "Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt."
  - [section] Theorem 3: "The regret of Algorithm 1 is lower bounded as R_EC(T) = Ω(log T)." Theorem 5: "R_AE(T) = Ω(log T)."
  - [corpus] Weak or missing direct evidence on lower bounds for commit-based policies in cascading bandits.
- Break condition: If the environment is non-stationary or if suboptimality gaps are known exactly a priori, the Ω(log T) lower bound may not apply directly.

### Mechanism 2
- Claim: Adaptive algorithms (Lower Confidence Bound, Thompson Sampling) achieve O(1) regret by continuously updating arm orderings based on observed feedback.
- Mechanism: LCB maintains confidence bounds on error probabilities and reorders arms each round by ascending lower confidence bound, naturally prioritizing arms with lower estimated error or higher uncertainty. Thompson Sampling maintains Beta posteriors over error probabilities and samples from them to randomize exploration while favoring low-error arms. Both avoid permanent commitment, allowing them to self-correct even after initial errors.
- Core assumption: Feedback is stochastic and immediate (not delayed). Arms' triggering probabilities μᵢ are stationary.
- Evidence anchors:
  - [abstract] "LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret."
  - [section] Theorem 6: "The regret obtained by Algorithm 3 is R_LCB(T) = O(1)." Theorem 7: "R_TS(T) = O(1)."
  - [corpus] Neighbor papers on Thompson Sampling (arXiv:2502.02140, arXiv:2511.02123) support the general behavior of TS in bandits but do not directly address cascade-specific O(1) regret.
- Break condition: If feedback is delayed or the number of arms K is very large with many near-optimal orderings, constant regret may not hold (the constants depend on gaps Δᵢ and triggering probabilities μ̄ᵢ).

### Mechanism 3
- Claim: The optimal static ordering sorts arms by increasing error probability pᵢ, independent of their sampling means μᵢ.
- Mechanism: The expected reward for ordering L is r_L = Σᵢ (1 - p_{lᵢ}) μ_{lᵢ} Π_{j<i} (1 - μ_{l_j}). Swapping any adjacent pair with p_{lᵢ} > p_{lᵢ₊₁} strictly increases reward, as shown by the contradiction proof in Theorem 1.
- Core assumption: Error probabilities pᵢ are strictly ordered (p₁ < p₂ < ... < p_K). All μᵢ > 0.
- Evidence anchors:
  - [section] Theorem 1: "The optimal static policy will order the arms in increasing order of their error probabilities (pᵢ)." Remark 1: "The optimal ordering of arms is independent of their means (μᵢ)."
  - [corpus] No direct corpus evidence on optimal ordering by error probability in cascading bandits.
- Break condition: If arms have identical error probabilities, the ordering is non-unique. If costs or latency are introduced, the optimal ordering may change.

## Foundational Learning

- Concept: Multi-armed bandit regret
  - Why needed here: The paper defines cumulative regret as the difference between the expected reward of the optimal static policy and the learner's policy. Understanding regret is essential to compare algorithm performance.
  - Quick check question: What is the difference between simple regret and cumulative regret?

- Concept: Confidence intervals and concentration inequalities (Hoeffding)
  - Why needed here: LCB constructs lower confidence bounds using √(2 log t / Sᵢ(t)). The proofs rely on Hoeffding's inequality to bound the probability of estimation errors.
  - Quick check question: How does the width of a confidence interval scale with the number of samples?

- Concept: Beta-Bernoulli conjugacy
  - Why needed here: Thompson Sampling uses Beta(αᵢ, βᵢ) priors/posteriors for Bernoulli error probabilities. Understanding this conjugacy is required to implement and analyze the algorithm.
  - Quick check question: If you observe 3 successes and 7 failures from a Beta(1, 1) prior, what are the posterior parameters?

## Architecture Onboarding

- Component map: `CascadeEnvironment` -> `Policy` -> `Simulator`
- Critical path:
  1. Initialize arm parameters (μ, p) and policy.
  2. For each round t: policy selects ordering L_t; environment determines I_t = min{j : X_{l_j}(t) = 1} (or ∞); observe Y if I_t ≠ ∞; policy updates.
  3. Compute instantaneous regret R_t = r_{L*} - r_{L_t}; accumulate.
- Design tradeoffs:
  - EC: Simple, but requires knowing gaps Δᵢ to set exploration budget N. Regret grows as O(log T).
  - AE: Eliminates arms adaptively, still commits, still O(log T).
  - LCB: No commitment, O(1) regret, but requires confidence bound tuning (√(2 log t / Sᵢ(t))).
  - TS: O(1) regret, naturally randomizes exploration, but stochasticity can increase variance in early rounds.
- Failure signatures:
  - Regret grows linearly → likely bug in ordering logic (not sorting by pᵢ or LCB/θ correctly).
  - EC/AE regret does not plateau after commit → commit condition never triggered or exploration budget too small.
  - TS/LCB regret grows faster than constant → confidence bounds or posterior updates implemented incorrectly.
- First 3 experiments:
  1. Reproduce Fig. 2: K=5 arms with given μ and p; run all four algorithms for T ∈ {10³, 10⁴, 10⁵}; plot cumulative regret vs. T to verify EC/AE ~ O(log T) and TS/LCB ~ O(1).
  2. Sensitivity to gaps: Vary p₁ and p₂ to change Δ₂; observe how the constant in O(1) regret for LCB/TS scales with 1/Δ².
  3. Robustness to initialization: Run TS with different prior strengths (Beta(1,1) vs. Beta(0.1,0.1) vs. Beta(10,10)); measure early-regret variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the $O(1)$ regret guarantees for LCB and Thompson Sampling be maintained in non-stationary environments where model error probabilities drift over time?
- Basis: [inferred] The theoretical analysis relies on the assumption that arm outputs are i.i.d. Bernoulli random variables with fixed means, which may not hold for dynamic edge inference scenarios mentioned in the introduction.
- Why unresolved: The current proofs for constant regret depend on stable confidence bounds and posterior convergence, which may fail if the underlying error distributions change.
- What evidence would resolve it: A theoretical analysis or empirical simulation showing regret bounds for LCB/TS under a standard drifting or switching environment model.

### Open Question 2
- Question: How can the cascade bandit framework be extended to incorporate contextual information regarding the specific inference request?
- Basis: [inferred] The paper treats model error probabilities as static values ($p_i$), whereas real-world edge inference often requires adapting model selection based on the specific input data characteristics.
- Why unresolved: The current algorithms order arms based on global estimates, lacking a mechanism to condition the ordering on query features (context).
- What evidence would resolve it: A modification of the proposed Thompson Sampling or LCB algorithm that inputs context vectors and maintains regret guarantees in a contextual cascade setting.

### Open Question 3
- Question: Does the optimal ordering policy change if a cost or latency constraint is integrated into the optimization objective?
- Basis: [inferred] The Introduction highlights "reducing latency" and computational burden as primary motivations, yet the mathematical model optimizes solely for error probability ($p_i$) without explicitly modeling inference cost or delay.
- Why unresolved: It is unclear if the optimal policy remains a simple ordering by error probability when models have heterogeneous latencies or costs.
- What evidence would resolve it: A proof of optimality or regret analysis for a modified reward function that penalizes latency (e.g., $r_L = \dots - \text{cost}$).

## Limitations

- The analysis assumes stationary arm parameters and i.i.d. rewards, which may not hold in dynamic edge inference environments
- The Explore-then-Commit algorithm requires oracle knowledge of suboptimality gaps Δᵢ, contradicting the assumption that these gaps are unknown
- The O(1) regret constants depend on problem-specific parameters that may be difficult to characterize in practice
- The simulation setup uses a specific 5-arm configuration that may not generalize to larger or more heterogeneous model pools

## Confidence

- **High confidence**: The theoretical analysis of optimal ordering by error probability pᵢ is rigorously proven (Theorem 1). The O(log T) lower bound for commit-based policies follows logically from the exploration-commitment tradeoff.
- **Medium confidence**: The O(1) regret analysis for LCB and Thompson Sampling relies on concentration inequalities and may be sensitive to parameter choices (confidence bound width, prior strength). The constants hidden in the O(1) notation depend on unknown problem-specific parameters.
- **Low confidence**: The simulation results showing clear separation between O(log T) and O(1) regimes, particularly for the EC algorithm which requires oracle knowledge of gaps. The exact implementation details for tie-breaking and initialization are not fully specified.

## Next Checks

1. **Robustness to initialization**: Run Thompson Sampling with varying prior strengths (Beta(0.1,0.1), Beta(1,1), Beta(10,10)) to quantify sensitivity of early-regret variance and convergence speed.

2. **Non-stationary stress test**: Modify the simulation to vary error probabilities pᵢ by ±10% after T/2 rounds. Measure how regret scaling changes for each algorithm, particularly testing the O(1) guarantees for LCB and TS.

3. **Delayed feedback scenario**: Introduce a delay of D rounds between arm selection and feedback observation. Evaluate how regret scaling changes for each algorithm, with particular attention to whether LCB and TS maintain their O(1) guarantees or degrade toward O(log T).