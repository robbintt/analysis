---
ver: rpa2
title: How Persuasive is Your Context?
arxiv_id: '2509.17879'
source_url: https://arxiv.org/abs/2509.17879
tags:
- context
- language
- reviews
- answer
- distance-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the targeted persuasion score (TPS) to measure
  how persuasive a context is to a language model. TPS uses Wasserstein distance to
  quantify how much a context shifts a model's probability distribution toward a target
  answer.
---

# How Persuasive is Your Context?

## Quick Facts
- arXiv ID: 2509.17879
- Source URL: https://arxiv.org/abs/2509.17879
- Reference count: 40
- Primary result: TPS uses Wasserstein distance to measure how contexts shift model probability distributions toward target answers

## Executive Summary
This paper introduces the Targeted Persuasion Score (TPS), a novel metric for measuring how persuasive a context is to a language model. Unlike previous metrics that rely on greedy decoding, TPS uses Wasserstein distance to quantify both the magnitude and direction of shifts in a model's probability distribution toward a target answer. The metric provides a more nuanced understanding of context influence on language model behavior.

The authors demonstrate TPS through experiments on various tasks, showing it captures subtle model behaviors invisible to traditional metrics. In political text annotation tasks, TPS reveals that detailed technical prompts induce only small, inconsistent shifts toward expert annotations. The work suggests TPS is valuable for analyzing context influence and controlling model behavior in applied settings.

## Method Summary
TPS measures contextual influence by computing the Wasserstein distance between a language model's probability distribution before and after exposure to a context, with respect to a target answer. The metric quantifies how much a context shifts the model's distribution toward the target, accounting for both magnitude and direction of change. The approach involves computing probability distributions over possible continuations with and without the context, then measuring the distance between these distributions using the Wasserstein metric.

## Key Results
- TPS captures nuanced model behavior invisible to greedy decoding metrics
- Political text annotation experiments show small, inconsistent shifts toward expert annotations with detailed technical prompts
- TPS effectively measures both magnitude and direction of context influence on probability distributions

## Why This Works (Mechanism)
TPS works by leveraging Wasserstein distance as a principled way to measure distributional shifts in probability space. When a context is introduced, it alters the language model's probability distribution over possible continuations. By measuring the distance between the original and shifted distributions relative to a target answer, TPS captures not just whether the model changes its behavior, but how much and in what direction it moves toward the desired outcome. This approach is more informative than simple success/failure metrics because it provides a continuous measure of influence that can detect subtle shifts even when the model doesn't fully converge to the target answer.

## Foundational Learning
- Wasserstein distance (Earth Mover's Distance): A metric for measuring the distance between probability distributions by quantifying the minimum "work" needed to transform one distribution into another. Needed to capture both magnitude and direction of distributional shifts in a mathematically principled way. Quick check: Verify that Wasserstein distance satisfies the properties of a metric (non-negativity, identity of indiscernibles, symmetry, triangle inequality).

- Probability distribution manipulation: Understanding how contexts alter a model's predictive distributions over tokens or sequences. Needed to quantify the effect of persuasion at the distributional level rather than just output level. Quick check: Ensure probability distributions sum to 1 and are properly normalized after context application.

- Targeted evaluation metrics: Metrics that measure progress toward specific target answers rather than just overall performance. Needed to assess how effectively contexts shift model behavior toward desired outcomes. Quick check: Validate that the target answer is correctly specified and that the metric properly measures distance to this target.

## Architecture Onboarding
- Component map: Context -> Language Model (with and without context) -> Probability Distributions -> Wasserstein Distance Calculation -> TPS Score
- Critical path: Context → Model inference → Distribution computation → Distance measurement → Persuasion quantification
- Design tradeoffs: TPS vs. greedy decoding (TPS captures magnitude/direction but is computationally heavier; greedy is faster but only binary success/failure)
- Failure signatures: TPS scores near zero indicate contexts have minimal persuasive effect; inconsistent scores across similar contexts suggest metric sensitivity to irrelevant factors
- First experiments: 1) Compute TPS for simple context-target pairs with known relationships 2) Compare TPS scores across different model sizes on identical tasks 3) Test TPS sensitivity to temperature scaling in model inference

## Open Questions the Paper Calls Out
The paper raises questions about whether Wasserstein distance is the optimal metric for measuring persuasion, suggesting alternative distance measures might be more appropriate. The effectiveness of TPS in practical applications, particularly for political text annotation, remains uncertain given the small and inconsistent shifts observed. The paper also questions how TPS generalizes across different domains beyond the tested political annotation task.

## Limitations
- Small and inconsistent shifts toward expert annotations in political text tasks suggest limited practical effectiveness
- Lack of comparison to alternative distributional distance metrics beyond greedy decoding
- Potential confounding factors like model size, training data composition, and temperature settings not fully addressed

## Confidence
- High confidence: The mathematical formulation of TPS using Wasserstein distance is well-defined and reproducible
- Medium confidence: TPS provides novel insights into context influence not captured by greedy decoding metrics
- Low confidence: TPS effectively measures persuasion in practical applications, particularly for political text annotation

## Next Checks
1. Test TPS across multiple model architectures (different families and sizes) to assess generalizability
2. Compare TPS performance against alternative distributional distance metrics (KL divergence, Jensen-Shannon) on the same tasks
3. Conduct ablation studies varying prompt complexity and technical detail levels to better understand what aspects of context drive TPS scores