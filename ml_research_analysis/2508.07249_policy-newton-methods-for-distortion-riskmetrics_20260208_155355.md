---
ver: rpa2
title: Policy Newton methods for Distortion Riskmetrics
arxiv_id: '2508.07249'
source_url: https://arxiv.org/abs/2508.07249
tags:
- policy
- algorithm
- gradient
- lemma
- hessian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses risk-sensitive reinforcement learning by optimizing
  distortion riskmetrics (DRMs) in finite-horizon Markov decision processes. The authors
  derive a policy Hessian theorem for DRM objectives and propose a cubic-regularized
  policy Newton algorithm (CRPN-DRM) that estimates both gradient and Hessian from
  sample trajectories.
---

# Policy Newton methods for Distortion Riskmetrics

## Quick Facts
- arXiv ID: 2508.07249
- Source URL: https://arxiv.org/abs/2508.07249
- Reference count: 40
- First convergence guarantee to ε-SOSP for risk-sensitive RL objectives with O(ε⁻³·⁵) sample complexity

## Executive Summary
This paper develops a policy Newton method for optimizing distortion riskmetrics (DRMs) in finite-horizon Markov decision processes. The authors derive a policy Hessian theorem for DRM objectives and propose a cubic-regularized policy Newton algorithm (CRPN-DRM) that converges to ε-second-order stationary points. The method uses likelihood ratio estimators for both gradient and Hessian, enabling sample-based optimization of risk-sensitive objectives. Experiments on cliff walk, cart pole, and humanoid environments demonstrate that DRM-sensitive policies achieve higher expected returns than risk-neutral counterparts by adopting risk-seeking behaviors.

## Method Summary
CRPN-DRM optimizes the DRM objective ρh(Rθ) by estimating both gradient and Hessian from sampled trajectories. The algorithm samples mk trajectories to estimate the gradient and bk trajectories to estimate the Hessian using likelihood ratio methods. The Hessian estimator uses order statistics of trajectory returns to efficiently compute the second-order information. A cubic-regularized Newton update with parameter α ≥ 3LH ensures convergence to ε-SOSP by escaping saddle points. The policy is updated by solving a cubic subproblem that balances first-order, second-order, and regularization terms.

## Key Results
- First convergence guarantee to ε-SOSP (not just FOSP) for risk-sensitive RL objectives
- Sample complexity of O(ε⁻³·⁵) to reach ε-SOSP, the first such guarantee for DRM objectives
- DRM-sensitive policies achieve mean return -13.6 to -14.1 vs -16.0 to -16.2 for risk-neutral in cliff walk
- Hessian estimation from sample trajectories without explicit integration via order statistics

## Why This Works (Mechanism)

### Mechanism 1
The policy Hessian for DRM objectives can be estimated from sample trajectories using the likelihood ratio method. The paper derives that ∇²ρh(θ) contains two terms—one involving h′′ weighted by outer products of ∇FRθ(x), and another involving h′ weighted by ∇²FRθ(x). By expressing ∇²FRθ(x) as an expectation over trajectories with indicator functions and log-policy derivatives, both terms become estimable from sampled episodes. Order statistics of trajectory returns enable efficient computation without explicit integration.

### Mechanism 2
Cubic regularization enables escape from saddle points and convergence to ε-SOSP. Standard Newton methods converge to any stationary point including saddles. Adding a cubic penalty -α‖θ-θk‖³/6 modifies the local quadratic model so that saddle points become repulsive—the cubic term pushes iterates away from regions with negative curvature. The regularization parameter α ≥ 3LH ensures the modified objective upper-bounds the true DRM locally, guaranteeing descent.

### Mechanism 3
Risk-seeking DRM policies achieve higher expected returns than risk-neutral counterparts in environments with asymmetric reward structures. DRMs with concave distortion functions assign higher weights to upper-tail returns via h′(1-i/m) coefficients in gradient estimates. In environments like cliff-walk where risky paths have higher upside, DRM policies bias toward these paths, increasing expected return despite higher variance.

## Foundational Learning

- **Distortion riskmetrics (DRMs)**: Why needed? The entire objective function is ρh(Rθ), a DRM of cumulative reward. Understanding that DRMs generalize expected value via distortion function h, and that h(t)=t recovers risk-neutrality, is essential to grasp why the algorithm behaves differently from standard policy gradient. Quick check: What distortion function h(t) makes DRM equivalent to expected value? What property of h determines risk-seeking vs risk-averse behavior?

- **First-order vs second-order stationary points (FOSP vs SOSP)**: Why needed? The paper's main theoretical contribution is convergence to ε-SOSP rather than just ε-FOSP. FOSPs include saddle points where ∇ρh=0 but Hessian has negative eigenvalues; SOSPs require ∇ρh≈0 AND Hessian positive semidefinite, which local maxima satisfy. Quick check: At a saddle point, what is the value of ∇ρh and the sign of λmax(∇²ρh)? Why does converging to FOSP not guarantee finding a local maximum?

- **Likelihood ratio method for policy derivatives**: Why needed? Both gradient and Hessian estimators rely on the identity ∇logπθ = ∇πθ/πθ to express derivatives as expectations over trajectories. The Hessian derivation specifically uses ∇²logπθ and cross-products of ∇logπθ terms. Quick check: Why can we write ∇E[f(Rθ)] = E[f(Rθ) · Σ∇logπθ(At|St)]? What role does the indicator 1{Rθ≤x} play in the CDF gradient ∇FRθ(x)?

## Architecture Onboarding

- Component map: Trajectory Sampler -> Order Statistics Sorter -> Gradient Estimator -> Hessian Estimator -> Cubic Subproblem Solver -> Policy Updater
- Critical path: Sample trajectories → Sort by return → Compute gradient/Hessian weights from gaps and h′/h′′ → Solve cubic subproblem → Update policy. The cubic subproblem solution is the bottleneck; it requires either iterative solvers or closed-form solutions from [23].
- Design tradeoffs:
  - Batch sizes (mk, bk): Larger batches reduce gradient/Hessian variance but increase sample complexity. Paper uses mk=bk=200 in experiments.
  - Regularization parameter α: Must satisfy α ≥ 3LH for theoretical guarantees, but LH is typically unknown. Paper uses α=10⁵ for Humanoid, tuned empirically. Too large α → conservative updates; too small → may not escape saddles.
  - Gradient vs Hessian batch ratio: Theory requires m≥b, but using same trajectories (m=b) simplifies implementation and reduces samples.
- Failure signatures:
  - Hessian estimate NaN/Inf: Check that Mr bounds are correctly set. Verify distortion function derivatives don't explode at boundaries.
  - No improvement over risk-neutral: Likely distortion function is near-identity or environment lacks asymmetric upside. Try dual-power with smaller α parameter.
  - Policy collapse to deterministic: Check if cubic solver is finding interior solutions. If ‖Δk‖ is consistently large, α may be too small relative to LH.
  - Variance explosion in Hessian: Ensure cross-trajectory terms are properly removed per Lemma 3 variance-reduction. Verify m≥b≥C(d)=4(1+2log2d).
- First 3 experiments:
  1. Validate Hessian estimator: Implement DRM Hessian estimate on a simple 1D bandit with known return distribution. Compare empirical mean against analytical ∇²ρh(θ) computed numerically. Check MSE decreases as 1/b.
  2. Saddle point escape test: Construct a 2-state MDP with a policy parameterization known to have saddle points. Run CRPN-DRM vs REINFORCE-DRM and verify CRPN-DRM converges to better local optima more consistently.
  3. Risk-seeking behavior verification: Implement cliff-walk environment from paper. Compare trajectory distributions of converged policies: DRM policies should show higher probability of cliff-adjacent paths, higher max returns, and higher variance. Confirm mean return improves as reported.

## Open Questions the Paper Calls Out

### Open Question 1
Can the CRPN-DRM algorithm maintain its convergence properties and sample efficiency when applied to large-scale policy architectures such as Transformers or LSTMs? The authors state: "We hypothesize the significance of second-order methods to be more helpful for larger policy networks such as deeper networks, transformers, LSTMs, etc, and such extensions can be an interesting direction for future empirical research." Current experiments are restricted to tabular, linear, or small MLP policies. Calculating Hessians for large parameter spaces is computationally prohibitive and theoretically unanalyzed.

### Open Question 2
Can the policy Hessian theorem and the ε-SOSP convergence guarantees be extended to infinite-horizon discounted MDPs? The problem formulation explicitly defines the cumulative reward Rθ and the objective within a "finite horizon MDP," limiting the theoretical scope. Extending to infinite horizons requires handling non-terminating trajectories and ensuring variance of likelihood-ratio estimators remains bounded without fixed T.

### Open Question 3
Is it possible to adapt the cubic-regularized policy Newton method for DRM to an off-policy setting using a replay buffer? The abstract and Section 5 specify that the proposed algorithm operates in an "on-policy RL setting." The gradient and Hessian estimators are derived using sample trajectories drawn strictly from the current policy πθ. Off-policy corrections for second-order information introduce high variance and bias that current theoretical bounds do not address.

## Limitations
- Theoretical framework relies on restrictive assumptions including bounded derivatives and Lipschitz Hessian, excluding common cases like VaR riskmetrics
- Cubic subproblem solver efficiency and convergence properties are not rigorously analyzed
- Empirical validation limited to three environments with tuned hyperparameters, lacking broader benchmarking
- No ablation studies examining impact of batch sizes, regularization strength, or distortion function choice

## Confidence

- Theoretical convergence guarantee (O(ε⁻³·⁵) to SOSP): **High** - Proven under stated assumptions with explicit constants
- Hessian estimator correctness: **Medium** - Formula derived but empirical validation limited to controlled experiments
- Practical performance advantage over risk-neutral: **Medium** - Demonstrable in tested environments but lacks broader benchmarking

## Next Checks

1. Test CRPN-DRM on environments where risk-seeking should hurt (e.g., symmetric rewards) to verify it doesn't always outperform risk-neutral baselines
2. Implement and verify the variance-reduced Hessian estimator's MSE scaling with batch size b on a simple 1D bandit with known DRM
3. Apply the algorithm to a policy parameterization that violates A4 (e.g., deep ReLU networks) to identify failure modes and potential fixes