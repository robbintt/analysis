---
ver: rpa2
title: 'BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over
  Bio-Molecular Interactions'
arxiv_id: '2506.05766'
source_url: https://arxiv.org/abs/2506.05766
tags:
- question
- graph
- questions
- information
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BioMol-MQA, a multi-modal question answering
  dataset for polypharmacy that combines knowledge graphs, text, and molecular structure
  data (SMILES) to test LLM reasoning capabilities. The dataset contains 1,683 questions
  requiring retrieval and reasoning over drug-drug and drug-protein interactions,
  with ground truth answers and associated context.
---

# BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions

## Quick Facts
- arXiv ID: 2506.05766
- Source URL: https://arxiv.org/abs/2506.05766
- Reference count: 40
- Novel multi-modal QA dataset for bio-molecular reasoning achieving 22% accuracy with LLMs

## Executive Summary
This paper introduces BioMol-MQA, a comprehensive multi-modal question answering dataset designed to evaluate large language models' reasoning capabilities over complex bio-molecular interactions. The dataset combines knowledge graphs, textual information, and molecular structure data (SMILES) to create challenging questions that require sophisticated retrieval and reasoning across multiple data modalities. The research focuses on polypharmacy scenarios involving drug-drug and drug-protein interactions, providing a valuable benchmark for testing LLM performance in healthcare-related applications.

## Method Summary
The BioMol-MQA dataset was constructed by curating questions from medical and biological domains, specifically focusing on polypharmacy scenarios. Each question requires reasoning across three data modalities: knowledge graphs representing drug interactions, textual descriptions of molecular properties, and SMILES notation for chemical structures. The dataset includes ground truth answers with associated context to enable retrieval-augmented generation experiments. Questions were designed to test various reasoning capabilities including path finding in knowledge graphs, understanding molecular structures, and synthesizing information across modalities.

## Key Results
- LLMs achieved only 22% exact match accuracy on zero-shot testing
- Performance improved significantly to 62% when provided with relevant context
- Simple retrieval baselines like BM25 and Neo4j outperformed complex multi-modal retrievers

## Why This Works (Mechanism)
The dataset works by creating questions that require multi-modal reasoning across knowledge graphs, text, and molecular structures. The complexity arises from the need to integrate information from different sources and apply domain-specific knowledge about drug interactions and molecular properties. The retrieval-augmented generation approach demonstrates that providing relevant context can substantially improve LLM performance, suggesting that the primary challenge lies in information retrieval rather than reasoning capabilities.

## Foundational Learning

**SMILES Notation**
- Why needed: Represents molecular structures in text format for computational processing
- Quick check: Can the model parse and understand chemical structures from SMILES strings?

**Knowledge Graphs**
- Why needed: Captures relationships between drugs, proteins, and other bio-molecular entities
- Quick check: Does the model correctly navigate graph structures to find interaction paths?

**Retrieval-Augmented Generation**
- Why needed: Combines information retrieval with language model generation capabilities
- Quick check: Does providing relevant context improve answer accuracy?

## Architecture Onboarding

**Component Map**
- Question generation -> Multi-modal data preparation -> Retriever module -> LLM reasoning -> Answer evaluation

**Critical Path**
1. Question formulation and categorization
2. Multi-modal data extraction and formatting
3. Retrieval system implementation
4. LLM response generation with/without context
5. Accuracy measurement and analysis

**Design Tradeoffs**
- Dataset size vs. complexity balance
- Choice of SMILES notation vs. other molecular representations
- Simple vs. complex retrieval methods

**Failure Signatures**
- Low accuracy on questions requiring cross-modal reasoning
- Performance gaps between retrieval-augmented and zero-shot approaches
- Simple baselines outperforming sophisticated models

**First Experiments**
1. Zero-shot LLM evaluation on all questions
2. Retrieval-augmented generation with different context sources
3. Comparison of simple vs. complex retrieval methods

## Open Questions the Paper Calls Out

The paper highlights several open questions regarding the generalizability of the dataset beyond polypharmacy, the effectiveness of different retriever architectures for multi-modal tasks, and the fundamental limitations of current LLMs in handling complex bio-molecular reasoning tasks.

## Limitations

- Dataset primarily focused on polypharmacy, limiting generalizability
- Small sample size may not capture full complexity of bio-molecular interactions
- Surprising results showing simple baselines outperforming complex models

## Confidence

- Dataset innovation: High
- Performance metrics validity: Medium
- Cross-domain generalizability: Low

## Next Checks

1. Test dataset with additional LLM architectures to verify performance patterns
2. Create parallel questions for other bio-molecular interaction types to assess generalizability
3. Conduct ablation studies to isolate whether performance limitations stem from retrieval failures or reasoning deficiencies