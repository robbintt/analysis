---
ver: rpa2
title: A survey of textual cyber abuse detection using cutting-edge language models
  and large language models
arxiv_id: '2501.05443'
source_url: https://arxiv.org/abs/2501.05443
tags:
- hate
- speech
- detection
- abuse
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews recent advancements in textual
  cyber abuse detection, with a focus on the role of large language models (LLMs)
  and language models (LMs). It covers major forms of abuse including hate speech,
  cyberbullying, doxing, trolling, impersonation, and emotional abuse.
---

# A survey of textual cyber abuse detection using cutting-edge language models and large language models

## Quick Facts
- arXiv ID: 2501.05443
- Source URL: https://arxiv.org/abs/2501.05443
- Reference count: 19
- Primary result: BERT-based methods dominate detection performance across various abuse types

## Executive Summary
This survey systematically reviews recent advancements in textual cyber abuse detection, with a focus on large language models (LLMs) and language models (LMs). It covers major forms of abuse including hate speech, cyberbullying, doxing, trolling, impersonation, and emotional abuse. The survey identifies BERT and its variants as the most widely used models, with BERT-based methods dominating detection performance across various abuse types. LLMs such as GPT and T5 are emerging as promising tools, particularly for data augmentation and few-shot learning. Despite progress, the paper highlights persistent challenges in addressing class imbalance and the need for more robust evaluation methods.

## Method Summary
The authors conducted a systematic literature review using Web of Science database with specific Boolean search queries across seven abuse categories. They analyzed 74 papers published between 2022-2024, focusing on journal articles that employed text-based analysis. The study examined model prevalence, evaluation metrics, and class imbalance handling approaches. Performance metrics analyzed include Accuracy, Macro-F1, and Weighted-F1. The survey found BERT-based architectures combined with Bi-LSTMs or CNNs as the dominant methodology, with SVM and TF-IDF serving as traditional baselines.

## Key Results
- BERT-based architectures demonstrate superior performance in textual cyber abuse detection compared to traditional methods
- LLMs such as GPT and T5 show promise for data augmentation and few-shot learning scenarios
- Persistent challenges remain in addressing class imbalance and evaluating less-studied abuse types like doxing and emotional abuse

## Why This Works (Mechanism)

### Mechanism 1
BERT-based architectures demonstrate superior performance in textual cyber abuse detection compared to traditional methods, primarily due to their capacity to capture contextual nuances in abusive language patterns. Pre-trained transformer representations encode bidirectional linguistic dependencies that correlate with abuse categories. The attention mechanism weighs token relationships, allowing detection of subtle abusive intent that keyword-based approaches miss. When combined with sequential models like BiLSTM, these embeddings capture both local and long-range contextual signals.

### Mechanism 2
Generative language models address class imbalance in cyber abuse detection through synthetic data augmentation, creating minority class examples that expand training distribution coverage. LLMs like GPT generate synthetic abuse examples that preserve semantic properties of the target class. These generated samples function similarly to oversampling techniques but with greater linguistic diversity. The augmented dataset improves model calibration on underrepresented abuse categories.

### Mechanism 3
Multi-task learning frameworks that jointly train on abuse detection alongside sentiment analysis and emotion recognition improve detection robustness by leveraging complementary signal types. Shared representations learn correlations between linguistic markers of sentiment/emotion and abuse categories. Negative sentiment and specific emotions (anger, disgust) serve as soft indicators that refine abuse probability estimates. The multi-task objective provides regularization against overfitting to spurious textual features.

## Foundational Learning

- **Transformer Attention Mechanisms**
  - Why needed here: Understanding how BERT encodes contextual relationships between words is essential for diagnosing why certain abuse patterns are detected while others fail, and for designing appropriate fine-tuning strategies.
  - Quick check question: Can you explain how bidirectional attention in BERT differs from unidirectional attention in GPT, and why this matters for detecting context-dependent abuse?

- **Evaluation Metrics for Imbalanced Classification**
  - Why needed here: Cyber abuse detection datasets are inherently imbalanced (abuse is rare). Selecting appropriate metrics (weighted F1, macro-averaged scores, AUC) determines whether model improvements are genuine or artifacts of majority class bias.
  - Quick check question: Given a dataset where only 5% of samples contain abuse, why would accuracy be a misleading metric, and which alternatives would you prioritize?

- **Class Imbalance Mitigation Techniques**
  - Why needed here: The survey identifies that 51.4% of hate speech papers did not adequately address class imbalance. Understanding oversampling (SMOTE), undersampling, cost-sensitive learning, and data augmentation is prerequisite to building functional systems.
  - Quick check question: What are the tradeoffs between oversampling the minority class versus using class-weighted loss functions during training?

## Architecture Onboarding

- **Component map:**
```
Input Text → Preprocessing (tokenization, normalization)
    ↓
Embedding Layer (BERT/RoBERTa/DistilBERT variant)
    ↓
Feature Extraction (BiLSTM/CNN/Attention pooling - often hybrid)
    ↓
Classification Head (Softmax over abuse categories)
    ↓
Output: Abuse probability + optionally: sentiment, emotion, target group
```

- **Critical path:**
  1. **Dataset audit first**: Assess class imbalance, label quality, and domain match before model selection. The survey shows many failures stem from inadequate imbalance handling.
  2. **Start with fine-tuned BERT variants**: BERT-base or language-specific variants (BETO for Spanish, AraBERT for Arabic) provide strong baselines. RoBERTa and DistilBERT offer speed/accuracy tradeoffs.
  3. **Implement weighted evaluation**: From day one, track macro-F1, per-class F1, and AUC alongside accuracy.

- **Design tradeoffs:**
  - **BERT vs. LLMs**: BERT variants dominate (77% of studies) for classification; LLMs excel at few-shot and augmentation but underperform on direct classification.
  - **Single-task vs. multi-task**: Multi-task improves robustness but increases complexity; start single-task until baseline is established.
  - **Data augmentation vs. oversampling**: Augmentation with LLMs generates diverse samples but may introduce artifacts; SMOTE is simpler but less linguistically sophisticated.

- **Failure signatures:**
  - High overall accuracy but near-zero recall on minority abuse classes → class imbalance not addressed
  - Strong performance on benchmark dataset, poor on new platform → overfitting to platform-specific language
  - Model flags profanity but misses subtle abuse (gaslighting, doxing indicators) → insufficient context modeling
  - Inconsistent predictions on semantically similar texts → lacking robustness to paraphrase

- **First 3 experiments:**
  1. **Baseline establishment**: Fine-tune BERT-base on target abuse type with weighted cross-entropy loss; report per-class precision, recall, F1, and macro-averaged metrics.
  2. **Imbalance ablation**: Compare three approaches on same dataset—(a) class-weighted loss, (b) SMOTE oversampling, (c) LLM-based augmentation using GPT to generate minority class samples. Measure impact on minority class recall.
  3. **Cross-dataset validation**: Train on one platform (e.g., Twitter), evaluate on another (e.g., Reddit) to assess generalization. The survey notes over-reliance on Twitter data limits model versatility.

## Open Questions the Paper Calls Out

### Open Question 1
How can detection methodologies be adapted to effectively identify underrepresented forms of cyber abuse, such as doxing, emotional abuse, and impersonation? The authors explicitly state that specific forms of cyber abuse like doxing and emotional abuse "remain underexplored and lack adequate detection solutions," noting that current literature is overshadowed by a focus on hate speech. This remains unresolved due to scarcity of labeled datasets and specialized model architectures for these specific abuse types.

### Open Question 2
What evaluation frameworks and metrics are required to standardize performance comparisons in cyber abuse detection given widespread class imbalance? The paper highlights that "a lack of standardization in how imbalanced classification is addressed remains a significant issue," making it difficult to compare models fairly across studies. This remains unresolved as current research relies heavily on custom datasets and inconsistent metrics, complicating the identification of best practices.

### Open Question 3
How can detection systems distinguish between human-generated and LLM-generated abusive content, particularly regarding automated trolling or hate speech? While the survey notes the "dual role" of LLMs, it reveals that detecting AI-generated abuse is nascent, citing only one study using GLTR for this purpose. This remains unresolved as current detection methods focus on content semantics rather than the "human vs. machine" origin of the text.

## Limitations

- The survey relies heavily on benchmarks (HatEval, OffensEval) that may not capture real-world deployment complexity, making generalizability claims uncertain
- The effectiveness of LLM-based augmentation remains theoretical without comparative studies against established resampling techniques
- Claims about multi-task learning benefits and specific model combinations lack sufficient supporting evidence from the corpus analysis

## Confidence

- **High confidence**: BERT and transformer variants represent the dominant methodology in recent textual abuse detection research, with clear performance advantages over traditional baselines
- **Medium confidence**: LLM-based data augmentation and few-shot learning show promise for addressing class imbalance and low-resource scenarios, though empirical validation is limited
- **Low confidence**: Claims about multi-task learning benefits and specific model combinations lack sufficient supporting evidence from the corpus analysis

## Next Checks

1. **Cross-platform robustness test**: Train the dominant BERT-based architecture on Twitter data and evaluate on Reddit/4chan to assess generalization beyond platform-specific language patterns.

2. **Imbalance mitigation comparison**: Implement controlled experiments comparing (a) class-weighted loss, (b) SMOTE oversampling, and (c) GPT-generated synthetic examples on the same abuse detection task, measuring impact on minority class recall.

3. **LLM direct classification evaluation**: Fine-tune GPT-3 or T5 for abuse detection and compare performance against BERT variants on identical benchmarks to validate whether LLMs are truly "emerging" or remain auxiliary tools.