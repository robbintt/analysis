---
ver: rpa2
title: Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and
  Attribution
arxiv_id: '2511.16541'
source_url: https://arxiv.org/abs/2511.16541
tags:
- images
- generators
- image
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for AI-generated image
  detection and source attribution that addresses the challenge of rapidly evolving
  generative models. The proposed approach employs supervised contrastive learning
  with a MambaVision backbone to extract discriminative embeddings, followed by k-nearest
  neighbors classification in a few-shot learning paradigm.
---

# Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution

## Quick Facts
- arXiv ID: 2511.16541
- Source URL: https://arxiv.org/abs/2511.16541
- Authors: Jaime Álvarez Urueña; David Camacho; Javier Huertas Tato
- Reference count: 40
- One-line primary result: Novel framework combining supervised contrastive learning with k-NN achieves 91.3% detection accuracy and 14.70% AUC improvement in source attribution using only 150 images per class.

## Executive Summary
This paper addresses the challenge of detecting and attributing AI-generated images from rapidly evolving generative models by introducing a two-stage framework that combines supervised contrastive learning with few-shot k-NN classification. The approach uses a MambaVision backbone to extract discriminative embeddings that cluster images by their source generator, enabling effective generalization to previously unseen models. By decoupling feature learning from classification, the system can adapt to new generators without retraining the heavy backbone, achieving significant improvements over state-of-the-art methods while requiring minimal training data.

## Method Summary
The framework employs a MambaVision-L3-21K backbone trained with Supervised Contrastive Loss (SupConLoss) to generate 1,000-dimensional embeddings from 256x256 input images. During training, images from different generators are pulled together in the embedding space while being pushed apart from other classes. For inference, a non-parametric k-NN classifier (k=11) uses 150 labeled support samples per class to attribute new images based on cosine similarity in the embedding space. The system is evaluated on the GenImage dataset (8 generators, 162k training images) and ForenSynths (ProGAN training, 12 generators testing), with batch sizes of 4,000-6,000 optimized for contrastive learning.

## Key Results
- Average detection accuracy of 91.3%, representing a 5.2 percentage point improvement over existing methods
- Open-set classification improvements of 14.70% in AUC and 4.27% in OSCR compared to state-of-the-art approaches
- Effective few-shot learning requiring only 150 images per class for reliable source attribution
- Successful generalization to previously unseen generators without requiring exhaustive retraining protocols

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised Contrastive Loss structures the latent space to improve generalization to unseen generators better than standard cross-entropy classification.
- **Mechanism:** SupConLoss optimizes embedding geometry by pulling embeddings of the same generator together while pushing apart embeddings of different classes, creating tight clusters that allow new generators to occupy distinct regions without requiring weight updates.
- **Core assumption:** The feature extractor learns universal forensic representations such that new generators' images map meaningfully into the existing space.
- **Evidence anchors:** Paper visualizes clustered embeddings (Figure 2c) and contrasts itself with training-free methods, suggesting explicit representation learning is preferred.

### Mechanism 2
- **Claim:** Decoupling the feature extractor from the classifier via k-NN enables few-shot adaptation without retraining the backbone.
- **Mechanism:** The system separates "learning of features" (MambaVision + SupConLoss, trained once) from "assignment of labels" (k-NN, trained dynamically) by using frozen embeddings and proximity-based classification.
- **Core assumption:** Distance metric in embedding space correlates directly with source attribution.
- **Evidence anchors:** Paper describes k-NN as non-parametric instance-based learning and claims effectiveness with merely 150 images per class.

### Mechanism 3
- **Claim:** MambaVision backbone captures long-range forensic dependencies more efficiently than pure Transformers.
- **Mechanism:** The architecture leverages State-Space Models for linear complexity processing supplemented by self-attention for global context, allowing processing of high-resolution inputs and large batch sizes.
- **Core assumption:** Synthetic image detection relies on long-range dependencies rather than just local pixel artifacts.
- **Evidence anchors:** Paper selects MambaVision for effective integration of long-range context modeling with computational efficiency and prioritizes large batch size (6000) for contrastive learning.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SupCon)**
  - **Why needed here:** Unlike standard Cross-Entropy, SupConLoss allows the model to learn a structured embedding space where "unseen" classes can still be distinctly clustered.
  - **Quick check question:** Does the loss function optimize the relative distance between all samples in a batch, or does it optimize the probability of a single label?

- **Concept: Non-Parametric Classification (k-NN)**
  - **Why needed here:** It solves the "scalability" and "retraining" bottleneck by adapting to new generators instantly by simply adding their embeddings to the reference pool.
  - **Quick check question:** Can the model classify a new generator type without running a single backpropagation step?

- **Concept: Few-Shot Learning**
  - **Why needed here:** This sets the operational constraint that the system must function with data scarcity (150 images) because massive datasets are often unavailable for proprietary or new generators.
  - **Quick check question:** Is the feature extractor robust enough to form coherent clusters with only 150 labeled examples to define the class centroid?

## Architecture Onboarding

- **Component map:** Input (256x256 Image) -> MambaVision-L3-21K Backbone -> 1000-dimensional Embedding Vector -> Supervised Contrastive Loss (Training) -> Frozen Backbone -> k-NN (k=11) Classification (Inference)

- **Critical path:** The relationship between Batch Size and Contrastive Loss. The paper forces a massive batch size (6000) to provide enough negative pairs for the contrastive loss to work effectively.

- **Design tradeoffs:**
  - Resolution vs. Batch Size: The paper explicitly sacrifices image resolution (dropping to 256px) to fit the massive batch sizes (4000-6000) required for SupConLoss into GPU memory.
  - k-NN Latency: k-NN requires storing and searching the embedding set of the support data. As the number of known generators grows, inference latency may increase compared to a single matrix multiplication in a linear classifier.

- **Failure signatures:**
  - Mode Collapse: If the backbone fails to converge, embeddings form a single giant ball.
  - Generator Confusion: High performance on "Seen" generators but near-random performance on "Unseen" generators indicates the backbone failed to learn universal forensic features.
  - Sensitivity to Support Size: If accuracy varies wildly with small changes in the support set, the embeddings are not sufficiently discriminative.

- **First 3 experiments:**
  1. Reproduce the "ESB1" Baseline: Train the MambaVision backbone on only ProGAN (ESB1 setup) using SupConLoss. Test generalization on Diffusion models (GenImage test set) to verify the cross-architecture claim.
  2. Ablate the Classifier: Swap the k-NN classifier for a simple Linear Probe (train a linear layer on top of frozen embeddings). Compare results to justify the non-parametric choice.
  3. Support Set Sensitivity: Run the inference pipeline varying the "few-shot" instances from 10 to 500 to plot the performance curve and confirm the "150 images" sweet spot claimed in the paper.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of specification for critical training hyperparameters including optimizer type, learning rate schedule, and training epochs
- No detailed analysis of failure cases when generators use fundamentally different architectural approaches
- Computational requirements for massive batch sizes (4000-6000) may limit reproducibility on standard hardware
- No detailed statistical significance analysis of claimed performance improvements

## Confidence

- **High Confidence**: The general framework design combining supervised contrastive learning with k-NN classification for few-shot adaptation is well-specified and theoretically sound.
- **Medium Confidence**: The claimed performance improvements (5.2% detection accuracy, 14.70% AUC improvement) are reported but lack detailed statistical significance analysis.
- **Low Confidence**: The exact reproducibility of results given the unspecified training hyperparameters and the generalization claims to unseen generator architectures without empirical validation on truly novel architectures.

## Next Checks

1. **Cross-Architecture Generalization Test**: Train the model exclusively on GAN-based generators (ProGAN, BigGAN) and evaluate detection performance on diffusion models (Midjourney, SD) to validate the claimed cross-architecture generalization.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary batch sizes (512, 1024, 2048, 4096) and temperature initialization values to establish the robustness of the contrastive learning approach to hyperparameter choices.

3. **Support Set Size Scaling Study**: Evaluate the model performance across a range of support set sizes (50, 150, 300, 500) to quantify the actual impact of the "few-shot" constraint and identify the minimum effective sample size for reliable attribution.