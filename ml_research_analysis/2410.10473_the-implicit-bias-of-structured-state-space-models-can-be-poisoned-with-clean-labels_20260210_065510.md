---
ver: rpa2
title: The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean
  Labels
arxiv_id: '2410.10473'
source_url: https://arxiv.org/abs/2410.10473
tags:
- training
- sequences
- which
- lemma
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Structured state space models (SSMs), such as S4 and Mamba, are\
  \ gaining prominence in deep learning, yet their implicit bias remains poorly understood.\
  \ This paper proves that SSMs can be poisoned with clean labels\u2014special sequences,\
  \ even when correctly labeled by a low-dimensional teacher SSM, can distort the\
  \ implicit bias and destroy generalization."
---

# The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels

## Quick Facts
- **arXiv ID:** 2410.10473
- **Source URL:** https://arxiv.org/abs/2410.10473
- **Reference count:** 40
- **Primary result:** Clean-label poisoning can disrupt SSM generalization by breaking greedy low-rank learning dynamics.

## Executive Summary
Structured State Space Models (SSMs) like S4 and Mamba are increasingly used in deep learning, but their implicit bias mechanisms remain poorly understood. This paper proves that SSMs can be poisoned with clean labels—specifically constructed sequences that, despite being correctly labeled, can distort the model's implicit bias and destroy generalization. The authors show that when trained with gradient flow, SSMs naturally learn a low-rank state transition matrix through a "greedy" sequential process, but specially structured clean sequences can break this bias, causing the model to converge to solutions that fit training data perfectly but fail on new inputs. Experiments confirm this vulnerability across synthetic and real-world settings including CIFAR-10, raising serious safety concerns for SSMs used in large language models.

## Method Summary
The paper models SSM training as continuous gradient flow and proves that near-zero initialization leads to "greedy low-rank learning" of the state matrix A through sequential evolution of diagonal entries. Clean-label poisoning exploits this by injecting sequences with large tail coefficients that inflate the factor γ(0)(t), disrupting the greedy learning pattern. The theoretical analysis uses non-resonance linearization to show poisoned trajectories converge to non-generalizing saddle points. Experiments validate this on synthetic datasets and CIFAR-10 using S4, Mamba-2, and LRU networks, while proposing monitoring of γ(0)(t) as a potential defense.

## Key Results
- SSMs exhibit implicit bias toward greedy low-rank learning of state matrix A during gradient flow
- Clean-label poisoning sequences with large tail coefficients can completely disrupt this bias
- Poisoned models achieve zero training loss but fail to generalize to new sequences
- Monitoring the factor γ(0)(t) during training may detect poisoning attempts
- Vulnerability confirmed across multiple SSM architectures including S4, Mamba-2, and LRU networks

## Why This Works (Mechanism)

### Mechanism 1: Greedy Low-Rank Learning via Polynomial Dynamics
The implicit bias of gradient flow in SSMs tends to induce "greedy low-rank learning" of the state transition matrix A, provided a specific dynamical factor remains small. The motion of diagonal entries follows a degree-κ polynomial determined by gradient flow. When the constant coefficient (proportional to γ(0)(t)) is negligible, optimization progresses by increasing entry magnitudes sequentially rather than simultaneously. This sequential learning is posited as sufficient for generalization. The condition fails if γ(0)(t) becomes large, introducing a significant constant term that forces simultaneous entry movement.

### Mechanism 2: Disruption via High-Magnitude Tail Coefficients
Clean-label poisoning exploits SSM dynamics by injecting sequences with large values in their final elements, inflating γ(0)(t) and destroying generalization. The factor γ(0)(t) depends linearly on the last elements of training sequences. By crafting sequences with large magnitudes at specific positions (cleanly labeled by the teacher), the attacker makes the constant term of the polynomial non-negligible. This disrupts the "greedy" sequential learning, causing convergence to solutions that fit training data but fail to generalize.

### Mechanism 3: Convergence to Non-Generalizing Critical Points
The poisoned trajectories converge to a critical point (specifically a saddle point) mathematically distinct from generalizing solutions but still minimizing training loss. The paper utilizes a "non-resonance linearization theorem" to prove that when poisoned, the optimization trajectory closely tracks a reference trajectory. This reference trajectory converges to a saddle point where the state matrix A doesn't match the teacher's low-rank structure, resulting in high generalization error despite zero training loss.

## Foundational Learning

- **Concept: Gradient Flow / Implicit Bias** - Why needed: The paper models optimization as continuous gradient flow to mathematically prove that the learning trajectory—not just final weights—determines generalization. Quick check: Can you explain why "implicit bias" of an algorithm matters more than the explicit loss function in over-parameterized models?

- **Concept: State Space Models (SSMs) & Impulse Response** - Why needed: Understanding the recurrence relation (s_{t+1} = As_t + Bx_t) and how impulse response (CA^k B) relates to generalization is the core subject of the proof. Quick check: How does the structure of state matrix A (e.g., diagonal vs. low-rank) affect how an SSM processes long sequences?

- **Concept: Dynamical Systems & Linearization** - Why needed: The theoretical contribution relies on analyzing stability of fixed points (saddle points) and linearizing dynamics around them to predict trajectory divergence. Quick check: Why might a neural network trajectory get "stuck" or diverge near a saddle point during optimization?

## Architecture Onboarding

- **Component map:** Scalar sequences x ∈ R^κ → SSM core (A, B, C) → Scalar output y = CA^{κ-1}Bx → Dynamics monitor γ(0)(t)
- **Critical path:** Training loop must track evolution of A's diagonal entries. Sequential evolution indicates healthy "greedy" learning; simultaneous growth indicates poisoning.
- **Design tradeoffs:** Data efficiency vs. robustness (finite training sets allow specific sequences to dominate γ factor); diagonal constraint assumption limits generalization to non-diagonal structures.
- **Failure signatures:** High γ(0)(t) magnitude during training; simultaneous growth of multiple A diagonal entries; zero training loss with high generalization error.
- **First 3 experiments:**
  1. Visualize greedy learning: Train SSM on benign synthetic dataset, plot diagonal entries of A over time to verify sequential growth.
  2. Inject tail sequences: Add single poison sequence with large values at index κ-1, plot A entries to confirm loss of greedy learning.
  3. Monitor γ(0)(t): Implement calculation from Eq. (8) during CIFAR-10 training, correlate magnitude with final generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can a robust defense against clean-label poisoning be implemented by monitoring the learning dynamics factor γ(0)(t)? The paper proposes monitoring γ(0)(t) as a theoretical defense strategy but doesn't implement, validate, or quantify its effectiveness in real-world scenarios. Empirical results demonstrating successful detection without high false positive rates would resolve this.

### Open Question 2
Does susceptibility to clean-label poisoning persist under modern initialization schemes designed to alleviate vanishing gradients? Section 5 acknowledges the limitation that theory pertains to near-zero initialization but doesn't account for modern SSM initializations. A theoretical or empirical demonstration of vulnerability under state-of-the-art techniques would resolve this.

### Open Question 3
Can theoretical guarantees extend to settings where SSM's input and output matrices are learned with standard learning rates? Theorem 1 assumes fixed B and C matrices, while Appendix B suggests learning them requires sufficiently small learning rate ratios. A formal proof showing special sequences disrupt greedy learning even when all parameters are optimized simultaneously would resolve this.

## Limitations

- The proof relies on strong assumptions including diagonal state matrices and specific near-zero initialization conditions
- The theoretical analysis uses continuous gradient flow rather than practical discrete gradient descent
- The vulnerability extension to real-world architectures like Mamba-2 and LRU networks is speculative, based only on CIFAR-10 experiments
- The practical impact on large language models remains untested

## Confidence

**High Confidence Claims:**
- Theoretical framework describing greedy low-rank learning in diagonal SSMs is mathematically sound
- Relationship between γ(0)(t) and evolution of diagonal entries is rigorously established

**Medium Confidence Claims:**
- Clean-label poisoning phenomenon exists in practice based on synthetic experiments
- Monitoring γ(0)(t) as defense mechanism is theoretically justified

**Low Confidence Claims:**
- Vulnerability extends to real-world architectures beyond synthetic settings
- Practical impact on large language models is significant

## Next Checks

1. **Gradient Descent vs. Gradient Flow:** Reproduce synthetic experiments using practical gradient descent with various learning rates and batch sizes to assess phenomenon robustness to optimization discretization.

2. **Architectural Robustness Testing:** Systematically evaluate whether vulnerability persists when diagonal constraint on A is relaxed or when common regularization techniques (L2, dropout, layer normalization) are applied.

3. **Real-World Impact Assessment:** Conduct controlled poisoning experiments on actual language model using SSM layers (e.g., Mamba-2) to measure severity in practical settings, including effects on training loss and downstream task performance.