---
ver: rpa2
title: Enhancing Cross-task Transfer of Large Language Models via Activation Steering
arxiv_id: '2507.13236'
source_url: https://arxiv.org/abs/2507.13236
tags:
- few-shot
- question
- tasks
- transfer
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-task transfer learning for large
  language models (LLMs) by steering activation patterns in the latent space rather
  than using traditional input expansion or parameter updates. The authors propose
  CAST (Cross-task Activation Steering Transfer), a method that first selects a small,
  influential, and diverse subset of examples from high-resource tasks using a similarity-based
  graph search, then extracts contrastive representation-enhanced activations from
  these examples and injects them into the forward pass of low-resource tasks.
---

# Enhancing Cross-task Transfer of Large Language Models via Activation Steering

## Quick Facts
- arXiv ID: 2507.13236
- Source URL: https://arxiv.org/abs/2507.13236
- Reference count: 40
- Achieves up to 81.97% accuracy in cross-domain tasks and 94.88% in cross-lingual tasks

## Executive Summary
This paper introduces CAST (Cross-task Activation Steering Transfer), a method that enables cross-task transfer learning for LLMs by steering activation patterns in the latent space rather than using traditional input expansion or parameter updates. The authors propose selecting a small, influential, and diverse subset of examples from high-resource tasks, extracting contrastive representation-enhanced activations from these examples, and injecting them into the forward pass of low-resource tasks. Experiments across cross-domain and cross-lingual settings show consistent improvements over competitive baselines, with CAST achieving superior scalability and lower computational costs compared to traditional in-context learning approaches.

## Method Summary
CAST operates by first encoding high-resource samples and building a similarity graph, then selecting a subset of n=20 samples using a combination of influence (information diffusion simulation) and diversity (hop-based overlap penalties). For each selected sample, the method extracts activation differences between few-shot and zero-shot prompts at a specific layer L, averaging these differences to create a steering vector C_L. During inference on low-resource tasks, this steering vector is injected into the final token's hidden state at layer L via ĥ_L = h_L + λ·C_L, with λ=1 found to be optimal. The injection layer is selected via validation performance, with middle layers (12-20 in 32-layer models) showing the best results.

## Key Results
- CAST consistently outperforms competitive baselines, achieving up to 81.97% accuracy in cross-domain tasks and 94.88% in cross-lingual tasks
- Performance improves with more examples in the steering subset, suggesting averaging helps isolate general features
- Removing either influence or diversity components from sample selection causes performance drops
- Middle layers (12-20) perform best for activation injection, with significant accuracy variation across layers

## Why This Works (Mechanism)

### Mechanism 1: Consistent Cross-Task Activation Shifts
In-context examples induce directional shifts in latent space that transfer across tasks. Few-shot prompting generates activations with higher matrix entropy than zero-shot, and the difference vectors (few-shot − zero-shot) between these states are nearly parallel across domains, suggesting task-agnostic "enhancement directions" that can be extracted and reinjected. This assumes the directional shift encodes task-general reasoning patterns, not domain-specific content.

### Mechanism 2: Contrastive Representation Extraction
Averaging activation differences across samples isolates task-level features from instance-specific noise. For each high-resource sample, compute A_L(few-shot) − A_L(zero-shot), then average across n=20 selected samples to produce steering vector C_L. This subtraction removes content-specific signals while preserving the "in-context enhancement" pattern, assuming noise averages out while signal persists across samples.

### Mechanism 3: Influence-Diversity Sample Selection
Selecting samples that balance influence (activation spread) and diversity (graph coverage) yields more representative steering vectors. Build similarity graph → compute influence via simulated diffusion → penalize overlap with selected nodes → greedily select top scorers. This avoids redundant samples and prioritizes those that activate many neighbors, assuming samples that trigger widespread activation encode more transferable patterns.

## Foundational Learning

- **Activation Steering / Representation Engineering**: Understanding how to locate, extract, and inject activations at specific layers is prerequisite. Quick check: Given a transformer forward pass, can you identify where to hook the final token's hidden state at layer L?

- **In-Context Learning (ICL) and Its Limitations**: CAST is motivated by ICL's context-length and quadratic-complexity constraints. Understanding what ICL provides (and why it's fragile) clarifies why steering is proposed. Quick check: Why does few-shot ICL performance plateau or degrade with too many examples?

- **Graph-Based Sample Selection (Centrality, Diversity, Greedy Search)**: The subset selection module uses a directed graph with influence diffusion and hop-based diversity penalties. Quick check: How does a diffusion-based influence score differ from simple degree centrality?

## Architecture Onboarding

- **Component map**: BGE encoder → similarity graph → influence/diversity scoring → greedy selection → activation extraction → steering vector injection
- **Critical path**: Encode high-resource samples and build graph (offline) → Select subset via Algorithm 2 (offline) → Extract C_L from selected samples (offline, cache per source task) → At inference, load cached C_L, inject during forward pass on target query
- **Design tradeoffs**: Layer selection: Middle layers (12–20 in 32-layer models) perform best; Early layers lack semantic abstraction, late layers are task-specific. Injection strength λ=1 is optimal; higher values disrupt semantics. Subset size n=20 balances coverage and computation; larger n improves performance but with diminishing returns. Requires model internals access → not applicable to closed-source APIs
- **Failure signatures**: Negative transfer when source/target domains are extremely dissimilar. Performance collapse if λ is too large or layer is misconfigured. No improvement if selected samples are low-influence or redundant
- **First 3 experiments**: 1) Replicate matrix entropy analysis (Figure 1) on your target model to confirm few-shot > zero-shot entropy in middle layers. 2) Validate parallel difference-vector claim (Figure 2) using t-SNE on your own domain pair. 3) Run minimal CAST pipeline: select 20 samples from source task, extract C_L, inject on single target query with λ=1 at layer 16; compare to zero-shot baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can activation steering methods like CAST be adapted for closed-source LLMs that do not expose internal representations? The authors state in Section C (Limitation): "our method requires access to the internal representations of the model, making it infeasible for closed-source LLMs." This is unresolved because the fundamental design depends on directly accessing and modifying hidden states during forward passes, which proprietary APIs typically do not expose.

### Open Question 2
Can the injection layer be selected automatically without requiring labeled validation data? Section 5.1 states "The injection layer is determined based on performance on the validation set," which assumes access to labeled target-task data for hyperparameter tuning. This creates a circular dependency for truly low-resource scenarios where validation labels may be unavailable, yet layer selection significantly impacts performance.

### Open Question 3
What conditions cause CAST to produce incorrect answers even when transferring from relevant high-resource tasks? The Case Studies (Appendix D) show instances where CAST produces wrong answers, yet no systematic failure analysis is provided. The paper does not investigate whether failures stem from noise in extracted activations, mismatched task representations, or interference between injected activations and target query semantics.

## Limitations
- Method requires full model access to inject steering vectors at specific layers, making it incompatible with closed-source APIs like GPT-4 or Claude
- Prompt templates and few-shot example counts used for activation extraction are underspecified, creating significant ambiguity in the C_L extraction phase
- Cross-lingual evaluation relies on MARC benchmark but doesn't address potential cultural bias or language-specific phenomena across diverse linguistic families

## Confidence

**High Confidence**: The empirical results showing CAST outperforming baselines in cross-domain and cross-lingual settings are well-supported by presented experiments. The ablation studies demonstrating the importance of each component are rigorous and convincing.

**Medium Confidence**: The mechanism explanation for why activation steering transfers across tasks is supported by visualizations but could benefit from more rigorous statistical analysis. The claim that averaging activation differences isolates task-level features while eliminating instance-specific noise is plausible but not extensively validated across diverse task types.

**Low Confidence**: The paper's claim that CAST "consistently" outperforms few-shot ICL across all settings is not fully substantiated - performance varies significantly by domain pair, and the paper doesn't thoroughly explore failure cases.

## Next Checks

1. **Directionality Consistency Validation**: For a representative source task, extract individual activation difference vectors AL(fi)−AL(zi) for all selected samples, then compute pairwise cosine similarities between these vectors. Low average similarity (<0.5) would indicate heterogeneous signals that averaging cannot effectively combine.

2. **Layer Sensitivity Analysis**: Systematically vary the injection layer L across [1, 4, 8, 12, 16, 20, 24, 28] for a single domain transfer pair (e.g., ARC-Easy→ARC-Challenge) while keeping all other parameters constant. Plot accuracy vs. layer number to empirically validate the claimed middle-layer optimum.

3. **Sample Selection Ablation**: Replace the influence-diversity greedy selection with three alternatives: (a) random selection of n=20 samples, (b) top-k most similar samples by BGE embedding distance, (c) bottom-k least similar samples. Run each variant on the same domain pair and compare to full CAST performance to isolate the contribution of the selection mechanism versus the steering approach itself.