---
ver: rpa2
title: 'SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with
  MLLMs'
arxiv_id: '2504.13172'
source_url: https://arxiv.org/abs/2504.13172
tags:
- retrieval
- generative
- semantic
- cross-modal
- identifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemCORE, a semantic-enhanced generative cross-modal
  retrieval framework that addresses limitations in existing generative methods. The
  approach constructs structured natural language identifiers (SID) combining clustering-derived
  global IDs with keyword-extracted lexical IDs, then employs a generative semantic
  verification strategy for fine-grained discrimination.
---

# SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs

## Quick Facts
- **arXiv ID:** 2504.13172
- **Source URL:** https://arxiv.org/abs/2504.13172
- **Reference count:** 40
- **Primary result:** Achieves an average 8.65-point improvement in Recall@1 for text-to-image retrieval across benchmark datasets

## Executive Summary
This paper introduces SemCORE, a generative cross-modal retrieval framework that addresses limitations in existing generative methods. The approach constructs structured natural language identifiers (SID) combining clustering-derived global IDs with keyword-extracted lexical IDs, then employs a generative semantic verification strategy for fine-grained discrimination. SemCORE is the first framework to simultaneously handle both text-to-image and image-to-text retrieval tasks within the generative paradigm. Extensive experiments show the framework achieves an average 8.65-point improvement in Recall@1 for text-to-image retrieval across benchmark datasets, while matching or exceeding traditional methods for image-to-text retrieval. The framework better activates the semantic understanding capabilities of pre-trained MLLMs through its enriched identifier construction and verification strategies.

## Method Summary
SemCORE constructs Structured natural language IDentifiers (SID) by combining Global IDs (from K-Means clustering of MLLM-generated captions, cluster size=128) with Lexical IDs (extracted via KeyBERT with TF-IDF deduplication, length=4). The framework trains an MLLM to predict these SIDs from queries using next-token prediction loss. For inference, it employs constrained beam search via a Trie structure to ensure valid identifier generation, followed by a Generative Semantic Verification (GSV) step that re-ranks top-K candidates by prompting the MLLM to select the best match. The framework handles both text-to-image and image-to-text retrieval by swapping the query and target modalities.

## Key Results
- Achieves an average 8.65-point improvement in Recall@1 for text-to-image retrieval across benchmark datasets
- Matches or exceeds traditional methods for image-to-text retrieval tasks
- Shows 69.0% Recall@1 for text-to-image retrieval on Flickr30K (vs. 51.9% without GSV)
- Demonstrates the effectiveness of natural language identifiers over numeric indices for generative retrieval

## Why This Works (Mechanism)

### Mechanism 1: Natural Language Identifier Alignment
The framework replaces numeric indices with structured natural language identifiers (SID) that better utilize the pre-training of Multi-modal Large Language Models (MLLMs), which are optimized for semantic text generation rather than arbitrary token sequences. By feeding these natural language descriptors into the MLLM as targets, the model leverages its existing linguistic knowledge to map queries to identifiers, rather than learning random "1_1_2" style IDs from scratch.

### Mechanism 2: Hierarchical Semantic Composition (Global + Lexical)
Decomposing the retrieval target into a "Global ID" (coarse semantic cluster) and "Lexical ID" (fine-grained keywords) creates a curriculum that simplifies the generation task. The Global ID narrows the search space to a macro-semantic cluster, while the Lexical ID discriminates within that cluster using specific keywords. This hierarchical approach mirrors the "coarse-to-fine" search strategy but embeds it directly into the identifier structure.

### Mechanism 3: Generative Semantic Verification (GSV) as Reranking
A secondary generative step forces the model to explicitly select the best match from top candidates, resolving the "semantic collision" problem inherent in single-pass generation. Instead of relying solely on the probability of the generated ID string, GSV presents the top-K candidates to the MLLM with a prompt to "select the image that best matches." This engages the model's cross-modal reasoning capability to distinguish between candidates that might share similar ID prefixes.

## Foundational Learning

- **Generative Retrieval vs. Embedding Retrieval**: The paper fundamentally shifts from "encoding queries and targets to match vectors" to "training a model to memorize and output the target's identifier." Without grasping this paradigm shift, the focus on "identifier construction" seems misplaced. *Quick check: Does the system calculate similarity scores between vectors during inference? (Answer: No, it generates tokens).*

- **Constrained Decoding (Trie-based)**: Generative models can hallucinate. The Trie structure forces the model to only generate valid identifiers that actually exist in the corpus. This is the "safety rail" for the generation process. *Quick check: How does the model ensure the generated ID "39_weimaraner_leash" actually corresponds to an image in the database and isn't a hallucinated sequence?*

- **Semantic Collision**: The paper identifies that similar items often share ID prefixes (e.g., images of dogs). Understanding "collision" is key to understanding why the "Lexical ID" and "GSV" components are necessary to distinguish Item A from Item B. *Quick check: In a hierarchical ID system, if two images of dogs are assigned to the same cluster, what differentiates their IDs?*

## Architecture Onboarding

- **Component map:** Image Corpus -> MLLM Captioning -> K-Means (Global ID) + KeyBERT (Lexical ID) -> Structured Natural Language Identifiers (SID) -> MLLM trained on <Query, SID> pairs -> Constrained Beam Search -> Top-K Candidate SIDs -> Generative Semantic Verification -> Final Selection

- **Critical path:** The SID Construction (Section 3.2). If the Global IDs are too broad or the Lexical IDs contain noise/redundancy, the model cannot learn the mapping. The deduplication step is vital to prevent the Global and Lexical IDs from repeating the same semantic information.

- **Design tradeoffs:**
  - Natural Language vs. Atomic IDs: SIDs leverage semantic priors but introduce synonym ambiguity. Numeric IDs are unique but harder for the model to learn.
  - Lexical Length: Shorter Lexical IDs (length=2) reduce noise but lower discrimination. Length=4 is the sweet spot found by authors.

- **Failure signatures:**
  - False Negative Phenomenon: The system retrieves an image that matches the query semantically but is labeled "incorrect" because it doesn't match the specific ground truth annotation. This suggests the evaluation metric may undervalue the model's actual semantic understanding.
  - Synonym Confusion: The model might generate "canine" instead of "dog" if the SID strictly requires "dog". Constrained decoding prevents this if "canine" isn't a valid token in the Trie, but it highlights the rigidity of the identifier map.

- **First 3 experiments:**
  1. Identifier Format Ablation: Compare R@1 when training with Numeric IDs vs. the proposed SID on a small subset (e.g., Flickr30K) to validate the "Natural Language Alignment" hypothesis.
  2. GSV Contribution: Run inference with and without the GSV step on the top-10 candidates to quantify the specific lift in fine-grained discrimination.
  3. Scaling Test: Test if a smaller backbone (e.g., InternVL2.5-1B) fails to converge on the SID task compared to the 4B/8B models, checking the "scaling law" mentioned.

## Open Questions the Paper Calls Out

- **Generalization to Dynamic Target Sets:** Current methods rely on LLMs memorizing "static mappings," while real-world sets are dynamic. Improving generalization capacity is identified as a "critical direction" for future deployment. Evidence would require experiments evaluating Recall@1 on a streaming dataset where new images and identifiers are introduced after initial training.

- **Minimizing Synonym Ambiguity:** SemCORE underperforms on R@5 compared to AVG because SIDs "inevitably suffer from issues such as synonym ambiguity that can confuse generative retrieval." A modified SID construction mechanism that explicitly maps synonyms to a canonical form could resolve this, resulting in R@5 scores that exceed the AVG baseline.

- **GSV Efficiency at Scale:** While the introduction claims generative methods avoid the quadratic cost of traditional methods, the GSV strategy adds an additional MLLM inference step for every candidate. The paper demonstrates effectiveness on small candidate sets, but computational overhead on hundreds or thousands of candidates is not analyzed. A latency vs. accuracy analysis would resolve this.

## Limitations

- **Identifier Construction Reliability:** The SID system depends heavily on the quality of automatically generated captions and clustering. Poor caption generation or semantically incoherent clusters could propagate errors through the entire retrieval pipeline.

- **Evaluation Metric Alignment:** The "false negative phenomenon" suggests a potential mismatch between the model's semantic understanding and the evaluation protocol. If ground truth annotations are overly strict or inconsistent, reported performance improvements may overstate practical utility.

- **Hyperparameter Sensitivity:** Critical design choices (K-Means cluster size=128, lexical ID length=4, candidate set size=10 for GSV) appear to be empirical findings. The paper lacks systematic sensitivity analysis showing these values are optimal or robust across datasets.

## Confidence

- **High Confidence:** The core mechanism of using structured natural language identifiers (SID) instead of numeric indices is well-supported by the alignment argument with MLLM pre-training objectives. The ablation showing GSV's contribution provides strong evidence for the verification step's necessity.

- **Medium Confidence:** The hierarchical decomposition (Global + Lexical) effectively simplifies the generation task, as evidenced by the significant performance gap when removed. However, the optimal balance between cluster granularity and lexical specificity remains dataset-dependent.

- **Low Confidence:** The claim that this is the first framework to handle both text-to-image and image-to-text retrieval within the generative paradigm requires verification against prior work.

## Next Checks

1. **Cross-Dataset Generalization Test:** Evaluate SemCORE on a third benchmark dataset (e.g., Conceptual Captions) without hyperparameter tuning to assess whether the K=128 and lexical length=4 settings generalize beyond Flickr30K and MS-COCO.

2. **Annotation Quality Audit:** Manually examine 50 randomly selected retrieval results where the model retrieves a semantically relevant but "incorrect" image to quantify the false negative phenomenon and assess whether evaluation metrics underestimate true semantic understanding.

3. **Scaling Law Validation:** Test whether smaller MLLM backbones (e.g., InternVL2.5-400M or BLIP-2) fail to converge on the SID task while larger models (8B+) succeed, directly validating the scaling relationship claimed.