---
ver: rpa2
title: What Level of Automation is "Good Enough"? A Benchmark of Large Language Models
  for Meta-Analysis Data Extraction
arxiv_id: '2507.15152'
source_url: https://arxiv.org/abs/2507.15152
tags:
- data
- extraction
- field
- value
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for automating
  structured data extraction from full-text randomised controlled trials (RCTs) in
  meta-analysis. It tests three models (Gemini-2.0-flash, Grok-3, GPT-4o-mini) using
  four prompting strategies (basic, self-reflective, ensemble, and customised) across
  three medical domains.
---

# What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction

## Quick Facts
- arXiv ID: 2507.15152
- Source URL: https://arxiv.org/abs/2507.15152
- Reference count: 40
- LLMs tested: Gemini-2.0-flash, Grok-3, GPT-4o-mini show high precision but poor recall in RCT data extraction

## Executive Summary
This study evaluates large language models (LLMs) for automating structured data extraction from full-text randomised controlled trials (RCTs) in meta-analysis. It tests three models (Gemini-2.0-flash, Grok-3, GPT-4o-mini) using four prompting strategies (basic, self-reflective, ensemble, and customised) across three medical domains. All models showed high precision but poor recall, with customised prompts yielding up to 15% recall improvement. Statistical results were most challenging, while study information was easiest. Based on empirical performance, the authors propose a three-tier automation guideline matching data types to automation levels based on task complexity and error risk, offering practical guidance for real-world meta-analytic workflows.

## Method Summary
The study evaluated three LLMs using zero-shot inference on 58 full-text RCT PDFs from six published meta-analyses across hypertension, diabetes, and orthopedics. Four prompting strategies were tested: baseline extraction, self-reflection, ensemble combination, and customised domain-specific prompts. Data was extracted into structured JSON format and evaluated against manual ground truth using a blinded LLM evaluator, with human validation on 900 samples achieving 96.1% agreement. Precision and recall were measured at the element level across three categories: statistical results, quality assessment, and study information.

## Key Results
- All models demonstrated high precision (>0.85) but consistently poor recall (<0.45) across all categories
- Customised prompts improved recall by up to 15% with minimal precision loss (-0.8%)
- Ensemble methods provided balanced improvement (~6% recall gain, 2% precision gain)
- Statistical results extraction was most challenging; study information was easiest
- 87.8% of errors were missing fields rather than hallucinations or incorrect values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Customized domain-specific prompts improve recall by up to 15% with minimal precision loss
- Mechanism: Explicit domain priming (e.g., specifying "orthopedic and metabolic bone disease expertise" and listing target outcomes like "Bone Mineral Density, Bone Turnover Markers") focuses LLM attention on specialized terminology and field structures that generic prompts miss, reducing omission errors in scattered or technically dense content
- Core assumption: LLMs encode domain patterns from training data that can be selectively activated via targeted prompting
- Evidence anchors:
  - [abstract] "customised prompts were the most effective, boosting recall by up to 15%"
  - [section 3.1.1] Customised EXT delivered 14.8% average recall gain with only -0.8% precision drop; Friedman-Nemenyi test confirmed statistical significance (p=0.0203)
  - [corpus] Related work on prompting strategies (ZSEE evaluation, arxiv 2512.15312) supports prompt specialization effects, though not directly replicated for meta-analysis tasks
- Break condition: When target fields are structurally ambiguous or inconsistently reported across papers, domain priming alone cannot compensate; performance gains plateau

### Mechanism 2
- Claim: Ensemble model combination provides balanced precision-recall improvement (~6% recall gain, 2% precision gain) through cross-model agreement voting
- Mechanism: Different LLMs have complementary biases—combining outputs via hierarchical rules (majority voting → confidence-weighted selection → completeness criteria) captures fields that any single model misses while filtering isolated hallucinations
- Core assumption: Models make independent errors; agreement correlates with correctness
- Evidence anchors:
  - [abstract] Four prompting strategies tested; ensemble among them
  - [section 2.2 Step 2c] Merging rules explicitly described: "If two models agreed on a value and the third differed, the majority value was retained"
  - [section 3.1.1] Combined EXT achieved 5.9% recall gain + 2.0% precision gain; particularly effective for weaker base models (GPT: +14.3% recall)
  - [corpus] Ensemble methods for biomedical concept extraction (Li et al., JAMIA 2024) show similar patterns, though not meta-analysis specific
- Break condition: When all models consistently omit the same field (shared blind spots), ensembling provides no recovery

### Mechanism 3
- Claim: Recall failure—not precision—is the dominant bottleneck; 87.8% of errors are missing fields, not hallucinations
- Mechanism: Information critical for meta-analysis (statistical outcomes, subgroup details) is often embedded in narrative text, tables, or figures with non-standard formatting. LLMs fail to detect rather than misinterpret these fields. High precision reflects conservative extraction behavior when uncertain
- Evidence anchors:
  - [abstract] "All models demonstrate high precision but consistently suffer from poor recall by omitting key information"
  - [section 3.3.1] "87.8% (19,470 instances)—were missing fields"; incorrect values only 10.3%, overgeneralization 1.2%, unit errors 0.7%
  - [section 4.1] "Frontier commercial LLM models most commonly made errors of omission rather than commission"
  - [corpus] Limited direct corpus evidence on recall vs. precision distribution in extraction tasks; this finding appears domain-specific
- Break condition: If recall were artificially boosted via aggressive extraction thresholds, precision would degrade rapidly—trade-off is inherent

## Foundational Learning

- Concept: **Precision-Recall Trade-off in Extraction**
  - Why needed here: The paper's central finding hinges on understanding that high precision (few false positives) does not imply completeness (high recall). Engineers must recognize that optimizing for one metric often degrades the other
  - Quick check question: If an LLM extracts 10 fields and 9 are correct, but 20 fields existed in the document, what are precision and recall?

- Concept: **Meta-Analysis Data Requirements (PICO, effect sizes, risk of bias)**
  - Why needed here: The three-tier automation framework maps task types to automation levels. Without understanding what statistical results vs. study information vs. quality assessment mean in evidence synthesis, tier assignment is arbitrary
  - Quick check question: Which would distort a pooled effect estimate more: a wrong sample size or a missing author name?

- Concept: **Prompt Engineering for Structured Output (JSON schemas, confidence fields, source attribution)**
  - Why needed here: All extraction methods in the paper required structured JSON output with meta-information (confidence, source). Understanding how to specify output schemas and enforce formatting is prerequisite to reproducing the pipeline
  - Quick check question: Why does the baseline prompt require both a "confidence" field and a "source" field for each extracted value?

## Architecture Onboarding

- Component map:
  Input Layer: Full-text RCT PDFs (58 papers from 6 meta-analyses, token range 775–3871)
  Extraction Layer: Three LLMs (GPT-4o-mini, Gemini-2.0-flash, Grok-3) with four prompting strategies (EXT, EXT+Self-reflection, Combined EXT, Customised EXT)
  Aggregation Layer: Ensemble merger (Gemini-based with rule-based hierarchical logic)
  Evaluation Layer: Blinded LLM evaluator (Gemini) comparing extracted JSON to ground truth; human validation on 900 samples (96.1% agreement)
  Output Layer: Three-tier automation recommendation mapped to data categories

- Critical path: Customised EXT with domain-specific prompts → JSON extraction with confidence/source metadata → human review for Tier 3 (statistical results). Self-reflection and ensemble are optional enhancements, not required for minimum viable pipeline

- Design tradeoffs:
  Customised prompts: Higher recall vs. prompt maintenance overhead per domain
  Ensemble: Better completeness vs. 3x API cost and latency
  Self-reflection: Minor recall gain (~1.8%) vs. additional inference step; not cost-effective for most use cases
  Model choice: Gemini for statistical extraction, Grok for quality assessment—no single model dominates all categories

- Failure signatures:
  High missing-field rate on Outcome Measures (95.2% of OM errors) → information buried in tables/figures
  Incorrect values spike in Participant Characteristics (24.6%) → group-specific misattribution
  Overgeneralization in Intervention/Exposure (28.6%) → missing subgroup distinctions
  GPT consistently underperforms on recall (<0.5 on complex statistical fields)

- First 3 experiments:
  1. **Baseline sanity check**: Run basic EXT prompt on 5 papers from a single meta-analysis; measure precision/recall by category. Expect high precision (>0.85), low recall on statistics (<0.45). This establishes the "omission problem" concretely
  2. **Customized prompt ablation**: Create a domain-specific prompt for your target meta-analysis topic (list exact outcome names, expected units, common abbreviations). Compare recall delta vs. baseline. Target: 10–15% improvement on statistical fields
  3. **Ensemble cost-benefit**: Run all three models on the same 5 papers, implement the majority-voting merger, measure precision/recall gain vs. per-model cost. Decision point: If recall gain <5% and cost >2x, skip ensemble for production

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs reliably extract study data that is embedded in non-machine-readable charts and figures?
- Basis in paper: [explicit] The authors state in the Limitations that "key study data was embedded in charts, figures or non-machine-readable graphical elements... Future work will focus on... exploring automated data extraction from these sources specifically."
- Why unresolved: The current study processed PDFs primarily as text streams, failing to capture information represented visually, which limits the completeness of extraction.
- What evidence would resolve it: A benchmark evaluation of LLMs with vision capabilities (multimodal models) on RCT datasets where ground truth is derived from graphical data.

### Open Question 2
- Question: Do newer models with "test-time compute" improve recall for complex statistical extraction compared to the standard models tested?
- Basis in paper: [explicit] The authors note in the Limitations that "Newer models... leveraging test-time compute for increased 'thinking' during inference, may achieve better performances than our results."
- Why unresolved: The study tested Gemini-2.0-flash, Grok-3, and GPT-4o-mini, but did not evaluate advanced reasoning models that allocate more compute during inference.
- What evidence would resolve it: A comparative performance analysis using reasoning-optimized models on the same complex statistical extraction tasks.

### Open Question 3
- Question: How do computational costs and API latency scale with the proposed extraction strategies?
- Basis in paper: [explicit] The authors explicitly state they "did not systematically evaluate computational costs, API expenses, or processing times. Future work will also factor in these operational aspects."
- Why unresolved: While Customised EXT improved recall, it is unclear if the performance gains justify the potential increase in token usage or financial cost compared to baseline methods.
- What evidence would resolve it: A cost-benefit analysis correlating the precision/recall improvements of customised or ensemble prompts against their specific token consumption and latency.

## Limitations
- Limited Domain Scope: Results based on three medical fields may not generalize to other disciplines with different reporting conventions
- Ground Truth Reliability: Manual extraction by LLM evaluators introduces potential bias; human validation achieved 96.1% agreement but remaining uncertainty affects benchmark accuracy
- Model Availability Constraints: Grok-3 access may be restricted, limiting reproducibility of the full ensemble approach

## Confidence
- High Confidence: Precision-Recall trade-off findings and the dominance of omission errors (87.8% missing fields)
- Medium Confidence: Custom prompt recall improvements (14.8% gain) - statistically significant but may vary with domain complexity
- Medium Confidence: Three-tier automation framework - practical guidance is empirically grounded but requires validation across diverse contexts

## Next Checks
1. **Domain Generalization Test**: Apply the three-tier framework to a meta-analysis from a non-medical field (e.g., social sciences or engineering) to assess cross-domain applicability
2. **Human-in-the-Loop Validation**: Implement the proposed workflow in a real-world meta-analysis project, measuring actual time savings versus traditional manual extraction
3. **Prompt Maintenance Cost Analysis**: Track the time and expertise required to create custom prompts for three new meta-analysis topics to quantify the maintenance overhead of the recommended approach