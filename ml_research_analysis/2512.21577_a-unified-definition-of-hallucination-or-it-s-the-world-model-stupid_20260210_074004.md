---
ver: rpa2
title: 'A Unified Definition of Hallucination, Or: It''s the World Model, Stupid'
arxiv_id: '2512.21577'
source_url: https://arxiv.org/abs/2512.21577
tags:
- hallucination
- world
- arxiv
- language
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of inconsistent definitions of
  "hallucination" across different domains (summarization, QA, RAG, agentic systems,
  VLMs), which hinders benchmark design and evaluation. The core idea is to unify
  these definitions by framing hallucination as "inaccurate (internal) world modeling
  observable to the user," defined relative to a reference world model W, view function
  V, and conflict policy P.
---

# A Unified Definition of Hallucination, Or: It's the World Model, Stupid

## Quick Facts
- arXiv ID: 2512.21577
- Source URL: https://arxiv.org/abs/2512.21577
- Reference count: 31
- One-line primary result: Proposes unifying hallucination definitions across domains via explicit specification of reference world model W, view function V, and conflict policy P.

## Executive Summary
This paper addresses the problem of inconsistent hallucination definitions across different domains by proposing a unified framework. The core insight is that all prior definitions implicitly specify a reference world model W, a view function V determining observable information, and a conflict policy P resolving contradictions. Hallucination is then defined as output claims that are false under T_W,P(x,c). The authors illustrate this with examples across summarization, QA, RAG, and agentic systems, and propose using synthetic environments like chess to create scalable benchmarks with programmatically defined truth functions.

## Method Summary
The method involves specifying three components: a reference world model W encoding truth, a view function V determining observable information, and a conflict policy P resolving contradictions. Hallucination is formally defined as the existence of a claim c in C(y) such that T_W,P(x, c) = false. For synthetic benchmarks, the paper proposes using environments like chess where W, V, and P can be fully specified programmatically. Document generators create textual context from game states, query generators probe understanding, and truth functions can be computed exactly from state and history. The framework requires implementing claim extraction to decompose model outputs into atomic claims for evaluation.

## Key Results
- Proposes a unified definition of hallucination as "inaccurate internal world modeling observable to the user"
- Formalizes hallucination as ∃c ∈ C(y) such that T_W,P(x, c) = false
- Outlines plans for synthetic benchmarks using fully specified world models
- Illustrates framework with chess example and plans for richer environments
- No quantitative metrics reported - this is a conceptual framework paper

## Why This Works (Mechanism)

### Mechanism 1: Unification via Explicit (W, V, P) Specification
- Claim: Fragmented hallucination definitions across domains share a common structure that, when made explicit, enables cross-benchmark comparison.
- Mechanism: All prior definitions implicitly specify (1) a reference world model W encoding truth, (2) a view function V determining observable information, and (3) a conflict policy P resolving contradictions. Hallucination = output claims that are false under T_W,P(x,c). Different domains merely instantiate these components differently.
- Core assumption: That meaningful comparison across domains requires only specifying W, V, P—not fundamentally different evaluation paradigms.
- Evidence anchors:
  - [abstract] "By varying the reference world model as well as the knowledge conflict policy (e.g., knowledge base vs. in-context), we arrive at the different existing definitions of hallucination present in the literature."
  - [section 3] Formal definition: hallucination occurs when ∃c ∈ C(y) such that T_W,P(x, c) = false.
  - [corpus] Weak direct support; HalluLERN (Bang et al.) evaluates hallucinations but does not unify definitions across tasks.
- Break condition: If hallucination fundamentally requires domain-specific evaluation logic beyond (W, V, P), the unification would add notation without practical benefit.

### Mechanism 2: Synthetic Environments Enable Programmatic Ground Truth
- Claim: Benchmarks built on synthetic, fully-specified worlds (games, databases) can generate scalable hallucination labels without human annotation.
- Mechanism: In environments like chess, the world model W is the game rules and state s; the truth function T_W,P can be computed exactly from (s, h). Document generator Γ_docs creates textual context from state; query generator Γ_query probes understanding; claims are automatically checked against W.
- Core assumption: That synthetic environment hallucination failures transfer to real-world settings, or at least stress-test the same world-modeling capabilities.
- Evidence anchors:
  - [abstract] "We outline plans for a family of benchmarks in which hallucinations are defined as mismatches with synthetic but fully specified world models in different environments."
  - [section 5.1] Chess case study: claims like "White can capture the queen in one move" are contradicted by board state + rules.
  - [corpus] Weak support; neighboring papers address definition extraction or time-series hallucination but not synthetic benchmarking strategies.
- Break condition: If models treat synthetic and real environments as fundamentally different reasoning regimes, synthetic benchmark gains may not generalize.

### Mechanism 3: Separating World-Modeling Errors from Planning/Incentive Errors
- Claim: Not all wrong outputs are hallucinations; the framework distinguishes (a) incorrect beliefs about W from (b) correct beliefs but poor actions (planning errors) or (c) misaligned incentives (reward errors).
- Mechanism: Only outputs where implied claims contradict W count as hallucinations. An agent claiming a nonexistent button exists hallucinates; an agent seeing the correct button but clicking the wrong one makes a planning error.
- Core assumption: That this distinction is actionable—that interventions targeting world modeling differ from those targeting planning or calibration.
- Evidence anchors:
  - [section 3.1.4] Agentic example: clicking "submit-btn" when DOM shows only "confirm-btn" is hallucination; clicking wrong button with correct DOM perception is not.
  - [section 4] "Error is about outputs; hallucination is about the implied world those outputs assume."
  - [corpus] No direct corpus support for this error taxonomy.
- Break condition: If empirically, world-modeling errors correlate strongly with planning errors (same root cause), the distinction may not guide different interventions.

## Foundational Learning

- Concept: Reference World Model (W) as Ground Truth vs. Learned Internal Model
  - Why needed here: The paper's core move is redefining hallucination as mismatch with an explicit W, not as a property of model internals. Without this distinction, you cannot apply the (W, V, P) framework.
  - Quick check question: Given a summarization task, what is W? What is W for open-domain QA?

- Concept: Atomic Claim Decomposition C(y)
  - Why needed here: The truth function operates on atomic claims, not whole outputs. Evaluation requires decomposing text into verifiable propositions.
  - Quick check question: "TechCorp exceeded expectations with $2.1B revenue" contains which atomic claims? Which would T_W,P mark false?

- Concept: Conflict Policy P and Knowledge Source Prioritization
  - Why needed here: RAG systems face conflicts between retrieved documents, parametric memory, and context. P specifies which source overrides others—this determines what counts as hallucination.
  - Quick check question: In RAG with P = "retrieved docs override memory," if a doc says X and model knows ¬X, does asserting X constitute hallucination?

## Architecture Onboarding

- Component map:
  - W (reference world) -> V (view function) -> P (conflict policy) -> T_W,P (truth function) -> C(y) (claim extractor)
  - Γ_docs, Γ_query (generators) -> synthetic benchmarks

- Critical path:
  1. Specify W, V, P for your evaluation setting
  2. Generate or collect inputs x
  3. Obtain model output y
  4. Extract claims C(y)
  5. Evaluate each claim c via T_W,P(x, c)
  6. Flag hallucination if any c evaluates to false

- Design tradeoffs:
  - Synthetic vs. real W: Synthetic enables scale and exact truth; real ensures ecological validity but requires annotation
  - Granularity of C(y): Fine-grained claims improve precision but increase extraction complexity and error propagation
  - Unknown handling: Treating "unknown" claims as errors vs. acceptable affects benchmark difficulty

- Failure signatures:
  - Claim extraction errors: C(y) misses claims or creates spurious ones, corrupting T_W,P evaluation
  - Underspecified P: When conflict policy is implicit, different evaluators may disagree on ground truth
  - W-reality gap: If W does not match what users expect as "true," benchmark measures alignment to wrong target

- First 3 experiments:
  1. Replicate chess benchmark pipeline: generate 100 board states, create textual descriptions via Γ_docs, query model on move legality, evaluate claims against game engine's T_W,P.
  2. Ablate V: test same W with different view functions (full board description vs. partial vs. move history only) to isolate observability effects on hallucination rate.
  3. Vary P in RAG setting: create synthetic retrieval corpus with known conflicts; evaluate whether model follows specified conflict policy when answering questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be trained to reliably follow explicit conflict policies (e.g., "retrieved context overrides parametric memory"), and do learned policies generalize to novel source conflicts?
- Basis in paper: [explicit] Section 8.1: "study whether models can be trained to follow a given policy reliably. This includes adversarial tests where sources conflict or are partially corrupted."
- Why unresolved: The paper notes conflict policy is often the "hidden source of disagreement" across hallucination papers but no systematic evaluation of policy-following exists.
- What evidence would resolve it: A benchmark suite systematically varying P across conflict scenarios, demonstrating transfer to held-out conflict types.

### Open Question 2
- Question: How should hallucination benchmarks assess models when the reference world W changes non-stationarily during long-horizon interactions?
- Basis in paper: [explicit] Section 8.2: "Designing hallucination benchmarks that can appropriately assess models in such scenarios with a significantly changing W represents an important next step."
- Why unresolved: Existing benchmarks assume static W; the paper explicitly notes this gap and calls for benchmarks handling dynamic W updates.
- What evidence would resolve it: Benchmarks with programmatically updated world states showing clear evaluation protocols for temporal drift in hallucination rates.

### Open Question 3
- Question: Do synthetic-environment benchmarks (e.g., chess, games) produce hallucination metrics that correlate with real-world deployment failures?
- Basis in paper: [inferred] Section 5.1 describes chess as a "clean base case" but Section 8.3 calls for "richer worlds... closer to deployment" including web/DOM and codebases.
- Why unresolved: The paper proposes synthetic worlds for scalable evaluation but provides no empirical validation that chess-world hallucinations predict real-domain failures.
- What evidence would resolve it: Correlation analysis between synthetic benchmark performance and hallucination rates on established benchmarks (e.g., RAG QA, summarization).

### Open Question 4
- Question: In multi-turn agentic settings, can world-modeling errors (hallucinations) be cleanly separated from planning errors and incentive misalignment?
- Basis in paper: [explicit] Section 8.4: "A next step is to build interactive benchmarks where W includes both state and execution traces, and where the evaluation separately scores claim-level state consistency, action validity, and task success."
- Why unresolved: The taxonomy exists (Section 4), but no benchmark currently decomposes agent failures into these distinct categories with appropriate ground-truth labels.
- What evidence would resolve it: An agentic benchmark with separate metrics for state-estimation accuracy, plan optimality, and goal-alignment, validated against human annotations of failure mode types.

## Limitations
- No quantitative metrics reported - this is a conceptual framework paper
- Claim extraction mechanism C(y) is not specified, creating a critical implementation gap
- Synthetic benchmark effectiveness and ecological validity remain unproven
- Error taxonomy (world-modeling vs. planning vs. reward errors) lacks empirical validation

## Confidence
- **High confidence**: The core insight that existing hallucination definitions can be unified by making (W, V, P) explicit is well-supported by the literature review and formal definition. The conceptual framework is internally consistent.
- **Medium confidence**: The synthetic benchmark proposal is methodologically sound, but its practical effectiveness and ecological validity remain unproven without empirical results. The claim that synthetic environments can adequately test real-world world-modeling capabilities is plausible but unverified.
- **Low confidence**: The proposed error taxonomy (world-modeling vs. planning vs. reward errors) lacks empirical validation. Whether this distinction is operationally meaningful for model improvement is unknown.

## Next Checks
1. Implement and evaluate the chess benchmark pipeline on at least two different LLMs, measuring hallucination rates and comparing to human evaluation on a subset to validate the synthetic setup.

2. Conduct an ablation study varying V (view function) while holding W and P constant to quantify how observability affects hallucination detection, establishing whether the framework captures meaningful differences.

3. Test the framework's claim about error taxonomy by inducing world-modeling errors (via corrupted context) and planning errors (via correct context but ambiguous queries) separately, measuring whether different interventions are needed for each.