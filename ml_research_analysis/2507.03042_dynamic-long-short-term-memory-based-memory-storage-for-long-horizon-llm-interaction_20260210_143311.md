---
ver: rpa2
title: Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction
arxiv_id: '2507.03042'
source_url: https://arxiv.org/abs/2507.03042
tags:
- memory
- preference
- user
- classifier
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Pref-LSTM, a dynamic preference-aware memory
  system for LLMs that combines a BERT-based classifier with an LSTM memory module
  to enable long-term personalization. The approach first filters user utterances
  to detect preference indicators, then encodes these into a dynamic memory embedding
  that is soft-prompt injected into a frozen LLM.
---

# Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction

## Quick Facts
- arXiv ID: 2507.03042
- Source URL: https://arxiv.org/abs/2507.03042
- Reference count: 19
- Primary result: BERT-based preference classifier achieves 98% accuracy but LSTM memory controller fails to improve LLM preference-following

## Executive Summary
This paper proposes Pref-LSTM, a dynamic preference-aware memory system for LLMs that combines a BERT-based classifier with an LSTM memory module to enable long-term personalization. The approach first filters user utterances to detect preference indicators, then encodes these into a dynamic memory embedding that is soft-prompt injected into a frozen LLM. Experiments show the BERT-based classifier achieves 98% accuracy on custom preference detection, reliably identifying explicit and implicit preferences. However, the LSTM-based memory controller did not yield strong results in improving LLM preference-following despite 10 epochs of training, likely due to limited compute, dataset mismatch, and soft-prompt injection challenges.

## Method Summary
The system implements a three-stage pipeline: (1) a frozen BERT encoder with lightweight MLP classification head filters user utterances to identify preference-indicating statements, (2) a MiniLM encoder converts preference utterances into embeddings, and (3) an LSTM memory controller with forget gates dynamically updates a memory vector that is projected to token embedding space and prepended as a soft prompt to a frozen LLM. The classifier achieves 98% accuracy on synthetic data but struggles with casual language. The LSTM controller is trained on OASST1 with cross-entropy loss but fails to improve downstream preference-following after 10 epochs.

## Key Results
- BERT-based classifier achieves 98% accuracy on custom preference detection test set
- Classifier performance drops significantly on casual language with informal slang
- LSTM-based memory controller did not yield strong results in improving LLM preference-following after 10 epochs of training

## Why This Works (Mechanism)

### Mechanism 1: BERT-Based Preference Filtering
A frozen pretrained BERT encoder with a lightweight MLP classification head can reliably discriminate preference-indicating utterances from neutral conversational turns, reducing memory noise. The [CLS] token representation aggregates sequence semantics; the MLP head maps this to a binary probability. Only utterances crossing a classification threshold proceed to memory encoding, filtering ~58% of turns (based on dataset composition: 4915 non-preference vs 3537 preference).

### Mechanism 2: LSTM-Inspired Gated Memory Update
An element-wise forget gate computed from the previous memory state and incoming preference embedding enables selective retention and overwriting of preference dimensions, in principle handling conflicting preferences. Mt = ft ⊙ Mt−1 + (1 − ft) ⊙ Ēt, where ft = σ(WMM·Mt−1 + WEM·Ēt + b). The sigmoid gate produces per-dimension retention weights; dimensions with high ft retain prior information, while low-ft dimensions accept new embeddings.

### Mechanism 3: Soft-Prompt Memory Injection
Projecting the memory embedding into the LLM's token embedding space and prepending it as a soft prompt can condition generation without fine-tuning, conditional on the projection matrix learning a meaningful alignment. Tt = Mt·WM projects memory vector Mt ∈ Rdm to pseudo-token Tt ∈ Rde, which is prepended to the input sequence. The frozen LLM attends to this soft token during generation.

## Foundational Learning

- **Concept: LSTM Gating (Input/Forget/Output Gates)** - The memory update mechanism directly implements a forget gate; understanding how σ(Wx + b) produces continuous retention weights is essential to debugging why the controller failed to learn.
- **Concept: Soft Prompting vs. Hard Prompting** - The system injects memory as continuous embeddings rather than discrete tokens; this differs fundamentally from RAG-style retrieval and requires different training strategies.
- **Concept: Distribution Shift in Synthetic Data** - The classifier was trained on LLM-generated conversations but tested on human-crafted examples; the performance gap (formal vs. casual) illustrates a core generalization failure mode.

## Architecture Onboarding

- **Component map:** User Utterance → BERT Classifier (frozen + MLP head) → [preference?] → (skip update) or MiniLM Encoder → Preference Embedding Ēt → LSTM Memory Controller (ft gate + update Mt) → Projection Matrix WM → Soft Token Tt → Frozen LLM → Response
- **Critical path:** BERT classifier → MiniLM encoder → LSTM gate computation → projection to token space → LLM attention over soft token. If any component produces noisy outputs, downstream performance collapses.
- **Design tradeoffs:** Lightweight vs. Expressive (MiniLM vs full BERT for preference encoding), Frozen LLM vs. Fine-tuned (no LLM fine-tuning enables scalability but limits soft-prompt interpretation), Binary Classification vs. Multi-class (binary simplifies training but discards preference type information).
- **Failure signatures:** LLM immediately emits `<end>` token → projection matrix producing out-of-distribution embeddings; Memory never updates → classifier threshold too strict or all utterances classified as non-preference; Memory updates but LLM ignores preferences → soft token in attention dead zone; Conflicting preferences both retained → forget gate not learning to overwrite.
- **First 3 experiments:** Classifier stress test on 50 human-written casual utterances; Projection ablation comparing learned WM against random projections; Memory inspection logging ft distributions and Mt norms across 100 turns.

## Open Questions the Paper Calls Out

### Open Question 1
Would alternative integration mechanisms beyond soft-prompt injection (e.g., cross-attention modules, gating layers, or concatenated latent representations) enable more effective transfer of preference information from the LSTM memory controller to the downstream LLM?

### Open Question 2
Does conditionally injecting memory embeddings only when the current task is preference-relevant improve downstream LLM performance compared to unconditional injection?

### Open Question 3
How much training data and compute are actually required for the LSTM memory controller to learn meaningful preference representations that improve LLM personalization?

### Open Question 4
Can the preference classifier's robustness to casual, informal language be improved through training data augmentation with diverse conversational styles and implicit preference expressions?

## Limitations
- LSTM-based memory controller failed to improve LLM preference-following after 10 epochs of training
- Soft-prompt injection often produces noise, causing LLMs to immediately emit `<end>` tokens
- Classifier performance degrades significantly on casual language with informal slang

## Confidence

- **High Confidence:** BERT-based preference filtering can reliably detect explicit and implicit preferences in controlled synthetic settings (98% accuracy on test set)
- **Medium Confidence:** The LSTM gating mechanism for memory updates is formally correct and can learn selective retention/overwrite in principle, but practical implementation challenges prevent demonstrated effectiveness
- **Low Confidence:** Soft-prompt memory injection can condition frozen LLM generation in a stable, useful manner without additional architectural modifications or fine-tuning

## Next Checks

1. **Classifier Robustness Audit:** Evaluate the BERT classifier on a diverse corpus of 100+ real-world user utterances (Reddit, Twitter, support tickets) spanning multiple dialects, slang patterns, and implicit preference expressions. Measure precision/recall against human annotations.

2. **Projection Matrix Sanity Check:** Implement an ablation study comparing learned WM against random projections and identity mappings. For each condition, measure (a) Mt norm stability over time, (b) soft token cosine similarity to actual LLM tokens, and (c) downstream preference-following scores on PrefEval.

3. **LSTM Gate Behavior Analysis:** Instrument the memory controller to log forget gate values (ft) and memory norms (||Mt||) across 500 interaction turns. Plot ft histograms and Mt trajectories to identify whether gates cluster around 0.5 (noisy retention) or Mt norms collapse toward zero.