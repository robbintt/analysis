---
ver: rpa2
title: Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search
arxiv_id: '2506.20330'
source_url: https://arxiv.org/abs/2506.20330
tags:
- retrieval
- item
- which
- image
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving semantic retrieval
  in e-commerce search by leveraging multimodal data, specifically incorporating image
  information alongside text. The authors propose a novel model, SMAR (Semantic-enhanced
  Modality-Asymmetric Retrieval), to tackle the unique problem of asymmetric modality
  retrieval, where queries are unimodal (text) but items are multimodal (text + image).
---

# Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search

## Quick Facts
- **arXiv ID:** 2506.20330
- **Source URL:** https://arxiv.org/abs/2506.20330
- **Reference count:** 30
- **Primary result:** 4.90% improvement in recall and 1.50% in precision over baseline models

## Executive Summary
This paper addresses the challenge of improving semantic retrieval in e-commerce search by incorporating image information alongside text for multimodal item representation. The authors propose SMAR (Semantic-enhanced Modality-Asymmetric Retrieval), a novel model that tackles asymmetric modality retrieval where queries are text-only but items are text+image. Through extensive offline experiments and online A/B tests, SMAR demonstrates significant improvements in retrieval performance and business metrics, particularly for fashion categories.

## Method Summary
SMAR employs a two-stage training strategy to address the unique challenges of asymmetric multimodal retrieval in e-commerce. In the pre-training stage, a multi-task learning approach handles modality fusion, alignment, and contributions through three tasks: text-text similarity, text-image alignment, and text-multimodal alignment. The fine-tuning stage uses adaptive embedding learning to dynamically determine the importance of image modality for different product categories. The architecture features separate towers for queries, item text, item image, and item multimodal (with cross-attention), trained on an industrial dataset with synthetic query augmentation.

## Key Results
- 4.90% improvement in recall@50 and 1.50% improvement in precision@50 over baseline models
- Significant online improvements: 1.19% GMV and 1.01% UCVR increase
- Fashion categories show the most substantial gains, validating the importance of image modality for visually-driven products
- Open-sourced industrial dataset (1.1M non-fashion, 1M fashion examples) to facilitate reproducibility

## Why This Works (Mechanism)
SMAR works by leveraging the complementary information from text and image modalities to create richer item representations. The two-stage training approach first learns general cross-modal relationships through multi-task learning, then adapts to the specific importance of image modality for different product categories. The prediction header P dynamically determines when to use multimodal embeddings versus text-only embeddings, optimizing for the varying importance of visual information across categories. This approach addresses the asymmetry between text queries and multimodal items while maintaining computational efficiency.

## Foundational Learning
- **Cross-modal alignment**: Why needed - Aligns text and image representations to capture semantic relationships between modalities. Quick check - Verify alignment loss decreases during training and improves retrieval accuracy.
- **Multi-task learning**: Why needed - Simultaneously learns different aspects of modality fusion (similarity, alignment, contribution). Quick check - Ensure all three task losses converge and contribute to overall performance.
- **Adaptive modality weighting**: Why needed - Different product categories require different levels of image information importance. Quick check - Compare performance when forcing uniform modality weighting versus adaptive approach.
- **Synthetic query generation**: Why needed - Addresses data sparsity by expanding the query space. Quick check - Evaluate retrieval performance with and without synthetic queries to quantify their impact.

## Architecture Onboarding

**Component Map:** Query tower → Similarity computation → Loss calculation
Item text tower ← Cross-attention ← Item multimodal tower → Similarity computation → Loss calculation
Item image tower ← Cross-attention ← Item multimodal tower

**Critical Path:** The critical path is the similarity computation between query embeddings and item embeddings (text or multimodal). The item multimodal tower with cross-attention is essential for learning effective fusion representations.

**Design Tradeoffs:** The two-stage training approach trades increased training complexity for better generalization and adaptability. Using separate towers for text and image before fusion allows for modality-specific feature learning, but adds model complexity compared to joint encoding.

**Failure Signatures:** 
- Poor performance on non-fashion categories indicates over-reliance on image modality
- Similar performance between SMAR and SMAR-nc (no cross-attention) suggests fusion isn't learning useful representations
- Inconsistent improvement across different β values in sensitivity analysis indicates unstable modality weighting

**3 First Experiments:**
1. Implement the multi-task pre-training with varying α, β, γ values to identify optimal modality weighting
2. Train SMAR and SMAR-nc (without cross-attention) to quantify the contribution of cross-modal fusion
3. Evaluate performance separately on fashion vs. non-fashion categories to validate category-specific modality importance

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on synthetic query generation without detailed methodology, raising questions about real-world generalization
- Sparse implementation details for the prediction header P that determines modality importance
- Relatively small number of real queries (37,032) compared to synthetic queries in the open-sourced dataset
- Potential overfitting to fashion categories where image modality is most important

## Confidence
- **High confidence:** Two-stage training approach is clearly described and reproducible; improvement metrics are directly reported
- **Medium confidence:** Architecture design with separate towers and cross-attention is well-specified, though hyperparameters are missing
- **Low confidence:** Adaptive embedding learning mechanism and prediction header P implementation details are not fully detailed

## Next Checks
1. Verify synthetic query generation strategy by reconstructing the process and testing its impact on retrieval performance
2. Test modality importance across categories by evaluating SMAR separately on fashion vs. non-fashion categories with varying β values
3. Conduct ablation study on prediction header by implementing SMAR without P (always using multimodal embeddings) to quantify adaptive modality selection contribution