---
ver: rpa2
title: Ovis2.5 Technical Report
arxiv_id: '2508.11737'
source_url: https://arxiv.org/abs/2508.11737
tags:
- ovis2
- zhang
- reasoning
- wang
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Ovis2.5 introduces native-resolution vision processing and enhanced\
  \ reasoning through self-reflection to overcome tiling artifacts and limited depth\
  \ in previous multimodal models. It replaces fixed-resolution ViT with NaViT to\
  \ preserve fine detail and global layout, and augments training with deep-reasoning\
  \ data that encourages reflection and revision, exposed as an optional \u201Cthinking\
  \ mode.\u201D A five-phase training curriculum and high-efficiency infrastructure\
  \ with data packing and hybrid parallelism enable effective scaling."
---

# Ovis2.5 Technical Report

## Quick Facts
- **arXiv ID:** 2508.11737
- **Source URL:** https://arxiv.org/abs/2508.11737
- **Reference count:** 40
- **Primary result:** 9B model achieves 78.3 on OpenCompass (SOTA among open-source MLLMs under 40B parameters); 2B model reaches 73.9 (SOTA for its size)

## Executive Summary
Ovis2.5 introduces native-resolution vision processing and enhanced reasoning through self-reflection to overcome tiling artifacts and limited depth in previous multimodal models. It replaces fixed-resolution ViT with NaViT to preserve fine detail and global layout, and augments training with deep-reasoning data that encourages reflection and revision, exposed as an optional "thinking mode." A five-phase training curriculum and high-efficiency infrastructure with data packing and hybrid parallelism enable effective scaling. The 9B model achieves 78.3 on OpenCompass, setting a new state of the art among open-source MLLMs under 40B parameters; the 2B model reaches 73.9, also SOTA for its size, and delivers leading performance on STEM, OCR/chart, grounding, and video tasks.

## Method Summary
Ovis2.5 uses a native-resolution vision transformer (NaViT) with variable-resolution inputs and visual embedding table (VET) alignment, paired with a Qwen3 LLM decoder. Training proceeds through five phases: pre-training P1 (VET-only on image-caption data), P2 (full-parameter training with OCR/grounding/caption data and RoPE), P3 (instruction tuning with thinking-style reasoning data), followed by post-training DPO+NLL on preference pairs, and GRPO on RLVR for verifiable-reward optimization. The architecture employs data packing and hybrid parallelism (DP+TP+CP) for efficiency. Optional thinking mode enables reflection-based reasoning at inference time.

## Key Results
- 9B model achieves 78.3 on OpenCompass (new SOTA among open-source MLLMs under 40B parameters)
- 2B model reaches 73.9 on OpenCompass (SOTA for its size)
- Leads on MMMU, OCRBench v2, ChartQA Pro, MathVista, RefCOCO, and VideoMME benchmarks
- Thinking mode provides 10-20% accuracy gains on complex reasoning at 2-3× latency

## Why This Works (Mechanism)

### Mechanism 1: Native-Resolution Visual Processing with NaViT
Processing images at their native resolution preserves fine details and global layout that fixed-resolution tiling destroys. NaViT replaces the conventional fixed-resolution ViT, processing variable-resolution inputs directly. RoPE embeddings in every ViT block reinforce spatial awareness, particularly effective for high-resolution inputs (up to ~3.2M pixels in P2/P3).

### Mechanism 2: Reflection-Based Reasoning Training
Training on data that explicitly demonstrates self-checking and revision improves reasoning robustness beyond linear CoT. The model is trained on "thinking-style" data with explicit `<think...</think` tags that supervise intermediate reflection steps. This teaches self-evaluation and correction, exposed at inference via optional "thinking mode" that trades latency for accuracy.

### Mechanism 3: Probabilistic Visual Token Embedding Alignment
A learnable Visual Embedding Table (VET) structurally aligns continuous visual representations with discrete textual embeddings better than simple MLP projection. The Visual Tokenizer outputs a probability distribution over a "visual vocabulary." The VET stores embeddings for each visual word. The final visual embedding is the expected value (weighted sum) under this distribution, creating structural parity with textual embedding lookup.

## Foundational Learning

- **Concept:** Native Resolution Vision Transformers (NaViT)
  - Why needed: Understand how NaViT handles variable-resolution inputs without tiling, and why RoPE is critical for spatial awareness
  - Quick check: How does NaViT's patch packing differ from standard ViT's fixed grid? Where does RoPE get injected?

- **Concept:** Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO)
  - Why needed: Post-training stages P1 and P2 use DPO for preference alignment and GRPO for verifiable-reward reasoning optimization
  - Quick check: Why does GRPO freeze vision modules and train only the LLM? What makes a reward "verifiable"?

- **Concept:** Chain-of-Thought vs. Reflection-Based Reasoning
  - Why needed: The paper distinguishes linear CoT (step-by-step) from reflection (self-check, revise). Understanding this distinction is essential for curating thinking-style training data
  - Quick check: Can you write a CoT solution vs. a reflection-augmented solution for the same math problem? Where does the `<think` tag content differ?

## Architecture Onboarding

- **Component map:** Input Image/Video → Visual Tokenizer (ViT + Visual Head) → Probability Distribution over Visual Vocabulary → Visual Embedding Table (VET) → Weighted Visual Embedding → LLM Decoder (Qwen3) → Text Output (with optional `<think...</think` reflection)

- **Critical path:**
  1. Pre-training P1: Freeze most ViT params; train only final ViT layer + visual head + VET on image-caption pairs (448²–896² pixels, no RoPE yet)
  2. Pre-training P2: Full-parameter training; activate RoPE in all ViT blocks; expand resolution to 448²–1792²; add OCR, grounding, caption data
  3. Pre-training P3: Continue full training; add thinking-style reasoning data; include text-only, multi-image, video inputs
  4. Post-training P1: Full-parameter DPO with auxiliary NLL loss; preference pairs from candidate responses
  5. Post-training P2: GRPO on RLVR dataset; freeze vision modules; train LLM only

- **Design tradeoffs:**
  - Thinking mode ON: Higher accuracy on complex reasoning; increased latency (longer generation)
  - Thinking mode OFF: Faster inference; may fail on multi-step visual reasoning
  - NaViT resolution range: Wider range (up to 1792²) improves chart/OCR but increases memory and compute

- **Failure signatures:**
  - Tiling artifacts on charts (lines broken, text split) → NaViT not properly configured or resolution too low
  - Reasoning hallucinations without self-correction → Thinking mode disabled or insufficient reflection data during training
  - Grounding errors (wrong bounding boxes) → Grounding data underrepresented in P2/P3
  - Video temporal confusion → Video data insufficient or temporal modeling weak

- **First 3 experiments:**
  1. Resolution ablation: Evaluate chart/OCR benchmarks at 448² vs. 896² vs. 1792² to quantify NaViT's contribution; expect degradation at lower resolutions for dense visuals
  2. Thinking mode latency-accuracy tradeoff: Measure MMMU/MathVista scores and inference time with thinking mode ON vs. OFF; expect 10–20% accuracy gain at 2–3× latency
  3. VET vs. MLP projector baseline: Replace VET with a simple 2-layer MLP; compare on OpenCompass benchmarks; expect degradation, particularly on grounding and alignment-heavy tasks

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the native-resolution vision transformer architecture be adapted to scale efficiently to 4K-level image resolutions while maintaining high accuracy?
  - Basis: The conclusion explicitly lists "scaling perception to 4K-level high-resolution images while maintaining accuracy" as a remaining avenue for future work
  - Why unresolved: While Ovis2.5 supports variable resolutions (up to ~1792px total pixel count), 4K images represent a significant jump in pixel count that current multimodal data packing and hybrid parallelism infrastructures may not handle efficiently
  - What evidence would resolve it: Future technical report or experiment demonstrating stable training and inference on standard 4K datasets without running into memory bottlenecks or loss of fine detail

- **Open Question 2:** What specific mechanisms are required to enable "richer temporal reasoning" for long-input videos in this architecture?
  - Basis: The conclusion identifies "handling long-input video with richer temporal reasoning" as a key area for future exploration
  - Why unresolved: The paper notes strong performance on video benchmarks, but standard transformer contexts often struggle with long temporal dependencies
  - What evidence would resolve it: Performance evaluations on long-context video benchmarks that specifically test event correlation over extended timeframes

- **Open Question 3:** How can the Ovis architecture integrate action-augmented reasoning through tool use?
  - Basis: The paper concludes by stating "tighter integration of tool use for action-augmented reasoning" is a promising direction that remains unexplored
  - Why unresolved: The current training pipeline focuses on perception and internal reasoning, but does not mention training on tool-calling trajectories
  - What evidence would resolve it: Demonstrating agentic capabilities where the model generates executable API calls or code to solve multimodal tasks that cannot be solved by internal reasoning alone

## Limitations

- Critical implementation specifics including exact dataset compositions, precise training durations, and hyperparameter configurations are omitted from the technical report
- The probabilistic visual token embedding mechanism (VET) lacks direct empirical comparison against simpler alternatives in the literature
- The thinking-mode reflection capability has unclear generalization boundaries - the report does not characterize failure modes when reflection steps become circular or when visual context is insufficient for self-correction

## Confidence

- **High confidence:** Native-resolution visual processing with NaViT improves performance on high-density visual tasks (charts, documents)
- **Medium confidence:** Reflection-based reasoning training yields measurable accuracy gains through thinking mode
- **Low confidence:** The probabilistic visual token embedding alignment (VET) provides meaningful advantage over simpler projection methods

## Next Checks

1. **Resolution sensitivity validation:** Systematically evaluate Ovis2.5 performance on chart/OCR benchmarks across a range of input resolutions (448² to 1792²) to quantify the practical impact of native-resolution processing versus tiling artifacts

2. **Reflection mechanism robustness:** Test thinking mode on problems where reflection steps could lead to infinite loops or contradictory conclusions, measuring both accuracy gains and failure rates compared to linear CoT

3. **Embedding alignment ablation:** Replace the VET with a simple 2-layer MLP projector and measure performance degradation on grounding and alignment-heavy tasks to empirically validate the claimed benefits of probabilistic visual token embedding