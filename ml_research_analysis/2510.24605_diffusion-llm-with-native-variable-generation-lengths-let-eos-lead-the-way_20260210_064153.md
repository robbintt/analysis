---
ver: rpa2
title: 'Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the
  Way'
arxiv_id: '2510.24605'
source_url: https://arxiv.org/abs/2510.24605
tags:
- mask
- diffusion
- generation
- block
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fixed generation lengths
  in diffusion-based large language models (dLLMs), which limits their efficiency
  and practical applications. The authors propose dLLM-Var, a novel training framework
  that enables native variable-length generation by teaching the model to accurately
  predict the [EOS] token during decoding.
---

# Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way

## Quick Facts
- arXiv ID: 2510.24605
- Source URL: https://arxiv.org/abs/2510.24605
- Authors: Yicun Yang; Cong Wang; Shaobo Wang; Zichen Wen; Biqing Qi; Hanlin Xu; Linfeng Zhang
- Reference count: 24
- Primary result: dLLM-Var achieves 30.1× speedup over traditional dLLM inference and 2.4× speedup over AR models while maintaining accuracy

## Executive Summary
This paper addresses the challenge of fixed generation lengths in diffusion-based large language models (dLLMs), which limits their efficiency and practical applications. The authors propose dLLM-Var, a novel training framework that enables native variable-length generation by teaching the model to accurately predict the [EOS] token during decoding. The method introduces two key innovations: a fixed masking schedule for [EOS] tokens and multi-sample packing with full attention to enhance the model's understanding of termination signals across unrelated contexts. Experimental results show that dLLM-Var achieves a 30.1× speedup over traditional dLLM inference paradigms and a 2.4× speedup relative to autoregressive models like Qwen and Llama, while maintaining comparable or superior accuracy across six standard benchmarks. The method also enables self-correction capabilities and better KV cache reuse, making dLLMs more practical for real-world applications.

## Method Summary
dLLM-Var trains a diffusion LLM to generate variable-length sequences by always masking [EOS] tokens during training and packing multiple unrelated dialogue pairs with full attention. The training process uses a fixed masking schedule where [EOS] tokens are always masked (p(mask|x₀[i]) = 1 if x₀[i] = EOS), while other tokens follow standard t-based masking. Multi-sample packing concatenates N dialogue pairs with EOS separators, applying full attention across the packed sequence. During inference, the model uses block diffusion with KV cache reuse, appending 64-token mask blocks iteratively until EOS appears. This approach achieves native variable-length generation while maintaining the efficiency benefits of diffusion models.

## Key Results
- 30.1× speedup over traditional dLLM inference paradigms
- 2.4× speedup relative to autoregressive models (Qwen, Llama)
- Maintains comparable or superior accuracy across six benchmarks (GSM8K, GPQA, BBH, MATH, HumanEval, MBPP)
- Enables self-correction capabilities and better KV cache reuse

## Why This Works (Mechanism)

### Mechanism 1: Fixed EOS Masking Schedule
Standard dLLM training masks each token with probability t ∈ [0,1]. The modification sets p(mask|[EOS]) = 1 unconditionally. This forces the model to predict EOS as part of the denoising process rather than treating it as a fixed boundary. Without this, instruction-tuned models tend to fill trailing positions with EOS prematurely.

### Mechanism 2: Multi-Sample Packing with Full Attention
Concatenating unrelated dialogue pairs under full attention teaches the model that EOS marks semantic boundaries between distinct contexts. Sample N dialogue pairs, concatenate with EOS separators, apply full attention (no causal/block masks). Prompts have p(mask)=0; responses follow standard t-based masking; separating EOS always masked.

### Mechanism 3: Block Diffusion Inference with KV Cache Reuse
Once trained with Mechanisms 1-2, the model can generate variable-length output by iteratively appending mask blocks until EOS appears, with prefix KV cache reuse. Initialize prompt + block of MASK tokens. Denoise. If no EOS, append another mask block. KV cache for prompt and completed blocks is frozen and reused.

## Foundational Learning

- **Masked Discrete Diffusion**: Why needed here - dLLMs operate by iteratively predicting masked tokens from noisy sequences. Quick check: Given a sequence [The, sky, MASK, blue], what does the denoising model predict?

- **Key-Value Caching in Transformers**: Why needed here - The inference speedup depends on reusing computed attention K/V tensors. Understanding what gets cached and when it becomes invalid is essential for implementing block diffusion correctly. Quick check: In standard autoregressive decoding, why can KV cache be reused across timesteps?

- **EOS Token Semantics**: Why needed here - The core innovation is teaching the model when to terminate. You must understand that EOS is not just another token—it signals completion to the decoding loop. Quick check: What happens during inference if a model never produces EOS?

## Architecture Onboarding

- **Component map**: Training data (packed dialogue pairs with EOS) → Fixed EOS masking + multi-sample packing → Block diffusion inference with KV cache reuse
- **Critical path**: 1. Training data must be packed with EOS separators (Mechanism 2) 2. EOS tokens must always be masked (Mechanism 1) 3. Full attention must be applied (no block/causal masks) 4. Inference uses block-wise decoding with cache freeze (Mechanism 3)
- **Design tradeoffs**: Block size: Larger → higher parallelism, but more wasted computation if EOS appears early. Paper uses 64 as default. Confidence threshold (0.9): Higher → better quality, lower speedup. Lower → faster but noisier early termination.
- **Failure signatures**: EOS overflow: Model fills trailing positions with EOS early, no meaningful output. Non-termination: Model never generates EOS, continues indefinitely. Quality collapse: With overly aggressive cache reuse or low confidence threshold, outputs become incoherent
- **First 3 experiments**:
  1. Pure diffusion baseline: Run dLLM-Var in pure diffusion mode (fixed length 1024) to isolate training improvements from inference tricks. Compare accuracy against LLaDA-Instruct.
  2. Block diffusion speed/accuracy tradeoff: Sweep block sizes [16, 32, 64, 128] and confidence thresholds [0.7, 0.8, 0.9, 0.95] on GSM8K. Plot accuracy vs. speedup.
  3. EOS placement analysis: On a held-out set, measure distribution of EOS position relative to ground-truth response length. Check for systematic early/late termination patterns.

## Open Questions the Paper Calls Out

### Open Question 1
How can efficient algorithms be developed to determine "when" and "what" to edit during the generation process? The Conclusion states, "Future work could focus on developing efficient algorithms to decide 'when' and 'what' to edit, balancing the trade-off between quality gains and computational cost." This is unresolved because the current dLLM-Var treats previously generated blocks as fixed to optimize KV cache reuse, lacking a mechanism to dynamically identify and rectify errors in earlier tokens based on new context.

### Open Question 2
Can mechanisms be designed to allow iterative refinement of previously generated blocks without sacrificing the inference speedups gained from KV caching? The Conclusion notes, "A promising direction is to explore mechanisms for iterative refinement and editing of generated content... [allowing] it could re-evaluate and selectively re-generate portions of earlier output." This is unresolved because while the model has full attention capabilities, the current inference paradigm does not utilize it for backward-looking corrections due to the computational cost of invalidating the cache.

### Open Question 3
What is the relationship between block size hyperparameters and performance on tasks with distinct output length distributions? Section 3.3 sets a "default block size of 64 tokens" and a confidence threshold of 0.9 to balance speed and quality, but provides no ablation study on how these specific values affect tasks like code generation versus simple QA. It is unclear if a static block size is optimal or if variable block sizes could further enhance the 30.1× speedup reported on standard benchmarks.

## Limitations

- **Dataset Dependency**: The method relies on multi-sample packing with N dialogue pairs, which may not generalize to single-turn or non-dialogue datasets.
- **Computational Overhead**: Training requires packing multiple samples per sequence, increasing memory usage and potentially limiting scalability to larger batch sizes or longer sequences.
- **Inference Quality Trade-offs**: The confidence threshold (0.9) and block size (64) are tuned for the specific benchmarks. Lower thresholds or larger blocks could lead to quality degradation or non-termination.

## Confidence

- **High Confidence**: The fixed EOS masking schedule and multi-sample packing mechanism are well-defined and produce consistent experimental results across multiple benchmarks.
- **Medium Confidence**: The speedup claims (30.1× vs baseline dLLM, 2.4× vs AR models) are based on controlled experiments but may not translate directly to all real-world deployment scenarios.
- **Low Confidence**: The claim that dLLM-Var "maintains comparable or superior accuracy" across all benchmarks is supported by the presented results, but the paper doesn't report variance or statistical significance tests.

## Next Checks

1. **Ablation Study on Multi-Sample Packing**: Test dLLM-Var with single-sample training (no packing) to isolate the contribution of multi-sample packing to EOS understanding. Compare accuracy and termination behavior against the full method.

2. **Robustness to Confidence Thresholds**: Systematically sweep confidence thresholds [0.7, 0.8, 0.9, 0.95] and block sizes [16, 32, 64, 128] on GSM8K and HumanEval. Plot accuracy vs. speedup curves to identify the Pareto frontier and failure points.

3. **Cross-Dataset Generalization**: Apply dLLM-Var to non-dialogue datasets (e.g., single-turn instructions, code generation without dialogue context) to test whether the multi-sample packing requirement is essential or if the method generalizes to other data modalities.