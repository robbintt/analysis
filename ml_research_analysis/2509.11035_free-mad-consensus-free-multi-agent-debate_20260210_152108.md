---
ver: rpa2
title: 'Free-MAD: Consensus-Free Multi-Agent Debate'
arxiv_id: '2509.11035'
source_url: https://arxiv.org/abs/2509.11035
tags:
- reasoning
- agents
- debate
- answer
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FREE-MAD, a consensus-free multi-agent debate
  framework that eliminates the need for multi-round consensus and majority voting.
  FREE-MAD introduces a score-based decision mechanism that evaluates the entire debate
  trajectory, tracking how each agent's reasoning evolves across rounds, and incorporates
  anti-conformity to mitigate the negative effects of LLM conformity.
---

# Free-MAD: Consensus-Free Multi-Agent Debate

## Quick Facts
- arXiv ID: 2509.11035
- Source URL: https://arxiv.org/abs/2509.11035
- Authors: Yu Cui; Hang Fu; Haibin Zhang; Licheng Wang; Cong Zuo
- Reference count: 40
- Improves reasoning accuracy by 13-16% compared to existing MAD approaches while reducing token consumption by ~50% through single-round operation

## Executive Summary
This paper proposes FREE-MAD, a consensus-free multi-agent debate framework that eliminates the need for multi-round consensus and majority voting. FREE-MAD introduces a score-based decision mechanism that evaluates the entire debate trajectory, tracking how each agent's reasoning evolves across rounds, and incorporates anti-conformity to mitigate the negative effects of LLM conformity. Extensive experiments on eight benchmark datasets demonstrate that FREE-MAD significantly improves reasoning accuracy by 13-16% compared to existing MAD approaches while requiring only a single-round debate, thereby reducing token consumption and improving scalability.

## Method Summary
FREE-MAD replaces traditional MAD consensus mechanisms with a score-based trajectory evaluation system. Agents debate in a single round (R=1) without seeking agreement, and their reasoning evolution is tracked through a dynamic score dictionary that rewards answer adoption and penalizes abandonment. The framework uses anti-conformity prompting to reduce LLM conformity bias, requiring agents to identify specific reasoning errors rather than seek majority agreement. Decision-making is performed by selecting the answer with the highest cumulative score across all debate rounds, with weights adjusted to emphasize meaningful opinion changes over conformity-driven shifts.

## Key Results
- 13-16% accuracy improvement over existing MAD approaches across eight benchmark datasets
- Single-round operation reduces token consumption by ~50% compared to multi-round consensus methods
- Maintains high accuracy (65%) even under 50% communication attacks, outperforming baselines that drop to 30%
- FREE-MAD-N (anti-conformity) outperforms FREE-MAD-C (conformity mode) on reasoning-heavy tasks like GSM-Level4 and MATH500

## Why This Works (Mechanism)

### Mechanism 1: Score-Based Trajectory Evaluation
Evaluating the entire debate trajectory (all rounds × all agents) improves accuracy over majority voting on only the final round. Maintains a score dictionary that updates based on opinion shifts—agents abandoning answers lose points, newly adopted answers gain points, with weights adjusted by round number to downweight later conformity-driven changes. Core assumption: Opinion changes indicate reasoning improvement rather than conformity pressure; agents shift toward correct answers more often than away from them.

### Mechanism 2: Anti-Conformity Prompting via Critical Reasoning
Structured prompts requiring agents to identify specific reasoning errors in peers (rather than seek agreement) reduces error propagation from incorrect majority opinions. Replaces consensus-oriented prompts with CoT-based adversarial framing—agents must enumerate errors, compare to their own reasoning, and justify any belief revision without using majority opinion as evidence. Core assumption: LLMs can reliably identify reasoning errors when explicitly prompted to do so; critical analysis mode doesn't introduce new systematic biases.

### Mechanism 3: Single-Round Consensus Elimination
Removing consensus requirements and operating with R=1 debate round maintains accuracy while reducing token costs by ~50%. Score-based decision operates on initial + 1 debate round (R+1 responses per agent) without needing convergence; eliminates multi-round iteration loop. Core assumption: Useful information emerges within one exchange; the marginal value of additional rounds is negative due to conformity accumulation.

## Foundational Learning

- **LLM Conformity Bias**: LLMs tend to adopt majority opinions even when incorrect; understanding this is essential to grasp why anti-conformity mechanisms matter. Quick check: Given three agents with answers [A, B, B] where A is correct, would a conformity-biased agent facing this context more likely output A or B?

- **Probabilistic Response Generation**: The formal model treats LLM responses as samples from conditional distributions P(r|C,p); score-based decisions aggregate across this uncertainty. Quick check: If agent ai produces rk_i by sampling from Pai(r|C(k−1),p), what does running the same query multiple times enable?

- **Opinion Dynamics & Belief Revision**: The scoring mechanism interprets opinion changes as signals; understanding when belief revision is rational vs. conformity-driven is critical for weight tuning. Quick check: An agent shifts from answer X to Y after seeing three peers output Y. What additional information would distinguish conformity-driven vs. evidence-driven revision?

## Architecture Onboarding

- **Component map**: Debate Stage -> Answer Matrix A -> Score Dictionary S -> Decision Module -> Attack Handler
- **Critical path**: 1. Initialize S←∅, A←empty matrix 2. For each agent: generate r0_i, update S with w1×f weight 3. For round k=1 to R: each agent receives context C, generates rk_i, update S based on whether rk_i ≠ rk-1_i 4. Select rfinal = argmax(S); tie-break via random selection
- **Design tradeoffs**: Anti-conformity (FREE-MAD-N) vs. Conformity (FREE-MAD-C): Anti-conformity better for reasoning-heavy tasks where models have capability; conformity better for knowledge-sparse tasks where models lack domain expertise. Weight tuning: Paper uses W={20,25,30,20} based on "theoretical analysis of debate logic"—higher w3 (answer adoption) rewards opinion shifts; empirical tuning likely improves performance. Round count: R=1 minimizes tokens but may cap accuracy on tasks requiring iterative refinement; R=2+ adds robustness at 2-3× token cost
- **Failure signatures**: Score ties with |MS|>1: Random selection reduces to chance—indicates either insufficient agent diversity or weight misconfiguration. Accuracy drops vs. initial responses: Anti-conformity causing excessive opinion flipping without improvement—switch to FREE-MAD-C mode. No opinion changes across rounds: Agents ignoring context—check prompt injection or context construction bugs
- **First 3 experiments**: 1. Ablation on GSM-Level4: Compare FREE-MAD-N, FREE-MAD-C, Baseline 1 (anti-conformity + majority voting), Baseline 2 (conformity + majority voting) with R=1 to isolate score mechanism vs. debate mode contributions 2. Weight sensitivity sweep: On StrategyQA, vary w2 (abandonment penalty) and w3 (adoption bonus) ±50% from defaults to identify optimal balance for logical reasoning tasks 3. Attack simulation: Configure 50% agents as compromised (V with |V|/N=0.5), measure accuracy degradation on AICrypto to validate robustness claims and identify failure modes under Byzantine conditions

## Open Questions the Paper Calls Out

1. **Weight Configuration Optimization**: The authors plan to investigate optimal configurations for the weighting coefficients (W) in the score-based decision mechanism, as current experiments used a single static configuration derived from theoretical analysis.

2. **Robustness Against Prompt Injection Attacks**: The paper calls for evaluating FREE-MAD's robustness against active prompt injection attacks, as current security analysis only covers passive communication attacks where compromised agents drop messages.

3. **Generalization to Heterogeneous Systems**: The authors propose validating the framework on the HLE benchmark using heterogeneous systems with specialized reasoning models like DeepSeek-R1 to test whether anti-conformity generalizes beyond Qwen models.

## Limitations

- Prompt template completeness: Conformity baseline prompt and SoM baseline prompt are not provided, creating ambiguity in isolating the specific contribution of the score-based mechanism
- Weight selection rationale: The weight vector W={20,25,30,20} is stated to derive from "theoretical analysis of debate logic" but no derivation or sensitivity analysis is provided
- Generalization across model families: Experiments use primarily Qwen and DeepSeek models; effectiveness with other model families (GPT, Claude) remains untested

## Confidence

**High Confidence**: The score-based trajectory evaluation mechanism is well-specified and reproducible. The core algorithm (Algorithm 1) and weight application are clearly defined, making implementation straightforward.

**Medium Confidence**: The anti-conformity prompting approach is well-described, but its effectiveness depends on model-specific behaviors around critical reasoning that aren't fully characterized across different model families.

**Medium Confidence**: The 13-16% accuracy improvement claims are supported by extensive benchmarking across eight datasets, but the exact baseline implementations (particularly for SoM) are incompletely specified, making precise replication challenging.

## Next Checks

1. **Weight Sensitivity Analysis**: Systematically vary w2 (abandonment penalty) and w3 (adoption bonus) ±50% from defaults on StrategyQA to identify optimal balance for logical reasoning tasks and validate that the default weights aren't overfit to specific datasets.

2. **Cross-Model Validation**: Implement FREE-MAD with GPT-4 and Claude models on GSM8K to test whether anti-conformity effectiveness transfers across model families, particularly examining whether models from different providers exhibit similar conformity biases.

3. **Multi-Round Accuracy-Token Tradeoff**: Compare FREE-MAD-N at R=1, R=2, and R=3 on MATH500, measuring both accuracy and token consumption to characterize the diminishing returns curve and identify the optimal R for different task complexities.