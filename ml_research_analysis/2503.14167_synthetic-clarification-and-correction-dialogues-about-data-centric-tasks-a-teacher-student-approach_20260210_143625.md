---
ver: rpa2
title: Synthetic Clarification and Correction Dialogues about Data-Centric Tasks --
  A Teacher-Student Approach
arxiv_id: '2503.14167'
source_url: https://arxiv.org/abs/2503.14167
tags:
- clarification
- student
- table
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a student-teacher framework to generate synthetic
  dialogues for table-based question answering, modeling clarification and correction
  scenarios. The approach uses a strong teacher LLM to ablate information from questions
  or tables, ensuring tasks become unsolvable without specific details, and then guides
  a student model to ask clarifying questions or integrate corrections.
---

# Synthetic Clarification and Correction Dialogues about Data-Centric Tasks -- A Teacher-Student Approach

## Quick Facts
- arXiv ID: 2503.14167
- Source URL: https://arxiv.org/abs/2503.14167
- Reference count: 13
- Key outcome: Introduces a student-teacher framework to generate synthetic dialogues for table-based question answering, modeling clarification and correction scenarios.

## Executive Summary
This work introduces a student-teacher framework to generate synthetic dialogues for table-based question answering, modeling clarification and correction scenarios. The approach uses a strong teacher LLM to ablate information from questions or tables, ensuring tasks become unsolvable without specific details, and then guides a student model to ask clarifying questions or integrate corrections. The synthetic data forms a curriculum for benchmarking and training, validated for quality through teacher verification. Experiments on TAT-QA and WikiTableQuestions show that even large models like GPT4-Turbo struggle with clarification, while smaller models over-rely on asking questions. Fine-tuning on synthetic data improves student performance, especially recall and final answer accuracy. The method enables controlled generation of realistic dialogues without manual annotation.

## Method Summary
The method uses a strong teacher LLM to ablate information from questions or tables, ensuring tasks become unsolvable without specific details, and then guides a student model to ask clarifying questions or integrate corrections. The synthetic data forms a curriculum for benchmarking and training, validated for quality through teacher verification. For fine-tuning: LoRA with rank/Î±=16, 4 epochs, learning rate 1e-4.

## Key Results
- Even large models like GPT4-Turbo struggle with clarification tasks.
- Smaller models over-rely on asking questions, showing high recall but low precision.
- Fine-tuning on synthetic data improves student performance, especially recall and final answer accuracy.
- The teacher-verified synthetic curriculum provides a clean dataset where a successful outcome is always conditional on a specific clarification or correction.

## Why This Works (Mechanism)

### Mechanism 1: Controlled Ablation Creates Recoverable Failure
- **Claim:** Removing critical information from a solved task creates a grounded, recoverable failure case for dialogue training.
- **Mechanism:** The teacher LLM starts with a solved table QA task. It then removes a specific piece of information (from the question or table) and verifies that the student model can no longer answer correctly. This guarantees any downstream dialogue is anchored in a known solvable problem, where the teacher possesses the exact missing information required for recovery.
- **Core Assumption:** The teacher LLM can correctly identify all necessary information for a solution and can generate a task variant that is broken in a specific, recoverable way.
- **Evidence Anchors:**
  - [abstract] "...a strong teacher LLM to ablate information from questions or tables, ensuring tasks become unsolvable without specific details..."
  - [section 3.1] "If the removal of the necessary information makes the student model fail... we add the new task... to a candidate set C... otherwise we disregard it."
  - [corpus] No direct corpus support for this specific ablation mechanism.
- **Break Condition:** The teacher fails to identify a truly necessary piece of information, or the ablation doesn't break the solution, leading to unverified or trivial dialogue data.

### Mechanism 2: Solvability-Verified Curriculum
- **Claim:** A curriculum built on verified recoverability provides a highly effective training signal for models to learn when and how to ask for help.
- **Mechanism:** The synthetic curriculum consists of tasks where a student model's failure is guaranteed by an ablation, and its recovery is validated by a teacher. This filters out ambiguous or unsolvable cases, providing a clean dataset where a successful outcome is always conditional on a specific clarification or correction.
- **Core Assumption:** Models can generalize from this controlled, teacher-verified recovery path to more ambiguous real-world interactions.
- **Evidence Anchors:**
  - [section 3.1] "Our synthetic curriculum is designed with solvability as a core requirement. In every dialogue, we ensure that (i) the student model is capable, in principle, of asking the right clarification question..."
  - [section 6] "...finetuning on the synthetic curriculum significantly improves student models' ability to generate clarification questions and correctly incorporate user-initiated corrections."
  - [corpus] Corpus neighbors support the broader utility of clarification in multi-turn conversations (e.g., MAC, ClarifyMT-Bench), but not this specific curriculum mechanism.
- **Break Condition:** The student model fails to generalize, learning only to respond to the specific ablation patterns in the synthetic data and not to novel, real-world ambiguities.

### Mechanism 3: Fine-Tuning Shifts Policy on Clarification
- **Claim:** Fine-tuning on the synthetic dialogue curriculum directly improves a model's policy for initiating clarifications and integrating corrections.
- **Mechanism:** Training on the curated data optimizes the student model to overcome its default behaviors. The paper shows this can correct smaller models' tendency to over-ask (high recall, low precision) and larger models' reluctance to ask (low recall, high precision).
- **Core Assumption:** The synthetic dialogues are of sufficient quality and realism for the model to learn a robust, generalizable policy, rather than overfitting to the teacher's generation style.
- **Evidence Anchors:**
  - [abstract] "Fine-tuning on synthetic data improves student performance, especially recall and final answer accuracy."
  - [table 5] Shows Llama3.1 8b recall increasing from 0.01 to 0.89 on TaT-QA after fine-tuning.
  - [corpus] Corpus papers (e.g., AskToAct) show related improvement in LLM tool use via self-correcting clarification, supporting the value of targeted training.
- **Break Condition:** The synthetic dialogues contain systematic biases or unrealistic phrasings, causing the fine-tuned model to perform well on the benchmark but fail in real user interactions.

## Foundational Learning

- **Concept: Teacher-Student Framework**
  - **Why needed here:** This is the core paradigm. Understanding that a stronger "teacher" model is used to guide, evaluate, and generate training data for a weaker "student" model is essential.
  - **Quick check question:** What is the primary role of the "teacher" model in this framework: to provide final answers, or to verify a recoverable solution path?

- **Concept: Data Ablation**
  - **Why needed here:** The paper's central method is not model ablation (removing layers/neurons) but data ablation (removing information from the input). Grasping this distinction is key to understanding how the dataset is constructed.
  - **Quick check question:** When the paper mentions "ablation," is it referring to removing parts of the model's architecture or removing specific pieces of information from the table or question?

- **Concept: Clarification vs. Correction Dialogue Acts**
  - **Why needed here:** The system is trained for two distinct scenarios. A user must understand that *Clarification* is AI-initiated (the model asks for missing info) while *Correction* is user-initiated (the model is told what was wrong).
  - **Quick check question:** In which scenario does the AI assistant proactively ask a question, and in which does it wait for user feedback?

## Architecture Onboarding

- **Component map:**
  - Source Dataset -> Teacher Model -> Student Model -> Curriculum Generator

- **Critical path:**
  1.  **Seed:** Teacher selects a solvable task (T) from the source dataset.
  2.  **Ablate:** Teacher creates an unsolvable variant (T-I) by removing critical info.
  3.  **Verify Failure:** Teacher confirms student fails on (T-I). If not, discard.
  4.  **Generate Recovery:** Teacher simulates user response (r) containing the missing info.
  5.  **Verify Success:** Teacher confirms student can solve T given (T-I) and (r). If successful, add to curriculum.

- **Design tradeoffs:**
  - **Teacher Strength vs. Cost:** A more capable teacher (e.g., GPT-4) creates higher-quality, more reliable data but at greater expense.
  - **Ablation Target:** Ablating from the table vs. the question creates different challenges (missing data vs. ambiguous query).
  - **Solvability Filter:** Strictly filtering for recoverability (AccCl/AccCo > 0) ensures quality but reduces the final dataset size.

- **Failure signatures:**
  - **Low Curriculum Yield:** The student model is too weak to recover even with teacher guidance, or the teacher is failing to create valid ablations.
  - **Over-asking (High Recall, Low Precision):** The model asks for clarification constantly. This is a known failure mode for smaller models before fine-tuning.
  - **Under-asking (Low Recall, High Precision):** The model is overly confident and rarely asks for help. This is a known failure mode for larger models before fine-tuning.

- **First 3 experiments:**
  1.  **Baseline Evaluation:** Establish the student model's accuracy (AccOr) on the source table QA datasets without any dialogue.
  2.  **Curriculum Generation:** Using a fixed teacher, generate a curriculum and measure the pass rate for clarification (AccCl) and correction (AccCo) tasks. This validates the data generation pipeline.
  3.  **Fine-Tuning Impact:** Train the student model on the generated curriculum and evaluate its precision/recall on clarification initiation, comparing it against the non-fine-tuned baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a synthetic curriculum generated for one specific student model architecture be effectively transferred to train a different student model without requiring regeneration of the dataset?
- Basis in paper: [explicit] The Limitations section states, "The curricula are dependent on the characteristics of a specific student model, meaning it may overlook failure cases that are common across other models."
- Why unresolved: While Appendix B tests smaller models on larger models' curricula, the results show poor zero-shot accuracy (e.g., 19-28%), leaving the general transferability of these specific datasets uncertain.
- What evidence would resolve it: Cross-model training experiments showing that a curriculum generated for Model A significantly improves Model B's performance compared to Model B's own generated curriculum.

### Open Question 2
- Question: Do the synthetic table ablation strategies (removing columns/values) accurately reflect the distribution of missing information in real-world human-AI interactions?
- Basis in paper: [inferred] While Section 4.1 validates question rephrasing against the human-annotated Pacific dataset, the validation for table ablations relies only on the teacher's internal code execution logic, not human error data.
- Why unresolved: There is no comparison of the synthetic table ablations against a dataset of actual user errors or missing table contexts found in natural settings.
- What evidence would resolve it: A human annotation study comparing synthetic table ablations to actual table deficiencies found in real-world query logs to measure distributional alignment.

### Open Question 3
- Question: To what extent does the performance ceiling and potential hallucinations of the teacher model limit the quality or correctness of the synthetic curriculum?
- Basis in paper: [explicit] The Limitations section acknowledges that "the teacher model may itself exhibit issues such as hallucinations, biases, or lower performance than human evaluators."
- Why unresolved: The framework relies on the teacher to identify necessary information for ablation; if the teacher hallucinates a dependency, the student is forced to learn a spurious clarification pathway.
- What evidence would resolve it: An error analysis of curriculum samples where the teacher model incorrectly identifies "necessary" information, resulting in invalid clarification dialogues.

## Limitations
- The core limitation is the heavy dependence on the teacher LLM's judgment for both task ablation and solution verification. If the teacher misidentifies critical information or incorrectly evaluates student responses, the entire synthetic curriculum could be compromised.
- The paper does not address potential distribution shift between synthetic and real user interactions, nor does it validate the model's performance on truly open-ended user queries.
- The ablation strategy may also create unrealistic scenarios that don't reflect common real-world ambiguities, potentially limiting practical applicability.

## Confidence

- **High Confidence:** The methodology for generating synthetic dialogues via teacher-guided ablation and verification is well-defined and internally consistent. The fine-tuning results showing improved precision/recall metrics are supported by quantitative evidence.
- **Medium Confidence:** The claim that models learn generalizable clarification/correction policies from synthetic data assumes the teacher-generated dialogues capture realistic interaction patterns. This assumption is plausible but not directly tested with real users.
- **Low Confidence:** The assertion that GPT4-Turbo "struggles" with clarification is based on comparison to human-written benchmarks, but the absolute performance levels and practical implications are not fully explored.

## Next Checks

1. **Human Evaluation of Synthetic Dialogues:** Recruit human annotators to assess the realism, clarity, and appropriateness of teacher-generated clarification and correction dialogues, comparing them to human-written interactions.
2. **Zero-Shot Transfer to Real User Queries:** Test the fine-tuned student models on a held-out set of real user questions from a different domain or dataset, measuring both dialogue quality and final answer accuracy without further adaptation.
3. **Ablation Study on Teacher Verification:** Run experiments where the verification step is relaxed (e.g., allowing AccCl/AccCo > 0), then measure the impact on downstream student performance and dialogue quality to quantify the value of strict solvability filtering.