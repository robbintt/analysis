---
ver: rpa2
title: 'StableMorph: High-Quality Face Morph Generation with Stable Diffusion'
arxiv_id: '2511.08090'
source_url: https://arxiv.org/abs/2511.08090
tags:
- images
- face
- image
- quality
- morphed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StableMorph is a novel face morphing generation method that produces
  high-quality, artifact-free morphed images using latent diffusion models. Unlike
  prior approaches that often generate blurry or artifact-laden images, StableMorph
  produces full-head images with sharp details and no visible morphing artifacts.
---

# StableMorph: High-Quality Face Morph Generation with Stable Diffusion

## Quick Facts
- arXiv ID: 2511.08090
- Source URL: https://arxiv.org/abs/2511.08090
- Reference count: 40
- StableMorph generates artifact-free, full-head face morphs using latent diffusion models that match or exceed genuine face image quality

## Executive Summary
StableMorph is a novel face morphing generation method that produces high-quality, artifact-free morphed images using latent diffusion models. Unlike prior approaches that often generate blurry or artifact-laden images, StableMorph produces full-head images with sharp details and no visible morphing artifacts. The method uses LoRA fine-tuning and identity embeddings to merge two subjects' faces, then generates realistic morphs conditioned on these identities.

## Method Summary
StableMorph builds on Stable Diffusion v1.5, fine-tuning separate LoRA weights for each subject using a single image per subject. Face embeddings are extracted from 10 images per subject using ArcFace, then merged via SLERP interpolation. The method combines these through IP-Adapter conditioning to generate morphs in the latent space, producing 512x512 full-head images. The process involves merging LoRA weights linearly, merging embeddings via SLERP, and using IP-Adapter to guide the diffusion process with the merged identity information.

## Key Results
- StableMorph images achieve superior face image quality (Sharpness 96.50±8.85 on FRLL) compared to landmark-based, GAN-based, and diffusion autoencoder methods
- General perceptual quality (MANIQA, PaQ-2-PiQ) matches or exceeds genuine face images
- Maintains strong morphing attack potential against face recognition systems while producing visually superior images
- Poses greater challenges to existing morphing attack detection solutions than previous methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LoRA fine-tuning enables efficient subject-specific visual learning without full model retraining.
- **Mechanism:** Low-Rank Adaptation freezes pretrained Stable Diffusion weights and injects trainable rank-decomposition matrices into the UNet cross-attention layers. For each subject, separate LoRA weights are learned from as few as 1 image, then merged via weighted averaging.
- **Core assumption:** The visual characteristics needed for morphing can be captured in low-rank perturbations to attention weights rather than requiring full-parameter updates.
- **Evidence anchors:** [section 3.1] describes the LoRA technique; [section 4.1.3] notes fine-tuning takes approximately 33 minutes per subject; related papers focus on detection, not LoRA-based morphing.
- **Break condition:** If subjects have dramatically different facial structures or the LoRA rank is too low, merged weights may produce identity drift or inconsistent facial features.

### Mechanism 2
- **Claim:** SLERP-merged identity embeddings provide geometrically meaningful identity interpolation in the face recognition embedding space.
- **Mechanism:** Face embeddings are extracted from 10 images per subject using ArcFace, then merged via spherical linear interpolation. This preserves unit-length constraints in the angular embedding space better than linear averaging.
- **Core assumption:** The ArcFace embedding space is sufficiently disentangled that midpoint interpolation yields a plausible "average identity" rather than an off-manifold result.
- **Evidence anchors:** [section 3.2] describes embedding extraction; [section 4.4 ablation] shows configuration without identity information "generated images almost completely ignore the subjects"; no direct corroboration of SLERP for morphing specifically.
- **Break condition:** If embedding extractors are biased, morphed identities may systematically favor one subject's features.

### Mechanism 3
- **Claim:** IP-Adapter conditions the diffusion denoising process on merged identity embeddings via decoupled cross-attention.
- **Mechanism:** IP-Adapter injects image features through a separate cross-attention branch, allowing merged embeddings to guide generation without requiring a text prompt. The merged LoRA weights modify the base UNet, while IP-Adapter steers identity alignment.
- **Core assumption:** Decoupled cross-attention can effectively combine subject appearance with identity structure without interference.
- **Evidence anchors:** [section 3.4] describes using IP-Adapter with only identity information; [figure 2] shows StableMorph outputs without ghost artifacts; no direct validation of IP-Adapter for morphing.
- **Break condition:** If the diffusion scheduler uses too few steps or guidance scale is misconfigured, identity conditioning may be weak, producing generic faces.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - Why needed here: StableMorph builds on Stable Diffusion, which performs denoising in a compressed 4× smaller latent space rather than pixel space, making LoRA fine-tuning tractable.
  - Quick check question: Can you explain why LDMs reduce computational cost compared to pixel-space diffusion, and what the VAE's role is in encoding/decoding?

- **Concept: Cross-Attention Conditioning in UNets**
  - Why needed here: Both the base SD model and IP-Adapter use cross-attention to inject conditional information (text or image features) into the denoising network.
  - Quick check question: In a UNet denoiser, how does cross-attention differ from self-attention, and where are the attention modules typically placed?

- **Concept: Face Recognition Embedding Spaces**
  - Why needed here: The quality of morphed identities depends on how ArcFace encodes identity—angular margin losses produce normalized embeddings where cosine distance correlates with identity similarity.
  - Quick check question: Why is spherical interpolation preferred over linear interpolation for angular embedding spaces like ArcFace?

## Architecture Onboarding

- **Component map:** Input Images (S1, S2) → [LoRA Fine-tuning × 2] → W1, W2 (merged: Wm) → [Face Encoder (ArcFace)] → E1, E2 (merged via SLERP: Em) → [Stable Diffusion UNet + Wm + IP-Adapter conditioned on Em] → Diffusion Reverse Process → Morphed Image M

- **Critical path:** LoRA weight merging → embedding SLERP → IP-Adapter conditioning → denoising loop. Errors in any stage propagate to final identity fidelity.

- **Design tradeoffs:**
  - Single-image vs. multi-image LoRA fine-tuning: Single image is faster but may require more generation attempts to avoid noise artifacts
  - 10 images for identity extraction vs. fewer: More images improve identity robustness but require more data availability
  - Fixed λ=0.5 for both LoRA and embedding merging: Simple but prevents asymmetric morphing

- **Failure signatures:**
  - "Double iris" or blurry nostrils → landmark-based artifact, should not occur in StableMorph
  - Noise on face / added accessories / wrong gender → diffusion randomness + insufficiently specific text prompt
  - Identity drift toward generic face → missing or weak identity conditioning

- **First 3 experiments:**
  1. **Reproduce quantitative quality metrics:** Run StableMorph on FRLL pairs, compute Sharpness, UQS, and MANIQA scores; verify they fall within reported ranges
  2. **Ablate identity image count:** Test with 3, 5, and 10 images for embedding extraction on a held-out subject pair; measure identity preservation using a separate FRS
  3. **Analyze defect rate:** Generate 100 morphs from random FRGC pairs; manually annotate defect types and correlate with number of denoising steps and guidance scale

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do state-of-the-art Morphing Attack Detection (MAD) algorithms specifically perform against StableMorph images compared to GAN-based and landmark-based attacks?
- **Basis in paper:** The authors claim StableMorph "poses a greater challenge to existing MAD solutions" and "sets a new standard," but the evaluation focuses on image quality metrics rather than benchmarking detection error rates against specific MAD algorithms.
- **Why unresolved:** While the paper demonstrates that images are high quality and can fool FRS (verification), it does not provide quantitative data on how difficult these images are for *detection* systems to classify as morphs.
- **What evidence would resolve it:** Benchmarks showing Equal Error Rates (EER) or Detection Error Tradeoff (DET) curves for standard MAD algorithms when applied to the StableMorph dataset.

### Open Question 2
- **Question:** Can the occurrence of "undesired defects" (e.g., hallucinated accessories, wrong gender, or background artifacts) be minimized algorithmically without relying on manual inspection or prompt tuning?
- **Basis in paper:** The Discussion section notes that StableMorph occasionally produces defects like "noise on some parts of the face, added accessories, confusing genders," attributing this to the randomness of the diffusion process and the use of a generic prompt.
- **Why unresolved:** The paper suggests manual selection or prompt engineering as solutions, but does not propose a method to constrain the latent space or generation process to inherently prevent these semantic errors during automated batch processing.
- **What evidence would resolve it:** A modified inference method that statistically reduces the rate of semantic artifacts while maintaining the high visual quality and MAP scores reported.

### Open Question 3
- **Question:** To what extent does the method generalize to newer latent diffusion architectures (e.g., SDXL) or higher resolutions required for regulatory compliance?
- **Basis in paper:** The implementation relies on Stable Diffusion v1.5, limiting outputs to 512x512 pixels, whereas the Introduction emphasizes the need for "full head" images meeting ICAO requirements, which often demand higher resolutions.
- **Why unresolved:** The paper does not validate if the LoRA merging and SLERP identity embedding strategy transfers effectively to larger, more complex model architectures or higher resolution outputs without introducing new artifacts.
- **What evidence would resolve it:** Evaluation results showing that the proposed LoRA merging technique functions correctly on SDXL or similar high-resolution models, maintaining identity fidelity at resolutions exceeding 1024x1024.

## Limitations

- Claims about LoRA effectiveness and IP-Adapter conditioning are built on general adapter literature rather than direct validation specific to this application
- IP-Adapter integration for identity conditioning lacks detailed implementation specifications, making exact reproduction challenging
- Single-image LoRA fine-tuning could lead to identity instability if the subject image contains noise or non-frontal poses

## Confidence

- **High confidence:** Face image quality metrics (Sharpness, UQS, SDD-FIQA) and general perceptual quality (MANIQA, PaQ-2-PiQ) comparisons with prior methods
- **Medium confidence:** Morphing attack potential results against four FRSs, given the standard evaluation protocol
- **Medium confidence:** Claims about artifact-free generation compared to landmark-based methods, supported by qualitative Figure 7 comparison
- **Low confidence:** Claims about IP-Adapter effectiveness for identity conditioning without detailed architectural validation

## Next Checks

1. **Reproduce quantitative quality metrics** on FRLL pairs and verify reported ranges for Sharpness (96.50±8.85), UQS, and MANIQA scores
2. **Ablate identity image count** by testing 3, 5, and 10 images for embedding extraction to measure identity preservation using an independent FRS
3. **Analyze defect rate** by generating 100 morphs from random FRGC pairs and correlating defect types with denoising parameters to establish optimal generation settings