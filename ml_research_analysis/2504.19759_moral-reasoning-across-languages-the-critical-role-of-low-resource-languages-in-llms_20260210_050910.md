---
ver: rpa2
title: 'Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages
  in LLMs'
arxiv_id: '2504.19759'
source_url: https://arxiv.org/abs/2504.19759
tags:
- moral
- reasoning
- languages
- llms
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Multilingual Moral Reasoning Benchmark
  (MMRB) to assess large language models' moral reasoning across five languages (English,
  Chinese, Russian, Vietnamese, Indonesian) and three context levels (sentence, paragraph,
  document). The researchers evaluated five prominent LLMs and found significant inconsistencies
  in moral reasoning performance across languages, with degradation as context complexity
  increases.
---

# Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs

## Quick Facts
- arXiv ID: 2504.19759
- Source URL: https://arxiv.org/abs/2504.19759
- Reference count: 7
- Primary result: Low-resource languages significantly impact multilingual moral reasoning performance, with Vietnamese showing stronger effects than high-resource languages in LLM evaluations.

## Executive Summary
This study introduces the Multilingual Moral Reasoning Benchmark (MMRB) to evaluate how large language models perform moral reasoning across five languages and three context levels. The researchers discovered significant performance inconsistencies across languages, with degradation increasing as context complexity grows. Notably, low-resource languages like Vietnamese had a more pronounced impact on multilingual reasoning performance than high-resource ones. Fine-tuning experiments with LLaMA-3-8B revealed that low-resource languages can both enhance and impair cross-lingual generalization, highlighting the critical role of data quality and representation for underrepresented languages in multilingual NLP.

## Method Summary
The researchers developed the Multilingual Moral Reasoning Benchmark (MMRB) to assess moral reasoning across five languages (English, Chinese, Russian, Vietnamese, Indonesian) at three context levels: sentence, paragraph, and document. They evaluated five prominent LLMs across these languages and contexts, measuring performance variations. To investigate the impact of low-resource languages, they fine-tuned the LLaMA-3-8B model on monolingual data, observing both enhancement and poisoning effects on cross-lingual generalization. The study systematically examined how language resource levels affect moral reasoning capabilities in multilingual settings.

## Key Results
- Performance degradation occurs as context complexity increases across all languages
- Low-resource languages (particularly Vietnamese) have stronger impact on multilingual reasoning than high-resource languages
- Fine-tuning LLaMA-3-8B on low-resource language data can both enhance and poison cross-lingual moral reasoning capabilities
- Significant inconsistencies in moral reasoning performance across the five evaluated languages

## Why This Works (Mechanism)
The observed effects stem from the fundamental challenge of cross-lingual transfer in multilingual models. When models are trained primarily on high-resource languages, they develop strong representations for those languages but lack the linguistic and cultural grounding needed for low-resource languages. The enhancement effect occurs when low-resource language data provides complementary moral reasoning patterns that improve generalization. Conversely, the poisoning effect arises when low-quality or culturally divergent data introduces conflicting reasoning patterns that disrupt the model's established moral frameworks.

## Foundational Learning
- **Cross-lingual transfer learning** - Why needed: Enables knowledge sharing across languages; Quick check: Evaluate model performance on zero-shot tasks across language pairs
- **Data quality assessment** - Why needed: Determines whether low-resource data enhances or poisons model performance; Quick check: Compare model performance using curated vs. raw low-resource data
- **Multilingual representation learning** - Why needed: Underlies the model's ability to reason consistently across languages; Quick check: Analyze embedding similarities across languages for moral reasoning concepts

## Architecture Onboarding
Component map: MMRB benchmark -> Model evaluation -> Fine-tuning pipeline -> Performance analysis

Critical path: Benchmark creation → Model evaluation across languages/contexts → Fine-tuning experiments → Analysis of enhancement/poisoning effects

Design tradeoffs: Balanced representation across languages vs. focusing on high-resource languages for optimal performance

Failure signatures: Performance degradation in low-resource languages, inconsistent moral judgments across languages, unexpected negative transfer during fine-tuning

First experiments:
1. Evaluate baseline models on MMRB to establish performance baselines
2. Conduct fine-tuning with varying quality levels of low-resource language data
3. Test cross-lingual transfer with zero-shot moral reasoning tasks

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Small language sample (5 languages total) limits generalizability to broader linguistic landscape
- Limited number of models (5 LLMs) constrains conclusions about model architecture effects
- Cultural differences in moral expression were not explicitly controlled for
- Distinction between genuine reasoning degradation and stylistic/cultural artifacts requires clarification

## Confidence
- High confidence: Performance inconsistencies across languages and degradation trends in low-resource languages
- Medium confidence: Mechanisms of enhancement and poisoning effects, implications for multilingual model development
- Low confidence: Universal applicability of findings to unstudied languages and models

## Next Checks
1. Replicate the study with additional low-resource languages from different language families to test generalizability
2. Conduct ablation studies varying data quality independently of language resource levels to isolate poisoning effects
3. Test the same models on established monolingual moral reasoning benchmarks in each language to verify genuine reasoning capabilities