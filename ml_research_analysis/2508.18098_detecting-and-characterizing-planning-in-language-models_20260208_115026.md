---
ver: rpa2
title: Detecting and Characterizing Planning in Language Models
arxiv_id: '2508.18098'
source_url: https://arxiv.org/abs/2508.18098
tags:
- planning
- token
- number
- digits
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework and semi-automated pipeline
  for detecting planning in large language models (LLMs). The key idea is to identify
  features that encode future tokens early in the generation process and causally
  influence intermediate steps toward those tokens.
---

# Detecting and Characterizing Planning in Language Models

## Quick Facts
- arXiv ID: 2508.18098
- Source URL: https://arxiv.org/abs/2508.18098
- Authors: Jatin Nainani, Sankaran Vaidyanathan, Connor Watts, Andre N. Assis, Alice Rigg
- Reference count: 38
- One-line primary result: Planning in LLMs is not universal—models sometimes improvise, and instruction tuning often refines rather than creates planning behavior.

## Executive Summary
This paper introduces a formal framework and semi-automated pipeline for detecting planning in large language models. The key idea is to identify features that encode future tokens early in the generation process and causally influence intermediate steps toward those tokens. By applying this method to Gemma-2-2B on MBPP code generation and rhyming tasks, the authors find that planning is not universal: the model sometimes improvises, and even when it plans, instruction tuning often refines rather than creates planning behavior. For example, on 13 MBPP tasks where instruction-tuned Gemma-2-2B showed planning, the base model solved only 54% correctly, often due to competing or incorrect plans. On a rhyming task, Gemma-2-2B improvised rather than planning like Claude 3.5 Haiku. The pipeline scales detection from infeasible exhaustive search to manageable computation while preserving accuracy. These findings highlight the variability of planning across tasks and models, and provide a reproducible foundation for mechanistic studies of LLM reasoning.

## Method Summary
The authors develop a semi-automated pipeline to detect planning in LLMs by identifying features that encode future tokens early and causally influence intermediate steps toward those tokens. The pipeline operates in four main steps: (1) circuit discovery via attribution patching to find a sparse feature set recovering ≥60% of correct token probability, (2) future-token encoding (FTE) filtering using logit lens to identify latents that promote future tokens, (3) cluster-level precursor influence (PI) checking via negative steering to test causal influence on intermediate steps, and (4) earliest-moment search and improvisation check within clusters. The method uses TopK sparse autoencoders on Gemma-2-2B to extract interpretable features and scales detection by ~10^4× through clustering and circuit discovery.

## Key Results
- Gemma-2-2B shows planning on 13 MBPP tasks, but the base model solves only 54% of these correctly due to competing plans.
- On a rhyming task, Gemma-2-2B improvises rather than planning like Claude 3.5 Haiku.
- Instruction tuning refines planning behavior by suppressing incorrect plans rather than creating new planning mechanisms.
- The pipeline scales detection from infeasible exhaustive search to manageable computation while preserving accuracy.

## Why This Works (Mechanism)

### Mechanism 1: Future-Token Encoding (FTE) via Logit-Lens Readout
- Claim: If a model is planning for a future token y_m during prediction of y_n, then some SAE latent in the circuit will linearly decode toward y_m through the unembedding matrix.
- Mechanism: For each SAE latent f at (layer, position), project its decoding direction W_l[f] through the unembedding matrix. If y_m appears in the top-K tokens, f is a future-token encoding candidate.
- Core assumption: Planning features that represent future goals must "write" in the direction of those goals in logit space.
- Evidence anchors:
  - [abstract] "identify features that encode future tokens early in the generation process"
  - [section 2.2, Definition 1] Formal FTE definition: "if y_m appears in the top K tokens when projecting W_l[f] through the unembedding matrix, then f is said to be a future-token encoding"
  - [corpus] Related work (arXiv:2601.20164) uses similar logit-lens probing for implicit planning in rhyme generation; weak direct corpus validation of this specific FTE formulation.
- Break condition: If y_m already appears in the prompt, FTE may detect attention rather than planning → classify as "Can't Say."

### Mechanism 2: Precursor Influence (PI) via Negative Steering
- Claim: A true planning feature must causally shape the entire trajectory toward the future token, not just activate before it.
- Mechanism: Subtract αW_l[f] from the residual stream at the feature's earliest activation. PI is satisfied if: (i) next token changes, (ii) ≥1 intermediate token changes, (iii) y_m disappears from output.
- Core assumption: Negative steering suppresses the concept/token a latent encodes; if planning, early suppression disrupts the whole path.
- Evidence anchors:
  - [abstract] "causally influence intermediate steps toward those tokens"
  - [section 2.2, Definition 2] Three-part PI criterion with explicit causal test
  - [section 3.2.1, Fig 2A] Suppressing "1" feature flips comma→")" and removes "1" entirely; [section 3.2.2, Fig 2B] Suppressing "2" feature switches from closed-form to recursive solution.
  - [corpus] arXiv:2601.20164 uses similar intervention-based planning metrics for rhyme tasks; moderate support.
- Break condition: If steering produces degenerate/nonsensical output, the intervention may push the model out-of-distribution → classify as "Can't Say" (Out-of-Distribution Steering).

### Mechanism 3: Scalable Search via Circuit Discovery + Clustering
- Claim: Exhaustively testing all ~4.2M (layer, latent, token) triples is infeasible; circuit discovery and clustering reduce search by ~10^4× while preserving true planning cases.
- Mechanism: (Step 0) Find smallest sparse feature circuit recovering ≥60% of original logit distribution via attribution patching/IG (~2-3×10^4 triples). (Step 1) FTE filter keeps only triples encoding some future token. (Step 2) Cluster triples by (layer, position, y_m) and test PI at cluster level (~50× cheaper). (Step 3-4) Within surviving clusters, find earliest planning moment via backward walk; test remaining for improvisation.
- Core assumption: Genuine planning features are recoverable by sparse circuits and cluster coherently around shared (layer, position, target).
- Evidence anchors:
  - [abstract] "The pipeline scales detection from infeasible exhaustive search to manageable computation while preserving accuracy."
  - [section 2.4, Steps 0-4] Explicit pipeline with factor ~10^4 reduction; cluster-level steering amortizes compute.
  - [corpus] No direct corpus comparison of this specific pipeline; this is a methodological contribution with limited external validation.
- Break condition: If 60% logit recovery threshold excludes important features, or clustering merges unrelated latents, pipeline may miss or mislabel planning.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) on MLP outputs
  - Why needed here: SAE latents provide interpretable, sparse representations; the entire pipeline relies on reading and steering these latents.
  - Quick check question: Given an SAE latent that activates on "rabbit"-related contexts, what does its decoding direction represent in the residual stream?

- Concept: Logit Lens / Unembedding Projection
  - Why needed here: FTE criterion requires projecting latent decoding directions through the unembedding matrix to see which tokens they promote.
  - Quick check question: If projecting W_l[f] through unembedding gives top tokens ["sort", "sorted", "sorting"], what does this suggest about f's function?

- Concept: Attribution Patching / Integrated Gradients for Circuit Discovery
  - Why needed here: Step 0 uses gradient-based approximations of indirect effects to find sparse circuits efficiently.
  - Quick check question: Why does attribution patching require only 2 forward passes + 1 backward pass compared to full ablation sweeps?

## Architecture Onboarding

- Component map:
  - Input -> Circuit Discovery (Step 0) -> FTE Filter (Step 1) -> Clustering -> Cluster-Level PI (Step 2) -> Earliest-Moment Search (Step 3) -> Improvisation Check (Step 4) -> Output Labels (PLAN / IMPROV / NEITHER / CAN'T SAY)

- Critical path: Circuit Discovery → FTE Filter → Cluster-Level PI → Earliest-Moment Search. If any step fails, subsequent steps don't execute.

- Design tradeoffs:
  - 60% logit recovery threshold: Higher preserves more features but increases compute; lower risks missing critical planning features.
  - Cluster-level steering before individual checks: ~50× faster but may mislabel if cluster mixes planning/improvisation features.
  - Top-K threshold for FTE: Larger K catches more candidates but increases false positives.

- Failure signatures:
  - Degenerate steered outputs (repeated tokens, incoherent text) → out-of-distribution, label CAN'T SAY
  - Target token y_m appears in prompt → FTE may reflect attention, not planning → CAN'T SAY (Overlap with Prompt)
  - Circuit recovery fails to reach 60% → pipeline may be analyzing incomplete causal graph

- First 3 experiments:
  1. Replicate rhyming couplet experiment (section 3.1): Run pipeline on "He saw a carrot and had to grab it" prompt; verify Gemma-2-2B shows IMPROV (no FTE+PI satisfied) unlike Claude 3.5 Haiku.
  2. Single MBPP task end-to-end: Pick "sort list of tuples by second element" (section 3.2.1); trace full pipeline, verify FTE detects "1" and PI suppresses it correctly.
  3. Base vs Instruct comparison on one planning task: Run pipeline on BASE model for tetrahedral number task; verify competing plans ("2" vs "-") and that suppressing "-" improves BASE performance toward INSTRUCT behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can planning be detected in real-time during inference without prior knowledge of the generated output?
- Basis in paper: [explicit] The authors state in Section 4.2 that current detection is "offline" (requiring knowledge of future tokens) and propose extending the approach to "detect planning at test time, where we have no knowledge of the future tokens."
- Why unresolved: The current pipeline validates Precursor Influence (PI) by checking if steering removes a *known* future token $y_m$; this creates a dependency on the completed sequence which must be broken for online detection.
- What evidence would resolve it: An algorithm that identifies latent features writing to tokens absent from the input context and successfully predicts the model's output trajectory before generation completes.

### Open Question 2
- Question: How can the pipeline definitively distinguish between the suppression of a planning mechanism and a general out-of-distribution (OOD) collapse?
- Basis in paper: [explicit] Section 4.2 notes that when steering results in "degenerate or nonsensical outputs," it is unclear if this represents valid evidence of planning or simply "pushed the model out of distribution."
- Why unresolved: The current "Can't Say" categorization handles this ambiguity by exclusion, but manual inspection is required to separate semantic plan cancellation from generic model failure.
- What evidence would resolve it: The establishment of automated metrics (e.g., perplexity thresholds, repetition penalties) or causal tracing methods that can verify the semantic integrity of the steered output.

### Open Question 3
- Question: How does the frequency and structure of planning circuits evolve with model scale?
- Basis in paper: [explicit] Section 4.1 lists "Scaling to larger models" as a limitation, stating the authors plan to apply the framework to "larger models in the Gemma family to understand how planning capabilities emerge as a function of scale."
- Why unresolved: The study is restricted to Gemma-2-2B, leaving unknown whether larger models exhibit universal planning, higher improvisation rates, or different circuit architectures for the same tasks.
- What evidence would resolve it: Comparative analysis applying the FTE and PI criteria to Gemma-2-9B or 27B on the identical MBPP and rhyming tasks.

### Open Question 4
- Question: Does instruction tuning create new suppression circuits for incorrect plans, or does it simply amplify existing correct ones?
- Basis in paper: [inferred] The paper notes in Section 3.3.1 that suppressing the incorrect "-" plan in the Base model recovers the Instruct behavior. This suggests instruction tuning induces a specific causal mechanism to filter bad plans, but the nature of this mechanism is not fully characterized.
- Why unresolved: While the paper concludes that tuning "refines" behavior, it does not detail if this is achieved by boosting the signal of the correct plan or actively inhibiting the base model's competing incorrect plans.
- What evidence would resolve it: A comparative circuit analysis between base and instruct models to identify specific "negative head" or inhibitory features that activate exclusively in the instruct model to suppress the alternative trajectories found in the base model.

## Limitations
- The pipeline's validity depends on three untested assumptions that could systematically bias results: arbitrary 60% logit recovery threshold, cluster-level steering assumptions about polysemantic latents, and unprincipled steering coefficient selection.
- Analysis is limited to a single model family (Gemma-2-2B) and task types (MBPP code generation, rhyming), leaving unknown whether planning mechanisms generalize to other architectures and tasks.
- The study cannot definitively distinguish between suppression of planning mechanisms and general out-of-distribution collapse when steering produces degenerate outputs.

## Confidence

**High Confidence Claims:**
- The pipeline correctly identifies cases where steering suppresses a future token and changes intermediate tokens (validated by MBPP task examples where suppressing "1" changes comma→")" and suppressing "2" switches solution strategies).
- Planning is not universal across tasks - Gemma-2-2B improvises on rhyming while Claude 3.5 Haiku plans, demonstrating clear between-model differences.
- Instruction tuning can refine planning behavior rather than create it - base model shows competing plans ("2" vs "-") that instruction tuning resolves.

**Medium Confidence Claims:**
- The factor-10^4 computational reduction is achieved as described, though external validation of pipeline efficiency is limited.
- Base model planning is less refined than instruction-tuned planning (54% accuracy on tasks where instruct plans), though alternative explanations include base model's general weakness rather than competing plans.

**Low Confidence Claims:**
- Planning is "not universal" across all LLMs - sample limited to Gemma-2-2B and Claude 3.5 Haiku comparisons.
- The pipeline finds "all planning cases" - no external validation that no planning features exist outside detected clusters.

## Next Checks
1. **Cross-Architecture Validation**: Apply the pipeline to LLaMA-3-8B and GPT-3.5-turbo on the same MBPP and rhyming tasks. Compare planning detection rates and feature characteristics to test whether Gemma-2-2B patterns generalize across architectures.

2. **Ablation of Thresholds**: Systematically vary the 60% logit recovery threshold (40%, 60%, 80%) and cluster size parameters. Track how many planning cases are gained/lost at each threshold to quantify sensitivity to arbitrary cutoffs.

3. **Steering Coefficient Sensitivity**: For one representative planning case (e.g., the "1" feature in MBPP), test a fine-grained grid of steering coefficients (-20, -40, -60, -80, -100). Plot how PI criteria change with α to determine if the current range captures the full planning signal or if weaker coefficients reveal additional cases.