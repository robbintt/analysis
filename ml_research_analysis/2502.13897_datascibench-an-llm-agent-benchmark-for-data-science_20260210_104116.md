---
ver: rpa2
title: 'DataSciBench: An LLM Agent Benchmark for Data Science'
arxiv_id: '2502.13897'
source_url: https://arxiv.org/abs/2502.13897
tags:
- data
- output
- task
- file
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DataSciBench is a comprehensive benchmark designed to evaluate
  large language models on data science tasks. It addresses limitations of existing
  benchmarks by incorporating challenging, real-world prompts with uncertain ground
  truth and complex evaluation metrics.
---

# DataSciBench: An LLM Agent Benchmark for Data Science

## Quick Facts
- arXiv ID: 2502.13897
- Source URL: https://arxiv.org/abs/2502.13897
- Reference count: 40
- 23 models evaluated on 222 data science prompts across 6 task types

## Executive Summary
DataSciBench is a comprehensive benchmark designed to evaluate large language models on data science tasks. It addresses limitations of existing benchmarks by incorporating challenging, real-world prompts with uncertain ground truth and complex evaluation metrics. The authors develop a semi-automated pipeline that leverages LLM self-consistency and human verification to generate ground truth and evaluation functions. They propose a Task-Function-Code (TFC) framework to assess each code execution outcome through precisely defined metrics and programmatic rules. Experimental results show that API-based models like GPT-4o significantly outperform open-source alternatives, though all models demonstrate substantial room for improvement in following detailed instructions, tool utilization, and output generation.

## Method Summary
The benchmark uses a semi-automated pipeline combining LLM self-consistency sampling with human verification to generate ground truth for 519 test cases across 222 prompts. The Task-Function-Code (TFC) framework maps each prompt to specific task types (6 types) and evaluation functions (25 aggregate functions) with programmatic rules. A Data Interpreter creates hierarchical DAGs for task execution, while a VLM-as-a-judge evaluates visualization outputs. The evaluation scores models on completion rate, success rate, and five fine-grained metrics, with API-based models like GPT-4o-mini selected for task type and function identification.

## Key Results
- API-based models (GPT-4o) achieve significantly higher scores than open-source alternatives
- All models show substantial room for improvement in instruction following and tool utilization
- Reasoning-specialized models like o1-mini fail primarily due to instruction non-compliance despite strong reasoning capabilities
- Larger models sometimes underperform smaller variants due to format and instruction compliance issues

## Why This Works (Mechanism)

### Mechanism 1
The semi-automated pipeline likely improves the reliability of Ground Truth (GT) for complex data science tasks where answers are not easily verifiable. The system samples multiple outputs from LLMs and applies a self-consistency strategy to converge on a candidate answer. This candidate is then verified by human experts to ensure the GT is accurate, addressing the "uncertain ground truth" problem inherent in open-ended data analysis.

### Mechanism 2
The Task-Function-Code (TFC) framework enables fine-grained evaluation by mapping abstract instructions to executable verification logic. Instead of relying on generic pass/fail, the framework identifies specific task types (e.g., Data Cleaning) and maps them to aggregate functions (e.g., "Data Completeness") with programmatic rules (e.g., checking null counts). This creates a structured tuple $(T, F, C)$ that explicitly defines the success criteria for code execution.

### Mechanism 3
Separating reasoning capability from instruction-following reveals specific bottlenecks in "reasoning-specialized" models like o1-mini when applied to data science. By evaluating models on both coarse-grained (Success/Completion Rate) and fine-grained metrics (specific function compliance), the benchmark detects cases where models can plan the task but fail to output the specific format or file required (instruction non-compliance).

## Foundational Learning

- **Self-Consistency in LLMs**
  - Why needed here: This concept underpins the Ground Truth generation pipeline. You must understand that sampling multiple reasoning paths and taking the majority vote helps stabilize outputs before human verification.
  - Quick check question: Can you explain why "majority voting" on code outputs might be riskier for data science than for text generation?

- **Programmatic Evaluation vs. Model-as-Judge**
  - Why needed here: DataSciBench distinguishes between checking code output programmatically (e.g., `df.isnull().sum() == 0`) and using a VLM to judge a plot. Understanding when to use deterministic rules vs. subjective AI scoring is crucial for interpreting the results.
  - Quick check question: For a "Predictive Modeling" task, would you use a VLM-as-a-judge or a programmatic rule? Why?

- **Directed Acyclic Graphs (DAG) in Agents**
  - Why needed here: The benchmark uses a "Data Interpreter" to generate a hierarchical DAG of tasks (Task -> Function -> Code). Understanding how agents break a complex prompt into a dependency graph is necessary to debug why a model might succeed at step 1 but fail at step 3.
  - Quick check question: If an agent generates a DAG where "Data Visualization" depends on "Data Cleaning," what happens to the downstream task if the cleaning code has a runtime error?

## Architecture Onboarding

- **Component map**: Prompt Source -> TFC Framework -> Evaluation Layer -> Subject System
- **Critical path**: Prompt Selection -> TFC Definition -> Execution -> Scoring
- **Design tradeoffs**: Cost vs. Reliability (LLM+Human verification slower but more accurate) vs. Generality vs. Specificity (25 functions general but may miss edge cases)
- **Failure signatures**: Instruction Non-Compliance (correct code but wrong output format), Hallucinated Libraries (importing non-existent libraries), Format Mismatch (larger models failing JSON output)
- **First 3 experiments**:
  1. Baseline Run: Evaluate local model against coarse-grained metrics (SR/CR) first
  2. Function Analysis: Isolate performance on "Data Visualization" using VLM-as-a-judge vs. programmatic rules
  3. Error Categorization: Run subset where model has high Reasoning but low SR to identify "Hallucination" vs. "Instruction Non-Compliance"

## Open Questions the Paper Calls Out

### Open Question 1
Can VLMs be trained as critic models to enhance the precision of fine-grained evaluations for data visualization tasks? The limitations section states that current VLM-as-a-judge metrics may lack precision and suggests employing VLMs to train critic models as a potential solution.

### Open Question 2
Why do larger-scale models sometimes underperform smaller models in following simple formatting instructions? Section 5.4 highlights that CodeLlama-34B-Instruct scores lower than its 13B and 7B counterparts, potentially due to training data format biases.

### Open Question 3
How can data science agents be improved to prevent hallucination of library functions and data columns? Section 5.4 identifies "hallucination about the column name" and false calls to non-existent methods as primary causes of execution failures.

## Limitations

- Potential bias in ground truth generation from LLM self-consistency sampling
- Reliance on single VLM-as-a-judge for visualization evaluation may not capture subjective quality
- Manual bottleneck in defining TFC functions limits scalability

## Confidence

- **High Confidence**: Experimental methodology clearly described with reproducible results for API-based models
- **Medium Confidence**: TFC framework effectiveness depends on assumption that 25 aggregate functions sufficiently capture data science task quality
- **Low Confidence**: Claim about o1-mini's specific failure mode (instruction non-compliance vs. reasoning) based on qualitative analysis that may not generalize

## Next Checks

1. Cross-dataset validation: Test DataSciBench prompts on external datasets to verify the 25 TFC functions generalize beyond curated corpus
2. VLM robustness test: Compare VLM-as-a-judge scores with human expert ratings across 50 randomly sampled visualization tasks
3. Open-source scalability: Run benchmark on 3-5 additional open-source models to determine if results reflect model size limitations or architectural constraints