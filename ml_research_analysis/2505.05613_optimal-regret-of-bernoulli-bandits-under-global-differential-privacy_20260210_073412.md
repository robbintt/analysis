---
ver: rpa2
title: Optimal Regret of Bernoulli Bandits under Global Differential Privacy
arxiv_id: '2505.05613'
source_url: https://arxiv.org/abs/2505.05613
tags:
- regret
- bound
- bandits
- basu
- azize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies regret minimization in stochastic bandits under\
  \ global differential privacy (DP), focusing on Bernoulli bandits. The main contributions\
  \ are: (1) a tighter regret lower bound that introduces a new information-theoretic\
  \ quantity d\u03F5, which smoothly interpolates between Kullback-Leibler divergence\
  \ and Total Variation distance depending on the privacy budget; (2) a new concentration\
  \ inequality for sums of Bernoulli variables under Laplace mechanism, which is a\
  \ DP version of the Chernoff bound; and (3) two DP algorithms, DP-KLUCB and DP-IMED,\
  \ that asymptotically match the lower bound up to a constant arbitrarily close to\
  \ 1."
---

# Optimal Regret of Bernoulli Bandits under Global Differential Privacy

## Quick Facts
- arXiv ID: 2505.05613
- Source URL: https://arxiv.org/abs/2505.05613
- Reference count: 40
- Primary result: Achieves asymptotically optimal regret under ε-global differential privacy for Bernoulli bandits using cumulative sum updates without reward-forgetting

## Executive Summary
This paper establishes the optimal regret bounds for stochastic multi-armed bandits with Bernoulli rewards under ε-global differential privacy. The authors introduce a new information-theoretic quantity d_ε that smoothly interpolates between KL divergence and total variation distance, characterizing the fundamental hardness of private bandit learning. They prove matching upper and lower bounds showing that optimal regret can be achieved without the previously conjectured necessity of reward-forgetting, by using geometrically increasing batch sizes with cumulative sum updates.

## Method Summary
The approach uses a unified algorithm framework (DP-KLUCB and DP-IMED) that runs in arm-dependent phases with geometrically increasing batch sizes. At each phase, cumulative rewards are updated with added Laplace noise to ensure ε-differential privacy, and private indices are computed using the d_ε optimization. The algorithms achieve optimal regret by carefully balancing the privacy cost against the KL-transport cost, with batch sizes growing as B_m ≈ n_0·α^m where α can be set arbitrarily close to 1 for asymptotic optimality.

## Key Results
- Introduces d_ε that characterizes hardness of ε-global DP, interpolating between KL and TV distances
- Proves asymptotically tight regret lower bound using double change of environment argument
- Develops new concentration inequality for Bernoulli sums under Laplace mechanism
- Shows optimal regret achievable without reward-forgetting, refuting previous conjecture
- DP-KLUCB and DP-IMED algorithms match lower bound up to constant arbitrarily close to 1

## Why This Works (Mechanism)

### Mechanism 1
The hardness of ε-global DP in stochastic bandits is characterized by a single information-theoretic quantity d_ε that smoothly interpolates between KL divergence and Total Variation distance depending on privacy budget ε. d_ε(x, y) = inf_{z∈[x∧y,x∨y]} {ε|z−x| + kl(z, y)} combines KL transport with TV transport through a double change of environment argument that optimizes over intermediate distributions.

### Mechanism 2
The sum of Bernoulli variables with Laplace noise concentrates with exponent d_ε rather than the looser union bound of separate KL and Laplace concentrations. Proposition 7 treats the convolution of Bernoulli sums and Laplace noise jointly via Chernoff-style analysis, showing that when the number of noise terms m is negligible compared to n Bernoulli samples, the dominant term behaves as if m=1.

### Mechanism 3
Optimal regret under ε-global DP does not require forgetting past rewards; accumulating all rewards while adding Laplace noise at each phase achieves the lower bound. Algorithms run in arm-dependent geometrically-increasing batches, releasing cumulative sums (not partial sums) with added Laplace noise per phase. The post-processing property of DP ensures that cumulative sums remain ε-DP even though they reuse prior data.

## Foundational Learning

- **ε-Differential Privacy and Laplace Mechanism**: Why needed - the entire paper assumes familiarity with ε-DP guarantees and Laplace mechanism for bounded-sensitivity queries. Quick check - Given a function f with sensitivity Δf, what Laplace scale parameter ensures ε-DP?

- **Regret in Stochastic Multi-Armed Bandits**: Why needed - the paper builds directly on KL-UCB and IMED, assuming familiarity with optimism-in-the-face-of-uncertainty and KL divergence in non-private lower bounds. Quick check - Why does standard UCB use √(log(t)/n) while KL-UCB uses KL-based index?

- **KL Divergence and Total Variation Distance**: Why needed - d_ε is defined in terms of kl and TV; understanding how these distances behave is essential to grasp why d_ε interpolates between regimes. Quick check - For Bernoulli(μ) and Bernoulli(μ'), is kl(μ, μ') larger or smaller than TV(μ, μ') when μ and μ' are close?

## Architecture Onboarding

- **Component map**: Batch scheduler → Private accumulator → Index calculator → Arm selector

- **Critical path**: 1) Initialize: Pull each arm B_0 times, compute initial noisy means. 2) Per-iteration: Compute indices for all arms → select arm i(t) → pull B_{m_i(t)+1} times → update Š_{i,m} with new rewards + one Laplace sample → recompute private mean. 3) Repeat until horizon T reached.

- **Design tradeoffs**: α selection (smaller α → regret closer to optimal but more batches/overhead); batch initialization n_0 (larger n_0 reduces early-phase noise impact but increases initial uniform exploration cost); index computation (solving d_ε optimization requires case analysis based on μ vs. μ*/(μ* + (1-μ*)e^ε)).

- **Failure signatures**: Regret exceeding α·log(T)/d_ε (check if batches are growing correctly and d_ε is computed using explicit solution); privacy breach assertion (verify cumulative sums are being released and Laplace scale is exactly 1/ε); numerical instability in index (d_ε involves log terms near boundaries, clamp [˜μ]₀¹ before computing).

- **First 3 experiments**: 1) Replicate Figure 1 baseline: Run DP-KLUCB and DP-IMED on μ₁=[0.75,0.7,0.7,0.7,0.7] with ε=0.25, T=10⁶, α=2, n₀=1; compare regret curves against DP-SE and AdaP-KLUCB. 2) Vary ε across regimes: Run DP-IMED on 5-arm instance, sweeping ε∈{0.01, 0.1, 0.5, 1.0}; plot regret vs. ε and verify smooth transition between KL-dominated and TV-dominated regimes. 3) Ablate forgetting: Modify DP-KLUCB to reset partial sums per-phase (mimicking DP-SE); confirm regret increases by predicted factor.

## Open Questions the Paper Calls Out

- Can the concentration inequality and matching regret bounds be generalized to distribution families beyond Bernoulli, such as sub-Gaussian or exponential families? The current proofs rely heavily on Bernoulli properties.

- Does the tight characterization extend to (ε, δ)-differential privacy? The lower bound proof relies on maximal couplings and properties of pure DP that may not hold for approximate DP.

- What are the finite-time constants in the regret bounds, and do they differ significantly from asymptotic limits? Asymptotic analysis does not capture lower-order terms that dominate in practical, finite-horizon scenarios.

## Limitations

- Analysis critically relies on Bernoulli assumption; generalization to other distributions remains unproven
- Concentration bound depends on asymptotic regime that may not hold in finite experiments
- Claim that forgetting is unnecessary is theoretically supported but not extensively experimentally validated

## Confidence

- **High confidence**: Information-theoretic characterization via d_ε; privacy analysis of cumulative sums
- **Medium confidence**: Coupled concentration bound; experimental validation against baselines
- **Low confidence**: Generalization to non-Bernoulli rewards; optimality claims requiring extremely small α

## Next Checks

1. **Numerical robustness test**: Implement DP-KLUCB with μ near boundaries (0.01, 0.99) and verify index computation remains stable; compare against numerical optimization to detect branching errors in d_ε implementation.

2. **Batch ratio sensitivity**: Run DP-IMED varying α∈{1.1, 1.5, 2.0, 3.0} on same environment and plot regret vs. α; verify smaller α achieves closer-to-optimal regret but with higher computational overhead.

3. **Forgetting ablation**: Implement DP-KLUCB variant that resets cumulative sums each phase (explicit forgetting); run on μ₁ environment with ε=0.25, T=10⁶; confirm regret increases by factor predicted by theory.