---
ver: rpa2
title: Reinforcement Learning-based Sequential Route Recommendation for System-Optimal
  Traffic Assignment
arxiv_id: '2505.20889'
source_url: https://arxiv.org/abs/2505.20889
tags:
- traffic
- assignment
- route
- network
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reformulates the static system-optimal (SO) traffic
  assignment problem as a sequential decision-making task solved using deep reinforcement
  learning (RL). The approach introduces an MSA-guided deep Q-learning algorithm that
  incrementally recommends routes to travelers as they appear, aiming to minimize
  total system travel time.
---

# Reinforcement Learning-based Sequential Route Recommendation for System-Optimal Traffic Assignment

## Quick Facts
- arXiv ID: 2505.20889
- Source URL: https://arxiv.org/abs/2505.20889
- Reference count: 26
- One-line primary result: RL-based sequential route recommendation achieves 0.35% deviation from theoretical SO solution on Ortuzar–Willumsen network

## Executive Summary
This paper reformulates the static system-optimal (SO) traffic assignment problem as a sequential decision-making task solved using deep reinforcement learning (RL). The approach introduces an MSA-guided deep Q-learning algorithm that incrementally recommends routes to travelers as they appear, aiming to minimize total system travel time. The RL agent learns to assign routes based on real-time network conditions and adapts its policy through interaction with the environment. Experiments on both the Braess paradox and Ortuzar–Willumsen (OW) networks demonstrate that the proposed method closely approximates the theoretical SO solution, achieving a deviation of only 0.35% in the OW network.

## Method Summary
The method treats static SO traffic assignment as a sequential decision-making MDP where a central agent recommends routes to travelers one-by-one as OD demands arrive. It uses MSA-guided deep Q-learning with a marginal travel time reward function that internalizes externalities. The agent operates on a state encoding that includes edge-level features (travel time, flow, marginal time) and OD-level features (one-hot OD encoding, route marginal times). Route action sets are predefined per OD pair (either k-shortest paths or SO-derived). The MSA guide maintains an assignment distribution that is iteratively updated using all-or-nothing shortest paths after each episode to bias exploration toward theoretically sound regions.

## Key Results
- Achieves 0.35% deviation from theoretical SO solution on OW network (54,809.8 min vs SO-MSA baseline)
- Ablation studies show RL-SO (SO-informed routes) achieves 0.26% deviation vs 3.49% for RL-10-SP (generic routes)
- MSA-guided RL converges to SO solution in Braess network (6 travelers, 498 min total travel time) within 240 episodes

## Why This Works (Mechanism)

### Mechanism 1
Integrating classical traffic assignment (MSA) into RL exploration accelerates convergence toward system-optimal solutions. The MSA-guided selection samples actions from an assignment distribution that is iteratively refined using traditional "all-or-nothing" shortest-path assignments. This biases exploration toward theoretically sound regions of the action space rather than relying solely on ε-greedy randomness, reducing the effective search space. Core assumption: The MSA iterative structure approximates meaningful directions toward SO, which holds for static assignment with known link performance functions.

### Mechanism 2
A marginal travel time reward function aligns individual route decisions with total system travel time minimization. The reward captures both the entering traveler's travel time and the additional delay imposed on all other travelers on that route. This internalizes the externality of each assignment, converting a global objective into a per-step local signal that the RL agent can optimize. Core assumption: The BPR link performance function and its derivative accurately model travel time delays; full traveler compliance ensures recommended routes are actually taken.

### Mechanism 3
Route action set design significantly impacts convergence speed and final optimality gap. The action space defines which routes the agent can select. SO-informed route sets (derived from analytical solutions) contain high-quality candidates, enabling faster learning. Larger generic sets (k-shortest paths) provide exploratory capacity but slow convergence. Core assumption: The SO route set is available or approximable; if unavailable, k-shortest paths must include near-optimal routes.

## Foundational Learning

- Concept: **Markov Decision Processes (MDP) and Q-Learning**
  - Why needed here: The paper reformulates traffic assignment as an MDP solved via deep Q-learning. Understanding Bellman equations, value functions, and experience replay is prerequisite.
  - Quick check question: Can you explain why Q-learning uses a target network and experience replay buffer?

- Concept: **Traffic Assignment Fundamentals (UE vs SO, BPR Functions)**
  - Why needed here: The reward function and MSA guidance depend on BPR link performance functions and the distinction between user equilibrium and system optimum objectives.
  - Quick check question: What is the difference between Wardrop's User Equilibrium and System Optimum, and why does the Braess paradox occur?

- Concept: **Method of Successive Averages (MSA)**
  - Why needed here: MSA is integrated into the RL training loop to guide exploration. Understanding its iterative averaging structure is essential for implementing Algorithm 1.
  - Quick check question: How does MSA update the assignment distribution at each iteration, and why does it converge?

## Architecture Onboarding

- Component map: Environment (network graph, BPR functions, OD demand sequence) -> State Encoder (edge-level features + OD-level features) -> Action Space (predefined route sets per OD) -> Reward Function (marginal travel time) -> Policy Network (Dueling Double DQN) -> MSA Guide (assignment distribution)

- Critical path:
  1. Initialize replay buffer, Q-networks, and MSA distribution
  2. For each episode: for each traveler in sequence, select route via MSA-guided ε-greedy
  3. Update network flows, compute marginal reward, transition to next state
  4. Store transition, sample mini-batch, update Q-network via TD loss
  5. After episode, run all-or-nothing assignment and update MSA distribution

- Design tradeoffs:
  - Action set size: Larger k-shortest paths improves optimality potential but slows convergence; SO-derived sets are optimal but require prior analytical solution
  - MSA guidance vs pure ε-greedy: MSA accelerates convergence in static settings but may not generalize to dynamic/stochastic environments
  - Full compliance assumption: Enables clean SO evaluation but limits real-world deployment; partial compliance is noted as future work

- Failure signatures:
  - Training curve oscillates without converging → Check reward scaling, learning rate, or action set completeness
  - Final policy deviates significantly from SO → Verify action set includes SO routes; check MSA update implementation
  - Reward does not decrease over episodes → Verify marginal travel time computation (derivative term in Eq. 9)

- First 3 experiments:
  1. Replicate Braess paradox network: 6 travelers, 3 routes; verify convergence to theoretical SO (498 min total travel time, no use of ACDB route)
  2. Ablation on action sets: Compare RL-10-SP, RL-15-SP, and RL-SO on OW network; measure convergence speed and final deviation from SO-MSA baseline
  3. MSA-guided vs standard DQN: Run both on OW network with identical hyperparameters; quantify episodes to reach within 1% of SO solution

## Open Questions the Paper Calls Out

### Open Question 1
How does the MSA-guided RL algorithm perform under partial user compliance, where travelers may deviate from recommended routes? The paper assumes full compliance but notes this is unrealistic; future work will extend to partial compliance scenarios where deviation rates (e.g., 50-90%) are tested.

### Open Question 2
Can the sequential RL framework approximate SO solutions under stochastic and time-varying OD demand rather than fixed demand matrices? Current experiments use predetermined OD matrices, but real-world demand is uncertain and time-dependent, requiring extension to dynamic settings.

### Open Question 3
What is the optimal strategy for designing route action sets when the analytical SO solution is unavailable? While ablation shows action set design critically impacts performance, the paper does not establish general principles for action set construction in networks without known SO solutions.

### Open Question 4
How does the approach scale to large urban networks with thousands of nodes and links? The largest tested network (OW) has only 13 nodes, 48 links, and 1700 travelers, suggesting potential scalability challenges for city-scale applications.

## Limitations
- Empirical validation relies on small synthetic networks where analytical SO solutions are tractable
- Critical hyperparameters for the OW network (link parameters, specific route sets) are not fully specified
- MSA-guided exploration assumes static network conditions; performance in dynamic/stochastic environments untested
- Full compliance assumption is a significant idealization; partial compliance is deferred to future work

## Confidence
- **High confidence** in the mechanism linking marginal travel time rewards to internalizing externalities, as the mathematical formulation is explicit and grounded in traffic flow theory
- **Medium confidence** in MSA-guided exploration accelerating convergence, supported by ablation studies but dependent on the static network assumption
- **Medium confidence** in the action set design findings, as the ablation is performed but the range of tested sets is limited and SO-informed sets require prior analytical solutions

## Next Checks
1. **Reproduce the Braess network results**: Implement the environment with BPR functions and verify convergence to the theoretical SO solution (498 min total travel time, no ACDB route used) within ~240 episodes
2. **Ablation study on action sets**: Train RL with different route sets (e.g., 10-SP, 15-SP, SO-derived) on the OW network and measure both convergence speed and final deviation from the SO-MSA baseline (54,809.8 min)
3. **Validate MSA vs. standard DQN**: Run both algorithms on the OW network with identical hyperparameters to quantify the number of episodes required to reach within 1% of the SO solution, isolating the impact of MSA guidance