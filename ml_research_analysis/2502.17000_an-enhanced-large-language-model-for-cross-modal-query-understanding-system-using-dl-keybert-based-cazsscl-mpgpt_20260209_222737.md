---
ver: rpa2
title: An Enhanced Large Language Model For Cross Modal Query Understanding System
  Using DL-KeyBERT Based CAZSSCL-MPGPT
arxiv_id: '2502.17000'
source_url: https://arxiv.org/abs/2502.17000
tags:
- image
- proposed
- cross-modal
- existing
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an enhanced large language model framework,
  DL-KeyBERT-based CAZSSCL-MPGPT, for cross-modal query understanding to address limitations
  in existing methods such as echo chamber effects, poor semantic relationship capture,
  and inability to handle complex images. The framework incorporates image preprocessing
  with Pareto-Gini CLAHE for contrast enhancement, E-YOLO with Easom function for
  robust object segmentation, conditional random knowledge graph construction to mitigate
  bias, and FOA for optimal feature selection.
---

# An Enhanced Large Language Model For Cross Modal Query Understanding System Using DL-KeyBERT Based CAZSSCL-MPGPT

## Quick Facts
- arXiv ID: 2502.17000
- Source URL: https://arxiv.org/abs/2502.17000
- Authors: Shreya Singh
- Reference count: 38
- Primary result: Achieved 99.14% accuracy, BLEU 0.991, METEOR 0.991 on COCO 2017 dataset

## Executive Summary
This paper presents DL-KeyBERT-based CAZSSCL-MPGPT, an enhanced large language model framework for cross-modal query understanding addressing limitations in existing methods. The framework integrates image preprocessing with Pareto-Gini CLAHE for contrast enhancement, E-YOLO with Easom function for robust object segmentation, and conditional random knowledge graph construction to mitigate bias. Text is embedded using DL-KeyBERT to capture semantic relationships. The model achieves state-of-the-art performance on COCO 2017 (99.14% accuracy, BLEU 0.991, METEOR 0.991) and vqav2-val datasets (98.43% accuracy, BLEU 0.984, METEOR 0.984).

## Method Summary
The CAZSSCL-MPGPT framework processes images through a multi-stage pipeline: resize and median filtering for noise removal, Pareto-Gini CLAHE for contrast enhancement, E-YOLO segmentation with Easom function scaling, and CRKG knowledge graph construction. Features are extracted using color, edge, texture, GLCM, HOG, and graph features, then selected via Fossa Optimization Algorithm. Text input undergoes tokenization and BERT embedding, followed by DL-KeyBERT keyword extraction using Damerau-Levenshtein distance. The CAZSSCL-MPGPT model incorporates mixup regularization, Phish activation, cross-attention layers, zero-shot learning, and semantic consistency loss for improved performance.

## Key Results
- Achieved 99.14% accuracy, BLEU 0.991, METEOR 0.991 on COCO 2017 dataset
- Reached 98.43% accuracy, BLEU 0.984, METEOR 0.984 on vqav2-val dataset
- Outperformed existing approaches on cross-modal query understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CRKG reduces echo chamber effect by diversifying pattern representation rather than reinforcing dominant ones
- Mechanism: Conditional probability labeling accounts for dependencies between entities and spatial relationships
- Core assumption: Echo chamber effect stems from biased entity-relation patterns in image data
- Evidence anchors: Abstract mentions CRKG for bias mitigation; section 3.5 describes graph design for balanced representation
- Break condition: If bias originates from embedding space geometry rather than graph structure

### Mechanism 2
- Claim: Cross-attention with zero-shot semantic consistency learning enables dynamic text-image alignment
- Mechanism: Cross-attention aligns modalities while semantic consistency loss minimizes L2 distance between representations
- Core assumption: Semantic gap between seen/unseen classes can be closed via L2 distance minimization
- Evidence anchors: Abstract mentions cross-attention and zero-shot learning; section 3.9 describes semantic consistency loss
- Break condition: If unseen class semantics diverge structurally from seen classes

### Mechanism 3
- Claim: DL-KeyBERT improves keyword extraction by combining semantic embedding with orthographic similarity
- Mechanism: KeyBERT generates embeddings, Damerau-Levenshtein distance ranks terms by edit-distance operations
- Core assumption: Orthographic similarity complements semantic similarity for identifying meaningful keywords
- Evidence anchors: Abstract mentions DL-KeyBERT for semantic relationships; section 3.8 describes keyword selection process
- Break condition: If domain terminology includes high synonymy with low orthographic overlap

## Foundational Learning

- Concept: **Conditional Random Fields (CRFs) for structured prediction**
  - Why needed here: CRKG uses conditional probability to label entities based on context and spatial relationships
  - Quick check question: Given an image region, how would a CRF assign entity labels differently than independent per-region classification?

- Concept: **Cross-attention in transformers**
  - Why needed here: CAZSSCL-MPGPT uses cross-attention where queries come from visual features and keys/values from text
  - Quick check question: In cross-attention, what happens to gradient flow if key modality embeddings have much higher variance than query embeddings?

- Concept: **Zero-shot learning and semantic gap**
  - Why needed here: Model claims to classify unseen data via zero-shot learning with semantic consistency loss
  - Quick check question: If semantic consistency loss minimizes ||generated - target||², what failure mode occurs when target representations for unseen classes are noisy?

## Architecture Onboarding

- Component map: Image Input → Resize → Median Filter → PG-CLAHE → E-YOLO Segmentation → Object Skeletons → CRKG → Feature Extraction → FOA Feature Selection → CAZSSCL-MPGPT → Caption/VQA Output; Text Input → Tokenization → BERT Embedding → DL-KeyBERT → CAZSSCL-MPGPT

- Critical path:
  1. PG-CLAHE clipping limit (Eq. 7-8): Incorrect Pareto-Gini parameters cause over/under-enhancement
  2. E-YOLO Easom scaling (Eq. 13-14): Wrong scaling factor mislocalizes small objects
  3. Cross-attention dimensionality (Eq. 40): Mismatched key dimension causes attention collapse
  4. Semantic consistency loss weight (Eq. 45): Over-regularization may suppress diversity

- Design tradeoffs:
  - Phish activation vs. GELU: Phish claims efficiency but introduces tanh-GELU composition
  - FOA vs. learned attention: FOA is metaheuristic (non-differentiable), may not adapt to distribution shifts
  - Mixup regularization: Reduces overfitting but may blur modality-specific features

- Failure signatures:
  - IOU drops on small objects → Easom scaling factor poorly calibrated
  - High BLEU but low semantic diversity → Mixup or semantic consistency loss over-constraining
  - Generated captions ignore rare objects → CRKG not sufficiently diversifying entity representation

- First 3 experiments:
  1. Ablate PG-CLAHE: Replace with standard CLAHE, measure MAE/MSE/RMSE and downstream caption accuracy
  2. Cross-attention dimensionality sweep: Vary key dimension d', monitor attention entropy and BLEU
  3. Semantic consistency loss weight tuning: Vary loss coefficient, evaluate on unseen class subset

## Open Questions the Paper Calls Out

- **Domain-specific contexts**: The model didn't concentrate on domain-specific contexts like healthcare, education, travel, and hospitality. Performance in specialized domains with distinct terminology remains unproven.

- **Cultural integration**: Integration of cultural event data and sentiment polarity could significantly improve context and nuance of generated captions, but this was not implemented.

- **Computational efficiency**: The sequential multi-stage pipeline (Preprocessing → E-YOLO → CRKG → FOA) compared to end-to-end Multimodal LLM approaches raises concerns about latency and resource overhead.

- **Echo chamber mitigation**: While CRKG claims to solve echo chamber effect, the validation uses standard benchmarks without specific ablation studies on bias metrics or rare object detection.

## Limitations

- **Unrealistically high accuracy**: Reported 99.14% accuracy on COCO 2017 is implausibly high for image captioning tasks, raising concerns about metric calculation methodology.

- **Missing hyperparameters**: Critical parameters including transformer architecture details, training configuration, and Pareto exponent for PG-CLAHE are not specified.

- **Non-differentiable feature selection**: FOA feature selection is metaheuristic and non-differentiable, potentially limiting adaptation to distribution shifts.

## Confidence

- **High confidence**: Technical mechanisms for PG-CLAHE contrast enhancement, cross-attention alignment, and Damerau-Levenshtein distance for keyword extraction are well-specified and theoretically sound

- **Medium confidence**: Phish activation function's efficiency claims and FOA feature selection effectiveness require empirical validation

- **Low confidence**: Reported accuracy metrics appear implausibly high for the stated datasets and may reflect methodological issues in evaluation

## Next Checks

1. **Metric validation check**: Compare baseline GPT/BERT performance from Table 2 with established benchmarks. If baseline scores also exceed 90%, verify whether accuracy is calculated per-token versus sequence-level, and whether BLEU/METEOR scores are averaged appropriately across samples.

2. **Phish activation gradient stability test**: Implement Phish activation (T_in × tanh(GELU(T_in))) in the transformer and monitor gradient norms during training. If gradients explode with negative inputs, apply gradient clipping or revert to GELU and measure performance degradation.

3. **Cross-attention dimensionality alignment verification**: Systematically vary the key dimension d' in the cross-attention computation (Eq. 40), monitoring attention entropy and BLEU scores. Identify the critical dimensionality threshold where attention collapse occurs, confirming the alignment mechanism's capacity limits.