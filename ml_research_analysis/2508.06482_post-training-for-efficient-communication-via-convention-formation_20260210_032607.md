---
ver: rpa2
title: Post-training for Efficient Communication via Convention Formation
arxiv_id: '2508.06482'
source_url: https://arxiv.org/abs/2508.06482
tags:
- formation
- convention
- thing
- reference
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient communication in
  large language models (LLMs), specifically their inability to form ad-hoc conventions
  that humans naturally develop during multi-turn interactions. The authors propose
  a post-training method that uses preference optimization on heuristically identified
  examples of convention formation from human conversations, along with special planning
  tokens to distinguish initial mentions from re-mentions.
---

# Post-training for Efficient Communication via Convention Formation

## Quick Facts
- arXiv ID: 2508.06482
- Source URL: https://arxiv.org/abs/2508.06482
- Reference count: 40
- Primary result: Post-training method achieves 26% message shortening while maintaining accuracy in convention formation tasks

## Executive Summary
This paper addresses the challenge of efficient communication in large language models by developing a post-training method that enables models to form ad-hoc conventions during multi-turn interactions. The authors identify that while humans naturally develop efficient communication protocols through repeated interactions, contemporary LLMs struggle to replicate this behavior. Their approach uses preference optimization on heuristically identified examples of convention formation from human conversations, combined with special planning tokens to distinguish initial mentions from re-mentions. The method is evaluated on two novel benchmarks: a text-only reference game and a document-grounded utterance completion task.

The results demonstrate significant improvements in convention formation efficiency, with post-trained models shortening messages by up to 26% compared to original models while maintaining accuracy. The approach also shows substantially higher consistency, as measured by lower word novelty distance, indicating better retention of established conventions across turns. The authors conclude that contemporary LLMs can be enhanced to communicate more efficiently through targeted post-training, bridging the gap between human communication patterns and current model capabilities.

## Method Summary
The proposed method employs preference optimization to train LLMs to form efficient communication conventions during multi-turn interactions. The approach uses heuristically identified examples of convention formation from human conversations as training data, combined with special planning tokens that help the model distinguish between initial mentions and re-mentions of concepts. The post-training process specifically targets the model's ability to shorten messages in subsequent turns while maintaining accuracy in communication tasks. The method is designed to be applicable across different model sizes and architectures, though the paper focuses on demonstrating improvements in efficiency and consistency rather than comprehensive capability preservation.

## Key Results
- Post-trained models achieved up to 26% reduction in message length compared to original models
- Significant improvement in consistency measured by lower word novelty distance scores
- Maintained accuracy levels while demonstrating enhanced convention formation capabilities

## Why This Works (Mechanism)
The post-training approach works by exposing models to examples of efficient convention formation patterns from human conversations, allowing them to learn the mapping between context and optimal message length. The use of planning tokens provides explicit signals about the conversational state, helping models differentiate between first-time concept introduction and subsequent references. Preference optimization enables the model to prioritize efficiency gains while preserving task accuracy, effectively teaching the model to balance informativeness with brevity.

## Foundational Learning

1. Convention Formation
   - Why needed: Humans naturally develop efficient communication protocols through repeated interactions
   - Quick check: Measure message length reduction across conversation turns

2. Preference Optimization
   - Why needed: Enables fine-tuning for specific behavioral patterns without catastrophic forgetting
   - Quick check: Compare performance before and after post-training on target tasks

3. Heuristic Identification
   - Why needed: Provides scalable method to extract convention formation examples from large conversation datasets
   - Quick check: Validate heuristic patterns against human-annotated examples

4. Planning Tokens
   - Why needed: Distinguishes between initial concept mentions and re-references for appropriate message length
   - Quick check: Test model performance with and without planning tokens

## Architecture Onboarding

Component Map:
LLM Base Model -> Post-training Module -> Enhanced Communication Model

Critical Path:
The critical path involves the preference optimization process where the model learns to map conversational contexts to efficient message patterns. This includes processing planning tokens, recognizing convention formation opportunities, and generating appropriately shortened responses while maintaining task accuracy.

Design Tradeoffs:
The approach trades some generalization capacity for specialized efficiency in communication tasks. The reliance on heuristic data extraction may introduce noise, while the use of planning tokens adds complexity to the model's input processing. The post-training process must balance efficiency gains against potential degradation of other capabilities.

Failure Signatures:
- Over-shortening leading to loss of critical information
- Inconsistent convention application across conversation turns
- Failure to recognize when new conventions should be established
- Degradation of performance on non-communication tasks

First Experiments:
1. Baseline evaluation of message length and accuracy without post-training
2. Post-training with simplified planning tokens to test core mechanism
3. A/B testing of different heuristic extraction methods for convention formation data

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Heuristic-based data extraction may introduce biases or noise into training
- Special planning tokens selection and validation not fully detailed
- Results limited to English-language conversations and two specific task types
- Evaluation focuses on efficiency and consistency but lacks comprehensive general capability assessment

## Confidence

| Claim | Confidence |
|-------|------------|
| Post-training significantly improves convention formation | High |
| Efficiency gains achieved without substantial capability degradation | Medium |
| Approach enables LLMs to communicate more like humans | Low |

## Next Checks

1. Conduct ablation studies removing different heuristic components to determine which aspects of the convention formation data most contribute to observed improvements.

2. Test the post-trained models on a wider variety of communication tasks and languages to assess generalizability beyond the two benchmarks studied.

3. Evaluate downstream task performance and user satisfaction with the post-trained models in real-world applications to determine practical utility beyond controlled benchmarks.