---
ver: rpa2
title: Language models can learn implicit multi-hop reasoning, but only if they have
  lots of training data
arxiv_id: '2505.17923'
source_url: https://arxiv.org/abs/2505.17923
tags:
- training
- data
- reasoning
- entity
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models can learn implicit
  multi-hop reasoning without explicit chain-of-thought prompting. The authors train
  GPT2-style transformers from scratch on synthetic k-hop reasoning datasets (k=2,3,4),
  where models must combine multiple facts to answer questions.
---

# Language models can learn implicit multi-hop reasoning, but only if they have lots of training data

## Quick Facts
- arXiv ID: 2505.17923
- Source URL: https://arxiv.org/abs/2505.17923
- Authors: Yuekun Yao; Yupei Du; Dawei Zhu; Michael Hahn; Alexander Koller
- Reference count: 40
- Primary result: Language models can learn implicit k-hop reasoning, but training data grows exponentially with k (×100 data for 4-hop vs ×10 for 3-hop).

## Executive Summary
This paper investigates whether language models can learn implicit multi-hop reasoning without explicit chain-of-thought prompting. The authors train GPT2-style transformers from scratch on synthetic k-hop reasoning datasets (k=2,3,4), where models must combine multiple facts to answer questions. They find that while such models can indeed learn implicit k-hop reasoning, the required training data grows exponentially with k, and the number of transformer layers must grow linearly with k. Mechanistic interpretability reveals that models solve these tasks through layer-wise lookup of intermediate "bridge entities." Curriculum learning significantly mitigates but does not eliminate the data requirements. The study concludes that language models can perform implicit reasoning, but this capability comes at substantial computational costs.

## Method Summary
The authors train GPT-2 small models (12 layers, 12 heads, 768 dim) with RoPE positional embeddings on synthetic k-hop reasoning datasets. They generate entity profiles with facts about relationships, then create k-hop questions requiring composition of multiple facts. Models are trained with AdamW optimizer (lr=5e-4, weight decay=0.1), batch size 512, and evaluated on held-out questions. They systematically vary training data budgets, model depth, and curriculum strategies to understand data efficiency and mechanistic underpinnings of implicit reasoning.

## Key Results
- Language models can learn implicit k-hop reasoning, achieving >80% accuracy on 2-hop, 3-hop, and 4-hop tasks
- Required training data grows exponentially with k (×100 data for 4-hop vs ×10 for 3-hop)
- Number of transformer layers must grow linearly with k (2-layer model fails 3-hop task)
- Mechanistic interpretability reveals layer-wise lookup of intermediate bridge entities
- Curriculum learning reduces but doesn't eliminate exponential data requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers perform k-hop reasoning through layer-wise lookup of intermediate bridge entities.
- Mechanism: Shallow transformer layers retrieve lower-hop entities (e.g., 1-hop), while deeper layers progressively retrieve higher-hop entities. The reasoning process concentrates at the last token position before answer generation.
- Core assumption: The model has sufficient training data to develop this layered circuit structure.
- Evidence anchors:
  - [abstract] "mechanistic interpretability reveals that models solve these tasks through layer-wise lookup of intermediate 'bridge entities'"
  - [section 5.2] Figure 7 shows causal interventions revealing "layer-wise lookup mechanism" where intervening on 1-hop entities affects layer 1, 2-hop affects layers 2-3, etc.
  - [corpus] Related work "How do Transformers Learn Implicit Reasoning?" investigates similar emergence mechanisms.

### Mechanism 2
- Claim: Training data requirements grow exponentially with reasoning depth k.
- Mechanism: The search space of valid fact combinations expands as |R|^k (where |R| is the number of relations), creating a combinatorial explosion that requires exponentially more examples to cover the distribution.
- Core assumption: Relations at each hop position are sampled from the same distribution and can combine freely.
- Evidence anchors:
  - [abstract] "required training data grows exponentially in k (e.g., ×100 data for 4-hop vs. ×10 for 3-hop)"
  - [section 4.3] Figure 5 shows fixing 1-hop and 2-hop relations reduces required budget to ×1, demonstrating "the main source of data inefficiency... is the exponential growth in the number of relation combinations"
  - [corpus] "Grokking in the Wild" paper notes similar data augmentation challenges for real-world multi-hop reasoning.

### Mechanism 3
- Claim: Curriculum learning builds reasoning circuits progressively, reducing (but not eliminating) data requirements.
- Mechanism: Training first establishes lower-hop circuits (e.g., 1-hop entity retrieval) in early stages; subsequent stages build higher-hop circuits atop these foundations rather than learning all circuits simultaneously.
- Core assumption: Lower-hop reasoning subproblems share computational substrates with higher-hop problems.
- Evidence anchors:
  - [abstract] "curriculum learning significantly mitigates but does not eliminate the data requirements"
  - [section 6.2] Figure 8 shows curriculum learning achieves 4-hop with ×5 budget vs. ×100 baseline; Appendix G.2 Figure 15 shows "circuits for higher-hop entities tend to be established on top of existing ones"
  - [corpus] No directly comparable curriculum learning studies in corpus.

## Foundational Learning

- **Transformer layer computation (residual stream + attention)**
  - Why needed here: The paper's mechanistic analysis relies on understanding how information flows through residual streams and how patching activations at specific layers reveals circuit structure.
  - Quick check question: Can you explain why patching the residual stream at layer 3 but not layer 1 would affect 2-hop entity retrieval?

- **k-hop reasoning task structure**
  - Why needed here: Understanding that answering "Who is r2 of r1 of e?" requires computing r1(e) → e', then r2(e') → answer is essential for interpreting the layer-wise mechanism.
  - Quick check question: For a 3-hop query r3(r2(r1(e))), identify the bridge entities and the final answer target.

- **Probing vs. causal intervention methods**
  - Why needed here: The paper distinguishes between *what information is encoded* (probing) and *what information is causally used* (activation patching); conflating these leads to incorrect conclusions.
  - Quick check question: If a probe classifier achieves 90% accuracy at layer 5 for predicting a bridge entity, what does this tell you? What does it *not* tell you?

## Architecture Onboarding

- **Component map:**
  GPT-2-small backbone (12 layers, 12 heads, 768 dim) -> RoPE positional embeddings -> Extended vocabulary for synthetic entity/relation names -> Causal language modeling objective over full prompt

- **Critical path:**
  Input prompt → entity profiles processed → final token `<space>` activates layer-wise retrieval → bridge entities computed sequentially → answer token predicted

- **Design tradeoffs:**
  - **Depth vs. k:** Theoretical bound (Theorem 5.1) suggests L ≥ k/(8pdH); fewer layers fail on higher-k tasks (Table 4 shows 2-layer model fails 3-hop).
  - **Data budget vs. task complexity:** Exponential data growth (Table 1) makes k > 4 impractical without curriculum strategies.
  - **Curriculum stages vs. training time:** More stages increase total steps but reduce required data (Table 8).

- **Failure signatures:**
  - Test accuracy ~1/|E| (random guessing): Insufficient data budget for given k
  - Accuracy plateaus at ~10-20%: Partial circuit formation but incomplete generalization
  - Probing succeeds but patching has no effect: Information encoded but not causally used (suggests memorization circuit)

- **First 3 experiments:**
  1. Replicate 2-hop vs. 3-hop data budget experiment on k-hop_small to confirm exponential scaling; verify with 3 random seeds.
  2. Run probing experiment (Section 5.1) on trained 4-hop model to identify which layers encode which bridge entities; confirm last-token concentration.
  3. Compare curriculum learning vs. mixed learning on 4-hop_large with ×5 budget; measure training curve and final accuracy to validate progressive circuit hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical lower bound on depth (Theorem 5.1) be proven without assuming attention patterns are independent of the query entity e?
- Basis in paper: [explicit] The authors state: "It remains open if this assumption can be relaxed. Intuitively, it does not seem clear how changing attention patterns could make the task easier. However, formally proving lower bounds for multi-layer transformer without either constraining attention patterns... remains challenging."
- Why unresolved: The current proof relies on limiting information flow through attention heads; removing this assumption requires new technical tools.
- What evidence would resolve it: A proof showing the same or similar depth bound without the attention pattern constraint, or a counterexample where query-dependent attention enables shallower k-hop reasoning.

### Open Question 2
- Question: Do the exponential data requirements and linear depth growth patterns persist for k-hop reasoning with k ≥ 5?
- Basis in paper: [explicit] The authors note: "Due to computational budget constraints, we also restrict our experiments to k-hop tasks with k < 5."
- Why unresolved: Computational costs become prohibitive for longer reasoning chains, making empirical verification difficult.
- What evidence would resolve it: Experiments on 5-hop and 6-hop tasks with scaled-up compute resources, tracking whether data budgets continue growing exponentially.

### Open Question 3
- Question: Can the observed layer-wise lookup mechanism and data requirements be replicated on naturalistic multi-hop reasoning datasets rather than synthetic templates?
- Basis in paper: [explicit] The authors state: "Applying the same analysis to realistic datasets is challenging due to the difficulty of collecting complex multi-hop questions (e.g. 4-hop questions) and corresponding facts."
- Why unresolved: Real datasets lack controlled structure, making it hard to isolate reasoning from other factors.
- What evidence would resolve it: Experiments on curated real-world k-hop datasets with verified fact chains, comparing data efficiency to synthetic baselines.

## Limitations

- **Mechanism generalizability:** The layer-wise lookup mechanism is well-supported for synthetic k-hop tasks but may not generalize to natural language multi-hop reasoning with diverse linguistic structures.
- **Exponential scaling boundary:** The paper demonstrates exponential data growth up to k=4 but doesn't explore whether this scaling breaks down at higher k values or if there are fundamental limits to implicit reasoning depth.
- **Curriculum learning effectiveness:** While curriculum learning reduces data requirements, the paper doesn't fully explore optimal curriculum designs or whether benefits persist at larger scales.

## Confidence

- **High confidence:** The exponential growth in training data requirements with reasoning depth k is well-established through systematic experiments across multiple configurations.
- **Medium confidence:** The layer-wise lookup mechanism is strongly supported by mechanistic interpretability, though the exact circuit details may vary with different model architectures or task formulations.
- **Medium confidence:** Curriculum learning's effectiveness is demonstrated, but the specific staging strategy used may not be optimal, and benefits could diminish at larger scales.

## Next Checks

1. **Mechanism transfer test:** Apply the same mechanistic interpretability approach (probing and activation patching) to a natural language multi-hop reasoning dataset like HotpotQA to verify if the layer-wise lookup pattern emerges.

2. **Curriculum design optimization:** Systematically vary curriculum stage boundaries and compare against random orderings to identify whether the observed benefits are due to the specific difficulty progression or just increased training time.

3. **Architecture scaling experiment:** Test whether increasing model width (instead of depth) provides similar benefits for multi-hop reasoning, potentially revealing whether the depth requirement is fundamental or an artifact of the current architecture.