---
ver: rpa2
title: 'PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing
  at Evaluation Time with LLMs'
arxiv_id: '2510.06730'
source_url: https://arxiv.org/abs/2510.06730
tags:
- pteb
- evaluation
- datasets
- table
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PTEB addresses benchmark saturation and overfitting in text embedding
  evaluation by dynamically generating meaning-preserving paraphrases at evaluation
  time using LLMs. The method uses an LLM judge to select a high-quality paraphraser
  based on semantic similarity ratings, then applies it to MTEB datasets to create
  novel test variants.
---

# PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs

## Quick Facts
- **arXiv ID:** 2510.06730
- **Source URL:** https://arxiv.org/abs/2510.06730
- **Reference count:** 26
- **Primary result:** Embedding models show consistent performance drops (avg. -3.89%) on LLM-generated paraphrased MTEB data vs. originals, revealing token-level overfitting.

## Executive Summary
PTEB introduces a novel approach to text embedding evaluation by dynamically generating meaning-preserving paraphrases at evaluation time using LLMs. The method addresses benchmark saturation and overfitting by testing whether embedding models rely on token-level shortcuts rather than true semantic understanding. Using an LLM judge to select high-quality paraphrasers, PTEB applies stochastic paraphrasing to MTEB datasets and shows that models consistently score lower on paraphrased data while maintaining semantic equivalence. The framework provides statistically robust performance estimates through multi-run aggregation and demonstrates effectiveness across 25 languages.

## Method Summary
PTEB uses a two-stage LLM pipeline where an LLM judge (Gemma-3-27b) first selects the best paraphrasing model based on semantic similarity ratings, then applies it to MTEB datasets to create novel test variants. The evaluation runs with n=6 different paraphrasing seeds to ensure statistical robustness, aggregating results using macro-averages and Hodges-Lehmann estimators with 95% confidence intervals. Human validation confirms paraphrases maintain high semantic similarity (mean 4.05/5.0 across 8 languages). The method tests embedding models on paraphrased versions of standard MTEB datasets across 7 tasks and 20 datasets in 25 languages, revealing performance sensitivity to token-level changes while preserving semantics.

## Key Results
- Embedding models consistently score lower on paraphrased data (average drop of 3.89%) compared to original benchmarks
- Smaller embedding models (4B vs 8B parameters) show similar robustness to paraphrasing
- Human evaluation confirms paraphrases maintain high semantic similarity (mean 4.05/5.0) across 8 languages
- Performance degradation is task-dependent: STS tasks show -3.90% average drop while classification shows -5.79% average drop

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated paraphrases expose token-space overfitting in embedding models while preserving semantic content.
- Mechanism: The paraphrasing process alters lexical patterns (synonym replacement, syntactic restructuring) that overfitted models rely on as shortcuts, while LLM judges validate semantic preservation (mean similarity ≥4.05/5.0 across 8 languages in human validation).
- Core assumption: Embedding models that rely on token-level shortcuts rather than semantic understanding will show performance degradation on paraphrased inputs.
- Evidence anchors: [abstract] "Across 7 MTEB tasks, we validate our hypothesis that the performance of sentence encoders is sensitive to changes in token space even when semantics remain fixed." [section 3.2.3] Average performance drops of -3.90% across STS tasks; classification shows -5.79% average drop.

### Mechanism 2
- Claim: Multi-run aggregation with stochastic paraphrasing provides statistically robust performance estimates.
- Mechanism: Six independent paraphrasing seeds generate different token variations per run; aggregating across runs (using macro-averages and Hodges-Lehmann estimators with 95% CIs) reduces single-run variance and provides effect size reliability.
- Core assumption: Variance across paraphrasing runs reflects meaningful generalization differences rather than noise.
- Evidence anchors: [section 2] "We run PTEB using n=6 different paraphrasing seeds since single-run estimates can be unreliable." [section 3.2.3] Standard deviations vary by dataset (σ≤0.09 for SICK-R vs. σ∈[1.01,2.23] for BIOSSES), justifying multi-run design.

### Mechanism 3
- Claim: Edit distance as a paraphrase selection criterion ensures sufficient token diversity to stress-test models.
- Mechanism: Selecting paraphrases with higher normalized edit distance (0.81 for Gemma-3-27b vs. 0.55 for GPT-OSS-20b) ensures lexical changes are substantial enough to disrupt token-level shortcuts while LLM judges verify semantic preservation.
- Core assumption: Larger edit distances correlate with harder evaluation cases that better expose generalization failures.
- Evidence anchors: [section 3.2.2] "Gemma-3-27b generated more textually diverse paraphrases at higher throughput: average ED 0.81 vs. 0.55 and 0.63." [section 6/Figure 8] Initial analysis shows paraphrase models generating larger EDs are associated with larger embedding model performance drops on STSBenchmark.

## Foundational Learning

- Concept: **Semantic Textual Similarity (STS) evaluation**
  - Why needed here: PTEB relies on STS datasets and metrics (Spearman's rank correlation) to validate both LLM judges and paraphrase quality; understanding bounded similarity scales (0-5) is essential.
  - Quick check question: Given two sentences "The cat sat on the mat" and "A feline rested on the rug," what STS rating (0-5) would you expect, and why might an embedding model score differently on paraphrased vs. original versions?

- Concept: **Benchmark contamination and overfitting in NLP**
  - Why needed here: PTEB's core motivation is detecting models that achieve inflated scores via data contamination or biased overtraining on static benchmarks.
  - Quick check question: If a model was trained on data that included STS12 sentences, would you expect its PTEB performance drop to be larger or smaller than a model trained only on different data? Why?

- Concept: **Statistical significance testing with multiple runs**
  - Why needed here: PTEB uses Wilcoxon signed-rank tests, Holm correction for multiple hypotheses, and Hodges-Lehmann estimators; understanding when to pool vs. aggregate runs is critical for valid conclusions.
  - Quick check question: If you run PTEB with n=6 seeds and observe consistent performance drops but high variance within a dataset (σ≈2.0), what statistical approach would strengthen your claim?

## Architecture Onboarding

- Component map: LLM Judge Selection Pipeline -> Paraphrase Model Selection Pipeline -> PTEB Evaluation Pipeline -> Human Validation Layer
- Critical path: LLM judge selection → Paraphraser selection → Multi-run PTEB evaluation → Statistical aggregation → Human validation (for new languages)
- Design tradeoffs:
  - **Judge/paraphraser separation vs. same-model**: Using same model (Gemma-3-27b) for both risks within-model bias; authors mitigate via cross-model-family validation but acknowledge limitation.
  - **Edit distance vs. semantic similarity**: Higher ED improves stress-testing but risks semantic drift; current threshold (≥0.81) empirically validated but not systematically optimized.
  - **Multi-run cost vs. statistical power**: n=6 seeds balances reliability with compute; more seeds improve CI precision but with diminishing returns.
  - **English-only judge selection vs. multilingual evaluation**: Judge/paraphraser selected on English STS; non-English relies on human validation (mean ≥3.34 for 8 languages, but 13% of Hausa pairs scored 0.0).
- Failure signatures:
  - **Low human similarity ratings (<3.0)**: Indicates paraphrases alter semantics; especially risky for low-resource languages (observed in Hausa, Swahili, French).
  - **High variance across runs (σ>1.5)**: Suggests paraphrases are inconsistent; may indicate unstable LLM outputs or prompt sensitivity.
  - **No performance drop vs. original**: Either model is genuinely robust OR paraphrases insufficiently diverse (check ED).
  - **LLM judge MAE spikes at similarity extremes**: Judges struggle with near-identical or completely dissimilar pairs; may misrate paraphrase quality.
- First 3 experiments:
  1. **Reproduce LLM judge selection on a held-out STS dataset**: Run Gemma-3-27b, GPT-OSS-20b, and Qwen3-32b on STS22 (long texts, avg 461 words) to verify judge performance transfers to longer inputs where current judges show lower correlation (31.59%-54.26% for ≥20 words).
  2. **Single-language PTEB pilot with human validation loop**: Run PTEB on one English classification dataset (e.g., Banking77) with n=6 seeds, compute performance drops, and validate a random sample of 50 paraphrases with human raters to confirm judge-paraphraser separation is not inflating semantic similarity scores.
  3. **Edit distance ablation**: Generate paraphrases at 3 ED levels (low ~0.4, medium ~0.6, high ~0.8) by adjusting paraphrase prompts, evaluate embedding models at each level, and plot ED vs. performance drop to validate that higher ED produces larger drops (testing the mechanism in Section 6/Figure 8).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the choice of paraphrasing model or the specific style of paraphrase significantly alter the final ranking of embedding models?
- Basis in paper: [explicit] The authors state in the Limitations section that "future work should examine whether the paraphrasing style itself affects embedding model rankings by running PTEB with different paraphrasers."
- Why unresolved: The study relied exclusively on `gemma3-27b` for generating all evaluation paraphrases, leaving the sensitivity of the benchmark to the generator's specific idiosyncrasies unknown.
- What evidence would resolve it: A correlation analysis of embedding model rankings derived from PTEB instances generated by multiple, distinct LLMs (e.g., comparing Gemma against GPT-OSS or Qwen).

### Open Question 2
- Question: Is the performance degradation in classification tasks primarily driven by the disruption of specific keyword triggers rather than a loss of semantic understanding?
- Basis in paper: [inferred] The authors "hypothesise that this occurs because classification relies heavily on specific keyword triggers that are disrupted by paraphrasing" when discussing the large performance drops in classification tasks.
- Why unresolved: This explanation is presented as a hypothesis to explain the observed data; the paper does not perform a controlled ablation to isolate the cause of the performance drop.
- What evidence would resolve it: An ablation study comparing model performance on paraphrases that specifically preserve keywords versus those that synonyms replace them.

### Open Question 3
- Question: Is there a quantifiable relationship between the edit distance of a paraphrase and the severity of the performance drop in embedding models?
- Basis in paper: [inferred] The authors note "initial evidence" that paraphrasers generating larger edit distances correlate with larger performance drops, but state "despite the need for further analysis."
- Why unresolved: This observation was based on a single analysis using the STSB dataset; the paper does not provide a comprehensive statistical validation of this relationship across all tasks.
- What evidence would resolve it: A regression analysis plotting normalized edit distance against the performance delta ($\Delta$) across the full PTEB suite.

## Limitations
- **Single-paraphraser bias**: All evaluations use Gemma-3-27b, leaving sensitivity to paraphraser choice unexplored
- **English-centric judge selection**: Judge and paraphraser selected on English STS data, potentially limiting generalization to other languages
- **Keyword trigger hypothesis**: Classification performance drops attributed to keyword disruption without controlled ablation studies

## Confidence
- **Method validity**: High - Clear pipeline, statistical rigor, human validation
- **Generalization claims**: Medium - Limited paraphraser diversity, English-centric design
- **Interpretability of results**: Medium - Mechanisms proposed but not all experimentally validated

## Next Checks
1. **Reproduce LLM judge selection** on a held-out STS dataset (e.g., STS22) to verify judge performance transfers to longer inputs
2. **Single-language PTEB pilot** with human validation loop on one English classification dataset to confirm judge-paraphraser separation
3. **Edit distance ablation study** across 3 ED levels to validate relationship between lexical diversity and performance drops