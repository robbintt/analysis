---
ver: rpa2
title: 'CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing
  in conversations'
arxiv_id: '2506.08504'
source_url: https://arxiv.org/abs/2506.08504
tags:
- discourse
- relation
- comumdr
- link
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoMuMDR, a large-scale code-mixed (Hindi-English)
  multimodal (text+audio) multi-domain corpus for discourse parsing in conversations.
  It contains 799 dialogues from customer call center interactions across five domains,
  annotated with nine discourse relation types.
---

# CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations

## Quick Facts
- **arXiv ID:** 2506.08504
- **Source URL:** https://arxiv.org/abs/2506.08504
- **Reference count:** 40
- **Key outcome:** Introduces CoMuMDR, a large-scale code-mixed (Hindi-English) multimodal (text+audio) multi-domain corpus for discourse parsing in conversations.

## Executive Summary
This paper introduces CoMuMDR, a large-scale code-mixed (Hindi-English) multimodal (text+audio) multi-domain corpus for discourse parsing in conversations. It contains 799 dialogues from customer call center interactions across five domains, annotated with nine discourse relation types. Experiments with state-of-the-art discourse parsing models show significantly lower performance on CoMuMDR compared to existing English-only datasets (STAC, Molweni), highlighting the challenges of multi-domain code-mixed conversations. GPT-4o also underperforms, emphasizing the need for specialized models. The corpus aims to support research in realistic, multilingual dialogue understanding.

## Method Summary
The method involves creating a code-mixed (Hindi-English) multimodal (text+audio) multi-domain corpus for discourse parsing. The corpus contains 799 dialogues from customer call center interactions across five domains (e-commerce, banking, pharma, telecom, and entertainment). Each dialogue is annotated with nine discourse relation types. The authors evaluate state-of-the-art discourse parsing models (Deep Sequential, Hierarchical, Structure-aware, SSP-BERT+SCIJE, and SDDP) on this dataset using text embeddings (English-only and multilingual) and release audio features for future multimodal research.

## Key Results
- State-of-the-art discourse parsers show significantly lower performance on CoMuMDR compared to English-only datasets (STAC, Molweni).
- SSP-BERT+SCIJE with multilingual embeddings achieves the best Link+Relation F1 score (0.5547) on CoMuMDR.
- GPT-4o underperforms SoTA models, emphasizing the need for specialized discourse parsing approaches for code-mixed conversations.
- Models fail to predict rare discourse relations like "Correction" and "Conditional" (F1=0.00), indicating challenges with class imbalance.

## Why This Works (Mechanism)

### Mechanism 1: Domain and Linguistic Distribution Shift
- **Claim:** SoTA discourse parsers fail on CoMuMDR primarily because their training priors (monolingual English, single-domain) do not generalize to multi-domain, code-mixed (Hindi-English) dialogues.
- **Mechanism:** Models trained on datasets like STAC (Catan game) or Molweni (Ubuntu chats) learn domain-specific lexical patterns and syntactic structures. When exposed to CoMuMDR, the vocabulary (Hinglish) and topic variance (e-commerce, pharma, etc.) disrupt the embedding spaces (e.g., RoBERTa), causing a misalignment between input features and learned decision boundaries.
- **Core assumption:** The performance drop is causal due to the linguistic and domain shift, rather than solely the dataset size.
- **Evidence anchors:** [abstract]: "Experiments with... SoTA baseline models show significantly lower performance on CoMuMDR... highlighting the challenges of multi-domain code-mixed conversations." [section 4 Results]: "CoMuMDR has the lowest performance [in relation classification], possibly due to the presence of multiple domains and the challenge of domain adaptation." [corpus]: Corpus signals support the existence of the domain shift (e.g., comparison to single-domain STAC/Molweni), though isolating "code-mixing" as the sole causal variable versus "conversational style" is not explicitly disentangled in ablations.

### Mechanism 2: Label Taxonomy Simplification
- **Claim:** Reducing the discourse label set from the standard 17 (SDRT) to 9 specific relations improves annotation feasibility for two-party customer service contexts.
- **Mechanism:** In spoken, task-oriented dialogues, fine-grained distinctions (e.g., Narration vs. Continuation) are often ambiguous or semantically overlapping. Merging these into broader categories (e.g., merging Alternation and Conditional) reduces annotator cognitive load and subjectivity, yielding a more consistent ground truth.
- **Core assumption:** The merged labels capture the essential logical flow without losing critical discourse information required for downstream tasks.
- **Evidence anchors:** [section 3 CoMuMDR Creation]: "In a pilot annotation experiment, we found that several discourse labels conveyed overlapping meanings. Hence, we merged them to get nine discourse labels." [section 3]: "We removed the Narration discourse label as it was not required in two-party customer conversations." [corpus]: Weak direct evidence; the Inter-Annotator Agreement (Structured Kappa 0.4044) is moderate, suggesting significant ambiguity remains despite simplification.

### Mechanism 3: Audio as a Diarization Corrective (Proposed)
- **Claim:** Incorporating audio features is posited to resolve discourse segmentation errors arising from imperfect text diarization.
- **Mechanism:** Text-only pipelines rely on ASR and diarization models which frequently split single speaker turns or merge overlapping speech. Audio signals (pauses, speaker timbre) provide orthogonal cues to verify segment boundaries where text transcripts fail.
- **Core assumption:** Audio features retain sufficient signal quality to distinguish speakers and turn boundaries better than the text transcripts alone.
- **Evidence anchors:** [section 3]: "We added another relation termed Diarization Continuation to fix the diarization issues... We plan to use these annotations to improve the diarization model." [section 1]: "The motivation... is to create a practical... system that handles audio conversations and is robust to transcription and diarization errors." [corpus]: Evidence is currently weak/absent for *performance improvement* via audio, as the paper releases only text/transcripts due to privacy, and experiments are text-only.

## Foundational Learning

- **Concept: Elementary Discourse Units (EDUs)**
  - **Why needed here:** Discourse parsing operates on EDUs (clauses), not just utterances. Understanding EDUs is required to interpret the "Link + relation" metrics versus simple utterance classification.
  - **Quick check question:** Can a single utterance contain multiple EDUs, and if so, how does that affect the graph structure?

- **Concept: Speaker Diarization**
  - **Why needed here:** The corpus creation relies on diarization to split audio into utterances. Errors in this step create noise (the "Diarization Continuation" relation) that models must handle.
  - **Quick check question:** What happens to the discourse graph if a single speaker's turn is incorrectly split into two utterances by the diarization model?

- **Concept: Code-mixing (Hinglish)**
  - **Why needed here:** Standard English embeddings (RoBERTa) underperform here. You must understand why multilingual embeddings (paraphrase-xlm-r) are benchmarked as an alternative.
  - **Quick check question:** Why might a monolingual English model fail to capture the semantic relationship in a sentence like "Mujhe return karna hai" (I want to return it)?

## Architecture Onboarding

- **Component map:** Raw Audio -> [ASR + Diarization Pipeline] -> Raw Text -> [Anonymization] -> **Annotated Corpus (CoMuMDR)** -> [Discourse Parser (e.g., SSP-BERT, SDDP)] -> Discourse Graph.

- **Critical path:** The **Link Prediction** task is the precursor to **Relation Classification**. If the model fails to predict a link between EDUs (low Link F1), the relation classification performance is structurally capped.

- **Design tradeoffs:**
  - **Tree vs. DAG:** The SDDP model assumes a tree structure (discarding edges), whereas CoMuMDR relations naturally form Directed Acyclic Graphs (DAGs). This assumption causes SDDP to fail on this dataset.
  - **Label Granularity:** The authors trade the richness of the 17-label SDRT standard for the consistency of 9 merged labels, potentially losing nuance (e.g., distinct "Correction" vs "Contrast").

- **Failure signatures:**
  - **SDDP Collapse:** Near-zero F1 scores in "Link + Relation" (Table 3) because the tree-constraint conflicts with the multi-parent nature of dialogue replies (DAGs).
  - **Rare Class Blindness:** Models fail to predict rare relations like "Correction" or "Conditional" (F1=0.00 in Table 4) even with weighted loss, indicating the embeddings do not capture these sparse features.

- **First 3 experiments:**
  1. **Establish Baseline:** Reproduce the SSP-BERT + SCIJE results using the provided text embeddings to verify the data loading and evaluation pipeline matches the paper's F1 scores.
  2. **Ablation on Embeddings:** Compare English-only (RoBERTa) vs. Multilingual (XLM-R) embeddings on the Link Prediction task to quantify the impact of code-mixing on the model's structural understanding.
  3. **Error Analysis on SDDP:** Run SDDP and analyze the discarded edges to confirm if the tree assumption specifically removes high-confidence "Background" or "Acknowledgment" links that create DAG structures.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does incorporating the audio modality improve performance on discourse parsing tasks compared to text-only baselines in code-mixed conversations?
  - **Basis in paper:** [explicit] The authors state: "In the future, we plan to develop more advanced models incorporating audio modality information."
  - **Why unresolved:** All reported baseline experiments (Deep Sequential, Hierarchical, SDDP, etc.) utilized only text embeddings (English or multilingual), leaving the audio features released in the dataset completely unexplored.
  - **What evidence would resolve it:** A multimodal model architecture that fuses the released audio features with text and demonstrates a statistically significant F1-score increase over the text-only SSP-BERT or Struct-aware baselines.

- **Open Question 2:** Can specialized modeling techniques (e.g., focal loss, hierarchy-aware classification) resolve the complete failure of current models to predict rare discourse relations?
  - **Basis in paper:** [inferred] Table 4 shows that despite using weighted loss, models yield 0.0 F1-scores on low-support relations like "Correction" and "Conditional," suggesting standard training procedures are insufficient for the dataset's distribution.
  - **Why unresolved:** The paper identifies this failure mode in the error analysis but does not experiment with specific architectural changes or sampling strategies designed to mitigate extreme class imbalance.
  - **What evidence would resolve it:** Successful prediction of "Correction" or "Conditional" relations (F1 > 0) by a model specifically trained with techniques targeting long-tail data distributions.

- **Open Question 3:** How can Large Language Models (LLMs) be effectively prompted or fine-tuned to distinguish between semantically overlapping relations (e.g., Question Extension vs. Continuation) in code-mixed dialogues?
  - **Basis in paper:** [inferred] The analysis of GPT-4o reveals significant misclassification between overlapping semantic classes, which the authors attribute to the "overlapping semantics of these relations in a two-party conversation."
  - **Why unresolved:** The paper demonstrates the problem (GPT-4o underperforms SoTA) but does not determine if this is a fundamental limitation of the model's reasoning or a failure of the specific 3-shot prompting strategy used.
  - **What evidence would resolve it:** A fine-tuning or chain-of-thought prompting experiment that improves GPT-4o's confusion matrix specifically for the "Question Extension" and "Continuation" classes.

## Limitations

- The paper does not isolate the effects of domain shift and code-mixing, making it unclear which factor primarily contributes to performance degradation.
- Audio features are released but not utilized in experiments, leaving the proposed mechanism for improving diarization unresolved.
- Moderate Inter-Annotator Agreement (Structured Kappa 0.4044) suggests potential subjectivity and inconsistency in the discourse relation annotations.

## Confidence

- **High Confidence:** The existence of the CoMuMDR corpus and its basic statistics (799 dialogues, 9 relation types) are well-established.
- **Medium Confidence:** The claim that SoTA discourse parsers perform significantly worse on CoMuMDR is supported by experiments, but the exact attribution to code-mixing vs. domain shift requires further investigation.
- **Low Confidence:** The proposed mechanism that audio features will resolve diarization issues lacks empirical validation in the current work.

## Next Checks

1. **Ablation Study Design:** Create a controlled experiment using a subset of CoMuMDR dialogues with only Hindi-English code-mixing but from a single domain (e.g., only e-commerce), then compare model performance to the full multi-domain version to isolate the code-mixing effect.
2. **Audio Feature Integration:** Implement and evaluate a simple model that uses both text and audio features (e.g., concatenating text embeddings with audio embeddings) to quantify the actual improvement in discourse parsing accuracy, particularly for the Diarization Continuation relation.
3. **Annotation Reliability Test:** Conduct a second round of annotations on a subset of 50 dialogues by the same annotators after a time gap, then compute the Kappa score again to assess the stability and reliability of the discourse relation labels.