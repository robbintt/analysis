---
ver: rpa2
title: One-Shot Federated Unsupervised Domain Adaptation with Scaled Entropy Attention
  and Multi-Source Smoothed Pseudo Labeling
arxiv_id: '2503.10020'
source_url: https://arxiv.org/abs/2503.10020
tags:
- domain
- target
- adaptation
- federated
- mspl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses federated unsupervised domain adaptation (FUDA),
  where multiple clients collaboratively train a model for an unlabeled target domain
  without sharing data. The main challenges tackled are domain shift, high communication
  overhead, and imbalanced model contributions.
---

# One-Shot Federated Unsupervised Domain Adaptation with Scaled Entropy Attention and Multi-Source Smoothed Pseudo Labeling

## Quick Facts
- arXiv ID: 2503.10020
- Source URL: https://arxiv.org/abs/2503.10020
- Authors: Ali Abedi; Q. M. Jonathan Wu; Ning Zhang; Farhad Pourpanah
- Reference count: 40
- Primary result: Up to 5.3% improvement in mean accuracy over state-of-the-art FUDA methods on standard image benchmarks

## Executive Summary
This paper addresses federated unsupervised domain adaptation (FUDA), where multiple clients collaboratively train a model for an unlabeled target domain without sharing data. The proposed method introduces Scaled Entropy Attention (SEA) for aggregating source models based on their prediction confidence on the target domain and Multi-Source Pseudo Labeling (MSPL) with Smoothed Soft-Label Cross-Entropy (SSCE) to adapt the global model to the target. SEA assigns higher weights to models with lower entropy (higher confidence), while MSPL generates pseudo labels and refines the model using SSCE to manage label noise. Experiments on four benchmarks show SEA + MSPL outperforms state-of-the-art methods while reducing communication overhead by freezing the backbone during training.

## Method Summary
The method tackles federated unsupervised domain adaptation through two key innovations: Scaled Entropy Attention for model aggregation and Multi-Source Smoothed Pseudo Labeling for target adaptation. SEA computes attention weights for each client's model based on the entropy of their predictions on the target domain, giving higher influence to more confident predictions. MSPL generates pseudo labels by aggregating predictions from all clients using their SEA weights, then applies Smoothed Soft-Label Cross-Entropy to train the global model on the target domain while mitigating label noise effects. The framework maintains efficiency by freezing the backbone during local training and only communicating the classifier layers.

## Key Results
- SEA + MSPL achieves up to 5.3% improvement in mean accuracy over state-of-the-art FUDA methods
- Significant performance gains across all four benchmark datasets (OfficeHome, Office-31, Office-Caltech, DomainNet)
- Reduces communication overhead by freezing the backbone during training
- Outperforms baseline methods in both accuracy and efficiency metrics

## Why This Works (Mechanism)
The method works by intelligently combining multiple source models while minimizing the negative effects of domain shift and label noise. SEA addresses the aggregation challenge by weighting models based on their confidence on the target domain - models that make more certain predictions (lower entropy) are trusted more. This prevents overconfident but incorrect models from dominating the adaptation process. MSPL then uses these weighted predictions to generate pseudo labels, while SSCE smooths the label distribution to reduce the impact of inevitable labeling errors. The combination allows the global model to effectively learn from all sources while being robust to noise and domain differences.

## Foundational Learning
- **Federated Learning**: Distributed model training where clients keep data local - needed to enable privacy-preserving multi-source adaptation without data sharing
- **Unsupervised Domain Adaptation**: Adapting models to new domains without labeled target data - essential for handling the unlabeled target scenario
- **Entropy-based confidence scoring**: Using prediction entropy as a quality metric - critical for SEA's attention mechanism
- **Pseudo-labeling**: Using model predictions as training labels - fundamental to MSPL's target adaptation approach
- **Smoothed label cross-entropy**: Modified loss function to handle noisy labels - necessary for robust pseudo-label training
- **Model aggregation**: Combining multiple models from different sources - core to the federated learning framework

## Architecture Onboarding
**Component Map**: Client Models -> SEA Module -> Weighted Aggregation -> MSPL Module -> Global Model -> Target Adaptation

**Critical Path**: Client predictions on target -> SEA entropy computation -> attention weight calculation -> weighted pseudo-label generation -> SSCE-based training

**Design Tradeoffs**: 
- Model freezing for efficiency vs. full fine-tuning for performance
- SEA confidence-based weighting vs. simpler averaging approaches
- SSCE smoothing strength vs. label fidelity preservation

**Failure Signatures**: 
- Poor performance if all source models have similar high entropy
- Degradation when source domains are too dissimilar
- Label noise accumulation if SSCE smoothing is insufficient

**First Experiments**:
1. Ablation test: SEA vs. equal-weight averaging for model aggregation
2. Baseline comparison: MSPL with standard cross-entropy vs. SSCE
3. Efficiency test: Communication cost with backbone freezing vs. full model updates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to image classification tasks, unclear generalizability to other domains
- Performance depends on source domain diversity and similarity to target
- Assumes reasonable entropy separation between good and poor models
- No theoretical convergence guarantees provided

## Confidence
- Empirical results and benchmark comparisons: **High** - controlled experiments with clear metrics
- Theoretical convergence guarantees: **Medium** - practical effectiveness shown but formal analysis limited
- Communication efficiency claims: **Medium** - reduction demonstrated but not benchmarked against alternatives
- Generalizability across domains/tasks: **Low** - no evidence beyond image classification

## Next Checks
1. Test robustness to backbone architecture changes (e.g., MobileNet, EfficientNet) and varying model capacity
2. Evaluate performance on non-image domains (text classification or speech recognition) to assess generalizability
3. Conduct ablation studies isolating SEA's contribution from pseudo-labeling benefits, including comparison with simpler aggregation methods like averaging