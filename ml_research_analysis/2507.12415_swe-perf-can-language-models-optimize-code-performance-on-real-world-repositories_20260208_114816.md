---
ver: rpa2
title: 'SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?'
arxiv_id: '2507.12415'
source_url: https://arxiv.org/abs/2507.12415
tags:
- performance
- code
- swe-perf
- unit
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SWE-Perf introduces the first benchmark for evaluating language
  models' ability to optimize code performance in real-world repositories. The benchmark
  consists of 140 curated instances from popular GitHub repositories, each containing
  a codebase, target functions, performance tests, expert patches, and executable
  environments.
---

# SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?

## Quick Facts
- arXiv ID: 2507.12415
- Source URL: https://arxiv.org/abs/2507.12415
- Authors: Xinyi He; Qian Liu; Mingzhe Du; Lin Yan; Zhijie Fan; Yiming Huang; Zejian Yuan; Zejun Ma
- Reference count: 29
- Primary result: Introduces first benchmark for evaluating language models' ability to optimize code performance in real-world repositories

## Executive Summary
SWE-Perf introduces the first benchmark for evaluating language models' ability to optimize code performance in real-world repositories. The benchmark consists of 140 curated instances from popular GitHub repositories, each containing a codebase, target functions, performance tests, expert patches, and executable environments. Models are evaluated on their ability to generate patches that improve runtime performance, with results compared against expert-level optimizations. Evaluations across leading LLMs under file-level (oracle) and repo-level (realistic) settings reveal substantial performance gaps compared to expert solutions, highlighting significant opportunities for advancing automated performance optimization capabilities in software engineering.

## Method Summary
The benchmark uses a 5-phase data collection pipeline to curate 140 instances from GitHub PRs with measurable performance improvements. Each instance includes a codebase, target functions (Oracle/Realistic), performance tests, expert patches, and Docker environments. Models are evaluated using a three-level framework: Apply (patch applies successfully), Correctness (all tests pass), and Performance (statistically significant minimum gain via Mann-Whitney U test with p<0.1 threshold). The benchmark tests both file-level (oracle) and repository-level (realistic) settings using different prompting strategies and evaluation frameworks.

## Key Results
- SWE-Perf establishes the first benchmark for real-world code performance optimization evaluation
- OpenHands agent-based approach outperforms direct prompting and pipeline methods (2.26% vs 0.41% performance)
- Substantial performance gaps remain between models and expert patches across all evaluated LLMs
- Models show flat performance on longer runtimes while expert performance continues to improve

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A rigorous statistical pipeline can reliably identify stable, performance-improving code changes from real-world PRs.
- Mechanism: The 5-phase collection pipeline uses repeated measurements (3 initial, 20 verification runs), IQR outlier filtering, and Mann-Whitney U tests (p < 0.1) to ensure improvements are statistically significant (δ > 0.05) rather than noise.
- Core assumption: Human-authored performance PRs with measurable gains represent valid, learnable optimization patterns.
- Evidence anchors: Section 3.2, Algorithm 1, and corpus validation with SWE-fficiency.

### Mechanism 2
- Claim: Providing target functions as optimization scope enables tractable evaluation while maintaining task realism.
- Mechanism: Two setting types—Oracle (file-level, directly modified functions from expert patch via AST + diff matching) and Realistic (repo-level, functions measured during test execution via yappr)—constrain the search space.
- Core assumption: Constraining to target functions doesn't artificially simplify the core optimization challenge.
- Evidence anchors: Section 3.1, Section 3.2 Phase 5, and corpus validation with DependEval.

### Mechanism 3
- Claim: Agent-based approaches (OpenHands) outperform direct model prompting and pipeline methods for repo-level performance optimization.
- Mechanism: OpenHands enables iterative reasoning, multi-step exploration, and tool use, allowing recovery from suboptimal initial choices.
- Core assumption: The performance gap reflects genuine capability differences, not just longer compute time.
- Evidence anchors: Table 2 showing OpenHands vs Agentless performance, Figure 5 showing performance trends, and corpus validation with GA4GC.

## Foundational Learning

- Concept: Repository-level code context and dependencies
  - Why needed here: Unlike function-level benchmarks, SWE-Perf requires understanding cross-file dependencies, imports, and module interactions.
  - Quick check question: Given a function `process_data()` that imports utilities from three other files, can you identify which file modifications would affect its runtime without breaking imports?

- Concept: Statistical performance measurement methodology
  - Why needed here: The benchmark uses δ (conservative minimum significant gain) rather than raw speedup percentages.
  - Quick check question: If a patch shows 15% average speedup but high variance across 20 runs, would Algorithm 1 likely report δ > 0.10? Why or why not?

- Concept: Performance optimization patterns vs. correctness preservation
  - Why needed here: The Correctness metric creates a dual-objective problem where models must optimize without breaking functionality.
  - Quick check question: A proposed optimization replaces O(n²) with O(n log n) but changes floating-point precision behavior. Should this count as a success under SWE-Perf's evaluation?

## Architecture Onboarding

- Component map:
  Data Collection Pipeline (5 phases) -> Docker Environment Builder -> pytest Executor -> Statistical Validator -> Target Extractor
  -> Benchmark Instance (Codebase + Docker + Targets + Tests + Expert Patch)
  -> Evaluation Framework (Apply/Correctness/Performance Metrics)
  -> Baseline Systems (Direct Prompting, Agentless, OpenHands)

- Critical path:
  1. Docker environment setup for each codebase (many fail here—only 19,499/34,397 succeeded in Phase 2)
  2. pytest execution with runtime capture (most time-consuming; xarray averages 58+ minutes per codebase)
  3. Statistical validation (20 repetitions + IQR filtering + Mann-Whitney U)
  4. Target function extraction via AST parsing and dynamic execution tracing (yappr)

- Design tradeoffs:
  - Target function scope: Narrower scope (Oracle) enables cleaner evaluation but reduces realism; broader scope (Realistic) increases difficulty and computational cost
  - Performance threshold (δ > 0.05): Higher thresholds yield fewer but more confident instances; lower thresholds include marginal improvements with more noise
  - Expert patch as gold standard: Human patches may not be optimal—potentially underestimating achievable performance ceilings

- Failure signatures:
  - Apply failures (30-50% for some models): Patches conflict with existing code structure, incorrect line numbers, malformed SEARCH/REPLACE blocks
  - Correctness failures: Optimization breaks edge cases, alters return values, modifies test behavior
  - Performance plateau: Models show flat performance as original runtime increases
  - Keyword analysis reveals shallow optimizations: Model patches focus on configuration/import changes; expert patches modify domain-specific computational logic

- First 3 experiments:
  1. Reproduce baseline on 10-instance subset: Run Claude-3.7-sonnet in Oracle setting on randomly sampled instances; verify Apply/Correctness/Performance metrics match reported ranges.
  2. Ablate target function information: Compare performance when providing Oracle targets vs. Realistic targets vs. no target guidance (just "optimize the repository").
  3. Analyze failure modes: For instances where models achieve <50% of expert performance, manually inspect expert patches. Categorize optimization types and identify which categories models systematically miss.

## Open Questions the Paper Calls Out
- Can language models identify and optimize performance bottlenecks in entire repositories without being provided with specific target functions or unit tests?
- What mechanisms can enable models to maintain optimization performance as the number of target functions or the codebase runtime increases?
- What is the true upper bound of optimization performance for the tasks in SWE-Perf, given that expert human patches may not be optimal?

## Limitations
- Dataset and codebase not publicly available, preventing independent validation of results
- Benchmark focuses on GitHub PRs with measurable improvements, potentially excluding optimization opportunities requiring larger architectural changes
- Expert patches may not represent optimal achievable performance, potentially underestimating true upper bounds

## Confidence
- High confidence: The statistical methodology (Algorithm 1) for measuring performance improvements is well-specified and reproducible given the dataset
- Medium confidence: The claim that OpenHands outperforms direct prompting and pipeline methods is supported by quantitative results
- Low confidence: The generalizability of findings beyond the 9 repositories in the benchmark

## Next Checks
1. Reimplement Algorithm 1 and validate it produces identical δ values for a subset of instances using publicly available runtime data or synthetic data
2. Design an ablation study where OpenHands is limited to the same number of iterations as Agentless to determine whether the performance advantage stems from architectural capabilities or simply more exploration attempts
3. Evaluate a subset of SWE-Perf instances on a different but related repository not in the original 9, measuring whether performance trends remain consistent