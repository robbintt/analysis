---
ver: rpa2
title: 'OTCR: Optimal Transmission, Compression and Representation for Multimodal
  Information Extraction'
arxiv_id: '2511.14766'
source_url: https://arxiv.org/abs/2511.14766
tags:
- information
- visual
- multimodal
- text
- otcr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OTCR, a two-stage framework for multimodal
  information extraction that addresses the challenge of uncontrolled information
  transmission and redundancy in cross-modal fusion. The method first employs cross-modal
  optimal transport to establish sparse, probabilistic alignments between text tokens
  and visual patches, using a context-aware gating mechanism to selectively inject
  visual information.
---

# OTCR: Optimal Transmission, Compression and Representation for Multimodal Information Extraction

## Quick Facts
- arXiv ID: 2511.14766
- Source URL: https://arxiv.org/abs/2511.14766
- Reference count: 0
- Primary result: On FUNSD, OTCR achieves 91.95% SER and 91.13% RE; on XFUND (ZH), it reaches 91.09% SER and 94.20% RE.

## Executive Summary
This paper proposes OTCR, a two-stage framework for multimodal information extraction that addresses the challenge of uncontrolled information transmission and redundancy in cross-modal fusion. The method first employs cross-modal optimal transport to establish sparse, probabilistic alignments between text tokens and visual patches, using a context-aware gating mechanism to selectively inject visual information. Then, it applies a variational information bottleneck to compress fused representations and filter task-irrelevant noise. On FUNSD, OTCR achieves 91.95% SER and 91.13% RE, while on XFUND (ZH), it reaches 91.09% SER and 94.20% RE, demonstrating competitive performance. Feature-level analyses confirm reduced modality redundancy and strengthened task signals.

## Method Summary
OTCR is a two-stage framework that first uses cross-modal optimal transport with a context-aware gate to selectively fuse text and visual information, then applies a variational information bottleneck to compress the fused representation. The method operates on OCR-extracted tokens and document images, using LayoutLMv3 as the backbone encoder. The optimal transport stage computes sparse alignments between text tokens and visual patches via multi-head Sinkhorn iterations, while the gating mechanism controls visual injection based on alignment confidence. The VIB stage compresses representations by regularizing the latent space toward a Gaussian prior, filtering task-irrelevant noise.

## Key Results
- On FUNSD dataset: 91.95% SER and 91.13% RE F1 scores
- On XFUND (ZH): 91.09% SER and 94.20% RE F1 scores
- Ablation study shows VIB contributes ~0.6 SER improvement when removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal transport alignment provides sparse, probabilistic cross-modal matching that reduces indiscriminate visual injection.
- Mechanism: Multi-head Sinkhorn algorithm computes doubly stochastic transport plans between text tokens and visual patches. Each head learns distinct alignment principles via parametric cost functions combining semantic similarity and spatial proximity. The transport plan π_ij defines probability mass from visual patch j to text token i, yielding globally consistent soft alignments.
- Core assumption: Text and visual modalities have asymmetric contributions—text dominates, vision supplements selectively.
- Evidence anchors:
  - [abstract]: "Cross-modal Optimal Transport (OT) yields sparse, probabilistic alignments between text tokens and visual patches, with a context-aware gate controlling visual injection."
  - [Section 3.3]: "By alternately scaling the rows and columns, Sinkhorn avoids collapse into a few matches and yields a globally consistent and mass-conserving soft alignment."
  - [corpus]: Weak direct evidence—neighboring papers discuss compression and multimodal systems but not OT specifically for MIE alignment.
- Break condition: If text and visual features share no semantic correspondence (e.g., purely decorative images), transport cost matrix becomes uninformative and alignment degrades to noise.

### Mechanism 2
- Claim: Context-aware gating adaptively controls visual contribution based on alignment confidence, preventing semantic dilution.
- Mechanism: Gate value g_i computed from original text embedding, fused feature, and alignment entropy. Entropy conf_i = -Σ_j P_ij log P_ij measures matching uncertainty. High entropy → low confidence → smaller gate → preserve text semantics. Final representation: T'_i = g_i · F_fusion(i) + (1-g_i) · T_i.
- Core assumption: Alignment entropy correlates with reliability of visual-text matching for the downstream task.
- Evidence anchors:
  - [Section 3.3]: "A higher entropy means lower confidence and thus a smaller gate value... when the alignment is confident, the model incorporates more visual evidence; when the alignment is uncertain, the model preserves the original text semantics."
  - [Section 3.3, Eq. 9-10]: Formal gate computation combining three signals per token.
  - [corpus]: No direct corpus validation; mechanism is paper-specific.
- Break condition: If alignment entropy does not reflect task-relevance (e.g., high entropy but useful visual cues), gating may incorrectly suppress beneficial visual features.

### Mechanism 3
- Claim: Variational information bottleneck compresses fused representations by collapsing task-irrelevant dimensions to the prior.
- Mechanism: Encoder parameterizes Gaussian posterior q(Z|T'). KL regularization pushes posterior toward isotropic N(0,I) prior. Loss: L = L_task + β · KL[q(Z|T') || N(0,I)]. Irrelevant dimensions collapse (μ→0, σ²→1); relevant dimensions retain non-zero means and lower variances.
- Core assumption: Task-relevant and task-irrelevant information occupy separable dimensions in latent space.
- Evidence anchors:
  - [abstract]: "Variational Information Bottleneck (VIB) compresses fused features, filtering task-irrelevant noise to produce compact, task-adaptive representations."
  - [Section 3.4]: "Notably, the closed-form KL solution improves interpretability: irrelevant dimensions collapse to the prior (μ→0 and σ²→1), while relevant ones keep non-zero means and lower variances."
  - [Section 4.4]: Ablation shows VIB removal reduces SER by 0.61 and RE by 0.48 on FUNSD.
  - [corpus]: Indirect support—neighbor papers on compression (Glyph, Q-KVComm) address redundancy but in different contexts (KV cache, long-context).
- Break condition: If β is too high, bottleneck over-compresses and discards task-relevant signals; if too low, redundancy persists.

## Foundational Learning

- Concept: Optimal Transport (Sinkhorn algorithm)
  - Why needed here: Core to Stage 1 alignment—computes minimum-cost transport plans between discrete distributions efficiently via entropy regularization.
  - Quick check question: Can you explain why Sinkhorn produces a doubly stochastic matrix and why this matters for cross-modal alignment?

- Concept: Variational Information Bottleneck
  - Why needed here: Stage 2 compression—formalizes sufficiency (maximize I(Z;Y)) and minimality (minimize I(Z;X)) trade-off via variational bounds.
  - Quick check question: How does KL divergence to a Gaussian prior serve as an upper bound proxy for I(Z;X)?

- Concept: Reparameterization trick
  - Why needed here: Enables differentiable sampling from Gaussian posterior during training (Eq. 12: Z_i = μ_i + σ_i ⊙ ε_i).
  - Quick check question: Why can't we backpropagate through direct sampling from q(Z|X)?

## Architecture Onboarding

- Component map:
  - Document image → OCR (tokens + bounding boxes) → LayoutLMv3 encoder → text-layout embeddings T ∈ R^(N×d), visual embeddings V ∈ R^(M×d)
  - Stage 1 (OT): Multi-head cost matrices → Sinkhorn → transport plans π^(h) → aggregated P → fusion (attention + OT paths) → context gate → gated T'
  - Stage 2 (VIB): Gaussian encoder (μ, σ²) → reparameterized Z → task head (entity labels + relations)
  - Loss: L_task (cross-entropy) + β · KL regularization

- Critical path:
  1. Cost matrix construction (Eq. 3): semantic similarity + spatial bias λ_h
  2. Sinkhorn iterations (Eq. 4): entropy-regularized OT
  3. Gate computation (Eq. 9): entropy-based confidence
  4. VIB encoding (Eq. 11-12): variational posterior
  5. Joint optimization (Eq. 14)

- Design tradeoffs:
  - Number of OT heads H: More heads capture diverse patterns but increase computation
  - Temperature τ in Sinkhorn: Lower τ → sparser alignments; higher τ → softer distributions
  - β hyperparameter: Controls compression strength; paper uses fixed β but optimal value likely task-dependent
  - Spatial bias λ_h: Learnable per-head; balances semantic vs spatial matching

- Failure signatures:
  - T-SNE shows fragmented/overlapping clusters → alignment or bottleneck not separating classes
  - Gate values stuck near 0 or 1 → check alignment entropy distribution; may indicate cost matrix issues
  - KL term collapsing to near-zero → β too low or encoder degenerate
  - SER/RE large gap → entity recognition vs relation extraction may need different β or alignment granularity

- First 3 experiments:
  1. Reproduce ablation: Train full OTCR, then w/o OT and w/o VIB on FUNSD. Verify SER drops of ~1.08 and ~0.61 respectively.
  2. β sweep: Test β ∈ {0.001, 0.01, 0.1, 1.0} on validation set. Plot SER/RE vs β to identify compression-utility trade-off curve.
  3. Alignment visualization: Extract transport plan P for sample documents. Visualize which visual patches align to which text tokens. Check if high-confidence alignments correspond to semantically meaningful regions (e.g., labels near their values).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cross-modal optimal transport alignment generalize effectively to document types with significantly different layout structures or languages beyond the tested English and Chinese forms?
- Basis in paper: [inferred] The experiments are limited to FUNSD and XFUND (ZH), leaving performance on the full multi-lingual XFUND suite or distinct domains (e.g., receipts, scientific papers) unverified.
- Why unresolved: The learnable spatial bias ($\lambda_h$) and cost function may overfit to the specific layout distributions of the training sets, limiting robustness to diverse visual structures.
- What evidence would resolve it: Evaluation results on the complete XFUND multilingual benchmark (JA, ES, FR, etc.) and other visual document datasets like CORD or DocBank.

### Open Question 2
- Question: What is the computational cost and latency impact of the iterative Sinkhorn algorithm compared to standard attention mechanisms during inference?
- Basis in paper: [inferred] The paper introduces optimal transport (Eq 4) to replace or augment standard fusion but provides no analysis of training/inference efficiency or resource consumption.
- Why unresolved: Optimal transport typically requires iterative matrix scaling (Sinkhorn) to converge, which adds computational overhead compared to single-pass dot-product attention.
- What evidence would resolve it: Comparative metrics on FLOPs, inference time (ms/sample), and memory footprint against the LayoutLMv3 baseline.

### Open Question 3
- Question: Is the isotropic Gaussian assumption for the variational encoder sufficiently expressive to model the latent space of complex multimodal document features?
- Basis in paper: [inferred] The VIB module (Eq 11-14) relies on a Gaussian prior $N(0, I)$ and posterior for analytical tractability.
- Why unresolved: Real-world document embeddings may exhibit multi-modal or complicated distributions that a simple isotropic Gaussian cannot capture, potentially leading to sub-optimal compression or information loss.
- What evidence would resolve it: Analysis of the aggregated posterior distribution geometry or experiments using more flexible priors (e.g., mixture models or flow-based distributions).

## Limitations
- Performance bounds by OCR accuracy—errors propagate through OT and VIB stages
- Limited dataset scope (FUNSD, XFUND) without validation on diverse document types
- Sensitive to multiple hyperparameters (H, τ, λ_h, β) without extensive sensitivity analysis

## Confidence
- **High confidence**: Experimental results on FUNSD and XFUND showing competitive SER/RE scores; ablation study demonstrating VIB contribution (~0.6 SER improvement); theoretical soundness of OT and VIB mechanisms.
- **Medium confidence**: Claims about cross-modal redundancy reduction and task signal strengthening—supported by feature-level analyses but lacking direct quantitative metrics.
- **Low confidence**: Generalization to other document types and tasks; robustness to OCR errors; optimal hyperparameter values across different domains.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically sweep H (2-8 heads), τ (0.01-1.0), and β (0.001-1.0) on FUNSD validation set. Plot SER/RE curves to identify stable operating regions and potential overfitting points.

2. **Cross-domain generalization**: Evaluate OTCR on receipt datasets (SROIE) and invoice datasets (CORD). Compare relative performance degradation against domain-specific models to quantify domain transfer capability.

3. **Error propagation study**: Intentionally inject OCR errors (character deletions, substitutions) at controlled rates. Measure how SER/RE degrade with error rate to establish performance bounds and identify error amplification points in the pipeline.