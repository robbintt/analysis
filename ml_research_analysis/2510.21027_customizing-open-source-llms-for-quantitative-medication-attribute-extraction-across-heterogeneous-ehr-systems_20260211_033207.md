---
ver: rpa2
title: Customizing Open Source LLMs for Quantitative Medication Attribute Extraction
  across Heterogeneous EHR Systems
arxiv_id: '2510.21027'
source_url: https://arxiv.org/abs/2510.21027
tags:
- extract
- drug
- data
- prescription
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework using open-source LLMs to extract
  and standardize medication attributes from heterogeneous EHR systems, enabling consistent
  cross-site analyses of opioid use disorder treatments. The system processes prescription
  records in a fixed JSON schema, followed by normalization and cross-field consistency
  checks.
---

# Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems

## Quick Facts
- arXiv ID: 2510.21027
- Source URL: https://arxiv.org/abs/2510.21027
- Reference count: 40
- Primary result: Framework using open-source LLMs achieves 93.4% coverage and 93.0% exact-match accuracy for extracting MOUD prescription attributes from heterogeneous EHRs.

## Executive Summary
This paper presents a framework using open-source LLMs to extract and standardize medication attributes from heterogeneous EHR systems, enabling consistent cross-site analyses of opioid use disorder treatments. The system processes prescription records in a fixed JSON schema, followed by normalization and cross-field consistency checks. Evaluated on 10,369 records from five clinics, larger models achieved the best performance: Qwen2.5-32B reached 93.4% coverage with 93.0% exact-match accuracy, and MedGemma-27B attained 93.1%/92.2%. Error analysis identified issues with missing dosage fields, handling monthly injectables, and unit misinterpretations, which were addressed with rule-based fixes. The approach removes site-specific ETL and supports privacy-preserving, local deployment, enabling standardized MOUD days calculation and supporting real-world clinical research.

## Method Summary
The framework processes raw EHR prescription records through instruction-tuned LLMs with constrained JSON generation enforced by Pydantic schemas. The LLM extracts key attributes (drug name, dose, duration, quantity) from heterogeneous field formats and free-text notes. Post-processing applies type normalization, cross-field validation (e.g., total quantity ≥ daily quantity), and MOUD days calculation using hierarchical logic. Models evaluated include Qwen2.5-32B, Qwen3 variants, Gemma 3, and MedGemma, deployed on 4× NVIDIA A6000 GPUs with tensor parallelism. Performance is measured against 10,369 manually annotated records from five clinics using coverage (%) and record-level exact match accuracy (%).

## Key Results
- Qwen2.5-32B achieved 93.4% coverage and 93.0% exact-match accuracy
- MedGemma-27B reached 93.1% coverage and 92.2% exact-match accuracy
- Smaller models (4B) showed significantly lower coverage (76.7% for Qwen3-4B)
- Rule-based fixes improved unit handling and monthly injectable interpretation

## Why This Works (Mechanism)

### Mechanism 1: Structured Output via Constrained Decoding
The system enforces a Pydantic-based JSON schema during inference, forcing the model to generate syntactically correct, structured data rather than open-ended text. This reduces parsing failures and ensures consistent output formatting.

### Mechanism 2: Semantic Normalization via In-Context Learning
The LLM maps heterogeneous EHR field names to canonical schema slots using pre-trained semantic understanding, eliminating the need for site-specific ETL code. It handles variations like `GENERIC_NAME` vs `DrugDescription` through instruction following.

### Mechanism 3: Hybrid Neuro-Symbolic Post-Processing
A hierarchical rule-based system validates and corrects LLM outputs, separating extraction (LLM strength) from quantitative consistency (rule strength). This handles arithmetic checks and unit normalization that LLMs struggle with.

## Foundational Learning

- **Structured Output / Constrained Generation**
  - Why needed: To transform unstructured EHR text into computable database-ready rows
  - Quick check: Does your inference framework support logit masking or grammar-constrained decoding to enforce the Pydantic schema?

- **Context Window Packing**
  - Why needed: EHR records contain varying numbers of fields that must fit within the model's context limit
  - Quick check: Have you measured the token length distribution of your raw EHR fields to ensure they fit within the context window?

- **Quantitative NLP vs. Entity Extraction**
  - Why needed: Extracting "medication" is easy; extracting "10.5 mg" and knowing it implies 30 days supply requires quantitative reasoning
  - Quick check: Can your model distinguish between a daily dose (1 tablet) and a total quantity (30 tablets) when both appear in the same sentence?

## Architecture Onboarding

- **Component map:** Raw JSON/Text from EHR -> Prompt Engine (injects record + schema) -> Inference Server (vLLM with Qwen2.5-32B) -> Validator (Python script parsing JSON) -> Canonical Prescription Record

- **Critical path:** 1) Prompt Engineering (defining Shared Extraction Rules), 2) Unit Resolution (ensuring correct unit extraction), 3) Schema Validation

- **Design tradeoffs:**
  - Accuracy vs. Cost: 32B models provide >90% accuracy but require multi-GPU setups; smaller models fail on complex records
  - Generality vs. Specificity: Rule-based fixes improve scores but hardcode medical logic, reducing portability

- **Failure signatures:**
  - Unit Collapse: LLM outputs "250" (count) when input was "250 g" (mass)
  - Duration Confusion: Misinterpreting "4 weeks" as "4 days" for injectables
  - Schema Drift: LLM returning null for required fields if input text is too sparse

- **First 3 experiments:**
  1. Schema Validation Test: Run 100 records through LLM with Pydantic schema enforced, measure valid_json_rate vs. open generation
  2. Unit Stress Test: Feed records with mixed units to verify daily_quantity summation logic
  3. Model Scaling: Compare small (4B) vs. large (32B) on held-out "messy" clinic to quantify coverage gap

## Open Questions the Paper Calls Out

- **Reasoning models for EHR heterogeneity:** Whether newer reasoning models will handle heterogeneous EHRs with less prompt customization compared to standard instruction-tuned models.

- **Parameter-efficient fine-tuning:** Whether LoRA or similar techniques can close the performance gap between smaller (4B-8B) and larger (27B-32B) models.

- **Framework generalizability:** Whether the approach generalizes to non-MOUD medication classes without significant schema modifications.

## Limitations

- Reliance on proprietary datasets (CTN-0102C clinical data) not publicly available, preventing independent validation
- Systematic weaknesses with sparse or non-standard records (20% missing dosage fields, 5% injectable misinterpretations)
- "Rule-based fixes" reduce portability to non-OUD domains and require domain knowledge
- Performance variance across clinics indicates sensitivity to EHR formatting idiosyncrasies

## Confidence

- **High confidence**: Constrained JSON schema approach demonstrably improves extraction consistency (93.4% vs. 76.7% coverage)
- **Medium confidence**: "Universal translator" claims rely on in-context learning with limits shown by performance degradation across clinics
- **Low confidence**: Assertion that approach "removes need for site-specific ETL" is overstated given clinic-specific schemas and manual fixes

## Next Checks

1. **Schema Robustness Test**: Run framework on EHR records from a sixth, unseen clinic to measure real-world generalization beyond the five training sites.

2. **Unit Consistency Audit**: Implement comprehensive unit validation rules and measure reduction in mass-to-count misinterpretations across all clinics.

3. **Model Efficiency Scaling**: Benchmark smaller quantized models (Qwen2.5-8B) with optimized prompts against 32B models to determine minimum viable model size for >90% accuracy.