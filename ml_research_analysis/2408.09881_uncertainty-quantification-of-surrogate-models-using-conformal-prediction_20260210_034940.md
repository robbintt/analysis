---
ver: rpa2
title: Uncertainty Quantification of Surrogate Models using Conformal Prediction
arxiv_id: '2408.09881'
source_url: https://arxiv.org/abs/2408.09881
tags:
- prediction
- coverage
- calibration
- data
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies conformal prediction to provide statistically
  guaranteed uncertainty quantification for surrogate models across diverse scientific
  domains. The method provides marginal coverage regardless of model architecture,
  training regime, or output dimensionality up to 20 million dimensions, requiring
  only seconds to minutes of calibration on standard hardware.
---

# Uncertainty Quantification of Surrogate Models using Conformal Prediction

## Quick Facts
- arXiv ID: 2408.09881
- Source URL: https://arxiv.org/abs/2408.09881
- Reference count: 40
- Primary result: Conformal prediction framework achieves statistically guaranteed marginal coverage across diverse scientific domains with up to 20 million dimensional outputs

## Executive Summary
This paper presents a framework for providing statistically guaranteed uncertainty quantification for surrogate models across scientific domains including PDEs, fusion diagnostics, and weather forecasting. The method achieves marginal coverage regardless of model architecture or output dimensionality, requiring only seconds to minutes of calibration on standard hardware. Through extensive experiments, the framework demonstrates near-perfect empirical coverage using different nonconformity scores while handling high-dimensional spatio-temporal outputs through cell-wise calibration that preserves tensorial structure.

## Method Summary
The framework applies inductive conformal prediction (ICP) to provide marginal coverage guarantees for surrogate models. It splits data into training and calibration sets, computes nonconformity scores on the calibration set, and uses quantile estimation to establish coverage thresholds. The method handles high-dimensional outputs by performing cell-wise calibration independently for each spatial location or time step, trading joint coverage for computational scalability. Exchangeability is maintained through an initial value problem formulation that treats each trajectory as an exchangeable draw, enabling application to time-series forecasting despite temporal dependencies.

## Key Results
- Achieves near-perfect empirical coverage (typically ~90%) across diverse domains including PDEs, fusion diagnostics, and weather forecasting
- Scales to high-dimensional outputs up to 20 million dimensions with calibration times of seconds to minutes
- Maintains coverage guarantees even for out-of-distribution predictions when physics regimes differ between training and deployment
- Handles high-dimensional spatio-temporal outputs through cell-wise calibration while preserving tensorial structure

## Why This Works (Mechanism)

### Mechanism 1: Finite-Sample Coverage via Quantile Estimation
The framework provides statistically guaranteed error bars through inductive conformal prediction by computing nonconformity scores on a calibration set and establishing a threshold via quantile estimation. Under exchangeability assumptions, this ensures a new test point will have a score below this threshold with probability ≥ 1-α.

### Mechanism 2: Scalability via Cell-wise Independence
Instead of constructing massive joint prediction regions, the framework performs calibration independently for each cell in the output tensor. This provides marginal coverage guarantees at every spatial and temporal location while keeping computational cost linear with dimensionality.

### Mechanism 3: Exchangeability Construction via IVP Formulation
Time-series forecasting is reframed as an initial value problem mapping, where each input-output pair represents a trajectory starting from a specific initial condition. By sampling trajectories from different starting times but similar physical regimes, the pairs are treated as exchangeable draws from the distribution of system states.

## Foundational Learning

**Concept: Exchangeability vs. I.I.D.**
Why needed: CP requires exchangeability, which is weaker than independent and identically distributed (i.i.d.). Data order can matter, but the joint distribution of calibration and test sets must be invariant to swapping.
Quick check: If I shuffle the order of my calibration simulations, should the resulting uncertainty bounds change? (Answer: No)

**Concept: Marginal vs. Conditional Coverage**
Why needed: The paper guarantees marginal coverage (correct 90% of the time on average), not conditional coverage (correct 90% of the time for this specific difficult input).
Quick check: If my model predicts a specific storm perfectly 90% of the time over the year, does it guarantee it will be accurate for the specific storm happening tomorrow? (Answer: Not necessarily)

**Concept: Nonconformity Scores**
Why needed: The choice of score (Absolute Error vs. Standard Deviation) determines the shape of the uncertainty bounds.
Quick check: If my model outputs a mean and variance, which score should I use to get tighter bounds on easy inputs? (Answer: Standard Deviation/STD score)

## Architecture Onboarding

**Component map:** Trained Surrogate → Calibration Set → Nonconformity Score Function → Quantile Estimator

**Critical path:** The choice of Nonconformity Score
- Use Absolute Error Residual (AER) for deterministic models (simple, constant width bars)
- Use Standard Deviation (STD) for probabilistic models (tighter bars on easy inputs)

**Design tradeoffs:**
- AER: Zero modification to model, faster, but produces input-independent (constant width) error bars
- STD/CQR: Requires model to output variance or quantiles (arch changes), but produces adaptive error bars that tighten when the model is confident

**Failure signatures:**
- Undercoverage (Coverage < Target): Exchangeability violation - calibration data likely from different physical regime
- Overly Wide Intervals: Poor model fit or too small calibration set (high variance in quantile estimation)

**First 3 experiments:**
1. 1D Poisson (Sanity Check): Train MLP on simple PDE, use AER to verify coverage hits ~90% on validation set
2. Wave Eq (Out-of-Distribution): Train on normal wave speed, calibrate on half-speed, confirm CP maintains coverage under physics shifts
3. Weather (High-Dim): Apply Cell-wise CP to GNN weather model, verify calibration time is manageable (minutes) and coverage holds across spatial cells

## Open Questions the Paper Calls Out

**Open Question 1:** Can spatial-temporal correlation structures be incorporated into conformal prediction for surrogate models while maintaining computational efficiency and scalability to high-dimensional outputs?

**Open Question 2:** Can conditional coverage guarantees be achieved for surrogate models without sacrificing model-agnostic and computationally efficient properties?

**Open Question 3:** How can systematic exchangeability violations be detected and mitigated in time-series surrogate model applications before coverage degrades?

## Limitations

- Framework provides marginal coverage guarantees but not conditional coverage, meaning uncertainty quantification is valid on average but may fail for specific challenging inputs
- Exchangeability assumptions must be carefully maintained; coverage degrades significantly when test data comes from different physical regimes than calibration data
- Cell-wise calibration trades joint coverage guarantees for computational scalability, making it unsuitable for applications requiring simultaneous bounds on entire fields

## Confidence

**High Confidence:** Marginal coverage guarantees and theoretical foundations (extensive empirical validation across 7+ domains with coverage consistently near target 90%)
**Medium Confidence:** Scalability claims and computational efficiency (well-documented but dependent on specific hardware and implementation details)
**Medium Confidence:** Exchangeability construction via IVP formulation (reasonable but requires careful validation for each new domain)

## Next Checks

1. Test exchangeability preservation by systematically varying calibration set physics parameters (viscosity, boundary conditions) and measuring coverage degradation
2. Validate joint vs marginal coverage trade-off by implementing small-scale joint calibration on 1D problems and comparing to cell-wise results
3. Benchmark computational scaling empirically by measuring calibration time as function of output dimensionality beyond reported 20 million dimensions