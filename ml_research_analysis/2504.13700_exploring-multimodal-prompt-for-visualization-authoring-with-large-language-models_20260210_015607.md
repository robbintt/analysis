---
ver: rpa2
title: Exploring Multimodal Prompt for Visualization Authoring with Large Language
  Models
arxiv_id: '2504.13700'
source_url: https://arxiv.org/abs/2504.13700
tags:
- visualization
- visual
- multimodal
- prompts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the limitations of using natural language
  prompts with large language models (LLMs) for visualization authoring. An empirical
  study of 814 utterances reveals that LLMs frequently misinterpret user intent due
  to limited expression of visual intent, inadequate guidance for LLM behavior, and
  misaligned human-LLM design preferences.
---

# Exploring Multimodal Prompt for Visualization Authoring with Large Language Models

## Quick Facts
- arXiv ID: 2504.13700
- Source URL: https://arxiv.org/abs/2504.13700
- Reference count: 40
- Primary result: Multimodal prompting (combining text and visual inputs) significantly improves LLM understanding of user intent for visualization authoring compared to text-only approaches

## Executive Summary
This paper investigates the limitations of using natural language prompts with large language models (LLMs) for visualization authoring. Through an empirical study of 814 utterances, the authors identify three core issues: limited expression of visual intent, inadequate guidance for LLM behavior, and misaligned human-LLM design preferences. To address these challenges, they introduce a multimodal prompting framework that combines text and visual inputs. The authors develop VisPilot, a system enabling users to create visualizations using multimodal prompts including sketches, annotations, and direct manipulations. A user study demonstrates that multimodal prompting achieves higher task accuracy and user satisfaction compared to text-only approaches, without significantly affecting task efficiency.

## Method Summary
The paper introduces a multimodal prompting framework for visualization authoring that combines text and visual inputs to overcome limitations of text-only LLM interactions. The authors developed VisPilot, a system that allows users to create visualizations using various input modalities including sketches, annotations, and direct manipulations alongside natural language descriptions. The approach was validated through a user study comparing multimodal prompting against text-only methods, measuring task accuracy, user satisfaction, and efficiency metrics.

## Key Results
- Multimodal prompting achieved higher task accuracy and user satisfaction compared to text-only approaches
- Visual prompts significantly improved LLM understanding of user intent for visualization tasks
- Multimodal approach reduced the effort required to express complex visualization demands without significantly affecting task efficiency

## Why This Works (Mechanism)
The multimodal prompting approach works by addressing three fundamental limitations of text-only LLM interactions for visualization authoring: limited expression of visual intent (visual inputs can directly convey what users want to see), inadequate guidance for LLM behavior (visual sketches provide explicit constraints and examples), and misaligned human-LLM design preferences (visual annotations help bridge the gap between user expectations and LLM interpretations). By combining text with visual inputs like sketches and annotations, users can communicate their visualization intent more precisely and reduce ambiguity that often leads to misinterpretation by LLMs.

## Foundational Learning
- **Multimodal prompting**: Why needed - Overcomes limitations of text-only communication with LLMs; Quick check - Compare task success rates between text-only and multimodal approaches
- **LLM visualization interpretation**: Why needed - Understanding how LLMs process visualization requests; Quick check - Analyze error patterns in LLM-generated visualizations from text prompts
- **User intent expression**: Why needed - Identifying gaps between user mental models and LLM capabilities; Quick check - Conduct think-aloud protocols during visualization task completion
- **Visual communication patterns**: Why needed - Understanding how sketches and annotations improve clarity; Quick check - Compare interpretation accuracy of text-only vs. visual-enhanced prompts

## Architecture Onboarding
Component map: User -> Input Modalities (Text, Sketch, Annotation, Direct Manipulation) -> VisPilot System -> LLM Processing Engine -> Visualization Output

Critical path: User input (multimodal) → VisPilot preprocessing and normalization → Prompt engineering with visual context → LLM visualization generation → Output refinement and presentation

Design tradeoffs: The system balances between providing rich visual context for improved accuracy versus maintaining user interface simplicity and avoiding cognitive overload from multiple input modalities

Failure signatures: Common failures include LLM misinterpretation of sketch intent, annotation ambiguity, and conflicts between text and visual input specifications

First experiments:
1. Baseline comparison: Text-only prompting vs. multimodal prompting for simple bar chart creation
2. Complexity scaling: Test multimodal approach effectiveness across increasing visualization complexity levels
3. Error analysis: Document specific misinterpretation patterns when visual inputs conflict with text descriptions

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical study analyzed 814 utterances but lacks clarity on whether these were real user queries or constructed examples, affecting generalizability
- User study sample size and participant demographics are not detailed, limiting external validity of findings
- Implementation details and scalability of VisPilot system remain unclear
- Quantitative metrics measuring reduction in user effort are not provided
- Potential drawbacks such as increased cognitive load and accessibility concerns are not extensively explored

## Confidence
- High confidence: The identification of three core limitations in text-only LLM visualization authoring (limited visual intent expression, inadequate LLM guidance, and preference misalignment)
- Medium confidence: The demonstration that multimodal prompting improves task accuracy and user satisfaction based on the user study results
- Medium confidence: The assertion that visual prompts reduce effort for complex visualization demands, though specific metrics are lacking

## Next Checks
1. Conduct a larger-scale user study with diverse participant demographics and explicit metrics for measuring cognitive load across both text-only and multimodal approaches
2. Perform systematic evaluation of accessibility features and alternative input methods for users with visual or motor impairments
3. Implement quantitative analysis measuring the exact reduction in user effort (time, steps, corrections) when using multimodal versus text-only prompting for various complexity levels of visualization tasks