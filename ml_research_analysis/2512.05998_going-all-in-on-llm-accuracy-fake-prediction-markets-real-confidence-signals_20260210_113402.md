---
ver: rpa2
title: 'Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals'
arxiv_id: '2512.05998'
source_url: https://arxiv.org/abs/2512.05998
tags:
- incentive
- accuracy
- round
- baseline
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This pilot study explored whether framing an LLM evaluation task
  as a betting game improves forecasting accuracy and confidence signals. Six Baseline
  models answered 100 math/logic questions, and three Predictor models forecasted
  Baseline correctness under Control (binary predictions) and Incentive (predictions
  plus wagers of 1-100,000 LLMCoin) conditions.
---

# Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals

## Quick Facts
- **arXiv ID:** 2512.05998
- **Source URL:** https://arxiv.org/abs/2512.05998
- **Reference count:** 1
- **Key finding:** Betting behavior created a legible confidence signal absent from binary outputs.

## Executive Summary
This pilot study explored whether framing an LLM evaluation task as a betting game improves forecasting accuracy and confidence signals. Six Baseline models answered 100 math/logic questions, and three Predictor models forecasted Baseline correctness under Control (binary predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin) conditions. Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence: bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is that the betting mechanic created a legible confidence signal absent from binary yes/no outputs, suggesting that simple financial framing may help transform LLMs into risk-aware forecasters.

## Method Summary
Six Baseline models completed 100 math/logic questions across four rounds. Three Predictor models forecasted Baseline correctness in two conditions: Control (binary yes/no) and Incentive (binary plus wager of 1-100,000 LLMCoin). Each Predictor made 5,400 predictions per condition, with wagers reflecting confidence levels. Accuracy, stake-accuracy correlation, and learning speed across rounds were measured to compare Control vs. Incentive conditions.

## Key Results
- **Accuracy gain:** Incentive condition achieved 81.5% vs. 79.1% in Control (p = .089).
- **Learning speed:** Incentive improved by 12.0 percentage points from Round 1 to 4 vs. 2.9 in Control (p = .011).
- **Confidence signal:** Bets ≥40,000 coins were correct ~99% of the time; bets <1,000 coins were correct only ~74%.

## Why This Works (Mechanism)
The betting mechanic taps into loss aversion and risk-reward tradeoffs, compelling models to quantify uncertainty rather than defaulting to binary outputs. By tying stakes to correctness, the model's token generation reflects calibrated confidence—higher stakes correlate with higher accuracy, revealing a self-reported uncertainty metric that binary predictions cannot capture.

## Foundational Learning
- **Loss aversion in AI:** Why needed—to understand if models respond to simulated stakes. Quick check—test stakes vs. accuracy monotonicity.
- **Confidence calibration:** Why needed—to measure if LLM outputs align with actual correctness. Quick check—compare stake size to empirical accuracy.
- **Synthetic vs. real incentives:** Why needed—to distinguish genuine risk sensitivity from superficial behavior. Quick check—test with real micro-stakes.

## Architecture Onboarding
- **Component map:** Baseline models → Predictor models → Betting condition (Control/Incentive) → Stake-accuracy analysis
- **Critical path:** Predictor forecasts → Wagers placed → Accuracy measured → Stake-accuracy correlation assessed
- **Design tradeoffs:** Synthetic vs. real incentives; simplicity vs. behavioral realism; prompt complexity vs. token cost
- **Failure signatures:** Non-monotonic stake-accuracy relationship; random betting patterns; negligible accuracy gains
- **First experiments:** 1) Replicate with 10× more models/questions. 2) Test with real micro-monetary stakes. 3) Vary prompt complexity to isolate betting effect.

## Open Questions the Paper Calls Out
None

## Limitations
- **Sample size:** Only six Baseline models and 100-question rounds limit generalizability.
- **Synthetic stakes:** Without real financial incentives, stakes may reflect artificial rather than genuine risk preferences.
- **Uncontrolled variables:** Token usage increased under betting, but reasoning differences unrelated to wagers were not controlled.

## Confidence
- **Effect existence:** Medium confidence—modest accuracy gain and strong stake-accuracy correlation observed.
- **Robustness:** Low confidence—small sample, lack of real stakes, and uncontrolled reasoning differences weaken reliability.

## Next Checks
1. Replicate with 10× more models and questions to confirm stake-accuracy monotonicity and test statistical power.
2. Introduce real (micro-)monetary incentives or alternative non-financial confidence prompts to distinguish genuine risk sensitivity from superficial betting effects.
3. Conduct ablation studies varying prompt complexity, reasoning depth, and question domain to isolate the betting mechanic's causal impact from other performance drivers.