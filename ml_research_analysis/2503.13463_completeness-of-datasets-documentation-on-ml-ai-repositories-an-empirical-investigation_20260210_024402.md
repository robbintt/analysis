---
ver: rpa2
title: 'Completeness of Datasets Documentation on ML/AI repositories: an Empirical
  Investigation'
arxiv_id: '2503.13463'
source_url: https://arxiv.org/abs/2503.13463
tags:
- documentation
- dataset
- datasets
- data
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the completeness of dataset documentation
  across popular ML/AI repositories. We created the Documentation Test Sheet (DTS),
  a schema based on literature recommendations, to measure documentation quality across
  100 datasets from Hugging Face, Kaggle, OpenML, and UCI.
---

# Completeness of Datasets Documentation on ML/AI repositories: an Empirical Investigation

## Quick Facts
- arXiv ID: 2503.13463
- Source URL: https://arxiv.org/abs/2503.13463
- Reference count: 39
- Key outcome: Study finds severe documentation gaps across ML/AI repositories, with most datasets lacking critical information about collection processes and data processing procedures

## Executive Summary
This empirical investigation examines dataset documentation completeness across four popular ML/AI repositories: Hugging Face, Kaggle, OpenML, and UCI. Using a Documentation Test Sheet (DTS) schema based on literature recommendations, researchers analyzed 100 datasets and found widespread documentation gaps. While basic metadata and usage information were relatively well-documented, critical aspects like collection processes, data processing procedures, funding sources, and ethical review processes were rarely included. Only a few datasets exceeded 50% completeness, with average scores ranging from 0.21 to 0.30 across repositories. The study highlights significant transparency and accountability concerns in the ML/AI ecosystem and calls for improved documentation practices.

## Method Summary
The study created a Documentation Test Sheet (DTS) schema based on literature recommendations to measure documentation quality. Researchers applied this schema to analyze 100 datasets across four popular ML/AI repositories: Hugging Face, Kaggle, OpenML, and UCI. Each dataset was evaluated for the presence or absence of 24 documentation elements, which were categorized into seven areas: basic metadata, data information, collection process, data processing, ethical considerations, quality assurance, and usage. The analysis employed a binary scoring approach (present/absent) to calculate completeness percentages for each dataset and repository.

## Key Results
- Severe documentation gaps found across all repositories, particularly for collection processes and data processing procedures
- Only a few datasets exceeded 50% completeness, with average scores ranging from 0.21 to 0.30 across repositories
- Critical information like funding sources, ethical review processes, and compensation for data workers was rarely documented

## Why This Works (Mechanism)
The study's approach works by providing a structured framework for evaluating dataset documentation completeness through the Documentation Test Sheet (DTS) schema. This schema systematically assesses 24 key documentation elements across seven categories, enabling researchers to quantify and compare documentation quality across different repositories. By using a binary presence/absence scoring method, the study creates clear, measurable criteria for evaluating whether essential information is included in dataset documentation, making it possible to identify specific gaps and patterns across the ML/AI ecosystem.

## Foundational Learning
The study establishes that current dataset documentation practices in ML/AI repositories are inadequate for ensuring transparency, reproducibility, and responsible use. The foundational learning is that while basic metadata is often present, critical information about data provenance, collection methods, processing procedures, and ethical considerations is frequently missing. This pattern suggests a systemic issue in how datasets are documented and shared, with potential implications for model development, evaluation, and deployment. The research also demonstrates that a structured documentation schema can effectively reveal these gaps, providing a foundation for improving documentation standards.

## Architecture Onboarding
The study's architecture consists of a Documentation Test Sheet (DTS) schema that serves as the evaluation framework, applied to datasets from four ML/AI repositories. The DTS includes 24 documentation elements organized into seven categories, with each element scored as present or absent. The scoring system calculates completeness percentages at both dataset and repository levels, enabling comparative analysis. The architecture is designed to be adaptable, allowing for potential expansion to include more repositories or additional documentation elements based on evolving needs in the ML/AI community.

## Open Questions the Paper Calls Out
- How can documentation standards be effectively implemented across diverse ML/AI repositories with different technical architectures and user communities?
- What documentation elements are most critical for different stakeholder groups (researchers, practitioners, regulators)?
- How can the balance be struck between comprehensive documentation requirements and practical usability for dataset creators?
- What incentives or mechanisms could encourage better documentation practices in the ML/AI community?

## Limitations
- Study based on relatively small sample of 100 datasets across four repositories, potentially limiting generalizability
- DTS schema represents one particular perspective on documentation requirements and may not capture all stakeholder considerations
- Binary scoring approach oversimplifies documentation complexity by not accounting for depth or accuracy of information
- The study doesn't establish clear causal relationships between documentation gaps and negative outcomes
- Limited analysis of how documentation completeness affects dataset usability and model performance

## Confidence
High confidence: The general pattern of incomplete documentation across repositories is well-supported by the data. The finding that basic metadata is more commonly documented than collection processes or data processing procedures is robust and aligns with the binary scoring results.

Medium confidence: The specific completeness scores and rankings between repositories should be interpreted cautiously due to the small sample size and potential sampling bias. The conclusion about the need for improved documentation practices is reasonable but may benefit from broader stakeholder input on what constitutes "adequate" documentation.

Low confidence: The study doesn't establish clear causal relationships between documentation gaps and negative outcomes, nor does it provide guidance on which specific documentation elements are most critical for different use cases.

## Next Checks
1. Conduct a larger-scale study across additional repositories to verify if the documented patterns hold across a broader sample of datasets.
2. Develop a more nuanced scoring system that accounts for documentation quality and depth, not just presence/absence.
3. Interview dataset creators and users to identify which documentation elements they consider most critical for responsible use and reuse.