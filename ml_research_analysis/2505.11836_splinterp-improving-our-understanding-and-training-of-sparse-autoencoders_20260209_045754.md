---
ver: rpa2
title: 'SplInterp: Improving our Understanding and Training of Sparse Autoencoders'
arxiv_id: '2505.11836'
source_url: https://arxiv.org/abs/2505.11836
tags:
- pam-sgd
- topk
- page
- saes
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies sparse autoencoders (SAEs) through the lens
  of spline theory to improve both theoretical understanding and training algorithms.
  The authors show that SAEs can be viewed as piecewise-affine splines and establish
  a connection between TopK SAEs and power diagrams (a generalization of Voronoi diagrams).
---

# SplInterp: Improving our Understanding and Training of Sparse Autoencoders

## Quick Facts
- arXiv ID: 2505.11836
- Source URL: https://arxiv.org/abs/2505.11836
- Reference count: 40
- One-line primary result: PAM-SGD algorithm outperforms standard SGD on SAE training, particularly in low-data regimes, by alternating between encoding updates via SGD and closed-form optimal decoding updates

## Executive Summary
This paper establishes a novel theoretical foundation for sparse autoencoders (SAEs) by characterizing them as piecewise-affine splines and connecting TopK SAEs to K-th-order power diagrams. Building on this geometric understanding, the authors propose PAM-SGD, a proximal alternating optimization method that alternates between SGD updates for the encoder and closed-form optimal updates for the decoder. The method demonstrates superior sample efficiency compared to standard SGD, particularly in low-data regimes, and produces sparser activations for LLM applications.

## Method Summary
PAM-SGD is a proximal alternating optimization algorithm for training SAEs that alternates between (a) M steps of SGD on the encoder (W_enc, b_enc) with proximal regularization terms μ_enc||W_enc - W_enc^t||²_F and ν_enc||b_enc - b_enc^t||², and (b) closed-form optimal decoder updates via least squares (W_dec, b_dec) given fixed encoder codes. The decoder update is computed exactly using all codes from the batch, with weight decay α, β and proximal costs μ_dec, ν_dec. This separation of geometry learning (encoder) from reconstruction (decoder) enables more efficient optimization, particularly when data is limited.

## Key Results
- PAM-SGD outperforms standard SGD on MNIST and LLM activation data, with most pronounced gains in low-data regimes (1-25% of data)
- For LLM SAEs, PAM-SGD produces approximately 15% sparser activations compared to standard training
- The method demonstrates stability with ReLU activation but shows instability with TopK sparsity unless sparsity level is sufficiently high
- Sample efficiency improvements are most dramatic at 1-10% data levels, with diminishing returns at higher data percentages

## Why This Works (Mechanism)

### Mechanism 1: Piecewise-Affine Spline Structure
SAEs with ReLU, JumpReLU, or TopK activations form piecewise-affine splines over convex polyhedral regions. Each activation function acts as a projection P_S that zeros non-active entries. On each region Ω^S (defined by which pre-activations exceed threshold or rank in top-K), the SAE is affine: Ŝ(x) = W_dec · P_S · (W_enc·x + b_enc) + b_dec. The structure requires encoder weights W_enc to have no degenerate rows (zero rows for JumpReLU; duplicate rows for TopK).

### Mechanism 2: TopK SAE Regions as K-th-Order Power Diagrams
The partition induced by TopK activation corresponds exactly to K-th-order power diagrams (weighted Voronoi generalizations). Encoder rows e_i^T · W_enc define centroids μ_i; biases define weights α_i = 2(b_enc)_i + ||W_enc^T e_i||². A point belongs to region Ω^TopK_S iff its K closest weighted-centroids form set S. This requires non-degenerate encoder weights and ignores measure-zero ties.

### Mechanism 3: PAM-SGD via Proximal Alternating Optimization
Alternating SGD encoder updates with closed-form optimal decoder updates improves sample efficiency, particularly in low-data regimes. Given fixed codes z_r = ρ(W_enc·x_r + b_enc), finding optimal W_dec, b_dec is linear least squares with closed-form solution. This separates geometry learning (encoder) from reconstruction (decoder), with quadratic "costs-to-move" (μ, ν) providing proximal regularization.

## Foundational Learning

**Voronoi and Power Diagrams**
- Why needed here: TopK SAE geometry is characterized as K-th-order power diagrams; understanding this enables geometric constraint engineering.
- Quick check question: Given 3 points in 2D with weights [0, 1, -1], sketch the power diagram cells.

**Proximal Alternating Methods / Kurdyka-Łojasiewicz Theory**
- Why needed here: PAM-SGD convergence guarantees rely on proximal alternating minimization framework; understanding proximal terms (μ, ν) is critical for hyperparameter tuning.
- Quick check question: Why does adding μ_t||W - W^t||²_F help convergence in non-convex alternating optimization?

**Local PCA / Eigenspectrum-Based Rank Selection**
- Why needed here: Theorem 2.6 shows optimal piecewise-affine autoencoder uses local PCA with rank determined by eigenvalue threshold λ; this benchmarks SAE accuracy-interpretability tradeoff.
- Quick check question: If local covariance has eigenvalues [5, 2, 0.3, 0.1] and sparsity penalty λ=1, what rank K_i minimizes loss?

## Architecture Onboarding

**Component map:**
Input x ∈ R^n → Encoder: z = ρ(W_enc·x + b_enc) [W_enc: d×n, b_enc: d] → Decoder: x̂ = W_dec·z + b_dec [W_dec: n×d, b_dec: n] → Loss: ||x̂ - x||² + λ·L_sparsity(z) + L_aux

**Critical path:**
1. Initialize W_enc, b_enc, W_dec, b_dec
2. **Encoder update**: M steps of SGD on encoding loss + proximal term (Eq. 3.1a)
3. **Decoder update**: Compute all codes z_r, solve closed-form (Eq. 3.1b, Theorem C.1)
4. Repeat until convergence (loss stabilizes per Theorem C.6)

**Design tradeoffs:**
- **ReLU vs TopK**: ReLU stable for PAM-SGD; TopK unstable at low K, requires K ≳ 8-15% of d for LLMs
- **Tied vs untied weights**: PAM-SGD requires untied decoder (solved analytically); standard SGD often uses tied weights
- **μ, ν values**: Too small → instability/divergence; too large → slow convergence. Start with 0.01-0.1 range
- **SGD steps M per batch**: 1-3 optimal for LLMs; MNIST less sensitive

**Failure signatures:**
- TopK test loss diverges rapidly → K too small (need K ≥ 320 for d=4096 on Gemma)
- Medium-data performance collapse (45% peak) → TopK + PAM-SGD numerical instability; try ReLU
- Encoder filters noisy/unstructured → increase weight decay α or cost-to-move μ_enc

**First 3 experiments:**
1. **Sanity check**: Train SAE on MNIST (n=784, d=256) with TopK K=15, comparing PAM-SGD vs SGD at 10% data. Expected: PAM-SGD achieves lower test loss faster (Figure 3).
2. **Activation ablation**: On same setup, test ReLU vs TopK with PAM-SGD. Expected: Both work; ReLU more stable, TopK produces sparser, more interpretable filters (Figure 11).
3. **LLM low-data regime**: Extract Gemma-2-2B layer-12 activations (n=2304), train d=4096 SAE with ReLU at 5% data (500 samples). Compare reconstruction MSE and activation sparsity. Expected: PAM-SGD ~15% sparser activations with competitive MSE (Figure 30).

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can the spline geometry characterization of TopK SAEs as Kth-order power diagrams be extended to JumpReLU and ReLU SAEs?
- Basis in paper: [explicit] "Our characterisation of the spline geometry of SAEs is currently limited to TopK SAEs; future work will seek to extend this..."
- Why unresolved: The power diagram characterization relies on the fixed-K selection mechanism of TopK; JumpReLU's threshold-based activation creates different partition structures that may not admit the same geometric formulation.
- What evidence would resolve it: A theorem establishing a correspondence between JumpReLU SAE partition regions and a specific geometric construct (e.g., weighted Voronoi with variable weights), or a proof that no such simple correspondence exists.

**Open Question 2**
- Question: Does PAM-SGD provably converge to critical points when SGD approximates the encoder update?
- Basis in paper: [explicit] "Theorem 3.1...does not prove that our PAM-SGD method converges, as the theorem assumes that (3.1a) is solved exactly, whilst in Algorithm 3.1 it will be only approximated via SGD."
- Why unresolved: The convergence proof requires exact solutions to proximal subproblems; SGD introduces approximation errors that break the theoretical guarantees.
- What evidence would resolve it: Extended convergence analysis accounting for SGD approximation error (e.g., bounded stochastic gradients, decreasing step sizes), or empirical convergence rate studies showing PAM-SGD consistently reaches critical points.

**Open Question 3**
- Question: What causes PAM-SGD's instability and underperformance with TopK activation at medium data regimes in LLM settings?
- Basis in paper: [inferred] The paper reports PAM-SGD underperforms SGD for TopK with a "major peak at 45% data" and states "we suspect it is some fundamental issue perhaps caused by numerical instability" without resolution.
- Why unresolved: The phenomenon is documented empirically across multiple runs but lacks theoretical explanation; the interaction between TopK's discrete selection and PAM-SGD's alternating optimization is poorly understood.
- What evidence would resolve it: Controlled experiments isolating numerical precision, gradient variance, and optimizer state effects; theoretical analysis of TopK's non-smoothness in the proximal alternating framework.

## Limitations

- The theoretical connection between power diagrams and TopK SAEs, while mathematically elegant, has practical limitations as not all power diagrams with (d choose K) cells can be realized as TopK SAEs
- PAM-SGD's convergence guarantees rely on Kurdyka-Łojasiewicz assumptions that may not hold for the smoothed TopK activation used in practice
- The empirical evaluation focuses primarily on reconstruction loss and sparsity metrics without investigating whether the theoretically-motivated training method actually produces more interpretable features or better downstream utility

## Confidence

- **High Confidence**: The piecewise-affine spline characterization of SAEs (Mechanism 1) - this follows directly from the ReLU/TopK activation structure and encoder-decoder linearity, with rigorous proof in Theorem B.1. The convergence proof for PAM-SGD under Kurdyka-Łojasiewicz assumptions (Theorem C.6) is also mathematically sound.
- **Medium Confidence**: The TopK-SAE power diagram connection (Mechanism 2) - while Theorem 2.3 provides a clean mathematical result, the practical relevance is unclear since the paper doesn't demonstrate how this geometric insight improves SAE design or interpretation. The counterexample showing non-surjectivity of the correspondence raises questions about the practical utility of this characterization.
- **Medium Confidence**: PAM-SGD's sample efficiency advantages (Mechanism 3) - the empirical results are compelling, but the experiments are limited to two datasets and don't explore whether the gains translate to improved interpretability or downstream task performance. The stability issues with TopK at low sparsity levels suggest the method may not generalize well across all use cases.

## Next Checks

1. **Geometric Validation**: For a trained TopK SAE on MNIST or Gemma activations, explicitly compute the induced power diagram partition and verify it matches the theoretical characterization. This would involve extracting encoder weights W_enc, computing centroids μ_i = e_i^T · W_enc and weights α_i = 2(b_enc)_i + ||W_enc^T e_i||², then checking that each point's K-nearest weighted centroids match the activation pattern. This validates whether the power diagram interpretation is practically meaningful or merely theoretical.

2. **Interpretability Assessment**: Compare feature interpretability between PAM-SGD and SGD-trained SAEs using established metrics like feature clustering quality, alignment with known concepts, or human evaluation. The paper shows PAM-SGD produces sparser activations, but sparsity alone doesn't guarantee interpretability. This would test whether the theoretical advantages translate to practical utility in interpretability applications.

3. **Downstream Task Transfer**: Evaluate whether SAEs trained with PAM-SGD show improved performance on downstream tasks like model editing, circuit discovery, or activation steering compared to SGD-trained SAEs. The sample efficiency gains suggest PAM-SGD might learn more robust feature representations, but this needs empirical validation beyond reconstruction metrics.