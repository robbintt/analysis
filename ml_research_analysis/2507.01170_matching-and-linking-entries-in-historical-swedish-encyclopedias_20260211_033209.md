---
ver: rpa2
title: Matching and Linking Entries in Historical Swedish Encyclopedias
arxiv_id: '2507.01170'
source_url: https://arxiv.org/abs/2507.01170
tags:
- entries
- editions
- edition
- first
- entry
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study processed two editions of the Nordisk familjebok encyclopedia
  to detect shifts in geographic focus over time. The authors scraped OCR text from
  Project Runeberg, segmented it into entries using bold tags, index matching, and
  a classifier, and then identified location entries with a transformer-based classifier.
---

# Matching and Linking Entries in Historical Swedish Encyclopedias

## Quick Facts
- arXiv ID: 2507.01170
- Source URL: https://arxiv.org/abs/2507.01170
- Reference count: 11
- Key outcome: This study processed two editions of the Nordisk familjebok encyclopedia to detect shifts in geographic focus over time. The authors scraped OCR text from Project Runeberg, segmented it into entries using bold tags, index matching, and a classifier, and then identified location entries with a transformer-based classifier. They matched entries between editions using sentence embeddings and linked them to Wikidata to obtain geographic coordinates. The geographic analysis revealed a small but significant shift away from Europe toward North America, Africa, Asia, Australia, and northern Scandinavia, reflecting the impact of World War I and industrialization. Performance metrics for location classification and entry matching were high (F1 > 0.9), while Wikidata linking within 25 km achieved 69-84% accuracy.

## Executive Summary
This study presents a comprehensive pipeline for processing historical Swedish encyclopedias to analyze geographic shifts over time. The authors developed methods to segment OCR text into entries, classify location entries, match corresponding entries across two editions, and link them to Wikidata for geographic coordinates. The geographic analysis revealed a statistically significant shift in focus from Europe to other continents and northern Scandinavia between the 1876-1899 and 1904-1926 editions, likely reflecting World War I and industrialization. The technical contributions include cascading segmentation strategies, semantic matching using sentence embeddings, and Wikidata linking with text similarity.

## Method Summary
The authors scraped OCR text from Project Runeberg for two editions of Nordisk familjebok, then segmented entries using a three-stage approach: bold tag matching (97.7% coverage for 1st edition), index matching with Levenshtein distance (threshold 0.15), and a CLD3-based classifier for remaining cases. Location entries were identified using a KB-BERT transformer classifier trained on 200 manually annotated samples. Entries were matched between editions using KB-SBERT embeddings with cosine similarity threshold 0.9. Wikidata linking employed cosine similarity between entry text and Wikidata candidate descriptions, with a threshold of 0.6, to retrieve geographic coordinates for spatial analysis.

## Key Results
- Entry segmentation achieved F1 > 0.96 for both editions using cascading approach
- Location classification achieved F1 0.90-0.92 on KB-BERT + logistic regression
- Cross-edition entry matching achieved F1 0.83 (vs baseline 0.76) using KB-SBERT embeddings
- Wikidata linking within 25km accuracy: 69% (1st ed) and 54% (2nd ed)
- Geographic analysis revealed significant shift from Europe to other continents and northern Scandinavia

## Why This Works (Mechanism)

### Mechanism 1: Cascading Segmentation with Fallback Strategies
A three-stage entry detection pipeline (bold tags → index matching → classifier) recovers entries from noisy OCR where single-strategy approaches would fail. Bold matching handles well-preserved markup (97.7% of 1st edition); Levenshtein-distance index matching catches entries with OCR errors; a CLD3-style classifier handles remaining ambiguous cases. Entry headwords retain distinctive orthographic patterns (capitalization, punctuation, parentheses) even when bold markup is lost.

### Mechanism 2: Semantic Embedding Similarity for Cross-Edition Matching
KB-SBERT sentence embeddings with cosine similarity threshold ≥0.9 match corresponding entries across editions better than headword string matching. Dense vector representations capture semantic similarity despite wording changes; threshold filtering prevents false positives from structurally similar but distinct entries. Entry content describing the same location remains semantically similar across editions despite editing.

### Mechanism 3: Wikidata Linking via Text Similarity
Encoding entry text and Wikidata candidate descriptions with KB-SBERT, then selecting highest cosine similarity ≥0.6, links entries to geographic coordinates. Wikipedia article snippets (preferred) or Wikidata descriptions provide comparison text; embedding similarity handles spelling variations and partial matches. Swedish Wikipedia articles for locations exist and resemble encyclopedia entry style.

## Foundational Learning

- **Sentence embeddings (SBERT architecture)**
  - Why needed here: Core to both cross-edition matching and Wikidata linking; cosine similarity on [CLS] or pooled representations
  - Quick check question: Given two Swedish paragraph embeddings with cosine similarity 0.87, would they pass the 0.9 matching threshold?

- **OCR error handling with edit distance**
  - Why needed here: Index matching uses relative Levenshtein distance (0.15 threshold) to match headwords against OCR text
  - Quick check question: If "Stockholm" is OCRed as "Stokholm", what's the relative Levenshtein distance?

- **Transformer-based text classification**
  - Why needed here: Location classifier uses KB-BERT [CLS] token with logistic regression head
  - Quick check question: Why truncate entries to 200 characters before classification?

## Architecture Onboarding

- **Component map:**
  HTML pages → Scraper → Raw text → Segmenter (bold→index→classifier) → Entries
                                                                    ↓
  Entries → Cross-reference resolver → Cleaned entries → Location classifier
                                                              ↓
  Location entries → KB-SBERT encoder → Vector DB (Qdrant) → Edition matcher
                                                              ↓
  Matched entries → Wikidata API querier → Candidate texts → Similarity ranker → QIDs + coordinates

- **Critical path:** Segmentation accuracy (especially classifier for 2nd edition's 12.5% non-bold entries) → Location classification → Wikidata linking; errors compound

- **Design tradeoffs:**
  - 5-candidate Wikidata limit: Faster but misses valid entities
  - 0.9 matching threshold: High precision but may miss legitimate matches with substantial content changes
  - 200-char truncation: Faster inference but loses long-entry context

- **Failure signatures:**
  - High false match rate on entries with identical templates (parish descriptions)
  - Wikidata linking fails for spelling-reform-affected names (q→k)
  - Cross-references chain incorrectly when multiple entries share headword

- **First 3 experiments:**
  1. Replicate segmentation on 100 random pages; verify bold/index/classifier proportions match Table 1
  2. Test location classifier on held-out 50 entries; confirm F1 ≥0.90
  3. Spot-check 20 Wikidata links; measure 25km accuracy against manual coordinates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning transformer models on historical Swedish texts from the late 19th and early 20th centuries improve performance on classification and matching tasks compared to models trained on contemporary Swedish?
- Basis in paper: [explicit] The authors state: "A possible improvement is to fine-tune the models on older Swedish texts."
- Why unresolved: KB-BERT was trained on Swedish texts from 1940–2019, while the encyclopedia spans 1876–1926. The language differences (including the q-to-k spelling reform around 1900) may introduce systematic errors or bias.
- What evidence would resolve it: Fine-tune KB-BERT on a corpus of historical Swedish texts and compare F1 scores on entry classification, location classification, and matching tasks against the baseline.

### Open Question 2
- Question: Would the Hungarian algorithm or other optimal assignment methods outperform the greedy matching strategy for aligning entries across editions?
- Basis in paper: [explicit] The authors note: "We could also explore alternative algorithms for matching entries, such as the Hungarian algorithm."
- Why unresolved: The greedy strategy (selecting the first candidate above threshold) may miss globally optimal matchings. The authors found only marginal improvement over simple headword matching (F1 0.83 vs. 0.76), suggesting room for algorithmic improvements.
- What evidence would resolve it: Implement Hungarian algorithm-based matching on the same entry pairs and compare matching precision, recall, and F1 scores against the greedy baseline.

### Open Question 3
- Question: Can a named entity recognizer improve cross-reference resolution by distinguishing homographic entries (e.g., "Bajesid" as sultan lineage vs. city)?
- Basis in paper: [explicit] The authors acknowledge their simple rule-based method "could probably be improved by using a named entity recognizer."
- Why unresolved: The current method matches cross-references to the first entry with an exact headword, causing errors when multiple entries share the same headword but describe different entity types.
- What evidence would resolve it: Apply a Swedish NER model to classify entity types before matching cross-references; evaluate resolution accuracy on a manually annotated test set of cross-references.

### Open Question 4
- Question: How much would Wikidata linking accuracy improve by expanding the search space beyond five candidates and incorporating spelling normalization for historical variants?
- Basis in paper: [inferred] The authors note that "the limited search space we set to reduce computation time" caused missed matches, and spelling reform changes (e.g., Qvenneberga → Kvenneberga) affected search results. Current within-25km accuracy is only 69–84%.
- Why unresolved: The tradeoff between computational cost and linking accuracy remains unexplored, and the impact of spelling normalization on retrieval has not been quantified.
- What evidence would resolve it: Systematically evaluate linking accuracy with increased candidate counts (10, 20, 50) and with spelling normalization applied; measure precision, recall, and computational cost.

## Limitations

- Wikidata linking accuracy is moderate (69-84% within 25km), with 5-candidate limit potentially missing correct entities
- Geographic conclusions depend on linking accuracy and don't weight entries by importance or length
- Cross-edition matching may miss entries with substantial content changes despite semantic similarity threshold
- Spelling reforms (q→k) and non-Latin scripts affect Wikidata search and linking performance

## Confidence

- **High confidence**: Entry segmentation pipeline effectiveness (F1 > 0.96) and location classification performance (F1 > 0.90)
- **Medium confidence**: Cross-edition matching accuracy (F1 0.83 vs baseline 0.76) and general geographic trend conclusions
- **Low confidence**: Exact magnitude of geographic shift and Wikidata linking precision within 25 km

## Next Checks

1. Measure the impact of expanding Wikidata candidate pool beyond 5 results on linking accuracy, particularly for spelling-reform-affected names and non-Latin script entries
2. Test cross-edition matching sensitivity by varying the cosine similarity threshold and measuring precision-recall tradeoffs
3. Validate geographic conclusions by stratifying analysis by entry length/type and comparing with independent historical data sources about population movements and settlement patterns during the relevant period