---
ver: rpa2
title: Near-Equilibrium Propagation training in nonlinear wave systems
arxiv_id: '2510.16084'
source_url: https://arxiv.org/abs/2510.16084
tags:
- training
- input
- systems
- output
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Near-Equilibrium Propagation (NEP), an extension
  of Equilibrium Propagation for training both discrete and continuous complex-valued
  wave systems in the weakly dissipative regime. Unlike previous EP implementations,
  NEP does not require a well-defined energy function and can train local potential
  parameters rather than inter-node weights, making it broadly applicable to physical
  neural networks.
---

# Near-Equilibrium Propagation training in nonlinear wave systems

## Quick Facts
- arXiv ID: 2510.16084
- Source URL: https://arxiv.org/abs/2510.16084
- Reference count: 0
- Introduces NEP for training physical wave systems, achieving ~85% MNIST accuracy

## Executive Summary
This work introduces Near-Equilibrium Propagation (NEP), an extension of Equilibrium Propagation for training both discrete and continuous complex-valued wave systems in the weakly dissipative regime. Unlike previous EP implementations, NEP does not require a well-defined energy function and can train local potential parameters rather than inter-node weights, making it broadly applicable to physical neural networks. The method is demonstrated in driven-dissipative exciton-polariton condensates governed by the Gross-Pitaevskii equation. Numerical experiments show stable convergence and high accuracy on benchmarks including a two-input XOR task and MNIST digit classification, achieving up to ~85% test accuracy on a subset of MNIST digits.

## Method Summary
NEP trains physical wave systems by evolving them through two phases: free evolution to steady state under input drive, then nudged evolution with a small perturbation in the output region. The key innovation is using the steady-state contrast between these phases to compute parameter updates for local potentials and pump weights, without requiring an energy function or weight-based training. The method is applied to driven-dissipative exciton-polariton condensates modeled by the Gross-Pitaevskii equation, discretized and integrated using RK4. Training uses MSE loss for XOR and categorical cross-entropy with softmax for MNIST, with batch updates computed from the contrasts in steady states.

## Key Results
- Achieved ~85% test accuracy on 5-digit MNIST subset (0,1,3,6,9) using PCA-reduced 25-component inputs
- Successfully trained XOR task with 9-node 1D grid, converging in ~10 epochs
- Demonstrated stable convergence for both discrete and continuous complex-valued wave systems
- Showed that NEP can train local potential parameters rather than just inter-node weights

## Why This Works (Mechanism)
NEP works by exploiting the linear response of a physical system near equilibrium to small perturbations. The two-phase training procedure—free evolution followed by nudged evolution—creates a measurable steady-state contrast that directly relates to the gradient of the loss with respect to trainable parameters. This gradient is computed through local interactions within the physical system itself, eliminating the need for backpropagated error signals. The method leverages the Lyapunov functional structure of the damped Gross-Pitaevskii equation to ensure convergence to stable steady states where parameter updates can be reliably computed.

## Foundational Learning
- **Gross-Pitaevskii Equation**: Nonlinear Schrödinger equation describing Bose-Einstein condensates; needed to model the physical wave system being trained
- **Equilibrium Propagation**: Learning method using energy minimization; provides theoretical foundation for NEP's two-phase approach
- **Driven-dissipative systems**: Open quantum systems with external pumping and losses; required to model realistic physical implementations
- **Lyapunov functional**: Mathematical structure ensuring convergence; justifies why NEP's steady states are stable and meaningful
- **RK4 integration**: Fourth-order Runge-Kutta method; needed for stable numerical discretization of continuous dynamics
- **Complex-valued neural networks**: Networks operating on complex numbers; essential since exciton-polariton condensates have complex wavefunctions

## Architecture Onboarding

Component Map:
Input Pump -> Wave Field (Ψ) -> Output Region -> Parameter Updates (V, w)

Critical Path:
Input Pump → Ψ initialization → Free evolution to steady state → Nudged evolution → Steady-state contrast → Parameter updates

Design Tradeoffs:
- β perturbation size: Small (0.01) for linear response validity vs. large enough for measurable contrast
- Integration timestep: Small (0.1) for accuracy vs. computational efficiency
- Nonlinearity strength: g=0.1 for XOR vs. g=0.001 for MNIST; stronger nonlinearities increase expressivity but risk instability
- Spatial discretization: 1D grid for XOR vs. 2D with PCA-reduced features for MNIST; trade spatial resolution for computational tractability

Failure Signatures:
- Divergence or blowup in free/nudged evolution phases
- Zero or near-zero steady-state contrast preventing parameter updates
- Oscillatory or non-convergent loss curves
- Phase wrapping issues in complex-valued updates

First Experiments:
1. Verify XOR convergence with 9-node grid, β=0.01, g=0.1, monitor loss and accuracy over 10 epochs
2. Test MNIST training on 5-digit subset with 5×25 grid, β=0.01, g=0.001, check accuracy reaches ~85%
3. Validate batch training by varying batch sizes and comparing loss convergence curves

## Open Questions the Paper Calls Out
None

## Limitations
- The extension from EP to NEP in continuous-time systems requires careful numerical discretization
- The claim that NEP is broadly applicable to in-situ learning in physical neural networks extrapolates from simulation results to real-world deployment without hardware validation
- Minor uncertainties exist around initialization (Ψ seed values) and batch training details

## Confidence
- NEP can train physical wave systems without energy functions and achieves competitive MNIST accuracy: **High**
- NEP generalizes EP beyond weight-based to local potential training: **Medium**
- NEP is broadly applicable to in-situ learning in physical neural networks: **Low**

## Next Checks
1. Test initialization sensitivity by comparing zero vs. random Ψ seeds on XOR convergence
2. Validate batch training consistency by varying batch size and monitoring loss curves
3. Benchmark NEP against standard backprop-trained RNNs on the same MNIST subset to quantify claimed efficiency gains