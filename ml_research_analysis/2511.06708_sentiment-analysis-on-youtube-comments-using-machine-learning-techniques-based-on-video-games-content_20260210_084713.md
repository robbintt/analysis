---
ver: rpa2
title: Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based
  On Video Games Content
arxiv_id: '2511.06708'
source_url: https://arxiv.org/abs/2511.06708
tags:
- sentiment
- comments
- gaming
- data
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study applied sentiment analysis to YouTube gaming comments\
  \ using TextBlob and machine learning classifiers (Na\xEFve Bayes, Logistic Regression,\
  \ SVM). The 11,781 comments from three IGN gaming review videos were pre-processed\
  \ (lowercasing, stop word removal, tokenization, lemmatization) and classified into\
  \ positive, negative, or neutral sentiments."
---

# Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content

## Quick Facts
- arXiv ID: 2511.06708
- Source URL: https://arxiv.org/abs/2511.06708
- Reference count: 0
- SVM achieved highest classification accuracy across all three YouTube gaming comment datasets

## Executive Summary
This study applies sentiment analysis to YouTube gaming comments using TextBlob for initial labeling and machine learning classifiers (Naïve Bayes, Logistic Regression, SVM). The 11,781 comments from three IGN gaming review videos were pre-processed and classified into positive, negative, or neutral sentiments. SVM consistently outperformed other classifiers, demonstrating superior accuracy across datasets ranging from 854 to 7,401 comments. The findings provide actionable insights for game developers to refine designs and improve user experience.

## Method Summary
The methodology involves collecting YouTube comments from three IGN gaming review videos, preprocessing text through lowercasing, stop word removal, tokenization, lemmatization, and special character removal. TextBlob generates initial polarity scores (-1 to +1) for sentiment labeling. Features are extracted using TF-IDF vectorization, and three classifiers (Naïve Bayes, Logistic Regression, SVM) are trained and evaluated using an 80-20 train-test split. SVM demonstrates superior performance across all datasets.

## Key Results
- SVM consistently achieved the highest classification accuracy across all three datasets
- The approach provides actionable insights for game developers regarding user sentiment
- Challenges identified include comment brevity, slang, and sarcasm affecting sentiment classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVM outperforms Naïve Bayes and Logistic Regression for gaming comment sentiment classification across varying dataset sizes.
- Mechanism: SVM identifies an optimal hyperplane that maximizes the margin between sentiment classes by finding the decision boundary with greatest distance from nearest data points (support vectors), generalizing better to unseen gaming comments with diverse vocabulary.
- Core assumption: Gaming comments form separable clusters in high-dimensional TF-IDF space where sentiment boundaries can be meaningfully drawn.
- Evidence anchors: SVM demonstrated superior performance, achieving highest classification accuracy across different datasets; the SVM algorithm consistently outperforms other methods across all datasets.
- Break condition: If gaming comments exhibit high class overlap (sarcasm, mixed sentiments within single comments), linear separability assumption fails.

### Mechanism 2
- Claim: Two-stage labeling (TextBlob polarity → ML classification) provides computationally efficient baseline labels for training classifiers.
- Mechanism: TextBlob assigns continuous polarity scores based on lexicon lookup, converting unlabeled comments into weakly-supervised training data. ML classifiers then learn decision boundaries that refine these initial assignments.
- Core assumption: TextBlob's lexicon-based scores correlate sufficiently with true sentiment to serve as adequate training labels.
- Evidence anchors: TextBlob's continuous polarity scoring provides suitable initial labels for training machine learning classifiers; each comment undergoes assessment to determine polarity based on linguistic patterns.
- Break condition: If gaming slang, sarcasm, or domain-specific terminology systematically mislead lexicon-based scoring, training labels will propagate errors.

### Mechanism 3
- Claim: TF-IDF feature extraction improves sentiment discrimination over Bag-of-Words by weighting gaming-specific vocabulary.
- Mechanism: TF-IDF downweights frequently occurring terms while upweighting distinctive terms that differentiate sentiment classes, reducing noise from generic gaming discourse.
- Core assumption: Sentiment-discriminative terms have characteristic frequency distributions that distinguish them from general gaming discussion terms.
- Evidence anchors: TF-IDF was selected for its effectiveness in weighting gaming-specific vocabulary while reducing common word impact; feature extraction converts preprocessed text into machine learning-compatible formats.
- Break condition: If important sentiment signals appear in common terms, TF-IDF may inappropriately downweight them.

## Foundational Learning

- Concept: **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: The paper explicitly selects TF-IDF over Bag-of-Words for feature extraction. Understanding term weighting affects classification is essential for debugging poor model performance.
  - Quick check question: If a word like "game" appears in 90% of comments, will TF-IDF assign it high or low weight, and why might this matter for sentiment classification?

- Concept: **Margin Maximization in SVM**
  - Why needed here: The paper credits SVM's success to its "margin maximization strategy." Understanding this principle helps predict when SVM will succeed or fail on new datasets.
  - Quick check question: If sentiment classes have significant overlap in feature space, how would this affect SVM's margin-based approach?

- Concept: **Train-Test Split and Cross-Validation**
  - Why needed here: The paper uses an 80-20 split. Understanding evaluation methodology is critical for assessing whether reported accuracy reflects real generalization.
  - Quick check question: The paper reports accuracy on one split. What additional evaluation approach would provide more robust performance estimates?

## Architecture Onboarding

- Component map: YouTube API → Raw comments → TextBlob (polarity scoring) → Preprocessing (lowercase → stop words → tokenization → lemmatization → special char removal) → TF-IDF Vectorizer → ML Classifiers (Naïve Bayes | Logistic Regression | SVM) → Evaluation (Accuracy, Precision, Recall, F1)

- Critical path: TextBlob polarity → TF-IDF transformation → SVM classification. Errors in TextBlob labeling propagate through the entire pipeline; SVM performance depends entirely on the quality of features from TF-IDF.

- Design tradeoffs:
  - **TextBlob vs. VADER/BERT**: Paper chose TextBlob for computational efficiency but acknowledges struggles with sarcasm and gaming slang. VADER handles social media better; BERT captures context but requires more compute.
  - **80-20 split vs. K-fold cross-validation**: Paper uses single split; related work used 10-fold CV for more robust estimates.
  - **Primary comments only vs. including replies**: Paper excludes reply threads for consistency, but this may miss sentiment clarification in follow-up discussions.

- Failure signatures:
  - **Sarcasm detection failure**: Positive words in negative context will mislead TextBlob labeling.
  - **Small dataset instability**: The Skull Island dataset showed "oversimplified categorization due to limited context."
  - **Gaming slang misclassification**: Domain-specific terms not in TextBlob's lexicon receive neutral or incorrect polarity scores.

- First 3 experiments:
  1. **Baseline replication**: Collect 500+ comments from a single gaming video, apply the paper's preprocessing pipeline, train all three classifiers, and verify SVM achieves highest accuracy.
  2. **Label quality audit**: Manually annotate 100 comments as positive/negative/neutral, compare against TextBlob scores, and measure labeling error rate.
  3. **Feature extraction comparison**: Run the same SVM classifier with Bag-of-Words vs. TF-IDF features on identical data. Compare accuracy to validate TF-IDF superiority claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deep learning models (e.g., LSTM, BERT) outperform the SVM baseline in classifying sentiment for gaming-specific YouTube comments?
- Basis in paper: The authors explicitly state the study used traditional algorithms "without exploring deep learning approaches like LSTM or BERT that could better understand contextual relationships."
- Why unresolved: The current study limited its scope to traditional machine learning, leaving the potential added value of deep learning architectures for this specific domain untested.
- What evidence would resolve it: Comparative performance metrics (Accuracy, F1-Score) of LSTM or BERT models against the SVM baseline using the same IGN comment datasets.

### Open Question 2
- Question: Does the use of contextual embeddings (Word2Vec, GloVe) provide superior semantic understanding compared to the TF-IDF feature extraction method used in this study?
- Basis in paper: The paper notes that feature extraction was "restricted to TF-IDF without considering contextual embeddings such as Word2Vec or GloVe."
- Why unresolved: TF-IDF focuses on word frequency rather than semantic context, which may limit the model's ability to interpret gaming slang or nuanced language.
- What evidence would resolve it: An ablation study comparing classification accuracy when using TF-IDF versus pre-trained contextual embeddings on the preprocessed data.

### Open Question 3
- Question: How do user sentiments evolve temporally in relation to major industry events or game updates?
- Basis in paper: The conclusion suggests that "Temporal analysis... including game releases or major changes... can provide a new perspective on sentiment patterns."
- Why unresolved: The current methodology analyzes comments as a static snapshot, failing to capture how opinions shift over time.
- What evidence would resolve it: A longitudinal study tracking sentiment polarity scores at specific time intervals for a specific game title.

## Limitations

- Reliance on TextBlob for initial labeling introduces systematic bias for gaming-specific slang and sarcasm
- Single 80-20 train-test split provides limited generalizability assessment compared to cross-validation
- Analyzing only primary comments while excluding reply threads may miss contextual sentiment information

## Confidence

- SVM performance superiority (High): Consistently supported across all three datasets with specific accuracy metrics reported
- TF-IDF feature superiority (Medium): Claimed but not directly validated against Bag-of-Words in this study
- TextBlob labeling effectiveness (Low): No direct validation against ground truth; acknowledged weakness with sarcasm and domain-specific terminology

## Next Checks

1. Manually annotate 100 randomly selected comments from each dataset to create ground truth labels, then measure TextBlob's labeling accuracy and precision across sentiment classes
2. Implement 10-fold cross-validation instead of single 80-20 split to assess model stability and variance in performance metrics
3. Compare SVM with modern transformer-based approaches (BERT, RoBERTa) on the same datasets to establish baseline performance against current state-of-the-art methods