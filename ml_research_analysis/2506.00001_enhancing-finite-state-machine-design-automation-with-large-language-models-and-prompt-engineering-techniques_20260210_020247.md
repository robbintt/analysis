---
ver: rpa2
title: Enhancing Finite State Machine Design Automation with Large Language Models
  and Prompt Engineering Techniques
arxiv_id: '2506.00001'
source_url: https://arxiv.org/abs/2506.00001
tags:
- design
- prompt
- llms
- patch
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the performance of three major LLMs\u2014\
  Claude 3 Opus, ChatGPT-4, and ChatGPT-4o\u2014in designing finite state machines\
  \ (FSMs) using systematic markdown format prompts derived from HDLBits problems.\
  \ The study introduces a novel prompt refinement technique called To-do-Oriented\
  \ Prompting (TOP) Patch, which enhances LLM performance by emphasizing key design\
  \ constraints and concepts."
---

# Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques

## Quick Facts
- arXiv ID: 2506.00001
- Source URL: https://arxiv.org/abs/2506.00001
- Reference count: 5
- Claude 3 Opus achieved 41% baseline success rate; TOP Patch improved ChatGPT-4o synchronous reset performance from 30% to 70%

## Executive Summary
This paper evaluates three major LLMs—Claude 3 Opus, ChatGPT-4, and ChatGPT-4o—on finite state machine design tasks using systematically formatted markdown prompts derived from HDLBits problems. The study introduces a novel prompt refinement technique called To-do-Oriented Prompting (TOP) Patch that emphasizes key design constraints and concepts. Results demonstrate that systematic prompt formatting combined with targeted refinement can substantially boost LLM effectiveness in HDL design automation, with Claude 3 Opus achieving 90% success on one-hot FSM designs and significant improvements for challenging synchronous reset implementations.

## Method Summary
The study evaluates LLM performance on 20 FSM design problems from HDLBits using systematic markdown format prompts with Specification, Example Behavior (waveforms as tables), and Module Declaration sections. Each problem is presented to three LLMs (Claude 3 Opus, ChatGPT-4, ChatGPT-4o) over 5 independent chat sessions. For challenging problems (synchronous reset and one-hot FSMs), a TOP Patch with numbered to-do items emphasizing key concepts is appended. Success is measured by whether generated SystemVerilog code passes HDLBits evaluation, using single-shot prompting without iterative refinement.

## Key Results
- Claude 3 Opus achieved the highest baseline success rate of 41% across all FSM problems
- TOP Patch improved ChatGPT-4o's synchronous reset FSM success rate from 30% to 70%
- Claude 3 Opus achieved 90% success rate on one-hot FSM designs with TOP Patch
- Systematic markdown formatting and targeted refinement substantially enhance LLM performance in HDL design automation

## Why This Works (Mechanism)
The TOP Patch technique works by explicitly structuring the prompt to guide LLM reasoning through sequential, concept-focused tasks before implementation. By breaking down complex FSM design into discrete to-do items that emphasize critical concepts like synchronous reset behavior or one-hot encoding constraints, the approach helps LLMs avoid common pitfalls such as defaulting to asynchronous resets or incorrectly deriving state transition logic. The systematic markdown format with clear sections provides structured context that reduces ambiguity in problem interpretation.

## Foundational Learning
- **HDLBits problem format**: Standardized digital design problems with clear specifications and testbenches; needed for consistent evaluation across LLM trials
- **SystemVerilog FSM implementation**: Knowledge of state encoding (binary, one-hot), synchronous vs asynchronous reset, and always block structure; needed for correct hardware description
- **Markdown prompt engineering**: Structured presentation of problem specifications, examples, and requirements; needed to provide clear context to LLMs
- **Single-shot vs iterative prompting**: Understanding the limitations of one-time generation versus feedback-driven refinement; needed to interpret performance metrics
- **One-hot state encoding**: Each state represented by single active bit; needed for efficient next-state logic in certain FSM applications
- **Synchronous reset behavior**: Reset signal sampled only on clock edge; needed to avoid incorrect asynchronous default implementations

## Architecture Onboarding
**Component Map**: Problem Specification → Markdown Formatting → LLM Prompt → SystemVerilog Code → HDLBits Testbench → Success/Failure
**Critical Path**: Prompt generation → LLM response → Code compilation → Testbench evaluation
**Design Tradeoffs**: Single-shot prompting maximizes efficiency but limits refinement opportunities; systematic formatting improves consistency but may constrain creative solutions
**Failure Signatures**: Multi-driver issues from conflicting assignments, incorrect reset type (async vs sync), improper state encoding equations
**First Experiments**: 1) Evaluate single TOP Patch to-do item removal to identify critical constraints, 2) Test alternative markdown structures while keeping TOP Patch constant, 3) Compare performance on problems with/without Example Behavior tables

## Open Questions the Paper Calls Out
None

## Limitations
- Binary pass/fail evaluation metric doesn't capture partial correctness or code quality
- 5-trial design may not adequately sample stochastic variability without reported temperature parameters
- TOP Patch effectiveness only demonstrated on synchronous reset and one-hot FSMs, not broader FSM types
- Single-shot prompting limits generalizability to real-world iterative design workflows

## Confidence
**High Confidence**: Claude 3 Opus baseline performance (41% success rate) and TOP Patch improvement on synchronous reset FSMs (30% → 70%)
**Medium Confidence**: Comparative performance ranking between Claude 3 Opus and ChatGPT variants
**Low Confidence**: Generalizability of TOP Patch to other FSM types and hardware description languages

## Next Checks
1. **Ablation Study on Prompt Components**: Systematically remove individual TOP Patch to-do items to identify which specific constraints drive performance improvements
2. **Multi-Iteration Evaluation Protocol**: Implement feedback-driven refinement cycles to measure performance changes beyond single-shot prompting
3. **Cross-Validation with Alternative Testbenches**: Re-evaluate successful designs using independent testbenches to verify functional correctness beyond HDLBits specific test vectors