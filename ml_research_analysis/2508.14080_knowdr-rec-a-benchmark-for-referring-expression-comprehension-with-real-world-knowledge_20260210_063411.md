---
ver: rpa2
title: 'KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World
  Knowledge'
arxiv_id: '2508.14080'
source_url: https://arxiv.org/abs/2508.14080
tags:
- visual
- arxiv
- reasoning
- negative
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KnowDR-REC, a benchmark for evaluating Multi-modal
  Large Language Models (MLLMs) on knowledge-driven Referring Expression Comprehension
  (REC) tasks. Traditional REC benchmarks rely on intra-image cues and lack fine-grained
  annotations, failing to assess MLLMs' ability to integrate external knowledge and
  perform multi-hop reasoning.
---

# KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge

## Quick Facts
- arXiv ID: 2508.14080
- Source URL: https://arxiv.org/abs/2508.14080
- Reference count: 9
- Introduces a benchmark addressing limitations in traditional REC benchmarks for MLLMs

## Executive Summary
KnowDR-REC addresses critical gaps in existing Referring Expression Comprehension (REC) benchmarks by incorporating real-world knowledge, requiring fine-grained multimodal reasoning, and including negative samples constructed via temporal knowledge graph-based expression editing. The benchmark evaluates Multi-modal Large Language Models (MLLMs) on knowledge-intensive visual grounding tasks, revealing that current state-of-the-art models struggle with genuine semantic understanding and often rely on memorized shortcuts. KnowDR-REC introduces novel evaluation metrics to probe models' internal reasoning processes and assess their robustness and anti-hallucination abilities.

## Method Summary
KnowDR-REC constructs a comprehensive benchmark that moves beyond traditional intra-image cues by integrating external knowledge and requiring multi-hop reasoning. The benchmark incorporates negative samples generated through temporal knowledge graph-based expression editing to test model robustness. Novel evaluation metrics are introduced to examine models' internal reasoning processes. The benchmark was evaluated across 16 state-of-the-art MLLMs to assess their performance on knowledge-driven visual grounding tasks.

## Key Results
- Existing MLLMs struggle with knowledge-intensive visual grounding tasks
- Models tend to rely on memorized shortcuts rather than genuine semantic understanding
- Traditional REC benchmarks fail to assess MLLMs' ability to integrate external knowledge
- KnowDR-REC successfully identifies limitations in current multimodal systems' reasoning capabilities

## Why This Works (Mechanism)
None

## Foundational Learning
- **Multi-modal Large Language Models (MLLMs)**: AI systems that process both visual and textual information - needed for understanding the target models being evaluated
- **Referring Expression Comprehension (REC)**: Task of identifying objects in images based on linguistic descriptions - the core problem domain
- **Temporal Knowledge Graphs**: Structured representations of knowledge that capture temporal relationships - used for constructing negative samples
- **Anti-hallucination**: Models' ability to avoid generating or relying on incorrect information - a key capability being tested
- **Multi-hop reasoning**: Capability to perform reasoning across multiple knowledge steps - required for complex knowledge integration
- **Fine-grained multimodal reasoning**: Detailed analysis combining visual and textual information - essential for the benchmark's evaluation criteria

## Architecture Onboarding
- **Component Map**: MLLMs -> KnowDR-REC Benchmark -> Evaluation Metrics -> Performance Analysis
- **Critical Path**: Knowledge Integration → Visual Grounding → Reasoning Assessment → Anti-hallucination Testing
- **Design Tradeoffs**: Manual negative sample construction vs. automation, comprehensive knowledge coverage vs. benchmark complexity, novel metrics vs. established evaluation standards
- **Failure Signatures**: Over-reliance on visual cues without knowledge integration, susceptibility to expression editing, inability to perform multi-step reasoning
- **First Experiments**: 1) Baseline performance comparison across 16 MLLMs, 2) Analysis of knowledge dependency patterns, 3) Evaluation of anti-hallucination robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Manual construction of negative samples via temporal knowledge graph-based expression editing may introduce construction biases
- Novel evaluation metrics lack external validation and correlation with human reasoning quality judgments
- Benchmark primarily focuses on object-centric scenes, potentially limiting generalizability to abstract concepts and complex real-world scenarios
- Limited characterization of multi-hop reasoning complexity across different knowledge dependency types

## Confidence
- **High Confidence**: Existing MLLMs struggle with knowledge-intensive visual grounding (supported by empirical evaluations)
- **Medium Confidence**: Models rely on memorized shortcuts rather than genuine semantic understanding (supported but needs ablation studies)
- **Medium Confidence**: Traditional REC benchmarks fail to assess external knowledge integration (reasonable but depends on definition of "external knowledge")

## Next Checks
1. Conduct human evaluation studies to validate correlation between novel reasoning metrics and human judgments of reasoning quality
2. Perform systematic analysis of negative sample construction process to quantify potential biases
3. Extend evaluations to diverse scene types including abstract concepts, actions, and non-visual knowledge domains to assess generalizability