---
ver: rpa2
title: 'Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model
  Performance in a Low-Resource Setting'
arxiv_id: '2510.20957'
source_url: https://arxiv.org/abs/2510.20957
tags:
- language
- irish
- linguistic
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Irish-BLiMP, a novel benchmark for evaluating
  the grammatical competence of language models in Irish, a low-resource language.
  The authors manually constructed 1,020 minimal pairs across 11 linguistic features,
  covering a range of syntactic phenomena specific to Irish.
---

# Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting

## Quick Facts
- **arXiv ID**: 2510.20957
- **Source URL**: https://arxiv.org/abs/2510.20957
- **Reference count**: 0
- **Primary result**: Humans achieve 90.1% accuracy on Irish grammatical competence vs. best model at 73.5%, with open-source models near random chance

## Executive Summary
Irish-BLiMP introduces the first linguistic benchmark for evaluating grammatical competence in Irish, a low-resource language. The benchmark consists of 1,020 manually constructed minimal pairs across 11 syntactic feature types, designed to test models' understanding of Irish-specific grammatical phenomena. The evaluation compares both open- and closed-source large language models against fluent human speakers using a multiple-choice task. Results reveal a significant performance gap between humans (90.1% accuracy) and the best model (gpt-5 at 73.5%), with open-source models performing near random chance. The study demonstrates that current language models, even when fine-tuned on Irish data, struggle to achieve human-level grammatical competence in extremely low-resource settings.

## Method Summary
The benchmark uses 1,020 minimal pairs created by fluent Irish speakers, each pair consisting of an acceptable and unacceptable sentence differing in one grammatical feature. These pairs span 11 linguistic features and 102 paradigms (10 pairs per paradigm). Models are evaluated using three prompting strategies: zero-shot (baseline), few-shot (5 examples), and grammar-book context (paradigm description provided). For open-source models, log-likelihoods of each choice are compared; for closed-source models, exact string matching of "A" or "B" responses is required with temperature=0. Human performance is established using 3 native speakers who annotated all 3,060 items.

## Key Results
- Humans achieve 90.1% average accuracy, significantly outperforming all models (best: gpt-5 at 73.5%)
- Open-source models perform near random chance (50%), indicating lack of Irish grammatical knowledge
- Large performance gap between closed-source (gpt-5: 73.5%) and open-source models
- Models and humans struggle on different grammatical aspects, suggesting models rely on surface pattern recognition rather than deep grammatical understanding

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its minimal pair design, which isolates specific grammatical features and forces models to demonstrate true syntactic competence rather than relying on semantic or lexical cues. The multiple-choice format provides clear evaluation criteria, while the inclusion of both open- and closed-source models allows comparison across different model architectures and training approaches.

## Foundational Learning
**Irish grammatical features** (why needed: Benchmark tests specific syntactic phenomena unique to Irish; quick check: Review the 11 feature types listed in the paper)
**Minimal pair construction** (why needed: Core methodology for isolating grammatical features; quick check: Verify that each pair differs in exactly one grammatical aspect)
**Log-likelihood comparison** (why needed: Method for evaluating open-source models without exact string matching; quick check: Confirm calculation of P(A) vs P(B) for each pair)
**Zero-shot prompting** (why needed: Establishes baseline human-comparable performance; quick check: Verify no additional examples provided beyond the test item)

## Architecture Onboarding

**Component map**: Dataset creation -> Prompt strategies -> Model evaluation -> Human baseline comparison

**Critical path**: Dataset → Prompting → Evaluation → Analysis. The pipeline flows from benchmark construction through different prompting approaches to final accuracy comparison between models and humans.

**Design tradeoffs**: The benchmark sacrifices breadth (limited to 11 features) for depth and precision in testing grammatical competence. Exact string matching for closed-source models is simple but fragile when models generate explanations instead of labels.

**Failure signatures**: Open-source models at ~50% accuracy indicate lack of Irish grammatical knowledge rather than evaluation error. Models generating explanations instead of "A"/"B" labels indicate prompt engineering issues.

**First experiments**: 1) Test few-shot prompting with 5 additional examples per paradigm, 2) Evaluate with grammar-book context descriptions provided, 3) Compare performance across different closed-source models using same exact-match protocol.

## Open Questions the Paper Calls Out
**Open Question 1**: How does LLM performance vary when evaluating dialect-specific Irish constructions compared to the standard written form (An Caighdeán Oifigiúil)? The authors explicitly state in the Conclusion and Limitations that future work should "expand the dataset to include dialect-specific constructions and phonologically conditioned variation." The current study deliberately restricted the benchmark to the official standard grammar to ensure consistency, leaving dialectal performance untested.

**Open Question 2**: What specific training methodologies are required to improve grammatical competence in low-resource languages, given that standard fine-tuning failed to yield improvements? The results show the Irish-specific fine-tuned model (UCCIX) underperformed its base model, leading the authors to conclude that "fine-tuning alone may be insufficient" and calling for "more efforts... for effective multilingual transfer."

**Open Question 3**: Can prompting strategies or training objectives be modified to force models to learn human-like internal representations of grammar rather than relying on surface pattern recognition? The authors find a weak correlation between model and human errors, concluding that models rely on "surface pattern recognition" and have not developed "human-like representation[s] of Irish grammar."

## Limitations
- Exact string matching evaluation is fragile for closed-source models that may generate explanations instead of labels
- No inter-annotator agreement statistics reported for the gold standard minimal pairs
- Human baseline had prior exposure to paradigms while models received only specified prompts
- Dataset limited to standard written Irish, excluding dialectal variation

## Confidence

**High confidence**: Humans significantly outperform all evaluated language models on Irish grammatical competence (90.1% vs. 73.5% for best model)

**Medium confidence**: Open-source models performing near random chance - specific models and exact performance values need verification

**Medium confidence**: Models and humans struggle on different grammatical phenomena - detailed breakdown across feature types not fully detailed in paper

## Next Checks

1. **Dataset verification**: Request access to Irish-BLiMP dataset and verify distribution of minimal pairs across 11 linguistic features, checking for potential imbalances

2. **Prompt template validation**: Obtain and test exact prompt templates used for each strategy (zero-shot, few-shot, grammar-book context) to ensure faithful reproduction

3. **Cross-model comparison**: Replicate evaluation on at least one additional closed-source model using exact-match protocol to confirm reported performance patterns