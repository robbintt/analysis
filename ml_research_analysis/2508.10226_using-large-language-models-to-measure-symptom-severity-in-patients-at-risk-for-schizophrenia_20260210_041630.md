---
ver: rpa2
title: Using Large Language Models to Measure Symptom Severity in Patients At Risk
  for Schizophrenia
arxiv_id: '2508.10226'
source_url: https://arxiv.org/abs/2508.10226
tags:
- bprs
- transcripts
- scores
- data
- patients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can accurately
  predict Brief Psychiatric Rating Scale (BPRS) scores from clinical interview transcripts
  of patients at clinical high risk (CHR) for schizophrenia. Using a zero-shot approach
  without model fine-tuning, the LLM achieved a median concordance of 0.84 and ICC
  of 0.73 on PSYCHS transcripts, approaching human inter- and intra-rater reliability
  levels.
---

# Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia

## Quick Facts
- arXiv ID: 2508.10226
- Source URL: https://arxiv.org/abs/2508.10226
- Reference count: 0
- This study demonstrates that large language models can accurately predict Brief Psychiatric Rating Scale scores from clinical interview transcripts of patients at clinical high risk for schizophrenia.

## Executive Summary
This study demonstrates that large language models (LLMs) can accurately predict Brief Psychiatric Rating Scale (BPRS) scores from clinical interview transcripts of patients at clinical high risk (CHR) for schizophrenia. Using a zero-shot approach without model fine-tuning, the LLM achieved a median concordance of 0.84 and ICC of 0.73 on PSYCHS transcripts, approaching human inter- and intra-rater reliability levels. Performance was better on semi-structured PSYCHS interviews compared to unstructured open interviews, with particular accuracy in predicting self-reported symptoms. The model also successfully processed foreign language transcripts with comparable accuracy and improved predictions when incorporating longitudinal data through one-shot or few-shot learning approaches.

## Method Summary
The study analyzed 409 CHR patients from the AMP-SCZ cohort, using paired BPRS assessments and clinical interview transcripts (308 open transcripts and 433 PSYCHS transcripts). Using OpenAI's o3-mini model in zero-shot mode, the researchers predicted BPRS scores by including the full 24-item BPRS instruction manual in the system prompt. For longitudinal analysis, they tested one-shot and two-shot learning by including previous transcripts with ground-truth scores. Performance was evaluated using median concordance, ICC(3,k), Pearson correlation, and RMSE metrics. The model successfully processed transcripts in multiple languages including Spanish, Korean, and Italian.

## Key Results
- Zero-shot LLM predictions achieved median concordance of 0.84 and ICC of 0.73 on PSYCHS transcripts
- Performance was better for semi-structured PSYCHS interviews (ICC 0.73) compared to open-ended interviews (ICC 0.42)
- Self-reported symptoms were predicted more accurately than observed symptoms (p=0.0001)
- Incorporating one prior transcript-score pair improved RMSE from 7.19 to 6.32

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Clinical Scale Mapping
Transformer-based models encode the BPRS instruction manual in-context and align transcript content against symptom definitions using learned semantic relationships between clinical descriptions and scale anchors. The model's pre-training corpus contains sufficient clinical language and rating scale concepts to ground the mapping, but domain-specific scale definitions are required in-context as the model could not reliably list the 24 BPRS items without the full instruction manual.

### Mechanism 2: Interview Structure Alignment
Semi-structured interviews yield more accurate predictions than open-ended conversations because they systematically elicit symptom-relevant content. PSYCHS interviews probe specific domains (hallucinations, unusual beliefs) that overlap with BPRS items, providing denser signal for the model to extract. Self-reported symptoms expressed in patient language are more recoverable from text than clinician-observed behaviors, which explains why affective and positive symptoms were consistently underpredicted from open interviews.

### Mechanism 3: Longitudinal Context Calibration
Providing prior transcripts with ground-truth scores improves current predictions through individual-specific calibration. N-shot prompting supplies exemplars that anchor the model's scoring to the patient's baseline, reducing systematic under/over-estimation. Prior human-rated scores serve as valid ground truth, and the model learns to adjust its interpretation threshold from examples, with RMSE improvement of approximately 12% with just one prior example.

## Foundational Learning

- Concept: **Zero-Shot vs. Fine-Tuned Inference** - Why needed here: The paper claims strong performance without model training, but this depends critically on what's already in the model's knowledge base. Quick check: Can you explain why the BPRS manual had to be included in the prompt, unlike PHQ-8 in depression studies?
- Concept: **Intraclass Correlation Coefficient (ICC) and Concordance** - Why needed here: These are the primary evaluation metrics; understanding them is necessary to interpret claims about "approaching human reliability." Quick check: Why is ICC(3,k) the appropriate variant for this use case per the authors' rationale?
- Concept: **Self-Reported vs. Observed Symptoms in Text-Based Assessment** - Why needed here: This distinction explains the asymmetric accuracy pattern and identifies fundamental limits of transcript-only approaches. Quick check: Which BPRS subscores would you expect to remain difficult even with perfect transcripts, and why?

## Architecture Onboarding

- Component map: Raw transcript text -> Prompt assembly (system instructions + BPRS manual + optional longitudinal context) -> LLM inference (OpenAI o3-mini API) -> Output parsing (extract 24 subscores + explanations) -> Evaluation (compute concordance, ICC, Pearson r)
- Critical path: Transcript availability -> BPRS manual in prompt (required) -> Structured output schema enforcement -> Subscore extraction -> Aggregation and metric computation
- Design tradeoffs: Zero-shot vs. n-shot (zero-shot is simpler but n-shot reduces RMSE by ~12% with one prior example); PSYCHS vs. open interviews (PSYCHS yields higher accuracy but requires structured protocol); JSON output vs. free text (structured output ensures all 24 items are scored but may constrain explanation quality)
- Failure signatures: Model outputs fewer than 24 subscores (incomplete extraction); systematic underestimation of affective symptoms (indicates insufficient symptom probing); non-English explanations when English was requested; large score variance across re-runs (prompt sensitivity)
- First 3 experiments: 1) Run the same transcript through the model 10× with identical prompts to measure subscore variance; 2) Remove the full BPRS manual from the prompt and measure accuracy drop; 3) With held-out patients having 3+ timepoints, test 0-shot through 2-shot to determine if RMSE improvement is linear or diminishing

## Open Questions the Paper Calls Out

Can LLMs accurately assess CHR-specific scales (SIPS, CAARMS, PSYCHS) that may be more sensitive for predicting psychosis conversion than BPRS? The authors note they cannot yet determine whether LLMs can assess these disease-specific scales because the PSYCHS is too new for the LLM to even list its component items, and the AMP-SCZ data for these instruments was too incomplete at the time of analysis.

Does LLM prediction accuracy continue to improve with additional longitudinal timepoints, or does performance saturate beyond 2-3 encounters? Only 45 participants had three usable time-points, and none had more than three. Larger longitudinal sequences would be necessary to probe whether performance improvements saturate or continue to accrue with additional context.

Can multimodal approaches incorporating video or audio data improve detection of clinician-observed BPRS symptoms? Future multimodal approaches that utilize video or direct audio data may have improved detection of these observed features, as observed symptoms were predicted significantly worse than self-reported symptoms.

## Limitations

The study demonstrates promising zero-shot BPRS prediction but has key limitations including unknown prompt stability across model versions, lack of independent validation dataset outside AMP-SCZ, and incomplete characterization of performance on rare or severe symptom presentations. The reliance on in-context BPRS manual inclusion raises questions about practical deployment without manual access, and the performance gap between semi-structured and open interviews suggests fundamental limits to unstructured data capture.

## Confidence

- High confidence: Zero-shot mapping capability for PSYCHS transcripts achieving median concordance ≥0.84 and ICC ≥0.73; n-shot learning improvement with prior exemplars
- Medium confidence: Generalizability to foreign language transcripts and open interviews; systematic under-prediction of affective/positive symptoms in unstructured data
- Low confidence: Performance on rare symptom combinations; robustness across different LLM model versions; clinical utility in real-world deployment

## Next Checks

1. **Prompt stability validation**: Test 10+ re-runs of identical transcripts with same prompt to quantify score variance and establish confidence intervals for individual predictions.
2. **External dataset validation**: Apply the same zero-shot approach to a held-out independent clinical interview dataset to assess generalizability beyond AMP-SCZ cohort.
3. **Ablation study on BPRS manual**: Systematically remove portions of the BPRS instruction manual from the prompt to identify minimum required content for reliable scoring.