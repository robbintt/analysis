---
ver: rpa2
title: 'VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking,
  and Viewing'
arxiv_id: '2509.22651'
source_url: https://arxiv.org/abs/2509.22651
tags:
- answer
- audio
- speaking
- assistant
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing

## Quick Facts
- arXiv ID: 2509.22651
- Source URL: https://arxiv.org/abs/2509.22651
- Reference count: 40
- Key outcome: Mid-sized models (7B) can outperform much larger models on audio understanding tasks

## Executive Summary
VoiceAssistant-Eval introduces a comprehensive benchmark for evaluating AI voice assistants across listening, speaking, and viewing capabilities. The benchmark addresses four key weaknesses in existing evaluations: lack of voice personalization, limited hands-free interaction assessment, neglect of diverse audio contexts, and insufficient multimodal integration testing. Through 10,497 curated examples spanning 13 task categories, the benchmark reveals that most models excel at speaking tasks but lag in audio understanding, and that well-designed smaller models can rival much larger ones.

## Method Summary
The benchmark evaluates models using a triadic system measuring content quality (via gpt-oss-20b scoring), speech quality (via UTMOS), and consistency (via modified WER). It includes 10,497 curated examples from 47 source datasets, supporting hands-free audio instructions and audio+image queries. Models are evaluated across 13 task categories including natural sounds, music, spoken dialogue, multi-turn conversation, role-play imitation, and highly heterogeneous images. The evaluation uses standardized generation parameters and provides both automated metrics and role-play extensions with speaker similarity scoring.

## Key Results
- Most models perform substantially better on speaking-oriented tasks than on audio understanding tasks (20 of 22 models score higher on speaking)
- Mid-sized models like Step-Audio-2-mini (7B) achieve more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual
- Audio+image queries show a significant performance gap compared to text+image queries (16.3-point accuracy drop for Qwen2.5-Omni-7B)

## Why This Works (Mechanism)
### Mechanism 1
A comprehensive, task-diverse benchmark can reveal hidden performance asymmetries and capability gaps that narrower benchmarks miss. By spanning 13 task categories designed to address documented weaknesses (voice personalization, hands-free interaction, diverse audio contexts, multimodal integration), the benchmark forces models to demonstrate capabilities across varied input modalities. The triadic evaluation multiplies error sources, exposing mismatches between what models say and how they say it.

### Mechanism 2
Current voice-enabled models exhibit a structural imbalance: speech generation ("speaking") is more developed than audio perception ("listening"), likely due to stronger LLM backbones versus weaker audio encoders. The consistent finding that 20 of 22 models score higher on speaking than listening suggests the bottleneck lies in processing input audio rather than generating output speech.

### Mechanism 3
Model size alone is a poor predictor of performance on complex, multimodal voice tasks; architecture, training data, and targeted alignment matter more. The benchmark demonstrates that mid-sized, well-designed models can outperform much larger ones, as shown by Step-Audio-2-mini (7B) doubling the listening accuracy of LLaMA-Omni2-32B.

## Foundational Learning
- **Multi-modal Integration (Audio-Visual Understanding)**: Essential for tasks requiring joint reasoning over images and audio. Quick check: Can you explain why a model might accurately answer a question when presented with an image and text, but fail when the same question is spoken aloud?
- **Speech-to-Speech Model Architecture**: Understanding the role of audio encoder, LLM backbone, and speech decoder is key to interpreting performance trade-offs. Quick check: What are the potential advantages and disadvantages of an end-to-end speech-to-speech model compared to a pipeline of ASR, text LLM, and TTS?
- **Benchmark Design for Robustness and Safety**: Models trained only on clean, well-formatted audio may underperform on robustness tasks involving background noise or interruptions. Quick check: How might a model trained only on clean, well-formatted audio underperform on a robustness task involving background noise or interruptions?

## Architecture Onboarding
- **Component map**: Dataset (10,497 curated examples) → Triadic Evaluation System (gpt-oss-20b content scorer, UTMOS speech quality, WER consistency) → Role-play extension (speaker similarity via Wespeaker)
- **Critical path**: Set up evaluation environment → Run inference on dataset with target model → Apply triadic scoring → Aggregate scores into Listening, Speaking, Viewing, and overall averages
- **Design tradeoffs**: Data diversity vs. curation effort; automated evaluation vs. human alignment; task breadth vs. depth
- **Failure signatures**: Context Loss Error in Listening (46%), Vision Perception Error in Viewing (50%), Requirement Deviation in Speaking (23%), Unsafe Content in Safety
- **First 3 experiments**: 1) Reproduce baseline scores for Qwen2.5-Omni-7B; 2) Compare performance on text+image vs. audio+image queries; 3) Compare safety-aligned vs. unaligned models on Safety and Robustness tasks

## Open Questions the Paper Calls Out
1. How can the architectural integration of audio and visual encoders be improved to close the significant performance gap between audio+image inputs and text+image inputs?
2. What training methodologies can resolve the observed trade-off between voice imitation fidelity and speech naturalness in role-play tasks?
3. How does the reliability and safety alignment of voice assistants degrade when evaluated under real-time constraints such as latency, barge-in, and continuous streaming?

## Limitations
- The benchmark relies on automated evaluation metrics that may introduce biases despite strong human correlation
- Dataset curation completeness cannot be fully established without additional field validation
- The controlled evaluation environment may not capture all deployment complexities of real-world voice assistant use

## Confidence
- **High confidence**: Structural asymmetry finding (speaking > listening) is robustly supported by systematic evaluation
- **Medium confidence**: Claims about architectural bottlenecks are mechanistically plausible but alternative explanations exist
- **Low confidence**: Extrapolations about real-world performance based solely on this benchmark should be treated cautiously

## Next Checks
1. Conduct human validation study on stratified sample of benchmark outputs to identify systematic scoring biases
2. Evaluate models on external, independently curated voice assistant dataset to assess generalization
3. Perform ablation study on task complexity to determine if listening-speaking asymmetry persists across difficulty levels