---
ver: rpa2
title: 'Table Question Answering in the Era of Large Language Models: A Comprehensive
  Survey of Tasks, Methods, and Evaluation'
arxiv_id: '2510.09671'
source_url: https://arxiv.org/abs/2510.09671
tags:
- table
- zhang
- linguistics
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically organizes table question answering (TQA)
  research in the LLM era, categorizing diverse task setups, modeling approaches,
  and evaluation methods. It highlights the shift from retrieval-only to reasoning-intensive
  setups, the integration of multimodal data (text, images, charts), and the growing
  role of RLVR and interpretable reasoning.
---

# Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation

## Quick Facts
- arXiv ID: 2510.09671
- Source URL: https://arxiv.org/abs/2510.09671
- Authors: Wei Zhou; Bolei Ma; Annemarie Friedrich; Mohsen Mesgar
- Reference count: 40
- This survey systematically organizes table question answering (TQA) research in the LLM era, categorizing diverse task setups, modeling approaches, and evaluation methods.

## Executive Summary
This survey systematically organizes table question answering (TQA) research in the LLM era, categorizing diverse task setups, modeling approaches, and evaluation methods. It highlights the shift from retrieval-only to reasoning-intensive setups, the integration of multimodal data (text, images, charts), and the growing role of RLVR and interpretable reasoning. Open-domain and multilingual benchmarks are emphasized as emerging frontiers. While (M)LLMs show promise, challenges remain in handling noisy, large, and hierarchical tables, and in achieving faithful, robust explanations. The work identifies key research gaps and offers a consolidated roadmap for future TQA development.

## Method Summary
The survey provides a comprehensive review of TQA methods by categorizing them into task types (SQA, MQA, OTA, TQA), modeling approaches (textual/visual modeling, tool-augmented reasoning, retrieval augmentation), and evaluation methods. It analyzes performance trends across different datasets and identifies key challenges such as reasoning complexity, interpretability, and robustness. The methodology involves synthesizing findings from existing literature, including experimental results from various TQA benchmarks, while acknowledging limitations in prompt engineering and rendering specifications.

## Key Results
- RLVR-trained models exhibit better generalizability and robustness to perturbations than SFT models
- Tool-augmented reasoning (code generation) increases numerical accuracy and reduces hallucination
- Multi-stage retrieval effectively mitigates LLM context length limits and noise sensitivity
- Current explanations often lack faithfulness, focusing on post-hoc justifications rather than genuine reasoning transparency

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Reasoning (Code Generation)
- Claim: If an LLM generates and executes code (Python/SQL) rather than predicting answers directly, numerical accuracy increases and hallucination decreases.
- Mechanism: The LLM functions as a semantic parser, translating natural language logic into deterministic machine code. The runtime environment handles the calculation, bypassing the LLM's probabilistic weakness in arithmetic.
- Core assumption: The LLM possesses sufficient coding proficiency to map table semantics to correct syntax (Assumption: requires strong base code capabilities).
- Evidence anchors:
  - [section 3.2] "Models generate and execute Python or SQL code to obtain reasoning results."
  - [corpus] "TableReasoner" paper abstract highlights a "programming-empowered" framework to address entity ambiguity and incomplete semantics.
- Break condition: If the table schema is ambiguous or the question requires implicit common-sense knowledge not translatable to code, the generated code will fail or produce empty results.

### Mechanism 2: Reinforcement Learning with Verifiable Rewards (RLVR)
- Claim: If models are trained using RLVR (e.g., rewarding correct final answers or executable programs), they exhibit better generalizability and robustness than models trained solely on supervised fine-tuning (SFT).
- Mechanism: RLVR encourages the model to explore diverse reasoning paths to maximize a verifiable outcome reward, rather than mimicking the specific token distribution of a reference dataset (SFT). This reduces spurious correlations.
- Core assumption: The reward signal (e.g., answer correctness) is reliable and attainable during the exploration phase.
- Evidence anchors:
  - [section 5] "RLVR-trained models exhibit better generalizability... and increased robustness to row and column perturbations."
  - [corpus] "Exploring Generative Process Reward Modeling..." abstract suggests process reward models enhance complex reasoning in semi-structured data.
- Break condition: If the reward is sparse or the task requires creative explanation (free-form answers) where "correctness" is subjective, RLVR optimization may be unstable.

### Mechanism 3: Multi-Stage Retrieval for Large Inputs
- Claim: If a system employs a retriever (fine-tuned or embedding-based) to select relevant sub-tables or cells before reasoning, it mitigates the context length limits and noise sensitivity of LLMs.
- Mechanism: This reduces the input space from a potentially massive database to a manageable context window, lowering the probability of the LLM getting "lost in the middle" or hallucinating on irrelevant data.
- Core assumption: The relevant information is localized and can be separated from noise without losing necessary context.
- Evidence anchors:
  - [section 3.3] Discusses "fine-tune retrievers to identify the most relevant cells" and embedding queries for semantic matching.
  - [corpus] "CRAFT: Training-Free Cascaded Retrieval..." abstract proposes cascaded retrieval specifically to handle large-scale retrieval costs and accuracy.
- Break condition: If a question requires global statistics (e.g., "What is the average of all rows?") or cross-table synthesis where relevant signals are dispersed, aggressive retrieval may filter out essential data.

## Foundational Learning

- Concept: **Text-to-SQL vs. Semantic Parsing**
  - Why needed here: The paper categorizes TQA methods by whether they use intermediate formal languages (SQL, Python). Understanding the difference between generating a deterministic query (Text-to-SQL) and direct answer generation is crucial for Section 3.2.
  - Quick check question: Can you explain why generating a SQL query is generally safer for numerical questions than generating a text answer directly?

- Concept: **Context Window & Attention**
  - Why needed here: The "Large Inputs" challenge (Section 3.3) is fundamentally about the quadratic scaling of attention mechanisms and hard token limits.
  - Quick check question: Why does increasing table size linearly often degrade LLM performance non-linearly?

- Concept: **Multimodal Representation Learning**
  - Why needed here: The survey distinguishes between "Textual Table Modeling" and "Visual Table Modeling" (Section 3.1). Understanding how encoders map pixel data vs. text embeddings is key to Section 5's discussion on hybrid representations.
  - Quick check question: What specific information might a vision encoder capture from a table image that a text serializer (Markdown/JSON) might lose?

## Architecture Onboarding

- Component map: Input Layer (Table Serializer/Vision Encoder) -> Retriever (Optional) -> Reasoner (LLM) -> Tool Interface (Python/SQL execution) -> Evaluator (LLM-as-Judge/EM logic)
- Critical path: Question + Table -> (Retrieval if large) -> LLM Reasoning (Plan) -> Code Generation -> Execution -> Final Answer
- Design tradeoffs:
  - **Tuning-Free (Agentic)**: High accuracy on complex reasoning, but slow/expensive inference
  - **Tuning-Based (Specialized)**: Faster inference, but risks "out-of-domain performance degradation" (Section 3.2 Discussion)
  - **Representation**: Text is robust for content; Images capture layout but suffer from OCR errors (Section 3.1)
- Failure signatures:
  - **Shuffling Sensitivity**: Performance drops when table rows are reordered, indicating the model relies on position rather than content (Section 4.2)
  - **Format Mismatch**: "Jan 1" vs "01-01" causing EM failures (Section 4.1)
  - **Empty Execution**: Generated code fails to execute against the specific table schema
- First 3 experiments:
  1. **Representation Baseline**: Test a standard LLM (e.g., GPT-4/Llama-3) on a small subset of tables (WTQ/TabFact) using Markdown serialization vs. Image cropping. Evaluate exact match.
  2. **Agentic Workflow**: Implement a simple Python-agent loop. Ask the model to write a Pandas script to answer a question, execute it, and return the result. Compare accuracy against the baseline.
  3. **Robustness Test**: Take the dataset from Exp 1, randomly shuffle the rows of the tables, and re-evaluate. A significant drop indicates the model lacks structural robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can TQA systems dynamically integrate textual and visual table representations to capture complementary signals (layout from images, semantics from text) without relying on inflexible ensemble strategies?
- **Basis in paper:** [explicit] The authors state that current ensemble approaches selecting optimal representations based on specific problem features "lack flexibility when new features emerge or when interactions among multiple features need to be considered." They explicitly advocate for integrating dedicated encoders to capture complementary signals.
- **Why unresolved:** Existing methods use late-selection ensembles rather than early-fusion architectures, and the optimal fusion strategy for table layout and semantic content remains unexplored.
- **What evidence would resolve it:** Development and evaluation of early-fusion models with separate vision and text encoders, demonstrating consistent improvements over single-modality and ensemble baselines across diverse table formats and sizes.

### Open Question 2
- **Question:** Can TQA systems be developed to natively handle low-resource and multilingual inputs, overcoming the distribution shifts and lexical diversity that limit translation-based approaches?
- **Basis in paper:** [explicit] The authors conclude that "Progress will require TQA systems that natively handle low-resource and multilingual inputs rather than relying solely on translation pipelines," citing uneven LLM performance across language families and inconsistent benefits from multilingual fine-tuning.
- **Why unresolved:** Translation-based approaches fail due to quality dependencies, and current LLMs exhibit performance disparities rooted in pretraining data representation.
- **What evidence would resolve it:** Creation of native multilingual TQA benchmarks for low-resource languages, and models trained jointly on multilingual table-question pairs that outperform translate-then-process pipelines.

### Open Question 3
- **Question:** How can TQA systems generate reasoning explanations that faithfully reflect the model's actual decision-making process, rather than producing plausible but unfaithful post-hoc justifications?
- **Basis in paper:** [explicit] The authors argue that "much of the current work on interpretability in TQA focuses on generating post-hoc justifications for answers, rather than genuine explanations that transparently reveal the reasoning process underlying answer derivation," citing the unfaithfulness problem from chain-of-thought research.
- **Why unresolved:** Current explanations may appear coherent while not representing actual model reasoning, and metrics for faithfulness specific to tabular reasoning are underdeveloped.
- **What evidence would resolve it:** Development of faithfulness evaluation protocols for TQA (e.g., intervention studies, counterfactual reasoning tests) and models trained with process-level supervision to align explanations with internal computation.

### Open Question 4
- **Question:** What retrieval-augmented methods can effectively locate relevant information from large or multiple tables without information loss and with generalizability across diverse table formats?
- **Basis in paper:** [inferred] The authors note that "Directly using LLMs to retrieve relevant cells or tables can lead to information loss" and that fine-tuned retrievers have "generalizability to diverse table formats remains limited," implying a need for robust cross-format retrieval.
- **Why unresolved:** Existing retrievers are format-specific, and LLM-based selection risks filtering out critical context for complex reasoning.
- **What evidence would resolve it:** Cross-format retrieval benchmarks and models demonstrating consistent performance across textual, visual, and hierarchical tables, with ablation studies quantifying information preservation.

## Limitations
- The survey's claims are largely based on aggregating findings from other works rather than presenting primary experimental results
- Specific performance numbers for different models on benchmarks are not provided
- Exact prompts and rendering specifications used in evaluations are not detailed, limiting reproducibility

## Confidence
- **High confidence**: The general categorization of TQA tasks (e.g., SQA, MQA, OTA) and the identification of core challenges (reasoning, interpretability, robustness) are well-supported by the literature
- **Medium confidence**: The claim that RLVR-trained models exhibit better generalizability is supported by citations, but the specific datasets and evaluation protocols are not fully detailed
- **Low confidence**: Specific performance numbers for different models on benchmarks are not provided, making it difficult to assess the relative effectiveness of different approaches

## Next Checks
1. **Representation Baseline**: Implement the reproduction plan's first experiment - test a standard LLM (e.g., GPT-4) on a small subset of WTQ/TabFact using both Markdown serialization and rendered images. Evaluate exact match accuracy to establish a baseline for comparing text vs. visual inputs.

2. **Agentic Workflow Validation**: Implement the second experiment - create a simple Python-agent loop where the model generates Pandas scripts to answer questions. Execute the scripts and compare accuracy against the baseline. This validates whether code generation improves numerical accuracy and reduces hallucination.

3. **Robustness Analysis**: Implement the third experiment - shuffle table rows in the dataset and re-evaluate. A significant performance drop would indicate the model relies on position rather than content, validating the survey's discussion of shuffling sensitivity as a failure mode.