---
ver: rpa2
title: Analysis of Robustness of a Large Game Corpus
arxiv_id: '2504.03940'
source_url: https://arxiv.org/abs/2504.03940
tags:
- levels
- game
- level
- games
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal measure of robustness for structured
  discrete data, applied to game levels, which are highly sensitive to small changes
  due to hard constraints like solvability. The authors define non-robustness as the
  probability that small input perturbations cause output label changes, and compare
  this across game datasets and standard ML datasets.
---

# Analysis of Robustness of a Large Game Corpus

## Quick Facts
- arXiv ID: 2504.03940
- Source URL: https://arxiv.org/abs/2504.03940
- Reference count: 40
- A formal measure of robustness for structured discrete data applied to game levels, showing high sensitivity to single-tile changes

## Executive Summary
This paper introduces a formal measure of robustness for structured discrete data, applied to game levels, which are highly sensitive to small changes due to hard constraints like solvability. The authors define non-robustness as the probability that small input perturbations cause output label changes, and compare this across game datasets and standard ML datasets. They generate a large corpus (GGLC) of over 80,000 levels across four 2D tile-based games, with solutions included. Experiments show that single-tile changes cause solvability changes in up to 79% of cases, far exceeding sensitivity in datasets like MNIST or CIFAR-10. The GGLC is intended to address data sparsity in procedural content generation and enable more robust ML models for structured domains.

## Method Summary
The authors construct a large dataset (GGLC) of 80,000+ game levels across four 2D tile-based games using a constraint-based generator called Sturgeon. Game rules are encoded as SAT/SMT/ASP constraints and solved to create valid levels. Robustness is measured in two ways: discrete non-robustness using Hamming distance (single-tile changes) and continuous non-robustness using CLIP embeddings in UMAP space. The key metric is the probability that perturbed inputs change their solvability/acceptability labels. They compare game datasets against standard ML benchmarks (MNIST, CIFAR-10) using the same embedding-based metric.

## Key Results
- Single-tile perturbations change solvability in up to 79% of game levels
- Game levels show significantly higher non-robustness than standard ML datasets
- The GGLC dataset contains over 80,000 levels with solutions across four distinct games
- Non-robustness is primarily driven by hard constraints (solvability) rather than local rules

## Why This Works (Mechanism)

### Mechanism 1: Hard Constraint Fragility
Game levels exhibit high non-robustness because they are structured discrete data governed by hard global constraints (e.g., solvability) and local constraints (e.g., connectivity). Unlike continuous image data where a pixel change adds noise, a single tile change in a game (e.g., removing a key or blocking a path) alters the logical state of the system, breaking the satisfaction of constraints required for the "solvable" label.

### Mechanism 2: Data Robustness Metric
The sensitivity of structured data can be quantified by measuring the probability that a data point $x$ and its nearest neighbor $x'$ (within radius $r$) share the same label. The authors adapt classifier robustness to data robustness by calculating the expectation of label agreement within a Hamming distance of 1 (discrete) or Euclidean distance $r$ (continuous via CLIP embedding), isolating the intrinsic volatility of the domain.

### Mechanism 3: Constraint-Based Generation (Sturgeon)
A valid dataset of solvable and unsolvable levels can be generated at scale by defining game mechanics as logical constraints and using a portfolio of solvers. Game rules are encoded as SAT/SMT/ASP constraints, and the generator finds variable assignments that satisfy these to create "solvable" levels, or deliberately fails/negates them for "unsolvable" levels.

## Foundational Learning

- **Concept: Constraint Satisfaction Problems (CSP)**
  - **Why needed here:** The entire dataset is built using Sturgeon, a system that frames level generation as a SAT/CSP problem. You must understand Boolean satisfiability to interpret how levels are constructed.
  - **Quick check question:** Can you explain why finding a valid assignment for "player reaches goal" is a Boolean satisfiability problem?

- **Concept: Discrete vs. Continuous Data Spaces**
  - **Why needed here:** The paper's core thesis contrasts game levels (discrete tiles) with image datasets (continuous pixels). Understanding Hamming distance vs. Euclidean distance is required to interpret the robustness results.
  - **Quick check question:** Why does a "small change" (distance $r=1$) mean something fundamentally different for a tile grid than for an image tensor?

- **Concept: Embedding Spaces (CLIP & UMAP)**
  - **Why needed here:** To compare the "robustness" of games against CIFAR-10, the authors map disparate data types into a shared feature space.
  - **Quick check question:** What information might be lost when reducing a 2D game level with complex mechanics into a static CLIP embedding for comparison?

## Architecture Onboarding

- **Component map:** Sturgeon (constraint generator) -> Solvers (SAT, ASP, MILP) -> Levels (text + image + metadata) -> Analyzer (perturbation + solvers + CLIP/UMAP)

- **Critical path:**
  1. Define Constraints: Encode game rules into Sturgeon's API
  2. Solve: Run the portfolio solver to generate a valid level state and solution path
  3. Store: Save the grid and solution
  4. Perturb & Test: For analysis, flip a tile, re-run the solver to check if it flips from Solvable -> Unsolvable

- **Design tradeoffs:**
  - Custom vs. Real Games: The authors built custom games rather than scraping existing ones to ensure full legal rights and perfect metadata, at the cost of simplicity/lack of enemies
  - Portfolio Solving: Using multiple solvers in parallel improves generation speed but increases software dependency complexity

- **Failure signatures:**
  - High Duplicate Rate: If solvers converge on identical solutions, dataset diversity drops
  - Trivial Levels: If constraints are too weak, levels may be solvable but boring
  - Parsing Errors: If the text-to-image mapping fails, the visual representation won't match the logical solvability

- **First 3 experiments:**
  1. Solvability Stress Test: Generate 100 levels for "Crates", apply 1-random-tile perturbations, and re-verify solvability to replicate the 78.9% non-robustness finding
  2. Metric Validation: Embed a subset of GGLC and CIFAR-10 using CLIP, calculate $D_r(D)$ for both, and verify that GGLC shows higher sensitivity
  3. Unsolvable Generation: Use Sturgeon's "unreachability constraint" to generate a set of broken levels and visually confirm they are logically impassable

## Open Questions the Paper Calls Out

- How can PCGML models be designed to handle multiple hard constraints simultaneously without requiring post hoc cleaning steps?
- How does non-robustness manifest in games with dynamic gameplay elements (enemies, moving platforms, items) compared to the static tile-based games studied here?
- Would embedding methods optimized specifically for tile-based games reveal different patterns of non-robustness when comparing game datasets to standard ML benchmarks?
- Can the non-robustness metric be used to guide the design of neural network architectures that are inherently more aware of discrete constraints?

## Limitations
- The robustness metric assumes CLIP embeddings preserve functional differences between game levels, but this encoding may collapse distinct mechanics into similar vectors
- Acceptability constraints are illustrated but not formally specified, requiring either manual inspection or custom validators for accurate reproduction
- Cross-dataset comparisons using CLIP embeddings require assumptions about embedding space semantics that aren't validated against game-specific knowledge

## Confidence

- **High Confidence:** The discrete robustness finding (78.9% solvability changes from single-tile perturbations) is directly measurable through the provided solvers and tile manipulation
- **Medium Confidence:** The GGLC generation pipeline (Sturgeon + solvers) is well-specified, but dataset diversity depends on constraint quality and solver behavior
- **Low Confidence:** Cross-dataset comparisons using CLIP embeddings require assumptions about embedding space semantics that aren't validated against game-specific knowledge

## Next Checks

1. Test the provided solvability checkers on the paper's example levels to verify they correctly identify solvable/unsolvable states before perturbing levels
2. Examine 10 randomly sampled levels from each game to identify whether generated levels exhibit meaningful design variety or repetitive patterns
3. Visualize UMAP embeddings of game levels with their solvability labels to check if similar-looking levels cluster together, validating that CLIP preserves relevant game semantics