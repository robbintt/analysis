---
ver: rpa2
title: Leveraging Textual Compositional Reasoning for Robust Change Captioning
arxiv_id: '2511.22903'
source_url: https://arxiv.org/abs/2511.22903
tags:
- image
- compositional
- reasoning
- cortex
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORTEX improves change captioning by integrating explicit compositional
  reasoning from VLM-generated text with visual features. It employs an RTE module
  to extract structured, relational descriptions and an ITDA module for static and
  dynamic alignment of visual and textual features.
---

# Leveraging Textual Compositional Reasoning for Robust Change Captioning

## Quick Facts
- **arXiv ID**: 2511.22903
- **Source URL**: https://arxiv.org/abs/2511.22903
- **Reference count**: 40
- **Primary result**: CORTEX achieves state-of-the-art change captioning performance across three benchmarks with BLEU-4=57.4, METEOR=43.0, and CIDEr=76.2 on CLEVR-Change

## Executive Summary
CORTEX introduces a novel approach to change captioning that leverages textual compositional reasoning generated by a vision-language model (VLM). The framework processes paired before/after images to generate natural language descriptions of the changes. By integrating explicit compositional reasoning extracted from VLM-generated text with visual features through a Reasoning-aware Text Extraction (RTE) module and an Image-Text Dynamic Alignment (ITDA) module, CORTEX captures object attributes, spatial relations, and changes that are often ambiguous in visual features alone. The method consistently outperforms state-of-the-art baselines across three benchmarks, demonstrating the effectiveness of combining compositional text with visual alignment for change captioning.

## Method Summary
CORTEX employs a three-module architecture: an Image-level Change Detector using ResNet-101 backbone, a Reasoning-aware Text Extraction (RTE) module that generates compositional descriptions from paired images using InternVL2-8B VLM and BERT encoder, and an Image-Text Dynamic Alignment (ITDA) module that performs static and dynamic alignment between visual and textual features. The static alignment uses cross-attention between visual features and corresponding scene texts, while dynamic alignment captures cross-scene relationships by attending to the other scene's text. The framework is trained end-to-end with a total loss combining captioning loss and alignment losses weighted by λ=10⁻⁴.

## Key Results
- **State-of-the-art performance**: CORTEX achieves best scores of 57.4 BLEU-4, 43.0 METEOR, and 76.2 CIDEr on CLEVR-Change benchmark
- **Consistent improvement**: Outperforms all baseline methods across CLEVR-Change, CLEVR-DC, and Spot-the-Diff datasets
- **Ablation validation**: Both RTE and ITDA modules contribute significantly to performance gains, with VLM-generated compositional cues being essential

## Why This Works (Mechanism)
CORTEX works by extracting explicit compositional reasoning from VLM-generated text that describes objects, attributes, and spatial relationships in both before and after images. The RTE module generates structured, relational descriptions that capture fine-grained details often missed by visual features alone. The ITDA module then aligns these textual descriptions with visual features through both static (within-scene) and dynamic (cross-scene) attention mechanisms. This dual alignment approach enables the model to precisely capture changes by leveraging the complementary strengths of textual and visual information, resulting in more accurate and detailed change captions.

## Foundational Learning
- **Change Captioning**: Generating natural language descriptions explaining differences between paired images - needed because traditional image captioning cannot handle temporal or relational changes between image pairs
- **Compositional Reasoning**: Breaking down scenes into objects, attributes, and relationships - needed because change detection requires understanding fine-grained scene elements beyond simple object presence
- **Vision-Language Models (VLMs)**: Models that can generate textual descriptions from images - needed to extract structured, relational descriptions that capture compositional details
- **Cross-Scene Alignment**: Matching features between before and after images - needed to identify what has changed by comparing corresponding scene elements
- **Static vs Dynamic Alignment**: Static aligns within each scene, dynamic aligns across scenes - needed to capture both individual scene semantics and their differences

## Architecture Onboarding

**Component Map**
Image-level Change Detector -> RTE (VLM + BERT) -> ITDA (Static + Dynamic Alignment) -> Caption Decoder

**Critical Path**
Before/After Images → ResNet-101 → ITDA (Static) → ITDA (Dynamic) → Decoder → Caption

**Design Tradeoffs**
- Uses large VLM (InternVL2-8B) for rich compositional text but incurs computational overhead
- Separate static/dynamic alignment captures both within-scene semantics and cross-scene changes
- Text-only features for spatial relations but visual features for appearance changes

**Failure Signatures**
- VLM generates incomplete or inaccurate compositional text, especially on real images
- Static alignment fails to capture fine-grained attribute changes
- Dynamic alignment produces noisy cross-scene attention when scenes are too dissimilar

**First 3 Experiments**
1. Verify VLM text generation quality by manually inspecting 50 random samples from Spot-the-Diff dataset
2. Test ITDA alignment by computing attention weight distributions between visual and textual features
3. Validate ablation results by training with only visual features and comparing to full CORTEX performance

## Open Questions the Paper Calls Out

**Open Question 1**: How can the computational overhead of the RTE module be reduced for real-time applications?
The paper explicitly identifies VLM computational overhead as a limitation and goal for future work, noting that while offline processing mitigates this, it remains a bottleneck for scalability.

**Open Question 2**: How do specific error types in VLM-generated text propagate through the ITDA module?
The paper provides error categorization but lacks quantitative analysis of how input noise affects final performance metrics.

**Open Question 3**: Can CORTEX maintain performance gains in specialized domains like medical imaging?
While the paper identifies medical imaging as a critical application, experiments are limited to synthetic and surveillance datasets, leaving domain-specific efficacy untested.

## Limitations
- Heavy reliance on VLM-generated text, which shows lower quality on real-world images compared to synthetic datasets
- Significant computational overhead from using large VLM models for text generation
- Lack of domain-specific adaptation, particularly for specialized applications like medical imaging
- Missing architectural details (decoder configuration, fusion strategy) that complicate exact reproduction

## Confidence

**High Confidence**: Performance improvements are consistent across all three benchmarks and ablation studies confirm the necessity of both RTE and ITDA modules

**Medium Confidence**: Claims about compositional reasoning being essential are supported by ablations, but dependence on high-quality VLM outputs remains a practical bottleneck

**Medium Confidence**: Statistical significance of performance gains is established, but magnitude may be influenced by synthetic dataset characteristics

## Next Checks

1. **Reimplement ITDA with full architectural details**: Reconstruct the transformer decoder, confirm cross-scene attention implementation, and validate the concatenation strategy for f_s and f_d outputs

2. **Stress-test VLM text generation**: Generate RTE texts on Spot-the-Diff validation set and compare against human annotations; measure attribute completeness and relational accuracy

3. **Hyperparameter sensitivity analysis**: Systematically vary λ (0.001-0.01) and learning rate (1e-4 to 1e-3) to confirm reported λ=1e-4 is optimal and not overfitting to validation set