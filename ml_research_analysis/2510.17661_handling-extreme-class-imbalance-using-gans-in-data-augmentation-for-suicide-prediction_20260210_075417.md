---
ver: rpa2
title: 'Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide
  Prediction'
arxiv_id: '2510.17661'
source_url: https://arxiv.org/abs/2510.17661
tags:
- data
- suicide
- positive
- samples
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses extreme class imbalance in suicide prediction
  by using Generative Adversarial Networks (GANs) to generate synthetic data. Starting
  with a dataset containing only four positive cases out of 656 samples, the researchers
  trained Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest
  (RF) models on both real and GAN-augmented data.
---

# Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction

## Quick Facts
- **arXiv ID:** 2510.17661
- **Source URL:** https://arxiv.org/abs/2510.17661
- **Authors:** Vaishnavi Visweswaraiah; Tanvi Banerjee; William Romine
- **Reference count:** 29
- **Primary result:** GAN-based data augmentation significantly improves suicide attempt detection in extreme class imbalance, with SVM emerging as the most reliable model.

## Executive Summary
This study tackles the challenge of predicting suicide attempts in datasets with extreme class imbalance, where only 4 out of 656 samples represent positive cases. The researchers employed Conditional GANs to generate synthetic minority class samples, augmenting the dataset for training Logistic Regression, SVM, and Random Forest models. SVM consistently outperformed other models, achieving the best balance between detecting rare positive cases and minimizing false positives. The study demonstrates that GAN-based data augmentation can transform an otherwise impossible prediction task into a viable one, particularly for tree-based models like Random Forest that failed entirely on the original imbalanced data.

## Method Summary
The researchers used the ABCD Study dataset, applying Rasch measurement theory to convert ordinal survey responses into continuous logit measures for three features: Sex, Panic, and Suicidal Ideation. They trained a Conditional GAN to generate synthetic data, creating a balanced dataset of approximately 260 samples (134 negative, 126 positive). Three classifiers (Logistic Regression, SVM, and Random Forest) were trained on both the original real training data and the synthetic GAN data. Models were evaluated on a held-out test set containing 130 negative and 1 positive case, with performance measured using sensitivity, specificity, precision, and F1 scores.

## Key Results
- SVM trained on GAN-generated data achieved the same recall as real data while maintaining lower false positives than other models
- Random Forest failed to detect any positive cases when trained on real data but succeeded with synthetic data, though with 85 false positives
- Logistic Regression showed higher false positive rates (87) when trained on synthetic data compared to SVM (33)
- The GAN successfully preserved Panic and Suicidal Ideation distributions but introduced demographic bias by generating more male-positive samples than reality

## Why This Works (Mechanism)

### Mechanism 1
Conditional GANs (CTGAN) generate synthetic minority class samples that allow classifiers to learn non-trivial decision boundaries for rare events. By modeling the joint distribution P(X|Y=minority), the GAN creates plausible "suicide attempt" profiles, moving the model from a "predict majority" default to a regime where the minority class is geometrically separable. The synthetic samples capture the underlying variance of the true positive class sufficiently to serve as a proxy for real training data.

### Mechanism 2
Margin-based classifiers (SVM) exhibit greater resilience to synthetic data noise than tree-based or probability-estimation models in extremely low-sample regimes. SVM optimizes for the maximum margin between classes, allowing the model to maintain a stable boundary even when synthetic data introduces distribution shifts. This allows the model to maintain a stable boundary even when the synthetic data introduces distribution shifts, whereas Random Forest splits aggressively on synthetic noise and Logistic Regression struggles with calibration.

### Mechanism 3
Transforming raw ordinal survey data into unidimensional logit measures (via Rasch modeling) improves GAN fidelity and downstream classifier performance. Raw ordinal data is difficult for standard GANs to model continuously. Rasch modeling converts these into continuous "logit" scores, allowing the GAN to learn a smoother Gaussian-like distribution rather than disjointed categorical buckets.

## Foundational Learning

- **Concept: Extreme Class Imbalance**
  - **Why needed here:** With only 4 positive cases in 656 samples (0.6%), accuracy is a useless metric. You must understand Recall/Sensitivity and Specificity trade-offs.
  - **Quick check question:** If a model predicts "no suicide" for every input, what is its Recall for the positive class? (Answer: 0).

- **Concept: Conditional GANs (CTGAN)**
  - **Why needed here:** Unlike standard GANs that generate random samples, CTGANs allow the generation of data conditioned on the label Y=1 (suicide attempt). This is essential for targeted oversampling.
  - **Quick check question:** Why is a standard Wasserstein GAN (WGAN) insufficient for this problem without modification?

- **Concept: Rasch Measurement Theory**
  - **Why needed here:** The paper uses this to preprocess features. Understanding that raw survey answers are converted to continuous "logits" is key to understanding the input features fed into the GAN.
  - **Quick check question:** Does the Rasch model treat the difference between survey scores "1" and "2" the same as "2" and "3"? (Answer: No, it converts to interval scale logits).

## Architecture Onboarding

- **Component map:** ABCD Study data -> Rasch Modeling (Ordinal -> Logits) -> Stratified Split (80:20) -> Conditional GAN (Generator/Discriminator) -> Synthetic Dataset (134 neg, 126 pos) -> Classifiers (LR, SVM, RF) -> Evaluation (Real-world hold-out set)

- **Critical path:** The reliability of the system depends on the Rasch -> GAN transition. If the GAN fails to replicate the distribution of the Rasch logits (specifically the "Suicidal" feature), the downstream classifiers will fail to detect the single positive test case.

- **Design tradeoffs:**
  - Real Data Training: High Specificity (few false alarms), but zero Sensitivity for complex models (RF misses 100% of positive cases)
  - GAN Data Training: High Sensitivity (catches the case), but lower Specificity (SVM jumps from 31 to 33 FP; RF jumps from 0 to 85 FP)
  - Model Selection: SVM is the "safe middle ground." RF is too volatile; LR is too conservative.

- **Failure signatures:**
  - RF Failure: Sensitivity = 0.0 on real data (collapsed to majority class)
  - GAN Drift: Skewed synthetic demographics (Fig 3a shows GAN generates more males in the positive class than reality, despite real data showing higher female attempts)
  - Evaluation Fragility: The test set contains only one positive case. A single misclassification changes Recall from 100% to 0%.

- **First 3 experiments:**
  1. Reproduce the Baseline: Train RF and SVM on the raw/imbalanced data (4 positives) to verify RF's collapse (Recall=0) vs SVM's stability.
  2. GAN Feature Validation: Plot Kernel Density Estimation (KDE) of "Panic" and "Suicidal" logits for Real vs. GAN data to confirm if the distributions align as claimed in Fig 1.
  3. Threshold Tuning (SVM): Retrain SVM_G with varied decision thresholds to see if the 33 false positives can be reduced without losing the 1 True Positive.

## Open Questions the Paper Calls Out

- **Cost-sensitive learning and threshold tuning:** To what extent can cost-sensitive learning and threshold tuning mitigate the high false positive rates observed in models trained on GAN-augmented data?
- **Alternative generative architectures:** How do alternative generative architectures, such as Variational Autoencoders (VAEs) or transformers, compare to Conditional GANs in preserving feature correlations for behavioral health data?
- **Clinician-in-the-loop evaluations:** Does the inclusion of clinician-in-the-loop evaluations improve the validity and deployment readiness of synthetic data for suicide prediction?
- **Demographic bias prevention:** Can the GAN architecture be refined to prevent the introduction of demographic bias during data augmentation?

## Limitations
- Extremely small test set with only one positive case makes evaluation metrics volatile and potentially misleading
- Synthetic data introduced demographic skew, generating more male-positive samples than reality despite female adolescents having higher suicide attempt rates
- Reproducibility barriers due to restricted access to ABCD dataset and missing specific preprocessing parameters

## Confidence
- **High Confidence:** GAN-based augmentation improves RF's ability to detect positive cases (from 0.0 to 1.0 sensitivity)
- **Medium Confidence:** SVM's superior balance between sensitivity and specificity is robust, though evaluation depends heavily on a single test case
- **Low Confidence:** The Rasch model's contribution to GAN fidelity and downstream classifier performance is assumed but not empirically validated

## Next Checks
1. **Threshold Analysis:** Retrain SVM_G with varied decision thresholds to quantify the sensitivity-specificity trade-off and identify optimal operating points.
2. **Feature Distribution Validation:** Compare Kernel Density Estimation plots of key features (Panic, Suicidal) between real and GAN data to verify distributional alignment.
3. **Real Data Baseline Reproduction:** Train LR, SVM, and RF on the highly imbalanced real training set (3 positives) to confirm the reported performance collapses before applying GAN augmentation.