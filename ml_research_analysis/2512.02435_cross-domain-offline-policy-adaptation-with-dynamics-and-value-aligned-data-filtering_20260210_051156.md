---
ver: rpa2
title: Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data
  Filtering
arxiv_id: '2512.02435'
source_url: https://arxiv.org/abs/2512.02435
tags:
- domain
- target
- source
- data
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DVDF (Dynamics- and Value-aligned Data Filtering),
  a novel method for cross-domain offline reinforcement learning that addresses the
  challenge of leveraging source domain data with dynamics misalignment for training
  agents in target domains. DVDF introduces a theoretically-grounded approach that
  jointly considers both dynamics alignment and value alignment when selecting source
  domain samples.
---

# Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering

## Quick Facts
- arXiv ID: 2512.02435
- Source URL: https://arxiv.org/abs/2512.02435
- Reference count: 40
- This paper proposes DVDF, a novel method for cross-domain offline RL that outperforms strong baselines by up to 71% on high-quality datasets.

## Executive Summary
This paper addresses the challenge of leveraging source domain data with dynamics misalignment for training agents in target domains within cross-domain offline RL. DVDF (Dynamics- and Value-aligned Data Filtering) introduces a theoretically-grounded approach that jointly considers both dynamics alignment and value alignment when selecting source domain samples. The method uses a pre-trained advantage function to measure value misalignment and combines it with dynamics alignment metrics within a unified framework. Extensive experiments across various kinematic and morphology shifts demonstrate that DVDF consistently outperforms strong baselines, including IQL, BOSA, DARA, IGDF, and OTDF.

## Method Summary
DVDF addresses cross-domain offline RL by pre-training a Sparse Q-Learning (SQL) agent on source data to obtain accurate advantage estimates, then training a contrastive dynamics scorer using target and source transitions. During training, each source batch is scored using a weighted combination of dynamics alignment (from contrastive learning) and value alignment (from normalized pre-trained advantage), with samples above a quantile threshold retained. This filtered source data is combined with full target data for IQL-style TD learning. The key insight is that both dynamics and value misalignment contribute to target-domain sub-optimality, requiring a dual-alignment approach rather than dynamics-only filtering.

## Key Results
- DVDF achieves up to 71% performance improvement over strong baselines on high-quality datasets
- Outperforms IQL, BOSA, DARA, IGDF, and OTDF across multiple tasks and dataset qualities
- Shows particular effectiveness in extremely low-data settings (5,000 target transitions)
- Ablation studies confirm λ=0.7 balances dynamics and value alignment optimally
- SQL pre-training provides more accurate advantage estimates than IQL, critical for DVDF's success

## Why This Works (Mechanism)

### Mechanism 1: Dual-alignment sub-optimality bound
- Claim: Minimizing only dynamics misalignment is insufficient; the target-domain sub-optimality gap decomposes into both dynamics and value misalignment terms.
- Mechanism: Theoretical analysis shows SubOpt ≤ value_misalignment + dynamics_misalignment + ε_opt. Prior methods tightening only the dynamics term address the wrong objective—they bound policy performance difference across domains, not sub-optimality in the target.
- Core assumption: The derived bound meaningfully predicts empirical policy quality; the constant terms (C₁, C₂) do not dominate in practice.
- Evidence anchors:
  - Proposition 4.1 derivation showing two-term decomposition; critique of Lemma 4.1 as misaligned with RL objective.
  - "both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework."
- Break condition: If dynamics shift is negligible or source dataset is uniformly high-quality, value alignment alone may suffice.

### Mechanism 2: Advantage-function as value misalignment proxy
- Claim: A pre-trained advantage function (from source domain) provides a tractable lower bound on value misalignment, enabling practical sample selection.
- Mechanism: Proposition 5.1 shows E[A^{π*}_{in-src}(s,a)] lower-bounds value misalignment under the assumption (π(a|s)−μ(a|s))A^μ(s,a) ≥ 0. SQL is chosen over IQL for pre-training because IQL's V-underestimation inflates advantage error.
- Core assumption: The pre-trained policy approximates the in-sample optimal policy well enough; SQL's advantage estimates are more accurate than IQL's on offline data.
- Evidence anchors:
  - Proposition 5.1 and Remark 1 on the policy improvement assumption; Equation 3.
  - Figure 2(b) showing SQL maintains lower advantage estimation error than IQL.
- Break condition: If source data quality is uniformly low, even accurate advantage estimates may not identify useful samples.

### Mechanism 3: Unified dynamics-value scoring and quantile filtering
- Claim: A weighted combination of dynamics score (contrastive) and normalized advantage score yields a practical filter; selecting top-ξ quantile source samples balances alignment with coverage.
- Mechanism: Score g(s,a,s') = λ·h(s,a,s') + (1−λ)·Norm(Â_pre(s,a)); indicator w selects samples above ξ-th quantile. This directly mirrors the additive bound in Proposition 4.1.
- Core assumption: Linear combination with fixed λ appropriately trades off dynamics vs. value across tasks; quantile filtering preserves sufficient data diversity.
- Evidence anchors:
  - Equation 7 and 8 defining g(·) and weighted TD loss.
  - Figure 3(a) showing λ=0.7 performs best across tested tasks; ξ=0.5 as robust default.
- Break condition: If dynamics and value signals are anti-correlated (high-value samples always have high dynamics shift), no λ may work well.

## Foundational Learning

- Concept: **Offline RL and OOD issues**
  - Why needed here: Cross-domain offline RL inherits offline RL's distributional shift problem; additional dynamics misalignment compounds this.
  - Quick check question: Can you explain why value overestimation occurs in offline RL and how conservative Q-learning or in-sample learning addresses it?

- Concept: **Advantage functions and policy improvement**
  - Why needed here: DVDF uses advantage as a value-alignment signal; understanding A(s,a) = Q(s,a) − V(s) and its role in policy gradient/AWR is essential.
  - Quick check question: Given a behavior policy μ and a learned policy π, what does E_{s∼d_μ, a∼μ}[A^π(s,a)] > 0 imply about policy improvement?

- Concept: **Contrastive learning for representation alignment**
  - Why needed here: IGDF's dynamics score h(s,a,s') uses NCE loss to distinguish target-domain transitions from source-domain transitions.
  - Quick check question: In the NCE loss (Equation 6), what does the score function h learn to discriminate, and how does it relate to dynamics alignment?

## Architecture Onboarding

- Component map: SQL pre-training on source data → contrastive dynamics scorer (h) → combined score g = λ·h + (1−λ)·Norm(Â_pre) → quantile filtering (ξ) → IQL training with filtered source + full target data

- Critical path:
  1. Pre-train SQL on full source dataset → extract and freeze Â_pre
  2. Train contrastive scorer h(·) using target and source transitions (7K steps)
  3. For each training iteration: compute g for source batch, filter top ξ%, combine with target batch, update Q/V/policy via IQL

- Design tradeoffs:
  - **SQL vs. IQL pre-training**: SQL chosen for more accurate advantage; IQL underestimates V → overestimates A
  - **λ tuning**: λ=0.7 works across tasks in experiments, but extreme dynamics shifts may require higher λ
  - **ξ selection**: Too low loses coverage; too high includes misaligned data. ξ=0.5 is a compromise

- Failure signatures:
  - **All source samples filtered**: ξ too low or g scores near-uniform → check score distribution and λ
  - **No improvement over baseline**: Advantage pre-training may have failed → verify SQL convergence
  - **Worse than IQL on merged data**: Dynamics scorer may be inverted → verify NCE loss

- First 3 experiments:
  1. **Sanity check**: Replicate motivating example with mixed random/expert source data; verify DVDF selects expert samples while IGDF selects random
  2. **Ablation on λ**: Run DVDF-IGDF on 4 tasks with λ ∈ {0.0, 0.3, 0.5, 0.7, 0.9, 1.0}; plot final performance
  3. **Ablation on SQL vs. IQL pre-training**: Pre-train both, run DVDF-IGDF, compare advantage estimation error and final policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trade-off coefficient λ between dynamics and value alignment be automatically adapted rather than fixed at 0.7?
- Basis in paper: The paper sets λ=0.7 uniformly across all experiments without theoretical justification, noting only that "neither excessive emphasis on dynamics alignment nor value alignment represents the best choice."
- Why unresolved: The optimal balance likely varies across tasks, dataset qualities, and severity of dynamics shift, but no adaptive mechanism is proposed.
- What evidence would resolve it: A study comparing fixed vs. adaptive λ strategies across diverse dynamics shift scenarios.

### Open Question 2
- Question: How does DVDF extend to settings with multiple source domains having different dynamics shifts?
- Basis in paper: The paper states it considers "a source domain M_src and a target domain M_tar" (Section 2), implying a single source domain.
- Why unresolved: The theoretical framework and filtering mechanism assume a single source domain; combining samples from multiple sources with varying dynamics misalignment creates additional complexity.
- What evidence would resolve it: Experiments on multiple source domains with different types/severities of dynamics shifts.

### Open Question 3
- Question: How robust is DVDF to severe dynamics shifts beyond the moderate kinematic and morphology shifts tested?
- Basis in paper: The dynamics shifts implemented involve relatively constrained modifications (e.g., joint angle ranges reduced by 10x, limb size modifications).
- Why unresolved: The advantage function pre-trained on source domain may become unreliable when source-target dynamics diverge significantly.
- What evidence would resolve it: Experiments with progressively more severe dynamics shifts, measuring performance degradation.

## Limitations
- The choice of λ=0.7 and ξ=0.5 is empirically justified but may not generalize to extreme dynamics shifts or dataset quality variations
- The method's performance in scenarios with severe anti-correlation between dynamics and value signals is not empirically tested
- SQL hyperparameters and specific implementation details are not provided, creating reproducibility challenges

## Confidence
**High Confidence**: The experimental methodology is rigorous with extensive ablation studies (λ tuning, SQL vs. IQL comparison, extreme low-data settings). The 71% improvement over baselines on high-quality datasets is well-documented across multiple tasks.

**Medium Confidence**: The theoretical foundation (Proposition 4.1, Proposition 5.1) is sound but relies on assumptions about policy improvement and advantage estimation accuracy that may not hold uniformly across all dataset qualities.

**Low Confidence**: The method's performance in scenarios with severe anti-correlation between dynamics and value signals, or when both are uniformly poor, is not empirically tested. The robustness of fixed hyperparameters across diverse cross-domain settings remains uncertain.

## Next Checks
1. **Extreme Anti-correlation Test**: Design a synthetic cross-domain scenario where high-value source samples always have high dynamics misalignment. Measure DVDF performance with varying λ to identify failure modes.

2. **Uniform Low-Quality Data**: Evaluate DVDF when both source and target datasets have uniformly poor quality (e.g., random data). Compare against baseline IQL to assess method degradation.

3. **Hyperparameter Robustness**: Systematically vary λ ∈ [0.3, 0.9] and ξ ∈ [0.25, 0.75] across 6-8 diverse cross-domain tasks to map the performance landscape and identify task-specific optimal settings.