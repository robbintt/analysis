---
ver: rpa2
title: 'TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language
  Learning'
arxiv_id: '2507.16844'
source_url: https://arxiv.org/abs/2507.16844
tags:
- data
- design
- timing
- td-interpreter
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TD-Interpreter is an ML tool that helps engineers interpret timing
  diagrams during digital design and verification. It was developed by fine-tuning
  the LLaVA model using synthetic data generated from Verilog modules and datasheets.
---

# TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning

## Quick Facts
- **arXiv ID:** 2507.16844
- **Source URL:** https://arxiv.org/abs/2507.16844
- **Reference count:** 40
- **Primary result:** A fine-tuned 7B VLM model achieves 95.9 Bleu-4 and 96.7 Rouge-1 scores on timing diagram VQA, outperforming untuned GPT-4o by a large margin.

## Executive Summary
TD-Interpreter is a machine learning tool designed to help engineers interpret timing diagrams during digital design and verification. It was developed by fine-tuning the LLaVA model using synthetic data generated from Verilog modules and datasheets. The fine-tuned model achieves 95.9 Bleu-4 and 96.7 Rouge-1 scores, significantly outperforming untuned GPT-4o on complex timing diagram questions across various hardware protocols and design scenarios.

## Method Summary
The method involves generating synthetic visual-text pairs by converting Verilog modules into simulation waveforms (.vcd), rendering them as images via WaveDrom, and creating aligned QA pairs. A 7B LLaVA model is fine-tuned using LoRA on this dataset, with separate caption-based and reasoning-based training data to enhance both low-level signal detail extraction and high-level protocol reasoning capabilities.

## Key Results
- TD-Interpreter achieves 95.9 Bleu-4 and 96.7 Rouge-1 scores on timing diagram VQA tasks
- Outperforms untuned GPT-4o by a large margin on complex timing analysis questions
- Successfully handles AXI and SPI protocol timing diagrams through fine-tuning on synthetic data

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Visual-Text Alignment
The model achieves high performance because it's trained on synthetically generated data where text is mathematically derived from visual sources, ensuring perfect ground-truth alignment. This bridges the data availability gap by creating noise-free datasets from Verilog-to-image pipelines.

### Mechanism 2: Caption-Enhanced Reasoning
Performance improves when the model learns both low-level signal details (captions) and high-level protocol logic (reasoning) together. This grounds reasoning in specific visual evidence, reducing hallucination of signal values.

### Mechanism 3: Efficient Domain Adaptation via LoRA
A lightweight 7B model outperforms general-purpose models by using Low-Rank Adaptation to efficiently specialize in timing constraint "language" without catastrophic forgetting, keeping pre-trained visual features intact while adapting cross-modal alignment.

## Foundational Learning

- **Concept: Finite State Machines (FSM) & State Transitions**
  - Why needed: Model tests include synthesizing FSM logic from waveforms, requiring understanding of state changes on clock edges
  - Quick check: Can you trace a signal `state` in a diagram and map its changes to input conditions `in` at specific clock cycles?

- **Concept: Multimodal Large Language Models (MLLM) Architecture**
  - Why needed: System relies on Vision Encoder to see diagrams and LLM to generate text; understanding visual token projection is crucial
  - Quick check: Does the model process image and text simultaneously, or convert image into token sequence processed like text?

- **Concept: BLEU and ROUGE Scores**
  - Why needed: Paper claims success based on these n-gram overlap metrics used in machine translation
  - Quick check: If model produces correct logic but uses different synonyms than ground truth, would BLEU score decrease?

## Architecture Onboarding

- **Component map:** TD Image + Text Question -> Vision Encoder (CLIP-ViT) -> Projection Layer -> LLM (Vicuna-7B) -> Answer
- **Critical path:** The Data Construction Pipeline (Verilog Module -> Testbench -> VCD -> JSON -> WaveDrom Image) is fundamental to TD-Interpreter's quality
- **Design tradeoffs:** Uses synthetic WaveDrom diagrams (clean, standardized) rather than real-world noise; compares against untuned GPT-4o rather than fine-tuned alternatives
- **Failure signatures:** Protocol Hallucination (generic textbook responses), Clock Domain Confusion (difficulty with subtle clock cues), Format Sensitivity (failure on non-WaveDrom styles)
- **First 3 experiments:**
  1. Run data generation pipeline on simple Verilog (e.g., 2-to-1 MUX) to verify Verilog-to-Image conversion and QA pair alignment
  2. Feed generated MUX image into base LLaVA-7B model to confirm "untuned" poor performance (should be generic/unrelated)
  3. Train LoRA adapters using provided sample data (~10k samples) and query resulting model with specific timing question to validate performance jump

## Open Questions the Paper Calls Out

1. Can reinforcement learning (RL) be effectively utilized to further enhance reasoning capabilities of MLLMs in interpreting timing diagrams? (Current study focused on supervised fine-tuning with LoRA; no RL experiments conducted)

2. How can backend verification be integrated into synthetic dataset construction to ensure logical correctness of generated question-answer pairs? (Current workflow lacks automated formal verification step for answer consistency)

3. To what extent does TD-Interpreter generalize to timing diagrams deviating from WaveDrom plotting style used during training? (Model trained/evaluated on WaveDrom format, potentially limiting robustness to hand-drawn diagrams or screenshots)

## Limitations

- Transferability concerns: High performance relies on perfect synthetic WaveDrom alignment; real-world diagrams from datasheets/oscilloscopes may vary significantly in style and quality
- Dataset composition gaps: Critical implementation details like "top-down-conditioned-random-generation" algorithm and exact prompt templates remain unspecified
- Protocol generalization uncertainty: Model evaluated on AXI/SPI protocols likely present in training set; no demonstration of performance on completely novel protocols

## Confidence

- **High Confidence (9/10):** Synthetic data generation mechanism through Verilog-to-WaveDrom conversion is well-documented and reproducible; 95.9 Bleu-4 and 96.7 Rouge-1 scores are directly measurable
- **Medium Confidence (6/10):** Superiority over untuned GPT-4o demonstrated but limited; LoRA adaptation effectiveness supported but not comprehensively explored across different rank sizes
- **Low Confidence (4/10):** Claims about handling "complex timing diagram questions across various hardware protocols" not fully substantiated; evaluation focuses on specific protocols without demonstrating robustness to novel or noisy real-world diagrams

## Next Checks

1. Test the fine-tuned model on timing diagrams extracted from actual datasheets and oscilloscope captures, comparing performance against synthetic WaveDrom diagrams used in training

2. Evaluate the model on hardware protocols not present in the training set (e.g., I2C, PCIe) to assess true generalization capabilities beyond AXI/SPI examples

3. Conduct ablation study by systematically removing either caption-based or reasoning-based training data to quantify each component's contribution to overall performance, particularly focusing on "caption-enhanced reasoning" mechanism