---
ver: rpa2
title: Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation
arxiv_id: '2512.14048'
source_url: https://arxiv.org/abs/2512.14048
tags:
- code
- icot
- generation
- routinggen
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RoutingGen addresses overthinking in code generation by dynamically
  routing simple tasks to direct few-shot generation and complex ones to a structured
  Intention Chain-of-Thought (ICoT) process that explicitly models task requirements
  and algorithmic design. Experiments across three models and six benchmarks show
  RoutingGen achieves state-of-the-art Pass@1 performance in most settings while reducing
  token usage by 46.37% on average.
---

# Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation

## Quick Facts
- arXiv ID: 2512.14048
- Source URL: https://arxiv.org/abs/2512.14048
- Authors: Shen Li; Li Huang; Shaoxiong Zhan; Weifeng Sun; Tao Yin; Zhongxin Liu; Meng Yan
- Reference count: 40
- Primary result: RoutingGen achieves SOTA Pass@1 on most benchmarks while reducing token usage by 46.37% on average through dynamic routing between few-shot generation and Intention Chain-of-Thought.

## Executive Summary
RoutingGen addresses overthinking in code generation by dynamically routing simple tasks to direct few-shot generation and complex ones to a structured Intention Chain-of-Thought (ICoT) process. The method uses a classifier to determine task complexity and applies appropriate strategies: simple problems use low-cost few-shot prompting while complex ones leverage ICoT to explicitly model task requirements and algorithmic design. Experiments across three models and six benchmarks demonstrate state-of-the-art Pass@1 performance with significant token efficiency gains.

## Method Summary
RoutingGen employs a two-stage approach with dynamic routing based on task complexity. A Qwen3-8B classifier determines whether problems are "Simple" (route to 3-shot prompting) or "Complex" (route to Intention Chain-of-Thought). The ICoT strategy explicitly models "Specification" (inputs/outputs) and "Idea" (algorithmic logic + complexity estimate) before generating code. The system uses 20 candidate samples per problem for most models, with sampling parameters (temp=0.8, top-p=0.95) and token limits (max 300) specified for different stages.

## Key Results
- RoutingGen achieves state-of-the-art Pass@1 performance in most settings across three models and six benchmarks
- The method reduces token usage by 46.37% on average compared to ICoT-only approaches
- ICoT consistently outperforms six prompting baselines on challenging benchmarks
- Ablation studies show removing the "Idea" stage causes Pass@1 to drop significantly (e.g., HumanEval 92.07% → 88.17%)

## Why This Works (Mechanism)

### Mechanism 1: Conditional Routing
Conditional routing prevents performance degradation and token waste by matching strategy difficulty to task complexity. A classifier ($M_{cls}$) assigns a difficulty label (Simple or Complex). Simple tasks trigger low-cost few-shot generation, bypassing verbose reasoning and preventing "overthinking" where excessive structured prompting leads to disorganized logic on trivial problems. This assumes a single lightweight classifier can sufficiently approximate task complexity to guide routing decisions.

### Mechanism 2: Intention Abstraction
Explicitly modeling algorithmic intent ("Idea") shifts focus from syntax to functional objectives. The ICoT strategy decouples "Idea" generation from coding, requiring the model to explicitly state core algorithmic logic (e.g., "Use binary search") and estimate time complexity before writing code. This "intention abstraction" is hypothesized to prevent the model from focusing solely on surface-level structures, assuming structuring reasoning into "Specifications" and "Ideas" effectively scaffolds the model's internal representations.

### Mechanism 3: Constraint Grounding
Defining input-output constraints ("Specification") grounds generation and reduces functional bugs. Before generating code, the model produces a "Specification" listing inputs and expected outputs, forcing resolution of ambiguity in natural language prompts into formal constraints. This assumes enforcing explicit constraint definition reduces the likelihood of the model ignoring edge cases or requirements.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Standard CoT generates intermediate reasoning steps that might hurt performance on simple tasks like "reverse a string" by adding unnecessary complexity.
- **Few-Shot Learning vs. Zero-Shot**: Few-shot uses demonstration examples to guide generation, while zero-shot relies on instructions alone. Few-shot has higher computational cost than direct generation but lower than multi-stage CoT processes.
- **Code Generation Metrics (Pass@k)**: Pass@1 measures functional correctness against test cases, not syntactic correctness. Understanding this metric is necessary to interpret the trade-off analysis in Table 1.

## Architecture Onboarding

- **Component map**: Question → Classifier ($M_{cls}$) → (Simple) Few-shot Prompt → Code OR (Complex) ICoT Prompt → Specification/Idea → Code Prompt → Code
- **Critical path**: The Classifier ($M_{cls}$) is critical since efficiency gains (46.37% token reduction) rely entirely on accurately identifying which problems are trivial enough for few-shot generation.
- **Design tradeoffs**: Routing adds inference latency before main generation. On "Complex" tasks, ICoT doubles inference calls (Idea generation + Code generation). Setting classifier threshold too aggressively toward "Simple" risks logical errors on complex problems.
- **Failure signatures**: "Overthinking" failure occurs when the model generates 200 tokens of reasoning for a 5-line string reversal function. "Specification Drift" occurs when generated code contradicts the "Specification" output from the previous step.
- **First 3 experiments**: 
  1. Run the Router on the benchmark test set without generating code to verify the Simple/Complex distribution matches Figure 3.
  2. Force all tasks to "Complex" (ICoT only) and measure the drop in token efficiency and change in Pass@1 to quantify the Router's contribution.
  3. Feed ambiguous prompts to verify if the "Specification" stage hallucinates constraints or asks for clarification.

## Open Questions the Paper Calls Out
None explicitly stated in the provided material.

## Limitations
- Classifier Generalization: The Qwen3-8B router's ability to generalize across unseen problem types is untested, with over-aggressive routing to "Simple" potentially harming complex problem performance.
- Token Usage Reporting: The claimed 46.37% reduction excludes router inference tokens, which would reduce the efficiency gain when fully accounted for.
- ICoT Dependency: The ICoT strategy's success heavily depends on the generator's ability to translate abstract "Idea" statements into working code, with no ablation on generator quality versus ICoT benefit.

## Confidence
- **High Confidence**: Pass@1 improvements on MBPP and HumanEval (direct, reproducible metrics).
- **Medium Confidence**: ICoT's mechanism of "intention abstraction" improving code correctness (supported by ablation but requires manual analysis of failure modes).
- **Low Confidence**: 46.37% token reduction claim (relies on router accuracy; overhead not fully accounted for).

## Next Checks
1. **Router Calibration Stress Test**: Run the classifier on a held-out validation set of 100 problems and verify the Simple/Complex split matches Figure 3 distributions.
2. **ICoT vs. Standard CoT Ablation**: Compare RoutingGen's ICoT stage to a standard Chain-of-Thought baseline on HumanEval-ET to isolate whether "intention abstraction" adds value beyond generic reasoning.
3. **Token Overhead Accounting**: Measure and report total tokens including router inference (input + output) to quantify true efficiency gains versus the ICoT-only baseline.