---
ver: rpa2
title: 'Foundry: Distilling 3D Foundation Models for the Edge'
arxiv_id: '2511.20721'
source_url: https://arxiv.org/abs/2511.20721
tags:
- supertokens
- tokens
- distillation
- foundry
- gate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Foundation Model Distillation (FMD), a novel
  paradigm for compressing large self-supervised learning (SSL) foundation models
  into compact, efficient, and general-purpose proxies that retain their transferability
  across diverse downstream tasks. The authors present Foundry, the first FMD framework
  for 3D point clouds, which trains a student model to learn a compressed set of learnable
  SuperTokens that reconstruct the teacher's token-level representations, capturing
  a compact basis of its latent space.
---

# Foundry: Distilling 3D Foundation Models for the Edge

## Quick Facts
- **arXiv ID**: 2511.20721
- **Source URL**: https://arxiv.org/abs/2511.20721
- **Reference count**: 40
- **Primary result**: Introduces Foundation Model Distillation (FMD) and Foundry framework for compressing 3D point cloud models, achieving strong performance across tasks with significant computational savings.

## Executive Summary
This paper introduces Foundation Model Distillation (FMD), a novel paradigm for compressing large self-supervised learning (SSL) foundation models into compact, efficient, and general-purpose proxies that retain transferability across diverse downstream tasks. The authors present Foundry, the first FMD framework for 3D point clouds, which trains a student model to learn a compressed set of learnable SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. Foundry outperforms task-specific distillation methods and simpler compression baselines while maintaining strong performance across classification, part segmentation, and few-shot scenarios using significantly fewer tokens and FLOPs.

## Method Summary
Foundry implements FMD through a two-stage process where a student model learns to reconstruct a teacher's token-level representations using fewer SuperTokens (s ≪ c). The core compression mechanism consists of a Dynamic Semantic Organization (DSO) module that uses cross-attention with learnable queries to assign tokens to SuperTokens, followed by a Compressed Attention Upsampling (CAU) module that reconstructs the full token set from the compressed representation. The student is trained end-to-end to minimize Smooth-L1 reconstruction loss between the teacher's output Y and the student's reconstruction Ŷ, with optional regularization through a gate mechanism that further compresses the representation by predicting token fusion probabilities.

## Key Results
- Foundry achieves 89.68% accuracy on ShapeNet55 classification vs. 76.08% for K-Means-Student and 87.56% for FPS-Student
- Maintains strong performance across classification (91.2% ModelNet40), part segmentation (mIoU 84.3%), and few-shot scenarios (87.5% 1-shot accuracy)
- Reduces inference memory by 67% and achieves 8.9× speedup compared to the teacher model
- Outperforms task-specific distillation methods while preserving transferability across diverse downstream tasks

## Why This Works (Mechanism)

### Mechanism 1: Information Bottleneck Enforces Basis Learning
The compress-and-reconstruct objective forces the student to learn a compact, transferable basis of the teacher's latent space rather than task-specific features. By requiring reconstruction of the full token-level representation from fewer SuperTokens, the student must identify and preserve the most informative dimensions of the teacher's latent manifold, creating pressure toward learning semantically meaningful basis vectors.

### Mechanism 2: Learned Semantic Grouping Outperforms Static Sampling
End-to-end learned token-to-SuperToken assignment captures semantic structure better than pre-computed geometric or clustering-based grouping. The DSO module uses cross-attention with learnable query projections to assign tokens to SuperTokens, allowing the grouping to adapt to the teacher's semantic space rather than input-space geometry.

### Mechanism 3: Residual Connection Preserves High-Frequency Details
The residual connection in CAU (adding original tokens to upsampled SuperTokens) is critical for high-fidelity reconstruction and downstream performance. Compression loses local, high-frequency details, and the residual connection reinjects these details from the original tokens, allowing the model to focus the SuperTokens on semantic content while preserving fine-grained information.

## Foundational Learning

- **Self-Supervised Learning (SSL) Foundation Models**: Understanding that SSL creates general-purpose representations (not task-specific) is essential to grasp why FMD aims to preserve transferability. Quick check: Can you explain why an SSL-pretrained model should generalize better than a supervised-pretrained model when fine-tuned on unseen tasks?

- **Knowledge Distillation Paradigms**: The paper positions FMD against traditional task-specific KD and feature mimicry. Understanding the spectrum (logit-based → feature-based → representation-space distillation) clarifies the novelty. Quick check: What is the difference between distilling logits for a specific task versus distilling the entire representation manifold?

- **Cross-Attention and Token Assignment**: DSO uses cross-attention with hard assignment (Gumbel-Softmax) to route tokens to SuperTokens. Understanding query/key/value mechanisms and differentiable hard assignment is prerequisite to implementing the core compression module. Quick check: How does Gumbel-Softmax enable differentiable discrete assignment, and why is this preferable to soft attention for compression?

## Architecture Onboarding

- **Component map**:
```
Input Point Cloud P
    ↓
Tokenizer (Point-BERT style) → T ∈ ℝ^{c×d} tokens + positional embeddings
    ↓
    ├─────────────────────────────────────────────┐
    ↓                                             ↓
DSO Module                                   [Residual path]
    - SuperTokens S ∈ ℝ^{s×d} (learnable params)
    - Cross-attention: Q=S·W_Q, K=T·W_K, V=T·W_V
    - Hard assignment via CAM (argmax)
    - Grouped average update
    ↓
Lightweight Student Encoder (ViT-T/S)
    ↓
CAU Module
    - Reuse CAM for routing
    - Upsample: MLP(T + CAM · S_processed)
    ↓
Reconstructed Ŷ ∈ ℝ^{c×d}
    ↓
Smooth-L1 Loss vs. Teacher Y

Optional: Gate Module (2-layer MLP) → fusion probability per token
```

- **Critical path**:
1. Initialize SuperTokens with truncated normal distribution
2. Train DSO/CAU end-to-end with frozen teacher encoder
3. Monitor distillation loss; expect ~0.07-0.10 for s=16 with unfrozen student
4. Fine-tune on downstream tasks (classification/segmentation) with frozen or unfrozen backbone
5. For Foundry-Gate: tune λ_gate in range [10^-15, 10^-10] to control compression ratio

- **Design tradeoffs**:
- s (SuperToken count): Lower s → more compression, faster inference, but performance drops (1-2% per halving typically)
- Frozen vs. unfrozen student during distillation: Unfrozen achieves lower distillation loss (0.0736 vs 0.1006) but requires more compute
- Gate λ_gate: Higher regularization → more compression but anti-correlation with downstream performance (Spearman -0.45 to -0.80)
- Encoder size: ViT-T (6.3M params) viable with s=16, but expect 1-2% accuracy drop vs. ViT-S (22.7M params)

- **Failure signatures**:
- Distillation loss stuck >0.15: Check SuperToken initialization, learning rate, or token assignment gradients
- Fine-tuning accuracy drops >10% vs. teacher: SuperToken count too low (s<4) or student encoder under-capacity
- OOM on large scenes: Reduce s or enable Foundry-Gate with token budget
- Reconstruction clusters mixed in t-SNE (Figure 8): CAU residual may not be propagating; verify skip connection

- **First 3 experiments**:
1. **Baseline reproduction**: Train Foundry with s=16 on ShapeNet55, freeze teacher (Point-JEPA ViT-S), measure distillation loss target ~0.07-0.10. Fine-tune on ModelNet40 classification to verify ~91% accuracy (within 2% of teacher).
2. **Ablation: compression method**: Compare DSO vs. K-Means vs. FPS on same architecture. Expect ~13% gap between learned and static grouping (Table 4). If gap <5%, check cross-attention gradient flow.
3. **Scaling test**: Run inference on 2^18 point scene (Table 7). Verify baseline OOMs while Foundry completes with ~4GB memory. If Foundry also OOMs, check token count at encoder input.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the compress-and-reconstruct objective successfully transfer to other data modalities (e.g., 2D images, video) or non-Transformer architectures?
- **Basis in paper**: [explicit] The conclusion states the authors "leave extending Foundry to other 3D foundation models and modalities for future work."
- **Why unresolved**: The current validation is restricted to 3D point cloud Transformers (Point-JEPA).
- **What evidence would resolve it**: Successful application of Foundry to 2D ViTs or sparse convolution networks with retained generalization.

### Open Question 2
- **Question**: Can the optimization conflict between the compression mechanism and self-supervised pre-training be resolved to enable end-to-end training?
- **Basis in paper**: [inferred] In supplementary material (Table 9), the authors observe that combining SuperTokens with pre-training leads to a "conflict," forcing them to use a two-stage distillation process.
- **Why unresolved**: The authors currently bypass this by decoupling pre-training and distillation, leaving the conflict unaddressed.
- **What evidence would resolve it**: A joint training scheme that achieves performance comparable to the two-stage distillation baseline.

### Open Question 3
- **Question**: Does the SuperToken mechanism inherently bias the model toward geometric edges at the expense of semantic content under high compression?
- **Basis in paper**: [inferred] Visualizations (Fig. 9, 11) show that compressed models often preserve edge structures but lose semantic details (e.g., bottle shape vs. glass content) compared to the teacher.
- **Why unresolved**: The paper demonstrates the bias visually but does not quantify or correct this specific trade-off in the loss function.
- **What evidence would resolve it**: Quantitative analysis showing semantic preservation rates relative to edge fidelity, or a modified loss balancing these factors.

## Limitations

- The mechanism claims about information bottleneck enforcing basis learning and learned semantic grouping outperforming static sampling lack theoretical justification and direct ablation evidence.
- The paper doesn't prove that token-level reconstruction preserves the teacher's semantic manifold rather than task-specific features through manifold analysis or transferability tests beyond reported tasks.
- The residual connection's contribution is claimed critical but not isolated through ablation studies, leaving uncertainty about its true necessity.

## Confidence

**High Confidence (80-100%)**: The empirical demonstrations that Foundry outperforms task-specific distillation methods and simpler compression baselines across multiple downstream tasks. The quantitative results (classification accuracy, segmentation mIoU, few-shot performance) are well-documented with clear comparisons and statistical significance.

**Medium Confidence (40-80%)**: The mechanism claims about information bottleneck enforcing basis learning and learned semantic grouping outperforming static sampling. While supported by experimental gaps (13.6% accuracy difference for grouping methods), these mechanisms lack theoretical justification and direct ablation evidence.

**Low Confidence (0-40%)**: The claim that FMD preserves transferability by learning a compact basis of the teacher's latent space. This foundational mechanism is asserted but not directly validated through manifold analysis, transferability tests beyond the reported tasks, or theoretical analysis of the reconstruction objective's properties.

## Next Checks

1. **Manifold Preservation Analysis**: Perform t-SNE or UMAP visualization comparing teacher vs. student token embeddings on a held-out validation set. Measure reconstruction fidelity in latent space (cosine similarity distributions) and verify that student embeddings maintain the teacher's semantic clustering structure rather than degrading to task-specific patterns.

2. **Ablation of Residual Connection**: Train Foundry variants with and without the CAU residual connection on the same distillation dataset. Compare both distillation loss trajectories and downstream task performance to quantify the exact contribution of the residual mechanism. Include computational overhead analysis to assess the cost-benefit tradeoff.

3. **Transferability Stress Test**: Evaluate the distilled models on tasks not seen during fine-tuning (e.g., part completion, normal estimation, or registration) to directly test the transferability claim. Compare against teacher fine-tuned on these novel tasks to measure performance degradation and validate whether the student truly learned general-purpose representations or memorized task-specific features.