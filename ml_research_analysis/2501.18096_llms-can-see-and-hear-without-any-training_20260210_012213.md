---
ver: rpa2
title: LLMs can see and hear without any training
arxiv_id: '2501.18096'
source_url: https://arxiv.org/abs/2501.18096
tags:
- image
- mils
- captioning
- generation
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MILS (Multimodal Iterative LLM Solver) is a training-free approach
  that leverages the reasoning capabilities of large language models (LLMs) to solve
  multimodal tasks across images, videos, and audio. The method works by iteratively
  generating candidate outputs (using an LLM as a GENERATOR) and scoring them with
  a multimodal model (SCORER), refining the outputs until convergence.
---

# LLMs can see and hear without any training

## Quick Facts
- arXiv ID: 2501.18096
- Source URL: https://arxiv.org/abs/2501.18096
- Reference count: 24
- Primary result: Training-free MILS framework achieves emergent zero-shot performance across multimodal tasks

## Executive Summary
MILS (Multimodal Iterative LLM Solver) introduces a novel training-free approach that enables large language models to process images, videos, and audio through iterative reasoning. The framework generates candidate outputs using an LLM as a GENERATOR and scores them with a multimodal model (SCORER), refining results until convergence. This method achieves state-of-the-art zero-shot captioning performance, improves text-to-image generation through prompt optimization, enables style transfer via Gram matrix optimization, and supports cross-modal arithmetic by inverting multimodal embeddings into text.

## Method Summary
The MILS framework works by iteratively generating candidate outputs using an LLM and scoring them with a multimodal model, refining the outputs until convergence. This training-free approach leverages the reasoning capabilities of LLMs to solve multimodal tasks across different modalities including images, videos, and audio. The method demonstrates emergent zero-shot performance on tasks like captioning, image generation, style transfer, and cross-modal arithmetic without requiring any task-specific training data. The iterative process involves the LLM generating multiple candidate solutions which are then evaluated and ranked by the multimodal SCORER, with the best candidates used to generate new improved outputs in subsequent iterations.

## Key Results
- Achieves state-of-the-art zero-shot captioning results compared to baselines
- Improves text-to-image generation quality through prompt optimization without task-specific training
- Enables style transfer capabilities via Gram matrix optimization using only iterative reasoning
- Supports cross-modal arithmetic by successfully inverting multimodal embeddings into coherent text

## Why This Works (Mechanism)
The MILS framework works by combining the generative capabilities of large language models with the evaluation capabilities of multimodal models in an iterative loop. The LLM acts as a GENERATOR, producing multiple candidate outputs for a given task, while the multimodal model serves as a SCORER, evaluating and ranking these candidates. Through repeated iterations, the system converges on high-quality outputs by progressively refining the LLM's generations based on the SCORER's feedback. This approach effectively harnesses the LLM's reasoning capabilities to navigate the solution space of multimodal tasks without requiring task-specific training data.

## Foundational Learning
- Multimodal embeddings: Vector representations that capture information from multiple data types (why needed: to enable cross-modal reasoning; quick check: can embeddings be inverted back to text?)
- Gram matrix optimization: Mathematical technique for capturing style information in images (why needed: enables style transfer without training; quick check: does Gram matrix capture perceptual style differences?)
- Iterative refinement: Process of progressively improving outputs through repeated cycles (why needed: allows convergence to optimal solutions; quick check: how many iterations are needed for stability?)
- Zero-shot learning: Ability to perform tasks without task-specific training data (why needed: demonstrates emergent capabilities of LLMs; quick check: how does performance compare to fine-tuned models?)
- Cross-modal arithmetic: Mathematical operations across different modalities (why needed: enables creative applications like image editing via text; quick check: are operations commutative across modalities?)
- Prompt optimization: Systematic improvement of input prompts for better model outputs (why needed: enhances generation quality without retraining; quick check: does optimization generalize across different tasks?)

## Architecture Onboarding

**Component map:** LLM GENERATOR -> Multimodal SCORER -> Output refinement -> (loop back to GENERATOR)

**Critical path:** Input task → LLM generates candidates → SCORER evaluates candidates → Best candidates selected → New candidates generated → Convergence check → Final output

**Design tradeoffs:** The framework trades computational efficiency for flexibility and zero-shot capability, as each iteration requires both LLM queries and multimodal scoring. The reliance on existing multimodal models as SCORERs limits performance to the capabilities of those models but enables rapid deployment across diverse tasks without specialized training.

**Failure signatures:** Poor convergence occurs when the SCORER provides inconsistent or noisy feedback, causing the LLM to generate divergent outputs. Tasks requiring fine-grained visual understanding may fail when the multimodal SCORER lacks sufficient resolution. Computational constraints manifest as incomplete iterations or truncated candidate generation.

**3 first experiments:** 1) Benchmark zero-shot captioning performance against traditional fine-tuned models on COCO dataset, 2) Test iterative convergence rates across different task types (captioning vs. style transfer), 3) Evaluate robustness by introducing adversarial perturbations to multimodal inputs.

## Open Questions the Paper Calls Out
None

## Limitations
- Substantial computational cost due to iterative LLM queries and multimodal scoring, potentially limiting real-time applications
- Performance bounded by the capabilities of the underlying multimodal SCORER model
- May not generalize well to novel modalities or highly complex tasks beyond those tested

## Confidence
- High: MILS can effectively generate captions and optimize text-to-image generation prompts
- Medium: MILS demonstrates style transfer and cross-modal arithmetic capabilities
- Low: Claims about emergent zero-shot performance on completely unseen tasks

## Next Checks
1. Benchmark MILS against specialized fine-tuned models on complex multimodal reasoning tasks to quantify the trade-off between zero-shot flexibility and task-specific performance
2. Conduct ablation studies to determine the minimum number of iterations needed for stable convergence across different task types
3. Evaluate the framework's robustness to noisy or adversarial inputs across all supported modalities to assess real-world applicability