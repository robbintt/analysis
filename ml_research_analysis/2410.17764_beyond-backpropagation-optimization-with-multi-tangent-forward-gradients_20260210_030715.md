---
ver: rpa2
title: 'Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients'
arxiv_id: '2410.17764'
source_url: https://arxiv.org/abs/2410.17764
tags:
- forward
- gradient
- gradients
- tangents
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes multi-tangent forward gradients as an alternative
  to backpropagation for training neural networks. The method approximates gradients
  by combining directional derivatives along multiple random tangents computed via
  forward-mode automatic differentiation, rather than using backpropagation's backward
  pass.
---

# Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients

## Quick Facts
- arXiv ID: 2410.17764
- Source URL: https://arxiv.org/abs/2410.17764
- Reference count: 40
- One-line primary result: Multi-tangent forward gradients provide a backpropagation-free training method that successfully trains modern architectures like ResNet18 and ViT by approximating gradients through directional derivatives along multiple random tangents.

## Executive Summary
This paper introduces multi-tangent forward gradients as an alternative to backpropagation for training neural networks. The method computes gradients by combining directional derivatives along multiple random tangents using forward-mode automatic differentiation, avoiding the backward pass entirely. The authors develop an improved aggregation approach using orthogonal projections that provides better gradient approximation quality than simple averaging. Experiments demonstrate that this method successfully trains modern architectures including fully-connected networks, ResNet18, and vision transformers on image classification tasks, with performance improving as the number of tangents increases.

## Method Summary
The method approximates gradients by computing directional derivatives along k random tangents using forward-mode automatic differentiation, then aggregating these using orthogonal projection onto the subspace spanned by the tangents. Instead of the backward pass used in backpropagation, the approach computes ∇f·v_i for multiple random vectors v_i in parallel forward passes, then projects the true gradient onto the k-dimensional subspace spanned by these tangents using V(V^⊤V)^(-1)V^⊤∇f. The authors use activity perturbation (computing gradients w.r.t. activations rather than weights) to reduce variance for larger networks. Training uses SGD without momentum or learning rate schedules, with early stopping based on validation performance.

## Key Results
- Increasing the number of tangents (k) consistently improves gradient approximation quality, with cosine similarity approaching 1 as k increases
- Orthogonal projection provides more accurate and robust gradient approximation than simple averaging or summation, particularly when tangents are correlated
- The method successfully trains ResNet18 and ViT on MNIST and CIFAR10 using activity perturbation, achieving competitive performance
- Single-tangent forward gradients fail on high-dimensional parameter spaces, but multi-tangent aggregation enables successful optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Increasing the number of random tangents (k) improves the approximation quality of the forward gradient relative to the true gradient ∇f.
- **Mechanism**: Forward gradients act as a Monte Carlo estimator. A single tangent projects the gradient onto a 1D subspace. Aggregating k tangents spans a k-dimensional subspace, capturing more information about the true gradient direction and magnitude.
- **Core assumption**: The sampled tangents are linearly independent and sufficiently random (i.i.d.) to span a representative subspace.
- **Evidence anchors**: [Section III.C]: "As we increase the number of tangents, the cosine similarity increases... indicating a more accurate approximation." [Section IV.A, Fig 3]: Shows cosine similarity approaching 1 as k increases.

### Mechanism 2
- **Claim**: Orthogonal projection (P_U) provides a more accurate and robust gradient approximation than simple averaging or summation, particularly when tangents are correlated.
- **Mechanism**: Instead of a conical combination (sum/average) which restricts the result to the cone spanned by tangents, P_U projects ∇f onto the subspace U spanned by V using V(V^⊤V)^(-1)V^⊤∇f. This "corrects" for non-orthogonality among tangents.
- **Core assumption**: The local approximation error (distance from ∇f to subspace U) determines optimization success.
- **Evidence anchors**: [Section III.D]: "P_U(∇f) is the most accurate approximation g of ∇f in U and minimizes ||∇f - g||_2." [Section IV.C]: "PU is entirely unaffected by [angle] α... while gV degrades as the cone around v_1 narrows."

### Mechanism 3
- **Claim**: Applying forward gradients to activations (activity perturbation) rather than weights enables training of larger networks (ResNet, ViT).
- **Mechanism**: The variance of forward gradients scales with dimension. The dimension of activations is typically much smaller than the dimension of weights. By computing gradients w.r.t. activations and deriving weight gradients analytically, the estimation variance is reduced.
- **Core assumption**: The network architecture allows analytical derivation of weight gradients from activation gradients.
- **Evidence anchors**: [Section IV]: "We use activity perturbation... which has been found to reduce the variance of forward gradients since the activations are typically of lower dimension than the weights." [Section IV.E, Table I]: Demonstrates successful training of ResNet18 and ViT on MNIST/CIFAR10.

## Foundational Learning

- **Concept**: **Forward-Mode Automatic Differentiation (AD)**
  - **Why needed here**: The method relies on forward-mode AD to compute directional derivatives (∇f·v) efficiently in a single forward pass, unlike backpropagation which uses reverse-mode.
  - **Quick check question**: Can you explain why forward-mode AD computes Jacobian-vector products efficiently but is inefficient for computing full gradients in high-dimensional parameter spaces?

- **Concept**: **Monte Carlo Integration/Estimation**
  - **Why needed here**: The single-tangent forward gradient is an unbiased estimator of the true gradient. Understanding Monte Carlo properties (variance reduction via sampling) explains why multi-tangent aggregation works.
  - **Quick check question**: Why does the variance of the gradient estimator increase with parameter dimension n?

- **Concept**: **Subspace Projection**
  - **Why needed here**: The improved aggregation method relies on projecting the true gradient onto the linear subspace spanned by the random tangents.
  - **Quick check question**: How does orthogonal projection (P_U) mathematically differ from simply averaging the individual forward gradient vectors?

## Architecture Onboarding

- **Component map**: Tangent Generator -> Forward-Mode AD Engine -> Aggregator -> Optimizer

- **Critical path**: The computation of the Gram matrix inverse (V^⊤V)^(-1) in the Aggregator is the distinct step differentiating this from standard forward-gradient methods. If k is small, this overhead is negligible compared to the forward pass.

- **Design tradeoffs**:
  - **Accuracy vs. Parallelism**: Higher k improves gradient accuracy (better convergence) but increases compute cost linearly (O(k)).
  - **Implementation**: The method trades the memory overhead of backpropagation (storing activations) for the compute overhead of multiple forward passes.
  - **Assumption**: Wall-clock time may still be higher than backpropagation unless the forward passes are heavily parallelized or the memory bandwidth is the bottleneck.

- **Failure signatures**:
  - **High-dimensional Collapse**: With single tangent (k=1) and large n, the optimizer fails to descend even simple convex functions (cosine similarity → 0).
  - **Correlation Sensitivity**: If using simple averaging (gV) with correlated tangents (e.g., specific initialization seeds), optimization degrades significantly compared to orthogonal projection (P_U).
  - **Learning Rate Mismatch**: The magnitude of gV scales differently than ∇f; fixed learning rates from backpropagation training will likely fail.

- **First 3 experiments**:
  1. **Variance Scaling**: On a small MLP (e.g., MNIST), plot the cosine similarity between the multi-tangent gradient and the backpropagation gradient as k increases (k=1, 2, 4, 8, 16). Verify the O(1/k) or similar improvement in alignment.
  2. **Ablation on Aggregation**: Optimize a synthetic function (e.g., Styblinski-Tang) using correlated tangents (angle α < 90°). Compare optimization trajectories of Sum, Average, and Orthogonal Projection to confirm P_U robustness.
  3. **Activity vs. Weight Perturbation**: Train a small CNN on CIFAR10. Compare training loss curves when applying forward gradients to weights directly vs. applying them to activations (activity perturbation) to validate the variance reduction claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-tangent forward gradients be significantly improved by replacing random tangent sampling with informed strategies, such as using local losses or adaptive sampling?
- Basis in paper: [explicit] The authors state in the Outlook: "Combining multi-tangent forward gradients with better tangent sampling approaches appears to be a promising approach to further improve these results," specifically mentioning "local losses, auxiliary networks, or adaptive sampling."
- Why unresolved: The current study relies on random tangents sampled from N(0, I_n), which results in a "considerable gap" in performance compared to backpropagation on complex tasks.
- What evidence would resolve it: Experiments integrating structured tangents (e.g., from synthetic gradients or local errors) showing higher cosine similarity to the true gradient and faster convergence than random sampling.

### Open Question 2
- Question: To what extent does parallelizing computations over multiple tangents mitigate the theoretical linear time complexity (O(k·ops(f))) relative to backpropagation?
- Basis in paper: [explicit] The authors note in the Outlook that "parallelizing over the tangents is a promising approach to mitigate computational overhead" because forward passes are independent and yield single scalar directional derivatives.
- Why unresolved: While theoretically linear in k, the actual runtime benefits of parallel execution versus the sequential nature of backpropagation were not measured or implemented in this study.
- What evidence would resolve it: System benchmarks on multi-core hardware comparing wall-clock time of parallelized multi-tangent passes against standard backpropagation for varying k.

### Open Question 3
- Question: How does an optimized implementation of multi-tangent forward gradients compare to backpropagation in terms of practical wall-clock time and memory efficiency?
- Basis in paper: [explicit] The authors list in the Outlook: "Optimizing the forward gradient implementation and measuring the wall-clock time with varying k compared to backpropagation" as important aspects to address in future work.
- Why unresolved: The paper relies on theoretical time complexity and unoptimized PyTorch code; practical benefits from "more advantageous memory-access patterns" remain unquantified.
- What evidence would resolve it: A study utilizing custom CUDA kernels or optimized forward-mode AD libraries to demonstrate a tangible wall-clock speedup or memory reduction during the training of large models.

## Limitations

- The method's scalability to very deep networks (>50 layers) or extremely high-dimensional weight spaces remains unproven
- Computational overhead from k forward passes could negate benefits unless highly parallelized
- The biological plausibility claim is theoretical and not empirically validated

## Confidence

- **High Confidence**: Forward-mode AD computes directional derivatives correctly; increasing k improves approximation quality (variance reduction); activity perturbation reduces variance vs. weight perturbation for large networks.
- **Medium Confidence**: Orthogonal projection provides robust aggregation, especially for correlated tangents; method successfully trains ResNet18/ViT on MNIST/CIFAR10.
- **Low Confidence**: Biological plausibility of the approach; computational efficiency relative to backpropagation on modern hardware; scalability to extremely large-scale models.

## Next Checks

1. **Correlation Robustness Test**: Intentionally generate non-orthogonal tangents (fixed angle α < 90°) for a small network and verify that P_U significantly outperforms gV in optimization stability and final loss, while both degrade when using orthogonal projection on nearly orthogonal tangents.

2. **Computational Overhead Benchmark**: Measure wall-clock training time for ResNet18 on CIFAR10 using the proposed method with k=4 vs. standard backpropagation across different batch sizes and hardware configurations (GPU vs. CPU) to quantify the practical efficiency trade-off.

3. **Large-Scale Scalability Probe**: Apply the method to train a ResNet50 or Vision Transformer Large on ImageNet. Monitor gradient approximation quality (cosine similarity) and optimization trajectory to identify at what scale (network depth, parameter count, dataset size) the multi-tangent approach becomes computationally prohibitive or loses approximation accuracy.