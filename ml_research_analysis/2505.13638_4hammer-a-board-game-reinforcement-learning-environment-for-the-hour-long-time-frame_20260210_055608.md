---
ver: rpa2
title: '4Hammer: a board-game reinforcement learning environment for the hour long
  time frame'
arxiv_id: '2505.13638'
source_url: https://arxiv.org/abs/2505.13638
tags:
- game
- rules
- learning
- games
- hammer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce 4Hammer, a reinforcement learning environment for
  complex board games with hour-long time horizons. 4Hammer implements a subset of
  Warhammer 40,000's Combat Patrol mode, featuring intricate rules and extensive game
  state tracking requirements.
---

# 4Hammer: a board-game reinforcement learning environment for the hour long time frame

## Quick Facts
- arXiv ID: 2505.13638
- Source URL: https://arxiv.org/abs/2505.13638
- Reference count: 19
- Introduces 4Hammer RL environment for complex board games with hour-long time horizons

## Executive Summary
4Hammer is a reinforcement learning environment implementing Warhammer 40,000's Combat Patrol mode, designed to test ML techniques on complex, long-duration tasks. The environment features intricate rules requiring extensive game state tracking and supports both perfect/imperfect information RL and LLM integration through textual state serialization. Using a custom Rulebook DSL to encode game rules, 4Hammer automatically generates simulation libraries, graphical interfaces, and tensor/textual representations. Experimental validation demonstrates successful PPO training with agents learning to maximize damage output and score differentials across simplified game variants.

## Method Summary
The 4Hammer environment implements a subset of Warhammer 40,000's Combat Patrol mode with 6 factions (22 units) on a 44x30 grid. The system uses a custom Rulebook DSL to encode game rules, automatically generating C++ simulation libraries, Python bindings, and tensor/textual state representations. PPO training is built-in through the Rulebook compiler with lr=0.00001. Two simplified experiments validate the approach: single_shooting_maximize.rl (1000 steps, maximize damage) and single_turn.rl (5000 steps, maximize piece differential with "active league play"). The graphical engine integrates with Gemini 2 Flash for LLM-based action selection using textual state serialization.

## Key Results
- Successful PPO training demonstrated with agents learning to maximize damage output and score differentials
- Graphical engine integration shows LLMs can correctly parse and reason about game states
- Rulebook DSL enables automated generation of simulation libraries and representations
- Environment provides benchmark for evaluating ML techniques on complex, long-duration tasks

## Why This Works (Mechanism)
4Hammer's success stems from its domain-specific language approach to encoding complex game rules, which automates the generation of simulation components and representations. The DSL abstracts away low-level implementation details while preserving game semantics, enabling efficient tensor/textual serialization for ML consumption. The combination of graphical interfaces and multiple representation formats (tensor/textual) allows for both human-in-the-loop development and automated training pipelines.

## Foundational Learning
- Rulebook DSL: Domain-specific language for encoding complex game rules - needed for automated simulation generation - quick check: examine generated C++ code from sample rules
- PPO training integration: Built-in reinforcement learning with proximal policy optimization - needed for autonomous agent training - quick check: verify learning curves match reported results
- Tensor/textual state serialization: Multiple representation formats for ML consumption - needed for flexible training approaches - quick check: test state round-trip conversion between formats

## Architecture Onboarding
**Component Map:** Rulebook DSL -> Rulebook Compiler (rlc) -> C++ Simulation Library -> Python Bindings -> RL Interface -> PPO Trainer/LLM Interface

**Critical Path:** Rulebook definition → Compiler processing → Library generation → State representation creation → Agent training/action selection

**Design Tradeoffs:** Custom DSL vs. hand-coded simulation (flexibility vs. complexity), tensor vs. textual representations (ML efficiency vs. human readability), 2D vs. 3D rendering (performance vs. fidelity)

**Failure Signatures:** Flat/declining returns during training (invalid actions or poor reward shaping), LLM producing invalid actions (state serialization format mismatch), compiler build failures (dependency/version issues)

**3 First Experiments:**
1. Compile and run single_shooting_maximize.rl for 1000 steps, verify learning curve matches Figure 3
2. Test tensor and textual state serialization round-trip conversion
3. Validate LLM integration by checking action selection accuracy on sample states

## Open Questions the Paper Calls Out
None

## Limitations
- Key PPO hyperparameters beyond learning rate remain unspecified
- "Active league play" configuration not clearly defined
- LLM integration lacks comprehensive performance metrics and error analysis
- Missing performance comparisons against existing board game RL environments

## Confidence
- **High Confidence**: Core 4Hammer environment implementation and basic PPO functionality
- **Medium Confidence**: LLM integration effectiveness and long-horizon RL capability claims
- **Low Confidence**: Relative advantages compared to existing board game RL benchmarks

## Next Checks
1. Replicate single_shooting_maximize experiment and verify learning curve matches Figure 3
2. Implement missing PPO hyperparameters based on standard configurations and re-run both experiments
3. Test LLM integration with alternative models (e.g., GPT-4) to validate textual state serialization approach