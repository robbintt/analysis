---
ver: rpa2
title: 'TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of
  the Collaborative Translation'
arxiv_id: '2507.00875'
source_url: https://arxiv.org/abs/2507.00875
tags:
- translation
- legal
- evaluation
- hong
- kong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents TransLaw, a multi-agent system that addresses
  challenges in Hong Kong legal judgment translation by combining three specialized
  agents: Translator, Annotator, and Proofreader. The framework integrates a Hong
  Kong legal glossary database, Retrieval-Augmented Generation (RAG), and iterative
  feedback to produce translations with high legal accuracy, stylistic fidelity, and
  structural coherence.'
---

# TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation

## Quick Facts
- arXiv ID: 2507.00875
- Source URL: https://arxiv.org/abs/2507.00875
- Authors: Xi Xuan; King-kui Sin; Yufei Zhou; Chunyu Kit
- Reference count: 40
- TransLaw multi-agent system achieves superior legal translation quality compared to single-agent baselines and GPT-4o

## Executive Summary
This study presents TransLaw, a multi-agent system that addresses challenges in Hong Kong legal judgment translation by combining three specialized agents: Translator, Annotator, and Proofreader. The framework integrates a Hong Kong legal glossary database, Retrieval-Augmented Generation (RAG), and iterative feedback to produce translations with high legal accuracy, stylistic fidelity, and structural coherence. Evaluated on the HKCFA Judgment 97-22 dataset with 13 open-source and commercial LLMs, TransLaw significantly outperforms single-agent baselines, achieving superior legal semantic accuracy, structural coherence, and stylistic fidelity compared to GPT-4o. Human evaluation confirms its effectiveness, though it still trails human experts in complex terminology contextualization and stylistic naturalness. The system reduces translation costs by nearly 4,000 times compared to professional human translation.

## Method Summary
The TransLaw framework employs a multi-agent system architecture where three specialized agents work collaboratively to translate Hong Kong legal judgments. The Translator agent performs initial translation using context-aware translation templates and the Hong Kong legal glossary database. The Annotator agent evaluates translations for legal semantic accuracy, structural coherence, and stylistic fidelity using specific evaluation criteria. The Proofreader agent provides targeted feedback to refine translations iteratively. The system incorporates Retrieval-Augmented Generation to access relevant legal precedents and terminology, and employs iterative feedback loops where agents' outputs inform subsequent refinements. The framework was evaluated on the HKCFA Judgment 97-22 dataset using both automated metrics and human evaluation across 13 different LLMs.

## Key Results
- TransLaw achieves superior legal semantic accuracy, structural coherence, and stylistic fidelity compared to single-agent baselines and GPT-4o
- Human evaluation confirms TransLaw's effectiveness in producing high-quality legal translations
- System reduces translation costs by nearly 4,000 times compared to professional human translation
- Legal-specific LLMs (like ChatLaw) do not consistently outperform general-purpose LLMs within the multi-agent framework

## Why This Works (Mechanism)
The multi-agent architecture leverages specialized agents with distinct roles to address the complexity of legal translation. The Translator focuses on initial conversion while maintaining legal context, the Annotator provides systematic evaluation against multiple quality dimensions, and the Proofreader delivers targeted refinements. This division of labor allows each agent to optimize for specific aspects of translation quality while the iterative feedback loop ensures continuous improvement. The integration of domain-specific resources (legal glossary, RAG system) provides contextual grounding that general translation approaches lack, enabling more accurate handling of legal terminology and structural conventions unique to Hong Kong judgments.

## Foundational Learning
- **Multi-agent collaboration in NLP**: Multiple specialized agents working together can solve complex tasks more effectively than single monolithic models by dividing cognitive labor
- **Retrieval-Augmented Generation (RAG)**: Combines retrieval of relevant documents with generation capabilities to ground responses in specific knowledge bases, critical for legal terminology accuracy
- **Iterative feedback loops**: Agent outputs inform subsequent refinements, enabling progressive improvement rather than one-shot generation
- **Legal domain-specific evaluation**: Legal translation requires assessment across semantic accuracy, structural coherence, and stylistic fidelity - standard translation metrics are insufficient
- **Cost-performance tradeoffs in translation**: Automated systems can achieve dramatic cost reductions while maintaining quality levels approaching professional human translation

## Architecture Onboarding

**Component Map:**
Translator Agent -> Annotator Agent -> Proofreader Agent -> (back to Translator for iteration)

**Critical Path:**
Initial translation → Semantic accuracy evaluation → Structural coherence check → Stylistic fidelity assessment → Targeted feedback → Refined translation

**Design Tradeoffs:**
- Specialized agents vs. unified model: Division of labor improves quality but increases system complexity
- Iterative refinement vs. one-shot generation: Multiple passes improve accuracy but increase processing time
- Domain-specific resources vs. general knowledge: Legal glossary ensures accuracy but limits generalizability

**Failure Signatures:**
- Agent disagreement cascades leading to infinite loops
- Over-reliance on glossary causing unnatural phrasing
- RAG retrieval failures producing context mismatches
- Metric optimization creating translation artifacts

**3 First Experiments:**
1. Run single-agent baseline using same LLM to establish performance floor
2. Test agent isolation by running each component independently
3. Perform ablation study removing RAG component to measure contribution

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the TransLaw multi-agent architecture generalize effectively to other legal jurisdictions or language pairs beyond Hong Kong Case Law?
- Basis in paper: Section 7 (Limitations) states the method "may not be directly applicable to judgments from other jurisdictions" due to HK's specific strict formatting constraints
- Why unresolved: The study exclusively evaluated the framework on the HKCFA Judgment 97-22 dataset
- Resolution: Benchmarking the framework on diverse legal corpora (e.g., US, EU) to test performance on different structural constraints

### Open Question 2
- Question: Why do legal-specific LLMs (like ChatLaw) fail to outperform general-purpose LLMs within the multi-agent framework?
- Basis in paper: Section 4.1 notes that "legal-specific LLMs do not always outperform general LLMs" and suggests a "necessity for further design to improve the performance of legal LLMs"
- Why unresolved: The authors speculate on base model limitations but did not conduct ablation studies to isolate the cause of the underperformance
- Resolution: A comparative analysis of agent performance controlling for base model size and legal-specific pre-training data

### Open Question 3
- Question: To what extent does integrating human expert feedback into the initial reasoning steps improve the system's legal semantic accuracy?
- Basis in paper: Section 6 (Conclusion) explicitly aims "to integrate human expert feedback to refine initial reasoning steps" in future work
- Why unresolved: The current system relies entirely on automated agent feedback loops without external human intervention during the reasoning phase
- Resolution: Experiments comparing the ACS metric scores of fully automated runs against runs with early-stage human intervention

## Limitations
- Evaluation relies on a single proprietary dataset (HKCFA Judgment 97-22) of 1,009 judgments, raising questions about generalizability
- Cost reduction estimates assume perfect parallelization and may not reflect real-world deployment constraints including API costs and maintenance overhead
- Translation quality assessment methodology lacks detail on human evaluation procedures, inter-rater reliability, and blinding

## Confidence
- **High**: The multi-agent architecture design and its theoretical advantages for complex translation tasks
- **Medium**: Performance improvements over single-agent baselines on the tested dataset
- **Medium**: Cost reduction estimates (methodology unclear)
- **Low**: Claims about applicability to other legal domains without cross-domain validation

## Next Checks
1. Conduct cross-domain testing on legal texts from other jurisdictions (e.g., US, UK, Singapore) to assess generalizability
2. Perform detailed cost-benefit analysis including API costs, system maintenance, and required human oversight for production deployment
3. Implement randomized controlled trials comparing TransLaw output to professional translations on blind evaluation with practicing lawyers