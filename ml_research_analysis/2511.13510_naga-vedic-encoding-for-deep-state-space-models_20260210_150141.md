---
ver: rpa2
title: 'Naga: Vedic Encoding for Deep State Space Models'
arxiv_id: '2511.13510'
source_url: https://arxiv.org/abs/2511.13510
tags:
- naga
- vedic
- time
- forecasting
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Naga introduces a Vedic encoding for deep State Space Models (SSMs)\
  \ that enhances long-term time series forecasting by structuring temporal interactions\
  \ through element-wise (Hadamard) multiplication of forward and reversed sequences.\
  \ Inspired by Vedic mathematics, this approach improves the model\u2019s ability\
  \ to capture long-range dependencies."
---

# Naga: Vedic Encoding for Deep State Space Models

## Quick Facts
- arXiv ID: 2511.13510
- Source URL: https://arxiv.org/abs/2511.13510
- Reference count: 19
- Key outcome: Vedic encoding for deep SSMs achieves state-of-the-art long-term time series forecasting performance with 26% average MSE reduction and 2.1M parameters

## Executive Summary
Naga introduces a Vedic encoding for deep State Space Models that enhances long-term time series forecasting by structuring temporal interactions through element-wise (Hadamard) multiplication of forward and reversed sequences. Inspired by Vedic mathematics, this approach improves the model's ability to capture long-range dependencies. Evaluated on seven benchmark datasets, Naga achieves state-of-the-art performance, outperforming 28 models with an average MSE reduction of up to 26%. It maintains computational efficiency, using only 2.1 million parameters compared to larger transformer-based architectures.

## Method Summary
Naga builds upon the Mamba2 deep State Space Model by introducing a Vedic encoding layer. The input sequence is projected into two representations using learned linear transformations - one from the forward sequence and one from the reversed sequence. The Hadamard (element-wise) product of these projections creates features that are products of inputs at time t and its symmetric counterpart T-t+1. This encoded representation is then fed into a Mamba2 block, which uses selective state spaces to process the sequence efficiently. The final output is generated by a linear layer mapping the last hidden state to the prediction. The architecture maintains computational efficiency with only 2.1 million parameters while achieving state-of-the-art performance on seven long-term time series forecasting benchmarks.

## Key Results
- Outperforms 28 current state-of-the-art models on seven benchmark datasets
- Achieves average MSE reduction of up to 26% compared to existing models
- Maintains computational efficiency with only 2.1 million parameters
- Shows consistent performance improvements across ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Vedic encoding enables direct learning of multiplicative interactions between temporally distant points, increasing representational capacity.
- Mechanism: The input sequence is projected into two representations, one from the forward sequence and one from the reversed sequence. The Hadamard (element-wise) product of these creates features that are products of inputs at time $t$ and its symmetric counterpart $T-t+1$. This single layer explicitly models cross-time bilinear dependencies that purely linear models cannot represent.
- Core assumption: Long-term dependencies in time series forecasting tasks have a significant bilinear (multiplicative) component.
- Evidence anchors:
  - [abstract] "...element-wise (Hadamard) interaction... enhances the model’s ability to capture temporal dependencies across distant time steps."
  - [section 8, Lemma 1] The paper provides a formal proof that the function class with bilinear features ($\mathcal{F}_{vedic}$) is a strict superset of the class of purely linear functions ($\mathcal{F}_{lin}$).
  - [corpus] The specific bilinear mechanism is not directly evidenced in the provided corpus. The "MS-SSM" and other neighbor papers focus on multi-scale or sparse structures, not element-wise cross-time products.
- Break condition: If the target time series dynamics are purely additive or if long-range dependencies are not structured symmetrically around the sequence center, this inductive bias may be irrelevant.

### Mechanism 2
- Claim: The Vedic decomposition creates a shortcut for gradient propagation, mitigating the vanishing gradient problem for long-range dependencies.
- Mechanism: In standard deep SSMs, error signals must travel through many recurrent steps. The Vedic encoding allows gradients from the output to directly influence weights based on products of inputs from distant time steps in a single backpropagation pass. This reduces the effective computational depth for learning these specific couplings.
- Core assumption: The primary bottleneck in learning long-range dependencies with SSMs is the difficulty of propagating gradient information across many time steps.
- Evidence anchors:
  - [abstract] "...improves gradient flow for learning multiplicative cross-time interactions."
  - [section 8.1, Lemma 2] The authors show that the gradient with respect to the weights contains explicit products of temporally remote inputs ($x_{t,a}x_{T-t+1,b}$), providing a direct path for error signals.
  - [corpus] No direct evidence in the provided corpus. The claim is a standard motivation in SSM literature but the specific "shortcut" mechanism is novel to this paper.
- Break condition: If the underlying SSM backbone already has excellent gradient flow, the additional benefit from this shortcut may be marginal.

### Mechanism 3
- Claim: The model's structured inductive bias allows it to achieve higher accuracy with significantly fewer parameters than Transformer baselines.
- Mechanism: By using a compact Mamba2 SSM backbone (~2.1M parameters) augmented with cheap element-wise operations, Naga avoids the quadratic complexity of self-attention. The strong inductive bias for temporal dependencies improves data efficiency, reducing the need for the massive parameter counts of Transformers.
- Core assumption: The inductive bias of the SSM combined with the Vedic encoding is more data-efficient and better suited for LTSF tasks than the flexible but data-hungry attention mechanism.
- Evidence anchors:
  - [abstract] "...outperforms 28 current state of the art models... maintains computational efficiency, using only 2.1 million parameters..."
  - [section 5.1] The paper shows Naga achieves lower MSE than models with "six to nine times more parameters."
  - [corpus] Supported. The "MS-SSM" paper abstract (arXiv:2512.23824) also positions SSMs as efficient, linear-time alternatives.
- Break condition: This efficiency-accuracy trade-off may not hold for tasks requiring more complex, non-recurrent reasoning where full attention is necessary.

## Foundational Learning

- Concept: **State Space Models (SSMs)**
  - Why needed here: Naga is built on Mamba2, a deep SSM. You must understand the core recurrent formulation ($h_{t+1} = Ah_t + Bx_t$) and how it contrasts with attention.
  - Quick check question: What is the computational complexity of generating an output for a sequence of length $T$ using a standard Transformer versus a recurrent SSM?

- Concept: **Hadamard (Element-wise) Product and Bilinear Forms**
  - Why needed here: The central innovation is the Hadamard product ($\odot$) in the Vedic encoding. You need to understand that this operation creates bilinear features ($x_i y_i$), which allows the model to learn multiplicative interactions between its inputs.
  - Quick check question: How does the function $z = (W_1x) \odot (W_2y)$ differ in representational capacity from a purely linear function of $x$ and $y$?

- Concept: **Inductive Bias**
  - Why needed here: The paper frames its contribution as introducing a useful inductive bias. You must grasp that this refers to the set of assumptions the model uses to generalize, in this case, the assumption that useful information is encoded in the multiplicative interaction between forward and reversed sequences.
  - Quick check question: What is the inductive bias introduced by a convolutional neural network (e.g., translation invariance), and how does it differ from Naga's bias for structured cross-time interactions?

## Architecture Onboarding

- Component map: Input -> Flip -> Linear Projections -> Hadamard Product -> Mamba2 Recurrence -> Output Head
- Critical path: Input -> Flip -> Linear Projections -> Hadamard Product -> Mamba2 Recurrence -> Output Head. The Hadamard product is the critical novel step.
- Design tradeoffs:
  - **Expressivity vs. Overfitting:** The bilinear encoding expands capacity but increases overfitting risk on small datasets (requires strong regularization)
  - **Efficiency vs. Complexity:** Gains parameter efficiency over Transformers but adds implementation complexity
  - **Symmetric Bias:** Assumes symmetric temporal dependencies; may fail if task dynamics are purely causal and asymmetric
- Failure signatures:
  - **Performance degradation on non-symmetric tasks:** If the data does not contain symmetric dependencies, the flip may add noise
  - **Overfitting:** High capacity without dropout/weight decay will cause validation loss to diverge
  - **Gradient issues:** Although designed to improve flow, the multiplicative path can still be a source of instability if not regularized
- First 3 experiments:
  1. **Ablation on Vedic Encoding:** Train Mamba2 alone vs. Mamba2 + Vedic Encoding to measure the direct performance gain (MSE reduction) from the proposed module
  2. **Ablation on Bidirectionality:** Train Naga with and without the flip operation to isolate the contribution of the reversed-sequence interaction
  3. **Hyperparameter Sensitivity:** Vary the dropout probability $p$ and hidden dimension $d_h$ to find the stability region, as warned by the theoretical caveats

## Open Questions the Paper Calls Out
- Can formal sample-complexity bounds be established for Naga when modeling generative sequence models with bilinear targets?
- Does the Vedic encoding mechanism generalize to non-time-series domains such as computer vision or natural language processing?
- How susceptible is the Naga architecture to overfitting in data-scarce environments compared to the baseline Mamba2?

## Limitations
- Theoretical claims about improved representational capacity and gradient flow lack empirical validation beyond final MSE numbers
- Critical hyperparameters (hidden dimensions, dropout rate, kernel size, number of layers) are unspecified, creating significant barriers to faithful reproduction
- Performance advantage is demonstrated against broad competitors but specific ablation studies to isolate Vedic encoding contribution are not reported

## Confidence

### High Confidence
- The Vedic encoding mechanism (flipping + projections + Hadamard product) is clearly specified and implementable
- The computational efficiency claim (2.1M parameters vs. larger Transformers) is directly supported by the ablation table

### Medium Confidence
- The state-of-the-art performance claims are supported by extensive benchmarking, but without reported variance across multiple random seeds, the statistical significance is unclear
- The efficiency-accuracy trade-off is plausible but not rigorously proven across diverse task types

### Low Confidence
- The theoretical analysis of gradient improvement and representational capacity increase is mathematically sound but lacks empirical validation showing that these mechanisms are actually responsible for the observed performance gains in real-world datasets

## Next Checks
1. **Ablation of Encoding Component:** Train Mamba2 alone versus Mamba2 with Vedic encoding (keeping all other hyperparameters constant) on at least two datasets to isolate the direct performance contribution of the proposed module
2. **Statistical Significance Testing:** Re-run the experiments with 10 different random seeds and report mean±std for all metrics to establish confidence intervals and determine if reported improvements are statistically significant
3. **Asymmetric Task Evaluation:** Test Naga on a synthetic or real-world time series task where temporal dependencies are known to be asymmetric (e.g., weather forecasting where today strongly influences tomorrow but not vice versa) to validate the assumption that symmetric bilinear interactions are universally beneficial