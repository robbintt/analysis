---
ver: rpa2
title: 'CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency'
arxiv_id: '2511.02603'
source_url: https://arxiv.org/abs/2511.02603
tags:
- calls
- confidence
- cges
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Confidence-Guided Early Stopping (CGES),
  a Bayesian framework that incorporates confidence signals into self-consistency
  for more efficient and accurate test-time scaling. CGES builds posterior distributions
  over candidate answers using scalar confidence scores (derived from token probabilities
  or reward models) and adaptively halts sampling once the posterior mass of a candidate
  exceeds a threshold.
---

# CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency

## Quick Facts
- **arXiv ID**: 2511.02603
- **Source URL**: https://arxiv.org/abs/2511.02603
- **Reference count**: 21
- **Primary result**: Reduces LLM calls by 69.4% while maintaining accuracy within 0.06 percentage points of self-consistency

## Executive Summary
This paper introduces Confidence-Guided Early Stopping (CGES), a Bayesian framework that incorporates confidence signals into self-consistency for more efficient and accurate test-time scaling. CGES builds posterior distributions over candidate answers using scalar confidence scores (derived from token probabilities or reward models) and adaptively halts sampling once the posterior mass of a candidate exceeds a threshold. The method provides theoretical guarantees under both ideal and noisy confidence conditions. Across five reasoning benchmarks, CGES reduces the average number of model calls by 69.4% (e.g., from 16.0 to 4.9) while maintaining accuracy within 0.06 percentage points of self-consistency. The framework offers flexible accuracy-efficiency trade-offs and demonstrates superior performance compared to standard self-consistency and early-stopping baselines, particularly in scenarios where the correct answer is infrequent but highly confident.

## Method Summary
CGES operates by maintaining a posterior distribution over candidate answers for each question, updated after each LLM sampling round using confidence-weighted likelihoods. The algorithm begins with one sample per question, computes initial posteriors via the SCORE method, and iteratively queries only those questions where the maximum posterior remains below threshold γ. This adaptive sampling continues until either all questions meet the stopping criterion or a maximum budget B is reached. Confidence signals are derived from token-level probabilities (using LNS arithmetic/geometric means or MARS step-weighted scores) or external reward models. The theoretical foundation guarantees posterior concentration under both ideal confidence conditions and noisy confidence with positive log-likelihood ratio drift.

## Key Results
- Reduces average LLM calls by 69.4% (from 16.0 to 4.9) while maintaining accuracy within 0.06 percentage points of self-consistency
- Provides theoretical guarantees for posterior concentration under both ideal and noisy confidence conditions
- Outperforms standard self-consistency and early-stopping baselines across five reasoning benchmarks
- Demonstrates superior recovery of correct answers when they are infrequent but highly confident

## Why This Works (Mechanism)

### Mechanism 1
Bayesian posterior aggregation over candidate answers outperforms majority voting when correct answers are infrequent but highly confident. Each response-confidence pair (Rt, Ct) is treated as probabilistic evidence, with likelihoods computed as Ct when Rt matches hypothesis ai, otherwise (1-Ct)/(K-1). This amplifies rare-but-confident answers that majority voting would suppress. The core assumption is that confidence signals correlate with correctness, either perfectly calibrated in ideal case or systematically informative in noisy case with µk > 0 drift. If confidence scores are miscalibrated such that µk < 0 for some k, posterior concentrates on wrong answer.

### Mechanism 2
Early stopping based on posterior concentration reduces LLM calls by ~69% while preserving accuracy. After each sampling round, CGES computes posteriors via SCORE and removes questions from Drem when max posterior exceeds threshold γ. Only unresolved questions receive additional LLM calls, exploiting the observation that many questions reach high confidence after few samples. If confidence signals are uninformative (Ct ≈ 1/K always), posteriors never concentrate, and CGES degrades to full budget sampling without accuracy gains.

### Mechanism 3
Token-level confidence estimates (LNS, MARS) provide practical signals; reward models offer near-optimal upper bounds. LNS computes geometric/arithmetic mean of token probabilities; MARS weights tokens by semantic importance. These scalars Ct ∈ (0,1) feed the Bayesian aggregator. Larger reward models (72B PRM) provide better-calibrated signals but are impractical for deployment. On out-of-domain tasks, PRM confidence may be miscalibrated, yielding accuracy drops despite call savings.

## Foundational Learning

- **Concept: Bayesian posterior updating**
  - Why needed here: Core to CGES—understanding how likelihoods multiply and normalize to form posteriors over hypotheses
  - Quick check question: Given three samples with confidences [0.8, 0.6, 0.9] all supporting answer A (K=4 candidates), compute the unnormalized posterior for A vs. a wrong answer B

- **Concept: Log-likelihood ratio (LLR) and drift**
  - Why needed here: Theoretical guarantees hinge on LLR drift µk > 0; understanding why positive drift ensures posterior concentration
  - Quick check question: If Ct > 1/K, explain why the LLR increment Y(t)k has positive expectation under the true hypothesis

- **Concept: Confidence calibration in LLMs**
  - Why needed here: CGES performance depends on whether token probabilities or reward scores reflect actual correctness; miscalibration breaks assumptions
  - Quick check question: What happens to CGES if the model is systematically overconfident (Ct near 1.0 for wrong answers)?

## Architecture Onboarding

- **Component map**: Confidence Estimator -> SCORE (Algorithm 1) -> CGES Loop (Algorithm 2) -> Output Label
- **Critical path**: Initial sampling (1 response per question) → Filtering (identify unresolved questions) → Adaptive sampling (query only below threshold) → Update (recompute posteriors) → Termination (return argmax label)
- **Design tradeoffs**: Threshold γ (lower = fewer calls, risk of premature stopping; higher = more calls, higher confidence); Confidence source (token-level cheap/noisy vs. reward model expensive/better-calibrated); Budget B (maximum calls per question)
- **Failure signatures**: Sticky low posteriors (never exceed γ, run to budget with no benefit); Wrong answer high confidence (miscalibrated confidence amplifies errors); High variance across seeds (confidence signals unreliable)
- **First 3 experiments**: 1) Implement SCORE with LNS on GSM8K with B=16, γ=0.9; verify ~4.5 average calls and ~94.3% accuracy; 2) On MATH500, sweep γ ∈ [0.70, 0.95] and plot accuracy vs. average calls; 3) Construct synthetic queries where correct answer appears in <30% of samples but has high confidence; compare CGES vs. majority vote recovery rate

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the required number of samples be predicted dynamically from confidence signals prior to or during the inference process? The paper explicitly lists this as future work, noting CGES currently operates on a fixed maximum budget B and stopping threshold γ without estimating optimal calls needed for specific query difficulty.
- **Open Question 2**: Can lightweight confidence estimators be developed that match the performance of large Process Reward Models (PRMs) without incurring their computational overhead? The paper identifies this as future work, noting the current trade-off where efficient probability-based signals cause small accuracy dips while effective reward models are computationally prohibitive.
- **Open Question 3**: How does the uniform error distribution assumption (Assumption 3) affect performance on tasks where incorrect answers are semantically clustered rather than uniformly distributed? The paper doesn't ablate or test robustness when likelihood of specific incorrect answers is highly correlated or skewed, which could impact minority-but-confident recovery capability.
- **Open Question 4**: Can the CGES framework be adapted for open-ended generation tasks where candidate answers are not discrete entities but continuous or complex text spans? The paper mentions "open-domain tasks" but methodology is restricted to discrete candidate sets, leaving applicability to generative tasks unexplored.

## Limitations
- Performance critically depends on confidence estimator quality, with token-level confidences less reliable than reward models that are computationally expensive
- Theoretical guarantees rely on assumptions about confidence signal properties that may not hold in practice with miscalibrated models
- The uniform error distribution assumption may not hold for tasks with semantically clustered incorrect answers

## Confidence
- **High Confidence**: 69.4% reduction in LLM calls (directly supported by Table 1 and Figure 3); Theoretical guarantees (Theorems 1 and 2); Accuracy preservation (<0.1% accuracy loss)
- **Medium Confidence**: Cross-task generalizability (results on five benchmarks suggest robustness); Practical deployment viability (token-level confidences are cheap but reliability varies)
- **Low Confidence**: Robustness to miscalibration (theoretical guarantees assume certain properties); Scaling to larger models (paper focuses on 7B models, performance on frontier models untested)

## Next Checks
1. **Confidence Signal Stress Test**: Construct synthetic datasets where correct answers appear in <20% of samples but with systematically higher confidence. Measure CGES recovery rate vs. majority voting and quantify minimum confidence gap needed for reliable performance.
2. **Domain Adaptation Analysis**: Evaluate CGES across progressively more