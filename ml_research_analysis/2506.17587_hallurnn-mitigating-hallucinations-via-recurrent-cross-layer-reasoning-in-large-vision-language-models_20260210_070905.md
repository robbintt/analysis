---
ver: rpa2
title: 'HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in
  Large Vision-Language Models'
arxiv_id: '2506.17587'
source_url: https://arxiv.org/abs/2506.17587
tags:
- hallurnn
- hallucinations
- arxiv
- large
- dg-dpu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HalluRNN introduces a recurrent cross-layer reasoning framework
  to mitigate hallucinations in Large Vision-Language Models (LVLMs). The method employs
  a Dual-Gated Depth Propagation Unit (DG-DPU) that recurrently refines hidden states
  across Transformer layers, enabling adaptive propagation of information and enforcing
  consistency to counteract representational drift.
---

# HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2506.17587
- Source URL: https://arxiv.org/abs/2506.17587
- Authors: Le Yu; Kaishen Wang; Jianlong Xiong; Yue Cao; Lei Zhang; Zhang Yi Tao He
- Reference count: 40
- Primary result: Reduces hallucinations by 17.4% on CHAIR benchmark via recurrent cross-layer reasoning

## Executive Summary
HalluRNN introduces a recurrent cross-layer reasoning framework to mitigate hallucinations in Large Vision-Language Models (LVLMs). The method employs a Dual-Gated Depth Propagation Unit (DG-DPU) that recurrently refines hidden states across Transformer layers, enabling adaptive propagation of information and enforcing consistency to counteract representational drift. By fine-tuning only the DG-DPU module, HalluRNN achieves robust performance across multiple benchmarks. On the POPE benchmark, it improves accuracy by 2.54% to 5.47% over strong baselines like DeCO and VCD. On MM-Vet, it scores 31.0, outperforming DOLA and DeCO. On CHAIR, it reduces hallucinations by 17.4% in object captions. Cross-model transferability is validated on INF-MLLM1 and Qwen-VL. Ablation studies confirm the necessity of both constraint and correction gates in the DG-DPU design.

## Method Summary
HalluRNN fine-tunes only a Dual-Gated Depth Propagation Unit (DG-DPU) module while keeping the LLaVA-1.5 7B backbone frozen. The DG-DPU is inserted after each Transformer layer to recurrently refine hidden states across the network depth. It operates through a dual-gate mechanism: a Constraint Gate that filters out abrupt changes in hidden states, and a Correction Gate that fine-tunes the balance between new evidence and stabilized context. The model is trained for 3 epochs on 17K examples from the LLaVA-Instruct-150 dataset, using a learning rate of 1e-5 and batch size of 10. Inference uses greedy decoding with temperature=0. The DG-DPU weights are shared across all 32 layers to maintain parameter efficiency.

## Key Results
- On POPE benchmark: improves accuracy by 2.54% to 5.47% over baselines like DeCO and VCD
- On MM-Vet: achieves score of 31.0, outperforming DOLA (30.5) and DeCO (29.9)
- On CHAIR: reduces hallucinations by 17.4% in object captions compared to vanilla LLaVA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating the sequential layers of a Transformer as a temporal process allows for the enforcement of consistency and the mitigation of "representational drift," which the authors identify as a primary source of hallucinations.
- **Mechanism:** HalluRNN injects a recurrent module (DG-DPU) between Transformer layers. Instead of allowing layer $i$ to feed forward into layer $i+1$ without external context, the model treats the depth dimension as a time step. The DG-DPU maintains a "dynamic memory" (recurrent state $v$) that accumulates and filters information, acting as a stabilizer that prevents later layers from diverging too far from earlier, grounded visual representations.
- **Core assumption:** Hallucinations in Large Vision-Language Models (LVLMs) are not random errors but the result of "reasoning inconsistencies" or drift that accumulates as data propagates through the network depth.
- **Evidence anchors:**
  - [abstract] ("mitigates hallucinations caused by representational drift")
  - [section 3.2] ("treating the sequence of hidden states across Transformer layers as a temporal process")
  - [corpus] Related work on "Inter-Layer Consistency Aggregation" suggests that stabilizing layer outputs is a valid approach to hallucination mitigation.
- **Break condition:** If the initial visual representations (early layers) are already ungrounded or corrupted, this recurrent mechanism may simply propagate and enforce the consistency of an incorrect premise, failing to correct the root error.

### Mechanism 2
- **Claim:** A dual-gate structure (Constraint and Correction) is necessary to distinguish between harmful representational drift and beneficial fine-grained reasoning updates.
- **Mechanism:** The DG-DPU operates a coarse-to-fine gating process.
    1.  **Constraint Gate:** Analyzes the deviation ($\Delta = m_t^i - v_t^i$) between the current layer update and the recurrent history. It acts as a filter to suppress abrupt changes that appear unstable.
    2.  **Correction Gate:** A fine-grained controller that decides whether to trust the new update or the stabilized history. It prevents the model from over-correcting (stagnating) or under-correcting (drifting).
- **Core assumption:** Not all deviations in later layers are hallucinations; some are valid reasoning steps. A single gate (like in a standard GRU) cannot effectively differentiate "reasoning drift" from "hallucination drift."
- **Evidence anchors:**
  - [section 3.4] ("operates at a finer granularity to decide whether to prioritize new evidence or rely on stabilized context")
  - [ablation] ("full DG-DPU outperforms both single-gate variants")
- **Break condition:** If the gating weights are not tuned effectively (e.g., via the ablation study mentioned), the gates may either block valid new information (causing stagnation) or fail to suppress noise (providing no improvement over Vanilla).

### Mechanism 3
- **Claim:** Freezing the pre-trained backbone and fine-tuning only the cross-layer DG-DPU module preserves the model's general capabilities while specifically optimizing the *flow* of information.
- **Mechanism:** The LVLM backbone (e.g., LLaVA-1.5) remains frozen. The training process optimizes only the lightweight DG-DPU parameters. This forces the module to learn a generalized recurrence pattern for stabilizing outputs without overfitting to specific data or altering the fundamental knowledge representations of the base model.
- **Core assumption:** The pre-trained backbone possesses sufficient visual grounding capabilities, and the primary failure mode is the *propagation* of this information, not the quality of the features themselves.
- **Evidence anchors:**
  - [abstract] ("By fine-tuning only the DG-DPU module...")
  - [section 4.3] Cross-model transferability (training on LLaVA, testing on INF-MLLM1) suggests the module learns a generalizable stability function rather than memorizing model-specific features.
- **Break condition:** This approach may be insufficient if the backbone's visual encoder is fundamentally misaligned with the language model, as the DG-DPU is designed to stabilize existing representations, not fundamentally realign them.

## Foundational Learning

- **Concept:** **Representational Drift**
  - **Why needed here:** The paper explicitly frames hallucinations as a failure of consistency across depth. Understanding that later layers can "forget" or distort visual constraints from earlier layers is central to the HalluRNN thesis.
  - **Quick check question:** If you froze the weights of a Transformer, would the output distribution of the 10th layer be identical to the 20th layer if no residual connections existed? (Hint: Consider how information evolves).
- **Concept:** **Gating Mechanisms (RNNs/GRUs)**
  - **Why needed here:** The DG-DPU is a derivative of a GRU. To understand why this works, one must grasp how sigmoid gates (0 to 1) control the flow of informationâ€”allowing the network to "forget" drift or "remember" grounding.
  - **Quick check question:** In a gating equation $g \cdot x + (1-g) \cdot y$, what happens to the output if $g \approx 1$?
- **Concept:** **Layer-wise Interaction / Skip Connections**
  - **Why needed here:** This work is positioned as an advancement over ResNet/DenseNet-style connections. It is a dynamic, learned interaction rather than a fixed skip connection.
  - **Quick check question:** How does a learned, adaptive connection (like DG-DPU) differ from a simple additive skip connection (ResNet) in terms of handling conflicting information between layers?

## Architecture Onboarding

- **Component map:** Frozen Transformer layers (LLaVA-1.5) -> Inter-layer Transition (m_i) -> DG-DPU (Constraint Gate + Correction Gate) -> Refined recurrent state (v_{i+1}) -> Updated hidden state (h_{i+1}) -> Next Layer
- **Critical path:** Forward Pass -> Compute m_i (Transition) -> DG-DPU (Constraint + Correction) -> Update Hidden State -> Next Layer
- **Design tradeoffs:**
  - **Compute vs. Stability:** Introduces recurrent operations at *every* layer, increasing inference latency compared to training-free methods (like VCD)
  - **Specialization vs. Generality:** A specialized DG-DPU outperforms a standard GRU (as per ablations), but requires specific implementation effort compared to off-the-shelf RNN cells
- **Failure signatures:**
  - **Stagnation:** If Correction Gates output near 0 for all layers, the model effectively ignores new visual inputs, defaulting to generic priors
  - **Error Propagation:** If early visual features are noisy, the recurrent unit may confidently amplify this noise rather than suppressing it
- **First 3 experiments:**
  1.  **Sanity Check (POPE Benchmark):** Run Vanilla LLaVA-1.5 vs. HalluRNN on the POPE (Polling-based Object Probing Evaluation) dataset to verify that the DG-DPU actually reduces the rate of object hallucinations (e.g., "Is there a tree?" when there isn't)
  2.  **Ablation (Single Gate vs. Dual Gate):** Replace the DG-DPU with a standard GRU or remove one of the two gates to confirm that the *dual* structure is necessary for the claimed performance gains (specifically checking CHAIR scores)
  3.  **Cross-Model Transfer:** Take the DG-DPU weights trained on LLaVA and apply them to a different architecture (like Qwen-VL or INF-MLLM1) without retraining to test if the learned "stability" is a universal logic or just overfitting to LLaVA's hidden states

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the HalluRNN framework be adapted to prevent the propagation of errors when early-layer representations are significantly corrupted by visual noise or adversarial inputs?
- Basis in paper: [explicit] Section E (Limitations) states that the model has a "dependence on the quality of intermediate representations" and warns that "if early-layer representations are significantly corrupted... the recurrent module may propagate errors."
- Why unresolved: The current DG-DPU design assumes that cross-layer consistency is beneficial, but lacks a mechanism to distinguish between valid feature refinement and the amplification of noise introduced in the initial layers.
- What evidence would resolve it: Experiments evaluating HalluRNN on adversarial robustness benchmarks (e.g., ImageNet-C or adversarially perturbed images) to measure if errors in early layers disproportionately degrade final output quality compared to non-recurrent baselines.

### Open Question 2
- Question: Can integrating explicit confidence estimation into the Dual-Gated Depth Propagation Unit (DG-DPU) improve the filtering of misleading signals during recurrent cross-layer reasoning?
- Basis in paper: [explicit] Section E (Limitations) suggests, "Addressing this issue [of error propagation] may require integrating robustness-aware mechanisms or confidence estimation into the cross-layer reasoning process."
- Why unresolved: The current gating mechanism operates on representational drift (the difference between adjacent layers) rather than an explicit metric of uncertainty or reliability for the visual features being propagated.
- What evidence would resolve it: Ablation studies comparing the standard DG-DPU against a variant augmented with uncertainty quantification (e.g., using entropy or variance to gate the flow of information), specifically tested on datasets with high visual ambiguity.

### Open Question 3
- Question: Does the improved fluency and coherence resulting from HalluRNN increase the risk of generating "persuasive" inaccurate content, complicating safety alignment?
- Basis in paper: [explicit] Section I (Broader Impacts) notes that "improved fluency and coherence of outputs may also inadvertently increase the risk of generating persuasive yet inaccurate or harmful content."
- Why unresolved: While the paper demonstrates reduced hallucination rates on benchmarks like POPE, it does not evaluate if the remaining errors are more difficult for humans to detect due to the model's enhanced stability and textual coherence.
- What evidence would resolve it: Human evaluation studies or automated safety benchmarks designed to measure the "deceptiveness" of model outputs, specifically comparing the severity of trust violations in HalluRNN versus vanilla baselines when errors do occur.

## Limitations

- HalluRNN's effectiveness is primarily evaluated on object hallucination detection benchmarks (POPE, CHAIR), with unclear performance on more nuanced hallucination types like factual errors
- The computational overhead of recurrent operations at every layer could impact deployment in latency-sensitive applications
- The method's ability to handle complex reasoning failures beyond object grounding remains unclear from current evaluations

## Confidence

- **High Confidence:** The dual-gate DG-DPU design and its superiority over single-gate variants are well-supported by ablation studies (CHAIR results showing 17.4% hallucination reduction)
- **Medium Confidence:** Cross-model transferability claims are supported but limited to only two additional models (INF-MLLM1, Qwen-VL) with modest improvements
- **Medium Confidence:** Generalization across diverse benchmarks (POPE, MM-Vet, CHAIR) suggests robustness, though each benchmark measures different aspects of hallucination

## Next Checks

1. **Multi-turn Dialogue Test:** Evaluate HalluRNN on a conversational benchmark to assess if recurrent cross-layer reasoning maintains consistency across extended interactions
2. **Complex Reasoning Benchmark:** Test on datasets requiring multi-step reasoning (e.g., HotpotQA) to determine if the method addresses reasoning drift beyond object grounding
3. **Ablation on Gate Sensitivity:** Systematically vary gate thresholds to identify the sensitivity range where HalluRNN provides optimal hallucination mitigation without over-stabilization