---
ver: rpa2
title: 'CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics'
arxiv_id: '2512.21877'
source_url: https://arxiv.org/abs/2512.21877
tags:
- domain
- queries
- schema
- data
- match
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CricBench, a multilingual benchmark for evaluating
  LLMs in cricket analytics. It addresses the gap between general-purpose Text-to-SQL
  models and the domain-specific reasoning required for cricket data.
---

# CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics

## Quick Facts
- arXiv ID: 2512.21877
- Source URL: https://arxiv.org/abs/2512.21877
- Reference count: 10
- Primary result: DeepSeek R1 achieves highest accuracy (50.6%) on cricket-specific Text-to-SQL tasks, with all models underperforming compared to general benchmarks

## Executive Summary
This paper introduces CricBench, a manually curated multilingual benchmark specifically designed to evaluate Large Language Models' performance on cricket analytics tasks. The benchmark addresses a critical gap in existing Text-to-SQL evaluation frameworks by focusing on domain-specific reasoning and structured data handling required for cricket statistics. The authors develop a dynamic complexity router that injects domain-specific rules when needed, enabling context-aware evaluation of model performance. Experiments reveal significant performance drops across six state-of-the-art models when applied to cricket-specific queries, with DeepSeek R1 achieving the highest accuracy of 50.6%. Notably, code-mixed Hindi queries sometimes outperform English, challenging assumptions about optimal prompt languages for specialized SQL tasks.

## Method Summary
The authors create CricBench by manually curating 41 cricket analytics queries in both English and Hindi, developed in consultation with domain experts to ensure authenticity and complexity. The benchmark employs a dynamic complexity router that analyzes query structure and injects cricket-specific rules when necessary, creating a context-aware evaluation framework. This router mechanism adapts to query complexity by determining when domain-specific knowledge should be incorporated into the SQL generation process. The evaluation framework measures both accuracy and formatting consistency across different model sizes and language combinations, providing insights into how multilingual and domain-specific factors affect performance.

## Key Results
- DeepSeek R1 achieves highest accuracy at 50.6%, but all models show significant performance drops compared to general benchmarks
- Dynamic complexity router improves accuracy for frontier models but causes formatting issues for smaller models
- Code-mixed Hindi queries sometimes outperform English, challenging assumptions about optimal prompt languages for specialized SQL tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its domain-specific focus on cricket analytics, which requires specialized reasoning patterns not captured in general Text-to-SQL benchmarks. The manual curation ensures queries reflect real-world complexity patterns in sports analytics, while the dynamic router mechanism provides context-aware adaptation that injects domain knowledge precisely when needed. The multilingual approach reveals language-specific performance variations that would be missed in monolingual evaluations.

## Foundational Learning
1. **Domain-specific SQL reasoning**: Cricket analytics requires understanding of sports-specific concepts like overs, wickets, and batting averages that don't exist in general databases - needed because general models lack this specialized knowledge
2. **Dynamic complexity routing**: Adaptive rule injection based on query structure enables context-aware SQL generation - needed because static prompting fails to capture varying complexity levels
3. **Multilingual evaluation frameworks**: Code-mixing and language-specific performance variations reveal hidden biases in model training - needed because most benchmarks assume monolingual evaluation
4. **Manual vs. automated benchmark curation**: Expert-curated queries ensure domain authenticity but limit scalability - needed because automated generation lacks domain expertise
5. **Model size performance scaling**: Router benefits vary significantly with model size - needed because larger models can better utilize additional context

## Architecture Onboarding
Component map: Query Parser -> Complexity Router -> SQL Generator -> Result Validator

Critical path: Query parsing identifies domain-specific terms and complexity indicators, which the router uses to determine rule injection points. The SQL generator incorporates these rules to produce structured queries, and the validator checks both syntactic correctness and semantic accuracy against cricket database schemas.

Design tradeoffs: Manual curation ensures quality but limits scalability; router complexity improves frontier models but introduces formatting inconsistencies; multilingual support reveals language-specific patterns but increases evaluation complexity.

Failure signatures: Router-induced formatting errors in smaller models; performance degradation on code-mixed queries; accuracy drops on queries requiring multiple domain-specific rules; inconsistent behavior across language pairs.

First experiments:
1. Test router performance on a subset of queries with varying complexity levels to isolate routing effectiveness
2. Compare performance of code-mixed queries against pure English and Hindi baselines
3. Evaluate model performance on queries requiring single vs. multiple domain-specific rules

## Open Questions the Paper Calls Out
None

## Limitations
- Manual curation limits scalability and may introduce selection bias toward certain query types
- Small test set (41 queries) constrains statistical power for robust comparisons
- Router mechanism improves frontier models but causes formatting inconsistencies in smaller models
- Hindi performance advantage may be dataset-specific rather than generalizable

## Confidence
- **High confidence**: General-purpose Text-to-SQL models struggle with domain-specific cricket analytics queries
- **Medium confidence**: Dynamic complexity router effectiveness varies by model size
- **Medium confidence**: Code-mixed Hindi performance advantage may be dataset-specific
- **Low confidence**: Scalability of manual benchmark approach

## Next Checks
1. Expand dataset to 200+ queries across multiple sports domains to validate Hindi performance patterns
2. Conduct ablation studies on router mechanism to isolate effectiveness from prompt engineering artifacts
3. Test cross-lingual transfer learning by training on multilingual cricket queries and evaluating on unseen languages