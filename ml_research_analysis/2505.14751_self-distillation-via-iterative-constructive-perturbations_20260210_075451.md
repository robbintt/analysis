---
ver: rpa2
title: Self Distillation via Iterative Constructive Perturbations
arxiv_id: '2505.14751'
source_url: https://arxiv.org/abs/2505.14751
tags:
- input
- 'true'
- self-distillation
- 'false'
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of balancing model performance
  and generalization in deep neural networks by introducing a novel framework that
  integrates Iterative Constructive Perturbation (ICP) with self-distillation. The
  method uses gradient-based iterative input refinement to construct enhanced representations,
  which are then aligned with original features through a self-distillation process.
---

# Self Distillation via Iterative Constructive Perturbations

## Quick Facts
- arXiv ID: 2505.14751
- Source URL: https://arxiv.org/abs/2505.14751
- Authors: Maheak Dave; Aniket Kumar Singh; Aryan Pareek; Harshita Jha; Debasis Chaudhuri; Manish Pratap Singh
- Reference count: 27
- Primary result: ICP self-distillation improves CIFAR-100 accuracy by up to 19.06% and reduces VAE FID on CUB dataset

## Executive Summary
This paper introduces a novel self-distillation framework that combines Iterative Constructive Perturbation (ICP) with self-distillation to improve deep neural network performance. The method uses gradient-based iterative input refinement to construct enhanced representations, which are then aligned with original features through a self-distillation process. This cyclic optimization approach alternates between adjusting the model to the data and the data to the model, improving feature quality and model performance. Experiments on image classification (CIFAR-100) and image generation (CUB dataset) demonstrate significant improvements over baseline methods, with AdEMAMix-ICP achieving up to 19.06% higher accuracy and better F1 scores in classification while also improving SSIM and reducing FID in image generation tasks.

## Method Summary
The framework integrates Iterative Constructive Perturbation (ICP) with self-distillation through a two-phase training process. During the first phase (k=25 epochs), the model trains with standard task loss only. In the second phase, ICP iteratively refines each input by minimizing the model's loss via gradient descent (x_t = x_{t-1} - ε · ∇L) over T=5 iterations. The perturbed input generates enhanced intermediate features that serve as targets for self-distillation, where layer-wise MSE losses align original and enhanced features. A weighted combination prioritizes deeper layers, and total loss combines task and distillation components with cosine-decay scheduling for the balance parameter. Three ICP variants (SGD-ICP, Adam-ICP, AdEMAMix-ICP) are evaluated on CIFAR-100 classification and CUB image generation tasks.

## Key Results
- AdEMAMix-ICP achieves up to 19.06% higher accuracy compared to baseline on CIFAR-100
- Self-distillation framework improves F1 scores in classification tasks
- Image generation quality improves with better SSIM and reduced FID scores on CUB dataset
- Training time increases by 25-30% due to iterative refinement process
- T=5 ICP iterations provides optimal balance between performance gain and computational overhead

## Why This Works (Mechanism)

### Mechanism 1
ICP applies gradient descent on the input: x_t = x_{t-1} - ε · ∇_{x_{t-1}} J(θ, x_{t-1}, y) over T iterations. Unlike adversarial perturbations that maximize loss, ICP minimizes loss, pushing samples toward class-interior regions and away from decision boundaries. The model's current parameters encode a useful feature manifold, and gradient-based input optimization yields semantically valid enhanced inputs rather than degenerate solutions.

### Mechanism 2
After generating ICP input I', both I and I' pass through the network. Layer-wise MSE losses L_i^{dist} = MSE(F_i, F'_i) align features, with a weighted combination that prioritizes deeper layers. ICP-enhanced features represent a "cleaner" target, and aligning to them improves generalization without requiring external teacher models.

### Mechanism 3
Total loss L_total = α_e · L_task + (1 - α_e) · Σ L_i^{dist} uses cosine-decay scheduling where α_e = 1 for first k epochs (baseline), then decays via cos(π(e-k)/(2(E-k))). This transitions from pure task learning to increasing distillation emphasis, enabling early task-focused learning followed by gradual representation refinement.

## Foundational Learning

- Concept: **Gradient-based input optimization (FGSM family)**
  - Why needed here: ICP is conceptually inverted FGSM; understanding adversarial perturbations clarifies why constructive perturbations work oppositely.
  - Quick check question: Can you explain why FGSM uses +ε·sign(∇_x L) while ICP uses -ε·∇_x L, and what this implies for decision boundary movement?

- Concept: **Knowledge distillation and self-distillation**
  - Why needed here: The framework uses self-distillation without external teachers; understanding soft labels and feature-based distillation is essential.
  - Quick check question: What is the difference between logit-based distillation and feature-based distillation, and why might intermediate features preserve more information?

- Concept: **Optimizer dynamics (SGD, Adam, AdEMAMix)**
  - Why needed here: ICP variants apply these optimizers to input space; understanding momentum and adaptive learning rates clarifies why AdEMAMix-ICP performs best.
  - Quick check question: How does Adam's first and second moment estimation differ from SGD, and why might this stabilize iterative input refinement?

## Architecture Onboarding

- Component map:
  Input batch -> Forward pass -> {O, F_i, L_task}
  ICP module: Takes L_task, computes ∇_x L, iterates T times -> I'
  Second forward pass on I' -> {F'_i}
  Distillation loss: Layer-wise MSE(F_i, F'_i) with optional weighting
  Loss combiner: α_e · L_task + (1-α_e) · Σ L_i^{dist}
  Backward pass: Updates model parameters only (not inputs permanently)

- Critical path:
  1. Baseline training for k epochs (α_e = 1, no ICP overhead)
  2. At epoch k+1: enable ICP (T iterations per sample) + distillation
  3. α_e decays; distillation weight increases until epoch E
  4. Monitor: task accuracy, distillation loss convergence

- Design tradeoffs:
  - T (ICP iterations): Higher T = better input refinement but ~linear compute increase. Table 1 shows T=5 often matches or beats T=10.
  - k (baseline epochs): k=25 optimal in experiments; too early = unstable features, too late = insufficient distillation epochs.
  - Weighted vs. unweighted distillation: Weighted (prioritizing deeper layers) generally better but adds hyperparameter sensitivity.

- Failure signatures:
  - Accuracy drops after epoch k: α_e decay may be too aggressive; try larger k or slower decay.
  - ICP inputs diverge (NaN/inf): ε too large; reduce learning rate for ICP optimizer.
  - No improvement over baseline: Check that ICP uses gradient *descent* (negative direction), not ascent.
  - Training time explodes: T too high; Table 1 shows T=5 adds ~25-40% overhead vs. baseline.

- First 3 experiments:
  1. **Sanity check**: Reproduce CIFAR-100 baseline (22.93%) without ICP; then enable SGD-ICP with k=25, T=5, weighted=True. Target: ~40% accuracy.
  2. **Ablation on k**: Fix T=5, test k ∈ {0, 25, 50}. Expect k=25 optimal; k=0 should underperform due to uninitialized features.
  3. **Optimizer comparison**: Compare SGD-ICP, Adam-ICP, AdEMAMix-ICP on same hyperparameters. Expect AdEMAMix-ICP ≥ Adam-ICP ≥ SGD-ICP based on Table 1.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the ICP-based self-distillation framework perform when applied to significantly larger architectures (e.g., Transformers, Large Language Models) and high-dimensional datasets like ImageNet?
  - Basis in paper: The conclusion states, "While our experiments were conducted on medium-sized tasks, the framework can be extended to larger models and more intricate datasets in the future."
  - Why unresolved: The provided experiments are restricted to ResNet20 on CIFAR-100 and a small VAE on CUB, leaving the scalability properties unproven.
  - What evidence would resolve it: Benchmarking results on standard large-scale datasets (e.g., ImageNet) and diverse architectures (e.g., Vision Transformers) showing convergence behavior and accuracy gains.

- **Open Question 2**: Why does the framework's performance deteriorate significantly when the self-distillation phase is initialized at later epochs (e.g., k=75), and is there a robust heuristic for selecting k?
  - Basis in paper: Table 1 shows a non-monotonic relationship where accuracy peaks at k=25 but drops sharply (e.g., from 41.99% to 26.15% for AdEMAMix-ICP) when k is increased to 75.
  - Why unresolved: The paper empirically selects k=25 via tuning but lacks a theoretical explanation for why delaying distillation harms the optimization trajectory.
  - What evidence would resolve it: An ablation study analyzing the feature space maturity at different epochs or a theoretical analysis of the gradient alignment between the baseline and perturbed inputs.

- **Open Question 3**: Can the computational overhead of the iterative refinement process be reduced to make the method viable for training regimes where the 25-30% time increase is prohibitive?
  - Basis in paper: The abstract claims a "computationally efficient approach," but Tables 1 and 2 consistently show a training time increase (e.g., Control: 29.25 mins vs. AdEMAMix-ICP: 37.37 mins).
  - Why unresolved: The method requires multiple forward and backward passes per batch (T iterations) during the distillation phase, increasing the wall-clock time.
  - What evidence would resolve it: A comparison of "accuracy-per-FLOP" or the development of a sparse ICP variant that applies perturbations less frequently or to fewer layers.

## Limitations
- The method requires 25-30% additional training time due to iterative refinement process
- Optimal hyperparameters (k=25 baseline epochs, T=5 iterations) appear dataset-specific and may not generalize
- The framework's effectiveness on large-scale architectures and datasets remains untested
- Exact weighting function for layer-wise distillation and specific layers selected are unspecified

## Confidence
- **High Confidence**: The core cyclic optimization framework and experimental validation on CIFAR-100 and CUB are well-specified and reproducible.
- **Medium Confidence**: The assumption that ICP produces semantically valid enhanced representations is supported by visualization but lacks quantitative validation.
- **Low Confidence**: The generalizability of optimal hyperparameters and framework effectiveness to other architectures and larger datasets remains unproven.

## Next Checks
1. **Input Semantic Preservation**: Apply ICP with varying ε to CIFAR-100 test images and measure: (a) classification accuracy of perturbed inputs using a frozen pretrained model, (b) Fréchet Distance between original and perturbed image feature distributions, (c) human perceptual study on perturbed image recognizability.

2. **Layer Distillation Weight Sensitivity**: Implement multiple weighting schemes (linear increase, exponential increase, uniform) for the 3 intermediate layers used in distillation. Train AdEMAMix-ICP variants with each scheme on CIFAR-100 for 100 epochs. Compare final accuracy and F1 scores.

3. **Cross-Dataset Generalization**: Apply the full AdEMAMix-ICP framework (k=25, T=5, weighted distillation) to a new image classification dataset (e.g., TinyImageNet or STL-10). Compare performance against: (a) standard baseline training, (b) AdEMAMix-ICP with k=0, (c) AdEMAMix-ICP with k=50.