---
ver: rpa2
title: Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems
arxiv_id: '2509.20989'
source_url: https://arxiv.org/abs/2509.20989
tags:
- loss
- items
- teacher
- student
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes Cross-Entropy (CE) loss in knowledge distillation\
  \ (KD) for recommender systems. It proves that CE loss maximizes a lower bound of\
  \ NDCG on a given item subset, but only if the subset satisfies an assumption of\
  \ closure\u2014requiring the student\u2019s top items to be included."
---

# Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems

## Quick Facts
- arXiv ID: 2509.20989
- Source URL: https://arxiv.org/abs/2509.20989
- Reference count: 40
- Primary result: Proposes RCE-KD to fix CE loss limitations in KD for recommender systems, showing significant NDCG improvements

## Executive Summary
This paper identifies a fundamental limitation in using standard Cross-Entropy (CE) loss for knowledge distillation in recommender systems. The authors prove that CE loss only maximizes NDCG lower bounds when a restrictive "closure" assumption holds - that the student's top items must include the teacher's top items. This assumption is frequently violated in practice, leading to suboptimal distillation. To address this, they propose Rejuvenated Cross-Entropy for Knowledge Distillation (RCE-KD), which adaptively splits the teacher's top items into subsets and applies CE loss differently based on whether items are also ranked highly by the student.

## Method Summary
The paper proposes RCE-KD to address CE loss limitations in knowledge distillation. The method splits the teacher's top items into two subsets: items also ranked high by the student (subset A), and those not (subset B). For subset A, CE loss is applied directly. For subset B, a sampling strategy approximates the closure assumption. The losses are adaptively combined based on subset sizes, with weighting determined by the relative importance of each subset. This approach allows the student to focus on learning from items it can actually rank well while still capturing information from items it initially struggles with.

## Key Results
- RCE-KD significantly outperforms state-of-the-art KD methods in both homogeneous and heterogeneous settings
- The method shows improved NDCG performance by addressing the closure assumption violation
- Experimental results validate the effectiveness of the adaptive subset-based approach

## Why This Works (Mechanism)
The mechanism works by recognizing that standard CE loss fails when the student's top recommendations don't align with the teacher's. By splitting items into subsets based on student ranking and applying different strategies, RCE-KD ensures the student learns effectively from items it can actually rank well while still capturing information from challenging items through sampling. The adaptive weighting ensures the learning process remains balanced and focused on the most informative signals.

## Foundational Learning
- Knowledge Distillation (KD): Transfer learning technique where a student model learns from a teacher model's outputs. Why needed: Core framework for the proposed method. Quick check: Understand how soft labels are used in KD.
- NDCG (Normalized Discounted Cumulative Gain): Ranking metric that accounts for position bias. Why needed: The theoretical foundation shows CE loss maximizes NDCG lower bounds. Quick check: Compare NDCG to other ranking metrics like MRR.
- Closure Assumption: Theoretical requirement that student's top items include teacher's top items for CE loss optimality. Why needed: Identifies the fundamental limitation being addressed. Quick check: Understand why this assumption often fails in practice.
- Sampling Strategy: Method to approximate closure assumption for items the student ranks poorly. Why needed: Enables learning from challenging items. Quick check: Consider variance implications of the sampling approach.
- Adaptive Weighting: Mechanism to balance losses from different item subsets. Why needed: Ensures optimal learning focus across item sets. Quick check: Analyze sensitivity to weighting parameters.

## Architecture Onboarding

Component Map:
Teacher Model -> Item Ranking -> Subset Division (High/Low Student Rank) -> RCE-KD Module -> Student Model

Critical Path:
1. Teacher generates item rankings
2. Items split into subsets based on student's current rankings
3. RCE-KD applies appropriate loss strategies to each subset
4. Combined loss updates student model

Design Tradeoffs:
- Sampling strategy introduces variance but enables learning from challenging items
- Adaptive weighting is heuristic but effective, lacking theoretical justification
- Subset division requires careful threshold selection

Failure Signatures:
- If teacher and student rankings are too dissimilar, subset B may become too large for effective sampling
- Poor threshold selection for subset division can lead to suboptimal loss allocation
- High variance in sampling strategy may cause unstable training

First Experiments:
1. Test RCE-KD with varying subset size thresholds to find optimal division
2. Compare performance with fixed vs. adaptive weighting schemes
3. Evaluate sampling strategy variance across different teacher-student similarity scenarios

## Open Questions the Paper Calls Out
The paper identifies several open questions: the closure assumption may be too restrictive for highly heterogeneous teacher-student architectures where item relevance signals diverge significantly; the sampling strategy lacks detailed analysis of variance and convergence properties; and the adaptive weighting scheme, while empirically effective, lacks theoretical justification.

## Limitations
- Closure assumption may be too restrictive for highly heterogeneous teacher-student architectures
- Sampling strategy lacks detailed analysis of variance and convergence properties
- Adaptive weighting scheme is heuristic without theoretical justification

## Confidence
- Theoretical claims: High
- Practical effectiveness: Medium (limited ablation studies on sampling strategy and adaptive weighting)

## Next Checks
1. Conduct ablation studies to isolate the contribution of the sampling strategy versus the adaptive weighting mechanism
2. Test RCE-KD performance across diverse ranking metrics (MRR, recall@k, MAP) beyond NDCG
3. Analyze the variance and convergence behavior of the proposed sampling approach under different teacher-student similarity scenarios