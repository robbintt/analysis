---
ver: rpa2
title: Robust Non-negative Proximal Gradient Algorithm for Inverse Problems
arxiv_id: '2510.23362'
source_url: https://arxiv.org/abs/2510.23362
tags:
- sso-pga
- gradient
- image
- proximal
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SSO-PGA, a novel robust non-negative proximal
  gradient algorithm that addresses the instability and non-negativity violations
  in traditional PGA for inverse problems. The key innovation is replacing the standard
  gradient descent step with a learnable Sliding Sigmoid Operator (SSO) that inherently
  enforces non-negativity and boundedness while transforming updates from subtractive
  to multiplicative.
---

# Robust Non-negative Proximal Gradient Algorithm for Inverse Problems

## Quick Facts
- arXiv ID: 2510.23362
- Source URL: https://arxiv.org/abs/2510.23362
- Reference count: 40
- Key outcome: SSO-PGA outperforms traditional PGA and state-of-the-art methods across multiple datasets, achieving superior convergence speed, hyperparameter stability, and robustness against perturbations.

## Executive Summary
This paper introduces SSO-PGA, a novel robust non-negative proximal gradient algorithm that addresses the instability and non-negativity violations in traditional PGA for inverse problems. The key innovation is replacing the standard gradient descent step with a learnable Sliding Sigmoid Operator (SSO) that inherently enforces non-negativity and boundedness while transforming updates from subtractive to multiplicative. The method is applied to multi-modal restoration problems and unfolded into a deep network architecture. Extensive experiments demonstrate that SSO-PGA significantly outperforms traditional PGA and state-of-the-art methods across multiple datasets, achieving superior convergence speed, hyperparameter stability, and robustness against perturbations.

## Method Summary
SSO-PGA modifies the proximal gradient algorithm by replacing the standard gradient descent step with a learnable Sliding Sigmoid Operator (SSO). The SSO takes the gradient as input and outputs a multiplicative factor that is always positive due to the sigmoid function properties. This transformation inherently enforces non-negativity while preserving gradient descent semantics. The method is then unfolded into a deep network architecture with learnable hyperparameters and degradation operators, applied to multispectral image fusion and flash-guided non-flash image denoising tasks. The network uses Multi-Scale Spatial-Frequency Blocks (MSFB) and learned proximal operators implemented as deep networks.

## Key Results
- SSO-PGA achieves 0.12-0.32 dB PSNR gains over state-of-the-art methods on multispectral fusion datasets
- Maintains stable performance across a wide range of learning rates (α ∈ {0.01, 0.1, 0.5, 1.0, 3.0, 5.0}) while PGA fails at higher values
- Outperforms traditional PGA and SOTA methods on both reduced and full-resolution tasks with consistent improvements across multiple metrics (PSNR, SAM, ERGAS, Q4/Q8, D_λ, D_s, HQNR)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing subtractive gradient descent with a multiplicative update via SSO inherently enforces non-negativity while preserving gradient descent semantics.
- Mechanism: The Sliding Sigmoid Operator $SSO_\alpha(z) = 2\sigma(-z-\alpha) + 2\sigma(\alpha)-1$ takes the gradient as input and outputs a multiplicative factor. Since $\sigma(\cdot) \in (0,1)$, the multiplier is always positive. Theorem 1 proves this is equivalent to standard gradient descent $y^t_i = y^{t-1}_i - \rho_i \nabla E(y^{t-1}_i)$ with some $\rho_i > 0$.
- Core assumption: The optimal solution lies in $[0, \infty)$; SSO-PGA exhibits oscillatory behavior when the true solution is large.
- Evidence anchors: [Abstract] "transforming traditional subtractive updates into multiplicative ones"; [Section 3.1, Definition 2] "yt = y^{t-1} ⊙ SSO_α(∇E(y^{t-1}))"

### Mechanism 2
- Claim: The sliding parameter α provides bounded, adaptive step-size control that reduces hyperparameter sensitivity compared to fixed learning rates.
- Mechanism: SSO constrains the multiplicative factor within $(2\sigma(\alpha)-1, 2\sigma(\alpha)+1)$. By adjusting α, the operator adaptively controls step size based on gradient magnitude. Table 2 shows SSO-PGA maintains stable performance across α∈{0.01, 0.1, 0.5, 1.0, 3.0, 5.0} while PGA fails at ρ∈{3.0, 5.0}.
- Core assumption: The bounded multiplicative factor provides implicit gradient clipping that prevents overshooting.
- Evidence anchors: [Section 3.1, Fig. 2] Shows SSO curves with different α values sliding through (0,1); [Table 2] PGA PSNR drops to 7.678 at ρ=5.0; SSO-PGA maintains 39.171

### Mechanism 3
- Claim: Unfolding SSO-PGA into a deep network enables learnable hyperparameters and degradation operators while maintaining interpretability.
- Mechanism: The iterative optimization (Eq. 18-23) is unrolled into T stages. Degradation operators K, S are replaced by learnable MSFB modules; hyperparameters (β, γ, α₁, α₂) are learned via backpropagation with Softplus to enforce non-negativity.
- Core assumption: The network can learn degradation patterns that may not be perfectly modeled by hand-crafted operators.
- Evidence anchors: [Section 3.3] "all hyperparameters in each iteration...are learnable and passed through a Softplus function"; [Table 3] SSO-PGA achieves 0.12-0.32 dB PSNR gains over SOTA methods

## Foundational Learning

- **Concept: Proximal Gradient Algorithm (PGA)**
  - Why needed here: SSO-PGA modifies the core gradient descent step; understanding standard PGA (Eq. 4) is prerequisite to appreciating the modification.
  - Quick check question: Can you explain why PGA splits the update into gradient descent on f followed by proximal projection on g?

- **Concept: Sigmoid Function Properties**
  - Why needed here: SSO is built from sigmoids; understanding σ(z) + σ(-z) = 1 and σ'(z) ∈ (0, 0.25] is essential for following Theorem 1's proof.
  - Quick check question: What is the derivative of σ(z) and why is it bounded?

- **Concept: Deep Unfolding / Algorithm Unrolling**
  - Why needed here: The method converts iterative optimization into a trainable network; this paradigm bridges optimization theory and deep learning.
  - Quick check question: How does unrolling an iterative algorithm into a network enable end-to-end learning of hyperparameters?

## Architecture Onboarding

- **Component map:**
  Input (X, Y) → Init Module → [Stage 1...T: H-update (MSFB + SSO + Prox) | T-update (MSFB + SSO)] → Output (H^T + H^0)

- **Critical path:**
  1. Gradient computation: ∇E(H) = 2K^T(KH - X) + 2γf*(f(H) - T)
  2. SSO transformation: multiplier = SSO_α(∇E)
  3. Multiplicative update: H^new = H ⊙ multiplier
  4. Proximal step: Apply deep prior network

- **Design tradeoffs:**
  - More iterations: Better convergence but higher memory/compute (15.20 GiB for fusion task)
  - Fixed α vs. learned α: Learned α (Table 11) outperforms fixed values by ~0.07-0.21 dB
  - Gradient clipping: Stabilizes large-solution cases but may slow convergence for normal cases

- **Failure signatures:**
  - Negative values in intermediate outputs: Indicates SSO not properly applied (check element-wise product)
  - PSNR degradation after iteration 3-4 in PGA baseline: Normal; SSO-PGA should continue improving
  - Oscillation in loss curves: Check if input normalization is missing; solutions >1 cause instability
  - Gradient explosion: Apply gradient clipping to SSO inputs ([-0.1, 0.1] range per Appendix A.2)

- **First 3 experiments:**
  1. **Numerical validation**: Replicate Problem I/II experiments (Eq. 24) with initial values {1, 4, 8, 16} to verify SSO-PGA converges faster than PGA; check for oscillation with large solutions.
  2. **Ablation on α initialization**: Test α ∈ {0.1, 0.5, 1.0, 3.0} with fixed values vs. learned to confirm Table 11 trends on a small subset of WV3 data.
  3. **Convergence curve comparison**: Train SSO-PGA and PGA baseline for 300 epochs on WV3, plot L1 loss curves (Fig. 1b style); verify PGA shows instability while SSO-PGA converges smoothly.

## Open Questions the Paper Calls Out

- **Question:** What is the theoretical convergence rate of SSO-PGA?
  - Basis in paper: [explicit] The conclusion states, "Future work will focus on analyzing the theoretical convergence rate of SSO-PGA."
  - Why unresolved: The paper proves the convergence of the algorithm (that the error is non-increasing) in Theorem 2, but it does not derive the specific convergence order (e.g., $O(1/T)$ or linear convergence).
  - What evidence would resolve it: A formal mathematical proof bounding the error reduction per iteration and classifying the convergence speed relative to standard PGA.

- **Question:** Can the SSO-PGA formulation be modified to guarantee stability for optimal solutions that lie significantly outside the [0, 1] range?
  - Basis in paper: [explicit] The appendix identifies a limitation where SSO-PGA "exhibits oscillatory, non-convergent behavior when the true solution is large," noting it relies on network normalization (clamping inputs to [0, 1]) to avoid failure.
  - Why unresolved: The multiplicative update is bounded by the sigmoid function, causing numerical issues when the optimization landscape requires large steps or values outside the normalized range.
  - What evidence would resolve it: A theoretical analysis or modified Sigmoid operator that prevents oscillation for large-valued solutions without requiring heuristic gradient clipping.

- **Question:** How does the SSO-based multiplicative update perform when generalized to inverse problems with non-convex priors?
  - Basis in paper: [inferred] While the paper derives the algorithm for convex $f(x)$ (Problem I/II) and tests it on image restoration, it mentions applying SSO to "non-convex problems" in the appendix but does not theoretically analyze convergence guarantees for non-convex $f$.
  - Why unresolved: Theorem 2 relies on the convexity of the energy function; applying this to complex, non-convex deep priors in broader tasks may violate the sufficient conditions for convergence provided.
  - What evidence would resolve it: Convergence analysis for non-convex objective functions or empirical testing on tasks with highly non-convex loss landscapes.

## Limitations

- **Oscillation with large solutions**: SSO-PGA exhibits oscillatory, non-convergent behavior when the true solution is large, requiring input normalization to avoid failure.
- **Theoretical gaps**: While convergence is proven, the sufficient conditions are conservative and may not reflect practical performance; theoretical convergence rate remains unanalyzed.
- **Architectural opacity**: MSFB module and Prox_φ network architectures are vaguely specified, making faithful reproduction challenging without access to the original codebase.

## Confidence

- **High Confidence**: The core mechanism of SSO-PGA (replacing gradient descent with multiplicative updates via SSO) is mathematically sound and well-validated through numerical experiments. The convergence guarantees in Theorem 1 and empirical improvements in PSNR/SAM metrics are robust.

- **Medium Confidence**: The architectural components (MSFB, Prox_φ) and their integration into the deep unfolding framework are reasonable given the ablation studies, but the exact implementations are underspecified. The superiority over PGA and state-of-the-art methods is demonstrated but relies on architectural choices not fully detailed.

- **Low Confidence**: The theoretical analysis (Theorem 2) provides conservative bounds that may not reflect practical performance. The handling of large-solution cases and the full implications of the oscillatory behavior observed in Appendix A.2 are not adequately addressed in the main claims.

## Next Checks

1. **Numerical Convergence Validation**: Reproduce the Problem I and II experiments from Appendix A.1 with initial values {1, 4, 8, 16} to verify SSO-PGA converges faster than PGA while checking for oscillation in large-solution cases.

2. **Ablation on Learnable Hyperparameters**: Implement the ablation study from Table 11 on WV3 data by comparing fixed α values (0.1, 0.5, 1.0, 3.0) versus learned α parameters.

3. **Convergence Curve Analysis**: Train SSO-PGA and PGA baseline for 300 epochs on WV3, plotting L1 loss curves similar to Figure 1b. Verify that PGA shows instability while SSO-PGA converges smoothly, and measure the exact iteration where SSO-PGA first exceeds PGA performance.