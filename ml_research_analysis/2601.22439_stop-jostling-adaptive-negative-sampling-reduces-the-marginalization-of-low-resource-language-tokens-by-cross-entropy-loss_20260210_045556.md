---
ver: rpa2
title: 'Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource
  Language Tokens by Cross-Entropy Loss'
arxiv_id: '2601.22439'
source_url: https://arxiv.org/abs/2601.22439
tags:
- margin
- proposed
- language
- tokens
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving the representation
  and performance of rare tokens and tokens from low-resource languages in neural
  language models. The core issue identified is that rare tokens are disproportionately
  affected by marginalization during training, where non-target embeddings are pushed
  away from irrelevant contexts, degrading their learned representations.
---

# Stop Jostling: Adaptive Negative Sampling Reduces the Marginalization of Low-Resource Language Tokens by Cross-Entropy Loss

## Quick Facts
- arXiv ID: 2601.22439
- Source URL: https://arxiv.org/abs/2601.22439
- Authors: Galim Turumtaev
- Reference count: 29
- One-line primary result: Logit thresholding technique significantly improves low-resource language performance by reducing marginalization gradients.

## Executive Summary
This paper addresses the challenge of improving the representation and performance of rare tokens and tokens from low-resource languages in neural language models. The core issue identified is that rare tokens are disproportionately affected by marginalization during training, where non-target embeddings are pushed away from irrelevant contexts, degrading their learned representations. To mitigate this, the paper proposes a thresholding technique that reduces the impact of marginalization by ignoring non-relevant tokens during the training process. Experiments with a character-level multilingual language model trained on simulated mixed low-resource and high-resource language data demonstrate that the proposed method significantly improves performance on low-resource language validation data.

## Method Summary
The method involves applying a threshold to the logits before calculating the cross-entropy loss, effectively preventing marginalization gradients for tokens with probabilities below the threshold. This is achieved by setting logits below a computed threshold (relative to the target token's logit) to negative infinity, making their post-softmax probabilities zero and zeroing out their marginalization gradients. Additionally, the "Separated Embeddings" technique is introduced to fully isolate rare tokens from unwanted updates when using the AdamW optimizer, as shared momentum can still update embeddings even when their calculated gradients are zeroed. The approach is validated on a character-level multilingual language model trained on a simulated dataset with 2% low-resource language data mixed with 98% high-resource data.

## Key Results
- Language modeling accuracy for the low-resource language increased from 0.31 with the baseline to 0.49 with the proposed method, approaching the high-resource language accuracy of 0.53.
- PPLbest for the low-resource language was reduced from 10.63 to 6.11, nearly reaching the PPLbest for the high-resource language of 4.97.
- The method not only improves performance metrics but also helps the model learn better representations, as evidenced by the alignment of "translations" of the same characters across different languages.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rare tokens are disproportionately harmed by non-target marginalization, a gradient effect from standard cross-entropy loss.
- Mechanism: In cross-entropy loss, the gradient for a token's embedding is a sum of three optimization types: 1) context alignment (pushing input embeddings toward the target), 2) target alignment (pushing the target embedding toward the model's hidden state), and 3) non-target marginalization (pushing *all other* embeddings away from the hidden state). For rare tokens, this third type accumulates noise from many irrelevant contexts, degrading their representation.
- Core assumption: The sum of "push-away" gradients from irrelevant contexts is a significant source of noise for rare tokens, outweighing beneficial alignment signals.
- Evidence anchors:
  - [abstract] "...rare tokens are disproportionately affected by marginalization, which prevents them from learning effectively."
  - [section 2.2] "For all $v_i \in V$, where $v_i \neq x_t$, $w_i$ is optimized to minimize $\langle h_t, w_i \rangle$. ... we refer to as marginalization."
  - [section 4.2] "It is clear that the problem of marginalization exists for low-resource language data: the 14 least frequent tokens have Ratio of Average below 1..."
- Break condition: If rare token representations improve with standard cross-entropy loss as dataset size and token frequency increase, then the primary issue is data scarcity, not marginalization noise.

### Mechanism 2
- Claim: Logit thresholding prevents non-target marginalization for tokens with probabilities far below the target token.
- Mechanism: Before calculating loss, a threshold is set relative to the target token's logit: `threshold = logits[target] - margin`. Any token with a logit below this value has its logit set to $-\infty$. This makes its post-softmax probability effectively zero, which zeroes out the marginalization gradient for that token and prevents its embedding from being updated by the current context.
- Core assumption: Tokens with very low probabilities relative to the target are irrelevant to the context and should not have their embeddings influenced.
- Evidence anchors:
  - [abstract] "We propose a thresholding technique that reduces the impact of this marginalization..."
  - [section 3.1] "By applying this thresholding, the probabilities $P_\theta(v_i) < P_\theta(x_t) \times e^{-margin}$ are effectively set to 0. This makes the marginalization gradients zero..."
  - [corpus] Corpus evidence for this specific mechanism is weak; related work addresses data scarcity or tokenization premiums, not gradient masking for rare tokens.
- Break condition: If a semantically relevant token (e.g., a synonym) consistently receives a low probability, thresholding could prevent it from receiving beneficial alignment updates.

### Mechanism 3
- Claim: The "Separated Embeddings" (SE) technique is required to fully isolate rare tokens from unwanted updates when using the AdamW optimizer.
- Mechanism: The standard AdamW optimizer shares momentum across all rows of the embedding matrix. Even if a token's calculated gradient is zeroed by thresholding, its embedding can still be updated if *any other token* has a non-zero gradient, due to shared momentum terms. SE stores each embedding as an independent parameter, allowing the optimizer to completely skip updates for tokens with zero gradients.
- Core assumption: Shared optimizer momentum is a non-negligible source of noise for rare token embeddings when using gradient masking.
- Evidence anchors:
  - [section 4.5] "Even after applying thresholding, the gradients applied are never exactly zero: AdamW treats embeddings as rows of a single matrix... This approach effectively isolates the updates for each token..."
  - [section 4.5] "Experiments show that using separated embeddings significantly improves model quality..."
  - [corpus] No direct corpus evidence; this is an implementation detail related to optimizer behavior.
- Break condition: If a different optimizer without shared momentum is used, the SE technique would not be necessary for thresholding to be effective.

## Foundational Learning

- Concept: **Cross-Entropy Loss and its Gradients**.
  - Why needed here: This is the core loss function being modified. You must understand how it pushes non-target embeddings away (non-target marginalization) to see why this is problematic for rare tokens.
  - Quick check question: In a model with weight tying, what is the gradient contribution to the embedding of a token that is *not* the current target?

- Concept: **Logits and the Softmax Function**.
  - Why needed here: The proposed method intervenes on the logits. Understanding the direct link between a logit's value and its resulting probability is critical for understanding how setting a logit to $-\infty$ prevents gradient flow.
  - Quick check question: If a logit is set to negative infinity before the softmax, what is the resulting probability for that token, and what is its gradient?

- Concept: **The Adam Optimizer and Momentum**.
  - Why needed here: The paper identifies a subtle interaction where optimizer momentum can undermine gradient masking. A grasp of how Adam uses past gradients is required to understand the motivation for the "Separated Embeddings" design.
  - Quick check question: How could an optimizer's momentum term cause a parameter to be updated even if its gradient for the current batch is zero?

## Architecture Onboarding

- Component map: Model computes hidden state and logits -> Logit Thresholding: For each position, identify the target logit, compute a `threshold`, and mask all logits below it to $-\infty$ -> Loss Calculation: Compute standard cross-entropy loss on the modified logits -> Backward Pass: Compute gradients -> Parameter Update: The `SeparatedEmbedding` module ensures that only embeddings with non-zero calculated gradients are touched by the optimizer.

- Critical path: 1. **Forward Pass:** Model computes hidden state and logits. 2. **Logit Thresholding:** For each position, identify the target logit, compute a `threshold`, and mask all logits below it to $-\infty$. 3. **Loss Calculation:** Compute standard cross-entropy loss on the modified logits. 4. **Backward Pass:** Compute gradients. 5. **Parameter Update:** The `SeparatedEmbedding` module ensures that only embeddings with non-zero calculated gradients are touched by the optimizer.

- Design tradeoffs: The main tradeoff is the `margin` hyperparameter. A low `margin` aggressively protects rare tokens but creates an "unreliable tail" of tokens with non-zero probabilities, potentially increasing perplexity. A high `margin` reduces the protective effect. There is also a tradeoff with SE, which adds implementation complexity and memory overhead by fragmenting the embedding matrix.

- Failure signatures:
  1. **Exploding Perplexity:** A `margin` that is too low (e.g., 0) will cause PPL to skyrocket, as the target token's probability is overwhelmed by the non-thresholded tail.
  2. **No Effect:** A `margin` that is too high (e.g., >14 for a 100k vocab) will result in no tokens being thresholded, yielding identical performance to the baseline.
  3. **Subtle Noise:** Using thresholding without the SE modification may show minimal improvement, as shared optimizer momentum still applies small, noisy updates to rare embeddings.

- First 3 experiments:
  1. **Baseline Measurement:** Train a standard model on a mixed dataset (e.g., 98% high-resource, 2% low-resource). Measure the ratio of gradient norms for marginalization vs. alignment on rare tokens to confirm the problem exists (Section 4.2).
  2. **Hyperparameter Search:** Run a sweep over the `margin` hyperparameter (e.g., values 0.5 to 8) on the same setup. Plot PPL, PPL_best (with temperature scaling), and accuracy for the low-resource language to find the effective range.
  3. **Component Ablation:** Compare the best thresholding model with and without the `SeparatedEmbedding` layer (Section 4.5) to quantify the impact of optimizer isolation on final performance metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the thresholding technique remain effective when applied to large-scale language models and industrial-size datasets?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "Experiments with larger models could potentially alter the results" and confirm they only used a small GPT-2 model.
- **Why unresolved:** The experimental validation is restricted to a small model (800k parameters) and a toy dataset (Shakespeare), leaving the scalability of the logit thresholding approach unproven.
- **What evidence would resolve it:** Empirical results from training models with parameters in the billions (e.g., Llama-scale) on extensive multilingual corpora like Common Crawl.

### Open Question 2
- **Question:** Can the method generalize to natural languages that lack the artificial 1:1 character correspondence used in the paper's simulation?
- **Basis in paper:** [explicit] The Limitations section notes that the simulated data relied on a 1:1 correspondence between characters, which "does not exist in natural multilingual data."
- **Why unresolved:** The "low-resource" language in the study was simulated by adding an offset to character IDs, ensuring perfect structural alignment with the high-resource language, a condition rarely met in reality.
- **What evidence would resolve it:** Experiments utilizing real low-resource languages (e.g., distinct scripts or morphologies) mixed with high-resource languages without artificial alignment.

### Open Question 3
- **Question:** Does the observed reduction in marginalization and improvement in validation metrics translate to better performance on downstream NLP tasks?
- **Basis in paper:** [explicit] The authors explicitly state that improved validation metrics "does not necessarily guarantee improved performance in downstream tasks" in the Limitations section.
- **Why unresolved:** The paper evaluates success using intrinsic metrics like perplexity and accuracy, but does not measure the utility of the resulting embeddings or model on specific applications.
- **What evidence would resolve it:** Benchmarking the thresholded models on downstream tasks such as machine translation, text classification, or named entity recognition for low-resource languages.

## Limitations

- The primary limitation is the focus on a synthetic low-resource setting, which may not generalize to truly low-resource natural languages with different sparsity patterns and morphological complexity.
- The method's effectiveness on large-scale models and industrial-size datasets is unproven, as the experiments were limited to a small GPT-2 model.
- The approach does not address how it interacts with subword tokenization, which is standard in modern language models.
- The claim that "all modern language models" use cross-entropy loss is an overstatement, as some models use different objectives.
- The hyperparameter `margin` requires careful tuning and its optimal value is context-dependent, potentially limiting practical applicability.

## Confidence

- **High Confidence:** The core mechanism of logit thresholding to prevent non-target marginalization for rare tokens is well-supported by the experimental results on the synthetic dataset.
- **Medium Confidence:** The claim that the "Separated Embeddings" technique is necessary for optimal performance when using AdamW is supported by the ablation study but is specific to that optimizer.
- **Medium Confidence:** The broader claim that this approach will significantly improve low-resource language modeling in general is extrapolated from the synthetic experiment and requires validation on real, diverse low-resource language datasets.
- **Low Confidence:** The assertion that this is a fundamental problem affecting "all modern language models" is not substantiated beyond the specific experimental setup.

## Next Checks

1. **Real-World Low-Resource Validation:** Replicate the main experiment on a real low-resource language dataset (e.g., a language from the OSCAR corpus with limited training data). Compare the performance of the baseline model, the thresholding model, and the thresholding + SE model on held-out test data.

2. **Cross-Entropy Bias Analysis:** Conduct a controlled experiment to measure the impact of thresholding on the distribution of gradients during training. Specifically, measure the average norm of the "non-target marginalization" gradient component for rare vs. frequent tokens with and without thresholding.

3. **Integration with Subword Tokenization:** Implement the thresholding technique in a standard transformer model (e.g., GPT-2 or BERT) that uses BPE or WordPiece tokenization. Train the model on a multilingual corpus and evaluate its performance on low-resource languages.