---
ver: rpa2
title: 'A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs:
  Experiments with OpenAI Models'
arxiv_id: '2504.04083'
source_url: https://arxiv.org/abs/2504.04083
tags:
- relations
- relation
- datasets
- entity
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a benchmark for evaluating zero-shot biomedical
  relation extraction using large language models. The authors test GPT-4, OpenAI
  o1, and GPT-OSS-120B on seven diverse biomedical datasets, finding that while LLMs
  perform well on simpler datasets with fewer relation types, they struggle with complex
  inputs containing multiple relations and predicates.
---

# A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models

## Quick Facts
- arXiv ID: 2504.04083
- Source URL: https://arxiv.org/abs/2504.04083
- Reference count: 16
- LLMs struggle with complex biomedical RE tasks despite strong performance on simpler datasets

## Executive Summary
This paper introduces a benchmark for evaluating zero-shot biomedical relation extraction using large language models (LLMs). The authors test GPT-4, OpenAI o1, and GPT-OSS-120B on seven diverse biomedical datasets. Results show that while LLMs perform well on simpler datasets with fewer relation types, they struggle with complex inputs containing multiple relations and predicates. The study reveals that models tend to under-predict relations in information-dense instances and often produce partial entity matches. GPT-OSS-120B achieves the best overall performance, outperforming both GPT-4 and o1 across most datasets.

## Method Summary
The benchmark uses zero-shot prompting with structured JSON output templates to constrain LLM generations to parseable relation formats. Seven biomedical RE datasets (ADE, DCE, ChemProt, DDI, CDR, GDA, BioRED) were evaluated using three OpenAI models: GPT-4, OpenAI o1, and GPT-OSS-120B. The approach relies on prompt engineering with JSON templates and OpenAI's structured output modes (explicit vs. inferred schema) rather than fine-tuning. Performance is measured using precision, recall, and F1 scores, with predicted relations matching gold if entity mentions, entity types, and relation types are correct.

## Key Results
- GPT-OSS-120B achieves the best overall performance with average F1 scores around 49.0
- GPT-4 lags behind other models, particularly on datasets with 4-8 distinct relation types
- All models show significant performance degradation as the number of relations per instance increases
- Under-prediction of relations is a key issue, with models extracting fewer relations than present in high-density instances

## Why This Works (Mechanism)

### Mechanism 1
Structured JSON output templates constrain LLM generations to parseable relation formats, reducing format inconsistency errors. The prompt includes a complete task description plus an explicit JSON schema with field definitions and a filled example. OpenAI's API modes (explicit schema or inferred schema) further enforce structure. This bypasses the need for post-hoc regex parsing of natural language outputs. Core assumption: The model's instruction-following training transfers reliably to biomedical RE without task-specific fine-tuning.

### Mechanism 2
Models with chain-of-thought reasoning (o1, GPT-OSS-120B) outperform standard instruction-tuned models (GPT-4) on datasets with many relation types. o1 and GPT-OSS-120B use test-time reasoning to "recognize and correct mistakes" and decompose complex steps. For datasets like ChemProt (5 predicates) and BioRED (8 predicates), this provides capacity to distinguish semantically close relation types. Core assumption: The reasoning capability generalizes to biomedical domain distinctions without domain-specific training.

### Mechanism 3
Generative models systematically under-predict relations as instance relation density increases, causing recall degradation. Autoregressive generation faces compounding error and length-related quality degradation. As gold relations per instance increase, models predict fewer than present, leaving relations unextracted. Evidence: All models tend to under-predict relations when an instance contains more than a few gold relations.

## Foundational Learning

- **Concept: End-to-End RE vs. Relation Classification**
  - Why needed here: The paper evaluates end-to-end RE where models must both identify entities AND extract relations, unlike RC which assumes pre-annotated entity spans.
  - Quick check question: Given text "Aspirin reduces fever," does your system receive (a) just the text, or (b) the text plus marked spans for "Aspirin" and "fever"?

- **Concept: Entity-Level vs. Mention-Level Relations**
  - Why needed here: EL relations link normalized concept IDs; ML relations link exact text spans. Evaluation differs—EL allows matching any gold mention for a concept; ML requires exact span matching.
  - Quick check question: If gold annotation has "hypertension" (concept ID: C0020538) and your model extracts "hypertensive," is this a match under EL or ML evaluation?

- **Concept: Zero-Shot Prompting with Structured Output**
  - Why needed here: The approach relies on prompt engineering with JSON templates and OpenAI's structured output modes (explicit vs. inferred schema) rather than fine-tuning.
  - Quick check question: What is the difference between providing a JSON schema explicitly to the API versus letting the model infer structure from the prompt?

## Architecture Onboarding

- **Component map:** Input Text → Prompt Template (task description + JSON schema + example) → LLM API (GPT-4/o1/GPT-OSS with explicit/inferred mode) → JSON Output → Entity Normalization (for EL datasets, map mentions to concept IDs) → Evaluation (match predicted relations to gold, compute P/R/F1)

- **Critical path:** Prompt design → Schema enforcement → Entity mention extraction → Relation typing. Errors in entity boundary detection (partial matches like "methamphetamine-induced psychosis" vs. "psychosis") propagate to relation correctness.

- **Design tradeoffs:**
  - Explicit vs. Inferred schema: Explicit provides stricter control; inferred may adapt better to edge cases.
  - Single-pass vs. iterative extraction: Single-pass fails on dense instances (>10 relations); iterative increases latency and cost.
  - Precision vs. recall optimization: Under-prediction suggests recall is the bottleneck; prompt adjustments (e.g., "extract ALL relations") may help but risk false positives.

- **Failure signatures:**
  - Partial entity mentions (extra/missing words): "methamphetamine-induced psychosis" extracted instead of "psychosis"
  - Under-prediction on high-density instances: Models predict ~5 relations when 15+ exist
  - Relation type confusion on semantically close predicates: ChemProt CPR:3 (upregulator) vs. CPR:4 (downregulator)
  - Hallucinated entity names not in source text

- **First 3 experiments:**
  1. Baseline schema comparison: Run GPT-4 with explicit vs. inferred JSON schema on 2 simple (ADE) and 2 complex (ChemProt, BioRED) datasets. Measure format compliance rate and F1.
  2. Relation density stress test: Sample instances by gold relation count (1-5, 6-10, 11-20, 20+). Plot predicted vs. gold relation counts and recall curves. Identify density threshold where performance collapses.
  3. Entity boundary error analysis: On CDR and GDA (simpler, single-relation-type), manually categorize errors: (a) exact match, (b) partial match (headword correct), (c) partial match (headword wrong), (d) hallucinated. Quantify error budget allocation.

## Open Questions the Paper Calls Out

- Do performance correlations between dataset complexity and F1 scores hold when analyzing specific entity and relation types individually? The authors state future work should test the hypothesis that observations may reflect spurious correlations by slicing datasets.

- Can LLMs be guided to extract correct entity mention boundaries in a zero-shot setting to reduce partial matching errors? The paper notes that partial matching was a major source of error and suggests future research focus on extracting correct boundaries.

- Would re-annotating benchmark datasets to correct for missed relations (false negatives) significantly improve the apparent performance of zero-shot LLMs? The paper notes that false positives often appear to be correct relations missed by annotators, suggesting re-annotation may be prudent.

## Limitations

- Dataset accessibility barrier: The seven benchmark datasets are encrypted in the repository, creating a practical barrier to immediate reproduction and affecting reproducibility of results.
- Under-prediction mechanism not empirically validated: The attribution of under-prediction to autoregressive generation's length-related quality degradation is plausible but not tested against alternative strategies like iterative extraction.
- Limited relation type confusion analysis: The error analysis focuses on entity mention extraction and under-prediction but does not provide detailed breakdowns of relation type confusion for semantically fine-grained distinctions.

## Confidence

- **High**: Structured JSON output templates effectively constrain model generations to parseable formats, reducing format inconsistency errors. Supported by direct evidence of near-perfect schema adherence in GPT-4 outputs.
- **Medium**: Chain-of-thought reasoning models (o1, GPT-OSS-120B) outperform standard models on datasets with many relation types. Supported by observed F1 improvements, but specific contribution of CoT versus other factors not isolated.
- **Low**: Generative models systematically under-predict relations as instance relation density increases. The mechanism is plausible but not empirically validated; no alternative strategies are tested.

## Next Checks

1. Run GPT-4 with explicit vs. inferred JSON schema modes on two simple (ADE) and two complex (ChemProt, BioRED) datasets. Measure format compliance and F1 scores to compare schema approaches.

2. Sample instances by gold relation count (1-5, 6-10, 11-20, 20+) and plot predicted vs. gold relation counts and recall curves. Identify the density threshold where performance collapses to quantify under-prediction.

3. On CDR and GDA datasets, manually categorize entity extraction errors: (a) exact match, (b) partial match (headword correct), (c) partial match (headword wrong), (d) hallucinated. Quantify error budget allocation to determine if post-hoc span alignment would improve results.