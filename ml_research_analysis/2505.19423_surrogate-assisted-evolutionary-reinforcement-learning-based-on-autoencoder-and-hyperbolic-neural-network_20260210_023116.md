---
ver: rpa2
title: Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder
  and Hyperbolic Neural Network
arxiv_id: '2505.19423'
source_url: https://arxiv.org/abs/2505.19423
tags:
- policy
- space
- learning
- policies
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the high computational cost and low search
  efficiency of evolutionary reinforcement learning (ERL) when using deep neural network
  policies. Existing surrogate-assisted approaches struggle due to the high dimensionality
  of policy representations.
---

# Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network

## Quick Facts
- arXiv ID: 2505.19423
- Source URL: https://arxiv.org/abs/2505.19423
- Reference count: 17
- The paper proposes a novel surrogate-assisted ERL method using AE for dimensionality reduction and HNN for classification-based policy selection, achieving up to 100% performance relative to baselines and 38% runtime reduction on Atari and Mujoco benchmarks.

## Executive Summary
This paper addresses the computational bottleneck in evolutionary reinforcement learning when using deep neural network policies. Traditional ERL requires expensive fitness evaluations for each candidate policy, making the search process slow and inefficient. The authors propose a novel approach that combines Autoencoders (AE) for compressing high-dimensional policy representations into low-dimensional embeddings, and Hyperbolic Neural Networks (HNN) as a surrogate model for pre-selecting promising policies without costly real evaluations. This dual-component system significantly reduces the number of expensive environment interactions while maintaining or improving learning performance across diverse benchmark tasks.

## Method Summary
The proposed method tackles ERL's high computational cost by introducing a two-stage surrogate system. First, an Autoencoder learns to compress high-dimensional neural network policies into low-dimensional embeddings that preserve essential policy characteristics. Second, a Hyperbolic Neural Network serves as a classification-based surrogate that predicts policy performance categories without requiring full environment rollouts. During the evolutionary search, the HNN rapidly screens candidate policies, allowing only the most promising ones to undergo expensive real evaluations. This approach addresses the fundamental challenge that traditional surrogate methods struggle with in high-dimensional policy spaces. The hyperbolic geometry of the HNN is particularly suited for capturing hierarchical relationships in policy structures, making it effective for both discrete action spaces (Atari) and continuous control tasks (Mujoco).

## Key Results
- Achieved up to 100% performance relative to best baselines on some tasks, particularly hierarchical environments like Alien
- Demonstrated 38% reduction in wall-clock time despite computational overhead from training AE and HNN components
- Outperformed previous surrogate-assisted ERL approaches across 10 Atari and 4 Mujoco benchmark tasks
- Ablation studies confirmed both AE compression and HNN classification are essential for performance gains

## Why This Works (Mechanism)
The method works by addressing the fundamental tension in ERL between exploration breadth and computational efficiency. High-dimensional policies create an enormous search space that requires many expensive evaluations. The Autoencoder learns a compressed representation that captures the most salient features of policies while discarding redundancy, effectively reducing the search space dimensionality. The Hyperbolic Neural Network then leverages this compressed space to build a fast, accurate surrogate model that can classify policies into performance tiers. Hyperbolic geometry is particularly effective because it naturally represents hierarchical structures and tree-like relationships that emerge in policy representations. This combination allows the system to explore more policies with fewer real evaluations, achieving better performance faster than traditional approaches.

## Foundational Learning

**Evolutionary Reinforcement Learning (ERL)** - Combines evolutionary algorithms with reinforcement learning for policy optimization. Needed because it provides a population-based search mechanism that can escape local optima. Quick check: Verify ERL maintains population diversity throughout training.

**Autoencoder (AE)** - Neural network that learns compressed representations by reconstructing inputs. Needed to reduce high-dimensional policy space dimensionality while preserving key features. Quick check: Confirm AE reconstruction error remains below threshold across policy variations.

**Hyperbolic Neural Networks (HNN)** - Neural networks operating in hyperbolic rather than Euclidean space. Needed because hyperbolic geometry better captures hierarchical and tree-like structures common in policy representations. Quick check: Validate HNN outperforms Euclidean counterpart on hierarchical policy classification tasks.

**Policy Surrogates** - Models that approximate policy performance without full environment evaluation. Needed to reduce computational cost of fitness evaluations in ERL. Quick check: Measure surrogate prediction accuracy against actual policy returns.

**Dimensionality Reduction** - Techniques for transforming high-dimensional data into lower-dimensional representations. Needed because high-dimensional policy spaces make evolutionary search computationally prohibitive. Quick check: Ensure reduced dimensions capture â‰¥90% of variance in policy performance.

## Architecture Onboarding

**Component Map:** Raw Policies -> AE Encoder -> Low-Dimensional Embeddings -> HNN Surrogate -> Policy Selection -> Environment Evaluation -> Performance Feedback -> AE/HNN Retraining

**Critical Path:** The most time-critical path runs from policy generation through AE encoding, HNN classification, and environment evaluation. The AE and HNN training occurs periodically rather than on every iteration, making them secondary paths. Real-time performance depends primarily on AE inference speed and HNN classification accuracy.

**Design Tradeoffs:** The main tradeoff involves balancing AE compression quality against computational overhead. Higher compression ratios reduce search space but may lose important policy features. The HNN's hyperbolic geometry provides better hierarchical modeling but may be more complex to train than Euclidean alternatives. The periodic retraining of both components adds overhead but is necessary for maintaining surrogate accuracy.

**Failure Signatures:** Performance degradation typically manifests as either AE collapse (loss of policy information during compression) or HNN misclassification (poor surrogate accuracy). AE failure shows up as reduced diversity in the embedding space and poor reconstruction quality. HNN failure appears as random policy selection patterns and poor correlation between surrogate predictions and actual performance.

**First Experiments:**
1. Test AE reconstruction quality on a held-out set of policies to verify information preservation during compression
2. Evaluate HNN classification accuracy on a validation set of policies with known performance categories
3. Measure end-to-end runtime breakdown to identify bottlenecks between AE inference, HNN classification, and environment evaluation

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Computational overhead from training both AE and HNN components creates significant trade-offs between reduced evaluations and increased surrogate training costs
- Performance gains, while substantial, are not uniformly distributed across all tasks - some environments show more benefit than others
- The method's effectiveness on Atari and Mujoco tasks doesn't guarantee generalization to other reinforcement learning domains with different characteristics
- Limited modularity means the entire approach may need redesign if either AE or HNN components fail in new domains

## Confidence
- Overall performance claims: **High** - supported by multiple benchmark tasks and consistent improvement patterns
- Runtime reduction claims: **Medium** - reduction achieved but with significant computational overhead trade-offs
- Hyperbolic geometry advantages: **Medium** - benefits demonstrated on specific tasks like Alien but not universally explained

## Next Checks
1. Conduct ablation studies isolating AE compression quality from HNN classification accuracy to quantify their individual contributions to performance gains
2. Test the method on additional benchmark suites beyond Atari and Mujoco, particularly tasks with different state-action dimensionality characteristics
3. Perform scaling analysis to determine how performance and runtime benefits change with increasing policy network sizes beyond those tested in the current study