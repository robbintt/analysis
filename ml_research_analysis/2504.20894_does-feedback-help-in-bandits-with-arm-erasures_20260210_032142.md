---
ver: rpa2
title: Does Feedback Help in Bandits with Arm Erasures?
arxiv_id: '2504.20894'
source_url: https://arxiv.org/abs/2504.20894
tags:
- erasure
- learner
- feedback
- regret
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the multi-armed bandit (MAB) problem with arm
  erasure channels, where a learner communicates arm choices to an agent through a
  channel that may erase transmissions. The key question is whether erasure feedback
  (knowing when transmissions are erased) improves regret performance compared to
  no-feedback settings.
---

# Does Feedback Help in Bandits with Arm Erasures?

## Quick Facts
- arXiv ID: 2504.20894
- Source URL: https://arxiv.org/abs/2504.20894
- Authors: Merve Karakas; Osama Hanna; Lin F. Yang; Christina Fragouli
- Reference count: 40
- Primary result: Erasure feedback does not improve worst-case regret order in single-agent M bandits; Stop-on-Success algorithm achieves Õ(√KT + K/(1-ϵ)) regret

## Executive Summary
This paper studies multi-armed bandits with arm erasure channels, where arm choices must be transmitted through channels that may erase transmissions. The authors prove that erasure feedback (knowing when transmissions are erased) does not improve the worst-case regret order compared to no-feedback settings, establishing a lower bound of Ω(√KT + K/(1-ϵ)). They introduce a Stop-on-Success algorithm that leverages feedback to stop repetitions after the first successful transmission, achieving near-optimal regret bounds when applied to Successive Arm Elimination. Empirical results show constant-factor improvements in regret, particularly as erasure probability increases.

## Method Summary
The authors analyze the MAB problem where a learner communicates arm choices to an agent through an erasure channel. They prove a regret lower bound showing erasure feedback doesn't improve worst-case regret order. They propose Stop-on-Success (SoS), which transmits each arm repeatedly until receiving feedback that the transmission succeeded (erasure feedback indicates et=0). SoS is applied to the Successive Arm Elimination algorithm, creating SAE_SoS that partitions time into exponentially growing batches where active arms are transmitted until success, then rewards are collected. The approach doesn't require knowledge of the erasure probability.

## Key Results
- Erasure feedback does not improve worst-case regret order: regret lower bound Ω(√KT + K/(1-ϵ)) matches no-feedback bounds
- Stop-on-Success algorithm achieves Õ(√KT + K/(1-ϵ)) regret when applied to Successive Arm Elimination
- Empirical evaluation shows constant-factor regret improvements, particularly as erasure probability increases
- Feedback enables simpler algorithm designs that adapt to erasure events without requiring knowledge of erasure probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Erasure feedback does not improve worst-case regret order compared to no-feedback settings.
- Mechanism: In worst-case channel realizations, the erasure sequence can concentrate in early rounds such that insufficient non-erased transmissions prevent the learner from distinguishing the best arm—even when the learner knows exactly which transmissions failed. The proof constructs K bandit instances where, with constant probability >1/2, at least K/4 arms remain essentially untested in the first K/(4(1-ϵ)) rounds.
- Core assumption: The channel can produce worst-case erasure sequences; the learner must guarantee performance across all possible realizations, not just typical ones.
- Evidence anchors:
  - [abstract]: "Surprisingly, we prove that erasure feedback does not improve the worst-case regret upper bound order over the previously studied no-feedback setting. In particular, we prove a regret lower bound of Ω(√KT + K/(1-ϵ))."
  - [Section III.A, Theorem 1]: Proof constructs K distinct instances and shows that even with erasure knowledge, channel realization can make the best arm indistinguishable.
  - [corpus]: Limited corpus support for this specific negative result on feedback utility.
- Break condition: Lower bound requires ϵ ≥ 0.5; for ϵ < 0.5, the √KT term dominates and matches standard MAB bounds (Remark 1).

### Mechanism 2
- Claim: Stop-on-Success (SoS) reduces transmission overhead by adapting to actual channel conditions rather than using fixed repetition schedules.
- Mechanism: Instead of blindly repeating each arm α = Ω(log T/log(1/ϵ)) times to guarantee success with high probability, SoS transmits until the first erasure feedback indicates success (et=0), then collects rewards. The number of attempts until success follows a geometric distribution with mean 1/(1-ϵ), which is smaller than fixed α in expectation.
- Core assumption: Erasure feedback is available immediately and reliably; the feedback channel has no loss.
- Evidence anchors:
  - [abstract]: "the availability of feedback enables simpler algorithm designs that may achieve better constants (albeit not better order) regret bounds"
  - [Section III.B]: "the learner can stop transmitting as soon as a success is observed, without needing to know the erasure probability ϵ in advance"
  - [corpus]: "Quantile Multi-Armed Bandits with 1-bit Feedback" supports effectiveness of minimal feedback mechanisms.
- Break condition: If feedback is delayed or unreliable, SoS cannot adapt and must fall back to fixed repetition strategies.

### Mechanism 3
- Claim: SoS applied to Successive Arm Elimination achieves near-optimal regret while eliminating the need to estimate erasure probability.
- Mechanism: SAE partitions time into exponentially growing batches. SoS ensures each active arm in batch i is transmitted until success, then collects mi = 4i rewards. With probability ≥ 1–1/T, transmit phase completes within α = ⌈2 log T/log(1/ϵ)⌉ rounds per arm. Confidence intervals shrink as samples accumulate, enabling correct elimination.
- Core assumption: Bounded rewards in [0,1]; i.i.d. Bernoulli erasures; agent always pulls last successfully received arm (known strategy).
- Evidence anchors:
  - [Section III.D, Theorem 2]: Formal proof that Algorithm 1 achieves RT = Õ(√KT + K/(1-ϵ)).
  - [Section IV, Figure 2]: Empirical validation shows constant-factor regret improvement over LSAE, increasing with ϵ.
  - [corpus]: "Optimal Arm Elimination Algorithms for Combinatorial Bandits" supports elimination-based approaches.
- Break condition: If erasure probability varies non-stationarily or adversarially, fixed confidence bounds may not hold.

## Foundational Learning

- Concept: Multi-Armed Bandit Fundamentals (regret, exploration-exploitation, UCB/SAE algorithms)
  - Why needed here: This paper extends classical MAB; understanding why standard lower bounds are Ω(√KT) and how elimination algorithms work is prerequisite.
  - Quick check question: Can you explain how Successive Arm Elimination uses batching and confidence intervals to balance exploration-exploitation?

- Concept: Erasure Channel Statistics (Bernoulli processes, geometric distribution)
  - Why needed here: The core problem involves transmissions failing with probability ϵ; understanding that expected attempts until success is 1/(1-ϵ) is essential for analyzing SoS.
  - Quick check question: If ϵ=0.8, what is the expected number of transmission attempts before one succeeds?

- Concept: Lower Bound Proof Techniques (constructing hard instances, Yao's minimax principle)
  - Why needed here: The paper's main theoretical contribution uses instance-based lower bounds; understanding this proof strategy helps evaluate claims.
  - Quick check question: Why does constructing multiple bandit instances help prove a lower bound that applies to all algorithms?

## Architecture Onboarding

- Component map:
  Learner Module -> SoS Wrapper -> Channel Model -> Agent Logic -> Batch Scheduler
  (Maintains base policy, processes feedback, transmits until success, pulls last received arm, implements exponential batching)

- Critical path:
  1. Initialize candidate arms A = [K], batch index i = 1
  2. Base policy determines active arms and required pulls (mi = 4i)
  3. For each active arm: transmit until et=0, then collect mi consecutive rewards
  4. Compute empirical means; eliminate arms outside confidence interval
  5. Increment batch; repeat until horizon T exhausted

- Design tradeoffs:
  - Adaptive vs. fixed repetition: SoS adapts without knowing ϵ but requires feedback channel; fixed repetition needs no feedback but wastes ~α – 1/(1-ϵ) transmissions per arm
  - Batch size growth: Faster growth reduces batches but delays elimination of suboptimal arms
  - Confidence width: Tighter bounds speed elimination but risk eliminating best arm under noise

- Failure signatures:
  - Linear regret: Occurs if algorithm assumes low ϵ but actual ϵ is high; insufficient exploration before exploitation
  - Premature best-arm elimination: Confidence bounds too tight for actual noise level from erasure-induced uncertainty
  - Budget overrun: Transmit phase exceeds allocated batch time if α underestimated

- First 3 experiments:
  1. Baseline comparison: Implement SAE_SoS vs. LSAE (no-feedback from prior work) with K=50, ϵ∈{0.5, 0.9, 0.95}, T=10^6; verify constant-factor improvement scales with ϵ as shown in Figure 2.
  2. Erasure probability sweep: Run SAE_SoS across ϵ∈{0.3, 0.5, 0.7, 0.9, 0.95}; confirm regret scales as Õ(√KT + K/(1-ϵ)) and identify where K/(1-ϵ) term dominates.
  3. Unknown ϵ robustness: Compare SAE_SoS (doesn't require ϵ) vs. LSAE with misestimated ϵ (±20% error); quantify practical advantage of feedback-based adaptability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the Stop-on-Success (SoS) approach robust under an adversarial erasure model where an adversary selects which transmissions fail?
- Basis in paper: [explicit] Section V states, "Understanding how robust our approach is under adversarial corruptions of arm transmissions is an open question."
- Why unresolved: The current theoretical analysis and lower bounds rely on the assumption that erasure events are i.i.d. Bernoulli random variables, failing to account for potentially malicious or correlated channel failures.
- What evidence would resolve it: A proof of regret bounds for an algorithm utilizing erasure feedback under an adversarial erasure model, or a counter-example showing that linear regret is unavoidable in this setting.

### Open Question 2
- Question: How does erasure feedback affect performance in a system with multiple agents ($M \geq 1$), particularly regarding the coordination of transmissions?
- Basis in paper: [explicit] Section V identifies "exploring setups with multiple agents" as a direction for future work, specifically noting that "coordinating transmissions among agents can introduce new challenges in the analysis."
- Why unresolved: The paper restricts its theoretical results and algorithm design to a single-agent setting, leaving the complexities of multi-agent scheduling and heterogeneous channels unaddressed.
- What evidence would resolve it: Extension of the lower bound analysis and the SoS algorithm to the multi-agent setting, including regret bounds that account for the scheduling overhead.

### Open Question 3
- Question: Does erasure feedback reduce the sample complexity for Best-Arm Identification (BAI) compared to the no-feedback setting?
- Basis in paper: [inferred] Section II notes that "best-arm identification (BAI) can also be considered... however, in this work, we concentrate on the standard regret objective."
- Why unresolved: The paper focuses exclusively on cumulative regret minimization; the impact of erasure feedback on the stopping time and confidence bounds required for pure exploration (BAI) remains unanalyzed.
- What evidence would resolve it: Derivation of sample complexity bounds for BAI with erasure feedback, comparing them to the known lower bounds for the no-feedback case.

## Limitations
- Results apply specifically to single-agent M model; extension to multi-agent or combinatorial settings unclear
- Analysis assumes i.i.d. erasure channels and reliable feedback channels, which may not hold in practical scenarios
- Paper doesn't explore heterogeneous erasure probabilities across arms or time-varying erasure rates

## Confidence
- **High Confidence**: The regret lower bound Ω(√KT + K/(1-ϵ)) and its matching upper bound for SoS - these are formally proven with clear instance constructions.
- **Medium Confidence**: The practical constant-factor improvements shown empirically - while demonstrated, the magnitude could vary with different problem structures or implementations.
- **Medium Confidence**: The claim that SoS doesn't require knowing ϵ - theoretically sound, but practical performance may degrade with severe misestimation.

## Next Checks
1. **Robustness to feedback delay**: Test SoS performance when erasure feedback is delayed by d rounds (d=1, 5, 10) to quantify the impact of feedback reliability assumptions.
2. **Multi-agent extension**: Adapt SoS to the two-agent M model where agents can communicate through erasure channels and compare regret performance to the single-agent case.
3. **Non-stationary erasures**: Evaluate SoS when ϵ changes periodically or adversarially during the horizon to assess algorithm adaptability beyond the i.i.d. assumption.