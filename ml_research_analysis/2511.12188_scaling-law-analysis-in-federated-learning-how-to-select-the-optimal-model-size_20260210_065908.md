---
ver: rpa2
title: 'Scaling Law Analysis in Federated Learning: How to Select the Optimal Model
  Size?'
arxiv_id: '2511.12188'
source_url: https://arxiv.org/abs/2511.12188
tags:
- training
- size
- optimal
- data
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how federated learning affects the optimal model
  size for large-scale training. It derives a PAC-Bayes generalization bound for federated
  SGD and uses it to analytically determine the optimal model size under convex assumptions.
---

# Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?

## Quick Facts
- arXiv ID: 2511.12188
- Source URL: https://arxiv.org/abs/2511.12188
- Authors: Xuanyu Chen; Nan Yang; Shuai Wang; Dong Yuan
- Reference count: 40
- Primary result: Federated learning requires smaller models for larger client counts due to generalization bounds

## Executive Summary
This paper investigates how federated learning environments affect the optimal model size for large-scale training. The authors derive a PAC-Bayes generalization bound for federated SGD under convex assumptions and use it to analytically determine the optimal model size. The key finding is that optimal model size in federated settings follows a negative power law with the number of clients—larger client counts require smaller models for the same compute. Empirical validation on Vision Transformers and ResNets across CIFAR-100 and ImageNet shows strong agreement with theoretical predictions, with R² > 0.88.

## Method Summary
The authors derive a PAC-Bayes generalization bound for federated stochastic gradient descent (FedSGD) under convex loss assumptions. This bound is then used to analytically determine the optimal model size that minimizes the generalization bound while maintaining constant computational cost. The theoretical analysis shows that optimal model size scales negatively with the number of clients. The empirical validation involves training Vision Transformers and ResNets on CIFAR-100 and ImageNet under federated settings with varying numbers of clients, measuring test accuracy and comparing against the predicted scaling law.

## Key Results
- Federated learning requires smaller models when scaling to more clients
- Optimal model size follows a negative power law with client count
- Empirical validation shows strong agreement (R² > 0.88) between theory and practice

## Why This Works (Mechanism)
The paper establishes that federated learning's generalization behavior differs fundamentally from centralized training due to the aggregation of updates from multiple clients. The PAC-Bayes framework provides a theoretical foundation for understanding how model capacity interacts with client heterogeneity. The mechanism operates through the interplay between model complexity, client diversity, and the averaging process in federated optimization.

## Foundational Learning

**PAC-Bayes bounds** - Why needed: Provides generalization guarantees for learning algorithms with data-dependent priors. Quick check: Can be verified by checking the KL divergence between posterior and prior distributions.

**Federated SGD convergence** - Why needed: Forms the basis for understanding how local updates aggregate in distributed settings. Quick check: Verify through tracking parameter variance across client updates.

**Model scaling laws** - Why needed: Establishes the relationship between model size and performance. Quick check: Compare training curves across different model sizes.

## Architecture Onboarding

**Component Map**: PAC-Bayes bound derivation -> Optimal model size calculation -> Empirical validation on federated datasets

**Critical Path**: Theoretical bound formulation → Analytical solution for optimal size → Experimental verification

**Design Tradeoffs**: Convex assumptions vs. real-world non-convexity, theoretical rigor vs. practical applicability, model simplicity vs. accuracy

**Failure Signatures**: Poor fit between theory and practice (low R²), divergence in non-convex scenarios, sensitivity to hyperparameter choices

**First Experiments**:
1. Validate PAC-Bayes bound on synthetic convex data
2. Test optimal size calculation with varying client counts
3. Compare federated vs centralized scaling behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes convex loss functions which may not hold for deep neural networks
- PAC-Bayes bound may not capture full complexity of non-convex federated learning
- Limited to vision tasks, uncertain generalization to other domains

## Confidence

**High Confidence**: Empirical demonstration that model size selection affects federated learning performance across multiple datasets and architectures.

**Medium Confidence**: Negative power law relationship between client count and optimal model size given theoretical assumptions and limited architectural diversity.

**Medium Confidence**: Practical guidance for model size selection may not account for real-world federated constraints.

## Next Checks

1. Test scaling law predictions on non-convex, state-of-the-art architectures (e.g., modern LLMs or diffusion models)
2. Evaluate framework across heterogeneous data distributions and communication constraints typical in production federated learning systems
3. Extend analysis to multi-task and personalized federated learning scenarios where optimal model size may vary across clients