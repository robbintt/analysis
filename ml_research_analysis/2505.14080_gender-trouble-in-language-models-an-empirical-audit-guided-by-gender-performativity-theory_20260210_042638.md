---
ver: rpa2
title: 'Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity
  Theory'
arxiv_id: '2505.14080'
source_url: https://arxiv.org/abs/2505.14080
tags:
- gender
- language
- gendered
- woman
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study audited 16 language models to examine how they encode
  gender, revealing that larger models reinforce narrow, binary gender constructs
  tied to biological sex, while marginalizing transgender and gender-diverse identities.
  The authors operationalized gender studies insights on performativity and pathologization,
  testing associations between gender, sex, and illness.
---

# Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory

## Quick Facts
- arXiv ID: 2505.14080
- Source URL: https://arxiv.org/abs/2505.14080
- Reference count: 40
- Larger language models reinforce narrow, binary gender constructs tied to biological sex while marginalizing transgender and gender-diverse identities

## Executive Summary
This study conducted an empirical audit of 16 language models to examine how they encode gender-related concepts, applying insights from gender studies and performativity theory. The research revealed that larger models demonstrate stronger sex-gender conflation, systematically pathologize transgender and gender-diverse identities as mental illness, and erase non-binary identities. The authors argue that current model auditing practices overlook these upstream conceptual harms and call for theory-informed evaluation approaches that engage social scientists to address systemic gender biases in language models.

## Method Summary
The authors operationalized gender studies insights on performativity and pathologization by testing associations between gender, sex, and illness in language models. They used WEAT (Word Embedding Association Test) and direct bias measurements to examine how gender terms correlate with sex and mental illness terminology. The audit examined 16 different language models, comparing smaller and larger models to assess how model size affects gender bias patterns. The methodology focused on static embeddings and prompt-based outputs to measure conceptual associations through cosine similarity between word vectors.

## Key Results
- Larger language models show stronger sex-gender conflation, reinforcing narrow binary gender constructs tied to biological sex
- Transgender and gender-diverse identities are systematically pathologized, being associated with mental illness terminology
- Non-binary gender identities are effectively erased from model representations, marginalizing gender-diverse experiences

## Why This Works (Mechanism)
The mechanism appears to operate through the statistical patterns in training data, where dominant cultural narratives about gender become amplified in larger models. As models increase in size, they more strongly capture and reproduce the most frequent associations present in their training corpora. This leads to the reinforcement of binary gender constructs that align with biological sex categories, while marginalizing less common or socially stigmatized gender identities. The pathologization emerges from historical medical literature and societal discourse that has often framed transgender and gender-diverse identities through a medical lens. The erasure of non-binary identities occurs because these concepts receive less representation in training data and are less likely to be associated with the dominant binary frameworks that larger models learn to prioritize.

## Foundational Learning
- Gender performativity theory - Explains how gender identities are constructed through repeated performances rather than being innate biological facts
- Why needed: Provides theoretical framework for understanding how language models might reinforce or challenge binary gender constructs
- Quick check: Compare model outputs when prompted with performativity-based versus essentialist gender concepts

- WEAT methodology - Statistical test measuring association strength between word sets in vector space
- Why needed: Quantifies semantic relationships between gender terms, sex terms, and mental illness terminology
- Quick check: Verify that positive/negative word sets maintain consistent semantic properties across different model sizes

- Direct bias measurement - Cosine similarity-based approach to assess conceptual associations in embeddings
- Why needed: Provides interpretable metric for how strongly gender terms associate with other concept categories
- Quick check: Test whether bias measurements change when using different normalization techniques for word vectors

## Architecture Onboarding
- Component map: Gender seed words -> WEAT tests -> Direct bias measurement -> Model size comparison
- Critical path: Seed word selection → embedding computation → association testing → bias quantification → interpretation
- Design tradeoffs: Comprehensive gender term coverage vs. manageable vocabulary size; statistical rigor vs. conceptual accessibility
- Failure signatures: False associations due to syntactic patterns; missing emerging terminology; cultural context blindness
- First experiments: 1) Replicate WEAT tests with expanded gender vocabulary 2) Test model outputs in conversational prompts 3) Compare bias patterns across different embedding architectures

## Open Questions the Paper Calls Out
- How do different training methodologies and data curation strategies influence the encoding of gender concepts in language models?
- What role does model architecture (transformers vs. other approaches) play in the reproduction of gender biases?
- How can auditing frameworks be designed to better capture the upstream conceptual harms that shape downstream model behaviors?
- What are the implications of these findings for the development of gender-affirming AI systems?
- How can social scientists and technical researchers collaborate more effectively to address systemic gender biases in language models?

## Limitations
- The audit relies on predefined seed word sets that may not capture the full spectrum of gender-related language or evolving terminology
- Focus on English-language models limits generalizability to multilingual contexts with different gender constructs
- Binary framing of "larger" versus "smaller" models without granular analysis of how bias scales across the full model size spectrum
- Static embedding approach may miss dynamic contextual associations that emerge in generation contexts
- WEAT methodology assumes linear separability of concepts, which may not capture complex semantic relationships
- Cultural specificity of gender constructs may not be adequately addressed in the analysis

## Confidence
High confidence: The finding that larger models show stronger sex-gender conflation through statistical tests
Medium confidence: The claim that transgender and gender-diverse identities are systematically marginalized
Medium confidence: The assertion that current model audits overlook upstream conceptual harms

## Next Checks
1. Replicate the audit using dynamic, community-curated vocabularies for gender identity terms that are updated regularly to reflect evolving language use
2. Conduct qualitative analysis of model outputs in conversational contexts where gender terms appear naturally
3. Test the hypothesis that model size correlates with bias magnitude using a continuous measure of model parameters rather than binary classification
4. Investigate how different training methodologies and data curation strategies influence gender bias patterns
5. Explore the potential for gender-affirming model architectures that actively counteract binary gender constructs