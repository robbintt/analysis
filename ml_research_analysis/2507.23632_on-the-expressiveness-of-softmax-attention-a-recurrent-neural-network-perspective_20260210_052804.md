---
ver: rpa2
title: 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective'
arxiv_id: '2507.23632'
source_url: https://arxiv.org/abs/2507.23632
tags:
- attention
- softmax
- linear
- recurrent
- gate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work derives a recurrent neural network formulation for softmax
  attention using Taylor series expansion, revealing that linear attention is a first-order
  approximation of softmax. The recurrent form shows softmax attention as a weighted
  sum of infinite RNNs, each modeling higher-order multiplicative interactions between
  query and key dimensions.
---

# On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective

## Quick Facts
- **arXiv ID**: 2507.23632
- **Source URL**: https://arxiv.org/abs/2507.23632
- **Reference count**: 40
- **Primary result**: Replacing softmax denominator with L2 norm achieves equivalent performance to standard softmax in Llama 2 models.

## Executive Summary
This work provides a novel recurrent neural network perspective on softmax attention by deriving its Taylor series expansion. The paper reveals that linear attention is merely a first-order approximation of softmax, while softmax itself can be viewed as a weighted sum of infinite RNNs modeling higher-order multiplicative interactions between query and key dimensions. Through extensive experiments on Llama 2 architectures, the authors demonstrate that the key component of softmax's effectiveness is vector normalization rather than the exact exponential probability distribution, with L2 norm normalization performing equivalently to standard softmax while gate-based alternatives perform slightly worse.

## Method Summary
The paper introduces a Taylor series expansion of the exponential function in softmax attention, deriving a recurrent formulation where the hidden state grows polynomially with the order of approximation. The method replaces standard softmax attention with a recursive computation that accumulates higher-order Kronecker products of keys and values. Three experimental conditions are tested: standard softmax, L2 norm normalization, and gate-based normalization. Models are trained on Llama 2 architecture with 300M and 2B parameters across three datasets (The Pile, SlimPajama, FineWeb) using AdamW optimizer with linear warmup and decay schedules.

## Key Results
- Linear attention is strictly a first-order term in the softmax Taylor expansion, missing higher-order multiplicative interactions
- L2 vector normalization in place of softmax denominator achieves equivalent performance to standard softmax
- Gate-based normalization variants perform slightly worse and can cause training instability without additional safeguards

## Why This Works (Mechanism)

### Mechanism 1: Higher-Order Multiplicative Interactions
The performance gap between linear and softmax attention stems from modeling multiplicative interactions between query and key dimensions. The Taylor expansion shows softmax attention as a weighted sum of infinite RNNs, where the n-th order term involves Kronecker products modeling n-th degree feature interactions. Linear attention captures only the first-order term, explaining its typically lower performance.

### Mechanism 2: Denominator as Stabilizing Normalizer
The softmax denominator primarily serves as a vector normalization mechanism for stability rather than computing exact exponential probabilities. Experiments show L2 norm normalization achieves equivalent performance, suggesting the "global competition" or "sharp focus" properties of the exponential distribution are less critical than basic magnitude normalization.

### Mechanism 3: Recurrent Formulation with Expanded State
Softmax attention can be computed as a recurrent process with hidden states that grow polynomially with approximation order. While this reveals the theoretical foundation of attention expressiveness, the exponential growth of state dimensions (d^n) makes higher-order approximations computationally intractable, explaining why standard linear attention with fixed state size is limited.

## Foundational Learning

- **Concept: Taylor Series Expansion**
  - Why needed here: Bridges exponential softmax function to polynomial linear attention, explaining the "order" of interactions
  - Quick check question: How does the first-order term of the Taylor expansion of e^x relate to linear attention?

- **Concept: Kronecker Product**
  - Why needed here: Defines "higher-order interactions" by showing how feature dimensions are multiplicatively combined to form larger hidden states
  - Quick check question: What is the dimensionality of the hidden state H_t for a 2nd-order approximation if the key dimension is d?

- **Concept: Softmax Decomposition (Numerator/Denominator)**
  - Why needed here: The paper ablates these parts independently; understanding their distinct roles (scoring vs. normalizing) is required to interpret results
  - Quick check question: According to the paper, is the exponential function in the denominator strictly necessary for the model to perform well?

## Architecture Onboarding

- **Component map**: Input (Q, K, V) -> Taylor RNN Core -> Hidden State (size d^n) -> Normalizer -> Output
- **Critical path**: The interaction between the Normalizer and the Taylor RNN Core. If the normalizer is detached or suboptimal, higher-order terms fail to stabilize, degrading performance.
- **Design tradeoffs**: Standard Softmax (exact, quadratic complexity) vs. Linear Attention (fast, linear complexity, low expressiveness) vs. Taylor Approx (high expressiveness, intractable state growth)
- **Failure signatures**: Loss spikes with gate-based normalization without sequence length division/clamping; performance collapse when denominator is detached from computation graph; stagnation with ReLU kernels even with normalization
- **First 3 experiments**:
  1. Validate L2 Equivalence: Train small Llama 2 model replacing Softmax(QK^T) with L2_Norm(QK^T) to verify normalization hypothesis
  2. Taylor Order Scaling: Implement recurrent form for n=1 (linear) up to n=3 to measure memory-growth vs loss-reduction tradeoff
  3. Denominator Ablation: Run controlled ablation detaching denominator gradient to confirm performance degradation

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the Taylor series recurrent formulation be extended to explain the expressiveness of state-space models like Mamba and RWKV?
  - Basis: The Conclusion states future work should "expand it to more complicated recurrent architectures such as RWKV... and state-space models like Mamba"
  - Why unresolved: Current derivation maps softmax to standard linear attention RNNs, leaving relationship to complex recurrent architectures undefined
  - Resolution: Mathematical derivation showing Mamba or RWKV update rules map to specific softmax Taylor terms

- **Open Question 2**: Is the performance gap between linear attention variants and softmax strictly caused by feature functions restricting the reachable vector space?
  - Basis: Section 4.4 observes adding higher-order terms to linear variants does not match softmax performance and hypothesizes this is due to "linear attention variants have functions... restricting the reachable vector space"
  - Why unresolved: Paper empirically observes performance ceiling but does not theoretically prove vector space restriction is sole cause
  - Resolution: Theoretical proof linking vector space restriction to capacity limits, or experiments using non-restrictive feature functions that close performance gap

- **Open Question 3**: Do denominator equivalences (vector norm vs. gate) generalize to bidirectional attention and tasks beyond next-token prediction?
  - Basis: The Conclusion notes that "Only the causal next token prediction task was investigated" and "further investigation is necessary" for other domains
  - Why unresolved: While bidirectional derivation exists in appendix, all empirical evidence for norm/softmax equivalence is from causal Llama 2 language models
  - Resolution: Ablation studies on bidirectional models (e.g., BERT) or non-language domains confirming simple vector normalization can replace softmax denominator

## Limitations

- The recurrent formulation requires maintaining hidden states of dimension d^n, making higher-order approximations computationally intractable despite theoretical validity
- Experiments only validate performance on Llama 2 architectures trained for next-token prediction, leaving generalization to other tasks open
- Ablation studies comparing L2 normalization to standard softmax do not fully isolate normalization effects from other architectural factors

## Confidence

**High Confidence**: The claim that linear attention is a first-order Taylor approximation of softmax attention is mathematically rigorous. The finding that L2 norm normalization achieves equivalent performance is demonstrated across multiple datasets and model scales. The recurrent formulation correctly shows exponential growth of state dimensions.

**Medium Confidence**: The assertion that higher-order interactions are the primary driver of performance differences relies on ablation studies that may not fully control for confounding factors. Stability analysis of gate-based variants shows promising directions but lacks systematic hyperparameter exploration.

**Low Confidence**: Claims about softmax attention being "a weighted sum of infinite RNNs" are primarily theoretical without experimental validation of intermediate approximation orders. Comparison to linear attention baselines does not account for potential architectural optimizations specific to linear variants.

## Next Checks

1. **Cross-Architecture Generalization**: Implement and test L2 normalization variant on BERT-style architectures for masked language modeling and classification tasks to verify normalization mechanism generalizes beyond next-token prediction.

2. **Intermediate Approximation Scaling**: Systematically train models using Taylor approximations of order n=1, 2, 3, 4 to measure exact performance trade-off curve and determine minimal n required for near-softmax performance.

3. **Dynamic Approximation**: Implement hybrid attention mechanism that adaptively selects Taylor approximation order based on sequence context or computational budget, testing whether model can learn to trade off between linear efficiency and softmax expressiveness dynamically.