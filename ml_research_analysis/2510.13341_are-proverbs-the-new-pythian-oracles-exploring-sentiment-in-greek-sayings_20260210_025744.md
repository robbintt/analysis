---
ver: rpa2
title: Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings
arxiv_id: '2510.13341'
source_url: https://arxiv.org/abs/2510.13341
tags:
- proverbs
- sentiment
- emotion
- proverb
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the sentiment and emotion of Greek proverbs\
  \ using NLP methods, addressing the challenge of their multidimensionality\u2014\
  where a single proverb can convey multiple, coexisting sentiments and emotions depending\
  \ on interpretation. Departing from an annotated dataset of 345 Greek proverbs,\
  \ we expand the analysis to local dialects and employ LLMs to predict sentiment\
  \ and emotion in both labeled and unannotated datasets of approximately 11k geographically\
  \ attributed proverbs."
---

# Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings

## Quick Facts
- **arXiv ID**: 2510.13341
- **Source URL**: https://arxiv.org/abs/2510.13341
- **Reference count**: 16
- **Primary result**: LLMs can capture the multidimensional sentiment and emotion of Greek proverbs, with GPT-5mini achieving sentiment F1 scores of 0.74-0.76 and Krikri-8B excelling in emotion prediction with F1 scores around 0.30.

## Executive Summary
This study explores the sentiment and emotion of Greek proverbs using NLP methods, addressing the challenge of their multidimensionality—where a single proverb can convey multiple, coexisting sentiments and emotions depending on interpretation. Departing from an annotated dataset of 345 Greek proverbs, we expand the analysis to local dialects and employ LLMs to predict sentiment and emotion in both labeled and unannotated datasets of approximately 11k geographically attributed proverbs. We propose a multi-label annotation framework and a novel evaluation approach that treats sentiment and emotion analysis as both multi-label and polarity-aware tasks. Our findings show that LLMs can effectively capture this multidimensionality, with GPT-5mini achieving the highest sentiment F1 scores (micro F1 = 0.76 zero-shot, 0.74 probabilistic) and Krikri-8B excelling in emotion prediction (micro F1 = 0.30 zero-shot, 0.30 probabilistic). However, models tend to overpredict positive emotions like happiness and anger, often at the expense of subtler emotions such as fear and sadness. Regional analysis reveals that anger and surprise are the dominant emotions across Greece, though local variation is significant and can be obscured by aggregation. This work establishes a foundation for understanding the complex emotional landscape of proverbs and highlights the importance of accounting for interpretation variability in NLP evaluations.

## Method Summary
The study employs a multi-label annotation framework where human annotators label Greek proverbs with both sentiment (positive, negative, neutral) and emotion (7 inner-ring categories: happy, angry, bad, fearful, sad, disgusted, surprised). Low inter-annotator agreement (Krippendorff's α = 0.312 for sentiment) is interpreted as evidence of genuine multidimensionality rather than annotation error. LLMs including GPT-5mini, Krikri-8B, Mistral-7B, and Llama3-70B are then used for zero-shot and probabilistic few-shot classification, with probabilistic prompting producing percentage distributions across labels. Geographic mapping links proverbs to specific locations, enabling analysis of local emotional variation that might be obscured by regional aggregation. The study uses 345 annotated proverbs and approximately 11,000 unannotated proverbs from Pavlopoulos et al. (2024), with evaluation metrics including macro-averaged F1 scores and MSE for probabilistic predictions.

## Key Results
- GPT-5mini achieved the highest sentiment F1 scores (micro F1 = 0.76 zero-shot, 0.74 probabilistic)
- Krikri-8B excelled in emotion prediction (micro F1 = 0.30 zero-shot, 0.30 probabilistic)
- Models tend to overpredict positive emotions like happiness and anger, often at the expense of subtler emotions such as fear and sadness
- Regional analysis reveals that anger and surprise are the dominant emotions across Greece, though local variation is significant and can be obscured by aggregation

## Why This Works (Mechanism)

### Mechanism 1: Multi-label Probabilistic Prompting Captures Semantic Ambiguity
- Claim: Probabilistic prompting with percentage-based outputs better captures the inherent multidimensionality of proverbs than single-label zero-shot classification.
- Mechanism: By instructing LLMs to output probability distributions (summing to 100%) across sentiment/emotion classes, the model explicitly represents uncertainty and co-occurring affective states rather than forcing a single dominant label.
- Core assumption: Proverbs legitimately carry multiple simultaneous interpretations rather than having a "correct" single sentiment.
- Evidence anchors:
  - [abstract]: "Models like GPT-5mini and Krikri-8B achieve F1 scores around 0.6–0.8 for sentiment and 0.2–0.5 for emotion, capturing proverbs' complexity better than conventional methods."
  - [section 4.2]: Probabilistic settings improve emotion performance significantly for GPT-5mini (micro F1 from 0.28 to 0.46) and Llama3-70B (macro F1 from 0.31 to 0.44).
  - [corpus]: Related work (BRoverbs, MAPS dataset) confirms LLMs struggle with cultural/figurative language understanding, suggesting conventional approaches are insufficient.
- Break condition: If annotation agreement were high (α > 0.6) for single labels, multidimensionality would not be the core issue.

### Mechanism 2: Low Inter-Annotator Agreement Signals True Multidimensionality, Not Noise
- Claim: Low Krippendorff's α scores (global α = 0.312 for sentiment) reflect genuine semantic variability in proverbs rather than poor annotation quality.
- Mechanism: Annotators provided justified interpretations that differed systematically—some labeling polarities as conflicting (positive vs. negative), others as co-occurring (multilabel). This variance is preserved and analyzed rather than resolved through majority voting.
- Core assumption: Human disagreement on proverb interpretation is meaningful signal, not annotation error.
- Evidence anchors:
  - [section 4.1]: Unimodal items show α = 0.67 (positive) and 0.47 (negative), while polarity disagreement items show α = 0.09–0.15, suggesting structured rather than random disagreement.
  - [section 3.1]: Annotators justified decisions with no room for resolution; the authors kept multi-label settings.
  - [corpus]: No direct corpus evidence on annotation disagreement interpretation; related proverb datasets (ePIC, JAWAHER) do not address this methodologically.
- Break condition: If annotators could not justify their differing labels with coherent interpretations, multidimensionality would be noise.

### Mechanism 3: Fine-Grained Geographic Units Preserve Local Emotional Variation
- Claim: Aggregating proverb sentiment/emotion to large regional units obscures meaningful local variation that exists at finer granularity.
- Mechanism: By mapping proverbs to specific locations and analyzing at the local level (e.g., individual islands), the analysis reveals that areas like Kefalonia show "happy" as dominant while the broader Ionian Islands region shows "anger."
- Core assumption: Proverb sentiment varies meaningfully across localities due to cultural/linguistic differences, not just random variation.
- Evidence anchors:
  - [abstract]: "Regional aggregation can obscure local variation."
  - [section 5]: Kefalonia shows "happy" emotion while the broader Ionian Islands region shows "anger"; simple averaging would erase this distinction.
  - [corpus]: Pavlopoulos et al. (2024) showed specific locations are classified with higher accuracy than others, supporting geographic variability.
- Break condition: If local-level analysis showed no systematic patterns and only noise, aggregation would not lose meaningful information.

## Foundational Learning

- **Multi-label vs. Multi-class Classification**:
  - Why needed here: Proverbs can simultaneously express multiple sentiments/emotions; single-label classification forces artificial choices that lose information.
  - Quick check question: Can a proverb be both "warning about danger" (fearful) and "encouraging preparedness" (hopeful)? If yes, you need multi-label.

- **Krippendorff's α for Agreement on Subjective Tasks**:
  - Why needed here: Interpreting low agreement as "noise" vs. "signal" determines whether you aggregate or preserve variance in annotations.
  - Quick check question: If α = 0.3 on sentiment labels but annotators provide coherent justifications, what does this tell you about the task?

- **Probabilistic Prompting for LLM Uncertainty**:
  - Why needed here: Zero-shot classification forces hard decisions; probabilistic outputs let models express uncertainty aligned with human disagreement patterns.
  - Quick check question: When should you use "predict one label" vs. "output probabilities summing to 100%"?

## Architecture Onboarding

- **Component map**: Data collection (standard + dialectal proverbs) → Multi-label human annotation (sentiment + emotion) → LLM inference (zero-shot + probabilistic prompting) → Geographic mapping → Regional aggregation analysis
- **Critical path**: Annotation quality determines ground truth validity; probabilistic prompting design determines whether LLMs can capture multidimensionality; geographic granularity determines whether local variation is preserved or lost.
- **Design tradeoffs**: (1) Multi-label annotation increases annotation complexity but captures ambiguity; (2) Probabilistic prompting improves recall but may overpredict certain classes (neutral, happy, angry); (3) Fine-grained geographic analysis reveals local patterns but requires sufficient data per location.
- **Failure signatures**: (1) High single-label agreement would suggest proverbs are not multidimensional—revisit task framing; (2) Probabilistic prompting with high MSE suggests model calibration issues—adjust thresholds or examples; (3) Empty geographic regions in visualization indicate data coverage gaps—expand collection or acknowledge limitations.
- **First 3 experiments**:
  1. Replicate the annotation protocol on a small sample (20–50 proverbs) to confirm multidimensionality manifests as structured disagreement, not random noise.
  2. Compare zero-shot vs. probabilistic prompting on a held-out set, measuring F1 and MSE to validate that probabilistic outputs better capture multi-label ground truth.
  3. Test threshold sensitivity for converting probabilistic outputs to discrete labels (currently >0.3); report how performance changes across thresholds (0.2, 0.3, 0.4, 0.5).

## Open Questions the Paper Calls Out

- **Optimal probability threshold**: Future work will examine threshold sensitivity to achieve optimal results for probabilistic few-shot classification of proverb sentiment and emotion.
- **Polarity disagreement vs. multilabeling**: Polarity conflicts are treated here as a multi-label phenomenon, although they ideally should be handled differently; integrating this distinction into evaluation remains an open challenge.
- **Contemporary relevance**: The extent to which these proverbs are actively used today is unknown, leaving their current relevance somewhat uncertain.

## Limitations
- Dataset representativeness: 345 proverbs may not fully capture regional dialectal variation across Greece
- Emotion framework: Relies on adapted Willcox (1982) emotion wheel rather than culturally specific Greek emotion frameworks
- Emotion classification performance: LLM predictions for emotion (F1 ~0.3) remain notably lower than sentiment (F1 ~0.7), suggesting incomplete capture of proverb complexity

## Confidence
- **High Confidence**: Multi-label annotation framework validity, geographic variation findings, LLM superiority over conventional methods for this task
- **Medium Confidence**: Exact impact of probabilistic prompting on capturing multidimensionality (improvement shown but mechanism not fully isolated)
- **Low Confidence**: Generalizability to other languages/cultures, completeness of emotional representation using adapted emotion wheel

## Next Checks
1. Replicate the annotation protocol on a small sample (20–50 proverbs) to verify that structured disagreement patterns persist and confirm multidimensionality is not artifact of original annotator pool.
2. Systematically test classification thresholds (0.2, 0.3, 0.4, 0.5) for converting probabilistic outputs to discrete labels, reporting F1 and MSE changes to validate current >0.3 threshold and identify optimal operating points per class.
3. Map proverb distribution across Greece to identify regions with sparse data; quantify how missing locations affect regional aggregation patterns and test whether interpolation methods introduce systematic bias in dominant emotion identification.