---
ver: rpa2
title: 'MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical
  Reasoning'
arxiv_id: '2505.10557'
source_url: https://arxiv.org/abs/2505.10557
tags:
- images
- code
- math
- arxiv
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MathCoder-VL addresses the challenge of enhancing multimodal mathematical
  reasoning in Large Multimodal Models (LMMs) by leveraging code as supervision for
  cross-modal alignment. Traditional image-caption datasets fail to capture the intricate
  details of mathematical figures critical for problem-solving, leading to poor performance
  in LMMs.
---

# MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning

## Quick Facts
- **arXiv ID**: 2505.10557
- **Source URL**: https://arxiv.org/abs/2505.10557
- **Reference count**: 18
- **Primary result**: State-of-the-art open-source LMM for multimodal math reasoning, surpassing GPT-4o and Claude 3.5 Sonnet on geometry by 8.9% and 9.2% respectively

## Executive Summary
MathCoder-VL addresses the challenge of enhancing multimodal mathematical reasoning in Large Multimodal Models (LMMs) by leveraging code as supervision for cross-modal alignment. Traditional image-caption datasets fail to capture the intricate details of mathematical figures critical for problem-solving, leading to poor performance in LMMs. To overcome this, the authors co-develop an image-to-code model, FigCodifier, and construct the largest image-code dataset to date, ImgCode-8.6M, using a model-in-the-loop approach. The FigCodifier synthesizes novel mathematical figures and generates MM-MathInstruct-3M, a high-quality multimodal math instruction dataset. MathCoder-VL is then trained on ImgCode-8.6M for cross-modal alignment and fine-tuned on MM-MathInstruct-3M for multimodal math problem-solving. The model achieves state-of-the-art performance among open-source LMMs, surpassing GPT-4o and Claude 3.5 Sonnet on the geometry problem-solving subset of MathVista by 8.9% and 9.2%, respectively. MathCoder-VL also excels in multi-step problem-solving and demonstrates outstanding geometry capabilities, with accuracy scores of 48.6%, 32.2%, and 32.1% on angle, area, and length subsets of MATH-Vision. The dataset and models will be open-sourced.

## Method Summary
MathCoder-VL addresses the challenge of enhancing multimodal mathematical reasoning in Large Multimodal Models (LMMs) by leveraging code as supervision for cross-modal alignment. The approach involves two key components: the FigCodifier, an image-to-code model, and the construction of the largest image-code dataset to date, ImgCode-8.6M, using a model-in-the-loop approach. The FigCodifier synthesizes novel mathematical figures and generates MM-MathInstruct-3M, a high-quality multimodal math instruction dataset. MathCoder-VL is then trained on ImgCode-8.6M for cross-modal alignment and fine-tuned on MM-MathInstruct-3M for multimodal math problem-solving. This approach enables the model to capture the intricate details of mathematical figures critical for problem-solving, leading to state-of-the-art performance among open-source LMMs.

## Key Results
- Surpasses GPT-4o and Claude 3.5 Sonnet on MathVista geometry by 8.9% and 9.2% respectively
- Achieves 48.6% accuracy on angle, 32.2% on area, and 32.1% on length subsets of MATH-Vision
- Excels in multi-step problem-solving and demonstrates outstanding geometry capabilities
- Open-sources dataset and models

## Why This Works (Mechanism)
The core insight is that code provides a precise, structured representation of mathematical figures that captions cannot capture. By training on code that describes geometric relationships, MathCoder-VL learns to extract the quantitative relationships and spatial reasoning needed for mathematical problem-solving. The model-in-the-loop approach ensures that the synthetic data covers diverse mathematical scenarios while maintaining quality through iterative refinement.

## Foundational Learning

**Cross-modal alignment**: Understanding how visual information maps to structured representations like code. *Why needed*: Mathematical reasoning requires extracting precise quantitative relationships from figures. *Quick check*: Can the model accurately convert between visual and code representations?

**Synthetic data generation**: Creating diverse, high-quality training data programmatically. *Why needed*: Real-world mathematical figures with solutions are limited and expensive to annotate. *Quick check*: Does the generated data cover the full range of mathematical problem types?

**Geometric reasoning**: Understanding spatial relationships and mathematical properties of shapes. *Why needed*: Core to solving geometry problems that require visual analysis. *Quick check*: Can the model correctly identify angles, areas, and lengths in novel figures?

## Architecture Onboarding

**Component map**: Raw Image → FigCodifier → Code Representation → MathCoder-VL → Multimodal Math Reasoning

**Critical path**: Image → FigCodifier encoding → Cross-modal attention → Code generation → Math reasoning

**Design tradeoffs**: The approach trades computational efficiency (generating large synthetic datasets) for better alignment between visual and mathematical representations, prioritizing accuracy over speed.

**Failure signatures**: Poor performance on non-geometric math problems, overfitting to synthetic data patterns, failure to generalize to real-world figures with noise or distortions.

**First experiments**:
1. Evaluate cross-modal alignment accuracy on held-out FigCodifier test set
2. Test MathCoder-VL on simple geometry problems with ground truth code supervision
3. Compare performance against baseline LMMs with standard image-caption training

## Open Questions the Paper Calls Out
None

## Limitations
- The comparison to GPT-4o and Claude 3.5 Sonnet may be unfair as MathCoder-VL is specifically fine-tuned while the others are general-purpose models
- The model shows strong geometry performance but limited evaluation on other mathematical domains
- The synthetic data generation approach may introduce biases or fail to capture real-world variations in mathematical figures

## Confidence

**High confidence**: The technical approach of using code as supervision for cross-modal alignment is well-justified and the experimental methodology is sound. The ablation studies and comparative analysis against open-source baselines are appropriately conducted.

**Medium confidence**: The claims about state-of-the-art performance relative to closed-source models, while supported by experimental results, may not represent a fair comparison given the different training objectives and data exposure.

**Low confidence**: The generalization claims beyond geometry and the assertion of "enhanced" multimodal mathematical reasoning across diverse mathematical domains lack sufficient supporting evidence.

## Next Checks

1. Conduct head-to-head comparisons between MathCoder-VL and general-purpose LMMs on the same training regimen, ensuring fair comparison by fine-tuning both models on the same math instruction datasets before evaluation.

2. Perform extensive error analysis on the FigCodifier-generated images and code to quantify the quality and potential biases in the ImgCode-8.6M dataset, including manual annotation of a representative sample to validate synthetic data quality.

3. Evaluate MathCoder-VL performance across a broader range of mathematical domains (algebra, calculus, statistics) using established benchmarks to assess whether the geometry-focused training generalizes to other mathematical reasoning tasks.