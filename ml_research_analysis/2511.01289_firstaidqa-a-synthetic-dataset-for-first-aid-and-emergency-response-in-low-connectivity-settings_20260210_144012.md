---
ver: rpa2
title: 'FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity
  Settings'
arxiv_id: '2511.01289'
source_url: https://arxiv.org/abs/2511.01289
tags:
- first
- dataset
- emergency
- pairs
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FirstAidQA, a synthetic dataset of 5,500
  high-quality question-answer pairs for first aid and emergency response. The dataset
  was generated using prompt-based in-context learning with ChatGPT-4o-mini, guided
  by texts from the Vital First Aid Book (2019).
---

# FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings

## Quick Facts
- arXiv ID: 2511.01289
- Source URL: https://arxiv.org/abs/2511.01289
- Reference count: 23
- Generates 5,500 synthetic QA pairs for first aid using in-context learning from a certified first aid book

## Executive Summary
This paper introduces FirstAidQA, a synthetic dataset of 5,500 high-quality question-answer pairs for first aid and emergency response. The dataset was generated using prompt-based in-context learning with ChatGPT-4o-mini, guided by texts from the Vital First Aid Book (2019). The authors applied preprocessing steps such as text cleaning, contextual chunking, and filtering, followed by human validation to ensure accuracy, safety, and practical relevance. The dataset is designed to support instruction-tuning and fine-tuning of LLMs and SLMs for edge deployment in bandwidth-constrained or offline settings. Expert evaluation of a random subset yielded mean scores of 4.2 for clarity, 4.7 for relevance, 4.0 for specificity and completeness, and 3.7 for safety and accuracy. The authors publicly release the dataset to advance research on safety-critical and resource-constrained AI applications in first aid and emergency response.

## Method Summary
The authors generated FirstAidQA using ChatGPT-4o-mini with prompt-based in-context learning. They sourced content from the Vital First Aid Book (2019), applied text cleaning and contextual chunking, then used structured prompts to generate ~20 QA pairs per batch. The process involved iterative prompting to expand coverage while avoiding duplicates. Generated pairs were filtered for real-world applicability and validated by three medical professionals who evaluated 200 randomly sampled pairs across four criteria. The dataset covers 9 emergency categories and is released in JSON format for use in instruction-tuning and fine-tuning of both large and small language models.

## Key Results
- Generated 5,500 high-quality QA pairs covering 9 emergency categories
- Expert evaluation (3 medical professionals, 200 samples) yielded scores: Clarity 4.2, Relevance 4.7, Specificity & Completeness 4.0, Safety & Accuracy 3.7
- Dataset designed for instruction-tuning and fine-tuning of LLMs and SLMs for edge deployment
- Public release via Hugging Face with JSON format

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic QA generation via in-context learning can produce domain-specific training data at scale when guided by authoritative source material.
- **Mechanism:** ChatGPT-4o-mini receives topic-specific chunks from the Vital First Aid Book (2019) alongside structured prompts that define role, output format (JSON), and quality constraints. The model generates ~20 QA pairs per batch, with iterative prompting to expand coverage while avoiding duplicates. This grounds responses in certified content rather than parametric knowledge alone.
- **Core assumption:** The source text is comprehensive and accurate; the LLM can faithfully extract and reformat procedural knowledge without introducing hallucinations beyond the provided context.
- **Evidence anchors:**
  - [abstract] "generated using a Large Language Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from the Vital First Aid Book (2019)"
  - [Section 3.3] Details the prompt template structuring role, context chunks, and JSON output requirements
  - [corpus] Weak direct evidence—neighbor papers focus on robotics/sensors rather than synthetic QA methodology
- **Break condition:** If source material contains outdated or region-specific protocols, generated QA will propagate those limitations; prompt adherence degrades if chunks exceed context windows or lack coherence.

### Mechanism 2
- **Claim:** Multi-criterion human evaluation on sampled subsets provides a feasible quality signal for synthetic datasets where full expert review is cost-prohibitive.
- **Mechanism:** Three medical professionals rated 200 randomly sampled QA pairs (3.6% of dataset) on clarity, relevance, specificity/completeness, and safety/accuracy using 1–5 scales. Mean scores above 3.7 across criteria suggest acceptable quality; flagged items (Appendix A) reveal systematic risks (e.g., incorrect advice on tick removal, anaphylaxis treatment).
- **Core assumption:** Random sampling is representative of overall dataset quality; evaluator expertise aligns with target use cases.
- **Evidence anchors:**
  - [Section 4.3] Reports mean scores: Clarity 4.2, Relevance 4.7, Specificity & Completeness 4.0, Safety & Accuracy 3.7
  - [Appendix A] Table 3 documents 8 flagged QA pairs with specific unsafe instructions
  - [corpus] No comparable validation frameworks in neighbor papers for synthetic medical datasets
- **Break condition:** Low safety scores (3.7/5) indicate residual risk—deployment without additional filtering or professional oversight could propagate harmful advice.

### Mechanism 3
- **Claim:** Domain-specific synthetic QA can instruction-tune smaller models for offline deployment when general-purpose LLMs are unavailable due to connectivity or compute constraints.
- **Mechanism:** FirstAidQA provides 5,500 structured QA pairs covering 9 emergency categories (e.g., CPR, burns, bites, spinal injuries). The dataset format supports instruction-tuning pipelines for SLMs (e.g., TinyLlama-1.1B mentioned in related work), enabling edge deployment without cloud dependency.
- **Core assumption:** Task-specific fine-tuning on 5.5K examples is sufficient for acceptable performance; SLMs can retain procedural knowledge without catastrophic forgetting.
- **Evidence anchors:**
  - [Section 1] "FirstAidQA is designed to support instruction-tuning and fine-tuning of LLMs and Small Language Models (SLMs)"
  - [Section 2] References Cahlen's TinyLlama-1.1B LoRA adapter as precedent for offline practical skills QA
  - [corpus] Neighbor paper on quantized YOLOv4-Tiny for emergency aerial imagery supports viability of lightweight edge deployment patterns
- **Break condition:** Dataset scale (5.5K pairs) may be insufficient for robust generalization; no fine-tuning experiments or benchmarks are reported in this paper.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The dataset generation relies on providing source text chunks within prompts so the LLM conditions on external knowledge rather than parametric memory.
  - **Quick check question:** Can you explain why including source text in the prompt reduces hallucination risk compared to zero-shot generation?

- **Concept: Instruction Tuning**
  - **Why needed here:** FirstAidQA is explicitly designed to support fine-tuning models to follow procedural instructions in emergency scenarios.
  - **Quick check question:** How does instruction tuning differ from continued pretraining, and why is QA format suitable for it?

- **Concept: Edge Deployment Constraints**
  - **Why needed here:** The paper targets offline/low-connectivity settings where model size, latency, and inference costs are critical.
  - **Quick check question:** What tradeoffs does reducing model size (e.g., 1.1B vs 7B parameters) introduce for safety-critical medical guidance?

## Architecture Onboarding

- **Component map:**
  - Vital First Aid Book (2019) → segmented into topic chunks
  - ChatGPT-4o-mini + structured prompts → JSON QA pairs (5,500 total)
  - Text cleaning, contextual chunking, filtering
  - Random sampling → 3 expert reviewers → scored criteria + flagged items
  - Hugging Face dataset (JSON format)

- **Critical path:**
  1. Chunk source text while preserving procedural coherence (e.g., keep multi-step instructions together)
  2. Design prompts specifying role, context, output format, and diversity constraints
  3. Generate in batches of 20 with deduplication checks
  4. Filter chunks unsuitable for real-world QA (theoretical/low-utility content)
  5. Expert sample validation before release

- **Design tradeoffs:**
  - **Scale vs. verification cost:** 5.5K pairs enables broader coverage but only 200 sampled for expert review (3.6%)
  - **Single-source grounding:** Reduces inconsistency risk but limits perspective diversity; no multi-guideline synthesis
  - **Synthetic generation:** Scalable but inherits LLM biases; safety score (3.7/5) reflects residual errors

- **Failure signatures:**
  - **Procedural errors:** Flagged items include incorrect advice (e.g., pressure immobilization for anaphylaxis, blind finger sweeps for choking)
  - **Context drift:** If chunks are too short or fragmented, generated QA may lack step completeness
  - **Overgeneralization:** Questions may not cover edge cases (pediatric, elderly, comorbidities) without explicit prompt instructions

- **First 3 experiments:**
  1. **Fine-tune an SLM (e.g., TinyLlama-1.1B) on FirstAidQA** and evaluate on held-out emergency scenarios; compare to zero-shot GPT-4 baseline for accuracy and safety
  2. **Expand expert validation to 10% of dataset** to quantify whether sampled scores generalize; analyze error categories (omission, conflation, outdated protocol)
  3. **Cross-validate against alternative guidelines** (e.g., Red Cross, ILCOR) to identify protocol discrepancies and create a multi-source augmented version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Small Language Models (SLMs) successfully fine-tuned on FirstAidQA achieve reliable performance in offline environments compared to large, cloud-dependent models?
- Basis in paper: [inferred] The paper concludes with a "call to action" for developing offline AI, positioning the dataset as a tool for SLMs, but does not actually train or evaluate these models.
- Why unresolved: The work focuses on dataset creation and human validation, leaving the actual efficacy of the dataset for model instruction-tuning unproven.
- What evidence would resolve it: Benchmarks showing SLMs fine-tuned on FirstAidQA performing accurately on held-out emergency queries without internet access.

### Open Question 2
- Question: How can synthetic data pipelines be improved to eliminate hallucinated medical advice that passes initial semantic filters?
- Basis in paper: [explicit] Appendix A lists specific "Flagged Q&A Pairs" with unsafe instructions, and Table 2 shows "Safety & Accuracy" scored lowest (3.7) during evaluation.
- Why unresolved: Despite context-based prompting, the generative model still produced dangerous suggestions (e.g., incorrect choking procedures), indicating current filtering methods are insufficient for safety-critical domains.
- What evidence would resolve it: A secondary validation study on the full 5,500 pairs showing zero instances of advice contradicting established medical guidelines.

### Open Question 3
- Question: Does the restriction to a single source text limit the robustness of the dataset across different international emergency protocols?
- Basis in paper: [inferred] Section 3.1 states the dataset is derived solely from the "Vital First Aid Book 2019."
- Why unresolved: While the authors note the book aligns with some international standards, regional variations in first aid (e.g., CPR compression depth or poison control) are not addressed in the synthetic generation process.
- What evidence would resolve it: Cross-referencing dataset answers with diverse global medical standards (e.g., Red Cross, WHO) to verify consistency and coverage.

## Limitations

- Dataset relies on a single source (Vital First Aid Book 2019), limiting perspective diversity and potential protocol variations across regions
- Expert validation covered only 3.6% of the dataset (200 pairs), leaving most content unreviewed for safety and accuracy
- No fine-tuning or benchmarking experiments were conducted to validate the dataset's utility for model training

## Confidence

- **High confidence**: Dataset generation methodology using in-context learning with source-grounded prompts
- **Medium confidence**: Expert evaluation scores and flagged items from the 200-sample subset
- **Low confidence**: Claims about dataset effectiveness for instruction-tuning without empirical validation

## Next Checks

1. Conduct full fine-tuning experiments with an SLM (e.g., TinyLlama-1.1B) on FirstAidQA, evaluating both accuracy and safety on held-out emergency scenarios
2. Expand expert validation to 10% of the dataset to assess whether the 3.6% sample scores generalize, with detailed error category analysis
3. Cross-validate the dataset against multiple authoritative first aid guidelines (e.g., Red Cross, ILCOR) to identify protocol discrepancies and create an augmented multi-source version