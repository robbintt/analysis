---
ver: rpa2
title: Improving LLMs for Machine Translation Using Synthetic Preference Data
arxiv_id: '2508.14951'
source_url: https://arxiv.org/abs/2508.14951
tags:
- translation
- training
- language
- preference
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to improve an open-source LLM for English-to-Slovene
  translation using synthetic preference data. The core idea is to generate translation
  pairs using two independent models, then filter and rank them using heuristics and
  COMET scores to create a high-quality preference dataset.
---

# Improving LLMs for Machine Translation Using Synthetic Preference Data

## Quick Facts
- arXiv ID: 2508.14951
- Source URL: https://arxiv.org/abs/2508.14951
- Authors: Dario Vajda; Domen Vreš; Marko Robnik-Šikonja
- Reference count: 34
- One-line primary result: GaMS-9B-Instruct improved from 12% translation error rate to 0.8% and COMET scores up to 0.757 using synthetic preference data

## Executive Summary
This paper presents a method to improve English-to-Slovene translation quality of the GaMS-9B-Instruct model using synthetic preference data without human annotation. The approach generates translation pairs using two independent LLMs, filters them through heuristic and metric-based criteria, then fine-tunes using Direct Preference Optimization (DPO). Results show significant improvements over the baseline model, reducing error rates from 12% to 0.8% and achieving COMET score gains of approximately 0.04 on Wikipedia articles. The method is particularly valuable for low-resource languages where human-annotated translation data is scarce.

## Method Summary
The method generates translation pairs by translating English Wikipedia and CC-News articles using two independent models (GaMS-9B-Instruct and EuroLLM-9B-Instruct). These pairs undergo hierarchical filtering: heuristic filters detect wrong language, truncation, and formatting artifacts, while COMET scores rank remaining pairs based on quality differences exceeding 0.05. The filtered dataset (~35K pairs) trains GaMS-9B-Instruct using DPO with LoRA (rank=64), β=0.1, learning rate 1e-6, over 3 epochs with ZeRO Stage 2 and gradient checkpointing on 16×A100 40GB GPUs.

## Key Results
- Error rate reduced from 12% to 0.8% on Wikipedia test set
- COMET score improved from 0.722 to 0.757 on Wikipedia articles
- COMET score improved from 0.680 to 0.715 on CC-News articles
- Performance gains achieved without human-annotated preference data

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Preference Pair Generation via Model Comparison
Two models with different training distributions produce translation pairs with distinguishable quality differences. When one model makes obvious errors (wrong language, truncation) while the other succeeds, this creates high-confidence preference pairs. For subtler differences, COMET scores provide ranking. The core assumption is that the two source models have sufficiently different error distributions to generate meaningful preference contrasts rather than correlated failures.

### Mechanism 2: Hierarchical Filtering Combining Heuristics and Learned Metrics
A staged filtering pipeline separates obvious failures from subtle quality differences, reducing noise in preference learning. Heuristics first capture failure modes with binary labels (language detection via FastText, truncation via length ratio, formatting artifacts via prefix detection). Remaining pairs are scored with COMET and only included if score difference exceeds threshold (0.05), preventing metric noise from being treated as signal.

### Mechanism 3: Direct Preference Optimization for Translation Alignment
DPO fine-tuning shifts the model toward higher-quality translations without requiring a separately trained reward model. DPO directly optimizes the log-likelihood ratio between chosen and rejected outputs using the reference model for regularization. The β parameter controls how strongly the model fits the preference data versus staying close to the reference model.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed here: DPO is the core alignment method used; understanding its loss function and β regularization is essential for hyperparameter tuning and debugging.
  - Quick check question: Can you explain why DPO avoids training a separate reward model compared to RLHF?

- **COMET and Reference-less Quality Estimation**
  - Why needed here: COMET scores are the primary metric for ranking subtler translation quality differences; understanding its limitations is critical for data quality.
  - Quick check question: What does "reference-less Direct Assessment" mean and why might it be preferred over BLEU for preference ranking?

- **LoRA and Memory-Efficient Fine-Tuning**
  - Why needed here: Training a 9B model with DPO requires memory optimizations; LoRA and ZeRO Stage 2 are employed to make this feasible.
  - Quick check question: What is the trade-off between LoRA rank size and model expressiveness during fine-tuning?

## Architecture Onboarding

- **Component map:** English Wikipedia/CC-News → GaMS-9B-Instruct + EuroLLM-9B-Instruct → translation pairs → FastText language detection → truncation check → formatting check → COMET scoring → threshold filtering → DPO training

- **Critical path:** 1) Generate translation pairs from source corpus using both models 2) Apply heuristic filters (language, truncation, formatting) 3) Score remaining pairs with COMET, retain only pairs with Δscore > 0.05 4) Run DPO training with grid search over β and learning rate 5) Evaluate on SloBench and custom Wikipedia/CC-News benchmarks

- **Design tradeoffs:**
  - COMET threshold (0.05): Higher threshold reduces dataset size but increases preference signal clarity
  - β parameter (0.1 vs 0.2): Lower β preserves reference model behavior; higher β fits preference data more aggressively
  - Synthetic formatting pairs (20% of dataset): Augments data but may over-represent artifacts not present in natural outputs

- **Failure signatures:**
  - High validation loss with low training loss: Overfitting to preference pairs; reduce epochs or increase β
  - Model outputs become verbose or repeat phrases: DPO over-optimization; check β and learning rate
  - No improvement in error rates: Preference pairs may lack diversity or COMET threshold is too permissive
  - Out-of-memory errors during training: ZeRO Stage 2 and gradient checkpointing may be insufficient; reduce LoRA rank or batch size

- **First 3 experiments:**
  1. Reproduce baseline result: Train DPO on Wikipedia-only preference data with reported hyperparameters (β=0.1, lr=1e-6, 3 epochs); verify validation loss approximates 0.315.
  2. Ablate COMET threshold: Retrain with threshold 0.03 and 0.08; measure impact on dataset size, training stability, and final COMET scores to validate the 0.05 choice.
  3. Domain transfer test: Evaluate the fine-tuned model on a held-out domain (e.g., legal or scientific texts not in SloBench) to assess whether Wikipedia-style training generalizes; identify domain gaps for future data expansion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the synthetic preference data pipeline scale effectively to significantly larger models, such as the 27B parameter variant?
- Basis in paper: [explicit] The authors state, "We plan to use the insight gained during this project to fine-tune the 27B parameter model with the same training pipeline."
- Why unresolved: The current study only validates the approach on a 9B model; scaling introduces challenges regarding computational resources and distributed training optimizations (e.g., ZeRO Stage 3).
- What evidence would resolve it: Successful fine-tuning and evaluation of the 27B model demonstrating similar or improved error reduction and COMET score gains relative to its baseline.

### Open Question 2
- Question: Can Curriculum DPO improve training efficiency or translation quality compared to the vanilla DPO implementation used?
- Basis in paper: [explicit] The authors propose "Curriculum DPO... instead of vanilla DPO" as a method to organize training by difficulty, separating heuristic errors from subtle COMET score differences.
- Why unresolved: The current implementation trains on all preference pairs simultaneously without a curriculum, leaving the potential benefits of structured learning untested.
- What evidence would resolve it: A comparative study measuring validation loss and translation metrics between models trained with vanilla DPO versus Curriculum DPO on the same dataset.

### Open Question 3
- Question: Does incorporating a broader range of domains into the preference dataset significantly improve performance on diverse benchmarks?
- Basis in paper: [explicit] The authors suggest "incorporating more conversational, legal, and news texts... would better capture all domains tested by SloBench and potentially increase its score."
- Why unresolved: The current model was trained predominantly on Wikipedia articles, creating a distribution mismatch with the diverse domains (e.g., legal, scientific) found in the SloBench benchmark.
- What evidence would resolve it: Evaluation results showing improved performance on underrepresented domains (legal, conversational) after fine-tuning on a domain-expanded preference dataset.

## Limitations

- Synthetic preference generation relies on assumption that two models have sufficiently different error patterns, which may not hold for all language pairs
- COMET-based ranking may not fully capture human preferences for translation quality, particularly for stylistic or domain-specific nuances
- Heavy reliance on synthetic formatting errors (20% of dataset) could overfit to artifact detection rather than genuine translation quality improvement

## Confidence

- **High confidence**: DPO training methodology and LoRA implementation details are well-specified and reproducible. The error rate reduction from 12% to 0.8% is substantial and directly measurable.
- **Medium confidence**: COMET score improvements (+0.04 on Wikipedia, +0.035 on CC-News) are meaningful but rely on the assumption that COMET correlates well with human translation quality for Slovene.
- **Medium confidence**: Language-agnostic claims are supported by methodology but not extensively validated beyond English-to-Slovene case study.

## Next Checks

1. **Cross-lingual validation**: Apply synthetic preference generation pipeline to a different low-resource language pair (e.g., English-to-Latvian) to test method's true language-agnostic nature and identify language-specific failure modes.
2. **Human evaluation correlation**: Conduct human evaluation study comparing COMET scores against human preference judgments for subset of translations to quantify reliability of COMET-based ranking for Slovene.
3. **Domain generalization test**: Evaluate fine-tuned model on out-of-domain texts (legal, technical, literary) to assess whether Wikipedia-style training generalizes beyond news and encyclopedic content.