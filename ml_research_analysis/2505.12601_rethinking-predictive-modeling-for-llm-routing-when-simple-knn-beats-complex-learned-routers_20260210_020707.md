---
ver: rpa2
title: 'Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex
  Learned Routers'
arxiv_id: '2505.12601'
source_url: https://arxiv.org/abs/2505.12601
tags:
- routing
- performance
- utility
- approaches
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the prevailing trend toward complex learned
  routing approaches for selecting the best large language model (LLM) for a given
  input. Instead, it demonstrates that simple k-Nearest Neighbors (kNN) methods, when
  properly implemented, can match or outperform sophisticated learned routers across
  diverse text and multi-modal routing tasks.
---

# Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers

## Quick Facts
- **arXiv ID**: 2505.12601
- **Source URL**: https://arxiv.org/abs/2505.12601
- **Reference count**: 40
- **Primary result**: Simple k-Nearest Neighbors (kNN) methods match or outperform complex learned routing approaches across diverse LLM routing tasks when properly implemented.

## Executive Summary
This paper challenges the prevailing trend toward complex learned routing approaches for selecting the best large language model (LLM) for a given input. Instead, it demonstrates that simple k-Nearest Neighbors (kNN) methods, when properly implemented, can match or outperform sophisticated learned routers across diverse text and multi-modal routing tasks. The study introduces standardized routing benchmarks spanning instruction-following, question-answering, reasoning, and vision-language tasks, enabling systematic evaluation of routing approaches. Theoretical analysis shows that kNN routers require lower sample complexity than parametric methods when the embedding space exhibits strong locality properties, explaining their effectiveness. Across all evaluated scenarios, kNN consistently achieved competitive or superior performance compared to complex approaches like Graph Neural Networks, attention mechanisms, and matrix factorization-based routers, with simple linear models also showing strong results. This finding suggests that practitioners should thoroughly evaluate simple baselines before investing in sophisticated routing solutions.

## Method Summary
The study evaluates LLM routing approaches that select the optimal model from a pool M for query x by maximizing utility u(x,m) = s(x,m) - λ·c(x,m), where s is performance score and c is cost. The method implements kNN routing using cosine similarity with k∈{10,100} and embeddings from BERT-base (768-dim) for text and VLM2Vec-Qwen7B (3584-dim) for multimodal tasks. The support set contains (embedding, model_id, score, cost) tuples indexed for fast retrieval. For utility prediction, kNN averages neighbor scores and costs; for model selection, it uses majority voting. The approach is evaluated on standardized benchmarks including RouterBench, AlpacaEval, HELM-Lite, and vision-language datasets like Blink and MMMU, with data split 70/10/20 for train/val/test.

## Key Results
- kNN with k=100 achieved competitive or superior performance compared to complex learned routers like GNNs, attention mechanisms, and matrix factorization across all evaluated benchmarks
- Simple linear models also demonstrated strong performance, suggesting over-engineering is common in routing approaches
- Theoretical analysis proved kNN requires lower sample complexity than parametric methods when embedding spaces exhibit strong locality properties
- Utility prediction formulation using kNN consistently matched or exceeded direct selection approaches across diverse routing scenarios

## Why This Works (Mechanism)

### Mechanism 1: Locality in Embedding Space
kNN routers achieve competitive routing accuracy because embedding spaces exhibit locality—semantically similar queries have similar optimal model assignments. When a query arrives, retrieve k nearest neighbors from the support set by embedding distance. Aggregate their observed model performance (via averaging or voting) to predict the best model for the new query. This requires well-structured embeddings where semantic similarity correlates with performance similarity, validated by empirical correlation (r = -0.815, -0.875) between embedding distance and model agreement rate.

### Mechanism 2: Sample Complexity Advantage
kNN requires fewer training samples than parametric routers to achieve equivalent regret bounds when embedding dimensionality is low and locality is strong. Non-parametric estimation uses direct neighborhood lookup (sample complexity Θ(C/δ^d · log(1/α))) rather than learning dense weight matrices across all layers (sample complexity Ω(L/ε²)). This advantage collapses when intrinsic dimension is high or locality weak (ε(δ) decreases slowly).

### Mechanism 3: Support Set Adaptability
Support sets provide test-time adaptability without retraining, enabling routing to respond to distribution shifts. Store (query, model, score, cost) tuples as a support set. At inference, retrieve relevant neighbors and aggregate their outcomes. New data can be appended incrementally. This requires sufficient support set coverage so every query region has enough neighbors within δ distance.

## Foundational Learning

- **Concept**: k-Nearest Neighbors (non-parametric estimation)
  - **Why needed here**: Core routing mechanism; understanding distance metrics, aggregation strategies (weighted averaging vs. voting), and curse of dimensionality is essential.
  - **Quick check question**: Given embeddings for "write a haiku" and "compose a sonnet," should they route to the same model? How would kNN decide?

- **Concept**: Locality and covering numbers in metric spaces
  - **Why needed here**: Theoretical justification for why kNN works; connects embedding quality to routing performance guarantees.
  - **Quick check question**: If your embedding space has intrinsic dimension 50 instead of 5, how does sample complexity change for kNN?

- **Concept**: Pareto frontier and utility functions (performance vs. cost tradeoffs)
  - **Why needed here**: Routing optimizes utility = score – λ·cost; AUC evaluation traces Pareto front across λ values.
  - **Quick check question**: If λ = 0.1/c_max favors performance and λ = 1.0/c_max favors cost, which queries might flip their optimal model assignment?

## Architecture Onboarding

- **Component map**: Query → Embedding encoder → kNN retriever → Aggregator → Model selection
- **Critical path**: Query → Embedding → kNN retrieval → Aggregate scores/costs → Select argmax utility model
- **Design tradeoffs**:
  - k value: Small k (10) is noisier but local; large k (100) smoother but may blur boundaries
  - Embedding model: BERT (768d) vs. SFR (4096d)—better embeddings help all methods similarly
  - Utility prediction vs. direct selection: Prediction gives full Pareto front; selection is simpler but less flexible
- **Failure signatures**:
  - kNN matches random baseline → locality violated or support set too sparse
  - Performance degrades on new model families → support set lacks coverage for that model's behavior
  - Non-monotonic cost-performance in selection mode → preference parameter λ doesn't directly control budget
- **First 3 experiments**:
  1. **Baseline validation**: Implement kNN (k=10, 100) with BERT embeddings on RouterBench; verify Table 2 AUC scores are reproducible within ±2 points.
  2. **Ablation on k**: Sweep k ∈ {5, 10, 25, 50, 100, 200} on one benchmark; plot AUC vs. k to find saturation point.
  3. **Embedding sensitivity**: Replace BERT with SFR embeddings; confirm improvements are modest (~1-3 AUC points) and consistent across methods per Table 6.

## Open Questions the Paper Calls Out

- **Question**: How does the performance of non-parametric routers compare to complex methods when trained on alternative signals such as reward models or pairwise preferences rather than direct evaluation metrics?
  - **Basis in paper**: The conclusion states that "Investigating how simple routers perform with alternative training signals (reward models, preferences) remains an important direction."
  - **Why unresolved**: The current study relies exclusively on direct evaluation metrics (e.g., accuracy, length-controlled win rates) derived from standardized benchmarks to train and evaluate routers.
  - **What evidence would resolve it**: Empirical comparisons of kNN versus parametric routers when the training objective relies on proxy signals like reward model scores rather than ground-truth task performance.

- **Question**: To what extent do simple routers generalize to completely new models or prompt distributions not seen in the support set, particularly in dynamic deployment scenarios?
  - **Basis in paper**: The authors call for "Further investigating the generalization capabilities of simple routers to new models, prompts, and unseen tasks."
  - **Why unresolved**: The evaluation uses fixed model pools within benchmarks, whereas real-world scenarios involve the continuous release of new models and shifting user query distributions.
  - **What evidence would resolve it**: Evaluation using temporal splits or hold-out model sets, testing how quickly simple routers adapt to "cold-start" scenarios involving models with no prior performance data.

- **Question**: Can kNN-based approaches be effectively extended to handle batch routing scenarios subject to global computational constraints?
  - **Basis in paper**: The discussion identifies "Batch Routing" as a future direction, noting that "Extending kNN approaches to batch routing with global computational constraints presents an interesting practical challenge."
  - **Why unresolved**: The current framework (Equation 1) optimizes routing on a per-query basis, failing to account for dependencies or aggregate budget limits across a batch of simultaneous requests.
  - **What evidence would resolve it**: Algorithms that modify kNN selection to satisfy a global cost cap across a batch of queries while maximizing total utility.

## Limitations
- Theoretical analysis assumes strong locality and low intrinsic embedding dimension, but these properties were not empirically validated across all routing scenarios
- Implementation details remain partially unspecified as code is not yet released, creating uncertainty about exact benchmark construction
- Sample complexity advantage remains largely theoretical without experimental validation showing kNN outperforming learned methods with limited training data

## Confidence
- **High confidence**: kNN achieves competitive or superior performance compared to complex learned routers on standardized benchmarks (Table 2). The empirical results across diverse tasks (text, vision-language) are robust and well-documented.
- **Medium confidence**: The theoretical sample complexity advantage for kNN when locality properties hold. While the proof framework is sound, empirical validation of this advantage under controlled conditions (varying training set sizes) is lacking.
- **Medium confidence**: Support set adaptability without retraining provides practical advantages for distribution shifts. The mechanism is well-described but quantitative evidence of adaptation to real distribution shifts is limited.

## Next Checks
1. **Locality property validation**: Systematically measure ε(δ) across all benchmarks to verify the δ-Locality assumption. Plot model agreement rate vs. embedding distance for each routing task and compute correlation coefficients.
2. **Sample complexity experiment**: Design controlled experiments varying training set sizes (10%, 25%, 50%, 100%) for both kNN and parametric methods. Measure performance convergence rates to empirically validate the theoretical sample complexity claims.
3. **Distribution shift evaluation**: Create out-of-distribution test sets by applying domain shifts (e.g., different prompt styles, languages, or visual domains). Compare kNN's adaptability against parametric methods that require retraining, measuring performance degradation and adaptation speed.