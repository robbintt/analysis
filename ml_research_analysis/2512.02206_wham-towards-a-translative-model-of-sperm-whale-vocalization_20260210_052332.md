---
ver: rpa2
title: 'WhAM: Towards A Translative Model of Sperm Whale Vocalization'
arxiv_id: '2512.02206'
source_url: https://arxiv.org/abs/2512.02206
tags:
- codas
- audio
- whale
- sperm
- wham
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WhAM (Whale Acoustics Model), the first transformer-based
  model for generating synthetic sperm whale codas from audio prompts. The model is
  built by fine-tuning VampNet, a masked acoustic token model pretrained on musical
  audio, using 10,000 coda recordings collected over two decades.
---

# WhAM: Towards A Translative Model of Sperm Whale Vocalization

## Quick Facts
- arXiv ID: 2512.02206
- Source URL: https://arxiv.org/abs/2512.02206
- Reference count: 40
- Primary result: First transformer-based model generating synthetic sperm whale codas from audio prompts

## Executive Summary
WhAM (Whale Acoustics Model) introduces a novel approach to modeling sperm whale vocalizations through a transformer-based architecture that generates synthetic codas from audio prompts. The model leverages 10,000 coda recordings collected over two decades and builds on VampNet, a masked acoustic token model pretrained on musical audio. By fine-tuning this architecture through iterative masked token prediction, WhAM produces high-fidelity synthetic codas that preserve key acoustic features while demonstrating capabilities in acoustic translation, pseudocoda generation, and useful representation learning for downstream classification tasks.

## Method Summary
WhAM employs a transformer-based architecture built by fine-tuning VampNet, which uses masked acoustic token prediction. The model processes audio inputs through an encoder that generates contextual representations, then uses iterative masked prediction to generate synthetic codas. Training occurs on 10,000 sperm whale coda recordings over five days using a single GPU. The architecture learns to preserve rhythmic patterns and click-level features while being capable of translating various audio inputs into sperm whale vocalization style. The model demonstrates strong performance on rhythm type, social unit, and vowel classification tasks despite being trained for generation rather than classification.

## Key Results
- Generates high-fidelity synthetic codas preserving key acoustic features of source recordings
- Achieves strong performance on rhythm type, social unit, and vowel classification tasks
- Expert marine biologists validate that model captures rhythmic patterns while some click-level features differ from natural codas

## Why This Works (Mechanism)
The model works by leveraging masked acoustic token prediction within a transformer framework, allowing it to learn contextual representations of sperm whale codas while preserving temporal structure. The fine-tuning process on specialized marine mammal vocalizations enables the model to capture domain-specific acoustic patterns that differ from its musical audio pretraining. The iterative generation process allows for refinement of synthetic outputs, while the encoder-decoder architecture facilitates both generation and representation learning for downstream tasks.

## Foundational Learning
- Masked acoustic token prediction: Why needed - Enables learning of contextual representations while preserving temporal structure; Quick check - Verify tokens capture both local and global acoustic patterns
- Transformer architecture: Why needed - Handles long-range dependencies in sequential data crucial for coda structure; Quick check - Test attention patterns for rhythmic consistency
- Domain transfer learning: Why needed - Adapts musical audio pretraining to specialized marine mammal vocalizations; Quick check - Compare performance with from-scratch training
- Iterative generation: Why needed - Allows refinement of synthetic outputs through multiple prediction steps; Quick check - Evaluate quality improvements across generation iterations
- Downstream classification adaptation: Why needed - Enables transfer of generative representations to discriminative tasks; Quick check - Test classification performance on held-out data

## Architecture Onboarding

Component Map: Raw Audio -> VampNet Encoder -> Masked Token Prediction -> Synthetic Coda Generation -> Downstream Classifier

Critical Path: The model's critical path involves encoding input audio into contextual representations, applying masked token prediction iteratively to generate synthetic codas, and evaluating both generation quality and downstream classification performance. The pretraining on musical audio followed by fine-tuning on whale codas represents the key adaptation pathway.

Design Tradeoffs: The model trades computational efficiency (five days on single GPU) for dataset size limitations, relying on transfer learning from musical audio rather than larger specialized whale datasets. The generation-focused training objective may limit direct optimization for classification tasks, though representations still prove useful.

Failure Signatures: Poor rhythmic pattern preservation indicates inadequate temporal modeling; unnatural click timing suggests insufficient domain adaptation; degraded classification performance reveals weak representation learning; excessive background noise in synthetic codas points to overfitting on training data characteristics.

First Experiments:
1. Generate synthetic codas from various input audio types and compare rhythmic patterns against ground truth
2. Test downstream classification performance on held-out test set across all three task types
3. Perform ablation study removing musical pretraining to assess transfer learning contribution

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Relatively small dataset (10,000 coda recordings) for acoustic modeling tasks
- Model trained for generation rather than classification, raising generalization questions
- Perceptual evaluation relies on qualitative expert assessment without comprehensive quantitative benchmarks

## Confidence

High confidence: Model architecture and training methodology are clearly described and technically sound
Medium confidence: Generation capabilities and downstream classification performance, as these are demonstrated but may not fully capture real-world variability
Medium confidence: Perceptual evaluation results, as they rely on expert judgment without comprehensive quantitative benchmarks

## Next Checks

1. Conduct systematic quantitative comparisons between synthetic and natural codas across multiple acoustic features (spectral characteristics, inter-click intervals, duration patterns) using established bioacoustic metrics

2. Test the model's translation capabilities across a broader range of input audio types (different animal vocalizations, environmental sounds) and assess generalization to unseen vocalization styles

3. Evaluate the model's performance on additional downstream tasks beyond classification, such as anomaly detection or vocalization segmentation, to better understand the learned representations' utility