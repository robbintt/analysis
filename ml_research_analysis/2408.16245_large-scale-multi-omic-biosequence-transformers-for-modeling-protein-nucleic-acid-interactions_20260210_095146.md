---
ver: rpa2
title: Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic
  Acid Interactions
arxiv_id: '2408.16245'
source_url: https://arxiv.org/abs/2408.16245
tags:
- protein
- omnibiote
- acid
- nucleic
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniBioTE is a family of multi-omic foundation models jointly trained
  on 250 billion tokens of mixed protein and nucleic acid sequences. Unlike existing
  models that focus on single omics, OmniBioTE learns joint representations that align
  genes to their corresponding proteins, enabling effective modeling of protein-nucleic
  acid interactions.
---

# Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions

## Quick Facts
- arXiv ID: 2408.16245
- Source URL: https://arxiv.org/abs/2408.16245
- Reference count: 40
- Primary result: OmniBioTE achieves Pearson correlation of 0.41 and MAE of 1.56 kcal/mol on binding affinity prediction, outperforming single-omic baselines

## Executive Summary
OmniBioTE is a family of foundation models trained jointly on 250 billion tokens of mixed protein and nucleic acid sequences, learning joint representations that align genes to their corresponding proteins. The model achieves state-of-the-art results on protein-nucleic acid interaction prediction, particularly for binding affinity (∆G), while also demonstrating superior performance-per-FLOP across both multi-omic and single-omic benchmarks. Remarkably, despite no explicit structural training, the model learns latent structural information that enables prediction of protein residues involved in binding.

## Method Summary
OmniBioTE uses a LLaMA-2 style encoder with RoPE positional encoding, non-causal attention, and SwiGLU MLP layers. The model is pretrained on 250B tokens from GenBank (nucleic acids) and UniRef100 (proteins) using masked language modeling with µP scaling. Two tokenization strategies are employed: BPE (1024 tokens) and single-residue (2048 chars). Fine-tuning for binding affinity uses concatenated protein-nucleic acid sequences with a linear head predicting ∆G. The model is evaluated on the ProNAB dataset with rigorous homology filtering (BLOSUM62 score <1.5).

## Key Results
- Achieves Pearson correlation of 0.41 and MAE of 1.56 kcal/mol on binding affinity prediction, outperforming single-omic baselines
- Demonstrates emergent joint representations between genes and proteins, validated through contrastive learning with 5% training pairs
- Despite no explicit structural training, learns latent structural information enabling contact prediction with F1 scores of 0.27-0.33
- Sets new state-of-the-art performance on single-omic benchmarks while maintaining superior performance-per-FLOP

## Why This Works (Mechanism)

### Mechanism 1
Joint training on mixed protein and nucleic acid sequences induces a modality-invariant latent space by forcing the model to reuse parameters across modalities to minimize reconstruction error. This shared parameter optimization causes gene and protein embeddings to align in a shared subspace without explicit cross-modal attention during pretraining.

### Mechanism 2
Fine-tuning on binding affinity (∆G) induces structural knowledge into attention weights because the model must attend to interaction sites to minimize the loss. This supervision shapes attention maps, causing specific heads to act as coarse contact predictors that can be extracted by lightweight convolutional probes.

### Mechanism 3
Multi-omic pretraining provides superior compute-efficient initialization for single-omic tasks by acting as a strong regularizer or meta-learning phase. Training on diverse data forces the model to learn more generalizable sequence features, allowing it to reach lower loss on downstream tasks with fewer total FLOPs compared to single-omic pretraining.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed: OmniBioTE is a BERT-style encoder that learns by reconstructing corrupted tokens, not a generative decoder
  - Quick check: Does the model use a causal mask (next token prediction) or bidirectional mask (hidden token reconstruction)?

- **Concept: Tokenization Strategies (BPE vs. Single-Residue)**
  - Why needed: The paper tests BPE vs. single-character tokenization, affecting context window size and resolution for tasks like contact prediction
  - Quick check: For per-residue contact prediction, which tokenizer version avoids information loss inside tokens?

- **Concept: Maximal Update Parameterization (µP)**
  - Why needed: µP scales learning rates from small proxy models to the 2.3B parameter version, enabling stable large-scale training
  - Quick check: How does the learning rate scale relative to model width under µP?

## Architecture Onboarding

- **Component map:** GenBank/UniRef sequences -> Tokenizer (BPE or single-residue) -> OmniBioTE Encoder (RoPE, non-causal attention, SwiGLU MLP, RMSNorm) -> Linear Head (for fine-tuning tasks)

- **Critical path:** 1) Data prep: Stream mixed GenBank and UniRef sequences, ensure no cross-modal concatenation during pretraining 2) Pretraining: Run MLM objective with µP scaling 3) Probing: Use contrastive learning on frozen embeddings to verify gene-protein alignment 4) Fine-tuning: Concatenate Protein + NA sequences → OmniBioTE Encoder → Linear Head → ∆G prediction

- **Design tradeoffs:** BPE compresses sequence length for longer effective context but loses per-residue resolution; single-char preserves resolution but limits max length. Unified encoder outperforms separate encoders despite seeing less per-modality data.

- **Failure signatures:** Low performance on per-residue tasks likely from using BPE without mode/mean pooling; high ProNAB performance might indicate data leakage from poor homology filtering; unstable training suggests incorrect µP scaling implementation.

- **First 3 experiments:** 1) Modality Alignment Check: Train low-rank linear projector on 5% of gene-protein pairs using contrastive loss, test retrieval on remaining 95% 2) Binding Affinity Regression: Fine-tune OmniBioTE on ProNAB with homology-filtered splits, compare against concatenated embeddings from separate models 3) Attention Probe: Fine-tune on ∆G, freeze, train ConvNet on attention maps to predict residue contacts, compare F1 scores against base model

## Open Questions the Paper Calls Out

- **Cross-Modal Attention:** The authors hypothesize that richer representations could be learned with explicit cross-attention between modalities during pretraining, which remains unexplored.

- **Autoregressive Training:** The study only investigated MLM pretraining, leaving the efficacy of autoregressive training for multi-omic sequences as an open research direction.

- **Aptamer Design:** While the methodology could be extended to aptamer design and property prediction with the right dataset, this application remains unexplored.

## Limitations

- The quality of gene-protein alignment remains unclear without explicit retrieval accuracy metrics
- Structural interpretability results show modest F1 scores (0.27-0.33) compared to specialized structural biology tools
- Compute efficiency advantages may be architecture-specific and not transfer to different model families or larger scales

## Confidence

- **High Confidence:** ProNAB binding affinity results (Pearson 0.41, MAE 1.56 kcal/mol) and compute efficiency claims on single-omic benchmarks
- **Medium Confidence:** Emergent joint representation claims and structural interpretability findings
- **Low Confidence:** Generalization of performance-per-FLOP advantages to different architectures and scales

## Next Checks

1. **Cross-Modal Alignment Quality:** Conduct comprehensive retrieval experiments measuring exact match accuracy, top-5 accuracy, and embedding space metrics between gene-protein pairs, comparing against modality-specific transformer baselines.

2. **Structural Knowledge Ablation:** Perform ablation study fine-tuning OmniBioTE on ∆G with and without structural supervision, comparing attention-based contact prediction F1 scores to determine if the effect is specific to binding affinity.

3. **Multi-Omic Transfer Robustness:** Evaluate OmniBioTE's performance-per-FLOP on single-omic tasks using a different backbone architecture (e.g., BERT or RoBERTa style) with the same multi-omic pretraining procedure.