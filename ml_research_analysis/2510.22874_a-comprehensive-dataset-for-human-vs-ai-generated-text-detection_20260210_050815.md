---
ver: rpa2
title: A Comprehensive Dataset for Human vs. AI Generated Text Detection
arxiv_id: '2510.22874'
source_url: https://arxiv.org/abs/2510.22874
tags:
- text
- arxiv
- dataset
- detection
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dataset for detecting AI-generated text.
  The dataset contains over 58,000 samples combining authentic New York Times articles
  with synthetic versions generated by multiple state-of-the-art LLMs including Gemma-2-9b,
  Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o.
---

# A Comprehensive Dataset for Human vs. AI Generated Text Detection

## Quick Facts
- **arXiv ID**: 2510.22874
- **Source URL**: https://arxiv.org/abs/2510.22874
- **Reference count**: 40
- **Primary result**: Dataset contains 58,502 samples for human vs. AI text detection and model attribution tasks

## Executive Summary
This paper presents RAIDAR (Reuters AI-Detection and Attribution Repository), a comprehensive dataset for detecting and attributing AI-generated text. The dataset contains over 58,000 samples combining authentic New York Times articles with synthetic versions generated by six state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o. The authors establish baseline results using a rewriting-based approach that leverages the observation that LLMs make fewer edits when rewriting their own outputs compared to human text, achieving 58.35% accuracy for human vs. AI detection and 8.92% accuracy for model attribution.

## Method Summary
The method uses a rewriting-based approach where a fixed rewriting model (e.g., GPT-3.5-Turbo) is prompted to paraphrase input text. Levenshtein edit distance is calculated between the original and rewritten versions. For attribution, the model yielding the minimum edit distance is predicted as the generator. For detection, if all edit distances exceed a threshold (median of maximum edit distances across training samples), the input is classified as human-written. The dataset contains NYT article abstracts as prompts paired with human-written stories and AI-generated stories from six different LLMs.

## Key Results
- Human vs. AI detection accuracy: 58.35% (only marginally better than random)
- Model attribution accuracy: 8.92% (near-random for 6-class problem)
- Dataset size: 58,502 samples combining NYT articles with synthetic versions
- Rewriter behavior: LLMs make fewer edits when rewriting AI-generated text compared to human text

## Why This Works (Mechanism)

### Mechanism 1: Rewrite-Based Detection via Edit Distance
LLMs make fewer edits when rewriting AI-generated text compared to human-authored text because they share stylistic priors and treat AI-generated content as already "optimized." This causes them to preserve AI-generated text more closely while making larger modifications to human writing.

### Mechanism 2: Model Attribution via Self-Rewriting Minimization
An LLM makes the fewest edits when rewriting text it originally generated, enabling model attribution. The model whose rewrite has minimum Levenshtein distance to the original is predicted as the source generator, exploiting within-model stylistic consistency.

### Mechanism 3: Threshold-Based Human Classification
Text is classified as human-written if all candidate rewriters produce edit distances exceeding a calibrated threshold. The threshold is set as the median of maximum edit distances across training samples, based on the assumption that human writing exhibits stylistic diversity that systematically exceeds AI models' priors.

## Foundational Learning

- **Levenshtein Edit Distance**: Why needed here: The baseline method quantifies rewriting changes via character-level edit distance to compare original vs. rewritten text. Quick check: Given strings "kitten" and "sitting," can you compute the minimum number of single-character edits (insert, delete, substitute) to transform one to the other?

- **LLM Token Distribution Priors**: Why needed here: The core hypothesis relies on understanding that LLMs have implicit preferences for certain token sequences, manifesting as lower loss when processing AI-generated vs. human text. Quick check: Why would a model trained on web text potentially "prefer" rewriting AI-generated content differently than human-edited journalism?

- **Threshold Calibration for Binary Classification**: Why needed here: The detection pipeline converts continuous edit distances into discrete human/AI labels via a data-driven threshold. Quick check: What are the trade-offs of setting the threshold at the median vs. a more conservative percentile of maximum edit distances?

## Architecture Onboarding

- **Component map**: Prompt → AI Generation → Rewriting → Edit Distance → Threshold Comparison → Classification
- **Critical path**: Prompt → AI Generation → Rewriting → Edit Distance → Threshold Comparison → Classification. The rewriting step is the inference cost bottleneck.
- **Design tradeoffs**: Single rewriter vs. multiple rewriters reduces cost but may introduce bias; character-level vs. token-level edit distance captures finer changes but is computationally heavier.
- **Failure signatures**: Low attribution accuracy (8.92%) indicates model fingerprints overlap significantly; near-random detection (58.35%) suggests weak rewriting signal; domain shift may limit generalizability.
- **First 3 experiments**:
  1. Replace GPT-3.5-Turbo with each source model as the rewriter to test if self-rewriting improves attribution accuracy
  2. Sweep threshold values from 25th to 75th percentile and plot precision-recall curves for Task A
  3. Train classifier on Gemma-generated text only, test on GPT-4-o outputs to measure cross-model generalization

## Open Questions the Paper Calls Out

### Open Question 1
Can more sophisticated detection architectures significantly improve upon the low baseline accuracy (58.35%) for human vs. AI classification? The conclusion states the "low baseline results underscore the difficulty... and the substantial room for methodological innovation" in detection.

### Open Question 2
How do in-context learning, fine-tuning, or agentic systems affect the detectability and "human-like" quality of the generated text? The authors propose future work where LLMs generate text via "in-context learning, finetunining or agentic systems" to create more challenging, human-like samples.

### Open Question 3
Do detectors trained on this dataset generalize effectively to text generated by unseen or newer LLM architectures? Section 3.5 identifies "Cross-Model Generalization" as a key research direction, asking if detectors trained on one model can generalize to others given rapid AI evolution.

### Open Question 4
Can the dataset structure and detection methodology be successfully adapted for multilingual text and multimodal content? The conclusion lists "expanding the dataset to other languages and modalities" as a primary avenue for future research.

## Limitations

- Rewriting mechanism ambiguity: Unclear whether a single fixed rewriter or all six source models should be used as rewriters
- Weak baseline performance: 58.35% detection accuracy is only marginally better than random guessing
- Dataset composition limitations: Domain-specific nature (journalism) may limit generalizability to other text types

## Confidence

**High Confidence**:
- Dataset contains over 58,000 samples combining NYT articles with synthetic versions from six LLMs
- Dataset supports two specific tasks: human vs. AI detection and model attribution
- Baseline uses rewriting approach with Levenshtein edit distance
- Threshold for human classification is median of maximum edit distances across training samples

**Medium Confidence**:
- Rewriting mechanism functions as described (single vs. multiple rewriters)
- Edit distance calculation methodology is correctly implemented
- Attribution mechanism based on minimum edit distance is theoretically sound

**Low Confidence**:
- Baseline accuracy results (58.35% and 8.92%) are reproducible with described methodology
- Rewriting mechanism captures meaningful stylistic differences between models
- Threshold calibration strategy optimizes for practical detection performance

## Next Checks

1. **Rewriter Architecture Validation**: Implement both variants of rewriting pipeline (single fixed rewriter vs. using each source model as its own rewriter) and measure impact on attribution accuracy.

2. **Threshold Sensitivity Analysis**: Systematically sweep classification threshold from 25th to 75th percentile of maximum edit distances across multiple data splits and plot precision-recall curves.

3. **Cross-Model Generalization Test**: Train classifier on Gemma-generated text only, test on GPT-4-o outputs to measure performance drop and assess feature transferability.