---
ver: rpa2
title: 'MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time'
arxiv_id: '2508.08641'
source_url: https://arxiv.org/abs/2508.08641
tags:
- migrate
- sampling
- wang
- search
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MIGRATE is a method for online test-time training of large language\
  \ models using mixed-policy Group Relative Policy Optimization (GRPO) to solve black-box\
  \ optimization tasks without external data. It combines on-policy sampling with\
  \ two off-policy techniques\u2014greedy sampling of top past completions and neighborhood\
  \ sampling of variations from those completions\u2014to balance exploration and\
  \ exploitation."
---

# MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time

## Quick Facts
- arXiv ID: 2508.08641
- Source URL: https://arxiv.org/abs/2508.08641
- Reference count: 40
- MiGrATe uses mixed-policy GRPO to adapt LLMs online at test-time for black-box optimization tasks, achieving up to 25 percentage point improvements over inference-only and TTT baselines.

## Executive Summary
MiGrATe is a method for online test-time training of large language models using mixed-policy Group Relative Policy Optimization (GRPO) to solve black-box optimization tasks without external data. It combines on-policy sampling with two off-policy techniques—greedy sampling of top past completions and neighborhood sampling of variations from those completions—to balance exploration and exploitation. Evaluated on word search, molecule optimization, and ARC program synthesis, MiGrATe consistently outperforms inference-only and TTT baselines, achieving up to 25 percentage point improvements in solution rates.

## Method Summary
MiGrATe is a method for online test-time training of large language models using mixed-policy Group Relative Policy Optimization (GRPO) to solve black-box optimization tasks without external data. It combines on-policy sampling with two off-policy techniques—greedy sampling of top past completions and neighborhood sampling of variations from those completions—to balance exploration and exploitation. Evaluated on word search, molecule optimization, and ARC program synthesis, MiGrATe consistently outperforms inference-only and TTT baselines, achieving up to 25 percentage point improvements in solution rates.

## Key Results
- Achieved up to 25 percentage point improvements in solution rates over inference-only and TTT baselines
- Consistently outperformed baselines across word search, molecule optimization, and ARC program synthesis tasks
- Demonstrated effectiveness of mixed-policy GRPO for online adaptation without external data

## Why This Works (Mechanism)
The mixed-policy design balances exploration (via on-policy sampling) and exploitation (via greedy and neighborhood sampling of past completions) to efficiently navigate solution spaces in black-box optimization. GRPO optimizes the policy relative to a group baseline, enabling stable updates during online adaptation. The combination of sampling strategies allows the model to leverage past successes while maintaining diversity in exploration.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: A reinforcement learning method that optimizes policy gradients relative to a group of samples, reducing variance and improving stability. Needed for stable online adaptation; quick check: compare variance in policy updates with and without group baseline.
- **Mixed-policy sampling**: Combining on-policy, greedy, and neighborhood sampling to balance exploration and exploitation. Needed for efficient search in solution spaces; quick check: measure solution diversity and convergence speed under different sampling strategies.
- **Test-time training**: Adapting models during inference without external data. Needed for real-time optimization in dynamic environments; quick check: evaluate performance degradation over time without adaptation.

## Architecture Onboarding
**Component Map**: LLM -> Sampling Strategies (On-policy, Greedy, Neighborhood) -> GRPO Updates -> Reward Computation -> LLM
**Critical Path**: LLM generates solutions → Sampling strategies select candidates → GRPO computes policy updates → Rewards guide adaptation → Updated LLM generates next solutions
**Design Tradeoffs**: Mixed-policy sampling increases computational cost but improves solution quality; GRPO requires multiple LLM calls per iteration but stabilizes training.
**Failure Signatures**: Poor exploration leads to local optima; unstable GRPO updates cause policy collapse; high computational overhead limits real-time applicability.
**First Experiments**: 1) Test individual sampling strategies in isolation, 2) Vary the ratio of on-policy to off-policy sampling, 3) Measure adaptation speed on simple optimization tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope is narrow, focusing on synthetic and benchmark-style problems with cheap reward computation
- May not scale to real-world domains with sparse, noisy, or expensive reward functions
- High computational cost due to multiple LLM calls per iteration could limit practical viability under latency constraints

## Confidence
- **High**: The method's core idea of using mixed-policy GRPO for online adaptation is well-defined and experimentally validated in the tested domains.
- **Medium**: The empirical performance gains over baselines are reproducible in the tested tasks, but broader applicability is uncertain.
- **Low**: The method's effectiveness in real-world scenarios with complex reward functions and high computational costs is not established.

## Next Checks
1. Evaluate MiGrATe on a real-world domain with sparse or noisy rewards, such as robotic control or drug discovery, to assess scalability and robustness.
2. Test the method's performance under strict latency constraints to determine its practical viability in time-sensitive applications.
3. Investigate the impact of varying the reward function's complexity and noise level on MiGrATe's performance to understand its limitations in ill-defined optimization problems.