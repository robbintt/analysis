---
ver: rpa2
title: 'MEG-GPT: A transformer-based foundation model for magnetoencephalography data'
arxiv_id: '2510.18080'
source_url: https://arxiv.org/abs/2510.18080
tags:
- data
- each
- training
- time
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEG-GPT is a transformer-based foundation model trained on large-scale
  MEG data that uses a novel tokeniser to preserve temporal resolution without lossy
  transformations. It achieves state-of-the-art zero-shot generalisation, improving
  cross-subject decoding accuracy from 0.41 to 0.49 and cross-session accuracy from
  0.54 to 0.59 compared to baseline methods.
---

# MEG-GPT: A transformer-based foundation model for magnetoencephalography data

## Quick Facts
- arXiv ID: 2510.18080
- Source URL: https://arxiv.org/abs/2510.18080
- Reference count: 24
- Primary result: Transformer-based foundation model trained on large-scale MEG data achieves state-of-the-art zero-shot generalisation

## Executive Summary
MEG-GPT is a transformer-based foundation model trained on large-scale magnetoencephalography (MEG) data that achieves state-of-the-art zero-shot generalisation. The model uses a novel tokeniser to convert continuous MEG signals into discrete tokens while preserving high temporal resolution without lossy transformations. When evaluated on a visual decoding task, MEG-GPT improves cross-subject decoding accuracy from 0.41 to 0.49 and cross-session accuracy from 0.54 to 0.59 compared to baseline methods.

The model captures realistic spatio-spectral properties including transient events and individual fingerprints, and can be efficiently fine-tuned on small labelled datasets for further performance gains. Trained on 612 subjects from the Cam-CAN dataset, MEG-GPT demonstrates strong generalisation capabilities when evaluated on the independent Wakeman-Henson dataset.

## Method Summary
MEG-GPT employs a three-stage pipeline: first, a data-driven tokeniser converts continuous MEG signals into discrete tokens using an autoencoder with GRU encoder and learned token kernels, achieving >97% percentage variance explained. Second, a transformer decoder with causal masking and next-token prediction is trained autoregressively on the tokenised data, using Perceiver AR to handle long sequences efficiently. Third, the pre-trained model is fine-tuned on target tasks by discarding learned subject embeddings and retraining them while freezing other embeddings. The architecture processes 52 cortical parcels at 250Hz, with a 320ms receptive field and patching mechanism for computational efficiency.

## Key Results
- Zero-shot cross-subject decoding accuracy improved from 0.41 to 0.49 compared to baseline methods
- Cross-session decoding accuracy increased from 0.54 to 0.59
- Generated data reproduces realistic spatio-spectral properties including transient events and individual fingerprints
- Model achieves high reconstruction fidelity (>97% PVE) across training, held-out subjects, and independent datasets

## Why This Works (Mechanism)

### Mechanism 1: Lossless Tokenisation Enables Discrete Modelling Without Information Loss
The autoencoder-based tokeniser learns K reusable temporal patterns (token kernels) from data, converting continuous MEG to discrete tokens while preserving temporal resolution. Each timepoint is assigned a categorical token label based on learned logits, and the decoder reconstructs signals via weighted sums of token kernels. Critically, no compression or regularisation is applied—the goal is faithful reconstruction, not dimensionality reduction.

### Mechanism 2: Autoregressive Next-Token Prediction Captures Non-Linear Spatiotemporal Dependencies
The transformer decoder uses masked multi-head attention to predict the next token from a context window (receptive field = 80 samples, 320ms). Unlike fixed-coefficient AR models, attention weights adapt dynamically to input patterns. This captures statistical dependencies that linear AR models cannot, particularly transient dynamics like bursts and oscillations.

### Mechanism 3: Subject Embeddings Encode Individual Variability for Transfer
Subject ID is encoded as an auxiliary input embedding that gets added to token, channel, and position embeddings. During training, the model learns to associate patterns with specific subjects. During fine-tuning, subject embeddings are discarded and retrained while other embeddings are frozen, enabling efficient adaptation to new individuals.

## Foundational Learning

- **Concept: Masked Self-Attention in Transformers**
  - Why needed here: MEG-GPT uses decoder-only transformers with causal masking. You must understand how attention selectively weighs historical tokens, and why masking prevents "looking ahead" during autoregressive training.
  - Quick check question: Given a sequence of tokens [t1, t2, t3, t4], which tokens can the attention mechanism use when predicting t5? (Answer: Only t1–t4, due to causal mask)

- **Concept: Cross-Entropy Loss vs. MSE for Sequence Modelling**
  - Why needed here: The paper explicitly switches to discrete tokenisation to use cross-entropy, claiming better convergence. Understanding why categorical loss helps with mode-covering in generative models is critical.
  - Quick check question: Why might MSE loss lead to overly smooth predictions when modelling multi-modal distributions? (Answer: MSE penalises average of modes, not mode selection)

- **Concept: Time-Delay Embedding (TDE) for Oscillatory Analysis**
  - Why needed here: The paper uses TDE-HMM as an independent validation method for burst detection. Understanding how lagged copies of signals capture oscillatory structure helps interpret the validation results.
  - Quick check question: If a signal has a dominant 10Hz oscillation, approximately how many lagged copies at 250Hz sampling are needed to span one full cycle? (Answer: 25 lags)

## Architecture Onboarding

- **Component map:**
  Tokeniser (GRU encoder + Dense layer → logits → argmax → token labels | Dense decoder with learned token kernels → reconstructed signal) → Input embeddings (Token embedding + Channel embedding + Position embedding + Subject embedding → summed vector) → Transformer decoder (Patched attention + Unpatched tail + Perceiver AR latent compression) → Prediction head (Dense layer → logits → softmax → next-token probability distribution)

- **Critical path:** Preprocessing (bandpass 0.5–125Hz, ICA artefact removal, source reconstruction, parcellation to 52 regions) → Tokeniser training (50 subjects) → Tokenisation of full dataset (612 subjects) → MEG-GPT pre-training → Fine-tuning on target task → Feature extraction (mean-pool decoder outputs) → Linear classifier

- **Design tradeoffs:**
  - Receptive field (80 samples) vs. computational cost: Longer context captures slower dynamics but attention is O(L²). Current choice limits <3Hz modelling.
  - Patching (Lp=4) vs. temporal precision: Reduces cost but loses fine-grained structure; mitigated by keeping last 16 samples unpatched.
  - Token vocabulary size (K*=61) vs. reconstruction fidelity: More tokens improve reconstruction but increase prediction complexity.

- **Failure signatures:**
  - Generated data lacks alpha/beta bursts → Likely: receptive field too short or patching too aggressive
  - Zero-shot features underperform baseline → Likely: pre-training data distribution mismatch (scanner, preprocessing, parcellation)
  - Fine-tuning degrades performance → Likely: learning rate too high (catastrophic forgetting) or wrong embeddings frozen
  - Tokeniser PVE drops on new dataset → Likely: token vocabulary learned on insufficiently diverse data

- **First 3 experiments:**
  1. **Tokeniser validation:** Train tokeniser on 50 Cam-CAN subjects, compute PVE on held-out subjects and a different dataset (e.g., Wakeman-Henson). Target: >95% PVE. If <90%, increase K or token width d_token.
  2. **Ablation on patching:** Compare generated data quality (PSD match, burst statistics) with patching disabled vs. enabled. Confirm patching doesn't degrade spectral/burst fidelity.
  3. **Embedding transfer test:** Pre-train with subject embeddings, then fine-tune on Wakeman-Henson with (a) frozen subject embeddings, (b) retrained subject embeddings, (c) no subject embeddings. Compare cross-subject decoding accuracy. Expected: (b) > (c) > (a).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating explicit cross-channel attention mechanisms enable the model to capture functional connectivity dynamics?
- Basis in paper: Section 4.2 states that the current lack of inter-channel dependencies prevents the generated signals from expressing known static or dynamic functional connectivity, identifying cross-channel attention as an important extension.
- Why unresolved: The current architecture encodes spatial information via learnable embeddings but processes channels independently, failing to model interactions between brain regions.
- What evidence would resolve it: A comparison of static and dynamic functional connectivity metrics (e.g., correlation matrices, phase coupling) calculated from real data versus data generated by an augmented MEG-GPT with cross-channel attention.

### Open Question 2
- Question: Does extending the context window beyond 320ms improve the model's ability to capture low-frequency neural fluctuations?
- Basis in paper: Section 4.2 notes that the current receptive field of 80 samples (320ms) limits the model's capacity to capture slow fluctuations, identifying context window extension as a future direction.
- Why unresolved: The self-attention mechanism's quadratic computational cost currently restricts the sequence length, potentially truncating the modeling of physiologically relevant slow-wave activity.
- What evidence would resolve it: Evaluation of the Power Spectral Density (PSD) of generated signals at low frequencies (e.g., < 1Hz) using a model trained with a significantly extended receptive field compared to the current baseline.

### Open Question 3
- Question: Can end-to-end fine-tuning of the foundation model yield superior decoding performance compared to the current linear probe method?
- Basis in paper: Section 4.6 suggests that while the current method uses a linear classifier on extracted features, prior work indicates fine-tuning the entire model and a flexible classification head yields superior results.
- Why unresolved: The current study decoupled feature extraction from classification to demonstrate zero-shot generalization, leaving the potential performance gains of joint optimization untested.
- What evidence would resolve it: A benchmark comparison on the Wakeman-Henson task where one condition fine-tunes all model weights (including the transformer) and the other uses the current frozen-feature approach.

## Limitations

- Pre-training data distribution constraints: Model relies heavily on resting-state MEG from healthy young adults, limiting generalisation to clinical populations or different age ranges
- Temporal resolution vs. biological plausibility: 320ms receptive field may be insufficient for capturing slower oscillatory dynamics (<3Hz)
- Generalisation vs. overfitting trade-off: Subject embedding approach assumes individual differences are low-dimensional and systematic, which may not hold for pathological populations

## Confidence

**High confidence:**
- Tokeniser achieves high reconstruction fidelity (>97% PVE) on both training and held-out subjects
- Autoregressive model captures realistic spatio-spectral properties including transient events
- Model can be fine-tuned on small labelled datasets for performance gains

**Medium confidence:**
- Zero-shot features improve cross-subject decoding from 0.41 to 0.49
- Generated data reproduces individual "fingerprints" with above-chance matching accuracy
- Subject embedding approach enables efficient transfer to new subjects

**Low confidence:**
- 320ms receptive field is optimal for capturing relevant MEG dynamics
- Patching does not degrade burst detection capabilities
- Model will generalise to clinical populations or different experimental paradigms

## Next Checks

1. **Receptive field sensitivity analysis:** Systematically vary the receptive field length (e.g., 40, 80, 160 samples) and evaluate impact on burst detection statistics (TDE-HMM analysis) and cross-subject decoding accuracy to determine optimal window length.

2. **Cross-population transfer test:** Fine-tune the pre-trained model on a clinically diverse dataset (e.g., epilepsy patients or elderly subjects) and compare performance to training from scratch to validate foundation model generalisation beyond healthy young adults.

3. **Ablation of patching mechanism:** Train MEG-GPT with patching disabled (Lp=1) and compare computational efficiency, generated data quality (PSD match, burst statistics), and downstream decoding performance to quantify the trade-off between efficiency and temporal precision.