---
ver: rpa2
title: 'AIM: Additional Image Guided Generation of Transferable Adversarial Attacks'
arxiv_id: '2501.01106'
source_url: https://arxiv.org/abs/2501.01106
tags:
- adversarial
- attacks
- image
- targeted
- untargeted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel generative approach for targeted transferable
  adversarial attacks using a semantic injection module (SIM) to incorporate additional
  guiding image semantics. The SIM is a lightweight plug-and-play component that extracts
  and injects semantic information from a guiding image into intermediate layers of
  an adversarial generator through affine transformations.
---

# AIM: Additional Image Guided Generation of Transferable Adversarial Attacks

## Quick Facts
- arXiv ID: 2501.01106
- Source URL: https://arxiv.org/abs/2501.01106
- Authors: Teng Li; Xingjun Ma; Yu-Gang Jiang
- Reference count: 6
- Primary result: Achieves 61.51% average accuracy for targeted cross-architecture attacks vs 5.90% for state-of-the-art methods

## Executive Summary
This paper introduces a novel generative approach for targeted transferable adversarial attacks that incorporates additional semantic guidance from external images. The method addresses a fundamental challenge in adversarial machine learning: improving the transferability of attacks across different model architectures. By introducing a semantic injection module (SIM) that extracts and injects semantic information from guiding images into intermediate layers of an adversarial generator, the approach significantly enhances attack performance, particularly against vision transformer models where it achieves 24.39% attack success rate compared to negligible effectiveness of previous methods.

## Method Summary
The proposed approach leverages a semantic injection module (SIM) as a lightweight plug-and-play component that extracts semantic information from a guiding image and injects it into intermediate layers of an adversarial generator through affine transformations. The SIM is designed to be context-agnostic, allowing it to incorporate additional semantics without requiring specific knowledge about the target model or dataset. The method introduces two new loss formulations: logit contrastive loss and enhanced similarity loss for targeted attacks, and enhanced mid-layer similarity loss for untargeted attacks. These losses work in conjunction with the SIM to guide the generation of adversarial examples that are more likely to transfer successfully to different architectures.

## Key Results
- Achieves 61.51% average accuracy for targeted cross-architecture attacks, significantly outperforming state-of-the-art methods (5.90%)
- Demonstrates 40.34% success rate for untargeted cross-domain attacks
- Shows particular effectiveness against vision transformer models with 24.39% attack success rate
- The semantic injection module provides consistent improvements across various attack scenarios

## Why This Works (Mechanism)
The approach works by incorporating additional semantic information from guiding images into the adversarial generation process. This semantic injection helps the generator create more robust adversarial examples that capture transferable features across different model architectures. The affine transformations applied through the SIM module allow for effective integration of these semantics into intermediate layers, creating perturbations that are more likely to fool diverse target models.

## Foundational Learning
- **Semantic Injection**: The process of incorporating external semantic information into intermediate network layers; needed to provide additional context for generating transferable attacks; quick check: verify the guiding image successfully influences intermediate feature maps
- **Transferable Adversarial Attacks**: Attacks designed to fool models they weren't specifically trained against; needed because real-world scenarios rarely allow direct access to target models; quick check: measure attack success across different architectures
- **Vision Transformer Robustness**: The vulnerability of transformer-based models to adversarial attacks; needed because transformers are increasingly prevalent in vision tasks; quick check: compare attack effectiveness on CNNs vs transformers
- **Logit Contrastive Loss**: A loss function that maximizes the distance between target and non-target class logits; needed to strengthen targeted attack specificity; quick check: verify target logit increases while others decrease
- **Mid-layer Similarity Loss**: A loss that encourages consistency between intermediate representations; needed to improve feature-level transferability; quick check: measure similarity between source and target intermediate features
- **Affine Transformations**: Linear transformations with translation; needed to integrate semantic information while preserving spatial relationships; quick check: verify transformed features maintain semantic structure

## Architecture Onboarding

Component Map:
Input Image -> Adversarial Generator -> Semantic Injection Module (SIM) -> Transformed Features -> Combined Features -> Adversarial Example

Critical Path:
Input Image → Adversarial Generator → SIM → Transformed Features → Combined with Original Features → Final Adversarial Output

Design Tradeoffs:
The method trades computational overhead (SIM module and additional losses) for improved transferability. While the SIM adds complexity, it remains lightweight and plug-and-play. The approach requires guiding images, which adds a dependency but provides significant performance gains.

Failure Signatures:
- Poor transferability when guiding images lack relevant semantic information
- Performance degradation with very dissimilar source and target architectures
- Computational bottlenecks if SIM parameters are not properly constrained
- Reduced effectiveness when target models employ strong defense mechanisms

First 3 Experiments:
1. Measure attack success rate on a single target model architecture with varying guiding image qualities
2. Compare transferability across different model families (CNNs, transformers) with the same source model
3. Ablation study removing SIM module to quantify its contribution to attack performance

## Open Questions the Paper Calls Out
None

## Limitations
- Requires additional guiding images which may not always be available in practical attack scenarios
- Computational overhead from SIM module and additional loss computations could impact real-time deployment
- Evaluation primarily focuses on white-box surrogate models with limited analysis of black-box conditions
- Effectiveness against modern defense mechanisms like adversarial training remains unexplored

## Confidence

High confidence in the technical implementation of the semantic injection module and its integration with existing generative adversarial attack frameworks
Medium confidence in the reported transferability improvements, given that results are primarily demonstrated in controlled experimental settings with specific model architectures
Medium confidence in the generalizability across different attack scenarios, as the study focuses on specific targeted and untargeted attack types

## Next Checks

1. Evaluate the method's performance when guiding images are unavailable or of poor quality, testing the method's robustness to incomplete semantic information
2. Test transferability against adversarially trained models and input preprocessing defenses to assess practical attack viability
3. Conduct large-scale experiments across diverse datasets and model architectures to verify the claimed improvements hold across broader conditions