---
ver: rpa2
title: An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing
  MLLMs' Sentimental Perception Capability
arxiv_id: '2505.16193'
source_url: https://arxiv.org/abs/2505.16193
tags:
- sentiment
- mllms
- shot
- demonstrations
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates configuring in-context learning (ICL)\
  \ demonstrations to enhance Multimodal Large Language Models\u2019 (MLLMs) sentiment\
  \ perception capability for multimodal sentiment analysis (MSA). The authors systematically\
  \ optimize three underexplored demonstration configuration factors\u2014similarity\
  \ measurement, modality presentation, and sentiment distribution\u2014while also\
  \ identifying and mitigating an inherent sentiment prediction bias in MLLMs."
---

# An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability

## Quick Facts
- arXiv ID: 2505.16193
- Source URL: https://arxiv.org/abs/2505.16193
- Reference count: 37
- This paper optimizes in-context learning demonstration configurations to boost MLLMs' sentiment perception, achieving 15.9% higher accuracy than zero-shot baselines.

## Executive Summary
This study investigates how to configure in-context learning demonstrations to enhance Multimodal Large Language Models' (MLLMs) sentiment perception in multimodal sentiment analysis (MSA). The authors systematically optimize three underexplored demonstration configuration factors: similarity measurement (with modality-specific weighting), modality presentation (original vs. converted inputs), and sentiment distribution (protocols to counteract predictive bias). They also identify and mitigate an inherent negative sentiment prediction bias in MLLMs. Their findings show that properly configured ICL can significantly improve MSA performance, achieving 15.9% accuracy gains over zero-shot and 11.2% over random ICL baselines across six datasets.

## Method Summary
The authors propose a comprehensive framework to optimize ICL demonstrations for MSA through three complementary strategies. First, they develop weighted similarity measurement (WIT for post-level, WITA for aspect-level tasks) that accounts for modality relevance through calibrated weights (α:β = 2:8 for WIT; α:β = 7:3, (α+β):γ = 2:8 for WITA). Second, they determine that presenting original image-text pairs (I, T) in demonstrations offers the optimal balance between information richness and complexity, outperforming converted modalities like captions or generated images. Third, they implement sentiment distribution protocols (Unlimited vs. Category Balanced) to counteract the MLLMs' inherent bias against negative sentiment predictions. These strategies are evaluated across six MSA datasets using IDEFICS-9B and Open-Flamingo-9B.

## Key Results
- The three complementary strategies yield average accuracy improvements of 15.9% over zero-shot and 11.2% over random ICL baselines across six MSA datasets
- Weighted similarity measurement (WIT with α:β = 2:8) outperforms both unweighted and unimodal approaches for post-level sentiment analysis
- Original image-text presentation (I, T) achieves mean accuracy of 60.2%, outperforming caption-only (50.4%) and augmented modalities
- Category Balanced sentiment distribution protocol improves negative recall while maintaining overall accuracy on skewed datasets

## Why This Works (Mechanism)

### Mechanism 1: Weighted Similarity Measurement for Demonstration Retrieval
- **Claim:** Retrieving demonstrations based on weighted multimodal similarity improves ICL performance over unweighted or unimodal approaches, provided weights reflect task-specific relevance.
- **Mechanism:** Standard image-text similarity (IT) treats modalities equally, but MSA tasks derive sentiment primarily from text and aspect terms. Weighted strategies (WIT with α:β = 2:8 for post-level; WITA with aspect prioritization for aspect-level) align retrieval relevance with actual predictive signals, yielding higher-quality demonstrations that better activate relevant model capabilities.
- **Core assumption:** The similarity function correlates with demonstration utility—more similar examples provide better inductive signal.
- **Evidence anchors:**
  - [abstract] "refine similarity metrics to account for aspect-specific relevance and modality weighting"
  - [section 4.2.1] WIT strategy achieves peak accuracy at α:β = 2:8 on MVSA-S (57.6) and MVSA-M (66.7); WITA peaks at α:β = 7:3, (α+β):γ = 2:8 for aspect-level tasks
  - [corpus] Related work on retrieval-augmented ICL (arXiv:2505.02087) supports task-aware demonstration selection, though corpus lacks direct replication of weighted MSA similarity.
- **Break condition:** If modality importance shifts (e.g., visual-dominant sentiment datasets), fixed weights may underperform; re-calibration required.

### Mechanism 2: Original Modality Presentation Preserves Task Learning
- **Claim:** Presenting original images and text (I, T) in demonstrations outperforms converted or augmented modalities because it balances information richness with input complexity.
- **Mechanism:** Captioning loses visual nuance; text-to-image generation adds noise. Augmenting inputs with auxiliary modalities (e.g., I, C, T) degrades the "Task Learning" effect—the model's ability to infer input-output mappings—by increasing sequence complexity without proportional signal gain. Original modalities maintain the strongest signal-to-complexity ratio.
- **Core assumption:** ICL's "Task Learning" component relies on concise, directly relevant input-output pairs; complexity impedes mapping inference.
- **Evidence anchors:**
  - [abstract] "determine that original image-text inputs offer the best balance between information richness and complexity"
  - [section 4.2.2] (I, T) achieves mean accuracy 60.2 vs. 55.3 for (I, C) and 50.4 for caption-only (C); "Task Learning" experiments (Figure 5) show accuracy declines as modalities are added
  - [corpus] SMMILE benchmark (arXiv:2506.21355) notes ICL sensitivity to input complexity in medical multimodal tasks, consistent with this finding.
- **Break condition:** If models gain stronger multimodal reasoning or longer context windows, auxiliary modalities may become viable.

### Mechanism 3: Bias Counteraction via Sentiment Distribution Protocols
- **Claim:** MLLMs exhibit an inherent predictive bias against negative sentiment; strategically biasing demonstration sentiment distributions can offset this and improve fairness.
- **Mechanism:** The Category Balanced protocol ensures equal representation across sentiment classes, counteracting the model's tendency to under-predict negative labels (evidenced by low negative recall). This rebalancing is most effective when the test set has skewed distributions favoring positive/neutral classes.
- **Core assumption:** The bias originates from pre-training data curation that filtered negative instances, and can be corrected at inference time via demonstration distribution.
- **Evidence anchors:**
  - [abstract] "A sentimental predictive bias inherent in MLLMs is also discovered and later effectively counteracted"
  - [section 4.2.3] Confusion matrices (Figure 8) show MLLM favors positive/neutral; Category Balanced protocol improves negative recall on Twitter-17; SLR (Same Label Rate) correlates with accuracy
  - [corpus] Limited direct corpus evidence on sentiment-specific bias; related work on ICL sensitivity (arXiv:2510.02480) discusses demonstration manipulation effects broadly.
- **Break condition:** If test set distribution already matches model bias (e.g., predominantly positive), balanced protocols may reduce accuracy; dataset-specific tuning required.

## Foundational Learning

- **Concept: In-Context Learning (ICL) decomposition**
  - **Why needed here:** The paper distinguishes "Task Recognition" (format understanding) from "Task Learning" (mapping inference); optimizing demonstrations requires targeting the latter.
  - **Quick check question:** Can you explain why adding more information (e.g., captions) might hurt ICL performance despite seeming helpful?

- **Concept: Same Label Rate (SLR)**
  - **Why needed here:** SLR quantifies demonstration-test alignment; understanding its correlation with accuracy is essential for distribution protocol selection.
  - **Quick check question:** If SLR-Overall is high but accuracy is low, what might explain the discrepancy?

- **Concept: Multimodal similarity fusion**
  - **Why needed here:** Weighted combination of image, text, and aspect similarities is central to the retrieval strategy; naive summation underperforms.
  - **Quick check question:** Why might text similarity deserve higher weight than image similarity in sentiment analysis?

## Architecture Onboarding

- **Component map:**
  - Retrieval module (computes weighted similarity using CLIP embeddings) -> Presentation module (formats original I, T inputs) -> Distribution controller (applies Unlimited/Category Balanced protocol) -> MLLM backbone (IDEFICS-9B or Open-Flamingo-9B)

- **Critical path:**
  1. Encode test sample (image, text, aspect) via CLIP
  2. Retrieve k demonstrations using weighted similarity (weights: α, β, γ per task type)
  3. Apply distribution protocol (check if test set has rare negative class → use Category Balanced; else Unlimited)
  4. Format prompt + demonstrations + test sample; feed to MLLM
  5. Extract predicted sentiment token

- **Design tradeoffs:**
  - **Retrieval computation vs. accuracy:** Larger support sets improve retrieval quality but increase latency (Table 8: +64ms for 1562 samples vs. 0ms for random)
  - **Information richness vs. complexity:** Original modalities (I, T) optimal; adding C/G degrades Task Learning
  - **Bias mitigation vs. distribution fidelity:** Category Balanced corrects bias but may reduce accuracy on balanced test sets

- **Failure signatures:**
  - **Low negative recall with high positive precision:** Inherent model bias; switch to Category Balanced protocol
  - **Random ICL outperforms similarity-based:** Check weight calibration; may need α:β re-tuning for new datasets
  - **Performance degrades with more shots:** Input length exceeding model context window; reduce k or simplify presentation

- **First 3 experiments:**
  1. **Baseline comparison:** Run zero-shot, random ICL (k=16), and RICES ICL on target MSA dataset; measure accuracy gap.
  2. **Weight sweep:** Grid search α:β ratios (0:10 to 10:10) for WIT on post-level task; identify optimal weighting.
  3. **Distribution protocol test:** Compare Unlimited vs. Category Balanced on dataset with known negative-class scarcity; measure per-class precision/recall shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the optimized ICL strategies transfer effectively to advanced, larger-scale MLLMs (e.g., GPT-4V, Gemini) with different pre-training corpora?
- Basis in paper: [explicit] The Conclusion states a "primary limitation... lies in the range of MLLMs investigated," noting that advanced models were beyond the research scope.
- Why unresolved: The study relies on IDEFICS-9B and Open-Flamingo; larger models may exhibit different sensitivities to demonstration ordering or modality weighting.
- What evidence would resolve it: Replicating the WIT/WITA retrieval and distribution protocols on closed-source or larger open-sourced MLLMs to observe if the 11.2% improvement holds.

### Open Question 2
- Question: To what extent does the pre-training data curation—specifically the filtering of negative instances—cause the observed negative sentiment predictive bias?
- Basis in paper: [explicit] The Discussion hypothesizes the bias "could potentially originate from the curation of the pre-training data" but acknowledges this lies outside the paper's primary scope.
- Why unresolved: The paper mitigates the bias via inference-level distribution protocols but does not verify the root cause within the model's pre-training history.
- What evidence would resolve it: Pre-training MLLMs with balanced emotional datasets and measuring the reduction in the "avoidance of negative predictions" without Category Balanced protocols.

### Open Question 3
- Question: Can the "Task Learning" effect be preserved when incorporating auxiliary modalities (captioned text or generated images) by modifying the attention mechanism?
- Basis in paper: [inferred] The authors find adding auxiliary modalities degrades performance because it complicates input-output mapping ("Task Learning"), but it remains unclear if this is a fundamental limitation of ICL or a model capacity issue.
- Why unresolved: The experiment shows degradation, but does not explore architectural solutions to handle the increased complexity.
- What evidence would resolve it: Experiments using MLLMs with extended context windows or modified attention biases to see if auxiliary data aids rather than hinders mapping.

## Limitations
- The fixed modality weighting schemes (α:β = 2:8 for post-level, 7:3 for aspect-level) may not transfer optimally to datasets with different sentiment-signal distributions or visual-textual balance
- The computational overhead of similarity-based retrieval (64ms per sample for 1562-support set) could limit real-time applicability
- The sentiment bias mitigation strategy assumes the model's pre-training corpus disproportionately lacks negative examples—a claim supported empirically but not through direct analysis of the training data itself

## Confidence
- **High confidence**: The effectiveness of original image-text demonstration presentation over converted modalities; the degradation of performance with increased input complexity (Task Learning mechanism)
- **Medium confidence**: The optimal modality weighting ratios for similarity retrieval; the sentiment distribution protocol's effectiveness in bias mitigation
- **Medium confidence**: The claim that MLLMs exhibit inherent negative sentiment bias requiring correction; the assertion that Category Balanced protocol improves fairness metrics

## Next Checks
1. **Cross-task generalization test**: Apply the three-configuration optimization pipeline to non-MSA multimodal tasks (e.g., VQA, medical image classification) to verify whether the similarity weighting and presentation principles transfer beyond sentiment analysis.

2. **Dynamic weighting calibration**: Implement an adaptive weighting scheme that learns α:β ratios per dataset rather than using fixed values, and compare against the current static approach on datasets with varying visual-textual sentiment salience.

3. **Bias source attribution**: Conduct an ablation study analyzing MLLM outputs on synthetic sentiment datasets with controlled negative-positive ratios to isolate whether the observed bias stems from pre-training data curation versus architecture-specific tendencies.