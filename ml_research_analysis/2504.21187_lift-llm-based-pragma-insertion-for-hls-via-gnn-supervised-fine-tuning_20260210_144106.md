---
ver: rpa2
title: 'LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning'
arxiv_id: '2504.21187'
source_url: https://arxiv.org/abs/2504.21187
tags:
- pragma
- design
- training
- code
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically inserting optimization
  pragmas for High-Level Synthesis (HLS) of C/C++ code to FPGAs. The authors propose
  LIFT, a novel framework that combines the strengths of large language models (LLMs)
  and graph neural networks (GNNs) for this task.
---

# LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning

## Quick Facts
- **arXiv ID:** 2504.21187
- **Source URL:** https://arxiv.org/abs/2504.21187
- **Reference count:** 40
- **Key outcome:** LIFT framework achieves 3.52x, 2.16x, and 66x improvements in design latency over AutoDSE, HARP, and GPT-4o respectively

## Executive Summary
This paper presents LIFT, a novel framework that combines large language models (LLMs) and graph neural networks (GNNs) for automatic optimization pragma insertion in High-Level Synthesis (HLS) of C/C++ code targeting FPGAs. The method fine-tunes an LLM using both autoregressive loss for pragma prediction and a graph-level loss derived from GNN embeddings of compiled code. The graph embeddings capture the structural and semantic effects of pragmas on the underlying microarchitecture, allowing the LLM to predict pragma values that result in semantically similar compiled graphs to ground truth while being guided by performance-based weights to prioritize low-latency designs. Experiments on the HLSyn benchmark demonstrate that LIFT significantly outperforms state-of-the-art methods including AutoDSE, HARP, and GPT-4o.

## Method Summary
LIFT addresses the challenge of automatic optimization pragma insertion for HLS by combining LLM fine-tuning with GNN-based semantic supervision. The framework uses DeepSeek-Coder 7B with LoRA adapters, fine-tuned on the HLSyn dataset using a combined loss function that includes both cross-entropy for pragma prediction and MSE between GNN embeddings of predicted versus ground-truth compiled code. The method incorporates latency-aware data resampling to prioritize low-latency designs during training. Code is compiled to LLVM IR and converted to graphs via ProGraML, with a pre-trained HARP GNN encoder providing the semantic supervision. The framework formulates pragma insertion as an infilling task to leverage bidirectional context, allowing the model to utilize code appearing both before and after pragmas for correct dependency analysis.

## Key Results
- Achieves 3.52x improvement in design latency over AutoDSE on HLSyn benchmark
- Outperforms HARP by 2.16x in latency reduction
- Demonstrates 66x improvement over GPT-4o for pragma insertion tasks
- Shows better generalization to new tool versions compared to prior GNN-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Graph-Semantic Supervision Alignment
Incorporating a GNN loss into LLM fine-tuning forces the model to internalize microarchitectural impacts of pragmas that text-only training fails to capture. The framework compiles LLM-predicted code into graphs and minimizes MSE between predicted and ground-truth graph embeddings, penalizing pragma predictions that are syntactically valid but structurally distant from optimal hardware implementations. This relies on the GNN encoder having learned a representation space where embedding distance correlates with semantic and performance similarity.

### Mechanism 2: Latency-Aware Data Resampling
Biasing the training distribution toward low-latency designs accelerates convergence and improves final quality of results by drowning out noise from invalid or suboptimal configurations. The method transforms raw latency values into inverse weights and resamples the dataset to oversample high-weight (fast) designs while retaining a fraction of low-weight ones, explicitly prioritizing learning from successful optimizations.

### Mechanism 3: Bidirectional Context via Infilling
Formulating pragma insertion as an infilling task allows the model to utilize future context, which is necessary for correct dependency analysis. The model uses Infilling by Language Modeling (ILM) framework, processing the sequence with the full code as conditioning context before predicting pragma values, giving the attention mechanism access to the complete loop structure.

## Foundational Learning

- **High-Level Synthesis (HLS) Pragmas**: Pragmas are control knobs for the hardware compiler. Understanding that `FACTOR=auto{}` is a variable to be solved, not just text to be completed, is the core problem definition. *Quick check:* Can you explain why a loop unrolling factor of 4 might fail to compile while a factor of 2 succeeds? (Hint: Resource constraints/Dependencies).

- **Graph Neural Networks (GNNs) for Code**: The paper relies on the premise that code structure (Control/Data Flow Graphs) conveys "semantic" hardware intent better than tokens. *Quick check:* How does a GNN use edges in a Control Flow Graph (CFG) to aggregate information differently than a Transformer uses positional encoding?

- **LoRA (Low-Rank Adaptation)**: The paper fine-tunes a 7B parameter model efficiently. Understanding LoRA is necessary to replicate the training infrastructure without massive compute. *Quick check:* In LoRA, which weights are frozen, and which are updated during the backpropagation of the GNN-guided loss?

## Architecture Onboarding

- **Component map:** Input C/C++ Code with `auto{}` placeholders -> Latency-weighted resampling (CPU) -> DeepSeek-Coder 7B + LoRA adapters (GPU) -> Compiler Chain (Vitis HLS → LLVM IR → ProGraML Graph) -> Pre-trained HARP GNN Encoder (Frozen) -> Combined Loss (L_CE + L_GNN)

- **Critical path:** The gradient flow from MSE Loss through the differentiable compilation/graph-conversion pipeline or the reinforcement of token probabilities that lead to low graph distance. The efficiency of the graph compilation step during training is a potential bottleneck.

- **Design tradeoffs:** GNN Complexity vs. LLM Speed (graph generation adds overhead), Generalization vs. Overfitting (LLM inherits GNN flaws if supervisor is poor).

- **Failure signatures:** Stagnant Loss (L_CE drops but L_GNN remains high), Tight Clustering (t-SNE shows tight clusters by kernel rather than pragma configuration).

- **First 3 experiments:**
  1. Sanity Check (Ablation): Fine-tune LLM without GNN loss to verify "stagnation" result and establish baseline for semantic gap.
  2. Supervisor Validation: Validate pre-trained HARP GNN can distinguish between optimal and suboptimal graphs in validation set before connecting to LLM.
  3. Tool Version Shift: Train on Vitis 2020.2 data and test on 2021.1 to measure "generalization" claim against GNN-only baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can "negative knowledge" from invalid or failed HLS designs be explicitly leveraged to improve model robustness? The authors note this remains an open problem, as invalid configurations contain rich supervision signals currently lost or merely down-weighted.

- **Open Question 2:** Can the framework generalize to the full complexity of vendor-specific HLS pragmas beyond the simplified Merlin compiler interface? The paper relies on simplified pragmas, and it's unclear if the GNN-based supervision is sufficient for complex syntax and interactions of raw vendor pragmas.

- **Open Question 3:** Does incorporating explicit resource utilization constraints into the loss function improve the balance between latency and area? The current approach optimizes purely for latency without explicit resource constraints, potentially leading to designs that excessively consume specific resources.

## Limitations

- The graph compilation step introduces significant computational overhead during training with no explicit analysis of its impact on training time or memory requirements.
- The quality of the GNN supervisor is critical - if the HARP GNN encoder hasn't truly learned a representation space where distance correlates with hardware performance, the entire semantic supervision mechanism fails.
- The exact impact of the latency-weighted resampling strategy is unclear without knowing specific hyperparameters (λ, γ values), making it difficult to assess whether performance gains stem from better learning or dataset curation.

## Confidence

- **High Confidence:** The LLM-based approach outperforms GPT-4o by 66x in latency reduction, demonstrating clear effectiveness of the core architecture. The infilling formulation and LoRA fine-tuning configuration are well-specified and reproducible.
- **Medium Confidence:** The claimed 3.52x improvement over AutoDSE and 2.16x over HARP requires validation, as these comparisons depend on the quality of baselines and dataset splits. The generalization claim needs explicit testing on new tool versions.
- **Low Confidence:** The exact impact of the latency-weighted resampling strategy is unclear without knowing the specific hyperparameters used, making it difficult to assess whether performance gains stem from better learning or dataset curation.

## Next Checks

1. **GNN Supervisor Validation:** Test the pre-trained HARP GNN on a held-out validation set to verify it can distinguish between optimal and suboptimal pragma configurations before using it to supervise LLM training.

2. **Ablation of GNN Loss:** Train the LLM with L_GNN=0 to establish a baseline and verify the "stagnation" result, directly testing whether GNN supervision is necessary for learning pragma effects.

3. **Tool Version Robustness:** Explicitly measure performance degradation when training on Vitis 2020.2 and testing on 2021.1 to validate the generalization claim against the stated weakness of GNN-only approaches.