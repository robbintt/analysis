---
ver: rpa2
title: Building Models of Neurological Language
arxiv_id: '2506.06208'
source_url: https://arxiv.org/abs/2506.06208
tags:
- language
- clinical
- data
- medical
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report details the development of domain-specific language
  models for neurology, shifting focus from bespoke fine-tuning to leveraging retrieval-augmented
  generation (RAG) and representational models for secure, local deployment. Key contributions
  include the creation of neurology-specific datasets (case reports, QA sets, textbook-derived
  data), tools for multi-word expression extraction, and graph-based analyses of medical
  terminology.
---

# Building Models of Neurological Language

## Quick Facts
- arXiv ID: 2506.06208
- Source URL: https://arxiv.org/abs/2506.06208
- Authors: Henry Watkins
- Reference count: 0
- One-line primary result: Local RAG-enabled Gemma-7b models approach GPT-4 accuracy on neurology QA tasks while summarisation remains challenging.

## Executive Summary
This work presents NeuroBase, a locally deployable large language model for neurology that shifts from fine-tuning-centric approaches to retrieval-augmented generation (RAG) for secure, privacy-preserving deployment. The project creates neurology-specific datasets including case reports, textbook-derived question-answer pairs, and radiology reports, along with tools for multi-word expression extraction and graph-based terminology analysis. Key contributions include Docker containers for local hosting, scripts for data processing and evaluation, and evidence that RAG consistently improves question-answering accuracy while providing minimal benefit for summarization tasks.

## Method Summary
The approach uses Gemma-7b with QLoRA fine-tuning on curated neurology corpora including case reports, radiology reports, and textbook-derived QA pairs, trained on a single RTX 4090. RAG employs BGE embeddings with LangChain vector search, where retrieved passages are prepended to LLM prompts. Textbooks and NICE guidelines are processed via GROBID into JSON format, and MCQA distractors are generated using GPT-4. Evaluation uses MRCPUK and textbook-derived datasets with accuracy metrics for QA and ROUGE-L, BLEU, METEOR for summarization.

## Key Results
- Fine-tuned Gemma-7b with RAG achieved QA accuracy approaching GPT-4 on both MRCPUK and textbook-derived datasets
- RAG consistently improved QA accuracy across model configurations (e.g., 0.855 → 0.958 on TextbookQA)
- For summarization tasks, RAG provided little benefit and fine-tuned models generally underperformed GPT-4 (ROUGE-L 0.26 vs 0.23)
- Graph-based PMI analysis revealed clinically meaningful semantic structure in neurological terminology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG improves question-answering accuracy for domain-specific neurology tasks.
- Mechanism: External knowledge base (textbooks, NICE guidelines) is embedded using BGE model; retrieved passages prepended to LLM prompts ground responses in domain-specific information rather than relying solely on parametric knowledge.
- Core assumption: The retrieval corpus contains relevant information for the query, and embedding similarity correlates with semantic relevance for neurology terminology.
- Evidence anchors:
  - [abstract] "RAG consistently improving QA accuracy"
  - [section 4.2, Table 1] Gemma-7b base improves from 0.855 to 0.958 on TextbookQA with RAG; fine-tuned variants show similar gains
  - [corpus] Related paper "Accurate and Energy Efficient: Local RAG Models Outperform Commercial LLMs" corroborates RAG effectiveness in medical domains (FMR=0.53)
- Break condition: When queries require synthesis across documents rather than retrieval of specific facts; summarisation tasks show RAG provides "little benefit" (Table 2).

### Mechanism 2
- Claim: Small models (7B parameters) with domain-specific fine-tuning can approach larger commercial model performance on neurology QA.
- Mechanism: QLoRA fine-tuning updates only adapter weights and attention parameters on curated neurology corpora (case reports, radiology reports, textbook-derived QA), reducing compute while preserving domain knowledge.
- Core assumption: Domain-specific training data quality and breadth matter more than model scale for targeted tasks.
- Evidence anchors:
  - [abstract] "Fine-tuned Gemma-7b models with RAG achieved strong performance... approaching GPT-4 accuracy"
  - [section 4.1] "Fine-tuning uses the QLoRA method... trains only a small subset of the total model"
  - [corpus] Limited direct corroboration; neighboring papers focus on RAG vs. fine-tuning tradeoffs but not neurology specifically
- Break condition: For summarisation tasks, fine-tuned models "generally underperform compared to GPT-4" (ROUGE-L 0.26 vs. 0.23 for best config).

### Mechanism 3
- Claim: Graph-based analysis of term co-occurrence reveals clinically meaningful semantic structure in neurological text.
- Mechanism: Pointwise Mutual Information (PMI) identifies statistically significant term associations; soft Louvain clustering and stochastic block models detect non-exclusive communities corresponding to pathological/diagnostic groupings.
- Core assumption: High PMI values reflect meaningful clinical relationships rather than documentation conventions or reporting bias.
- Evidence anchors:
  - [section 7.2] Sub-graphs for meningioma, encephalitis, aneurysm show clinically coherent term clusters (e.g., "coiling" prominent in aneurysm network)
  - [section 3] pyMWE toolkit extracts multi-word expressions like "small vessel ischaemia"
  - [corpus] No direct corpus corroboration for this specific graph-based approach in neurology
- Break condition: Non-specific terms like "inflammation" belong to multiple communities; manual validation needed to confirm clinical utility.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture choice for grounding LLM outputs in domain knowledge without requiring massive model retraining.
  - Quick check question: Can you explain why RAG helps QA but not summarisation?

- Concept: **Parameter-Efficient Fine-Tuning (QLoRA/PEFT)**
  - Why needed here: Enables domain adaptation on consumer hardware (single RTX 4090) without full model retraining.
  - Quick check question: What subset of model parameters does QLoRA update?

- Concept: **Pointwise Mutual Information (PMI) for term extraction**
  - Why needed here: Foundation for vocabulary generation and graph construction from unlabeled clinical text.
  - Quick check question: What does high PMI indicate about a word pair?

## Architecture Onboarding

- Component map:
  - Data layer (radiology reports, case reports, textbooks, NICE guidelines) -> GROBID processing -> JSON
  - Embedding service (BGE-large-en-v1.5) -> Docker container
  - LLM backbone (Gemma-7b with QLoRA adapters) -> Hugging Face Transformers
  - RAG API (LangChain vector search + retrieval) -> Docker container
  - Orchestration (bash scripts for training, evaluation, hosting)

- Critical path:
  1. Process raw documents → JSON via GROBID
  2. Generate embeddings → vector store
  3. Fine-tune adapters on domain QA pairs
  4. Deploy RAG API + LLM container
  5. Run evaluation scripts (MRCPUK, TextbookQA datasets)

- Design tradeoffs:
  - Local deployment (security) vs. cloud API convenience
  - Small model efficiency vs. GPT-4 accuracy ceiling
  - RAG retrieval overhead vs. parametric knowledge only

- Failure signatures:
  - RAG provides no improvement: Check embedding quality, chunk size, retrieval top-k
  - Summarisation underperforms: Expected limitation per Table 2; consider larger model or task-specific training
  - Vocabulary extraction noisy: Adjust PMI thresholds, add domain-specific stop words

- First 3 experiments:
  1. Reproduce Table 1 baseline: Run `evaluations.sh` on MRCPUK dataset with/without RAG to validate setup
  2. Test embedding model swap: Replace BGE with domain-specific embedder, measure retrieval precision
  3. Ablate fine-tuning data: Train separate adapters on case reports vs. textbooks only, compare QA accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: Can open-source multimodal architectures effectively integrate free-text clinical records with radiological imaging data to improve phenotypic representations?
  - Basis in paper: Explicit (Section 9: "Future Directions").
  - Why unresolved: The project shifted focus to text-based RAG systems; multimodal integration remains a proposed next step rather than an implemented feature.
  - What evidence: Performance metrics from a joint text-image model compared against text-only baselines on diagnostic tasks.

- **Open Question 2**: What specific architectural or training modifications are required for smaller models (e.g., Gemma-7b) to close the performance gap with GPT-4 on summarization tasks where RAG provided no benefit?
  - Basis in paper: Inferred (Section 4.2).
  - Why unresolved: The results show RAG improves QA but not summarization, indicating the smaller model struggles with synthesis tasks, but the paper offers no solution.
  - What evidence: Ablation studies evaluating alternative fine-tuning objectives (e.g., instruction tuning focused on synthesis) on the case report summarization dataset.

- **Open Question 3**: Do the data-driven terminology ontologies generated via stochastic block models align with established clinical knowledge, and can they generalize to external hospital datasets?
  - Basis in paper: Inferred (Section 7.3).
  - Why unresolved: The graphs reveal "subtler term relationships that merit further clinical investigation," implying clinical validation is pending.
  - What evidence: Qualitative evaluation by neurologists comparing the automated clusters to standard ontologies (e.g., SNOMED-CT) and testing on data from King's College Hospital.

## Limitations

- Model Scale vs. Task Performance: Claims of 7B models approaching GPT-4 accuracy lack direct ablation studies comparing different model sizes; summarization performance (ROUGE-L 0.26 vs 0.23) suggests limited generalization beyond QA tasks.
- Dataset Generalisability: MRCPUK and TextbookQA datasets represent curated question-answer pairs, with real-world applicability to unstructured clinical queries or multimodal cases (imaging + text) remaining untested.
- Embedding Quality Impact: BGE-large-en-v1.5 used without comparison to domain-specific alternatives or ablation studies on embedding dimensionality/chunk size effects on retrieval accuracy.

## Confidence

- **High**: RAG improves QA accuracy when retrieval corpus is relevant; QLoRA fine-tuning on domain data works on consumer hardware
- **Medium**: Small models can approach GPT-4 on targeted QA tasks; Graph-based term analysis reveals clinically meaningful clusters
- **Low**: Summarisation performance with fine-tuned models vs GPT-4 is definitively worse (limited data points); MWE extraction via PMI is optimal without alternative methods tested

## Next Checks

1. **Ablation on Model Scale**: Compare Gemma-7b with RAG against larger open models (e.g., Llama-2-70b) on MRCPUK without RAG to quantify RAG's contribution versus parametric knowledge alone.

2. **Cross-Dataset Generalisation**: Evaluate the fine-tuned model on a held-out subset of case reports (unseen during training) to measure performance decay and identify failure modes in less structured text.

3. **Embedding Model Comparison**: Replace BGE with a domain-specific embedder (e.g., clinical BERT variants) and measure retrieval precision@5 on a manually annotated subset of 50 queries to assess embedding quality impact.