---
ver: rpa2
title: 'From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics'
arxiv_id: '2509.05617'
source_url: https://arxiv.org/abs/2509.05617
tags:
- emotion
- emotional
- lyrics
- emotions
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of multi-label emotional attribution
  of song lyrics, aiming to predict six emotional intensity scores (joy, sadness,
  anger, fear, surprise, and disgust) using large language models (LLMs). A manually
  labeled dataset is constructed using the Mean Opinion Score (MOS) methodology, aggregating
  annotations from multiple human raters to ensure reliable ground-truth labels.
---

# From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics

## Quick Facts
- arXiv ID: 2509.05617
- Source URL: https://arxiv.org/abs/2509.05617
- Reference count: 0
- BERT-based models fine-tuned on manually labeled lyrics significantly outperform zero-shot LLMs like Grok 3 in multi-label emotion estimation

## Executive Summary
This paper presents a benchmark study on multi-label emotion estimation in pop song lyrics, comparing fine-tuned transformer models against zero-shot large language models. The authors construct a manually labeled dataset using Mean Opinion Score methodology and evaluate six emotional intensity scores: joy, sadness, anger, fear, surprise, and disgust. Their experiments demonstrate that fine-tuned BERT-based models achieve significantly lower Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) compared to zero-shot models, establishing a strong baseline for emotion recognition in creative texts and suggesting potential applications in music information retrieval.

## Method Summary
The authors develop a dataset using Mean Opinion Score (MOS) methodology, aggregating annotations from multiple human raters to create reliable ground-truth emotion labels for song lyrics. They evaluate two approaches: publicly available LLMs tested in zero-shot scenarios, and a BERT-based model fine-tuned specifically for multi-label emotion prediction. The evaluation uses standard regression metrics (MAE and RMSE) across six emotions, with results showing fine-tuned models substantially outperform zero-shot approaches.

## Key Results
- Fine-tuned BERT-based models achieve MAE of 0.33 and RMSE of 0.1507, significantly outperforming zero-shot Grok LLM (MAE 0.50, RMSE 0.367)
- Aggregated MAE across all emotions shows clear advantage for fine-tuned models (0.33) over zero-shot approaches (0.50)
- Fear and joy emotions show the largest performance gaps between fine-tuned and zero-shot models, suggesting these emotions may be more challenging for general-purpose LLMs

## Why This Works (Mechanism)
Fine-tuned transformer models leverage domain-specific training on song lyrics, allowing them to capture the unique linguistic patterns, metaphors, and cultural references that characterize emotional expression in music. The MOS methodology ensures high-quality ground truth by aggregating multiple human judgments, providing the model with robust training signals. BERT's bidirectional attention mechanism effectively captures contextual relationships in lyrics that are crucial for emotion recognition, while zero-shot LLMs lack this specialized training and must rely on general language understanding capabilities.

## Foundational Learning
- Mean Opinion Score (MOS) methodology: Essential for establishing reliable ground truth in subjective tasks like emotion labeling; quick check: verify inter-annotator agreement metrics are reported
- Multi-label emotion regression: Critical for capturing the complex, overlapping emotional content in song lyrics; quick check: confirm models can predict multiple emotions simultaneously with independent intensity scores
- BERT transformer architecture: Provides bidirectional context understanding crucial for interpreting lyrical metaphors; quick check: verify attention mechanism is properly configured for the task

## Architecture Onboarding

**Component Map:** Data preprocessing -> BERT fine-tuning -> Emotion score prediction -> MAE/RMSE evaluation

**Critical Path:** Lyrics input → Tokenization → BERT encoder → Dense output layer → Emotion intensity scores → Error calculation

**Design Tradeoffs:** Fine-tuning BERT provides superior performance but requires labeled training data and computational resources, while zero-shot LLMs offer flexibility without training but underperform significantly on specialized tasks.

**Failure Signatures:** High MAE for specific emotions (particularly fear and joy) may indicate model confusion between similar emotional expressions or insufficient training examples for those classes.

**First Experiments:**
1. Compare performance across different transformer architectures (RoBERTa, DeBERTa) to establish whether BERT's advantage is architecture-specific
2. Test different learning rates and batch sizes during fine-tuning to optimize convergence and generalization
3. Evaluate model performance on lyrics from underrepresented genres to assess generalization beyond the training distribution

## Open Questions the Paper Calls Out
### Open Question 1
- Question: To what extent can advanced prompting strategies, such as chain-of-thought reasoning, close the performance gap between zero-shot LLMs and fine-tuned transformer models?
- Basis in paper: The authors state they "plan to investigate the effectiveness of advanced prompting strategies... to enhance zero-shot performance" in the Future Directions section.
- Why unresolved: The current results show fine-tuned models significantly outperform zero-shot models (e.g., Aggregated MAE 0.33 vs 0.50), but the study only utilized standard prompting for the zero-shot condition.
- What evidence would resolve it: Experimental results demonstrating that specific prompting techniques (e.g., chain-of-thought) lower the MAE of zero-shot models to a level statistically indistinguishable from fine-tuned baselines.

### Open Question 2
- Question: Does the integration of audio features with lyrics improve emotion estimation accuracy compared to text-only analysis?
- Basis in paper: The paper concludes by aiming to "investigate... multimodal approaches that integrate both lyrics and audio features."
- Why unresolved: This benchmark focuses exclusively on textual features (lyrics), yet the introduction acknowledges that musical elements like melody and rhythm contribute significantly to emotional perception.
- What evidence would resolve it: A comparative study showing that a multimodal model (audio + lyrics) yields lower Mean Absolute Error (MAE) scores than the text-only BERT/RoBERTa baselines established in this paper.

### Open Question 3
- Question: How robust are the current emotion estimation models when applied to a broader range of languages and lyrical genres outside the current dataset?
- Basis in paper: Future work includes "expanding the dataset to include a broader range of lyrical genres and languages, which may improve model generalization."
- Why unresolved: The current dataset is limited, and the paper notes specific genre correlations (e.g., anger in rap), suggesting that models trained on the current data may not generalize well to underrepresented genres or different languages.
- What evidence would resolve it: Evaluation metrics (MAE/RMSE) demonstrating that models trained on the expanded, multilingual dataset maintain high performance across diverse genres and languages without significant error increases.

## Limitations
- Cultural bias in emotion annotation may affect the generalizability of results across different demographic groups and musical traditions
- Six-emotion framework may not capture the full complexity and nuance of emotional expression in song lyrics
- Zero-shot comparison does not account for architectural differences in how models process musical versus textual information

## Confidence
- Dataset construction methodology: High confidence - MOS approach is well-established and clearly described
- BERT vs LLM performance comparison: Medium confidence - results are statistically significant but may not generalize to other LLM architectures
- Six-emotion framework applicability: Low confidence - may not capture full emotional complexity of song lyrics

## Next Checks
1. Conduct cross-cultural validation by having annotators from different cultural backgrounds label the same subset of lyrics to assess cultural bias in emotion annotations
2. Perform ablation studies varying the number of basic emotions to determine if the six-emotion framework captures sufficient emotional nuance for song lyrics
3. Test additional transformer-based architectures (e.g., RoBERTa, DeBERTa) and smaller language models to establish whether BERT's performance advantage is specific to this architecture or represents a broader trend in fine-tuning effectiveness