---
ver: rpa2
title: 'ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025'
arxiv_id: '2511.16205'
source_url: https://arxiv.org/abs/2511.16205
tags:
- reasoning
- visual
- problem
- problems
- icho
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChemO introduces a benchmark for chemistry olympiad problems by
  reformulating visual-output tasks into machine-readable symbolic formats via Assessment-Equivalent
  Reformulation (AER). To further diagnose model capabilities, it offers Structured
  Visual Enhancement (SVE) that provides symbolic encodings of visual elements like
  molecular structures.
---

# ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025

## Quick Facts
- **arXiv ID**: 2511.16205
- **Source URL**: https://arxiv.org/abs/2511.16205
- **Reference count**: 40
- **Primary result**: Gemini-2.5 Pro + SVE + MAS achieves 93.6/100, surpassing estimated human gold medal threshold

## Executive Summary
ChemLabs on ChemO introduces a benchmark and multi-agent system for evaluating multimodal models on International Chemistry Olympiad (IChO) 2025 problems. The ChemO benchmark reformulates visual-output tasks into machine-readable symbolic formats via Assessment-Equivalent Reformulation (AER), while Structured Visual Enhancement (SVE) provides symbolic encodings of visual elements like molecular structures. ChemLabs employs a hierarchical multi-agent system that decomposes problems into perception, solving, and auditing modules. Experiments demonstrate that SVE combined with ChemLabs yields the best results, with Gemini-2.5 Pro achieving 93.6/100 and outperforming estimated human gold medal performance.

## Method Summary
The approach introduces ChemO, a benchmark for chemistry olympiad problems reformulated via AER to produce machine-readable outputs instead of drawings. ChemLabs is a hierarchical multi-agent system consisting of a Manager Agent that decomposes problems, a Perception Lab that converts images to structured text, five domain-specific Solving Labs, and an Audit Lab with chemical and general auditors for quality control. The system uses zero-shot inference via API, with SVE providing ground-truth symbolic encodings for visual elements to improve performance on structure-heavy problems.

## Key Results
- Gemini-2.5 Pro + SVE + MAS achieves 93.6/100 on ChemO benchmark
- Outperforms estimated human gold medal threshold
- SVE component significantly improves performance on visual perception tasks
- Hierarchical multi-agent architecture shows superior results compared to single-model approaches

## Why This Works (Mechanism)
The hierarchical decomposition allows each agent to specialize in specific tasks, reducing cognitive load and improving accuracy. SVE addresses the visual perception bottleneck by providing symbolic representations that models can process more reliably than raw images. The audit stage with revision capability catches and corrects errors before final submission, mimicking human problem-solving workflows.

## Foundational Learning
- **Assessment-Equivalent Reformulation (AER)**: Converting visual-output tasks to machine-readable formats while preserving assessment criteria. Why needed: Enables automated evaluation of chemistry problems that traditionally require human grading. Quick check: Verify reformulated problems can be automatically scored using provided rubrics.
- **Structured Visual Enhancement (SVE)**: Providing ground-truth symbolic encodings for visual elements like molecular structures. Why needed: Addresses visual perception limitations of current multimodal models. Quick check: Compare scores with and without SVE on structure-heavy problems.
- **Hierarchical multi-agent decomposition**: Breaking problems into perception, solving, and auditing stages. Why needed: Reduces complexity and allows specialized processing for different problem aspects. Quick check: Measure performance gains from adding audit stage versus single-pass approach.

## Architecture Onboarding
**Component map**: MLLM-Only -> MAS (Manager -> Perception Lab -> Solving Lab -> Audit Lab) -> SVE enhancement
**Critical path**: Problem input → Manager Agent → Perception Lab → Domain Solver → Audit Lab → Final Output
**Design tradeoffs**: Single large model vs. specialized agents (accuracy vs. complexity), raw images vs. SVE (native capability vs. enhanced performance)
**Failure signatures**: Visual perception errors on molecular structures, JSON format non-compliance, infinite audit loop cycling
**First experiments**: 1) Baseline MLLM-Only performance, 2) Add MAS layer without SVE, 3) Add SVE to evaluate visual perception improvement

## Open Questions the Paper Calls Out
### Open Question 1
How can MLLMs bridge the visual perception gap to achieve "gold medal" performance without relying on Structured Visual Enhancement (SVE)? The paper identifies visual perception as the "primary bottleneck" and demonstrates that SVE is required to reach 93.6 score, but doesn't propose methods to improve native visual parsing capabilities.

### Open Question 2
Does Assessment-Equivalent Reformulation (AER) fully preserve the spatial-reasoning difficulty of original visual drawing tasks? Converting spatial tasks to symbolic strings fundamentally changes the cognitive modality, raising questions about whether SMILES string generation implies real spatial understanding.

### Open Question 3
Does the hierarchical multi-agent architecture scale efficiently to real-world chemical research? The framework is designed for IChO problem structure and may be overfitted to exam formats rather than unstructured research queries.

## Limitations
- Visual perception remains a bottleneck requiring SVE enhancement for top performance
- Architecture may be overfitted to Olympiad problem structure rather than general chemical reasoning
- Missing prompt templates and implementation details limit faithful reproduction

## Confidence
- **High confidence**: Benchmark creation methodology and general ChemLabs architecture are clearly described
- **Medium confidence**: Decomposition strategy and specialized solvers framework are conceptually clear but depend on missing prompt templates
- **Low confidence**: SVE implementation details, audit loop termination, and LLM-as-a-Judge scoring are underspecified

## Next Checks
1. Compare +SVE vs. MLLM-Only performance on structure-heavy problems (P1, P3) to quantify visual perception improvement
2. Implement schema validation logging for all solver outputs before audit stage to measure format compliance rates
3. Test audit loop termination by setting maximum 2-3 revision iterations and analyzing iteration distribution across problems