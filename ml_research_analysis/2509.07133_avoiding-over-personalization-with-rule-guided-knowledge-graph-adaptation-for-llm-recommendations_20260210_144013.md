---
ver: rpa2
title: Avoiding Over-Personalization with Rule-Guided Knowledge Graph Adaptation for
  LLM Recommendations
arxiv_id: '2509.07133'
source_url: https://arxiv.org/abs/2509.07133
tags:
- user
- adaptation
- knowledge
- personalized
- tomato
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of over-personalization in LLM-based
  recommender systems, which can lead to Personalized Information Environments (PIEs)
  or filter bubbles that limit content diversity. The authors propose a lightweight
  neuro-symbolic framework that adapts user-side Knowledge Graphs (PKGs) at inference
  time to mitigate this issue, without requiring model retraining.
---

# Avoiding Over-Personalization with Rule-Guided Knowledge Graph Adaptation for LLM Recommendations

## Quick Facts
- arXiv ID: 2509.07133
- Source URL: https://arxiv.org/abs/2509.07133
- Reference count: 14
- Primary result: Neuro-symbolic PKG adaptation increases recommendation diversity (32.4% Out-PIE) while reducing Invalid outputs (46.0%) compared to baseline (25.2% Out-PIE, 49.0% Invalid).

## Executive Summary
This paper addresses over-personalization in LLM-based recommender systems that can create filter bubbles or Personalized Information Environments (PIEs). The authors propose a lightweight neuro-symbolic framework that adapts user-side Knowledge Graphs (PKGs) at inference time to mitigate this issue, without requiring model retraining. Their approach involves detecting PIE-inducing feature pairs and applying symbolic adaptation strategies (soft reweighting, hard inversion, or targeted removal of biased triples) to the PKG. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty compared to global adaptation and naive prompt-based methods, while maintaining recommendation quality.

## Method Summary
The framework combines symbolic manipulation of user Personalized Knowledge Graphs (PKGs) with neural LLM inference. PIE detection computes bias scores for feature pairs using user ratings against a neutral baseline (threshold ±0.5). Three adaptation strategies modify PIE-aligned triples: soft adaptation inverts ratings around a neutral midpoint while preserving preference order, hard adaptation assigns extreme opposite ratings, and removal adaptation deletes PIE-aligned triples entirely. The adapted PKG is serialized into structured prompts for the LLM, which generates KG completion recommendations. A client-side feedback-driven algorithm tunes adaptProportion per user to optimize the tradeoff between diversity and relevance, learned through simulation rather than real user feedback.

## Key Results
- Soft adaptation with personalized tuning achieved 32.4% Out-PIE recommendations versus 25.2% for the baseline
- Invalid rate reduced from 49.0% to 46.0% with personalized adaptation
- Prompt-based adaptation (natural language instructions) performed worst: 19.3% Out-PIE, 62.1% Invalid
- Personalized adaptProportion tuning outperformed global adaptation settings across all metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detecting feature-pair biases in a user's Personalized Knowledge Graph (PKG) identifies over-personalization patterns before they reinforce filter bubbles.
- Mechanism: The system computes a bias score $q_{bias}(F_{given}, F_{bias})$ by comparing the user's average rating for items containing both features against a neutral baseline (e.g., 2.5 on a 0–5 scale). When this score exceeds a threshold (±0.5), the feature pair is flagged as PIE-inducing.
- Core assumption: Users rate over-represented feature combinations differently than neutral items, and this deviation reflects algorithmic reinforcement rather than genuine preference.
- Evidence anchors:
  - [abstract] "our method restructures a user's Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs)"
  - [section 2.2] "We define a PIE as a user-specific bias toward certain co-occurring pairs of features... When $q_{bias}$ exceeds a set threshold (e.g., ±0.5), we consider the feature pair to be PIE-inducing"
  - [corpus] Related work (OP-Bench) addresses similar over-personalization detection but in conversational agents; corpus provides limited direct evidence for this specific bias-score formulation.

### Mechanism 2
- Claim: Symbolic adaptation of the PKG at inference time steers LLM recommendations toward diverse content without retraining.
- Mechanism: Three strategies modify PIE-aligned triples: (1) Soft adaptation inverts ratings around a neutral midpoint while preserving preference order; (2) Hard adaptation assigns extreme opposite ratings; (3) Removal adaptation deletes PIE-aligned triples entirely. The adapted PKG is serialized into the LLM prompt.
- Core assumption: The LLM treats the PKG as authoritative context and will adjust its recommendations based on the modified ratings or absence of biased associations.
- Evidence anchors:
  - [abstract] "These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance."
  - [section 2.3] "Soft Adaptation: Adjusts ratings of PIE-aligned items by symmetrically inverting their strength around a neutral midpoint, preserving relative preference order."
  - [corpus] LlamaRec-LKG-RAG and related work show KG-augmented prompts can influence LLM ranking behavior, supporting the general mechanism.

### Mechanism 3
- Claim: Per-user tuning of adaptation intensity (adaptProportion) optimizes the tradeoff between diversity and relevance.
- Mechanism: A feedback-driven algorithm simulates PIE-avoidance scenarios, incrementally adjusting adaptProportion: increase if recommendations remain In-PIE, decrease if they become Invalid, hold if Out-PIE. This converges to a user-specific adaptation strength.
- Core assumption: A single scalar parameter can capture the optimal intervention level for each user's PKG structure and bias profile.
- Evidence anchors:
  - [abstract] "a client-side learning algorithm that optimizes their application per user"
  - [section 2.3] "We learn a personalized adaptProportion for each user using a feedback-driven tuning algorithm... This iterative procedure converges on a personalized adaptation strength tailored to each user's PKG structure and feature biases."
  - [section 3] "soft adaptation with personalized tuning achieves the best overall performance... increases Out-PIE recommendations from 25.2% to 32.4%, while simultaneously reducing the Invalid rate from 49.0% to 46.0%"

## Foundational Learning

- Concept: **Knowledge Graph Completion as a generation task**
  - Why needed here: The LLM is prompted to perform KG completion—recommending a new triple to add to the user's graph—rather than free-form text generation. Understanding this framing is essential for prompt design and evaluation.
  - Quick check question: Can you explain why the system message instructs the model to "recommend a new triple" rather than just "suggest a recipe"?

- Concept: **Feature co-occurrence bias**
  - Why needed here: PIEs are defined by biased feature pairs (e.g., Italian + tomato), not single-feature preferences. Detecting and adapting these pairs is the core intervention.
  - Quick check question: Given a user who rates all Italian dishes highly but rates tomato-based Italian dishes higher than non-tomato ones, how would you compute $q_{bias}$ for the pair (Italian, tomato)?

- Concept: **Neuro-symbolic integration**
  - Why needed here: The framework combines symbolic manipulation (PKG adaptation rules) with neural inference (LLM generation). Understanding where each operates clarifies what is editable vs. opaque.
  - Quick check question: Which parts of this system are interpretable by design, and which rely on the LLM's internal reasoning?

## Architecture Onboarding

- Component map:
  - **PKG Store**: User-specific knowledge graphs containing rated items with features (ingredients, tags)
  - **PIE Detector**: Computes bias scores for feature pairs; flags PIE-inducing pairs when scores exceed threshold
  - **Adaptation Engine**: Applies soft/hard/removal strategies to PIE-aligned triples based on adaptProportion
  - **Prompt Constructor**: Serializes adapted PKG into system message; formats user query with relation type and trait value
  - **Fine-tuned LLM (Qwen3-0.6B with KTO)**: Generates KG completion recommendations
  - **Tuning Algorithm**: Client-side feedback loop to learn personalized adaptProportion values

- Critical path:
  1. User issues query (e.g., "Recommend a recipe with trait hasTag -> Italian")
  2. System checks query against detected PIE-inducing feature pairs
  3. If PIE risk exists, adaptation engine modifies PKG using learned adaptProportion
  4. Prompt constructor builds system + user messages with adapted PKG
  5. LLM generates recommendation triple
  6. Output classified as Out-PIE, In-PIE, or Invalid

- Design tradeoffs:
  - **Soft vs. Hard vs. Removal**: Soft preserves preference order but may be weaker; Hard is aggressive but risks Invalid outputs; Removal is interpretable but loses signal. Paper shows Soft (personalized) achieves best Out-PIE (32.4%) with lowest Invalid (46.0%).
  - **Personalized vs. Global adaptProportion**: Personalized tuning outperforms global settings but requires per-user simulation and storage.
  - **Prompt-based vs. PKG adaptation**: Prompt-based methods (natural language instructions) performed worst (19.3% Out-PIE, 62.1% Invalid)—symbolic structure changes are more effective than textual instructions.

- Failure signatures:
  - **High Invalid rate**: LLM ignores PKG context or query constraints; may indicate over-aggressive adaptation or insufficient fine-tuning.
  - **High In-PIE rate**: Adaptation insufficient; adaptProportion too low or threshold too permissive.
  - **Cold-start users**: No PKG or sparse ratings; bias scores unreliable; consider falling back to global adaptation.

- First 3 experiments:
  1. **Reproduce PIE detection on a sample user**: Select a user PKG, compute $q_{bias}$ for candidate feature pairs, verify threshold behavior matches paper description.
  2. **Compare adaptation strategies on held-out PIEs**: For 5 users, apply soft/hard/removal with fixed adaptProportion=0.5; measure Out-PIE, In-PIE, Invalid proportions.
  3. **Ablate adaptProportion learning**: Compare personalized tuning vs. global adaptProportion on evaluation PIEs; confirm personalization improves Out-PIE without increasing Invalid.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the efficacy of symbolic PKG adaptation scale with larger, state-of-the-art foundation models compared to the lightweight Qwen3-0.6B used in this study?
  - Basis in paper: [inferred] The methodology section specifies the use of a specific, relatively small model (Qwen3-0.6B) for fine-tuning, leaving the impact of model scale on the symbolic adaptation strategies unexplored.
  - Why unresolved: Larger models may exhibit different adherence levels to structured prompts or possess internal biases that are more resistant to KG manipulation than smaller models.
  - What evidence would resolve it: Comparative evaluation of the Soft, Hard, and Removal strategies using larger models (e.g., Llama-3-70B or GPT-4) on the same benchmark.

- **Open Question 2**: Can the bias detection mechanism be extended to identify and mitigate "PIEs" arising from complex, higher-order correlations involving more than two features?
  - Basis in paper: [inferred] Section 2.2 explicitly defines a PIE strictly as a "bias toward certain co-occurring pairs of features" ($F_{given}, F_{bias}$).
  - Why unresolved: Real-world filter bubbles are often driven by multi-faceted interactions (e.g., Italian + Tomato + Spicy + Dinner), which the current pairwise formalization may fail to capture or oversimplify.
  - What evidence would resolve it: A reformulation of the bias score $q_{bias}$ to handle n-ary feature sets and experimental results showing successful mitigation of these complex bubbles.

- **Open Question 3**: Does the client-side learning algorithm for tuning `adaptProportion` maintain robustness and convergence when exposed to noisy, real-time user feedback as opposed to synthetic simulation?
  - Basis in paper: [inferred] Section 2.3 notes that the tuning algorithm "simulates PIE-avoidance scenarios using synthetic data points" rather than actual user interaction logs.
  - Why unresolved: Synthetic data may lack the noise, contradiction, and sparsity inherent in real user feedback, potentially masking failure modes in the online learning process.
  - What evidence would resolve it: Results from a user study or online A/B test where `adaptProportion` is updated dynamically based on live user actions.

## Limitations
- **Unknown implementation details**: KTO fine-tuning hyperparameters, adaptProportion tuning algorithm initialization and convergence criteria, and PIE pair selection method are not fully specified.
- **Synthetic data dependency**: adaptProportion tuning relies on simulated scenarios rather than real user feedback, potentially limiting real-world robustness.
- **Cold-start vulnerability**: The approach requires dense user PKGs to compute reliable bias scores, with no clear fallback for new users.

## Confidence
- **High confidence** in the general claim that symbolic PKG adaptation can increase recommendation diversity (Out-PIE) while maintaining quality, as supported by clear metric improvements (32.4% vs 25.2% baseline Out-PIE) and multiple ablation studies.
- **Medium confidence** in the specific mechanism of PIE detection via q_bias thresholds (±0.5), as the formula is specified but empirical justification for the threshold choice is limited to domain intuition rather than cross-validated tuning.
- **Low confidence** in the reproducibility of personalized adaptProportion tuning without access to the exact algorithm implementation, initialization, or synthetic data generation process.

## Next Checks
1. **Reproduce PIE detection thresholds**: Using a sample user PKG from the Food.com dataset, manually compute q_bias for several candidate feature pairs and verify that the threshold (±0.5) correctly identifies PIE-inducing pairs as described in the paper.
2. **Ablate the adaptation strategies**: Implement soft, hard, and removal adaptation with a fixed adaptProportion (e.g., 0.5) on held-out PIE pairs from 5 users; measure and compare Out-PIE, In-PIE, and Invalid rates to confirm the relative effectiveness reported in the paper.
3. **Test adaptProportion personalization**: Compare the full personalized tuning algorithm against a global adaptProportion setting on the evaluation set; measure whether personalization consistently improves Out-PIE without increasing Invalid rates across users.