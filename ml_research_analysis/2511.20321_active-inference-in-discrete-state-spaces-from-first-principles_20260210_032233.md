---
ver: rpa2
title: Active Inference in Discrete State Spaces from First Principles
arxiv_id: '2511.20321'
source_url: https://arxiv.org/abs/2511.20321
tags:
- energy
- free
- divergence
- inference
- variational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding active inference
  in discrete state spaces without relying on the Free Energy Principle. The core
  method idea involves formulating the optimizations required for active inference
  as constrained divergence minimization problems solvable by standard mean field
  methods.
---

# Active Inference in Discrete State Spaces from First Principles

## Quick Facts
- arXiv ID: 2511.20321
- Source URL: https://arxiv.org/abs/2511.20321
- Authors: Patrick Kenny
- Reference count: 40
- One-line primary result: Provides a self-contained and mathematically rigorous account of active inference in discrete state spaces, treating the perception/action cycle in a unified way by optimizing a single Kullback-Leibler divergence criterion rather than separate criteria for perception and action.

## Executive Summary
This paper presents a mathematically rigorous derivation of active inference in discrete state spaces from first principles, without relying on the Free Energy Principle. The author formulates the perception and action optimizations as constrained divergence minimization problems that can be solved using standard mean field methods. The key insight is that when modeling perception, the proposed divergence criterion coincides with variational free energy, while for action it differs from expected free energy functionals by an entropy regularizer. This unified approach provides a principled foundation for understanding active inference that is both self-contained and mathematically transparent.

## Method Summary
The paper develops active inference in discrete state spaces by treating perception and action as optimization problems over a single Kullback-Leibler divergence criterion. The method formulates these optimizations as constrained divergence minimization problems solvable by standard mean field methods. For perception, this approach recovers variational free energy, while for action it yields a formulation that differs from expected free energy by an entropy regularizer. The paper also demonstrates how these mean field methods apply to Bayesian learning of Hidden Markov Model parameters and updating beliefs about policies, providing a unified treatment of the perception/action cycle.

## Key Results
- Perception/action divergence criterion coincides with variational free energy when used to model perception
- Action formulation differs from expected free energy functionals by an entropy regularizer
- Mean field methods successfully apply to Bayesian learning of Hidden Markov Model parameters and policy updates

## Why This Works (Mechanism)
The approach works by unifying the perception and action components of active inference through a single optimization criterion based on Kullback-Leibler divergence. By formulating both perception and action as constrained divergence minimization problems, the method leverages well-established mean field techniques to solve these optimizations efficiently. The mathematical rigor ensures that the resulting formulations are consistent with established principles of variational inference while providing new insights into the relationship between perception and action in active inference.

## Foundational Learning
- Kullback-Leibler divergence: Fundamental measure of difference between probability distributions; needed to formulate the unified optimization criterion
- Mean field methods: Approximation techniques for complex joint distributions; needed to solve the constrained optimization problems efficiently
- Variational free energy: Information-theoretic quantity that balances accuracy and complexity; needed to understand the perception component
- Hidden Markov Models: Probabilistic models for sequential data; needed to demonstrate the application to policy learning
- Entropy regularizers: Terms that promote uncertainty; needed to understand the difference between action and perception formulations

## Architecture Onboarding
**Component map:** Perception module -> Action module -> Policy update -> Parameter learning (all unified through KL divergence optimization)

**Critical path:** KL divergence minimization -> Mean field approximation -> Parameter updates -> Policy selection

**Design tradeoffs:** The unified KL divergence approach trades some implementation simplicity for mathematical rigor and conceptual clarity. The mean field approximation introduces computational efficiency at the cost of some approximation error.

**Failure signatures:** Potential failure modes include poor convergence of mean field approximations in highly coupled systems, breakdown of the entropy regularizer in high-noise environments, and computational intractability for very large state spaces.

**First experiments:** 1) Verify perception module recovers variational free energy on simple HMMs; 2) Test action module against standard expected free energy on discrete MDPs; 3) Validate policy updates on benchmark sequential decision tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses exclusively on discrete state spaces, limiting applicability to continuous domains
- Lacks empirical validation or experiments demonstrating practical utility
- Relationship to existing active inference implementations remains somewhat abstract

## Confidence
High: Mathematical derivations following standard variational inference principles
Medium: Claims about relationship to existing active inference methods
Low: Practical performance and empirical validation (not provided)

## Next Checks
1) Conduct empirical comparisons between this approach and standard active inference implementations on benchmark discrete decision-making tasks
2) Extend the mathematical framework to continuous state spaces and compare with existing continuous active inference methods
3) Investigate computational efficiency of the proposed mean field methods compared to traditional active inference implementations, particularly for larger state spaces