---
ver: rpa2
title: 'Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework
  and Benchmark'
arxiv_id: '2508.19005'
source_url: https://arxiv.org/abs/2508.19005
tags:
- must
- agent
- task
- your
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Experience-driven Lifelong Learning (ELL),
  a framework for building self-evolving AI agents that learn continuously from real-world
  interactions. The ELL framework consists of four core principles: Experience Exploration,
  Long-term Memory, Skill Learning, and Knowledge Internalization.'
---

# Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark

## Quick Facts
- arXiv ID: 2508.19005
- Source URL: https://arxiv.org/abs/2508.19005
- Reference count: 40
- Primary result: Introduces ELL framework and StuLife benchmark; best model achieves only 17.9/100, revealing vast gap toward AGI

## Executive Summary
This paper introduces Experience-driven Lifelong Learning (ELL), a framework for building AI agents that continuously learn and evolve from real-world interactions. The framework consists of four core principles: Experience Exploration, Long-term Memory, Skill Learning, and Knowledge Internalization. To evaluate ELL agents, the authors create StuLife, a comprehensive benchmark simulating a student's college journey across three phases and ten detailed scenarios. The benchmark is designed to test agents' abilities in long-term memory retention, proactive behavior, skill transfer, and autonomous decision-making. Experiments show that even the best model, GPT-5, achieves only 17.9/100 on the benchmark, revealing a vast gap toward artificial general intelligence.

## Method Summary
The ELL framework models agents operating in a Partially Observable Markov Decision Process (POMDP) environment, where they interact, abstract experiences into structured knowledge, and use this knowledge to condition future behavior. The StuLife benchmark provides a stateful simulation of college life with 1,284 tasks across 10 scenarios. Agents are evaluated using the StuGPA metric (0-100) composed of Exam Performance (50%), Class Performance (30%), and Campus Daily Life (20%), along with auxiliary metrics for memory retention and proactive behavior. The evaluation uses context engineering approaches including proactive prompts, skill guides, and memory-augmented systems to elicit better performance from stateless LLMs.

## Key Results
- Best-performing model (GPT-5) achieves only 17.9/100 on StuLife benchmark
- All-in-One context engineering approach achieves highest StuGPA (21.07) by combining proactive, skill, and memory components
- Current agents fail in long-term memory retention and self-motivated behavior
- Significant performance gap remains between current LLMs and artificial general intelligence

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-to-Knowledge Abstraction Loop
The framework posits that self-evolution occurs by converting raw interaction trajectories (ξ) into structured, reusable knowledge (K) through a formal abstraction function (Φ_learn), rather than relying solely on static pre-training. The agent interacts with environment E to generate trajectory ξ. The function Φ_learn processes ξ to perform operations (Add, Update, Delete, Combine) on the knowledge base K. This knowledge conditions the policy π for future actions: a_t = π(o_t|K_t). Core assumption: recurring patterns in interaction trajectories can be reliably abstracted into "Skills" (procedural) and "Memory" (episodic/declarative) that generalize to future tasks.

### Mechanism 2: Separation of Memory and Skills in Knowledge Structuring
Effective lifelong learning requires explicitly separating the knowledge base K into "Memory" (episodic/declarative facts) and "Skills" (procedural heuristics), allowing for distinct management strategies. Definition 4 formalizes Knowledge K=(M, F). M stores trajectory, declarative, and structural knowledge. F stores procedural, meta-, and heuristic knowledge. This separation allows the agent to "internalize" frequent procedures into skills while retaining episodic details as memory. Core assumption: distinct storage mechanisms improve retrieval efficiency and reduce catastrophic forgetting compared to a monolithic context window.

### Mechanism 3: Integrated Context Engineering for Proactive Agency
In stateless LLMs, proactive, long-term behavior is best elicited by combining time-aware prompting ("Proactive"), structured procedural guidance ("Skill"), and structured memory retrieval ("Memory"), rather than any single intervention. The "All-in-One" prompt integrates three distinct strategies: (1) Proactive prompts enforce time-checking loops; (2) Skill prompts provide procedural recipes; (3) Memory systems (e.g., MemGPT) manage long-term state. This combination yielded the highest StuGPA (21.07) in experiments. Core assumption: current LLMs lack the intrinsic metacognition to juggle "when," "how," and "what" without explicit, structured external scaffolding.

## Foundational Learning

- **Concept: POMDP (Partially Observable Markov Decision Process)**
  - Why needed here: The paper models the environment E as a POMDP (Definition 1). Understanding this is prerequisite to understanding why agents require "Long-term Memory" (to handle partial observability o_t) and "Experience Exploration" (to learn transition dynamics T).
  - Quick check question: Can you explain why an agent in a POMDP cannot rely solely on the current observation o_t to make optimal decisions?

- **Concept: Catastrophic Forgetting**
  - Why needed here: This is the core problem the ELL framework attempts to solve. The benchmark (StuLife) specifically measures this via the Long-Term Retention Rate (LTRR). Recognizing this phenomenon explains the necessity of the "Knowledge Validation" step in the framework.
  - Quick check question: In the context of sequential tasks, what happens to a model's performance on Task A after it is trained/finetuned on Task B without mitigation strategies?

- **Concept: Intrinsic Motivation / Proactive Behavior**
  - Why needed here: The paper identifies the shift "From Passive to Proactive" as a key paradigm. Unlike standard RL which relies on external rewards, ELL agents must generate goals internally.
  - Quick check question: How does the "Proactive Initiative Score" (PIS) metric differ from standard task success rates in evaluating agent autonomy?

## Architecture Onboarding

- **Component map:** Environment (StuLife) -> Agent Core (LLM) -> Knowledge System (K) -> Context Manager -> Policy π

- **Critical path:** The flow is defined in Definition 6:
  1. Interaction: Agent generates ξ using current policy π
  2. Abstraction: Φ_learn updates Knowledge K based on ξ
  3. Validation: Performance gain V is checked to ensure positive transfer
  4. Conditioning: Updated K conditions the next policy iteration

- **Design tradeoffs:**
  - Vanilla RAG vs. Structured Memory: Table 5 shows Vanilla RAG actually hurts performance (StuGPA 10.98) due to noise, whereas structured systems (MemGPT) help. Simpler retrieval is not always better.
  - StuLife Simulation vs. Real-World: The benchmark uses deterministic scripts for logic (e.g., pathfinding) to ensure reproducibility, trading off the randomness of the real world for rigorous evaluation.

- **Failure signatures:**
  - Low PIS (Proactive Initiative Score): Agent ignores time cues (e.g., "It is 8:00" triggers no action)
  - Low LTRR (Long-Term Retention Rate): Agent fails exams despite attending classes, indicating failure to persist "key knowledge points" to K
  - Context Collapse: Agent repeats actions or hallucinates tool calls when K becomes too large or unstructured (Case 3 in Appendix E)

- **First 3 experiments:**
  1. Baseline Run: Run a stateless LLM (e.g., GPT-4o/GPT-5) on StuLife with "Vanilla Prompt" to establish the lower bound of memory/proactive failure (expect StuGPA < 18)
  2. Memory Ablation: Implement the "All-in-One" prompt with a simple RAG system vs. MemGPT. Compare LTRR scores to measure the impact of noise vs. structure in retrieval
  3. Skill Extraction Test: Task the agent with a recurring complex task (e.g., Course Selection). Measure if the "AvgTurns" decreases over multiple semesters, indicating successful Skill Abstraction and reuse

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can ELL frameworks support dynamic and flexible rule evolution at runtime?
- **Basis in paper:** The authors state that future development requires "Dynamic and Flexible Rule Evolution" where rules and policies change during execution to prevent agents from overfitting to static conditions.
- **Why unresolved:** Current benchmarks operate under deterministic, fixed rules; introducing runtime changes complicates the stability-plasticity dilemma.
- **What evidence would resolve it:** A benchmark implementation where agents successfully adapt to mid-simulation policy changes (e.g., shifting graduation requirements) without catastrophic forgetting.

### Open Question 2
- **Question:** What is the optimal granularity for skill abstraction in self-evolving agents?
- **Basis in paper:** The paper highlights the challenge of "Skill Abstraction and Management," asking if a skill should represent a low-level action or a high-level strategy and how to define the "right granularity."
- **Why unresolved:** There is no consensus on how to reliably extract, validate, and organize skills of varying complexity from interaction trajectories.
- **What evidence would resolve it:** An automated method for detecting the correct skill "life cycle" (acquisition vs. pruning) that maximizes transfer learning efficiency in new tasks.

### Open Question 3
- **Question:** How can a general-purpose framework be built to adapt the StuLife paradigm to other domains?
- **Basis in paper:** The paper calls for the "Development of a General-Purpose Benchmarking Framework" to move beyond student life simulation to domains like workplace onboarding or healthcare.
- **Why unresolved:** The current StuLife benchmark is highly specific to a student narrative; a modular framework for rapid adaptation does not yet exist.
- **What evidence would resolve it:** A plug-and-play system where domain-specific modules can be swapped while maintaining standardized ELL evaluation metrics.

## Limitations
- The benchmark relies heavily on deterministic simulation scripts for tasks like pathfinding and course popularity, which may not capture the full complexity and stochasticity of real-world environments
- The context engineering approach shows significant performance improvements but may be computationally expensive and difficult to scale
- The framework assumes the ability to reliably abstract generalizable patterns from interaction trajectories, but this may be challenging with noisy or ambiguous real-world data

## Confidence
- **High Confidence:** The fundamental observation that current LLMs struggle with long-term memory retention (low LTRR scores) and proactive behavior is well-supported by experimental results
- **Medium Confidence:** The effectiveness of the trajectory-to-knowledge abstraction mechanism is theoretically sound but would benefit from more diverse real-world validation
- **Medium Confidence:** The separation of memory and skills provides clear theoretical advantages, though practical implementation challenges remain

## Next Checks
1. Test the ELL framework on a real-world dataset with inherent noise and uncertainty to evaluate robustness beyond the deterministic StuLife benchmark
2. Conduct ablation studies specifically isolating the impact of different context engineering components (Proactive, Skill, Memory) on long-horizon task performance
3. Implement the framework in a multi-agent environment where agents must collaborate, testing the scalability of the knowledge abstraction and sharing mechanisms