---
ver: rpa2
title: 'RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing'
arxiv_id: '2602.00296'
source_url: https://arxiv.org/abs/2602.00296
tags:
- retrieval
- query
- graphrag
- corpus
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAGRouter-Bench is the first dataset and benchmark for adaptive
  RAG routing, systematically evaluating five RAG paradigms across 7,727 queries and
  21,460 documents spanning four domains. It characterizes corpora with fine-grained
  structural and semantic metrics and adopts unified evaluation for generation quality
  and resource consumption.
---

# RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing

## Quick Facts
- arXiv ID: 2602.00296
- Source URL: https://arxiv.org/abs/2602.00296
- Reference count: 40
- Key outcome: First dataset/benchmark evaluating 5 RAG paradigms across 7,727 queries and 21,460 documents spanning 4 domains, showing no single paradigm universally dominates.

## Executive Summary
RAGRouter-Bench introduces a comprehensive dataset and benchmark for evaluating adaptive RAG routing across diverse query-corpus combinations. The benchmark characterizes four domain corpora with fine-grained structural and semantic metrics, then systematically evaluates five RAG paradigms (LLM-only, NaiveRAG, GraphRAG, HybridRAG, IterativeRAG) across 7,727 queries. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that paradigm effectiveness depends jointly on query characteristics and corpus properties, establishing the necessity of routing-aware evaluation for next-generation RAG systems.

## Method Summary
The benchmark constructs four domain corpora (Wikipedia, Literature, Legal, Medical) through sliding-window chunking (512 tokens, 100 overlap) and builds both vector indices (text-embedding-3-small) and knowledge graphs (LLM-based triplet extraction with DeepSeek-V3). It generates 7,727 queries across three types (factual 30%, reasoning 52.9%, summary 17.1%) using LLM with verify-then-filter validation. Five RAG paradigms are evaluated using unified hyperparameters (8k token budget, 0.3 generation temperature) with dual-axis metrics: generation quality (Semantic F1, Coverage, Faithfulness, LLM-as-Judge) and resource efficiency (token consumption). Corpus characterization employs structural metrics (LCC Ratio, Density, Clustering Coefficient) and semantic metrics (Intrinsic Dimensionality, Dispersion, Hubness).

## Key Results
- No single RAG paradigm universally dominates across all query-corpus combinations.
- Paradigm applicability is strongly shaped by query-corpus interactions rather than query complexity alone.
- More complex methods (GraphRAG, IterativeRAG) do not guarantee better effectiveness-efficiency trade-offs.
- Corpus structure (connectivity, density) constrains GraphRAG effectiveness, while semantic space quality determines NaiveRAG reliability.

## Why This Works (Mechanism)

### Mechanism 1: Query-Corpus Compatibility Drives Paradigm Selection
The optimal RAG paradigm depends jointly on query characteristics and corpus properties, not query complexity alone. Query type interacts with corpus structure and semantics to determine which retrieval strategy can successfully locate and synthesize relevant information.

### Mechanism 2: Corpus Structure Constrains Graph-Based Retrieval Effectiveness
GraphRAG performance depends on knowledge graph topology—specifically connectivity, relation diversity, and clustering. High relation diversity and explicit entity links enable precise graph traversal for multi-hop reasoning.

### Mechanism 3: Semantic Space Quality Determines Vector Retrieval Reliability
High intrinsic dimensionality, low dispersion, and high hubness degrade vector-based retrieval, creating a ceiling for NaiveRAG effectiveness. Semantic crowding and hub interference reduce the discriminability of distance-based similarity.

## Foundational Learning

- **RAG Paradigms (NaiveRAG, GraphRAG, HybridRAG, IterativeRAG)**: Understanding their retrieval mechanisms is essential for interpreting routing decisions.
  - Quick check: Can you explain why HybridRAG might fail when both component retrievers return poor results?

- **Knowledge Graph Construction and Metrics (LCC Ratio, Clustering Coefficient, Hubness)**: Corpus characterization uses graph-theoretic and embedding-space metrics; these quantify retrieval constraints.
  - Quick check: What does a low LCC Ratio indicate about multi-hop reasoning feasibility?

- **Effectiveness-Efficiency Trade-offs in Evaluation**: The benchmark explicitly measures both quality and token cost; practical deployment requires balancing these.
  - Quick check: When might IterativeRAG's higher retrieval cost be justified despite lower generation cost?

## Architecture Onboarding

- **Component map**: Data layer (4 corpora → chunking → vector indexing + knowledge graph) → Query layer (3 query types via LLM validation) → Retrieval layer (2 base retrievers → 5 paradigm compositions) → Evaluation layer (dual-axis metrics + token cost)

- **Critical path**: Corpus preprocessing must complete before retrieval evaluation; query validation gates benchmark quality; paradigm instantiation requires unified hyperparameters for fair comparison

- **Design tradeoffs**: Chunking strategy balances granularity vs. context preservation; LLM-based KG construction ensures determinism but may miss implicit relations; LLM-as-Judge adds expense but provides human-aligned correctness

- **Failure signatures**: Paradigm conflict (same query type shows >30% accuracy variance across corpora); Graph over-retrieval (Avg-Ctx >150k tokens); Iterative amplification (sub-queries inherit errors)

- **First 3 experiments**:
  1. Baseline establishment: Run all 5 paradigms on MuSiQue with fixed query distribution
  2. Corpus variation: Compare paradigm performance across all 4 corpora using same query set
  3. Query-type isolation: Evaluate each paradigm on each query type separately within one corpus

## Open Questions the Paper Calls Out

### Open Question 1
How can we build a practical adaptive router that dynamically selects RAG paradigms based on query-corpus characteristics identified in this benchmark? The paper characterizes which paradigms work for which query-corpus combinations but does not propose or train a predictive routing mechanism.

### Open Question 2
How well do routing decisions transfer to real-world query distributions that contain noise, ambiguity, and adversarial inputs? All 7,727 queries were generated via a controlled LLM pipeline that may not reflect authentic user queries.

### Open Question 3
How can the cost-performance trade-off be formalized and optimized for domain-specific deployment constraints? The paper reports token costs but treats cost as a post-hoc observation without formal utility function optimization.

### Open Question 4
Are the corpus metrics (intrinsic dimensionality, hubness, clustering coefficient) sufficient predictors of paradigm performance, or are there latent factors not captured? The paper demonstrates correlation but not causation in metric-paradigm relationships.

## Limitations
- Benchmark generalizability constrained by four curated English-language corpora and relatively small query sets per domain
- LLM-based validation pipeline may not capture all types of factual errors, particularly in specialized domains
- Token cost measurements assume fixed hardware configurations and don't account for API pricing variability

## Confidence

- **High confidence**: Query-corpus interaction effects, corpus characterization methodology, effectiveness-efficiency trade-off observations
- **Medium confidence**: Specific numerical thresholds (30% accuracy variance), corpus metric interpretations (LCC Ratio, hubness impact)
- **Low confidence**: Universal routing rules from limited corpus diversity, extrapolation to non-English or specialized technical domains

## Next Checks
1. Test paradigm routing decisions on an additional corpus from a different domain (e.g., scientific literature) to verify query-corpus interaction patterns
2. Compare LLM-as-Judge outputs with human expert annotations on a stratified sample of 100+ queries to quantify alignment
3. Measure retrieval and generation latency under varying hardware configurations to validate reported token cost metrics reflect real-world deployment scenarios