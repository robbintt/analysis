---
ver: rpa2
title: Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers
arxiv_id: '2506.05038'
source_url: https://arxiv.org/abs/2506.05038
tags:
- question
- llms
- hecker
- target
- ar-c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AR-Checker, an automatic robustness stress testing
  framework for LLMs as mathematical problem solvers. The framework dynamically generates
  problem variants via iterative rewriting and verification using multiple parallel
  streams, maintaining semantic meaning while aiming to cause LLM failure.
---

# Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers

## Quick Facts
- arXiv ID: 2506.05038
- Source URL: https://arxiv.org/abs/2506.05038
- Reference count: 26
- Primary result: AR-Checker framework achieves significant accuracy drops (48.04% and 31.35% averaged decrease) on GSM8K and MATH-500 benchmarks across various LLMs

## Executive Summary
AR-Checker is an automatic robustness stress testing framework designed to evaluate and improve the resilience of Large Language Models (LLMs) when solving mathematical problems. The framework generates semantically equivalent problem variants through iterative rewriting and verification using multiple parallel streams, aiming to cause LLM failures while maintaining the original problem's meaning. Experiments demonstrate substantial accuracy drops across different LLMs, with effectiveness extending beyond mathematical benchmarks to general knowledge tasks like MMLU and CommonsenseQA.

## Method Summary
The AR-Checker framework operates through a multi-stream iterative process that dynamically generates problem variants. It employs multiple parallel streams to rewrite mathematical problems while preserving semantic meaning through LLM-based verification. The system continuously generates and validates new problem variants until the target LLM fails on them. This dynamic approach contrasts with static robustness benchmarks and scales according to the capabilities of both the rewriting mechanism and the target model. The framework was tested on GSM8K and MATH-500 benchmarks, showing significant effectiveness in reducing LLM accuracy.

## Key Results
- AR-Checker achieves 48.04% average accuracy drop on GSM8K benchmark across tested LLMs
- AR-Checker achieves 31.35% average accuracy drop on MATH-500 benchmark across tested LLMs
- Framework demonstrates good generalization to non-mathematical benchmarks including MMLU, MMLU-Pro, and CommonsenseQA

## Why This Works (Mechanism)
The framework's effectiveness stems from its dynamic problem generation approach that systematically explores the space of semantically equivalent mathematical problems. By maintaining semantic meaning while varying problem structure and presentation, AR-Checker identifies specific failure modes in LLM reasoning. The parallel stream architecture enables efficient exploration of the problem space, while the verification step ensures generated variants remain meaningful test cases rather than exploiting arbitrary prompt engineering tricks.

## Foundational Learning

**Semantic Equivalence Verification**: LLM-based judgment of whether rewritten problems maintain original meaning
- Why needed: Ensures stress tests target actual mathematical reasoning rather than semantic drift
- Quick check: Compare LLM judgment with human expert validation on sample problems

**Iterative Rewriting**: Multiple rounds of problem transformation with validation
- Why needed: Allows systematic exploration of problem space variations
- Quick check: Track rewriting success rate and failure patterns across iterations

**Multi-stream Parallel Processing**: Simultaneous generation of problem variants
- Why needed: Increases coverage of potential failure modes
- Quick check: Measure diversity of generated problems across streams

## Architecture Onboarding

**Component Map**: Problem Input -> Multiple Parallel Streams -> Iterative Rewriting -> Semantic Verification -> Output Variants -> LLM Testing

**Critical Path**: The core workflow follows: original problem → parallel stream generation → iterative rewriting → semantic verification → LLM evaluation → accuracy measurement

**Design Tradeoffs**: The framework balances between generating diverse problem variants and maintaining semantic equivalence. More aggressive rewriting increases failure rates but risks semantic drift. The verification step adds computational overhead but ensures meaningful stress tests.

**Failure Signatures**: Common failure modes include arithmetic errors under semantic variation, sensitivity to problem structure changes, and brittleness to word order modifications. The framework specifically targets these through systematic variation of problem presentation.

**First Experiments**:
1. Run AR-Checker on a single stream with GSM8K to establish baseline effectiveness
2. Compare results with static robustness benchmarks to validate dynamic advantage
3. Test framework on a simple LLM (e.g., GPT-3.5) before scaling to more capable models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited primarily to GSM8K and MATH-500 benchmarks with brief generalization tests
- Semantic equivalence verification relies on LLM-based judgment, potentially introducing bias
- Dynamic generation raises questions about whether failures expose true model weaknesses or prompt engineering vulnerabilities
- 2,000 generated samples per stream may not capture full space of problem variants

## Confidence
- Effectiveness of AR-Checker: Medium - significant accuracy drops are compelling but evaluation framework reliability depends on verification quality
- Generalizability: Low-Medium - limited evidence beyond accuracy metrics on three additional benchmarks
- Scalability: Medium - demonstrated empirically but relationship appears linear without clear scaling laws

## Next Checks
1. Implement human-in-the-loop validation phase where domain experts verify random samples of generated problem variants to assess semantic equivalence preservation across difficulty levels

2. Conduct ablation studies removing the verification step to quantify how much observed robustness improvement comes from verification versus rewriting mechanism

3. Test AR-Checker on broader mathematical domains (geometry, statistics, calculus) to evaluate framework effectiveness beyond arithmetic word problems