---
ver: rpa2
title: 'Entropy-Lens: Uncovering Decision Strategies in LLMs'
arxiv_id: '2502.16570'
source_url: https://arxiv.org/abs/2502.16570
tags:
- entropy
- profiles
- token
- across
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Entropy-Lens addresses the challenge of analyzing token-space dynamics
  in large language models (LLMs), where high dimensionality and the lack of intrinsic
  order in token distributions hinder standard statistical descriptors. The method
  introduces entropy of logit-lens predictions as a per-layer, permutation-invariant
  scalar metric to overcome these limitations.
---

# Entropy-Lens: Uncovering Decision Strategies in LLMs
## Quick Facts
- arXiv ID: 2502.16570
- Source URL: https://arxiv.org/abs/2502.16570
- Reference count: 40
- Key outcome: Entropy profiles from logit-lens predictions enable family-specific, task-discriminating, and format-sensitive analysis of LLM decision strategies

## Executive Summary
Entropy-Lens introduces a method to analyze token-space dynamics in large language models by computing entropy of logit-lens predictions at each layer. This approach overcomes the challenge of high-dimensional, unordered token distributions by providing a low-dimensional, permutation-invariant metric that captures the model's expansion (entropy increase) and pruning (entropy decrease) strategies. The method enables systematic investigation of how different model families, tasks, and output formats shape internal decision-making processes.

## Method Summary
The method extracts residual stream activations from each layer of decoder-only LLMs, applies the final LayerNorm and unembedding head (logit-lens), computes softmax probabilities, and calculates Shannon entropy. This produces a per-layer entropy profile that is concatenated and used for classification tasks. The approach validates expansion/pruning proxies by correlating entropy changes with top-p candidate count changes, and demonstrates family/task/format discrimination using k-NN classifiers with high AUC scores.

## Key Results
- Entropy profiles are family-specific and invariant under depth rescaling, showing consistent dynamics across model sizes
- Profiles exhibit task- and output-format-dependent characteristics, enabling reliable discrimination (kNN AUC > 90%)
- Intervention experiments reveal expansion strategies are generally more critical than pruning for downstream performance
- High classification accuracy achieved for model families, tasks, and output formats

## Why This Works (Mechanism)
The method works by leveraging the mathematical property that entropy provides a scalar summary of probability distributions regardless of token ordering. By computing entropy at each layer through logit-lens predictions, it captures the model's internal decision-making process as it expands the candidate space and then prunes to a final prediction. This creates a continuous signal that reflects how models allocate computational resources across layers.

## Foundational Learning
- **Logit-Lens**: A technique to extract intermediate predictions by applying the output head to hidden states - needed to access per-layer token distributions
- **Shannon Entropy**: Mathematical measure of uncertainty in probability distributions - needed to create permutation-invariant scalar metrics
- **Top-p Sampling**: Probability-based token selection method - used as proxy for measuring model decision boundaries
- **Permutation Invariance**: Property where function output doesn't depend on input ordering - critical for comparing token distributions
- **Residual Streams**: Information pathways through transformer layers - the source of activation data for entropy calculation
- **Spearman Correlation**: Non-parametric rank correlation - validates entropy changes correlate with candidate count changes

## Architecture Onboarding
- **Component Map**: Input Prompt -> Residual Stream Extraction -> LayerNorm + Unembedding -> Softmax -> Entropy Calculation -> Profile Aggregation
- **Critical Path**: Layer activation → logit-lens application → entropy computation → profile concatenation → classification
- **Design Tradeoffs**: Fixed-length profiles vs. model-specific depths (linear interpolation used), sampling temperature for stability vs. determinism
- **Failure Signatures**: NaNs in entropy calculations indicate numerical instability; mismatched final layer entropy suggests incorrect logit-lens application
- **First Experiments**:
  1. Verify entropy at final layer matches standard model output entropy
  2. Test numerical stability with small epsilon in log calculations
  3. Validate expansion/pruning proxy by correlating Δentropy with top-p candidate changes

## Open Questions the Paper Calls Out
- Which specific architectural components or training schemes causally determine a model's characteristic entropy profile?
- How do specific training stages, such as RLHF, alter the relative functional importance of expansion versus pruning strategies?
- How do token prediction dynamics manifest in architectures with sparse routing, such as Mixture-of-Experts?

## Limitations
- Validation relies on strong assumptions about top-p token count as decision-making proxy
- Correlation established but causal mechanisms between architecture and profiles remain unclear
- Method relies on next-token predictions which may miss hierarchical decision processes
- Intervention analysis doesn't account for potential layer redundancy or compensation mechanisms

## Confidence
- **High Confidence**: Mathematical validity of entropy calculation, family-specificity and depth-invariance of profiles
- **Medium Confidence**: Interpretation of entropy changes as expansion/pruning strategies, classification accuracy results
- **Low Confidence**: Causal claims about relative importance of expansion vs pruning for performance

## Next Checks
1. Systematically validate the top-p candidate count proxy by comparing it against alternative measures of model decision-making
2. Apply Entropy-Lens to models trained on non-natural language domains to assess generalization
3. Track entropy profile evolution during model fine-tuning to determine stability of observed patterns