---
ver: rpa2
title: Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and
  Prognosis
arxiv_id: '2504.13645'
source_url: https://arxiv.org/abs/2504.13645
tags:
- segmentation
- adaptation
- data
- modalities
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of adapting deep learning models
  for multi-modal medical image segmentation and prognosis when PET scans are scarce.
  The authors propose PEMMA, a parameter-efficient multi-modal adaptation framework
  that leverages LoRA and DoRA to efficiently adapt a transformer-based segmentation
  model trained on CT scans for PET scans.
---

# Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis

## Quick Facts
- arXiv ID: 2504.13645
- Source URL: https://arxiv.org/abs/2504.13645
- Reference count: 40
- Primary result: PEMMA achieves comparable segmentation performance to early fusion using only 8% of trainable parameters and improves prognosis concordance index by +10% to +23%

## Executive Summary
PEMMA addresses the challenge of adapting deep learning models for multi-modal medical image segmentation and prognosis when PET scans are scarce. The framework uses LoRA/DoRA to efficiently adapt a CT-pretrained transformer model for PET scans, minimizing cross-modal entanglement through modality-specific pathways. This enables single-modality fine-tuning without catastrophic forgetting while achieving comparable performance to early fusion with significantly fewer parameters. PEMMA demonstrates strong results on head and neck cancer segmentation and improves prognosis predictions when incorporating additional modalities.

## Method Summary
PEMMA adapts a transformer-based segmentation model (UNETR/Swin UNETR) pretrained on CT scans to incorporate PET scans efficiently using LoRA/DoRA. The framework adds modality-specific pathways for PET (patch embeddings and skip connections) while freezing the CT backbone. Only the adaptation modules (LoRA/DoRA, PET pathways) are trainable during multi-modal training. Self-attention enables cross-modal knowledge transfer while only CT tokens pass to the decoder. The approach supports single-modality inference and continual learning across clinical centers without catastrophic forgetting.

## Key Results
- Segmentation: Comparable Dice scores to early fusion with only 8% of trainable parameters
- PET-only performance: +28% Dice score improvement when adapting CT-pretrained model to include PET
- Prognosis: +10% concordance index improvement when adapting CT-pretrained model for PET, +23% when adapting for both PET and EHR data

## Why This Works (Mechanism)

### Mechanism 1
LoRA/DoRA decomposes weight updates into low-rank matrices, constraining adaptation to a low-dimensional subspace. This reduces overfitting risk on limited PET data while maintaining performance. The framework achieves efficiency by applying LoRA/DoRA only to attention weights rather than full fine-tuning.

### Mechanism 2
Modality-specific pathways (separate patch embeddings and skip connections) minimize cross-modal entanglement. This architectural isolation enables single-modality fine-tuning without catastrophic forgetting by preventing gradient flow from one modality's updates from degrading the other's representations.

### Mechanism 3
Self-attention naturally distills PET token information into CT tokens, enabling the decoder to access cross-modal knowledge while receiving only CT tokens. This avoids architectural changes while maintaining cross-modal context through attention mechanisms.

## Foundational Learning

- **Concept:** LoRA (Low-Rank Adaptation)
  - Why needed: Core PEFT technique for efficient parameter adaptation
  - Quick check: Can you explain why BA (rank-r product) has fewer parameters than full weight matrix W ∈ ℝ^(d×d)?

- **Concept:** Transformer Self-Attention
  - Why needed: Mechanism for cross-modal knowledge transfer between token sequences
  - Quick check: In multi-head attention, how do Q, K, V projections enable tokens to exchange information?

- **Concept:** Catastrophic Forgetting
  - Why needed: Problem PEMMA's architecture explicitly addresses through modality separation
  - Quick check: Why does fine-tuning on CT-only data degrade PET performance in early fusion but not PEMMA?

## Architecture Onboarding

- **Component map:** Input: CT volume → E^C_θ (frozen patch embedding) → PET volume → E^P_θ (new trainable embedding) → Adapter Layer (Swin UNETR only) → Transformer Blocks (×12) → Skip Connections: z_C (frozen) + β·z_P (trainable) → Decoder: Frozen, receives only CT tokens

- **Critical path:**
  1. Pre-train base model on CT-only data (freeze Θ)
  2. Add PET pathway (θ^P_PE, θ^P_SK) + PEFT modules (θ_PEFT)
  3. Train multi-modal on CT+PET (only θ^P_PE, θ^P_SK, θ_PEFT update)
  4. Continual learning: fine-tune only θ_PEFT on new centers

- **Design tradeoffs:**
  - LoRA vs. DoRA: DoRA separates magnitude/direction, shows +2-3% Dice improvement but adds complexity
  - UNETR vs. Swin UNETR: Swin achieves higher performance but requires Adapter layer for input handling
  - Token selection: Passing only CT tokens to decoder avoids architectural changes but risks losing PET-specific spatial details

- **Failure signatures:**
  - PET-only inference fails catastrophically (Dice <0.1): check adapter layer activation, modality dropout schedule
  - Continual learning degrades CT performance: verify θ^P_SK is frozen, only θ_PEFT updating
  - DoRA underperforms LoRA: check normalization implementation, scaling factor α

- **First 3 experiments:**
  1. Train early fusion model on CT+PET; verify PEMMA-LoRA achieves within 2% Dice with ~12× fewer parameters
  2. Train on CT+PET, test on CT-only and PET-only; confirm no catastrophic drop (>50% relative) compared to dual-modality inference
  3. Fine-tune on HGJ center using CT-only data; verify PET-only inference maintains >50% of original performance

## Open Questions the Paper Calls Out

- **Open Question 1:** Can PEMMA be extended to handle unregistered multi-modal inputs where spatial alignment is absent? The current framework relies on registered inputs to effectively concatenate patch embeddings and utilize skip connections.

- **Open Question 2:** How effective is the proposed low-rank adaptation for modalities with significantly different structural characteristics, such as MRI? The framework is optimized for the specific anatomical-metabolic relationship of CT/PET, leaving its generalization to other modality pairings untested.

- **Open Question 3:** Can PEMMA be combined with generative synthesis methods to improve performance when modalities are missing? The adapter layer handles missing inputs structurally but does not replace the missing information, whereas synthesis might provide richer context.

## Limitations

- **Data dependency risk:** Performance improvements depend heavily on MDA center data (197 patients) with limited cross-center validation (HGJ: 55, HMR: 18 patients)
- **Unproven architectural assumptions:** Effectiveness relies on the assumption that modality-specific pathways sufficiently capture cross-modal dependencies through self-attention alone
- **Incomplete efficiency comparison:** "8% of trainable parameters" assumes comparison with full fine-tuning without validating against early fusion with CT-pretrained weights

## Confidence

- **High confidence:** LoRA/DoRA parameter efficiency mechanism
- **Medium confidence:** Cross-modal distillation through self-attention
- **Medium confidence:** Single-modality adaptation without catastrophic forgetting
- **Low confidence:** Generalization across centers and clinical settings

## Next Checks

1. Systematically vary modality dropout schedules (ct=0.2, pet=0.2, ctpet=0.6) to determine optimal values for balancing cross-modal learning vs. modality-specific feature preservation

2. Evaluate on external PET-only datasets without any CT data to validate PET pathway generalization beyond CT+PET training distribution

3. Implement early fusion baseline with CT-pretrained weights and PET random initialization, training with the same LoRA/DoRA parameter budget (~8%) for fair comparison