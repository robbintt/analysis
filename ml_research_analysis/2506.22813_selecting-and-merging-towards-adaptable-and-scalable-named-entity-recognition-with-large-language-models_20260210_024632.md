---
ver: rpa2
title: 'Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition
  with Large Language Models'
arxiv_id: '2506.22813'
source_url: https://arxiv.org/abs/2506.22813
tags:
- domain
- data
- merging
- expert
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the SaM framework to improve named entity
  recognition (NER) with large language models by dynamically selecting and merging
  expert models. Instead of training a single unified model, SaM first trains multiple
  domain-specific expert models and then selects relevant experts for a target domain
  based on domain similarity and sampled-instance evaluation.
---

# Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models

## Quick Facts
- arXiv ID: 2506.22813
- Source URL: https://arxiv.org/abs/2506.22813
- Reference count: 28
- This paper introduces the SaM framework to improve named entity recognition (NER) with large language models by dynamically selecting and merging expert models.

## Executive Summary
This paper proposes SaM (Selecting and Merging), a framework that improves named entity recognition (NER) with large language models by dynamically selecting and merging expert models trained on specific domains. Instead of training a single unified model, SaM first trains multiple domain-specific expert models and then selects relevant experts for a target domain based on domain similarity and sampled-instance evaluation. These selected experts are merged using parameter-efficient fine-tuning to create task-specific models. Experiments on multiple benchmarks show that SaM outperforms a fully-trained unified model by an average of 10%, with improvements up to 20% on certain domains. The framework also offers adaptability and scalability, allowing experts to be conveniently added or removed without extra training.

## Method Summary
The SaM framework trains domain-specific expert models using LoRA fine-tuning on NER datasets from various domains. For a target domain, it selects relevant experts using two strategies: domain similarity (comparing domain embeddings) and sampling evaluation (ranking experts by performance on sampled target instances). The top-k experts from each strategy are merged using TIES-Merging to create task-specific models. Final predictions are generated by combining the outputs of both merged models. The framework is evaluated on CrossNER and MIT benchmarks, comparing against unified models and expert selection baselines.

## Key Results
- SaM outperforms a fully-trained unified model by an average of 10% in entity-level micro-F1
- Improvements reach up to 20% on certain target domains
- The framework offers adaptability and scalability, allowing experts to be conveniently added or removed without extra training
- Both domain similarity and sampling evaluation selection strategies are effective and complementary

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Expert Selection via Domain Similarity
- Claim: Selecting expert models based on domain similarity improves target-domain NER performance.
- Mechanism: The framework computes domain embeddings (centroids of data embeddings) for both expert domains and the target domain. Cosine similarity between these embeddings identifies experts trained on domains most similar to the target distribution, which are then selected for merging.
- Core assumption: Text embeddings capture meaningful data distribution characteristics such that similar embeddings imply transferable NER performance.
- Evidence anchors:
  - [abstract]: "select domain-specific experts... based on (i) domain similarity to the target domain."
  - [section 3.2]: "The domain embedding inherently captures the data distribution in the embedding space. Thus, in theory, the selected expert models... are expected to perform well... due to the resemblance in these data distributions."
  - [corpus]: Neighbor papers (e.g., ESNERA, Konooz) explore domain-specific NER and dataset merging, suggesting domain alignment is relevant, but do not validate this specific selection method.
- Break condition: If domain embeddings fail to capture transfer-relevant features (e.g., superficial similarity without task relevance), or if the target domain is highly distinct from all expert domains, this mechanism will fail to select beneficial experts.

### Mechanism 2: Empirical Selection via Sampling Evaluation
- Claim: Selecting experts based on their performance on sampled target-domain instances provides a practical, fine-grained complement to domain similarity.
- Mechanism: The framework samples instances from the target domain. Each expert generates predictions, which are aggregated via majority voting to create pseudo-labels. Experts are ranked by their performance against these pseudo-labels, prioritizing practical effectiveness over theoretical similarity.
- Core assumption: Performance on a small, unlabeled sample (with pseudo-labels) is a reliable proxy for full target-domain performance; ensemble predictions provide sufficient signal.
- Evidence anchors:
  - [abstract]: "select... based on (ii) performance on sampled instances."
  - [section 3.2]: "sampling evaluation delivers a fine-grained, empirical quantification of expert performance to yield actionable practical guidance."
  - [corpus]: Neighbor papers (e.g., MME-RAG, Zero-Shot NER Multi-Agent) utilize in-context learning and multi-model collaboration for adaptation, supporting the idea of leveraging model outputs on target data, but do not validate this pseudo-label evaluation method.
- Break condition: If sampled instances are unrepresentative, or if pseudo-labels are highly noisy (poor ensemble quality), the evaluation signal will be misleading, leading to suboptimal expert selection.

### Mechanism 3: Synergistic Merging of Complementary Experts
- Claim: Merging selected experts creates a task-specific model that outperforms both a unified multi-domain model and individual experts by combining complementary strengths.
- Mechanism: Two expert sets are selected (via similarity and sampling). Each set is merged using TIES-Merging (which handles parameter redundancy/sign inconsistency) to form task-specific models ($M_{DS}$, $M_{SE}$). Their predictions are combined via union, leveraging both theoretical priors and empirical performance.
- Core assumption: Parameter merging effectively fuses capabilities without destructive interference; union of outputs from two differently-specialized merged models is more beneficial than intersection.
- Evidence anchors:
  - [abstract]: "improve generalization... outperforms the unified model by an average of 10%."
  - [section 4.3]: "Overall, both expert selection strategies are effective and complementary, yielding the best results when combined."
  - [section 3.3]: "our two task-specific models... capture different and complementary perspectives. Therefore, we adopt their union as the final result."
  - [corpus]: Neighbor paper "Twin-merging" (Lu et al., 2024) directly explores dynamic integration of modular expertise in model merging, providing correlative support for the approach.
- Break condition: If merged experts have conflicting parameter updates not resolved by TIES-Merging, or if the two selected sets are highly redundant, performance gains may diminish.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) for NER**
  - Why needed here: To understand how domain-specific expert models are initially created. The paper's experts are all SFT models.
  - Quick check question: Can you explain why the paper uses LoRA for SFT instead of full fine-tuning? (Answer: Efficiency; LoRA minimizes storage overhead for multiple experts.)

- **Model Merging / Task Arithmetic**
  - Why needed here: This is the core technical operation enabling the framework. Understanding delta parameters ($\delta_{sft} = \theta_{sft} - \theta_{base}$) and their fusion is essential.
  - Quick check question: What problem does TIES-Merging aim to solve that simple averaging might not? (Answer: Parameter redundancy and sign inconsistency, which can cause interference.)

- **Cross-Domain / Zero-Shot Generalization**
  - Why needed here: The paper's primary evaluation setting is zero-shot on target domains. The problem motivation and benefit must be understood in this context.
  - Quick check question: Why is the method considered "zero-shot" if it uses sampled instances from the target domain? (Answer: It uses *unlabeled* samples; no ground-truth labels are required.)

## Architecture Onboarding

- **Component map**: (Target domain raw text) -> [Text Encoder] -> (Domain Embedding & Sampled Instances) -> [Selection Engine] -> (Two expert sets) -> [Merger Module] -> (Two task-specific models) -> [Inference Manager] -> (Final predictions)

- **Critical path**: Target domain text is encoded to create domain embeddings and sampled instances. The selection engine uses these to choose experts via domain similarity and sampling evaluation. The merger module fuses the selected experts using TIES-Merging. The inference manager runs both merged models and combines their outputs via union.

- **Design tradeoffs**:
  - **Performance vs. Inference Cost**: Using both $M_{DS}$ and $M_{SE}$ doubles inference cost vs. a single model, but yields best results. Economic modes (SaMeco) reduce cost with potential slight performance drops.
  - **Selection Precision vs. Sample Availability**: Sampling Evaluation is more precise but requires some target text; Domain Similarity works with minimal samples but may be less accurate.
  - **Number of Experts ($k$)**: Fixed $k=3$ was chosen for average best performance, but optimal $k$ varies by domain (2-4).

- **Failure signatures**:
  - **Performance worse than baseline**: Check if target domain is extremely distinct, if pseudo-labels are noisy (poor ensemble), or if linear merging was used (paper notes severe degradation).
  - **Unstable results across runs**: Sampling evaluation relies on random samples; if sample count is too small (e.g., 1), results become unstable.

- **First 3 experiments**:
  1. **Reproduce Core Result**: Train 2-3 simple expert models (e.g., News, Biomedical). Implement domain similarity selection only. Merge and evaluate on a held-out target domain. Compare F1 against a fully-trained unified model to validate the ~10% gain.
  2. **Ablate Selection Strategies**: Evaluate three conditions: (a) Domain Similarity only, (b) Sampling Evaluation only, (c) Both (SaM). Confirm that (c) yields highest F1 and merged models outperform individual experts.
  3. **Test Resource-Efficient Variant**: Implement Mode2 (normalize metrics, select top-3 from combined pool) to create a single merged model. Compare its F1 and inference time against full SaM to assess the efficiency-performance trade-off.

## Open Questions the Paper Calls Out

- **Can the SaM framework be effectively extended to other information extraction tasks beyond NER, such as relation extraction and event extraction?**
  - Basis in paper: [explicit] Future Work section states "Our framework can be naturally extended to a broader IE setting by incorporating additional IE data to train IE experts. Additionally, our method extends beyond these tasks to a wide range of applications."
  - Why unresolved: The paper acknowledges in Limitations that "Our analysis is limited to Named Entity Recognition (NER). Further experiments are needed for other IE tasks."
  - What evidence would resolve it: Experiments applying SaM to relation extraction (RE) and event extraction (EE) tasks, comparing against unified models and task-specific baselines.

- **How can the optimal number of merged experts (k) be dynamically determined for each target domain rather than fixed?**
  - Basis in paper: [explicit] Future Work section mentions "Dynamically determining the number of merged models, k. Model merging may be unnecessary for certain target domains... the best choice of k varies across different target domains."
  - Why unresolved: Section 4.5 shows optimal k ranges from 2-4 depending on domain, and Appendix B notes k=1 may be optimal when target matches source domain. The paper uses a fixed k=3.
  - What evidence would resolve it: An adaptive selection algorithm that predicts optimal k from target domain characteristics, validated across diverse domains with performance comparisons.

- **Would incorporating entity-type similarity alongside domain similarity improve expert selection?**
  - Basis in paper: [explicit] Future Work section proposes: "Incorporating more task-specific designs. For instance, alongside domain-level similarity, we could also leverage entity-type similarity when selecting source models."
  - Why unresolved: Current selection relies only on domain similarity (embeddings) and sampling evaluation; entity type overlap between source and target is not considered.
  - What evidence would resolve it: Ablation experiments adding entity-type similarity metrics to the selection process, with analysis of complementary information provided.

## Limitations

- The quality of pseudo-labels generated from ensemble predictions on sampled target instances is not explicitly validated, which could lead to unreliable expert selection.
- The framework's scalability to very large expert pools is not thoroughly analyzed, with potential computational costs in selection and storage requirements.
- The effectiveness of domain similarity embeddings in capturing transfer-relevant features is assumed but not empirically validated across domains with subtle differences.

## Confidence

- **High Confidence**: The core mechanism of dynamic expert selection and parameter merging using TIES-Merging is technically sound and well-supported by experimental results.
- **Medium Confidence**: The specific hyperparameter choices (k=3, 10 samples) are presented as optimal but may not generalize without further tuning.
- **Low Confidence**: The long-term scalability and robustness to extreme domain shifts or very large expert pools are not thoroughly explored.

## Next Checks

1. **Validate Pseudo-label Quality**: For a held-out target domain, generate pseudo-labels using the ensemble method. Manually annotate a small subset of these samples and compute the accuracy of the pseudo-labels. Repeat this process across multiple target domains to quantify the reliability of the sampling evaluation strategy.

2. **Test Scalability Limits**: Extend the framework to a larger set of expert models (e.g., 10-15 domains). Measure the computational cost of selection (both domain similarity and sampling evaluation) and the storage requirements for all LoRA adapters. Evaluate if the performance gains justify the increased resource usage.

3. **Analyze Domain Similarity Embedding Space**: For a pair of target domains (one where SaM performs well, one where it performs poorly), visualize the domain embeddings of the experts and the target domain in the embedding space. Correlate the geometric distance with the actual NER performance of the selected experts to assess if the embedding space is a reliable proxy for transferable expertise.