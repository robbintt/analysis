---
ver: rpa2
title: 'CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for
  Learning with Noisy Labels'
arxiv_id: '2507.11807'
source_url: https://arxiv.org/abs/2507.11807
tags:
- learning
- noisy
- labels
- noise
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLID-MU tackles noisy label learning without requiring a clean
  meta-dataset by introducing an unsupervised Cross-Layer Information Divergence (CLID)
  metric. CLID measures divergence between feature and output layer data distributions,
  capturing model performance independent of label quality.
---

# CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels

## Quick Facts
- arXiv ID: 2507.11807
- Source URL: https://arxiv.org/abs/2507.11807
- Reference count: 40
- CLID-MU achieves 89.07% accuracy on CIFAR-10N and 67.53% on CIFAR-100N without clean meta-dataset

## Executive Summary
CLID-MU introduces a novel meta-learning approach for handling noisy labels in deep learning without requiring a clean meta-dataset. The method employs Cross-Layer Information Divergence (CLID) to measure divergence between feature and output layer data distributions, providing an unsupervised metric for model performance evaluation. Experiments demonstrate that CLID-MU outperforms state-of-the-art meta-learning methods and semi-supervised learning frameworks across multiple benchmarks, including synthetic noise on CIFAR datasets and real-world noise on Clothing1M.

## Method Summary
CLID-MU tackles noisy label learning by introducing an unsupervised Cross-Layer Information Divergence (CLID) metric that measures divergence between feature and output layer data distributions. This metric captures model performance independent of label quality, enabling effective meta-update strategies without requiring a clean meta-dataset. The approach combines meta-learning principles with CLID-based performance estimation to guide training and reduce overfitting to noisy labels, achieving superior performance across various noise patterns and intensities.

## Key Results
- CLID-MU achieves 89.07% accuracy on CIFAR-10N and 67.53% on CIFAR-100N
- Outperforms state-of-the-art meta-learning methods (WNet, VRI) and semi-supervised learning frameworks (FlexMatch, FixMatch)
- Reaches 72.93% accuracy on Clothing1M real-world noisy label dataset
- Maintains stable performance across hyperparameter settings

## Why This Works (Mechanism)
CLID-MU works by using the Cross-Layer Information Divergence metric to capture the discrepancy between feature representations and output predictions across network layers. This unsupervised measure effectively identifies when the model is overfitting to noisy labels by detecting when the divergence between layers becomes large, indicating poor generalization. The meta-update strategy then uses this information to adjust training dynamically, focusing on examples where the model's internal representations align well while down-weighting or correcting examples with high divergence.

## Foundational Learning
- Meta-learning concepts: Why needed - enables adaptation to noisy label scenarios; Quick check - understanding MAML and Reptile frameworks
- Information divergence metrics: Why needed - provides unsupervised performance measurement; Quick check - familiarity with KL divergence and other divergence measures
- Semi-supervised learning principles: Why needed - for comparing with related approaches; Quick check - understanding consistency regularization and pseudo-labeling
- Deep neural network layer interactions: Why needed - CLID operates across feature and output layers; Quick check - understanding how representations evolve through network depth

## Architecture Onboarding

Component map: Input data -> Feature extractor -> CLID computation -> Meta-update module -> Model parameters -> Output predictions

Critical path: Input data flows through the feature extractor to produce representations, which are compared with output predictions via CLID computation. The meta-update module uses this divergence information to adjust model parameters, creating a feedback loop that improves noisy label handling.

Design tradeoffs: The method trades computational overhead from CLID computation against improved robustness to noisy labels. The unsupervised nature eliminates dependency on clean meta-datasets but requires careful calibration of divergence thresholds. The cross-layer approach captures richer information than single-layer metrics but increases complexity.

Failure signatures: High CLID values consistently across all examples may indicate fundamental model architecture issues rather than label noise. If CLID-MU performance degrades with increasing noise rates beyond certain thresholds, the divergence metric may lose discriminative power. Poor convergence could signal that the meta-update strategy needs adjustment for specific noise patterns.

First experiments: 1) Verify CLID computation on clean data to establish baseline divergence values; 2) Test CLID sensitivity to synthetic noise injection at controlled rates; 3) Compare CLID-based meta-updates against random meta-updates on noisy datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on synthetic noise injection and one real-world dataset (Clothing1M), limiting generalizability claims
- Comparison with semi-supervised learning methods may not be entirely fair due to different problem formulations
- No theoretical guarantees provided for CLID metric's effectiveness in capturing model performance independent of label quality

## Confidence

High confidence in experimental methodology and results presentation
Medium confidence in generalizability to diverse real-world noisy label scenarios
Low confidence in theoretical foundations of the CLID metric

## Next Checks
1. Test CLID-MU on additional real-world datasets with noisy labels beyond Clothing1M to assess generalizability
2. Conduct ablation studies to quantify individual contributions of CLID metric and meta-learning components
3. Evaluate CLID-MU's performance under different noise patterns and intensities not covered in current synthetic noise experiments