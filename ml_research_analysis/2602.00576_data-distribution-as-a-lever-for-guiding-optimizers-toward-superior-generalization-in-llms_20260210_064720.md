---
ver: rpa2
title: Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization
  in LLMs
arxiv_id: '2602.00576'
source_url: https://arxiv.org/abs/2602.00576
tags:
- training
- learning
- loss
- data
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether modifying the training data distribution
  can guide optimizers toward solutions with improved generalization when training
  large language models (LLMs). Theoretically, it analyzes a multi-head linear self-attention
  model for in-context linear regression, comparing the training dynamics of gradient
  descent (GD) and sharpness-aware minimization (SAM).
---

# Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs

## Quick Facts
- arXiv ID: 2602.00576
- Source URL: https://arxiv.org/abs/2602.00576
- Reference count: 40
- Primary result: Data distribution modification (upsampling hard examples) achieves up to 18% accuracy gains on math reasoning tasks, matching SAM's benefits at lower computational cost.

## Executive Summary
This paper demonstrates that modifying training data distribution—specifically by identifying and upsampling "hard" examples—can guide optimizers toward solutions with improved generalization in LLMs. The authors provide both theoretical and empirical evidence that optimizers exhibit "simplicity bias," preferentially learning simple features first, and that reducing this bias improves performance. They show that Sharpness-Aware Minimization (SAM) naturally reduces simplicity bias, and propose a data-centric alternative: training a small proxy model briefly to identify hard examples via loss trajectories, then upsampling these during fine-tuning. Extensive experiments on multiple LLMs show consistent gains on mathematical reasoning tasks without the computational overhead of SAM.

## Method Summary
The method involves training a small proxy LLM (e.g., Pythia-70M) for a short duration (1/3 epoch) while tracking per-example loss trajectories across checkpoints. These trajectories are clustered into easy and hard groups, with hard examples defined as those with higher final loss or flatter learning curves. During fine-tuning of the target LLM, hard examples are upsampled through duplication or synthetic generation. The approach is evaluated with standard optimizers (AdamW, Muon) and LoRA fine-tuning, showing that upsampling hard examples yields consistent performance improvements on mathematical reasoning benchmarks without the computational cost of SAM.

## Key Results
- Hard-example upsampling improves LLM performance on mathematical reasoning tasks with up to 18% relative accuracy gains.
- SAM reduces simplicity bias compared to standard gradient descent, and the data-centric approach achieves similar benefits at lower computational cost.
- Upsampling random or easy examples does not improve performance; only upsampling hard examples identified via loss trajectories yields gains.
- The approach works across multiple model scales (0.6B-2.7B parameters) and optimizers (AdamW, Muon).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM reduces simplicity bias by making feature learning more uniform across data covariance directions.
- Mechanism: SAM introduces a perturbation-dependent term in the gradient flow ODE (τ̇v_i = λ²ᵢv²ᵢ - 2ρ√3 λ²ᵢvᵢ vs. GD's τ̇v_i = λ²ᵢv²ᵢ), which non-uniformly slows down learning and accelerates complex feature acquisition.
- Core assumption: The multi-head linear self-attention model with rank-one key/query matrices approximates transformer dynamics sufficiently for theoretical insights to transfer.
- Evidence anchors:
  - [abstract]: "SAM induces a lower simplicity bias (SB)... and identify this reduction as a key factor underlying its improved generalization performance."
  - [section 4.3, Theorem 4.4]: Formal proof that SAM yields higher entropy in feature learning times than GD under stated conditions.
- Break condition: If the ansatz (3)-(5) fails—i.e., heads do not learn one eigenvector at a time in order of decreasing eigenvalue—the ODE reduction may not hold.

### Mechanism 2
- Claim: Examples learned later in training contain more complex features; identifying them via a proxy model's loss trajectory enables targeted data modification.
- Mechanism: Train a small proxy model for ~1/3 epoch, track per-example loss trajectories, and cluster examples into easy (steady loss decrease) and difficult (higher final loss or flatter trajectories).
- Core assumption: The proxy model's simplicity bias correlates with the target model's; what appears "hard" to the proxy is also hard for larger models.
- Evidence anchors:
  - [section 5]: "We train a smaller proxy model for a small number of iterations (e.g. 1/3 of an epoch) and track the loss trajectory... for all examples."
  - [Figure 4(a)]: Performance is comparable whether using Pythia-70M or Phi2-2.7B as proxy.
- Break condition: If the proxy model is too small to meaningfully represent the feature space, or if training duration is insufficient to observe loss trajectory divergence, clustering will be noisy.

### Mechanism 3
- Claim: Upsampling or augmenting hard examples reduces simplicity bias and improves generalization for standard optimizers (AdamW, Muon).
- Mechanism: By increasing the effective signal from complex-feature examples, the optimizer spends relatively more compute on difficult features, approximating SAM's uniformization of feature learning.
- Core assumption: Upsampling does not induce overfitting to the specific hard examples; generalization gains stem from better feature coverage, not memorization.
- Evidence anchors:
  - [abstract]: "Our extensive experiments show that our strategy improves the performance of multiple LLMs... achieving relative accuracy gains up to 18%."
  - [Figure 4(d)]: Only hard-example upsampling improves performance; upsampling easy examples degrades performance.
- Break condition: If hard examples are upsampled to the point of dominating the batch distribution, the model may underfit simple features essential for base performance.

## Foundational Learning

- Concept: **Simplicity Bias (SB)**
  - Why needed here: The core theoretical contribution links SB reduction to improved generalization. Understanding SB as sequential feature learning (easiest first) is prerequisite for interpreting the ODE analysis.
  - Quick check question: Given a data covariance with eigenvalues [λ₁=10, λ₂=3, λ₃=0.5], which eigenvector will GD learn first? (Answer: e₁, the one with largest eigenvalue.)

- Concept: **Sharpness-Aware Minimization (SAM)**
  - Why needed here: The paper positions its data-centric approach as a cheaper alternative to SAM. Understanding SAM's perturbation mechanism (w(t+1) = w(t) - η∇L(w + ρ∇L/||∇L||₂)) clarifies why SAM finds flatter minima.
  - Quick check question: What is the computational cost of SAM vs. GD per step? (Answer: SAM requires two gradient computations per step—one for the perturbation direction, one for the update.)

- Concept: **Saddle-to-Saddle Learning Dynamics**
  - Why needed here: The theoretical model exhibits stage-wise loss drops as the model moves between fixed-point manifolds (M(S_m)), each corresponding to learning a subset of eigenvectors.
  - Quick check question: In the multi-head attention model, what determines the order of eigenvector learning? (Answer: Decreasing eigenvalues of the data covariance matrix Σ.)

## Architecture Onboarding

- Component map:
  - **Proxy model** -> **Trajectory storage** -> **Clustering module** -> **Data loader modifier** -> **Target training**

- Critical path:
  1. Train proxy model on original dataset for K checkpoints, saving loss per example.
  2. Extract loss trajectories (shape: [n_examples × K]).
  3. Cluster into 2 groups; label higher-mean-loss cluster as "hard."
  4. Upsample hard examples (paper uses simple duplication; synthesis is optional enhancement).
  5. Fine-tune target model on upsampled dataset.

- Design tradeoffs:
  - **Proxy size vs. accuracy**: Smaller proxies are faster but may misidentify hard examples. Paper shows Pythia-70M works well for 0.6B–2.7B target models.
  - **Trajectory length vs. compute**: More checkpoints (K=15) provide more signal but no clear performance trend in ablations.
  - **Duplication vs. synthesis**: Duplication is simpler and performs better in isolation; synthesis + duplication combined yields marginal additional gains (~50% of synthetic examples reclassified as hard).

- Failure signatures:
  - No performance gain: Likely the proxy training duration is too short (loss trajectories haven't diverged) or the dataset lacks meaningful complexity variation.
  - Performance degradation: Check if easy examples were accidentally upsampled (Figure 4(d) shows this actively harms performance).
  - High variance across seeds: Clustering boundary may be unstable; consider using final loss values instead of full trajectories for robustness.

- First 3 experiments:
  1. **Baseline replication**: Train Pythia-70M on MathInstruct for 5 checkpoints, cluster loss trajectories, upsample hard cluster 2×, fine-tune Qwen3-0.6B-Base with AdamW. Compare GSM8K/MATH accuracy to non-upsampled baseline.
  2. **Ablation—trajectory vs. final loss**: Repeat experiment 1 using only the final checkpoint loss for clustering. Compare to trajectory-based clustering (paper shows trajectories slightly outperform final-loss-only).
  3. **Sanity check—wrong direction**: Upsample the *easy* cluster instead of hard. Verify that performance degrades (confirms the mechanism is not just "more data = better").

## Open Questions the Paper Calls Out

- Can the data-centric upsampling approach improve generalization in non-mathematical domains such as natural language understanding, code generation, or multimodal reasoning tasks?
- How does the choice of proxy model architecture, size, and pretraining data affect the quality of identified "hard" examples and downstream performance gains?
- Can variational problem synthesis be improved to consistently generate examples that remain in the "hard" cluster, and what synthesis strategies maximize generalization?

## Limitations

- The proxy model methodology may not generalize to all domains beyond mathematical reasoning tasks.
- The theoretical analysis relies on a simplified linear self-attention model that may not fully capture standard transformer complexity.
- The paper does not address potential overfitting risks when hard examples are heavily upsampled.

## Confidence

- **High confidence**: The empirical effectiveness of hard-example upsampling across multiple LLMs and benchmarks (Figures 4a-d, 5a-b).
- **Medium confidence**: The theoretical mechanism linking SAM's simplicity bias reduction to improved generalization (Section 4.3).
- **Medium confidence**: The proxy model methodology for identifying hard examples (demonstrated with Pythia-70M but may require tuning for different domains).

## Next Checks

1. **Domain Generalization Test**: Apply the proxy-based upsampling method to non-mathematical domains (e.g., code generation, commonsense reasoning) to verify the approach generalizes beyond mathematical reasoning tasks.

2. **Scale Sensitivity Analysis**: Systematically vary proxy model size and training duration to quantify the relationship between proxy quality and target model performance gains.

3. **Overfitting Boundary Test**: Conduct experiments with increasingly aggressive hard-example upsampling (e.g., 3×, 5×, 10×) to empirically determine the point where overfitting begins to degrade generalization.