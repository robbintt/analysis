---
ver: rpa2
title: Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large
  Language Models
arxiv_id: '2508.18609'
source_url: https://arxiv.org/abs/2508.18609
tags:
- scaling
- knowledge
- size
- quantization
- qwen3-14b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes the first task-stratified knowledge scaling
  laws for post-training quantized LLMs by incorporating model size, bit-width, group
  size, and calibration set size into a unified power-law framework. The empirical
  analysis on 293 diverse configurations reveals distinct sensitivities across knowledge
  capabilities: reasoning is precision-critical (most sensitive to bit-width and group
  size), knowledge application is scale-responsive (highest scaling exponent with
  model size), and memorization is calibration-sensitive (highest sensitivity to calibration
  data).'
---

# Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models

## Quick Facts
- **arXiv ID:** 2508.18609
- **Source URL:** https://arxiv.org/abs/2508.18609
- **Authors:** Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu
- **Reference count:** 40
- **Primary result:** Establishes first task-stratified scaling laws for PTQ LLMs incorporating model size, bit-width, group size, and calibration set size with Adj.R² > 0.92

## Executive Summary
This paper introduces the first task-stratified scaling laws for post-training quantized large language models, revealing how different knowledge capabilities respond uniquely to quantization parameters. The unified power-law framework demonstrates that reasoning capabilities are most sensitive to precision (bit-width and group size), knowledge application benefits most from model scaling, and memorization performance depends heavily on calibration data quality. The framework achieves strong empirical fit across 293 configurations and validates cross-architecture consistency, providing a principled approach for optimizing quantized model deployment.

## Method Summary
The authors fit a unified power-law scaling law: `-ln(Acc_norm) = A · N^α · (log2 B)^β · (log2 Cb)^γ · G^δ`, where N is model size, B is bit-width, Cb is calibration set size, and G is group size. They evaluate Qwen3 models (0.6B-14B for fitting, 32B for validation) and Llama-3 family across a grid of 293 configurations using GPTQ quantization. Accuracy is normalized per task using `(Acc - Acc_random)/(1 - Acc_random)` and transformed to loss space via `-ln(Acc_norm)`. The authors stratify 14 benchmarks into three knowledge levels: Memorization (KM), Application (KA), and Reasoning (KR), then perform OLS regression on log-transformed data to extract task-specific scaling exponents.

## Key Results
- Establishes first task-stratified scaling laws for post-training quantized LLMs with Adj.R² > 0.92
- Reveals distinct sensitivity patterns: reasoning most sensitive to precision (β = -1.356), application benefits most from scale (α = -0.409), memorization requires calibration (γ = -0.040)
- Achieves cross-architecture consistency by validating on both Qwen3 and Llama-3 families
- Identifies critical 2-bit "phase transition" where scaling laws break down

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning capabilities degrade most severely under low-precision quantization due to error propagation in multi-step inference chains.
- Mechanism: Reasoning tasks require sequential logical steps where quantization noise accumulates across each step. The authors report that bit-width sensitivity (β = -1.356) for reasoning exceeds that of memorization and application by nearly 40%, indicating that precision loss compounds in multi-hop inference.
- Core assumption: Reasoning involves error-propagating computation chains where noise at each step amplifies downstream.
- Evidence anchors:
  - [section 4.3.1] "L3 tasks exhibit the highest sensitivity to bit-width (β=-1.356) and group granularity (δ=0.087)... nearly 40% higher than KM and KA."
  - [abstract] "reasoning is most sensitive to precision (bit-width and group size)"
  - [corpus] Limited direct evidence; related work on quantization-optimizer interactions shows task-specific degradation but doesn't isolate reasoning chains.
- Break condition: If reasoning tasks can be reformulated to reduce step-dependency, precision sensitivity may decrease.

### Mechanism 2
- Claim: Knowledge application benefits disproportionately from model scale because it relies on pattern generalization rather than exact retrieval.
- Mechanism: Application tasks (α = -0.409) show higher scaling exponents than memorization (α = -0.315), meaning larger models provide greater gains for flexible knowledge use. This aligns with "emergence" properties where higher cognitive functions unlock with scale.
- Core assumption: Knowledge application depends on distributed representations that scale with model capacity.
- Evidence anchors:
  - [section 4.3.1] "KA exhibits a high scaling exponent (α=-0.409), contrasting with the notably lower exponent of KM (α=-0.315)"
  - [section 4.3.1] "application benefits significantly from scaling up, consistent with the 'emergence' properties"
  - [corpus] No direct corpus support for this specific finding.
- Break condition: If application tasks are re-framed as memorization (exact lookup), scale benefits would diminish.

### Mechanism 3
- Claim: Memorization performance depends heavily on calibration data because exact fact recall requires precise activation alignment with FFN key-value pairs.
- Mechanism: Knowledge memorization shows pronounced calibration sensitivity (γ = -0.040, nearly double that of application). The authors attribute this to memorization's reliance on precise activation patterns to trigger stored facts in feedforward layers.
- Core assumption: Memorized facts are stored as discrete key-value associations in FFN layers that require accurate distribution matching during calibration.
- Evidence anchors:
  - [section 4.3.1] "L1 tasks show a pronounced sensitivity to calibration data (γ=-0.040), nearly double that of the more robust KA tasks"
  - [section 4.3.1] "KM's 'exact lookup' mechanism is susceptible to distribution shifts, necessitating richer calibration data"
  - [corpus] Corpus papers acknowledge calibration importance but don't differentiate by task type.
- Break condition: If calibration data distribution diverges significantly from pre-training distribution, even large calibration sets may not help.

## Foundational Learning

- Concept: Post-Training Quantization (PTQ)
  - Why needed here: The entire framework models how PTQ parameters affect different capabilities; understanding weight-only quantization without retraining is essential.
  - Quick check question: Can you explain why GPTQ uses calibration data and how group size affects quantization granularity?

- Concept: Power-law scaling in neural networks
  - Why needed here: The proposed scaling law uses multiplicative power-law formulations (following Kaplan et al.); understanding log-log transformations and exponent interpretation is required.
  - Quick check question: In the equation -ln(Acc_norm) = A·N^α, what does a more negative α imply about model size sensitivity?

- Concept: Knowledge capability taxonomy (Bloom's Taxonomy adaptation)
  - Why needed here: The paper stratifies tasks into memorization/application/reasoning based on cognitive complexity; correctly classifying tasks determines which scaling law applies.
  - Quick check question: Would you classify GSM8K as memorization or reasoning, and why?

## Architecture Onboarding

- Component map:
  - Input variables (N, B, Cb, G) -> Log-log transformation -> OLS regression -> Task-specific scaling exponents

- Critical path:
  1. Normalize accuracy per task using Acc_norm = (Acc - Acc_random) / (1 - Acc_random)
  2. Transform to loss space: compute -ln(Acc_norm)
  3. Fit OLS regression on ln(-ln(Acc_norm)) against ln(N), ln(log2(B)), ln(log2(Cb)), ln(G)
  4. Extract task-specific coefficients for prediction

- Design tradeoffs:
  - 2-bit models excluded from fitting due to "phase transition" collapse (scaling laws don't converge)
  - Logarithmic transformation of B and Cb captures diminishing returns but may obscure edge cases
  - Weight-only quantization focus limits applicability to activation-quantized models

- Failure signatures:
  - Adj. R²_O < 0 in 2-bit regime indicates power-law breakdown
  - Reasoning surface flattening (Adj. R²_O ≈ 0.22) signals capability collapse regardless of configuration
  - Small models (N < 2B) at 2-bit show universal performance collapse

- First 3 experiments:
  1. Replicate baseline fit using provided coefficients (Table 2) on Qwen3-4B with 3-bit, G=128, Cb=128 across all three task types to validate framework.
  2. Ablate group size (G ∈ {32, 64, 128, 1024}) at 3-bit to verify reasoning's heightened precision sensitivity (δ = 0.087 vs. 0.064 for KM).
  3. Test extrapolation by fitting on Qwen3 models ≤ 8B and predicting Qwen3-14B performance to assess scaling law transfer.

## Open Questions the Paper Calls Out

- **Question:** Do the established scaling laws for dense transformers apply to Mixture-of-Experts (MoE) architectures, given their sparse activation patterns?
  - **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that "future research could extend these laws to... alternative architectures, such as Mixture-of-Experts (MoE)."
  - **Why unresolved:** MoE models distribute parameters across sparse experts rather than dense layers, potentially altering the relationship between model size (N) and task-specific sensitivities like reasoning or memorization during quantization.
  - **What evidence would resolve it:** Fitting the proposed four-factor power-law formula on quantized MoE models (e.g., Mixtral or Qwen-MoE) and comparing the resulting task-stratified exponents to those of dense models.

- **Question:** How do task-stratified scaling sensitivities change when extending the analysis from weight-only to weight-activation quantization?
  - **Basis in paper:** [explicit] The authors acknowledge their study focuses on "weight-only quantization" and suggest extending laws to "other quantization paradigms (e.g., activation quantization)."
  - **Why unresolved:** Activation quantization introduces input-dependent noise, which may affect dynamic capabilities like Knowledge Reasoning (KR) differently than static Knowledge Memorization (KM), potentially shifting the observed "precision-critical" thresholds.
  - **What evidence would resolve it:** Empirical validation of the scaling law framework incorporating activation bit-widths to see if the multiplicative power-law holds and if the sensitivity hierarchy (KR > KM) remains consistent.

- **Question:** Are the derived scaling coefficients robust across different PTQ algorithms (e.g., AWQ, QuIP#), or are they specific to the GPTQ method used?
  - **Basis in paper:** [inferred] While the paper validates across architectures, it relies exclusively on GPTQ. Different algorithms minimize quantization error using distinct strategies (e.g., activation scales vs. Hessian information), which could alter the impact of calibration set size ($C_b$) and group size ($G$).
  - **Why unresolved:** The observed sensitivity of memorization to calibration size might be an artifact of GPTQ's specific error compensation mechanism rather than a universal law of quantization.
  - **What evidence would resolve it:** Replicating the 293-configuration experimental grid using alternative quantization methods like AWQ or QuIP# and analyzing the variance in the fitted scaling exponents ($\gamma$ and $\delta$).

## Limitations

- Exclusion of 2-bit quantization from scaling law fitting due to observed "phase transition" collapse, leaving a critical gap in understanding ultra-low-precision regimes.
- Framework validity depends on assumption that C4 calibration dataset adequately represents all knowledge domains, which may not hold for specialized benchmarks.
- Focus on weight-only GPTQ quantization limits generalizability to activation-quantized or mixed-precision approaches.

## Confidence

**High Confidence**: The differential sensitivity patterns across task types (reasoning most sensitive to precision, application benefits most from scale, memorization requires calibration) are supported by comprehensive grid searches across 293 configurations and show consistent cross-architecture patterns in validation tests.

**Medium Confidence**: The proposed unified power-law formulation accurately predicts performance within the tested range (3-8 bit, models up to 14B parameters), but extrapolation to extreme regimes (2-bit, >32B models) remains unverified and may fail due to the excluded phase transition behavior.

**Low Confidence**: The attribution of reasoning's precision sensitivity to "error propagation in multi-step inference chains" lacks direct mechanistic evidence—the study observes correlation but doesn't establish causal pathways for quantization noise accumulation across reasoning steps.

## Next Checks

1. **Phase Transition Investigation**: Systematically characterize the 2-bit quantization collapse by measuring intermediate calibration metrics (weight distribution, activation histograms) to identify the precise failure mechanism and determine if alternative calibration strategies could recover performance.

2. **Cross-Domain Calibration Testing**: Validate whether C4-based calibration generalizes by testing performance when using domain-specific calibration data (e.g., scientific papers for ARC-Challenge, math problems for GSM8K) versus generic web text.

3. **Error Propagation Analysis**: Design a controlled experiment tracking quantization error accumulation across sequential reasoning steps by instrumenting intermediate activations in multi-hop reasoning tasks, comparing error magnification patterns between high-precision and low-precision configurations.