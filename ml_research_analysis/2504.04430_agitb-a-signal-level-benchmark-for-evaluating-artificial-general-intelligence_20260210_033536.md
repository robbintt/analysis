---
ver: rpa2
title: 'AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence'
arxiv_id: '2504.04430'
source_url: https://arxiv.org/abs/2504.04430
tags:
- agitb
- input
- intelligence
- such
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AGITB introduces a novel benchmark for evaluating artificial general
  intelligence by focusing on low-level, signal-based prediction rather than high-level
  task performance. It comprises twelve automated tests grounded in axioms that reflect
  core computational properties observed in biological systems, such as determinism,
  adaptability, and generalization.
---

# AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence

## Quick Facts
- **arXiv ID**: 2504.04430
- **Source URL**: https://arxiv.org/abs/2504.04430
- **Reference count**: 5
- **Primary result**: Introduces a novel benchmark focusing on low-level signal-based prediction for evaluating artificial general intelligence

## Executive Summary
AGITB introduces a novel benchmark for evaluating artificial general intelligence by focusing on low-level, signal-based prediction rather than high-level task performance. It comprises twelve automated tests grounded in axioms that reflect core computational properties observed in biological systems, such as determinism, adaptability, and generalization. The framework evaluates models on their ability to learn and predict temporal sequences of binary inputs without relying on prior knowledge, pretraining, or symbolic representations. Unlike existing benchmarks, AGITB avoids conventional correctness metrics, instead employing self-referential comparisons and requiring models to satisfy all twelve axioms. Preliminary application shows that no contemporary AI system meets all criteria, indicating that AGITB provides a structured means of assessing progress toward general intelligence.

## Method Summary
AGITB is a signal-level benchmark comprising twelve automated tests based on axioms derived from biological computation principles. The framework evaluates models' ability to learn and predict temporal binary sequences without pretraining or symbolic representations. Models must satisfy all twelve axioms through self-referential comparisons rather than conventional correctness metrics. The benchmark emphasizes autonomous learning across unseen environments and resists brute-force or memorization-based strategies.

## Key Results
- Introduces twelve axiom-based automated tests for AGI evaluation
- No contemporary AI systems meet all twelve criteria
- Successfully resists memorization-based strategies through self-referential metrics

## Why This Works (Mechanism)
AGITB works by evaluating fundamental computational properties through low-level signal processing rather than task-specific performance. The twelve axioms capture essential characteristics of biological intelligence including determinism, adaptability, and generalization. By requiring models to predict temporal binary sequences without prior knowledge or symbolic representations, the benchmark tests core learning capabilities that transcend domain-specific skills. The self-referential comparison approach prevents models from exploiting predefined metrics or memorization strategies.

## Foundational Learning
- **Temporal sequence prediction**: Models must predict future binary inputs based on past patterns; needed for testing dynamic learning capabilities; quick check: measure prediction accuracy across varying sequence lengths
- **Deterministic processing**: Systems must produce consistent outputs for identical inputs; ensures reliability and reproducibility; quick check: verify output consistency across multiple runs
- **Adaptive learning**: Models must adjust to changing input patterns without catastrophic forgetting; tests generalization beyond memorized sequences; quick check: introduce pattern shifts and measure adaptation speed
- **Autonomous operation**: Systems function without external guidance or domain-specific knowledge; ensures true general capability; quick check: verify no dependency on task-specific training data
- **Self-referential evaluation**: Models assess their own performance without external benchmarks; prevents metric gaming; quick check: confirm internal consistency of self-evaluation metrics
- **Binary signal processing**: Focus on fundamental signal processing rather than high-level abstractions; tests core computational abilities; quick check: validate binary input/output handling

## Architecture Onboarding
**Component map**: Binary signal generator -> Temporal buffer -> Prediction engine -> Self-evaluation module -> Output comparator

**Critical path**: Signal generation → Pattern extraction → Prediction → Self-validation → Result aggregation

**Design tradeoffs**: The framework prioritizes fundamental learning capabilities over task-specific performance, sacrificing domain expertise for generality. Self-referential metrics prevent metric exploitation but may be harder to interpret than conventional scores.

**Failure signatures**: 
- Consistent prediction errors indicate pattern recognition limitations
- Unstable self-evaluation suggests unreliable internal metrics
- Inability to adapt to pattern shifts reveals catastrophic forgetting
- Dependence on symbolic representations violates core axioms

**First experiments**:
1. Test basic binary sequence prediction with varying complexity levels
2. Evaluate adaptation to pattern shifts in previously learned sequences
3. Measure consistency of self-evaluation metrics across multiple runs

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation data from actual model testing
- Unclear robustness of self-referential measures to implementation variations
- Exclusion of pretraining and prior knowledge may limit real-world applicability

## Confidence
- **High confidence**: Theoretical foundation based on axioms reflecting biological computation principles
- **Medium confidence**: Framework's ability to resist memorization-based strategies and provide meaningful discrimination
- **Low confidence**: Practical applicability given exclusion of pretraining and prior knowledge

## Next Checks
1. Conduct systematic benchmarking of diverse AI architectures (transformer-based, recurrent, neuromorphic) to establish baseline performance distributions and identify whether the benchmark can meaningfully differentiate between architectures.

2. Perform ablation studies to determine which axioms are most critical for discriminating intelligence levels and whether subsets of the twelve tests could provide equivalent or better assessment capability.

3. Test the benchmark's sensitivity to variations in implementation details (sampling rates, noise levels, sequence lengths) to establish the robustness of self-referential comparison metrics across different experimental conditions.