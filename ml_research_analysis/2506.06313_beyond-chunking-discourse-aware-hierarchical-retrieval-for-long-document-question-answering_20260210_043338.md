---
ver: rpa2
title: 'Beyond Chunking: Discourse-Aware Hierarchical Retrieval for Long Document
  Question Answering'
arxiv_id: '2506.06313'
source_url: https://arxiv.org/abs/2506.06313
tags:
- uni00000013
- discourse
- retrieval
- tree
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DISRetrieval is a discourse-aware hierarchical retrieval framework
  for long-document question answering that leverages rhetorical structure theory
  (RST) to improve retrieval quality. The method adapts RST parsing to sentence-level
  granularity and cross-lingual applicability, enhances tree nodes using LLM-based
  summarization, and employs structure-guided hierarchical evidence selection.
---

# Beyond Chunking: Discourse-Aware Hierarchical Retrieval for Long Document Question Answering

## Quick Facts
- arXiv ID: 2506.06313
- Source URL: https://arxiv.org/abs/2506.06313
- Reference count: 40
- DISRetrieval achieves up to 2.66% higher F1 scores on QASPER and 1.30% higher F1 on Chinese documents compared to baselines.

## Executive Summary
This paper introduces DISRetrieval, a discourse-aware hierarchical retrieval framework for long-document question answering that leverages Rhetorical Structure Theory (RST) to improve retrieval quality. Unlike traditional chunking approaches that partition text into fixed-size segments, DISRetrieval builds discourse trees that capture the semantic and structural relationships between sentences, preserving context that arbitrary chunking severs. The method adapts RST parsing to sentence-level granularity and cross-lingual applicability, enhances tree nodes using LLM-based summarization, and employs structure-guided hierarchical evidence selection to optimize precision and context coverage.

## Method Summary
DISRetrieval constructs discourse trees using a transition-based RST parser trained on sentence-level data from RST-DT with Chinese translation augmentation. The framework builds paragraph-level trees, enhances internal nodes with LLM-generated summaries based on a threshold τ, integrates these into document-level trees, and encodes all nodes using dense embeddings. Retrieval employs a dual-selection strategy that either directly retrieves the most relevant leaf or expands from a high-scoring internal node to its top-k relevant leaf sentences, balancing semantic relevance with discourse coherence.

## Key Results
- DISRetrieval achieves consistent improvements across four datasets: QASPER (+2.66% F1), QuALITY (+0.88% accuracy), NarrativeQA (competitive BLEU/ROUGE scores), and MultiFieldQA-zh (+1.30% F1 on Chinese documents).
- The framework shows strong robustness across document types including research papers, fiction, books, and scripts, with preprocessing costs amortized across multiple queries.
- Computational efficiency is demonstrated with approximately 3× speedup compared to existing hierarchical methods like RAPTOR, while maintaining superior retrieval quality.

## Why This Works (Mechanism)

### Mechanism 1
Partitioning text based on rhetorical relations preserves semantic coherence better than fixed-size or semantic-clustering chunking. The system uses RST to build hierarchies where sentences are grouped by functional relationships (e.g., elaboration, contrast), maintaining context that arbitrary chunking severs.

### Mechanism 2
Generating LLM summaries for internal tree nodes bridges the gap between abstract structural relations and concrete semantic retrieval. Internal nodes represent relationships but lack searchable text content; LLM summarization creates semantic anchors allowing queries to match high-level concepts, not just keywords in leaf sentences.

### Mechanism 3
A dual-selection retrieval strategy (leaf selection vs. subtree expansion) optimizes evidence precision and context coverage. Instead of returning just the single best node, the algorithm checks if a high-scoring node is internal and, if so, retrieves the top-k relevant leaf sentences from that node's subtree, capturing both the specific answer and necessary surrounding context.

## Foundational Learning

- **Rhetorical Structure Theory (RST)**: The theoretical backbone where "nucleus" (core info) vs. "satellite" (supporting info) relationships determine how sentences are grouped in trees. Quick check: How does an RST parser treat a sentence that elaborates on a previous point compared to one that contrasts it?

- **Transition-Based Parsing**: The shift-reduce parser (stack/queue system) used to build trees efficiently. Quick check: In a shift-reduce parser, what happens to the queue when a "reduce" action is triggered?

- **Dense Retrieval (Bi-Encoders)**: The framework relies on cosine similarity between query embeddings and node embeddings (SBERT/OpenAI). Quick check: Why might a bi-encoder struggle with exact keyword matches compared to BM25, and how does hierarchical approach mitigate that?

## Architecture Onboarding

- **Component map**: RST Adapter -> Tree Builder -> Node Enhancer -> Indexer -> Retriever
- **Critical path**: 1) RST parser must output valid trees; 2) LLM summarization is computational bottleneck; 3) Dual-selection logic determines final context window
- **Design tradeoffs**: Sentence-level parsing (faster/robust vs. EDU-level fine-grained relations), τ threshold (semantic clarity vs. computational cost), Top-k selection (precision vs. context coverage)
- **Failure signatures**: Flat performance indicates RST parser failure to build deep trees; high latency suggests LLM summarization issues; hallucinated context indicates summarization errors
- **First 3 experiments**: 1) Visualize RST parser output on 5 sample documents; 2) Ablate τ values (0 vs 50) on validation set; 3) Compare against "Bisection" baseline to isolate linguistic structure value

## Open Questions the Paper Calls Out

- **Parser Generalization**: Can the language-universal discourse parser generalize to typologically distant languages beyond English and Chinese without requiring additional translated training data through current augmentation strategy?

- **Adaptive Thresholding**: Would content-adaptive or position-aware dynamic thresholding yield meaningful improvements over the fixed τ threshold approach for LLM-enhanced node summarization?

- **Evaluation Metrics**: How can evaluation metrics better capture discourse-aware retrieval benefits such as structural coherence preservation and hierarchical information flow?

## Limitations

- **Parser Domain Transfer**: RST parser trained on RST-DT may not generalize effectively to domain-specific jargon in scientific papers or fiction without domain adaptation.

- **LLM Dependency**: Computational efficiency claims rely on preprocessing costs, but inference-time differences and LLM hallucination risks are not fully addressed.

- **Cross-Lingual Structure Transfer**: The paper doesn't analyze whether English RST structures effectively translate to Chinese rhetorical patterns or identify language-specific limitations.

## Confidence

- **High Confidence**: Core claim that discourse-aware hierarchical retrieval improves long-document QA performance is well-supported by consistent dataset improvements and methodology specification.
- **Medium Confidence**: LLM-based node enhancement contribution is well-motivated but lacks detailed quality analysis and hallucination assessment.
- **Low Confidence**: Claims about robustness across document types and languages lack comprehensive error analysis and domain-specific failure mode documentation.

## Next Checks

1. **Parser Domain Transfer Analysis**: Evaluate RST parser accuracy separately on each dataset (QASPER, QuALITY, NarrativeQA, MultiFieldQA-zh) to identify performance degradation patterns and domain-specific limitations.

2. **Ablation on Node Enhancement**: Systematically vary the τ threshold (0, 25, 50, 75, 100) on validation sets for each dataset to quantify LLM summarization contribution versus pure structural information.

3. **Cross-Lingual Discourse Structure Comparison**: Analyze whether the same discourse relations appear with similar frequencies in English and Chinese documents, and whether parser performance correlates with discourse relation distribution differences.