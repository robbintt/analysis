---
ver: rpa2
title: 'HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization'
arxiv_id: '2401.09716'
source_url: https://arxiv.org/abs/2401.09716
tags:
- domain
- prompt
- learning
- hcvp
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain generalization in computer
  vision, where the goal is to create models that can perform well on unseen target
  domains by learning invariant features across multiple source domains. The authors
  propose a novel method called Hierarchical Contrastive Visual Prompt (HCVP), which
  supplements the model with domain-level and task-specific characteristics to enhance
  the separation of invariant features from specific ones.
---

# HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization

## Quick Facts
- arXiv ID: 2401.09716
- Source URL: https://arxiv.org/abs/2401.09716
- Reference count: 40
- Primary result: State-of-the-art average accuracy of 79.1% across five domain generalization benchmark datasets

## Executive Summary
This paper addresses the domain generalization problem in computer vision by proposing Hierarchical Contrastive Visual Prompt (HCVP). The method generates instance-dependent visual prompts that capture both domain-level and task-specific information through a hierarchical prompt generation network. These prompts are integrated into a frozen Vision Transformer backbone using a prompt modulation network, with contrastive learning losses that enforce feature alignment across domains while preserving class information. Experiments demonstrate superior performance compared to existing methods across five benchmark datasets.

## Method Summary
HCVP generates domain-level and task-specific prompts through a hierarchical prompt generation network (HPGN) using frozen ResNet features. These prompts are transformed by a prompt modulation network (PMN) and injected into each layer of a frozen ViT backbone. The training objective combines classification loss with two contrastive losses: Prompt Contrastive Learning (PCL) for clustering prompts by domain and task, and Class-conditional Contrastive Invariance (CCI) for aligning features of the same class across different domains. The method is trained with AdamW optimizer for 3000 steps using a batch size of 32.

## Key Results
- Achieves state-of-the-art average accuracy of 79.1% across five benchmark datasets
- Outperforms previous methods by 1.4% on PACS, 1.3% on VLCS, 1.7% on OfficeHome, and 1.2% on TerraIncognita
- Demonstrates significant improvement on challenging Sketch domain (PACS) where generalization is typically difficult

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Feature Disentanglement via Prompts
The HPGN forces structural separation between domain (style) and task (content) attributes by processing features through different branches - an MLP with Global Average Pooling for domain prompts and convolutional layers for task prompts. This decomposition allows more effective isolation of invariant features compared to monolithic prompts.

### Mechanism 2: Invariant Representation via Contrastive Regularization
PCL and CCI losses create dual pressure: PCL clusters prompts by domain and task while CCI aligns features of the same class across domains. This theoretically reduces the conditional mutual information between representation and domain label given the class, promoting domain-invariant features.

### Mechanism 3: Dynamic Modulation of Frozen Foundation Models
By modulating a frozen ViT with instance-dependent prompts rather than updating parameters, the method adapts to distribution shifts without destroying generic features learned during pre-training. This allows contextual "steering" of the backbone for each instance.

## Foundational Learning

- **Vision Transformers (ViT) & Prompt Tuning:** Understanding how tokens/patches are processed and where prompts can be injected is essential for grasping the modulation mechanism.
  - *Quick check:* Do you understand the difference between prepending prompts to the input space vs. deep prompt tuning (modifying attention layers)?

- **Domain Generalization (DG) vs. Domain Adaptation (DA):** DG assumes target domain data is inaccessible during training, making mechanisms relying on target statistics invalid.
  - *Quick check:* Can you explain why a standard domain discriminator (like in DANN) might struggle in a DG setting compared to the method proposed here?

- **Mutual Information in Representation Learning:** The authors frame their losses around reducing I(Z; D|Y) and increasing I(Z; Y), which is central to understanding their theoretical justification.
  - *Quick check:* Does minimizing the distance between same-class samples from different domains conceptually increase or decrease the dependence of the representation Z on the domain D?

## Architecture Onboarding

- **Component map:** Input image -> Frozen ResNet -> HPGN (Domain Prompt + Task Prompt) -> PMN (Layer-wise Prompt Vectors) -> ViT Layers -> Output Token -> Classifier Head & Losses

- **Critical path:** 1) Input image -> ResNet (Frozen). 2) ResNet Features -> HPGN -> Generate p_domain and p_task. 3) p_domain + p_task -> PMN -> Layer-wise Prompt Vectors. 4) Image Patches + Prompts -> ViT Layers -> Output Token. 5) Output Token -> Classifier Head & CCI Loss. 6) Prompts -> PCL Loss.

- **Design tradeoffs:** HCVP introduces significant overhead (HPGN + PMN) compared to simple Linear Probing, but outperforms it significantly (Table I). The authors freeze the backbone to preserve generalization, trading off potential specificity to source domains for better out-of-distribution robustness.

- **Failure signatures:** High Inter-Domain Distance indicates PCL/CCI losses are not weighted correctly. Prompt Collapse (no clustering by domain/class in t-SNE) suggests vanishing gradients. Performance Drop on "Sketch" domain indicates insufficient texture-to-shape invariance.

- **First 3 experiments:** 1) PCL/CCI Ablation: Replicate "HCVP w/o L_PCL" and "w/o L_CCI" settings to verify contrastive losses' contribution. 2) Hyperparameter Sensitivity: Run grid search on λ_PCL and λ_CCI on validation set. 3) Prompt Visualization: Generate t-SNE plots of prompts on held-out domain to verify hierarchical disentanglement.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can cross-modal information (e.g., textual descriptions) improve generalization in domains where visual features are ambiguous? The current architecture relies solely on visual inputs and has not experimented with multi-modal fusion.

- **Open Question 2:** Can the hypothesis that HCVP reduces conditional mutual information I(Z, D|Y) be empirically validated using direct estimation methods rather than proxy metrics? The authors used inter-domain feature distances as an alternative due to computational challenges.

- **Open Question 3:** Does the dependency on a frozen ResNet encoder limit prompt diversity and adaptability for domains significantly different from ImageNet? The paper assumes the pre-trained encoder provides sufficient basis for all source domains without ablation.

## Limitations

- The hierarchical prompt generation assumes linear separability of domain and task features, which may not hold for highly entangled datasets.
- Heavy reliance on contrastive learning introduces sensitivity to batch composition and hyperparameter settings.
- Frozen backbone assumption limits adaptation potential for extreme domain shifts.

## Confidence

- **High:** The core mechanism of hierarchical prompt generation and modulation is well-defined and technically sound.
- **Medium:** The empirical results showing state-of-the-art performance are robust, though dependent on specific hyperparameter settings.
- **Low:** The theoretical claims about feature disentanglement and causal invariance lack rigorous mathematical proof.

## Next Checks

1. Conduct ablation studies on frozen vs. fine-tuned backbones to quantify the trade-off between generalization and adaptation.
2. Test performance on more diverse datasets with extreme domain shifts (e.g., Sketch to Photo in PACS) to validate robustness claims.
3. Analyze prompt distribution and feature alignment across domains using t-SNE visualizations to verify the hierarchical disentanglement mechanism.