---
ver: rpa2
title: 'From Markov to Laplace: How Mamba In-Context Learns Markov Chains'
arxiv_id: '2502.10178'
source_url: https://arxiv.org/abs/2502.10178
tags:
- mamba
- markov
- estimator
- optimal
- convolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the in-context learning (ICL) capabilities of
  Mamba, a selective state space model, on Markov chain prediction tasks. The authors
  propose a novel framework using random Markov chains to theoretically and empirically
  analyze Mamba's ICL performance.
---

# From Markov to Laplace: How Mamba In-Context Learns Markov Chains

## Quick Facts
- **arXiv ID:** 2502.10178
- **Source URL:** https://arxiv.org/abs/2502.10178
- **Reference count:** 40
- **Key outcome:** Even a single-layer Mamba efficiently learns the optimal Laplacian smoothing estimator for all Markovian orders, unlike transformers which require multiple layers.

## Executive Summary
This paper studies the in-context learning (ICL) capabilities of Mamba, a selective state space model, on Markov chain prediction tasks. The authors propose a novel framework using random Markov chains to theoretically and empirically analyze Mamba's ICL performance. They show that even a single-layer Mamba can efficiently learn the optimal Laplacian smoothing estimator for all Markovian orders, unlike transformers which require multiple layers. Theoretically, the authors characterize Mamba's representation capacity and demonstrate that convolution plays a fundamental role in enabling it to represent the optimal estimator. Empirically, they validate these findings on both Markovian and natural language data, showing that convolution significantly improves Mamba's performance on language modeling tasks. The paper provides the first formal connection between Mamba and optimal statistical estimators, offering insights into Mamba's learning mechanisms and suggesting promising research directions.

## Method Summary
The authors analyze Mamba's in-context learning on random k-th order Markov chains with transition matrix P sampled from a Dirichlet prior Dir(β·1). The model learns to predict the next token given a context sequence. They use a Mamba-2 architecture (1 layer for Markov, 4 layers for WikiText-103) trained with AdamW (β1=0.9, β2=0.95), LR=0.001, cosine scheduler, 10000 iterations, weight decay 1e-3, dropout=0. They systematically vary batch size, embedding dim, sequence length, and convolution window size across multiple repetitions. The key metric is cross-entropy loss compared against the optimal Laplacian smoothing estimator (count+β)/(total+2β).

## Key Results
- Single-layer Mamba efficiently learns the optimal Laplacian smoothing estimator for all Markovian orders.
- Convolution with window size w ≥ k+1 is fundamental for enabling higher-order context identification.
- On WikiText-103, Mamba outperforms transformers with comparable parameters when both use convolution.
- The selective gating mechanism enables Mamba to reset its state when the underlying data distribution changes.

## Why This Works (Mechanism)

### Mechanism 1: Convolution for Context Identification
The convolution layer is the critical component enabling the model to identify the specific k-length context (prefix) required for higher-order prediction. A convolution with window size w ≥ k+1 processes the current token x_t and its immediate history, allowing the model to distinguish between sequences that share identical token counts but different transition dynamics (e.g., alternating vs. block sequences). The convolution output conditions the state update, ensuring counts are accumulated for the correct transition pair (x_{t-k}, ..., x_t).

### Mechanism 2: Recurrent State as Frequency Counter
The selective state space model (S6) layer functions as a linear recurrent accumulator of transition counts, implementing the Laplacian smoothing estimator. The hidden state H_t is updated via H_t = a_t H_{t-1} + e_{xt} b_t^T. By setting the decay a_t ≈ 1, the state retains information from all previous timesteps. The projections b_t and c_t (influenced by convolution) effectively increment specific entries in H_t corresponding to observed transitions, turning the state matrix into a storage for the transition counts n_ij required by the estimator.

### Mechanism 3: Selective Gating for Non-Stationarity
The input-dependent decay parameter a_t enables the model to reset its belief state when the underlying data distribution changes, supporting non-stationary contexts. In a "Switching Markov" setup, the optimal strategy is to discard old counts when a new regime begins. The model learns to set the decay a_t close to 0 when a "switch" token is observed (via the input-dependency of Δ_t), effectively wiping the state H_t. For stationary sequences, a_t stays ≈ 1.

## Foundational Learning

- **Concept: Laplacian Smoothing (Add-β Estimator)**
  - **Why needed here:** This is the theoretical "optimal estimator" the paper proves Mamba approximates. It estimates probability as (count + β) / (total + 2β) to handle unseen transitions.
  - **Quick check question:** Why does adding β prevent the "zero probability" problem in prediction?

- **Concept: Markov Chains of Order k**
  - **Why needed here:** The data generating process. The next token depends only on the previous k tokens. The paper explicitly links the convolution window size to this order (w = k+1).
  - **Quick check question:** If a chain is Order 2, why is looking at only the last token insufficient for optimal prediction?

- **Concept: Selective State Space Models (S6)**
  - **Why needed here:** Understanding that the core mechanic is a recurrent update H_t = a_t H_{t-1} + ... where parameters depend on the input. This distinguishes it from fixed RNNs.
  - **Quick check question:** How does making the decay a_t dependent on the input x_t differ from a standard Linear Time Invariant (LTI) system?

## Architecture Onboarding

- **Component map:** Embedding -> Conv1d (Fundamental) -> Selective Scan (SSM) -> Output Projection
- **Critical path:** The Conv1d -> SSM parameters (a_t, b_t, c_t) path is the primary driver of performance identified in this paper.
- **Design tradeoffs:**
  - Window Size (w) vs. Order (k): To handle Order k dependencies, you must set w ≥ k+1. Increasing w increases computation slightly but is necessary for expressivity on complex dependencies.
  - State Size (N) vs. Vocabulary: Theorem 2 suggests state capacity must scale with the complexity of the transition matrix (2^k).
- **Failure signatures:**
  - Loss plateau: If loss converges but remains strictly above the optimal Laplacian loss, check if the convolution window is too small (w ≤ k) or if the state dimension is insufficient.
  - Amnesia: If the model fails on long sequences, check if a_t is erroneously decaying too fast (< 1) on stationary data.
- **First 3 experiments:**
  1. **Order-1 Sanity Check:** Train a 1-layer Mamba on binary Order-1 Markov data. Verify the predicted probabilities match the (n_1 + β)/(n + 2β) formula exactly.
  2. **Ablation of Convolution:** Retrain with convolution removed (or w=1). Confirm performance degrades to random guessing or sub-optimal levels (cannot distinguish confusable sequences).
  3. **Switching Task:** Train on "Switching Markov" data. Plot a_t values. Verify they drop to 0 precisely at switch tokens and remain at 1 otherwise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical characterization of Mamba's representation capacity be extended to deeper (multi-layer) Mamba models, and what role does depth play in Mamba's ICL capabilities?
- **Basis in paper:** [explicit] The conclusion states: "Extending our results to deeper Mamba models and understanding the role of depth are some interesting future directions."
- **Why unresolved:** All theoretical results in the paper (Theorem 1) address only single-layer Mamba (MambaZero). The proof construction for Laplacian smoothing representation is specific to one layer.
- **What evidence would resolve it:** A formal theorem characterizing the representation capacity of L-layer Mamba for Laplacian smoothing, or empirical studies showing whether additional layers provide complementary ICL capabilities beyond what a single layer achieves.

### Open Question 2
- **Question:** Can the conjecture that MambaZero represents order-k Laplacian smoothing with d = N = 2^{k+1} dimensions be formally proven?
- **Basis in paper:** [explicit] Conjecture 1 states this claim, but authors note "the proof becomes intractable due to the difficulty in tracking the correlations between the transition counts as k increases."
- **Why unresolved:** The first-order proof relies on specific correlations between transition counts (n_01 and n_10 differ by at most one) that become more complex for higher orders, making the construction difficult to generalize.
- **What evidence would resolve it:** A constructive proof showing explicit parameter choices that realize the add-β estimator for arbitrary k, or alternatively, a counterexample showing the conjectured dimensions are insufficient.

### Open Question 3
- **Question:** How does the gating mechanism introduced in Mamba-2 affect the convolutional interpretation that enables Laplacian smoothing representation?
- **Basis in paper:** [inferred] The related work section states that gating in Mamba-2 and similar SSM architectures "invalidates to a certain extent the convolutional view of the architecture," yet the paper demonstrates convolution is fundamental for ICL.
- **Why unresolved:** The theoretical analysis focuses on MambaZero without gating, while empirical results use Mamba-2. The interaction between gating and convolution-based ICL mechanisms remains uncharacterized.
- **What evidence would resolve it:** Ablation studies on Mamba-2 isolating the gating mechanism's effect on Markov chain prediction, or theoretical analysis showing whether gating preserves or disrupts the Laplacian smoothing representation.

## Limitations
- The theoretical claims are primarily validated on synthetic Markov chain data, with less rigorous analysis of generalization to natural language data.
- The paper does not fully explore scaling behavior of Mamba's ICL performance with respect to model size, sequence length, or data complexity.
- The ablation studies do not exhaustively explore the design space of the Mamba architecture (activation functions, normalization schemes, MLP block impact).
- Analysis focuses on Mamba-2 variant, and results may not generalize to other Mamba variants or similar SSM architectures.

## Confidence
- **High Confidence:** Theoretical characterization of Mamba's representation capacity for Laplacian smoothing on Markov chains, and empirical validation on synthetic data.
- **Medium Confidence:** Generalization of Mamba's ICL capabilities to natural language data on WikiText-103.
- **Medium Confidence:** Identification of convolution as the critical architectural component enabling higher-order context identification.

## Next Checks
1. **Scaling Analysis:** Systematically investigate how Mamba's ICL performance scales with model size (layers, hidden dimension), sequence length, and Markov chain order to determine practical limits and inform architecture design.
2. **Architecture Ablation:** Perform exhaustive ablation of Mamba architecture exploring activation functions beyond ReLU, normalization schemes, and the role of the MLP block to understand architectural factors contributing to performance.
3. **Real-World Data Diversity:** Evaluate Mamba's ICL performance on broader range of natural language datasets with more complex dependencies and longer-range contexts to assess robustness and practical applicability.