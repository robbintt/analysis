---
ver: rpa2
title: 'Single Index Bandits: Generalized Linear Contextual Bandits with Unknown Reward
  Functions'
arxiv_id: '2506.12751'
source_url: https://arxiv.org/abs/2506.12751
tags:
- function
- reward
- bound
- have
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of contextual bandits with unknown
  reward functions (single index bandits), where existing generalized linear bandit
  methods fail due to reliance on known link functions. The authors propose a novel
  Stein's method-based estimator that achieves optimal error rates under mild assumptions,
  without requiring knowledge of the reward function.
---

# Single Index Bandits: Generalized Linear Contextual Bandits with Unknown Reward Functions

## Quick Facts
- **arXiv ID**: 2506.12751
- **Source URL**: https://arxiv.org/abs/2506.12751
- **Reference count**: 40
- **Primary result**: Novel Stein's method-based estimator for contextual bandits with unknown reward functions, achieving optimal error rates without requiring knowledge of the link function

## Executive Summary
This paper addresses the problem of contextual bandits where the reward function is unknown, extending beyond the traditional generalized linear bandit framework that requires knowledge of the link function. The authors propose a novel approach using Stein's identity to estimate the unknown parameter without requiring explicit knowledge of the reward function's form. Their method achieves optimal regret bounds under mild assumptions and extends naturally to sparse high-dimensional settings. The algorithms are computationally efficient and demonstrate superior performance compared to existing methods, particularly in cases of model misspecification.

## Method Summary
The authors develop a Stein's method-based estimator that leverages the score function and Stein's identity to relate expectations of transformed observations to the unknown parameter θ*. By combining this with truncation techniques to control variance, they create estimators that work without knowledge of the reward function's form. The framework includes three main algorithms: STOR (Explore-then-Commit) achieving O(T^{2/3}) regret, ESTOR (epoch-based) achieving O(√T) regret under monotonicity assumptions, and GSTOR for general reward functions achieving O(T^{3/4}) regret. The methods extend naturally to sparse high-dimensional settings with sparsity-dependent regret bounds.

## Key Results
- Novel Stein's method-based estimator achieves optimal error rates without requiring knowledge of the reward function
- Three algorithms developed: STOR (O(T^{2/3}) regret), ESTOR (O(√T) regret under monotonicity), and GSTOR (O(T^{3/4}) regret)
- Methods extend to sparse high-dimensional settings with sparsity-dependent regret bounds
- Experiments show superior performance compared to existing GLB algorithms, especially under model misspecification
- Computationally efficient (hundreds to thousands of times faster than baselines)

## Why This Works (Mechanism)
The key insight is using Stein's identity to relate E[y_i S(x_i)] to the unknown parameter θ* through the score function, combined with truncation to control variance. This allows estimation of θ* without explicit knowledge of the reward function's form. The score function's smoothness and the existence of a Stein kernel enable this approach to work under mild assumptions. The truncation parameter τ provides a practical mechanism for balancing bias and variance in the estimator.

## Foundational Learning

**Stein's Identity**: A mathematical relationship between expectations of functions of random variables and their derivatives, used here to bypass the need for explicit knowledge of the reward function form. Why needed: Enables estimation without knowing the link function. Quick check: Verify that the score function S satisfies Stein's identity for the assumed exponential family.

**Contextual Bandits Framework**: Sequential decision-making setting where an agent selects actions based on context and receives rewards. Why needed: Provides the problem structure for the algorithm. Quick check: Confirm that the context vectors x_t satisfy the required assumptions (boundedness, etc.).

**Explore-Then-Commit (ETC) Strategy**: A bandit algorithm that explores for a fixed period, then commits to the best arm. Why needed: Provides a simple baseline algorithm with provable regret bounds. Quick check: Verify that the exploration phase is sufficiently long to estimate parameters accurately.

## Architecture Onboarding

**Component Map**: Context generation -> Action selection -> Reward observation -> Stein estimator update -> Parameter estimate update -> Next action selection

**Critical Path**: The most time-critical sequence is Context generation → Action selection → Reward observation → Stein estimator update → Parameter estimate update. This loop must complete within the time budget for each time step.

**Design Tradeoffs**: The truncation parameter τ trades off between bias and variance - smaller τ reduces variance but increases bias. The choice between STOR, ESTOR, and GSTOR depends on whether monotonicity assumptions hold and the desired regret bound.

**Failure Signatures**: Poor performance may indicate incorrect truncation parameter choice, violation of the smoothness assumption for the score function, or inappropriate assumptions about the noise distribution. Non-convergence of the estimator suggests issues with the Stein identity assumptions.

**Three First Experiments**:
1. Verify Stein identity holds for the assumed exponential family distributions with the chosen score function
2. Test the truncation mechanism's effect on variance reduction across different noise levels
3. Compare regret performance of STOR vs ESTOR on synthetic data with known monotonicity properties

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely heavily on smoothness assumptions of the score function and existence of Stein kernel
- Truncation parameter τ requires careful tuning but lacks systematic practical guidance
- Assumes independent noise across time steps, which may be violated in real-world applications
- Extension to arbitrary reward functions may not be robust beyond the tested cases

## Confidence

- **High confidence**: Theoretical regret bounds for STOR and ESTOR under stated assumptions; computational efficiency claims from synthetic experiments
- **Medium confidence**: Extension to sparse high-dimensional settings; practical performance comparisons with baselines
- **Low confidence**: Generalization to arbitrary reward functions; robustness to moderate misspecification beyond shown experiments

## Next Checks

1. **Sensitivity analysis of the truncation parameter**: Systematically evaluate how different choices of τ affect both regret performance and estimator variance across various reward function families

2. **Distribution robustness testing**: Evaluate algorithm performance when noise distribution deviates from assumed exponential family, including heavy-tailed and multimodal distributions

3. **Non-i.i.d. noise evaluation**: Test algorithms in settings where noise exhibits temporal correlation or heteroscedasticity to assess practical impact of independence assumption