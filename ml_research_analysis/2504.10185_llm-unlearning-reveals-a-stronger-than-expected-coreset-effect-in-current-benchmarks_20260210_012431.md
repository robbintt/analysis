---
ver: rpa2
title: LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks
arxiv_id: '2504.10185'
source_url: https://arxiv.org/abs/2504.10185
tags:
- unlearning
- coreset
- forget
- selection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper uncovers a surprising coreset effect in LLM unlearning:
  benchmarks like WMDP and MUSE can be effectively unlearned using as little as 5%
  of the original forget set, even with random selection. This effect holds across
  multiple unlearning methods (NPO and RMU), benchmarks (WMDP-Bio, WMDP-Cyber, MUSE-Books,
  MUSE-News), and coreset selection strategies (random, GRAND, MODERATE, MIN-K% PROB).'
---

# LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks
## Quick Facts
- **arXiv ID**: 2504.10185
- **Source URL**: https://arxiv.org/abs/2504.10185
- **Reference count**: 40
- **Primary result**: Current LLM unlearning benchmarks can be effectively unlearned using as little as 5% of the original forget set, revealing significant redundancy in benchmark design.

## Executive Summary
This paper reveals a surprising coreset effect in current LLM unlearning benchmarks, demonstrating that benchmarks like WMDP and MUSE can achieve comparable unlearning performance using only 5% of the original forget set data. The effect holds across multiple unlearning methods (NPO and RMU), benchmarks (WMDP-Bio, WMDP-Cyber, MUSE-Books, MUSE-News), and coreset selection strategies (random, GRAND, MODERATE, MIN-K% PROB). Keyword analysis shows that high-impact tokens extracted from small coresets drive most unlearning performance, suggesting that current benchmarks contain significant redundancy and that unlearning effectiveness is determined by compact token sets rather than entire datasets.

## Method Summary
The paper investigates the effectiveness of using small coreset subsets of forget sets for LLM unlearning across multiple benchmarks and unlearning methods. Experiments employ both random and specialized coreset selection strategies (GRAND, MODERATE, MIN-K% PROB) to identify minimal subsets that can achieve comparable unlearning performance to full forget sets. The study evaluates unlearning effectiveness using UE and UT metrics across WMDP-Bio, WMDP-Cyber, MUSE-Books, and MUSE-News benchmarks, with LLaMA-2-7B-Chat as the base model. The analysis includes keyword extraction to identify high-impact tokens driving unlearning performance and assesses downstream effects on model connectivity, jailbreaking robustness, and utility on auxiliary tasks.

## Key Results
- 5% coreset unlearning achieves comparable UE (up to 72.03±1.78) and UT (MMLU accuracy ~56.69) to full forget set unlearning (UE ~69.46, UT ~57.48) on WMDP-Bio
- High-impact tokens extracted from small coresets drive most unlearning performance, revealing token-level redundancy in forget sets
- Coreset-unlearned models exhibit similar mode connectivity, jailbreaking robustness, and utility on auxiliary tasks as full-set models, though slightly more vulnerable to relearning attacks

## Why This Works (Mechanism)
The paper demonstrates that unlearning performance is driven by a small subset of high-impact tokens rather than entire datasets, revealing fundamental redundancy in current benchmark design. The coreset effect emerges because forget sets contain overlapping or redundant information, where specific tokens and patterns are sufficient to achieve target unlearning objectives. This suggests that current unlearning benchmarks may not be testing the full scope of unlearning capabilities but rather the model's response to specific token-level patterns.

## Foundational Learning
**LLM Unlearning**: Why needed - To remove specific knowledge from pre-trained models for privacy, safety, or compliance reasons; Quick check - Verify that target knowledge is no longer accessible while preserving general capabilities
**Coreset Selection**: Why needed - To identify minimal subsets that capture essential information for downstream tasks; Quick check - Measure performance degradation when using smaller subsets
**Unlearning Metrics (UE/UT)**: Why needed - To quantify effectiveness of knowledge removal and utility preservation; Quick check - Compare pre/post unlearning performance on target and auxiliary tasks
**Keyword Extraction**: Why needed - To identify high-impact tokens driving specific model behaviors; Quick check - Validate that extracted keywords capture semantic content of target knowledge
**Mode Connectivity**: Why needed - To assess similarity between full and coreset-unlearned models in weight space; Quick check - Measure linear path connectivity between model checkpoints

## Architecture Onboarding
**Component Map**: LLM Base Model -> Unlearning Method (NPO/RMU) -> Coreset Selection -> Evaluation Metrics (UE, UT, Keyword Analysis)
**Critical Path**: Data preparation (forget set/coreset) → Unlearning application → Performance evaluation → Analysis of token impact and transferability
**Design Tradeoffs**: Full forget set unlearning provides comprehensive knowledge removal but is computationally expensive; coreset unlearning offers efficiency but may miss nuanced knowledge patterns
**Failure Signatures**: Incomplete unlearning when coreset misses critical token patterns; performance degradation when coreset selection ignores domain-specific token distributions
**First Experiments**: 1) Compare random vs. specialized coreset selection strategies on WMDP-Bio; 2) Analyze keyword overlap between coreset and full forget sets; 3) Test relearning vulnerability of coreset-unlearned models

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on single base model (LLaMA-2-7B-Chat) with three benchmarks, limiting generalizability to other model families and domains
- Reliance on specific evaluation metrics (UE, UT) may not capture all aspects of unlearning effectiveness or failure modes
- Implications for practical unlearning deployment in safety-critical domains are not fully explored

## Confidence
- Coreset effectiveness across benchmarks and methods: **High confidence**
- Token-level redundancy characterization: **High confidence**
- Fundamental benchmark flaws claim: **Medium confidence** (requires broader validation)
- Generalizability to other model families: **Low confidence** (not directly tested)

## Next Checks
1. Test coreset effectiveness across additional model families (e.g., GPT, Claude) and scales (13B, 70B parameters) to assess generalizability
2. Evaluate unlearning performance on domains with different token distributions (legal, medical, financial) to identify domain-specific limitations
3. Conduct ablation studies isolating the contribution of different token types (names, technical terms, function words) to unlearning performance