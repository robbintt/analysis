---
ver: rpa2
title: 'KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge
  Graphs'
arxiv_id: '2511.18364'
source_url: https://arxiv.org/abs/2511.18364
tags:
- pipelines
- entity
- integration
- data
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KGpipe introduces a framework for defining and executing reproducible
  pipelines to integrate heterogeneous data into knowledge graphs, supporting RDF,
  JSON, and text sources via reusable tools and LLM functionality. The paper also
  proposes a benchmark for evaluating KG integration pipelines using a movie-domain
  reference KG, seed KG, and overlapping sources in multiple formats.
---

# KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge Graphs

## Quick Facts
- arXiv ID: 2511.18364
- Source URL: https://arxiv.org/abs/2511.18364
- Reference count: 39
- Primary result: KGpipe framework enables reproducible KG integration pipelines; RDF-based pipelines achieve highest quality, with LLM-assisted RDFc best matching reference KG size

## Executive Summary
KGpipe introduces a framework for defining and executing reproducible pipelines to integrate heterogeneous data into knowledge graphs, supporting RDF, JSON, and text sources via reusable tools and LLM functionality. The paper also proposes a benchmark for evaluating KG integration pipelines using a movie-domain reference KG, seed KG, and overlapping sources in multiple formats. Nine single-source and six multi-source pipelines were evaluated, measuring coverage, consistency, and efficiency. Results show that RDF-based pipelines achieve highest quality, with LLM-assisted RDFc matching the reference KG size best, while text pipelines struggle due to extraction and linking limitations. Multi-source pipelines perform similarly regardless of source order. The study demonstrates the viability of KGpipe for systematic KG construction and highlights the need for improved text integration and pipeline configuration.

## Method Summary
KGpipe provides a modular pipeline framework where tasks specify typed I/O contracts and exchange data via intermediate formats (JSON_ER, JSON_KE). Three execution backends (Python, Docker, HTTP) abstract tool implementations. The framework was evaluated using a movie-domain benchmark with 10k films split across RDF, JSON, and text sources. Nine single-source pipelines (SSP) and six multi-source pipelines (MSP) were executed, measuring coverage, consistency, and efficiency. LLM tasks (gpt-5-mini) were evaluated on a 1k subset due to cost constraints. The evaluation used four metric groups (statistical, semantic, reference overlap, efficiency) aggregated into a single score with configurable weights.

## Key Results
- RDF-based pipelines achieve highest quality metrics, with LLM-assisted RDFc best matching reference KG size
- Text pipelines struggle with coverage and consistency due to extraction and linking limitations
- Multi-source pipelines show similar performance regardless of source order
- LLM tasks excel for RDF sources but are too slow/inefficient for JSON and text at scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular task composition with typed I/O contracts enables reproducible pipeline assembly across heterogeneous tools and runtimes.
- Mechanism: Tasks specify input/output formats (e.g., TEXT→JSON_KE→RDF), validated statically before execution. Intermediate exchange formats (JSON_ER for entity matches, JSON_KE for extracted triples/links) act as glue between independently developed components. Three execution backends (Python, Docker, HTTP) abstract implementation details.
- Core assumption: Tools producing/consuming standard exchange formats can be composed reliably; file-based I/O between tasks preserves reproducibility despite overhead.
- Evidence anchors:
  - [abstract] "KGpipe for defining and executing integration pipelines that can combine existing tools or LLM functionality"
  - [section 4.2] "We introduce intermediate transformation tasks that convert outputs into the formats expected by subsequent components. These exchange tasks act as glue between independently developed modules."
  - [corpus] Related work (Plumber, Valentine) similarly uses modular composition but KGpipe extends to multi-format KG integration.

### Mechanism 2
- Claim: Source-format-aware pipeline design aligns task sequences with data structure complexity, improving quality outcomes.
- Mechanism: RDF pipelines skip extraction (already structured) and focus on schema alignment and entity resolution. JSON pipelines add data mapping to RDF before alignment. Text pipelines require extraction (NER, relation extraction), linking, then alignment—each step propagates uncertainty downstream.
- Core assumption: Matching extraction complexity to source format reduces error accumulation; structured sources yield higher quality because fewer uncertain steps.
- Evidence anchors:
  - [abstract] "Results show that RDF-based pipelines achieve highest quality, with LLM-assisted RDFc matching the reference KG size best, while text pipelines struggle due to extraction and linking limitations."
  - [section 3.2] "Because uncertainty at these tasks propagates downstream, knowledge extraction and linking dominate the pipeline."
  - [corpus] EMERGE benchmark similarly notes text-to-KG update challenges due to extraction uncertainty.

### Mechanism 3
- Claim: Benchmark-driven evaluation with aggregated metrics enables systematic comparison and ranking of alternative pipelines.
- Mechanism: Four metric groups (statistical: size/density; semantic: ontology compliance; reference: overlap with gold KG; efficiency: time/memory) are normalized and weighted into a single score. Configurable weights (α,β,γ,δ) allow different optimization targets (quality vs. efficiency).
- Core assumption: Aggregated scores meaningfully capture pipeline effectiveness; reference KG overlap correlates with real-world utility.
- Evidence anchors:
  - [abstract] "Nine single-source and six multi-source pipelines were evaluated, measuring coverage, consistency, and efficiency."
  - [section 5.3] "The total aggregated score M_total(p) of a pipeline p is the weighted average of the group metrics."
  - [corpus] OGB and KG completion benchmarks focus on link prediction but not upstream extraction quality; KGpipe addresses full pipeline evaluation.

## Foundational Learning

- Concept: RDF and Knowledge Graph Fundamentals
  - Why needed here: KGpipe assumes RDF representation; understanding triples (subject-predicate-object), ontologies (classes, properties, constraints), and SPARQL-like patterns is prerequisite for designing pipelines.
  - Quick check question: Can you explain why owl:disjointWith violations indicate semantic errors in a KG?

- Concept: Entity Resolution and Schema Matching
  - Why needed here: Core integration tasks (ER for identifying same real-world entities; schema/ontology matching for aligning relations) directly determine fusion quality. The benchmark shows ER precision/recall impacts final KG overlap.
  - Quick check question: Why might CSV-based matching (RDFb pipeline) yield more untyped entities than graph-based alignment (RDFa)?

- Concept: Information Extraction from Unstructured Text
  - Why needed here: Text pipelines require NER, relation extraction, co-reference resolution. Understanding uncertainty propagation explains why text pipelines underperform RDF pipelines.
  - Quick check question: If OpenIE extracts "directed" as a relation but the ontology uses "director," what linking step is required?

## Architecture Onboarding

- Component map:
  - Pipeline definition: YAML/Python config specifying task sequence and execution backend (Python/Docker/HTTP)
  - Task library: PARIS (RDF alignment), JedAI (record linkage), Valentine (schema matching), OpenIE (text extraction), Spotlight (entity linking), LLM tasks (LLMExtract, LLMMapping, LLMMatcher)
  - Exchange formats: JSON_ER (entity/relation matches), JSON_KE (extracted triples/links)
  - Validation layer: Static type checking before execution
  - Execution engine: Sequential task runner with file-based I/O
  - Benchmark suite: Reference KG, seed KG, 3 source splits (RDF/JSON/Text), evaluation metrics

- Critical path: Seed KG + source data → pipeline config → task execution (per backend) → intermediate exchange files → fusion → final KG → metric computation → aggregated score

- Design tradeoffs:
  - File-based I/O vs. in-memory: Chosen for reproducibility/debugging; incurs disk overhead
  - Sequential execution vs. parallel: Sequential for simplicity; limits throughput
  - LLM tasks: High semantic quality (RDFc) but high cost/time; not viable for JSON/Text at scale
  - Fixed ontology: Simplifies alignment but cannot handle schema evolution

- Failure signatures:
  - High untyped entity count (UT): Indicates failed entity/property matching (e.g., RDFb: 7,512 untyped)
  - Low reference KG overlap (R_KG): Suggests extraction/linking failures (e.g., text pipelines: ~1% overlap)
  - Disjoint type violations (O_DT): Schema confusion (e.g., entity typed as both Person and Company)
  - High runtime with low quality: Expensive steps (CSV conversion, LLM calls) without matching benefit

- First 3 experiments:
  1. Run SSP_RDFa on 1k benchmark subset; verify entity matching precision/recall (expect ~0.98/1.0 per Table 4)
  2. Compare SSP_TEXTa vs. SSP_TEXTb on same subset; analyze why Spotlight (T_A) outperforms embedding linking (T_B) for film entities
  3. Execute MSP pipeline RJT (RDF→JSON→Text) and inspect intermediate exchange files at each increment to trace where entity count drops

## Open Questions the Paper Calls Out

- Question: Can automatic pipeline generation and configuration be achieved through data-driven methods or LLM-based agents?
  - Basis in paper: [explicit] "In future work, we envision utilizing KGpipe to evaluate many pipelines for different settings to obtain training data for a largely automatic generation and configuration of pipelines."
  - Why unresolved: Current pipelines are handcrafted; the authors propose accumulating pipeline-performance pairs to train models that recommend designs and configurations, but this remains unimplemented.
  - What evidence would resolve it: A trained model or agent that, given a target ontology, seed graph, and source descriptors, outputs pipeline designs with competitive quality-efficiency trade-offs compared to manual configurations.

- Question: How can text-based integration pipelines be improved to achieve coverage and consistency comparable to RDF pipelines?
  - Basis in paper: [explicit] "Text pipelines struggle due to extraction and linking limitations" and achieve "only about half of the number of entities and less than 45% of the triples of the reference graph."
  - Why unresolved: Text pipelines suffer from inherent extraction difficulty, poor entity linking recall (e.g., TEXTb achieves only 0.12 film linking recall), and relation direction errors.
  - What evidence would resolve it: Text pipeline variants achieving entity and fact counts within 80-90% of RDF-based pipelines, with semantic validation scores comparable to RDFa/RDFc.

- Question: How generalizable are the pipeline rankings to domains beyond the movie benchmark?
  - Basis in paper: [explicit] "Extending the benchmark to additional domains is possible but time-consuming and left for future work."
  - Why unresolved: All experiments use a single movie-domain benchmark with 10,000 films; whether RDF-based pipelines would similarly outperform text pipelines in other domains is unknown.
  - What evidence would resolve it: Replication of the benchmark methodology in at least two additional domains (e.g., biomedical, product catalogs) showing consistent ranking patterns or documented domain-specific variations.

- Question: Can LLM-based integration tasks achieve efficiency suitable for large-scale JSON and text integration?
  - Basis in paper: [explicit] "The use of LLMs was only successful for RDF sources, but too slow and inefficient for JSON and text data" and these "had to be executed on the smaller 1k version... to limit execution time and monetary expenses."
  - Why unresolved: LLM pipelines (JSONc, TEXTc) were excluded from the main 10k evaluation due to cost and latency, limiting conclusions about their scalability.
  - What evidence would resolve it: LLM-based JSON and text pipelines completing the full 10k benchmark with duration and costs within 2-3x of non-LLM alternatives, while maintaining or improving quality metrics.

## Limitations

- LLM tasks show superior semantic quality for RDFc but introduce significant runtime overhead and resource requirements that may not scale to production scenarios
- Text pipelines' poor performance appears fundamentally tied to extraction and linking limitations, suggesting current text integration capabilities are inadequate for high-quality KG construction
- Benchmark is domain-specific to movies and may not generalize to other knowledge domains with substantially different entity structures and relationships

## Confidence

- High confidence: RDF pipeline quality superiority, text pipeline limitations, MSP source order independence
- Medium confidence: LLM task scalability and cost-effectiveness, benchmark domain generalizability
- Low confidence: Optimal weight configurations for aggregated metrics, long-term viability of current text integration approaches

## Next Checks

1. Scale the LLM-assisted RDFc pipeline to 10k+ entities and measure actual runtime/cost versus quality gains to determine practical viability thresholds.

2. Implement and evaluate more sophisticated multi-source fusion strategies (e.g., simultaneous multi-way alignment) to test whether source order effects emerge with advanced merging techniques.

3. Apply KGpipe to a non-movie domain (e.g., biomedical or geographic knowledge) to validate benchmark metric applicability and identify domain-specific integration challenges.