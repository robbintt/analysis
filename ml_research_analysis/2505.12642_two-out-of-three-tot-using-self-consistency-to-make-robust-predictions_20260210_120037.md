---
ver: rpa2
title: 'Two out of Three (ToT): using self-consistency to make robust predictions'
arxiv_id: '2505.12642'
source_url: https://arxiv.org/abs/2505.12642
tags:
- predictions
- prediction
- original
- adversarial
- second
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study proposes Two out of Three (ToT), an algorithm that improves\
  \ deep learning model robustness by allowing models to abstain from answering when\
  \ uncertain. Inspired by human conflict detection, ToT generates two additional\
  \ predictions\u2014one from a second view of the input using foreground object detection,\
  \ and another from hidden layer features\u2014and uses self-consistency to decide\
  \ whether to make a prediction."
---

# Two out of Three (ToT): using self-consistency to make robust predictions

## Quick Facts
- arXiv ID: 2505.12642
- Source URL: https://arxiv.org/abs/2505.12642
- Reference count: 40
- Primary result: Algorithm achieves 80%+ detection of adversarial inputs with 60-90% accuracy on confident predictions

## Executive Summary
The Two out of Three (ToT) algorithm improves deep learning model robustness by enabling selective abstention when predictions are uncertain. Inspired by human conflict detection mechanisms, ToT generates three independent predictions—from the original image, a foreground ROI view, and hidden layer features—and uses self-consistency to determine whether to make a prediction or abstain. Evaluated on image classifiers (ResNet18, ResNet50, DenseNet121, VGG19, ViT) using ImageNet subsets, ToT detects 80%+ of adversarial inputs crafted by PGD and AutoAttack while maintaining 60-90% accuracy on confident predictions. This approach offers a promising method for deploying DL models in safety-critical domains by minimizing high-stakes errors.

## Method Summary
ToT creates three independent predictions and outputs an answer only when at least two agree. The first prediction comes from the original image forward pass. The second uses an ROI extracted via open-set segmentation (Grounding DINO) with superclass prompts, optionally blurred before inference. The third derives from penultimate-layer features: these are coarse-sampled, projected via UMAP, clustered with k-means (1000 clusters), and mapped to class labels through correlation analysis. If P_orig and P_2nd agree, ToT outputs high-confidence prediction; otherwise it compares P_3rd's top-2 predictions against the others. If neither matches, ToT abstains ("Null"). This self-consistency mechanism approximates human conflict detection to improve adversarial robustness and selective prediction.

## Key Results
- Achieves 80%+ detection rate of adversarial inputs crafted by PGD and AutoAttack
- Maintains 60-90% accuracy on final predictions when provided
- Reduces incorrect answers while retaining high accuracy on confident predictions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-consistency across multiple prediction pathways provides a proxy for confidence estimation, enabling selective abstention.
- **Mechanism:** ToT generates three independent predictions and uses agreement (≥2 out of 3) to decide whether to output or abstain. Disagreement correlates with higher error probability.
- **Core assumption:** Assumption: Disagreement across views correlates with higher error probability; conversely, agreement correlates with correctness. This assumes adversarial perturbations and out-of-distribution inputs disrupt internal representations differently than clean inputs.
- **Evidence anchors:**
  - [abstract] "ToT creates two alternative predictions in addition to the original model prediction and uses the alternative predictions to decide whether it should provide an answer or not."
  - [section 2.3] "ToT makes decisions only when at least two out of three predictions (original, second and third) are consistent to minimize its mistakes."
  - [corpus] Weak direct evidence; neighbor papers address self-consistency in LLM prompting (UTSA-NLP) and regularization contexts, but not multi-view prediction for adversarial robustness.
- **Break condition:** If the three prediction pathways become correlated (e.g., ROI extraction fails to provide independent signal, or hidden features don't cluster cleanly by class), self-consistency loses discriminative power and abstention rates may rise without accuracy gains.

### Mechanism 2
- **Claim:** Gaussian blur preprocessing amplifies disagreement between original and second predictions on adversarial inputs, improving detection rates.
- **Mechanism:** Adversarial perturbations are optimized against full-resolution inputs. When ROI regions are blurred before the second prediction, the perturbation structure is disrupted, causing original and second predictions to diverge more frequently for adversarial examples than for clean inputs.
- **Core assumption:** Assumption: Adversarial perturbations are localized and fragile to spatial transformations like blur; clean predictions are more robust to moderate blur.
- **Evidence anchors:**
  - [section 3.1] "if ROIs are blurred, the accuracy of high-confident predictions increases"
  - [section 3.2, Figure 4] "Around at σ = 1.5, the ratio becomes higher than 80%" for low-confidence detection on adversarial inputs
  - [corpus] No direct corpus support; neighboring papers do not address blur-based adversarial detection.
- **Break condition:** If adversaries incorporate blur-augmented perturbations during attack generation, or if blur significantly degrades clean-image accuracy, this detection mechanism may fail or cause excessive abstention on legitimate inputs.

### Mechanism 3
- **Claim:** Penultimate-layer features, when clustered via UMAP + k-means, encode class-correlated "functional symbols" that provide a prediction pathway robust to input-space perturbations.
- **Mechanism:** Training samples are mapped to 9-dimensional symbol vectors based on their penultimate-layer feature cluster assignments. A correlation map between symbols and labels enables probabilistic inference for test inputs. This internal representation pathway may be less sensitive to adversarial pixel-space noise.
- **Core assumption:** Assumption: Functional clusters capture class-relevant structure; adversarial perturbations do not fully corrupt the symbol-to-label mapping. This assumes penultimate representations are more stable than output logits under attack.
- **Evidence anchors:**
  - [section 2.2, steps 6-9] Describes symbol extraction and correlation map construction for third prediction
  - [section 3.2, Figure 6] Final answer accuracy of 60-90% on adversarial inputs when ToT produces a prediction
  - [corpus] No direct corpus evidence for this specific clustering-to-prediction approach.
- **Break condition:** If the clustering is unstable across model checkpoints, or if adversarial attacks are crafted with access to the feature-space clustering (stronger threat model), the third prediction pathway could be compromised.

## Foundational Learning

- **Concept: Selective Prediction / Abstention**
  - **Why needed here:** ToT's core value proposition is allowing models to refuse predictions when uncertain. Understanding abstention tradeoffs (coverage vs. accuracy) is essential for evaluating deployment viability.
  - **Quick check question:** For a safety-critical application, what is the minimum acceptable "prediction rate" (1 - abstention rate) at which ToT must operate to be useful?

- **Concept: Adversarial Attacks (PGD, AutoAttack)**
  - **Why needed here:** ToT is evaluated primarily on adversarial robustness. Without grasping threat models (white-box vs. black-box, L∞ perturbation budgets), you cannot assess whether ToT's 80% detection is meaningful.
  - **Quick check question:** If an attacker knows ToT is deployed, what modifications to their attack strategy might reduce detection effectiveness?

- **Concept: Open-Set Segmentation (Grounding DINO)**
  - **Why needed here:** The second prediction relies on ROI extraction via an external segmentation model. Understanding its failure modes (missed detections, spurious regions) is critical for diagnosing ToT pipeline issues.
  - **Quick check question:** What happens to ToT's behavior if the segmentation model returns no ROI for an input?

## Architecture Onboarding

- **Component map:** Input preprocessing (optional Gaussian blur) -> Primary model (ResNet/VGG/DenseNet/ViT) -> ROI extractor (Grounding DINO) -> Second prediction (blurred ROI) -> Penultimate feature extraction -> UMAP + k-means clustering -> Correlation map lookup -> Consensus logic

- **Critical path:** 
  1. Forward pass on full image → P_orig
  2. Segmentation → ROIs → blur → resize → forward pass → P_2nd (multiple candidates)
  3. Feature extraction → symbol lookup → P_3rd (top-2)
  4. Agreement check → final output or abstention

- **Design tradeoffs:**
  - **Blur kernel size (σ):** Higher σ increases adversarial detection but also increases abstention on clean inputs. Paper finds σ ≈ 1.5–2.0 as a working range, but this is dataset-dependent.
  - **Number of top predictions from symbols:** Using more symbol predictions reduces abstention but lowers final accuracy (Figure 7C–D).
  - **Segmentation prompt granularity:** Superclass labels outperform fine-grained class labels for ROI detection, but this limits applicability to datasets with hierarchical label structures.

- **Failure signatures:**
  - **High abstention on clean inputs (>10%):** May indicate blur kernel too aggressive or segmentation failures
  - **Low adversarial detection (<60%):** Check if attack strength (L∞) is below threshold; verify blur is being applied to ROI pathway
  - **Low final accuracy on confident predictions (<70%):** Investigate clustering quality; correlation map may be underfitting due to insufficient training samples per class

- **First 3 experiments:**
  1. **Reproduce detection curves:** Run ToT on Mixed_13 with PGD attacks (L∞=0.03), sweep σ ∈ {0, 0.5, 1.0, 1.5, 2.0}, and verify detection rate approaches 80%+ at σ ≥ 1.5. This validates the core blur-based detection mechanism.
  2. **Ablate third prediction pathway:** Disable the symbol-based prediction and rely only on P_orig vs. P_2nd agreement. Measure impact on final accuracy and abstention rate to quantify the hidden-feature contribution.
  3. **Stress-test with adaptive attacks:** Craft PGD attacks that include blur augmentation in the perturbation optimization loop. Assess whether detection rate degrades significantly, which would inform threat-model assumptions for deployment.

## Open Questions the Paper Calls Out

- **Question:** Can the ToT self-consistency principle be effectively generalized to non-image domains (e.g., NLP, audio) or different tasks?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that while the principle applies broadly, "testing self-consistency explicitly and confirming whether it could be used in more general cases may be necessary."
  - **Why unresolved:** The current study restricted evaluation to image classification tasks using specific architectures (ResNet, ViT, etc.) on ImageNet subsets.
  - **What evidence would resolve it:** Successful application and evaluation of the ToT algorithm on non-vision datasets or distinct deep learning tasks.

- **Question:** Do alternative preprocessing techniques outperform the Gaussian blur method used for generating second views?
  - **Basis in paper:** [explicit] The paper notes that "preprocessing, different from Gaussian blur, could have been used and outperform Gaussian blur used in ToT."
  - **Why unresolved:** The authors selected Gaussian blur empirically because it improved high-confidence prediction accuracy in their tests, but did not conduct a comparative study against other transformations.
  - **What evidence would resolve it:** Ablation studies comparing Gaussian blur against other preprocessing methods (e.g., JPEG compression, quantization) regarding adversarial detection rates and final accuracy.

- **Question:** What are the most efficient or effective methods for deriving the "third decision" from hidden layer features?
  - **Basis in paper:** [explicit] The authors acknowledge in the Limitations that "decisions from hidden layer features can be derived in more efficient ways" than the UMAP and k-means clustering approach currently employed.
  - **Why unresolved:** The current implementation relies on a specific pipeline of clustering and correlation mapping, but it is untested whether simpler or different projection methods yield better robustness or speed.
  - **What evidence would resolve it:** Experiments substituting the UMAP/k-means pipeline with alternative feature extraction or similarity metrics to compare computational cost and prediction accuracy.

## Limitations

- **Limited evaluation scope:** Experiments conducted on small ImageNet subsets (13 and 16 classes) and narrow range of adversarial attacks (PGD, AutoAttack with L∞=0.03). Generalization to larger, more diverse datasets and attack types is uncertain.
- **Hyperparameter sensitivity:** Key hyperparameters such as UMAP settings, Grounding DINO thresholds, and blur kernel size are not fully specified, limiting reproducibility.
- **Lack of ablation studies:** The paper does not clearly isolate the contribution of each prediction pathway, making it unclear how much each component contributes to final performance gains.

## Confidence

- **High confidence:** The core mechanism of using self-consistency across three predictions to enable selective abstention is well-supported by the results (80%+ adversarial detection, 60-90% accuracy on confident predictions).
- **Medium confidence:** The effectiveness of Gaussian blur in amplifying prediction disagreement on adversarial inputs is supported by Figure 4, but the robustness of this approach to adaptive attacks is untested.
- **Low confidence:** The claim that penultimate-layer feature clustering provides a robust third prediction pathway is weakly supported; the clustering process and its stability under attack are not thoroughly validated.

## Next Checks

1. **Ablate the third prediction pathway:** Disable the feature-clustering-based prediction and evaluate ToT using only P_orig vs. P_2nd agreement. Measure the impact on final accuracy and abstention rate to quantify the contribution of the hidden-feature pathway.

2. **Test with adaptive attacks:** Craft PGD attacks that incorporate blur augmentation into the perturbation optimization loop. Assess whether detection rate degrades significantly, which would inform threat-model assumptions for deployment.

3. **Scale up to full ImageNet:** Evaluate ToT on the full ImageNet-1K validation set (1000 classes) with a broader range of attack methods (e.g., CW, DeepFool). This will test scalability and robustness beyond the small subsets used in the paper.