---
ver: rpa2
title: 'CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical
  Reasoning'
arxiv_id: '2601.13262'
source_url: https://arxiv.org/abs/2601.13262
tags:
- language
- reasoning
- medical
- multilingual
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CUREMED-BENCH, a multilingual medical reasoning
  dataset spanning 13 languages, including low-resource ones, with open-ended queries
  and single verifiable answers. It also proposes CURE-MED, a two-stage training framework
  that combines code-switching-aware supervised fine-tuning with curriculum-informed
  reinforcement learning to improve both logical correctness and language consistency.
---

# CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning

## Quick Facts
- arXiv ID: 2601.13262
- Source URL: https://arxiv.org/abs/2601.13262
- Reference count: 40
- Primary result: Two-stage framework achieving 94.96% language consistency and 70.04% logical correctness at 32B parameters on 13-language medical reasoning benchmark

## Executive Summary
CURE-MED introduces a two-stage training framework combining code-switching-aware supervised fine-tuning with curriculum-informed reinforcement learning to improve multilingual medical reasoning. The approach addresses language consistency and logical correctness simultaneously by allowing flexible intermediate reasoning while enforcing target-language outputs, and transferring reasoning competence from high- to low-resource languages through staged curriculum learning. Experiments demonstrate state-of-the-art performance across 13 languages including Amharic, Yoruba, Hausa, and Swahili.

## Method Summary
The method employs a two-stage training pipeline. Stage 1 uses code-switching-aware supervised fine-tuning on reasoning traces that allow intermediate steps in any language while requiring final answers in the query language. Stage 2 applies curriculum-informed GRPO that trains first on high-resource languages, then progressively expands to medium and low-resource tiers while retaining 85% of prior data to prevent catastrophic forgetting. The composite reward function balances accuracy (65%), language consistency (30%), and format compliance (5%) with GPT-4.1 serving as the primary verifier.

## Key Results
- Achieves 94.96% language consistency and 70.04% logical correctness at 32B parameters
- Outperforms strong baselines on 13-language medical reasoning benchmark
- Demonstrates robustness across diverse linguistic settings including low-resource languages
- Shows curriculum training improves transfer to low-resource languages compared to uniform sampling

## Why This Works (Mechanism)

### Mechanism 1: Code-switching-aware supervised fine-tuning
- Claim: Intermediate code-switching stabilizes multilingual reasoning by allowing optimal language use during inference while maintaining output language fidelity
- Evidence: Abstract and Section 3.2 explicitly describe code-switching in intermediate reasoning steps with final answer in target language
- Break condition: If target language lacks vocabulary for required clinical concepts, code-switching may persist into final answers

### Mechanism 2: Curriculum-informed GRPO transfer
- Claim: Stable reward signals in high-resource languages enable transfer to low-resource settings
- Evidence: Abstract and Section 3.4 describe tiered training with 85% data retention to prevent forgetting
- Break condition: If reasoning patterns are language-specific rather than transferable, curriculum provides no benefit beyond data mixing

### Mechanism 3: Composite reward optimization
- Claim: Explicit language-consistency term jointly optimizes correctness and fidelity
- Evidence: Section 3.3 defines composite reward with 30% weight on language consistency
- Break condition: If language-consistency reward conflicts with accuracy, optimization may oscillate

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: RL backbone that estimates advantages using group-relative differences rather than value functions. Quick check: Can you explain how GRPO estimates advantages without a separate value model?
- **Code-switching in NLP**: Central to SFT stage where models mix languages during intermediate reasoning. Quick check: What linguistic and cognitive factors drive code-switching in multilingual speakers?
- **Catastrophic forgetting in continual learning**: Curriculum design mitigates forgetting via 85% data retention. Quick check: Why does mixing prior-phase data help, and what are the computational costs?

## Architecture Onboarding

- **Component map**: MedlinePlus → GPT-4o MCQs → human verification → CUREMED-BENCH → code-switched SFT → curriculum GRPO → composite reward
- **Critical path**: Dataset quality (4.89/5 human score) → SFT effectiveness → RL reward stability → curriculum tier assignment based on baseline accuracy
- **Design tradeoffs**: Code-switching flexibility vs. language purity; reward weights (0.65/0.30/0.05) prioritize correctness; retention ratio α=0.85 balances forgetting vs. convergence
- **Failure signatures**: Language collapse (English outputs for non-English prompts); reasoning degradation (fluent but incorrect logic); low-resource stall (reward plateaus)
- **First 3 experiments**: 1) Baseline sanity check on Qwen2.5-{7B,32B}; 2) SFT-only ablation; 3) Curriculum vs. random ordering comparison

## Open Questions the Paper Calls Out
- Can open-source reward models replace proprietary API-based models (e.g., GPT-4.1) in the verification loop without degrading reinforcement learning signals?
- How does CURE-MED perform in clinical settings requiring longitudinal reasoning or multimodal data integration?
- Does the LLM-as-a-judge (GPT-4.1) for correctness reward introduce systematic bias against valid medical reasoning that deviates from ground truth phrasing?

## Limitations
- Relies on API-based models (GPT-4.1) for generation and verification, hindering reproducibility
- Does not capture longitudinal care trajectories, multi-visit decision-making, or multimodal clinical evidence
- Limited empirical evidence for robustness of reasoning patterns in extremely low-resource languages (10-50 training samples)

## Confidence
- **High confidence**: Core two-stage framework and dataset creation pipeline are well-specified
- **Medium confidence**: Code-switching mechanism and curriculum design benefits depend on implementation details
- **Low confidence**: Transferability claims across extremely low-resource languages based on limited evidence

## Next Checks
1. Baseline reproduction check: Verify Qwen2.5-7B and Qwen2.5-32B achieve reported baseline scores on CUREMED-BENCH
2. SFT contribution isolation: Train with code-switched SFT only and compare to "CURE-MED (w/o RL)" column
3. Curriculum sensitivity analysis: Implement random language sampling baseline and compare low-resource performance