---
ver: rpa2
title: 'Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct
  Preference Optimization'
arxiv_id: '2502.14187'
source_url: https://arxiv.org/abs/2502.14187
tags:
- fine-tuning
- data
- federated
- learning
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares Kahneman-Tversky Optimization (KTO) with Direct
  Preference Optimization (DPO) for fine-tuning large language models in federated
  learning settings. KTO is evaluated against DPO using Alpaca-7B as the base model,
  with experiments conducted on a realistic dataset and assessed using MT-Bench-1,
  Vicuna, and AdvBench benchmarks.
---

# Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct Preference Optimization

## Quick Facts
- **arXiv ID**: 2502.14187
- **Source URL**: https://arxiv.org/abs/2502.14187
- **Reference count**: 20
- **Primary result**: KTO outperforms DPO across MT-Bench-1, Vicuna, and AdvBench benchmarks in federated settings, particularly when data is redistributed across clients

## Executive Summary
This paper compares Kahneman-Tversky Optimization (KTO) with Direct Preference Optimization (DPO) for fine-tuning large language models in federated learning settings. Using Alpaca-7B as the base model and LoRA for parameter-efficient fine-tuning, the study evaluates both methods across seven federated learning aggregation algorithms and three benchmark suites. KTO's key advantage is its ability to work with single-response feedback ("good" or "bad" labels), while DPO requires paired preference data. The experiments introduce a redistributed dataset setup where data points are randomly reassigned across clients, revealing that KTO maintains superior performance even when DPO cannot operate due to missing preference pairs.

## Method Summary
The paper evaluates KTO against DPO using Alpaca-7B as the base model with LoRA adapters for parameter-efficient fine-tuning. Data preparation converts DPO's paired responses into KTO's single-response format by splitting each preference pair into two labeled examples. Three experimental configurations are tested: KTOO (original data allocation), KTOR (redistributed data), and DPO (paired preferences). Seven federated learning aggregation methods are compared: FedAvg, FedProx, SCAFFOLD, FedAvgM, FedYogi, FedAdagrad, and FedAdam. All models are evaluated using JudgeLM-13B as a GPT-4 proxy across three benchmarks: MT-Bench-1, Vicuna, and AdvBench.

## Key Results
- KTO consistently outperforms DPO across all three benchmarks (MT-Bench-1, Vicuna, AdvBench) in federated settings
- KTOR maintains superior performance even in scenarios where DPO cannot be applied due to missing preference pairs
- FedAvgM shows particularly strong results with KTOR, leveraging momentum to handle client distribution differences
- KTO's single-response feedback structure enables training when clients hold incomplete preference pairs, a common scenario in distributed environments

## Why This Works (Mechanism)

### Mechanism 1: Single-Response Feedback Structure
- Claim: KTO's requirement for only single-response labels reduces data collection friction in distributed settings
- Mechanism: KTO assigns binary "good"/"bad" labels to individual responses, enabling training when clients hold incomplete preference pairs. DPO requires both a chosen and rejected response per prompt, which breaks when data is sparse or non-IID across clients
- Core assumption: Single-response labeling preserves sufficient signal for preference learning; this is suggested but not theoretically proven
- Evidence anchors: [abstract] "KTO's ability to handle single-response feedback gives it an advantage over DPO"; [Section 1.3] "KTO is a simpler, more flexible fine-tuning method that requires only a single response per prompt"
- Break condition: If single-response feedback quality degrades significantly (e.g., noisy labels from untrained annotators), KTO's advantage may collapse

### Mechanism 2: Redistributed Data Robustness
- Claim: KTO maintains performance when "good" and "bad" responses for the same prompt are distributed across different clients
- Mechanism: In the KTOR setup, data points are randomly reassigned across clients. Since KTO treats each response independently with its label, client-level heterogeneity in prompt coverage does not invalidate training. DPO's paired structure breaks when preference pairs are split across clients
- Core assumption: Random redistribution approximates realistic non-IID conditions
- Evidence anchors: [abstract] "KTOR maintaining superior performance even in scenarios where DPO cannot be applied"; [Section 4] "KTOR even surpasses KTO in certain scenarios"
- Break condition: If redistribution creates extreme class imbalance (e.g., a client receives only "bad" labels), local training could destabilize

### Mechanism 3: Aggregation Method Interaction
- Claim: KTO's performance varies by FL aggregation algorithm, with FedAvgM showing particularly strong results for the redistributed configuration
- Mechanism: FedAvgM incorporates momentum to handle client distribution differences, which appears to better leverage KTOR's redistributed data pattern. Standard FedAvg favors KTOO (original allocation)
- Core assumption: The observed aggregation-method interactions generalize beyond the specific Alpaca-7B + LoRA setup tested
- Evidence anchors: [Section 4] "FedAvgM's design, which better handles client distribution differences, explains its superior performance with KTOR compared to FedAvg"
- Break condition: If momentum-based aggregators amplify noisy updates from heterogeneous clients, KTOR's gains could reverse

## Foundational Learning

- **Federated Learning Aggregation Algorithms** (FedAvg, FedProx, SCAFFOLD, etc.)
  - Why needed here: The paper evaluates KTO/DPO across seven aggregation methods; understanding how each combines local updates is essential for interpreting results
  - Quick check question: Can you explain why FedAvgM might outperform FedAvg when client data distributions differ significantly?

- **Preference Optimization Methods** (DPO vs. KTO objective functions)
  - Why needed here: The core comparison hinges on structural differences between paired-preference (DPO) and single-response (KTO) optimization
  - Quick check question: What is the minimum data structure required to compute one DPO gradient step vs. one KTO gradient step?

- **Parameter-Efficient Fine-Tuning** (LoRA)
  - Why needed here: All experiments use LoRA to make 7B model fine-tuning tractable in FL; understanding rank decomposition is necessary for reproduction
  - Quick check question: How does LoRA reduce communication overhead in federated settings compared to full-parameter fine-tuning?

## Architecture Onboarding

- Component map: Alpaca-7B -> LoRA adapters -> KTO/DPO optimization -> FL aggregation -> JudgeLM-13B evaluation

- Critical path:
  1. Convert DPO paired dataset → KTO single-response format (split each pair into two labeled examples)
  2. Distribute data to simulated clients (original allocation for KTOO; random redistribution for KTOR)
  3. Run local LoRA fine-tuning with KTO loss per client
  4. Aggregate adapter weights using chosen FL algorithm
  5. Evaluate aggregated model on three benchmarks using JudgeLM-13B

- Design tradeoffs:
  - JudgeLM-13B vs. GPT-4: Authors chose internal consistency over comparability with prior work; expect higher absolute scores but valid relative rankings
  - No quantization: Training ran at full precision for experimental control; real deployments may need 8-bit or 4-bit quantization
  - Fixed redistribution seed: Reproducibility prioritized over distribution diversity characterization

- Failure signatures:
  - DPO fails silently when preference pairs are split across clients (no paired data locally)
  - KTO performance degrades if label noise is high (single bad label cannot be contrasted against a known good response)
  - AdvBench scores show high variance across methods (11.35–17.50); safety metrics may be unstable

- First 3 experiments:
  1. Reproduce KTOO vs. DPO on FedAvg: Establish baseline using the paper's OpenFedLLM codebase (seed=2023); verify MT-Bench-1 scores within ±0.2 of reported results
  2. Ablate redistribution intensity: Test KTOR with varying shuffle ratios (25%, 50%, 100% redistribution) to characterize robustness boundaries
  3. Stress-test label noise: Inject 10–30% label flips into KTO training data to determine if single-response advantage persists under realistic annotation noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quantization (e.g., 8-bit or lower precision) affect KTO's performance and efficiency in federated learning deployments compared to full-precision training?
- Basis in paper: [explicit] "exploring quantization, such as 8-bit or lower precision, in future studies could provide insights into KTO's performance under resource-constrained settings."
- Why unresolved: All experiments were conducted without quantization to isolate fine-tuning method effects, leaving resource-constrained scenarios unexplored
- What evidence would resolve it: Benchmark results comparing KTOO and KTOR with 4-bit and 8-bit quantization against full-precision baselines across MT-Bench-1, Vicuna, and AdvBench

### Open Question 2
- Question: What are the comparative resource utilization metrics (memory usage, communication overhead, convergence rates) for KTO versus DPO in federated settings?
- Basis in paper: [explicit] "future work should measure and compare resource utilization for DPO and KTO. Metrics such as memory usage, communication overhead, and convergence rates could provide practical insights."
- Why unresolved: The study focused on benchmarking performance quality but did not instrument or report any resource consumption measurements
- What evidence would resolve it: Controlled experiments measuring peak GPU memory, total bytes transferred per training round, and rounds to convergence for both methods under identical FL configurations

### Open Question 3
- Question: Would KTO's demonstrated advantages persist when evaluated with GPT-4 or a multi-judge ensemble instead of JudgeLM-13B?
- Basis in paper: [explicit] "employing GPT-4, a widely recognized standard, would facilitate direct comparisons with other studies. Additionally, incorporating evaluations from multiple judges could further validate the robustness."
- Why unresolved: JudgeLM-13B produces systematically higher absolute scores than GPT-4; while relative rankings may be preserved, cross-study comparability and potential judge-specific biases remain unverified
- What evidence would resolve it: Re-evaluation of DPO, KTOO, and KTOR model outputs using GPT-4 and at least two other judge models, with inter-judge agreement analysis

### Open Question 4
- Question: Can KTO maintain its performance advantages when trained on natively collected single-response feedback datasets rather than datasets derived by splitting DPO's paired responses?
- Basis in paper: [explicit] "developing a dedicated dataset tailored to KTO's single-response feedback framework" would "minimize biases introduced by adapting datasets created for DPO and provide a more realistic foundation."
- Why unresolved: The KTOR experiments used a redistributed version of a DPO dataset, which may not reflect real-world single-response data distributions
- What evidence would resolve it: Experiments comparing KTO performance on derived datasets versus natively collected single-response feedback datasets across the same benchmarks

## Limitations

- The mechanism by which single-response labeling preserves sufficient preference signal remains theoretically unproven
- The redistribution protocol uses a fixed random seed without characterizing statistical properties of resulting client distributions
- AdvBench safety evaluation shows high variance across methods (11.35-17.50), suggesting potential instability in safety metric measurement

## Confidence

- **High confidence**: KTO outperforms DPO in federated settings when preference pairs are split across clients
- **Medium confidence**: KTO's single-response structure provides practical advantages in distributed data collection
- **Medium confidence**: FedAvgM's momentum-based aggregation particularly benefits KTOR's redistributed data pattern

## Next Checks

1. **Label quality ablation**: Systematically vary the percentage of noisy labels (0%, 10%, 20%, 30%) in KTO training to determine the threshold where single-response advantage degrades relative to DPO's paired structure

2. **Redistribution sensitivity analysis**: Characterize KTOR performance across different redistribution intensities (25%, 50%, 75%, 100% client-shuffling) and measure how performance correlates with client-level class balance statistics

3. **Cross-architecture replication**: Replicate the KTO vs. DPO comparison using a different base model (e.g., Mistral-7B or Vicuna-13B) and fine-tuning method (full fine-tuning vs. LoRA) to assess whether observed advantages generalize beyond the specific experimental setup