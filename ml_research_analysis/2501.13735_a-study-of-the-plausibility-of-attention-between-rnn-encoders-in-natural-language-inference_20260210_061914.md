---
ver: rpa2
title: A Study of the Plausibility of Attention between RNN Encoders in Natural Language
  Inference
arxiv_id: '2501.13735'
source_url: https://arxiv.org/abs/2501.13735
tags:
- attention
- heuristic
- words
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the plausibility of attention-based explanations
  in sentence comparison, specifically natural language inference (NLI). The authors
  propose a heuristic attention map based on word similarity between premise and hypothesis
  sentences, focusing on verbs, nouns, and adjectives.
---

# A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference

## Quick Facts
- arXiv ID: 2501.13735
- Source URL: https://arxiv.org/abs/2501.13735
- Reference count: 31
- This paper evaluates the plausibility of attention-based explanations in NLI, finding that heuristic attention maps based on word similarity outperform model-generated attention weights in aligning with human annotations.

## Executive Summary
This study examines the plausibility of attention mechanisms as explanations for decisions made by RNN encoders in Natural Language Inference (NLI). The authors propose a heuristic attention map based on word embedding similarity between premise and hypothesis sentences, which better correlates with human-annotated explanations than the learned attention weights. Their experiments demonstrate that while attention mechanisms achieve high task accuracy, the resulting attention distributions poorly align with what humans consider plausible explanations, often concentrating on unimportant tokens like stop words.

## Method Summary
The method involves a cross-attention mechanism between two shared-weight BiLSTM encoders processing premise and hypothesis sentences. Each word's hidden state is compared against the final sentence embedding of the opposite sentence using cosine similarity, with softmax normalization producing attention weights. These weights generate context vectors that are concatenated with sentence embeddings for classification. The authors evaluate plausibility by comparing attention distributions to human-annotated highlight maps from the eSNLI corpus using ROC curves, correlation metrics, and Jensen-Shannon divergence.

## Key Results
- Model-generated attention weights show poor alignment with human annotations, often concentrating on stop words and punctuation
- The heuristic attention method based on word embedding similarity achieves higher AUC and correlation with human annotations than learned attention weights
- Attention plausibility metrics reveal that high task accuracy does not necessarily correlate with plausible attention distributions

## Why This Works (Mechanism)

### Mechanism 1: Cross-Attention for Sentence Pair Comparison
Cross-attention between two RNN encoders produces interpretable relevance weights, but raw weights show low plausibility. Each word hidden state h_i from one sentence is compared against the final sentence embedding h_n of the opposite sentence via cosine similarity, with attention weights α_i computed via softmax normalization. This produces a context vector c concatenated with h_n for classification. Core assumption: Words semantically relevant to the opposite sentence should receive higher attention weights. Evidence shows attention concentrates on stop words rather than content words, degrading plausibility.

### Mechanism 2: Semantic Similarity Heuristic as Plausibility Baseline
A heuristic based on word embedding similarity between premise and hypothesis correlates better with human annotations than model-generated attention. For each word w_i, cumulative cosine similarity to all non-stop words in the opposite sentence creates a pseudo-attention map. Core assumption: In entailment scenarios, plausible explanations emphasize words with close meanings between premise and hypothesis (particularly verbs, nouns, adjectives). Evidence shows better coverage of these words compared to model attention, though effectiveness depends on embedding quality.

### Mechanism 3: Plausibility Evaluation via Correlation with Human Annotations
Plausibility is assessed by measuring correlation between attention distributions and human-annotated highlight maps from eSNLI. Continuous attention values are converted to binary via threshold ε, generating ROC curves against human annotations with AUC and correlation metrics (Pearson, Spearman). Jensen-Shannon divergence measures distributional similarity per instance. Core assumption: Human-annotated highlighted words represent ground truth for what constitutes a "plausible" explanation. Evidence shows low correlation between model attention and human highlights despite high task accuracy.

## Foundational Learning

- Concept: **Faithfulness vs. Plausibility**
  - Why needed here: The paper distinguishes faithfulness (does attention reflect model reasoning?) from plausibility (is attention useful for human interpretation?). Confounding these leads to misaligned evaluation goals.
  - Quick check question: If a model achieves high accuracy but attention focuses on stopwords, is it faithful, plausible, both, or neither?

- Concept: **Cross-Attention in Sequence Pair Modeling**
  - Why needed here: Unlike self-attention, cross-attention links two distinct sequences. Understanding how each sequence's representations interact is essential for debugging attention quality.
  - Quick check question: What happens to cross-attention weights if premise and hypothesis share no vocabulary?

- Concept: **eSNLI Corpus Structure**
  - Why needed here: The corpus provides human highlight annotations; knowing its annotation protocol ("focus on non-obvious elements") clarifies what plausibility ground truth represents.
  - Quick check question: Would highlighting identical repeated words across sentences be consistent with eSNLI annotation guidelines?

## Architecture Onboarding

- Component map:
  - Embedding Layer -> GloVe pretrained vectors (300d), trainable weights
  - Contextualization Layer -> Shared BiLSTM encoder (1 layer, 300d) for both premise and hypothesis
  - Attention Layer -> Cross-attention computing α_i = softmax(h_i^T h_n) against opposite sentence's final hidden state
  - Fusion -> Context vector c concatenated with sentence embedding h_n, passed through fully connected layer
  - Classifier -> MLP with 1 hidden layer, ReLU activation, 3-class softmax output

- Critical path:
  1. Tokenize premise and hypothesis → embeddings
  2. BiLSTM contextualization → hidden sequences h and final states h_n
  3. Cross-attention: premise words attend to hypothesis embedding (and vice versa)
  4. Merge attention-weighted context with sentence embeddings
  5. Concatenate merged representations → classify

- Design tradeoffs:
  - Shared vs. separate encoders: Shared weights assume identical statistical properties; may underperform if premise/hypothesis distributions differ.
  - Attention target: Using final hidden state h_n (sentence-level) vs. all hidden states (word-level) trades granularity for simplicity.
  - Threshold ε selection: Binary conversion for ROC evaluation requires choosing ε; different values yield different plausibility conclusions.

- Failure signatures:
  - Attention concentrated on determinants, punctuation, stop words → low plausibility, scattered weights
  - High task accuracy with low annotation correlation → model exploits dataset artifacts rather than semantic reasoning
  - Large JS-divergence between model attention and human highlights → distribution mismatch at instance level

- First 3 experiments:
  1. Replicate ROC analysis: Plot true positive vs. false positive rates for model attention and heuristic against human annotations across ε values; verify heuristic AUC exceeds model AUC for most thresholds.
  2. POS-tag attention analysis: Compute attention weight distribution across grammatical categories (verbs, nouns, adjectives vs. determinants, adpositions); confirm stop-word concentration.
  3. Heuristic as regularization signal: Implement the similarity-based heuristic as a supervision target for attention weights; measure impact on plausibility (correlation with annotations) vs. faithfulness (task performance).

## Open Questions the Paper Calls Out
None

## Limitations
- The heuristic method's effectiveness depends heavily on embedding quality, which may vary across domains and languages
- The cross-attention formulation using only final sentence embeddings may not capture nuanced word-level interactions compared to full cross-attention
- Plausibility evaluation depends on eSNLI's human highlight annotations, which may contain inherent biases or inconsistencies

## Confidence
- High confidence: The empirical finding that model-generated attention weights concentrate on stop words and show poor correlation with human annotations
- Medium confidence: The heuristic method's superiority as a plausibility baseline, though specific numerical results depend on underspecified hyperparameters
- Low confidence: The broader claim that attention mechanisms in general fail at providing plausible explanations may overgeneralize from this specific RNN+cross-attention setup

## Next Checks
1. **Cross-architecture generalization test**: Replicate the plausibility evaluation framework using transformer-based models (e.g., BERT) with self-attention instead of RNN cross-attention. Compare attention plausibility metrics across architectures to determine if the stop-word concentration problem is architecture-specific or general.

2. **Heuristic supervision ablation study**: Implement the semantic similarity heuristic as an explicit attention supervision signal during training. Measure the trade-off between plausibility improvement (correlation with human annotations) and task performance degradation, establishing whether plausibility and faithfulness can be jointly optimized.

3. **Annotation bias analysis**: Conduct a statistical analysis of eSNLI's human highlight distribution to identify potential annotation biases (e.g., preference for certain POS categories, sentence length effects). Recompute plausibility metrics after correcting for identified biases to determine if the observed attention plausibility gap is partially attributable to corpus artifacts rather than model limitations.