---
ver: rpa2
title: 'COLE: a Comprehensive Benchmark for French Language Understanding Evaluation'
arxiv_id: '2510.05046'
source_url: https://arxiv.org/abs/2510.05046
tags:
- task
- french
- sentence
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The COLE benchmark addresses the need for a comprehensive evaluation
  of French language understanding by introducing 23 diverse tasks covering sentiment
  analysis, paraphrase detection, grammatical judgment, and reasoning. It benchmarks
  95 large language models using a zero-shot evaluation setup and identifies significant
  performance gaps between closed- and open-weights models.
---

# COLE: a Comprehensive Benchmark for French Language Understanding Evaluation

## Quick Facts
- **arXiv ID:** 2510.05046
- **Source URL:** https://arxiv.org/abs/2510.05046
- **Reference count:** 40
- **Key outcome:** COLE benchmark identifies significant performance gaps between closed- and open-weights models on French NLU tasks, with top closed models achieving 70.12% composite score versus 49.14% for best open models.

## Executive Summary
COLE introduces a comprehensive benchmark for evaluating French language understanding across 23 diverse tasks. The benchmark reveals significant performance disparities between closed- and open-weights large language models, with closed models achieving substantially higher composite scores. While models excel at grammatical judgment and coarse-grained semantic tasks, they struggle with zero-shot extractive question-answering, fine-grained word sense disambiguation, and understanding regional language variations. The benchmark provides critical insights into the current capabilities and limitations of French language models.

## Method Summary
COLE evaluates 95 large language models using zero-shot prompting across 23 diverse French NLU tasks. Tasks are categorized into single-sentence, similarity/paraphrase, and inference types, covering sentiment analysis, grammatical judgment, and reasoning. Each task uses specific automatic metrics (Accuracy, Exact Match, F1 Score), and an unweighted arithmetic mean creates a composite score. The benchmark includes both open-weights and closed-weights models, enabling direct comparison of their capabilities without task-specific fine-tuning.

## Key Results
- Closed-weights models dominate top ranks with 70.12% composite score versus 49.14% for best open-weights model
- Models struggle with zero-shot extractive question-answering and fine-grained word sense disambiguation
- Performance gaps persist despite parameter count differences, suggesting data quality and alignment as key factors
- Grammatical tasks show strong performance but don't translate to regional language understanding capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: **Closed-weight models outperform open-weights models on French NLU due to superior data quality and alignment, not solely parameter count.**
- Mechanism: Proprietary high-quality training datasets and extensive post-training alignment procedures (e.g., RLHF) hyper-focus models on language understanding and instruction-following. Knowledge distillation allows smaller models to inherit capabilities from larger "teacher" models.
- Core assumption: Training data composition and post-training alignment of closed models are fundamentally superior to those of open-weights models.
- Evidence anchors:
  - [abstract] "...significant performance gap between closed- and open-weights models..."
  - [section] (Page 6, 5.1) "This suggests that the vast and high-quality proprietary training datasets... and extensive post-training alignment procedures of closed-weights LLMs are the determining factors, rather than raw parameter count."
- Break condition: Open-weights model trained on comparable dataset with similar alignment fails to close performance gap.

### Mechanism 2
- Claim: **Proficiency in grammatical judgment does not uniformly translate to superior performance on semantic or regional language understanding tasks.**
- Mechanism: Grammatical tasks rely on recognizing common online linguistic patterns and syntactic rules learned from web-scraped corpora. This shallow proficiency doesn't extend to deep cultural, dialectical, or fine-grained semantic knowledge requiring specialized data sources.
- Core assumption: Performance on MultiBLiMP-Fr reflects recognition of common online linguistic patterns rather than formal grammatical rules.
- Evidence anchors:
  - [abstract] "LLMs excel at high-level semantic and grammatical tasks but struggle with... regional language understanding..."
  - [section] (Page 7, 5.2) "While language-specific pre-training is effective for capturing syntactic and grammatical nuances, it may not be sufficient to bridge the gap in broader NLU capabilities."
- Break condition: Model achieving near-perfect scores on grammatical tasks also achieves top-tier scores on regional language tasks.

### Mechanism 3
- Claim: **Zero-shot extractive QA is severely limited because models tend to rephrase or generate answers rather than strictly follow extraction instructions.**
- Mechanism: Task requires copying verbatim text from passage, but models generate fluent non-extractive answers, indicating misalignment between rigid extraction constraint and generative nature.
- Core assumption: Low Exact Match scores primarily due to failure of instruction-following (generative rephrasing), not fundamental lack of reading comprehension.
- Evidence anchors:
  - [abstract] "LLMs... struggle with zero-shot extractive question-answering (QA)..."
  - [section] (Page 7, 5.3.2) "...inability to extract verbatim text suggests a strong tendency to rephrase or generate answers rather than strictly follow the extraction instruction."
- Break condition: Same models achieve high EM scores when fine-tuned or few-shot prompted for extraction.

## Foundational Learning

- **Concept: Zero-Shot Evaluation Paradigm**
  - Why needed here: Entire benchmark relies on zero-shot prompting to measure inherent model knowledge, not learned task performance.
  - Quick check question: In a zero-shot setup, does providing the model with a few examples from the training set constitute a change in the evaluation paradigm?

- **Concept: Composite Score**
  - Why needed here: Unweighted arithmetic mean of 23 diverse tasks creates single ranking metric treating all tasks equally.
  - Quick check question: If a model excels at sentiment analysis (one task) but fails at all 10 inference tasks, how would this be reflected in the composite score?

- **Concept: Data Contamination**
  - Why needed here: Public availability of datasets introduces risk of models being trained on them, invalidating generalization claims.
  - Quick check question: If a model was trained on Wikipedia text and FQuAD is sourced from Wikipedia, how might this affect the validity of its FQuAD performance score?

## Architecture Onboarding

- **Component map:**
  1. **23 Diverse Tasks:** Single-sentence, similarity/paraphrase, and inference tasks
  2. **Zero-Shot Prompts:** Natural language prompts for each task
  3. **Model:** LLM receives prompt and generates output
  4. **Evaluation Metrics:** Task-specific automatic metrics (Accuracy, Exact Match, F1)
  5. **Aggregator:** Script computing unweighted mean for final Composite Score

- **Critical path:** Zero-Shot Prompt -> Model Output -> Metric Evaluation. Failure here (e.g., generating sentence instead of character) cascades into zero score, heavily impacting task score.

- **Design tradeoffs:** Comprehensiveness vs. granularity (23 tasks aggregated into one score obscures specific strengths/weaknesses). Convenience vs. validity (public datasets introduce contamination risk).

- **Failure signatures:**
  - Low Extractive QA EM with Higher F1: Model generates correct, non-verbatim answer
  - Near-Baseline Performance on Regional Tasks: Model lacks specific dialectal knowledge
  - High Grammar, Low Semantic Scores: Model learned syntactic rules but lacks deeper understanding

- **First 3 experiments:**
  1. **Prompt Engineering for Extraction:** Test if FQuAD/PIAF EM scores improve by changing prompt to "copy the exact words from the text" or providing single in-context example.
  2. **Ablation of Regional Tasks:** Create composite score excluding QFrCoRE and QFrCoRT and re-rank models to reveal strengths in "standard" French NLU.
  3. **Model Scale Analysis for Open Weights:** Take family of open-weights models (Qwen2.5-0.5B, 1.5B, 3B, 7B, 14B) and plot composite score against parameter count to test if scale alone closes gap with closed-weights models.

## Open Questions the Paper Calls Out

None

## Limitations

- **Data contamination risk:** Public availability of datasets may lead to inflated performance scores from models trained on them.
- **Zero-shot evaluation sensitivity:** Results highly sensitive to prompt phrasing and format, not accounted for in reported results.
- **Limited cross-linguistic validation:** Findings about model superiority and extractive QA challenges may not generalize to other languages.

## Confidence

**High Confidence:**
- Significant performance gap between closed- and open-weights models on French NLU tasks
- General pattern of model strengths/weaknesses (coarse-grained semantic understanding vs. extractive QA challenges)
- Benchmark's comprehensive coverage of diverse French NLU tasks

**Medium Confidence:**
- Attribution of performance gap primarily to data quality and alignment procedures
- Claim that grammatical proficiency doesn't uniformly translate to semantic/regional understanding
- Assertion that low extractive QA scores reflect generative bias rather than comprehension failure

**Low Confidence:**
- Specific mechanisms by which closed-weights models achieve performance advantage
- Generalizability of identified challenging frontiers to other languages
- Extent to which contamination affects specific model rankings

## Next Checks

1. **Prompt sensitivity analysis:** Systematically vary prompt phrasing and format for 5-10 challenging tasks across 3-4 representative models to quantify impact on performance and establish robustness bounds.

2. **Contamination assessment:** Conduct provenance analysis to estimate likelihood that major models were trained on COLE datasets, and perform sensitivity analysis by excluding potentially contaminated datasets to observe changes in model rankings.

3. **Cross-linguistic generalization test:** Apply same evaluation protocol to comparable multilingual model family (e.g., mBERT or XLM-R) across 3-4 languages to determine whether performance patterns are language-specific or universal.