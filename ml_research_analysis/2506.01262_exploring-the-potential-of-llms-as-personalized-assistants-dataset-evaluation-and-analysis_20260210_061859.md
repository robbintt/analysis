---
ver: rpa2
title: 'Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation,
  and Analysis'
arxiv_id: '2506.01262'
source_url: https://arxiv.org/abs/2506.01262
tags:
- user
- prompt
- persona
- personalized
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HiCUPID, a benchmark designed to evaluate
  large language models (LLMs) as personalized AI assistants. HiCUPID addresses the
  lack of open-source datasets for personalization by providing a synthetic conversational
  dataset with 1,500 users, each defined by 25 personas, profile information, and
  schedules.
---

# Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis

## Quick Facts
- arXiv ID: 2506.01262
- Source URL: https://arxiv.org/abs/2506.01262
- Reference count: 40
- Introduces HiCUPID benchmark with 1,500 synthetic users to evaluate LLM personalization capabilities

## Executive Summary
This paper introduces HiCUPID, a benchmark designed to evaluate large language models (LLMs) as personalized AI assistants. HiCUPID addresses the lack of open-source datasets for personalization by providing a synthetic conversational dataset with 1,500 users, each defined by 25 personas, profile information, and schedules. The dataset includes single-info and multi-info question-answer pairs to assess LLMs' ability to adhere to user information, understand implicit information, reason from multiple sources, handle long contexts, and provide proactive responses. The authors also introduce a Llama-3.2-based automated evaluation model that closely aligns with human preferences. Extensive experiments with state-of-the-art LLMs reveal significant challenges in personalization, particularly in long-context handling and multi-info reasoning.

## Method Summary
The authors create HiCUPID by first generating synthetic users with diverse personas, profiles, and schedules using GPT-4o. These users engage in dialogues where their personal information is revealed contextually, creating realistic conversation scenarios. From these dialogues, single-info and multi-info question-answer pairs are extracted to form the benchmark. To enable efficient evaluation, the authors train a Llama-3.2-3B model as a proxy evaluator through preference distillation from GPT-4o judgments on 400k samples. They then conduct extensive experiments using various methods including zero-shot, few-shot, RAG-based retrieval, supervised fine-tuning (SFT), direct preference optimization (DPO), and SFT+DPO combinations across multiple LLM architectures.

## Key Results
- SFT+DPO consistently outperforms individual methods across Llama, Mistral, and Qwen model families on the HiCUPID benchmark
- Performance significantly degrades when using full dialogue history versus gold-only context, confirming long-context handling as a major challenge
- The Llama-3.2-based automated evaluator shows strong correlation with human preferences, outperforming traditional n-gram metrics like BLEU and ROUGE-L
- Models struggle most with multi-info reasoning tasks and implicit information comprehension

## Why This Works (Mechanism)

### Mechanism 1
Training a smaller evaluator model on GPT-4o preference judgments produces an automated metric that correlates with human evaluations better than n-gram overlap metrics. Preference distillation transfers GPT-4o's alignment with human judgment patterns into a smaller Llama-3.2-3B model via supervised fine-tuning on 400k evaluation samples. Core assumption: GPT-4o's evaluation behavior approximates human preferences well enough that distillation preserves this alignment. Break condition: If GPT-4o exhibits systematic biases not present in human evaluation, distillation would amplify rather than correct these.

### Mechanism 2
Combining supervised fine-tuning followed by direct preference optimization (SFT+DPO) yields the largest performance gains on personalization tasks compared to either method alone or inference-time approaches. SFT first grounds the model on personalized response patterns from the dataset; DPO then sharpens the preference boundary between personalized and generic responses using contrastive learning. Core assumption: The personalized answers in HiCUPID represent ground-truth personalized responses, and the general answers serve as valid negative examples for DPO. Break condition: If personalized/general answer pairs contain annotation noise, DPO's contrastive loss may optimize for spurious patterns.

### Mechanism 3
Long dialogue contexts and implicit information distribution significantly challenge current LLMs' personalization capabilities, as evidenced by performance drops when using full dialogue history versus gold-only context. Personal information scattered across ~17k token dialogue histories requires both long-context retention and implicit inference; retrieval helps but doesn't fully compensate for context window limitations. Core assumption: The synthetic dialogue structure realistically simulates how personal information accumulates in real assistant interactions. Break condition: If synthetic dialogues are structurally different from real user-assistant conversations, the observed challenges may not transfer to real deployments.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) with LoRA**
  - Why needed here: SFT is the primary method for adapting base LLMs to personalized response generation using HiCUPID's training data
  - Quick check question: Can you explain why LoRA (Low-Rank Adaptation) is used instead of full parameter fine-tuning for personalization tasks?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: DPO is applied after SFT to further align model outputs with personalized vs. generic response preferences
  - Quick check question: How does DPO differ from traditional reinforcement learning from human feedback (RLHF) in terms of training complexity?

- **Concept: Retrieval-Augmented Generation (RAG) for Personalization**
  - Why needed here: RAG (via BM25 or Contriever) is tested as an inference-time approach to surface relevant personal information without full context
  - Quick check question: Why might utterance-level retrieval outperform dialogue-level retrieval for personalization in HiCUPID?

## Architecture Onboarding

- **Component map:** Dataset Generation (GPT-4o synthetic users/dialogues) -> QA Pairs Extraction -> Human/GPT-4o Evaluation Alignment -> Proxy Evaluator Training (Llama-3.2-3B) -> Model-Method Experiments (SFT, DPO, SFT+DPO) -> Ablation Studies

- **Critical path:** Synthetic data generation → Evaluation model training → Baseline establishment → Method comparison → Context ablation → Analysis

- **Design tradeoffs:** Synthetic data enables controlled experiments but may lack real-world complexity; proxy evaluator reduces cost but may inherit biases; LoRA enables efficient fine-tuning but limits model capacity

- **Failure signatures:** DPO-only training often fails to converge → indicates need for SFT grounding first; models struggle with multi-info QA even after SFT → multi-hop reasoning remains challenging; performance drops when instructions in system prompt vs. user role → instruction following degrades with long context

- **First 3 experiments:** 1) Establish baselines with zero-shot/3-shot inference on Test Set 2 (unseen users) using GPT-4o-mini and one open-source model; 2) Apply SFT and SFT+DPO to Llama-3.1-8B with LoRA rank=256, LR=1e-4 (SFT) / 3e-6 (DPO) and compare total scores; 3) Compare 0-shot performance using full dialogue history vs. gold-only dialogue for persona QA pairs to quantify long-context impact

## Open Questions the Paper Calls Out

- What is the optimal level of personalization preferred by human users in AI assistants, and does over-personalization degrade the user experience compared to general responses?
- Can a specialized reward model be developed for the HiCUPID benchmark to stabilize reinforcement learning-based training methods like DPO?
- How can privacy-preserving measures be effectively integrated into personalized LLMs to prevent PII extraction without compromising adherence to user information?
- Can LLMs be trained to effectively comprehend and utilize implicit user information involving negative sentiments or negations?

## Limitations

- The synthetic data generated by GPT-4o may not capture the full complexity and diversity of real-world personalization scenarios
- The automated evaluation model, while showing good correlation with human preferences, may still inherit biases from its training data
- The study focuses on English-language interactions, limiting generalizability to multilingual contexts

## Confidence

- **High Confidence:** The effectiveness of the Llama-3.2-based automated evaluator and the superiority of SFT+DPO over individual methods
- **Medium Confidence:** The challenges posed by long contexts and implicit information, and the overall benchmark design
- **Low Confidence:** The generalizability of findings to real-world deployment scenarios and the robustness of the evaluation methodology across different personalization domains

## Next Checks

1. Collect a small corpus of real user-assistant conversations with personalized information and evaluate whether HiCUPID-trained models show similar performance patterns on authentic data versus synthetic data

2. Test the best-performing SFT+DPO models on personalization tasks in domains not represented in HiCUPID (e.g., healthcare, financial services) to assess domain transfer capabilities

3. Evaluate whether models trained on HiCUPID can maintain consistent personalization quality across extended interaction sequences (multiple sessions over time) rather than single dialogues, as real personal assistants would require