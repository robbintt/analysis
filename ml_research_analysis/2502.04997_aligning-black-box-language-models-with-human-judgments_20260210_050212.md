---
ver: rpa2
title: Aligning Black-box Language Models with Human Judgments
arxiv_id: '2502.04997'
source_url: https://arxiv.org/abs/2502.04997
tags:
- human
- judgments
- alignment
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of aligning large language model
  (LLM) judgments with human evaluations, particularly important for subjective tasks
  where systems are designed for human use. The core method involves learning a linear
  mapping between LLM categorical outputs and human judgments using a small set of
  calibration examples, without requiring model retraining or access to model logits.
---

# Aligning Black-box Language Models with Human Judgments

## Quick Facts
- **arXiv ID:** 2502.04997
- **Source URL:** https://arxiv.org/abs/2502.04997
- **Reference count:** 13
- **Primary result:** 142% average improvement in LLM-human agreement using linear alignment with 1-2 examples per category

## Executive Summary
This paper addresses the challenge of aligning large language model (LLM) categorical judgments with human evaluations, particularly for subjective tasks where systems are designed for human use. The authors propose a simple yet effective method that learns a linear mapping between LLM outputs and human labels using a small set of calibration examples, without requiring model retraining or access to model logits. The approach is evaluated across 29 tasks with three different LLMs, demonstrating significant improvements in alignment while remaining computationally efficient and sample-efficient.

## Method Summary
The method involves learning a ridge regression mapping Ŵ that transforms LLM categorical outputs into aligned judgments matching human distributions. Given calibration data with paired LLM outputs and human labels, both are one-hot encoded into matrices Z (LLM) and Y (human). The mapping Ŵ is computed via closed-form ridge regression: Ŵ = (Z^⊤Z + λI)^(-1)Z^⊤Y with λ = 10^-6. During inference, the aligned judgment is obtained by multiplying the LLM's output vector by Ŵ and taking the argmax. The approach works in both zero-shot and few-shot settings, requires no model fine-tuning, and can use calibration data from similar tasks when task-specific data is unavailable.

## Key Results
- Achieves 142% average improvement in agreement with human judgments across 29 tasks
- Works effectively with only 1-2 calibration examples per category (sample efficient)
- Smaller LLMs (Mixtral 8x7B) can achieve performance comparable to larger models (Claude-3 Sonnet) after alignment
- Exceeds inter-human agreement on four out of six multi-annotator tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Linear mapping corrects systematic response style biases in LLM judgments.
- **Mechanism:** LLMs exhibit consistent distributional shifts (e.g., overly positive labels). The learned transformation matrix W captures the statistical relationship between LLM categorical distributions and human label distributions, effectively remapping biased outputs to calibrated ones.
- **Core assumption:** The misalignment is primarily a label-level distributional shift rather than a fundamental reasoning failure—LLMs make valid distinctions but assign labels differently than humans.
- **Evidence anchors:** Abstract states LLMs can exhibit "overly positive" response styles; Figure 1a shows LLMs avoid negative judgments entirely; Figure 1b shows alignment closes this gap.
- **Break condition:** If LLM and human judgments have near-zero correlation on the underlying construct (not just label mismatch), the mapping will amplify noise rather than correct bias.

### Mechanism 2
- **Claim:** Ridge regression enables sample-efficient learning with 1-2 examples per category.
- **Mechanism:** The closed-form solution Ŵ = (Z^⊤Z + λI)^(-1)Z^⊤Y adds regularization (λ = 10^-6) to prevent singular matrices when samples are sparse, allowing stable estimation even with minimal calibration data.
- **Core assumption:** The mapping is approximately linear—LLM label probabilities map linearly to human label probabilities.
- **Evidence anchors:** Figure 2 shows the method achieves alignment using only one or two examples per category; Equation 2 provides the mathematical foundation for sample efficiency.
- **Break condition:** When calibration examples are unrepresentative of the true LLM→human relationship (e.g., skewed sampling), the learned mapping will be biased.

### Mechanism 3
- **Claim:** Alignment enables smaller models to match larger model performance by correcting their specific biases.
- **Mechanism:** Each model has distinct response patterns. By learning model-specific mappings on the same human ground truth, the corrected outputs converge—different raw distributions become aligned to the same target.
- **Core assumption:** Smaller models retain sufficient discriminative ability; their primary limitation is miscalibrated label assignment rather than inferior reasoning.
- **Evidence anchors:** Abstract states the method "enables smaller LLMs to achieve performance comparable to that of larger models"; Section 3.1 notes Mixtral 8x7B (smaller) matches Claude-3 Sonnet post-alignment.
- **Break condition:** If smaller models fundamentally lack the capacity to make the required distinctions (not just mislabel them), alignment cannot recover performance.

## Foundational Learning

- **Concept:** Ridge regression (L2-regularized least squares)
  - **Why needed here:** Understanding why λI prevents matrix inversion failure with sparse, collinear data (one-hot encodings with few samples)
  - **Quick check question:** What happens to Ŵ when Z^⊤Z is singular and λ = 0?

- **Concept:** One-hot encoding for categorical data
  - **Why needed here:** Converting ordinal judgments to matrices enables linear algebra operations; understanding why this preserves categorical structure
  - **Quick check question:** If Z has m categories and N samples, what are the dimensions of Z^⊤Z?

- **Concept:** Response style bias in evaluation
  - **Why needed here:** Recognizing that LLMs have systematic tendencies (e.g., positivity bias) distinct from human evaluators
  - **Quick check question:** Why can't in-context examples alone fix response style bias (per Table 2)?

## Architecture Onboarding

- **Component map:** Input: (instance x) → LLM → raw judgment z (one-hot) → multiply by Ŵ → argmax → aligned judgment → ↓ Calibration: {(x_i, y_i)} → prompt LLM → Z matrix → solve Eq. 2 → Ŵ

- **Critical path:** Calibration data quality → LLM consistency on calibration set → Ŵ estimation stability → test-time alignment accuracy

- **Design tradeoffs:**
  - **Fewer categories (smaller |Y|):** Simpler mapping, fewer samples needed, but may lose granularity
  - **More calibration samples:** More robust Ŵ, but requires more human labels (diminishing returns after ~5-10 per category based on Figure 2)
  - **Per-task vs. transferred alignment:** Per-task is more accurate; transferred (Section 3.3) works when tasks share label semantics

- **Failure signatures:**
  - Identity mapping result (no change post-alignment): LLM already aligned OR calibration data too small/noisy
  - Large accuracy variance across splits: Calibration set unrepresentative
  - Alignment hurts performance: LLM-human correlation is negative on key categories

- **First 3 experiments:**
  1. Replicate on 2-3 tasks from Table 1 with 100, 20, and 5 calibration samples to validate sample efficiency claims
  2. Test alignment transfer between tasks with identical label spaces (as in Table 4) to characterize when transfer works
  3. Compare Ŵ learned from different human annotators on multi-annotator tasks to measure annotator-specific alignment variance

## Open Questions the Paper Calls Out

- **Open Question 1:** Does using model logits (probability distributions) instead of one-hot encoded outputs improve alignment quality when logits are accessible?
  - **Basis in paper:** Section 5 states: "when model logits are available, the one-hot encoded vectors representing LLM judgments can be replaced with the judgment probabilities derived from logits values. This modification could enable potentially finer alignment."
  - **Why unresolved:** The black-box setting was prioritized; experiments only used one-hot encodings from categorical outputs.
  - **What evidence would resolve it:** Direct comparison of alignment accuracy between logit-based and one-hot-based mappings on the same tasks using open-weight models.

- **Open Question 2:** Can concatenating outputs from multiple LLMs and learning a joint mapping to human labels yield higher alignment than single-model approaches?
  - **Basis in paper:** Section 5 suggests: "we could explore concatenating the outputs of multiple models and mapping them to human labels, effectively combining the strengths of various language models."
  - **Why unresolved:** Only single-model alignment was evaluated; the potential complementarity of different LLMs' judgment patterns remains unexplored.
  - **What evidence would resolve it:** Experiments combining outputs from multiple models (e.g., Claude-3, Mixtral, Llama-3) into a single mapping, compared against individual model baselines.

- **Open Question 3:** Does allowing LLMs to use judgment spaces with different sizes or interpretations than human evaluators (via prompt optimization) improve alignment performance?
  - **Basis in paper:** Section 5 states: "Enabling LLMs to have judgment spaces that differ from human evaluators offers several advantages. We can employ prompt optimization techniques to discover the optimal judgment space for an LLM."
  - **Why unresolved:** Experiments primarily used Z = Y (identical output spaces); the flexibility to use different spaces was not systematically explored.
  - **What evidence would resolve it:** Experiments varying LLM judgment space configurations (e.g., 5-point vs 3-point scales, different label semantics) and measuring resulting alignment quality.

- **Open Question 4:** How robust is alignment transfer across more diverse tasks and domains beyond the structurally similar ROSCOE datasets tested?
  - **Basis in paper:** Section 3.3 shows promising transfer within ROSCOE coherency tasks, but Table 4 results are only slightly worse than task-specific alignment. The broader generalization question remains open.
  - **Why unresolved:** Transfer was only tested between similar reasoning coherence tasks; cross-domain transfer (e.g., medical safety to translation quality) was not evaluated.
  - **What evidence would resolve it:** Experiments transferring learned alignments between dissimilar tasks from the benchmark (e.g., Medical Safety → WMT translation, SummEval → Newsroom) and measuring performance degradation.

## Limitations
- **Prompt dependency:** Results depend on Judge-Bench prompts being optimal for eliciting categorical judgments; no ablation on prompt variations
- **Sample efficiency boundary:** Claims of 1-2 examples per category are impressive but untested at scale (Figure 2 shows limited sample range)
- **Transfer learning ceiling:** Cross-task alignment success is reported but mechanism for when it works isn't characterized (what makes tasks "similar enough"?)

## Confidence
- **High confidence:** Linear alignment improves agreement with human judgments (146% average improvement, n=29 tasks)
- **Medium confidence:** 1-2 examples per category suffice for alignment (sample efficiency claim, but limited testing)
- **Medium confidence:** Smaller models can match larger models post-alignment (Mixtral vs Claude-3 comparison, but only one case shown)

## Next Checks
1. **Sample size scalability test:** Run alignment with 1, 2, 5, 10, 50 calibration examples per category on 3 diverse tasks to map the full learning curve and identify where gains plateau
2. **Prompt variation ablation:** Test alignment sensitivity to different prompt formulations (instruction wording, output format requirements) to quantify prompt dependency
3. **Correlation pre-check validation:** Measure Pearson correlation between LLM and human judgments before alignment on 10 tasks to test the "near-zero correlation" break condition prediction