---
ver: rpa2
title: Taming LLMs by Scaling Learning Rates with Gradient Grouping
arxiv_id: '2506.01049'
source_url: https://arxiv.org/abs/2506.01049
tags:
- learning
- arxiv
- training
- lora
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scaling with Gradient Grouping (SGG) addresses training instability
  and slow convergence in large language models (LLMs) by dynamically clustering gradient
  statistics within each layer and applying cluster-specific scaling factors to modulate
  parameter-wise learning rates. This approach maintains parameter-wise adaptation
  while imposing group-wise constraints to promote learning homogeneity across layers
  and clusters.
---

# Taming LLMs by Scaling Learning Rates with Gradient Grouping

## Quick Facts
- **arXiv ID:** 2506.01049
- **Source URL:** https://arxiv.org/abs/2506.01049
- **Reference count:** 39
- **Key outcome:** SGG improves LLM training stability and convergence by dynamically clustering gradient statistics and applying cluster-specific learning rate scaling

## Executive Summary
Scaling with Gradient Grouping (SGG) is an optimizer wrapper that addresses training instability and slow convergence in large language models by dynamically clustering gradient statistics within each layer and applying cluster-specific scaling factors to modulate parameter-wise learning rates. This approach maintains parameter-wise adaptation while imposing group-wise constraints to promote learning homogeneity across layers and clusters. Experiments demonstrate SGG consistently improves validation perplexity and accelerates convergence across diverse LLM and MLLM benchmarks, including C4 pre-training (e.g., 14.30 PPL vs 15.56 for 1B models), GLUE fine-tuning, commonsense reasoning tasks, and multimodal benchmarks. Notably, SGG enables low-rank pre-training to match full-rank performance for the first time and exhibits exceptional robustness to learning rate and batch size scaling, maintaining stable training across extreme configurations.

## Method Summary
SGG dynamically clusters momentum vectors within each layer using mini-batch K-means based on gradient similarity, then computes cluster-specific scaling factors based on Median of Deviation to Average (MDA) to modulate parameter-wise learning rates. The method wraps existing optimizers (like Adam/AdamW) and applies cluster scaling multiplicatively to preserve parameter-wise adaptation while promoting cross-layer homogenization. Clustering occurs every T iterations on CPU to avoid GPU memory overhead, with scaling factors smoothed via exponential moving average for stability.

## Key Results
- Consistently improves validation perplexity across diverse benchmarks (C4 pre-training, GLUE, commonsense reasoning, multimodal tasks)
- Enables low-rank pre-training to match full-rank performance for the first time
- Demonstrates exceptional robustness to learning rate and batch size scaling
- Integrates seamlessly with existing optimizers and PEFT techniques without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1: Gradient Clustering Exploits LLM Optimization Structure
LLM parameters exhibit non-independent optimization behaviors with natural clustering patterns that can be leveraged for grouped learning rate control. Momentum vectors within each layer are partitioned into K clusters via mini-batch K-means based on gradient similarity, capturing internally consistent groups while retaining parameter-wise variation within clusters.

### Mechanism 2: MDA-Based Scaling Promotes Cross-Layer Homogenization
Scaling learning rates inversely to cluster deviation from global statistics reduces layer-wise optimization discrepancies that cause loss spikes. Clusters with lower relative deviation receive larger scaling factors (up to 10×), while divergent clusters are scaled down (as low as 0.1×), suppressing disruptive updates from outlier clusters.

### Mechanism 3: Hybrid Control Preserves Fine-Grained Optimization Signals
Scaling (rather than replacing) parameter-wise learning rates captures benefits of both group-level regularization and parameter-level precision. Unlike Adam-mini which replaces per-parameter LRs with group means, SGG multiplies existing adaptive LRs by cluster-specific scaling factors while preserving the base optimizer's second-moment estimates.

## Foundational Learning

- **Exponential Moving Average (EMA)**
  - Why needed here: Used for both momentum estimation (β₁) and scaling factor updates (β₃). Understanding EMA decay rates is critical for tuning SGG's stability.
  - Quick check question: If β₃=0.99, approximately what weight does the current iteration's scaling factor receive versus historical values?

- **Adaptive Learning Rate Optimizers (Adam family)**
  - Why needed here: SGG wraps existing optimizers and relies on their first/second moment estimates. You must understand how Adam computes per-parameter learning rates to see what SGG modifies.
  - Quick check question: In Adam, what does the second-moment estimate (v_t) represent, and how does it affect the effective learning rate?

- **K-means Clustering and Distance Metrics**
  - Why needed here: Mini-batch K-means partitions momentum vectors. Understanding clustering quality vs. computational cost tradeoffs is essential for selecting K and interval T.
  - Quick check question: Why might mini-batch K-means outperform standard K-means for online gradient clustering in terms of speed-accuracy tradeoff?

## Architecture Onboarding

**Component Map:**
Base Optimizer (Adam/AdamW/etc.) -> SGG Wrapper (Clustering Module, Scaling Computer, LR Modulation) -> Parameter Update

**Critical Path:**
1. Standard gradient and momentum computation (per base optimizer)
2. Every T iterations: cluster momentum vectors per layer (CPU)
3. Compute MDA per cluster, derive scaling factors, apply EMA
4. Multiply base LR by cluster-specific scaling factor
5. Execute parameter update

**Design Tradeoffs:**
| Decision | Option A | Option B | Guidance |
|----------|----------|----------|----------|
| Clustering location | GPU | CPU | CPU adds ~4-8% time but zero GPU memory overhead; use GPU only if memory abundant |
| Cluster count K | 2-3 | 4+ | Paper finds K=2-3 optimal; higher K increases cost without clear gains |
| Recluster interval T | 5% iterations | 1% iterations | T=500 (5% of 10K steps) works well; more frequent clustering adds overhead |

**Failure Signatures:**
- No improvement over baseline: K may be wrong for task (run diagnostics in Table 9), or base optimizer hyperparameters need retuning
- Training divergence: Scaling factors may be too extreme; check if clamping to [0.1, 10] is active, reduce β₃
- Excessive memory usage: Ensure cluster indices C and scaling factors S are on CPU, not GPU

**First 3 Experiments:**
1. Sanity check on small model: Train LLaMA-60M on C4 with Adam+SGG vs Adam baseline. Verify PPL improvement of ~3-4 points. Test K∈{2,3} to find optimal cluster count.
2. CPU vs GPU overhead measurement: Profile training time and peak memory with clustering on CPU vs GPU for LLaMA-130M. Confirm CPU version adds <10% time overhead and zero GPU memory.
3. LoRA compatibility test: Fine-tune with LoRA (r=32) on a single commonsense reasoning task. Verify SGG improves over LoRA baseline by ~2-3% accuracy without breaking low-rank constraints.

## Open Questions the Paper Calls Out

### Open Question 1
Can learned grouping functions or heuristic-based static partitioning offer superior performance-efficiency trade-offs compared to SGG's online mini-batch K-means clustering? The authors identify this as a potential alternative in the Limitations section, noting that online clustering is "only one specific choice."

### Open Question 2
Is it possible to approximate the benefits of explicit gradient clustering through lightweight operations to eliminate computational overhead? The authors suggest future work focus on "operations that could approximate grouping benefits without explicit clustering" to address the significant costs of online clustering.

### Open Question 3
How does SGG perform when applied to Mixture-of-Experts (MoE) architectures or generative vision tasks such as diffusion models? The authors identify the need to extend evaluation to "architectures (e.g., vision backbones and Mixture-of-Experts)... and image generation."

## Limitations

- Limited generalization to non-transformer architectures like CNNs, RNNs, or graph neural networks
- Uncertainty about scaling behavior at extreme model sizes (10B+ parameters)
- Dependency on base optimizer quality - effectiveness with suboptimal base optimizers unclear

## Confidence

- **High Confidence (8-10/10):** Core mechanism of gradient clustering with MDA-based scaling is technically sound and empirically validated
- **Medium Confidence (5-7/10):** Claims about exceptional robustness to learning rate and batch size scaling are supported but edge cases at extreme configurations weren't fully explored
- **Low Confidence (1-4/10):** Claims about being "first" to enable low-rank pre-training to match full-rank performance need independent verification

## Next Checks

1. **Cross-architecture validation test:** Apply SGG to a CNN architecture (e.g., ResNet-50) on ImageNet and compare against standard optimizers. Measure both final accuracy and training stability across different learning rates to determine if gradient clustering assumptions transfer.

2. **Extreme scale experiment:** Test SGG on a 10B+ parameter model using gradient checkpointing and sharded data parallelism. Measure the absolute and relative overhead of the CPU clustering component and verify that scaling benefits persist at frontier model sizes.

3. **Cluster dynamics analysis:** Instrument SGG to log cluster composition changes throughout training (e.g., cluster purity metrics, cluster center movement). Analyze whether clusters remain stable during different training phases and correlate any cluster instability with performance degradation.