---
ver: rpa2
title: An Information Theoretic Perspective on Agentic System Design
arxiv_id: '2512.21720'
source_url: https://arxiv.org/abs/2512.21720
tags:
- information
- compressor
- predictor
- research
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework for designing
  compressor-predictor agentic systems, where smaller compressor LMs distill context
  into summaries consumed by larger predictor LMs. The authors develop a practical
  mutual information estimator that quantifies compression quality without requiring
  task-specific evaluation.
---

# An Information Theoretic Perspective on Agentic System Design

## Quick Facts
- arXiv ID: 2512.21720
- Source URL: https://arxiv.org/abs/2512.21720
- Reference count: 40
- Primary result: Mutual information between raw context and compression strongly predicts downstream task performance, independent of specific tasks.

## Executive Summary
This paper introduces an information-theoretic framework for designing compressor-predictor agentic systems, where smaller compressor language models distill context into summaries consumed by larger predictor LMs. The authors develop a practical mutual information estimator that quantifies compression quality without requiring task-specific evaluation. Empirical results across five datasets demonstrate that larger compressors not only improve accuracy but also achieve greater token efficiency, conveying up to 5.5× more information per token than smaller models. Scaling compressors proves substantially more effective than scaling predictors, with compressor model family and size outweighing predictor size in importance. A Deep Research application validates these principles, achieving 99% of frontier-LM accuracy at 26% of API costs using local 3B parameter compressors.

## Method Summary
The paper develops a Monte Carlo mutual information estimator to evaluate compression quality in compressor-predictor systems without task-specific labels. The method estimates I(X;Z|Q) by computing log probabilities from LM inference engines, using N=20 documents and M=20 compressions per document. The framework tests various compressor-predictor pairs (Qwen-2.5, Llama-3, Gemma-3 compressors with Llama and GPT predictors) across five datasets including LongHealth, FinanceBench, QASPER, WildChat, and FineWeb. Key implementation details include using proxy models (7-8B from different families) for small compressor MI estimation, temperature settings of 0.7 for compressors and 0.6 for predictors, and rate-distortion analysis to optimize system design.

## Key Results
- Larger compressors generate more concise outputs with higher information density, improving downstream accuracy
- Scaling compressors is 5× more effective than scaling predictors for accuracy gains
- Local 3B parameter compressors achieve 99% of frontier-LM accuracy at 26% of API costs in Deep Research application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual information between raw context and compression serves as a task-agnostic proxy for downstream performance.
- Mechanism: The compressor is modeled as a noisy channel; higher MI indicates more task-relevant information is preserved. A Monte Carlo estimator computes MI using log probabilities from LM inference engines without requiring full vocabulary distributions.
- Core assumption: MI correlates with downstream accuracy across tasks and model families. Assumption: The estimator is robust to proxy model choice at small scales.
- Evidence anchors:
  - [abstract] "We show that mutual information strongly predicts downstream performance, independent of any specific task."
  - [section 3.3] "mutual information is also strongly correlated with perplexity (r = −0.84, R² = 0.71)"
  - [corpus] Limited direct corpus support; related work addresses compression techniques but not MI-based evaluation.
- Break condition: Small compressors (1–3B) may produce miscalibrated log probabilities, requiring proxy models; the estimator can yield small negative values from finite-sample variance.

### Mechanism 2
- Claim: Scaling compressor model size yields greater accuracy gains than scaling predictor size, with sublinear FLOPs-per-generation growth.
- Mechanism: Larger compressors generate shorter, higher-information-density outputs. Because output token count decreases while quality increases, total compute scales sublinearly with parameters.
- Core assumption: Larger compressors are more concise within the same model family. Assumption: FLOPs-per-token scales linearly with parameter count for dense transformers.
- Evidence anchors:
  - [abstract] "scaling compressors being 5× more effective than scaling predictors"
  - [section 3.1] "scaling the Qwen-2.5 compressor from 1B to 7B improves accuracy by 60% whereas scaling the predictor from 70B to 405B yields only a 12% improvement"
  - [corpus] Neighbor papers (ACoRN, ACON) focus on compression methods, not scaling comparisons.
- Break condition: Trends are model-family dependent; Llama compressors show different conciseness scaling than Qwen.

### Mechanism 3
- Claim: Front-loading compute into local compressors reduces cloud predictor costs while maintaining accuracy.
- Mechanism: Local compressors (3–14B) distill context for smaller cloud predictors. High MI compressions enable predictors as small as 8–70B to recover frontier-model accuracy.
- Core assumption: Consumer hardware can run models up to 27B with acceptable latency. Assumption: API cost dominates over local compute cost in target deployments.
- Evidence anchors:
  - [abstract] "local compressors (as small as 3B parameters) to recover 99% of frontier-LM accuracy at 26% of API costs"
  - [section 3.5] "Qwen-2.5-14B compressor paired with a GPT-4o predictor achieves 2.3% higher RACE scores at only 28.1% of the API cost"
  - [corpus] No corpus papers validate the cost-analysis methodology.
- Break condition: Latency-sensitive applications may not tolerate local compression overhead; output token limits on predictors constrain synthesis quality.

## Foundational Learning

- Concept: **Mutual Information (Shannon)**
  - Why needed here: Core metric for evaluating compression quality independent of specific tasks.
  - Quick check question: Can you explain why I(X;Z) measures how much information Z preserves about X?

- Concept: **Rate-Distortion Theory**
  - Why needed here: Provides the theoretical framing for trading compression rate (bits/token) against distortion (accuracy loss).
  - Quick check question: How would you interpret a rate-distortion curve where distortion plateaus as rate increases?

- Concept: **Compressor-Predictor Architecture**
  - Why needed here: This is the system topology the paper analyzes—a smaller model summarizes, a larger model reasons over summaries.
  - Quick check question: In a two-stage pipeline, what information must the compressor preserve for the predictor to succeed?

## Architecture Onboarding

- Component map:
  - **Compressor LM**: Local model (1–14B) that generates query-conditioned summaries of raw context; outputs feed predictor.
  - **Predictor LM**: Cloud model (8B–405B or GPT-4o) that reasons over compressed summaries to produce final answers.
  - **MI Estimator**: Monte Carlo estimator using log probabilities from a proxy model (7–8B) to compute I(X;Z|Q).
  - **Rate-Distortion Analyzer**: Fits exponential decay to accuracy vs. bit-efficiency for model selection.

- Critical path:
  1. Ingest raw context X and query Q
  2. Compressor generates summary Z conditioned on Q
  3. Estimate MI to validate compression quality
  4. Predictor ingests Z and outputs answer Y
  5. Evaluate downstream accuracy; if insufficient, iterate on compressor size/family

- Design tradeoffs:
  - Compressor size vs. predictor size: Prioritize compressor scaling for accuracy gains
  - Local vs. cloud compute: Larger local compressors reduce API token costs but increase latency
  - Model family selection: Qwen-2.5 compressors are more compute-efficient than Llama; test your domain

- Failure signatures:
  - Compressor outputs too verbose → check prompt conciseness instructions; larger compressors should naturally be shorter
  - MI estimate negative or unstable → increase Monte Carlo samples (N, M); verify proxy model calibration
  - Predictor accuracy plateaus despite compressor scaling → may have hit irreducible distortion floor; check data quality

- First 3 experiments:
  1. **Baseline sweep**: On your target dataset, compare Qwen-2.5 compressors (1.5B, 3B, 7B) with a fixed predictor (e.g., Llama-70B). Measure accuracy, compression length, and estimated MI.
  2. **Predictor ablation**: Fix compressor at 7B, vary predictor (8B, 70B, 405B). Confirm accuracy gains diminish beyond 70B.
  3. **Cost simulation**: For your API pricing and latency constraints, model total cost per task across compressor sizes; identify the Pareto-optimal configuration.

## Open Questions the Paper Calls Out

- **Cross-task generalizability**: Does the compressor-predictor framework and mutual information metric generalize to reasoning-augmented models or iterative multi-agent workflows? The current study focuses primarily on GPT-style non-reasoning models with single-round communication, leaving the dynamics of reasoning traces unexplored.

- **Rate-distortion optimization**: Can training compressors with specific rate-distortion objectives optimize the compressor-predictor communication channel better than frozen models? The paper relies on off-the-shelf instruction-tuned models rather than training models to maximize information density.

- **Structured compression**: Is the correlation between mutual information and downstream performance maintained when compression takes the form of structured extraction or function calls? The empirical analysis is restricted to summarization (text-in, text-out) tasks across datasets like LongHealth and FinanceBench.

## Limitations

- Mutual information estimator reliability depends on proxy model calibration for small compressors (1-3B)
- Scaling analysis constrained by available model families and sizes, may not generalize to all architectures
- Empirical results rely on synthetic queries for some datasets, may not capture full real-world complexity

## Confidence

**High Confidence** (Multiple independent evidence sources, well-understood mechanisms):
- MI strongly predicts downstream accuracy across the tested datasets
- Larger compressors produce more concise outputs within the same model family
- Qwen-2.5 compressors demonstrate better compute efficiency than Llama compressors

**Medium Confidence** (Reasonable evidence but some assumptions or limited testing):
- Compressor scaling is 5× more effective than predictor scaling (based on specific model families tested)
- Sublinear FLOPs scaling holds across compressor sizes (assumes dense transformer architecture)
- Local compression reduces API costs by 74% in Deep Research application (single application case)

**Low Confidence** (Limited evidence, strong assumptions, or untested conditions):
- MI estimator works reliably for all compressor sizes without proxy models
- Findings generalize to non-QA tasks like creative writing or mathematical reasoning
- Cost benefits remain stable as API pricing models evolve

## Next Checks

1. **Cross-task generalizability test**: Apply the MI estimator and compressor-predictor architecture to a diverse set of tasks including mathematical reasoning (MATH dataset), code generation (HumanEval), and creative writing prompts. Measure whether MI continues to predict accuracy and whether the same scaling relationships hold across task types.

2. **Proxy model calibration study**: Systematically evaluate MI estimator accuracy across compressor sizes by comparing proxy-model-based estimates with ground-truth MI (where feasible via dimensionality reduction) or downstream accuracy correlations. Test multiple proxy model families and sizes to establish robustness bounds and determine minimum reliable compressor sizes.

3. **Real-world deployment stress test**: Implement the compressor-predictor system in a production-like environment with live user queries, measuring not just accuracy and cost but also latency distributions, error rates, and user satisfaction. Include edge cases like ambiguous queries, out-of-distribution topics, and adversarial inputs to identify failure modes not captured in synthetic evaluation.