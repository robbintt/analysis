---
ver: rpa2
title: 'PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language
  Models'
arxiv_id: '2502.14504'
source_url: https://arxiv.org/abs/2502.14504
tags:
- vision
- plphp
- attention
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Per-Layer Per-Head Vision Token Pruning (PLPHP),
  a two-level fine-grained pruning method for efficient large vision-language models
  (LVLMs). PLPHP dynamically allocates token retention rates layer-by-layer based
  on vision attention scores and performs head-level vision token pruning to preserve
  critical context for each attention head.
---

# PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models

## Quick Facts
- arXiv ID: 2502.14504
- Source URL: https://arxiv.org/abs/2502.14504
- Reference count: 10
- This paper proposes a two-level fine-grained pruning method that achieves 18% faster decoding speed and over 50% KV Cache size reduction with only 0.46% average performance degradation.

## Executive Summary
This paper introduces Per-Layer Per-Head Vision Token Pruning (PLPHP), a method for efficient inference in large vision-language models that dynamically prunes vision tokens based on their attention importance. The approach leverages the observation that vision token importance varies across layers, with attention scores dropping in intermediate layers but resurging in deeper layers. PLPHP operates by assigning layer-specific retention rates based on vision attention scores and performing head-level pruning to preserve diverse contextual information. The method achieves significant efficiency gains while maintaining or even improving performance on multi-image tasks.

## Method Summary
PLPHP is a plug-and-play vision token pruning method that operates during the prefilling stage of autoregressive decoding. It consists of two key mechanisms: (1) Layer-Level Retention Rate Allocation, where each layer is classified as vision-attentive, vision-indifferent, or vision-balanced based on its Vision Attention Score, and retention rates are assigned accordingly; and (2) Head-Level Vision Token Pruning, where each attention head independently selects the top-rl proportion of vision tokens to retain based on their attention scores. The method prunes layers 3 through N-1 only, preserving the global focus of the first and last layers. Default hyperparameters are (r, Δr, α, β) = (0.4, 0.3, 0.25, 0.1).

## Key Results
- Achieves 18% faster decoding speed and over 50% KV Cache size reduction across multiple benchmarks
- Demonstrates only 0.46% average performance degradation compared to uncompressed models
- Shows notable performance improvements on multi-image tasks (Spot-the-Diff +0.65 ROUGE-L, Multi-View +0.17 Overall Score)
- Outperforms baseline methods like FastV and VTW while maintaining higher retention of critical visual information

## Why This Works (Mechanism)

### Mechanism 1: Layer-Adaptive Retention Based on Vision Token Re-attention
PLPHP dynamically varies token retention rates per layer based on Vision Token Re-attention, where attention to vision tokens drops in intermediate layers but resurges in deeper layers. The method computes Vision Attention Scores per layer and assigns higher retention rates to layers showing strong vision attention, preserving critical visual information that would be lost with uniform pruning.

### Mechanism 2: Head-Level Token Selection Preserves Diverse Contextual Focus
By pruning at the head level rather than layer level, PLPHP maintains distinct contextual information that different heads specialize in. Each head independently selects vision tokens based on its attention patterns, preserving the diversity of visual reasoning across the multi-head attention mechanism.

### Mechanism 3: Redundancy Elimination Improves Multi-Image Task Performance
PLPHP's selective retention filters low-attention tokens in multi-image contexts, which the authors hypothesize are redundant. This pruning of redundant visual information can actually improve performance on multi-image tasks by reducing distracting or conflicting visual signals.

## Foundational Learning

- **KV Cache in Autoregressive Decoding**: Essential for understanding PLPHP's operation, as the method modifies the KV cache after prefilling. During decoding step t, the K and V tensors are retrieved from cache while Q is recomputed, making cache pruning effective for both memory and speed.
  - Quick check: During decoding step t, which tensors are recomputed vs. retrieved from cache for attention computation?

- **Multi-Head Attention and Head Specialization**: Critical for understanding why per-head pruning is meaningful. Heads project Q/K/V independently and attend in parallel, with different heads potentially specializing in different visual regions or features.
  - Quick check: If all heads in layer l had identical attention weights, would head-level pruning provide any benefit over layer-level pruning?

- **Attention Score as Importance Proxy**: Fundamental to PLPHP's token selection mechanism. While attention reflects token relevance to the current query, it's not a guaranteed importance signal, and low attention scores don't necessarily indicate unimportant tokens.
  - Quick check: What are two scenarios where a token could have low attention score but still be critical for correct generation?

## Architecture Onboarding

- **Component map**: Prefilling Stage → Layer-Level Retention Rate Allocation → Head-Level Vision Token Pruning → Decoding Stage
- **Critical path**: 1) Run prefilling through layer 2 (no pruning). 2) For layers 3 through N-1: compute γl, determine rl, apply head-level pruning immediately after that layer's prefilling completes. 3) Layer N (final layer): no pruning.
- **Design tradeoffs**: (r, Δr) control average retention and adaptivity range. Lower r → more compression, higher performance risk. Higher Δr → more layer-specific variation, sensitive to α/β thresholds. Pruning window (layers 3 to N-1) protects early/late layers with presumed global focus.
- **Failure signatures**: Sudden performance drop on image captioning suggests α too high. No speedup indicates pruning applied post-hoc. Multi-image task degradation suggests Δr too aggressive.
- **First 3 experiments**: 1) Baseline validation on LLaVA-OneVision-7B with default hyperparameters. 2) Ablation on Δr (fixed vs. adaptive retention). 3) Head-level vs. layer-level pruning comparison to quantify per-head selection benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the mechanistic causes of the "Vision Token Re-attention" phenomenon in deep decoder layers, and does it correlate with specific reasoning capabilities?
- **Basis in paper**: [explicit] The authors state, "We uncover the widespread phenomenon of Vision Token Re-attention," identifying it as a key motivation, but they treat it as an empirical observation rather than a fully understood mechanism.
- **Why unresolved**: The paper demonstrates that re-attention occurs but does not investigate *why* certain layers refocus on visual tokens or what specific computational functions these layers perform.
- **What evidence would resolve it**: A probing analysis linking the re-emergence of high attention scores to specific reasoning tasks or functional head specializations.

### Open Question 2
- **Question**: Can the static hyperparameters for retention rates (α, β, r) be replaced with input-adaptive mechanisms to minimize performance loss on high-complexity tasks?
- **Basis in paper**: [inferred] The method relies on manually tuned, fixed thresholds. While ablation studies demonstrate sensitivity to these values, the paper does not explore a dynamic, data-driven approach to setting them.
- **Why unresolved**: Different inputs likely require different amounts of visual context; static thresholds may aggressively prune difficult examples while retaining redundant tokens in simple ones.
- **What evidence would resolve it**: A comparison of the current fixed-threshold approach against a "learned" retention policy that adjusts rates based on input complexity or prompt type.

### Open Question 3
- **Question**: How does PLPHP perform on video understanding tasks where temporal redundancy must be managed alongside spatial redundancy?
- **Basis in paper**: [inferred] The paper evaluates static image tasks and cites video models in related work, but excludes video benchmarks from the experimental results.
- **Why unresolved**: Video tokens introduce temporal dependencies that static image pruning strategies might disrupt, potentially causing flickering or loss of temporal context.
- **What evidence would resolve it**: Benchmarking PLPHP on standard video question-answering datasets to measure both efficiency gains and temporal consistency.

## Limitations

- The paper does not provide direct comparison between head-level and layer-level pruning to quantify the benefit of per-head selection, leaving this key mechanism under-validated.
- Vision token identification across different model architectures is not specified, creating ambiguity in reproducing the method faithfully.
- The claim that pruning improves multi-image task performance by eliminating redundancy remains speculative without controlled experiments isolating redundancy effects.

## Confidence

**High Confidence**: The claim that PLPHP achieves 18% faster decoding speed and over 50% KV Cache size reduction is well-supported by presented experimental results and follows directly from the documented pruning methodology.

**Medium Confidence**: The assertion that layer-adaptive retention rates improve upon uniform pruning is reasonable given the documented Vision Token Re-attention phenomenon, but relies on the assumption that vision attention scores accurately reflect token importance.

**Low Confidence**: The claim that head-level pruning provides significant benefits over layer-level pruning lacks empirical support, as no direct comparison is provided to quantify this benefit.

## Next Checks

1. **Implement Head-Level vs Layer-Level Pruning Comparison**: Create a controlled experiment comparing PLPHP's head-level selection against a variant that applies identical token masks across all heads within each layer. Measure performance differences on multi-image benchmarks to quantify the benefit of per-head selection.

2. **Validate Vision Attention Score Reliability**: Conduct ablation studies where tokens are pruned based on alternative importance metrics (e.g., gradient-based saliency, random selection) versus attention scores. This would test whether the Vision Token Re-attention phenomenon genuinely reflects token utility.

3. **Test Cross-Model Generalization**: Apply PLPHP to at least one additional LVLM architecture beyond those tested using the same hyperparameters. Document whether the Vision Attention Score patterns and pruning effectiveness transfer across different vision-language model designs.