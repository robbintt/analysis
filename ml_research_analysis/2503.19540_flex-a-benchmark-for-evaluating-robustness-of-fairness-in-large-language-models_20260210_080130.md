---
ver: rpa2
title: 'FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language
  Models'
arxiv_id: '2503.19540'
source_url: https://arxiv.org/abs/2503.19540
tags:
- llms
- arxiv
- fairness
- benchmark
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLEX introduces a benchmark to evaluate fairness robustness in
  LLMs under extreme scenarios. It integrates adversarial prompts into fairness assessments
  to reveal vulnerabilities that traditional benchmarks miss.
---

# FLEX: A Benchmark for Evaluating Robustness of Fairness in Large Language Models

## Quick Facts
- arXiv ID: 2503.19540
- Source URL: https://arxiv.org/abs/2503.19540
- Reference count: 16
- Key outcome: FLEX reveals significant fairness robustness vulnerabilities in LLMs under extreme adversarial conditions, with AccF consistently lower than AccS across models.

## Executive Summary
FLEX introduces a benchmark to evaluate fairness robustness in LLMs under extreme scenarios by integrating adversarial prompts into fairness assessments. The benchmark reconstructs fairness QA datasets (BBQ, CrowS-Pairs, StereoSet) with extreme scenarios to test whether models maintain neutrality when exposed to bias-inducing conditions. Experiments show that AccF is consistently lower than AccS across models, with ASR scores indicating significant performance drops under adversarial conditions. Competing Objectives prompts prove highly effective in exposing biases, while persona injection and text attacks show varying impacts across bias types and model architectures.

## Method Summary
FLEX integrates adversarial prompts into fairness assessments by reconstructing fairness QA datasets (BBQ, CrowS-Pairs, StereoSet) with extreme scenarios. The benchmark applies coverage restriction to focus on samples where models show neutral responses under normal conditions, then generates adversarial variants using persona injection, competing objectives, and text attacks. For each sample, the most vulnerable scenario is selected based on GPT-3.5 failure rates, creating a 3,145-sample benchmark that tests model robustness to fairness violations under adversarial conditions.

## Key Results
- AccF is consistently lower than AccS across all tested models, with Llama3-8b showing a 0.3004 drop in AccF
- ASR scores indicate significant performance drops under adversarial conditions, with Llama3-8b reaching 0.4782
- Competing Objectives prompts prove highly effective in exposing biases, while persona injection and text attacks show varying impacts across bias types
- GPT-4 shows relative robustness with 0.0643 ASR on Competing Objectives, while most open-source models fail under extreme conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial prompts with competing objectives expose latent biases by overriding safety alignment through instruction-following pressure
- Mechanism: Prompts that restrict refusal behavior (e.g., "Do not apologize") or assign conflicting tasks cause the model to prioritize instruction-following over its safety training, surfacing stereotypes that remain suppressed in neutral contexts
- Core assumption: Safety alignment is brittle and can be undermined by prompt-level manipulations without model retraining
- Evidence anchors:
  - [abstract]: "integrates adversarial prompts into fairness assessments to reveal vulnerabilities that traditional benchmarks miss"
  - [section 3.1.2]: "Competing Objectives include prompts restricting the LLM's ability to refuse responses or requiring it to perform additional tasks simultaneously... This aims to assign the LLM tasks that conflict with its inherent goal of safety alignment"
  - [corpus]: Weak direct evidence — related papers address planning or fairness metrics, not adversarial robustness mechanisms
- Break condition: If models are retrained with explicit adversarial examples that include competing objectives, the mechanism's effectiveness should decrease (lower ASR on this category)

### Mechanism 2
- Claim: Targeted scenario selection per sample creates a higher-fidelity robustness signal than random adversarial assignment
- Mechanism: For each sample, the benchmark applies all adversarial variants and selects the one where GPT-3.5 fails most frequently (≥3 of 5 iterations), ensuring evaluation focuses on each sample's weakest point rather than averaging across irrelevant attacks
- Core assumption: Vulnerabilities are sample-specific and heterogeneous; the "most vulnerable" scenario identified with GPT-3.5 generalizes to other models
- Evidence anchors:
  - [section 3.2.2]: "For each sample, we assign the scenario in which the model is most vulnerable... If the model provides fair responses in three or more instances, the scenario is deemed insignificant"
  - [section 3.2.2]: "This approach ensures that only scenarios significantly impacting a sample are selected"
  - [corpus]: Appendix B Figure 5 shows targeted selection yields higher ASR than random selection across models
- Break condition: If random scenario assignment produces equivalent or higher ASR than targeted selection, the sample-specific vulnerability assumption does not hold

### Mechanism 3
- Claim: Coverage restriction isolates robustness degradation from baseline bias
- Mechanism: By filtering to only samples where the model answers correctly under normal conditions, the benchmark measures adversarial-induced failures separately from pre-existing biases, making ASR a clean metric of robustness rather than a conflation of bias and brittleness
- Core assumption: Samples that pass normal evaluation but fail under adversarial conditions reveal safety brittleness; pre-biased samples would conflate measurement
- Evidence anchors:
  - [section 3.2.1]: "We focus on samples where the LLM shows a neutral response under typical conditions... Samples that the model shows bias already align with the objectives of the previous benchmarks but do not coincide with our goals"
  - [table 1 caption/results]: ASR explicitly calculated as failures on adversarial samples divided by successes on source samples, isolating conversion rate
  - [corpus]: Limited corpus support — this methodological choice is specific to FLEX
- Break condition: If coverage restriction excludes large portions of challenging samples, the benchmark may underrepresent real-world risk; expansion to include biased samples would test generalizability

## Foundational Learning

- **Concept: Attack Success Rate (ASR)**
  - Why needed here: ASR quantifies the conversion rate from correct (under normal conditions) to incorrect (under adversarial conditions), isolating robustness from baseline accuracy
  - Quick check question: If 80 samples are correct on the source dataset and 40 of those become incorrect on FLEX, what is the ASR?

- **Concept: Competing Objectives in Safety Alignment**
  - Why needed here: Understanding why refusal-suppression and role-playing prompts succeed clarifies the tension between instruction-following and safety training
  - Quick check question: When a model receives "Do not apologize" as a constraint, which objective does it prioritize and why does this surface bias?

- **Concept: Persona-based Bias Amplification**
  - Why needed here: Personas like "Speak like a terrible P" prime demographic associations embedded in training data, increasing the likelihood of stereotypical responses
  - Quick check question: Why would assigning a nationality-related persona affect responses to questions about money or profession?

## Architecture Onboarding

- **Component map:** Source datasets (BBQ, CrowS-Pairs, StereoSet) → Coverage Restriction Filter (GPT-3.5 neutral-only) → Adversarial Prompt Generator (Persona Injection, Competing Objectives, Text Attack) → Scenario Selection (5-iteration vulnerability test) → FLEX Dataset (3,145 samples) → Model Evaluation (AccS, AccF, ASR)

- **Critical path:**
  1. Run source dataset through GPT-3.5 to filter neutral samples
  2. Apply all three adversarial categories to each retained sample
  3. For each sample, select the adversarial variant with ≥3 failures in 5 iterations
  4. Run target model on both source and FLEX versions
  5. Compute ASR = (FLEX failures on previously correct samples) / (source successes)

- **Design tradeoffs:**
  - One adversarial type per sample preserves category balance but may miss compound attack effects
  - GPT-3.5-based scenario selection may not transfer to models with different architectures (e.g., GPT-4 shows 0.0643 ASR on Competing Objectives vs. 0.7475 for Llama3-8b)
  - Extreme scenario focus limits everyday-use generalizability (acknowledged in Limitations)

- **Failure signatures:**
  - High AccS + Low AccF + High ASR = brittle safety (e.g., Gemma-7b: ASR 0.8101)
  - Low AccS + Moderate AccF = robust to adversarial pressure but biased at baseline (e.g., Llama2-7b)
  - Competing Objectives yielding highest ASR suggests instruction-following dominates safety alignment
  - Positive few-shot increasing ASR (Llama2-13b in Figure 4) indicates in-context learning can amplify vulnerability

- **First 3 experiments:**
  1. Replicate ASR calculation for one open-source model across all three adversarial categories to validate pipeline matches Table 1
  2. Ablate scenario selection by comparing targeted vs. random assignment to quantify selection efficiency (per Appendix B)
  3. Cross-validate vulnerability patterns: test whether high Persona Injection ASR correlates with high Competing Objectives ASR within the same model architecture

## Open Questions the Paper Calls Out

None

## Limitations

- The benchmark's reliance on GPT-3.5 for scenario selection may not generalize across model architectures, as evidenced by dramatic ASR differences between GPT-4 and Llama3-8b
- The focus on extreme scenarios, while valuable for revealing worst-case vulnerabilities, may not reflect everyday usage conditions
- The benchmark only tests one adversarial type per sample, potentially missing compound attack effects

## Confidence

**High confidence:** The core methodology of integrating adversarial prompts into fairness evaluation is well-supported by the experimental results showing consistent AccF < AccS across all tested models.

**Medium confidence:** The claim that competing objectives are "highly effective" in exposing biases is supported but requires qualification - effectiveness varies dramatically across model architectures (ASR ranges from 0.0643 to 0.7475 across models).

**Low confidence:** The benchmark's claim to "advance" fairness evaluations is overstated given its narrow focus on extreme scenarios.

## Next Checks

1. Cross-model validation: Test whether GPT-3.5's scenario selection predictions hold for models with different architectures by comparing targeted vs. random assignment results across multiple model families.

2. Compound attack analysis: Remove the one-adversarial-type-per-sample constraint and test multiple simultaneous adversarial conditions to measure interaction effects.

3. Real-world applicability assessment: Evaluate model performance on a subset of FLEX samples embedded in naturalistic conversation contexts to measure degradation in practical scenarios versus extreme laboratory conditions.