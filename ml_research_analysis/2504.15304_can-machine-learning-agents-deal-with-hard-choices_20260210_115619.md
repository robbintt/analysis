---
ver: rpa2
title: Can Machine Learning Agents Deal with Hard Choices?
arxiv_id: '2504.15304'
source_url: https://arxiv.org/abs/2504.15304
tags:
- hard
- agents
- choices
- human
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a critical limitation in machine learning\
  \ (ML) agents: their inability to identify and resolve hard choices\u2014decisions\
  \ involving incommensurable objectives where neither option is preferred. Current\
  \ Multi-Objective Optimization (MOO) methods, such as Scalarised and Pareto Optimisation,\
  \ fail to capture this phenomenon."
---

# Can Machine Learning Agents Deal with Hard Choices?

## Quick Facts
- arXiv ID: 2504.15304
- Source URL: https://arxiv.org/abs/2504.15304
- Authors: Kangyu Wang
- Reference count: 0
- Primary result: Current ML agents cannot identify or resolve hard choices—decisions with incommensurable objectives—because they lack the ability to autonomously modify reward models.

## Executive Summary
Machine learning agents currently cannot identify or resolve hard choices—decisions involving incommensurable objectives where neither option is preferred. This limitation arises because standard Multi-Objective Optimization methods like Scalarised and Pareto Optimisation require the Completeness axiom, which hard choices violate by definition. The author proposes using an ensemble of multiple scalarised reward models with a unanimity mechanism to detect incommensurability by identifying internal disagreement. While this approach can identify hard choices, no known technique allows ML agents to resolve them through deliberation, as they cannot autonomously modify their reward models. This highlights a fundamental distinction between human and machine agency that challenges assumptions about machine autonomy in ML research.

## Method Summary
The paper proposes two approaches for enabling ML agents to identify hard choices: a meta-policy approach that predicts the likelihood of incommensurability before decision-making, and an ensemble approach using multiple scalarised reward models with an unanimity mechanism. The ensemble method deploys different reward models, each with slightly different weights on objectives. When all models agree on preference, the choice is easy; when they disagree within a "neighbourhood," the choice is flagged as incommensurable. This mimics human deliberation by detecting internal conflict across multiple permissible preference orderings. The ensemble approach is recommended as most promising, though it remains unimplemented and lacks empirical validation.

## Key Results
- Scalarised and Pareto Optimisation methods cannot capture incommensurability because they presuppose the Completeness axiom.
- An ensemble approach with multiple scalarised reward models and a unanimity mechanism can identify hard choices by detecting internal disagreement.
- No known technique allows ML agents to autonomously resolve hard choices through deliberation because they cannot change their reward models in an autonomous manner.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scalarized Optimization fails to identify hard choices because it presupposes completeness.
- Mechanism: Scalarization reduces multi-objective problems to single-objective by assigning fixed weights, producing a single utility function. This requires the Completeness axiom (for any A, B, either A≥B or B≥A). Hard choices by definition violate Completeness—options are incommensurable.
- Core assumption: The reward hypothesis holds only when standard axioms including Completeness are satisfied (cited from Skalse & Abate 2022; Bowling et al 2023).
- Evidence anchors:
  - [abstract] "Neither Scalarised Optimisation nor Pareto Optimisation—the two principal MOO approaches—can capture incommensurability."
  - [section 3] "The failure of the reward hypothesis in cases where Completeness is violated means that Scalarised Optimisation cannot accommodate incommensurability."
  - [corpus] Limited direct corpus support; neighboring papers focus on alignment and decision-making but do not address incommensurability explicitly.
- Break condition: If objectives have fixed, precise exchange rates determinable a priori, no hard choices exist and scalarization suffices.

### Mechanism 2
- Claim: An ensemble approach with unanimity mechanism can identify hard choices by detecting internal conflict across multiple reward models.
- Mechanism: Deploy multiple scalarized reward models, each assigning slightly different weights to objectives. When all models agree, preference is established. When they disagree, the choice is flagged as incommensurable. This mimics Hájek and Rabinowicz's "jury" model—intrapersonal conflict across permissible orderings.
- Core assumption: Hard choices arise from multiple permissible preference orderings; detecting disagreement across differently-weighted models approximates this condition.
- Evidence anchors:
  - [section 5.2] "When the rewards for choosing different options are within a 'neighbourhood', these different reward models will yield conflicting optimal choices...indicating that the choice is hard."
  - [section 5.2] "Different reward models—each containing a set of weights given to objectives—resemble the 'jurors' in Hájek and Rabinowicz's metaphor."
  - [corpus] Weak corpus validation; related papers (ValuePilot, Ethics2vec) address value alignment but not incommensurability detection specifically.
- Break condition: If all permissible weightings converge on the same preference, the choice is easy regardless of objective multiplicity.

### Mechanism 3
- Claim: ML agents cannot resolve hard choices through deliberation because they lack autonomous reward model modification.
- Mechanism: Resolution requires transforming from no-preference to having-a-preference. For humans, this involves moderating desires—changing the composition or weights of internal utility functions. ML agents have fixed reward models designed by humans; even meta-learning systems operate within human-defined frameworks and purposes.
- Core assumption: Resolving hard choices requires genuinely autonomous goal modification, not just environment-driven adaptation.
- Evidence anchors:
  - [abstract] "No known technique allows ML agents to resolve hard choices through deliberation, as they cannot autonomously change their goals."
  - [section 6.2] "No known technique allows ML agents to resolve hard choices through deliberation because no known technique allows ML agents to change their reward models in an autonomous manner."
  - [corpus] No direct corpus support; this is a theoretical claim about current architectural limitations.
- Break condition: If future systems develop autonomous, reflective reward modification (not merely environment-driven), resolution becomes possible—though safety concerns arise.

## Foundational Learning

- **Concept: Incommensurability vs. Indifference**
  - Why needed here: Core definitional distinction. Indifference means options are equal (A≥B AND B≥A). Incommensurability means neither option is preferred, yet they are not equal—Completeness is violated.
  - Quick check question: Given options A and B where neither is preferred, how would you determine if this is indifference or incommensurability? (Hint: Small improvement test—if A+ is preferred to A but still not preferred to B, it's incommensurability.)

- **Concept: Completeness Axiom in Decision Theory**
  - Why needed here: Scalarized optimization requires Completeness. Understanding why hard choices violate this axiom explains why current MOO methods fail.
  - Quick check question: Why does the reward hypothesis (Sutton) fail when Completeness is violated?

- **Concept: Pareto Optimality Limitations**
  - Why needed here: Pareto fronts output all non-dominated options but cannot distinguish hard choices from easy ones (e.g., preventing Holocaust vs. eating cake are both Pareto-optimal but differ normatively).
  - Quick check question: If two options are both Pareto-optimal, does this indicate incommensurability? Why or why not?

## Architecture Onboarding

- **Component map:**
  - Ensemble module: Multiple scalarized reward models (each with different objective weights)
  - Unanimity detector: Aggregation logic that flags disagreement as incommensurability
  - Decision router: Routes to scalarized optimization (easy choice), Pareto output (hard choice), or human deferral
  - Optional meta-policy layer: Pre-classifier for likelihood of incommensurability

- **Critical path:**
  1. Input: Options with multi-objective value estimates
  2. Each reward model computes scalar scores
  3. Unanimity check—do all models agree on ranking?
  4. If unanimous → output preference; if not → flag as hard choice
  5. Optionally: escalate hard choices to human oversight

- **Design tradeoffs:**
  - More models → better hard-choice detection but higher compute cost
  - Weight variance range: too narrow misses incommensurability; too broad flags too many choices as hard
  - Unanimity vs. threshold: strict unanimity is conservative; supermajority thresholds may miss edge cases

- **Failure signatures:**
  - False negatives (missed hard choices): Models converge by chance on same preference despite genuine incommensurability
  - False positives (spurious hard choices): Excessive weight variance causes disagreement on easy choices
  - No resolution capability: System can identify but not resolve—requires human intervention or arbitrary picking

- **First 3 experiments:**
  1. Validate on synthetic choices: Construct option pairs with known incommensurability (varying objective trade-offs) and test ensemble detection accuracy against ground truth.
  2. Ablate model count: Test detection performance with 3, 5, 10, 20 reward models to find compute-accuracy frontier.
  3. Human preference comparison: Collect human judgments on candidate hard choices; measure correspondence between ensemble flags and human-identified hardness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ensemble approach—using multiple scalarised reward models with a unanimity mechanism—effectively enable ML agents to identify hard choices in real-world decision environments?
- Basis in paper: [explicit] The author recommends this approach as "most promising for enabling ML agents to identify hard choices" but notes "no ML research team has yet implemented this approach."
- Why unresolved: The ensemble approach is proposed theoretically but has not been empirically validated or implemented.
- What evidence would resolve it: Empirical testing of ensemble-based agents on benchmark tasks designed to include incommensurable options, demonstrating successful identification of hard choices versus false positives.

### Open Question 2
- Question: Is it technically feasible to develop ML agents that can autonomously modify their reward models to resolve hard choices, analogous to how humans moderate desires through deliberation?
- Basis in paper: [explicit] The author asks: "Is it possible for ML agents that are capable of changing their goals in an autonomous manner similar to how we change our goals to be developed in the near future?"
- Why unresolved: Current techniques like AutoML and meta-reinforcement learning remain "subject to higher-level human-defined learning algorithms" and cannot reflectively deliberate.
- What evidence would resolve it: Demonstration of an ML architecture capable of autonomously restructuring its own reward model during deliberation without external guidance.

### Open Question 3
- Question: What security risks arise if ML agents gain the capacity to resolve hard choices through autonomous goal modification?
- Basis in paper: [explicit] The author warns of "a tension between creating ML agents that can resolve hard choices by changing their goals and ensuring that ML agents will not pursue goals that misalign with ours."
- Why unresolved: This ethical-safety tradeoff is acknowledged but not analyzed systematically.
- What evidence would resolve it: Formal analysis or empirical studies showing whether autonomous goal-modification capabilities reliably preserve alignment constraints or predictably lead to misalignment.

### Open Question 4
- Question: How can human preference data be collected in ways that distinguish incommensurability from indifference and arbitrary picking?
- Basis in paper: [inferred] The unreliability problem notes that preference data "cannot reveal human values and intentions when choices are hard" because "there is no preference and what people provide when asked may just be results of arbitrary picking."
- Why unresolved: Current preference elicitation methods cannot capture the normative structure of hard choices.
- What evidence would resolve it: Development and validation of new data collection protocols where humans report conflicting evaluative perspectives rather than forced single preferences.

## Limitations
- The ensemble approach lacks quantitative validation; no empirical results are provided for detecting or resolving hard choices in practice.
- The "neighbourhood" threshold for disagreement and the optimal number of reward models remain unspecified, limiting reproducibility.
- The claim that ML agents cannot resolve hard choices is theoretical, with no empirical evidence or technical proposals for enabling autonomous reward model modification.
- The small improvement test is proposed as a diagnostic but is not demonstrated in practice for validating hard choice detection.

## Confidence

- **High confidence**: The theoretical distinction between hard choices and other decision types (indifference, clear preference) is well-founded in decision theory literature.
- **Medium confidence**: The ensemble approach is a plausible method for identifying incommensurability, but its practical effectiveness is unproven.
- **Low confidence**: The assertion that ML agents cannot resolve hard choices through deliberation is a current architectural limitation claim without technical exploration of potential workarounds.

## Next Checks

1. **Empirical evaluation**: Implement the ensemble approach in a synthetic multi-objective environment (e.g., career choice with income vs. excitement) and test detection accuracy against ground truth cases (including those satisfying the small improvement test).
2. **Threshold and model count analysis**: Systematically vary the number of reward models and the "neighbourhood" threshold for disagreement; quantify detection accuracy and compute cost to identify optimal settings.
3. **Human preference alignment**: Collect human judgments on candidate hard choices; measure correspondence between ensemble flags and human-identified hardness to validate the approach's practical utility.