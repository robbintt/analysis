---
ver: rpa2
title: Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal
  Transport
arxiv_id: '2510.27340'
source_url: https://arxiv.org/abs/2510.27340
tags:
- drag
- regularization
- convergence
- have
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DRAG, a stochastic gradient descent algorithm
  with decreasing entropic regularization for semi-discrete optimal transport. The
  method addresses the bias introduced by fixed regularization in entropic OT by gradually
  reducing the regularization parameter as optimization progresses.
---

# Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport

## Quick Facts
- **arXiv ID**: 2510.27340
- **Source URL**: https://arxiv.org/abs/2510.27340
- **Reference count**: 40
- **Primary result**: DRAG achieves unbiased O(1/t) sample and iteration complexity for both optimal transport cost and potential estimation, and O(1/√t) convergence rate for the transport map.

## Executive Summary
This paper introduces DRAG, a stochastic gradient descent algorithm with decreasing entropic regularization for semi-discrete optimal transport. The method addresses the bias introduced by fixed regularization in entropic OT by gradually reducing the regularization parameter as optimization progresses. DRAG achieves unbiased O(1/t) sample and iteration complexity for both optimal transport cost and potential estimation, and O(1/√t) convergence rate for the transport map. The key innovation is leveraging the improved restricted strong convexity properties that emerge locally around the optimal solution as regularization decreases.

## Method Summary
DRAG is a stochastic gradient descent algorithm that combines decreasing entropic regularization with iterate averaging. The algorithm maintains bounded iterates through projection onto a convex set and uses a carefully designed regularization schedule ε_t = ε_1 · t^-a with decreasing step sizes γ_t = γ_1 · t^-b. The gradient is computed using softmax probabilities that depend on both the current iterate and the regularization parameter. The method maintains O(dtM) computational complexity and O(dM) spatial complexity, making it suitable for large-scale problems.

## Key Results
- DRAG achieves O(1/t) convergence rates for potential and OT cost estimation
- O(1/√t) convergence rate for transport map estimation
- Theoretical guarantees hold with probability approaching 1 as sample size increases
- Numerical experiments confirm theoretical convergence rates and demonstrate practical advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decreasing regularization progressively removes entropic bias while maintaining acceleration benefits.
- Mechanism: Fixed ε > 0 introduces a bias term proportional to ε in the OT solution. By setting ε_t = t^-a (with a ≈ 1/3), regularization decays as samples accumulate—initially providing strong smoothing for fast early convergence, then vanishing to eliminate asymptotic bias.
- Core assumption: The source measure μ is α-Hölder continuous on a bounded convex domain (Assumption 1), ensuring entropic potentials converge faster than linearly as ε' → ε.
- Evidence anchors:
  - [abstract] "DRAG... addresses the bias introduced by fixed regularization in entropic OT by gradually reducing the regularization parameter as optimization progresses."
  - [Section 2.3, paragraph 3] "Studies show that regularization must decrease as the number of samples increases."
  - [corpus] Weak direct support; related work on annealing exists (Sinkhorn ε-scaling) but lacks theoretical guarantees for this specific scheme.
- Break condition: If ε_t decays too fast (a ≥ b/2), iterates may escape the local RSC region; if too slow, bias persists longer than necessary.

### Mechanism 2
- Claim: Local restricted strong convexity (RSC) around the optimum becomes independent of ε, enabling faster convergence without ill-conditioning.
- Mechanism: The semi-dual H_ε has global RSC constant scaling as O(1/ε), but within an ε/2 ball of g*_ε, the local RSC constant is ρ* (independent of ε). DRAG's decreasing ε keeps iterates in this increasingly favorable region as optimization progresses.
- Core assumption: Proposition 2 holds—P(||g_t - g*_{ε_t}|| ≥ ε_t) decays polynomially fast, ensuring high-probability residence in the RSC zone.
- Evidence anchors:
  - [Section 3.3] Lemma 2 explicitly derives: global RSC ~ ρ*/ε, local RSC ~ ρ*(1-e^-1) within ε/2 radius.
  - [Section 3.4] "This result is key to leveraging the locally enhanced RSC of H_{ε_t} and guides how quickly the regularization can decay."
  - [corpus] No direct external validation; mechanism is internally proven.
- Break condition: Violation of Assumption 1 (unbounded support, discontinuous density) could break the RSC analysis.

### Mechanism 3
- Claim: Averaging SGD iterates accelerates convergence to O(1/t) for both OT cost and potential estimation.
- Mechanism: Polyak-Juditsky averaging adapts to unknown local strong convexity. Despite the time-varying objective (due to decreasing ε), Theorem 2 shows that g_t = (1/t)Σg_k achieves E[||g_t - g*||²] ≲ 1/t^s where s = min{1, 2a + 2aα'}, recovering O(1/t) when α > 1/2.
- Core assumption: Parameters satisfy 2a < b, a + b < 1, 1 + a + aα > 2b (feasible with a ≈ 1/3, b ≈ 2/3).
- Evidence anchors:
  - [Section 3.4, Theorem 2] "DRAG fully exploits the acceleration thanks to averaging."
  - [Section 4.1, Corollary 1] "When α > 1/2... we achieve an O(1/t) convergence rate, which is optimal for strongly convex objectives."
  - [corpus] General ASGD theory is standard (Polyak & Juditsky), but application to decreasing-regularization OT is novel.
- Break condition: Poor initialization may require weighted averaging (Section 8.2) to overcome slow early averaging.

## Foundational Learning

- Concept: **Semi-discrete Optimal Transport**
  - Why needed here: The entire paper operates in this setting (continuous source μ, discrete target ν). The semi-dual reduces to a finite-dimensional convex problem in R^M, enabling SGD.
  - Quick check question: Can you explain why semi-discrete OT reduces to estimating Laguerre cells?

- Concept: **Entropic Regularization (EOT)**
  - Why needed here: DRAG modifies standard EOT by making ε time-dependent. Understanding the bias-speed trade-off is essential.
  - Quick check question: Why does adding ε·KL(π||μ⊗ν) make the objective 1/ε-smooth?

- Concept: **Restricted Strong Convexity (RSC)**
  - Why needed here: RSC is strictly weaker than strong convexity but sufficient for SGD convergence guarantees. The local-versus-global distinction in Lemma 2 is the technical core.
  - Quick check question: How does RSC differ from standard strong convexity, and why does it suffice for convergence analysis?

## Architecture Onboarding

- Component map: Initialize g_0 → Sample X_t ~ μ → Compute softmax probabilities χ^ε_t → Update: g_t ← Proj_C(g_{t-1} - γ_t · ∇h) → Average: g_t ← (t/(t+1))·g_{t-1} + (1/(t+1))·g_t → Return g_t

- Critical path:
  1. Initialize g_0 = 0, choose (γ_1, a, b) with a ≈ 1/3, b ≈ 2/3
  2. Sample X_t ~ μ, compute softmax probabilities χ^ε_t
  3. Update: g_t ← Proj_C(g_{t-1} - γ_t · ∇h)
  4. Average: g_t ← (t/(t+1))·g_{t-1} + (1/(t+1))·g_t
  5. Return g_t for OT cost; return T(g_t)(x) = y_j for x in L_j(g_t) for map estimation

- Design tradeoffs:
  - **a vs b**: Larger a → faster bias removal but risk of exiting RSC zone; larger b → slower step-size decay. Default: a = 0.33, b = 0.67
  - **Mini-batching**: Increases GPU parallelism, reduces variance. Scale γ_1 by √(batch_size).
  - **Weighted averaging**: Use log(k+1)^ω weighting to reduce sensitivity to initialization.

- Failure signatures:
  - Iterates diverge or oscillate: γ_1 too large relative to problem scale
  - Slow convergence to biased solution: a too small (ε decays too slowly)
  - Instability at late iterations: b too small relative to a (violates 2a < b)
  - Map estimate has holes: Poor coverage of target support; consider increasing M

- First 3 experiments:
  1. **Synthetic validation** (Examples 1–3 from paper): Uniform source on [0,1]^d, uniform/discrete target with known g*. Verify ||g_t - g*||² ~ 1/t and ||T - T_t||² ~ 1/√t. Use M=1000, d=10.
  2. **Comparison vs fixed-ε ASGD**: Run DRAG (a=0.33, b=0.67) against ASGD with ε ∈ {0.5, 0.1, 0.05, 0.01}. Plot error vs iterations. Expect DRAG to dominate across all stages.
  3. **Generative modeling sanity check**: Replicate Figure 4—map standard Gaussian prior to Swiss roll / spiral arms target. Compare DRAG vs Adam for mode coverage. DRAG should show better coverage on multimodal targets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical acceleration benefits of decreasing regularization be rigorously proven for the fully discrete OT setting (annealing/ε-scaling schemes for Sinkhorn)?
- Basis in paper: [explicit] "Our results also motivate further investigation of decreasing regularization in (i) discrete OT, by adapting our approach to demonstrate the acceleration benefits of annealing schemes..."
- Why unresolved: While annealing is used heuristically in discrete OT and improves empirical performance, the paper notes "their theoretical analysis remains largely open [52, 48, 12]."
- What evidence would resolve it: A theoretical analysis extending DRAG's techniques to the discrete setting, proving convergence rates for annealed Sinkhorn algorithms.

### Open Question 2
- Question: Can DRAG be extended to general cost functions beyond the quadratic cost while preserving its convergence guarantees?
- Basis in paper: [explicit] "Proposition 1... is only given for the quadratic cost and is therefore the limiting factor in our analysis for broadening the class of cost functions for DRAG."
- Why unresolved: The fast convergence of entropic potentials (Proposition 1) relies on results specific to quadratic costs.
- What evidence would resolve it: Proof of analogous convergence rates for entropic potentials under alternative cost functions, or a modified algorithm that achieves similar rates without this dependency.

### Open Question 3
- Question: Can adaptive step-size schemes (e.g., Adam, Adagrad variants) be incorporated into DRAG while preserving theoretical guarantees?
- Basis in paper: [explicit] "...by developing new optimized versions of DRAG, such as those incorporating adaptive step sizes in a similar way to Adam or Adagrad."
- Why unresolved: DRAG's analysis depends on specific step-size schedules (γt = γ₁t⁻ᵇ) tied to the decreasing regularization rate.
- What evidence would resolve it: Convergence analysis for DRAG with adaptive step sizes, showing maintained or improved rates.

### Open Question 4
- Question: Can optimal O(1/t) rates for potential estimation and O(1/√t) rates for map estimation be achieved when the source density has low regularity (α ≤ 1/2)?
- Basis in paper: [inferred] The paper notes DRAG achieves optimal rates when α > 1/2, but Pooladian et al. [45] "achieves the optimal rate for any α ∈ (0, 1)" using a different approach.
- Why unresolved: DRAG's convergence rate depends on s = min{1, 2a + 2aα′}, which degrades for smaller α.
- What evidence would resolve it: Modified algorithm or analysis achieving minimax rates across all α ∈ (0,1].

## Limitations

- Theoretical guarantees critically depend on Hölder continuity of the source measure, which may not hold for many practical datasets
- The decreasing regularization schedule requires careful parameter tuning (a ≈ 1/3, b ≈ 2/3); violations of constraints could severely degrade performance
- The local RSC property is proven but relies on polynomial decay bounds that may be loose in practice

## Confidence

- **High confidence** in the O(1/t) convergence for potential estimation and O(1/√t) for transport map—these follow from standard averaging theory adapted to the RSC setting with rigorous proofs
- **Medium confidence** in practical performance on real-world data—the synthetic experiments validate theory but may not capture dataset-specific challenges
- **Medium confidence** in the bias-elimination mechanism—while the theoretical framework is sound, the rate of ε_t decay (a ≈ 1/3) is derived from worst-case analysis

## Next Checks

1. **Robustness to Assumption Violations**: Test DRAG on source measures with discontinuous densities or unbounded support to quantify performance degradation when Assumption 1 fails

2. **Parameter Sensitivity Analysis**: Systematically vary (a, b) around the theoretical defaults to map the convergence rate landscape and identify practical optimal values

3. **Real-World Benchmark Comparison**: Evaluate DRAG against fixed-ε ASGD and non-stochastic methods on standard OT benchmarks like image color transfer or domain adaptation tasks