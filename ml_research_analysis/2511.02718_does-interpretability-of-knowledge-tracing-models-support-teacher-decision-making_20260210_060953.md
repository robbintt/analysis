---
ver: rpa2
title: Does Interpretability of Knowledge Tracing Models Support Teacher Decision
  Making?
arxiv_id: '2511.02718'
source_url: https://arxiv.org/abs/2511.02718
tags:
- teachers
- knowledge
- simulation
- task
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether interpretable knowledge tracing
  (KT) models help teachers make better pedagogical decisions compared to non-interpretable
  models. A simulation environment was created where teachers selected tasks and decided
  when to stop teaching based on information from three KT models: Bayesian Knowledge
  Tracing (BKT), Performance Factors Analysis (PFA), and Deep Knowledge Tracing (DKT).'
---

# Does Interpretability of Knowledge Tracing Models Support Teacher Decision Making?

## Quick Facts
- arXiv ID: 2511.02718
- Source URL: https://arxiv.org/abs/2511.02718
- Reference count: 7
- Teachers rated interpretable models higher in usability and trustworthiness but often selected suboptimal tasks

## Executive Summary
This study investigates whether interpretable knowledge tracing models help teachers make better pedagogical decisions compared to non-interpretable models. A simulation environment was created where teachers selected tasks and decided when to stop teaching based on information from three KT models: Bayesian Knowledge Tracing (BKT), Performance Factors Analysis (PFA), and Deep Knowledge Tracing (DKT). The research reveals a complex relationship between model interpretability and teacher decision-making, showing that while teachers prefer and trust interpretable models more, this preference doesn't necessarily translate to better decision outcomes.

## Method Summary
The study employed a simulation environment where teachers made pedagogical decisions based on information from three knowledge tracing models. The simulation involved selecting tasks and determining when to stop teaching, with the models providing different levels of interpretability. Both automated simulations and human teacher experiments were conducted to compare decision-making outcomes across the BKT, PFA, and DKT models. The study measured mastery time, usability ratings, and task selection quality to evaluate the impact of interpretability on teaching decisions.

## Key Results
- Interpretable models (BKT, PFA) achieved mastery faster and more reliably than non-interpretable DKT in automated simulations
- Human teachers showed no significant difference in mastery time across models when making decisions
- Teachers rated interpretable models higher in usability and trustworthiness but often selected suboptimal tasks

## Why This Works (Mechanism)
The study suggests that teachers may be misinterpreting model information despite preferring interpretable models. The mechanism appears to involve teachers relying on additional factors beyond KT model information, potentially due to cognitive biases or lack of proper training in interpreting model outputs. This indicates that interpretability alone may not be sufficient for effective decision support without proper understanding of how to use the information.

## Foundational Learning
- Knowledge Tracing (KT) Models: Why needed - to track student learning progress and predict performance
  Quick check: Understanding how KT models differ in architecture and interpretability
- Interpretability vs Performance: Why needed - to balance model transparency with prediction accuracy
  Quick check: Identifying which model features contribute to interpretability
- Pedagogical Decision Making: Why needed - to evaluate how teachers use model information
  Quick check: Understanding common decision patterns and biases

## Architecture Onboarding
Component Map: Teacher -> KT Model (BKT/PFA/DKT) -> Task Selection -> Mastery Assessment
Critical Path: Model information interpretation → Task selection decision → Student progress evaluation
Design Tradeoffs: Interpretability vs prediction accuracy; simplicity vs comprehensiveness
Failure Signatures: Suboptimal task selection despite interpretable model information; preference for interpretability without performance improvement
First Experiments: 1) Test interpretation accuracy with explicit model training 2) Compare decision quality across multiple subject domains 3) Measure eye-tracking patterns during model interpretation

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The artificial simulation environment may not accurately reflect real-world teaching conditions
- Small sample size of 12 teachers limits generalizability of findings
- Focus on linear functions in mathematics may not generalize to other subject areas

## Confidence
- **High confidence**: Automated simulation showing interpretable models achieve mastery faster than DKT
- **Medium confidence**: Human teachers showing no significant mastery time differences across models
- **Low confidence**: Teachers misinterpreting model information due to suboptimal task selection

## Next Checks
1. Conduct a larger-scale study with more teachers across multiple educational domains
2. Implement eye-tracking or think-aloud protocols during teacher decision-making
3. Design experiment testing hypothesis that teachers misinterpret model information with explicit training