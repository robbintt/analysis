---
ver: rpa2
title: 'SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake
  Content on Social Media Platforms'
arxiv_id: '2506.05538'
source_url: https://arxiv.org/abs/2506.05538
tags:
- dataset
- deepfake
- content
- detection
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SocialDF, a real-world dataset of 2,126 short-form
  videos containing 1,071 real and 1,055 manipulated deepfakes from social media.
  The dataset captures complex scenarios such as multiple speakers, scene changes,
  and overlays, addressing the gap in existing datasets that are overly simplistic
  and controlled.
---

# SocialDF: Benchmark Dataset and Detection Model for Mitigating Harmful Deepfake Content on Social Media Platforms

## Quick Facts
- **arXiv ID:** 2506.05538
- **Source URL:** https://arxiv.org/abs/2506.05538
- **Reference count:** 26
- **Primary result:** Proposed multimodal deepfake detection framework achieves 90.4% accuracy and 0.93 F1-score on SocialDF dataset, significantly outperforming lip-sync detection baselines.

## Executive Summary
This work introduces SocialDF, a real-world dataset of 2,126 short-form social media videos containing 1,071 real and 1,055 manipulated deepfakes. Unlike existing datasets that use controlled, single-speaker scenarios, SocialDF captures complex editing techniques including scene changes, multiple speakers, and overlays common in Instagram Reels and Stories. The authors propose a two-stage multimodal framework that combines facial recognition, automatic speech recognition, and a multi-agent LLM pipeline to verify both identity attribution and content plausibility. When evaluated against state-of-the-art lip-sync detection methods, the framework achieves 90.4% accuracy with 0.93 F1-score, demonstrating superior performance on realistic social media content.

## Method Summary
The framework operates in two stages: Stage 1 extracts identity features using YOLO face detection and FaceNet embeddings (cosine similarity matching against a database of 869 influential people) while also transcribing audio with Whisper Large V3 Turbo. Stage 2 employs a three-agent LLM system using DeepSeek R-1 (Llama 8B distillation) with temperature 0.5 - Agent 1 verifies speaker plausibility, Agent 2 performs fact-checking via DuckDuckGo search, and Agent 3 synthesizes the final real/fake classification. The approach requires no model training, instead relying on frozen pretrained models and carefully designed prompts.

## Key Results
- Proposed framework achieves 90.4% accuracy and 0.93 F1-score on SocialDF test set
- Outperforms LipFD lip-sync detection baseline (51.24% accuracy, 0.63 F1-score) by 39 percentage points
- DeepSeek R-1 (90.4%) slightly outperforms Llama 3.3 (89.5%) in the same pipeline configuration
- LipFD exhibits 99% false negative rate on SocialDF due to scene cuts and occlusions

## Why This Works (Mechanism)

### Mechanism 1
Content-level plausibility verification outperforms temporal lip-sync analysis for real-world social media deepfakes. The framework extracts identity and speech content, then uses an LLM to assess whether attributed speech is consistent with known communication patterns and factual accuracy. This bypasses reliance on continuous facial visibility. Core assumption: Deepfakes designed for misinformation will contain implausible or factually incorrect statements that an LLM with web search can identify. Evidence: LipFD plateaus at 51.24% accuracy and fails with occlusions, scene changes. Break condition: If deepfakes contain only plausible, factually accurate statements.

### Mechanism 2
Multi-agent LLM decomposition improves detection reliability over single-model inference. Agent-1 assesses attribution plausibility, Agent-2 evaluates factual correctness via web search, and a final agent synthesizes both analyses. This separation creates redundant verification paths. Core assumption: Decomposing the verification task reduces hallucination risk and allows independent fact-checking. Evidence: Multi-agent approach achieves 90.4% accuracy. Break condition: If agents share common failure modes.

### Mechanism 3
Dataset realism - incorporating scene changes, multiple speakers, and overlays - exposes limitations of models trained on controlled benchmarks. SocialDF samples from actual social media where creators use editing techniques that break assumptions of lip-sync detectors requiring continuous single-speaker close-ups. Core assumption: Models achieving high accuracy on controlled datasets will generalize poorly to real-world distribution shifts. Evidence: 39-point accuracy gap between LipFD on controlled data vs. SocialDF. Break condition: If future detection models are trained on similarly complex data.

## Foundational Learning

**Multimodal Embedding Alignment**
- Why needed: Framework requires understanding how facial embeddings (FaceNet 512-dim) and audio transcripts map to the same identity for cross-verification.
- Quick check: Can you explain why cosine similarity is used for face matching rather than Euclidean distance?

**LLM Chain-of-Thought Reasoning**
- Why needed: DeepSeek R-1's superior performance (90.4% vs. 89.5%) is attributed to its reinforcement learning training for self-verification and reflection.
- Quick check: How does temperature=0.5 balance determinism vs. hallucination risk in fact-checking tasks?

**Distribution Shift in Detection Benchmarks**
- Why needed: 39-point accuracy gap between LipFD on controlled data vs. SocialDF demonstrates how benchmark design affects model evaluation.
- Quick check: What specific real-world factors (occlusions, scene cuts, overlays) would cause a lip-sync detector's false negative rate to spike to 99%?

## Architecture Onboarding

**Component map:** Video input → YOLO face detection → FaceNet embedding → Cosine similarity match → Parallel: (1) Whisper transcription, (2) Web search retrieval → Agent-1 (attribution) + Agent-2 (fact-check) → Final Agent (synthesis) → Binary classification

**Critical path:** Video input → Face detection (YOLO) → Face embedding (FaceNet) → Identity match → Parallel: (1) Transcript extraction (Whisper), (2) Web search retrieval → LLM agent pipeline → Deepfake probability

**Design tradeoffs:**
- Lip-sync detection (LipFD): Fast, no external dependencies, but fails on scene cuts/occlusions (FNR=99%)
- Content-based fact-checking (proposed): Handles complex edits, but requires web access and LLM inference latency
- Temperature selection: 0.3 = more deterministic but may miss nuanced reasoning; 0.7 = more creative but higher hallucination risk

**Failure signatures:**
- LipFD: Near-perfect FPR (0%) but FNR≈99% → predicts "real" for everything
- Proposed framework: Would fail on (1) non-public figures not in the 869-person database, (2) deepfakes with factually plausible content, (3) videos in languages poorly supported by Whisper or the LLM

**First 3 experiments:**
1. Baseline replication: Run LipFD on SocialDF's test split to confirm 51.24% accuracy plateau and 99% FNR
2. Ablation on LLM temperature: Test full pipeline at temperatures 0.3, 0.5, 0.7 to verify 0.5 is optimal; measure both accuracy and inference time
3. Agent decomposition test: Compare full multi-agent pipeline vs. single-LLM prompting on held-out subset to quantify whether agent separation provides measurable gains

## Open Questions the Paper Calls Out

**Open Question 1:** How resilient is the proposed multi-agent LLM framework against adversarial perturbations designed to evade detection? The authors state future work will include adversarial resistance testing against perturbed videos. Unresolved because current study evaluates only organic content. Evidence needed: Detection accuracy on adversarially perturbed SocialDF versions.

**Open Question 2:** Can the framework maintain high detection accuracy for non-public figures or "long-tail" individuals not present in the pre-existing database? Limitations section notes focus on celebrities may restrict generalization. Unresolved because framework relies on 869-person database for identity matching. Evidence needed: Performance metrics on deepfakes featuring individuals excluded from facial recognition database.

**Open Question 3:** How do other state-of-the-art detection paradigms perform when benchmarked on the SocialDF dataset? Authors note plan to benchmark additional SOTA models beyond LipFD. Unresolved because paper compares primarily against lip-sync specific model. Evidence needed: Comparative analysis table of various SOTA models on SocialDF.

## Limitations

**Dataset authenticity uncertainty:** While claiming real social media videos, there's no public verification that "manipulated" samples weren't artificially generated rather than repurposed content.

**Identity database vulnerability:** Framework's reliance on fixed 869-person database creates blind spot for non-celebrity figures, potentially causing systematic false negatives for private individuals.

**LLM dependence and latency:** 90.4% accuracy depends on DeepSeek R-1's web search capability and inference time; three sequential LLM calls and external searches make real-time deployment impractical.

## Confidence

- **90.4% accuracy claim (High):** Directly measured on SocialDF test set with specified methodology; ablation showing 89.5% for Llama 3.3 provides supporting evidence.
- **LipFD 51.24% accuracy claim (High):** Baseline comparison clearly demonstrates performance gap, assuming successful replication.
- **Content-level verification superiority (Medium):** Mechanism is sound but untested against deepfakes containing only plausible content.
- **Multi-agent decomposition benefit (Low):** Paper claims improvement but lacks direct ablation comparing single vs. multi-agent performance.

## Next Checks

1. **Prompt replication test:** Attempt to reproduce exact agent behavior by systematically testing different prompt formulations for three-agent system; measure consistency across temperature settings and verify whether 90.4% accuracy is reproducible.

2. **Identity database scalability test:** Evaluate pipeline performance on videos featuring non-celebrity speakers (randomly selected social media users not in 869-person database); measure false negative rate when identity matching fails.

3. **Plausible deepfake stress test:** Generate or curate subset of deepfakes containing factually plausible content (celebrities saying believable things); test whether LLM-based fact-checking correctly identifies these as manipulated or is vulnerable to high-quality contextually appropriate deepfakes.