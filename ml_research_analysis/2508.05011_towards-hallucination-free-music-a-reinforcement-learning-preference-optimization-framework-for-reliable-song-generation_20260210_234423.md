---
ver: rpa2
title: 'Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization
  Framework for Reliable Song Generation'
arxiv_id: '2508.05011'
source_url: https://arxiv.org/abs/2508.05011
tags:
- reward
- preference
- arxiv
- songs
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in AI-driven
  lyric-to-song generation, where generated songs often deviate from input lyrics,
  undermining musical coherence. To mitigate this, the authors propose a reinforcement
  learning framework leveraging preference optimization based on phoneme error rate
  (PER) to align generated songs with lyrics.
---

# Towards Hallucination-Free Music: A Reinforcement Learning Preference Optimization Framework for Reliable Song Generation

## Quick Facts
- arXiv ID: 2508.05011
- Source URL: https://arxiv.org/abs/2508.05011
- Authors: Huaicheng Zhang; Wei Tan; Guangzheng Li; Yixuan Zhang; Hangting Chen; Shun Lei; Chenyu Yang; Zhiyong Wu; Shuai Wang; Qijun Huang; Dong Yu
- Reference count: 7
- Primary result: Achieves 16.5% reduction in hallucinated song samples and 38.47% increase in hallucination-free outputs using reinforcement learning preference optimization

## Executive Summary
This paper addresses a critical challenge in AI-driven music generation: hallucination, where generated songs deviate from input lyrics, compromising musical coherence. The authors propose a novel reinforcement learning framework that leverages preference optimization based on phoneme error rate (PER) to align generated songs with their corresponding lyrics. By developing a hallucination preference dataset through PER-guided filtering and implementing three optimization strategies (DPO, PPO, GRPO), the framework significantly reduces lyric-to-song misalignment while preserving musical quality. DPO with reject sampling emerges as the most effective approach, demonstrating both technical innovation and practical utility in creating more reliable music generation systems.

## Method Summary
The framework introduces a reinforcement learning approach to mitigate lyric-to-song hallucination in AI music generation. It begins by constructing a hallucination preference dataset using PER-based filtering to identify and rank instances of lyric misalignment. Three optimization strategies are then implemented: Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), and Group Relative Policy Optimization (GRPO). The preference optimization process involves pairwise comparisons between generated song pairs, where the model learns to prefer outputs with lower PER scores. DPO with reject sampling proves most effective, achieving significant reductions in hallucinated outputs while maintaining musical quality. The framework demonstrates transferability to other music generation tasks beyond the initial implementation.

## Key Results
- Achieves 16.5% reduction in hallucinated song samples compared to baseline models
- Increases hallucination-free outputs by 38.47% while maintaining musical quality
- Demonstrates 9.60% improvement in reward score metrics using PER-based evaluation
- Shows successful transferability to other music generation tasks beyond lyric-to-song conversion

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to directly optimize for phonetic alignment between lyrics and generated music through preference learning. By using PER as a quantifiable metric for hallucination, the system can systematically identify and reduce instances where the generated song deviates from the input lyrics. The preference optimization approach allows the model to learn from pairwise comparisons rather than absolute quality judgments, making the training process more stable and effective. The combination of PER-guided data filtering with reinforcement learning creates a feedback loop that progressively improves alignment between lyrics and music generation.

## Foundational Learning
**Reinforcement Learning**: Why needed - To enable the model to learn from preferences rather than fixed labels, allowing for nuanced optimization of musical generation. Quick check - Verify that the reward function properly captures both phonetic alignment and musical quality.

**Preference Optimization**: Why needed - To handle the subjective nature of music quality by learning from pairwise comparisons rather than absolute scores. Quick check - Ensure the preference dataset contains diverse and representative examples of good and bad alignments.

**Phoneme Error Rate (PER)**: Why needed - To provide an objective, quantifiable metric for measuring lyric-to-song alignment accuracy. Quick check - Validate that PER calculations accurately reflect human perception of lyric coherence.

**Data Filtering Techniques**: Why needed - To create a high-quality preference dataset by identifying and removing hallucinated or misaligned examples. Quick check - Confirm that filtering doesn't remove stylistically valid but phonetically divergent examples.

**Multi-strategy Optimization**: Why needed - To compare different reinforcement learning approaches and identify the most effective method for this specific task. Quick check - Verify that all three strategies (DPO, PPO, GRPO) are implemented correctly and fairly compared.

## Architecture Onboarding

**Component Map**: Lyrics -> PER Analysis -> Preference Dataset -> RL Model (DPO/PPO/GRPO) -> Generated Music -> PER Evaluation -> Feedback Loop

**Critical Path**: Input lyrics undergo PER analysis to create preference pairs, which train the RL model. The trained model generates music, which is evaluated using PER, creating a closed optimization loop.

**Design Tradeoffs**: The framework prioritizes phonetic alignment over semantic meaning, potentially missing cases where lyrics are pronounced correctly but contextually inappropriate. The PER-based approach may exclude stylistically valid variations that deviate from exact phonetic matches but remain musically coherent.

**Failure Signatures**: If the framework fails, it may show: (1) over-correction leading to robotic-sounding music, (2) PER improvements without corresponding gains in human perceptual quality, (3) inability to handle out-of-domain lyrics or music styles, or (4) degradation in other musical qualities like melody complexity or rhythm variation.

**First Experiments**:
1. Test DPO with reject sampling on a held-out validation set to confirm reproducibility of reported gains
2. Compare PER-based evaluation against human perceptual ratings to validate the metric's effectiveness
3. Apply the framework to a different music generation task (e.g., melody generation from chords) to test transferability claims

## Open Questions the Paper Calls Out
The paper acknowledges that its PER-based approach focuses on phoneme-level alignment rather than semantic meaning, potentially missing cases where lyrics are pronounced correctly but contextually inappropriate. The authors also note that the training data filtering process using PER-guided sampling may inadvertently exclude stylistically valid variations that deviate from exact phonetic matches but remain musically coherent.

## Limitations
- Performance gains are measured primarily against PER-based hallucination metrics rather than human perceptual evaluation
- Focus on phoneme-level alignment may miss semantically inappropriate but phonetically correct lyrics
- PER-guided filtering may exclude stylistically valid musical variations that deviate from exact phonetic matches
- Reliance on a single automated metric introduces uncertainty about real-world applicability

## Confidence
- **High Confidence**: The technical implementation of the reinforcement learning framework and preference optimization methods is sound and reproducible
- **Medium Confidence**: The quantitative improvements in PER-based hallucination metrics are reliable, but their translation to perceptual quality remains uncertain
- **Medium Confidence**: The transferability claim to other music generation tasks is plausible but requires empirical validation

## Next Checks
1. Conduct human evaluation studies comparing DPO with reject sampling against baseline models, focusing on both phonetic accuracy and musical coherence
2. Test the framework's performance on out-of-domain lyrics and music styles not represented in the training data
3. Evaluate the trade-off between hallucination reduction and other musical qualities (melody complexity, rhythm variation) to ensure no unintended degradation occurs