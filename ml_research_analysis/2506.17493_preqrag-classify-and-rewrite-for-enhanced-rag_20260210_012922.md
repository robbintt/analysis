---
ver: rpa2
title: PreQRAG -- Classify and Rewrite for Enhanced RAG
arxiv_id: '2506.17493'
source_url: https://arxiv.org/abs/2506.17493
tags:
- question
- retrieval
- questions
- context
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PreQRAG, a retrieval-augmented generation
  system that improves performance by classifying and rewriting questions before retrieval.
  It distinguishes between single-document and multi-document questions, using question
  rewriting for single-document queries and decomposition for multi-document queries
  to optimize retrieval.
---

# PreQRAG -- Classify and Rewrite for Enhanced RAG

## Quick Facts
- arXiv ID: 2506.17493
- Source URL: https://arxiv.org/abs/2506.17493
- Authors: Damian Martinez; Catalina Riano; Hui Fang
- Reference count: 13
- Primary result: Second place in SIGIR 2025 LiveRAG Challenge with 0.977 equivalence, 0.884 relevance, and 0.9419 faithfulness scores

## Executive Summary
PreQRAG is a retrieval-augmented generation system that improves RAG performance by classifying and preprocessing questions before retrieval. The system distinguishes between single-document and multi-document questions, applying different preprocessing strategies to each type. Single-document questions undergo retrieval-optimized rewriting while multi-document questions are decomposed into sub-questions. This targeted preprocessing, combined with hybrid retrieval and cross-encoder reranking, significantly improves retrieval quality and answer generation. The system achieved second place in the SIGIR 2025 LiveRAG Challenge, demonstrating the effectiveness of question-type-aware preprocessing.

## Method Summary
PreQRAG employs a five-stage pipeline: question classification (single-document vs. multi-document), query rewriting/decomposition using Falcon3-10B-Instruct, hybrid retrieval combining BM25 sparse and E5-base dense retrieval, cross-encoder reranking with bge-reranker-v2, and answer generation with Falcon3-10B-Instruct using greedy decoding and TOP-3 context. The system uses a rule-based classifier achieving 95.3% multi-doc accuracy and generates retrieval-model-specific rewrites optimized for either sparse (keyword-rich) or dense (semantic-focused) retrieval. For multi-document questions, the system decomposes queries into two sub-questions for independent retrieval and aggregation.

## Key Results
- Second place in SIGIR 2025 LiveRAG Challenge
- Achieved 0.977 equivalence, 0.884 relevance, and 0.9419 faithfulness scores
- MRR improvements of 13.34% (sparse) and 14.2% (dense) from query rewriting
- Top-10 retrieval improved from 41% to 55% (sparse) and 36% to 56% (dense)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question-type-aware preprocessing improves retrieval quality by matching retrieval strategy to question structure.
- Mechanism: Classifying questions as single-document vs. multi-document enables differentiated handling—single-doc queries get retrieval-optimized rewrites, multi-doc queries get decomposed into sub-questions that can be independently retrieved and aggregated.
- Core assumption: Different question types have fundamentally different retrieval requirements that cannot be optimally served by a single unified approach.
- Evidence anchors:
  - [abstract] "PreQRAG incorporates a pipeline that first classifies each input question as either single-document or multi-document type... This classification and rewriting strategy improves the RAG performance."
  - [section 2.2.1] "Sparse-optimized rewrite achieved a 13.34% improvement in average MRR... dense-optimized rewrite resulted in a 14.2% increase in MRR"
  - [section 2.2.2] "Top-10 retrieval improved from 41% to 55% [sparse]... dense retrieval performance at Top-10 rose from 36% to 56%"
  - [corpus] LiveRAG Challenge Report and TopClustRAG submissions also employed hybrid retrieval, suggesting convergence on this approach among top performers.

### Mechanism 2
- Claim: Retrieval-model-specific query rewriting optimizes alignment between query formulation and retrieval model characteristics.
- Mechanism: Sparse retrieval (BM25) benefits from keyword-rich, search-engine-style phrasing; dense retrieval benefits from semantically focused, premise-stripped queries. Generating two distinct rewrites per question allows each retrieval model to operate on optimally-formulated input.
- Core assumption: The same query formulation cannot simultaneously maximize performance across fundamentally different retrieval paradigms (lexical matching vs. semantic embedding).
- Evidence anchors:
  - [section 2.2.1] "The first rewriting prompt is designed to produce a version optimized for web-style search, aiming to enhance performance with BM25-based sparse retrieval. The second rewriting prompt focuses on dense retrieval, instructing the LLM to remove unnecessary premises and incorporate key terms directly."
  - [section 2.3] "some questions were better served by either the dense or the sparse retrieval model, depending on their structure and content"
  - [corpus] TopClustRAG and other LiveRAG submissions similarly combined sparse and dense retrieval, but PreQRAG's query-differentiation strategy appears unique.

### Mechanism 3
- Claim: Hybrid retrieval with cross-encoder reranking captures complementary relevance signals that neither sparse nor dense retrieval provides alone.
- Mechanism: Sparse retrieval captures exact term matches; dense retrieval captures semantic similarity. Merging candidate sets increases recall, while cross-encoder reranking provides fine-grained relevance scoring that evaluates question-document pairs jointly rather than independently.
- Core assumption: Initial retrieval stages (sparse/dense) produce sufficiently high recall that reranking can identify truly relevant documents from the candidate pool.
- Evidence anchors:
  - [section 2.4] "bge-reranker-v2, a cross-encoder model that jointly encodes both the question and each candidate document to produce highly accurate relevance scores"
  - [table 2 vs. table 3] Hybrid + Rerank improved Top-1 from 37.5% to 64.2% (single-doc) and from 32% to 42% (multi-doc)
  - [corpus] Multiple LiveRAG submissions (RMIT-ADM+S, TopClustRAG, HLTCOE) employed hybrid retrieval strategies, suggesting this is a validated pattern.

## Foundational Learning

- Concept: **Sparse vs. Dense Retrieval Paradigms**
  - Why needed here: PreQRAG's hybrid approach requires understanding that BM25 matches on term frequency/inverse document frequency while dense retrievers match on semantic embeddings—each has distinct failure modes.
  - Quick check question: Given a query with typos but clear intent, which retrieval paradigm would likely perform better, and why?

- Concept: **Cross-Encoder vs. Bi-Encoder Architectures**
  - Why needed here: Reranking uses a cross-encoder (bge-reranker-v2) which jointly processes query-document pairs; understanding this distinction explains why reranking is more accurate but computationally expensive.
  - Quick check question: Why can't the cross-encoder be used for initial retrieval over the full corpus?

- Concept: **Query Decomposition for Multi-Hop Reasoning**
  - Why needed here: Multi-document questions require retrieving evidence from multiple sources; decomposition allows independent retrieval before synthesis.
  - Quick check question: What happens if decomposed sub-questions have overlapping or contradictory evidence requirements?

## Architecture Onboarding

- Component map:
  Question Classification -> Question Rewriting/Decomposition -> Hybrid Retrieval (BM25 + Dense) -> Reranking (bge-reranker-v2) -> Answer Generation (Falcon3-10B-Instruct)

- Critical path:
  1. Classification accuracy directly determines which preprocessing path is triggered—errors here propagate downstream
  2. Retrieval recall sets upper bound on answer quality; reranking only reorders what was retrieved
  3. Context length (TOP-K documents) affects generation quality—too few misses evidence, too many introduces noise

- Design tradeoffs:
  - Rule-based vs. LLM classification: Rule-based achieved 95.3% multi-doc accuracy vs. 73.0% for Falcon, but may lack generalization to novel query types
  - TOP-3 vs. TOP-10 context: Paper selected TOP-3 for efficiency; TOP-5/TOP-10 showed comparable metrics but higher computational cost
  - Non-sampling (greedy) decoding: Higher faithfulness and reproducibility, but sacrifices output diversity

- Failure signatures:
  - Multi-doc misclassified as single-doc → insufficient context → incomplete answers
  - Query rewrite drift → semantic meaning altered, retrieval returns wrong documents
  - Low retrieval recall → reranking receives no relevant candidates, answer grounded in irrelevant context
  - Context overload (high TOP-K) → model generates less concise answers, may conflate conflicting information

- First 3 experiments:
  1. Validate classification accuracy on your domain: Create labeled single-doc/multi-doc test set; compare rule-based vs. LLM classifier performance on your query distribution
  2. Measure retrieval improvement from query rewriting: A/B test original vs. rewritten queries on both sparse and dense indices; compute MRR and recall@K
  3. Establish reranking ROI: Compare answer quality (equivalence, relevance, faithfulness) with and without reranking step; measure latency impact to determine if quality gain justifies cost

## Open Questions the Paper Calls Out

- **Question 1**: Can a classifier be developed to predict whether a query is better suited for dense or sparse retrieval, rather than always using hybrid retrieval?
  - Basis: The authors note "we observed that certain questions are better represented using either dense or sparse retrieval methods" but don't explore selective retrieval.
  - Why unresolved: Hybrid retrieval is computationally expensive; selective use could improve efficiency.
  - What evidence would resolve it: Comparative study showing selective retrieval maintains accuracy while reducing computational cost.

- **Question 2**: Would adaptive query decomposition (varying sub-question count based on query complexity) improve performance over fixed two-sub-question approach?
  - Basis: Paper always uses exactly two sub-questions for multi-doc queries, but complex queries might need more decomposition.
  - Why unresolved: Fixed decomposition may over-split simple queries or under-split complex ones.
  - What evidence would resolve it: Experiments comparing fixed vs. adaptive decomposition across varying complexity queries.

- **Question 3**: Why does the rule-based classifier significantly outperform the LLM-based classifier (95.3% vs 73.0% accuracy) for multi-document questions?
  - Basis: Table 1 shows this substantial gap without explaining the underlying cause.
  - Why unresolved: Understanding this counterintuitive finding could inform better LLM prompting strategies.
  - What evidence would resolve it: Error analysis comparing misclassifications and probing experiments on feature capture differences.

## Limitations

- Rule-based classifier implementation details are not specified, making faithful reproduction challenging
- Exact hybrid retrieval fusion strategy (how sparse and dense results are merged) is unspecified
- Number of candidates retrieved from each index before reranking is not documented

## Confidence

- **High Confidence**: Core architectural components (hybrid retrieval, cross-encoder reranking, question rewriting) are well-established with strong empirical support
- **Medium Confidence**: Question-type-aware preprocessing effectiveness relies on specific performance metrics but lacks implementation details for the key rule-based classifier
- **Medium Confidence**: Query rewriting optimization claims are supported by MRR improvements but exact prompt formulations are not provided

## Next Checks

1. Implement multiple rule-based classification strategies using common multi-doc indicators and benchmark their accuracy on a labeled test set to achieve the reported 95.3% multi-doc accuracy

2. Test different candidate fusion strategies (reciprocal rank fusion, score-weighted averaging, simple concatenation) on a validation set to determine which maximizes Top-1 answer accuracy when combined with reranking

3. Conduct an ablation study measuring MRR@10 and recall@K differences between original queries and both sparse-optimized and dense-optimized rewrites to quantify each rewriting strategy's contribution