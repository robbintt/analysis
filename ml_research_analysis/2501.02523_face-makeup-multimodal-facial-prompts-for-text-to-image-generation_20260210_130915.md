---
ver: rpa2
title: 'Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation'
arxiv_id: '2501.02523'
source_url: https://arxiv.org/abs/2501.02523
tags:
- facial
- image
- images
- text
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Face-MakeUp, a method for generating high-quality
  facial images using multimodal prompts. The authors address the challenge of generating
  desired facial images using only text prompts by leveraging both image and text
  inputs.
---

# Face-MakeUp: Multimodal Facial Prompts for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2501.02523
- Source URL: https://arxiv.org/abs/2501.02523
- Reference count: 27
- Key outcome: A multimodal approach that combines image and text prompts to generate high-quality facial images with improved identity preservation, outperforming existing methods on facial similarity, attribute prediction, and realism metrics.

## Executive Summary
Face-MakeUp addresses the challenge of generating desired facial images using only text prompts by leveraging both image and text inputs. The authors introduce a new large-scale dataset, FaceCaptionHQ-4M, and develop a specialized model that integrates multi-scale facial features, pose information, and text prompts into a diffusion model. The model uses facial encoders, including ArcFace, to extract identity features and a pose detector to maintain structural consistency. Experiments on two test datasets show that Face-MakeUp outperforms existing methods in facial similarity, attribute prediction, and realism, achieving state-of-the-art results across multiple metrics.

## Method Summary
Face-MakeUp is a multimodal conditioning approach for text-to-image generation that preserves facial identity while following text prompts. It uses a pre-trained Stable Diffusion v1.5 model and adds new conditioning mechanisms: a face-ID cross-attention module that fuses features from both CLIP and ArcFace encoders, a pose detection and embedding system, and an additive fusion strategy. The model is trained on FaceCaptionHQ-4M, a cleaned dataset of 4.2M face image-text pairs. The training procedure involves freezing the base diffusion model and training only the new modules (PoseNet, projector, cross-attentions) for 500K steps with Adam optimizer.

## Key Results
- Outperforms existing methods on facial similarity metrics (FaceSim, DINO) on both Unsplash-Face and FaceCaption test sets
- Achieves state-of-the-art results across multiple metrics including CLIP-I, FID, Attr_c, and VLM-score
- Demonstrates strong performance in identity mixing and stylization applications
- Shows trade-off between high facial identity preservation and lower text-image alignment compared to competitors

## Why This Works (Mechanism)
The method works by combining specialized face recognition features (ArcFace) with general visual-language features (CLIP) through a cross-attention projector, then injecting these fused features into the diffusion model via dedicated cross-attention layers. The pose detector and PoseNet ensure structural consistency with the reference image. This multi-scale feature integration allows the model to maintain fine-grained facial details while following text prompts, overcoming the limitations of text-only conditioning.

## Foundational Learning
- **Concept: Stable Diffusion (Latent Diffusion Models)** - Why needed: This is the base generative model that Face-MakeUp modifies. Understanding its latent space, U-Net architecture, and cross-attention mechanism is essential to grasp how new conditioning (face, pose) is injected. Quick check: How does a cross-attention layer in a diffusion U-Net typically use a text prompt to condition the generated image?
- **Concept: Feature Extraction for Identity (ArcFace vs. CLIP)** - Why needed: The core innovation is combining general (CLIP) and specialized (ArcFace) features. One must understand that CLIP captures semantic alignment while ArcFace is optimized for identity discrimination. Quick check: Why would an embedding from a face recognition model (ArcFace) be better for preserving identity than an embedding from a general vision-language model (CLIP)?
- **Concept: Conditioning Diffusion Models** - Why needed: Face-MakeUp adds multiple new conditioning signals (face ID, pose) on top of text. Understanding methods like ControlNet or adapter layers is key to understanding the paper's PoseNet and fusion strategy. Quick check: What are the different ways to inject an external condition (like a pose map) into a pre-trained diffusion model (e.g., concatenation, cross-attention, additive)?

## Architecture Onboarding
- **Component map:** Reference Image -> Image Encoder (CLIP ViT-H/14) + Face Encoder (ArcFace) + Pose Detector + PoseNet -> Projector (cross-attention fusion) -> Face-ID Cross-Attn modules in SD U-Net + Text Encoder (CLIP) -> Stable Diffusion v1.5 U-Net
- **Critical path:** The novel path is the dual-encoder face feature extraction. The model must successfully combine the general CLIP features with the discriminative ArcFace features via the Projector before feeding them into the new Face-ID Cross-Attn modules within the frozen SD U-Net
- **Design tradeoffs:** Uses multiple encoders (CLIP, ArcFace, PoseNet) which increases inference complexity compared to a single-adapter approach, but is shown to improve ID fidelity. Text fidelity vs. ID preservation: manual evaluation shows lower text fidelity compared to other methods, suggesting the strong ID conditioning may sometimes override text instructions
- **Failure signatures:** Pose mismatch: generated face doesn't match the pose of the reference. Check PoseNet convergence or pose detector accuracy. Identity drift: generated face looks like a different person. Investigate the fusion between CLIP and ArcFace features or the training of the Face-ID Cross-Attn layers. Artifacting in non-face regions: as the model is optimized for faces, backgrounds or bodies might have more artifacts compared to general-purpose models
- **First 3 experiments:** 1) Reproduce main result: train or use provided Face-MakeUp checkpoint on CelebA-HQ references, generate with various prompts, compare ID similarity against IP-Adapter baseline. 2) Ablation on encoders: run inference using only CLIP features and then only ArcFace features to confirm contribution of each encoder. 3) PoseNet effectiveness: generate with and without pose map input using same random seed, compare structural alignment to reference

## Open Questions the Paper Calls Out
- **Open Question 1:** How can the trade-off between high facial identity preservation and low text-image alignment be mitigated in the Face-MakeUp architecture? The authors acknowledge that "In terms of image-text similarity, our model is slightly lower than other models," and manual evaluation confirms only 16.5% preference for text fidelity.
- **Open Question 2:** Can quantitative metrics be established to objectively evaluate the quality and balance of identity mixing? The paper states that for identity mixing, "due to the lack of metrics, we rely solely on visual observation."
- **Open Question 3:** Does the strict integration of pose information limit the model's generative flexibility when text prompts explicitly request expression or pose changes? It is unclear if the PoseNet overrides text-conditional control when the user desires a different facial angle or expression than the one extracted from the reference image.

## Limitations
- Lower text-image alignment compared to competitors due to strong prioritization of identity features
- Limited evaluation on non-facial regions and background generation quality
- Lack of quantitative metrics for identity mixing evaluation
- Dataset construction requires extensive filtering with unspecified thresholds

## Confidence
- **High confidence:** The core claim that multimodal conditioning improves facial identity preservation is well-supported by quantitative metrics and qualitative examples
- **Medium confidence:** Dataset quality claims are plausible but exact replication is uncertain due to unspecified filtering thresholds
- **Medium confidence:** Improvement over existing methods is demonstrated on reported benchmarks, but generalization to other datasets remains untested

## Next Checks
1. **Dataset fidelity check:** Reconstruct a 10K subset of FaceCaptionHQ using the described pipeline with reasonable default thresholds. Compare distribution of face attributes, image quality, and caption diversity to expected outputs
2. **Encoder contribution ablation:** Systematically disable either ArcFace or CLIP encoder during inference. Quantify impact on identity preservation (FaceSim) and text alignment (CLIP-T) to validate claimed synergy
3. **Pose conditioning validation:** Generate matched pairs (same seed) with and without pose conditioning on held-out references. Use keypoint detector to measure structural alignment between reference and generated faces